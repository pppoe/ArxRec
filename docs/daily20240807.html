<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240806.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "An Object is Worth 64x64 Pixels: Generating 3D Object via Image\n  Diffusion", "author": "Xingguang Yan and Han-Hung Lee and Ziyu Wan and Angel X. Chang", "abstract": "  We introduce a new approach for generating realistic 3D models with UV maps\nthrough a representation termed \"Object Images.\" This approach encapsulates\nsurface geometry, appearance, and patch structures within a 64x64 pixel image,\neffectively converting complex 3D shapes into a more manageable 2D format. By\ndoing so, we address the challenges of both geometric and semantic irregularity\ninherent in polygonal meshes. This method allows us to use image generation\nmodels, such as Diffusion Transformers, directly for 3D shape generation.\nEvaluated on the ABO dataset, our generated shapes with patch structures\nachieve point cloud FID comparable to recent 3D generative models, while\nnaturally supporting PBR material generation.\n", "link": "http://arxiv.org/abs/2408.03178v1", "date": "2024-08-06", "relevancy": 3.2081, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6474}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6474}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.63}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Object%20is%20Worth%2064x64%20Pixels%3A%20Generating%203D%20Object%20via%20Image%0A%20%20Diffusion&body=Title%3A%20An%20Object%20is%20Worth%2064x64%20Pixels%3A%20Generating%203D%20Object%20via%20Image%0A%20%20Diffusion%0AAuthor%3A%20Xingguang%20Yan%20and%20Han-Hung%20Lee%20and%20Ziyu%20Wan%20and%20Angel%20X.%20Chang%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20approach%20for%20generating%20realistic%203D%20models%20with%20UV%20maps%0Athrough%20a%20representation%20termed%20%22Object%20Images.%22%20This%20approach%20encapsulates%0Asurface%20geometry%2C%20appearance%2C%20and%20patch%20structures%20within%20a%2064x64%20pixel%20image%2C%0Aeffectively%20converting%20complex%203D%20shapes%20into%20a%20more%20manageable%202D%20format.%20By%0Adoing%20so%2C%20we%20address%20the%20challenges%20of%20both%20geometric%20and%20semantic%20irregularity%0Ainherent%20in%20polygonal%20meshes.%20This%20method%20allows%20us%20to%20use%20image%20generation%0Amodels%2C%20such%20as%20Diffusion%20Transformers%2C%20directly%20for%203D%20shape%20generation.%0AEvaluated%20on%20the%20ABO%20dataset%2C%20our%20generated%20shapes%20with%20patch%20structures%0Aachieve%20point%20cloud%20FID%20comparable%20to%20recent%203D%20generative%20models%2C%20while%0Anaturally%20supporting%20PBR%20material%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Object%2520is%2520Worth%252064x64%2520Pixels%253A%2520Generating%25203D%2520Object%2520via%2520Image%250A%2520%2520Diffusion%26entry.906535625%3DXingguang%2520Yan%2520and%2520Han-Hung%2520Lee%2520and%2520Ziyu%2520Wan%2520and%2520Angel%2520X.%2520Chang%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520approach%2520for%2520generating%2520realistic%25203D%2520models%2520with%2520UV%2520maps%250Athrough%2520a%2520representation%2520termed%2520%2522Object%2520Images.%2522%2520This%2520approach%2520encapsulates%250Asurface%2520geometry%252C%2520appearance%252C%2520and%2520patch%2520structures%2520within%2520a%252064x64%2520pixel%2520image%252C%250Aeffectively%2520converting%2520complex%25203D%2520shapes%2520into%2520a%2520more%2520manageable%25202D%2520format.%2520By%250Adoing%2520so%252C%2520we%2520address%2520the%2520challenges%2520of%2520both%2520geometric%2520and%2520semantic%2520irregularity%250Ainherent%2520in%2520polygonal%2520meshes.%2520This%2520method%2520allows%2520us%2520to%2520use%2520image%2520generation%250Amodels%252C%2520such%2520as%2520Diffusion%2520Transformers%252C%2520directly%2520for%25203D%2520shape%2520generation.%250AEvaluated%2520on%2520the%2520ABO%2520dataset%252C%2520our%2520generated%2520shapes%2520with%2520patch%2520structures%250Aachieve%2520point%2520cloud%2520FID%2520comparable%2520to%2520recent%25203D%2520generative%2520models%252C%2520while%250Anaturally%2520supporting%2520PBR%2520material%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Object%20is%20Worth%2064x64%20Pixels%3A%20Generating%203D%20Object%20via%20Image%0A%20%20Diffusion&entry.906535625=Xingguang%20Yan%20and%20Han-Hung%20Lee%20and%20Ziyu%20Wan%20and%20Angel%20X.%20Chang&entry.1292438233=%20%20We%20introduce%20a%20new%20approach%20for%20generating%20realistic%203D%20models%20with%20UV%20maps%0Athrough%20a%20representation%20termed%20%22Object%20Images.%22%20This%20approach%20encapsulates%0Asurface%20geometry%2C%20appearance%2C%20and%20patch%20structures%20within%20a%2064x64%20pixel%20image%2C%0Aeffectively%20converting%20complex%203D%20shapes%20into%20a%20more%20manageable%202D%20format.%20By%0Adoing%20so%2C%20we%20address%20the%20challenges%20of%20both%20geometric%20and%20semantic%20irregularity%0Ainherent%20in%20polygonal%20meshes.%20This%20method%20allows%20us%20to%20use%20image%20generation%0Amodels%2C%20such%20as%20Diffusion%20Transformers%2C%20directly%20for%203D%20shape%20generation.%0AEvaluated%20on%20the%20ABO%20dataset%2C%20our%20generated%20shapes%20with%20patch%20structures%0Aachieve%20point%20cloud%20FID%20comparable%20to%20recent%203D%20generative%20models%2C%20while%0Anaturally%20supporting%20PBR%20material%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03178v1&entry.124074799=Read"},
{"title": "BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical\n  Applications", "author": "G. Manni and C. Lauretti and F. Prata and R. Papalia and L. Zollo and P. Soda", "abstract": "  Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Simultaneous\nLocalization and Mapping (SLAM) has emerged as a promising solution to address\nthese limitations, its implementation in endoscopic procedures presents\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents a\nrobust deep learning-based SLAM approach that combines state-of-the-art and\nnewly developed models. It consists of three main parts: the Monocular Pose\nEstimation Module that introduces a novel unsupervised method based on the\nCycleGAN architecture, the Monocular Depth Estimation Module that leverages the\nnovel Zoe architecture, and the 3D Reconstruction Module which uses information\nfrom the previous models to create a coherent surgical map. The performance of\nthe procedure was rigorously evaluated using three publicly available datasets\n(Hamlyn, EndoSLAM, and SCARED) and benchmarked against two state-of-the-art\nmethods, EndoSFMLearner and EndoDepth. The integration of Zoe in the MDEM\ndemonstrated superior performance compared to state-of-the-art depth estimation\nalgorithms in endoscopy, whereas the novel approach in the MPEM exhibited\ncompetitive performance and the lowest inference time. The results showcase the\nrobustness of our approach in laparoscopy, gastroscopy, and colonoscopy, three\ndifferent scenarios in endoscopic surgery. The proposed SLAM approach has the\npotential to improve the accuracy and efficiency of endoscopic procedures by\nproviding surgeons with enhanced depth perception and 3D reconstruction\ncapabilities.\n", "link": "http://arxiv.org/abs/2408.03078v1", "date": "2024-08-06", "relevancy": 3.0139, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6209}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.603}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BodySLAM%3A%20A%20Generalized%20Monocular%20Visual%20SLAM%20Framework%20for%20Surgical%0A%20%20Applications&body=Title%3A%20BodySLAM%3A%20A%20Generalized%20Monocular%20Visual%20SLAM%20Framework%20for%20Surgical%0A%20%20Applications%0AAuthor%3A%20G.%20Manni%20and%20C.%20Lauretti%20and%20F.%20Prata%20and%20R.%20Papalia%20and%20L.%20Zollo%20and%20P.%20Soda%0AAbstract%3A%20%20%20Endoscopic%20surgery%20relies%20on%20two-dimensional%20views%2C%20posing%20challenges%20for%0Asurgeons%20in%20depth%20perception%20and%20instrument%20manipulation.%20While%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20has%20emerged%20as%20a%20promising%20solution%20to%20address%0Athese%20limitations%2C%20its%20implementation%20in%20endoscopic%20procedures%20presents%0Asignificant%20challenges%20due%20to%20hardware%20limitations%2C%20such%20as%20the%20use%20of%20a%0Amonocular%20camera%20and%20the%20absence%20of%20odometry%20sensors.%20This%20study%20presents%20a%0Arobust%20deep%20learning-based%20SLAM%20approach%20that%20combines%20state-of-the-art%20and%0Anewly%20developed%20models.%20It%20consists%20of%20three%20main%20parts%3A%20the%20Monocular%20Pose%0AEstimation%20Module%20that%20introduces%20a%20novel%20unsupervised%20method%20based%20on%20the%0ACycleGAN%20architecture%2C%20the%20Monocular%20Depth%20Estimation%20Module%20that%20leverages%20the%0Anovel%20Zoe%20architecture%2C%20and%20the%203D%20Reconstruction%20Module%20which%20uses%20information%0Afrom%20the%20previous%20models%20to%20create%20a%20coherent%20surgical%20map.%20The%20performance%20of%0Athe%20procedure%20was%20rigorously%20evaluated%20using%20three%20publicly%20available%20datasets%0A%28Hamlyn%2C%20EndoSLAM%2C%20and%20SCARED%29%20and%20benchmarked%20against%20two%20state-of-the-art%0Amethods%2C%20EndoSFMLearner%20and%20EndoDepth.%20The%20integration%20of%20Zoe%20in%20the%20MDEM%0Ademonstrated%20superior%20performance%20compared%20to%20state-of-the-art%20depth%20estimation%0Aalgorithms%20in%20endoscopy%2C%20whereas%20the%20novel%20approach%20in%20the%20MPEM%20exhibited%0Acompetitive%20performance%20and%20the%20lowest%20inference%20time.%20The%20results%20showcase%20the%0Arobustness%20of%20our%20approach%20in%20laparoscopy%2C%20gastroscopy%2C%20and%20colonoscopy%2C%20three%0Adifferent%20scenarios%20in%20endoscopic%20surgery.%20The%20proposed%20SLAM%20approach%20has%20the%0Apotential%20to%20improve%20the%20accuracy%20and%20efficiency%20of%20endoscopic%20procedures%20by%0Aproviding%20surgeons%20with%20enhanced%20depth%20perception%20and%203D%20reconstruction%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBodySLAM%253A%2520A%2520Generalized%2520Monocular%2520Visual%2520SLAM%2520Framework%2520for%2520Surgical%250A%2520%2520Applications%26entry.906535625%3DG.%2520Manni%2520and%2520C.%2520Lauretti%2520and%2520F.%2520Prata%2520and%2520R.%2520Papalia%2520and%2520L.%2520Zollo%2520and%2520P.%2520Soda%26entry.1292438233%3D%2520%2520Endoscopic%2520surgery%2520relies%2520on%2520two-dimensional%2520views%252C%2520posing%2520challenges%2520for%250Asurgeons%2520in%2520depth%2520perception%2520and%2520instrument%2520manipulation.%2520While%2520Simultaneous%250ALocalization%2520and%2520Mapping%2520%2528SLAM%2529%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520to%2520address%250Athese%2520limitations%252C%2520its%2520implementation%2520in%2520endoscopic%2520procedures%2520presents%250Asignificant%2520challenges%2520due%2520to%2520hardware%2520limitations%252C%2520such%2520as%2520the%2520use%2520of%2520a%250Amonocular%2520camera%2520and%2520the%2520absence%2520of%2520odometry%2520sensors.%2520This%2520study%2520presents%2520a%250Arobust%2520deep%2520learning-based%2520SLAM%2520approach%2520that%2520combines%2520state-of-the-art%2520and%250Anewly%2520developed%2520models.%2520It%2520consists%2520of%2520three%2520main%2520parts%253A%2520the%2520Monocular%2520Pose%250AEstimation%2520Module%2520that%2520introduces%2520a%2520novel%2520unsupervised%2520method%2520based%2520on%2520the%250ACycleGAN%2520architecture%252C%2520the%2520Monocular%2520Depth%2520Estimation%2520Module%2520that%2520leverages%2520the%250Anovel%2520Zoe%2520architecture%252C%2520and%2520the%25203D%2520Reconstruction%2520Module%2520which%2520uses%2520information%250Afrom%2520the%2520previous%2520models%2520to%2520create%2520a%2520coherent%2520surgical%2520map.%2520The%2520performance%2520of%250Athe%2520procedure%2520was%2520rigorously%2520evaluated%2520using%2520three%2520publicly%2520available%2520datasets%250A%2528Hamlyn%252C%2520EndoSLAM%252C%2520and%2520SCARED%2529%2520and%2520benchmarked%2520against%2520two%2520state-of-the-art%250Amethods%252C%2520EndoSFMLearner%2520and%2520EndoDepth.%2520The%2520integration%2520of%2520Zoe%2520in%2520the%2520MDEM%250Ademonstrated%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520depth%2520estimation%250Aalgorithms%2520in%2520endoscopy%252C%2520whereas%2520the%2520novel%2520approach%2520in%2520the%2520MPEM%2520exhibited%250Acompetitive%2520performance%2520and%2520the%2520lowest%2520inference%2520time.%2520The%2520results%2520showcase%2520the%250Arobustness%2520of%2520our%2520approach%2520in%2520laparoscopy%252C%2520gastroscopy%252C%2520and%2520colonoscopy%252C%2520three%250Adifferent%2520scenarios%2520in%2520endoscopic%2520surgery.%2520The%2520proposed%2520SLAM%2520approach%2520has%2520the%250Apotential%2520to%2520improve%2520the%2520accuracy%2520and%2520efficiency%2520of%2520endoscopic%2520procedures%2520by%250Aproviding%2520surgeons%2520with%2520enhanced%2520depth%2520perception%2520and%25203D%2520reconstruction%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BodySLAM%3A%20A%20Generalized%20Monocular%20Visual%20SLAM%20Framework%20for%20Surgical%0A%20%20Applications&entry.906535625=G.%20Manni%20and%20C.%20Lauretti%20and%20F.%20Prata%20and%20R.%20Papalia%20and%20L.%20Zollo%20and%20P.%20Soda&entry.1292438233=%20%20Endoscopic%20surgery%20relies%20on%20two-dimensional%20views%2C%20posing%20challenges%20for%0Asurgeons%20in%20depth%20perception%20and%20instrument%20manipulation.%20While%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20has%20emerged%20as%20a%20promising%20solution%20to%20address%0Athese%20limitations%2C%20its%20implementation%20in%20endoscopic%20procedures%20presents%0Asignificant%20challenges%20due%20to%20hardware%20limitations%2C%20such%20as%20the%20use%20of%20a%0Amonocular%20camera%20and%20the%20absence%20of%20odometry%20sensors.%20This%20study%20presents%20a%0Arobust%20deep%20learning-based%20SLAM%20approach%20that%20combines%20state-of-the-art%20and%0Anewly%20developed%20models.%20It%20consists%20of%20three%20main%20parts%3A%20the%20Monocular%20Pose%0AEstimation%20Module%20that%20introduces%20a%20novel%20unsupervised%20method%20based%20on%20the%0ACycleGAN%20architecture%2C%20the%20Monocular%20Depth%20Estimation%20Module%20that%20leverages%20the%0Anovel%20Zoe%20architecture%2C%20and%20the%203D%20Reconstruction%20Module%20which%20uses%20information%0Afrom%20the%20previous%20models%20to%20create%20a%20coherent%20surgical%20map.%20The%20performance%20of%0Athe%20procedure%20was%20rigorously%20evaluated%20using%20three%20publicly%20available%20datasets%0A%28Hamlyn%2C%20EndoSLAM%2C%20and%20SCARED%29%20and%20benchmarked%20against%20two%20state-of-the-art%0Amethods%2C%20EndoSFMLearner%20and%20EndoDepth.%20The%20integration%20of%20Zoe%20in%20the%20MDEM%0Ademonstrated%20superior%20performance%20compared%20to%20state-of-the-art%20depth%20estimation%0Aalgorithms%20in%20endoscopy%2C%20whereas%20the%20novel%20approach%20in%20the%20MPEM%20exhibited%0Acompetitive%20performance%20and%20the%20lowest%20inference%20time.%20The%20results%20showcase%20the%0Arobustness%20of%20our%20approach%20in%20laparoscopy%2C%20gastroscopy%2C%20and%20colonoscopy%2C%20three%0Adifferent%20scenarios%20in%20endoscopic%20surgery.%20The%20proposed%20SLAM%20approach%20has%20the%0Apotential%20to%20improve%20the%20accuracy%20and%20efficiency%20of%20endoscopic%20procedures%20by%0Aproviding%20surgeons%20with%20enhanced%20depth%20perception%20and%203D%20reconstruction%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03078v1&entry.124074799=Read"},
{"title": "CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking", "author": "Nicolas Baumann and Michael Baumgartner and Edoardo Ghignone and Jonas K\u00fchne and Tobias Fischer and Yung-Hsu Yang and Marc Pollefeys and Michele Magno", "abstract": "  To enable self-driving vehicles accurate detection and tracking of\nsurrounding objects is essential. While Light Detection and Ranging (LiDAR)\nsensors have set the benchmark for high-performance systems, the appeal of\ncamera-only solutions lies in their cost-effectiveness. Notably, despite the\nprevalent use of Radio Detection and Ranging (RADAR) sensors in automotive\nsystems, their potential in 3D detection and tracking has been largely\ndisregarded due to data sparsity and measurement noise. As a recent\ndevelopment, the combination of RADARs and cameras is emerging as a promising\nsolution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a\ncamera-RADAR fusion model for 3D object detection, and Multi-Object Tracking\n(MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only\nBEVDet architecture, CR3DT demonstrates substantial improvements in both\ndetection and tracking capabilities, by incorporating the spatial and velocity\ninformation of the RADAR sensor. Experimental results demonstrate an absolute\nimprovement in detection performance of 5.3% in mean Average Precision (mAP)\nand a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the\nnuScenes dataset when leveraging both modalities. CR3DT bridges the gap between\nhigh-performance and cost-effective perception systems in autonomous driving,\nby capitalizing on the ubiquitous presence of RADAR in automotive applications.\nThe code is available at: https://github.com/ETH-PBL/CR3DT.\n", "link": "http://arxiv.org/abs/2403.15313v2", "date": "2024-08-06", "relevancy": 3.0063, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6028}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6005}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CR3DT%3A%20Camera-RADAR%20Fusion%20for%203D%20Detection%20and%20Tracking&body=Title%3A%20CR3DT%3A%20Camera-RADAR%20Fusion%20for%203D%20Detection%20and%20Tracking%0AAuthor%3A%20Nicolas%20Baumann%20and%20Michael%20Baumgartner%20and%20Edoardo%20Ghignone%20and%20Jonas%20K%C3%BChne%20and%20Tobias%20Fischer%20and%20Yung-Hsu%20Yang%20and%20Marc%20Pollefeys%20and%20Michele%20Magno%0AAbstract%3A%20%20%20To%20enable%20self-driving%20vehicles%20accurate%20detection%20and%20tracking%20of%0Asurrounding%20objects%20is%20essential.%20While%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%0Asensors%20have%20set%20the%20benchmark%20for%20high-performance%20systems%2C%20the%20appeal%20of%0Acamera-only%20solutions%20lies%20in%20their%20cost-effectiveness.%20Notably%2C%20despite%20the%0Aprevalent%20use%20of%20Radio%20Detection%20and%20Ranging%20%28RADAR%29%20sensors%20in%20automotive%0Asystems%2C%20their%20potential%20in%203D%20detection%20and%20tracking%20has%20been%20largely%0Adisregarded%20due%20to%20data%20sparsity%20and%20measurement%20noise.%20As%20a%20recent%0Adevelopment%2C%20the%20combination%20of%20RADARs%20and%20cameras%20is%20emerging%20as%20a%20promising%0Asolution.%20This%20paper%20presents%20Camera-RADAR%203D%20Detection%20and%20Tracking%20%28CR3DT%29%2C%20a%0Acamera-RADAR%20fusion%20model%20for%203D%20object%20detection%2C%20and%20Multi-Object%20Tracking%0A%28MOT%29.%20Building%20upon%20the%20foundations%20of%20the%20State-of-the-Art%20%28SotA%29%20camera-only%0ABEVDet%20architecture%2C%20CR3DT%20demonstrates%20substantial%20improvements%20in%20both%0Adetection%20and%20tracking%20capabilities%2C%20by%20incorporating%20the%20spatial%20and%20velocity%0Ainformation%20of%20the%20RADAR%20sensor.%20Experimental%20results%20demonstrate%20an%20absolute%0Aimprovement%20in%20detection%20performance%20of%205.3%25%20in%20mean%20Average%20Precision%20%28mAP%29%0Aand%20a%2014.9%25%20increase%20in%20Average%20Multi-Object%20Tracking%20Accuracy%20%28AMOTA%29%20on%20the%0AnuScenes%20dataset%20when%20leveraging%20both%20modalities.%20CR3DT%20bridges%20the%20gap%20between%0Ahigh-performance%20and%20cost-effective%20perception%20systems%20in%20autonomous%20driving%2C%0Aby%20capitalizing%20on%20the%20ubiquitous%20presence%20of%20RADAR%20in%20automotive%20applications.%0AThe%20code%20is%20available%20at%3A%20https%3A//github.com/ETH-PBL/CR3DT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15313v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCR3DT%253A%2520Camera-RADAR%2520Fusion%2520for%25203D%2520Detection%2520and%2520Tracking%26entry.906535625%3DNicolas%2520Baumann%2520and%2520Michael%2520Baumgartner%2520and%2520Edoardo%2520Ghignone%2520and%2520Jonas%2520K%25C3%25BChne%2520and%2520Tobias%2520Fischer%2520and%2520Yung-Hsu%2520Yang%2520and%2520Marc%2520Pollefeys%2520and%2520Michele%2520Magno%26entry.1292438233%3D%2520%2520To%2520enable%2520self-driving%2520vehicles%2520accurate%2520detection%2520and%2520tracking%2520of%250Asurrounding%2520objects%2520is%2520essential.%2520While%2520Light%2520Detection%2520and%2520Ranging%2520%2528LiDAR%2529%250Asensors%2520have%2520set%2520the%2520benchmark%2520for%2520high-performance%2520systems%252C%2520the%2520appeal%2520of%250Acamera-only%2520solutions%2520lies%2520in%2520their%2520cost-effectiveness.%2520Notably%252C%2520despite%2520the%250Aprevalent%2520use%2520of%2520Radio%2520Detection%2520and%2520Ranging%2520%2528RADAR%2529%2520sensors%2520in%2520automotive%250Asystems%252C%2520their%2520potential%2520in%25203D%2520detection%2520and%2520tracking%2520has%2520been%2520largely%250Adisregarded%2520due%2520to%2520data%2520sparsity%2520and%2520measurement%2520noise.%2520As%2520a%2520recent%250Adevelopment%252C%2520the%2520combination%2520of%2520RADARs%2520and%2520cameras%2520is%2520emerging%2520as%2520a%2520promising%250Asolution.%2520This%2520paper%2520presents%2520Camera-RADAR%25203D%2520Detection%2520and%2520Tracking%2520%2528CR3DT%2529%252C%2520a%250Acamera-RADAR%2520fusion%2520model%2520for%25203D%2520object%2520detection%252C%2520and%2520Multi-Object%2520Tracking%250A%2528MOT%2529.%2520Building%2520upon%2520the%2520foundations%2520of%2520the%2520State-of-the-Art%2520%2528SotA%2529%2520camera-only%250ABEVDet%2520architecture%252C%2520CR3DT%2520demonstrates%2520substantial%2520improvements%2520in%2520both%250Adetection%2520and%2520tracking%2520capabilities%252C%2520by%2520incorporating%2520the%2520spatial%2520and%2520velocity%250Ainformation%2520of%2520the%2520RADAR%2520sensor.%2520Experimental%2520results%2520demonstrate%2520an%2520absolute%250Aimprovement%2520in%2520detection%2520performance%2520of%25205.3%2525%2520in%2520mean%2520Average%2520Precision%2520%2528mAP%2529%250Aand%2520a%252014.9%2525%2520increase%2520in%2520Average%2520Multi-Object%2520Tracking%2520Accuracy%2520%2528AMOTA%2529%2520on%2520the%250AnuScenes%2520dataset%2520when%2520leveraging%2520both%2520modalities.%2520CR3DT%2520bridges%2520the%2520gap%2520between%250Ahigh-performance%2520and%2520cost-effective%2520perception%2520systems%2520in%2520autonomous%2520driving%252C%250Aby%2520capitalizing%2520on%2520the%2520ubiquitous%2520presence%2520of%2520RADAR%2520in%2520automotive%2520applications.%250AThe%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/ETH-PBL/CR3DT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15313v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CR3DT%3A%20Camera-RADAR%20Fusion%20for%203D%20Detection%20and%20Tracking&entry.906535625=Nicolas%20Baumann%20and%20Michael%20Baumgartner%20and%20Edoardo%20Ghignone%20and%20Jonas%20K%C3%BChne%20and%20Tobias%20Fischer%20and%20Yung-Hsu%20Yang%20and%20Marc%20Pollefeys%20and%20Michele%20Magno&entry.1292438233=%20%20To%20enable%20self-driving%20vehicles%20accurate%20detection%20and%20tracking%20of%0Asurrounding%20objects%20is%20essential.%20While%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%0Asensors%20have%20set%20the%20benchmark%20for%20high-performance%20systems%2C%20the%20appeal%20of%0Acamera-only%20solutions%20lies%20in%20their%20cost-effectiveness.%20Notably%2C%20despite%20the%0Aprevalent%20use%20of%20Radio%20Detection%20and%20Ranging%20%28RADAR%29%20sensors%20in%20automotive%0Asystems%2C%20their%20potential%20in%203D%20detection%20and%20tracking%20has%20been%20largely%0Adisregarded%20due%20to%20data%20sparsity%20and%20measurement%20noise.%20As%20a%20recent%0Adevelopment%2C%20the%20combination%20of%20RADARs%20and%20cameras%20is%20emerging%20as%20a%20promising%0Asolution.%20This%20paper%20presents%20Camera-RADAR%203D%20Detection%20and%20Tracking%20%28CR3DT%29%2C%20a%0Acamera-RADAR%20fusion%20model%20for%203D%20object%20detection%2C%20and%20Multi-Object%20Tracking%0A%28MOT%29.%20Building%20upon%20the%20foundations%20of%20the%20State-of-the-Art%20%28SotA%29%20camera-only%0ABEVDet%20architecture%2C%20CR3DT%20demonstrates%20substantial%20improvements%20in%20both%0Adetection%20and%20tracking%20capabilities%2C%20by%20incorporating%20the%20spatial%20and%20velocity%0Ainformation%20of%20the%20RADAR%20sensor.%20Experimental%20results%20demonstrate%20an%20absolute%0Aimprovement%20in%20detection%20performance%20of%205.3%25%20in%20mean%20Average%20Precision%20%28mAP%29%0Aand%20a%2014.9%25%20increase%20in%20Average%20Multi-Object%20Tracking%20Accuracy%20%28AMOTA%29%20on%20the%0AnuScenes%20dataset%20when%20leveraging%20both%20modalities.%20CR3DT%20bridges%20the%20gap%20between%0Ahigh-performance%20and%20cost-effective%20perception%20systems%20in%20autonomous%20driving%2C%0Aby%20capitalizing%20on%20the%20ubiquitous%20presence%20of%20RADAR%20in%20automotive%20applications.%0AThe%20code%20is%20available%20at%3A%20https%3A//github.com/ETH-PBL/CR3DT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15313v2&entry.124074799=Read"},
{"title": "Segment anything model 2: an application to 2D and 3D medical images", "author": "Haoyu Dong and Hanxue Gu and Yaqian Chen and Jichen Yang and Maciej A. Mazurowski", "abstract": "  Segment Anything Model (SAM) has gained significant attention because of its\nability to segment varous objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we extensively evaluate SAM 2's ability\nto segment both 2D and 3D medical images by first collecting 18 medical imaging\ndatasets, including common 3D modalities such as computed tomography (CT),\nmagnetic resonance imaging (MRI), and positron emission tomography (PET) as\nwell as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines of\nSAM 2 are considered: (1) multi-frame 3D segmentation, where prompts are\nprovided to one or multiple slice(s) selected from the volume, and (2)\nsingle-frame 2D segmentation, where prompts are provided to each slice. The\nformer is only applicable to 3D modalities, while the latter applies to both 2D\nand 3D modalities. Our results show that SAM 2 exhibits similar performance as\nSAM under single-frame 2D segmentation, and has variable performance under\nmulti-frame 3D segmentation depending on the choices of slices to annotate, the\ndirection of the propagation, the predictions utilized during the propagation,\netc.\n", "link": "http://arxiv.org/abs/2408.00756v2", "date": "2024-08-06", "relevancy": 2.9862, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5973}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20anything%20model%202%3A%20an%20application%20to%202D%20and%203D%20medical%20images&body=Title%3A%20Segment%20anything%20model%202%3A%20an%20application%20to%202D%20and%203D%20medical%20images%0AAuthor%3A%20Haoyu%20Dong%20and%20Hanxue%20Gu%20and%20Yaqian%20Chen%20and%20Jichen%20Yang%20and%20Maciej%20A.%20Mazurowski%0AAbstract%3A%20%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20gained%20significant%20attention%20because%20of%20its%0Aability%20to%20segment%20varous%20objects%20in%20images%20given%20a%20prompt.%20The%20recently%0Adeveloped%20SAM%202%20has%20extended%20this%20ability%20to%20video%20inputs.%20This%20opens%20an%0Aopportunity%20to%20apply%20SAM%20to%203D%20images%2C%20one%20of%20the%20fundamental%20tasks%20in%20the%0Amedical%20imaging%20field.%20In%20this%20paper%2C%20we%20extensively%20evaluate%20SAM%202%27s%20ability%0Ato%20segment%20both%202D%20and%203D%20medical%20images%20by%20first%20collecting%2018%20medical%20imaging%0Adatasets%2C%20including%20common%203D%20modalities%20such%20as%20computed%20tomography%20%28CT%29%2C%0Amagnetic%20resonance%20imaging%20%28MRI%29%2C%20and%20positron%20emission%20tomography%20%28PET%29%20as%0Awell%20as%202D%20modalities%20such%20as%20X-ray%20and%20ultrasound.%20Two%20evaluation%20pipelines%20of%0ASAM%202%20are%20considered%3A%20%281%29%20multi-frame%203D%20segmentation%2C%20where%20prompts%20are%0Aprovided%20to%20one%20or%20multiple%20slice%28s%29%20selected%20from%20the%20volume%2C%20and%20%282%29%0Asingle-frame%202D%20segmentation%2C%20where%20prompts%20are%20provided%20to%20each%20slice.%20The%0Aformer%20is%20only%20applicable%20to%203D%20modalities%2C%20while%20the%20latter%20applies%20to%20both%202D%0Aand%203D%20modalities.%20Our%20results%20show%20that%20SAM%202%20exhibits%20similar%20performance%20as%0ASAM%20under%20single-frame%202D%20segmentation%2C%20and%20has%20variable%20performance%20under%0Amulti-frame%203D%20segmentation%20depending%20on%20the%20choices%20of%20slices%20to%20annotate%2C%20the%0Adirection%20of%20the%20propagation%2C%20the%20predictions%20utilized%20during%20the%20propagation%2C%0Aetc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00756v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520anything%2520model%25202%253A%2520an%2520application%2520to%25202D%2520and%25203D%2520medical%2520images%26entry.906535625%3DHaoyu%2520Dong%2520and%2520Hanxue%2520Gu%2520and%2520Yaqian%2520Chen%2520and%2520Jichen%2520Yang%2520and%2520Maciej%2520A.%2520Mazurowski%26entry.1292438233%3D%2520%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520gained%2520significant%2520attention%2520because%2520of%2520its%250Aability%2520to%2520segment%2520varous%2520objects%2520in%2520images%2520given%2520a%2520prompt.%2520The%2520recently%250Adeveloped%2520SAM%25202%2520has%2520extended%2520this%2520ability%2520to%2520video%2520inputs.%2520This%2520opens%2520an%250Aopportunity%2520to%2520apply%2520SAM%2520to%25203D%2520images%252C%2520one%2520of%2520the%2520fundamental%2520tasks%2520in%2520the%250Amedical%2520imaging%2520field.%2520In%2520this%2520paper%252C%2520we%2520extensively%2520evaluate%2520SAM%25202%2527s%2520ability%250Ato%2520segment%2520both%25202D%2520and%25203D%2520medical%2520images%2520by%2520first%2520collecting%252018%2520medical%2520imaging%250Adatasets%252C%2520including%2520common%25203D%2520modalities%2520such%2520as%2520computed%2520tomography%2520%2528CT%2529%252C%250Amagnetic%2520resonance%2520imaging%2520%2528MRI%2529%252C%2520and%2520positron%2520emission%2520tomography%2520%2528PET%2529%2520as%250Awell%2520as%25202D%2520modalities%2520such%2520as%2520X-ray%2520and%2520ultrasound.%2520Two%2520evaluation%2520pipelines%2520of%250ASAM%25202%2520are%2520considered%253A%2520%25281%2529%2520multi-frame%25203D%2520segmentation%252C%2520where%2520prompts%2520are%250Aprovided%2520to%2520one%2520or%2520multiple%2520slice%2528s%2529%2520selected%2520from%2520the%2520volume%252C%2520and%2520%25282%2529%250Asingle-frame%25202D%2520segmentation%252C%2520where%2520prompts%2520are%2520provided%2520to%2520each%2520slice.%2520The%250Aformer%2520is%2520only%2520applicable%2520to%25203D%2520modalities%252C%2520while%2520the%2520latter%2520applies%2520to%2520both%25202D%250Aand%25203D%2520modalities.%2520Our%2520results%2520show%2520that%2520SAM%25202%2520exhibits%2520similar%2520performance%2520as%250ASAM%2520under%2520single-frame%25202D%2520segmentation%252C%2520and%2520has%2520variable%2520performance%2520under%250Amulti-frame%25203D%2520segmentation%2520depending%2520on%2520the%2520choices%2520of%2520slices%2520to%2520annotate%252C%2520the%250Adirection%2520of%2520the%2520propagation%252C%2520the%2520predictions%2520utilized%2520during%2520the%2520propagation%252C%250Aetc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00756v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20anything%20model%202%3A%20an%20application%20to%202D%20and%203D%20medical%20images&entry.906535625=Haoyu%20Dong%20and%20Hanxue%20Gu%20and%20Yaqian%20Chen%20and%20Jichen%20Yang%20and%20Maciej%20A.%20Mazurowski&entry.1292438233=%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20gained%20significant%20attention%20because%20of%20its%0Aability%20to%20segment%20varous%20objects%20in%20images%20given%20a%20prompt.%20The%20recently%0Adeveloped%20SAM%202%20has%20extended%20this%20ability%20to%20video%20inputs.%20This%20opens%20an%0Aopportunity%20to%20apply%20SAM%20to%203D%20images%2C%20one%20of%20the%20fundamental%20tasks%20in%20the%0Amedical%20imaging%20field.%20In%20this%20paper%2C%20we%20extensively%20evaluate%20SAM%202%27s%20ability%0Ato%20segment%20both%202D%20and%203D%20medical%20images%20by%20first%20collecting%2018%20medical%20imaging%0Adatasets%2C%20including%20common%203D%20modalities%20such%20as%20computed%20tomography%20%28CT%29%2C%0Amagnetic%20resonance%20imaging%20%28MRI%29%2C%20and%20positron%20emission%20tomography%20%28PET%29%20as%0Awell%20as%202D%20modalities%20such%20as%20X-ray%20and%20ultrasound.%20Two%20evaluation%20pipelines%20of%0ASAM%202%20are%20considered%3A%20%281%29%20multi-frame%203D%20segmentation%2C%20where%20prompts%20are%0Aprovided%20to%20one%20or%20multiple%20slice%28s%29%20selected%20from%20the%20volume%2C%20and%20%282%29%0Asingle-frame%202D%20segmentation%2C%20where%20prompts%20are%20provided%20to%20each%20slice.%20The%0Aformer%20is%20only%20applicable%20to%203D%20modalities%2C%20while%20the%20latter%20applies%20to%20both%202D%0Aand%203D%20modalities.%20Our%20results%20show%20that%20SAM%202%20exhibits%20similar%20performance%20as%0ASAM%20under%20single-frame%202D%20segmentation%2C%20and%20has%20variable%20performance%20under%0Amulti-frame%203D%20segmentation%20depending%20on%20the%20choices%20of%20slices%20to%20annotate%2C%20the%0Adirection%20of%20the%20propagation%2C%20the%20predictions%20utilized%20during%20the%20propagation%2C%0Aetc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00756v2&entry.124074799=Read"},
{"title": "IMAGDressing-v1: Customizable Virtual Dressing", "author": "Fei Shen and Xin Jiang and Xin He and Hu Ye and Cong Wang and Xiaoyu Du and Zechao Li and Jinhui Tang", "abstract": "  Latest advances have achieved realistic virtual try-on (VTON) through\nlocalized garment inpainting using latent diffusion models, significantly\nenhancing consumers' online shopping experience. However, existing VTON\ntechnologies neglect the need for merchants to showcase garments\ncomprehensively, including flexible control over garments, optional faces,\nposes, and scenes. To address this issue, we define a virtual dressing (VD)\ntask focused on generating freely editable human images with fixed garments and\noptional conditions. Meanwhile, we design a comprehensive affinity metric index\n(CAMI) to evaluate the consistency between generated images and reference\ngarments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet\nthat captures semantic features from CLIP and texture features from VAE. We\npresent a hybrid attention module, including a frozen self-attention and a\ntrainable cross-attention, to integrate garment features from the garment UNet\ninto a frozen denoising UNet, ensuring users can control different scenes\nthrough text. IMAGDressing-v1 can be combined with other extension plugins,\nsuch as ControlNet and IP-Adapter, to enhance the diversity and controllability\nof generated images. Furthermore, to address the lack of data, we release the\ninteractive garment pairing (IGPair) dataset, containing over 300,000 pairs of\nclothing and dressed images, and establish a standard pipeline for data\nassembly. Extensive experiments demonstrate that our IMAGDressing-v1 achieves\nstate-of-the-art human image synthesis performance under various controlled\nconditions. The code and model will be available at\nhttps://github.com/muzishen/IMAGDressing.\n", "link": "http://arxiv.org/abs/2407.12705v2", "date": "2024-08-06", "relevancy": 2.8369, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.713}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7087}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.7057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMAGDressing-v1%3A%20Customizable%20Virtual%20Dressing&body=Title%3A%20IMAGDressing-v1%3A%20Customizable%20Virtual%20Dressing%0AAuthor%3A%20Fei%20Shen%20and%20Xin%20Jiang%20and%20Xin%20He%20and%20Hu%20Ye%20and%20Cong%20Wang%20and%20Xiaoyu%20Du%20and%20Zechao%20Li%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20Latest%20advances%20have%20achieved%20realistic%20virtual%20try-on%20%28VTON%29%20through%0Alocalized%20garment%20inpainting%20using%20latent%20diffusion%20models%2C%20significantly%0Aenhancing%20consumers%27%20online%20shopping%20experience.%20However%2C%20existing%20VTON%0Atechnologies%20neglect%20the%20need%20for%20merchants%20to%20showcase%20garments%0Acomprehensively%2C%20including%20flexible%20control%20over%20garments%2C%20optional%20faces%2C%0Aposes%2C%20and%20scenes.%20To%20address%20this%20issue%2C%20we%20define%20a%20virtual%20dressing%20%28VD%29%0Atask%20focused%20on%20generating%20freely%20editable%20human%20images%20with%20fixed%20garments%20and%0Aoptional%20conditions.%20Meanwhile%2C%20we%20design%20a%20comprehensive%20affinity%20metric%20index%0A%28CAMI%29%20to%20evaluate%20the%20consistency%20between%20generated%20images%20and%20reference%0Agarments.%20Then%2C%20we%20propose%20IMAGDressing-v1%2C%20which%20incorporates%20a%20garment%20UNet%0Athat%20captures%20semantic%20features%20from%20CLIP%20and%20texture%20features%20from%20VAE.%20We%0Apresent%20a%20hybrid%20attention%20module%2C%20including%20a%20frozen%20self-attention%20and%20a%0Atrainable%20cross-attention%2C%20to%20integrate%20garment%20features%20from%20the%20garment%20UNet%0Ainto%20a%20frozen%20denoising%20UNet%2C%20ensuring%20users%20can%20control%20different%20scenes%0Athrough%20text.%20IMAGDressing-v1%20can%20be%20combined%20with%20other%20extension%20plugins%2C%0Asuch%20as%20ControlNet%20and%20IP-Adapter%2C%20to%20enhance%20the%20diversity%20and%20controllability%0Aof%20generated%20images.%20Furthermore%2C%20to%20address%20the%20lack%20of%20data%2C%20we%20release%20the%0Ainteractive%20garment%20pairing%20%28IGPair%29%20dataset%2C%20containing%20over%20300%2C000%20pairs%20of%0Aclothing%20and%20dressed%20images%2C%20and%20establish%20a%20standard%20pipeline%20for%20data%0Aassembly.%20Extensive%20experiments%20demonstrate%20that%20our%20IMAGDressing-v1%20achieves%0Astate-of-the-art%20human%20image%20synthesis%20performance%20under%20various%20controlled%0Aconditions.%20The%20code%20and%20model%20will%20be%20available%20at%0Ahttps%3A//github.com/muzishen/IMAGDressing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMAGDressing-v1%253A%2520Customizable%2520Virtual%2520Dressing%26entry.906535625%3DFei%2520Shen%2520and%2520Xin%2520Jiang%2520and%2520Xin%2520He%2520and%2520Hu%2520Ye%2520and%2520Cong%2520Wang%2520and%2520Xiaoyu%2520Du%2520and%2520Zechao%2520Li%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520Latest%2520advances%2520have%2520achieved%2520realistic%2520virtual%2520try-on%2520%2528VTON%2529%2520through%250Alocalized%2520garment%2520inpainting%2520using%2520latent%2520diffusion%2520models%252C%2520significantly%250Aenhancing%2520consumers%2527%2520online%2520shopping%2520experience.%2520However%252C%2520existing%2520VTON%250Atechnologies%2520neglect%2520the%2520need%2520for%2520merchants%2520to%2520showcase%2520garments%250Acomprehensively%252C%2520including%2520flexible%2520control%2520over%2520garments%252C%2520optional%2520faces%252C%250Aposes%252C%2520and%2520scenes.%2520To%2520address%2520this%2520issue%252C%2520we%2520define%2520a%2520virtual%2520dressing%2520%2528VD%2529%250Atask%2520focused%2520on%2520generating%2520freely%2520editable%2520human%2520images%2520with%2520fixed%2520garments%2520and%250Aoptional%2520conditions.%2520Meanwhile%252C%2520we%2520design%2520a%2520comprehensive%2520affinity%2520metric%2520index%250A%2528CAMI%2529%2520to%2520evaluate%2520the%2520consistency%2520between%2520generated%2520images%2520and%2520reference%250Agarments.%2520Then%252C%2520we%2520propose%2520IMAGDressing-v1%252C%2520which%2520incorporates%2520a%2520garment%2520UNet%250Athat%2520captures%2520semantic%2520features%2520from%2520CLIP%2520and%2520texture%2520features%2520from%2520VAE.%2520We%250Apresent%2520a%2520hybrid%2520attention%2520module%252C%2520including%2520a%2520frozen%2520self-attention%2520and%2520a%250Atrainable%2520cross-attention%252C%2520to%2520integrate%2520garment%2520features%2520from%2520the%2520garment%2520UNet%250Ainto%2520a%2520frozen%2520denoising%2520UNet%252C%2520ensuring%2520users%2520can%2520control%2520different%2520scenes%250Athrough%2520text.%2520IMAGDressing-v1%2520can%2520be%2520combined%2520with%2520other%2520extension%2520plugins%252C%250Asuch%2520as%2520ControlNet%2520and%2520IP-Adapter%252C%2520to%2520enhance%2520the%2520diversity%2520and%2520controllability%250Aof%2520generated%2520images.%2520Furthermore%252C%2520to%2520address%2520the%2520lack%2520of%2520data%252C%2520we%2520release%2520the%250Ainteractive%2520garment%2520pairing%2520%2528IGPair%2529%2520dataset%252C%2520containing%2520over%2520300%252C000%2520pairs%2520of%250Aclothing%2520and%2520dressed%2520images%252C%2520and%2520establish%2520a%2520standard%2520pipeline%2520for%2520data%250Aassembly.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520IMAGDressing-v1%2520achieves%250Astate-of-the-art%2520human%2520image%2520synthesis%2520performance%2520under%2520various%2520controlled%250Aconditions.%2520The%2520code%2520and%2520model%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/muzishen/IMAGDressing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMAGDressing-v1%3A%20Customizable%20Virtual%20Dressing&entry.906535625=Fei%20Shen%20and%20Xin%20Jiang%20and%20Xin%20He%20and%20Hu%20Ye%20and%20Cong%20Wang%20and%20Xiaoyu%20Du%20and%20Zechao%20Li%20and%20Jinhui%20Tang&entry.1292438233=%20%20Latest%20advances%20have%20achieved%20realistic%20virtual%20try-on%20%28VTON%29%20through%0Alocalized%20garment%20inpainting%20using%20latent%20diffusion%20models%2C%20significantly%0Aenhancing%20consumers%27%20online%20shopping%20experience.%20However%2C%20existing%20VTON%0Atechnologies%20neglect%20the%20need%20for%20merchants%20to%20showcase%20garments%0Acomprehensively%2C%20including%20flexible%20control%20over%20garments%2C%20optional%20faces%2C%0Aposes%2C%20and%20scenes.%20To%20address%20this%20issue%2C%20we%20define%20a%20virtual%20dressing%20%28VD%29%0Atask%20focused%20on%20generating%20freely%20editable%20human%20images%20with%20fixed%20garments%20and%0Aoptional%20conditions.%20Meanwhile%2C%20we%20design%20a%20comprehensive%20affinity%20metric%20index%0A%28CAMI%29%20to%20evaluate%20the%20consistency%20between%20generated%20images%20and%20reference%0Agarments.%20Then%2C%20we%20propose%20IMAGDressing-v1%2C%20which%20incorporates%20a%20garment%20UNet%0Athat%20captures%20semantic%20features%20from%20CLIP%20and%20texture%20features%20from%20VAE.%20We%0Apresent%20a%20hybrid%20attention%20module%2C%20including%20a%20frozen%20self-attention%20and%20a%0Atrainable%20cross-attention%2C%20to%20integrate%20garment%20features%20from%20the%20garment%20UNet%0Ainto%20a%20frozen%20denoising%20UNet%2C%20ensuring%20users%20can%20control%20different%20scenes%0Athrough%20text.%20IMAGDressing-v1%20can%20be%20combined%20with%20other%20extension%20plugins%2C%0Asuch%20as%20ControlNet%20and%20IP-Adapter%2C%20to%20enhance%20the%20diversity%20and%20controllability%0Aof%20generated%20images.%20Furthermore%2C%20to%20address%20the%20lack%20of%20data%2C%20we%20release%20the%0Ainteractive%20garment%20pairing%20%28IGPair%29%20dataset%2C%20containing%20over%20300%2C000%20pairs%20of%0Aclothing%20and%20dressed%20images%2C%20and%20establish%20a%20standard%20pipeline%20for%20data%0Aassembly.%20Extensive%20experiments%20demonstrate%20that%20our%20IMAGDressing-v1%20achieves%0Astate-of-the-art%20human%20image%20synthesis%20performance%20under%20various%20controlled%0Aconditions.%20The%20code%20and%20model%20will%20be%20available%20at%0Ahttps%3A//github.com/muzishen/IMAGDressing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12705v2&entry.124074799=Read"},
{"title": "Segment Anything in Medical Images and Videos: Benchmark and Deployment", "author": "Jun Ma and Sumin Kim and Feifei Li and Mohammed Baharoon and Reza Asakereh and Hongwei Lyu and Bo Wang", "abstract": "  Recent advances in segmentation foundation models have enabled accurate and\nefficient segmentation across a wide range of natural images and videos, but\ntheir utility to medical data remains unclear. In this work, we first present a\ncomprehensive benchmarking of the Segment Anything Model 2 (SAM2) across 11\nmedical image modalities and videos and point out its strengths and weaknesses\nby comparing it to SAM1 and MedSAM. Then, we develop a transfer learning\npipeline and demonstrate SAM2 can be quickly adapted to medical domain by\nfine-tuning. Furthermore, we implement SAM2 as a 3D slicer plugin and Gradio\nAPI for efficient 3D image and video segmentation. The code has been made\npublicly available at \\url{https://github.com/bowang-lab/MedSAM}.\n", "link": "http://arxiv.org/abs/2408.03322v1", "date": "2024-08-06", "relevancy": 2.7251, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5533}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5409}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Anything%20in%20Medical%20Images%20and%20Videos%3A%20Benchmark%20and%20Deployment&body=Title%3A%20Segment%20Anything%20in%20Medical%20Images%20and%20Videos%3A%20Benchmark%20and%20Deployment%0AAuthor%3A%20Jun%20Ma%20and%20Sumin%20Kim%20and%20Feifei%20Li%20and%20Mohammed%20Baharoon%20and%20Reza%20Asakereh%20and%20Hongwei%20Lyu%20and%20Bo%20Wang%0AAbstract%3A%20%20%20Recent%20advances%20in%20segmentation%20foundation%20models%20have%20enabled%20accurate%20and%0Aefficient%20segmentation%20across%20a%20wide%20range%20of%20natural%20images%20and%20videos%2C%20but%0Atheir%20utility%20to%20medical%20data%20remains%20unclear.%20In%20this%20work%2C%20we%20first%20present%20a%0Acomprehensive%20benchmarking%20of%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20across%2011%0Amedical%20image%20modalities%20and%20videos%20and%20point%20out%20its%20strengths%20and%20weaknesses%0Aby%20comparing%20it%20to%20SAM1%20and%20MedSAM.%20Then%2C%20we%20develop%20a%20transfer%20learning%0Apipeline%20and%20demonstrate%20SAM2%20can%20be%20quickly%20adapted%20to%20medical%20domain%20by%0Afine-tuning.%20Furthermore%2C%20we%20implement%20SAM2%20as%20a%203D%20slicer%20plugin%20and%20Gradio%0AAPI%20for%20efficient%203D%20image%20and%20video%20segmentation.%20The%20code%20has%20been%20made%0Apublicly%20available%20at%20%5Curl%7Bhttps%3A//github.com/bowang-lab/MedSAM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Anything%2520in%2520Medical%2520Images%2520and%2520Videos%253A%2520Benchmark%2520and%2520Deployment%26entry.906535625%3DJun%2520Ma%2520and%2520Sumin%2520Kim%2520and%2520Feifei%2520Li%2520and%2520Mohammed%2520Baharoon%2520and%2520Reza%2520Asakereh%2520and%2520Hongwei%2520Lyu%2520and%2520Bo%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520segmentation%2520foundation%2520models%2520have%2520enabled%2520accurate%2520and%250Aefficient%2520segmentation%2520across%2520a%2520wide%2520range%2520of%2520natural%2520images%2520and%2520videos%252C%2520but%250Atheir%2520utility%2520to%2520medical%2520data%2520remains%2520unclear.%2520In%2520this%2520work%252C%2520we%2520first%2520present%2520a%250Acomprehensive%2520benchmarking%2520of%2520the%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520across%252011%250Amedical%2520image%2520modalities%2520and%2520videos%2520and%2520point%2520out%2520its%2520strengths%2520and%2520weaknesses%250Aby%2520comparing%2520it%2520to%2520SAM1%2520and%2520MedSAM.%2520Then%252C%2520we%2520develop%2520a%2520transfer%2520learning%250Apipeline%2520and%2520demonstrate%2520SAM2%2520can%2520be%2520quickly%2520adapted%2520to%2520medical%2520domain%2520by%250Afine-tuning.%2520Furthermore%252C%2520we%2520implement%2520SAM2%2520as%2520a%25203D%2520slicer%2520plugin%2520and%2520Gradio%250AAPI%2520for%2520efficient%25203D%2520image%2520and%2520video%2520segmentation.%2520The%2520code%2520has%2520been%2520made%250Apublicly%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/bowang-lab/MedSAM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Anything%20in%20Medical%20Images%20and%20Videos%3A%20Benchmark%20and%20Deployment&entry.906535625=Jun%20Ma%20and%20Sumin%20Kim%20and%20Feifei%20Li%20and%20Mohammed%20Baharoon%20and%20Reza%20Asakereh%20and%20Hongwei%20Lyu%20and%20Bo%20Wang&entry.1292438233=%20%20Recent%20advances%20in%20segmentation%20foundation%20models%20have%20enabled%20accurate%20and%0Aefficient%20segmentation%20across%20a%20wide%20range%20of%20natural%20images%20and%20videos%2C%20but%0Atheir%20utility%20to%20medical%20data%20remains%20unclear.%20In%20this%20work%2C%20we%20first%20present%20a%0Acomprehensive%20benchmarking%20of%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20across%2011%0Amedical%20image%20modalities%20and%20videos%20and%20point%20out%20its%20strengths%20and%20weaknesses%0Aby%20comparing%20it%20to%20SAM1%20and%20MedSAM.%20Then%2C%20we%20develop%20a%20transfer%20learning%0Apipeline%20and%20demonstrate%20SAM2%20can%20be%20quickly%20adapted%20to%20medical%20domain%20by%0Afine-tuning.%20Furthermore%2C%20we%20implement%20SAM2%20as%20a%203D%20slicer%20plugin%20and%20Gradio%0AAPI%20for%20efficient%203D%20image%20and%20video%20segmentation.%20The%20code%20has%20been%20made%0Apublicly%20available%20at%20%5Curl%7Bhttps%3A//github.com/bowang-lab/MedSAM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03322v1&entry.124074799=Read"},
{"title": "Temporal Correlation Meets Embedding: Towards a 2nd Generation of\n  JDE-based Real-Time Multi-Object Tracking", "author": "Yunfei Zhang and Chao Liang and Jin Gao and Zhipeng Zhang and Weiming Hu and Stephen Maybank and Xue Zhou and Liang Li", "abstract": "  Joint Detection and Embedding (JDE) trackers have demonstrated excellent\nperformance in Multi-Object Tracking (MOT) tasks by incorporating the\nextraction of appearance features as auxiliary tasks through embedding\nRe-Identification task (ReID) into the detector, achieving a balance between\ninference speed and tracking performance. However, solving the competition\nbetween the detector and the feature extractor has always been a challenge.\nMeanwhile, the issue of directly embedding the ReID task into MOT has remained\nunresolved. The lack of high discriminability in appearance features results in\ntheir limited utility. In this paper, a new learning approach using\ncross-correlation to capture temporal information of objects is proposed. The\nfeature extraction network is no longer trained solely on appearance features\nfrom each frame but learns richer motion features by utilizing feature heatmaps\nfrom consecutive frames, which addresses the challenge of inter-class feature\nsimilarity. Furthermore, our learning approach is applied to a more lightweight\nfeature extraction network, and treat the feature matching scores as strong\ncues rather than auxiliary cues, with an appropriate weight calculation to\nreflect the compatibility between our obtained features and the MOT task. Our\ntracker, named TCBTrack, achieves state-of-the-art performance on multiple\npublic benchmarks, i.e., MOT17, MOT20, and DanceTrack datasets. Specifically,\non the DanceTrack test set, we achieve 56.8 HOTA, 58.1 IDF1 and 92.5 MOTA,\nmaking it the best online tracker capable of achieving real-time performance.\nComparative evaluations with other trackers prove that our tracker achieves the\nbest balance between speed, robustness and accuracy. Code is available at\nhttps://github.com/yfzhang1214/TCBTrack.\n", "link": "http://arxiv.org/abs/2407.14086v2", "date": "2024-08-06", "relevancy": 2.723, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5524}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5436}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Correlation%20Meets%20Embedding%3A%20Towards%20a%202nd%20Generation%20of%0A%20%20JDE-based%20Real-Time%20Multi-Object%20Tracking&body=Title%3A%20Temporal%20Correlation%20Meets%20Embedding%3A%20Towards%20a%202nd%20Generation%20of%0A%20%20JDE-based%20Real-Time%20Multi-Object%20Tracking%0AAuthor%3A%20Yunfei%20Zhang%20and%20Chao%20Liang%20and%20Jin%20Gao%20and%20Zhipeng%20Zhang%20and%20Weiming%20Hu%20and%20Stephen%20Maybank%20and%20Xue%20Zhou%20and%20Liang%20Li%0AAbstract%3A%20%20%20Joint%20Detection%20and%20Embedding%20%28JDE%29%20trackers%20have%20demonstrated%20excellent%0Aperformance%20in%20Multi-Object%20Tracking%20%28MOT%29%20tasks%20by%20incorporating%20the%0Aextraction%20of%20appearance%20features%20as%20auxiliary%20tasks%20through%20embedding%0ARe-Identification%20task%20%28ReID%29%20into%20the%20detector%2C%20achieving%20a%20balance%20between%0Ainference%20speed%20and%20tracking%20performance.%20However%2C%20solving%20the%20competition%0Abetween%20the%20detector%20and%20the%20feature%20extractor%20has%20always%20been%20a%20challenge.%0AMeanwhile%2C%20the%20issue%20of%20directly%20embedding%20the%20ReID%20task%20into%20MOT%20has%20remained%0Aunresolved.%20The%20lack%20of%20high%20discriminability%20in%20appearance%20features%20results%20in%0Atheir%20limited%20utility.%20In%20this%20paper%2C%20a%20new%20learning%20approach%20using%0Across-correlation%20to%20capture%20temporal%20information%20of%20objects%20is%20proposed.%20The%0Afeature%20extraction%20network%20is%20no%20longer%20trained%20solely%20on%20appearance%20features%0Afrom%20each%20frame%20but%20learns%20richer%20motion%20features%20by%20utilizing%20feature%20heatmaps%0Afrom%20consecutive%20frames%2C%20which%20addresses%20the%20challenge%20of%20inter-class%20feature%0Asimilarity.%20Furthermore%2C%20our%20learning%20approach%20is%20applied%20to%20a%20more%20lightweight%0Afeature%20extraction%20network%2C%20and%20treat%20the%20feature%20matching%20scores%20as%20strong%0Acues%20rather%20than%20auxiliary%20cues%2C%20with%20an%20appropriate%20weight%20calculation%20to%0Areflect%20the%20compatibility%20between%20our%20obtained%20features%20and%20the%20MOT%20task.%20Our%0Atracker%2C%20named%20TCBTrack%2C%20achieves%20state-of-the-art%20performance%20on%20multiple%0Apublic%20benchmarks%2C%20i.e.%2C%20MOT17%2C%20MOT20%2C%20and%20DanceTrack%20datasets.%20Specifically%2C%0Aon%20the%20DanceTrack%20test%20set%2C%20we%20achieve%2056.8%20HOTA%2C%2058.1%20IDF1%20and%2092.5%20MOTA%2C%0Amaking%20it%20the%20best%20online%20tracker%20capable%20of%20achieving%20real-time%20performance.%0AComparative%20evaluations%20with%20other%20trackers%20prove%20that%20our%20tracker%20achieves%20the%0Abest%20balance%20between%20speed%2C%20robustness%20and%20accuracy.%20Code%20is%20available%20at%0Ahttps%3A//github.com/yfzhang1214/TCBTrack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14086v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Correlation%2520Meets%2520Embedding%253A%2520Towards%2520a%25202nd%2520Generation%2520of%250A%2520%2520JDE-based%2520Real-Time%2520Multi-Object%2520Tracking%26entry.906535625%3DYunfei%2520Zhang%2520and%2520Chao%2520Liang%2520and%2520Jin%2520Gao%2520and%2520Zhipeng%2520Zhang%2520and%2520Weiming%2520Hu%2520and%2520Stephen%2520Maybank%2520and%2520Xue%2520Zhou%2520and%2520Liang%2520Li%26entry.1292438233%3D%2520%2520Joint%2520Detection%2520and%2520Embedding%2520%2528JDE%2529%2520trackers%2520have%2520demonstrated%2520excellent%250Aperformance%2520in%2520Multi-Object%2520Tracking%2520%2528MOT%2529%2520tasks%2520by%2520incorporating%2520the%250Aextraction%2520of%2520appearance%2520features%2520as%2520auxiliary%2520tasks%2520through%2520embedding%250ARe-Identification%2520task%2520%2528ReID%2529%2520into%2520the%2520detector%252C%2520achieving%2520a%2520balance%2520between%250Ainference%2520speed%2520and%2520tracking%2520performance.%2520However%252C%2520solving%2520the%2520competition%250Abetween%2520the%2520detector%2520and%2520the%2520feature%2520extractor%2520has%2520always%2520been%2520a%2520challenge.%250AMeanwhile%252C%2520the%2520issue%2520of%2520directly%2520embedding%2520the%2520ReID%2520task%2520into%2520MOT%2520has%2520remained%250Aunresolved.%2520The%2520lack%2520of%2520high%2520discriminability%2520in%2520appearance%2520features%2520results%2520in%250Atheir%2520limited%2520utility.%2520In%2520this%2520paper%252C%2520a%2520new%2520learning%2520approach%2520using%250Across-correlation%2520to%2520capture%2520temporal%2520information%2520of%2520objects%2520is%2520proposed.%2520The%250Afeature%2520extraction%2520network%2520is%2520no%2520longer%2520trained%2520solely%2520on%2520appearance%2520features%250Afrom%2520each%2520frame%2520but%2520learns%2520richer%2520motion%2520features%2520by%2520utilizing%2520feature%2520heatmaps%250Afrom%2520consecutive%2520frames%252C%2520which%2520addresses%2520the%2520challenge%2520of%2520inter-class%2520feature%250Asimilarity.%2520Furthermore%252C%2520our%2520learning%2520approach%2520is%2520applied%2520to%2520a%2520more%2520lightweight%250Afeature%2520extraction%2520network%252C%2520and%2520treat%2520the%2520feature%2520matching%2520scores%2520as%2520strong%250Acues%2520rather%2520than%2520auxiliary%2520cues%252C%2520with%2520an%2520appropriate%2520weight%2520calculation%2520to%250Areflect%2520the%2520compatibility%2520between%2520our%2520obtained%2520features%2520and%2520the%2520MOT%2520task.%2520Our%250Atracker%252C%2520named%2520TCBTrack%252C%2520achieves%2520state-of-the-art%2520performance%2520on%2520multiple%250Apublic%2520benchmarks%252C%2520i.e.%252C%2520MOT17%252C%2520MOT20%252C%2520and%2520DanceTrack%2520datasets.%2520Specifically%252C%250Aon%2520the%2520DanceTrack%2520test%2520set%252C%2520we%2520achieve%252056.8%2520HOTA%252C%252058.1%2520IDF1%2520and%252092.5%2520MOTA%252C%250Amaking%2520it%2520the%2520best%2520online%2520tracker%2520capable%2520of%2520achieving%2520real-time%2520performance.%250AComparative%2520evaluations%2520with%2520other%2520trackers%2520prove%2520that%2520our%2520tracker%2520achieves%2520the%250Abest%2520balance%2520between%2520speed%252C%2520robustness%2520and%2520accuracy.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/yfzhang1214/TCBTrack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14086v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Correlation%20Meets%20Embedding%3A%20Towards%20a%202nd%20Generation%20of%0A%20%20JDE-based%20Real-Time%20Multi-Object%20Tracking&entry.906535625=Yunfei%20Zhang%20and%20Chao%20Liang%20and%20Jin%20Gao%20and%20Zhipeng%20Zhang%20and%20Weiming%20Hu%20and%20Stephen%20Maybank%20and%20Xue%20Zhou%20and%20Liang%20Li&entry.1292438233=%20%20Joint%20Detection%20and%20Embedding%20%28JDE%29%20trackers%20have%20demonstrated%20excellent%0Aperformance%20in%20Multi-Object%20Tracking%20%28MOT%29%20tasks%20by%20incorporating%20the%0Aextraction%20of%20appearance%20features%20as%20auxiliary%20tasks%20through%20embedding%0ARe-Identification%20task%20%28ReID%29%20into%20the%20detector%2C%20achieving%20a%20balance%20between%0Ainference%20speed%20and%20tracking%20performance.%20However%2C%20solving%20the%20competition%0Abetween%20the%20detector%20and%20the%20feature%20extractor%20has%20always%20been%20a%20challenge.%0AMeanwhile%2C%20the%20issue%20of%20directly%20embedding%20the%20ReID%20task%20into%20MOT%20has%20remained%0Aunresolved.%20The%20lack%20of%20high%20discriminability%20in%20appearance%20features%20results%20in%0Atheir%20limited%20utility.%20In%20this%20paper%2C%20a%20new%20learning%20approach%20using%0Across-correlation%20to%20capture%20temporal%20information%20of%20objects%20is%20proposed.%20The%0Afeature%20extraction%20network%20is%20no%20longer%20trained%20solely%20on%20appearance%20features%0Afrom%20each%20frame%20but%20learns%20richer%20motion%20features%20by%20utilizing%20feature%20heatmaps%0Afrom%20consecutive%20frames%2C%20which%20addresses%20the%20challenge%20of%20inter-class%20feature%0Asimilarity.%20Furthermore%2C%20our%20learning%20approach%20is%20applied%20to%20a%20more%20lightweight%0Afeature%20extraction%20network%2C%20and%20treat%20the%20feature%20matching%20scores%20as%20strong%0Acues%20rather%20than%20auxiliary%20cues%2C%20with%20an%20appropriate%20weight%20calculation%20to%0Areflect%20the%20compatibility%20between%20our%20obtained%20features%20and%20the%20MOT%20task.%20Our%0Atracker%2C%20named%20TCBTrack%2C%20achieves%20state-of-the-art%20performance%20on%20multiple%0Apublic%20benchmarks%2C%20i.e.%2C%20MOT17%2C%20MOT20%2C%20and%20DanceTrack%20datasets.%20Specifically%2C%0Aon%20the%20DanceTrack%20test%20set%2C%20we%20achieve%2056.8%20HOTA%2C%2058.1%20IDF1%20and%2092.5%20MOTA%2C%0Amaking%20it%20the%20best%20online%20tracker%20capable%20of%20achieving%20real-time%20performance.%0AComparative%20evaluations%20with%20other%20trackers%20prove%20that%20our%20tracker%20achieves%20the%0Abest%20balance%20between%20speed%2C%20robustness%20and%20accuracy.%20Code%20is%20available%20at%0Ahttps%3A//github.com/yfzhang1214/TCBTrack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14086v2&entry.124074799=Read"},
{"title": "GraphLearner: Graph Node Clustering with Fully Learnable Augmentation", "author": "Xihong Yang and Erxue Min and Ke Liang and Yue Liu and Siwei Wang and Sihang Zhou and Huijun Wu and Xinwang Liu and En Zhu", "abstract": "  Contrastive deep graph clustering (CDGC) leverages the power of contrastive\nlearning to group nodes into different clusters. The quality of contrastive\nsamples is crucial for achieving better performance, making augmentation\ntechniques a key factor in the process. However, the augmentation samples in\nexisting methods are always predefined by human experiences, and agnostic from\nthe downstream task clustering, thus leading to high human resource costs and\npoor performance. To overcome these limitations, we propose a Graph Node\nClustering with Fully Learnable Augmentation, termed GraphLearner. It\nintroduces learnable augmentors to generate high-quality and task-specific\naugmented samples for CDGC. GraphLearner incorporates two learnable augmentors\nspecifically designed for capturing attribute and structural information.\nMoreover, we introduce two refinement matrices, including the high-confidence\npseudo-label matrix and the cross-view sample similarity matrix, to enhance the\nreliability of the learned affinity matrix. During the training procedure, we\nnotice the distinct optimization goals for training learnable augmentors and\ncontrastive learning networks. In other words, we should both guarantee the\nconsistency of the embeddings as well as the diversity of the augmented\nsamples. To address this challenge, we propose an adversarial learning\nmechanism within our method. Besides, we leverage a two-stage training strategy\nto refine the high-confidence matrices. Extensive experimental results on six\nbenchmark datasets validate the effectiveness of GraphLearner.The code and\nappendix of GraphLearner are available at\nhttps://github.com/xihongyang1999/GraphLearner on Github.\n", "link": "http://arxiv.org/abs/2212.03559v3", "date": "2024-08-06", "relevancy": 2.6639, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5402}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5352}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphLearner%3A%20Graph%20Node%20Clustering%20with%20Fully%20Learnable%20Augmentation&body=Title%3A%20GraphLearner%3A%20Graph%20Node%20Clustering%20with%20Fully%20Learnable%20Augmentation%0AAuthor%3A%20Xihong%20Yang%20and%20Erxue%20Min%20and%20Ke%20Liang%20and%20Yue%20Liu%20and%20Siwei%20Wang%20and%20Sihang%20Zhou%20and%20Huijun%20Wu%20and%20Xinwang%20Liu%20and%20En%20Zhu%0AAbstract%3A%20%20%20Contrastive%20deep%20graph%20clustering%20%28CDGC%29%20leverages%20the%20power%20of%20contrastive%0Alearning%20to%20group%20nodes%20into%20different%20clusters.%20The%20quality%20of%20contrastive%0Asamples%20is%20crucial%20for%20achieving%20better%20performance%2C%20making%20augmentation%0Atechniques%20a%20key%20factor%20in%20the%20process.%20However%2C%20the%20augmentation%20samples%20in%0Aexisting%20methods%20are%20always%20predefined%20by%20human%20experiences%2C%20and%20agnostic%20from%0Athe%20downstream%20task%20clustering%2C%20thus%20leading%20to%20high%20human%20resource%20costs%20and%0Apoor%20performance.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20Graph%20Node%0AClustering%20with%20Fully%20Learnable%20Augmentation%2C%20termed%20GraphLearner.%20It%0Aintroduces%20learnable%20augmentors%20to%20generate%20high-quality%20and%20task-specific%0Aaugmented%20samples%20for%20CDGC.%20GraphLearner%20incorporates%20two%20learnable%20augmentors%0Aspecifically%20designed%20for%20capturing%20attribute%20and%20structural%20information.%0AMoreover%2C%20we%20introduce%20two%20refinement%20matrices%2C%20including%20the%20high-confidence%0Apseudo-label%20matrix%20and%20the%20cross-view%20sample%20similarity%20matrix%2C%20to%20enhance%20the%0Areliability%20of%20the%20learned%20affinity%20matrix.%20During%20the%20training%20procedure%2C%20we%0Anotice%20the%20distinct%20optimization%20goals%20for%20training%20learnable%20augmentors%20and%0Acontrastive%20learning%20networks.%20In%20other%20words%2C%20we%20should%20both%20guarantee%20the%0Aconsistency%20of%20the%20embeddings%20as%20well%20as%20the%20diversity%20of%20the%20augmented%0Asamples.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20adversarial%20learning%0Amechanism%20within%20our%20method.%20Besides%2C%20we%20leverage%20a%20two-stage%20training%20strategy%0Ato%20refine%20the%20high-confidence%20matrices.%20Extensive%20experimental%20results%20on%20six%0Abenchmark%20datasets%20validate%20the%20effectiveness%20of%20GraphLearner.The%20code%20and%0Aappendix%20of%20GraphLearner%20are%20available%20at%0Ahttps%3A//github.com/xihongyang1999/GraphLearner%20on%20Github.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.03559v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphLearner%253A%2520Graph%2520Node%2520Clustering%2520with%2520Fully%2520Learnable%2520Augmentation%26entry.906535625%3DXihong%2520Yang%2520and%2520Erxue%2520Min%2520and%2520Ke%2520Liang%2520and%2520Yue%2520Liu%2520and%2520Siwei%2520Wang%2520and%2520Sihang%2520Zhou%2520and%2520Huijun%2520Wu%2520and%2520Xinwang%2520Liu%2520and%2520En%2520Zhu%26entry.1292438233%3D%2520%2520Contrastive%2520deep%2520graph%2520clustering%2520%2528CDGC%2529%2520leverages%2520the%2520power%2520of%2520contrastive%250Alearning%2520to%2520group%2520nodes%2520into%2520different%2520clusters.%2520The%2520quality%2520of%2520contrastive%250Asamples%2520is%2520crucial%2520for%2520achieving%2520better%2520performance%252C%2520making%2520augmentation%250Atechniques%2520a%2520key%2520factor%2520in%2520the%2520process.%2520However%252C%2520the%2520augmentation%2520samples%2520in%250Aexisting%2520methods%2520are%2520always%2520predefined%2520by%2520human%2520experiences%252C%2520and%2520agnostic%2520from%250Athe%2520downstream%2520task%2520clustering%252C%2520thus%2520leading%2520to%2520high%2520human%2520resource%2520costs%2520and%250Apoor%2520performance.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520Graph%2520Node%250AClustering%2520with%2520Fully%2520Learnable%2520Augmentation%252C%2520termed%2520GraphLearner.%2520It%250Aintroduces%2520learnable%2520augmentors%2520to%2520generate%2520high-quality%2520and%2520task-specific%250Aaugmented%2520samples%2520for%2520CDGC.%2520GraphLearner%2520incorporates%2520two%2520learnable%2520augmentors%250Aspecifically%2520designed%2520for%2520capturing%2520attribute%2520and%2520structural%2520information.%250AMoreover%252C%2520we%2520introduce%2520two%2520refinement%2520matrices%252C%2520including%2520the%2520high-confidence%250Apseudo-label%2520matrix%2520and%2520the%2520cross-view%2520sample%2520similarity%2520matrix%252C%2520to%2520enhance%2520the%250Areliability%2520of%2520the%2520learned%2520affinity%2520matrix.%2520During%2520the%2520training%2520procedure%252C%2520we%250Anotice%2520the%2520distinct%2520optimization%2520goals%2520for%2520training%2520learnable%2520augmentors%2520and%250Acontrastive%2520learning%2520networks.%2520In%2520other%2520words%252C%2520we%2520should%2520both%2520guarantee%2520the%250Aconsistency%2520of%2520the%2520embeddings%2520as%2520well%2520as%2520the%2520diversity%2520of%2520the%2520augmented%250Asamples.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%2520adversarial%2520learning%250Amechanism%2520within%2520our%2520method.%2520Besides%252C%2520we%2520leverage%2520a%2520two-stage%2520training%2520strategy%250Ato%2520refine%2520the%2520high-confidence%2520matrices.%2520Extensive%2520experimental%2520results%2520on%2520six%250Abenchmark%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520GraphLearner.The%2520code%2520and%250Aappendix%2520of%2520GraphLearner%2520are%2520available%2520at%250Ahttps%253A//github.com/xihongyang1999/GraphLearner%2520on%2520Github.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.03559v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphLearner%3A%20Graph%20Node%20Clustering%20with%20Fully%20Learnable%20Augmentation&entry.906535625=Xihong%20Yang%20and%20Erxue%20Min%20and%20Ke%20Liang%20and%20Yue%20Liu%20and%20Siwei%20Wang%20and%20Sihang%20Zhou%20and%20Huijun%20Wu%20and%20Xinwang%20Liu%20and%20En%20Zhu&entry.1292438233=%20%20Contrastive%20deep%20graph%20clustering%20%28CDGC%29%20leverages%20the%20power%20of%20contrastive%0Alearning%20to%20group%20nodes%20into%20different%20clusters.%20The%20quality%20of%20contrastive%0Asamples%20is%20crucial%20for%20achieving%20better%20performance%2C%20making%20augmentation%0Atechniques%20a%20key%20factor%20in%20the%20process.%20However%2C%20the%20augmentation%20samples%20in%0Aexisting%20methods%20are%20always%20predefined%20by%20human%20experiences%2C%20and%20agnostic%20from%0Athe%20downstream%20task%20clustering%2C%20thus%20leading%20to%20high%20human%20resource%20costs%20and%0Apoor%20performance.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20Graph%20Node%0AClustering%20with%20Fully%20Learnable%20Augmentation%2C%20termed%20GraphLearner.%20It%0Aintroduces%20learnable%20augmentors%20to%20generate%20high-quality%20and%20task-specific%0Aaugmented%20samples%20for%20CDGC.%20GraphLearner%20incorporates%20two%20learnable%20augmentors%0Aspecifically%20designed%20for%20capturing%20attribute%20and%20structural%20information.%0AMoreover%2C%20we%20introduce%20two%20refinement%20matrices%2C%20including%20the%20high-confidence%0Apseudo-label%20matrix%20and%20the%20cross-view%20sample%20similarity%20matrix%2C%20to%20enhance%20the%0Areliability%20of%20the%20learned%20affinity%20matrix.%20During%20the%20training%20procedure%2C%20we%0Anotice%20the%20distinct%20optimization%20goals%20for%20training%20learnable%20augmentors%20and%0Acontrastive%20learning%20networks.%20In%20other%20words%2C%20we%20should%20both%20guarantee%20the%0Aconsistency%20of%20the%20embeddings%20as%20well%20as%20the%20diversity%20of%20the%20augmented%0Asamples.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20adversarial%20learning%0Amechanism%20within%20our%20method.%20Besides%2C%20we%20leverage%20a%20two-stage%20training%20strategy%0Ato%20refine%20the%20high-confidence%20matrices.%20Extensive%20experimental%20results%20on%20six%0Abenchmark%20datasets%20validate%20the%20effectiveness%20of%20GraphLearner.The%20code%20and%0Aappendix%20of%20GraphLearner%20are%20available%20at%0Ahttps%3A//github.com/xihongyang1999/GraphLearner%20on%20Github.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.03559v3&entry.124074799=Read"},
{"title": "Multi-Modal Dataset Creation for Federated Learning with DICOM\n  Structured Reports", "author": "Malte T\u00f6lle and Lukas Burger and Halvar Kelm and Florian Andr\u00e9 and Peter Bannas and Gerhard Diller and Norbert Frey and Philipp Garthe and Stefan Gro\u00df and Anja Hennemuth and Lars Kaderali and Nina Kr\u00fcger and Andreas Leha and Simon Martin and Alexander Meyer and Eike Nagel and Stefan Orwat and Clemens Scherer and Moritz Seiffert and Jan Moritz Seliger and Stefan Simm and Tim Friede and Tim Seidler and Sandy Engelhardt", "abstract": "  Purpose: Federated training is often hindered by heterogeneous datasets due\nto divergent data storage options, inconsistent naming schemes, varied\nannotation procedures, and disparities in label quality. This is particularly\nevident in the emerging multi-modal learning paradigms, where dataset\nharmonization including a uniform data representation and filtering options are\nof paramount importance.\n  Methods: DICOM structured reports enable the standardized linkage of\narbitrary information beyond the imaging domain and can be used within Python\ndeep learning pipelines with highdicom. Building on this, we developed an open\nplatform for data integration and interactive filtering capabilities that\nsimplifies the process of assembling multi-modal datasets.\n  Results: In this study, we extend our prior work by showing its applicability\nto more and divergent data types, as well as streamlining datasets for\nfederated training within an established consortium of eight university\nhospitals in Germany. We prove its concurrent filtering ability by creating\nharmonized multi-modal datasets across all locations for predicting the outcome\nafter minimally invasive heart valve replacement. The data includes DICOM data\n(i.e. computed tomography images, electrocardiography scans) as well as\nannotations (i.e. calcification segmentations, pointsets and pacemaker\ndependency), and metadata (i.e. prosthesis and diagnoses).\n  Conclusion: Structured reports bridge the traditional gap between imaging\nsystems and information systems. Utilizing the inherent DICOM reference system\narbitrary data types can be queried concurrently to create meaningful cohorts\nfor clinical studies. The graphical interface as well as example structured\nreport templates will be made publicly available.\n", "link": "http://arxiv.org/abs/2407.09064v2", "date": "2024-08-06", "relevancy": 2.6619, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5496}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5238}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Dataset%20Creation%20for%20Federated%20Learning%20with%20DICOM%0A%20%20Structured%20Reports&body=Title%3A%20Multi-Modal%20Dataset%20Creation%20for%20Federated%20Learning%20with%20DICOM%0A%20%20Structured%20Reports%0AAuthor%3A%20Malte%20T%C3%B6lle%20and%20Lukas%20Burger%20and%20Halvar%20Kelm%20and%20Florian%20Andr%C3%A9%20and%20Peter%20Bannas%20and%20Gerhard%20Diller%20and%20Norbert%20Frey%20and%20Philipp%20Garthe%20and%20Stefan%20Gro%C3%9F%20and%20Anja%20Hennemuth%20and%20Lars%20Kaderali%20and%20Nina%20Kr%C3%BCger%20and%20Andreas%20Leha%20and%20Simon%20Martin%20and%20Alexander%20Meyer%20and%20Eike%20Nagel%20and%20Stefan%20Orwat%20and%20Clemens%20Scherer%20and%20Moritz%20Seiffert%20and%20Jan%20Moritz%20Seliger%20and%20Stefan%20Simm%20and%20Tim%20Friede%20and%20Tim%20Seidler%20and%20Sandy%20Engelhardt%0AAbstract%3A%20%20%20Purpose%3A%20Federated%20training%20is%20often%20hindered%20by%20heterogeneous%20datasets%20due%0Ato%20divergent%20data%20storage%20options%2C%20inconsistent%20naming%20schemes%2C%20varied%0Aannotation%20procedures%2C%20and%20disparities%20in%20label%20quality.%20This%20is%20particularly%0Aevident%20in%20the%20emerging%20multi-modal%20learning%20paradigms%2C%20where%20dataset%0Aharmonization%20including%20a%20uniform%20data%20representation%20and%20filtering%20options%20are%0Aof%20paramount%20importance.%0A%20%20Methods%3A%20DICOM%20structured%20reports%20enable%20the%20standardized%20linkage%20of%0Aarbitrary%20information%20beyond%20the%20imaging%20domain%20and%20can%20be%20used%20within%20Python%0Adeep%20learning%20pipelines%20with%20highdicom.%20Building%20on%20this%2C%20we%20developed%20an%20open%0Aplatform%20for%20data%20integration%20and%20interactive%20filtering%20capabilities%20that%0Asimplifies%20the%20process%20of%20assembling%20multi-modal%20datasets.%0A%20%20Results%3A%20In%20this%20study%2C%20we%20extend%20our%20prior%20work%20by%20showing%20its%20applicability%0Ato%20more%20and%20divergent%20data%20types%2C%20as%20well%20as%20streamlining%20datasets%20for%0Afederated%20training%20within%20an%20established%20consortium%20of%20eight%20university%0Ahospitals%20in%20Germany.%20We%20prove%20its%20concurrent%20filtering%20ability%20by%20creating%0Aharmonized%20multi-modal%20datasets%20across%20all%20locations%20for%20predicting%20the%20outcome%0Aafter%20minimally%20invasive%20heart%20valve%20replacement.%20The%20data%20includes%20DICOM%20data%0A%28i.e.%20computed%20tomography%20images%2C%20electrocardiography%20scans%29%20as%20well%20as%0Aannotations%20%28i.e.%20calcification%20segmentations%2C%20pointsets%20and%20pacemaker%0Adependency%29%2C%20and%20metadata%20%28i.e.%20prosthesis%20and%20diagnoses%29.%0A%20%20Conclusion%3A%20Structured%20reports%20bridge%20the%20traditional%20gap%20between%20imaging%0Asystems%20and%20information%20systems.%20Utilizing%20the%20inherent%20DICOM%20reference%20system%0Aarbitrary%20data%20types%20can%20be%20queried%20concurrently%20to%20create%20meaningful%20cohorts%0Afor%20clinical%20studies.%20The%20graphical%20interface%20as%20well%20as%20example%20structured%0Areport%20templates%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09064v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Dataset%2520Creation%2520for%2520Federated%2520Learning%2520with%2520DICOM%250A%2520%2520Structured%2520Reports%26entry.906535625%3DMalte%2520T%25C3%25B6lle%2520and%2520Lukas%2520Burger%2520and%2520Halvar%2520Kelm%2520and%2520Florian%2520Andr%25C3%25A9%2520and%2520Peter%2520Bannas%2520and%2520Gerhard%2520Diller%2520and%2520Norbert%2520Frey%2520and%2520Philipp%2520Garthe%2520and%2520Stefan%2520Gro%25C3%259F%2520and%2520Anja%2520Hennemuth%2520and%2520Lars%2520Kaderali%2520and%2520Nina%2520Kr%25C3%25BCger%2520and%2520Andreas%2520Leha%2520and%2520Simon%2520Martin%2520and%2520Alexander%2520Meyer%2520and%2520Eike%2520Nagel%2520and%2520Stefan%2520Orwat%2520and%2520Clemens%2520Scherer%2520and%2520Moritz%2520Seiffert%2520and%2520Jan%2520Moritz%2520Seliger%2520and%2520Stefan%2520Simm%2520and%2520Tim%2520Friede%2520and%2520Tim%2520Seidler%2520and%2520Sandy%2520Engelhardt%26entry.1292438233%3D%2520%2520Purpose%253A%2520Federated%2520training%2520is%2520often%2520hindered%2520by%2520heterogeneous%2520datasets%2520due%250Ato%2520divergent%2520data%2520storage%2520options%252C%2520inconsistent%2520naming%2520schemes%252C%2520varied%250Aannotation%2520procedures%252C%2520and%2520disparities%2520in%2520label%2520quality.%2520This%2520is%2520particularly%250Aevident%2520in%2520the%2520emerging%2520multi-modal%2520learning%2520paradigms%252C%2520where%2520dataset%250Aharmonization%2520including%2520a%2520uniform%2520data%2520representation%2520and%2520filtering%2520options%2520are%250Aof%2520paramount%2520importance.%250A%2520%2520Methods%253A%2520DICOM%2520structured%2520reports%2520enable%2520the%2520standardized%2520linkage%2520of%250Aarbitrary%2520information%2520beyond%2520the%2520imaging%2520domain%2520and%2520can%2520be%2520used%2520within%2520Python%250Adeep%2520learning%2520pipelines%2520with%2520highdicom.%2520Building%2520on%2520this%252C%2520we%2520developed%2520an%2520open%250Aplatform%2520for%2520data%2520integration%2520and%2520interactive%2520filtering%2520capabilities%2520that%250Asimplifies%2520the%2520process%2520of%2520assembling%2520multi-modal%2520datasets.%250A%2520%2520Results%253A%2520In%2520this%2520study%252C%2520we%2520extend%2520our%2520prior%2520work%2520by%2520showing%2520its%2520applicability%250Ato%2520more%2520and%2520divergent%2520data%2520types%252C%2520as%2520well%2520as%2520streamlining%2520datasets%2520for%250Afederated%2520training%2520within%2520an%2520established%2520consortium%2520of%2520eight%2520university%250Ahospitals%2520in%2520Germany.%2520We%2520prove%2520its%2520concurrent%2520filtering%2520ability%2520by%2520creating%250Aharmonized%2520multi-modal%2520datasets%2520across%2520all%2520locations%2520for%2520predicting%2520the%2520outcome%250Aafter%2520minimally%2520invasive%2520heart%2520valve%2520replacement.%2520The%2520data%2520includes%2520DICOM%2520data%250A%2528i.e.%2520computed%2520tomography%2520images%252C%2520electrocardiography%2520scans%2529%2520as%2520well%2520as%250Aannotations%2520%2528i.e.%2520calcification%2520segmentations%252C%2520pointsets%2520and%2520pacemaker%250Adependency%2529%252C%2520and%2520metadata%2520%2528i.e.%2520prosthesis%2520and%2520diagnoses%2529.%250A%2520%2520Conclusion%253A%2520Structured%2520reports%2520bridge%2520the%2520traditional%2520gap%2520between%2520imaging%250Asystems%2520and%2520information%2520systems.%2520Utilizing%2520the%2520inherent%2520DICOM%2520reference%2520system%250Aarbitrary%2520data%2520types%2520can%2520be%2520queried%2520concurrently%2520to%2520create%2520meaningful%2520cohorts%250Afor%2520clinical%2520studies.%2520The%2520graphical%2520interface%2520as%2520well%2520as%2520example%2520structured%250Areport%2520templates%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09064v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Dataset%20Creation%20for%20Federated%20Learning%20with%20DICOM%0A%20%20Structured%20Reports&entry.906535625=Malte%20T%C3%B6lle%20and%20Lukas%20Burger%20and%20Halvar%20Kelm%20and%20Florian%20Andr%C3%A9%20and%20Peter%20Bannas%20and%20Gerhard%20Diller%20and%20Norbert%20Frey%20and%20Philipp%20Garthe%20and%20Stefan%20Gro%C3%9F%20and%20Anja%20Hennemuth%20and%20Lars%20Kaderali%20and%20Nina%20Kr%C3%BCger%20and%20Andreas%20Leha%20and%20Simon%20Martin%20and%20Alexander%20Meyer%20and%20Eike%20Nagel%20and%20Stefan%20Orwat%20and%20Clemens%20Scherer%20and%20Moritz%20Seiffert%20and%20Jan%20Moritz%20Seliger%20and%20Stefan%20Simm%20and%20Tim%20Friede%20and%20Tim%20Seidler%20and%20Sandy%20Engelhardt&entry.1292438233=%20%20Purpose%3A%20Federated%20training%20is%20often%20hindered%20by%20heterogeneous%20datasets%20due%0Ato%20divergent%20data%20storage%20options%2C%20inconsistent%20naming%20schemes%2C%20varied%0Aannotation%20procedures%2C%20and%20disparities%20in%20label%20quality.%20This%20is%20particularly%0Aevident%20in%20the%20emerging%20multi-modal%20learning%20paradigms%2C%20where%20dataset%0Aharmonization%20including%20a%20uniform%20data%20representation%20and%20filtering%20options%20are%0Aof%20paramount%20importance.%0A%20%20Methods%3A%20DICOM%20structured%20reports%20enable%20the%20standardized%20linkage%20of%0Aarbitrary%20information%20beyond%20the%20imaging%20domain%20and%20can%20be%20used%20within%20Python%0Adeep%20learning%20pipelines%20with%20highdicom.%20Building%20on%20this%2C%20we%20developed%20an%20open%0Aplatform%20for%20data%20integration%20and%20interactive%20filtering%20capabilities%20that%0Asimplifies%20the%20process%20of%20assembling%20multi-modal%20datasets.%0A%20%20Results%3A%20In%20this%20study%2C%20we%20extend%20our%20prior%20work%20by%20showing%20its%20applicability%0Ato%20more%20and%20divergent%20data%20types%2C%20as%20well%20as%20streamlining%20datasets%20for%0Afederated%20training%20within%20an%20established%20consortium%20of%20eight%20university%0Ahospitals%20in%20Germany.%20We%20prove%20its%20concurrent%20filtering%20ability%20by%20creating%0Aharmonized%20multi-modal%20datasets%20across%20all%20locations%20for%20predicting%20the%20outcome%0Aafter%20minimally%20invasive%20heart%20valve%20replacement.%20The%20data%20includes%20DICOM%20data%0A%28i.e.%20computed%20tomography%20images%2C%20electrocardiography%20scans%29%20as%20well%20as%0Aannotations%20%28i.e.%20calcification%20segmentations%2C%20pointsets%20and%20pacemaker%0Adependency%29%2C%20and%20metadata%20%28i.e.%20prosthesis%20and%20diagnoses%29.%0A%20%20Conclusion%3A%20Structured%20reports%20bridge%20the%20traditional%20gap%20between%20imaging%0Asystems%20and%20information%20systems.%20Utilizing%20the%20inherent%20DICOM%20reference%20system%0Aarbitrary%20data%20types%20can%20be%20queried%20concurrently%20to%20create%20meaningful%20cohorts%0Afor%20clinical%20studies.%20The%20graphical%20interface%20as%20well%20as%20example%20structured%0Areport%20templates%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09064v2&entry.124074799=Read"},
{"title": "Iterative CT Reconstruction via Latent Variable Optimization of Shallow\n  Diffusion Models", "author": "Sho Ozaki and Shizuo Kaji and Toshikazu Imae and Kanabu Nawa and Hideomi Yamashita and Keiichi Nakagawa", "abstract": "  Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.\n", "link": "http://arxiv.org/abs/2408.03156v1", "date": "2024-08-06", "relevancy": 2.6244, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6654}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6542}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20CT%20Reconstruction%20via%20Latent%20Variable%20Optimization%20of%20Shallow%0A%20%20Diffusion%20Models&body=Title%3A%20Iterative%20CT%20Reconstruction%20via%20Latent%20Variable%20Optimization%20of%20Shallow%0A%20%20Diffusion%20Models%0AAuthor%3A%20Sho%20Ozaki%20and%20Shizuo%20Kaji%20and%20Toshikazu%20Imae%20and%20Kanabu%20Nawa%20and%20Hideomi%20Yamashita%20and%20Keiichi%20Nakagawa%0AAbstract%3A%20%20%20Image%20generative%20AI%20has%20garnered%20significant%20attention%20in%20recent%20years.%20In%0Aparticular%2C%20the%20diffusion%20model%2C%20a%20core%20component%20of%20recent%20generative%20AI%2C%0Aproduces%20high-quality%20images%20with%20rich%20diversity.%20In%20this%20study%2C%20we%20propose%20a%0Anovel%20CT%20reconstruction%20method%20by%20combining%20the%20denoising%20diffusion%0Aprobabilistic%20model%20with%20iterative%20CT%20reconstruction.%20In%20sharp%20contrast%20to%0Aprevious%20studies%2C%20we%20optimize%20the%20fidelity%20loss%20of%20CT%20reconstruction%20with%0Arespect%20to%20the%20latent%20variable%20of%20the%20diffusion%20model%2C%20instead%20of%20the%20image%20and%0Amodel%20parameters.%20To%20suppress%20anatomical%20structure%20changes%20produced%20by%20the%0Adiffusion%20model%2C%20we%20shallow%20the%20diffusion%20and%20reverse%20processes%2C%20and%20fix%20a%20set%0Aof%20added%20noises%20in%20the%20reverse%20process%20to%20make%20it%20deterministic%20during%0Ainference.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20through%0Asparse%20view%20CT%20reconstruction%20of%201/10%20view%20projection%20data.%20Despite%20the%0Asimplicity%20of%20the%20implementation%2C%20the%20proposed%20method%20shows%20the%20capability%20of%0Areconstructing%20high-quality%20images%20while%20preserving%20the%20patient%27s%20anatomical%0Astructure%2C%20and%20outperforms%20existing%20methods%20including%20iterative%20reconstruction%2C%0Aiterative%20reconstruction%20with%20total%20variation%2C%20and%20the%20diffusion%20model%20alone%20in%0Aterms%20of%20quantitative%20indices%20such%20as%20SSIM%20and%20PSNR.%20We%20also%20explore%20further%0Asparse%20view%20CT%20using%201/20%20view%20projection%20data%20with%20the%20same%20trained%20diffusion%0Amodel.%20As%20the%20number%20of%20iterations%20increases%2C%20image%20quality%20improvement%0Acomparable%20to%20that%20of%201/10%20sparse%20view%20CT%20reconstruction%20is%20achieved.%20In%0Aprinciple%2C%20the%20proposed%20method%20can%20be%20widely%20applied%20not%20only%20to%20CT%20but%20also%20to%0Aother%20imaging%20modalities%20such%20as%20MRI%2C%20PET%2C%20and%20SPECT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520CT%2520Reconstruction%2520via%2520Latent%2520Variable%2520Optimization%2520of%2520Shallow%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DSho%2520Ozaki%2520and%2520Shizuo%2520Kaji%2520and%2520Toshikazu%2520Imae%2520and%2520Kanabu%2520Nawa%2520and%2520Hideomi%2520Yamashita%2520and%2520Keiichi%2520Nakagawa%26entry.1292438233%3D%2520%2520Image%2520generative%2520AI%2520has%2520garnered%2520significant%2520attention%2520in%2520recent%2520years.%2520In%250Aparticular%252C%2520the%2520diffusion%2520model%252C%2520a%2520core%2520component%2520of%2520recent%2520generative%2520AI%252C%250Aproduces%2520high-quality%2520images%2520with%2520rich%2520diversity.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%250Anovel%2520CT%2520reconstruction%2520method%2520by%2520combining%2520the%2520denoising%2520diffusion%250Aprobabilistic%2520model%2520with%2520iterative%2520CT%2520reconstruction.%2520In%2520sharp%2520contrast%2520to%250Aprevious%2520studies%252C%2520we%2520optimize%2520the%2520fidelity%2520loss%2520of%2520CT%2520reconstruction%2520with%250Arespect%2520to%2520the%2520latent%2520variable%2520of%2520the%2520diffusion%2520model%252C%2520instead%2520of%2520the%2520image%2520and%250Amodel%2520parameters.%2520To%2520suppress%2520anatomical%2520structure%2520changes%2520produced%2520by%2520the%250Adiffusion%2520model%252C%2520we%2520shallow%2520the%2520diffusion%2520and%2520reverse%2520processes%252C%2520and%2520fix%2520a%2520set%250Aof%2520added%2520noises%2520in%2520the%2520reverse%2520process%2520to%2520make%2520it%2520deterministic%2520during%250Ainference.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520through%250Asparse%2520view%2520CT%2520reconstruction%2520of%25201/10%2520view%2520projection%2520data.%2520Despite%2520the%250Asimplicity%2520of%2520the%2520implementation%252C%2520the%2520proposed%2520method%2520shows%2520the%2520capability%2520of%250Areconstructing%2520high-quality%2520images%2520while%2520preserving%2520the%2520patient%2527s%2520anatomical%250Astructure%252C%2520and%2520outperforms%2520existing%2520methods%2520including%2520iterative%2520reconstruction%252C%250Aiterative%2520reconstruction%2520with%2520total%2520variation%252C%2520and%2520the%2520diffusion%2520model%2520alone%2520in%250Aterms%2520of%2520quantitative%2520indices%2520such%2520as%2520SSIM%2520and%2520PSNR.%2520We%2520also%2520explore%2520further%250Asparse%2520view%2520CT%2520using%25201/20%2520view%2520projection%2520data%2520with%2520the%2520same%2520trained%2520diffusion%250Amodel.%2520As%2520the%2520number%2520of%2520iterations%2520increases%252C%2520image%2520quality%2520improvement%250Acomparable%2520to%2520that%2520of%25201/10%2520sparse%2520view%2520CT%2520reconstruction%2520is%2520achieved.%2520In%250Aprinciple%252C%2520the%2520proposed%2520method%2520can%2520be%2520widely%2520applied%2520not%2520only%2520to%2520CT%2520but%2520also%2520to%250Aother%2520imaging%2520modalities%2520such%2520as%2520MRI%252C%2520PET%252C%2520and%2520SPECT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20CT%20Reconstruction%20via%20Latent%20Variable%20Optimization%20of%20Shallow%0A%20%20Diffusion%20Models&entry.906535625=Sho%20Ozaki%20and%20Shizuo%20Kaji%20and%20Toshikazu%20Imae%20and%20Kanabu%20Nawa%20and%20Hideomi%20Yamashita%20and%20Keiichi%20Nakagawa&entry.1292438233=%20%20Image%20generative%20AI%20has%20garnered%20significant%20attention%20in%20recent%20years.%20In%0Aparticular%2C%20the%20diffusion%20model%2C%20a%20core%20component%20of%20recent%20generative%20AI%2C%0Aproduces%20high-quality%20images%20with%20rich%20diversity.%20In%20this%20study%2C%20we%20propose%20a%0Anovel%20CT%20reconstruction%20method%20by%20combining%20the%20denoising%20diffusion%0Aprobabilistic%20model%20with%20iterative%20CT%20reconstruction.%20In%20sharp%20contrast%20to%0Aprevious%20studies%2C%20we%20optimize%20the%20fidelity%20loss%20of%20CT%20reconstruction%20with%0Arespect%20to%20the%20latent%20variable%20of%20the%20diffusion%20model%2C%20instead%20of%20the%20image%20and%0Amodel%20parameters.%20To%20suppress%20anatomical%20structure%20changes%20produced%20by%20the%0Adiffusion%20model%2C%20we%20shallow%20the%20diffusion%20and%20reverse%20processes%2C%20and%20fix%20a%20set%0Aof%20added%20noises%20in%20the%20reverse%20process%20to%20make%20it%20deterministic%20during%0Ainference.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20through%0Asparse%20view%20CT%20reconstruction%20of%201/10%20view%20projection%20data.%20Despite%20the%0Asimplicity%20of%20the%20implementation%2C%20the%20proposed%20method%20shows%20the%20capability%20of%0Areconstructing%20high-quality%20images%20while%20preserving%20the%20patient%27s%20anatomical%0Astructure%2C%20and%20outperforms%20existing%20methods%20including%20iterative%20reconstruction%2C%0Aiterative%20reconstruction%20with%20total%20variation%2C%20and%20the%20diffusion%20model%20alone%20in%0Aterms%20of%20quantitative%20indices%20such%20as%20SSIM%20and%20PSNR.%20We%20also%20explore%20further%0Asparse%20view%20CT%20using%201/20%20view%20projection%20data%20with%20the%20same%20trained%20diffusion%0Amodel.%20As%20the%20number%20of%20iterations%20increases%2C%20image%20quality%20improvement%0Acomparable%20to%20that%20of%201/10%20sparse%20view%20CT%20reconstruction%20is%20achieved.%20In%0Aprinciple%2C%20the%20proposed%20method%20can%20be%20widely%20applied%20not%20only%20to%20CT%20but%20also%20to%0Aother%20imaging%20modalities%20such%20as%20MRI%2C%20PET%2C%20and%20SPECT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03156v1&entry.124074799=Read"},
{"title": "Understanding How Blind Users Handle Object Recognition Errors:\n  Strategies and Challenges", "author": "Jonggi Hong and Hernisa Kacorri", "abstract": "  Object recognition technologies hold the potential to support blind and\nlow-vision people in navigating the world around them. However, the gap between\nbenchmark performances and practical usability remains a significant challenge.\nThis paper presents a study aimed at understanding blind users' interaction\nwith object recognition systems for identifying and avoiding errors. Leveraging\na pre-existing object recognition system, URCam, fine-tuned for our experiment,\nwe conducted a user study involving 12 blind and low-vision participants.\nThrough in-depth interviews and hands-on error identification tasks, we gained\ninsights into users' experiences, challenges, and strategies for identifying\nerrors in camera-based assistive technologies and object recognition systems.\nDuring interviews, many participants preferred independent error review, while\nexpressing apprehension toward misrecognitions. In the error identification\ntask, participants varied viewpoints, backgrounds, and object sizes in their\nimages to avoid and overcome errors. Even after repeating the task,\nparticipants identified only half of the errors, and the proportion of errors\nidentified did not significantly differ from their first attempts. Based on\nthese insights, we offer implications for designing accessible interfaces\ntailored to the needs of blind and low-vision users in identifying object\nrecognition errors.\n", "link": "http://arxiv.org/abs/2408.03303v1", "date": "2024-08-06", "relevancy": 2.6139, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.534}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5288}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20How%20Blind%20Users%20Handle%20Object%20Recognition%20Errors%3A%0A%20%20Strategies%20and%20Challenges&body=Title%3A%20Understanding%20How%20Blind%20Users%20Handle%20Object%20Recognition%20Errors%3A%0A%20%20Strategies%20and%20Challenges%0AAuthor%3A%20Jonggi%20Hong%20and%20Hernisa%20Kacorri%0AAbstract%3A%20%20%20Object%20recognition%20technologies%20hold%20the%20potential%20to%20support%20blind%20and%0Alow-vision%20people%20in%20navigating%20the%20world%20around%20them.%20However%2C%20the%20gap%20between%0Abenchmark%20performances%20and%20practical%20usability%20remains%20a%20significant%20challenge.%0AThis%20paper%20presents%20a%20study%20aimed%20at%20understanding%20blind%20users%27%20interaction%0Awith%20object%20recognition%20systems%20for%20identifying%20and%20avoiding%20errors.%20Leveraging%0Aa%20pre-existing%20object%20recognition%20system%2C%20URCam%2C%20fine-tuned%20for%20our%20experiment%2C%0Awe%20conducted%20a%20user%20study%20involving%2012%20blind%20and%20low-vision%20participants.%0AThrough%20in-depth%20interviews%20and%20hands-on%20error%20identification%20tasks%2C%20we%20gained%0Ainsights%20into%20users%27%20experiences%2C%20challenges%2C%20and%20strategies%20for%20identifying%0Aerrors%20in%20camera-based%20assistive%20technologies%20and%20object%20recognition%20systems.%0ADuring%20interviews%2C%20many%20participants%20preferred%20independent%20error%20review%2C%20while%0Aexpressing%20apprehension%20toward%20misrecognitions.%20In%20the%20error%20identification%0Atask%2C%20participants%20varied%20viewpoints%2C%20backgrounds%2C%20and%20object%20sizes%20in%20their%0Aimages%20to%20avoid%20and%20overcome%20errors.%20Even%20after%20repeating%20the%20task%2C%0Aparticipants%20identified%20only%20half%20of%20the%20errors%2C%20and%20the%20proportion%20of%20errors%0Aidentified%20did%20not%20significantly%20differ%20from%20their%20first%20attempts.%20Based%20on%0Athese%20insights%2C%20we%20offer%20implications%20for%20designing%20accessible%20interfaces%0Atailored%20to%20the%20needs%20of%20blind%20and%20low-vision%20users%20in%20identifying%20object%0Arecognition%20errors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03303v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520How%2520Blind%2520Users%2520Handle%2520Object%2520Recognition%2520Errors%253A%250A%2520%2520Strategies%2520and%2520Challenges%26entry.906535625%3DJonggi%2520Hong%2520and%2520Hernisa%2520Kacorri%26entry.1292438233%3D%2520%2520Object%2520recognition%2520technologies%2520hold%2520the%2520potential%2520to%2520support%2520blind%2520and%250Alow-vision%2520people%2520in%2520navigating%2520the%2520world%2520around%2520them.%2520However%252C%2520the%2520gap%2520between%250Abenchmark%2520performances%2520and%2520practical%2520usability%2520remains%2520a%2520significant%2520challenge.%250AThis%2520paper%2520presents%2520a%2520study%2520aimed%2520at%2520understanding%2520blind%2520users%2527%2520interaction%250Awith%2520object%2520recognition%2520systems%2520for%2520identifying%2520and%2520avoiding%2520errors.%2520Leveraging%250Aa%2520pre-existing%2520object%2520recognition%2520system%252C%2520URCam%252C%2520fine-tuned%2520for%2520our%2520experiment%252C%250Awe%2520conducted%2520a%2520user%2520study%2520involving%252012%2520blind%2520and%2520low-vision%2520participants.%250AThrough%2520in-depth%2520interviews%2520and%2520hands-on%2520error%2520identification%2520tasks%252C%2520we%2520gained%250Ainsights%2520into%2520users%2527%2520experiences%252C%2520challenges%252C%2520and%2520strategies%2520for%2520identifying%250Aerrors%2520in%2520camera-based%2520assistive%2520technologies%2520and%2520object%2520recognition%2520systems.%250ADuring%2520interviews%252C%2520many%2520participants%2520preferred%2520independent%2520error%2520review%252C%2520while%250Aexpressing%2520apprehension%2520toward%2520misrecognitions.%2520In%2520the%2520error%2520identification%250Atask%252C%2520participants%2520varied%2520viewpoints%252C%2520backgrounds%252C%2520and%2520object%2520sizes%2520in%2520their%250Aimages%2520to%2520avoid%2520and%2520overcome%2520errors.%2520Even%2520after%2520repeating%2520the%2520task%252C%250Aparticipants%2520identified%2520only%2520half%2520of%2520the%2520errors%252C%2520and%2520the%2520proportion%2520of%2520errors%250Aidentified%2520did%2520not%2520significantly%2520differ%2520from%2520their%2520first%2520attempts.%2520Based%2520on%250Athese%2520insights%252C%2520we%2520offer%2520implications%2520for%2520designing%2520accessible%2520interfaces%250Atailored%2520to%2520the%2520needs%2520of%2520blind%2520and%2520low-vision%2520users%2520in%2520identifying%2520object%250Arecognition%2520errors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03303v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20How%20Blind%20Users%20Handle%20Object%20Recognition%20Errors%3A%0A%20%20Strategies%20and%20Challenges&entry.906535625=Jonggi%20Hong%20and%20Hernisa%20Kacorri&entry.1292438233=%20%20Object%20recognition%20technologies%20hold%20the%20potential%20to%20support%20blind%20and%0Alow-vision%20people%20in%20navigating%20the%20world%20around%20them.%20However%2C%20the%20gap%20between%0Abenchmark%20performances%20and%20practical%20usability%20remains%20a%20significant%20challenge.%0AThis%20paper%20presents%20a%20study%20aimed%20at%20understanding%20blind%20users%27%20interaction%0Awith%20object%20recognition%20systems%20for%20identifying%20and%20avoiding%20errors.%20Leveraging%0Aa%20pre-existing%20object%20recognition%20system%2C%20URCam%2C%20fine-tuned%20for%20our%20experiment%2C%0Awe%20conducted%20a%20user%20study%20involving%2012%20blind%20and%20low-vision%20participants.%0AThrough%20in-depth%20interviews%20and%20hands-on%20error%20identification%20tasks%2C%20we%20gained%0Ainsights%20into%20users%27%20experiences%2C%20challenges%2C%20and%20strategies%20for%20identifying%0Aerrors%20in%20camera-based%20assistive%20technologies%20and%20object%20recognition%20systems.%0ADuring%20interviews%2C%20many%20participants%20preferred%20independent%20error%20review%2C%20while%0Aexpressing%20apprehension%20toward%20misrecognitions.%20In%20the%20error%20identification%0Atask%2C%20participants%20varied%20viewpoints%2C%20backgrounds%2C%20and%20object%20sizes%20in%20their%0Aimages%20to%20avoid%20and%20overcome%20errors.%20Even%20after%20repeating%20the%20task%2C%0Aparticipants%20identified%20only%20half%20of%20the%20errors%2C%20and%20the%20proportion%20of%20errors%0Aidentified%20did%20not%20significantly%20differ%20from%20their%20first%20attempts.%20Based%20on%0Athese%20insights%2C%20we%20offer%20implications%20for%20designing%20accessible%20interfaces%0Atailored%20to%20the%20needs%20of%20blind%20and%20low-vision%20users%20in%20identifying%20object%0Arecognition%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03303v1&entry.124074799=Read"},
{"title": "Learning to Learn without Forgetting using Attention", "author": "Anna Vettoruzzo and Joaquin Vanschoren and Mohamed-Rafik Bouguelia and Thorsteinn R\u00f6gnvaldsson", "abstract": "  Continual learning (CL) refers to the ability to continually learn over time\nby accommodating new knowledge while retaining previously learned experience.\nWhile this concept is inherent in human learning, current machine learning\nmethods are highly prone to overwrite previously learned patterns and thus\nforget past experience. Instead, model parameters should be updated selectively\nand carefully, avoiding unnecessary forgetting while optimally leveraging\npreviously learned patterns to accelerate future learning. Since hand-crafting\neffective update mechanisms is difficult, we propose meta-learning a\ntransformer-based optimizer to enhance CL. This meta-learned optimizer uses\nattention to learn the complex relationships between model parameters across a\nstream of tasks, and is designed to generate effective weight updates for the\ncurrent task while preventing catastrophic forgetting on previously encountered\ntasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and\nSplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both\nforward and backward transfer, even on small sets of labeled data, highlighting\nthe advantages of integrating a meta-learned optimizer within the continual\nlearning framework.\n", "link": "http://arxiv.org/abs/2408.03219v1", "date": "2024-08-06", "relevancy": 2.5871, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5338}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5321}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Learn%20without%20Forgetting%20using%20Attention&body=Title%3A%20Learning%20to%20Learn%20without%20Forgetting%20using%20Attention%0AAuthor%3A%20Anna%20Vettoruzzo%20and%20Joaquin%20Vanschoren%20and%20Mohamed-Rafik%20Bouguelia%20and%20Thorsteinn%20R%C3%B6gnvaldsson%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20refers%20to%20the%20ability%20to%20continually%20learn%20over%20time%0Aby%20accommodating%20new%20knowledge%20while%20retaining%20previously%20learned%20experience.%0AWhile%20this%20concept%20is%20inherent%20in%20human%20learning%2C%20current%20machine%20learning%0Amethods%20are%20highly%20prone%20to%20overwrite%20previously%20learned%20patterns%20and%20thus%0Aforget%20past%20experience.%20Instead%2C%20model%20parameters%20should%20be%20updated%20selectively%0Aand%20carefully%2C%20avoiding%20unnecessary%20forgetting%20while%20optimally%20leveraging%0Apreviously%20learned%20patterns%20to%20accelerate%20future%20learning.%20Since%20hand-crafting%0Aeffective%20update%20mechanisms%20is%20difficult%2C%20we%20propose%20meta-learning%20a%0Atransformer-based%20optimizer%20to%20enhance%20CL.%20This%20meta-learned%20optimizer%20uses%0Aattention%20to%20learn%20the%20complex%20relationships%20between%20model%20parameters%20across%20a%0Astream%20of%20tasks%2C%20and%20is%20designed%20to%20generate%20effective%20weight%20updates%20for%20the%0Acurrent%20task%20while%20preventing%20catastrophic%20forgetting%20on%20previously%20encountered%0Atasks.%20Evaluations%20on%20benchmark%20datasets%20like%20SplitMNIST%2C%20RotatedMNIST%2C%20and%0ASplitCIFAR-100%20affirm%20the%20efficacy%20of%20the%20proposed%20approach%20in%20terms%20of%20both%0Aforward%20and%20backward%20transfer%2C%20even%20on%20small%20sets%20of%20labeled%20data%2C%20highlighting%0Athe%20advantages%20of%20integrating%20a%20meta-learned%20optimizer%20within%20the%20continual%0Alearning%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Learn%2520without%2520Forgetting%2520using%2520Attention%26entry.906535625%3DAnna%2520Vettoruzzo%2520and%2520Joaquin%2520Vanschoren%2520and%2520Mohamed-Rafik%2520Bouguelia%2520and%2520Thorsteinn%2520R%25C3%25B6gnvaldsson%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520refers%2520to%2520the%2520ability%2520to%2520continually%2520learn%2520over%2520time%250Aby%2520accommodating%2520new%2520knowledge%2520while%2520retaining%2520previously%2520learned%2520experience.%250AWhile%2520this%2520concept%2520is%2520inherent%2520in%2520human%2520learning%252C%2520current%2520machine%2520learning%250Amethods%2520are%2520highly%2520prone%2520to%2520overwrite%2520previously%2520learned%2520patterns%2520and%2520thus%250Aforget%2520past%2520experience.%2520Instead%252C%2520model%2520parameters%2520should%2520be%2520updated%2520selectively%250Aand%2520carefully%252C%2520avoiding%2520unnecessary%2520forgetting%2520while%2520optimally%2520leveraging%250Apreviously%2520learned%2520patterns%2520to%2520accelerate%2520future%2520learning.%2520Since%2520hand-crafting%250Aeffective%2520update%2520mechanisms%2520is%2520difficult%252C%2520we%2520propose%2520meta-learning%2520a%250Atransformer-based%2520optimizer%2520to%2520enhance%2520CL.%2520This%2520meta-learned%2520optimizer%2520uses%250Aattention%2520to%2520learn%2520the%2520complex%2520relationships%2520between%2520model%2520parameters%2520across%2520a%250Astream%2520of%2520tasks%252C%2520and%2520is%2520designed%2520to%2520generate%2520effective%2520weight%2520updates%2520for%2520the%250Acurrent%2520task%2520while%2520preventing%2520catastrophic%2520forgetting%2520on%2520previously%2520encountered%250Atasks.%2520Evaluations%2520on%2520benchmark%2520datasets%2520like%2520SplitMNIST%252C%2520RotatedMNIST%252C%2520and%250ASplitCIFAR-100%2520affirm%2520the%2520efficacy%2520of%2520the%2520proposed%2520approach%2520in%2520terms%2520of%2520both%250Aforward%2520and%2520backward%2520transfer%252C%2520even%2520on%2520small%2520sets%2520of%2520labeled%2520data%252C%2520highlighting%250Athe%2520advantages%2520of%2520integrating%2520a%2520meta-learned%2520optimizer%2520within%2520the%2520continual%250Alearning%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Learn%20without%20Forgetting%20using%20Attention&entry.906535625=Anna%20Vettoruzzo%20and%20Joaquin%20Vanschoren%20and%20Mohamed-Rafik%20Bouguelia%20and%20Thorsteinn%20R%C3%B6gnvaldsson&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20refers%20to%20the%20ability%20to%20continually%20learn%20over%20time%0Aby%20accommodating%20new%20knowledge%20while%20retaining%20previously%20learned%20experience.%0AWhile%20this%20concept%20is%20inherent%20in%20human%20learning%2C%20current%20machine%20learning%0Amethods%20are%20highly%20prone%20to%20overwrite%20previously%20learned%20patterns%20and%20thus%0Aforget%20past%20experience.%20Instead%2C%20model%20parameters%20should%20be%20updated%20selectively%0Aand%20carefully%2C%20avoiding%20unnecessary%20forgetting%20while%20optimally%20leveraging%0Apreviously%20learned%20patterns%20to%20accelerate%20future%20learning.%20Since%20hand-crafting%0Aeffective%20update%20mechanisms%20is%20difficult%2C%20we%20propose%20meta-learning%20a%0Atransformer-based%20optimizer%20to%20enhance%20CL.%20This%20meta-learned%20optimizer%20uses%0Aattention%20to%20learn%20the%20complex%20relationships%20between%20model%20parameters%20across%20a%0Astream%20of%20tasks%2C%20and%20is%20designed%20to%20generate%20effective%20weight%20updates%20for%20the%0Acurrent%20task%20while%20preventing%20catastrophic%20forgetting%20on%20previously%20encountered%0Atasks.%20Evaluations%20on%20benchmark%20datasets%20like%20SplitMNIST%2C%20RotatedMNIST%2C%20and%0ASplitCIFAR-100%20affirm%20the%20efficacy%20of%20the%20proposed%20approach%20in%20terms%20of%20both%0Aforward%20and%20backward%20transfer%2C%20even%20on%20small%20sets%20of%20labeled%20data%2C%20highlighting%0Athe%20advantages%20of%20integrating%20a%20meta-learned%20optimizer%20within%20the%20continual%0Alearning%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03219v1&entry.124074799=Read"},
{"title": "Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry\n  Reconstruction in Field Conditions", "author": "Muhammad Arbab Arshad and Talukder Jubery and James Afful and Anushrut Jignasu and Aditya Balu and Baskar Ganapathysubramanian and Soumik Sarkar and Adarsh Krishnamurthy", "abstract": "  We evaluate different Neural Radiance Fields (NeRFs) techniques for the 3D\nreconstruction of plants in varied environments, from indoor settings to\noutdoor fields. Traditional methods usually fail to capture the complex\ngeometric details of plants, which is crucial for phenotyping and breeding\nstudies. We evaluate the reconstruction fidelity of NeRFs in three scenarios\nwith increasing complexity and compare the results with the point cloud\nobtained using LiDAR as ground truth. In the most realistic field scenario, the\nNeRF models achieve a 74.6% F1 score after 30 minutes of training on the GPU,\nhighlighting the efficacy of NeRFs for 3D reconstruction in challenging\nenvironments. Additionally, we propose an early stopping technique for NeRF\ntraining that almost halves the training time while achieving only a reduction\nof 7.4% in the average F1 score. This optimization process significantly\nenhances the speed and efficiency of 3D reconstruction using NeRFs. Our\nfindings demonstrate the potential of NeRFs in detailed and realistic 3D plant\nreconstruction and suggest practical approaches for enhancing the speed and\nefficiency of NeRFs in the 3D reconstruction process.\n", "link": "http://arxiv.org/abs/2402.10344v3", "date": "2024-08-06", "relevancy": 2.5853, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5561}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4975}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Neural%20Radiance%20Fields%20%28NeRFs%29%20for%203D%20Plant%20Geometry%0A%20%20Reconstruction%20in%20Field%20Conditions&body=Title%3A%20Evaluating%20Neural%20Radiance%20Fields%20%28NeRFs%29%20for%203D%20Plant%20Geometry%0A%20%20Reconstruction%20in%20Field%20Conditions%0AAuthor%3A%20Muhammad%20Arbab%20Arshad%20and%20Talukder%20Jubery%20and%20James%20Afful%20and%20Anushrut%20Jignasu%20and%20Aditya%20Balu%20and%20Baskar%20Ganapathysubramanian%20and%20Soumik%20Sarkar%20and%20Adarsh%20Krishnamurthy%0AAbstract%3A%20%20%20We%20evaluate%20different%20Neural%20Radiance%20Fields%20%28NeRFs%29%20techniques%20for%20the%203D%0Areconstruction%20of%20plants%20in%20varied%20environments%2C%20from%20indoor%20settings%20to%0Aoutdoor%20fields.%20Traditional%20methods%20usually%20fail%20to%20capture%20the%20complex%0Ageometric%20details%20of%20plants%2C%20which%20is%20crucial%20for%20phenotyping%20and%20breeding%0Astudies.%20We%20evaluate%20the%20reconstruction%20fidelity%20of%20NeRFs%20in%20three%20scenarios%0Awith%20increasing%20complexity%20and%20compare%20the%20results%20with%20the%20point%20cloud%0Aobtained%20using%20LiDAR%20as%20ground%20truth.%20In%20the%20most%20realistic%20field%20scenario%2C%20the%0ANeRF%20models%20achieve%20a%2074.6%25%20F1%20score%20after%2030%20minutes%20of%20training%20on%20the%20GPU%2C%0Ahighlighting%20the%20efficacy%20of%20NeRFs%20for%203D%20reconstruction%20in%20challenging%0Aenvironments.%20Additionally%2C%20we%20propose%20an%20early%20stopping%20technique%20for%20NeRF%0Atraining%20that%20almost%20halves%20the%20training%20time%20while%20achieving%20only%20a%20reduction%0Aof%207.4%25%20in%20the%20average%20F1%20score.%20This%20optimization%20process%20significantly%0Aenhances%20the%20speed%20and%20efficiency%20of%203D%20reconstruction%20using%20NeRFs.%20Our%0Afindings%20demonstrate%20the%20potential%20of%20NeRFs%20in%20detailed%20and%20realistic%203D%20plant%0Areconstruction%20and%20suggest%20practical%20approaches%20for%20enhancing%20the%20speed%20and%0Aefficiency%20of%20NeRFs%20in%20the%203D%20reconstruction%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10344v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520for%25203D%2520Plant%2520Geometry%250A%2520%2520Reconstruction%2520in%2520Field%2520Conditions%26entry.906535625%3DMuhammad%2520Arbab%2520Arshad%2520and%2520Talukder%2520Jubery%2520and%2520James%2520Afful%2520and%2520Anushrut%2520Jignasu%2520and%2520Aditya%2520Balu%2520and%2520Baskar%2520Ganapathysubramanian%2520and%2520Soumik%2520Sarkar%2520and%2520Adarsh%2520Krishnamurthy%26entry.1292438233%3D%2520%2520We%2520evaluate%2520different%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520techniques%2520for%2520the%25203D%250Areconstruction%2520of%2520plants%2520in%2520varied%2520environments%252C%2520from%2520indoor%2520settings%2520to%250Aoutdoor%2520fields.%2520Traditional%2520methods%2520usually%2520fail%2520to%2520capture%2520the%2520complex%250Ageometric%2520details%2520of%2520plants%252C%2520which%2520is%2520crucial%2520for%2520phenotyping%2520and%2520breeding%250Astudies.%2520We%2520evaluate%2520the%2520reconstruction%2520fidelity%2520of%2520NeRFs%2520in%2520three%2520scenarios%250Awith%2520increasing%2520complexity%2520and%2520compare%2520the%2520results%2520with%2520the%2520point%2520cloud%250Aobtained%2520using%2520LiDAR%2520as%2520ground%2520truth.%2520In%2520the%2520most%2520realistic%2520field%2520scenario%252C%2520the%250ANeRF%2520models%2520achieve%2520a%252074.6%2525%2520F1%2520score%2520after%252030%2520minutes%2520of%2520training%2520on%2520the%2520GPU%252C%250Ahighlighting%2520the%2520efficacy%2520of%2520NeRFs%2520for%25203D%2520reconstruction%2520in%2520challenging%250Aenvironments.%2520Additionally%252C%2520we%2520propose%2520an%2520early%2520stopping%2520technique%2520for%2520NeRF%250Atraining%2520that%2520almost%2520halves%2520the%2520training%2520time%2520while%2520achieving%2520only%2520a%2520reduction%250Aof%25207.4%2525%2520in%2520the%2520average%2520F1%2520score.%2520This%2520optimization%2520process%2520significantly%250Aenhances%2520the%2520speed%2520and%2520efficiency%2520of%25203D%2520reconstruction%2520using%2520NeRFs.%2520Our%250Afindings%2520demonstrate%2520the%2520potential%2520of%2520NeRFs%2520in%2520detailed%2520and%2520realistic%25203D%2520plant%250Areconstruction%2520and%2520suggest%2520practical%2520approaches%2520for%2520enhancing%2520the%2520speed%2520and%250Aefficiency%2520of%2520NeRFs%2520in%2520the%25203D%2520reconstruction%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10344v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Neural%20Radiance%20Fields%20%28NeRFs%29%20for%203D%20Plant%20Geometry%0A%20%20Reconstruction%20in%20Field%20Conditions&entry.906535625=Muhammad%20Arbab%20Arshad%20and%20Talukder%20Jubery%20and%20James%20Afful%20and%20Anushrut%20Jignasu%20and%20Aditya%20Balu%20and%20Baskar%20Ganapathysubramanian%20and%20Soumik%20Sarkar%20and%20Adarsh%20Krishnamurthy&entry.1292438233=%20%20We%20evaluate%20different%20Neural%20Radiance%20Fields%20%28NeRFs%29%20techniques%20for%20the%203D%0Areconstruction%20of%20plants%20in%20varied%20environments%2C%20from%20indoor%20settings%20to%0Aoutdoor%20fields.%20Traditional%20methods%20usually%20fail%20to%20capture%20the%20complex%0Ageometric%20details%20of%20plants%2C%20which%20is%20crucial%20for%20phenotyping%20and%20breeding%0Astudies.%20We%20evaluate%20the%20reconstruction%20fidelity%20of%20NeRFs%20in%20three%20scenarios%0Awith%20increasing%20complexity%20and%20compare%20the%20results%20with%20the%20point%20cloud%0Aobtained%20using%20LiDAR%20as%20ground%20truth.%20In%20the%20most%20realistic%20field%20scenario%2C%20the%0ANeRF%20models%20achieve%20a%2074.6%25%20F1%20score%20after%2030%20minutes%20of%20training%20on%20the%20GPU%2C%0Ahighlighting%20the%20efficacy%20of%20NeRFs%20for%203D%20reconstruction%20in%20challenging%0Aenvironments.%20Additionally%2C%20we%20propose%20an%20early%20stopping%20technique%20for%20NeRF%0Atraining%20that%20almost%20halves%20the%20training%20time%20while%20achieving%20only%20a%20reduction%0Aof%207.4%25%20in%20the%20average%20F1%20score.%20This%20optimization%20process%20significantly%0Aenhances%20the%20speed%20and%20efficiency%20of%203D%20reconstruction%20using%20NeRFs.%20Our%0Afindings%20demonstrate%20the%20potential%20of%20NeRFs%20in%20detailed%20and%20realistic%203D%20plant%0Areconstruction%20and%20suggest%20practical%20approaches%20for%20enhancing%20the%20speed%20and%0Aefficiency%20of%20NeRFs%20in%20the%203D%20reconstruction%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10344v3&entry.124074799=Read"},
{"title": "SGSR: Structure-Guided Multi-Contrast MRI Super-Resolution via\n  Spatio-Frequency Co-Query Attention", "author": "Shaoming Zheng and Yinsong Wang and Siyi Du and Chen Qin", "abstract": "  Magnetic Resonance Imaging (MRI) is a leading diagnostic modality for a wide\nrange of exams, where multiple contrast images are often acquired for\ncharacterizing different tissues. However, acquiring high-resolution MRI\ntypically extends scan time, which can introduce motion artifacts.\nSuper-resolution of MRI therefore emerges as a promising approach to mitigate\nthese challenges. Earlier studies have investigated the use of multiple\ncontrasts for MRI super-resolution (MCSR), whereas majority of them did not\nfully exploit the rich contrast-invariant structural information. To fully\nutilize such crucial prior knowledge of multi-contrast MRI, in this work, we\npropose a novel structure-guided MCSR (SGSR) framework based on a new\nspatio-frequency co-query attention (CQA) mechanism. Specifically, CQA performs\nattention on features of multiple contrasts with a shared structural query,\nwhich is particularly designed to extract, fuse, and refine the common\nstructures from different contrasts. We further propose a novel\nfrequency-domain CQA module in addition to the spatial domain, to enable more\nfine-grained structural refinement. Extensive experiments on fastMRI knee data\nand low-field brain MRI show that SGSR outperforms state-of-the-art MCSR\nmethods with statistical significance.\n", "link": "http://arxiv.org/abs/2408.03194v1", "date": "2024-08-06", "relevancy": 2.5792, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5678}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4899}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGSR%3A%20Structure-Guided%20Multi-Contrast%20MRI%20Super-Resolution%20via%0A%20%20Spatio-Frequency%20Co-Query%20Attention&body=Title%3A%20SGSR%3A%20Structure-Guided%20Multi-Contrast%20MRI%20Super-Resolution%20via%0A%20%20Spatio-Frequency%20Co-Query%20Attention%0AAuthor%3A%20Shaoming%20Zheng%20and%20Yinsong%20Wang%20and%20Siyi%20Du%20and%20Chen%20Qin%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20a%20leading%20diagnostic%20modality%20for%20a%20wide%0Arange%20of%20exams%2C%20where%20multiple%20contrast%20images%20are%20often%20acquired%20for%0Acharacterizing%20different%20tissues.%20However%2C%20acquiring%20high-resolution%20MRI%0Atypically%20extends%20scan%20time%2C%20which%20can%20introduce%20motion%20artifacts.%0ASuper-resolution%20of%20MRI%20therefore%20emerges%20as%20a%20promising%20approach%20to%20mitigate%0Athese%20challenges.%20Earlier%20studies%20have%20investigated%20the%20use%20of%20multiple%0Acontrasts%20for%20MRI%20super-resolution%20%28MCSR%29%2C%20whereas%20majority%20of%20them%20did%20not%0Afully%20exploit%20the%20rich%20contrast-invariant%20structural%20information.%20To%20fully%0Autilize%20such%20crucial%20prior%20knowledge%20of%20multi-contrast%20MRI%2C%20in%20this%20work%2C%20we%0Apropose%20a%20novel%20structure-guided%20MCSR%20%28SGSR%29%20framework%20based%20on%20a%20new%0Aspatio-frequency%20co-query%20attention%20%28CQA%29%20mechanism.%20Specifically%2C%20CQA%20performs%0Aattention%20on%20features%20of%20multiple%20contrasts%20with%20a%20shared%20structural%20query%2C%0Awhich%20is%20particularly%20designed%20to%20extract%2C%20fuse%2C%20and%20refine%20the%20common%0Astructures%20from%20different%20contrasts.%20We%20further%20propose%20a%20novel%0Afrequency-domain%20CQA%20module%20in%20addition%20to%20the%20spatial%20domain%2C%20to%20enable%20more%0Afine-grained%20structural%20refinement.%20Extensive%20experiments%20on%20fastMRI%20knee%20data%0Aand%20low-field%20brain%20MRI%20show%20that%20SGSR%20outperforms%20state-of-the-art%20MCSR%0Amethods%20with%20statistical%20significance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGSR%253A%2520Structure-Guided%2520Multi-Contrast%2520MRI%2520Super-Resolution%2520via%250A%2520%2520Spatio-Frequency%2520Co-Query%2520Attention%26entry.906535625%3DShaoming%2520Zheng%2520and%2520Yinsong%2520Wang%2520and%2520Siyi%2520Du%2520and%2520Chen%2520Qin%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520is%2520a%2520leading%2520diagnostic%2520modality%2520for%2520a%2520wide%250Arange%2520of%2520exams%252C%2520where%2520multiple%2520contrast%2520images%2520are%2520often%2520acquired%2520for%250Acharacterizing%2520different%2520tissues.%2520However%252C%2520acquiring%2520high-resolution%2520MRI%250Atypically%2520extends%2520scan%2520time%252C%2520which%2520can%2520introduce%2520motion%2520artifacts.%250ASuper-resolution%2520of%2520MRI%2520therefore%2520emerges%2520as%2520a%2520promising%2520approach%2520to%2520mitigate%250Athese%2520challenges.%2520Earlier%2520studies%2520have%2520investigated%2520the%2520use%2520of%2520multiple%250Acontrasts%2520for%2520MRI%2520super-resolution%2520%2528MCSR%2529%252C%2520whereas%2520majority%2520of%2520them%2520did%2520not%250Afully%2520exploit%2520the%2520rich%2520contrast-invariant%2520structural%2520information.%2520To%2520fully%250Autilize%2520such%2520crucial%2520prior%2520knowledge%2520of%2520multi-contrast%2520MRI%252C%2520in%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520structure-guided%2520MCSR%2520%2528SGSR%2529%2520framework%2520based%2520on%2520a%2520new%250Aspatio-frequency%2520co-query%2520attention%2520%2528CQA%2529%2520mechanism.%2520Specifically%252C%2520CQA%2520performs%250Aattention%2520on%2520features%2520of%2520multiple%2520contrasts%2520with%2520a%2520shared%2520structural%2520query%252C%250Awhich%2520is%2520particularly%2520designed%2520to%2520extract%252C%2520fuse%252C%2520and%2520refine%2520the%2520common%250Astructures%2520from%2520different%2520contrasts.%2520We%2520further%2520propose%2520a%2520novel%250Afrequency-domain%2520CQA%2520module%2520in%2520addition%2520to%2520the%2520spatial%2520domain%252C%2520to%2520enable%2520more%250Afine-grained%2520structural%2520refinement.%2520Extensive%2520experiments%2520on%2520fastMRI%2520knee%2520data%250Aand%2520low-field%2520brain%2520MRI%2520show%2520that%2520SGSR%2520outperforms%2520state-of-the-art%2520MCSR%250Amethods%2520with%2520statistical%2520significance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGSR%3A%20Structure-Guided%20Multi-Contrast%20MRI%20Super-Resolution%20via%0A%20%20Spatio-Frequency%20Co-Query%20Attention&entry.906535625=Shaoming%20Zheng%20and%20Yinsong%20Wang%20and%20Siyi%20Du%20and%20Chen%20Qin&entry.1292438233=%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20a%20leading%20diagnostic%20modality%20for%20a%20wide%0Arange%20of%20exams%2C%20where%20multiple%20contrast%20images%20are%20often%20acquired%20for%0Acharacterizing%20different%20tissues.%20However%2C%20acquiring%20high-resolution%20MRI%0Atypically%20extends%20scan%20time%2C%20which%20can%20introduce%20motion%20artifacts.%0ASuper-resolution%20of%20MRI%20therefore%20emerges%20as%20a%20promising%20approach%20to%20mitigate%0Athese%20challenges.%20Earlier%20studies%20have%20investigated%20the%20use%20of%20multiple%0Acontrasts%20for%20MRI%20super-resolution%20%28MCSR%29%2C%20whereas%20majority%20of%20them%20did%20not%0Afully%20exploit%20the%20rich%20contrast-invariant%20structural%20information.%20To%20fully%0Autilize%20such%20crucial%20prior%20knowledge%20of%20multi-contrast%20MRI%2C%20in%20this%20work%2C%20we%0Apropose%20a%20novel%20structure-guided%20MCSR%20%28SGSR%29%20framework%20based%20on%20a%20new%0Aspatio-frequency%20co-query%20attention%20%28CQA%29%20mechanism.%20Specifically%2C%20CQA%20performs%0Aattention%20on%20features%20of%20multiple%20contrasts%20with%20a%20shared%20structural%20query%2C%0Awhich%20is%20particularly%20designed%20to%20extract%2C%20fuse%2C%20and%20refine%20the%20common%0Astructures%20from%20different%20contrasts.%20We%20further%20propose%20a%20novel%0Afrequency-domain%20CQA%20module%20in%20addition%20to%20the%20spatial%20domain%2C%20to%20enable%20more%0Afine-grained%20structural%20refinement.%20Extensive%20experiments%20on%20fastMRI%20knee%20data%0Aand%20low-field%20brain%20MRI%20show%20that%20SGSR%20outperforms%20state-of-the-art%20MCSR%0Amethods%20with%20statistical%20significance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03194v1&entry.124074799=Read"},
{"title": "PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single\n  Highly-Ambiguous RGB Images", "author": "Yiheng Xiong and Angela Dai", "abstract": "  Generating 3D shapes from single RGB images is essential in various\napplications such as robotics. Current approaches typically target images\ncontaining clear and complete visual descriptions of the object, without\nconsidering common realistic cases where observations of objects that are\nlargely occluded or truncated. We thus propose a transformer-based\nautoregressive model to generate the probabilistic distribution of 3D shapes\nconditioned on an RGB image containing potentially highly ambiguous\nobservations of the object. To handle realistic scenarios such as occlusion or\nfield-of-view truncation, we create simulated image-to-shape training pairs\nthat enable improved fine-tuning for real-world scenarios. We then adopt\ncross-attention to effectively identify the most relevant region of interest\nfrom the input image for shape generation. This enables inference of sampled\nshapes with reasonable diversity and strong alignment with the input image. We\ntrain and test our model on our synthetic data then fine-tune and test it on\nreal-world data. Experiments demonstrate that our model outperforms state of\nthe art in both scenarios.\n", "link": "http://arxiv.org/abs/2405.11914v2", "date": "2024-08-06", "relevancy": 2.5058, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6397}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6301}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PT43D%3A%20A%20Probabilistic%20Transformer%20for%20Generating%203D%20Shapes%20from%20Single%0A%20%20Highly-Ambiguous%20RGB%20Images&body=Title%3A%20PT43D%3A%20A%20Probabilistic%20Transformer%20for%20Generating%203D%20Shapes%20from%20Single%0A%20%20Highly-Ambiguous%20RGB%20Images%0AAuthor%3A%20Yiheng%20Xiong%20and%20Angela%20Dai%0AAbstract%3A%20%20%20Generating%203D%20shapes%20from%20single%20RGB%20images%20is%20essential%20in%20various%0Aapplications%20such%20as%20robotics.%20Current%20approaches%20typically%20target%20images%0Acontaining%20clear%20and%20complete%20visual%20descriptions%20of%20the%20object%2C%20without%0Aconsidering%20common%20realistic%20cases%20where%20observations%20of%20objects%20that%20are%0Alargely%20occluded%20or%20truncated.%20We%20thus%20propose%20a%20transformer-based%0Aautoregressive%20model%20to%20generate%20the%20probabilistic%20distribution%20of%203D%20shapes%0Aconditioned%20on%20an%20RGB%20image%20containing%20potentially%20highly%20ambiguous%0Aobservations%20of%20the%20object.%20To%20handle%20realistic%20scenarios%20such%20as%20occlusion%20or%0Afield-of-view%20truncation%2C%20we%20create%20simulated%20image-to-shape%20training%20pairs%0Athat%20enable%20improved%20fine-tuning%20for%20real-world%20scenarios.%20We%20then%20adopt%0Across-attention%20to%20effectively%20identify%20the%20most%20relevant%20region%20of%20interest%0Afrom%20the%20input%20image%20for%20shape%20generation.%20This%20enables%20inference%20of%20sampled%0Ashapes%20with%20reasonable%20diversity%20and%20strong%20alignment%20with%20the%20input%20image.%20We%0Atrain%20and%20test%20our%20model%20on%20our%20synthetic%20data%20then%20fine-tune%20and%20test%20it%20on%0Areal-world%20data.%20Experiments%20demonstrate%20that%20our%20model%20outperforms%20state%20of%0Athe%20art%20in%20both%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11914v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPT43D%253A%2520A%2520Probabilistic%2520Transformer%2520for%2520Generating%25203D%2520Shapes%2520from%2520Single%250A%2520%2520Highly-Ambiguous%2520RGB%2520Images%26entry.906535625%3DYiheng%2520Xiong%2520and%2520Angela%2520Dai%26entry.1292438233%3D%2520%2520Generating%25203D%2520shapes%2520from%2520single%2520RGB%2520images%2520is%2520essential%2520in%2520various%250Aapplications%2520such%2520as%2520robotics.%2520Current%2520approaches%2520typically%2520target%2520images%250Acontaining%2520clear%2520and%2520complete%2520visual%2520descriptions%2520of%2520the%2520object%252C%2520without%250Aconsidering%2520common%2520realistic%2520cases%2520where%2520observations%2520of%2520objects%2520that%2520are%250Alargely%2520occluded%2520or%2520truncated.%2520We%2520thus%2520propose%2520a%2520transformer-based%250Aautoregressive%2520model%2520to%2520generate%2520the%2520probabilistic%2520distribution%2520of%25203D%2520shapes%250Aconditioned%2520on%2520an%2520RGB%2520image%2520containing%2520potentially%2520highly%2520ambiguous%250Aobservations%2520of%2520the%2520object.%2520To%2520handle%2520realistic%2520scenarios%2520such%2520as%2520occlusion%2520or%250Afield-of-view%2520truncation%252C%2520we%2520create%2520simulated%2520image-to-shape%2520training%2520pairs%250Athat%2520enable%2520improved%2520fine-tuning%2520for%2520real-world%2520scenarios.%2520We%2520then%2520adopt%250Across-attention%2520to%2520effectively%2520identify%2520the%2520most%2520relevant%2520region%2520of%2520interest%250Afrom%2520the%2520input%2520image%2520for%2520shape%2520generation.%2520This%2520enables%2520inference%2520of%2520sampled%250Ashapes%2520with%2520reasonable%2520diversity%2520and%2520strong%2520alignment%2520with%2520the%2520input%2520image.%2520We%250Atrain%2520and%2520test%2520our%2520model%2520on%2520our%2520synthetic%2520data%2520then%2520fine-tune%2520and%2520test%2520it%2520on%250Areal-world%2520data.%2520Experiments%2520demonstrate%2520that%2520our%2520model%2520outperforms%2520state%2520of%250Athe%2520art%2520in%2520both%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11914v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PT43D%3A%20A%20Probabilistic%20Transformer%20for%20Generating%203D%20Shapes%20from%20Single%0A%20%20Highly-Ambiguous%20RGB%20Images&entry.906535625=Yiheng%20Xiong%20and%20Angela%20Dai&entry.1292438233=%20%20Generating%203D%20shapes%20from%20single%20RGB%20images%20is%20essential%20in%20various%0Aapplications%20such%20as%20robotics.%20Current%20approaches%20typically%20target%20images%0Acontaining%20clear%20and%20complete%20visual%20descriptions%20of%20the%20object%2C%20without%0Aconsidering%20common%20realistic%20cases%20where%20observations%20of%20objects%20that%20are%0Alargely%20occluded%20or%20truncated.%20We%20thus%20propose%20a%20transformer-based%0Aautoregressive%20model%20to%20generate%20the%20probabilistic%20distribution%20of%203D%20shapes%0Aconditioned%20on%20an%20RGB%20image%20containing%20potentially%20highly%20ambiguous%0Aobservations%20of%20the%20object.%20To%20handle%20realistic%20scenarios%20such%20as%20occlusion%20or%0Afield-of-view%20truncation%2C%20we%20create%20simulated%20image-to-shape%20training%20pairs%0Athat%20enable%20improved%20fine-tuning%20for%20real-world%20scenarios.%20We%20then%20adopt%0Across-attention%20to%20effectively%20identify%20the%20most%20relevant%20region%20of%20interest%0Afrom%20the%20input%20image%20for%20shape%20generation.%20This%20enables%20inference%20of%20sampled%0Ashapes%20with%20reasonable%20diversity%20and%20strong%20alignment%20with%20the%20input%20image.%20We%0Atrain%20and%20test%20our%20model%20on%20our%20synthetic%20data%20then%20fine-tune%20and%20test%20it%20on%0Areal-world%20data.%20Experiments%20demonstrate%20that%20our%20model%20outperforms%20state%20of%0Athe%20art%20in%20both%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11914v2&entry.124074799=Read"},
{"title": "TSC: A Simple Two-Sided Constraint against Over-Smoothing", "author": "Furong Peng and Kang Liu and Xuan Lu and Yuhua Qian and Hongren Yan and Chao Ma", "abstract": "  Graph Convolutional Neural Network (GCN), a widely adopted method for\nanalyzing relational data, enhances node discriminability through the\naggregation of neighboring information. Usually, stacking multiple layers can\nimprove the performance of GCN by leveraging information from high-order\nneighbors. However, the increase of the network depth will induce the\nover-smoothing problem, which can be attributed to the quality and quantity of\nneighbors changing: (a) neighbor quality, node's neighbors become overlapping\nin high order, leading to aggregated information becoming indistinguishable,\n(b) neighbor quantity, the exponentially growing aggregated neighbors submerges\nthe node's initial feature by recursively aggregating operations. Current\nsolutions mainly focus on one of the above causes and seldom consider both at\nonce.\n  Aiming at tackling both causes of over-smoothing in one shot, we introduce a\nsimple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet\npotent techniques: random masking and contrastive constraint. The random\nmasking acts on the representation matrix's columns to regulate the degree of\ninformation aggregation from neighbors, thus preventing the convergence of node\nrepresentations. Meanwhile, the contrastive constraint, applied to the\nrepresentation matrix's rows, enhances the discriminability of the nodes.\nDesigned as a plug-in module, TSC can be easily coupled with GCN or SGC\narchitectures. Experimental analyses on diverse real-world graph datasets\nverify that our approach markedly reduces the convergence of node's\nrepresentation and the performance degradation in deeper GCN.\n", "link": "http://arxiv.org/abs/2408.03152v1", "date": "2024-08-06", "relevancy": 2.4907, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.503}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4964}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSC%3A%20A%20Simple%20Two-Sided%20Constraint%20against%20Over-Smoothing&body=Title%3A%20TSC%3A%20A%20Simple%20Two-Sided%20Constraint%20against%20Over-Smoothing%0AAuthor%3A%20Furong%20Peng%20and%20Kang%20Liu%20and%20Xuan%20Lu%20and%20Yuhua%20Qian%20and%20Hongren%20Yan%20and%20Chao%20Ma%0AAbstract%3A%20%20%20Graph%20Convolutional%20Neural%20Network%20%28GCN%29%2C%20a%20widely%20adopted%20method%20for%0Aanalyzing%20relational%20data%2C%20enhances%20node%20discriminability%20through%20the%0Aaggregation%20of%20neighboring%20information.%20Usually%2C%20stacking%20multiple%20layers%20can%0Aimprove%20the%20performance%20of%20GCN%20by%20leveraging%20information%20from%20high-order%0Aneighbors.%20However%2C%20the%20increase%20of%20the%20network%20depth%20will%20induce%20the%0Aover-smoothing%20problem%2C%20which%20can%20be%20attributed%20to%20the%20quality%20and%20quantity%20of%0Aneighbors%20changing%3A%20%28a%29%20neighbor%20quality%2C%20node%27s%20neighbors%20become%20overlapping%0Ain%20high%20order%2C%20leading%20to%20aggregated%20information%20becoming%20indistinguishable%2C%0A%28b%29%20neighbor%20quantity%2C%20the%20exponentially%20growing%20aggregated%20neighbors%20submerges%0Athe%20node%27s%20initial%20feature%20by%20recursively%20aggregating%20operations.%20Current%0Asolutions%20mainly%20focus%20on%20one%20of%20the%20above%20causes%20and%20seldom%20consider%20both%20at%0Aonce.%0A%20%20Aiming%20at%20tackling%20both%20causes%20of%20over-smoothing%20in%20one%20shot%2C%20we%20introduce%20a%0Asimple%20Two-Sided%20Constraint%20%28TSC%29%20for%20GCNs%2C%20comprising%20two%20straightforward%20yet%0Apotent%20techniques%3A%20random%20masking%20and%20contrastive%20constraint.%20The%20random%0Amasking%20acts%20on%20the%20representation%20matrix%27s%20columns%20to%20regulate%20the%20degree%20of%0Ainformation%20aggregation%20from%20neighbors%2C%20thus%20preventing%20the%20convergence%20of%20node%0Arepresentations.%20Meanwhile%2C%20the%20contrastive%20constraint%2C%20applied%20to%20the%0Arepresentation%20matrix%27s%20rows%2C%20enhances%20the%20discriminability%20of%20the%20nodes.%0ADesigned%20as%20a%20plug-in%20module%2C%20TSC%20can%20be%20easily%20coupled%20with%20GCN%20or%20SGC%0Aarchitectures.%20Experimental%20analyses%20on%20diverse%20real-world%20graph%20datasets%0Averify%20that%20our%20approach%20markedly%20reduces%20the%20convergence%20of%20node%27s%0Arepresentation%20and%20the%20performance%20degradation%20in%20deeper%20GCN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSC%253A%2520A%2520Simple%2520Two-Sided%2520Constraint%2520against%2520Over-Smoothing%26entry.906535625%3DFurong%2520Peng%2520and%2520Kang%2520Liu%2520and%2520Xuan%2520Lu%2520and%2520Yuhua%2520Qian%2520and%2520Hongren%2520Yan%2520and%2520Chao%2520Ma%26entry.1292438233%3D%2520%2520Graph%2520Convolutional%2520Neural%2520Network%2520%2528GCN%2529%252C%2520a%2520widely%2520adopted%2520method%2520for%250Aanalyzing%2520relational%2520data%252C%2520enhances%2520node%2520discriminability%2520through%2520the%250Aaggregation%2520of%2520neighboring%2520information.%2520Usually%252C%2520stacking%2520multiple%2520layers%2520can%250Aimprove%2520the%2520performance%2520of%2520GCN%2520by%2520leveraging%2520information%2520from%2520high-order%250Aneighbors.%2520However%252C%2520the%2520increase%2520of%2520the%2520network%2520depth%2520will%2520induce%2520the%250Aover-smoothing%2520problem%252C%2520which%2520can%2520be%2520attributed%2520to%2520the%2520quality%2520and%2520quantity%2520of%250Aneighbors%2520changing%253A%2520%2528a%2529%2520neighbor%2520quality%252C%2520node%2527s%2520neighbors%2520become%2520overlapping%250Ain%2520high%2520order%252C%2520leading%2520to%2520aggregated%2520information%2520becoming%2520indistinguishable%252C%250A%2528b%2529%2520neighbor%2520quantity%252C%2520the%2520exponentially%2520growing%2520aggregated%2520neighbors%2520submerges%250Athe%2520node%2527s%2520initial%2520feature%2520by%2520recursively%2520aggregating%2520operations.%2520Current%250Asolutions%2520mainly%2520focus%2520on%2520one%2520of%2520the%2520above%2520causes%2520and%2520seldom%2520consider%2520both%2520at%250Aonce.%250A%2520%2520Aiming%2520at%2520tackling%2520both%2520causes%2520of%2520over-smoothing%2520in%2520one%2520shot%252C%2520we%2520introduce%2520a%250Asimple%2520Two-Sided%2520Constraint%2520%2528TSC%2529%2520for%2520GCNs%252C%2520comprising%2520two%2520straightforward%2520yet%250Apotent%2520techniques%253A%2520random%2520masking%2520and%2520contrastive%2520constraint.%2520The%2520random%250Amasking%2520acts%2520on%2520the%2520representation%2520matrix%2527s%2520columns%2520to%2520regulate%2520the%2520degree%2520of%250Ainformation%2520aggregation%2520from%2520neighbors%252C%2520thus%2520preventing%2520the%2520convergence%2520of%2520node%250Arepresentations.%2520Meanwhile%252C%2520the%2520contrastive%2520constraint%252C%2520applied%2520to%2520the%250Arepresentation%2520matrix%2527s%2520rows%252C%2520enhances%2520the%2520discriminability%2520of%2520the%2520nodes.%250ADesigned%2520as%2520a%2520plug-in%2520module%252C%2520TSC%2520can%2520be%2520easily%2520coupled%2520with%2520GCN%2520or%2520SGC%250Aarchitectures.%2520Experimental%2520analyses%2520on%2520diverse%2520real-world%2520graph%2520datasets%250Averify%2520that%2520our%2520approach%2520markedly%2520reduces%2520the%2520convergence%2520of%2520node%2527s%250Arepresentation%2520and%2520the%2520performance%2520degradation%2520in%2520deeper%2520GCN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSC%3A%20A%20Simple%20Two-Sided%20Constraint%20against%20Over-Smoothing&entry.906535625=Furong%20Peng%20and%20Kang%20Liu%20and%20Xuan%20Lu%20and%20Yuhua%20Qian%20and%20Hongren%20Yan%20and%20Chao%20Ma&entry.1292438233=%20%20Graph%20Convolutional%20Neural%20Network%20%28GCN%29%2C%20a%20widely%20adopted%20method%20for%0Aanalyzing%20relational%20data%2C%20enhances%20node%20discriminability%20through%20the%0Aaggregation%20of%20neighboring%20information.%20Usually%2C%20stacking%20multiple%20layers%20can%0Aimprove%20the%20performance%20of%20GCN%20by%20leveraging%20information%20from%20high-order%0Aneighbors.%20However%2C%20the%20increase%20of%20the%20network%20depth%20will%20induce%20the%0Aover-smoothing%20problem%2C%20which%20can%20be%20attributed%20to%20the%20quality%20and%20quantity%20of%0Aneighbors%20changing%3A%20%28a%29%20neighbor%20quality%2C%20node%27s%20neighbors%20become%20overlapping%0Ain%20high%20order%2C%20leading%20to%20aggregated%20information%20becoming%20indistinguishable%2C%0A%28b%29%20neighbor%20quantity%2C%20the%20exponentially%20growing%20aggregated%20neighbors%20submerges%0Athe%20node%27s%20initial%20feature%20by%20recursively%20aggregating%20operations.%20Current%0Asolutions%20mainly%20focus%20on%20one%20of%20the%20above%20causes%20and%20seldom%20consider%20both%20at%0Aonce.%0A%20%20Aiming%20at%20tackling%20both%20causes%20of%20over-smoothing%20in%20one%20shot%2C%20we%20introduce%20a%0Asimple%20Two-Sided%20Constraint%20%28TSC%29%20for%20GCNs%2C%20comprising%20two%20straightforward%20yet%0Apotent%20techniques%3A%20random%20masking%20and%20contrastive%20constraint.%20The%20random%0Amasking%20acts%20on%20the%20representation%20matrix%27s%20columns%20to%20regulate%20the%20degree%20of%0Ainformation%20aggregation%20from%20neighbors%2C%20thus%20preventing%20the%20convergence%20of%20node%0Arepresentations.%20Meanwhile%2C%20the%20contrastive%20constraint%2C%20applied%20to%20the%0Arepresentation%20matrix%27s%20rows%2C%20enhances%20the%20discriminability%20of%20the%20nodes.%0ADesigned%20as%20a%20plug-in%20module%2C%20TSC%20can%20be%20easily%20coupled%20with%20GCN%20or%20SGC%0Aarchitectures.%20Experimental%20analyses%20on%20diverse%20real-world%20graph%20datasets%0Averify%20that%20our%20approach%20markedly%20reduces%20the%20convergence%20of%20node%27s%0Arepresentation%20and%20the%20performance%20degradation%20in%20deeper%20GCN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03152v1&entry.124074799=Read"},
{"title": "SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery\n  Videos via Physics-embedded 3D Gaussians", "author": "Zhenya Yang and Kai Chen and Yonghao Long and Qi Dou", "abstract": "  Surgical scene simulation plays a crucial role in surgical education and\nsimulator-based robot learning. Traditional approaches for creating these\nenvironments with surgical scene involve a labor-intensive process where\ndesigners hand-craft tissues models with textures and geometries for soft body\nsimulations. This manual approach is not only time-consuming but also limited\nin the scalability and realism. In contrast, data-driven simulation offers a\ncompelling alternative. It has the potential to automatically reconstruct 3D\nsurgical scenes from real-world surgical video data, followed by the\napplication of soft body physics. This area, however, is relatively uncharted.\nIn our research, we introduce 3D Gaussian as a learnable representation for\nsurgical scene, which is learned from stereo endoscopic video. To prevent\nover-fitting and ensure the geometrical correctness of these scenes, we\nincorporate depth supervision and anisotropy regularization into the Gaussian\nlearning process. Furthermore, we apply the Material Point Method, which is\nintegrated with physical properties, to the 3D Gaussians to achieve realistic\nscene deformations. Our method was evaluated on our collected in-house and\npublic surgical videos datasets. Results show that it can reconstruct and\nsimulate surgical scenes from endoscopic videos efficiently-taking only a few\nminutes to reconstruct the surgical scene-and produce both visually and\nphysically plausible deformations at a speed approaching real-time. The results\ndemonstrate great potential of our proposed method to enhance the efficiency\nand variety of simulations available for surgical education and robot learning.\n", "link": "http://arxiv.org/abs/2405.00956v3", "date": "2024-08-06", "relevancy": 2.4773, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6365}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6183}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimEndoGS%3A%20Efficient%20Data-driven%20Scene%20Simulation%20using%20Robotic%20Surgery%0A%20%20Videos%20via%20Physics-embedded%203D%20Gaussians&body=Title%3A%20SimEndoGS%3A%20Efficient%20Data-driven%20Scene%20Simulation%20using%20Robotic%20Surgery%0A%20%20Videos%20via%20Physics-embedded%203D%20Gaussians%0AAuthor%3A%20Zhenya%20Yang%20and%20Kai%20Chen%20and%20Yonghao%20Long%20and%20Qi%20Dou%0AAbstract%3A%20%20%20Surgical%20scene%20simulation%20plays%20a%20crucial%20role%20in%20surgical%20education%20and%0Asimulator-based%20robot%20learning.%20Traditional%20approaches%20for%20creating%20these%0Aenvironments%20with%20surgical%20scene%20involve%20a%20labor-intensive%20process%20where%0Adesigners%20hand-craft%20tissues%20models%20with%20textures%20and%20geometries%20for%20soft%20body%0Asimulations.%20This%20manual%20approach%20is%20not%20only%20time-consuming%20but%20also%20limited%0Ain%20the%20scalability%20and%20realism.%20In%20contrast%2C%20data-driven%20simulation%20offers%20a%0Acompelling%20alternative.%20It%20has%20the%20potential%20to%20automatically%20reconstruct%203D%0Asurgical%20scenes%20from%20real-world%20surgical%20video%20data%2C%20followed%20by%20the%0Aapplication%20of%20soft%20body%20physics.%20This%20area%2C%20however%2C%20is%20relatively%20uncharted.%0AIn%20our%20research%2C%20we%20introduce%203D%20Gaussian%20as%20a%20learnable%20representation%20for%0Asurgical%20scene%2C%20which%20is%20learned%20from%20stereo%20endoscopic%20video.%20To%20prevent%0Aover-fitting%20and%20ensure%20the%20geometrical%20correctness%20of%20these%20scenes%2C%20we%0Aincorporate%20depth%20supervision%20and%20anisotropy%20regularization%20into%20the%20Gaussian%0Alearning%20process.%20Furthermore%2C%20we%20apply%20the%20Material%20Point%20Method%2C%20which%20is%0Aintegrated%20with%20physical%20properties%2C%20to%20the%203D%20Gaussians%20to%20achieve%20realistic%0Ascene%20deformations.%20Our%20method%20was%20evaluated%20on%20our%20collected%20in-house%20and%0Apublic%20surgical%20videos%20datasets.%20Results%20show%20that%20it%20can%20reconstruct%20and%0Asimulate%20surgical%20scenes%20from%20endoscopic%20videos%20efficiently-taking%20only%20a%20few%0Aminutes%20to%20reconstruct%20the%20surgical%20scene-and%20produce%20both%20visually%20and%0Aphysically%20plausible%20deformations%20at%20a%20speed%20approaching%20real-time.%20The%20results%0Ademonstrate%20great%20potential%20of%20our%20proposed%20method%20to%20enhance%20the%20efficiency%0Aand%20variety%20of%20simulations%20available%20for%20surgical%20education%20and%20robot%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00956v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimEndoGS%253A%2520Efficient%2520Data-driven%2520Scene%2520Simulation%2520using%2520Robotic%2520Surgery%250A%2520%2520Videos%2520via%2520Physics-embedded%25203D%2520Gaussians%26entry.906535625%3DZhenya%2520Yang%2520and%2520Kai%2520Chen%2520and%2520Yonghao%2520Long%2520and%2520Qi%2520Dou%26entry.1292438233%3D%2520%2520Surgical%2520scene%2520simulation%2520plays%2520a%2520crucial%2520role%2520in%2520surgical%2520education%2520and%250Asimulator-based%2520robot%2520learning.%2520Traditional%2520approaches%2520for%2520creating%2520these%250Aenvironments%2520with%2520surgical%2520scene%2520involve%2520a%2520labor-intensive%2520process%2520where%250Adesigners%2520hand-craft%2520tissues%2520models%2520with%2520textures%2520and%2520geometries%2520for%2520soft%2520body%250Asimulations.%2520This%2520manual%2520approach%2520is%2520not%2520only%2520time-consuming%2520but%2520also%2520limited%250Ain%2520the%2520scalability%2520and%2520realism.%2520In%2520contrast%252C%2520data-driven%2520simulation%2520offers%2520a%250Acompelling%2520alternative.%2520It%2520has%2520the%2520potential%2520to%2520automatically%2520reconstruct%25203D%250Asurgical%2520scenes%2520from%2520real-world%2520surgical%2520video%2520data%252C%2520followed%2520by%2520the%250Aapplication%2520of%2520soft%2520body%2520physics.%2520This%2520area%252C%2520however%252C%2520is%2520relatively%2520uncharted.%250AIn%2520our%2520research%252C%2520we%2520introduce%25203D%2520Gaussian%2520as%2520a%2520learnable%2520representation%2520for%250Asurgical%2520scene%252C%2520which%2520is%2520learned%2520from%2520stereo%2520endoscopic%2520video.%2520To%2520prevent%250Aover-fitting%2520and%2520ensure%2520the%2520geometrical%2520correctness%2520of%2520these%2520scenes%252C%2520we%250Aincorporate%2520depth%2520supervision%2520and%2520anisotropy%2520regularization%2520into%2520the%2520Gaussian%250Alearning%2520process.%2520Furthermore%252C%2520we%2520apply%2520the%2520Material%2520Point%2520Method%252C%2520which%2520is%250Aintegrated%2520with%2520physical%2520properties%252C%2520to%2520the%25203D%2520Gaussians%2520to%2520achieve%2520realistic%250Ascene%2520deformations.%2520Our%2520method%2520was%2520evaluated%2520on%2520our%2520collected%2520in-house%2520and%250Apublic%2520surgical%2520videos%2520datasets.%2520Results%2520show%2520that%2520it%2520can%2520reconstruct%2520and%250Asimulate%2520surgical%2520scenes%2520from%2520endoscopic%2520videos%2520efficiently-taking%2520only%2520a%2520few%250Aminutes%2520to%2520reconstruct%2520the%2520surgical%2520scene-and%2520produce%2520both%2520visually%2520and%250Aphysically%2520plausible%2520deformations%2520at%2520a%2520speed%2520approaching%2520real-time.%2520The%2520results%250Ademonstrate%2520great%2520potential%2520of%2520our%2520proposed%2520method%2520to%2520enhance%2520the%2520efficiency%250Aand%2520variety%2520of%2520simulations%2520available%2520for%2520surgical%2520education%2520and%2520robot%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00956v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimEndoGS%3A%20Efficient%20Data-driven%20Scene%20Simulation%20using%20Robotic%20Surgery%0A%20%20Videos%20via%20Physics-embedded%203D%20Gaussians&entry.906535625=Zhenya%20Yang%20and%20Kai%20Chen%20and%20Yonghao%20Long%20and%20Qi%20Dou&entry.1292438233=%20%20Surgical%20scene%20simulation%20plays%20a%20crucial%20role%20in%20surgical%20education%20and%0Asimulator-based%20robot%20learning.%20Traditional%20approaches%20for%20creating%20these%0Aenvironments%20with%20surgical%20scene%20involve%20a%20labor-intensive%20process%20where%0Adesigners%20hand-craft%20tissues%20models%20with%20textures%20and%20geometries%20for%20soft%20body%0Asimulations.%20This%20manual%20approach%20is%20not%20only%20time-consuming%20but%20also%20limited%0Ain%20the%20scalability%20and%20realism.%20In%20contrast%2C%20data-driven%20simulation%20offers%20a%0Acompelling%20alternative.%20It%20has%20the%20potential%20to%20automatically%20reconstruct%203D%0Asurgical%20scenes%20from%20real-world%20surgical%20video%20data%2C%20followed%20by%20the%0Aapplication%20of%20soft%20body%20physics.%20This%20area%2C%20however%2C%20is%20relatively%20uncharted.%0AIn%20our%20research%2C%20we%20introduce%203D%20Gaussian%20as%20a%20learnable%20representation%20for%0Asurgical%20scene%2C%20which%20is%20learned%20from%20stereo%20endoscopic%20video.%20To%20prevent%0Aover-fitting%20and%20ensure%20the%20geometrical%20correctness%20of%20these%20scenes%2C%20we%0Aincorporate%20depth%20supervision%20and%20anisotropy%20regularization%20into%20the%20Gaussian%0Alearning%20process.%20Furthermore%2C%20we%20apply%20the%20Material%20Point%20Method%2C%20which%20is%0Aintegrated%20with%20physical%20properties%2C%20to%20the%203D%20Gaussians%20to%20achieve%20realistic%0Ascene%20deformations.%20Our%20method%20was%20evaluated%20on%20our%20collected%20in-house%20and%0Apublic%20surgical%20videos%20datasets.%20Results%20show%20that%20it%20can%20reconstruct%20and%0Asimulate%20surgical%20scenes%20from%20endoscopic%20videos%20efficiently-taking%20only%20a%20few%0Aminutes%20to%20reconstruct%20the%20surgical%20scene-and%20produce%20both%20visually%20and%0Aphysically%20plausible%20deformations%20at%20a%20speed%20approaching%20real-time.%20The%20results%0Ademonstrate%20great%20potential%20of%20our%20proposed%20method%20to%20enhance%20the%20efficiency%0Aand%20variety%20of%20simulations%20available%20for%20surgical%20education%20and%20robot%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00956v3&entry.124074799=Read"},
{"title": "Topic Modeling with Fine-tuning LLMs and Bag of Sentences", "author": "Johannes Schneider", "abstract": "  Large language models (LLM)'s are increasingly used for topic modeling\noutperforming classical topic models such as LDA. Commonly, pre-trained LLM\nencoders such as BERT are used out-of-the-box despite the fact that fine-tuning\nis known to improve LLMs considerably. The challenge lies in obtaining a\nsuitable (labeled) dataset for fine-tuning. In this paper, we use the recent\nidea to use bag of sentences as the elementary unit in computing topics. In\nturn, we derive an approach FT-Topic to perform unsupervised fine-tuning\nrelying primarily on two steps for constructing a training dataset in an\nautomatic fashion. First, a heuristic method to identifies pairs of sentence\ngroups that are either assumed to be of the same or different topics. Second,\nwe remove sentence pairs that are likely labeled incorrectly. The dataset is\nthen used to fine-tune an encoder LLM, which can be leveraged by any topic\nmodeling approach using embeddings. However, in this work, we demonstrate its\neffectiveness by deriving a novel state-of-the-art topic modeling method called\nSenClu, which achieves fast inference through an expectation-maximization\nalgorithm and hard assignments of sentence groups to a single topic, while\ngiving users the possibility to encode prior knowledge on the topic-document\ndistribution. Code is at \\url{https://github.com/JohnTailor/FT-Topic}\n", "link": "http://arxiv.org/abs/2408.03099v1", "date": "2024-08-06", "relevancy": 2.4611, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.496}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.494}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topic%20Modeling%20with%20Fine-tuning%20LLMs%20and%20Bag%20of%20Sentences&body=Title%3A%20Topic%20Modeling%20with%20Fine-tuning%20LLMs%20and%20Bag%20of%20Sentences%0AAuthor%3A%20Johannes%20Schneider%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLM%29%27s%20are%20increasingly%20used%20for%20topic%20modeling%0Aoutperforming%20classical%20topic%20models%20such%20as%20LDA.%20Commonly%2C%20pre-trained%20LLM%0Aencoders%20such%20as%20BERT%20are%20used%20out-of-the-box%20despite%20the%20fact%20that%20fine-tuning%0Ais%20known%20to%20improve%20LLMs%20considerably.%20The%20challenge%20lies%20in%20obtaining%20a%0Asuitable%20%28labeled%29%20dataset%20for%20fine-tuning.%20In%20this%20paper%2C%20we%20use%20the%20recent%0Aidea%20to%20use%20bag%20of%20sentences%20as%20the%20elementary%20unit%20in%20computing%20topics.%20In%0Aturn%2C%20we%20derive%20an%20approach%20FT-Topic%20to%20perform%20unsupervised%20fine-tuning%0Arelying%20primarily%20on%20two%20steps%20for%20constructing%20a%20training%20dataset%20in%20an%0Aautomatic%20fashion.%20First%2C%20a%20heuristic%20method%20to%20identifies%20pairs%20of%20sentence%0Agroups%20that%20are%20either%20assumed%20to%20be%20of%20the%20same%20or%20different%20topics.%20Second%2C%0Awe%20remove%20sentence%20pairs%20that%20are%20likely%20labeled%20incorrectly.%20The%20dataset%20is%0Athen%20used%20to%20fine-tune%20an%20encoder%20LLM%2C%20which%20can%20be%20leveraged%20by%20any%20topic%0Amodeling%20approach%20using%20embeddings.%20However%2C%20in%20this%20work%2C%20we%20demonstrate%20its%0Aeffectiveness%20by%20deriving%20a%20novel%20state-of-the-art%20topic%20modeling%20method%20called%0ASenClu%2C%20which%20achieves%20fast%20inference%20through%20an%20expectation-maximization%0Aalgorithm%20and%20hard%20assignments%20of%20sentence%20groups%20to%20a%20single%20topic%2C%20while%0Agiving%20users%20the%20possibility%20to%20encode%20prior%20knowledge%20on%20the%20topic-document%0Adistribution.%20Code%20is%20at%20%5Curl%7Bhttps%3A//github.com/JohnTailor/FT-Topic%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopic%2520Modeling%2520with%2520Fine-tuning%2520LLMs%2520and%2520Bag%2520of%2520Sentences%26entry.906535625%3DJohannes%2520Schneider%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLM%2529%2527s%2520are%2520increasingly%2520used%2520for%2520topic%2520modeling%250Aoutperforming%2520classical%2520topic%2520models%2520such%2520as%2520LDA.%2520Commonly%252C%2520pre-trained%2520LLM%250Aencoders%2520such%2520as%2520BERT%2520are%2520used%2520out-of-the-box%2520despite%2520the%2520fact%2520that%2520fine-tuning%250Ais%2520known%2520to%2520improve%2520LLMs%2520considerably.%2520The%2520challenge%2520lies%2520in%2520obtaining%2520a%250Asuitable%2520%2528labeled%2529%2520dataset%2520for%2520fine-tuning.%2520In%2520this%2520paper%252C%2520we%2520use%2520the%2520recent%250Aidea%2520to%2520use%2520bag%2520of%2520sentences%2520as%2520the%2520elementary%2520unit%2520in%2520computing%2520topics.%2520In%250Aturn%252C%2520we%2520derive%2520an%2520approach%2520FT-Topic%2520to%2520perform%2520unsupervised%2520fine-tuning%250Arelying%2520primarily%2520on%2520two%2520steps%2520for%2520constructing%2520a%2520training%2520dataset%2520in%2520an%250Aautomatic%2520fashion.%2520First%252C%2520a%2520heuristic%2520method%2520to%2520identifies%2520pairs%2520of%2520sentence%250Agroups%2520that%2520are%2520either%2520assumed%2520to%2520be%2520of%2520the%2520same%2520or%2520different%2520topics.%2520Second%252C%250Awe%2520remove%2520sentence%2520pairs%2520that%2520are%2520likely%2520labeled%2520incorrectly.%2520The%2520dataset%2520is%250Athen%2520used%2520to%2520fine-tune%2520an%2520encoder%2520LLM%252C%2520which%2520can%2520be%2520leveraged%2520by%2520any%2520topic%250Amodeling%2520approach%2520using%2520embeddings.%2520However%252C%2520in%2520this%2520work%252C%2520we%2520demonstrate%2520its%250Aeffectiveness%2520by%2520deriving%2520a%2520novel%2520state-of-the-art%2520topic%2520modeling%2520method%2520called%250ASenClu%252C%2520which%2520achieves%2520fast%2520inference%2520through%2520an%2520expectation-maximization%250Aalgorithm%2520and%2520hard%2520assignments%2520of%2520sentence%2520groups%2520to%2520a%2520single%2520topic%252C%2520while%250Agiving%2520users%2520the%2520possibility%2520to%2520encode%2520prior%2520knowledge%2520on%2520the%2520topic-document%250Adistribution.%2520Code%2520is%2520at%2520%255Curl%257Bhttps%253A//github.com/JohnTailor/FT-Topic%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topic%20Modeling%20with%20Fine-tuning%20LLMs%20and%20Bag%20of%20Sentences&entry.906535625=Johannes%20Schneider&entry.1292438233=%20%20Large%20language%20models%20%28LLM%29%27s%20are%20increasingly%20used%20for%20topic%20modeling%0Aoutperforming%20classical%20topic%20models%20such%20as%20LDA.%20Commonly%2C%20pre-trained%20LLM%0Aencoders%20such%20as%20BERT%20are%20used%20out-of-the-box%20despite%20the%20fact%20that%20fine-tuning%0Ais%20known%20to%20improve%20LLMs%20considerably.%20The%20challenge%20lies%20in%20obtaining%20a%0Asuitable%20%28labeled%29%20dataset%20for%20fine-tuning.%20In%20this%20paper%2C%20we%20use%20the%20recent%0Aidea%20to%20use%20bag%20of%20sentences%20as%20the%20elementary%20unit%20in%20computing%20topics.%20In%0Aturn%2C%20we%20derive%20an%20approach%20FT-Topic%20to%20perform%20unsupervised%20fine-tuning%0Arelying%20primarily%20on%20two%20steps%20for%20constructing%20a%20training%20dataset%20in%20an%0Aautomatic%20fashion.%20First%2C%20a%20heuristic%20method%20to%20identifies%20pairs%20of%20sentence%0Agroups%20that%20are%20either%20assumed%20to%20be%20of%20the%20same%20or%20different%20topics.%20Second%2C%0Awe%20remove%20sentence%20pairs%20that%20are%20likely%20labeled%20incorrectly.%20The%20dataset%20is%0Athen%20used%20to%20fine-tune%20an%20encoder%20LLM%2C%20which%20can%20be%20leveraged%20by%20any%20topic%0Amodeling%20approach%20using%20embeddings.%20However%2C%20in%20this%20work%2C%20we%20demonstrate%20its%0Aeffectiveness%20by%20deriving%20a%20novel%20state-of-the-art%20topic%20modeling%20method%20called%0ASenClu%2C%20which%20achieves%20fast%20inference%20through%20an%20expectation-maximization%0Aalgorithm%20and%20hard%20assignments%20of%20sentence%20groups%20to%20a%20single%20topic%2C%20while%0Agiving%20users%20the%20possibility%20to%20encode%20prior%20knowledge%20on%20the%20topic-document%0Adistribution.%20Code%20is%20at%20%5Curl%7Bhttps%3A//github.com/JohnTailor/FT-Topic%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03099v1&entry.124074799=Read"},
{"title": "Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4\n  Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation", "author": "Tung Phung and Victor-Alexandru P\u0103durean and Anjali Singh and Christopher Brooks and Jos\u00e9 Cambronero and Sumit Gulwani and Adish Singla and Gustavo Soares", "abstract": "  Generative AI and large language models hold great promise in enhancing\nprogramming education by automatically generating individualized feedback for\nstudents. We investigate the role of generative AI models in providing human\ntutor-style programming hints to help students resolve errors in their buggy\nprograms. Recent works have benchmarked state-of-the-art models for various\nfeedback generation scenarios; however, their overall quality is still inferior\nto human tutors and not yet ready for real-world deployment. In this paper, we\nseek to push the limits of generative AI models toward providing high-quality\nprogramming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a\nfirst step, our technique leverages GPT-4 as a ``tutor'' model to generate\nhints -- it boosts the generative quality by using symbolic information of\nfailing test cases and fixes in prompts. As a next step, our technique\nleverages GPT-3.5, a weaker model, as a ``student'' model to further validate\nthe hint quality -- it performs an automatic quality validation by simulating\nthe potential utility of providing this feedback. We show the efficacy of our\ntechnique via extensive evaluation using three real-world datasets of Python\nprograms covering a variety of concepts ranging from basic algorithms to\nregular expressions and data analysis using pandas library.\n", "link": "http://arxiv.org/abs/2310.03780v4", "date": "2024-08-06", "relevancy": 2.4572, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5065}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4958}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automating%20Human%20Tutor-Style%20Programming%20Feedback%3A%20Leveraging%20GPT-4%0A%20%20Tutor%20Model%20for%20Hint%20Generation%20and%20GPT-3.5%20Student%20Model%20for%20Hint%20Validation&body=Title%3A%20Automating%20Human%20Tutor-Style%20Programming%20Feedback%3A%20Leveraging%20GPT-4%0A%20%20Tutor%20Model%20for%20Hint%20Generation%20and%20GPT-3.5%20Student%20Model%20for%20Hint%20Validation%0AAuthor%3A%20Tung%20Phung%20and%20Victor-Alexandru%20P%C4%83durean%20and%20Anjali%20Singh%20and%20Christopher%20Brooks%20and%20Jos%C3%A9%20Cambronero%20and%20Sumit%20Gulwani%20and%20Adish%20Singla%20and%20Gustavo%20Soares%0AAbstract%3A%20%20%20Generative%20AI%20and%20large%20language%20models%20hold%20great%20promise%20in%20enhancing%0Aprogramming%20education%20by%20automatically%20generating%20individualized%20feedback%20for%0Astudents.%20We%20investigate%20the%20role%20of%20generative%20AI%20models%20in%20providing%20human%0Atutor-style%20programming%20hints%20to%20help%20students%20resolve%20errors%20in%20their%20buggy%0Aprograms.%20Recent%20works%20have%20benchmarked%20state-of-the-art%20models%20for%20various%0Afeedback%20generation%20scenarios%3B%20however%2C%20their%20overall%20quality%20is%20still%20inferior%0Ato%20human%20tutors%20and%20not%20yet%20ready%20for%20real-world%20deployment.%20In%20this%20paper%2C%20we%0Aseek%20to%20push%20the%20limits%20of%20generative%20AI%20models%20toward%20providing%20high-quality%0Aprogramming%20hints%20and%20develop%20a%20novel%20technique%2C%20GPT4Hints-GPT3.5Val.%20As%20a%0Afirst%20step%2C%20our%20technique%20leverages%20GPT-4%20as%20a%20%60%60tutor%27%27%20model%20to%20generate%0Ahints%20--%20it%20boosts%20the%20generative%20quality%20by%20using%20symbolic%20information%20of%0Afailing%20test%20cases%20and%20fixes%20in%20prompts.%20As%20a%20next%20step%2C%20our%20technique%0Aleverages%20GPT-3.5%2C%20a%20weaker%20model%2C%20as%20a%20%60%60student%27%27%20model%20to%20further%20validate%0Athe%20hint%20quality%20--%20it%20performs%20an%20automatic%20quality%20validation%20by%20simulating%0Athe%20potential%20utility%20of%20providing%20this%20feedback.%20We%20show%20the%20efficacy%20of%20our%0Atechnique%20via%20extensive%20evaluation%20using%20three%20real-world%20datasets%20of%20Python%0Aprograms%20covering%20a%20variety%20of%20concepts%20ranging%20from%20basic%20algorithms%20to%0Aregular%20expressions%20and%20data%20analysis%20using%20pandas%20library.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03780v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomating%2520Human%2520Tutor-Style%2520Programming%2520Feedback%253A%2520Leveraging%2520GPT-4%250A%2520%2520Tutor%2520Model%2520for%2520Hint%2520Generation%2520and%2520GPT-3.5%2520Student%2520Model%2520for%2520Hint%2520Validation%26entry.906535625%3DTung%2520Phung%2520and%2520Victor-Alexandru%2520P%25C4%2583durean%2520and%2520Anjali%2520Singh%2520and%2520Christopher%2520Brooks%2520and%2520Jos%25C3%25A9%2520Cambronero%2520and%2520Sumit%2520Gulwani%2520and%2520Adish%2520Singla%2520and%2520Gustavo%2520Soares%26entry.1292438233%3D%2520%2520Generative%2520AI%2520and%2520large%2520language%2520models%2520hold%2520great%2520promise%2520in%2520enhancing%250Aprogramming%2520education%2520by%2520automatically%2520generating%2520individualized%2520feedback%2520for%250Astudents.%2520We%2520investigate%2520the%2520role%2520of%2520generative%2520AI%2520models%2520in%2520providing%2520human%250Atutor-style%2520programming%2520hints%2520to%2520help%2520students%2520resolve%2520errors%2520in%2520their%2520buggy%250Aprograms.%2520Recent%2520works%2520have%2520benchmarked%2520state-of-the-art%2520models%2520for%2520various%250Afeedback%2520generation%2520scenarios%253B%2520however%252C%2520their%2520overall%2520quality%2520is%2520still%2520inferior%250Ato%2520human%2520tutors%2520and%2520not%2520yet%2520ready%2520for%2520real-world%2520deployment.%2520In%2520this%2520paper%252C%2520we%250Aseek%2520to%2520push%2520the%2520limits%2520of%2520generative%2520AI%2520models%2520toward%2520providing%2520high-quality%250Aprogramming%2520hints%2520and%2520develop%2520a%2520novel%2520technique%252C%2520GPT4Hints-GPT3.5Val.%2520As%2520a%250Afirst%2520step%252C%2520our%2520technique%2520leverages%2520GPT-4%2520as%2520a%2520%2560%2560tutor%2527%2527%2520model%2520to%2520generate%250Ahints%2520--%2520it%2520boosts%2520the%2520generative%2520quality%2520by%2520using%2520symbolic%2520information%2520of%250Afailing%2520test%2520cases%2520and%2520fixes%2520in%2520prompts.%2520As%2520a%2520next%2520step%252C%2520our%2520technique%250Aleverages%2520GPT-3.5%252C%2520a%2520weaker%2520model%252C%2520as%2520a%2520%2560%2560student%2527%2527%2520model%2520to%2520further%2520validate%250Athe%2520hint%2520quality%2520--%2520it%2520performs%2520an%2520automatic%2520quality%2520validation%2520by%2520simulating%250Athe%2520potential%2520utility%2520of%2520providing%2520this%2520feedback.%2520We%2520show%2520the%2520efficacy%2520of%2520our%250Atechnique%2520via%2520extensive%2520evaluation%2520using%2520three%2520real-world%2520datasets%2520of%2520Python%250Aprograms%2520covering%2520a%2520variety%2520of%2520concepts%2520ranging%2520from%2520basic%2520algorithms%2520to%250Aregular%2520expressions%2520and%2520data%2520analysis%2520using%2520pandas%2520library.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03780v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automating%20Human%20Tutor-Style%20Programming%20Feedback%3A%20Leveraging%20GPT-4%0A%20%20Tutor%20Model%20for%20Hint%20Generation%20and%20GPT-3.5%20Student%20Model%20for%20Hint%20Validation&entry.906535625=Tung%20Phung%20and%20Victor-Alexandru%20P%C4%83durean%20and%20Anjali%20Singh%20and%20Christopher%20Brooks%20and%20Jos%C3%A9%20Cambronero%20and%20Sumit%20Gulwani%20and%20Adish%20Singla%20and%20Gustavo%20Soares&entry.1292438233=%20%20Generative%20AI%20and%20large%20language%20models%20hold%20great%20promise%20in%20enhancing%0Aprogramming%20education%20by%20automatically%20generating%20individualized%20feedback%20for%0Astudents.%20We%20investigate%20the%20role%20of%20generative%20AI%20models%20in%20providing%20human%0Atutor-style%20programming%20hints%20to%20help%20students%20resolve%20errors%20in%20their%20buggy%0Aprograms.%20Recent%20works%20have%20benchmarked%20state-of-the-art%20models%20for%20various%0Afeedback%20generation%20scenarios%3B%20however%2C%20their%20overall%20quality%20is%20still%20inferior%0Ato%20human%20tutors%20and%20not%20yet%20ready%20for%20real-world%20deployment.%20In%20this%20paper%2C%20we%0Aseek%20to%20push%20the%20limits%20of%20generative%20AI%20models%20toward%20providing%20high-quality%0Aprogramming%20hints%20and%20develop%20a%20novel%20technique%2C%20GPT4Hints-GPT3.5Val.%20As%20a%0Afirst%20step%2C%20our%20technique%20leverages%20GPT-4%20as%20a%20%60%60tutor%27%27%20model%20to%20generate%0Ahints%20--%20it%20boosts%20the%20generative%20quality%20by%20using%20symbolic%20information%20of%0Afailing%20test%20cases%20and%20fixes%20in%20prompts.%20As%20a%20next%20step%2C%20our%20technique%0Aleverages%20GPT-3.5%2C%20a%20weaker%20model%2C%20as%20a%20%60%60student%27%27%20model%20to%20further%20validate%0Athe%20hint%20quality%20--%20it%20performs%20an%20automatic%20quality%20validation%20by%20simulating%0Athe%20potential%20utility%20of%20providing%20this%20feedback.%20We%20show%20the%20efficacy%20of%20our%0Atechnique%20via%20extensive%20evaluation%20using%20three%20real-world%20datasets%20of%20Python%0Aprograms%20covering%20a%20variety%20of%20concepts%20ranging%20from%20basic%20algorithms%20to%0Aregular%20expressions%20and%20data%20analysis%20using%20pandas%20library.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03780v4&entry.124074799=Read"},
{"title": "Understanding Retrieval Robustness for Retrieval-Augmented Image\n  Captioning", "author": "Wenyan Li and Jiaang Li and Rita Ramos and Raphael Tang and Desmond Elliott", "abstract": "  Recent advances in retrieval-augmented models for image captioning highlight\nthe benefit of retrieving related captions for efficient, lightweight models\nwith strong domain-transfer capabilities. While these models demonstrate the\nsuccess of retrieval augmentation, retrieval models are still far from perfect\nin practice: the retrieved information can sometimes mislead the model,\nresulting in incorrect generation and worse performance. In this paper, we\nanalyze the robustness of a retrieval-augmented captioning model SmallCap. Our\nanalysis shows that the model is sensitive to tokens that appear in the\nmajority of the retrieved captions, and the input attribution shows that those\ntokens are likely copied into the generated output. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This decreases the chance that the model learns to copy majority tokens,\nand improves both in-domain and cross-domain performance.\n", "link": "http://arxiv.org/abs/2406.02265v3", "date": "2024-08-06", "relevancy": 2.4565, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4974}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4886}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Retrieval%20Robustness%20for%20Retrieval-Augmented%20Image%0A%20%20Captioning&body=Title%3A%20Understanding%20Retrieval%20Robustness%20for%20Retrieval-Augmented%20Image%0A%20%20Captioning%0AAuthor%3A%20Wenyan%20Li%20and%20Jiaang%20Li%20and%20Rita%20Ramos%20and%20Raphael%20Tang%20and%20Desmond%20Elliott%0AAbstract%3A%20%20%20Recent%20advances%20in%20retrieval-augmented%20models%20for%20image%20captioning%20highlight%0Athe%20benefit%20of%20retrieving%20related%20captions%20for%20efficient%2C%20lightweight%20models%0Awith%20strong%20domain-transfer%20capabilities.%20While%20these%20models%20demonstrate%20the%0Asuccess%20of%20retrieval%20augmentation%2C%20retrieval%20models%20are%20still%20far%20from%20perfect%0Ain%20practice%3A%20the%20retrieved%20information%20can%20sometimes%20mislead%20the%20model%2C%0Aresulting%20in%20incorrect%20generation%20and%20worse%20performance.%20In%20this%20paper%2C%20we%0Aanalyze%20the%20robustness%20of%20a%20retrieval-augmented%20captioning%20model%20SmallCap.%20Our%0Aanalysis%20shows%20that%20the%20model%20is%20sensitive%20to%20tokens%20that%20appear%20in%20the%0Amajority%20of%20the%20retrieved%20captions%2C%20and%20the%20input%20attribution%20shows%20that%20those%0Atokens%20are%20likely%20copied%20into%20the%20generated%20output.%20Given%20these%20findings%2C%20we%0Apropose%20to%20train%20the%20model%20by%20sampling%20retrieved%20captions%20from%20more%20diverse%0Asets.%20This%20decreases%20the%20chance%20that%20the%20model%20learns%20to%20copy%20majority%20tokens%2C%0Aand%20improves%20both%20in-domain%20and%20cross-domain%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02265v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Retrieval%2520Robustness%2520for%2520Retrieval-Augmented%2520Image%250A%2520%2520Captioning%26entry.906535625%3DWenyan%2520Li%2520and%2520Jiaang%2520Li%2520and%2520Rita%2520Ramos%2520and%2520Raphael%2520Tang%2520and%2520Desmond%2520Elliott%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520retrieval-augmented%2520models%2520for%2520image%2520captioning%2520highlight%250Athe%2520benefit%2520of%2520retrieving%2520related%2520captions%2520for%2520efficient%252C%2520lightweight%2520models%250Awith%2520strong%2520domain-transfer%2520capabilities.%2520While%2520these%2520models%2520demonstrate%2520the%250Asuccess%2520of%2520retrieval%2520augmentation%252C%2520retrieval%2520models%2520are%2520still%2520far%2520from%2520perfect%250Ain%2520practice%253A%2520the%2520retrieved%2520information%2520can%2520sometimes%2520mislead%2520the%2520model%252C%250Aresulting%2520in%2520incorrect%2520generation%2520and%2520worse%2520performance.%2520In%2520this%2520paper%252C%2520we%250Aanalyze%2520the%2520robustness%2520of%2520a%2520retrieval-augmented%2520captioning%2520model%2520SmallCap.%2520Our%250Aanalysis%2520shows%2520that%2520the%2520model%2520is%2520sensitive%2520to%2520tokens%2520that%2520appear%2520in%2520the%250Amajority%2520of%2520the%2520retrieved%2520captions%252C%2520and%2520the%2520input%2520attribution%2520shows%2520that%2520those%250Atokens%2520are%2520likely%2520copied%2520into%2520the%2520generated%2520output.%2520Given%2520these%2520findings%252C%2520we%250Apropose%2520to%2520train%2520the%2520model%2520by%2520sampling%2520retrieved%2520captions%2520from%2520more%2520diverse%250Asets.%2520This%2520decreases%2520the%2520chance%2520that%2520the%2520model%2520learns%2520to%2520copy%2520majority%2520tokens%252C%250Aand%2520improves%2520both%2520in-domain%2520and%2520cross-domain%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02265v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Retrieval%20Robustness%20for%20Retrieval-Augmented%20Image%0A%20%20Captioning&entry.906535625=Wenyan%20Li%20and%20Jiaang%20Li%20and%20Rita%20Ramos%20and%20Raphael%20Tang%20and%20Desmond%20Elliott&entry.1292438233=%20%20Recent%20advances%20in%20retrieval-augmented%20models%20for%20image%20captioning%20highlight%0Athe%20benefit%20of%20retrieving%20related%20captions%20for%20efficient%2C%20lightweight%20models%0Awith%20strong%20domain-transfer%20capabilities.%20While%20these%20models%20demonstrate%20the%0Asuccess%20of%20retrieval%20augmentation%2C%20retrieval%20models%20are%20still%20far%20from%20perfect%0Ain%20practice%3A%20the%20retrieved%20information%20can%20sometimes%20mislead%20the%20model%2C%0Aresulting%20in%20incorrect%20generation%20and%20worse%20performance.%20In%20this%20paper%2C%20we%0Aanalyze%20the%20robustness%20of%20a%20retrieval-augmented%20captioning%20model%20SmallCap.%20Our%0Aanalysis%20shows%20that%20the%20model%20is%20sensitive%20to%20tokens%20that%20appear%20in%20the%0Amajority%20of%20the%20retrieved%20captions%2C%20and%20the%20input%20attribution%20shows%20that%20those%0Atokens%20are%20likely%20copied%20into%20the%20generated%20output.%20Given%20these%20findings%2C%20we%0Apropose%20to%20train%20the%20model%20by%20sampling%20retrieved%20captions%20from%20more%20diverse%0Asets.%20This%20decreases%20the%20chance%20that%20the%20model%20learns%20to%20copy%20majority%20tokens%2C%0Aand%20improves%20both%20in-domain%20and%20cross-domain%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02265v3&entry.124074799=Read"},
{"title": "LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for\n  Accurate Robotic Grasping Under the Occlusion", "author": "Jinyu Zhang and Yongchong Gu and Jianxiong Gao and Haitao Lin and Qiang Sun and Xinwei Sun and Xiangyang Xue and Yanwei Fu", "abstract": "  This paper addresses the challenge of perceiving complete object shapes\nthrough visual perception. While prior studies have demonstrated encouraging\noutcomes in segmenting the visible parts of objects within a scene, amodal\nsegmentation, in particular, has the potential to allow robots to infer the\noccluded parts of objects. To this end, this paper introduces a new framework\nthat explores amodal segmentation for robotic grasping in cluttered scenes,\nthus greatly enhancing robotic grasping abilities. Initially, we use a\nconventional segmentation algorithm to detect the visible segments of the\ntarget object, which provides shape priors for completing the full object mask.\nParticularly, to explore how to utilize semantic features from RGB images and\ngeometric information from depth images, we propose a Linear-fusion\nAttention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the\nlinear-fusion strategy to effectively fuse this cross-modal data, and then uses\nthe prior visible mask as attention map to guide the network to focus on target\nfeature locations for further complete mask recovery. Using the amodal mask of\nthe target object provides advantages in selecting more accurate and robust\ngrasp points compared to relying solely on the visible segments. The results on\ndifferent datasets show that our method achieves state-of-the-art performance.\nFurthermore, the robot experiments validate the feasibility and robustness of\nthis method in the real world. Our code and demonstrations are available on the\nproject page: https://jrryzh.github.io/LAC-Net.\n", "link": "http://arxiv.org/abs/2408.03238v1", "date": "2024-08-06", "relevancy": 2.4347, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6146}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6051}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAC-Net%3A%20Linear-Fusion%20Attention-Guided%20Convolutional%20Network%20for%0A%20%20Accurate%20Robotic%20Grasping%20Under%20the%20Occlusion&body=Title%3A%20LAC-Net%3A%20Linear-Fusion%20Attention-Guided%20Convolutional%20Network%20for%0A%20%20Accurate%20Robotic%20Grasping%20Under%20the%20Occlusion%0AAuthor%3A%20Jinyu%20Zhang%20and%20Yongchong%20Gu%20and%20Jianxiong%20Gao%20and%20Haitao%20Lin%20and%20Qiang%20Sun%20and%20Xinwei%20Sun%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20perceiving%20complete%20object%20shapes%0Athrough%20visual%20perception.%20While%20prior%20studies%20have%20demonstrated%20encouraging%0Aoutcomes%20in%20segmenting%20the%20visible%20parts%20of%20objects%20within%20a%20scene%2C%20amodal%0Asegmentation%2C%20in%20particular%2C%20has%20the%20potential%20to%20allow%20robots%20to%20infer%20the%0Aoccluded%20parts%20of%20objects.%20To%20this%20end%2C%20this%20paper%20introduces%20a%20new%20framework%0Athat%20explores%20amodal%20segmentation%20for%20robotic%20grasping%20in%20cluttered%20scenes%2C%0Athus%20greatly%20enhancing%20robotic%20grasping%20abilities.%20Initially%2C%20we%20use%20a%0Aconventional%20segmentation%20algorithm%20to%20detect%20the%20visible%20segments%20of%20the%0Atarget%20object%2C%20which%20provides%20shape%20priors%20for%20completing%20the%20full%20object%20mask.%0AParticularly%2C%20to%20explore%20how%20to%20utilize%20semantic%20features%20from%20RGB%20images%20and%0Ageometric%20information%20from%20depth%20images%2C%20we%20propose%20a%20Linear-fusion%0AAttention-guided%20Convolutional%20Network%20%28LAC-Net%29.%20LAC-Net%20utilizes%20the%0Alinear-fusion%20strategy%20to%20effectively%20fuse%20this%20cross-modal%20data%2C%20and%20then%20uses%0Athe%20prior%20visible%20mask%20as%20attention%20map%20to%20guide%20the%20network%20to%20focus%20on%20target%0Afeature%20locations%20for%20further%20complete%20mask%20recovery.%20Using%20the%20amodal%20mask%20of%0Athe%20target%20object%20provides%20advantages%20in%20selecting%20more%20accurate%20and%20robust%0Agrasp%20points%20compared%20to%20relying%20solely%20on%20the%20visible%20segments.%20The%20results%20on%0Adifferent%20datasets%20show%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0AFurthermore%2C%20the%20robot%20experiments%20validate%20the%20feasibility%20and%20robustness%20of%0Athis%20method%20in%20the%20real%20world.%20Our%20code%20and%20demonstrations%20are%20available%20on%20the%0Aproject%20page%3A%20https%3A//jrryzh.github.io/LAC-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAC-Net%253A%2520Linear-Fusion%2520Attention-Guided%2520Convolutional%2520Network%2520for%250A%2520%2520Accurate%2520Robotic%2520Grasping%2520Under%2520the%2520Occlusion%26entry.906535625%3DJinyu%2520Zhang%2520and%2520Yongchong%2520Gu%2520and%2520Jianxiong%2520Gao%2520and%2520Haitao%2520Lin%2520and%2520Qiang%2520Sun%2520and%2520Xinwei%2520Sun%2520and%2520Xiangyang%2520Xue%2520and%2520Yanwei%2520Fu%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520perceiving%2520complete%2520object%2520shapes%250Athrough%2520visual%2520perception.%2520While%2520prior%2520studies%2520have%2520demonstrated%2520encouraging%250Aoutcomes%2520in%2520segmenting%2520the%2520visible%2520parts%2520of%2520objects%2520within%2520a%2520scene%252C%2520amodal%250Asegmentation%252C%2520in%2520particular%252C%2520has%2520the%2520potential%2520to%2520allow%2520robots%2520to%2520infer%2520the%250Aoccluded%2520parts%2520of%2520objects.%2520To%2520this%2520end%252C%2520this%2520paper%2520introduces%2520a%2520new%2520framework%250Athat%2520explores%2520amodal%2520segmentation%2520for%2520robotic%2520grasping%2520in%2520cluttered%2520scenes%252C%250Athus%2520greatly%2520enhancing%2520robotic%2520grasping%2520abilities.%2520Initially%252C%2520we%2520use%2520a%250Aconventional%2520segmentation%2520algorithm%2520to%2520detect%2520the%2520visible%2520segments%2520of%2520the%250Atarget%2520object%252C%2520which%2520provides%2520shape%2520priors%2520for%2520completing%2520the%2520full%2520object%2520mask.%250AParticularly%252C%2520to%2520explore%2520how%2520to%2520utilize%2520semantic%2520features%2520from%2520RGB%2520images%2520and%250Ageometric%2520information%2520from%2520depth%2520images%252C%2520we%2520propose%2520a%2520Linear-fusion%250AAttention-guided%2520Convolutional%2520Network%2520%2528LAC-Net%2529.%2520LAC-Net%2520utilizes%2520the%250Alinear-fusion%2520strategy%2520to%2520effectively%2520fuse%2520this%2520cross-modal%2520data%252C%2520and%2520then%2520uses%250Athe%2520prior%2520visible%2520mask%2520as%2520attention%2520map%2520to%2520guide%2520the%2520network%2520to%2520focus%2520on%2520target%250Afeature%2520locations%2520for%2520further%2520complete%2520mask%2520recovery.%2520Using%2520the%2520amodal%2520mask%2520of%250Athe%2520target%2520object%2520provides%2520advantages%2520in%2520selecting%2520more%2520accurate%2520and%2520robust%250Agrasp%2520points%2520compared%2520to%2520relying%2520solely%2520on%2520the%2520visible%2520segments.%2520The%2520results%2520on%250Adifferent%2520datasets%2520show%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance.%250AFurthermore%252C%2520the%2520robot%2520experiments%2520validate%2520the%2520feasibility%2520and%2520robustness%2520of%250Athis%2520method%2520in%2520the%2520real%2520world.%2520Our%2520code%2520and%2520demonstrations%2520are%2520available%2520on%2520the%250Aproject%2520page%253A%2520https%253A//jrryzh.github.io/LAC-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAC-Net%3A%20Linear-Fusion%20Attention-Guided%20Convolutional%20Network%20for%0A%20%20Accurate%20Robotic%20Grasping%20Under%20the%20Occlusion&entry.906535625=Jinyu%20Zhang%20and%20Yongchong%20Gu%20and%20Jianxiong%20Gao%20and%20Haitao%20Lin%20and%20Qiang%20Sun%20and%20Xinwei%20Sun%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20perceiving%20complete%20object%20shapes%0Athrough%20visual%20perception.%20While%20prior%20studies%20have%20demonstrated%20encouraging%0Aoutcomes%20in%20segmenting%20the%20visible%20parts%20of%20objects%20within%20a%20scene%2C%20amodal%0Asegmentation%2C%20in%20particular%2C%20has%20the%20potential%20to%20allow%20robots%20to%20infer%20the%0Aoccluded%20parts%20of%20objects.%20To%20this%20end%2C%20this%20paper%20introduces%20a%20new%20framework%0Athat%20explores%20amodal%20segmentation%20for%20robotic%20grasping%20in%20cluttered%20scenes%2C%0Athus%20greatly%20enhancing%20robotic%20grasping%20abilities.%20Initially%2C%20we%20use%20a%0Aconventional%20segmentation%20algorithm%20to%20detect%20the%20visible%20segments%20of%20the%0Atarget%20object%2C%20which%20provides%20shape%20priors%20for%20completing%20the%20full%20object%20mask.%0AParticularly%2C%20to%20explore%20how%20to%20utilize%20semantic%20features%20from%20RGB%20images%20and%0Ageometric%20information%20from%20depth%20images%2C%20we%20propose%20a%20Linear-fusion%0AAttention-guided%20Convolutional%20Network%20%28LAC-Net%29.%20LAC-Net%20utilizes%20the%0Alinear-fusion%20strategy%20to%20effectively%20fuse%20this%20cross-modal%20data%2C%20and%20then%20uses%0Athe%20prior%20visible%20mask%20as%20attention%20map%20to%20guide%20the%20network%20to%20focus%20on%20target%0Afeature%20locations%20for%20further%20complete%20mask%20recovery.%20Using%20the%20amodal%20mask%20of%0Athe%20target%20object%20provides%20advantages%20in%20selecting%20more%20accurate%20and%20robust%0Agrasp%20points%20compared%20to%20relying%20solely%20on%20the%20visible%20segments.%20The%20results%20on%0Adifferent%20datasets%20show%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0AFurthermore%2C%20the%20robot%20experiments%20validate%20the%20feasibility%20and%20robustness%20of%0Athis%20method%20in%20the%20real%20world.%20Our%20code%20and%20demonstrations%20are%20available%20on%20the%0Aproject%20page%3A%20https%3A//jrryzh.github.io/LAC-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03238v1&entry.124074799=Read"},
{"title": "LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action\n  Localization", "author": "Akshita Gupta and Gaurav Mittal and Ahmed Magooda and Ye Yu and Graham W. Taylor and Mei Chen", "abstract": "  Temporal Action Localization (TAL) involves localizing and classifying action\nsnippets in an untrimmed video. The emergence of large video foundation models\nhas led RGB-only video backbones to outperform previous methods needing both\nRGB and optical flow modalities. Leveraging these large models is often limited\nto training only the TAL head due to the prohibitively large GPU memory\nrequired to adapt the video backbone for TAL. To overcome this limitation, we\nintroduce LoSA, the first memory-and-parameter-efficient backbone adapter\ndesigned specifically for TAL to handle untrimmed videos. LoSA specializes for\nTAL by introducing Long-Short-range Adapters that adapt the intermediate layers\nof the video backbone over different temporal ranges. These adapters run\nparallel to the video backbone to significantly reduce memory footprint. LoSA\nalso includes Long-Short-range Gated Fusion that strategically combines the\noutput of these adapters from the video backbone layers to enhance the video\nfeatures provided to the TAL head. Experiments show that LoSA significantly\noutperforms all existing methods on standard TAL benchmarks, THUMOS-14 and\nActivityNet-v1.3, by scaling end-to-end backbone adaptation to\nbillion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging them\nbeyond head-only transfer learning.\n", "link": "http://arxiv.org/abs/2404.01282v2", "date": "2024-08-06", "relevancy": 2.4126, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6298}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6104}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoSA%3A%20Long-Short-range%20Adapter%20for%20Scaling%20End-to-End%20Temporal%20Action%0A%20%20Localization&body=Title%3A%20LoSA%3A%20Long-Short-range%20Adapter%20for%20Scaling%20End-to-End%20Temporal%20Action%0A%20%20Localization%0AAuthor%3A%20Akshita%20Gupta%20and%20Gaurav%20Mittal%20and%20Ahmed%20Magooda%20and%20Ye%20Yu%20and%20Graham%20W.%20Taylor%20and%20Mei%20Chen%0AAbstract%3A%20%20%20Temporal%20Action%20Localization%20%28TAL%29%20involves%20localizing%20and%20classifying%20action%0Asnippets%20in%20an%20untrimmed%20video.%20The%20emergence%20of%20large%20video%20foundation%20models%0Ahas%20led%20RGB-only%20video%20backbones%20to%20outperform%20previous%20methods%20needing%20both%0ARGB%20and%20optical%20flow%20modalities.%20Leveraging%20these%20large%20models%20is%20often%20limited%0Ato%20training%20only%20the%20TAL%20head%20due%20to%20the%20prohibitively%20large%20GPU%20memory%0Arequired%20to%20adapt%20the%20video%20backbone%20for%20TAL.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20LoSA%2C%20the%20first%20memory-and-parameter-efficient%20backbone%20adapter%0Adesigned%20specifically%20for%20TAL%20to%20handle%20untrimmed%20videos.%20LoSA%20specializes%20for%0ATAL%20by%20introducing%20Long-Short-range%20Adapters%20that%20adapt%20the%20intermediate%20layers%0Aof%20the%20video%20backbone%20over%20different%20temporal%20ranges.%20These%20adapters%20run%0Aparallel%20to%20the%20video%20backbone%20to%20significantly%20reduce%20memory%20footprint.%20LoSA%0Aalso%20includes%20Long-Short-range%20Gated%20Fusion%20that%20strategically%20combines%20the%0Aoutput%20of%20these%20adapters%20from%20the%20video%20backbone%20layers%20to%20enhance%20the%20video%0Afeatures%20provided%20to%20the%20TAL%20head.%20Experiments%20show%20that%20LoSA%20significantly%0Aoutperforms%20all%20existing%20methods%20on%20standard%20TAL%20benchmarks%2C%20THUMOS-14%20and%0AActivityNet-v1.3%2C%20by%20scaling%20end-to-end%20backbone%20adaptation%20to%0Abillion-parameter-plus%20models%20like%20VideoMAEv2~%28ViT-g%29%20and%20leveraging%20them%0Abeyond%20head-only%20transfer%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01282v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoSA%253A%2520Long-Short-range%2520Adapter%2520for%2520Scaling%2520End-to-End%2520Temporal%2520Action%250A%2520%2520Localization%26entry.906535625%3DAkshita%2520Gupta%2520and%2520Gaurav%2520Mittal%2520and%2520Ahmed%2520Magooda%2520and%2520Ye%2520Yu%2520and%2520Graham%2520W.%2520Taylor%2520and%2520Mei%2520Chen%26entry.1292438233%3D%2520%2520Temporal%2520Action%2520Localization%2520%2528TAL%2529%2520involves%2520localizing%2520and%2520classifying%2520action%250Asnippets%2520in%2520an%2520untrimmed%2520video.%2520The%2520emergence%2520of%2520large%2520video%2520foundation%2520models%250Ahas%2520led%2520RGB-only%2520video%2520backbones%2520to%2520outperform%2520previous%2520methods%2520needing%2520both%250ARGB%2520and%2520optical%2520flow%2520modalities.%2520Leveraging%2520these%2520large%2520models%2520is%2520often%2520limited%250Ato%2520training%2520only%2520the%2520TAL%2520head%2520due%2520to%2520the%2520prohibitively%2520large%2520GPU%2520memory%250Arequired%2520to%2520adapt%2520the%2520video%2520backbone%2520for%2520TAL.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Aintroduce%2520LoSA%252C%2520the%2520first%2520memory-and-parameter-efficient%2520backbone%2520adapter%250Adesigned%2520specifically%2520for%2520TAL%2520to%2520handle%2520untrimmed%2520videos.%2520LoSA%2520specializes%2520for%250ATAL%2520by%2520introducing%2520Long-Short-range%2520Adapters%2520that%2520adapt%2520the%2520intermediate%2520layers%250Aof%2520the%2520video%2520backbone%2520over%2520different%2520temporal%2520ranges.%2520These%2520adapters%2520run%250Aparallel%2520to%2520the%2520video%2520backbone%2520to%2520significantly%2520reduce%2520memory%2520footprint.%2520LoSA%250Aalso%2520includes%2520Long-Short-range%2520Gated%2520Fusion%2520that%2520strategically%2520combines%2520the%250Aoutput%2520of%2520these%2520adapters%2520from%2520the%2520video%2520backbone%2520layers%2520to%2520enhance%2520the%2520video%250Afeatures%2520provided%2520to%2520the%2520TAL%2520head.%2520Experiments%2520show%2520that%2520LoSA%2520significantly%250Aoutperforms%2520all%2520existing%2520methods%2520on%2520standard%2520TAL%2520benchmarks%252C%2520THUMOS-14%2520and%250AActivityNet-v1.3%252C%2520by%2520scaling%2520end-to-end%2520backbone%2520adaptation%2520to%250Abillion-parameter-plus%2520models%2520like%2520VideoMAEv2~%2528ViT-g%2529%2520and%2520leveraging%2520them%250Abeyond%2520head-only%2520transfer%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01282v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoSA%3A%20Long-Short-range%20Adapter%20for%20Scaling%20End-to-End%20Temporal%20Action%0A%20%20Localization&entry.906535625=Akshita%20Gupta%20and%20Gaurav%20Mittal%20and%20Ahmed%20Magooda%20and%20Ye%20Yu%20and%20Graham%20W.%20Taylor%20and%20Mei%20Chen&entry.1292438233=%20%20Temporal%20Action%20Localization%20%28TAL%29%20involves%20localizing%20and%20classifying%20action%0Asnippets%20in%20an%20untrimmed%20video.%20The%20emergence%20of%20large%20video%20foundation%20models%0Ahas%20led%20RGB-only%20video%20backbones%20to%20outperform%20previous%20methods%20needing%20both%0ARGB%20and%20optical%20flow%20modalities.%20Leveraging%20these%20large%20models%20is%20often%20limited%0Ato%20training%20only%20the%20TAL%20head%20due%20to%20the%20prohibitively%20large%20GPU%20memory%0Arequired%20to%20adapt%20the%20video%20backbone%20for%20TAL.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20LoSA%2C%20the%20first%20memory-and-parameter-efficient%20backbone%20adapter%0Adesigned%20specifically%20for%20TAL%20to%20handle%20untrimmed%20videos.%20LoSA%20specializes%20for%0ATAL%20by%20introducing%20Long-Short-range%20Adapters%20that%20adapt%20the%20intermediate%20layers%0Aof%20the%20video%20backbone%20over%20different%20temporal%20ranges.%20These%20adapters%20run%0Aparallel%20to%20the%20video%20backbone%20to%20significantly%20reduce%20memory%20footprint.%20LoSA%0Aalso%20includes%20Long-Short-range%20Gated%20Fusion%20that%20strategically%20combines%20the%0Aoutput%20of%20these%20adapters%20from%20the%20video%20backbone%20layers%20to%20enhance%20the%20video%0Afeatures%20provided%20to%20the%20TAL%20head.%20Experiments%20show%20that%20LoSA%20significantly%0Aoutperforms%20all%20existing%20methods%20on%20standard%20TAL%20benchmarks%2C%20THUMOS-14%20and%0AActivityNet-v1.3%2C%20by%20scaling%20end-to-end%20backbone%20adaptation%20to%0Abillion-parameter-plus%20models%20like%20VideoMAEv2~%28ViT-g%29%20and%20leveraging%20them%0Abeyond%20head-only%20transfer%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01282v2&entry.124074799=Read"},
{"title": "Position: Topological Deep Learning is the New Frontier for Relational\n  Learning", "author": "Theodore Papamarkou and Tolga Birdal and Michael Bronstein and Gunnar Carlsson and Justin Curry and Yue Gao and Mustafa Hajij and Roland Kwitt and Pietro Li\u00f2 and Paolo Di Lorenzo and Vasileios Maroulas and Nina Miolane and Farzana Nasrin and Karthikeyan Natesan Ramamurthy and Bastian Rieck and Simone Scardapane and Michael T. Schaub and Petar Veli\u010dkovi\u0107 and Bei Wang and Yusu Wang and Guo-Wei Wei and Ghada Zamzmi", "abstract": "  Topological deep learning (TDL) is a rapidly evolving field that uses\ntopological features to understand and design deep learning models. This paper\nposits that TDL is the new frontier for relational learning. TDL may complement\ngraph representation learning and geometric deep learning by incorporating\ntopological concepts, and can thus provide a natural choice for various machine\nlearning settings. To this end, this paper discusses open problems in TDL,\nranging from practical benefits to theoretical foundations. For each problem,\nit outlines potential solutions and future research opportunities. At the same\ntime, this paper serves as an invitation to the scientific community to\nactively participate in TDL research to unlock the potential of this emerging\nfield.\n", "link": "http://arxiv.org/abs/2402.08871v3", "date": "2024-08-06", "relevancy": 2.4075, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4974}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.483}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Topological%20Deep%20Learning%20is%20the%20New%20Frontier%20for%20Relational%0A%20%20Learning&body=Title%3A%20Position%3A%20Topological%20Deep%20Learning%20is%20the%20New%20Frontier%20for%20Relational%0A%20%20Learning%0AAuthor%3A%20Theodore%20Papamarkou%20and%20Tolga%20Birdal%20and%20Michael%20Bronstein%20and%20Gunnar%20Carlsson%20and%20Justin%20Curry%20and%20Yue%20Gao%20and%20Mustafa%20Hajij%20and%20Roland%20Kwitt%20and%20Pietro%20Li%C3%B2%20and%20Paolo%20Di%20Lorenzo%20and%20Vasileios%20Maroulas%20and%20Nina%20Miolane%20and%20Farzana%20Nasrin%20and%20Karthikeyan%20Natesan%20Ramamurthy%20and%20Bastian%20Rieck%20and%20Simone%20Scardapane%20and%20Michael%20T.%20Schaub%20and%20Petar%20Veli%C4%8Dkovi%C4%87%20and%20Bei%20Wang%20and%20Yusu%20Wang%20and%20Guo-Wei%20Wei%20and%20Ghada%20Zamzmi%0AAbstract%3A%20%20%20Topological%20deep%20learning%20%28TDL%29%20is%20a%20rapidly%20evolving%20field%20that%20uses%0Atopological%20features%20to%20understand%20and%20design%20deep%20learning%20models.%20This%20paper%0Aposits%20that%20TDL%20is%20the%20new%20frontier%20for%20relational%20learning.%20TDL%20may%20complement%0Agraph%20representation%20learning%20and%20geometric%20deep%20learning%20by%20incorporating%0Atopological%20concepts%2C%20and%20can%20thus%20provide%20a%20natural%20choice%20for%20various%20machine%0Alearning%20settings.%20To%20this%20end%2C%20this%20paper%20discusses%20open%20problems%20in%20TDL%2C%0Aranging%20from%20practical%20benefits%20to%20theoretical%20foundations.%20For%20each%20problem%2C%0Ait%20outlines%20potential%20solutions%20and%20future%20research%20opportunities.%20At%20the%20same%0Atime%2C%20this%20paper%20serves%20as%20an%20invitation%20to%20the%20scientific%20community%20to%0Aactively%20participate%20in%20TDL%20research%20to%20unlock%20the%20potential%20of%20this%20emerging%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08871v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Topological%2520Deep%2520Learning%2520is%2520the%2520New%2520Frontier%2520for%2520Relational%250A%2520%2520Learning%26entry.906535625%3DTheodore%2520Papamarkou%2520and%2520Tolga%2520Birdal%2520and%2520Michael%2520Bronstein%2520and%2520Gunnar%2520Carlsson%2520and%2520Justin%2520Curry%2520and%2520Yue%2520Gao%2520and%2520Mustafa%2520Hajij%2520and%2520Roland%2520Kwitt%2520and%2520Pietro%2520Li%25C3%25B2%2520and%2520Paolo%2520Di%2520Lorenzo%2520and%2520Vasileios%2520Maroulas%2520and%2520Nina%2520Miolane%2520and%2520Farzana%2520Nasrin%2520and%2520Karthikeyan%2520Natesan%2520Ramamurthy%2520and%2520Bastian%2520Rieck%2520and%2520Simone%2520Scardapane%2520and%2520Michael%2520T.%2520Schaub%2520and%2520Petar%2520Veli%25C4%258Dkovi%25C4%2587%2520and%2520Bei%2520Wang%2520and%2520Yusu%2520Wang%2520and%2520Guo-Wei%2520Wei%2520and%2520Ghada%2520Zamzmi%26entry.1292438233%3D%2520%2520Topological%2520deep%2520learning%2520%2528TDL%2529%2520is%2520a%2520rapidly%2520evolving%2520field%2520that%2520uses%250Atopological%2520features%2520to%2520understand%2520and%2520design%2520deep%2520learning%2520models.%2520This%2520paper%250Aposits%2520that%2520TDL%2520is%2520the%2520new%2520frontier%2520for%2520relational%2520learning.%2520TDL%2520may%2520complement%250Agraph%2520representation%2520learning%2520and%2520geometric%2520deep%2520learning%2520by%2520incorporating%250Atopological%2520concepts%252C%2520and%2520can%2520thus%2520provide%2520a%2520natural%2520choice%2520for%2520various%2520machine%250Alearning%2520settings.%2520To%2520this%2520end%252C%2520this%2520paper%2520discusses%2520open%2520problems%2520in%2520TDL%252C%250Aranging%2520from%2520practical%2520benefits%2520to%2520theoretical%2520foundations.%2520For%2520each%2520problem%252C%250Ait%2520outlines%2520potential%2520solutions%2520and%2520future%2520research%2520opportunities.%2520At%2520the%2520same%250Atime%252C%2520this%2520paper%2520serves%2520as%2520an%2520invitation%2520to%2520the%2520scientific%2520community%2520to%250Aactively%2520participate%2520in%2520TDL%2520research%2520to%2520unlock%2520the%2520potential%2520of%2520this%2520emerging%250Afield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08871v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Topological%20Deep%20Learning%20is%20the%20New%20Frontier%20for%20Relational%0A%20%20Learning&entry.906535625=Theodore%20Papamarkou%20and%20Tolga%20Birdal%20and%20Michael%20Bronstein%20and%20Gunnar%20Carlsson%20and%20Justin%20Curry%20and%20Yue%20Gao%20and%20Mustafa%20Hajij%20and%20Roland%20Kwitt%20and%20Pietro%20Li%C3%B2%20and%20Paolo%20Di%20Lorenzo%20and%20Vasileios%20Maroulas%20and%20Nina%20Miolane%20and%20Farzana%20Nasrin%20and%20Karthikeyan%20Natesan%20Ramamurthy%20and%20Bastian%20Rieck%20and%20Simone%20Scardapane%20and%20Michael%20T.%20Schaub%20and%20Petar%20Veli%C4%8Dkovi%C4%87%20and%20Bei%20Wang%20and%20Yusu%20Wang%20and%20Guo-Wei%20Wei%20and%20Ghada%20Zamzmi&entry.1292438233=%20%20Topological%20deep%20learning%20%28TDL%29%20is%20a%20rapidly%20evolving%20field%20that%20uses%0Atopological%20features%20to%20understand%20and%20design%20deep%20learning%20models.%20This%20paper%0Aposits%20that%20TDL%20is%20the%20new%20frontier%20for%20relational%20learning.%20TDL%20may%20complement%0Agraph%20representation%20learning%20and%20geometric%20deep%20learning%20by%20incorporating%0Atopological%20concepts%2C%20and%20can%20thus%20provide%20a%20natural%20choice%20for%20various%20machine%0Alearning%20settings.%20To%20this%20end%2C%20this%20paper%20discusses%20open%20problems%20in%20TDL%2C%0Aranging%20from%20practical%20benefits%20to%20theoretical%20foundations.%20For%20each%20problem%2C%0Ait%20outlines%20potential%20solutions%20and%20future%20research%20opportunities.%20At%20the%20same%0Atime%2C%20this%20paper%20serves%20as%20an%20invitation%20to%20the%20scientific%20community%20to%0Aactively%20participate%20in%20TDL%20research%20to%20unlock%20the%20potential%20of%20this%20emerging%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08871v3&entry.124074799=Read"},
{"title": "EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with\n  Audio2Video Diffusion Model under Weak Conditions", "author": "Linrui Tian and Qi Wang and Bang Zhang and Liefeng Bo", "abstract": "  In this work, we tackle the challenge of enhancing the realism and\nexpressiveness in talking head video generation by focusing on the dynamic and\nnuanced relationship between audio cues and facial movements. We identify the\nlimitations of traditional techniques that often fail to capture the full\nspectrum of human expressions and the uniqueness of individual facial styles.\nTo address these issues, we propose EMO, a novel framework that utilizes a\ndirect audio-to-video synthesis approach, bypassing the need for intermediate\n3D models or facial landmarks. Our method ensures seamless frame transitions\nand consistent identity preservation throughout the video, resulting in highly\nexpressive and lifelike animations. Experimental results demonsrate that EMO is\nable to produce not only convincing speaking videos but also singing videos in\nvarious styles, significantly outperforming existing state-of-the-art\nmethodologies in terms of expressiveness and realism.\n", "link": "http://arxiv.org/abs/2402.17485v2", "date": "2024-08-06", "relevancy": 2.3698, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5997}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5884}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMO%3A%20Emote%20Portrait%20Alive%20--%20Generating%20Expressive%20Portrait%20Videos%20with%0A%20%20Audio2Video%20Diffusion%20Model%20under%20Weak%20Conditions&body=Title%3A%20EMO%3A%20Emote%20Portrait%20Alive%20--%20Generating%20Expressive%20Portrait%20Videos%20with%0A%20%20Audio2Video%20Diffusion%20Model%20under%20Weak%20Conditions%0AAuthor%3A%20Linrui%20Tian%20and%20Qi%20Wang%20and%20Bang%20Zhang%20and%20Liefeng%20Bo%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20tackle%20the%20challenge%20of%20enhancing%20the%20realism%20and%0Aexpressiveness%20in%20talking%20head%20video%20generation%20by%20focusing%20on%20the%20dynamic%20and%0Anuanced%20relationship%20between%20audio%20cues%20and%20facial%20movements.%20We%20identify%20the%0Alimitations%20of%20traditional%20techniques%20that%20often%20fail%20to%20capture%20the%20full%0Aspectrum%20of%20human%20expressions%20and%20the%20uniqueness%20of%20individual%20facial%20styles.%0ATo%20address%20these%20issues%2C%20we%20propose%20EMO%2C%20a%20novel%20framework%20that%20utilizes%20a%0Adirect%20audio-to-video%20synthesis%20approach%2C%20bypassing%20the%20need%20for%20intermediate%0A3D%20models%20or%20facial%20landmarks.%20Our%20method%20ensures%20seamless%20frame%20transitions%0Aand%20consistent%20identity%20preservation%20throughout%20the%20video%2C%20resulting%20in%20highly%0Aexpressive%20and%20lifelike%20animations.%20Experimental%20results%20demonsrate%20that%20EMO%20is%0Aable%20to%20produce%20not%20only%20convincing%20speaking%20videos%20but%20also%20singing%20videos%20in%0Avarious%20styles%2C%20significantly%20outperforming%20existing%20state-of-the-art%0Amethodologies%20in%20terms%20of%20expressiveness%20and%20realism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMO%253A%2520Emote%2520Portrait%2520Alive%2520--%2520Generating%2520Expressive%2520Portrait%2520Videos%2520with%250A%2520%2520Audio2Video%2520Diffusion%2520Model%2520under%2520Weak%2520Conditions%26entry.906535625%3DLinrui%2520Tian%2520and%2520Qi%2520Wang%2520and%2520Bang%2520Zhang%2520and%2520Liefeng%2520Bo%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520tackle%2520the%2520challenge%2520of%2520enhancing%2520the%2520realism%2520and%250Aexpressiveness%2520in%2520talking%2520head%2520video%2520generation%2520by%2520focusing%2520on%2520the%2520dynamic%2520and%250Anuanced%2520relationship%2520between%2520audio%2520cues%2520and%2520facial%2520movements.%2520We%2520identify%2520the%250Alimitations%2520of%2520traditional%2520techniques%2520that%2520often%2520fail%2520to%2520capture%2520the%2520full%250Aspectrum%2520of%2520human%2520expressions%2520and%2520the%2520uniqueness%2520of%2520individual%2520facial%2520styles.%250ATo%2520address%2520these%2520issues%252C%2520we%2520propose%2520EMO%252C%2520a%2520novel%2520framework%2520that%2520utilizes%2520a%250Adirect%2520audio-to-video%2520synthesis%2520approach%252C%2520bypassing%2520the%2520need%2520for%2520intermediate%250A3D%2520models%2520or%2520facial%2520landmarks.%2520Our%2520method%2520ensures%2520seamless%2520frame%2520transitions%250Aand%2520consistent%2520identity%2520preservation%2520throughout%2520the%2520video%252C%2520resulting%2520in%2520highly%250Aexpressive%2520and%2520lifelike%2520animations.%2520Experimental%2520results%2520demonsrate%2520that%2520EMO%2520is%250Aable%2520to%2520produce%2520not%2520only%2520convincing%2520speaking%2520videos%2520but%2520also%2520singing%2520videos%2520in%250Avarious%2520styles%252C%2520significantly%2520outperforming%2520existing%2520state-of-the-art%250Amethodologies%2520in%2520terms%2520of%2520expressiveness%2520and%2520realism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMO%3A%20Emote%20Portrait%20Alive%20--%20Generating%20Expressive%20Portrait%20Videos%20with%0A%20%20Audio2Video%20Diffusion%20Model%20under%20Weak%20Conditions&entry.906535625=Linrui%20Tian%20and%20Qi%20Wang%20and%20Bang%20Zhang%20and%20Liefeng%20Bo&entry.1292438233=%20%20In%20this%20work%2C%20we%20tackle%20the%20challenge%20of%20enhancing%20the%20realism%20and%0Aexpressiveness%20in%20talking%20head%20video%20generation%20by%20focusing%20on%20the%20dynamic%20and%0Anuanced%20relationship%20between%20audio%20cues%20and%20facial%20movements.%20We%20identify%20the%0Alimitations%20of%20traditional%20techniques%20that%20often%20fail%20to%20capture%20the%20full%0Aspectrum%20of%20human%20expressions%20and%20the%20uniqueness%20of%20individual%20facial%20styles.%0ATo%20address%20these%20issues%2C%20we%20propose%20EMO%2C%20a%20novel%20framework%20that%20utilizes%20a%0Adirect%20audio-to-video%20synthesis%20approach%2C%20bypassing%20the%20need%20for%20intermediate%0A3D%20models%20or%20facial%20landmarks.%20Our%20method%20ensures%20seamless%20frame%20transitions%0Aand%20consistent%20identity%20preservation%20throughout%20the%20video%2C%20resulting%20in%20highly%0Aexpressive%20and%20lifelike%20animations.%20Experimental%20results%20demonsrate%20that%20EMO%20is%0Aable%20to%20produce%20not%20only%20convincing%20speaking%20videos%20but%20also%20singing%20videos%20in%0Avarious%20styles%2C%20significantly%20outperforming%20existing%20state-of-the-art%0Amethodologies%20in%20terms%20of%20expressiveness%20and%20realism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17485v2&entry.124074799=Read"},
{"title": "RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential\n  Recommenders", "author": "Danil Gusak and Gleb Mezentsev and Ivan Oseledets and Evgeny Frolov", "abstract": "  Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.\n", "link": "http://arxiv.org/abs/2408.02354v2", "date": "2024-08-06", "relevancy": 2.3552, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4795}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4728}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RECE%3A%20Reduced%20Cross-Entropy%20Loss%20for%20Large-Catalogue%20Sequential%0A%20%20Recommenders&body=Title%3A%20RECE%3A%20Reduced%20Cross-Entropy%20Loss%20for%20Large-Catalogue%20Sequential%0A%20%20Recommenders%0AAuthor%3A%20Danil%20Gusak%20and%20Gleb%20Mezentsev%20and%20Ivan%20Oseledets%20and%20Evgeny%20Frolov%0AAbstract%3A%20%20%20Scalability%20is%20a%20major%20challenge%20in%20modern%20recommender%20systems.%20In%20sequential%0Arecommendations%2C%20full%20Cross-Entropy%20%28CE%29%20loss%20achieves%20state-of-the-art%0Arecommendation%20quality%20but%20consumes%20excessive%20GPU%20memory%20with%20large%20item%0Acatalogs%2C%20limiting%20its%20practicality.%20Using%20a%20GPU-efficient%20locality-sensitive%0Ahashing-like%20algorithm%20for%20approximating%20large%20tensor%20of%20logits%2C%20this%20paper%0Aintroduces%20a%20novel%20RECE%20%28REduced%20Cross-Entropy%29%20loss.%20RECE%20significantly%0Areduces%20memory%20consumption%20while%20allowing%20one%20to%20enjoy%20the%20state-of-the-art%0Aperformance%20of%20full%20CE%20loss.%20Experimental%20results%20on%20various%20datasets%20show%20that%0ARECE%20cuts%20training%20peak%20memory%20usage%20by%20up%20to%2012%20times%20compared%20to%20existing%0Amethods%20while%20retaining%20or%20exceeding%20performance%20metrics%20of%20CE%20loss.%20The%0Aapproach%20also%20opens%20up%20new%20possibilities%20for%20large-scale%20applications%20in%20other%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02354v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRECE%253A%2520Reduced%2520Cross-Entropy%2520Loss%2520for%2520Large-Catalogue%2520Sequential%250A%2520%2520Recommenders%26entry.906535625%3DDanil%2520Gusak%2520and%2520Gleb%2520Mezentsev%2520and%2520Ivan%2520Oseledets%2520and%2520Evgeny%2520Frolov%26entry.1292438233%3D%2520%2520Scalability%2520is%2520a%2520major%2520challenge%2520in%2520modern%2520recommender%2520systems.%2520In%2520sequential%250Arecommendations%252C%2520full%2520Cross-Entropy%2520%2528CE%2529%2520loss%2520achieves%2520state-of-the-art%250Arecommendation%2520quality%2520but%2520consumes%2520excessive%2520GPU%2520memory%2520with%2520large%2520item%250Acatalogs%252C%2520limiting%2520its%2520practicality.%2520Using%2520a%2520GPU-efficient%2520locality-sensitive%250Ahashing-like%2520algorithm%2520for%2520approximating%2520large%2520tensor%2520of%2520logits%252C%2520this%2520paper%250Aintroduces%2520a%2520novel%2520RECE%2520%2528REduced%2520Cross-Entropy%2529%2520loss.%2520RECE%2520significantly%250Areduces%2520memory%2520consumption%2520while%2520allowing%2520one%2520to%2520enjoy%2520the%2520state-of-the-art%250Aperformance%2520of%2520full%2520CE%2520loss.%2520Experimental%2520results%2520on%2520various%2520datasets%2520show%2520that%250ARECE%2520cuts%2520training%2520peak%2520memory%2520usage%2520by%2520up%2520to%252012%2520times%2520compared%2520to%2520existing%250Amethods%2520while%2520retaining%2520or%2520exceeding%2520performance%2520metrics%2520of%2520CE%2520loss.%2520The%250Aapproach%2520also%2520opens%2520up%2520new%2520possibilities%2520for%2520large-scale%2520applications%2520in%2520other%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02354v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RECE%3A%20Reduced%20Cross-Entropy%20Loss%20for%20Large-Catalogue%20Sequential%0A%20%20Recommenders&entry.906535625=Danil%20Gusak%20and%20Gleb%20Mezentsev%20and%20Ivan%20Oseledets%20and%20Evgeny%20Frolov&entry.1292438233=%20%20Scalability%20is%20a%20major%20challenge%20in%20modern%20recommender%20systems.%20In%20sequential%0Arecommendations%2C%20full%20Cross-Entropy%20%28CE%29%20loss%20achieves%20state-of-the-art%0Arecommendation%20quality%20but%20consumes%20excessive%20GPU%20memory%20with%20large%20item%0Acatalogs%2C%20limiting%20its%20practicality.%20Using%20a%20GPU-efficient%20locality-sensitive%0Ahashing-like%20algorithm%20for%20approximating%20large%20tensor%20of%20logits%2C%20this%20paper%0Aintroduces%20a%20novel%20RECE%20%28REduced%20Cross-Entropy%29%20loss.%20RECE%20significantly%0Areduces%20memory%20consumption%20while%20allowing%20one%20to%20enjoy%20the%20state-of-the-art%0Aperformance%20of%20full%20CE%20loss.%20Experimental%20results%20on%20various%20datasets%20show%20that%0ARECE%20cuts%20training%20peak%20memory%20usage%20by%20up%20to%2012%20times%20compared%20to%20existing%0Amethods%20while%20retaining%20or%20exceeding%20performance%20metrics%20of%20CE%20loss.%20The%0Aapproach%20also%20opens%20up%20new%20possibilities%20for%20large-scale%20applications%20in%20other%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02354v2&entry.124074799=Read"},
{"title": "GenAI Arena: An Open Evaluation Platform for Generative Models", "author": "Dongfu Jiang and Max Ku and Tianle Li and Yuansheng Ni and Shizhuo Sun and Rongqi Fan and Wenhu Chen", "abstract": "  Generative AI has made remarkable strides to revolutionize fields such as\nimage and video generation. These advancements are driven by innovative\nalgorithms, architecture, and data. However, the rapid proliferation of\ngenerative models has highlighted a critical gap: the absence of trustworthy\nevaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc\noften fail to capture the nuanced quality and user satisfaction associated with\ngenerative outputs. This paper proposes an open platform GenAI-Arena to\nevaluate different image and video generative models, where users can actively\nparticipate in evaluating these models. By leveraging collective user feedback\nand votes, GenAI-Arena aims to provide a more democratic and accurate measure\nof model performance. It covers three arenas for text-to-image generation,\ntext-to-video generation, and image editing respectively. Currently, we cover a\ntotal of 27 open-source generative models. GenAI-Arena has been operating for\nfour months, amassing over 6000 votes from the community. We describe our\nplatform, analyze the data, and explain the statistical methods for ranking the\nmodels. To further promote the research in building model-based evaluation\nmetrics, we release a cleaned version of our preference data for the three\ntasks, namely GenAI-Bench. We prompt the existing multi-modal models like\nGemini, GPT-4o to mimic human voting. We compute the correlation between model\nvoting with human voting to understand their judging abilities. Our results\nshow existing multimodal models are still lagging in assessing the generated\nvisual content, even the best model GPT-4o only achieves a Pearson correlation\nof 0.22 in the quality subscore, and behaves like random guessing in others.\n", "link": "http://arxiv.org/abs/2406.04485v3", "date": "2024-08-06", "relevancy": 2.3458, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6272}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5666}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenAI%20Arena%3A%20An%20Open%20Evaluation%20Platform%20for%20Generative%20Models&body=Title%3A%20GenAI%20Arena%3A%20An%20Open%20Evaluation%20Platform%20for%20Generative%20Models%0AAuthor%3A%20Dongfu%20Jiang%20and%20Max%20Ku%20and%20Tianle%20Li%20and%20Yuansheng%20Ni%20and%20Shizhuo%20Sun%20and%20Rongqi%20Fan%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Generative%20AI%20has%20made%20remarkable%20strides%20to%20revolutionize%20fields%20such%20as%0Aimage%20and%20video%20generation.%20These%20advancements%20are%20driven%20by%20innovative%0Aalgorithms%2C%20architecture%2C%20and%20data.%20However%2C%20the%20rapid%20proliferation%20of%0Agenerative%20models%20has%20highlighted%20a%20critical%20gap%3A%20the%20absence%20of%20trustworthy%0Aevaluation%20metrics.%20Current%20automatic%20assessments%20such%20as%20FID%2C%20CLIP%2C%20FVD%2C%20etc%0Aoften%20fail%20to%20capture%20the%20nuanced%20quality%20and%20user%20satisfaction%20associated%20with%0Agenerative%20outputs.%20This%20paper%20proposes%20an%20open%20platform%20GenAI-Arena%20to%0Aevaluate%20different%20image%20and%20video%20generative%20models%2C%20where%20users%20can%20actively%0Aparticipate%20in%20evaluating%20these%20models.%20By%20leveraging%20collective%20user%20feedback%0Aand%20votes%2C%20GenAI-Arena%20aims%20to%20provide%20a%20more%20democratic%20and%20accurate%20measure%0Aof%20model%20performance.%20It%20covers%20three%20arenas%20for%20text-to-image%20generation%2C%0Atext-to-video%20generation%2C%20and%20image%20editing%20respectively.%20Currently%2C%20we%20cover%20a%0Atotal%20of%2027%20open-source%20generative%20models.%20GenAI-Arena%20has%20been%20operating%20for%0Afour%20months%2C%20amassing%20over%206000%20votes%20from%20the%20community.%20We%20describe%20our%0Aplatform%2C%20analyze%20the%20data%2C%20and%20explain%20the%20statistical%20methods%20for%20ranking%20the%0Amodels.%20To%20further%20promote%20the%20research%20in%20building%20model-based%20evaluation%0Ametrics%2C%20we%20release%20a%20cleaned%20version%20of%20our%20preference%20data%20for%20the%20three%0Atasks%2C%20namely%20GenAI-Bench.%20We%20prompt%20the%20existing%20multi-modal%20models%20like%0AGemini%2C%20GPT-4o%20to%20mimic%20human%20voting.%20We%20compute%20the%20correlation%20between%20model%0Avoting%20with%20human%20voting%20to%20understand%20their%20judging%20abilities.%20Our%20results%0Ashow%20existing%20multimodal%20models%20are%20still%20lagging%20in%20assessing%20the%20generated%0Avisual%20content%2C%20even%20the%20best%20model%20GPT-4o%20only%20achieves%20a%20Pearson%20correlation%0Aof%200.22%20in%20the%20quality%20subscore%2C%20and%20behaves%20like%20random%20guessing%20in%20others.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04485v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenAI%2520Arena%253A%2520An%2520Open%2520Evaluation%2520Platform%2520for%2520Generative%2520Models%26entry.906535625%3DDongfu%2520Jiang%2520and%2520Max%2520Ku%2520and%2520Tianle%2520Li%2520and%2520Yuansheng%2520Ni%2520and%2520Shizhuo%2520Sun%2520and%2520Rongqi%2520Fan%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Generative%2520AI%2520has%2520made%2520remarkable%2520strides%2520to%2520revolutionize%2520fields%2520such%2520as%250Aimage%2520and%2520video%2520generation.%2520These%2520advancements%2520are%2520driven%2520by%2520innovative%250Aalgorithms%252C%2520architecture%252C%2520and%2520data.%2520However%252C%2520the%2520rapid%2520proliferation%2520of%250Agenerative%2520models%2520has%2520highlighted%2520a%2520critical%2520gap%253A%2520the%2520absence%2520of%2520trustworthy%250Aevaluation%2520metrics.%2520Current%2520automatic%2520assessments%2520such%2520as%2520FID%252C%2520CLIP%252C%2520FVD%252C%2520etc%250Aoften%2520fail%2520to%2520capture%2520the%2520nuanced%2520quality%2520and%2520user%2520satisfaction%2520associated%2520with%250Agenerative%2520outputs.%2520This%2520paper%2520proposes%2520an%2520open%2520platform%2520GenAI-Arena%2520to%250Aevaluate%2520different%2520image%2520and%2520video%2520generative%2520models%252C%2520where%2520users%2520can%2520actively%250Aparticipate%2520in%2520evaluating%2520these%2520models.%2520By%2520leveraging%2520collective%2520user%2520feedback%250Aand%2520votes%252C%2520GenAI-Arena%2520aims%2520to%2520provide%2520a%2520more%2520democratic%2520and%2520accurate%2520measure%250Aof%2520model%2520performance.%2520It%2520covers%2520three%2520arenas%2520for%2520text-to-image%2520generation%252C%250Atext-to-video%2520generation%252C%2520and%2520image%2520editing%2520respectively.%2520Currently%252C%2520we%2520cover%2520a%250Atotal%2520of%252027%2520open-source%2520generative%2520models.%2520GenAI-Arena%2520has%2520been%2520operating%2520for%250Afour%2520months%252C%2520amassing%2520over%25206000%2520votes%2520from%2520the%2520community.%2520We%2520describe%2520our%250Aplatform%252C%2520analyze%2520the%2520data%252C%2520and%2520explain%2520the%2520statistical%2520methods%2520for%2520ranking%2520the%250Amodels.%2520To%2520further%2520promote%2520the%2520research%2520in%2520building%2520model-based%2520evaluation%250Ametrics%252C%2520we%2520release%2520a%2520cleaned%2520version%2520of%2520our%2520preference%2520data%2520for%2520the%2520three%250Atasks%252C%2520namely%2520GenAI-Bench.%2520We%2520prompt%2520the%2520existing%2520multi-modal%2520models%2520like%250AGemini%252C%2520GPT-4o%2520to%2520mimic%2520human%2520voting.%2520We%2520compute%2520the%2520correlation%2520between%2520model%250Avoting%2520with%2520human%2520voting%2520to%2520understand%2520their%2520judging%2520abilities.%2520Our%2520results%250Ashow%2520existing%2520multimodal%2520models%2520are%2520still%2520lagging%2520in%2520assessing%2520the%2520generated%250Avisual%2520content%252C%2520even%2520the%2520best%2520model%2520GPT-4o%2520only%2520achieves%2520a%2520Pearson%2520correlation%250Aof%25200.22%2520in%2520the%2520quality%2520subscore%252C%2520and%2520behaves%2520like%2520random%2520guessing%2520in%2520others.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04485v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenAI%20Arena%3A%20An%20Open%20Evaluation%20Platform%20for%20Generative%20Models&entry.906535625=Dongfu%20Jiang%20and%20Max%20Ku%20and%20Tianle%20Li%20and%20Yuansheng%20Ni%20and%20Shizhuo%20Sun%20and%20Rongqi%20Fan%20and%20Wenhu%20Chen&entry.1292438233=%20%20Generative%20AI%20has%20made%20remarkable%20strides%20to%20revolutionize%20fields%20such%20as%0Aimage%20and%20video%20generation.%20These%20advancements%20are%20driven%20by%20innovative%0Aalgorithms%2C%20architecture%2C%20and%20data.%20However%2C%20the%20rapid%20proliferation%20of%0Agenerative%20models%20has%20highlighted%20a%20critical%20gap%3A%20the%20absence%20of%20trustworthy%0Aevaluation%20metrics.%20Current%20automatic%20assessments%20such%20as%20FID%2C%20CLIP%2C%20FVD%2C%20etc%0Aoften%20fail%20to%20capture%20the%20nuanced%20quality%20and%20user%20satisfaction%20associated%20with%0Agenerative%20outputs.%20This%20paper%20proposes%20an%20open%20platform%20GenAI-Arena%20to%0Aevaluate%20different%20image%20and%20video%20generative%20models%2C%20where%20users%20can%20actively%0Aparticipate%20in%20evaluating%20these%20models.%20By%20leveraging%20collective%20user%20feedback%0Aand%20votes%2C%20GenAI-Arena%20aims%20to%20provide%20a%20more%20democratic%20and%20accurate%20measure%0Aof%20model%20performance.%20It%20covers%20three%20arenas%20for%20text-to-image%20generation%2C%0Atext-to-video%20generation%2C%20and%20image%20editing%20respectively.%20Currently%2C%20we%20cover%20a%0Atotal%20of%2027%20open-source%20generative%20models.%20GenAI-Arena%20has%20been%20operating%20for%0Afour%20months%2C%20amassing%20over%206000%20votes%20from%20the%20community.%20We%20describe%20our%0Aplatform%2C%20analyze%20the%20data%2C%20and%20explain%20the%20statistical%20methods%20for%20ranking%20the%0Amodels.%20To%20further%20promote%20the%20research%20in%20building%20model-based%20evaluation%0Ametrics%2C%20we%20release%20a%20cleaned%20version%20of%20our%20preference%20data%20for%20the%20three%0Atasks%2C%20namely%20GenAI-Bench.%20We%20prompt%20the%20existing%20multi-modal%20models%20like%0AGemini%2C%20GPT-4o%20to%20mimic%20human%20voting.%20We%20compute%20the%20correlation%20between%20model%0Avoting%20with%20human%20voting%20to%20understand%20their%20judging%20abilities.%20Our%20results%0Ashow%20existing%20multimodal%20models%20are%20still%20lagging%20in%20assessing%20the%20generated%0Avisual%20content%2C%20even%20the%20best%20model%20GPT-4o%20only%20achieves%20a%20Pearson%20correlation%0Aof%200.22%20in%20the%20quality%20subscore%2C%20and%20behaves%20like%20random%20guessing%20in%20others.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04485v3&entry.124074799=Read"},
{"title": "Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition", "author": "Jinfu Liu and Chen Chen and Mengyuan Liu", "abstract": "  Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.\n", "link": "http://arxiv.org/abs/2407.15706v5", "date": "2024-08-06", "relevancy": 2.34, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6207}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5714}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition&body=Title%3A%20Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition%0AAuthor%3A%20Jinfu%20Liu%20and%20Chen%20Chen%20and%20Mengyuan%20Liu%0AAbstract%3A%20%20%20Skeleton-based%20action%20recognition%20has%20garnered%20significant%20attention%20due%20to%0Athe%20utilization%20of%20concise%20and%20resilient%20skeletons.%20Nevertheless%2C%20the%20absence%0Aof%20detailed%20body%20information%20in%20skeletons%20restricts%20performance%2C%20while%20other%0Amultimodal%20methods%20require%20substantial%20inference%20resources%20and%20are%20inefficient%0Awhen%20using%20multimodal%20data%20during%20both%20training%20and%20inference%20stages.%20To%0Aaddress%20this%20and%20fully%20harness%20the%20complementary%20multimodal%20features%2C%20we%0Apropose%20a%20novel%20multi-modality%20co-learning%20%28MMCL%29%20framework%20by%20leveraging%20the%0Amultimodal%20large%20language%20models%20%28LLMs%29%20as%20auxiliary%20networks%20for%20efficient%0Askeleton-based%20action%20recognition%2C%20which%20engages%20in%20multi-modality%20co-learning%0Aduring%20the%20training%20stage%20and%20keeps%20efficiency%20by%20employing%20only%20concise%0Askeletons%20in%20inference.%20Our%20MMCL%20framework%20primarily%20consists%20of%20two%20modules.%0AFirst%2C%20the%20Feature%20Alignment%20Module%20%28FAM%29%20extracts%20rich%20RGB%20features%20from%20video%0Aframes%20and%20aligns%20them%20with%20global%20skeleton%20features%20via%20contrastive%20learning.%0ASecond%2C%20the%20Feature%20Refinement%20Module%20%28FRM%29%20uses%20RGB%20images%20with%20temporal%0Ainformation%20and%20text%20instruction%20to%20generate%20instructive%20features%20based%20on%20the%0Apowerful%20generalization%20of%20multimodal%20LLMs.%20These%20instructive%20text%20features%0Awill%20further%20refine%20the%20classification%20scores%20and%20the%20refined%20scores%20will%0Aenhance%20the%20model%27s%20robustness%20and%20generalization%20in%20a%20manner%20similar%20to%20soft%0Alabels.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%20and%20Northwestern-UCLA%0Abenchmarks%20consistently%20verify%20the%20effectiveness%20of%20our%20MMCL%2C%20which%20outperforms%0Athe%20existing%20skeleton-based%20action%20recognition%20methods.%20Meanwhile%2C%20experiments%0Aon%20UTD-MHAD%20and%20SYSU-Action%20datasets%20demonstrate%20the%20commendable%20generalization%0Aof%20our%20MMCL%20in%20zero-shot%20and%20domain-adaptive%20action%20recognition.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/liujf69/MMCL-Action.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15706v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modality%2520Co-Learning%2520for%2520Efficient%2520Skeleton-based%2520Action%250A%2520%2520Recognition%26entry.906535625%3DJinfu%2520Liu%2520and%2520Chen%2520Chen%2520and%2520Mengyuan%2520Liu%26entry.1292438233%3D%2520%2520Skeleton-based%2520action%2520recognition%2520has%2520garnered%2520significant%2520attention%2520due%2520to%250Athe%2520utilization%2520of%2520concise%2520and%2520resilient%2520skeletons.%2520Nevertheless%252C%2520the%2520absence%250Aof%2520detailed%2520body%2520information%2520in%2520skeletons%2520restricts%2520performance%252C%2520while%2520other%250Amultimodal%2520methods%2520require%2520substantial%2520inference%2520resources%2520and%2520are%2520inefficient%250Awhen%2520using%2520multimodal%2520data%2520during%2520both%2520training%2520and%2520inference%2520stages.%2520To%250Aaddress%2520this%2520and%2520fully%2520harness%2520the%2520complementary%2520multimodal%2520features%252C%2520we%250Apropose%2520a%2520novel%2520multi-modality%2520co-learning%2520%2528MMCL%2529%2520framework%2520by%2520leveraging%2520the%250Amultimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520auxiliary%2520networks%2520for%2520efficient%250Askeleton-based%2520action%2520recognition%252C%2520which%2520engages%2520in%2520multi-modality%2520co-learning%250Aduring%2520the%2520training%2520stage%2520and%2520keeps%2520efficiency%2520by%2520employing%2520only%2520concise%250Askeletons%2520in%2520inference.%2520Our%2520MMCL%2520framework%2520primarily%2520consists%2520of%2520two%2520modules.%250AFirst%252C%2520the%2520Feature%2520Alignment%2520Module%2520%2528FAM%2529%2520extracts%2520rich%2520RGB%2520features%2520from%2520video%250Aframes%2520and%2520aligns%2520them%2520with%2520global%2520skeleton%2520features%2520via%2520contrastive%2520learning.%250ASecond%252C%2520the%2520Feature%2520Refinement%2520Module%2520%2528FRM%2529%2520uses%2520RGB%2520images%2520with%2520temporal%250Ainformation%2520and%2520text%2520instruction%2520to%2520generate%2520instructive%2520features%2520based%2520on%2520the%250Apowerful%2520generalization%2520of%2520multimodal%2520LLMs.%2520These%2520instructive%2520text%2520features%250Awill%2520further%2520refine%2520the%2520classification%2520scores%2520and%2520the%2520refined%2520scores%2520will%250Aenhance%2520the%2520model%2527s%2520robustness%2520and%2520generalization%2520in%2520a%2520manner%2520similar%2520to%2520soft%250Alabels.%2520Extensive%2520experiments%2520on%2520NTU%2520RGB%252BD%252C%2520NTU%2520RGB%252BD%2520120%2520and%2520Northwestern-UCLA%250Abenchmarks%2520consistently%2520verify%2520the%2520effectiveness%2520of%2520our%2520MMCL%252C%2520which%2520outperforms%250Athe%2520existing%2520skeleton-based%2520action%2520recognition%2520methods.%2520Meanwhile%252C%2520experiments%250Aon%2520UTD-MHAD%2520and%2520SYSU-Action%2520datasets%2520demonstrate%2520the%2520commendable%2520generalization%250Aof%2520our%2520MMCL%2520in%2520zero-shot%2520and%2520domain-adaptive%2520action%2520recognition.%2520Our%2520code%2520is%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/liujf69/MMCL-Action.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15706v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition&entry.906535625=Jinfu%20Liu%20and%20Chen%20Chen%20and%20Mengyuan%20Liu&entry.1292438233=%20%20Skeleton-based%20action%20recognition%20has%20garnered%20significant%20attention%20due%20to%0Athe%20utilization%20of%20concise%20and%20resilient%20skeletons.%20Nevertheless%2C%20the%20absence%0Aof%20detailed%20body%20information%20in%20skeletons%20restricts%20performance%2C%20while%20other%0Amultimodal%20methods%20require%20substantial%20inference%20resources%20and%20are%20inefficient%0Awhen%20using%20multimodal%20data%20during%20both%20training%20and%20inference%20stages.%20To%0Aaddress%20this%20and%20fully%20harness%20the%20complementary%20multimodal%20features%2C%20we%0Apropose%20a%20novel%20multi-modality%20co-learning%20%28MMCL%29%20framework%20by%20leveraging%20the%0Amultimodal%20large%20language%20models%20%28LLMs%29%20as%20auxiliary%20networks%20for%20efficient%0Askeleton-based%20action%20recognition%2C%20which%20engages%20in%20multi-modality%20co-learning%0Aduring%20the%20training%20stage%20and%20keeps%20efficiency%20by%20employing%20only%20concise%0Askeletons%20in%20inference.%20Our%20MMCL%20framework%20primarily%20consists%20of%20two%20modules.%0AFirst%2C%20the%20Feature%20Alignment%20Module%20%28FAM%29%20extracts%20rich%20RGB%20features%20from%20video%0Aframes%20and%20aligns%20them%20with%20global%20skeleton%20features%20via%20contrastive%20learning.%0ASecond%2C%20the%20Feature%20Refinement%20Module%20%28FRM%29%20uses%20RGB%20images%20with%20temporal%0Ainformation%20and%20text%20instruction%20to%20generate%20instructive%20features%20based%20on%20the%0Apowerful%20generalization%20of%20multimodal%20LLMs.%20These%20instructive%20text%20features%0Awill%20further%20refine%20the%20classification%20scores%20and%20the%20refined%20scores%20will%0Aenhance%20the%20model%27s%20robustness%20and%20generalization%20in%20a%20manner%20similar%20to%20soft%0Alabels.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%20and%20Northwestern-UCLA%0Abenchmarks%20consistently%20verify%20the%20effectiveness%20of%20our%20MMCL%2C%20which%20outperforms%0Athe%20existing%20skeleton-based%20action%20recognition%20methods.%20Meanwhile%2C%20experiments%0Aon%20UTD-MHAD%20and%20SYSU-Action%20datasets%20demonstrate%20the%20commendable%20generalization%0Aof%20our%20MMCL%20in%20zero-shot%20and%20domain-adaptive%20action%20recognition.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/liujf69/MMCL-Action.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15706v5&entry.124074799=Read"},
{"title": "RSB-Pose: Robust Short-Baseline Binocular 3D Human Pose Estimation with\n  Occlusion Handling", "author": "Xiaoyue Wan and Zhuo Chen and Yiming Bao and Xu Zhao", "abstract": "  In the domain of 3D Human Pose Estimation, which finds widespread daily\napplications, the requirement for convenient acquisition equipment continues to\ngrow. To satisfy this demand, we set our sights on a short-baseline binocular\nsetting that offers both portability and a geometric measurement property that\nradically mitigates depth ambiguity. However, as the binocular baseline\nshortens, two serious challenges emerge: first, the robustness of 3D\nreconstruction against 2D errors deteriorates; and second, occlusion reoccurs\ndue to the limited visual differences between two views. To address the first\nchallenge, we propose the Stereo Co-Keypoints Estimation module to improve the\nview consistency of 2D keypoints and enhance the 3D robustness. In this module,\nthe disparity is utilized to represent the correspondence of binocular 2D\npoints and the Stereo Volume Feature is introduced to contain binocular\nfeatures across different disparities. Through the regression of SVF, two-view\n2D keypoints are simultaneously estimated in a collaborative way which\nrestricts their view consistency. Furthermore, to deal with occlusions, a\nPre-trained Pose Transformer module is introduced. Through this module, 3D\nposes are refined by perceiving pose coherence, a representation of joint\ncorrelations. This perception is injected by the Pose Transformer network and\nlearned through a pre-training task that recovers iterative masked joints.\nComprehensive experiments carried out on H36M and MHAD datasets, complemented\nby visualizations, validate the effectiveness of our approach in the\nshort-baseline binocular 3D Human Pose Estimation and occlusion handling.\n", "link": "http://arxiv.org/abs/2311.14242v2", "date": "2024-08-06", "relevancy": 2.3155, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5847}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5778}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSB-Pose%3A%20Robust%20Short-Baseline%20Binocular%203D%20Human%20Pose%20Estimation%20with%0A%20%20Occlusion%20Handling&body=Title%3A%20RSB-Pose%3A%20Robust%20Short-Baseline%20Binocular%203D%20Human%20Pose%20Estimation%20with%0A%20%20Occlusion%20Handling%0AAuthor%3A%20Xiaoyue%20Wan%20and%20Zhuo%20Chen%20and%20Yiming%20Bao%20and%20Xu%20Zhao%0AAbstract%3A%20%20%20In%20the%20domain%20of%203D%20Human%20Pose%20Estimation%2C%20which%20finds%20widespread%20daily%0Aapplications%2C%20the%20requirement%20for%20convenient%20acquisition%20equipment%20continues%20to%0Agrow.%20To%20satisfy%20this%20demand%2C%20we%20set%20our%20sights%20on%20a%20short-baseline%20binocular%0Asetting%20that%20offers%20both%20portability%20and%20a%20geometric%20measurement%20property%20that%0Aradically%20mitigates%20depth%20ambiguity.%20However%2C%20as%20the%20binocular%20baseline%0Ashortens%2C%20two%20serious%20challenges%20emerge%3A%20first%2C%20the%20robustness%20of%203D%0Areconstruction%20against%202D%20errors%20deteriorates%3B%20and%20second%2C%20occlusion%20reoccurs%0Adue%20to%20the%20limited%20visual%20differences%20between%20two%20views.%20To%20address%20the%20first%0Achallenge%2C%20we%20propose%20the%20Stereo%20Co-Keypoints%20Estimation%20module%20to%20improve%20the%0Aview%20consistency%20of%202D%20keypoints%20and%20enhance%20the%203D%20robustness.%20In%20this%20module%2C%0Athe%20disparity%20is%20utilized%20to%20represent%20the%20correspondence%20of%20binocular%202D%0Apoints%20and%20the%20Stereo%20Volume%20Feature%20is%20introduced%20to%20contain%20binocular%0Afeatures%20across%20different%20disparities.%20Through%20the%20regression%20of%20SVF%2C%20two-view%0A2D%20keypoints%20are%20simultaneously%20estimated%20in%20a%20collaborative%20way%20which%0Arestricts%20their%20view%20consistency.%20Furthermore%2C%20to%20deal%20with%20occlusions%2C%20a%0APre-trained%20Pose%20Transformer%20module%20is%20introduced.%20Through%20this%20module%2C%203D%0Aposes%20are%20refined%20by%20perceiving%20pose%20coherence%2C%20a%20representation%20of%20joint%0Acorrelations.%20This%20perception%20is%20injected%20by%20the%20Pose%20Transformer%20network%20and%0Alearned%20through%20a%20pre-training%20task%20that%20recovers%20iterative%20masked%20joints.%0AComprehensive%20experiments%20carried%20out%20on%20H36M%20and%20MHAD%20datasets%2C%20complemented%0Aby%20visualizations%2C%20validate%20the%20effectiveness%20of%20our%20approach%20in%20the%0Ashort-baseline%20binocular%203D%20Human%20Pose%20Estimation%20and%20occlusion%20handling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSB-Pose%253A%2520Robust%2520Short-Baseline%2520Binocular%25203D%2520Human%2520Pose%2520Estimation%2520with%250A%2520%2520Occlusion%2520Handling%26entry.906535625%3DXiaoyue%2520Wan%2520and%2520Zhuo%2520Chen%2520and%2520Yiming%2520Bao%2520and%2520Xu%2520Zhao%26entry.1292438233%3D%2520%2520In%2520the%2520domain%2520of%25203D%2520Human%2520Pose%2520Estimation%252C%2520which%2520finds%2520widespread%2520daily%250Aapplications%252C%2520the%2520requirement%2520for%2520convenient%2520acquisition%2520equipment%2520continues%2520to%250Agrow.%2520To%2520satisfy%2520this%2520demand%252C%2520we%2520set%2520our%2520sights%2520on%2520a%2520short-baseline%2520binocular%250Asetting%2520that%2520offers%2520both%2520portability%2520and%2520a%2520geometric%2520measurement%2520property%2520that%250Aradically%2520mitigates%2520depth%2520ambiguity.%2520However%252C%2520as%2520the%2520binocular%2520baseline%250Ashortens%252C%2520two%2520serious%2520challenges%2520emerge%253A%2520first%252C%2520the%2520robustness%2520of%25203D%250Areconstruction%2520against%25202D%2520errors%2520deteriorates%253B%2520and%2520second%252C%2520occlusion%2520reoccurs%250Adue%2520to%2520the%2520limited%2520visual%2520differences%2520between%2520two%2520views.%2520To%2520address%2520the%2520first%250Achallenge%252C%2520we%2520propose%2520the%2520Stereo%2520Co-Keypoints%2520Estimation%2520module%2520to%2520improve%2520the%250Aview%2520consistency%2520of%25202D%2520keypoints%2520and%2520enhance%2520the%25203D%2520robustness.%2520In%2520this%2520module%252C%250Athe%2520disparity%2520is%2520utilized%2520to%2520represent%2520the%2520correspondence%2520of%2520binocular%25202D%250Apoints%2520and%2520the%2520Stereo%2520Volume%2520Feature%2520is%2520introduced%2520to%2520contain%2520binocular%250Afeatures%2520across%2520different%2520disparities.%2520Through%2520the%2520regression%2520of%2520SVF%252C%2520two-view%250A2D%2520keypoints%2520are%2520simultaneously%2520estimated%2520in%2520a%2520collaborative%2520way%2520which%250Arestricts%2520their%2520view%2520consistency.%2520Furthermore%252C%2520to%2520deal%2520with%2520occlusions%252C%2520a%250APre-trained%2520Pose%2520Transformer%2520module%2520is%2520introduced.%2520Through%2520this%2520module%252C%25203D%250Aposes%2520are%2520refined%2520by%2520perceiving%2520pose%2520coherence%252C%2520a%2520representation%2520of%2520joint%250Acorrelations.%2520This%2520perception%2520is%2520injected%2520by%2520the%2520Pose%2520Transformer%2520network%2520and%250Alearned%2520through%2520a%2520pre-training%2520task%2520that%2520recovers%2520iterative%2520masked%2520joints.%250AComprehensive%2520experiments%2520carried%2520out%2520on%2520H36M%2520and%2520MHAD%2520datasets%252C%2520complemented%250Aby%2520visualizations%252C%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520the%250Ashort-baseline%2520binocular%25203D%2520Human%2520Pose%2520Estimation%2520and%2520occlusion%2520handling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSB-Pose%3A%20Robust%20Short-Baseline%20Binocular%203D%20Human%20Pose%20Estimation%20with%0A%20%20Occlusion%20Handling&entry.906535625=Xiaoyue%20Wan%20and%20Zhuo%20Chen%20and%20Yiming%20Bao%20and%20Xu%20Zhao&entry.1292438233=%20%20In%20the%20domain%20of%203D%20Human%20Pose%20Estimation%2C%20which%20finds%20widespread%20daily%0Aapplications%2C%20the%20requirement%20for%20convenient%20acquisition%20equipment%20continues%20to%0Agrow.%20To%20satisfy%20this%20demand%2C%20we%20set%20our%20sights%20on%20a%20short-baseline%20binocular%0Asetting%20that%20offers%20both%20portability%20and%20a%20geometric%20measurement%20property%20that%0Aradically%20mitigates%20depth%20ambiguity.%20However%2C%20as%20the%20binocular%20baseline%0Ashortens%2C%20two%20serious%20challenges%20emerge%3A%20first%2C%20the%20robustness%20of%203D%0Areconstruction%20against%202D%20errors%20deteriorates%3B%20and%20second%2C%20occlusion%20reoccurs%0Adue%20to%20the%20limited%20visual%20differences%20between%20two%20views.%20To%20address%20the%20first%0Achallenge%2C%20we%20propose%20the%20Stereo%20Co-Keypoints%20Estimation%20module%20to%20improve%20the%0Aview%20consistency%20of%202D%20keypoints%20and%20enhance%20the%203D%20robustness.%20In%20this%20module%2C%0Athe%20disparity%20is%20utilized%20to%20represent%20the%20correspondence%20of%20binocular%202D%0Apoints%20and%20the%20Stereo%20Volume%20Feature%20is%20introduced%20to%20contain%20binocular%0Afeatures%20across%20different%20disparities.%20Through%20the%20regression%20of%20SVF%2C%20two-view%0A2D%20keypoints%20are%20simultaneously%20estimated%20in%20a%20collaborative%20way%20which%0Arestricts%20their%20view%20consistency.%20Furthermore%2C%20to%20deal%20with%20occlusions%2C%20a%0APre-trained%20Pose%20Transformer%20module%20is%20introduced.%20Through%20this%20module%2C%203D%0Aposes%20are%20refined%20by%20perceiving%20pose%20coherence%2C%20a%20representation%20of%20joint%0Acorrelations.%20This%20perception%20is%20injected%20by%20the%20Pose%20Transformer%20network%20and%0Alearned%20through%20a%20pre-training%20task%20that%20recovers%20iterative%20masked%20joints.%0AComprehensive%20experiments%20carried%20out%20on%20H36M%20and%20MHAD%20datasets%2C%20complemented%0Aby%20visualizations%2C%20validate%20the%20effectiveness%20of%20our%20approach%20in%20the%0Ashort-baseline%20binocular%203D%20Human%20Pose%20Estimation%20and%20occlusion%20handling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14242v2&entry.124074799=Read"},
{"title": "ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually\n  Synced Facial Performer", "author": "Jiazhi Guan and Zhiliang Xu and Hang Zhou and Kaisiyuan Wang and Shengyi He and Zhanwang Zhang and Borong Liang and Haocheng Feng and Errui Ding and Jingtuo Liu and Jingdong Wang and Youjian Zhao and Ziwei Liu", "abstract": "  Lip-syncing videos with given audio is the foundation for various\napplications including the creation of virtual presenters or performers. While\nrecent studies explore high-fidelity lip-sync with different techniques, their\ntask-orientated models either require long-term videos for clip-specific\ntraining or retain visible artifacts. In this paper, we propose a unified and\neffective framework ReSyncer, that synchronizes generalized audio-visual facial\ninformation. The key design is revisiting and rewiring the Style-based\ngenerator to efficiently adopt 3D facial dynamics predicted by a principled\nstyle-injected Transformer. By simply re-configuring the information insertion\nmechanisms within the noise and style space, our framework fuses motion and\nappearance with unified training. Extensive experiments demonstrate that\nReSyncer not only produces high-fidelity lip-synced videos according to audio,\nbut also supports multiple appealing properties that are suitable for creating\nvirtual presenters and performers, including fast personalized fine-tuning,\nvideo-driven lip-syncing, the transfer of speaking styles, and even face\nswapping. Resources can be found at\nhttps://guanjz20.github.io/projects/ReSyncer.\n", "link": "http://arxiv.org/abs/2408.03284v1", "date": "2024-08-06", "relevancy": 2.3022, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.587}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5788}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReSyncer%3A%20Rewiring%20Style-based%20Generator%20for%20Unified%20Audio-Visually%0A%20%20Synced%20Facial%20Performer&body=Title%3A%20ReSyncer%3A%20Rewiring%20Style-based%20Generator%20for%20Unified%20Audio-Visually%0A%20%20Synced%20Facial%20Performer%0AAuthor%3A%20Jiazhi%20Guan%20and%20Zhiliang%20Xu%20and%20Hang%20Zhou%20and%20Kaisiyuan%20Wang%20and%20Shengyi%20He%20and%20Zhanwang%20Zhang%20and%20Borong%20Liang%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Jingtuo%20Liu%20and%20Jingdong%20Wang%20and%20Youjian%20Zhao%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Lip-syncing%20videos%20with%20given%20audio%20is%20the%20foundation%20for%20various%0Aapplications%20including%20the%20creation%20of%20virtual%20presenters%20or%20performers.%20While%0Arecent%20studies%20explore%20high-fidelity%20lip-sync%20with%20different%20techniques%2C%20their%0Atask-orientated%20models%20either%20require%20long-term%20videos%20for%20clip-specific%0Atraining%20or%20retain%20visible%20artifacts.%20In%20this%20paper%2C%20we%20propose%20a%20unified%20and%0Aeffective%20framework%20ReSyncer%2C%20that%20synchronizes%20generalized%20audio-visual%20facial%0Ainformation.%20The%20key%20design%20is%20revisiting%20and%20rewiring%20the%20Style-based%0Agenerator%20to%20efficiently%20adopt%203D%20facial%20dynamics%20predicted%20by%20a%20principled%0Astyle-injected%20Transformer.%20By%20simply%20re-configuring%20the%20information%20insertion%0Amechanisms%20within%20the%20noise%20and%20style%20space%2C%20our%20framework%20fuses%20motion%20and%0Aappearance%20with%20unified%20training.%20Extensive%20experiments%20demonstrate%20that%0AReSyncer%20not%20only%20produces%20high-fidelity%20lip-synced%20videos%20according%20to%20audio%2C%0Abut%20also%20supports%20multiple%20appealing%20properties%20that%20are%20suitable%20for%20creating%0Avirtual%20presenters%20and%20performers%2C%20including%20fast%20personalized%20fine-tuning%2C%0Avideo-driven%20lip-syncing%2C%20the%20transfer%20of%20speaking%20styles%2C%20and%20even%20face%0Aswapping.%20Resources%20can%20be%20found%20at%0Ahttps%3A//guanjz20.github.io/projects/ReSyncer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReSyncer%253A%2520Rewiring%2520Style-based%2520Generator%2520for%2520Unified%2520Audio-Visually%250A%2520%2520Synced%2520Facial%2520Performer%26entry.906535625%3DJiazhi%2520Guan%2520and%2520Zhiliang%2520Xu%2520and%2520Hang%2520Zhou%2520and%2520Kaisiyuan%2520Wang%2520and%2520Shengyi%2520He%2520and%2520Zhanwang%2520Zhang%2520and%2520Borong%2520Liang%2520and%2520Haocheng%2520Feng%2520and%2520Errui%2520Ding%2520and%2520Jingtuo%2520Liu%2520and%2520Jingdong%2520Wang%2520and%2520Youjian%2520Zhao%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Lip-syncing%2520videos%2520with%2520given%2520audio%2520is%2520the%2520foundation%2520for%2520various%250Aapplications%2520including%2520the%2520creation%2520of%2520virtual%2520presenters%2520or%2520performers.%2520While%250Arecent%2520studies%2520explore%2520high-fidelity%2520lip-sync%2520with%2520different%2520techniques%252C%2520their%250Atask-orientated%2520models%2520either%2520require%2520long-term%2520videos%2520for%2520clip-specific%250Atraining%2520or%2520retain%2520visible%2520artifacts.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520unified%2520and%250Aeffective%2520framework%2520ReSyncer%252C%2520that%2520synchronizes%2520generalized%2520audio-visual%2520facial%250Ainformation.%2520The%2520key%2520design%2520is%2520revisiting%2520and%2520rewiring%2520the%2520Style-based%250Agenerator%2520to%2520efficiently%2520adopt%25203D%2520facial%2520dynamics%2520predicted%2520by%2520a%2520principled%250Astyle-injected%2520Transformer.%2520By%2520simply%2520re-configuring%2520the%2520information%2520insertion%250Amechanisms%2520within%2520the%2520noise%2520and%2520style%2520space%252C%2520our%2520framework%2520fuses%2520motion%2520and%250Aappearance%2520with%2520unified%2520training.%2520Extensive%2520experiments%2520demonstrate%2520that%250AReSyncer%2520not%2520only%2520produces%2520high-fidelity%2520lip-synced%2520videos%2520according%2520to%2520audio%252C%250Abut%2520also%2520supports%2520multiple%2520appealing%2520properties%2520that%2520are%2520suitable%2520for%2520creating%250Avirtual%2520presenters%2520and%2520performers%252C%2520including%2520fast%2520personalized%2520fine-tuning%252C%250Avideo-driven%2520lip-syncing%252C%2520the%2520transfer%2520of%2520speaking%2520styles%252C%2520and%2520even%2520face%250Aswapping.%2520Resources%2520can%2520be%2520found%2520at%250Ahttps%253A//guanjz20.github.io/projects/ReSyncer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReSyncer%3A%20Rewiring%20Style-based%20Generator%20for%20Unified%20Audio-Visually%0A%20%20Synced%20Facial%20Performer&entry.906535625=Jiazhi%20Guan%20and%20Zhiliang%20Xu%20and%20Hang%20Zhou%20and%20Kaisiyuan%20Wang%20and%20Shengyi%20He%20and%20Zhanwang%20Zhang%20and%20Borong%20Liang%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Jingtuo%20Liu%20and%20Jingdong%20Wang%20and%20Youjian%20Zhao%20and%20Ziwei%20Liu&entry.1292438233=%20%20Lip-syncing%20videos%20with%20given%20audio%20is%20the%20foundation%20for%20various%0Aapplications%20including%20the%20creation%20of%20virtual%20presenters%20or%20performers.%20While%0Arecent%20studies%20explore%20high-fidelity%20lip-sync%20with%20different%20techniques%2C%20their%0Atask-orientated%20models%20either%20require%20long-term%20videos%20for%20clip-specific%0Atraining%20or%20retain%20visible%20artifacts.%20In%20this%20paper%2C%20we%20propose%20a%20unified%20and%0Aeffective%20framework%20ReSyncer%2C%20that%20synchronizes%20generalized%20audio-visual%20facial%0Ainformation.%20The%20key%20design%20is%20revisiting%20and%20rewiring%20the%20Style-based%0Agenerator%20to%20efficiently%20adopt%203D%20facial%20dynamics%20predicted%20by%20a%20principled%0Astyle-injected%20Transformer.%20By%20simply%20re-configuring%20the%20information%20insertion%0Amechanisms%20within%20the%20noise%20and%20style%20space%2C%20our%20framework%20fuses%20motion%20and%0Aappearance%20with%20unified%20training.%20Extensive%20experiments%20demonstrate%20that%0AReSyncer%20not%20only%20produces%20high-fidelity%20lip-synced%20videos%20according%20to%20audio%2C%0Abut%20also%20supports%20multiple%20appealing%20properties%20that%20are%20suitable%20for%20creating%0Avirtual%20presenters%20and%20performers%2C%20including%20fast%20personalized%20fine-tuning%2C%0Avideo-driven%20lip-syncing%2C%20the%20transfer%20of%20speaking%20styles%2C%20and%20even%20face%0Aswapping.%20Resources%20can%20be%20found%20at%0Ahttps%3A//guanjz20.github.io/projects/ReSyncer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03284v1&entry.124074799=Read"},
{"title": "Contrastive Learning for Image Complexity Representation", "author": "Shipeng Liu and Liang Zhao and Dengfeng Chen and Zhanping Song", "abstract": "  Quantifying and evaluating image complexity can be instrumental in enhancing\nthe performance of various computer vision tasks. Supervised learning can\neffectively learn image complexity features from well-annotated datasets.\nHowever, creating such datasets requires expensive manual annotation costs. The\nmodels may learn human subjective biases from it. In this work, we introduce\nthe MoCo v2 framework. We utilize contrastive learning to represent image\ncomplexity, named CLIC (Contrastive Learning for Image Complexity). We find\nthat there are complexity differences between different local regions of an\nimage, and propose Random Crop and Mix (RCM), which can produce positive\nsamples consisting of multi-scale local crops. RCM can also expand the train\nset and increase data diversity without introducing additional data. We conduct\nextensive experiments with CLIC, comparing it with both unsupervised and\nsupervised methods. The results demonstrate that the performance of CLIC is\ncomparable to that of state-of-the-art supervised methods. In addition, we\nestablish the pipelines that can apply CLIC to computer vision tasks to\neffectively improve their performance.\n", "link": "http://arxiv.org/abs/2408.03230v1", "date": "2024-08-06", "relevancy": 2.2588, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5791}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Learning%20for%20Image%20Complexity%20Representation&body=Title%3A%20Contrastive%20Learning%20for%20Image%20Complexity%20Representation%0AAuthor%3A%20Shipeng%20Liu%20and%20Liang%20Zhao%20and%20Dengfeng%20Chen%20and%20Zhanping%20Song%0AAbstract%3A%20%20%20Quantifying%20and%20evaluating%20image%20complexity%20can%20be%20instrumental%20in%20enhancing%0Athe%20performance%20of%20various%20computer%20vision%20tasks.%20Supervised%20learning%20can%0Aeffectively%20learn%20image%20complexity%20features%20from%20well-annotated%20datasets.%0AHowever%2C%20creating%20such%20datasets%20requires%20expensive%20manual%20annotation%20costs.%20The%0Amodels%20may%20learn%20human%20subjective%20biases%20from%20it.%20In%20this%20work%2C%20we%20introduce%0Athe%20MoCo%20v2%20framework.%20We%20utilize%20contrastive%20learning%20to%20represent%20image%0Acomplexity%2C%20named%20CLIC%20%28Contrastive%20Learning%20for%20Image%20Complexity%29.%20We%20find%0Athat%20there%20are%20complexity%20differences%20between%20different%20local%20regions%20of%20an%0Aimage%2C%20and%20propose%20Random%20Crop%20and%20Mix%20%28RCM%29%2C%20which%20can%20produce%20positive%0Asamples%20consisting%20of%20multi-scale%20local%20crops.%20RCM%20can%20also%20expand%20the%20train%0Aset%20and%20increase%20data%20diversity%20without%20introducing%20additional%20data.%20We%20conduct%0Aextensive%20experiments%20with%20CLIC%2C%20comparing%20it%20with%20both%20unsupervised%20and%0Asupervised%20methods.%20The%20results%20demonstrate%20that%20the%20performance%20of%20CLIC%20is%0Acomparable%20to%20that%20of%20state-of-the-art%20supervised%20methods.%20In%20addition%2C%20we%0Aestablish%20the%20pipelines%20that%20can%20apply%20CLIC%20to%20computer%20vision%20tasks%20to%0Aeffectively%20improve%20their%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Learning%2520for%2520Image%2520Complexity%2520Representation%26entry.906535625%3DShipeng%2520Liu%2520and%2520Liang%2520Zhao%2520and%2520Dengfeng%2520Chen%2520and%2520Zhanping%2520Song%26entry.1292438233%3D%2520%2520Quantifying%2520and%2520evaluating%2520image%2520complexity%2520can%2520be%2520instrumental%2520in%2520enhancing%250Athe%2520performance%2520of%2520various%2520computer%2520vision%2520tasks.%2520Supervised%2520learning%2520can%250Aeffectively%2520learn%2520image%2520complexity%2520features%2520from%2520well-annotated%2520datasets.%250AHowever%252C%2520creating%2520such%2520datasets%2520requires%2520expensive%2520manual%2520annotation%2520costs.%2520The%250Amodels%2520may%2520learn%2520human%2520subjective%2520biases%2520from%2520it.%2520In%2520this%2520work%252C%2520we%2520introduce%250Athe%2520MoCo%2520v2%2520framework.%2520We%2520utilize%2520contrastive%2520learning%2520to%2520represent%2520image%250Acomplexity%252C%2520named%2520CLIC%2520%2528Contrastive%2520Learning%2520for%2520Image%2520Complexity%2529.%2520We%2520find%250Athat%2520there%2520are%2520complexity%2520differences%2520between%2520different%2520local%2520regions%2520of%2520an%250Aimage%252C%2520and%2520propose%2520Random%2520Crop%2520and%2520Mix%2520%2528RCM%2529%252C%2520which%2520can%2520produce%2520positive%250Asamples%2520consisting%2520of%2520multi-scale%2520local%2520crops.%2520RCM%2520can%2520also%2520expand%2520the%2520train%250Aset%2520and%2520increase%2520data%2520diversity%2520without%2520introducing%2520additional%2520data.%2520We%2520conduct%250Aextensive%2520experiments%2520with%2520CLIC%252C%2520comparing%2520it%2520with%2520both%2520unsupervised%2520and%250Asupervised%2520methods.%2520The%2520results%2520demonstrate%2520that%2520the%2520performance%2520of%2520CLIC%2520is%250Acomparable%2520to%2520that%2520of%2520state-of-the-art%2520supervised%2520methods.%2520In%2520addition%252C%2520we%250Aestablish%2520the%2520pipelines%2520that%2520can%2520apply%2520CLIC%2520to%2520computer%2520vision%2520tasks%2520to%250Aeffectively%2520improve%2520their%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning%20for%20Image%20Complexity%20Representation&entry.906535625=Shipeng%20Liu%20and%20Liang%20Zhao%20and%20Dengfeng%20Chen%20and%20Zhanping%20Song&entry.1292438233=%20%20Quantifying%20and%20evaluating%20image%20complexity%20can%20be%20instrumental%20in%20enhancing%0Athe%20performance%20of%20various%20computer%20vision%20tasks.%20Supervised%20learning%20can%0Aeffectively%20learn%20image%20complexity%20features%20from%20well-annotated%20datasets.%0AHowever%2C%20creating%20such%20datasets%20requires%20expensive%20manual%20annotation%20costs.%20The%0Amodels%20may%20learn%20human%20subjective%20biases%20from%20it.%20In%20this%20work%2C%20we%20introduce%0Athe%20MoCo%20v2%20framework.%20We%20utilize%20contrastive%20learning%20to%20represent%20image%0Acomplexity%2C%20named%20CLIC%20%28Contrastive%20Learning%20for%20Image%20Complexity%29.%20We%20find%0Athat%20there%20are%20complexity%20differences%20between%20different%20local%20regions%20of%20an%0Aimage%2C%20and%20propose%20Random%20Crop%20and%20Mix%20%28RCM%29%2C%20which%20can%20produce%20positive%0Asamples%20consisting%20of%20multi-scale%20local%20crops.%20RCM%20can%20also%20expand%20the%20train%0Aset%20and%20increase%20data%20diversity%20without%20introducing%20additional%20data.%20We%20conduct%0Aextensive%20experiments%20with%20CLIC%2C%20comparing%20it%20with%20both%20unsupervised%20and%0Asupervised%20methods.%20The%20results%20demonstrate%20that%20the%20performance%20of%20CLIC%20is%0Acomparable%20to%20that%20of%20state-of-the-art%20supervised%20methods.%20In%20addition%2C%20we%0Aestablish%20the%20pipelines%20that%20can%20apply%20CLIC%20to%20computer%20vision%20tasks%20to%0Aeffectively%20improve%20their%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03230v1&entry.124074799=Read"},
{"title": "Getting it Right: Improving Spatial Consistency in Text-to-Image Models", "author": "Agneet Chatterjee and Gabriela Ben Melech Stan and Estelle Aflalo and Sayak Paul and Dhruba Ghosh and Tejas Gokhale and Ludwig Schmidt and Hannaneh Hajishirzi and Vasudev Lal and Chitta Baral and Yezhou Yang", "abstract": "  One of the key shortcomings in current text-to-image (T2I) models is their\ninability to consistently generate images which faithfully follow the spatial\nrelationships specified in the text prompt. In this paper, we offer a\ncomprehensive investigation of this limitation, while also developing datasets\nand methods that support algorithmic solutions to improve spatial reasoning in\nT2I models. We find that spatial relationships are under-represented in the\nimage descriptions found in current vision-language datasets. To alleviate this\ndata bottleneck, we create SPRIGHT, the first spatially focused, large-scale\ndataset, by re-captioning 6 million images from 4 widely used vision datasets\nand through a 3-fold evaluation and analysis pipeline, show that SPRIGHT\nimproves the proportion of spatial relationships in existing datasets. We show\nthe efficacy of SPRIGHT data by showing that using only $\\sim$0.25% of SPRIGHT\nresults in a 22% improvement in generating spatially accurate images while also\nimproving FID and CMMD scores. We also find that training on images containing\na larger number of objects leads to substantial improvements in spatial\nconsistency, including state-of-the-art results on T2I-CompBench with a spatial\nscore of 0.2133, by fine-tuning on <500 images. Through a set of controlled\nexperiments and ablations, we document additional findings that could support\nfuture work that seeks to understand factors that affect spatial consistency in\ntext-to-image models.\n", "link": "http://arxiv.org/abs/2404.01197v2", "date": "2024-08-06", "relevancy": 2.2405, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5786}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5664}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Getting%20it%20Right%3A%20Improving%20Spatial%20Consistency%20in%20Text-to-Image%20Models&body=Title%3A%20Getting%20it%20Right%3A%20Improving%20Spatial%20Consistency%20in%20Text-to-Image%20Models%0AAuthor%3A%20Agneet%20Chatterjee%20and%20Gabriela%20Ben%20Melech%20Stan%20and%20Estelle%20Aflalo%20and%20Sayak%20Paul%20and%20Dhruba%20Ghosh%20and%20Tejas%20Gokhale%20and%20Ludwig%20Schmidt%20and%20Hannaneh%20Hajishirzi%20and%20Vasudev%20Lal%20and%20Chitta%20Baral%20and%20Yezhou%20Yang%0AAbstract%3A%20%20%20One%20of%20the%20key%20shortcomings%20in%20current%20text-to-image%20%28T2I%29%20models%20is%20their%0Ainability%20to%20consistently%20generate%20images%20which%20faithfully%20follow%20the%20spatial%0Arelationships%20specified%20in%20the%20text%20prompt.%20In%20this%20paper%2C%20we%20offer%20a%0Acomprehensive%20investigation%20of%20this%20limitation%2C%20while%20also%20developing%20datasets%0Aand%20methods%20that%20support%20algorithmic%20solutions%20to%20improve%20spatial%20reasoning%20in%0AT2I%20models.%20We%20find%20that%20spatial%20relationships%20are%20under-represented%20in%20the%0Aimage%20descriptions%20found%20in%20current%20vision-language%20datasets.%20To%20alleviate%20this%0Adata%20bottleneck%2C%20we%20create%20SPRIGHT%2C%20the%20first%20spatially%20focused%2C%20large-scale%0Adataset%2C%20by%20re-captioning%206%20million%20images%20from%204%20widely%20used%20vision%20datasets%0Aand%20through%20a%203-fold%20evaluation%20and%20analysis%20pipeline%2C%20show%20that%20SPRIGHT%0Aimproves%20the%20proportion%20of%20spatial%20relationships%20in%20existing%20datasets.%20We%20show%0Athe%20efficacy%20of%20SPRIGHT%20data%20by%20showing%20that%20using%20only%20%24%5Csim%240.25%25%20of%20SPRIGHT%0Aresults%20in%20a%2022%25%20improvement%20in%20generating%20spatially%20accurate%20images%20while%20also%0Aimproving%20FID%20and%20CMMD%20scores.%20We%20also%20find%20that%20training%20on%20images%20containing%0Aa%20larger%20number%20of%20objects%20leads%20to%20substantial%20improvements%20in%20spatial%0Aconsistency%2C%20including%20state-of-the-art%20results%20on%20T2I-CompBench%20with%20a%20spatial%0Ascore%20of%200.2133%2C%20by%20fine-tuning%20on%20%3C500%20images.%20Through%20a%20set%20of%20controlled%0Aexperiments%20and%20ablations%2C%20we%20document%20additional%20findings%20that%20could%20support%0Afuture%20work%20that%20seeks%20to%20understand%20factors%20that%20affect%20spatial%20consistency%20in%0Atext-to-image%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01197v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGetting%2520it%2520Right%253A%2520Improving%2520Spatial%2520Consistency%2520in%2520Text-to-Image%2520Models%26entry.906535625%3DAgneet%2520Chatterjee%2520and%2520Gabriela%2520Ben%2520Melech%2520Stan%2520and%2520Estelle%2520Aflalo%2520and%2520Sayak%2520Paul%2520and%2520Dhruba%2520Ghosh%2520and%2520Tejas%2520Gokhale%2520and%2520Ludwig%2520Schmidt%2520and%2520Hannaneh%2520Hajishirzi%2520and%2520Vasudev%2520Lal%2520and%2520Chitta%2520Baral%2520and%2520Yezhou%2520Yang%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520key%2520shortcomings%2520in%2520current%2520text-to-image%2520%2528T2I%2529%2520models%2520is%2520their%250Ainability%2520to%2520consistently%2520generate%2520images%2520which%2520faithfully%2520follow%2520the%2520spatial%250Arelationships%2520specified%2520in%2520the%2520text%2520prompt.%2520In%2520this%2520paper%252C%2520we%2520offer%2520a%250Acomprehensive%2520investigation%2520of%2520this%2520limitation%252C%2520while%2520also%2520developing%2520datasets%250Aand%2520methods%2520that%2520support%2520algorithmic%2520solutions%2520to%2520improve%2520spatial%2520reasoning%2520in%250AT2I%2520models.%2520We%2520find%2520that%2520spatial%2520relationships%2520are%2520under-represented%2520in%2520the%250Aimage%2520descriptions%2520found%2520in%2520current%2520vision-language%2520datasets.%2520To%2520alleviate%2520this%250Adata%2520bottleneck%252C%2520we%2520create%2520SPRIGHT%252C%2520the%2520first%2520spatially%2520focused%252C%2520large-scale%250Adataset%252C%2520by%2520re-captioning%25206%2520million%2520images%2520from%25204%2520widely%2520used%2520vision%2520datasets%250Aand%2520through%2520a%25203-fold%2520evaluation%2520and%2520analysis%2520pipeline%252C%2520show%2520that%2520SPRIGHT%250Aimproves%2520the%2520proportion%2520of%2520spatial%2520relationships%2520in%2520existing%2520datasets.%2520We%2520show%250Athe%2520efficacy%2520of%2520SPRIGHT%2520data%2520by%2520showing%2520that%2520using%2520only%2520%2524%255Csim%25240.25%2525%2520of%2520SPRIGHT%250Aresults%2520in%2520a%252022%2525%2520improvement%2520in%2520generating%2520spatially%2520accurate%2520images%2520while%2520also%250Aimproving%2520FID%2520and%2520CMMD%2520scores.%2520We%2520also%2520find%2520that%2520training%2520on%2520images%2520containing%250Aa%2520larger%2520number%2520of%2520objects%2520leads%2520to%2520substantial%2520improvements%2520in%2520spatial%250Aconsistency%252C%2520including%2520state-of-the-art%2520results%2520on%2520T2I-CompBench%2520with%2520a%2520spatial%250Ascore%2520of%25200.2133%252C%2520by%2520fine-tuning%2520on%2520%253C500%2520images.%2520Through%2520a%2520set%2520of%2520controlled%250Aexperiments%2520and%2520ablations%252C%2520we%2520document%2520additional%2520findings%2520that%2520could%2520support%250Afuture%2520work%2520that%2520seeks%2520to%2520understand%2520factors%2520that%2520affect%2520spatial%2520consistency%2520in%250Atext-to-image%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01197v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Getting%20it%20Right%3A%20Improving%20Spatial%20Consistency%20in%20Text-to-Image%20Models&entry.906535625=Agneet%20Chatterjee%20and%20Gabriela%20Ben%20Melech%20Stan%20and%20Estelle%20Aflalo%20and%20Sayak%20Paul%20and%20Dhruba%20Ghosh%20and%20Tejas%20Gokhale%20and%20Ludwig%20Schmidt%20and%20Hannaneh%20Hajishirzi%20and%20Vasudev%20Lal%20and%20Chitta%20Baral%20and%20Yezhou%20Yang&entry.1292438233=%20%20One%20of%20the%20key%20shortcomings%20in%20current%20text-to-image%20%28T2I%29%20models%20is%20their%0Ainability%20to%20consistently%20generate%20images%20which%20faithfully%20follow%20the%20spatial%0Arelationships%20specified%20in%20the%20text%20prompt.%20In%20this%20paper%2C%20we%20offer%20a%0Acomprehensive%20investigation%20of%20this%20limitation%2C%20while%20also%20developing%20datasets%0Aand%20methods%20that%20support%20algorithmic%20solutions%20to%20improve%20spatial%20reasoning%20in%0AT2I%20models.%20We%20find%20that%20spatial%20relationships%20are%20under-represented%20in%20the%0Aimage%20descriptions%20found%20in%20current%20vision-language%20datasets.%20To%20alleviate%20this%0Adata%20bottleneck%2C%20we%20create%20SPRIGHT%2C%20the%20first%20spatially%20focused%2C%20large-scale%0Adataset%2C%20by%20re-captioning%206%20million%20images%20from%204%20widely%20used%20vision%20datasets%0Aand%20through%20a%203-fold%20evaluation%20and%20analysis%20pipeline%2C%20show%20that%20SPRIGHT%0Aimproves%20the%20proportion%20of%20spatial%20relationships%20in%20existing%20datasets.%20We%20show%0Athe%20efficacy%20of%20SPRIGHT%20data%20by%20showing%20that%20using%20only%20%24%5Csim%240.25%25%20of%20SPRIGHT%0Aresults%20in%20a%2022%25%20improvement%20in%20generating%20spatially%20accurate%20images%20while%20also%0Aimproving%20FID%20and%20CMMD%20scores.%20We%20also%20find%20that%20training%20on%20images%20containing%0Aa%20larger%20number%20of%20objects%20leads%20to%20substantial%20improvements%20in%20spatial%0Aconsistency%2C%20including%20state-of-the-art%20results%20on%20T2I-CompBench%20with%20a%20spatial%0Ascore%20of%200.2133%2C%20by%20fine-tuning%20on%20%3C500%20images.%20Through%20a%20set%20of%20controlled%0Aexperiments%20and%20ablations%2C%20we%20document%20additional%20findings%20that%20could%20support%0Afuture%20work%20that%20seeks%20to%20understand%20factors%20that%20affect%20spatial%20consistency%20in%0Atext-to-image%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01197v2&entry.124074799=Read"},
{"title": "Feature Clock: High-Dimensional Effects in Two-Dimensional Plots", "author": "Olga Ovcharenko and Rita Sevastjanova and Valentina Boeva", "abstract": "  Humans struggle to perceive and interpret high-dimensional data. Therefore,\nhigh-dimensional data are often projected into two dimensions for\nvisualization. Many applications benefit from complex nonlinear dimensionality\nreduction techniques, but the effects of individual high-dimensional features\nare hard to explain in the two-dimensional space. Most visualization solutions\nuse multiple two-dimensional plots, each showing the effect of one\nhigh-dimensional feature in two dimensions; this approach creates a need for a\nvisual inspection of k plots for a k-dimensional input space. Our solution,\nFeature Clock, provides a novel approach that eliminates the need to inspect\nthese k plots to grasp the influence of original features on the data structure\ndepicted in two dimensions. Feature Clock enhances the explainability and\ncompactness of visualizations of embedded data and is available in an\nopen-source Python library.\n", "link": "http://arxiv.org/abs/2408.01294v2", "date": "2024-08-06", "relevancy": 2.239, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4655}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4655}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Clock%3A%20High-Dimensional%20Effects%20in%20Two-Dimensional%20Plots&body=Title%3A%20Feature%20Clock%3A%20High-Dimensional%20Effects%20in%20Two-Dimensional%20Plots%0AAuthor%3A%20Olga%20Ovcharenko%20and%20Rita%20Sevastjanova%20and%20Valentina%20Boeva%0AAbstract%3A%20%20%20Humans%20struggle%20to%20perceive%20and%20interpret%20high-dimensional%20data.%20Therefore%2C%0Ahigh-dimensional%20data%20are%20often%20projected%20into%20two%20dimensions%20for%0Avisualization.%20Many%20applications%20benefit%20from%20complex%20nonlinear%20dimensionality%0Areduction%20techniques%2C%20but%20the%20effects%20of%20individual%20high-dimensional%20features%0Aare%20hard%20to%20explain%20in%20the%20two-dimensional%20space.%20Most%20visualization%20solutions%0Ause%20multiple%20two-dimensional%20plots%2C%20each%20showing%20the%20effect%20of%20one%0Ahigh-dimensional%20feature%20in%20two%20dimensions%3B%20this%20approach%20creates%20a%20need%20for%20a%0Avisual%20inspection%20of%20k%20plots%20for%20a%20k-dimensional%20input%20space.%20Our%20solution%2C%0AFeature%20Clock%2C%20provides%20a%20novel%20approach%20that%20eliminates%20the%20need%20to%20inspect%0Athese%20k%20plots%20to%20grasp%20the%20influence%20of%20original%20features%20on%20the%20data%20structure%0Adepicted%20in%20two%20dimensions.%20Feature%20Clock%20enhances%20the%20explainability%20and%0Acompactness%20of%20visualizations%20of%20embedded%20data%20and%20is%20available%20in%20an%0Aopen-source%20Python%20library.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01294v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Clock%253A%2520High-Dimensional%2520Effects%2520in%2520Two-Dimensional%2520Plots%26entry.906535625%3DOlga%2520Ovcharenko%2520and%2520Rita%2520Sevastjanova%2520and%2520Valentina%2520Boeva%26entry.1292438233%3D%2520%2520Humans%2520struggle%2520to%2520perceive%2520and%2520interpret%2520high-dimensional%2520data.%2520Therefore%252C%250Ahigh-dimensional%2520data%2520are%2520often%2520projected%2520into%2520two%2520dimensions%2520for%250Avisualization.%2520Many%2520applications%2520benefit%2520from%2520complex%2520nonlinear%2520dimensionality%250Areduction%2520techniques%252C%2520but%2520the%2520effects%2520of%2520individual%2520high-dimensional%2520features%250Aare%2520hard%2520to%2520explain%2520in%2520the%2520two-dimensional%2520space.%2520Most%2520visualization%2520solutions%250Ause%2520multiple%2520two-dimensional%2520plots%252C%2520each%2520showing%2520the%2520effect%2520of%2520one%250Ahigh-dimensional%2520feature%2520in%2520two%2520dimensions%253B%2520this%2520approach%2520creates%2520a%2520need%2520for%2520a%250Avisual%2520inspection%2520of%2520k%2520plots%2520for%2520a%2520k-dimensional%2520input%2520space.%2520Our%2520solution%252C%250AFeature%2520Clock%252C%2520provides%2520a%2520novel%2520approach%2520that%2520eliminates%2520the%2520need%2520to%2520inspect%250Athese%2520k%2520plots%2520to%2520grasp%2520the%2520influence%2520of%2520original%2520features%2520on%2520the%2520data%2520structure%250Adepicted%2520in%2520two%2520dimensions.%2520Feature%2520Clock%2520enhances%2520the%2520explainability%2520and%250Acompactness%2520of%2520visualizations%2520of%2520embedded%2520data%2520and%2520is%2520available%2520in%2520an%250Aopen-source%2520Python%2520library.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01294v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Clock%3A%20High-Dimensional%20Effects%20in%20Two-Dimensional%20Plots&entry.906535625=Olga%20Ovcharenko%20and%20Rita%20Sevastjanova%20and%20Valentina%20Boeva&entry.1292438233=%20%20Humans%20struggle%20to%20perceive%20and%20interpret%20high-dimensional%20data.%20Therefore%2C%0Ahigh-dimensional%20data%20are%20often%20projected%20into%20two%20dimensions%20for%0Avisualization.%20Many%20applications%20benefit%20from%20complex%20nonlinear%20dimensionality%0Areduction%20techniques%2C%20but%20the%20effects%20of%20individual%20high-dimensional%20features%0Aare%20hard%20to%20explain%20in%20the%20two-dimensional%20space.%20Most%20visualization%20solutions%0Ause%20multiple%20two-dimensional%20plots%2C%20each%20showing%20the%20effect%20of%20one%0Ahigh-dimensional%20feature%20in%20two%20dimensions%3B%20this%20approach%20creates%20a%20need%20for%20a%0Avisual%20inspection%20of%20k%20plots%20for%20a%20k-dimensional%20input%20space.%20Our%20solution%2C%0AFeature%20Clock%2C%20provides%20a%20novel%20approach%20that%20eliminates%20the%20need%20to%20inspect%0Athese%20k%20plots%20to%20grasp%20the%20influence%20of%20original%20features%20on%20the%20data%20structure%0Adepicted%20in%20two%20dimensions.%20Feature%20Clock%20enhances%20the%20explainability%20and%0Acompactness%20of%20visualizations%20of%20embedded%20data%20and%20is%20available%20in%20an%0Aopen-source%20Python%20library.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01294v2&entry.124074799=Read"},
{"title": "DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training\n  Quantization for Vision Transformers", "author": "Lianwei Yang and Haisong Gong", "abstract": "  Vision transformers (ViTs) have garnered significant attention for their\nperformance in vision tasks; however, the high computational cost and\nsignificant latency issues have hinder widespread adoption. Post-training\nquantization (PTQ), a promising method for model compression, still faces\naccuracy degradation challenges with ViTs. There are two reasons for this: the\nexisting quantization paradigm does not fit the power-law distribution of\npost-Softmax activations well, and accuracy inevitably decreases after\nreparameterizing post-LayerNorm activations. We propose a Distribution-Friendly\nand Outlier-Aware Post-training Quantization method for Vision Transformers,\nnamed DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and\nintroduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more\non values near 1, more accurately preserving the power-law distribution of\npost-Softmax activations, and achieves favorable results. Moreover, when\nreparameterizing post-LayerNorm activations from channel-wise to layer-wise\nquantization, the accuracy degradation is mainly due to the significant impact\nof outliers in the scaling factors. Therefore, DopQ-ViT proposes a method to\nSearch for the Optimal Scaling Factor, denoted as SOSF, which compensates for\nthe influence of outliers and preserves the performance of the quantization\nmodel. DopQ-ViT has undergone extensive validation and demonstrates significant\nperformance improvements in quantization models, particularly in low-bit\nsettings.\n", "link": "http://arxiv.org/abs/2408.03291v1", "date": "2024-08-06", "relevancy": 2.2243, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5638}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5589}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DopQ-ViT%3A%20Towards%20Distribution-Friendly%20and%20Outlier-Aware%20Post-Training%0A%20%20Quantization%20for%20Vision%20Transformers&body=Title%3A%20DopQ-ViT%3A%20Towards%20Distribution-Friendly%20and%20Outlier-Aware%20Post-Training%0A%20%20Quantization%20for%20Vision%20Transformers%0AAuthor%3A%20Lianwei%20Yang%20and%20Haisong%20Gong%0AAbstract%3A%20%20%20Vision%20transformers%20%28ViTs%29%20have%20garnered%20significant%20attention%20for%20their%0Aperformance%20in%20vision%20tasks%3B%20however%2C%20the%20high%20computational%20cost%20and%0Asignificant%20latency%20issues%20have%20hinder%20widespread%20adoption.%20Post-training%0Aquantization%20%28PTQ%29%2C%20a%20promising%20method%20for%20model%20compression%2C%20still%20faces%0Aaccuracy%20degradation%20challenges%20with%20ViTs.%20There%20are%20two%20reasons%20for%20this%3A%20the%0Aexisting%20quantization%20paradigm%20does%20not%20fit%20the%20power-law%20distribution%20of%0Apost-Softmax%20activations%20well%2C%20and%20accuracy%20inevitably%20decreases%20after%0Areparameterizing%20post-LayerNorm%20activations.%20We%20propose%20a%20Distribution-Friendly%0Aand%20Outlier-Aware%20Post-training%20Quantization%20method%20for%20Vision%20Transformers%2C%0Anamed%20DopQ-ViT.%20DopQ-ViT%20analyzes%20the%20inefficiencies%20of%20current%20quantizers%20and%0Aintroduces%20a%20distribution-friendly%20Tan%20Quantizer%20called%20TanQ.%20TanQ%20focuses%20more%0Aon%20values%20near%201%2C%20more%20accurately%20preserving%20the%20power-law%20distribution%20of%0Apost-Softmax%20activations%2C%20and%20achieves%20favorable%20results.%20Moreover%2C%20when%0Areparameterizing%20post-LayerNorm%20activations%20from%20channel-wise%20to%20layer-wise%0Aquantization%2C%20the%20accuracy%20degradation%20is%20mainly%20due%20to%20the%20significant%20impact%0Aof%20outliers%20in%20the%20scaling%20factors.%20Therefore%2C%20DopQ-ViT%20proposes%20a%20method%20to%0ASearch%20for%20the%20Optimal%20Scaling%20Factor%2C%20denoted%20as%20SOSF%2C%20which%20compensates%20for%0Athe%20influence%20of%20outliers%20and%20preserves%20the%20performance%20of%20the%20quantization%0Amodel.%20DopQ-ViT%20has%20undergone%20extensive%20validation%20and%20demonstrates%20significant%0Aperformance%20improvements%20in%20quantization%20models%2C%20particularly%20in%20low-bit%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDopQ-ViT%253A%2520Towards%2520Distribution-Friendly%2520and%2520Outlier-Aware%2520Post-Training%250A%2520%2520Quantization%2520for%2520Vision%2520Transformers%26entry.906535625%3DLianwei%2520Yang%2520and%2520Haisong%2520Gong%26entry.1292438233%3D%2520%2520Vision%2520transformers%2520%2528ViTs%2529%2520have%2520garnered%2520significant%2520attention%2520for%2520their%250Aperformance%2520in%2520vision%2520tasks%253B%2520however%252C%2520the%2520high%2520computational%2520cost%2520and%250Asignificant%2520latency%2520issues%2520have%2520hinder%2520widespread%2520adoption.%2520Post-training%250Aquantization%2520%2528PTQ%2529%252C%2520a%2520promising%2520method%2520for%2520model%2520compression%252C%2520still%2520faces%250Aaccuracy%2520degradation%2520challenges%2520with%2520ViTs.%2520There%2520are%2520two%2520reasons%2520for%2520this%253A%2520the%250Aexisting%2520quantization%2520paradigm%2520does%2520not%2520fit%2520the%2520power-law%2520distribution%2520of%250Apost-Softmax%2520activations%2520well%252C%2520and%2520accuracy%2520inevitably%2520decreases%2520after%250Areparameterizing%2520post-LayerNorm%2520activations.%2520We%2520propose%2520a%2520Distribution-Friendly%250Aand%2520Outlier-Aware%2520Post-training%2520Quantization%2520method%2520for%2520Vision%2520Transformers%252C%250Anamed%2520DopQ-ViT.%2520DopQ-ViT%2520analyzes%2520the%2520inefficiencies%2520of%2520current%2520quantizers%2520and%250Aintroduces%2520a%2520distribution-friendly%2520Tan%2520Quantizer%2520called%2520TanQ.%2520TanQ%2520focuses%2520more%250Aon%2520values%2520near%25201%252C%2520more%2520accurately%2520preserving%2520the%2520power-law%2520distribution%2520of%250Apost-Softmax%2520activations%252C%2520and%2520achieves%2520favorable%2520results.%2520Moreover%252C%2520when%250Areparameterizing%2520post-LayerNorm%2520activations%2520from%2520channel-wise%2520to%2520layer-wise%250Aquantization%252C%2520the%2520accuracy%2520degradation%2520is%2520mainly%2520due%2520to%2520the%2520significant%2520impact%250Aof%2520outliers%2520in%2520the%2520scaling%2520factors.%2520Therefore%252C%2520DopQ-ViT%2520proposes%2520a%2520method%2520to%250ASearch%2520for%2520the%2520Optimal%2520Scaling%2520Factor%252C%2520denoted%2520as%2520SOSF%252C%2520which%2520compensates%2520for%250Athe%2520influence%2520of%2520outliers%2520and%2520preserves%2520the%2520performance%2520of%2520the%2520quantization%250Amodel.%2520DopQ-ViT%2520has%2520undergone%2520extensive%2520validation%2520and%2520demonstrates%2520significant%250Aperformance%2520improvements%2520in%2520quantization%2520models%252C%2520particularly%2520in%2520low-bit%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DopQ-ViT%3A%20Towards%20Distribution-Friendly%20and%20Outlier-Aware%20Post-Training%0A%20%20Quantization%20for%20Vision%20Transformers&entry.906535625=Lianwei%20Yang%20and%20Haisong%20Gong&entry.1292438233=%20%20Vision%20transformers%20%28ViTs%29%20have%20garnered%20significant%20attention%20for%20their%0Aperformance%20in%20vision%20tasks%3B%20however%2C%20the%20high%20computational%20cost%20and%0Asignificant%20latency%20issues%20have%20hinder%20widespread%20adoption.%20Post-training%0Aquantization%20%28PTQ%29%2C%20a%20promising%20method%20for%20model%20compression%2C%20still%20faces%0Aaccuracy%20degradation%20challenges%20with%20ViTs.%20There%20are%20two%20reasons%20for%20this%3A%20the%0Aexisting%20quantization%20paradigm%20does%20not%20fit%20the%20power-law%20distribution%20of%0Apost-Softmax%20activations%20well%2C%20and%20accuracy%20inevitably%20decreases%20after%0Areparameterizing%20post-LayerNorm%20activations.%20We%20propose%20a%20Distribution-Friendly%0Aand%20Outlier-Aware%20Post-training%20Quantization%20method%20for%20Vision%20Transformers%2C%0Anamed%20DopQ-ViT.%20DopQ-ViT%20analyzes%20the%20inefficiencies%20of%20current%20quantizers%20and%0Aintroduces%20a%20distribution-friendly%20Tan%20Quantizer%20called%20TanQ.%20TanQ%20focuses%20more%0Aon%20values%20near%201%2C%20more%20accurately%20preserving%20the%20power-law%20distribution%20of%0Apost-Softmax%20activations%2C%20and%20achieves%20favorable%20results.%20Moreover%2C%20when%0Areparameterizing%20post-LayerNorm%20activations%20from%20channel-wise%20to%20layer-wise%0Aquantization%2C%20the%20accuracy%20degradation%20is%20mainly%20due%20to%20the%20significant%20impact%0Aof%20outliers%20in%20the%20scaling%20factors.%20Therefore%2C%20DopQ-ViT%20proposes%20a%20method%20to%0ASearch%20for%20the%20Optimal%20Scaling%20Factor%2C%20denoted%20as%20SOSF%2C%20which%20compensates%20for%0Athe%20influence%20of%20outliers%20and%20preserves%20the%20performance%20of%20the%20quantization%0Amodel.%20DopQ-ViT%20has%20undergone%20extensive%20validation%20and%20demonstrates%20significant%0Aperformance%20improvements%20in%20quantization%20models%2C%20particularly%20in%20low-bit%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03291v1&entry.124074799=Read"},
{"title": "Line-based 6-DoF Object Pose Estimation and Tracking With an Event\n  Camera", "author": "Zibin Liu and Banglei Guan and Yang Shang and Qifeng Yu and Laurent Kneip", "abstract": "  Pose estimation and tracking of objects is a fundamental application in 3D\nvision. Event cameras possess remarkable attributes such as high dynamic range,\nlow latency, and resilience against motion blur, which enables them to address\nchallenging high dynamic range scenes or high-speed motion. These features make\nevent cameras an ideal complement over standard cameras for object pose\nestimation. In this work, we propose a line-based robust pose estimation and\ntracking method for planar or non-planar objects using an event camera.\nFirstly, we extract object lines directly from events, then provide an initial\npose using a globally-optimal Branch-and-Bound approach, where 2D-3D line\ncorrespondences are not known in advance. Subsequently, we utilize event-line\nmatching to establish correspondences between 2D events and 3D models.\nFurthermore, object poses are refined and continuously tracked by minimizing\nevent-line distances. Events are assigned different weights based on these\ndistances, employing robust estimation algorithms. To evaluate the precision of\nthe proposed methods in object pose estimation and tracking, we have devised\nand established an event-based moving object dataset. Compared against\nstate-of-the-art methods, the robustness and accuracy of our methods have been\nvalidated both on synthetic experiments and the proposed dataset. The source\ncode is available at https://github.com/Zibin6/LOPET.\n", "link": "http://arxiv.org/abs/2408.03225v1", "date": "2024-08-06", "relevancy": 2.2131, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.556}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5556}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Line-based%206-DoF%20Object%20Pose%20Estimation%20and%20Tracking%20With%20an%20Event%0A%20%20Camera&body=Title%3A%20Line-based%206-DoF%20Object%20Pose%20Estimation%20and%20Tracking%20With%20an%20Event%0A%20%20Camera%0AAuthor%3A%20Zibin%20Liu%20and%20Banglei%20Guan%20and%20Yang%20Shang%20and%20Qifeng%20Yu%20and%20Laurent%20Kneip%0AAbstract%3A%20%20%20Pose%20estimation%20and%20tracking%20of%20objects%20is%20a%20fundamental%20application%20in%203D%0Avision.%20Event%20cameras%20possess%20remarkable%20attributes%20such%20as%20high%20dynamic%20range%2C%0Alow%20latency%2C%20and%20resilience%20against%20motion%20blur%2C%20which%20enables%20them%20to%20address%0Achallenging%20high%20dynamic%20range%20scenes%20or%20high-speed%20motion.%20These%20features%20make%0Aevent%20cameras%20an%20ideal%20complement%20over%20standard%20cameras%20for%20object%20pose%0Aestimation.%20In%20this%20work%2C%20we%20propose%20a%20line-based%20robust%20pose%20estimation%20and%0Atracking%20method%20for%20planar%20or%20non-planar%20objects%20using%20an%20event%20camera.%0AFirstly%2C%20we%20extract%20object%20lines%20directly%20from%20events%2C%20then%20provide%20an%20initial%0Apose%20using%20a%20globally-optimal%20Branch-and-Bound%20approach%2C%20where%202D-3D%20line%0Acorrespondences%20are%20not%20known%20in%20advance.%20Subsequently%2C%20we%20utilize%20event-line%0Amatching%20to%20establish%20correspondences%20between%202D%20events%20and%203D%20models.%0AFurthermore%2C%20object%20poses%20are%20refined%20and%20continuously%20tracked%20by%20minimizing%0Aevent-line%20distances.%20Events%20are%20assigned%20different%20weights%20based%20on%20these%0Adistances%2C%20employing%20robust%20estimation%20algorithms.%20To%20evaluate%20the%20precision%20of%0Athe%20proposed%20methods%20in%20object%20pose%20estimation%20and%20tracking%2C%20we%20have%20devised%0Aand%20established%20an%20event-based%20moving%20object%20dataset.%20Compared%20against%0Astate-of-the-art%20methods%2C%20the%20robustness%20and%20accuracy%20of%20our%20methods%20have%20been%0Avalidated%20both%20on%20synthetic%20experiments%20and%20the%20proposed%20dataset.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/Zibin6/LOPET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLine-based%25206-DoF%2520Object%2520Pose%2520Estimation%2520and%2520Tracking%2520With%2520an%2520Event%250A%2520%2520Camera%26entry.906535625%3DZibin%2520Liu%2520and%2520Banglei%2520Guan%2520and%2520Yang%2520Shang%2520and%2520Qifeng%2520Yu%2520and%2520Laurent%2520Kneip%26entry.1292438233%3D%2520%2520Pose%2520estimation%2520and%2520tracking%2520of%2520objects%2520is%2520a%2520fundamental%2520application%2520in%25203D%250Avision.%2520Event%2520cameras%2520possess%2520remarkable%2520attributes%2520such%2520as%2520high%2520dynamic%2520range%252C%250Alow%2520latency%252C%2520and%2520resilience%2520against%2520motion%2520blur%252C%2520which%2520enables%2520them%2520to%2520address%250Achallenging%2520high%2520dynamic%2520range%2520scenes%2520or%2520high-speed%2520motion.%2520These%2520features%2520make%250Aevent%2520cameras%2520an%2520ideal%2520complement%2520over%2520standard%2520cameras%2520for%2520object%2520pose%250Aestimation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520line-based%2520robust%2520pose%2520estimation%2520and%250Atracking%2520method%2520for%2520planar%2520or%2520non-planar%2520objects%2520using%2520an%2520event%2520camera.%250AFirstly%252C%2520we%2520extract%2520object%2520lines%2520directly%2520from%2520events%252C%2520then%2520provide%2520an%2520initial%250Apose%2520using%2520a%2520globally-optimal%2520Branch-and-Bound%2520approach%252C%2520where%25202D-3D%2520line%250Acorrespondences%2520are%2520not%2520known%2520in%2520advance.%2520Subsequently%252C%2520we%2520utilize%2520event-line%250Amatching%2520to%2520establish%2520correspondences%2520between%25202D%2520events%2520and%25203D%2520models.%250AFurthermore%252C%2520object%2520poses%2520are%2520refined%2520and%2520continuously%2520tracked%2520by%2520minimizing%250Aevent-line%2520distances.%2520Events%2520are%2520assigned%2520different%2520weights%2520based%2520on%2520these%250Adistances%252C%2520employing%2520robust%2520estimation%2520algorithms.%2520To%2520evaluate%2520the%2520precision%2520of%250Athe%2520proposed%2520methods%2520in%2520object%2520pose%2520estimation%2520and%2520tracking%252C%2520we%2520have%2520devised%250Aand%2520established%2520an%2520event-based%2520moving%2520object%2520dataset.%2520Compared%2520against%250Astate-of-the-art%2520methods%252C%2520the%2520robustness%2520and%2520accuracy%2520of%2520our%2520methods%2520have%2520been%250Avalidated%2520both%2520on%2520synthetic%2520experiments%2520and%2520the%2520proposed%2520dataset.%2520The%2520source%250Acode%2520is%2520available%2520at%2520https%253A//github.com/Zibin6/LOPET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Line-based%206-DoF%20Object%20Pose%20Estimation%20and%20Tracking%20With%20an%20Event%0A%20%20Camera&entry.906535625=Zibin%20Liu%20and%20Banglei%20Guan%20and%20Yang%20Shang%20and%20Qifeng%20Yu%20and%20Laurent%20Kneip&entry.1292438233=%20%20Pose%20estimation%20and%20tracking%20of%20objects%20is%20a%20fundamental%20application%20in%203D%0Avision.%20Event%20cameras%20possess%20remarkable%20attributes%20such%20as%20high%20dynamic%20range%2C%0Alow%20latency%2C%20and%20resilience%20against%20motion%20blur%2C%20which%20enables%20them%20to%20address%0Achallenging%20high%20dynamic%20range%20scenes%20or%20high-speed%20motion.%20These%20features%20make%0Aevent%20cameras%20an%20ideal%20complement%20over%20standard%20cameras%20for%20object%20pose%0Aestimation.%20In%20this%20work%2C%20we%20propose%20a%20line-based%20robust%20pose%20estimation%20and%0Atracking%20method%20for%20planar%20or%20non-planar%20objects%20using%20an%20event%20camera.%0AFirstly%2C%20we%20extract%20object%20lines%20directly%20from%20events%2C%20then%20provide%20an%20initial%0Apose%20using%20a%20globally-optimal%20Branch-and-Bound%20approach%2C%20where%202D-3D%20line%0Acorrespondences%20are%20not%20known%20in%20advance.%20Subsequently%2C%20we%20utilize%20event-line%0Amatching%20to%20establish%20correspondences%20between%202D%20events%20and%203D%20models.%0AFurthermore%2C%20object%20poses%20are%20refined%20and%20continuously%20tracked%20by%20minimizing%0Aevent-line%20distances.%20Events%20are%20assigned%20different%20weights%20based%20on%20these%0Adistances%2C%20employing%20robust%20estimation%20algorithms.%20To%20evaluate%20the%20precision%20of%0Athe%20proposed%20methods%20in%20object%20pose%20estimation%20and%20tracking%2C%20we%20have%20devised%0Aand%20established%20an%20event-based%20moving%20object%20dataset.%20Compared%20against%0Astate-of-the-art%20methods%2C%20the%20robustness%20and%20accuracy%20of%20our%20methods%20have%20been%0Avalidated%20both%20on%20synthetic%20experiments%20and%20the%20proposed%20dataset.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/Zibin6/LOPET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03225v1&entry.124074799=Read"},
{"title": "Efficient NeRF Optimization -- Not All Samples Remain Equally Hard", "author": "Juuso Korhonen and Goutham Rangu and Hamed R. Tavakoli and Juho Kannala", "abstract": "  We propose an application of online hard sample mining for efficient training\nof Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality\nfor many 3D reconstruction and rendering tasks but require substantial\ncomputational resources. The encoding of the scene information within the NeRF\nnetwork parameters necessitates stochastic sampling. We observe that during the\ntraining, a major part of the compute time and memory usage is spent on\nprocessing already learnt samples, which no longer affect the model update\nsignificantly. We identify the backward pass on the stochastic samples as the\ncomputational bottleneck during the optimization. We thus perform the first\nforward pass in inference mode as a relatively low-cost search for hard\nsamples. This is followed by building the computational graph and updating the\nNeRF network parameters using only the hard samples. To demonstrate the\neffectiveness of the proposed approach, we apply our method to Instant-NGP,\nresulting in significant improvements of the view-synthesis quality over the\nbaseline (1 dB improvement on average per training time, or 2x speedup to reach\nthe same PSNR level) along with approx. 40% memory savings coming from using\nonly the hard samples to build the computational graph. As our method only\ninterfaces with the network module, we expect it to be widely applicable.\n", "link": "http://arxiv.org/abs/2408.03193v1", "date": "2024-08-06", "relevancy": 2.1815, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5549}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5481}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20NeRF%20Optimization%20--%20Not%20All%20Samples%20Remain%20Equally%20Hard&body=Title%3A%20Efficient%20NeRF%20Optimization%20--%20Not%20All%20Samples%20Remain%20Equally%20Hard%0AAuthor%3A%20Juuso%20Korhonen%20and%20Goutham%20Rangu%20and%20Hamed%20R.%20Tavakoli%20and%20Juho%20Kannala%0AAbstract%3A%20%20%20We%20propose%20an%20application%20of%20online%20hard%20sample%20mining%20for%20efficient%20training%0Aof%20Neural%20Radiance%20Fields%20%28NeRF%29.%20NeRF%20models%20produce%20state-of-the-art%20quality%0Afor%20many%203D%20reconstruction%20and%20rendering%20tasks%20but%20require%20substantial%0Acomputational%20resources.%20The%20encoding%20of%20the%20scene%20information%20within%20the%20NeRF%0Anetwork%20parameters%20necessitates%20stochastic%20sampling.%20We%20observe%20that%20during%20the%0Atraining%2C%20a%20major%20part%20of%20the%20compute%20time%20and%20memory%20usage%20is%20spent%20on%0Aprocessing%20already%20learnt%20samples%2C%20which%20no%20longer%20affect%20the%20model%20update%0Asignificantly.%20We%20identify%20the%20backward%20pass%20on%20the%20stochastic%20samples%20as%20the%0Acomputational%20bottleneck%20during%20the%20optimization.%20We%20thus%20perform%20the%20first%0Aforward%20pass%20in%20inference%20mode%20as%20a%20relatively%20low-cost%20search%20for%20hard%0Asamples.%20This%20is%20followed%20by%20building%20the%20computational%20graph%20and%20updating%20the%0ANeRF%20network%20parameters%20using%20only%20the%20hard%20samples.%20To%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20approach%2C%20we%20apply%20our%20method%20to%20Instant-NGP%2C%0Aresulting%20in%20significant%20improvements%20of%20the%20view-synthesis%20quality%20over%20the%0Abaseline%20%281%20dB%20improvement%20on%20average%20per%20training%20time%2C%20or%202x%20speedup%20to%20reach%0Athe%20same%20PSNR%20level%29%20along%20with%20approx.%2040%25%20memory%20savings%20coming%20from%20using%0Aonly%20the%20hard%20samples%20to%20build%20the%20computational%20graph.%20As%20our%20method%20only%0Ainterfaces%20with%20the%20network%20module%2C%20we%20expect%20it%20to%20be%20widely%20applicable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520NeRF%2520Optimization%2520--%2520Not%2520All%2520Samples%2520Remain%2520Equally%2520Hard%26entry.906535625%3DJuuso%2520Korhonen%2520and%2520Goutham%2520Rangu%2520and%2520Hamed%2520R.%2520Tavakoli%2520and%2520Juho%2520Kannala%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520application%2520of%2520online%2520hard%2520sample%2520mining%2520for%2520efficient%2520training%250Aof%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529.%2520NeRF%2520models%2520produce%2520state-of-the-art%2520quality%250Afor%2520many%25203D%2520reconstruction%2520and%2520rendering%2520tasks%2520but%2520require%2520substantial%250Acomputational%2520resources.%2520The%2520encoding%2520of%2520the%2520scene%2520information%2520within%2520the%2520NeRF%250Anetwork%2520parameters%2520necessitates%2520stochastic%2520sampling.%2520We%2520observe%2520that%2520during%2520the%250Atraining%252C%2520a%2520major%2520part%2520of%2520the%2520compute%2520time%2520and%2520memory%2520usage%2520is%2520spent%2520on%250Aprocessing%2520already%2520learnt%2520samples%252C%2520which%2520no%2520longer%2520affect%2520the%2520model%2520update%250Asignificantly.%2520We%2520identify%2520the%2520backward%2520pass%2520on%2520the%2520stochastic%2520samples%2520as%2520the%250Acomputational%2520bottleneck%2520during%2520the%2520optimization.%2520We%2520thus%2520perform%2520the%2520first%250Aforward%2520pass%2520in%2520inference%2520mode%2520as%2520a%2520relatively%2520low-cost%2520search%2520for%2520hard%250Asamples.%2520This%2520is%2520followed%2520by%2520building%2520the%2520computational%2520graph%2520and%2520updating%2520the%250ANeRF%2520network%2520parameters%2520using%2520only%2520the%2520hard%2520samples.%2520To%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520approach%252C%2520we%2520apply%2520our%2520method%2520to%2520Instant-NGP%252C%250Aresulting%2520in%2520significant%2520improvements%2520of%2520the%2520view-synthesis%2520quality%2520over%2520the%250Abaseline%2520%25281%2520dB%2520improvement%2520on%2520average%2520per%2520training%2520time%252C%2520or%25202x%2520speedup%2520to%2520reach%250Athe%2520same%2520PSNR%2520level%2529%2520along%2520with%2520approx.%252040%2525%2520memory%2520savings%2520coming%2520from%2520using%250Aonly%2520the%2520hard%2520samples%2520to%2520build%2520the%2520computational%2520graph.%2520As%2520our%2520method%2520only%250Ainterfaces%2520with%2520the%2520network%2520module%252C%2520we%2520expect%2520it%2520to%2520be%2520widely%2520applicable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20NeRF%20Optimization%20--%20Not%20All%20Samples%20Remain%20Equally%20Hard&entry.906535625=Juuso%20Korhonen%20and%20Goutham%20Rangu%20and%20Hamed%20R.%20Tavakoli%20and%20Juho%20Kannala&entry.1292438233=%20%20We%20propose%20an%20application%20of%20online%20hard%20sample%20mining%20for%20efficient%20training%0Aof%20Neural%20Radiance%20Fields%20%28NeRF%29.%20NeRF%20models%20produce%20state-of-the-art%20quality%0Afor%20many%203D%20reconstruction%20and%20rendering%20tasks%20but%20require%20substantial%0Acomputational%20resources.%20The%20encoding%20of%20the%20scene%20information%20within%20the%20NeRF%0Anetwork%20parameters%20necessitates%20stochastic%20sampling.%20We%20observe%20that%20during%20the%0Atraining%2C%20a%20major%20part%20of%20the%20compute%20time%20and%20memory%20usage%20is%20spent%20on%0Aprocessing%20already%20learnt%20samples%2C%20which%20no%20longer%20affect%20the%20model%20update%0Asignificantly.%20We%20identify%20the%20backward%20pass%20on%20the%20stochastic%20samples%20as%20the%0Acomputational%20bottleneck%20during%20the%20optimization.%20We%20thus%20perform%20the%20first%0Aforward%20pass%20in%20inference%20mode%20as%20a%20relatively%20low-cost%20search%20for%20hard%0Asamples.%20This%20is%20followed%20by%20building%20the%20computational%20graph%20and%20updating%20the%0ANeRF%20network%20parameters%20using%20only%20the%20hard%20samples.%20To%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20approach%2C%20we%20apply%20our%20method%20to%20Instant-NGP%2C%0Aresulting%20in%20significant%20improvements%20of%20the%20view-synthesis%20quality%20over%20the%0Abaseline%20%281%20dB%20improvement%20on%20average%20per%20training%20time%2C%20or%202x%20speedup%20to%20reach%0Athe%20same%20PSNR%20level%29%20along%20with%20approx.%2040%25%20memory%20savings%20coming%20from%20using%0Aonly%20the%20hard%20samples%20to%20build%20the%20computational%20graph.%20As%20our%20method%20only%0Ainterfaces%20with%20the%20network%20module%2C%20we%20expect%20it%20to%20be%20widely%20applicable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03193v1&entry.124074799=Read"},
{"title": "TextIM: Part-aware Interactive Motion Synthesis from Text", "author": "Siyuan Fan and Bo Du and Xiantao Cai and Bo Peng and Longling Sun", "abstract": "  In this work, we propose TextIM, a novel framework for synthesizing\nTEXT-driven human Interactive Motions, with a focus on the precise alignment of\npart-level semantics. Existing methods often overlook the critical roles of\ninteractive body parts and fail to adequately capture and align part-level\nsemantics, resulting in inaccuracies and even erroneous movement outcomes. To\naddress these issues, TextIM utilizes a decoupled conditional diffusion\nframework to enhance the detailed alignment between interactive movements and\ncorresponding semantic intents from textual descriptions. Our approach\nleverages large language models, functioning as a human brain, to identify\ninteracting human body parts and to comprehend interaction semantics to\ngenerate complicated and subtle interactive motion. Guided by the refined\nmovements of the interacting parts, TextIM further extends these movements into\na coherent whole-body motion. We design a spatial coherence module to\ncomplement the entire body movements while maintaining consistency and harmony\nacross body parts using a part graph convolutional network. For training and\nevaluation, we carefully selected and re-labeled interactive motions from\nHUMANML3D to develop a specialized dataset. Experimental results demonstrate\nthat TextIM produces semantically accurate human interactive motions,\nsignificantly enhancing the realism and applicability of synthesized\ninteractive motions in diverse scenarios, even including interactions with\ndeformable and dynamically changing objects.\n", "link": "http://arxiv.org/abs/2408.03302v1", "date": "2024-08-06", "relevancy": 2.1762, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5871}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5362}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextIM%3A%20Part-aware%20Interactive%20Motion%20Synthesis%20from%20Text&body=Title%3A%20TextIM%3A%20Part-aware%20Interactive%20Motion%20Synthesis%20from%20Text%0AAuthor%3A%20Siyuan%20Fan%20and%20Bo%20Du%20and%20Xiantao%20Cai%20and%20Bo%20Peng%20and%20Longling%20Sun%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20TextIM%2C%20a%20novel%20framework%20for%20synthesizing%0ATEXT-driven%20human%20Interactive%20Motions%2C%20with%20a%20focus%20on%20the%20precise%20alignment%20of%0Apart-level%20semantics.%20Existing%20methods%20often%20overlook%20the%20critical%20roles%20of%0Ainteractive%20body%20parts%20and%20fail%20to%20adequately%20capture%20and%20align%20part-level%0Asemantics%2C%20resulting%20in%20inaccuracies%20and%20even%20erroneous%20movement%20outcomes.%20To%0Aaddress%20these%20issues%2C%20TextIM%20utilizes%20a%20decoupled%20conditional%20diffusion%0Aframework%20to%20enhance%20the%20detailed%20alignment%20between%20interactive%20movements%20and%0Acorresponding%20semantic%20intents%20from%20textual%20descriptions.%20Our%20approach%0Aleverages%20large%20language%20models%2C%20functioning%20as%20a%20human%20brain%2C%20to%20identify%0Ainteracting%20human%20body%20parts%20and%20to%20comprehend%20interaction%20semantics%20to%0Agenerate%20complicated%20and%20subtle%20interactive%20motion.%20Guided%20by%20the%20refined%0Amovements%20of%20the%20interacting%20parts%2C%20TextIM%20further%20extends%20these%20movements%20into%0Aa%20coherent%20whole-body%20motion.%20We%20design%20a%20spatial%20coherence%20module%20to%0Acomplement%20the%20entire%20body%20movements%20while%20maintaining%20consistency%20and%20harmony%0Aacross%20body%20parts%20using%20a%20part%20graph%20convolutional%20network.%20For%20training%20and%0Aevaluation%2C%20we%20carefully%20selected%20and%20re-labeled%20interactive%20motions%20from%0AHUMANML3D%20to%20develop%20a%20specialized%20dataset.%20Experimental%20results%20demonstrate%0Athat%20TextIM%20produces%20semantically%20accurate%20human%20interactive%20motions%2C%0Asignificantly%20enhancing%20the%20realism%20and%20applicability%20of%20synthesized%0Ainteractive%20motions%20in%20diverse%20scenarios%2C%20even%20including%20interactions%20with%0Adeformable%20and%20dynamically%20changing%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextIM%253A%2520Part-aware%2520Interactive%2520Motion%2520Synthesis%2520from%2520Text%26entry.906535625%3DSiyuan%2520Fan%2520and%2520Bo%2520Du%2520and%2520Xiantao%2520Cai%2520and%2520Bo%2520Peng%2520and%2520Longling%2520Sun%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520TextIM%252C%2520a%2520novel%2520framework%2520for%2520synthesizing%250ATEXT-driven%2520human%2520Interactive%2520Motions%252C%2520with%2520a%2520focus%2520on%2520the%2520precise%2520alignment%2520of%250Apart-level%2520semantics.%2520Existing%2520methods%2520often%2520overlook%2520the%2520critical%2520roles%2520of%250Ainteractive%2520body%2520parts%2520and%2520fail%2520to%2520adequately%2520capture%2520and%2520align%2520part-level%250Asemantics%252C%2520resulting%2520in%2520inaccuracies%2520and%2520even%2520erroneous%2520movement%2520outcomes.%2520To%250Aaddress%2520these%2520issues%252C%2520TextIM%2520utilizes%2520a%2520decoupled%2520conditional%2520diffusion%250Aframework%2520to%2520enhance%2520the%2520detailed%2520alignment%2520between%2520interactive%2520movements%2520and%250Acorresponding%2520semantic%2520intents%2520from%2520textual%2520descriptions.%2520Our%2520approach%250Aleverages%2520large%2520language%2520models%252C%2520functioning%2520as%2520a%2520human%2520brain%252C%2520to%2520identify%250Ainteracting%2520human%2520body%2520parts%2520and%2520to%2520comprehend%2520interaction%2520semantics%2520to%250Agenerate%2520complicated%2520and%2520subtle%2520interactive%2520motion.%2520Guided%2520by%2520the%2520refined%250Amovements%2520of%2520the%2520interacting%2520parts%252C%2520TextIM%2520further%2520extends%2520these%2520movements%2520into%250Aa%2520coherent%2520whole-body%2520motion.%2520We%2520design%2520a%2520spatial%2520coherence%2520module%2520to%250Acomplement%2520the%2520entire%2520body%2520movements%2520while%2520maintaining%2520consistency%2520and%2520harmony%250Aacross%2520body%2520parts%2520using%2520a%2520part%2520graph%2520convolutional%2520network.%2520For%2520training%2520and%250Aevaluation%252C%2520we%2520carefully%2520selected%2520and%2520re-labeled%2520interactive%2520motions%2520from%250AHUMANML3D%2520to%2520develop%2520a%2520specialized%2520dataset.%2520Experimental%2520results%2520demonstrate%250Athat%2520TextIM%2520produces%2520semantically%2520accurate%2520human%2520interactive%2520motions%252C%250Asignificantly%2520enhancing%2520the%2520realism%2520and%2520applicability%2520of%2520synthesized%250Ainteractive%2520motions%2520in%2520diverse%2520scenarios%252C%2520even%2520including%2520interactions%2520with%250Adeformable%2520and%2520dynamically%2520changing%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextIM%3A%20Part-aware%20Interactive%20Motion%20Synthesis%20from%20Text&entry.906535625=Siyuan%20Fan%20and%20Bo%20Du%20and%20Xiantao%20Cai%20and%20Bo%20Peng%20and%20Longling%20Sun&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20TextIM%2C%20a%20novel%20framework%20for%20synthesizing%0ATEXT-driven%20human%20Interactive%20Motions%2C%20with%20a%20focus%20on%20the%20precise%20alignment%20of%0Apart-level%20semantics.%20Existing%20methods%20often%20overlook%20the%20critical%20roles%20of%0Ainteractive%20body%20parts%20and%20fail%20to%20adequately%20capture%20and%20align%20part-level%0Asemantics%2C%20resulting%20in%20inaccuracies%20and%20even%20erroneous%20movement%20outcomes.%20To%0Aaddress%20these%20issues%2C%20TextIM%20utilizes%20a%20decoupled%20conditional%20diffusion%0Aframework%20to%20enhance%20the%20detailed%20alignment%20between%20interactive%20movements%20and%0Acorresponding%20semantic%20intents%20from%20textual%20descriptions.%20Our%20approach%0Aleverages%20large%20language%20models%2C%20functioning%20as%20a%20human%20brain%2C%20to%20identify%0Ainteracting%20human%20body%20parts%20and%20to%20comprehend%20interaction%20semantics%20to%0Agenerate%20complicated%20and%20subtle%20interactive%20motion.%20Guided%20by%20the%20refined%0Amovements%20of%20the%20interacting%20parts%2C%20TextIM%20further%20extends%20these%20movements%20into%0Aa%20coherent%20whole-body%20motion.%20We%20design%20a%20spatial%20coherence%20module%20to%0Acomplement%20the%20entire%20body%20movements%20while%20maintaining%20consistency%20and%20harmony%0Aacross%20body%20parts%20using%20a%20part%20graph%20convolutional%20network.%20For%20training%20and%0Aevaluation%2C%20we%20carefully%20selected%20and%20re-labeled%20interactive%20motions%20from%0AHUMANML3D%20to%20develop%20a%20specialized%20dataset.%20Experimental%20results%20demonstrate%0Athat%20TextIM%20produces%20semantically%20accurate%20human%20interactive%20motions%2C%0Asignificantly%20enhancing%20the%20realism%20and%20applicability%20of%20synthesized%0Ainteractive%20motions%20in%20diverse%20scenarios%2C%20even%20including%20interactions%20with%0Adeformable%20and%20dynamically%20changing%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03302v1&entry.124074799=Read"},
{"title": "Don't Think It Twice: Exploit Shift Invariance for Efficient Online\n  Streaming Inference of CNNs", "author": "Christodoulos Kechris and Jonathan Dan and Jose Miranda and David Atienza", "abstract": "  Deep learning time-series processing often relies on convolutional neural\nnetworks with overlapping windows. This overlap allows the network to produce\nan output faster than the window length. However, it introduces additional\ncomputations. This work explores the potential to optimize computational\nefficiency during inference by exploiting convolution's shift-invariance\nproperties to skip the calculation of layer activations between successive\noverlapping windows. Although convolutions are shift-invariant, zero-padding\nand pooling operations, widely used in such networks, are not efficient and\ncomplicate efficient streaming inference. We introduce StreamiNNC, a strategy\nto deploy Convolutional Neural Networks for online streaming inference. We\nexplore the adverse effects of zero padding and pooling on the accuracy of\nstreaming inference, deriving theoretical error upper bounds for pooling during\nstreaming. We address these limitations by proposing signal padding and pooling\nalignment and provide guidelines for designing and deploying models for\nStreamiNNC. We validate our method in simulated data and on three real-world\nbiomedical signal processing applications. StreamiNNC achieves a low deviation\nbetween streaming output and normal inference for all three networks (2.03 -\n3.55% NRMSE). This work demonstrates that it is possible to linearly speed up\nthe inference of streaming CNNs processing overlapping windows, negating the\nadditional computation typically incurred by overlapping windows.\n", "link": "http://arxiv.org/abs/2408.03223v1", "date": "2024-08-06", "relevancy": 2.1559, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5487}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5427}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Think%20It%20Twice%3A%20Exploit%20Shift%20Invariance%20for%20Efficient%20Online%0A%20%20Streaming%20Inference%20of%20CNNs&body=Title%3A%20Don%27t%20Think%20It%20Twice%3A%20Exploit%20Shift%20Invariance%20for%20Efficient%20Online%0A%20%20Streaming%20Inference%20of%20CNNs%0AAuthor%3A%20Christodoulos%20Kechris%20and%20Jonathan%20Dan%20and%20Jose%20Miranda%20and%20David%20Atienza%0AAbstract%3A%20%20%20Deep%20learning%20time-series%20processing%20often%20relies%20on%20convolutional%20neural%0Anetworks%20with%20overlapping%20windows.%20This%20overlap%20allows%20the%20network%20to%20produce%0Aan%20output%20faster%20than%20the%20window%20length.%20However%2C%20it%20introduces%20additional%0Acomputations.%20This%20work%20explores%20the%20potential%20to%20optimize%20computational%0Aefficiency%20during%20inference%20by%20exploiting%20convolution%27s%20shift-invariance%0Aproperties%20to%20skip%20the%20calculation%20of%20layer%20activations%20between%20successive%0Aoverlapping%20windows.%20Although%20convolutions%20are%20shift-invariant%2C%20zero-padding%0Aand%20pooling%20operations%2C%20widely%20used%20in%20such%20networks%2C%20are%20not%20efficient%20and%0Acomplicate%20efficient%20streaming%20inference.%20We%20introduce%20StreamiNNC%2C%20a%20strategy%0Ato%20deploy%20Convolutional%20Neural%20Networks%20for%20online%20streaming%20inference.%20We%0Aexplore%20the%20adverse%20effects%20of%20zero%20padding%20and%20pooling%20on%20the%20accuracy%20of%0Astreaming%20inference%2C%20deriving%20theoretical%20error%20upper%20bounds%20for%20pooling%20during%0Astreaming.%20We%20address%20these%20limitations%20by%20proposing%20signal%20padding%20and%20pooling%0Aalignment%20and%20provide%20guidelines%20for%20designing%20and%20deploying%20models%20for%0AStreamiNNC.%20We%20validate%20our%20method%20in%20simulated%20data%20and%20on%20three%20real-world%0Abiomedical%20signal%20processing%20applications.%20StreamiNNC%20achieves%20a%20low%20deviation%0Abetween%20streaming%20output%20and%20normal%20inference%20for%20all%20three%20networks%20%282.03%20-%0A3.55%25%20NRMSE%29.%20This%20work%20demonstrates%20that%20it%20is%20possible%20to%20linearly%20speed%20up%0Athe%20inference%20of%20streaming%20CNNs%20processing%20overlapping%20windows%2C%20negating%20the%0Aadditional%20computation%20typically%20incurred%20by%20overlapping%20windows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Think%2520It%2520Twice%253A%2520Exploit%2520Shift%2520Invariance%2520for%2520Efficient%2520Online%250A%2520%2520Streaming%2520Inference%2520of%2520CNNs%26entry.906535625%3DChristodoulos%2520Kechris%2520and%2520Jonathan%2520Dan%2520and%2520Jose%2520Miranda%2520and%2520David%2520Atienza%26entry.1292438233%3D%2520%2520Deep%2520learning%2520time-series%2520processing%2520often%2520relies%2520on%2520convolutional%2520neural%250Anetworks%2520with%2520overlapping%2520windows.%2520This%2520overlap%2520allows%2520the%2520network%2520to%2520produce%250Aan%2520output%2520faster%2520than%2520the%2520window%2520length.%2520However%252C%2520it%2520introduces%2520additional%250Acomputations.%2520This%2520work%2520explores%2520the%2520potential%2520to%2520optimize%2520computational%250Aefficiency%2520during%2520inference%2520by%2520exploiting%2520convolution%2527s%2520shift-invariance%250Aproperties%2520to%2520skip%2520the%2520calculation%2520of%2520layer%2520activations%2520between%2520successive%250Aoverlapping%2520windows.%2520Although%2520convolutions%2520are%2520shift-invariant%252C%2520zero-padding%250Aand%2520pooling%2520operations%252C%2520widely%2520used%2520in%2520such%2520networks%252C%2520are%2520not%2520efficient%2520and%250Acomplicate%2520efficient%2520streaming%2520inference.%2520We%2520introduce%2520StreamiNNC%252C%2520a%2520strategy%250Ato%2520deploy%2520Convolutional%2520Neural%2520Networks%2520for%2520online%2520streaming%2520inference.%2520We%250Aexplore%2520the%2520adverse%2520effects%2520of%2520zero%2520padding%2520and%2520pooling%2520on%2520the%2520accuracy%2520of%250Astreaming%2520inference%252C%2520deriving%2520theoretical%2520error%2520upper%2520bounds%2520for%2520pooling%2520during%250Astreaming.%2520We%2520address%2520these%2520limitations%2520by%2520proposing%2520signal%2520padding%2520and%2520pooling%250Aalignment%2520and%2520provide%2520guidelines%2520for%2520designing%2520and%2520deploying%2520models%2520for%250AStreamiNNC.%2520We%2520validate%2520our%2520method%2520in%2520simulated%2520data%2520and%2520on%2520three%2520real-world%250Abiomedical%2520signal%2520processing%2520applications.%2520StreamiNNC%2520achieves%2520a%2520low%2520deviation%250Abetween%2520streaming%2520output%2520and%2520normal%2520inference%2520for%2520all%2520three%2520networks%2520%25282.03%2520-%250A3.55%2525%2520NRMSE%2529.%2520This%2520work%2520demonstrates%2520that%2520it%2520is%2520possible%2520to%2520linearly%2520speed%2520up%250Athe%2520inference%2520of%2520streaming%2520CNNs%2520processing%2520overlapping%2520windows%252C%2520negating%2520the%250Aadditional%2520computation%2520typically%2520incurred%2520by%2520overlapping%2520windows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Think%20It%20Twice%3A%20Exploit%20Shift%20Invariance%20for%20Efficient%20Online%0A%20%20Streaming%20Inference%20of%20CNNs&entry.906535625=Christodoulos%20Kechris%20and%20Jonathan%20Dan%20and%20Jose%20Miranda%20and%20David%20Atienza&entry.1292438233=%20%20Deep%20learning%20time-series%20processing%20often%20relies%20on%20convolutional%20neural%0Anetworks%20with%20overlapping%20windows.%20This%20overlap%20allows%20the%20network%20to%20produce%0Aan%20output%20faster%20than%20the%20window%20length.%20However%2C%20it%20introduces%20additional%0Acomputations.%20This%20work%20explores%20the%20potential%20to%20optimize%20computational%0Aefficiency%20during%20inference%20by%20exploiting%20convolution%27s%20shift-invariance%0Aproperties%20to%20skip%20the%20calculation%20of%20layer%20activations%20between%20successive%0Aoverlapping%20windows.%20Although%20convolutions%20are%20shift-invariant%2C%20zero-padding%0Aand%20pooling%20operations%2C%20widely%20used%20in%20such%20networks%2C%20are%20not%20efficient%20and%0Acomplicate%20efficient%20streaming%20inference.%20We%20introduce%20StreamiNNC%2C%20a%20strategy%0Ato%20deploy%20Convolutional%20Neural%20Networks%20for%20online%20streaming%20inference.%20We%0Aexplore%20the%20adverse%20effects%20of%20zero%20padding%20and%20pooling%20on%20the%20accuracy%20of%0Astreaming%20inference%2C%20deriving%20theoretical%20error%20upper%20bounds%20for%20pooling%20during%0Astreaming.%20We%20address%20these%20limitations%20by%20proposing%20signal%20padding%20and%20pooling%0Aalignment%20and%20provide%20guidelines%20for%20designing%20and%20deploying%20models%20for%0AStreamiNNC.%20We%20validate%20our%20method%20in%20simulated%20data%20and%20on%20three%20real-world%0Abiomedical%20signal%20processing%20applications.%20StreamiNNC%20achieves%20a%20low%20deviation%0Abetween%20streaming%20output%20and%20normal%20inference%20for%20all%20three%20networks%20%282.03%20-%0A3.55%25%20NRMSE%29.%20This%20work%20demonstrates%20that%20it%20is%20possible%20to%20linearly%20speed%20up%0Athe%20inference%20of%20streaming%20CNNs%20processing%20overlapping%20windows%2C%20negating%20the%0Aadditional%20computation%20typically%20incurred%20by%20overlapping%20windows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03223v1&entry.124074799=Read"},
{"title": "VCHAR:Variance-Driven Complex Human Activity Recognition framework with\n  Generative Representation", "author": "Yuan Sun and Navid Salami Pargoo and Taqiya Ehsan and Zhao Zhang and Jorge Ortiz", "abstract": "  Complex human activity recognition (CHAR) remains a pivotal challenge within\nubiquitous computing, especially in the context of smart environments. Existing\nstudies typically require meticulous labeling of both atomic and complex\nactivities, a task that is labor-intensive and prone to errors due to the\nscarcity and inaccuracies of available datasets. Most prior research has\nfocused on datasets that either precisely label atomic activities or, at\nminimum, their sequence approaches that are often impractical in real world\nsettings.In response, we introduce VCHAR (Variance-Driven Complex Human\nActivity Recognition), a novel framework that treats the outputs of atomic\nactivities as a distribution over specified intervals. Leveraging generative\nmethodologies, VCHAR elucidates the reasoning behind complex activity\nclassifications through video-based explanations, accessible to users without\nprior machine learning expertise. Our evaluation across three publicly\navailable datasets demonstrates that VCHAR enhances the accuracy of complex\nactivity recognition without necessitating precise temporal or sequential\nlabeling of atomic activities. Furthermore, user studies confirm that VCHAR's\nexplanations are more intelligible compared to existing methods, facilitating a\nbroader understanding of complex activity recognition among non-experts.\n", "link": "http://arxiv.org/abs/2407.03291v2", "date": "2024-08-06", "relevancy": 2.1194, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.53}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5299}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCHAR%3AVariance-Driven%20Complex%20Human%20Activity%20Recognition%20framework%20with%0A%20%20Generative%20Representation&body=Title%3A%20VCHAR%3AVariance-Driven%20Complex%20Human%20Activity%20Recognition%20framework%20with%0A%20%20Generative%20Representation%0AAuthor%3A%20Yuan%20Sun%20and%20Navid%20Salami%20Pargoo%20and%20Taqiya%20Ehsan%20and%20Zhao%20Zhang%20and%20Jorge%20Ortiz%0AAbstract%3A%20%20%20Complex%20human%20activity%20recognition%20%28CHAR%29%20remains%20a%20pivotal%20challenge%20within%0Aubiquitous%20computing%2C%20especially%20in%20the%20context%20of%20smart%20environments.%20Existing%0Astudies%20typically%20require%20meticulous%20labeling%20of%20both%20atomic%20and%20complex%0Aactivities%2C%20a%20task%20that%20is%20labor-intensive%20and%20prone%20to%20errors%20due%20to%20the%0Ascarcity%20and%20inaccuracies%20of%20available%20datasets.%20Most%20prior%20research%20has%0Afocused%20on%20datasets%20that%20either%20precisely%20label%20atomic%20activities%20or%2C%20at%0Aminimum%2C%20their%20sequence%20approaches%20that%20are%20often%20impractical%20in%20real%20world%0Asettings.In%20response%2C%20we%20introduce%20VCHAR%20%28Variance-Driven%20Complex%20Human%0AActivity%20Recognition%29%2C%20a%20novel%20framework%20that%20treats%20the%20outputs%20of%20atomic%0Aactivities%20as%20a%20distribution%20over%20specified%20intervals.%20Leveraging%20generative%0Amethodologies%2C%20VCHAR%20elucidates%20the%20reasoning%20behind%20complex%20activity%0Aclassifications%20through%20video-based%20explanations%2C%20accessible%20to%20users%20without%0Aprior%20machine%20learning%20expertise.%20Our%20evaluation%20across%20three%20publicly%0Aavailable%20datasets%20demonstrates%20that%20VCHAR%20enhances%20the%20accuracy%20of%20complex%0Aactivity%20recognition%20without%20necessitating%20precise%20temporal%20or%20sequential%0Alabeling%20of%20atomic%20activities.%20Furthermore%2C%20user%20studies%20confirm%20that%20VCHAR%27s%0Aexplanations%20are%20more%20intelligible%20compared%20to%20existing%20methods%2C%20facilitating%20a%0Abroader%20understanding%20of%20complex%20activity%20recognition%20among%20non-experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03291v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCHAR%253AVariance-Driven%2520Complex%2520Human%2520Activity%2520Recognition%2520framework%2520with%250A%2520%2520Generative%2520Representation%26entry.906535625%3DYuan%2520Sun%2520and%2520Navid%2520Salami%2520Pargoo%2520and%2520Taqiya%2520Ehsan%2520and%2520Zhao%2520Zhang%2520and%2520Jorge%2520Ortiz%26entry.1292438233%3D%2520%2520Complex%2520human%2520activity%2520recognition%2520%2528CHAR%2529%2520remains%2520a%2520pivotal%2520challenge%2520within%250Aubiquitous%2520computing%252C%2520especially%2520in%2520the%2520context%2520of%2520smart%2520environments.%2520Existing%250Astudies%2520typically%2520require%2520meticulous%2520labeling%2520of%2520both%2520atomic%2520and%2520complex%250Aactivities%252C%2520a%2520task%2520that%2520is%2520labor-intensive%2520and%2520prone%2520to%2520errors%2520due%2520to%2520the%250Ascarcity%2520and%2520inaccuracies%2520of%2520available%2520datasets.%2520Most%2520prior%2520research%2520has%250Afocused%2520on%2520datasets%2520that%2520either%2520precisely%2520label%2520atomic%2520activities%2520or%252C%2520at%250Aminimum%252C%2520their%2520sequence%2520approaches%2520that%2520are%2520often%2520impractical%2520in%2520real%2520world%250Asettings.In%2520response%252C%2520we%2520introduce%2520VCHAR%2520%2528Variance-Driven%2520Complex%2520Human%250AActivity%2520Recognition%2529%252C%2520a%2520novel%2520framework%2520that%2520treats%2520the%2520outputs%2520of%2520atomic%250Aactivities%2520as%2520a%2520distribution%2520over%2520specified%2520intervals.%2520Leveraging%2520generative%250Amethodologies%252C%2520VCHAR%2520elucidates%2520the%2520reasoning%2520behind%2520complex%2520activity%250Aclassifications%2520through%2520video-based%2520explanations%252C%2520accessible%2520to%2520users%2520without%250Aprior%2520machine%2520learning%2520expertise.%2520Our%2520evaluation%2520across%2520three%2520publicly%250Aavailable%2520datasets%2520demonstrates%2520that%2520VCHAR%2520enhances%2520the%2520accuracy%2520of%2520complex%250Aactivity%2520recognition%2520without%2520necessitating%2520precise%2520temporal%2520or%2520sequential%250Alabeling%2520of%2520atomic%2520activities.%2520Furthermore%252C%2520user%2520studies%2520confirm%2520that%2520VCHAR%2527s%250Aexplanations%2520are%2520more%2520intelligible%2520compared%2520to%2520existing%2520methods%252C%2520facilitating%2520a%250Abroader%2520understanding%2520of%2520complex%2520activity%2520recognition%2520among%2520non-experts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03291v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCHAR%3AVariance-Driven%20Complex%20Human%20Activity%20Recognition%20framework%20with%0A%20%20Generative%20Representation&entry.906535625=Yuan%20Sun%20and%20Navid%20Salami%20Pargoo%20and%20Taqiya%20Ehsan%20and%20Zhao%20Zhang%20and%20Jorge%20Ortiz&entry.1292438233=%20%20Complex%20human%20activity%20recognition%20%28CHAR%29%20remains%20a%20pivotal%20challenge%20within%0Aubiquitous%20computing%2C%20especially%20in%20the%20context%20of%20smart%20environments.%20Existing%0Astudies%20typically%20require%20meticulous%20labeling%20of%20both%20atomic%20and%20complex%0Aactivities%2C%20a%20task%20that%20is%20labor-intensive%20and%20prone%20to%20errors%20due%20to%20the%0Ascarcity%20and%20inaccuracies%20of%20available%20datasets.%20Most%20prior%20research%20has%0Afocused%20on%20datasets%20that%20either%20precisely%20label%20atomic%20activities%20or%2C%20at%0Aminimum%2C%20their%20sequence%20approaches%20that%20are%20often%20impractical%20in%20real%20world%0Asettings.In%20response%2C%20we%20introduce%20VCHAR%20%28Variance-Driven%20Complex%20Human%0AActivity%20Recognition%29%2C%20a%20novel%20framework%20that%20treats%20the%20outputs%20of%20atomic%0Aactivities%20as%20a%20distribution%20over%20specified%20intervals.%20Leveraging%20generative%0Amethodologies%2C%20VCHAR%20elucidates%20the%20reasoning%20behind%20complex%20activity%0Aclassifications%20through%20video-based%20explanations%2C%20accessible%20to%20users%20without%0Aprior%20machine%20learning%20expertise.%20Our%20evaluation%20across%20three%20publicly%0Aavailable%20datasets%20demonstrates%20that%20VCHAR%20enhances%20the%20accuracy%20of%20complex%0Aactivity%20recognition%20without%20necessitating%20precise%20temporal%20or%20sequential%0Alabeling%20of%20atomic%20activities.%20Furthermore%2C%20user%20studies%20confirm%20that%20VCHAR%27s%0Aexplanations%20are%20more%20intelligible%20compared%20to%20existing%20methods%2C%20facilitating%20a%0Abroader%20understanding%20of%20complex%20activity%20recognition%20among%20non-experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03291v2&entry.124074799=Read"},
{"title": "Prototype Learning for Micro-gesture Classification", "author": "Guoliang Chen and Fei Wang and Kun Li and Zhiliang Wu and Hehe Fan and Yi Yang and Meng Wang and Dan Guo", "abstract": "  In this paper, we briefly introduce the solution developed by our team,\nHFUT-VUT, for the track of Micro-gesture Classification in the MiGA challenge\nat IJCAI 2024. The task of micro-gesture classification task involves\nrecognizing the category of a given video clip, which focuses on more\nfine-grained and subtle body movements compared to typical action recognition\ntasks. Given the inherent complexity of micro-gesture recognition, which\nincludes large intra-class variability and minimal inter-class differences, we\nutilize two innovative modules, i.e., the cross-modal fusion module and\nprototypical refinement module, to improve the discriminative ability of MG\nfeatures, thereby improving the classification accuracy. Our solution achieved\nsignificant success, ranking 1st in the track of Micro-gesture Classification.\nWe surpassed the performance of last year's leading team by a substantial\nmargin, improving Top-1 accuracy by 6.13%.\n", "link": "http://arxiv.org/abs/2408.03097v1", "date": "2024-08-06", "relevancy": 2.1185, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.562}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5203}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototype%20Learning%20for%20Micro-gesture%20Classification&body=Title%3A%20Prototype%20Learning%20for%20Micro-gesture%20Classification%0AAuthor%3A%20Guoliang%20Chen%20and%20Fei%20Wang%20and%20Kun%20Li%20and%20Zhiliang%20Wu%20and%20Hehe%20Fan%20and%20Yi%20Yang%20and%20Meng%20Wang%20and%20Dan%20Guo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20briefly%20introduce%20the%20solution%20developed%20by%20our%20team%2C%0AHFUT-VUT%2C%20for%20the%20track%20of%20Micro-gesture%20Classification%20in%20the%20MiGA%20challenge%0Aat%20IJCAI%202024.%20The%20task%20of%20micro-gesture%20classification%20task%20involves%0Arecognizing%20the%20category%20of%20a%20given%20video%20clip%2C%20which%20focuses%20on%20more%0Afine-grained%20and%20subtle%20body%20movements%20compared%20to%20typical%20action%20recognition%0Atasks.%20Given%20the%20inherent%20complexity%20of%20micro-gesture%20recognition%2C%20which%0Aincludes%20large%20intra-class%20variability%20and%20minimal%20inter-class%20differences%2C%20we%0Autilize%20two%20innovative%20modules%2C%20i.e.%2C%20the%20cross-modal%20fusion%20module%20and%0Aprototypical%20refinement%20module%2C%20to%20improve%20the%20discriminative%20ability%20of%20MG%0Afeatures%2C%20thereby%20improving%20the%20classification%20accuracy.%20Our%20solution%20achieved%0Asignificant%20success%2C%20ranking%201st%20in%20the%20track%20of%20Micro-gesture%20Classification.%0AWe%20surpassed%20the%20performance%20of%20last%20year%27s%20leading%20team%20by%20a%20substantial%0Amargin%2C%20improving%20Top-1%20accuracy%20by%206.13%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototype%2520Learning%2520for%2520Micro-gesture%2520Classification%26entry.906535625%3DGuoliang%2520Chen%2520and%2520Fei%2520Wang%2520and%2520Kun%2520Li%2520and%2520Zhiliang%2520Wu%2520and%2520Hehe%2520Fan%2520and%2520Yi%2520Yang%2520and%2520Meng%2520Wang%2520and%2520Dan%2520Guo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520briefly%2520introduce%2520the%2520solution%2520developed%2520by%2520our%2520team%252C%250AHFUT-VUT%252C%2520for%2520the%2520track%2520of%2520Micro-gesture%2520Classification%2520in%2520the%2520MiGA%2520challenge%250Aat%2520IJCAI%25202024.%2520The%2520task%2520of%2520micro-gesture%2520classification%2520task%2520involves%250Arecognizing%2520the%2520category%2520of%2520a%2520given%2520video%2520clip%252C%2520which%2520focuses%2520on%2520more%250Afine-grained%2520and%2520subtle%2520body%2520movements%2520compared%2520to%2520typical%2520action%2520recognition%250Atasks.%2520Given%2520the%2520inherent%2520complexity%2520of%2520micro-gesture%2520recognition%252C%2520which%250Aincludes%2520large%2520intra-class%2520variability%2520and%2520minimal%2520inter-class%2520differences%252C%2520we%250Autilize%2520two%2520innovative%2520modules%252C%2520i.e.%252C%2520the%2520cross-modal%2520fusion%2520module%2520and%250Aprototypical%2520refinement%2520module%252C%2520to%2520improve%2520the%2520discriminative%2520ability%2520of%2520MG%250Afeatures%252C%2520thereby%2520improving%2520the%2520classification%2520accuracy.%2520Our%2520solution%2520achieved%250Asignificant%2520success%252C%2520ranking%25201st%2520in%2520the%2520track%2520of%2520Micro-gesture%2520Classification.%250AWe%2520surpassed%2520the%2520performance%2520of%2520last%2520year%2527s%2520leading%2520team%2520by%2520a%2520substantial%250Amargin%252C%2520improving%2520Top-1%2520accuracy%2520by%25206.13%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototype%20Learning%20for%20Micro-gesture%20Classification&entry.906535625=Guoliang%20Chen%20and%20Fei%20Wang%20and%20Kun%20Li%20and%20Zhiliang%20Wu%20and%20Hehe%20Fan%20and%20Yi%20Yang%20and%20Meng%20Wang%20and%20Dan%20Guo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20briefly%20introduce%20the%20solution%20developed%20by%20our%20team%2C%0AHFUT-VUT%2C%20for%20the%20track%20of%20Micro-gesture%20Classification%20in%20the%20MiGA%20challenge%0Aat%20IJCAI%202024.%20The%20task%20of%20micro-gesture%20classification%20task%20involves%0Arecognizing%20the%20category%20of%20a%20given%20video%20clip%2C%20which%20focuses%20on%20more%0Afine-grained%20and%20subtle%20body%20movements%20compared%20to%20typical%20action%20recognition%0Atasks.%20Given%20the%20inherent%20complexity%20of%20micro-gesture%20recognition%2C%20which%0Aincludes%20large%20intra-class%20variability%20and%20minimal%20inter-class%20differences%2C%20we%0Autilize%20two%20innovative%20modules%2C%20i.e.%2C%20the%20cross-modal%20fusion%20module%20and%0Aprototypical%20refinement%20module%2C%20to%20improve%20the%20discriminative%20ability%20of%20MG%0Afeatures%2C%20thereby%20improving%20the%20classification%20accuracy.%20Our%20solution%20achieved%0Asignificant%20success%2C%20ranking%201st%20in%20the%20track%20of%20Micro-gesture%20Classification.%0AWe%20surpassed%20the%20performance%20of%20last%20year%27s%20leading%20team%20by%20a%20substantial%0Amargin%2C%20improving%20Top-1%20accuracy%20by%206.13%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03097v1&entry.124074799=Read"},
{"title": "GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models\n  Evaluation", "author": "Yi Zong and Xipeng Qiu", "abstract": "  The Large Vision-Language Models (LVLMs) have demonstrated great abilities in\nimage perception and language understanding. However, existing multimodal\nbenchmarks focus on primary perception abilities and commonsense knowledge\nwhich are insufficient to reflect the comprehensive capabilities of LVLMs. We\npropose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance\nExamination (GAOKAO), comprising of 8 subjects and 12 types of images, such as\ndiagrams, function graphs, maps and photos. GAOKAO-MM derives from native\nChinese context and sets human-level requirements for the model's abilities,\nincluding perception, understanding, knowledge and reasoning. We evaluate 10\nLVLMs and find that the accuracies of all of them are lower than 50%, with\nGPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking\nin the top three positions. The results of our multi-dimension analysis\nindicate that LVLMs have moderate distance towards Artificial General\nIntelligence (AGI) and provide insights facilitating the development of\nmultilingual LVLMs.\n", "link": "http://arxiv.org/abs/2402.15745v2", "date": "2024-08-06", "relevancy": 2.1144, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5449}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5176}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAOKAO-MM%3A%20A%20Chinese%20Human-Level%20Benchmark%20for%20Multimodal%20Models%0A%20%20Evaluation&body=Title%3A%20GAOKAO-MM%3A%20A%20Chinese%20Human-Level%20Benchmark%20for%20Multimodal%20Models%0A%20%20Evaluation%0AAuthor%3A%20Yi%20Zong%20and%20Xipeng%20Qiu%0AAbstract%3A%20%20%20The%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20great%20abilities%20in%0Aimage%20perception%20and%20language%20understanding.%20However%2C%20existing%20multimodal%0Abenchmarks%20focus%20on%20primary%20perception%20abilities%20and%20commonsense%20knowledge%0Awhich%20are%20insufficient%20to%20reflect%20the%20comprehensive%20capabilities%20of%20LVLMs.%20We%0Apropose%20GAOKAO-MM%2C%20a%20multimodal%20benchmark%20based%20on%20the%20Chinese%20College%20Entrance%0AExamination%20%28GAOKAO%29%2C%20comprising%20of%208%20subjects%20and%2012%20types%20of%20images%2C%20such%20as%0Adiagrams%2C%20function%20graphs%2C%20maps%20and%20photos.%20GAOKAO-MM%20derives%20from%20native%0AChinese%20context%20and%20sets%20human-level%20requirements%20for%20the%20model%27s%20abilities%2C%0Aincluding%20perception%2C%20understanding%2C%20knowledge%20and%20reasoning.%20We%20evaluate%2010%0ALVLMs%20and%20find%20that%20the%20accuracies%20of%20all%20of%20them%20are%20lower%20than%2050%25%2C%20with%0AGPT-4-Vison%20%2848.1%25%29%2C%20Qwen-VL-Plus%20%2841.2%25%29%20and%20Gemini-Pro-Vision%20%2835.1%25%29%20ranking%0Ain%20the%20top%20three%20positions.%20The%20results%20of%20our%20multi-dimension%20analysis%0Aindicate%20that%20LVLMs%20have%20moderate%20distance%20towards%20Artificial%20General%0AIntelligence%20%28AGI%29%20and%20provide%20insights%20facilitating%20the%20development%20of%0Amultilingual%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15745v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAOKAO-MM%253A%2520A%2520Chinese%2520Human-Level%2520Benchmark%2520for%2520Multimodal%2520Models%250A%2520%2520Evaluation%26entry.906535625%3DYi%2520Zong%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3D%2520%2520The%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520great%2520abilities%2520in%250Aimage%2520perception%2520and%2520language%2520understanding.%2520However%252C%2520existing%2520multimodal%250Abenchmarks%2520focus%2520on%2520primary%2520perception%2520abilities%2520and%2520commonsense%2520knowledge%250Awhich%2520are%2520insufficient%2520to%2520reflect%2520the%2520comprehensive%2520capabilities%2520of%2520LVLMs.%2520We%250Apropose%2520GAOKAO-MM%252C%2520a%2520multimodal%2520benchmark%2520based%2520on%2520the%2520Chinese%2520College%2520Entrance%250AExamination%2520%2528GAOKAO%2529%252C%2520comprising%2520of%25208%2520subjects%2520and%252012%2520types%2520of%2520images%252C%2520such%2520as%250Adiagrams%252C%2520function%2520graphs%252C%2520maps%2520and%2520photos.%2520GAOKAO-MM%2520derives%2520from%2520native%250AChinese%2520context%2520and%2520sets%2520human-level%2520requirements%2520for%2520the%2520model%2527s%2520abilities%252C%250Aincluding%2520perception%252C%2520understanding%252C%2520knowledge%2520and%2520reasoning.%2520We%2520evaluate%252010%250ALVLMs%2520and%2520find%2520that%2520the%2520accuracies%2520of%2520all%2520of%2520them%2520are%2520lower%2520than%252050%2525%252C%2520with%250AGPT-4-Vison%2520%252848.1%2525%2529%252C%2520Qwen-VL-Plus%2520%252841.2%2525%2529%2520and%2520Gemini-Pro-Vision%2520%252835.1%2525%2529%2520ranking%250Ain%2520the%2520top%2520three%2520positions.%2520The%2520results%2520of%2520our%2520multi-dimension%2520analysis%250Aindicate%2520that%2520LVLMs%2520have%2520moderate%2520distance%2520towards%2520Artificial%2520General%250AIntelligence%2520%2528AGI%2529%2520and%2520provide%2520insights%2520facilitating%2520the%2520development%2520of%250Amultilingual%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15745v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAOKAO-MM%3A%20A%20Chinese%20Human-Level%20Benchmark%20for%20Multimodal%20Models%0A%20%20Evaluation&entry.906535625=Yi%20Zong%20and%20Xipeng%20Qiu&entry.1292438233=%20%20The%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20great%20abilities%20in%0Aimage%20perception%20and%20language%20understanding.%20However%2C%20existing%20multimodal%0Abenchmarks%20focus%20on%20primary%20perception%20abilities%20and%20commonsense%20knowledge%0Awhich%20are%20insufficient%20to%20reflect%20the%20comprehensive%20capabilities%20of%20LVLMs.%20We%0Apropose%20GAOKAO-MM%2C%20a%20multimodal%20benchmark%20based%20on%20the%20Chinese%20College%20Entrance%0AExamination%20%28GAOKAO%29%2C%20comprising%20of%208%20subjects%20and%2012%20types%20of%20images%2C%20such%20as%0Adiagrams%2C%20function%20graphs%2C%20maps%20and%20photos.%20GAOKAO-MM%20derives%20from%20native%0AChinese%20context%20and%20sets%20human-level%20requirements%20for%20the%20model%27s%20abilities%2C%0Aincluding%20perception%2C%20understanding%2C%20knowledge%20and%20reasoning.%20We%20evaluate%2010%0ALVLMs%20and%20find%20that%20the%20accuracies%20of%20all%20of%20them%20are%20lower%20than%2050%25%2C%20with%0AGPT-4-Vison%20%2848.1%25%29%2C%20Qwen-VL-Plus%20%2841.2%25%29%20and%20Gemini-Pro-Vision%20%2835.1%25%29%20ranking%0Ain%20the%20top%20three%20positions.%20The%20results%20of%20our%20multi-dimension%20analysis%0Aindicate%20that%20LVLMs%20have%20moderate%20distance%20towards%20Artificial%20General%0AIntelligence%20%28AGI%29%20and%20provide%20insights%20facilitating%20the%20development%20of%0Amultilingual%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15745v2&entry.124074799=Read"},
{"title": "SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast\n  and Reliable Surface Defect Detection", "author": "Bla\u017e Rolih and Matic Fu\u010dka and Danijel Sko\u010daj", "abstract": "  The aim of surface defect detection is to identify and localise abnormal\nregions on the surfaces of captured objects, a task that's increasingly\ndemanded across various industries. Current approaches frequently fail to\nfulfil the extensive demands of these industries, which encompass high\nperformance, consistency, and fast operation, along with the capacity to\nleverage the entirety of the available training data. Addressing these gaps, we\nintroduce SuperSimpleNet, an innovative discriminative model that evolved from\nSimpleNet. This advanced model significantly enhances its predecessor's\ntraining consistency, inference time, as well as detection performance.\nSuperSimpleNet operates in an unsupervised manner using only normal training\nimages but also benefits from labelled abnormal training images when they are\navailable. SuperSimpleNet achieves state-of-the-art results in both the\nsupervised and the unsupervised settings, as demonstrated by experiments across\nfour challenging benchmark datasets. Code:\nhttps://github.com/blaz-r/SuperSimpleNet .\n", "link": "http://arxiv.org/abs/2408.03143v1", "date": "2024-08-06", "relevancy": 2.0981, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5444}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5341}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperSimpleNet%3A%20Unifying%20Unsupervised%20and%20Supervised%20Learning%20for%20Fast%0A%20%20and%20Reliable%20Surface%20Defect%20Detection&body=Title%3A%20SuperSimpleNet%3A%20Unifying%20Unsupervised%20and%20Supervised%20Learning%20for%20Fast%0A%20%20and%20Reliable%20Surface%20Defect%20Detection%0AAuthor%3A%20Bla%C5%BE%20Rolih%20and%20Matic%20Fu%C4%8Dka%20and%20Danijel%20Sko%C4%8Daj%0AAbstract%3A%20%20%20The%20aim%20of%20surface%20defect%20detection%20is%20to%20identify%20and%20localise%20abnormal%0Aregions%20on%20the%20surfaces%20of%20captured%20objects%2C%20a%20task%20that%27s%20increasingly%0Ademanded%20across%20various%20industries.%20Current%20approaches%20frequently%20fail%20to%0Afulfil%20the%20extensive%20demands%20of%20these%20industries%2C%20which%20encompass%20high%0Aperformance%2C%20consistency%2C%20and%20fast%20operation%2C%20along%20with%20the%20capacity%20to%0Aleverage%20the%20entirety%20of%20the%20available%20training%20data.%20Addressing%20these%20gaps%2C%20we%0Aintroduce%20SuperSimpleNet%2C%20an%20innovative%20discriminative%20model%20that%20evolved%20from%0ASimpleNet.%20This%20advanced%20model%20significantly%20enhances%20its%20predecessor%27s%0Atraining%20consistency%2C%20inference%20time%2C%20as%20well%20as%20detection%20performance.%0ASuperSimpleNet%20operates%20in%20an%20unsupervised%20manner%20using%20only%20normal%20training%0Aimages%20but%20also%20benefits%20from%20labelled%20abnormal%20training%20images%20when%20they%20are%0Aavailable.%20SuperSimpleNet%20achieves%20state-of-the-art%20results%20in%20both%20the%0Asupervised%20and%20the%20unsupervised%20settings%2C%20as%20demonstrated%20by%20experiments%20across%0Afour%20challenging%20benchmark%20datasets.%20Code%3A%0Ahttps%3A//github.com/blaz-r/SuperSimpleNet%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperSimpleNet%253A%2520Unifying%2520Unsupervised%2520and%2520Supervised%2520Learning%2520for%2520Fast%250A%2520%2520and%2520Reliable%2520Surface%2520Defect%2520Detection%26entry.906535625%3DBla%25C5%25BE%2520Rolih%2520and%2520Matic%2520Fu%25C4%258Dka%2520and%2520Danijel%2520Sko%25C4%258Daj%26entry.1292438233%3D%2520%2520The%2520aim%2520of%2520surface%2520defect%2520detection%2520is%2520to%2520identify%2520and%2520localise%2520abnormal%250Aregions%2520on%2520the%2520surfaces%2520of%2520captured%2520objects%252C%2520a%2520task%2520that%2527s%2520increasingly%250Ademanded%2520across%2520various%2520industries.%2520Current%2520approaches%2520frequently%2520fail%2520to%250Afulfil%2520the%2520extensive%2520demands%2520of%2520these%2520industries%252C%2520which%2520encompass%2520high%250Aperformance%252C%2520consistency%252C%2520and%2520fast%2520operation%252C%2520along%2520with%2520the%2520capacity%2520to%250Aleverage%2520the%2520entirety%2520of%2520the%2520available%2520training%2520data.%2520Addressing%2520these%2520gaps%252C%2520we%250Aintroduce%2520SuperSimpleNet%252C%2520an%2520innovative%2520discriminative%2520model%2520that%2520evolved%2520from%250ASimpleNet.%2520This%2520advanced%2520model%2520significantly%2520enhances%2520its%2520predecessor%2527s%250Atraining%2520consistency%252C%2520inference%2520time%252C%2520as%2520well%2520as%2520detection%2520performance.%250ASuperSimpleNet%2520operates%2520in%2520an%2520unsupervised%2520manner%2520using%2520only%2520normal%2520training%250Aimages%2520but%2520also%2520benefits%2520from%2520labelled%2520abnormal%2520training%2520images%2520when%2520they%2520are%250Aavailable.%2520SuperSimpleNet%2520achieves%2520state-of-the-art%2520results%2520in%2520both%2520the%250Asupervised%2520and%2520the%2520unsupervised%2520settings%252C%2520as%2520demonstrated%2520by%2520experiments%2520across%250Afour%2520challenging%2520benchmark%2520datasets.%2520Code%253A%250Ahttps%253A//github.com/blaz-r/SuperSimpleNet%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperSimpleNet%3A%20Unifying%20Unsupervised%20and%20Supervised%20Learning%20for%20Fast%0A%20%20and%20Reliable%20Surface%20Defect%20Detection&entry.906535625=Bla%C5%BE%20Rolih%20and%20Matic%20Fu%C4%8Dka%20and%20Danijel%20Sko%C4%8Daj&entry.1292438233=%20%20The%20aim%20of%20surface%20defect%20detection%20is%20to%20identify%20and%20localise%20abnormal%0Aregions%20on%20the%20surfaces%20of%20captured%20objects%2C%20a%20task%20that%27s%20increasingly%0Ademanded%20across%20various%20industries.%20Current%20approaches%20frequently%20fail%20to%0Afulfil%20the%20extensive%20demands%20of%20these%20industries%2C%20which%20encompass%20high%0Aperformance%2C%20consistency%2C%20and%20fast%20operation%2C%20along%20with%20the%20capacity%20to%0Aleverage%20the%20entirety%20of%20the%20available%20training%20data.%20Addressing%20these%20gaps%2C%20we%0Aintroduce%20SuperSimpleNet%2C%20an%20innovative%20discriminative%20model%20that%20evolved%20from%0ASimpleNet.%20This%20advanced%20model%20significantly%20enhances%20its%20predecessor%27s%0Atraining%20consistency%2C%20inference%20time%2C%20as%20well%20as%20detection%20performance.%0ASuperSimpleNet%20operates%20in%20an%20unsupervised%20manner%20using%20only%20normal%20training%0Aimages%20but%20also%20benefits%20from%20labelled%20abnormal%20training%20images%20when%20they%20are%0Aavailable.%20SuperSimpleNet%20achieves%20state-of-the-art%20results%20in%20both%20the%0Asupervised%20and%20the%20unsupervised%20settings%2C%20as%20demonstrated%20by%20experiments%20across%0Afour%20challenging%20benchmark%20datasets.%20Code%3A%0Ahttps%3A//github.com/blaz-r/SuperSimpleNet%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03143v1&entry.124074799=Read"},
{"title": "Integrated Intention Prediction and Decision-Making with Spectrum\n  Attention Net and Proximal Policy Optimization", "author": "Xiao Zhou and Chengzhen Meng and Wenru Liu and Zengqi Peng and Ming Liu and Jun Ma", "abstract": "  For autonomous driving in highly dynamic environments, it is anticipated to\npredict the future behaviors of surrounding vehicles (SVs) and make safe and\neffective decisions. However, modeling the inherent coupling effect between the\nprediction and decision-making modules has been a long-standing challenge,\nespecially when there is a need to maintain appropriate computational\nefficiency. To tackle these problems, we propose a novel integrated intention\nprediction and decision-making approach, which explicitly models the coupling\nrelationship and achieves efficient computation. Specifically, a spectrum\nattention net is designed to predict the intentions of SVs by capturing the\ntrends of each frequency component over time and their interrelations. Fast\ncomputation of the intention prediction module is attained as the predicted\nintentions are not decoded to trajectories in the executing process.\nFurthermore, the proximal policy optimization (PPO) algorithm is employed to\naddress the non-stationary problem in the framework through a modest policy\nupdate enabled by a clipping mechanism within its objective function. On the\nbasis of these developments, the intention prediction and decision-making\nmodules are integrated through joint learning. Experiments are conducted in\nrepresentative traffic scenarios, and the results reveal that the proposed\nintegrated framework demonstrates superior performance over several deep\nreinforcement learning (DRL) baselines in terms of success rate, efficiency,\nand safety in driving tasks.\n", "link": "http://arxiv.org/abs/2408.03191v1", "date": "2024-08-06", "relevancy": 2.077, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5841}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5197}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrated%20Intention%20Prediction%20and%20Decision-Making%20with%20Spectrum%0A%20%20Attention%20Net%20and%20Proximal%20Policy%20Optimization&body=Title%3A%20Integrated%20Intention%20Prediction%20and%20Decision-Making%20with%20Spectrum%0A%20%20Attention%20Net%20and%20Proximal%20Policy%20Optimization%0AAuthor%3A%20Xiao%20Zhou%20and%20Chengzhen%20Meng%20and%20Wenru%20Liu%20and%20Zengqi%20Peng%20and%20Ming%20Liu%20and%20Jun%20Ma%0AAbstract%3A%20%20%20For%20autonomous%20driving%20in%20highly%20dynamic%20environments%2C%20it%20is%20anticipated%20to%0Apredict%20the%20future%20behaviors%20of%20surrounding%20vehicles%20%28SVs%29%20and%20make%20safe%20and%0Aeffective%20decisions.%20However%2C%20modeling%20the%20inherent%20coupling%20effect%20between%20the%0Aprediction%20and%20decision-making%20modules%20has%20been%20a%20long-standing%20challenge%2C%0Aespecially%20when%20there%20is%20a%20need%20to%20maintain%20appropriate%20computational%0Aefficiency.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%20novel%20integrated%20intention%0Aprediction%20and%20decision-making%20approach%2C%20which%20explicitly%20models%20the%20coupling%0Arelationship%20and%20achieves%20efficient%20computation.%20Specifically%2C%20a%20spectrum%0Aattention%20net%20is%20designed%20to%20predict%20the%20intentions%20of%20SVs%20by%20capturing%20the%0Atrends%20of%20each%20frequency%20component%20over%20time%20and%20their%20interrelations.%20Fast%0Acomputation%20of%20the%20intention%20prediction%20module%20is%20attained%20as%20the%20predicted%0Aintentions%20are%20not%20decoded%20to%20trajectories%20in%20the%20executing%20process.%0AFurthermore%2C%20the%20proximal%20policy%20optimization%20%28PPO%29%20algorithm%20is%20employed%20to%0Aaddress%20the%20non-stationary%20problem%20in%20the%20framework%20through%20a%20modest%20policy%0Aupdate%20enabled%20by%20a%20clipping%20mechanism%20within%20its%20objective%20function.%20On%20the%0Abasis%20of%20these%20developments%2C%20the%20intention%20prediction%20and%20decision-making%0Amodules%20are%20integrated%20through%20joint%20learning.%20Experiments%20are%20conducted%20in%0Arepresentative%20traffic%20scenarios%2C%20and%20the%20results%20reveal%20that%20the%20proposed%0Aintegrated%20framework%20demonstrates%20superior%20performance%20over%20several%20deep%0Areinforcement%20learning%20%28DRL%29%20baselines%20in%20terms%20of%20success%20rate%2C%20efficiency%2C%0Aand%20safety%20in%20driving%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrated%2520Intention%2520Prediction%2520and%2520Decision-Making%2520with%2520Spectrum%250A%2520%2520Attention%2520Net%2520and%2520Proximal%2520Policy%2520Optimization%26entry.906535625%3DXiao%2520Zhou%2520and%2520Chengzhen%2520Meng%2520and%2520Wenru%2520Liu%2520and%2520Zengqi%2520Peng%2520and%2520Ming%2520Liu%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520For%2520autonomous%2520driving%2520in%2520highly%2520dynamic%2520environments%252C%2520it%2520is%2520anticipated%2520to%250Apredict%2520the%2520future%2520behaviors%2520of%2520surrounding%2520vehicles%2520%2528SVs%2529%2520and%2520make%2520safe%2520and%250Aeffective%2520decisions.%2520However%252C%2520modeling%2520the%2520inherent%2520coupling%2520effect%2520between%2520the%250Aprediction%2520and%2520decision-making%2520modules%2520has%2520been%2520a%2520long-standing%2520challenge%252C%250Aespecially%2520when%2520there%2520is%2520a%2520need%2520to%2520maintain%2520appropriate%2520computational%250Aefficiency.%2520To%2520tackle%2520these%2520problems%252C%2520we%2520propose%2520a%2520novel%2520integrated%2520intention%250Aprediction%2520and%2520decision-making%2520approach%252C%2520which%2520explicitly%2520models%2520the%2520coupling%250Arelationship%2520and%2520achieves%2520efficient%2520computation.%2520Specifically%252C%2520a%2520spectrum%250Aattention%2520net%2520is%2520designed%2520to%2520predict%2520the%2520intentions%2520of%2520SVs%2520by%2520capturing%2520the%250Atrends%2520of%2520each%2520frequency%2520component%2520over%2520time%2520and%2520their%2520interrelations.%2520Fast%250Acomputation%2520of%2520the%2520intention%2520prediction%2520module%2520is%2520attained%2520as%2520the%2520predicted%250Aintentions%2520are%2520not%2520decoded%2520to%2520trajectories%2520in%2520the%2520executing%2520process.%250AFurthermore%252C%2520the%2520proximal%2520policy%2520optimization%2520%2528PPO%2529%2520algorithm%2520is%2520employed%2520to%250Aaddress%2520the%2520non-stationary%2520problem%2520in%2520the%2520framework%2520through%2520a%2520modest%2520policy%250Aupdate%2520enabled%2520by%2520a%2520clipping%2520mechanism%2520within%2520its%2520objective%2520function.%2520On%2520the%250Abasis%2520of%2520these%2520developments%252C%2520the%2520intention%2520prediction%2520and%2520decision-making%250Amodules%2520are%2520integrated%2520through%2520joint%2520learning.%2520Experiments%2520are%2520conducted%2520in%250Arepresentative%2520traffic%2520scenarios%252C%2520and%2520the%2520results%2520reveal%2520that%2520the%2520proposed%250Aintegrated%2520framework%2520demonstrates%2520superior%2520performance%2520over%2520several%2520deep%250Areinforcement%2520learning%2520%2528DRL%2529%2520baselines%2520in%2520terms%2520of%2520success%2520rate%252C%2520efficiency%252C%250Aand%2520safety%2520in%2520driving%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrated%20Intention%20Prediction%20and%20Decision-Making%20with%20Spectrum%0A%20%20Attention%20Net%20and%20Proximal%20Policy%20Optimization&entry.906535625=Xiao%20Zhou%20and%20Chengzhen%20Meng%20and%20Wenru%20Liu%20and%20Zengqi%20Peng%20and%20Ming%20Liu%20and%20Jun%20Ma&entry.1292438233=%20%20For%20autonomous%20driving%20in%20highly%20dynamic%20environments%2C%20it%20is%20anticipated%20to%0Apredict%20the%20future%20behaviors%20of%20surrounding%20vehicles%20%28SVs%29%20and%20make%20safe%20and%0Aeffective%20decisions.%20However%2C%20modeling%20the%20inherent%20coupling%20effect%20between%20the%0Aprediction%20and%20decision-making%20modules%20has%20been%20a%20long-standing%20challenge%2C%0Aespecially%20when%20there%20is%20a%20need%20to%20maintain%20appropriate%20computational%0Aefficiency.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%20novel%20integrated%20intention%0Aprediction%20and%20decision-making%20approach%2C%20which%20explicitly%20models%20the%20coupling%0Arelationship%20and%20achieves%20efficient%20computation.%20Specifically%2C%20a%20spectrum%0Aattention%20net%20is%20designed%20to%20predict%20the%20intentions%20of%20SVs%20by%20capturing%20the%0Atrends%20of%20each%20frequency%20component%20over%20time%20and%20their%20interrelations.%20Fast%0Acomputation%20of%20the%20intention%20prediction%20module%20is%20attained%20as%20the%20predicted%0Aintentions%20are%20not%20decoded%20to%20trajectories%20in%20the%20executing%20process.%0AFurthermore%2C%20the%20proximal%20policy%20optimization%20%28PPO%29%20algorithm%20is%20employed%20to%0Aaddress%20the%20non-stationary%20problem%20in%20the%20framework%20through%20a%20modest%20policy%0Aupdate%20enabled%20by%20a%20clipping%20mechanism%20within%20its%20objective%20function.%20On%20the%0Abasis%20of%20these%20developments%2C%20the%20intention%20prediction%20and%20decision-making%0Amodules%20are%20integrated%20through%20joint%20learning.%20Experiments%20are%20conducted%20in%0Arepresentative%20traffic%20scenarios%2C%20and%20the%20results%20reveal%20that%20the%20proposed%0Aintegrated%20framework%20demonstrates%20superior%20performance%20over%20several%20deep%0Areinforcement%20learning%20%28DRL%29%20baselines%20in%20terms%20of%20success%20rate%2C%20efficiency%2C%0Aand%20safety%20in%20driving%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03191v1&entry.124074799=Read"},
{"title": "Pre-training and in-context learning IS Bayesian inference a la De\n  Finetti", "author": "Naimeng Ye and Hanming Yang and Andrew Siah and Hongseok Namkoong", "abstract": "  Accurately gauging uncertainty on the underlying environment is a\nlongstanding goal of intelligent systems. We characterize which latent concepts\npre-trained sequence models are naturally able to reason with. We go back to De\nFinetti's predictive view of Bayesian reasoning: instead of modeling latent\nparameters through priors and likelihoods like topic models do, De Finetti has\nlong advocated for modeling exchangeable (permutation invariant) sequences of\nobservables. According to this view, pre-training autoregressive models\nformulates informed beliefs based on prior observations (\"empirical Bayes\"),\nand forward generation is a simulated instantiation of an environment\n(\"posterior inference\"). This connection allows extending in-context learning\n(ICL) beyond predictive settings, highlighting sequence models' ability to\nperform explicit statistical inference. In particular, we show the sequence\nprediction loss over exchangeable documents controls performance on downstream\ntasks where uncertainty quantification is key. Empirically, we propose and\ndemonstrate several approaches for encoding exchangeability in sequence model\narchitectures: data augmentation, regularization, and causal masking.\n", "link": "http://arxiv.org/abs/2408.03307v1", "date": "2024-08-06", "relevancy": 2.0405, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5451}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5048}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-training%20and%20in-context%20learning%20IS%20Bayesian%20inference%20a%20la%20De%0A%20%20Finetti&body=Title%3A%20Pre-training%20and%20in-context%20learning%20IS%20Bayesian%20inference%20a%20la%20De%0A%20%20Finetti%0AAuthor%3A%20Naimeng%20Ye%20and%20Hanming%20Yang%20and%20Andrew%20Siah%20and%20Hongseok%20Namkoong%0AAbstract%3A%20%20%20Accurately%20gauging%20uncertainty%20on%20the%20underlying%20environment%20is%20a%0Alongstanding%20goal%20of%20intelligent%20systems.%20We%20characterize%20which%20latent%20concepts%0Apre-trained%20sequence%20models%20are%20naturally%20able%20to%20reason%20with.%20We%20go%20back%20to%20De%0AFinetti%27s%20predictive%20view%20of%20Bayesian%20reasoning%3A%20instead%20of%20modeling%20latent%0Aparameters%20through%20priors%20and%20likelihoods%20like%20topic%20models%20do%2C%20De%20Finetti%20has%0Along%20advocated%20for%20modeling%20exchangeable%20%28permutation%20invariant%29%20sequences%20of%0Aobservables.%20According%20to%20this%20view%2C%20pre-training%20autoregressive%20models%0Aformulates%20informed%20beliefs%20based%20on%20prior%20observations%20%28%22empirical%20Bayes%22%29%2C%0Aand%20forward%20generation%20is%20a%20simulated%20instantiation%20of%20an%20environment%0A%28%22posterior%20inference%22%29.%20This%20connection%20allows%20extending%20in-context%20learning%0A%28ICL%29%20beyond%20predictive%20settings%2C%20highlighting%20sequence%20models%27%20ability%20to%0Aperform%20explicit%20statistical%20inference.%20In%20particular%2C%20we%20show%20the%20sequence%0Aprediction%20loss%20over%20exchangeable%20documents%20controls%20performance%20on%20downstream%0Atasks%20where%20uncertainty%20quantification%20is%20key.%20Empirically%2C%20we%20propose%20and%0Ademonstrate%20several%20approaches%20for%20encoding%20exchangeability%20in%20sequence%20model%0Aarchitectures%3A%20data%20augmentation%2C%20regularization%2C%20and%20causal%20masking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-training%2520and%2520in-context%2520learning%2520IS%2520Bayesian%2520inference%2520a%2520la%2520De%250A%2520%2520Finetti%26entry.906535625%3DNaimeng%2520Ye%2520and%2520Hanming%2520Yang%2520and%2520Andrew%2520Siah%2520and%2520Hongseok%2520Namkoong%26entry.1292438233%3D%2520%2520Accurately%2520gauging%2520uncertainty%2520on%2520the%2520underlying%2520environment%2520is%2520a%250Alongstanding%2520goal%2520of%2520intelligent%2520systems.%2520We%2520characterize%2520which%2520latent%2520concepts%250Apre-trained%2520sequence%2520models%2520are%2520naturally%2520able%2520to%2520reason%2520with.%2520We%2520go%2520back%2520to%2520De%250AFinetti%2527s%2520predictive%2520view%2520of%2520Bayesian%2520reasoning%253A%2520instead%2520of%2520modeling%2520latent%250Aparameters%2520through%2520priors%2520and%2520likelihoods%2520like%2520topic%2520models%2520do%252C%2520De%2520Finetti%2520has%250Along%2520advocated%2520for%2520modeling%2520exchangeable%2520%2528permutation%2520invariant%2529%2520sequences%2520of%250Aobservables.%2520According%2520to%2520this%2520view%252C%2520pre-training%2520autoregressive%2520models%250Aformulates%2520informed%2520beliefs%2520based%2520on%2520prior%2520observations%2520%2528%2522empirical%2520Bayes%2522%2529%252C%250Aand%2520forward%2520generation%2520is%2520a%2520simulated%2520instantiation%2520of%2520an%2520environment%250A%2528%2522posterior%2520inference%2522%2529.%2520This%2520connection%2520allows%2520extending%2520in-context%2520learning%250A%2528ICL%2529%2520beyond%2520predictive%2520settings%252C%2520highlighting%2520sequence%2520models%2527%2520ability%2520to%250Aperform%2520explicit%2520statistical%2520inference.%2520In%2520particular%252C%2520we%2520show%2520the%2520sequence%250Aprediction%2520loss%2520over%2520exchangeable%2520documents%2520controls%2520performance%2520on%2520downstream%250Atasks%2520where%2520uncertainty%2520quantification%2520is%2520key.%2520Empirically%252C%2520we%2520propose%2520and%250Ademonstrate%2520several%2520approaches%2520for%2520encoding%2520exchangeability%2520in%2520sequence%2520model%250Aarchitectures%253A%2520data%2520augmentation%252C%2520regularization%252C%2520and%2520causal%2520masking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-training%20and%20in-context%20learning%20IS%20Bayesian%20inference%20a%20la%20De%0A%20%20Finetti&entry.906535625=Naimeng%20Ye%20and%20Hanming%20Yang%20and%20Andrew%20Siah%20and%20Hongseok%20Namkoong&entry.1292438233=%20%20Accurately%20gauging%20uncertainty%20on%20the%20underlying%20environment%20is%20a%0Alongstanding%20goal%20of%20intelligent%20systems.%20We%20characterize%20which%20latent%20concepts%0Apre-trained%20sequence%20models%20are%20naturally%20able%20to%20reason%20with.%20We%20go%20back%20to%20De%0AFinetti%27s%20predictive%20view%20of%20Bayesian%20reasoning%3A%20instead%20of%20modeling%20latent%0Aparameters%20through%20priors%20and%20likelihoods%20like%20topic%20models%20do%2C%20De%20Finetti%20has%0Along%20advocated%20for%20modeling%20exchangeable%20%28permutation%20invariant%29%20sequences%20of%0Aobservables.%20According%20to%20this%20view%2C%20pre-training%20autoregressive%20models%0Aformulates%20informed%20beliefs%20based%20on%20prior%20observations%20%28%22empirical%20Bayes%22%29%2C%0Aand%20forward%20generation%20is%20a%20simulated%20instantiation%20of%20an%20environment%0A%28%22posterior%20inference%22%29.%20This%20connection%20allows%20extending%20in-context%20learning%0A%28ICL%29%20beyond%20predictive%20settings%2C%20highlighting%20sequence%20models%27%20ability%20to%0Aperform%20explicit%20statistical%20inference.%20In%20particular%2C%20we%20show%20the%20sequence%0Aprediction%20loss%20over%20exchangeable%20documents%20controls%20performance%20on%20downstream%0Atasks%20where%20uncertainty%20quantification%20is%20key.%20Empirically%2C%20we%20propose%20and%0Ademonstrate%20several%20approaches%20for%20encoding%20exchangeability%20in%20sequence%20model%0Aarchitectures%3A%20data%20augmentation%2C%20regularization%2C%20and%20causal%20masking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03307v1&entry.124074799=Read"},
{"title": "Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing", "author": "Yizhak Elboher and Raya Elsaleh and Omri Isac and M\u00e9lanie Ducoffe and Audrey Galametz and Guillaume Pov\u00e9da and Ryma Boumazouza and No\u00e9mie Cohen and Guy Katz", "abstract": "  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n", "link": "http://arxiv.org/abs/2402.00035v4", "date": "2024-08-06", "relevancy": 2.0175, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5182}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5053}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness%20Assessment%20of%20a%20Runway%20Object%20Classifier%20for%20Safe%20Aircraft%0A%20%20Taxiing&body=Title%3A%20Robustness%20Assessment%20of%20a%20Runway%20Object%20Classifier%20for%20Safe%20Aircraft%0A%20%20Taxiing%0AAuthor%3A%20Yizhak%20Elboher%20and%20Raya%20Elsaleh%20and%20Omri%20Isac%20and%20M%C3%A9lanie%20Ducoffe%20and%20Audrey%20Galametz%20and%20Guillaume%20Pov%C3%A9da%20and%20Ryma%20Boumazouza%20and%20No%C3%A9mie%20Cohen%20and%20Guy%20Katz%0AAbstract%3A%20%20%20As%20deep%20neural%20networks%20%28DNNs%29%20are%20becoming%20the%20prominent%20solution%20for%20many%0Acomputational%20problems%2C%20the%20aviation%20industry%20seeks%20to%20explore%20their%20potential%0Ain%20alleviating%20pilot%20workload%20and%20in%20improving%20operational%20safety.%20However%2C%20the%0Ause%20of%20DNNs%20in%20this%20type%20of%20safety-critical%20applications%20requires%20a%20thorough%0Acertification%20process.%20This%20need%20can%20be%20addressed%20through%20formal%20verification%2C%0Awhich%20provides%20rigorous%20assurances%20--%20e.g.%2C~by%20proving%20the%20absence%20of%20certain%0Amispredictions.%20In%20this%20case-study%20paper%2C%20we%20demonstrate%20this%20process%20using%20an%0Aimage-classifier%20DNN%20currently%20under%20development%20at%20Airbus%20and%20intended%20for%20use%0Aduring%20the%20aircraft%20taxiing%20phase.%20We%20use%20formal%20methods%20to%20assess%20this%20DNN%27s%0Arobustness%20to%20three%20common%20image%20perturbation%20types%3A%20noise%2C%20brightness%20and%0Acontrast%2C%20and%20some%20of%20their%20combinations.%20This%20process%20entails%20multiple%0Ainvocations%20of%20the%20underlying%20verifier%2C%20which%20might%20be%20computationally%0Aexpensive%3B%20and%20we%20therefore%20propose%20a%20method%20that%20leverages%20the%20monotonicity%20of%0Athese%20robustness%20properties%2C%20as%20well%20as%20the%20results%20of%20past%20verification%0Aqueries%2C%20in%20order%20to%20reduce%20the%20overall%20number%20of%20verification%20queries%20required%0Aby%20nearly%2060%25.%20Our%20results%20provide%20an%20indication%20of%20the%20level%20of%20robustness%0Aachieved%20by%20the%20DNN%20classifier%20under%20study%2C%20and%20indicate%20that%20it%20is%0Aconsiderably%20more%20vulnerable%20to%20noise%20than%20to%20brightness%20or%20contrast%0Aperturbations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00035v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness%2520Assessment%2520of%2520a%2520Runway%2520Object%2520Classifier%2520for%2520Safe%2520Aircraft%250A%2520%2520Taxiing%26entry.906535625%3DYizhak%2520Elboher%2520and%2520Raya%2520Elsaleh%2520and%2520Omri%2520Isac%2520and%2520M%25C3%25A9lanie%2520Ducoffe%2520and%2520Audrey%2520Galametz%2520and%2520Guillaume%2520Pov%25C3%25A9da%2520and%2520Ryma%2520Boumazouza%2520and%2520No%25C3%25A9mie%2520Cohen%2520and%2520Guy%2520Katz%26entry.1292438233%3D%2520%2520As%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520becoming%2520the%2520prominent%2520solution%2520for%2520many%250Acomputational%2520problems%252C%2520the%2520aviation%2520industry%2520seeks%2520to%2520explore%2520their%2520potential%250Ain%2520alleviating%2520pilot%2520workload%2520and%2520in%2520improving%2520operational%2520safety.%2520However%252C%2520the%250Ause%2520of%2520DNNs%2520in%2520this%2520type%2520of%2520safety-critical%2520applications%2520requires%2520a%2520thorough%250Acertification%2520process.%2520This%2520need%2520can%2520be%2520addressed%2520through%2520formal%2520verification%252C%250Awhich%2520provides%2520rigorous%2520assurances%2520--%2520e.g.%252C~by%2520proving%2520the%2520absence%2520of%2520certain%250Amispredictions.%2520In%2520this%2520case-study%2520paper%252C%2520we%2520demonstrate%2520this%2520process%2520using%2520an%250Aimage-classifier%2520DNN%2520currently%2520under%2520development%2520at%2520Airbus%2520and%2520intended%2520for%2520use%250Aduring%2520the%2520aircraft%2520taxiing%2520phase.%2520We%2520use%2520formal%2520methods%2520to%2520assess%2520this%2520DNN%2527s%250Arobustness%2520to%2520three%2520common%2520image%2520perturbation%2520types%253A%2520noise%252C%2520brightness%2520and%250Acontrast%252C%2520and%2520some%2520of%2520their%2520combinations.%2520This%2520process%2520entails%2520multiple%250Ainvocations%2520of%2520the%2520underlying%2520verifier%252C%2520which%2520might%2520be%2520computationally%250Aexpensive%253B%2520and%2520we%2520therefore%2520propose%2520a%2520method%2520that%2520leverages%2520the%2520monotonicity%2520of%250Athese%2520robustness%2520properties%252C%2520as%2520well%2520as%2520the%2520results%2520of%2520past%2520verification%250Aqueries%252C%2520in%2520order%2520to%2520reduce%2520the%2520overall%2520number%2520of%2520verification%2520queries%2520required%250Aby%2520nearly%252060%2525.%2520Our%2520results%2520provide%2520an%2520indication%2520of%2520the%2520level%2520of%2520robustness%250Aachieved%2520by%2520the%2520DNN%2520classifier%2520under%2520study%252C%2520and%2520indicate%2520that%2520it%2520is%250Aconsiderably%2520more%2520vulnerable%2520to%2520noise%2520than%2520to%2520brightness%2520or%2520contrast%250Aperturbations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00035v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness%20Assessment%20of%20a%20Runway%20Object%20Classifier%20for%20Safe%20Aircraft%0A%20%20Taxiing&entry.906535625=Yizhak%20Elboher%20and%20Raya%20Elsaleh%20and%20Omri%20Isac%20and%20M%C3%A9lanie%20Ducoffe%20and%20Audrey%20Galametz%20and%20Guillaume%20Pov%C3%A9da%20and%20Ryma%20Boumazouza%20and%20No%C3%A9mie%20Cohen%20and%20Guy%20Katz&entry.1292438233=%20%20As%20deep%20neural%20networks%20%28DNNs%29%20are%20becoming%20the%20prominent%20solution%20for%20many%0Acomputational%20problems%2C%20the%20aviation%20industry%20seeks%20to%20explore%20their%20potential%0Ain%20alleviating%20pilot%20workload%20and%20in%20improving%20operational%20safety.%20However%2C%20the%0Ause%20of%20DNNs%20in%20this%20type%20of%20safety-critical%20applications%20requires%20a%20thorough%0Acertification%20process.%20This%20need%20can%20be%20addressed%20through%20formal%20verification%2C%0Awhich%20provides%20rigorous%20assurances%20--%20e.g.%2C~by%20proving%20the%20absence%20of%20certain%0Amispredictions.%20In%20this%20case-study%20paper%2C%20we%20demonstrate%20this%20process%20using%20an%0Aimage-classifier%20DNN%20currently%20under%20development%20at%20Airbus%20and%20intended%20for%20use%0Aduring%20the%20aircraft%20taxiing%20phase.%20We%20use%20formal%20methods%20to%20assess%20this%20DNN%27s%0Arobustness%20to%20three%20common%20image%20perturbation%20types%3A%20noise%2C%20brightness%20and%0Acontrast%2C%20and%20some%20of%20their%20combinations.%20This%20process%20entails%20multiple%0Ainvocations%20of%20the%20underlying%20verifier%2C%20which%20might%20be%20computationally%0Aexpensive%3B%20and%20we%20therefore%20propose%20a%20method%20that%20leverages%20the%20monotonicity%20of%0Athese%20robustness%20properties%2C%20as%20well%20as%20the%20results%20of%20past%20verification%0Aqueries%2C%20in%20order%20to%20reduce%20the%20overall%20number%20of%20verification%20queries%20required%0Aby%20nearly%2060%25.%20Our%20results%20provide%20an%20indication%20of%20the%20level%20of%20robustness%0Aachieved%20by%20the%20DNN%20classifier%20under%20study%2C%20and%20indicate%20that%20it%20is%0Aconsiderably%20more%20vulnerable%20to%20noise%20than%20to%20brightness%20or%20contrast%0Aperturbations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00035v4&entry.124074799=Read"},
{"title": "Closing the gap between SVRG and TD-SVRG with Gradient Splitting", "author": "Arsenii Mustafin and Alex Olshevsky and Ioannis Ch. Paschalidis", "abstract": "  Temporal difference (TD) learning is a policy evaluation in reinforcement\nlearning whose performance can be enhanced by variance reduction methods.\nRecently, multiple works have sought to fuse TD learning with Stochastic\nVariance Reduced Gradient (SVRG) method to achieve a geometric rate of\nconvergence. However, the resulting convergence rate is significantly weaker\nthan what is achieved by SVRG in the setting of convex optimization. In this\nwork we utilize a recent interpretation of TD-learning as the splitting of the\ngradient of an appropriately chosen function, thus simplifying the algorithm\nand fusing TD with SVRG. Our main result is a geometric convergence bound with\npredetermined learning rate of $1/8$, which is identical to the convergence\nbound available for SVRG in the convex setting. Our theoretical findings are\nsupported by a set of experiments.\n", "link": "http://arxiv.org/abs/2211.16237v4", "date": "2024-08-06", "relevancy": 2.0157, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.513}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5005}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20gap%20between%20SVRG%20and%20TD-SVRG%20with%20Gradient%20Splitting&body=Title%3A%20Closing%20the%20gap%20between%20SVRG%20and%20TD-SVRG%20with%20Gradient%20Splitting%0AAuthor%3A%20Arsenii%20Mustafin%20and%20Alex%20Olshevsky%20and%20Ioannis%20Ch.%20Paschalidis%0AAbstract%3A%20%20%20Temporal%20difference%20%28TD%29%20learning%20is%20a%20policy%20evaluation%20in%20reinforcement%0Alearning%20whose%20performance%20can%20be%20enhanced%20by%20variance%20reduction%20methods.%0ARecently%2C%20multiple%20works%20have%20sought%20to%20fuse%20TD%20learning%20with%20Stochastic%0AVariance%20Reduced%20Gradient%20%28SVRG%29%20method%20to%20achieve%20a%20geometric%20rate%20of%0Aconvergence.%20However%2C%20the%20resulting%20convergence%20rate%20is%20significantly%20weaker%0Athan%20what%20is%20achieved%20by%20SVRG%20in%20the%20setting%20of%20convex%20optimization.%20In%20this%0Awork%20we%20utilize%20a%20recent%20interpretation%20of%20TD-learning%20as%20the%20splitting%20of%20the%0Agradient%20of%20an%20appropriately%20chosen%20function%2C%20thus%20simplifying%20the%20algorithm%0Aand%20fusing%20TD%20with%20SVRG.%20Our%20main%20result%20is%20a%20geometric%20convergence%20bound%20with%0Apredetermined%20learning%20rate%20of%20%241/8%24%2C%20which%20is%20identical%20to%20the%20convergence%0Abound%20available%20for%20SVRG%20in%20the%20convex%20setting.%20Our%20theoretical%20findings%20are%0Asupported%20by%20a%20set%20of%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.16237v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosing%2520the%2520gap%2520between%2520SVRG%2520and%2520TD-SVRG%2520with%2520Gradient%2520Splitting%26entry.906535625%3DArsenii%2520Mustafin%2520and%2520Alex%2520Olshevsky%2520and%2520Ioannis%2520Ch.%2520Paschalidis%26entry.1292438233%3D%2520%2520Temporal%2520difference%2520%2528TD%2529%2520learning%2520is%2520a%2520policy%2520evaluation%2520in%2520reinforcement%250Alearning%2520whose%2520performance%2520can%2520be%2520enhanced%2520by%2520variance%2520reduction%2520methods.%250ARecently%252C%2520multiple%2520works%2520have%2520sought%2520to%2520fuse%2520TD%2520learning%2520with%2520Stochastic%250AVariance%2520Reduced%2520Gradient%2520%2528SVRG%2529%2520method%2520to%2520achieve%2520a%2520geometric%2520rate%2520of%250Aconvergence.%2520However%252C%2520the%2520resulting%2520convergence%2520rate%2520is%2520significantly%2520weaker%250Athan%2520what%2520is%2520achieved%2520by%2520SVRG%2520in%2520the%2520setting%2520of%2520convex%2520optimization.%2520In%2520this%250Awork%2520we%2520utilize%2520a%2520recent%2520interpretation%2520of%2520TD-learning%2520as%2520the%2520splitting%2520of%2520the%250Agradient%2520of%2520an%2520appropriately%2520chosen%2520function%252C%2520thus%2520simplifying%2520the%2520algorithm%250Aand%2520fusing%2520TD%2520with%2520SVRG.%2520Our%2520main%2520result%2520is%2520a%2520geometric%2520convergence%2520bound%2520with%250Apredetermined%2520learning%2520rate%2520of%2520%25241/8%2524%252C%2520which%2520is%2520identical%2520to%2520the%2520convergence%250Abound%2520available%2520for%2520SVRG%2520in%2520the%2520convex%2520setting.%2520Our%2520theoretical%2520findings%2520are%250Asupported%2520by%2520a%2520set%2520of%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.16237v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20gap%20between%20SVRG%20and%20TD-SVRG%20with%20Gradient%20Splitting&entry.906535625=Arsenii%20Mustafin%20and%20Alex%20Olshevsky%20and%20Ioannis%20Ch.%20Paschalidis&entry.1292438233=%20%20Temporal%20difference%20%28TD%29%20learning%20is%20a%20policy%20evaluation%20in%20reinforcement%0Alearning%20whose%20performance%20can%20be%20enhanced%20by%20variance%20reduction%20methods.%0ARecently%2C%20multiple%20works%20have%20sought%20to%20fuse%20TD%20learning%20with%20Stochastic%0AVariance%20Reduced%20Gradient%20%28SVRG%29%20method%20to%20achieve%20a%20geometric%20rate%20of%0Aconvergence.%20However%2C%20the%20resulting%20convergence%20rate%20is%20significantly%20weaker%0Athan%20what%20is%20achieved%20by%20SVRG%20in%20the%20setting%20of%20convex%20optimization.%20In%20this%0Awork%20we%20utilize%20a%20recent%20interpretation%20of%20TD-learning%20as%20the%20splitting%20of%20the%0Agradient%20of%20an%20appropriately%20chosen%20function%2C%20thus%20simplifying%20the%20algorithm%0Aand%20fusing%20TD%20with%20SVRG.%20Our%20main%20result%20is%20a%20geometric%20convergence%20bound%20with%0Apredetermined%20learning%20rate%20of%20%241/8%24%2C%20which%20is%20identical%20to%20the%20convergence%0Abound%20available%20for%20SVRG%20in%20the%20convex%20setting.%20Our%20theoretical%20findings%20are%0Asupported%20by%20a%20set%20of%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.16237v4&entry.124074799=Read"},
{"title": "Adaptive-Sliding Mode Trajectory Control of Robot Manipulators with\n  Uncertainties", "author": "Mustafa M. Mustafa and Carl D. Crane and Ibrahim Hamarash", "abstract": "  In this paper, we propose and demonstrate an adaptive-sliding mode control\nfor trajectory tracking control of robot manipulators subjected to uncertain\ndynamics, vibration disturbance, and payload variation disturbance. Throughout\nthis work we seek a controller that is, robust to the uncertainty and\ndisturbance, accurate, and implementable. To perform these requirements, we use\na nonlinear Lyapunov-based approach for designing the controller and\nguaranteeing its stability. MATLAB-SIMULINK software is used to validate the\napproach and demonstrate the performance of the controller. Simulation results\nshow that the derived controller is stable, robust to the disturbance and\nuncertainties, accurate, and implementable.\n", "link": "http://arxiv.org/abs/2408.03102v1", "date": "2024-08-06", "relevancy": 2.0066, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5298}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5046}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive-Sliding%20Mode%20Trajectory%20Control%20of%20Robot%20Manipulators%20with%0A%20%20Uncertainties&body=Title%3A%20Adaptive-Sliding%20Mode%20Trajectory%20Control%20of%20Robot%20Manipulators%20with%0A%20%20Uncertainties%0AAuthor%3A%20Mustafa%20M.%20Mustafa%20and%20Carl%20D.%20Crane%20and%20Ibrahim%20Hamarash%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20and%20demonstrate%20an%20adaptive-sliding%20mode%20control%0Afor%20trajectory%20tracking%20control%20of%20robot%20manipulators%20subjected%20to%20uncertain%0Adynamics%2C%20vibration%20disturbance%2C%20and%20payload%20variation%20disturbance.%20Throughout%0Athis%20work%20we%20seek%20a%20controller%20that%20is%2C%20robust%20to%20the%20uncertainty%20and%0Adisturbance%2C%20accurate%2C%20and%20implementable.%20To%20perform%20these%20requirements%2C%20we%20use%0Aa%20nonlinear%20Lyapunov-based%20approach%20for%20designing%20the%20controller%20and%0Aguaranteeing%20its%20stability.%20MATLAB-SIMULINK%20software%20is%20used%20to%20validate%20the%0Aapproach%20and%20demonstrate%20the%20performance%20of%20the%20controller.%20Simulation%20results%0Ashow%20that%20the%20derived%20controller%20is%20stable%2C%20robust%20to%20the%20disturbance%20and%0Auncertainties%2C%20accurate%2C%20and%20implementable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive-Sliding%2520Mode%2520Trajectory%2520Control%2520of%2520Robot%2520Manipulators%2520with%250A%2520%2520Uncertainties%26entry.906535625%3DMustafa%2520M.%2520Mustafa%2520and%2520Carl%2520D.%2520Crane%2520and%2520Ibrahim%2520Hamarash%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520and%2520demonstrate%2520an%2520adaptive-sliding%2520mode%2520control%250Afor%2520trajectory%2520tracking%2520control%2520of%2520robot%2520manipulators%2520subjected%2520to%2520uncertain%250Adynamics%252C%2520vibration%2520disturbance%252C%2520and%2520payload%2520variation%2520disturbance.%2520Throughout%250Athis%2520work%2520we%2520seek%2520a%2520controller%2520that%2520is%252C%2520robust%2520to%2520the%2520uncertainty%2520and%250Adisturbance%252C%2520accurate%252C%2520and%2520implementable.%2520To%2520perform%2520these%2520requirements%252C%2520we%2520use%250Aa%2520nonlinear%2520Lyapunov-based%2520approach%2520for%2520designing%2520the%2520controller%2520and%250Aguaranteeing%2520its%2520stability.%2520MATLAB-SIMULINK%2520software%2520is%2520used%2520to%2520validate%2520the%250Aapproach%2520and%2520demonstrate%2520the%2520performance%2520of%2520the%2520controller.%2520Simulation%2520results%250Ashow%2520that%2520the%2520derived%2520controller%2520is%2520stable%252C%2520robust%2520to%2520the%2520disturbance%2520and%250Auncertainties%252C%2520accurate%252C%2520and%2520implementable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive-Sliding%20Mode%20Trajectory%20Control%20of%20Robot%20Manipulators%20with%0A%20%20Uncertainties&entry.906535625=Mustafa%20M.%20Mustafa%20and%20Carl%20D.%20Crane%20and%20Ibrahim%20Hamarash&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20and%20demonstrate%20an%20adaptive-sliding%20mode%20control%0Afor%20trajectory%20tracking%20control%20of%20robot%20manipulators%20subjected%20to%20uncertain%0Adynamics%2C%20vibration%20disturbance%2C%20and%20payload%20variation%20disturbance.%20Throughout%0Athis%20work%20we%20seek%20a%20controller%20that%20is%2C%20robust%20to%20the%20uncertainty%20and%0Adisturbance%2C%20accurate%2C%20and%20implementable.%20To%20perform%20these%20requirements%2C%20we%20use%0Aa%20nonlinear%20Lyapunov-based%20approach%20for%20designing%20the%20controller%20and%0Aguaranteeing%20its%20stability.%20MATLAB-SIMULINK%20software%20is%20used%20to%20validate%20the%0Aapproach%20and%20demonstrate%20the%20performance%20of%20the%20controller.%20Simulation%20results%0Ashow%20that%20the%20derived%20controller%20is%20stable%2C%20robust%20to%20the%20disturbance%20and%0Auncertainties%2C%20accurate%2C%20and%20implementable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03102v1&entry.124074799=Read"},
{"title": "Unveiling LLMs: The Evolution of Latent Representations in a Dynamic\n  Knowledge Graph", "author": "Marco Bronzini and Carlo Nicolini and Bruno Lepri and Jacopo Staiano and Andrea Passerini", "abstract": "  Large Language Models (LLMs) demonstrate an impressive capacity to recall a\nvast range of factual knowledge. However, understanding their underlying\nreasoning and internal mechanisms in exploiting this knowledge remains a key\nresearch area. This work unveils the factual information an LLM represents\ninternally for sentence-level claim verification. We propose an end-to-end\nframework to decode factual knowledge embedded in token representations from a\nvector space to a set of ground predicates, showing its layer-wise evolution\nusing a dynamic knowledge graph. Our framework employs activation patching, a\nvector-level technique that alters a token representation during inference, to\nextract encoded knowledge. Accordingly, we neither rely on training nor\nexternal models. Using factual and common-sense claims from two claim\nverification datasets, we showcase interpretability analyses at local and\nglobal levels. The local analysis highlights entity centrality in LLM\nreasoning, from claim-related information and multi-hop reasoning to\nrepresentation errors causing erroneous evaluation. On the other hand, the\nglobal reveals trends in the underlying evolution, such as word-based knowledge\nevolving into claim-related facts. By interpreting semantics from LLM latent\nrepresentations and enabling graph-related analyses, this work enhances the\nunderstanding of the factual knowledge resolution process.\n", "link": "http://arxiv.org/abs/2404.03623v2", "date": "2024-08-06", "relevancy": 1.9996, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.518}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4996}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20LLMs%3A%20The%20Evolution%20of%20Latent%20Representations%20in%20a%20Dynamic%0A%20%20Knowledge%20Graph&body=Title%3A%20Unveiling%20LLMs%3A%20The%20Evolution%20of%20Latent%20Representations%20in%20a%20Dynamic%0A%20%20Knowledge%20Graph%0AAuthor%3A%20Marco%20Bronzini%20and%20Carlo%20Nicolini%20and%20Bruno%20Lepri%20and%20Jacopo%20Staiano%20and%20Andrea%20Passerini%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20an%20impressive%20capacity%20to%20recall%20a%0Avast%20range%20of%20factual%20knowledge.%20However%2C%20understanding%20their%20underlying%0Areasoning%20and%20internal%20mechanisms%20in%20exploiting%20this%20knowledge%20remains%20a%20key%0Aresearch%20area.%20This%20work%20unveils%20the%20factual%20information%20an%20LLM%20represents%0Ainternally%20for%20sentence-level%20claim%20verification.%20We%20propose%20an%20end-to-end%0Aframework%20to%20decode%20factual%20knowledge%20embedded%20in%20token%20representations%20from%20a%0Avector%20space%20to%20a%20set%20of%20ground%20predicates%2C%20showing%20its%20layer-wise%20evolution%0Ausing%20a%20dynamic%20knowledge%20graph.%20Our%20framework%20employs%20activation%20patching%2C%20a%0Avector-level%20technique%20that%20alters%20a%20token%20representation%20during%20inference%2C%20to%0Aextract%20encoded%20knowledge.%20Accordingly%2C%20we%20neither%20rely%20on%20training%20nor%0Aexternal%20models.%20Using%20factual%20and%20common-sense%20claims%20from%20two%20claim%0Averification%20datasets%2C%20we%20showcase%20interpretability%20analyses%20at%20local%20and%0Aglobal%20levels.%20The%20local%20analysis%20highlights%20entity%20centrality%20in%20LLM%0Areasoning%2C%20from%20claim-related%20information%20and%20multi-hop%20reasoning%20to%0Arepresentation%20errors%20causing%20erroneous%20evaluation.%20On%20the%20other%20hand%2C%20the%0Aglobal%20reveals%20trends%20in%20the%20underlying%20evolution%2C%20such%20as%20word-based%20knowledge%0Aevolving%20into%20claim-related%20facts.%20By%20interpreting%20semantics%20from%20LLM%20latent%0Arepresentations%20and%20enabling%20graph-related%20analyses%2C%20this%20work%20enhances%20the%0Aunderstanding%20of%20the%20factual%20knowledge%20resolution%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03623v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520LLMs%253A%2520The%2520Evolution%2520of%2520Latent%2520Representations%2520in%2520a%2520Dynamic%250A%2520%2520Knowledge%2520Graph%26entry.906535625%3DMarco%2520Bronzini%2520and%2520Carlo%2520Nicolini%2520and%2520Bruno%2520Lepri%2520and%2520Jacopo%2520Staiano%2520and%2520Andrea%2520Passerini%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520an%2520impressive%2520capacity%2520to%2520recall%2520a%250Avast%2520range%2520of%2520factual%2520knowledge.%2520However%252C%2520understanding%2520their%2520underlying%250Areasoning%2520and%2520internal%2520mechanisms%2520in%2520exploiting%2520this%2520knowledge%2520remains%2520a%2520key%250Aresearch%2520area.%2520This%2520work%2520unveils%2520the%2520factual%2520information%2520an%2520LLM%2520represents%250Ainternally%2520for%2520sentence-level%2520claim%2520verification.%2520We%2520propose%2520an%2520end-to-end%250Aframework%2520to%2520decode%2520factual%2520knowledge%2520embedded%2520in%2520token%2520representations%2520from%2520a%250Avector%2520space%2520to%2520a%2520set%2520of%2520ground%2520predicates%252C%2520showing%2520its%2520layer-wise%2520evolution%250Ausing%2520a%2520dynamic%2520knowledge%2520graph.%2520Our%2520framework%2520employs%2520activation%2520patching%252C%2520a%250Avector-level%2520technique%2520that%2520alters%2520a%2520token%2520representation%2520during%2520inference%252C%2520to%250Aextract%2520encoded%2520knowledge.%2520Accordingly%252C%2520we%2520neither%2520rely%2520on%2520training%2520nor%250Aexternal%2520models.%2520Using%2520factual%2520and%2520common-sense%2520claims%2520from%2520two%2520claim%250Averification%2520datasets%252C%2520we%2520showcase%2520interpretability%2520analyses%2520at%2520local%2520and%250Aglobal%2520levels.%2520The%2520local%2520analysis%2520highlights%2520entity%2520centrality%2520in%2520LLM%250Areasoning%252C%2520from%2520claim-related%2520information%2520and%2520multi-hop%2520reasoning%2520to%250Arepresentation%2520errors%2520causing%2520erroneous%2520evaluation.%2520On%2520the%2520other%2520hand%252C%2520the%250Aglobal%2520reveals%2520trends%2520in%2520the%2520underlying%2520evolution%252C%2520such%2520as%2520word-based%2520knowledge%250Aevolving%2520into%2520claim-related%2520facts.%2520By%2520interpreting%2520semantics%2520from%2520LLM%2520latent%250Arepresentations%2520and%2520enabling%2520graph-related%2520analyses%252C%2520this%2520work%2520enhances%2520the%250Aunderstanding%2520of%2520the%2520factual%2520knowledge%2520resolution%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03623v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20LLMs%3A%20The%20Evolution%20of%20Latent%20Representations%20in%20a%20Dynamic%0A%20%20Knowledge%20Graph&entry.906535625=Marco%20Bronzini%20and%20Carlo%20Nicolini%20and%20Bruno%20Lepri%20and%20Jacopo%20Staiano%20and%20Andrea%20Passerini&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20an%20impressive%20capacity%20to%20recall%20a%0Avast%20range%20of%20factual%20knowledge.%20However%2C%20understanding%20their%20underlying%0Areasoning%20and%20internal%20mechanisms%20in%20exploiting%20this%20knowledge%20remains%20a%20key%0Aresearch%20area.%20This%20work%20unveils%20the%20factual%20information%20an%20LLM%20represents%0Ainternally%20for%20sentence-level%20claim%20verification.%20We%20propose%20an%20end-to-end%0Aframework%20to%20decode%20factual%20knowledge%20embedded%20in%20token%20representations%20from%20a%0Avector%20space%20to%20a%20set%20of%20ground%20predicates%2C%20showing%20its%20layer-wise%20evolution%0Ausing%20a%20dynamic%20knowledge%20graph.%20Our%20framework%20employs%20activation%20patching%2C%20a%0Avector-level%20technique%20that%20alters%20a%20token%20representation%20during%20inference%2C%20to%0Aextract%20encoded%20knowledge.%20Accordingly%2C%20we%20neither%20rely%20on%20training%20nor%0Aexternal%20models.%20Using%20factual%20and%20common-sense%20claims%20from%20two%20claim%0Averification%20datasets%2C%20we%20showcase%20interpretability%20analyses%20at%20local%20and%0Aglobal%20levels.%20The%20local%20analysis%20highlights%20entity%20centrality%20in%20LLM%0Areasoning%2C%20from%20claim-related%20information%20and%20multi-hop%20reasoning%20to%0Arepresentation%20errors%20causing%20erroneous%20evaluation.%20On%20the%20other%20hand%2C%20the%0Aglobal%20reveals%20trends%20in%20the%20underlying%20evolution%2C%20such%20as%20word-based%20knowledge%0Aevolving%20into%20claim-related%20facts.%20By%20interpreting%20semantics%20from%20LLM%20latent%0Arepresentations%20and%20enabling%20graph-related%20analyses%2C%20this%20work%20enhances%20the%0Aunderstanding%20of%20the%20factual%20knowledge%20resolution%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03623v2&entry.124074799=Read"},
{"title": "Preservation of Feature Stability in Machine Learning Under Data\n  Uncertainty for Decision Support in Critical Domains", "author": "Karol Capa\u0142a and Paulina Tworek and Jose Sousa", "abstract": "  In a world where Machine Learning (ML) is increasingly deployed to support\ndecision-making in critical domains, providing decision-makers with\nexplainable, stable, and relevant inputs becomes fundamental. Understanding how\nmachine learning works under missing data and how this affects feature\nvariability is paramount. This is even more relevant as machine learning\napproaches focus on standardising decision-making approaches that rely on an\nidealised set of features. However, decision-making in human activities often\nrelies on incomplete data, even in critical domains. This paper addresses this\ngap by conducting a set of experiments using traditional machine learning\nmethods that look for optimal decisions in comparison to a recently deployed\nmachine learning method focused on a classification that is more descriptive\nand mimics human decision making, allowing for the natural integration of\nexplainability. We found that the ML descriptive approach maintains higher\nclassification accuracy while ensuring the stability of feature selection as\ndata incompleteness increases. This suggests that descriptive classification\nmethods can be helpful in uncertain decision-making scenarios.\n", "link": "http://arxiv.org/abs/2401.11044v2", "date": "2024-08-06", "relevancy": 1.9987, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5494}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4938}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preservation%20of%20Feature%20Stability%20in%20Machine%20Learning%20Under%20Data%0A%20%20Uncertainty%20for%20Decision%20Support%20in%20Critical%20Domains&body=Title%3A%20Preservation%20of%20Feature%20Stability%20in%20Machine%20Learning%20Under%20Data%0A%20%20Uncertainty%20for%20Decision%20Support%20in%20Critical%20Domains%0AAuthor%3A%20Karol%20Capa%C5%82a%20and%20Paulina%20Tworek%20and%20Jose%20Sousa%0AAbstract%3A%20%20%20In%20a%20world%20where%20Machine%20Learning%20%28ML%29%20is%20increasingly%20deployed%20to%20support%0Adecision-making%20in%20critical%20domains%2C%20providing%20decision-makers%20with%0Aexplainable%2C%20stable%2C%20and%20relevant%20inputs%20becomes%20fundamental.%20Understanding%20how%0Amachine%20learning%20works%20under%20missing%20data%20and%20how%20this%20affects%20feature%0Avariability%20is%20paramount.%20This%20is%20even%20more%20relevant%20as%20machine%20learning%0Aapproaches%20focus%20on%20standardising%20decision-making%20approaches%20that%20rely%20on%20an%0Aidealised%20set%20of%20features.%20However%2C%20decision-making%20in%20human%20activities%20often%0Arelies%20on%20incomplete%20data%2C%20even%20in%20critical%20domains.%20This%20paper%20addresses%20this%0Agap%20by%20conducting%20a%20set%20of%20experiments%20using%20traditional%20machine%20learning%0Amethods%20that%20look%20for%20optimal%20decisions%20in%20comparison%20to%20a%20recently%20deployed%0Amachine%20learning%20method%20focused%20on%20a%20classification%20that%20is%20more%20descriptive%0Aand%20mimics%20human%20decision%20making%2C%20allowing%20for%20the%20natural%20integration%20of%0Aexplainability.%20We%20found%20that%20the%20ML%20descriptive%20approach%20maintains%20higher%0Aclassification%20accuracy%20while%20ensuring%20the%20stability%20of%20feature%20selection%20as%0Adata%20incompleteness%20increases.%20This%20suggests%20that%20descriptive%20classification%0Amethods%20can%20be%20helpful%20in%20uncertain%20decision-making%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11044v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreservation%2520of%2520Feature%2520Stability%2520in%2520Machine%2520Learning%2520Under%2520Data%250A%2520%2520Uncertainty%2520for%2520Decision%2520Support%2520in%2520Critical%2520Domains%26entry.906535625%3DKarol%2520Capa%25C5%2582a%2520and%2520Paulina%2520Tworek%2520and%2520Jose%2520Sousa%26entry.1292438233%3D%2520%2520In%2520a%2520world%2520where%2520Machine%2520Learning%2520%2528ML%2529%2520is%2520increasingly%2520deployed%2520to%2520support%250Adecision-making%2520in%2520critical%2520domains%252C%2520providing%2520decision-makers%2520with%250Aexplainable%252C%2520stable%252C%2520and%2520relevant%2520inputs%2520becomes%2520fundamental.%2520Understanding%2520how%250Amachine%2520learning%2520works%2520under%2520missing%2520data%2520and%2520how%2520this%2520affects%2520feature%250Avariability%2520is%2520paramount.%2520This%2520is%2520even%2520more%2520relevant%2520as%2520machine%2520learning%250Aapproaches%2520focus%2520on%2520standardising%2520decision-making%2520approaches%2520that%2520rely%2520on%2520an%250Aidealised%2520set%2520of%2520features.%2520However%252C%2520decision-making%2520in%2520human%2520activities%2520often%250Arelies%2520on%2520incomplete%2520data%252C%2520even%2520in%2520critical%2520domains.%2520This%2520paper%2520addresses%2520this%250Agap%2520by%2520conducting%2520a%2520set%2520of%2520experiments%2520using%2520traditional%2520machine%2520learning%250Amethods%2520that%2520look%2520for%2520optimal%2520decisions%2520in%2520comparison%2520to%2520a%2520recently%2520deployed%250Amachine%2520learning%2520method%2520focused%2520on%2520a%2520classification%2520that%2520is%2520more%2520descriptive%250Aand%2520mimics%2520human%2520decision%2520making%252C%2520allowing%2520for%2520the%2520natural%2520integration%2520of%250Aexplainability.%2520We%2520found%2520that%2520the%2520ML%2520descriptive%2520approach%2520maintains%2520higher%250Aclassification%2520accuracy%2520while%2520ensuring%2520the%2520stability%2520of%2520feature%2520selection%2520as%250Adata%2520incompleteness%2520increases.%2520This%2520suggests%2520that%2520descriptive%2520classification%250Amethods%2520can%2520be%2520helpful%2520in%2520uncertain%2520decision-making%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11044v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preservation%20of%20Feature%20Stability%20in%20Machine%20Learning%20Under%20Data%0A%20%20Uncertainty%20for%20Decision%20Support%20in%20Critical%20Domains&entry.906535625=Karol%20Capa%C5%82a%20and%20Paulina%20Tworek%20and%20Jose%20Sousa&entry.1292438233=%20%20In%20a%20world%20where%20Machine%20Learning%20%28ML%29%20is%20increasingly%20deployed%20to%20support%0Adecision-making%20in%20critical%20domains%2C%20providing%20decision-makers%20with%0Aexplainable%2C%20stable%2C%20and%20relevant%20inputs%20becomes%20fundamental.%20Understanding%20how%0Amachine%20learning%20works%20under%20missing%20data%20and%20how%20this%20affects%20feature%0Avariability%20is%20paramount.%20This%20is%20even%20more%20relevant%20as%20machine%20learning%0Aapproaches%20focus%20on%20standardising%20decision-making%20approaches%20that%20rely%20on%20an%0Aidealised%20set%20of%20features.%20However%2C%20decision-making%20in%20human%20activities%20often%0Arelies%20on%20incomplete%20data%2C%20even%20in%20critical%20domains.%20This%20paper%20addresses%20this%0Agap%20by%20conducting%20a%20set%20of%20experiments%20using%20traditional%20machine%20learning%0Amethods%20that%20look%20for%20optimal%20decisions%20in%20comparison%20to%20a%20recently%20deployed%0Amachine%20learning%20method%20focused%20on%20a%20classification%20that%20is%20more%20descriptive%0Aand%20mimics%20human%20decision%20making%2C%20allowing%20for%20the%20natural%20integration%20of%0Aexplainability.%20We%20found%20that%20the%20ML%20descriptive%20approach%20maintains%20higher%0Aclassification%20accuracy%20while%20ensuring%20the%20stability%20of%20feature%20selection%20as%0Adata%20incompleteness%20increases.%20This%20suggests%20that%20descriptive%20classification%0Amethods%20can%20be%20helpful%20in%20uncertain%20decision-making%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11044v2&entry.124074799=Read"},
{"title": "FedBAT: Communication-Efficient Federated Learning via Learnable\n  Binarization", "author": "Shiwei Li and Wenchao Xu and Haozhao Wang and Xing Tang and Yining Qi and Shijie Xu and Weihong Luo and Yuhua Li and Xiuqiang He and Ruixuan Li", "abstract": "  Federated learning is a promising distributed machine learning paradigm that\ncan effectively exploit large-scale data without exposing users' privacy.\nHowever, it may incur significant communication overhead, thereby potentially\nimpairing the training efficiency. To address this challenge, numerous studies\nsuggest binarizing the model updates. Nonetheless, traditional methods usually\nbinarize model updates in a post-training manner, resulting in significant\napproximation errors and consequent degradation in model accuracy. To this end,\nwe propose Federated Binarization-Aware Training (FedBAT), a novel framework\nthat directly learns binary model updates during the local training process,\nthus inherently reducing the approximation errors. FedBAT incorporates an\ninnovative binarization operator, along with meticulously designed derivatives\nto facilitate efficient learning. In addition, we establish theoretical\nguarantees regarding the convergence of FedBAT. Extensive experiments are\nconducted on four popular datasets. The results show that FedBAT significantly\naccelerates the convergence and exceeds the accuracy of baselines by up to 9\\%,\neven surpassing that of FedAvg in some cases.\n", "link": "http://arxiv.org/abs/2408.03215v1", "date": "2024-08-06", "relevancy": 1.9768, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5112}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4821}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedBAT%3A%20Communication-Efficient%20Federated%20Learning%20via%20Learnable%0A%20%20Binarization&body=Title%3A%20FedBAT%3A%20Communication-Efficient%20Federated%20Learning%20via%20Learnable%0A%20%20Binarization%0AAuthor%3A%20Shiwei%20Li%20and%20Wenchao%20Xu%20and%20Haozhao%20Wang%20and%20Xing%20Tang%20and%20Yining%20Qi%20and%20Shijie%20Xu%20and%20Weihong%20Luo%20and%20Yuhua%20Li%20and%20Xiuqiang%20He%20and%20Ruixuan%20Li%0AAbstract%3A%20%20%20Federated%20learning%20is%20a%20promising%20distributed%20machine%20learning%20paradigm%20that%0Acan%20effectively%20exploit%20large-scale%20data%20without%20exposing%20users%27%20privacy.%0AHowever%2C%20it%20may%20incur%20significant%20communication%20overhead%2C%20thereby%20potentially%0Aimpairing%20the%20training%20efficiency.%20To%20address%20this%20challenge%2C%20numerous%20studies%0Asuggest%20binarizing%20the%20model%20updates.%20Nonetheless%2C%20traditional%20methods%20usually%0Abinarize%20model%20updates%20in%20a%20post-training%20manner%2C%20resulting%20in%20significant%0Aapproximation%20errors%20and%20consequent%20degradation%20in%20model%20accuracy.%20To%20this%20end%2C%0Awe%20propose%20Federated%20Binarization-Aware%20Training%20%28FedBAT%29%2C%20a%20novel%20framework%0Athat%20directly%20learns%20binary%20model%20updates%20during%20the%20local%20training%20process%2C%0Athus%20inherently%20reducing%20the%20approximation%20errors.%20FedBAT%20incorporates%20an%0Ainnovative%20binarization%20operator%2C%20along%20with%20meticulously%20designed%20derivatives%0Ato%20facilitate%20efficient%20learning.%20In%20addition%2C%20we%20establish%20theoretical%0Aguarantees%20regarding%20the%20convergence%20of%20FedBAT.%20Extensive%20experiments%20are%0Aconducted%20on%20four%20popular%20datasets.%20The%20results%20show%20that%20FedBAT%20significantly%0Aaccelerates%20the%20convergence%20and%20exceeds%20the%20accuracy%20of%20baselines%20by%20up%20to%209%5C%25%2C%0Aeven%20surpassing%20that%20of%20FedAvg%20in%20some%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedBAT%253A%2520Communication-Efficient%2520Federated%2520Learning%2520via%2520Learnable%250A%2520%2520Binarization%26entry.906535625%3DShiwei%2520Li%2520and%2520Wenchao%2520Xu%2520and%2520Haozhao%2520Wang%2520and%2520Xing%2520Tang%2520and%2520Yining%2520Qi%2520and%2520Shijie%2520Xu%2520and%2520Weihong%2520Luo%2520and%2520Yuhua%2520Li%2520and%2520Xiuqiang%2520He%2520and%2520Ruixuan%2520Li%26entry.1292438233%3D%2520%2520Federated%2520learning%2520is%2520a%2520promising%2520distributed%2520machine%2520learning%2520paradigm%2520that%250Acan%2520effectively%2520exploit%2520large-scale%2520data%2520without%2520exposing%2520users%2527%2520privacy.%250AHowever%252C%2520it%2520may%2520incur%2520significant%2520communication%2520overhead%252C%2520thereby%2520potentially%250Aimpairing%2520the%2520training%2520efficiency.%2520To%2520address%2520this%2520challenge%252C%2520numerous%2520studies%250Asuggest%2520binarizing%2520the%2520model%2520updates.%2520Nonetheless%252C%2520traditional%2520methods%2520usually%250Abinarize%2520model%2520updates%2520in%2520a%2520post-training%2520manner%252C%2520resulting%2520in%2520significant%250Aapproximation%2520errors%2520and%2520consequent%2520degradation%2520in%2520model%2520accuracy.%2520To%2520this%2520end%252C%250Awe%2520propose%2520Federated%2520Binarization-Aware%2520Training%2520%2528FedBAT%2529%252C%2520a%2520novel%2520framework%250Athat%2520directly%2520learns%2520binary%2520model%2520updates%2520during%2520the%2520local%2520training%2520process%252C%250Athus%2520inherently%2520reducing%2520the%2520approximation%2520errors.%2520FedBAT%2520incorporates%2520an%250Ainnovative%2520binarization%2520operator%252C%2520along%2520with%2520meticulously%2520designed%2520derivatives%250Ato%2520facilitate%2520efficient%2520learning.%2520In%2520addition%252C%2520we%2520establish%2520theoretical%250Aguarantees%2520regarding%2520the%2520convergence%2520of%2520FedBAT.%2520Extensive%2520experiments%2520are%250Aconducted%2520on%2520four%2520popular%2520datasets.%2520The%2520results%2520show%2520that%2520FedBAT%2520significantly%250Aaccelerates%2520the%2520convergence%2520and%2520exceeds%2520the%2520accuracy%2520of%2520baselines%2520by%2520up%2520to%25209%255C%2525%252C%250Aeven%2520surpassing%2520that%2520of%2520FedAvg%2520in%2520some%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedBAT%3A%20Communication-Efficient%20Federated%20Learning%20via%20Learnable%0A%20%20Binarization&entry.906535625=Shiwei%20Li%20and%20Wenchao%20Xu%20and%20Haozhao%20Wang%20and%20Xing%20Tang%20and%20Yining%20Qi%20and%20Shijie%20Xu%20and%20Weihong%20Luo%20and%20Yuhua%20Li%20and%20Xiuqiang%20He%20and%20Ruixuan%20Li&entry.1292438233=%20%20Federated%20learning%20is%20a%20promising%20distributed%20machine%20learning%20paradigm%20that%0Acan%20effectively%20exploit%20large-scale%20data%20without%20exposing%20users%27%20privacy.%0AHowever%2C%20it%20may%20incur%20significant%20communication%20overhead%2C%20thereby%20potentially%0Aimpairing%20the%20training%20efficiency.%20To%20address%20this%20challenge%2C%20numerous%20studies%0Asuggest%20binarizing%20the%20model%20updates.%20Nonetheless%2C%20traditional%20methods%20usually%0Abinarize%20model%20updates%20in%20a%20post-training%20manner%2C%20resulting%20in%20significant%0Aapproximation%20errors%20and%20consequent%20degradation%20in%20model%20accuracy.%20To%20this%20end%2C%0Awe%20propose%20Federated%20Binarization-Aware%20Training%20%28FedBAT%29%2C%20a%20novel%20framework%0Athat%20directly%20learns%20binary%20model%20updates%20during%20the%20local%20training%20process%2C%0Athus%20inherently%20reducing%20the%20approximation%20errors.%20FedBAT%20incorporates%20an%0Ainnovative%20binarization%20operator%2C%20along%20with%20meticulously%20designed%20derivatives%0Ato%20facilitate%20efficient%20learning.%20In%20addition%2C%20we%20establish%20theoretical%0Aguarantees%20regarding%20the%20convergence%20of%20FedBAT.%20Extensive%20experiments%20are%0Aconducted%20on%20four%20popular%20datasets.%20The%20results%20show%20that%20FedBAT%20significantly%0Aaccelerates%20the%20convergence%20and%20exceeds%20the%20accuracy%20of%20baselines%20by%20up%20to%209%5C%25%2C%0Aeven%20surpassing%20that%20of%20FedAvg%20in%20some%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03215v1&entry.124074799=Read"},
{"title": "Dedicated Nonlinear Control of Robot Manipulators in the Presence of\n  External Vibration and Uncertain Payload", "author": "Mustafa M. Mustafa and Carl D. Crane and Ibrahim Hamarash", "abstract": "  Robot manipulators are often tasked with working in environments with\nvibrations and are subject to load uncertainty. Providing an accurate tracking\ncontrol design with implementable torque input for these robots is a complex\ntopic. This paper presents two approaches to solve this problem. The approaches\nconsider joint space tracking control design in the presence of nonlinear\nuncertain torques caused by external vibration and payload variation. The\nproperties of the uncertain torques are used in both approaches. The first\napproach is based on the boundedness property, while the second approach\nconsiders the differentiability and boundedness together. The controllers\nderived from each approach differ from the perspectives of accuracy, control\neffort, and disturbance properties. A Lyapunov-based analysis is utilized to\nguarantee the stability of the control design in each case. Simulation results\nvalidate the approaches and demonstrate the performance of the controllers. The\nderived controllers show stable results at the cost of the mentioned\nproperties.\n", "link": "http://arxiv.org/abs/2408.03098v1", "date": "2024-08-06", "relevancy": 1.9752, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5221}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4961}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dedicated%20Nonlinear%20Control%20of%20Robot%20Manipulators%20in%20the%20Presence%20of%0A%20%20External%20Vibration%20and%20Uncertain%20Payload&body=Title%3A%20Dedicated%20Nonlinear%20Control%20of%20Robot%20Manipulators%20in%20the%20Presence%20of%0A%20%20External%20Vibration%20and%20Uncertain%20Payload%0AAuthor%3A%20Mustafa%20M.%20Mustafa%20and%20Carl%20D.%20Crane%20and%20Ibrahim%20Hamarash%0AAbstract%3A%20%20%20Robot%20manipulators%20are%20often%20tasked%20with%20working%20in%20environments%20with%0Avibrations%20and%20are%20subject%20to%20load%20uncertainty.%20Providing%20an%20accurate%20tracking%0Acontrol%20design%20with%20implementable%20torque%20input%20for%20these%20robots%20is%20a%20complex%0Atopic.%20This%20paper%20presents%20two%20approaches%20to%20solve%20this%20problem.%20The%20approaches%0Aconsider%20joint%20space%20tracking%20control%20design%20in%20the%20presence%20of%20nonlinear%0Auncertain%20torques%20caused%20by%20external%20vibration%20and%20payload%20variation.%20The%0Aproperties%20of%20the%20uncertain%20torques%20are%20used%20in%20both%20approaches.%20The%20first%0Aapproach%20is%20based%20on%20the%20boundedness%20property%2C%20while%20the%20second%20approach%0Aconsiders%20the%20differentiability%20and%20boundedness%20together.%20The%20controllers%0Aderived%20from%20each%20approach%20differ%20from%20the%20perspectives%20of%20accuracy%2C%20control%0Aeffort%2C%20and%20disturbance%20properties.%20A%20Lyapunov-based%20analysis%20is%20utilized%20to%0Aguarantee%20the%20stability%20of%20the%20control%20design%20in%20each%20case.%20Simulation%20results%0Avalidate%20the%20approaches%20and%20demonstrate%20the%20performance%20of%20the%20controllers.%20The%0Aderived%20controllers%20show%20stable%20results%20at%20the%20cost%20of%20the%20mentioned%0Aproperties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDedicated%2520Nonlinear%2520Control%2520of%2520Robot%2520Manipulators%2520in%2520the%2520Presence%2520of%250A%2520%2520External%2520Vibration%2520and%2520Uncertain%2520Payload%26entry.906535625%3DMustafa%2520M.%2520Mustafa%2520and%2520Carl%2520D.%2520Crane%2520and%2520Ibrahim%2520Hamarash%26entry.1292438233%3D%2520%2520Robot%2520manipulators%2520are%2520often%2520tasked%2520with%2520working%2520in%2520environments%2520with%250Avibrations%2520and%2520are%2520subject%2520to%2520load%2520uncertainty.%2520Providing%2520an%2520accurate%2520tracking%250Acontrol%2520design%2520with%2520implementable%2520torque%2520input%2520for%2520these%2520robots%2520is%2520a%2520complex%250Atopic.%2520This%2520paper%2520presents%2520two%2520approaches%2520to%2520solve%2520this%2520problem.%2520The%2520approaches%250Aconsider%2520joint%2520space%2520tracking%2520control%2520design%2520in%2520the%2520presence%2520of%2520nonlinear%250Auncertain%2520torques%2520caused%2520by%2520external%2520vibration%2520and%2520payload%2520variation.%2520The%250Aproperties%2520of%2520the%2520uncertain%2520torques%2520are%2520used%2520in%2520both%2520approaches.%2520The%2520first%250Aapproach%2520is%2520based%2520on%2520the%2520boundedness%2520property%252C%2520while%2520the%2520second%2520approach%250Aconsiders%2520the%2520differentiability%2520and%2520boundedness%2520together.%2520The%2520controllers%250Aderived%2520from%2520each%2520approach%2520differ%2520from%2520the%2520perspectives%2520of%2520accuracy%252C%2520control%250Aeffort%252C%2520and%2520disturbance%2520properties.%2520A%2520Lyapunov-based%2520analysis%2520is%2520utilized%2520to%250Aguarantee%2520the%2520stability%2520of%2520the%2520control%2520design%2520in%2520each%2520case.%2520Simulation%2520results%250Avalidate%2520the%2520approaches%2520and%2520demonstrate%2520the%2520performance%2520of%2520the%2520controllers.%2520The%250Aderived%2520controllers%2520show%2520stable%2520results%2520at%2520the%2520cost%2520of%2520the%2520mentioned%250Aproperties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dedicated%20Nonlinear%20Control%20of%20Robot%20Manipulators%20in%20the%20Presence%20of%0A%20%20External%20Vibration%20and%20Uncertain%20Payload&entry.906535625=Mustafa%20M.%20Mustafa%20and%20Carl%20D.%20Crane%20and%20Ibrahim%20Hamarash&entry.1292438233=%20%20Robot%20manipulators%20are%20often%20tasked%20with%20working%20in%20environments%20with%0Avibrations%20and%20are%20subject%20to%20load%20uncertainty.%20Providing%20an%20accurate%20tracking%0Acontrol%20design%20with%20implementable%20torque%20input%20for%20these%20robots%20is%20a%20complex%0Atopic.%20This%20paper%20presents%20two%20approaches%20to%20solve%20this%20problem.%20The%20approaches%0Aconsider%20joint%20space%20tracking%20control%20design%20in%20the%20presence%20of%20nonlinear%0Auncertain%20torques%20caused%20by%20external%20vibration%20and%20payload%20variation.%20The%0Aproperties%20of%20the%20uncertain%20torques%20are%20used%20in%20both%20approaches.%20The%20first%0Aapproach%20is%20based%20on%20the%20boundedness%20property%2C%20while%20the%20second%20approach%0Aconsiders%20the%20differentiability%20and%20boundedness%20together.%20The%20controllers%0Aderived%20from%20each%20approach%20differ%20from%20the%20perspectives%20of%20accuracy%2C%20control%0Aeffort%2C%20and%20disturbance%20properties.%20A%20Lyapunov-based%20analysis%20is%20utilized%20to%0Aguarantee%20the%20stability%20of%20the%20control%20design%20in%20each%20case.%20Simulation%20results%0Avalidate%20the%20approaches%20and%20demonstrate%20the%20performance%20of%20the%20controllers.%20The%0Aderived%20controllers%20show%20stable%20results%20at%20the%20cost%20of%20the%20mentioned%0Aproperties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03098v1&entry.124074799=Read"},
{"title": "Comprehensive Attribution: Inherently Explainable Vision Model with\n  Feature Detector", "author": "Xianren Zhang and Dongwon Lee and Suhang Wang", "abstract": "  As deep vision models' popularity rapidly increases, there is a growing\nemphasis on explanations for model predictions. The inherently explainable\nattribution method aims to enhance the understanding of model behavior by\nidentifying the important regions in images that significantly contribute to\npredictions. It is achieved by cooperatively training a selector (generating an\nattribution map to identify important features) and a predictor (making\npredictions using the identified features). Despite many advancements, existing\nmethods suffer from the incompleteness problem, where discriminative features\nare masked out, and the interlocking problem, where the non-optimized selector\ninitially selects noise, causing the predictor to fit on this noise and\nperpetuate the cycle. To address these problems, we introduce a new objective\nthat discourages the presence of discriminative features in the masked-out\nregions thus enhancing the comprehensiveness of feature selection. A\npre-trained detector is introduced to detect discriminative features in the\nmasked-out region. If the selector selects noise instead of discriminative\nfeatures, the detector can observe and break the interlocking situation by\npenalizing the selector. Extensive experiments show that our model makes\naccurate predictions with higher accuracy than the regular black-box model, and\nproduces attribution maps with high feature coverage, localization ability,\nfidelity and robustness. Our code will be available at\n\\href{https://github.com/Zood123/COMET}{https://github.com/Zood123/COMET}.\n", "link": "http://arxiv.org/abs/2407.19308v2", "date": "2024-08-06", "relevancy": 1.962, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5083}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.489}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Attribution%3A%20Inherently%20Explainable%20Vision%20Model%20with%0A%20%20Feature%20Detector&body=Title%3A%20Comprehensive%20Attribution%3A%20Inherently%20Explainable%20Vision%20Model%20with%0A%20%20Feature%20Detector%0AAuthor%3A%20Xianren%20Zhang%20and%20Dongwon%20Lee%20and%20Suhang%20Wang%0AAbstract%3A%20%20%20As%20deep%20vision%20models%27%20popularity%20rapidly%20increases%2C%20there%20is%20a%20growing%0Aemphasis%20on%20explanations%20for%20model%20predictions.%20The%20inherently%20explainable%0Aattribution%20method%20aims%20to%20enhance%20the%20understanding%20of%20model%20behavior%20by%0Aidentifying%20the%20important%20regions%20in%20images%20that%20significantly%20contribute%20to%0Apredictions.%20It%20is%20achieved%20by%20cooperatively%20training%20a%20selector%20%28generating%20an%0Aattribution%20map%20to%20identify%20important%20features%29%20and%20a%20predictor%20%28making%0Apredictions%20using%20the%20identified%20features%29.%20Despite%20many%20advancements%2C%20existing%0Amethods%20suffer%20from%20the%20incompleteness%20problem%2C%20where%20discriminative%20features%0Aare%20masked%20out%2C%20and%20the%20interlocking%20problem%2C%20where%20the%20non-optimized%20selector%0Ainitially%20selects%20noise%2C%20causing%20the%20predictor%20to%20fit%20on%20this%20noise%20and%0Aperpetuate%20the%20cycle.%20To%20address%20these%20problems%2C%20we%20introduce%20a%20new%20objective%0Athat%20discourages%20the%20presence%20of%20discriminative%20features%20in%20the%20masked-out%0Aregions%20thus%20enhancing%20the%20comprehensiveness%20of%20feature%20selection.%20A%0Apre-trained%20detector%20is%20introduced%20to%20detect%20discriminative%20features%20in%20the%0Amasked-out%20region.%20If%20the%20selector%20selects%20noise%20instead%20of%20discriminative%0Afeatures%2C%20the%20detector%20can%20observe%20and%20break%20the%20interlocking%20situation%20by%0Apenalizing%20the%20selector.%20Extensive%20experiments%20show%20that%20our%20model%20makes%0Aaccurate%20predictions%20with%20higher%20accuracy%20than%20the%20regular%20black-box%20model%2C%20and%0Aproduces%20attribution%20maps%20with%20high%20feature%20coverage%2C%20localization%20ability%2C%0Afidelity%20and%20robustness.%20Our%20code%20will%20be%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Zood123/COMET%7D%7Bhttps%3A//github.com/Zood123/COMET%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19308v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Attribution%253A%2520Inherently%2520Explainable%2520Vision%2520Model%2520with%250A%2520%2520Feature%2520Detector%26entry.906535625%3DXianren%2520Zhang%2520and%2520Dongwon%2520Lee%2520and%2520Suhang%2520Wang%26entry.1292438233%3D%2520%2520As%2520deep%2520vision%2520models%2527%2520popularity%2520rapidly%2520increases%252C%2520there%2520is%2520a%2520growing%250Aemphasis%2520on%2520explanations%2520for%2520model%2520predictions.%2520The%2520inherently%2520explainable%250Aattribution%2520method%2520aims%2520to%2520enhance%2520the%2520understanding%2520of%2520model%2520behavior%2520by%250Aidentifying%2520the%2520important%2520regions%2520in%2520images%2520that%2520significantly%2520contribute%2520to%250Apredictions.%2520It%2520is%2520achieved%2520by%2520cooperatively%2520training%2520a%2520selector%2520%2528generating%2520an%250Aattribution%2520map%2520to%2520identify%2520important%2520features%2529%2520and%2520a%2520predictor%2520%2528making%250Apredictions%2520using%2520the%2520identified%2520features%2529.%2520Despite%2520many%2520advancements%252C%2520existing%250Amethods%2520suffer%2520from%2520the%2520incompleteness%2520problem%252C%2520where%2520discriminative%2520features%250Aare%2520masked%2520out%252C%2520and%2520the%2520interlocking%2520problem%252C%2520where%2520the%2520non-optimized%2520selector%250Ainitially%2520selects%2520noise%252C%2520causing%2520the%2520predictor%2520to%2520fit%2520on%2520this%2520noise%2520and%250Aperpetuate%2520the%2520cycle.%2520To%2520address%2520these%2520problems%252C%2520we%2520introduce%2520a%2520new%2520objective%250Athat%2520discourages%2520the%2520presence%2520of%2520discriminative%2520features%2520in%2520the%2520masked-out%250Aregions%2520thus%2520enhancing%2520the%2520comprehensiveness%2520of%2520feature%2520selection.%2520A%250Apre-trained%2520detector%2520is%2520introduced%2520to%2520detect%2520discriminative%2520features%2520in%2520the%250Amasked-out%2520region.%2520If%2520the%2520selector%2520selects%2520noise%2520instead%2520of%2520discriminative%250Afeatures%252C%2520the%2520detector%2520can%2520observe%2520and%2520break%2520the%2520interlocking%2520situation%2520by%250Apenalizing%2520the%2520selector.%2520Extensive%2520experiments%2520show%2520that%2520our%2520model%2520makes%250Aaccurate%2520predictions%2520with%2520higher%2520accuracy%2520than%2520the%2520regular%2520black-box%2520model%252C%2520and%250Aproduces%2520attribution%2520maps%2520with%2520high%2520feature%2520coverage%252C%2520localization%2520ability%252C%250Afidelity%2520and%2520robustness.%2520Our%2520code%2520will%2520be%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/Zood123/COMET%257D%257Bhttps%253A//github.com/Zood123/COMET%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19308v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Attribution%3A%20Inherently%20Explainable%20Vision%20Model%20with%0A%20%20Feature%20Detector&entry.906535625=Xianren%20Zhang%20and%20Dongwon%20Lee%20and%20Suhang%20Wang&entry.1292438233=%20%20As%20deep%20vision%20models%27%20popularity%20rapidly%20increases%2C%20there%20is%20a%20growing%0Aemphasis%20on%20explanations%20for%20model%20predictions.%20The%20inherently%20explainable%0Aattribution%20method%20aims%20to%20enhance%20the%20understanding%20of%20model%20behavior%20by%0Aidentifying%20the%20important%20regions%20in%20images%20that%20significantly%20contribute%20to%0Apredictions.%20It%20is%20achieved%20by%20cooperatively%20training%20a%20selector%20%28generating%20an%0Aattribution%20map%20to%20identify%20important%20features%29%20and%20a%20predictor%20%28making%0Apredictions%20using%20the%20identified%20features%29.%20Despite%20many%20advancements%2C%20existing%0Amethods%20suffer%20from%20the%20incompleteness%20problem%2C%20where%20discriminative%20features%0Aare%20masked%20out%2C%20and%20the%20interlocking%20problem%2C%20where%20the%20non-optimized%20selector%0Ainitially%20selects%20noise%2C%20causing%20the%20predictor%20to%20fit%20on%20this%20noise%20and%0Aperpetuate%20the%20cycle.%20To%20address%20these%20problems%2C%20we%20introduce%20a%20new%20objective%0Athat%20discourages%20the%20presence%20of%20discriminative%20features%20in%20the%20masked-out%0Aregions%20thus%20enhancing%20the%20comprehensiveness%20of%20feature%20selection.%20A%0Apre-trained%20detector%20is%20introduced%20to%20detect%20discriminative%20features%20in%20the%0Amasked-out%20region.%20If%20the%20selector%20selects%20noise%20instead%20of%20discriminative%0Afeatures%2C%20the%20detector%20can%20observe%20and%20break%20the%20interlocking%20situation%20by%0Apenalizing%20the%20selector.%20Extensive%20experiments%20show%20that%20our%20model%20makes%0Aaccurate%20predictions%20with%20higher%20accuracy%20than%20the%20regular%20black-box%20model%2C%20and%0Aproduces%20attribution%20maps%20with%20high%20feature%20coverage%2C%20localization%20ability%2C%0Afidelity%20and%20robustness.%20Our%20code%20will%20be%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Zood123/COMET%7D%7Bhttps%3A//github.com/Zood123/COMET%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19308v2&entry.124074799=Read"},
{"title": "ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural\n  Networks", "author": "Muhammad Kashif and Muhammad Shafique", "abstract": "  In this paper, we present a novel framework for enhancing the performance of\nQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional\nlayers and addressing the critical challenges associated with them. Traditional\nquanvolutional layers, although beneficial for feature extraction, have largely\nbeen static, offering limited adaptability. Unlike state-of-the-art, our\nresearch overcomes this limitation by enabling training within these layers,\nsignificantly increasing the flexibility and potential of QuNNs. However, the\nintroduction of multiple trainable quanvolutional layers induces complexities\nin gradient-based optimization, primarily due to the difficulty in accessing\ngradients across these layers. To resolve this, we propose a novel\narchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging\nthe concept of residual learning, which facilitates the flow of gradients by\nadding skip connections between layers. By inserting residual blocks between\nquanvolutional layers, we ensure enhanced gradient access throughout the\nnetwork, leading to improved training performance. Moreover, we provide\nempirical evidence on the strategic placement of these residual blocks within\nQuNNs. Through extensive experimentation, we identify an efficient\nconfiguration of residual blocks, which enables gradients across all the layers\nin the network that eventually results in efficient training. Our findings\nsuggest that the precise location of residual blocks plays a crucial role in\nmaximizing the performance gains in QuNNs. Our results mark a substantial step\nforward in the evolution of quantum deep learning, offering new avenues for\nboth theoretical development and practical quantum computing applications.\n", "link": "http://arxiv.org/abs/2402.09146v4", "date": "2024-08-06", "relevancy": 1.9581, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.515}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4797}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ResQuNNs%3ATowards%20Enabling%20Deep%20Learning%20in%20Quantum%20Convolution%20Neural%0A%20%20Networks&body=Title%3A%20ResQuNNs%3ATowards%20Enabling%20Deep%20Learning%20in%20Quantum%20Convolution%20Neural%0A%20%20Networks%0AAuthor%3A%20Muhammad%20Kashif%20and%20Muhammad%20Shafique%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%20for%20enhancing%20the%20performance%20of%0AQuanvolutional%20Neural%20Networks%20%28QuNNs%29%20by%20introducing%20trainable%20quanvolutional%0Alayers%20and%20addressing%20the%20critical%20challenges%20associated%20with%20them.%20Traditional%0Aquanvolutional%20layers%2C%20although%20beneficial%20for%20feature%20extraction%2C%20have%20largely%0Abeen%20static%2C%20offering%20limited%20adaptability.%20Unlike%20state-of-the-art%2C%20our%0Aresearch%20overcomes%20this%20limitation%20by%20enabling%20training%20within%20these%20layers%2C%0Asignificantly%20increasing%20the%20flexibility%20and%20potential%20of%20QuNNs.%20However%2C%20the%0Aintroduction%20of%20multiple%20trainable%20quanvolutional%20layers%20induces%20complexities%0Ain%20gradient-based%20optimization%2C%20primarily%20due%20to%20the%20difficulty%20in%20accessing%0Agradients%20across%20these%20layers.%20To%20resolve%20this%2C%20we%20propose%20a%20novel%0Aarchitecture%2C%20Residual%20Quanvolutional%20Neural%20Networks%20%28ResQuNNs%29%2C%20leveraging%0Athe%20concept%20of%20residual%20learning%2C%20which%20facilitates%20the%20flow%20of%20gradients%20by%0Aadding%20skip%20connections%20between%20layers.%20By%20inserting%20residual%20blocks%20between%0Aquanvolutional%20layers%2C%20we%20ensure%20enhanced%20gradient%20access%20throughout%20the%0Anetwork%2C%20leading%20to%20improved%20training%20performance.%20Moreover%2C%20we%20provide%0Aempirical%20evidence%20on%20the%20strategic%20placement%20of%20these%20residual%20blocks%20within%0AQuNNs.%20Through%20extensive%20experimentation%2C%20we%20identify%20an%20efficient%0Aconfiguration%20of%20residual%20blocks%2C%20which%20enables%20gradients%20across%20all%20the%20layers%0Ain%20the%20network%20that%20eventually%20results%20in%20efficient%20training.%20Our%20findings%0Asuggest%20that%20the%20precise%20location%20of%20residual%20blocks%20plays%20a%20crucial%20role%20in%0Amaximizing%20the%20performance%20gains%20in%20QuNNs.%20Our%20results%20mark%20a%20substantial%20step%0Aforward%20in%20the%20evolution%20of%20quantum%20deep%20learning%2C%20offering%20new%20avenues%20for%0Aboth%20theoretical%20development%20and%20practical%20quantum%20computing%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09146v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResQuNNs%253ATowards%2520Enabling%2520Deep%2520Learning%2520in%2520Quantum%2520Convolution%2520Neural%250A%2520%2520Networks%26entry.906535625%3DMuhammad%2520Kashif%2520and%2520Muhammad%2520Shafique%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520framework%2520for%2520enhancing%2520the%2520performance%2520of%250AQuanvolutional%2520Neural%2520Networks%2520%2528QuNNs%2529%2520by%2520introducing%2520trainable%2520quanvolutional%250Alayers%2520and%2520addressing%2520the%2520critical%2520challenges%2520associated%2520with%2520them.%2520Traditional%250Aquanvolutional%2520layers%252C%2520although%2520beneficial%2520for%2520feature%2520extraction%252C%2520have%2520largely%250Abeen%2520static%252C%2520offering%2520limited%2520adaptability.%2520Unlike%2520state-of-the-art%252C%2520our%250Aresearch%2520overcomes%2520this%2520limitation%2520by%2520enabling%2520training%2520within%2520these%2520layers%252C%250Asignificantly%2520increasing%2520the%2520flexibility%2520and%2520potential%2520of%2520QuNNs.%2520However%252C%2520the%250Aintroduction%2520of%2520multiple%2520trainable%2520quanvolutional%2520layers%2520induces%2520complexities%250Ain%2520gradient-based%2520optimization%252C%2520primarily%2520due%2520to%2520the%2520difficulty%2520in%2520accessing%250Agradients%2520across%2520these%2520layers.%2520To%2520resolve%2520this%252C%2520we%2520propose%2520a%2520novel%250Aarchitecture%252C%2520Residual%2520Quanvolutional%2520Neural%2520Networks%2520%2528ResQuNNs%2529%252C%2520leveraging%250Athe%2520concept%2520of%2520residual%2520learning%252C%2520which%2520facilitates%2520the%2520flow%2520of%2520gradients%2520by%250Aadding%2520skip%2520connections%2520between%2520layers.%2520By%2520inserting%2520residual%2520blocks%2520between%250Aquanvolutional%2520layers%252C%2520we%2520ensure%2520enhanced%2520gradient%2520access%2520throughout%2520the%250Anetwork%252C%2520leading%2520to%2520improved%2520training%2520performance.%2520Moreover%252C%2520we%2520provide%250Aempirical%2520evidence%2520on%2520the%2520strategic%2520placement%2520of%2520these%2520residual%2520blocks%2520within%250AQuNNs.%2520Through%2520extensive%2520experimentation%252C%2520we%2520identify%2520an%2520efficient%250Aconfiguration%2520of%2520residual%2520blocks%252C%2520which%2520enables%2520gradients%2520across%2520all%2520the%2520layers%250Ain%2520the%2520network%2520that%2520eventually%2520results%2520in%2520efficient%2520training.%2520Our%2520findings%250Asuggest%2520that%2520the%2520precise%2520location%2520of%2520residual%2520blocks%2520plays%2520a%2520crucial%2520role%2520in%250Amaximizing%2520the%2520performance%2520gains%2520in%2520QuNNs.%2520Our%2520results%2520mark%2520a%2520substantial%2520step%250Aforward%2520in%2520the%2520evolution%2520of%2520quantum%2520deep%2520learning%252C%2520offering%2520new%2520avenues%2520for%250Aboth%2520theoretical%2520development%2520and%2520practical%2520quantum%2520computing%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09146v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResQuNNs%3ATowards%20Enabling%20Deep%20Learning%20in%20Quantum%20Convolution%20Neural%0A%20%20Networks&entry.906535625=Muhammad%20Kashif%20and%20Muhammad%20Shafique&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%20for%20enhancing%20the%20performance%20of%0AQuanvolutional%20Neural%20Networks%20%28QuNNs%29%20by%20introducing%20trainable%20quanvolutional%0Alayers%20and%20addressing%20the%20critical%20challenges%20associated%20with%20them.%20Traditional%0Aquanvolutional%20layers%2C%20although%20beneficial%20for%20feature%20extraction%2C%20have%20largely%0Abeen%20static%2C%20offering%20limited%20adaptability.%20Unlike%20state-of-the-art%2C%20our%0Aresearch%20overcomes%20this%20limitation%20by%20enabling%20training%20within%20these%20layers%2C%0Asignificantly%20increasing%20the%20flexibility%20and%20potential%20of%20QuNNs.%20However%2C%20the%0Aintroduction%20of%20multiple%20trainable%20quanvolutional%20layers%20induces%20complexities%0Ain%20gradient-based%20optimization%2C%20primarily%20due%20to%20the%20difficulty%20in%20accessing%0Agradients%20across%20these%20layers.%20To%20resolve%20this%2C%20we%20propose%20a%20novel%0Aarchitecture%2C%20Residual%20Quanvolutional%20Neural%20Networks%20%28ResQuNNs%29%2C%20leveraging%0Athe%20concept%20of%20residual%20learning%2C%20which%20facilitates%20the%20flow%20of%20gradients%20by%0Aadding%20skip%20connections%20between%20layers.%20By%20inserting%20residual%20blocks%20between%0Aquanvolutional%20layers%2C%20we%20ensure%20enhanced%20gradient%20access%20throughout%20the%0Anetwork%2C%20leading%20to%20improved%20training%20performance.%20Moreover%2C%20we%20provide%0Aempirical%20evidence%20on%20the%20strategic%20placement%20of%20these%20residual%20blocks%20within%0AQuNNs.%20Through%20extensive%20experimentation%2C%20we%20identify%20an%20efficient%0Aconfiguration%20of%20residual%20blocks%2C%20which%20enables%20gradients%20across%20all%20the%20layers%0Ain%20the%20network%20that%20eventually%20results%20in%20efficient%20training.%20Our%20findings%0Asuggest%20that%20the%20precise%20location%20of%20residual%20blocks%20plays%20a%20crucial%20role%20in%0Amaximizing%20the%20performance%20gains%20in%20QuNNs.%20Our%20results%20mark%20a%20substantial%20step%0Aforward%20in%20the%20evolution%20of%20quantum%20deep%20learning%2C%20offering%20new%20avenues%20for%0Aboth%20theoretical%20development%20and%20practical%20quantum%20computing%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09146v4&entry.124074799=Read"},
{"title": "FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation", "author": "Xiang Gao and Jiaying Liu", "abstract": "  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nwonderful image generation with natural-language text prompt. However, the\nissue of lacking controllability of such models restricts their practical\napplicability for real-life content creation. Thus, attention has been focused\non leveraging a reference image to control text-to-image synthesis, which is\nalso regarded as manipulating (or editing) a reference image as per a text\nprompt, namely, text-driven image-to-image translation. This paper contributes\na novel, concise, and efficient approach that adapts pre-trained large-scale\ntext-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a\nplug-and-play manner, realizing high-quality and versatile text-driven I2I\ntranslation without any model training, model fine-tuning, or online\noptimization process. To guide T2I generation with a reference image, we\npropose to decompose diverse guiding factors with different frequency bands of\ndiffusion features in the DCT spectral space, and accordingly devise a novel\nfrequency band substitution layer which realizes dynamic control of the\nreference image to the T2I generation result in a plug-and-play manner. We\ndemonstrate that our method allows flexible control over both guiding factor\nand guiding intensity of the reference image simply by tuning the type and\nbandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability. The code is publicly available at:\nhttps://github.com/XiangGao1102/FBSDiff.\n", "link": "http://arxiv.org/abs/2408.00998v2", "date": "2024-08-06", "relevancy": 1.9483, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6666}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6457}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FBSDiff%3A%20Plug-and-Play%20Frequency%20Band%20Substitution%20of%20Diffusion%20Features%0A%20%20for%20Highly%20Controllable%20Text-Driven%20Image%20Translation&body=Title%3A%20FBSDiff%3A%20Plug-and-Play%20Frequency%20Band%20Substitution%20of%20Diffusion%20Features%0A%20%20for%20Highly%20Controllable%20Text-Driven%20Image%20Translation%0AAuthor%3A%20Xiang%20Gao%20and%20Jiaying%20Liu%0AAbstract%3A%20%20%20Large-scale%20text-to-image%20diffusion%20models%20have%20been%20a%20revolutionary%0Amilestone%20in%20the%20evolution%20of%20generative%20AI%20and%20multimodal%20technology%2C%20allowing%0Awonderful%20image%20generation%20with%20natural-language%20text%20prompt.%20However%2C%20the%0Aissue%20of%20lacking%20controllability%20of%20such%20models%20restricts%20their%20practical%0Aapplicability%20for%20real-life%20content%20creation.%20Thus%2C%20attention%20has%20been%20focused%0Aon%20leveraging%20a%20reference%20image%20to%20control%20text-to-image%20synthesis%2C%20which%20is%0Aalso%20regarded%20as%20manipulating%20%28or%20editing%29%20a%20reference%20image%20as%20per%20a%20text%0Aprompt%2C%20namely%2C%20text-driven%20image-to-image%20translation.%20This%20paper%20contributes%0Aa%20novel%2C%20concise%2C%20and%20efficient%20approach%20that%20adapts%20pre-trained%20large-scale%0Atext-to-image%20%28T2I%29%20diffusion%20model%20to%20the%20image-to-image%20%28I2I%29%20paradigm%20in%20a%0Aplug-and-play%20manner%2C%20realizing%20high-quality%20and%20versatile%20text-driven%20I2I%0Atranslation%20without%20any%20model%20training%2C%20model%20fine-tuning%2C%20or%20online%0Aoptimization%20process.%20To%20guide%20T2I%20generation%20with%20a%20reference%20image%2C%20we%0Apropose%20to%20decompose%20diverse%20guiding%20factors%20with%20different%20frequency%20bands%20of%0Adiffusion%20features%20in%20the%20DCT%20spectral%20space%2C%20and%20accordingly%20devise%20a%20novel%0Afrequency%20band%20substitution%20layer%20which%20realizes%20dynamic%20control%20of%20the%0Areference%20image%20to%20the%20T2I%20generation%20result%20in%20a%20plug-and-play%20manner.%20We%0Ademonstrate%20that%20our%20method%20allows%20flexible%20control%20over%20both%20guiding%20factor%0Aand%20guiding%20intensity%20of%20the%20reference%20image%20simply%20by%20tuning%20the%20type%20and%0Abandwidth%20of%20the%20substituted%20frequency%20band%2C%20respectively.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20verify%20superiority%20of%20our%20approach%0Aover%20related%20methods%20in%20I2I%20translation%20visual%20quality%2C%20versatility%2C%20and%0Acontrollability.%20The%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/XiangGao1102/FBSDiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00998v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFBSDiff%253A%2520Plug-and-Play%2520Frequency%2520Band%2520Substitution%2520of%2520Diffusion%2520Features%250A%2520%2520for%2520Highly%2520Controllable%2520Text-Driven%2520Image%2520Translation%26entry.906535625%3DXiang%2520Gao%2520and%2520Jiaying%2520Liu%26entry.1292438233%3D%2520%2520Large-scale%2520text-to-image%2520diffusion%2520models%2520have%2520been%2520a%2520revolutionary%250Amilestone%2520in%2520the%2520evolution%2520of%2520generative%2520AI%2520and%2520multimodal%2520technology%252C%2520allowing%250Awonderful%2520image%2520generation%2520with%2520natural-language%2520text%2520prompt.%2520However%252C%2520the%250Aissue%2520of%2520lacking%2520controllability%2520of%2520such%2520models%2520restricts%2520their%2520practical%250Aapplicability%2520for%2520real-life%2520content%2520creation.%2520Thus%252C%2520attention%2520has%2520been%2520focused%250Aon%2520leveraging%2520a%2520reference%2520image%2520to%2520control%2520text-to-image%2520synthesis%252C%2520which%2520is%250Aalso%2520regarded%2520as%2520manipulating%2520%2528or%2520editing%2529%2520a%2520reference%2520image%2520as%2520per%2520a%2520text%250Aprompt%252C%2520namely%252C%2520text-driven%2520image-to-image%2520translation.%2520This%2520paper%2520contributes%250Aa%2520novel%252C%2520concise%252C%2520and%2520efficient%2520approach%2520that%2520adapts%2520pre-trained%2520large-scale%250Atext-to-image%2520%2528T2I%2529%2520diffusion%2520model%2520to%2520the%2520image-to-image%2520%2528I2I%2529%2520paradigm%2520in%2520a%250Aplug-and-play%2520manner%252C%2520realizing%2520high-quality%2520and%2520versatile%2520text-driven%2520I2I%250Atranslation%2520without%2520any%2520model%2520training%252C%2520model%2520fine-tuning%252C%2520or%2520online%250Aoptimization%2520process.%2520To%2520guide%2520T2I%2520generation%2520with%2520a%2520reference%2520image%252C%2520we%250Apropose%2520to%2520decompose%2520diverse%2520guiding%2520factors%2520with%2520different%2520frequency%2520bands%2520of%250Adiffusion%2520features%2520in%2520the%2520DCT%2520spectral%2520space%252C%2520and%2520accordingly%2520devise%2520a%2520novel%250Afrequency%2520band%2520substitution%2520layer%2520which%2520realizes%2520dynamic%2520control%2520of%2520the%250Areference%2520image%2520to%2520the%2520T2I%2520generation%2520result%2520in%2520a%2520plug-and-play%2520manner.%2520We%250Ademonstrate%2520that%2520our%2520method%2520allows%2520flexible%2520control%2520over%2520both%2520guiding%2520factor%250Aand%2520guiding%2520intensity%2520of%2520the%2520reference%2520image%2520simply%2520by%2520tuning%2520the%2520type%2520and%250Abandwidth%2520of%2520the%2520substituted%2520frequency%2520band%252C%2520respectively.%2520Extensive%250Aqualitative%2520and%2520quantitative%2520experiments%2520verify%2520superiority%2520of%2520our%2520approach%250Aover%2520related%2520methods%2520in%2520I2I%2520translation%2520visual%2520quality%252C%2520versatility%252C%2520and%250Acontrollability.%2520The%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/XiangGao1102/FBSDiff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00998v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FBSDiff%3A%20Plug-and-Play%20Frequency%20Band%20Substitution%20of%20Diffusion%20Features%0A%20%20for%20Highly%20Controllable%20Text-Driven%20Image%20Translation&entry.906535625=Xiang%20Gao%20and%20Jiaying%20Liu&entry.1292438233=%20%20Large-scale%20text-to-image%20diffusion%20models%20have%20been%20a%20revolutionary%0Amilestone%20in%20the%20evolution%20of%20generative%20AI%20and%20multimodal%20technology%2C%20allowing%0Awonderful%20image%20generation%20with%20natural-language%20text%20prompt.%20However%2C%20the%0Aissue%20of%20lacking%20controllability%20of%20such%20models%20restricts%20their%20practical%0Aapplicability%20for%20real-life%20content%20creation.%20Thus%2C%20attention%20has%20been%20focused%0Aon%20leveraging%20a%20reference%20image%20to%20control%20text-to-image%20synthesis%2C%20which%20is%0Aalso%20regarded%20as%20manipulating%20%28or%20editing%29%20a%20reference%20image%20as%20per%20a%20text%0Aprompt%2C%20namely%2C%20text-driven%20image-to-image%20translation.%20This%20paper%20contributes%0Aa%20novel%2C%20concise%2C%20and%20efficient%20approach%20that%20adapts%20pre-trained%20large-scale%0Atext-to-image%20%28T2I%29%20diffusion%20model%20to%20the%20image-to-image%20%28I2I%29%20paradigm%20in%20a%0Aplug-and-play%20manner%2C%20realizing%20high-quality%20and%20versatile%20text-driven%20I2I%0Atranslation%20without%20any%20model%20training%2C%20model%20fine-tuning%2C%20or%20online%0Aoptimization%20process.%20To%20guide%20T2I%20generation%20with%20a%20reference%20image%2C%20we%0Apropose%20to%20decompose%20diverse%20guiding%20factors%20with%20different%20frequency%20bands%20of%0Adiffusion%20features%20in%20the%20DCT%20spectral%20space%2C%20and%20accordingly%20devise%20a%20novel%0Afrequency%20band%20substitution%20layer%20which%20realizes%20dynamic%20control%20of%20the%0Areference%20image%20to%20the%20T2I%20generation%20result%20in%20a%20plug-and-play%20manner.%20We%0Ademonstrate%20that%20our%20method%20allows%20flexible%20control%20over%20both%20guiding%20factor%0Aand%20guiding%20intensity%20of%20the%20reference%20image%20simply%20by%20tuning%20the%20type%20and%0Abandwidth%20of%20the%20substituted%20frequency%20band%2C%20respectively.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20verify%20superiority%20of%20our%20approach%0Aover%20related%20methods%20in%20I2I%20translation%20visual%20quality%2C%20versatility%2C%20and%0Acontrollability.%20The%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/XiangGao1102/FBSDiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00998v2&entry.124074799=Read"},
{"title": "Active Learning for Level Set Estimation Using Randomized Straddle\n  Algorithms", "author": "Yu Inatsu and Shion Takeno and Kentaro Kutsukake and Ichiro Takeuchi", "abstract": "  Level set estimation (LSE), the problem of identifying the set of input\npoints where a function takes value above (or below) a given threshold, is\nimportant in practical applications. When the function is expensive-to-evaluate\nand black-box, the \\textit{straddle} algorithm, which is a representative\nheuristic for LSE based on Gaussian process models, and its extensions having\ntheoretical guarantees have been developed. However, many of existing methods\ninclude a confidence parameter $\\beta^{1/2}_t$ that must be specified by the\nuser, and methods that choose $\\beta^{1/2}_t$ heuristically do not provide\ntheoretical guarantees. In contrast, theoretically guaranteed values of\n$\\beta^{1/2}_t$ need to be increased depending on the number of iterations and\ncandidate points, and are conservative and not good for practical performance.\nIn this study, we propose a novel method, the \\textit{randomized straddle}\nalgorithm, in which $\\beta_t$ in the straddle algorithm is replaced by a random\nsample from the chi-squared distribution with two degrees of freedom. The\nconfidence parameter in the proposed method has the advantages of not needing\nadjustment, not depending on the number of iterations and candidate points, and\nnot being conservative. Furthermore, we show that the proposed method has\ntheoretical guarantees that depend on the sample complexity and the number of\niterations. Finally, we confirm the usefulness of the proposed method through\nnumerical experiments using synthetic and real data.\n", "link": "http://arxiv.org/abs/2408.03144v1", "date": "2024-08-06", "relevancy": 1.9397, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5046}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.485}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20for%20Level%20Set%20Estimation%20Using%20Randomized%20Straddle%0A%20%20Algorithms&body=Title%3A%20Active%20Learning%20for%20Level%20Set%20Estimation%20Using%20Randomized%20Straddle%0A%20%20Algorithms%0AAuthor%3A%20Yu%20Inatsu%20and%20Shion%20Takeno%20and%20Kentaro%20Kutsukake%20and%20Ichiro%20Takeuchi%0AAbstract%3A%20%20%20Level%20set%20estimation%20%28LSE%29%2C%20the%20problem%20of%20identifying%20the%20set%20of%20input%0Apoints%20where%20a%20function%20takes%20value%20above%20%28or%20below%29%20a%20given%20threshold%2C%20is%0Aimportant%20in%20practical%20applications.%20When%20the%20function%20is%20expensive-to-evaluate%0Aand%20black-box%2C%20the%20%5Ctextit%7Bstraddle%7D%20algorithm%2C%20which%20is%20a%20representative%0Aheuristic%20for%20LSE%20based%20on%20Gaussian%20process%20models%2C%20and%20its%20extensions%20having%0Atheoretical%20guarantees%20have%20been%20developed.%20However%2C%20many%20of%20existing%20methods%0Ainclude%20a%20confidence%20parameter%20%24%5Cbeta%5E%7B1/2%7D_t%24%20that%20must%20be%20specified%20by%20the%0Auser%2C%20and%20methods%20that%20choose%20%24%5Cbeta%5E%7B1/2%7D_t%24%20heuristically%20do%20not%20provide%0Atheoretical%20guarantees.%20In%20contrast%2C%20theoretically%20guaranteed%20values%20of%0A%24%5Cbeta%5E%7B1/2%7D_t%24%20need%20to%20be%20increased%20depending%20on%20the%20number%20of%20iterations%20and%0Acandidate%20points%2C%20and%20are%20conservative%20and%20not%20good%20for%20practical%20performance.%0AIn%20this%20study%2C%20we%20propose%20a%20novel%20method%2C%20the%20%5Ctextit%7Brandomized%20straddle%7D%0Aalgorithm%2C%20in%20which%20%24%5Cbeta_t%24%20in%20the%20straddle%20algorithm%20is%20replaced%20by%20a%20random%0Asample%20from%20the%20chi-squared%20distribution%20with%20two%20degrees%20of%20freedom.%20The%0Aconfidence%20parameter%20in%20the%20proposed%20method%20has%20the%20advantages%20of%20not%20needing%0Aadjustment%2C%20not%20depending%20on%20the%20number%20of%20iterations%20and%20candidate%20points%2C%20and%0Anot%20being%20conservative.%20Furthermore%2C%20we%20show%20that%20the%20proposed%20method%20has%0Atheoretical%20guarantees%20that%20depend%20on%20the%20sample%20complexity%20and%20the%20number%20of%0Aiterations.%20Finally%2C%20we%20confirm%20the%20usefulness%20of%20the%20proposed%20method%20through%0Anumerical%20experiments%20using%20synthetic%20and%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520for%2520Level%2520Set%2520Estimation%2520Using%2520Randomized%2520Straddle%250A%2520%2520Algorithms%26entry.906535625%3DYu%2520Inatsu%2520and%2520Shion%2520Takeno%2520and%2520Kentaro%2520Kutsukake%2520and%2520Ichiro%2520Takeuchi%26entry.1292438233%3D%2520%2520Level%2520set%2520estimation%2520%2528LSE%2529%252C%2520the%2520problem%2520of%2520identifying%2520the%2520set%2520of%2520input%250Apoints%2520where%2520a%2520function%2520takes%2520value%2520above%2520%2528or%2520below%2529%2520a%2520given%2520threshold%252C%2520is%250Aimportant%2520in%2520practical%2520applications.%2520When%2520the%2520function%2520is%2520expensive-to-evaluate%250Aand%2520black-box%252C%2520the%2520%255Ctextit%257Bstraddle%257D%2520algorithm%252C%2520which%2520is%2520a%2520representative%250Aheuristic%2520for%2520LSE%2520based%2520on%2520Gaussian%2520process%2520models%252C%2520and%2520its%2520extensions%2520having%250Atheoretical%2520guarantees%2520have%2520been%2520developed.%2520However%252C%2520many%2520of%2520existing%2520methods%250Ainclude%2520a%2520confidence%2520parameter%2520%2524%255Cbeta%255E%257B1/2%257D_t%2524%2520that%2520must%2520be%2520specified%2520by%2520the%250Auser%252C%2520and%2520methods%2520that%2520choose%2520%2524%255Cbeta%255E%257B1/2%257D_t%2524%2520heuristically%2520do%2520not%2520provide%250Atheoretical%2520guarantees.%2520In%2520contrast%252C%2520theoretically%2520guaranteed%2520values%2520of%250A%2524%255Cbeta%255E%257B1/2%257D_t%2524%2520need%2520to%2520be%2520increased%2520depending%2520on%2520the%2520number%2520of%2520iterations%2520and%250Acandidate%2520points%252C%2520and%2520are%2520conservative%2520and%2520not%2520good%2520for%2520practical%2520performance.%250AIn%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520method%252C%2520the%2520%255Ctextit%257Brandomized%2520straddle%257D%250Aalgorithm%252C%2520in%2520which%2520%2524%255Cbeta_t%2524%2520in%2520the%2520straddle%2520algorithm%2520is%2520replaced%2520by%2520a%2520random%250Asample%2520from%2520the%2520chi-squared%2520distribution%2520with%2520two%2520degrees%2520of%2520freedom.%2520The%250Aconfidence%2520parameter%2520in%2520the%2520proposed%2520method%2520has%2520the%2520advantages%2520of%2520not%2520needing%250Aadjustment%252C%2520not%2520depending%2520on%2520the%2520number%2520of%2520iterations%2520and%2520candidate%2520points%252C%2520and%250Anot%2520being%2520conservative.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520proposed%2520method%2520has%250Atheoretical%2520guarantees%2520that%2520depend%2520on%2520the%2520sample%2520complexity%2520and%2520the%2520number%2520of%250Aiterations.%2520Finally%252C%2520we%2520confirm%2520the%2520usefulness%2520of%2520the%2520proposed%2520method%2520through%250Anumerical%2520experiments%2520using%2520synthetic%2520and%2520real%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20for%20Level%20Set%20Estimation%20Using%20Randomized%20Straddle%0A%20%20Algorithms&entry.906535625=Yu%20Inatsu%20and%20Shion%20Takeno%20and%20Kentaro%20Kutsukake%20and%20Ichiro%20Takeuchi&entry.1292438233=%20%20Level%20set%20estimation%20%28LSE%29%2C%20the%20problem%20of%20identifying%20the%20set%20of%20input%0Apoints%20where%20a%20function%20takes%20value%20above%20%28or%20below%29%20a%20given%20threshold%2C%20is%0Aimportant%20in%20practical%20applications.%20When%20the%20function%20is%20expensive-to-evaluate%0Aand%20black-box%2C%20the%20%5Ctextit%7Bstraddle%7D%20algorithm%2C%20which%20is%20a%20representative%0Aheuristic%20for%20LSE%20based%20on%20Gaussian%20process%20models%2C%20and%20its%20extensions%20having%0Atheoretical%20guarantees%20have%20been%20developed.%20However%2C%20many%20of%20existing%20methods%0Ainclude%20a%20confidence%20parameter%20%24%5Cbeta%5E%7B1/2%7D_t%24%20that%20must%20be%20specified%20by%20the%0Auser%2C%20and%20methods%20that%20choose%20%24%5Cbeta%5E%7B1/2%7D_t%24%20heuristically%20do%20not%20provide%0Atheoretical%20guarantees.%20In%20contrast%2C%20theoretically%20guaranteed%20values%20of%0A%24%5Cbeta%5E%7B1/2%7D_t%24%20need%20to%20be%20increased%20depending%20on%20the%20number%20of%20iterations%20and%0Acandidate%20points%2C%20and%20are%20conservative%20and%20not%20good%20for%20practical%20performance.%0AIn%20this%20study%2C%20we%20propose%20a%20novel%20method%2C%20the%20%5Ctextit%7Brandomized%20straddle%7D%0Aalgorithm%2C%20in%20which%20%24%5Cbeta_t%24%20in%20the%20straddle%20algorithm%20is%20replaced%20by%20a%20random%0Asample%20from%20the%20chi-squared%20distribution%20with%20two%20degrees%20of%20freedom.%20The%0Aconfidence%20parameter%20in%20the%20proposed%20method%20has%20the%20advantages%20of%20not%20needing%0Aadjustment%2C%20not%20depending%20on%20the%20number%20of%20iterations%20and%20candidate%20points%2C%20and%0Anot%20being%20conservative.%20Furthermore%2C%20we%20show%20that%20the%20proposed%20method%20has%0Atheoretical%20guarantees%20that%20depend%20on%20the%20sample%20complexity%20and%20the%20number%20of%0Aiterations.%20Finally%2C%20we%20confirm%20the%20usefulness%20of%20the%20proposed%20method%20through%0Anumerical%20experiments%20using%20synthetic%20and%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03144v1&entry.124074799=Read"},
{"title": "Masked Random Noise for Communication Efficient Federaetd Learning", "author": "Shiwei Li and Yingyi Cheng and Haozhao Wang and Xing Tang and Shijie Xu and Weihong Luo and Yuhua Li and Dugang Liu and Xiuqiang He and and Ruixuan Li", "abstract": "  Federated learning is a promising distributed training paradigm that\neffectively safeguards data privacy. However, it may involve significant\ncommunication costs, which hinders training efficiency. In this paper, we aim\nto enhance communication efficiency from a new perspective. Specifically, we\nrequest the distributed clients to find optimal model updates relative to\nglobal model parameters within predefined random noise. For this purpose, we\npropose Federated Masked Random Noise (FedMRN), a novel framework that enables\nclients to learn a 1-bit mask for each model parameter and apply masked random\nnoise (i.e., the Hadamard product of random noise and masks) to represent model\nupdates. To make FedMRN feasible, we propose an advanced mask training\nstrategy, called progressive stochastic masking (PSM). After local training,\neach client only need to transmit local masks and a random seed to the server.\nAdditionally, we provide theoretical guarantees for the convergence of FedMRN\nunder both strongly convex and non-convex assumptions. Extensive experiments\nare conducted on four popular datasets. The results show that FedMRN exhibits\nsuperior convergence speed and test accuracy compared to relevant baselines,\nwhile attaining a similar level of accuracy as FedAvg.\n", "link": "http://arxiv.org/abs/2408.03220v1", "date": "2024-08-06", "relevancy": 1.9361, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4939}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4893}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Random%20Noise%20for%20Communication%20Efficient%20Federaetd%20Learning&body=Title%3A%20Masked%20Random%20Noise%20for%20Communication%20Efficient%20Federaetd%20Learning%0AAuthor%3A%20Shiwei%20Li%20and%20Yingyi%20Cheng%20and%20Haozhao%20Wang%20and%20Xing%20Tang%20and%20Shijie%20Xu%20and%20Weihong%20Luo%20and%20Yuhua%20Li%20and%20Dugang%20Liu%20and%20Xiuqiang%20He%20and%20and%20Ruixuan%20Li%0AAbstract%3A%20%20%20Federated%20learning%20is%20a%20promising%20distributed%20training%20paradigm%20that%0Aeffectively%20safeguards%20data%20privacy.%20However%2C%20it%20may%20involve%20significant%0Acommunication%20costs%2C%20which%20hinders%20training%20efficiency.%20In%20this%20paper%2C%20we%20aim%0Ato%20enhance%20communication%20efficiency%20from%20a%20new%20perspective.%20Specifically%2C%20we%0Arequest%20the%20distributed%20clients%20to%20find%20optimal%20model%20updates%20relative%20to%0Aglobal%20model%20parameters%20within%20predefined%20random%20noise.%20For%20this%20purpose%2C%20we%0Apropose%20Federated%20Masked%20Random%20Noise%20%28FedMRN%29%2C%20a%20novel%20framework%20that%20enables%0Aclients%20to%20learn%20a%201-bit%20mask%20for%20each%20model%20parameter%20and%20apply%20masked%20random%0Anoise%20%28i.e.%2C%20the%20Hadamard%20product%20of%20random%20noise%20and%20masks%29%20to%20represent%20model%0Aupdates.%20To%20make%20FedMRN%20feasible%2C%20we%20propose%20an%20advanced%20mask%20training%0Astrategy%2C%20called%20progressive%20stochastic%20masking%20%28PSM%29.%20After%20local%20training%2C%0Aeach%20client%20only%20need%20to%20transmit%20local%20masks%20and%20a%20random%20seed%20to%20the%20server.%0AAdditionally%2C%20we%20provide%20theoretical%20guarantees%20for%20the%20convergence%20of%20FedMRN%0Aunder%20both%20strongly%20convex%20and%20non-convex%20assumptions.%20Extensive%20experiments%0Aare%20conducted%20on%20four%20popular%20datasets.%20The%20results%20show%20that%20FedMRN%20exhibits%0Asuperior%20convergence%20speed%20and%20test%20accuracy%20compared%20to%20relevant%20baselines%2C%0Awhile%20attaining%20a%20similar%20level%20of%20accuracy%20as%20FedAvg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Random%2520Noise%2520for%2520Communication%2520Efficient%2520Federaetd%2520Learning%26entry.906535625%3DShiwei%2520Li%2520and%2520Yingyi%2520Cheng%2520and%2520Haozhao%2520Wang%2520and%2520Xing%2520Tang%2520and%2520Shijie%2520Xu%2520and%2520Weihong%2520Luo%2520and%2520Yuhua%2520Li%2520and%2520Dugang%2520Liu%2520and%2520Xiuqiang%2520He%2520and%2520and%2520Ruixuan%2520Li%26entry.1292438233%3D%2520%2520Federated%2520learning%2520is%2520a%2520promising%2520distributed%2520training%2520paradigm%2520that%250Aeffectively%2520safeguards%2520data%2520privacy.%2520However%252C%2520it%2520may%2520involve%2520significant%250Acommunication%2520costs%252C%2520which%2520hinders%2520training%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520aim%250Ato%2520enhance%2520communication%2520efficiency%2520from%2520a%2520new%2520perspective.%2520Specifically%252C%2520we%250Arequest%2520the%2520distributed%2520clients%2520to%2520find%2520optimal%2520model%2520updates%2520relative%2520to%250Aglobal%2520model%2520parameters%2520within%2520predefined%2520random%2520noise.%2520For%2520this%2520purpose%252C%2520we%250Apropose%2520Federated%2520Masked%2520Random%2520Noise%2520%2528FedMRN%2529%252C%2520a%2520novel%2520framework%2520that%2520enables%250Aclients%2520to%2520learn%2520a%25201-bit%2520mask%2520for%2520each%2520model%2520parameter%2520and%2520apply%2520masked%2520random%250Anoise%2520%2528i.e.%252C%2520the%2520Hadamard%2520product%2520of%2520random%2520noise%2520and%2520masks%2529%2520to%2520represent%2520model%250Aupdates.%2520To%2520make%2520FedMRN%2520feasible%252C%2520we%2520propose%2520an%2520advanced%2520mask%2520training%250Astrategy%252C%2520called%2520progressive%2520stochastic%2520masking%2520%2528PSM%2529.%2520After%2520local%2520training%252C%250Aeach%2520client%2520only%2520need%2520to%2520transmit%2520local%2520masks%2520and%2520a%2520random%2520seed%2520to%2520the%2520server.%250AAdditionally%252C%2520we%2520provide%2520theoretical%2520guarantees%2520for%2520the%2520convergence%2520of%2520FedMRN%250Aunder%2520both%2520strongly%2520convex%2520and%2520non-convex%2520assumptions.%2520Extensive%2520experiments%250Aare%2520conducted%2520on%2520four%2520popular%2520datasets.%2520The%2520results%2520show%2520that%2520FedMRN%2520exhibits%250Asuperior%2520convergence%2520speed%2520and%2520test%2520accuracy%2520compared%2520to%2520relevant%2520baselines%252C%250Awhile%2520attaining%2520a%2520similar%2520level%2520of%2520accuracy%2520as%2520FedAvg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Random%20Noise%20for%20Communication%20Efficient%20Federaetd%20Learning&entry.906535625=Shiwei%20Li%20and%20Yingyi%20Cheng%20and%20Haozhao%20Wang%20and%20Xing%20Tang%20and%20Shijie%20Xu%20and%20Weihong%20Luo%20and%20Yuhua%20Li%20and%20Dugang%20Liu%20and%20Xiuqiang%20He%20and%20and%20Ruixuan%20Li&entry.1292438233=%20%20Federated%20learning%20is%20a%20promising%20distributed%20training%20paradigm%20that%0Aeffectively%20safeguards%20data%20privacy.%20However%2C%20it%20may%20involve%20significant%0Acommunication%20costs%2C%20which%20hinders%20training%20efficiency.%20In%20this%20paper%2C%20we%20aim%0Ato%20enhance%20communication%20efficiency%20from%20a%20new%20perspective.%20Specifically%2C%20we%0Arequest%20the%20distributed%20clients%20to%20find%20optimal%20model%20updates%20relative%20to%0Aglobal%20model%20parameters%20within%20predefined%20random%20noise.%20For%20this%20purpose%2C%20we%0Apropose%20Federated%20Masked%20Random%20Noise%20%28FedMRN%29%2C%20a%20novel%20framework%20that%20enables%0Aclients%20to%20learn%20a%201-bit%20mask%20for%20each%20model%20parameter%20and%20apply%20masked%20random%0Anoise%20%28i.e.%2C%20the%20Hadamard%20product%20of%20random%20noise%20and%20masks%29%20to%20represent%20model%0Aupdates.%20To%20make%20FedMRN%20feasible%2C%20we%20propose%20an%20advanced%20mask%20training%0Astrategy%2C%20called%20progressive%20stochastic%20masking%20%28PSM%29.%20After%20local%20training%2C%0Aeach%20client%20only%20need%20to%20transmit%20local%20masks%20and%20a%20random%20seed%20to%20the%20server.%0AAdditionally%2C%20we%20provide%20theoretical%20guarantees%20for%20the%20convergence%20of%20FedMRN%0Aunder%20both%20strongly%20convex%20and%20non-convex%20assumptions.%20Extensive%20experiments%0Aare%20conducted%20on%20four%20popular%20datasets.%20The%20results%20show%20that%20FedMRN%20exhibits%0Asuperior%20convergence%20speed%20and%20test%20accuracy%20compared%20to%20relevant%20baselines%2C%0Awhile%20attaining%20a%20similar%20level%20of%20accuracy%20as%20FedAvg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03220v1&entry.124074799=Read"},
{"title": "AMES: Asymmetric and Memory-Efficient Similarity Estimation for\n  Instance-level Retrieval", "author": "Pavel Suma and Giorgos Kordopatis-Zilos and Ahmet Iscen and Giorgos Tolias", "abstract": "  This work investigates the problem of instance-level image retrieval\nre-ranking with the constraint of memory efficiency, ultimately aiming to limit\nmemory usage to 1KB per image. Departing from the prevalent focus on\nperformance enhancements, this work prioritizes the crucial trade-off between\nperformance and memory requirements. The proposed model uses a\ntransformer-based architecture designed to estimate image-to-image similarity\nby capturing interactions within and across images based on their local\ndescriptors. A distinctive property of the model is the capability for\nasymmetric similarity estimation. Database images are represented with a\nsmaller number of descriptors compared to query images, enabling performance\nimprovements without increasing memory consumption. To ensure adaptability\nacross different applications, a universal model is introduced that adjusts to\na varying number of local descriptors during the testing phase. Results on\nstandard benchmarks demonstrate the superiority of our approach over both\nhand-crafted and learned models. In particular, compared with current\nstate-of-the-art methods that overlook their memory footprint, our approach not\nonly attains superior performance but does so with a significantly reduced\nmemory footprint. The code and pretrained models are publicly available at:\nhttps://github.com/pavelsuma/ames\n", "link": "http://arxiv.org/abs/2408.03282v1", "date": "2024-08-06", "relevancy": 1.9316, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4864}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4856}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMES%3A%20Asymmetric%20and%20Memory-Efficient%20Similarity%20Estimation%20for%0A%20%20Instance-level%20Retrieval&body=Title%3A%20AMES%3A%20Asymmetric%20and%20Memory-Efficient%20Similarity%20Estimation%20for%0A%20%20Instance-level%20Retrieval%0AAuthor%3A%20Pavel%20Suma%20and%20Giorgos%20Kordopatis-Zilos%20and%20Ahmet%20Iscen%20and%20Giorgos%20Tolias%0AAbstract%3A%20%20%20This%20work%20investigates%20the%20problem%20of%20instance-level%20image%20retrieval%0Are-ranking%20with%20the%20constraint%20of%20memory%20efficiency%2C%20ultimately%20aiming%20to%20limit%0Amemory%20usage%20to%201KB%20per%20image.%20Departing%20from%20the%20prevalent%20focus%20on%0Aperformance%20enhancements%2C%20this%20work%20prioritizes%20the%20crucial%20trade-off%20between%0Aperformance%20and%20memory%20requirements.%20The%20proposed%20model%20uses%20a%0Atransformer-based%20architecture%20designed%20to%20estimate%20image-to-image%20similarity%0Aby%20capturing%20interactions%20within%20and%20across%20images%20based%20on%20their%20local%0Adescriptors.%20A%20distinctive%20property%20of%20the%20model%20is%20the%20capability%20for%0Aasymmetric%20similarity%20estimation.%20Database%20images%20are%20represented%20with%20a%0Asmaller%20number%20of%20descriptors%20compared%20to%20query%20images%2C%20enabling%20performance%0Aimprovements%20without%20increasing%20memory%20consumption.%20To%20ensure%20adaptability%0Aacross%20different%20applications%2C%20a%20universal%20model%20is%20introduced%20that%20adjusts%20to%0Aa%20varying%20number%20of%20local%20descriptors%20during%20the%20testing%20phase.%20Results%20on%0Astandard%20benchmarks%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20both%0Ahand-crafted%20and%20learned%20models.%20In%20particular%2C%20compared%20with%20current%0Astate-of-the-art%20methods%20that%20overlook%20their%20memory%20footprint%2C%20our%20approach%20not%0Aonly%20attains%20superior%20performance%20but%20does%20so%20with%20a%20significantly%20reduced%0Amemory%20footprint.%20The%20code%20and%20pretrained%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/pavelsuma/ames%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMES%253A%2520Asymmetric%2520and%2520Memory-Efficient%2520Similarity%2520Estimation%2520for%250A%2520%2520Instance-level%2520Retrieval%26entry.906535625%3DPavel%2520Suma%2520and%2520Giorgos%2520Kordopatis-Zilos%2520and%2520Ahmet%2520Iscen%2520and%2520Giorgos%2520Tolias%26entry.1292438233%3D%2520%2520This%2520work%2520investigates%2520the%2520problem%2520of%2520instance-level%2520image%2520retrieval%250Are-ranking%2520with%2520the%2520constraint%2520of%2520memory%2520efficiency%252C%2520ultimately%2520aiming%2520to%2520limit%250Amemory%2520usage%2520to%25201KB%2520per%2520image.%2520Departing%2520from%2520the%2520prevalent%2520focus%2520on%250Aperformance%2520enhancements%252C%2520this%2520work%2520prioritizes%2520the%2520crucial%2520trade-off%2520between%250Aperformance%2520and%2520memory%2520requirements.%2520The%2520proposed%2520model%2520uses%2520a%250Atransformer-based%2520architecture%2520designed%2520to%2520estimate%2520image-to-image%2520similarity%250Aby%2520capturing%2520interactions%2520within%2520and%2520across%2520images%2520based%2520on%2520their%2520local%250Adescriptors.%2520A%2520distinctive%2520property%2520of%2520the%2520model%2520is%2520the%2520capability%2520for%250Aasymmetric%2520similarity%2520estimation.%2520Database%2520images%2520are%2520represented%2520with%2520a%250Asmaller%2520number%2520of%2520descriptors%2520compared%2520to%2520query%2520images%252C%2520enabling%2520performance%250Aimprovements%2520without%2520increasing%2520memory%2520consumption.%2520To%2520ensure%2520adaptability%250Aacross%2520different%2520applications%252C%2520a%2520universal%2520model%2520is%2520introduced%2520that%2520adjusts%2520to%250Aa%2520varying%2520number%2520of%2520local%2520descriptors%2520during%2520the%2520testing%2520phase.%2520Results%2520on%250Astandard%2520benchmarks%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520both%250Ahand-crafted%2520and%2520learned%2520models.%2520In%2520particular%252C%2520compared%2520with%2520current%250Astate-of-the-art%2520methods%2520that%2520overlook%2520their%2520memory%2520footprint%252C%2520our%2520approach%2520not%250Aonly%2520attains%2520superior%2520performance%2520but%2520does%2520so%2520with%2520a%2520significantly%2520reduced%250Amemory%2520footprint.%2520The%2520code%2520and%2520pretrained%2520models%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/pavelsuma/ames%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMES%3A%20Asymmetric%20and%20Memory-Efficient%20Similarity%20Estimation%20for%0A%20%20Instance-level%20Retrieval&entry.906535625=Pavel%20Suma%20and%20Giorgos%20Kordopatis-Zilos%20and%20Ahmet%20Iscen%20and%20Giorgos%20Tolias&entry.1292438233=%20%20This%20work%20investigates%20the%20problem%20of%20instance-level%20image%20retrieval%0Are-ranking%20with%20the%20constraint%20of%20memory%20efficiency%2C%20ultimately%20aiming%20to%20limit%0Amemory%20usage%20to%201KB%20per%20image.%20Departing%20from%20the%20prevalent%20focus%20on%0Aperformance%20enhancements%2C%20this%20work%20prioritizes%20the%20crucial%20trade-off%20between%0Aperformance%20and%20memory%20requirements.%20The%20proposed%20model%20uses%20a%0Atransformer-based%20architecture%20designed%20to%20estimate%20image-to-image%20similarity%0Aby%20capturing%20interactions%20within%20and%20across%20images%20based%20on%20their%20local%0Adescriptors.%20A%20distinctive%20property%20of%20the%20model%20is%20the%20capability%20for%0Aasymmetric%20similarity%20estimation.%20Database%20images%20are%20represented%20with%20a%0Asmaller%20number%20of%20descriptors%20compared%20to%20query%20images%2C%20enabling%20performance%0Aimprovements%20without%20increasing%20memory%20consumption.%20To%20ensure%20adaptability%0Aacross%20different%20applications%2C%20a%20universal%20model%20is%20introduced%20that%20adjusts%20to%0Aa%20varying%20number%20of%20local%20descriptors%20during%20the%20testing%20phase.%20Results%20on%0Astandard%20benchmarks%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20both%0Ahand-crafted%20and%20learned%20models.%20In%20particular%2C%20compared%20with%20current%0Astate-of-the-art%20methods%20that%20overlook%20their%20memory%20footprint%2C%20our%20approach%20not%0Aonly%20attains%20superior%20performance%20but%20does%20so%20with%20a%20significantly%20reduced%0Amemory%20footprint.%20The%20code%20and%20pretrained%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/pavelsuma/ames%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03282v1&entry.124074799=Read"},
{"title": "Calo-VQ: Vector-Quantized Two-Stage Generative Model in Calorimeter\n  Simulation", "author": "Qibin Liu and Chase Shimmin and Xiulong Liu and Eli Shlizerman and Shu Li and Shih-Chieh Hsu", "abstract": "  We introduce a novel machine learning method developed for the fast\nsimulation of calorimeter detector response, adapting vector-quantized\nvariational autoencoder (VQ-VAE). Our model adopts a two-stage generation\nstrategy: initially compressing geometry-aware calorimeter data into a discrete\nlatent space, followed by the application of a sequence model to learn and\ngenerate the latent tokens. Extensive experimentation on the Calo-challenge\ndataset underscores the efficiency of our approach, showcasing a remarkable\nimprovement in the generation speed compared with conventional method by a\nfactor of 2000. Remarkably, our model achieves the generation of calorimeter\nshowers within milliseconds. Furthermore, comprehensive quantitative\nevaluations across various metrics are performed to validate physics\nperformance of generation.\n", "link": "http://arxiv.org/abs/2405.06605v3", "date": "2024-08-06", "relevancy": 1.9284, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4961}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.487}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calo-VQ%3A%20Vector-Quantized%20Two-Stage%20Generative%20Model%20in%20Calorimeter%0A%20%20Simulation&body=Title%3A%20Calo-VQ%3A%20Vector-Quantized%20Two-Stage%20Generative%20Model%20in%20Calorimeter%0A%20%20Simulation%0AAuthor%3A%20Qibin%20Liu%20and%20Chase%20Shimmin%20and%20Xiulong%20Liu%20and%20Eli%20Shlizerman%20and%20Shu%20Li%20and%20Shih-Chieh%20Hsu%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20machine%20learning%20method%20developed%20for%20the%20fast%0Asimulation%20of%20calorimeter%20detector%20response%2C%20adapting%20vector-quantized%0Avariational%20autoencoder%20%28VQ-VAE%29.%20Our%20model%20adopts%20a%20two-stage%20generation%0Astrategy%3A%20initially%20compressing%20geometry-aware%20calorimeter%20data%20into%20a%20discrete%0Alatent%20space%2C%20followed%20by%20the%20application%20of%20a%20sequence%20model%20to%20learn%20and%0Agenerate%20the%20latent%20tokens.%20Extensive%20experimentation%20on%20the%20Calo-challenge%0Adataset%20underscores%20the%20efficiency%20of%20our%20approach%2C%20showcasing%20a%20remarkable%0Aimprovement%20in%20the%20generation%20speed%20compared%20with%20conventional%20method%20by%20a%0Afactor%20of%202000.%20Remarkably%2C%20our%20model%20achieves%20the%20generation%20of%20calorimeter%0Ashowers%20within%20milliseconds.%20Furthermore%2C%20comprehensive%20quantitative%0Aevaluations%20across%20various%20metrics%20are%20performed%20to%20validate%20physics%0Aperformance%20of%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06605v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalo-VQ%253A%2520Vector-Quantized%2520Two-Stage%2520Generative%2520Model%2520in%2520Calorimeter%250A%2520%2520Simulation%26entry.906535625%3DQibin%2520Liu%2520and%2520Chase%2520Shimmin%2520and%2520Xiulong%2520Liu%2520and%2520Eli%2520Shlizerman%2520and%2520Shu%2520Li%2520and%2520Shih-Chieh%2520Hsu%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520machine%2520learning%2520method%2520developed%2520for%2520the%2520fast%250Asimulation%2520of%2520calorimeter%2520detector%2520response%252C%2520adapting%2520vector-quantized%250Avariational%2520autoencoder%2520%2528VQ-VAE%2529.%2520Our%2520model%2520adopts%2520a%2520two-stage%2520generation%250Astrategy%253A%2520initially%2520compressing%2520geometry-aware%2520calorimeter%2520data%2520into%2520a%2520discrete%250Alatent%2520space%252C%2520followed%2520by%2520the%2520application%2520of%2520a%2520sequence%2520model%2520to%2520learn%2520and%250Agenerate%2520the%2520latent%2520tokens.%2520Extensive%2520experimentation%2520on%2520the%2520Calo-challenge%250Adataset%2520underscores%2520the%2520efficiency%2520of%2520our%2520approach%252C%2520showcasing%2520a%2520remarkable%250Aimprovement%2520in%2520the%2520generation%2520speed%2520compared%2520with%2520conventional%2520method%2520by%2520a%250Afactor%2520of%25202000.%2520Remarkably%252C%2520our%2520model%2520achieves%2520the%2520generation%2520of%2520calorimeter%250Ashowers%2520within%2520milliseconds.%2520Furthermore%252C%2520comprehensive%2520quantitative%250Aevaluations%2520across%2520various%2520metrics%2520are%2520performed%2520to%2520validate%2520physics%250Aperformance%2520of%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06605v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calo-VQ%3A%20Vector-Quantized%20Two-Stage%20Generative%20Model%20in%20Calorimeter%0A%20%20Simulation&entry.906535625=Qibin%20Liu%20and%20Chase%20Shimmin%20and%20Xiulong%20Liu%20and%20Eli%20Shlizerman%20and%20Shu%20Li%20and%20Shih-Chieh%20Hsu&entry.1292438233=%20%20We%20introduce%20a%20novel%20machine%20learning%20method%20developed%20for%20the%20fast%0Asimulation%20of%20calorimeter%20detector%20response%2C%20adapting%20vector-quantized%0Avariational%20autoencoder%20%28VQ-VAE%29.%20Our%20model%20adopts%20a%20two-stage%20generation%0Astrategy%3A%20initially%20compressing%20geometry-aware%20calorimeter%20data%20into%20a%20discrete%0Alatent%20space%2C%20followed%20by%20the%20application%20of%20a%20sequence%20model%20to%20learn%20and%0Agenerate%20the%20latent%20tokens.%20Extensive%20experimentation%20on%20the%20Calo-challenge%0Adataset%20underscores%20the%20efficiency%20of%20our%20approach%2C%20showcasing%20a%20remarkable%0Aimprovement%20in%20the%20generation%20speed%20compared%20with%20conventional%20method%20by%20a%0Afactor%20of%202000.%20Remarkably%2C%20our%20model%20achieves%20the%20generation%20of%20calorimeter%0Ashowers%20within%20milliseconds.%20Furthermore%2C%20comprehensive%20quantitative%0Aevaluations%20across%20various%20metrics%20are%20performed%20to%20validate%20physics%0Aperformance%20of%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06605v3&entry.124074799=Read"},
{"title": "RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning", "author": "Jiapeng Zhu and Zichen Ding and Jianxiang Yu and Jiaqi Tan and Xiang Li and Weining Qian", "abstract": "  The advent of the \"pre-train, prompt\" paradigm has recently extended its\ngeneralization ability and data efficiency to graph representation learning,\nfollowing its achievements in Natural Language Processing (NLP). Initial graph\nprompt tuning approaches tailored specialized prompting functions for Graph\nNeural Network (GNN) models pre-trained with specific strategies, such as edge\nprediction, thus limiting their applicability. In contrast, another pioneering\nline of research has explored universal prompting via adding prompts to the\ninput graph's feature space, thereby removing the reliance on specific\npre-training strategies. However, the necessity to add feature prompts to all\nnodes remains an open question. Motivated by findings from prompt tuning\nresearch in the NLP domain, which suggest that highly capable pre-trained\nmodels need less conditioning signal to achieve desired behaviors, we advocate\nfor strategically incorporating necessary and lightweight feature prompts to\ncertain graph nodes to enhance downstream task performance. This introduces a\ncombinatorial optimization problem, requiring a policy to decide 1) which nodes\nto prompt and 2) what specific feature prompts to attach. We then address the\nproblem by framing the prompt incorporation process as a sequential\ndecision-making problem and propose our method, RELIEF, which employs\nReinforcement Learning (RL) to optimize it. At each step, the RL agent selects\na node (discrete action) and determines the prompt content (continuous action),\naiming to maximize cumulative performance gain. Extensive experiments on graph\nand node-level tasks with various pre-training strategies in few-shot scenarios\ndemonstrate that our RELIEF outperforms fine-tuning and other prompt-based\napproaches in classification performance and data efficiency.\n", "link": "http://arxiv.org/abs/2408.03195v1", "date": "2024-08-06", "relevancy": 1.9274, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.492}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4819}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RELIEF%3A%20Reinforcement%20Learning%20Empowered%20Graph%20Feature%20Prompt%20Tuning&body=Title%3A%20RELIEF%3A%20Reinforcement%20Learning%20Empowered%20Graph%20Feature%20Prompt%20Tuning%0AAuthor%3A%20Jiapeng%20Zhu%20and%20Zichen%20Ding%20and%20Jianxiang%20Yu%20and%20Jiaqi%20Tan%20and%20Xiang%20Li%20and%20Weining%20Qian%0AAbstract%3A%20%20%20The%20advent%20of%20the%20%22pre-train%2C%20prompt%22%20paradigm%20has%20recently%20extended%20its%0Ageneralization%20ability%20and%20data%20efficiency%20to%20graph%20representation%20learning%2C%0Afollowing%20its%20achievements%20in%20Natural%20Language%20Processing%20%28NLP%29.%20Initial%20graph%0Aprompt%20tuning%20approaches%20tailored%20specialized%20prompting%20functions%20for%20Graph%0ANeural%20Network%20%28GNN%29%20models%20pre-trained%20with%20specific%20strategies%2C%20such%20as%20edge%0Aprediction%2C%20thus%20limiting%20their%20applicability.%20In%20contrast%2C%20another%20pioneering%0Aline%20of%20research%20has%20explored%20universal%20prompting%20via%20adding%20prompts%20to%20the%0Ainput%20graph%27s%20feature%20space%2C%20thereby%20removing%20the%20reliance%20on%20specific%0Apre-training%20strategies.%20However%2C%20the%20necessity%20to%20add%20feature%20prompts%20to%20all%0Anodes%20remains%20an%20open%20question.%20Motivated%20by%20findings%20from%20prompt%20tuning%0Aresearch%20in%20the%20NLP%20domain%2C%20which%20suggest%20that%20highly%20capable%20pre-trained%0Amodels%20need%20less%20conditioning%20signal%20to%20achieve%20desired%20behaviors%2C%20we%20advocate%0Afor%20strategically%20incorporating%20necessary%20and%20lightweight%20feature%20prompts%20to%0Acertain%20graph%20nodes%20to%20enhance%20downstream%20task%20performance.%20This%20introduces%20a%0Acombinatorial%20optimization%20problem%2C%20requiring%20a%20policy%20to%20decide%201%29%20which%20nodes%0Ato%20prompt%20and%202%29%20what%20specific%20feature%20prompts%20to%20attach.%20We%20then%20address%20the%0Aproblem%20by%20framing%20the%20prompt%20incorporation%20process%20as%20a%20sequential%0Adecision-making%20problem%20and%20propose%20our%20method%2C%20RELIEF%2C%20which%20employs%0AReinforcement%20Learning%20%28RL%29%20to%20optimize%20it.%20At%20each%20step%2C%20the%20RL%20agent%20selects%0Aa%20node%20%28discrete%20action%29%20and%20determines%20the%20prompt%20content%20%28continuous%20action%29%2C%0Aaiming%20to%20maximize%20cumulative%20performance%20gain.%20Extensive%20experiments%20on%20graph%0Aand%20node-level%20tasks%20with%20various%20pre-training%20strategies%20in%20few-shot%20scenarios%0Ademonstrate%20that%20our%20RELIEF%20outperforms%20fine-tuning%20and%20other%20prompt-based%0Aapproaches%20in%20classification%20performance%20and%20data%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRELIEF%253A%2520Reinforcement%2520Learning%2520Empowered%2520Graph%2520Feature%2520Prompt%2520Tuning%26entry.906535625%3DJiapeng%2520Zhu%2520and%2520Zichen%2520Ding%2520and%2520Jianxiang%2520Yu%2520and%2520Jiaqi%2520Tan%2520and%2520Xiang%2520Li%2520and%2520Weining%2520Qian%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520the%2520%2522pre-train%252C%2520prompt%2522%2520paradigm%2520has%2520recently%2520extended%2520its%250Ageneralization%2520ability%2520and%2520data%2520efficiency%2520to%2520graph%2520representation%2520learning%252C%250Afollowing%2520its%2520achievements%2520in%2520Natural%2520Language%2520Processing%2520%2528NLP%2529.%2520Initial%2520graph%250Aprompt%2520tuning%2520approaches%2520tailored%2520specialized%2520prompting%2520functions%2520for%2520Graph%250ANeural%2520Network%2520%2528GNN%2529%2520models%2520pre-trained%2520with%2520specific%2520strategies%252C%2520such%2520as%2520edge%250Aprediction%252C%2520thus%2520limiting%2520their%2520applicability.%2520In%2520contrast%252C%2520another%2520pioneering%250Aline%2520of%2520research%2520has%2520explored%2520universal%2520prompting%2520via%2520adding%2520prompts%2520to%2520the%250Ainput%2520graph%2527s%2520feature%2520space%252C%2520thereby%2520removing%2520the%2520reliance%2520on%2520specific%250Apre-training%2520strategies.%2520However%252C%2520the%2520necessity%2520to%2520add%2520feature%2520prompts%2520to%2520all%250Anodes%2520remains%2520an%2520open%2520question.%2520Motivated%2520by%2520findings%2520from%2520prompt%2520tuning%250Aresearch%2520in%2520the%2520NLP%2520domain%252C%2520which%2520suggest%2520that%2520highly%2520capable%2520pre-trained%250Amodels%2520need%2520less%2520conditioning%2520signal%2520to%2520achieve%2520desired%2520behaviors%252C%2520we%2520advocate%250Afor%2520strategically%2520incorporating%2520necessary%2520and%2520lightweight%2520feature%2520prompts%2520to%250Acertain%2520graph%2520nodes%2520to%2520enhance%2520downstream%2520task%2520performance.%2520This%2520introduces%2520a%250Acombinatorial%2520optimization%2520problem%252C%2520requiring%2520a%2520policy%2520to%2520decide%25201%2529%2520which%2520nodes%250Ato%2520prompt%2520and%25202%2529%2520what%2520specific%2520feature%2520prompts%2520to%2520attach.%2520We%2520then%2520address%2520the%250Aproblem%2520by%2520framing%2520the%2520prompt%2520incorporation%2520process%2520as%2520a%2520sequential%250Adecision-making%2520problem%2520and%2520propose%2520our%2520method%252C%2520RELIEF%252C%2520which%2520employs%250AReinforcement%2520Learning%2520%2528RL%2529%2520to%2520optimize%2520it.%2520At%2520each%2520step%252C%2520the%2520RL%2520agent%2520selects%250Aa%2520node%2520%2528discrete%2520action%2529%2520and%2520determines%2520the%2520prompt%2520content%2520%2528continuous%2520action%2529%252C%250Aaiming%2520to%2520maximize%2520cumulative%2520performance%2520gain.%2520Extensive%2520experiments%2520on%2520graph%250Aand%2520node-level%2520tasks%2520with%2520various%2520pre-training%2520strategies%2520in%2520few-shot%2520scenarios%250Ademonstrate%2520that%2520our%2520RELIEF%2520outperforms%2520fine-tuning%2520and%2520other%2520prompt-based%250Aapproaches%2520in%2520classification%2520performance%2520and%2520data%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RELIEF%3A%20Reinforcement%20Learning%20Empowered%20Graph%20Feature%20Prompt%20Tuning&entry.906535625=Jiapeng%20Zhu%20and%20Zichen%20Ding%20and%20Jianxiang%20Yu%20and%20Jiaqi%20Tan%20and%20Xiang%20Li%20and%20Weining%20Qian&entry.1292438233=%20%20The%20advent%20of%20the%20%22pre-train%2C%20prompt%22%20paradigm%20has%20recently%20extended%20its%0Ageneralization%20ability%20and%20data%20efficiency%20to%20graph%20representation%20learning%2C%0Afollowing%20its%20achievements%20in%20Natural%20Language%20Processing%20%28NLP%29.%20Initial%20graph%0Aprompt%20tuning%20approaches%20tailored%20specialized%20prompting%20functions%20for%20Graph%0ANeural%20Network%20%28GNN%29%20models%20pre-trained%20with%20specific%20strategies%2C%20such%20as%20edge%0Aprediction%2C%20thus%20limiting%20their%20applicability.%20In%20contrast%2C%20another%20pioneering%0Aline%20of%20research%20has%20explored%20universal%20prompting%20via%20adding%20prompts%20to%20the%0Ainput%20graph%27s%20feature%20space%2C%20thereby%20removing%20the%20reliance%20on%20specific%0Apre-training%20strategies.%20However%2C%20the%20necessity%20to%20add%20feature%20prompts%20to%20all%0Anodes%20remains%20an%20open%20question.%20Motivated%20by%20findings%20from%20prompt%20tuning%0Aresearch%20in%20the%20NLP%20domain%2C%20which%20suggest%20that%20highly%20capable%20pre-trained%0Amodels%20need%20less%20conditioning%20signal%20to%20achieve%20desired%20behaviors%2C%20we%20advocate%0Afor%20strategically%20incorporating%20necessary%20and%20lightweight%20feature%20prompts%20to%0Acertain%20graph%20nodes%20to%20enhance%20downstream%20task%20performance.%20This%20introduces%20a%0Acombinatorial%20optimization%20problem%2C%20requiring%20a%20policy%20to%20decide%201%29%20which%20nodes%0Ato%20prompt%20and%202%29%20what%20specific%20feature%20prompts%20to%20attach.%20We%20then%20address%20the%0Aproblem%20by%20framing%20the%20prompt%20incorporation%20process%20as%20a%20sequential%0Adecision-making%20problem%20and%20propose%20our%20method%2C%20RELIEF%2C%20which%20employs%0AReinforcement%20Learning%20%28RL%29%20to%20optimize%20it.%20At%20each%20step%2C%20the%20RL%20agent%20selects%0Aa%20node%20%28discrete%20action%29%20and%20determines%20the%20prompt%20content%20%28continuous%20action%29%2C%0Aaiming%20to%20maximize%20cumulative%20performance%20gain.%20Extensive%20experiments%20on%20graph%0Aand%20node-level%20tasks%20with%20various%20pre-training%20strategies%20in%20few-shot%20scenarios%0Ademonstrate%20that%20our%20RELIEF%20outperforms%20fine-tuning%20and%20other%20prompt-based%0Aapproaches%20in%20classification%20performance%20and%20data%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03195v1&entry.124074799=Read"},
{"title": "QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction", "author": "Siddhant Dutta and Nouhaila Innan and Alberto Marchisio and Sadok Ben Yahia and Muhammad Shafique", "abstract": "  Financial market prediction and optimal trading strategy development remain\nchallenging due to market complexity and volatility. Our research in quantum\nfinance and reinforcement learning for decision-making demonstrates the\napproach of quantum-classical hybrid algorithms to tackling real-world\nfinancial challenges. In this respect, we corroborate the concept with rigorous\nbacktesting and validate the framework's performance under realistic market\nconditions, by including fixed transaction cost per trade. This paper\nintroduces a Quantum Attention Deep Q-Network (QADQN) approach to address these\nchallenges through quantum-enhanced reinforcement learning. Our QADQN\narchitecture uses a variational quantum circuit inside a traditional deep\nQ-learning framework to take advantage of possible quantum advantages in\ndecision-making. We gauge the QADQN agent's performance on historical data from\nmajor market indices, including the S&P 500. We evaluate the agent's learning\nprocess by examining its reward accumulation and the effectiveness of its\nexperience replay mechanism. Our empirical results demonstrate the QADQN's\nsuperior performance, achieving better risk-adjusted returns with Sortino\nratios of 1.28 and 1.19 for non-overlapping and overlapping test periods\nrespectively, indicating effective downside risk management.\n", "link": "http://arxiv.org/abs/2408.03088v1", "date": "2024-08-06", "relevancy": 1.9146, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.496}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4719}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QADQN%3A%20Quantum%20Attention%20Deep%20Q-Network%20for%20Financial%20Market%20Prediction&body=Title%3A%20QADQN%3A%20Quantum%20Attention%20Deep%20Q-Network%20for%20Financial%20Market%20Prediction%0AAuthor%3A%20Siddhant%20Dutta%20and%20Nouhaila%20Innan%20and%20Alberto%20Marchisio%20and%20Sadok%20Ben%20Yahia%20and%20Muhammad%20Shafique%0AAbstract%3A%20%20%20Financial%20market%20prediction%20and%20optimal%20trading%20strategy%20development%20remain%0Achallenging%20due%20to%20market%20complexity%20and%20volatility.%20Our%20research%20in%20quantum%0Afinance%20and%20reinforcement%20learning%20for%20decision-making%20demonstrates%20the%0Aapproach%20of%20quantum-classical%20hybrid%20algorithms%20to%20tackling%20real-world%0Afinancial%20challenges.%20In%20this%20respect%2C%20we%20corroborate%20the%20concept%20with%20rigorous%0Abacktesting%20and%20validate%20the%20framework%27s%20performance%20under%20realistic%20market%0Aconditions%2C%20by%20including%20fixed%20transaction%20cost%20per%20trade.%20This%20paper%0Aintroduces%20a%20Quantum%20Attention%20Deep%20Q-Network%20%28QADQN%29%20approach%20to%20address%20these%0Achallenges%20through%20quantum-enhanced%20reinforcement%20learning.%20Our%20QADQN%0Aarchitecture%20uses%20a%20variational%20quantum%20circuit%20inside%20a%20traditional%20deep%0AQ-learning%20framework%20to%20take%20advantage%20of%20possible%20quantum%20advantages%20in%0Adecision-making.%20We%20gauge%20the%20QADQN%20agent%27s%20performance%20on%20historical%20data%20from%0Amajor%20market%20indices%2C%20including%20the%20S%26P%20500.%20We%20evaluate%20the%20agent%27s%20learning%0Aprocess%20by%20examining%20its%20reward%20accumulation%20and%20the%20effectiveness%20of%20its%0Aexperience%20replay%20mechanism.%20Our%20empirical%20results%20demonstrate%20the%20QADQN%27s%0Asuperior%20performance%2C%20achieving%20better%20risk-adjusted%20returns%20with%20Sortino%0Aratios%20of%201.28%20and%201.19%20for%20non-overlapping%20and%20overlapping%20test%20periods%0Arespectively%2C%20indicating%20effective%20downside%20risk%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQADQN%253A%2520Quantum%2520Attention%2520Deep%2520Q-Network%2520for%2520Financial%2520Market%2520Prediction%26entry.906535625%3DSiddhant%2520Dutta%2520and%2520Nouhaila%2520Innan%2520and%2520Alberto%2520Marchisio%2520and%2520Sadok%2520Ben%2520Yahia%2520and%2520Muhammad%2520Shafique%26entry.1292438233%3D%2520%2520Financial%2520market%2520prediction%2520and%2520optimal%2520trading%2520strategy%2520development%2520remain%250Achallenging%2520due%2520to%2520market%2520complexity%2520and%2520volatility.%2520Our%2520research%2520in%2520quantum%250Afinance%2520and%2520reinforcement%2520learning%2520for%2520decision-making%2520demonstrates%2520the%250Aapproach%2520of%2520quantum-classical%2520hybrid%2520algorithms%2520to%2520tackling%2520real-world%250Afinancial%2520challenges.%2520In%2520this%2520respect%252C%2520we%2520corroborate%2520the%2520concept%2520with%2520rigorous%250Abacktesting%2520and%2520validate%2520the%2520framework%2527s%2520performance%2520under%2520realistic%2520market%250Aconditions%252C%2520by%2520including%2520fixed%2520transaction%2520cost%2520per%2520trade.%2520This%2520paper%250Aintroduces%2520a%2520Quantum%2520Attention%2520Deep%2520Q-Network%2520%2528QADQN%2529%2520approach%2520to%2520address%2520these%250Achallenges%2520through%2520quantum-enhanced%2520reinforcement%2520learning.%2520Our%2520QADQN%250Aarchitecture%2520uses%2520a%2520variational%2520quantum%2520circuit%2520inside%2520a%2520traditional%2520deep%250AQ-learning%2520framework%2520to%2520take%2520advantage%2520of%2520possible%2520quantum%2520advantages%2520in%250Adecision-making.%2520We%2520gauge%2520the%2520QADQN%2520agent%2527s%2520performance%2520on%2520historical%2520data%2520from%250Amajor%2520market%2520indices%252C%2520including%2520the%2520S%2526P%2520500.%2520We%2520evaluate%2520the%2520agent%2527s%2520learning%250Aprocess%2520by%2520examining%2520its%2520reward%2520accumulation%2520and%2520the%2520effectiveness%2520of%2520its%250Aexperience%2520replay%2520mechanism.%2520Our%2520empirical%2520results%2520demonstrate%2520the%2520QADQN%2527s%250Asuperior%2520performance%252C%2520achieving%2520better%2520risk-adjusted%2520returns%2520with%2520Sortino%250Aratios%2520of%25201.28%2520and%25201.19%2520for%2520non-overlapping%2520and%2520overlapping%2520test%2520periods%250Arespectively%252C%2520indicating%2520effective%2520downside%2520risk%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QADQN%3A%20Quantum%20Attention%20Deep%20Q-Network%20for%20Financial%20Market%20Prediction&entry.906535625=Siddhant%20Dutta%20and%20Nouhaila%20Innan%20and%20Alberto%20Marchisio%20and%20Sadok%20Ben%20Yahia%20and%20Muhammad%20Shafique&entry.1292438233=%20%20Financial%20market%20prediction%20and%20optimal%20trading%20strategy%20development%20remain%0Achallenging%20due%20to%20market%20complexity%20and%20volatility.%20Our%20research%20in%20quantum%0Afinance%20and%20reinforcement%20learning%20for%20decision-making%20demonstrates%20the%0Aapproach%20of%20quantum-classical%20hybrid%20algorithms%20to%20tackling%20real-world%0Afinancial%20challenges.%20In%20this%20respect%2C%20we%20corroborate%20the%20concept%20with%20rigorous%0Abacktesting%20and%20validate%20the%20framework%27s%20performance%20under%20realistic%20market%0Aconditions%2C%20by%20including%20fixed%20transaction%20cost%20per%20trade.%20This%20paper%0Aintroduces%20a%20Quantum%20Attention%20Deep%20Q-Network%20%28QADQN%29%20approach%20to%20address%20these%0Achallenges%20through%20quantum-enhanced%20reinforcement%20learning.%20Our%20QADQN%0Aarchitecture%20uses%20a%20variational%20quantum%20circuit%20inside%20a%20traditional%20deep%0AQ-learning%20framework%20to%20take%20advantage%20of%20possible%20quantum%20advantages%20in%0Adecision-making.%20We%20gauge%20the%20QADQN%20agent%27s%20performance%20on%20historical%20data%20from%0Amajor%20market%20indices%2C%20including%20the%20S%26P%20500.%20We%20evaluate%20the%20agent%27s%20learning%0Aprocess%20by%20examining%20its%20reward%20accumulation%20and%20the%20effectiveness%20of%20its%0Aexperience%20replay%20mechanism.%20Our%20empirical%20results%20demonstrate%20the%20QADQN%27s%0Asuperior%20performance%2C%20achieving%20better%20risk-adjusted%20returns%20with%20Sortino%0Aratios%20of%201.28%20and%201.19%20for%20non-overlapping%20and%20overlapping%20test%20periods%0Arespectively%2C%20indicating%20effective%20downside%20risk%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03088v1&entry.124074799=Read"},
{"title": "Convergence Analysis of Natural Gradient Descent for Over-parameterized\n  Physics-Informed Neural Networks", "author": "Xianliang Xu and Ting Du and Wang Kong and Ye Li and Zhongyi Huang", "abstract": "  First-order methods, such as gradient descent (GD) and stochastic gradient\ndescent (SGD), have been proven effective in training neural networks. In the\ncontext of over-parameterization, there is a line of work demonstrating that\nrandomly initialized (stochastic) gradient descent converges to a globally\noptimal solution at a linear convergence rate for the quadratic loss function.\nHowever, the learning rate of GD for training two-layer neural networks\nexhibits poor dependence on the sample size and the Gram matrix, leading to a\nslow training process. In this paper, we show that for the $L^2$ regression\nproblems, the learning rate can be improved from $\\mathcal{O}(\\lambda_0/n^2)$\nto $\\mathcal{O}(1/\\|\\bm{H}^{\\infty}\\|_2)$, which implies that GD actually\nenjoys a faster convergence rate. Furthermore, we generalize the method to GD\nin training two-layer Physics-Informed Neural Networks (PINNs), showing a\nsimilar improvement for the learning rate. Although the improved learning rate\nhas a mild dependence on the Gram matrix, we still need to set it small enough\nin practice due to the unknown eigenvalues of the Gram matrix. More\nimportantly, the convergence rate is tied to the least eigenvalue of the Gram\nmatrix, which can lead to slow convergence. In this work, we provide the\nconvergence analysis of natural gradient descent (NGD) in training two-layer\nPINNs, demonstrating that the learning rate can be $\\mathcal{O}(1)$, and at\nthis rate, the convergence rate is independent of the Gram matrix.\n", "link": "http://arxiv.org/abs/2408.00573v2", "date": "2024-08-06", "relevancy": 1.9143, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4901}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4731}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20Analysis%20of%20Natural%20Gradient%20Descent%20for%20Over-parameterized%0A%20%20Physics-Informed%20Neural%20Networks&body=Title%3A%20Convergence%20Analysis%20of%20Natural%20Gradient%20Descent%20for%20Over-parameterized%0A%20%20Physics-Informed%20Neural%20Networks%0AAuthor%3A%20Xianliang%20Xu%20and%20Ting%20Du%20and%20Wang%20Kong%20and%20Ye%20Li%20and%20Zhongyi%20Huang%0AAbstract%3A%20%20%20First-order%20methods%2C%20such%20as%20gradient%20descent%20%28GD%29%20and%20stochastic%20gradient%0Adescent%20%28SGD%29%2C%20have%20been%20proven%20effective%20in%20training%20neural%20networks.%20In%20the%0Acontext%20of%20over-parameterization%2C%20there%20is%20a%20line%20of%20work%20demonstrating%20that%0Arandomly%20initialized%20%28stochastic%29%20gradient%20descent%20converges%20to%20a%20globally%0Aoptimal%20solution%20at%20a%20linear%20convergence%20rate%20for%20the%20quadratic%20loss%20function.%0AHowever%2C%20the%20learning%20rate%20of%20GD%20for%20training%20two-layer%20neural%20networks%0Aexhibits%20poor%20dependence%20on%20the%20sample%20size%20and%20the%20Gram%20matrix%2C%20leading%20to%20a%0Aslow%20training%20process.%20In%20this%20paper%2C%20we%20show%20that%20for%20the%20%24L%5E2%24%20regression%0Aproblems%2C%20the%20learning%20rate%20can%20be%20improved%20from%20%24%5Cmathcal%7BO%7D%28%5Clambda_0/n%5E2%29%24%0Ato%20%24%5Cmathcal%7BO%7D%281/%5C%7C%5Cbm%7BH%7D%5E%7B%5Cinfty%7D%5C%7C_2%29%24%2C%20which%20implies%20that%20GD%20actually%0Aenjoys%20a%20faster%20convergence%20rate.%20Furthermore%2C%20we%20generalize%20the%20method%20to%20GD%0Ain%20training%20two-layer%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%2C%20showing%20a%0Asimilar%20improvement%20for%20the%20learning%20rate.%20Although%20the%20improved%20learning%20rate%0Ahas%20a%20mild%20dependence%20on%20the%20Gram%20matrix%2C%20we%20still%20need%20to%20set%20it%20small%20enough%0Ain%20practice%20due%20to%20the%20unknown%20eigenvalues%20of%20the%20Gram%20matrix.%20More%0Aimportantly%2C%20the%20convergence%20rate%20is%20tied%20to%20the%20least%20eigenvalue%20of%20the%20Gram%0Amatrix%2C%20which%20can%20lead%20to%20slow%20convergence.%20In%20this%20work%2C%20we%20provide%20the%0Aconvergence%20analysis%20of%20natural%20gradient%20descent%20%28NGD%29%20in%20training%20two-layer%0APINNs%2C%20demonstrating%20that%20the%20learning%20rate%20can%20be%20%24%5Cmathcal%7BO%7D%281%29%24%2C%20and%20at%0Athis%20rate%2C%20the%20convergence%20rate%20is%20independent%20of%20the%20Gram%20matrix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00573v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520Analysis%2520of%2520Natural%2520Gradient%2520Descent%2520for%2520Over-parameterized%250A%2520%2520Physics-Informed%2520Neural%2520Networks%26entry.906535625%3DXianliang%2520Xu%2520and%2520Ting%2520Du%2520and%2520Wang%2520Kong%2520and%2520Ye%2520Li%2520and%2520Zhongyi%2520Huang%26entry.1292438233%3D%2520%2520First-order%2520methods%252C%2520such%2520as%2520gradient%2520descent%2520%2528GD%2529%2520and%2520stochastic%2520gradient%250Adescent%2520%2528SGD%2529%252C%2520have%2520been%2520proven%2520effective%2520in%2520training%2520neural%2520networks.%2520In%2520the%250Acontext%2520of%2520over-parameterization%252C%2520there%2520is%2520a%2520line%2520of%2520work%2520demonstrating%2520that%250Arandomly%2520initialized%2520%2528stochastic%2529%2520gradient%2520descent%2520converges%2520to%2520a%2520globally%250Aoptimal%2520solution%2520at%2520a%2520linear%2520convergence%2520rate%2520for%2520the%2520quadratic%2520loss%2520function.%250AHowever%252C%2520the%2520learning%2520rate%2520of%2520GD%2520for%2520training%2520two-layer%2520neural%2520networks%250Aexhibits%2520poor%2520dependence%2520on%2520the%2520sample%2520size%2520and%2520the%2520Gram%2520matrix%252C%2520leading%2520to%2520a%250Aslow%2520training%2520process.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520for%2520the%2520%2524L%255E2%2524%2520regression%250Aproblems%252C%2520the%2520learning%2520rate%2520can%2520be%2520improved%2520from%2520%2524%255Cmathcal%257BO%257D%2528%255Clambda_0/n%255E2%2529%2524%250Ato%2520%2524%255Cmathcal%257BO%257D%25281/%255C%257C%255Cbm%257BH%257D%255E%257B%255Cinfty%257D%255C%257C_2%2529%2524%252C%2520which%2520implies%2520that%2520GD%2520actually%250Aenjoys%2520a%2520faster%2520convergence%2520rate.%2520Furthermore%252C%2520we%2520generalize%2520the%2520method%2520to%2520GD%250Ain%2520training%2520two-layer%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%252C%2520showing%2520a%250Asimilar%2520improvement%2520for%2520the%2520learning%2520rate.%2520Although%2520the%2520improved%2520learning%2520rate%250Ahas%2520a%2520mild%2520dependence%2520on%2520the%2520Gram%2520matrix%252C%2520we%2520still%2520need%2520to%2520set%2520it%2520small%2520enough%250Ain%2520practice%2520due%2520to%2520the%2520unknown%2520eigenvalues%2520of%2520the%2520Gram%2520matrix.%2520More%250Aimportantly%252C%2520the%2520convergence%2520rate%2520is%2520tied%2520to%2520the%2520least%2520eigenvalue%2520of%2520the%2520Gram%250Amatrix%252C%2520which%2520can%2520lead%2520to%2520slow%2520convergence.%2520In%2520this%2520work%252C%2520we%2520provide%2520the%250Aconvergence%2520analysis%2520of%2520natural%2520gradient%2520descent%2520%2528NGD%2529%2520in%2520training%2520two-layer%250APINNs%252C%2520demonstrating%2520that%2520the%2520learning%2520rate%2520can%2520be%2520%2524%255Cmathcal%257BO%257D%25281%2529%2524%252C%2520and%2520at%250Athis%2520rate%252C%2520the%2520convergence%2520rate%2520is%2520independent%2520of%2520the%2520Gram%2520matrix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00573v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20Analysis%20of%20Natural%20Gradient%20Descent%20for%20Over-parameterized%0A%20%20Physics-Informed%20Neural%20Networks&entry.906535625=Xianliang%20Xu%20and%20Ting%20Du%20and%20Wang%20Kong%20and%20Ye%20Li%20and%20Zhongyi%20Huang&entry.1292438233=%20%20First-order%20methods%2C%20such%20as%20gradient%20descent%20%28GD%29%20and%20stochastic%20gradient%0Adescent%20%28SGD%29%2C%20have%20been%20proven%20effective%20in%20training%20neural%20networks.%20In%20the%0Acontext%20of%20over-parameterization%2C%20there%20is%20a%20line%20of%20work%20demonstrating%20that%0Arandomly%20initialized%20%28stochastic%29%20gradient%20descent%20converges%20to%20a%20globally%0Aoptimal%20solution%20at%20a%20linear%20convergence%20rate%20for%20the%20quadratic%20loss%20function.%0AHowever%2C%20the%20learning%20rate%20of%20GD%20for%20training%20two-layer%20neural%20networks%0Aexhibits%20poor%20dependence%20on%20the%20sample%20size%20and%20the%20Gram%20matrix%2C%20leading%20to%20a%0Aslow%20training%20process.%20In%20this%20paper%2C%20we%20show%20that%20for%20the%20%24L%5E2%24%20regression%0Aproblems%2C%20the%20learning%20rate%20can%20be%20improved%20from%20%24%5Cmathcal%7BO%7D%28%5Clambda_0/n%5E2%29%24%0Ato%20%24%5Cmathcal%7BO%7D%281/%5C%7C%5Cbm%7BH%7D%5E%7B%5Cinfty%7D%5C%7C_2%29%24%2C%20which%20implies%20that%20GD%20actually%0Aenjoys%20a%20faster%20convergence%20rate.%20Furthermore%2C%20we%20generalize%20the%20method%20to%20GD%0Ain%20training%20two-layer%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%2C%20showing%20a%0Asimilar%20improvement%20for%20the%20learning%20rate.%20Although%20the%20improved%20learning%20rate%0Ahas%20a%20mild%20dependence%20on%20the%20Gram%20matrix%2C%20we%20still%20need%20to%20set%20it%20small%20enough%0Ain%20practice%20due%20to%20the%20unknown%20eigenvalues%20of%20the%20Gram%20matrix.%20More%0Aimportantly%2C%20the%20convergence%20rate%20is%20tied%20to%20the%20least%20eigenvalue%20of%20the%20Gram%0Amatrix%2C%20which%20can%20lead%20to%20slow%20convergence.%20In%20this%20work%2C%20we%20provide%20the%0Aconvergence%20analysis%20of%20natural%20gradient%20descent%20%28NGD%29%20in%20training%20two-layer%0APINNs%2C%20demonstrating%20that%20the%20learning%20rate%20can%20be%20%24%5Cmathcal%7BO%7D%281%29%24%2C%20and%20at%0Athis%20rate%2C%20the%20convergence%20rate%20is%20independent%20of%20the%20Gram%20matrix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00573v2&entry.124074799=Read"},
{"title": "Hummer: Towards Limited Competitive Preference Dataset", "author": "Li Jiang and Yusen Wu and Junwu Xiong and Jingqing Ruan and Yichuan Ding and Qingpei Guo and Zujie Wen and Jun Zhou and Xiaotie Deng", "abstract": "  Preference datasets are essential for incorporating human preferences into\npre-trained language models, playing a key role in the success of Reinforcement\nLearning from Human Feedback. However, these datasets often demonstrate\nconflicting alignment objectives, leading to increased vulnerability to\njailbreak attacks and challenges in adapting downstream tasks to prioritize\nspecific alignment objectives without negatively impacting others. In this\nwork, we introduce a novel statistical metric, Alignment Dimension Conflict, to\nquantify the degree of conflict within preference datasets. We then present\n\\texttt{Hummer} and its fine-grained variant, \\texttt{Hummer-F}, as innovative\npairwise preference datasets with reduced-conflict alignment objectives.\n\\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback\nfrom GPT-4, marking as the first preference dataset aimed at reducing the\ncompetition between alignment objectives. Furthermore, we develop reward\nmodels, HummerRM and HummerRM-F, which employ a hybrid sampling approach to\nbalance diverse alignment objectives effectively. This sampling method\npositions HummerRM as an ideal model for domain-specific further fine-tuning\nand reducing vulnerabilities to attacks.\n", "link": "http://arxiv.org/abs/2405.11647v3", "date": "2024-08-06", "relevancy": 1.9079, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4807}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4786}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hummer%3A%20Towards%20Limited%20Competitive%20Preference%20Dataset&body=Title%3A%20Hummer%3A%20Towards%20Limited%20Competitive%20Preference%20Dataset%0AAuthor%3A%20Li%20Jiang%20and%20Yusen%20Wu%20and%20Junwu%20Xiong%20and%20Jingqing%20Ruan%20and%20Yichuan%20Ding%20and%20Qingpei%20Guo%20and%20Zujie%20Wen%20and%20Jun%20Zhou%20and%20Xiaotie%20Deng%0AAbstract%3A%20%20%20Preference%20datasets%20are%20essential%20for%20incorporating%20human%20preferences%20into%0Apre-trained%20language%20models%2C%20playing%20a%20key%20role%20in%20the%20success%20of%20Reinforcement%0ALearning%20from%20Human%20Feedback.%20However%2C%20these%20datasets%20often%20demonstrate%0Aconflicting%20alignment%20objectives%2C%20leading%20to%20increased%20vulnerability%20to%0Ajailbreak%20attacks%20and%20challenges%20in%20adapting%20downstream%20tasks%20to%20prioritize%0Aspecific%20alignment%20objectives%20without%20negatively%20impacting%20others.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20statistical%20metric%2C%20Alignment%20Dimension%20Conflict%2C%20to%0Aquantify%20the%20degree%20of%20conflict%20within%20preference%20datasets.%20We%20then%20present%0A%5Ctexttt%7BHummer%7D%20and%20its%20fine-grained%20variant%2C%20%5Ctexttt%7BHummer-F%7D%2C%20as%20innovative%0Apairwise%20preference%20datasets%20with%20reduced-conflict%20alignment%20objectives.%0A%5Ctexttt%7BHummer%7D%20is%20built%20based%20on%20UltraFeedback%20and%20is%20enhanced%20by%20AI%20feedback%0Afrom%20GPT-4%2C%20marking%20as%20the%20first%20preference%20dataset%20aimed%20at%20reducing%20the%0Acompetition%20between%20alignment%20objectives.%20Furthermore%2C%20we%20develop%20reward%0Amodels%2C%20HummerRM%20and%20HummerRM-F%2C%20which%20employ%20a%20hybrid%20sampling%20approach%20to%0Abalance%20diverse%20alignment%20objectives%20effectively.%20This%20sampling%20method%0Apositions%20HummerRM%20as%20an%20ideal%20model%20for%20domain-specific%20further%20fine-tuning%0Aand%20reducing%20vulnerabilities%20to%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11647v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHummer%253A%2520Towards%2520Limited%2520Competitive%2520Preference%2520Dataset%26entry.906535625%3DLi%2520Jiang%2520and%2520Yusen%2520Wu%2520and%2520Junwu%2520Xiong%2520and%2520Jingqing%2520Ruan%2520and%2520Yichuan%2520Ding%2520and%2520Qingpei%2520Guo%2520and%2520Zujie%2520Wen%2520and%2520Jun%2520Zhou%2520and%2520Xiaotie%2520Deng%26entry.1292438233%3D%2520%2520Preference%2520datasets%2520are%2520essential%2520for%2520incorporating%2520human%2520preferences%2520into%250Apre-trained%2520language%2520models%252C%2520playing%2520a%2520key%2520role%2520in%2520the%2520success%2520of%2520Reinforcement%250ALearning%2520from%2520Human%2520Feedback.%2520However%252C%2520these%2520datasets%2520often%2520demonstrate%250Aconflicting%2520alignment%2520objectives%252C%2520leading%2520to%2520increased%2520vulnerability%2520to%250Ajailbreak%2520attacks%2520and%2520challenges%2520in%2520adapting%2520downstream%2520tasks%2520to%2520prioritize%250Aspecific%2520alignment%2520objectives%2520without%2520negatively%2520impacting%2520others.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520novel%2520statistical%2520metric%252C%2520Alignment%2520Dimension%2520Conflict%252C%2520to%250Aquantify%2520the%2520degree%2520of%2520conflict%2520within%2520preference%2520datasets.%2520We%2520then%2520present%250A%255Ctexttt%257BHummer%257D%2520and%2520its%2520fine-grained%2520variant%252C%2520%255Ctexttt%257BHummer-F%257D%252C%2520as%2520innovative%250Apairwise%2520preference%2520datasets%2520with%2520reduced-conflict%2520alignment%2520objectives.%250A%255Ctexttt%257BHummer%257D%2520is%2520built%2520based%2520on%2520UltraFeedback%2520and%2520is%2520enhanced%2520by%2520AI%2520feedback%250Afrom%2520GPT-4%252C%2520marking%2520as%2520the%2520first%2520preference%2520dataset%2520aimed%2520at%2520reducing%2520the%250Acompetition%2520between%2520alignment%2520objectives.%2520Furthermore%252C%2520we%2520develop%2520reward%250Amodels%252C%2520HummerRM%2520and%2520HummerRM-F%252C%2520which%2520employ%2520a%2520hybrid%2520sampling%2520approach%2520to%250Abalance%2520diverse%2520alignment%2520objectives%2520effectively.%2520This%2520sampling%2520method%250Apositions%2520HummerRM%2520as%2520an%2520ideal%2520model%2520for%2520domain-specific%2520further%2520fine-tuning%250Aand%2520reducing%2520vulnerabilities%2520to%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11647v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hummer%3A%20Towards%20Limited%20Competitive%20Preference%20Dataset&entry.906535625=Li%20Jiang%20and%20Yusen%20Wu%20and%20Junwu%20Xiong%20and%20Jingqing%20Ruan%20and%20Yichuan%20Ding%20and%20Qingpei%20Guo%20and%20Zujie%20Wen%20and%20Jun%20Zhou%20and%20Xiaotie%20Deng&entry.1292438233=%20%20Preference%20datasets%20are%20essential%20for%20incorporating%20human%20preferences%20into%0Apre-trained%20language%20models%2C%20playing%20a%20key%20role%20in%20the%20success%20of%20Reinforcement%0ALearning%20from%20Human%20Feedback.%20However%2C%20these%20datasets%20often%20demonstrate%0Aconflicting%20alignment%20objectives%2C%20leading%20to%20increased%20vulnerability%20to%0Ajailbreak%20attacks%20and%20challenges%20in%20adapting%20downstream%20tasks%20to%20prioritize%0Aspecific%20alignment%20objectives%20without%20negatively%20impacting%20others.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20statistical%20metric%2C%20Alignment%20Dimension%20Conflict%2C%20to%0Aquantify%20the%20degree%20of%20conflict%20within%20preference%20datasets.%20We%20then%20present%0A%5Ctexttt%7BHummer%7D%20and%20its%20fine-grained%20variant%2C%20%5Ctexttt%7BHummer-F%7D%2C%20as%20innovative%0Apairwise%20preference%20datasets%20with%20reduced-conflict%20alignment%20objectives.%0A%5Ctexttt%7BHummer%7D%20is%20built%20based%20on%20UltraFeedback%20and%20is%20enhanced%20by%20AI%20feedback%0Afrom%20GPT-4%2C%20marking%20as%20the%20first%20preference%20dataset%20aimed%20at%20reducing%20the%0Acompetition%20between%20alignment%20objectives.%20Furthermore%2C%20we%20develop%20reward%0Amodels%2C%20HummerRM%20and%20HummerRM-F%2C%20which%20employ%20a%20hybrid%20sampling%20approach%20to%0Abalance%20diverse%20alignment%20objectives%20effectively.%20This%20sampling%20method%0Apositions%20HummerRM%20as%20an%20ideal%20model%20for%20domain-specific%20further%20fine-tuning%0Aand%20reducing%20vulnerabilities%20to%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11647v3&entry.124074799=Read"},
{"title": "ClassiFIM: An Unsupervised Method To Detect Phase Transitions", "author": "Victor Kasatkin and Evgeny Mozgunov and Nicholas Ezzell and Utkarsh Mishra and Itay Hen and Daniel Lidar", "abstract": "  Estimation of the Fisher Information Metric (FIM-estimation) is an important\ntask that arises in unsupervised learning of phase transitions, a problem\nproposed by physicists. This work completes the definition of the task by\ndefining rigorous evaluation metrics distMSE, distMSEPS, and distRE and\nintroduces ClassiFIM, a novel machine learning method designed to solve the\nFIM-estimation task. Unlike existing methods for unsupervised learning of phase\ntransitions, ClassiFIM directly estimates a well-defined quantity (the FIM),\nallowing it to be rigorously compared to any present and future other methods\nthat estimate the same. ClassiFIM transforms a dataset for the FIM-estimation\ntask into a dataset for an auxiliary binary classification task and involves\nselecting and training a model for the latter. We prove that the output of\nClassiFIM approaches the exact FIM in the limit of infinite dataset size and\nunder certain regularity conditions. We implement ClassiFIM on multiple\ndatasets, including datasets describing classical and quantum phase\ntransitions, and find that it achieves a good ground truth approximation with\nmodest computational resources. Furthermore, we independently implement two\nalternative state-of-the-art methods for unsupervised estimation of phase\ntransition locations on the same datasets and find that ClassiFIM predicts such\nlocations at least as well as these other methods. To emphasize the generality\nof our method, we also propose and generate the MNIST-CNN dataset, which\nconsists of the output of CNNs trained on MNIST for different hyperparameter\nchoices. Using ClassiFIM on this dataset suggests there is a phase transition\nin the distribution of image-prediction pairs for CNNs trained on MNIST,\ndemonstrating the broad scope of FIM-estimation beyond physics.\n", "link": "http://arxiv.org/abs/2408.03323v1", "date": "2024-08-06", "relevancy": 1.8996, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4906}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4828}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClassiFIM%3A%20An%20Unsupervised%20Method%20To%20Detect%20Phase%20Transitions&body=Title%3A%20ClassiFIM%3A%20An%20Unsupervised%20Method%20To%20Detect%20Phase%20Transitions%0AAuthor%3A%20Victor%20Kasatkin%20and%20Evgeny%20Mozgunov%20and%20Nicholas%20Ezzell%20and%20Utkarsh%20Mishra%20and%20Itay%20Hen%20and%20Daniel%20Lidar%0AAbstract%3A%20%20%20Estimation%20of%20the%20Fisher%20Information%20Metric%20%28FIM-estimation%29%20is%20an%20important%0Atask%20that%20arises%20in%20unsupervised%20learning%20of%20phase%20transitions%2C%20a%20problem%0Aproposed%20by%20physicists.%20This%20work%20completes%20the%20definition%20of%20the%20task%20by%0Adefining%20rigorous%20evaluation%20metrics%20distMSE%2C%20distMSEPS%2C%20and%20distRE%20and%0Aintroduces%20ClassiFIM%2C%20a%20novel%20machine%20learning%20method%20designed%20to%20solve%20the%0AFIM-estimation%20task.%20Unlike%20existing%20methods%20for%20unsupervised%20learning%20of%20phase%0Atransitions%2C%20ClassiFIM%20directly%20estimates%20a%20well-defined%20quantity%20%28the%20FIM%29%2C%0Aallowing%20it%20to%20be%20rigorously%20compared%20to%20any%20present%20and%20future%20other%20methods%0Athat%20estimate%20the%20same.%20ClassiFIM%20transforms%20a%20dataset%20for%20the%20FIM-estimation%0Atask%20into%20a%20dataset%20for%20an%20auxiliary%20binary%20classification%20task%20and%20involves%0Aselecting%20and%20training%20a%20model%20for%20the%20latter.%20We%20prove%20that%20the%20output%20of%0AClassiFIM%20approaches%20the%20exact%20FIM%20in%20the%20limit%20of%20infinite%20dataset%20size%20and%0Aunder%20certain%20regularity%20conditions.%20We%20implement%20ClassiFIM%20on%20multiple%0Adatasets%2C%20including%20datasets%20describing%20classical%20and%20quantum%20phase%0Atransitions%2C%20and%20find%20that%20it%20achieves%20a%20good%20ground%20truth%20approximation%20with%0Amodest%20computational%20resources.%20Furthermore%2C%20we%20independently%20implement%20two%0Aalternative%20state-of-the-art%20methods%20for%20unsupervised%20estimation%20of%20phase%0Atransition%20locations%20on%20the%20same%20datasets%20and%20find%20that%20ClassiFIM%20predicts%20such%0Alocations%20at%20least%20as%20well%20as%20these%20other%20methods.%20To%20emphasize%20the%20generality%0Aof%20our%20method%2C%20we%20also%20propose%20and%20generate%20the%20MNIST-CNN%20dataset%2C%20which%0Aconsists%20of%20the%20output%20of%20CNNs%20trained%20on%20MNIST%20for%20different%20hyperparameter%0Achoices.%20Using%20ClassiFIM%20on%20this%20dataset%20suggests%20there%20is%20a%20phase%20transition%0Ain%20the%20distribution%20of%20image-prediction%20pairs%20for%20CNNs%20trained%20on%20MNIST%2C%0Ademonstrating%20the%20broad%20scope%20of%20FIM-estimation%20beyond%20physics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassiFIM%253A%2520An%2520Unsupervised%2520Method%2520To%2520Detect%2520Phase%2520Transitions%26entry.906535625%3DVictor%2520Kasatkin%2520and%2520Evgeny%2520Mozgunov%2520and%2520Nicholas%2520Ezzell%2520and%2520Utkarsh%2520Mishra%2520and%2520Itay%2520Hen%2520and%2520Daniel%2520Lidar%26entry.1292438233%3D%2520%2520Estimation%2520of%2520the%2520Fisher%2520Information%2520Metric%2520%2528FIM-estimation%2529%2520is%2520an%2520important%250Atask%2520that%2520arises%2520in%2520unsupervised%2520learning%2520of%2520phase%2520transitions%252C%2520a%2520problem%250Aproposed%2520by%2520physicists.%2520This%2520work%2520completes%2520the%2520definition%2520of%2520the%2520task%2520by%250Adefining%2520rigorous%2520evaluation%2520metrics%2520distMSE%252C%2520distMSEPS%252C%2520and%2520distRE%2520and%250Aintroduces%2520ClassiFIM%252C%2520a%2520novel%2520machine%2520learning%2520method%2520designed%2520to%2520solve%2520the%250AFIM-estimation%2520task.%2520Unlike%2520existing%2520methods%2520for%2520unsupervised%2520learning%2520of%2520phase%250Atransitions%252C%2520ClassiFIM%2520directly%2520estimates%2520a%2520well-defined%2520quantity%2520%2528the%2520FIM%2529%252C%250Aallowing%2520it%2520to%2520be%2520rigorously%2520compared%2520to%2520any%2520present%2520and%2520future%2520other%2520methods%250Athat%2520estimate%2520the%2520same.%2520ClassiFIM%2520transforms%2520a%2520dataset%2520for%2520the%2520FIM-estimation%250Atask%2520into%2520a%2520dataset%2520for%2520an%2520auxiliary%2520binary%2520classification%2520task%2520and%2520involves%250Aselecting%2520and%2520training%2520a%2520model%2520for%2520the%2520latter.%2520We%2520prove%2520that%2520the%2520output%2520of%250AClassiFIM%2520approaches%2520the%2520exact%2520FIM%2520in%2520the%2520limit%2520of%2520infinite%2520dataset%2520size%2520and%250Aunder%2520certain%2520regularity%2520conditions.%2520We%2520implement%2520ClassiFIM%2520on%2520multiple%250Adatasets%252C%2520including%2520datasets%2520describing%2520classical%2520and%2520quantum%2520phase%250Atransitions%252C%2520and%2520find%2520that%2520it%2520achieves%2520a%2520good%2520ground%2520truth%2520approximation%2520with%250Amodest%2520computational%2520resources.%2520Furthermore%252C%2520we%2520independently%2520implement%2520two%250Aalternative%2520state-of-the-art%2520methods%2520for%2520unsupervised%2520estimation%2520of%2520phase%250Atransition%2520locations%2520on%2520the%2520same%2520datasets%2520and%2520find%2520that%2520ClassiFIM%2520predicts%2520such%250Alocations%2520at%2520least%2520as%2520well%2520as%2520these%2520other%2520methods.%2520To%2520emphasize%2520the%2520generality%250Aof%2520our%2520method%252C%2520we%2520also%2520propose%2520and%2520generate%2520the%2520MNIST-CNN%2520dataset%252C%2520which%250Aconsists%2520of%2520the%2520output%2520of%2520CNNs%2520trained%2520on%2520MNIST%2520for%2520different%2520hyperparameter%250Achoices.%2520Using%2520ClassiFIM%2520on%2520this%2520dataset%2520suggests%2520there%2520is%2520a%2520phase%2520transition%250Ain%2520the%2520distribution%2520of%2520image-prediction%2520pairs%2520for%2520CNNs%2520trained%2520on%2520MNIST%252C%250Ademonstrating%2520the%2520broad%2520scope%2520of%2520FIM-estimation%2520beyond%2520physics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClassiFIM%3A%20An%20Unsupervised%20Method%20To%20Detect%20Phase%20Transitions&entry.906535625=Victor%20Kasatkin%20and%20Evgeny%20Mozgunov%20and%20Nicholas%20Ezzell%20and%20Utkarsh%20Mishra%20and%20Itay%20Hen%20and%20Daniel%20Lidar&entry.1292438233=%20%20Estimation%20of%20the%20Fisher%20Information%20Metric%20%28FIM-estimation%29%20is%20an%20important%0Atask%20that%20arises%20in%20unsupervised%20learning%20of%20phase%20transitions%2C%20a%20problem%0Aproposed%20by%20physicists.%20This%20work%20completes%20the%20definition%20of%20the%20task%20by%0Adefining%20rigorous%20evaluation%20metrics%20distMSE%2C%20distMSEPS%2C%20and%20distRE%20and%0Aintroduces%20ClassiFIM%2C%20a%20novel%20machine%20learning%20method%20designed%20to%20solve%20the%0AFIM-estimation%20task.%20Unlike%20existing%20methods%20for%20unsupervised%20learning%20of%20phase%0Atransitions%2C%20ClassiFIM%20directly%20estimates%20a%20well-defined%20quantity%20%28the%20FIM%29%2C%0Aallowing%20it%20to%20be%20rigorously%20compared%20to%20any%20present%20and%20future%20other%20methods%0Athat%20estimate%20the%20same.%20ClassiFIM%20transforms%20a%20dataset%20for%20the%20FIM-estimation%0Atask%20into%20a%20dataset%20for%20an%20auxiliary%20binary%20classification%20task%20and%20involves%0Aselecting%20and%20training%20a%20model%20for%20the%20latter.%20We%20prove%20that%20the%20output%20of%0AClassiFIM%20approaches%20the%20exact%20FIM%20in%20the%20limit%20of%20infinite%20dataset%20size%20and%0Aunder%20certain%20regularity%20conditions.%20We%20implement%20ClassiFIM%20on%20multiple%0Adatasets%2C%20including%20datasets%20describing%20classical%20and%20quantum%20phase%0Atransitions%2C%20and%20find%20that%20it%20achieves%20a%20good%20ground%20truth%20approximation%20with%0Amodest%20computational%20resources.%20Furthermore%2C%20we%20independently%20implement%20two%0Aalternative%20state-of-the-art%20methods%20for%20unsupervised%20estimation%20of%20phase%0Atransition%20locations%20on%20the%20same%20datasets%20and%20find%20that%20ClassiFIM%20predicts%20such%0Alocations%20at%20least%20as%20well%20as%20these%20other%20methods.%20To%20emphasize%20the%20generality%0Aof%20our%20method%2C%20we%20also%20propose%20and%20generate%20the%20MNIST-CNN%20dataset%2C%20which%0Aconsists%20of%20the%20output%20of%20CNNs%20trained%20on%20MNIST%20for%20different%20hyperparameter%0Achoices.%20Using%20ClassiFIM%20on%20this%20dataset%20suggests%20there%20is%20a%20phase%20transition%0Ain%20the%20distribution%20of%20image-prediction%20pairs%20for%20CNNs%20trained%20on%20MNIST%2C%0Ademonstrating%20the%20broad%20scope%20of%20FIM-estimation%20beyond%20physics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03323v1&entry.124074799=Read"},
{"title": "T-Explainer: A Model-Agnostic Explainability Framework Based on\n  Gradients", "author": "Evandro S. Ortigossa and F\u00e1bio F. Dias and Brian Barr and Claudio T. Silva and Luis Gustavo Nonato", "abstract": "  The development of machine learning applications has increased significantly\nin recent years, motivated by the remarkable ability of learning-powered\nsystems to discover and generalize intricate patterns hidden in massive\ndatasets. Modern learning models, while powerful, often have a level of\ncomplexity that renders them opaque black boxes, resulting in a notable lack of\ntransparency that hinders our ability to decipher their reasoning. Opacity\nchallenges the interpretability and practical application of machine learning,\nespecially in critical domains where understanding the underlying reasons is\nessential for informed decision-making. Explainable Artificial Intelligence\n(XAI) rises to address that challenge, unraveling the complexity of black boxes\nby providing elucidating explanations. Among the various XAI approaches,\nfeature attribution/importance stands out for its capacity to delineate the\nsignificance of input features in the prediction process. However, most\nexisting attribution methods have limitations, such as instability, when\ndivergent explanations may result from similar or even the same instance. This\nwork introduces T-Explainer, a novel local additive attribution explainer based\non Taylor expansion. It has desirable properties, such as local accuracy and\nconsistency, making T-Explainer stable over multiple runs. We demonstrate\nT-Explainer's effectiveness in quantitative benchmark experiments against\nwell-known attribution methods. Additionally, we provide several tools to\nevaluate and visualize explanations, turning T-Explainer into a comprehensive\nXAI framework.\n", "link": "http://arxiv.org/abs/2404.16495v2", "date": "2024-08-06", "relevancy": 1.8993, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4982}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4733}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-Explainer%3A%20A%20Model-Agnostic%20Explainability%20Framework%20Based%20on%0A%20%20Gradients&body=Title%3A%20T-Explainer%3A%20A%20Model-Agnostic%20Explainability%20Framework%20Based%20on%0A%20%20Gradients%0AAuthor%3A%20Evandro%20S.%20Ortigossa%20and%20F%C3%A1bio%20F.%20Dias%20and%20Brian%20Barr%20and%20Claudio%20T.%20Silva%20and%20Luis%20Gustavo%20Nonato%0AAbstract%3A%20%20%20The%20development%20of%20machine%20learning%20applications%20has%20increased%20significantly%0Ain%20recent%20years%2C%20motivated%20by%20the%20remarkable%20ability%20of%20learning-powered%0Asystems%20to%20discover%20and%20generalize%20intricate%20patterns%20hidden%20in%20massive%0Adatasets.%20Modern%20learning%20models%2C%20while%20powerful%2C%20often%20have%20a%20level%20of%0Acomplexity%20that%20renders%20them%20opaque%20black%20boxes%2C%20resulting%20in%20a%20notable%20lack%20of%0Atransparency%20that%20hinders%20our%20ability%20to%20decipher%20their%20reasoning.%20Opacity%0Achallenges%20the%20interpretability%20and%20practical%20application%20of%20machine%20learning%2C%0Aespecially%20in%20critical%20domains%20where%20understanding%20the%20underlying%20reasons%20is%0Aessential%20for%20informed%20decision-making.%20Explainable%20Artificial%20Intelligence%0A%28XAI%29%20rises%20to%20address%20that%20challenge%2C%20unraveling%20the%20complexity%20of%20black%20boxes%0Aby%20providing%20elucidating%20explanations.%20Among%20the%20various%20XAI%20approaches%2C%0Afeature%20attribution/importance%20stands%20out%20for%20its%20capacity%20to%20delineate%20the%0Asignificance%20of%20input%20features%20in%20the%20prediction%20process.%20However%2C%20most%0Aexisting%20attribution%20methods%20have%20limitations%2C%20such%20as%20instability%2C%20when%0Adivergent%20explanations%20may%20result%20from%20similar%20or%20even%20the%20same%20instance.%20This%0Awork%20introduces%20T-Explainer%2C%20a%20novel%20local%20additive%20attribution%20explainer%20based%0Aon%20Taylor%20expansion.%20It%20has%20desirable%20properties%2C%20such%20as%20local%20accuracy%20and%0Aconsistency%2C%20making%20T-Explainer%20stable%20over%20multiple%20runs.%20We%20demonstrate%0AT-Explainer%27s%20effectiveness%20in%20quantitative%20benchmark%20experiments%20against%0Awell-known%20attribution%20methods.%20Additionally%2C%20we%20provide%20several%20tools%20to%0Aevaluate%20and%20visualize%20explanations%2C%20turning%20T-Explainer%20into%20a%20comprehensive%0AXAI%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16495v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-Explainer%253A%2520A%2520Model-Agnostic%2520Explainability%2520Framework%2520Based%2520on%250A%2520%2520Gradients%26entry.906535625%3DEvandro%2520S.%2520Ortigossa%2520and%2520F%25C3%25A1bio%2520F.%2520Dias%2520and%2520Brian%2520Barr%2520and%2520Claudio%2520T.%2520Silva%2520and%2520Luis%2520Gustavo%2520Nonato%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520machine%2520learning%2520applications%2520has%2520increased%2520significantly%250Ain%2520recent%2520years%252C%2520motivated%2520by%2520the%2520remarkable%2520ability%2520of%2520learning-powered%250Asystems%2520to%2520discover%2520and%2520generalize%2520intricate%2520patterns%2520hidden%2520in%2520massive%250Adatasets.%2520Modern%2520learning%2520models%252C%2520while%2520powerful%252C%2520often%2520have%2520a%2520level%2520of%250Acomplexity%2520that%2520renders%2520them%2520opaque%2520black%2520boxes%252C%2520resulting%2520in%2520a%2520notable%2520lack%2520of%250Atransparency%2520that%2520hinders%2520our%2520ability%2520to%2520decipher%2520their%2520reasoning.%2520Opacity%250Achallenges%2520the%2520interpretability%2520and%2520practical%2520application%2520of%2520machine%2520learning%252C%250Aespecially%2520in%2520critical%2520domains%2520where%2520understanding%2520the%2520underlying%2520reasons%2520is%250Aessential%2520for%2520informed%2520decision-making.%2520Explainable%2520Artificial%2520Intelligence%250A%2528XAI%2529%2520rises%2520to%2520address%2520that%2520challenge%252C%2520unraveling%2520the%2520complexity%2520of%2520black%2520boxes%250Aby%2520providing%2520elucidating%2520explanations.%2520Among%2520the%2520various%2520XAI%2520approaches%252C%250Afeature%2520attribution/importance%2520stands%2520out%2520for%2520its%2520capacity%2520to%2520delineate%2520the%250Asignificance%2520of%2520input%2520features%2520in%2520the%2520prediction%2520process.%2520However%252C%2520most%250Aexisting%2520attribution%2520methods%2520have%2520limitations%252C%2520such%2520as%2520instability%252C%2520when%250Adivergent%2520explanations%2520may%2520result%2520from%2520similar%2520or%2520even%2520the%2520same%2520instance.%2520This%250Awork%2520introduces%2520T-Explainer%252C%2520a%2520novel%2520local%2520additive%2520attribution%2520explainer%2520based%250Aon%2520Taylor%2520expansion.%2520It%2520has%2520desirable%2520properties%252C%2520such%2520as%2520local%2520accuracy%2520and%250Aconsistency%252C%2520making%2520T-Explainer%2520stable%2520over%2520multiple%2520runs.%2520We%2520demonstrate%250AT-Explainer%2527s%2520effectiveness%2520in%2520quantitative%2520benchmark%2520experiments%2520against%250Awell-known%2520attribution%2520methods.%2520Additionally%252C%2520we%2520provide%2520several%2520tools%2520to%250Aevaluate%2520and%2520visualize%2520explanations%252C%2520turning%2520T-Explainer%2520into%2520a%2520comprehensive%250AXAI%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16495v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-Explainer%3A%20A%20Model-Agnostic%20Explainability%20Framework%20Based%20on%0A%20%20Gradients&entry.906535625=Evandro%20S.%20Ortigossa%20and%20F%C3%A1bio%20F.%20Dias%20and%20Brian%20Barr%20and%20Claudio%20T.%20Silva%20and%20Luis%20Gustavo%20Nonato&entry.1292438233=%20%20The%20development%20of%20machine%20learning%20applications%20has%20increased%20significantly%0Ain%20recent%20years%2C%20motivated%20by%20the%20remarkable%20ability%20of%20learning-powered%0Asystems%20to%20discover%20and%20generalize%20intricate%20patterns%20hidden%20in%20massive%0Adatasets.%20Modern%20learning%20models%2C%20while%20powerful%2C%20often%20have%20a%20level%20of%0Acomplexity%20that%20renders%20them%20opaque%20black%20boxes%2C%20resulting%20in%20a%20notable%20lack%20of%0Atransparency%20that%20hinders%20our%20ability%20to%20decipher%20their%20reasoning.%20Opacity%0Achallenges%20the%20interpretability%20and%20practical%20application%20of%20machine%20learning%2C%0Aespecially%20in%20critical%20domains%20where%20understanding%20the%20underlying%20reasons%20is%0Aessential%20for%20informed%20decision-making.%20Explainable%20Artificial%20Intelligence%0A%28XAI%29%20rises%20to%20address%20that%20challenge%2C%20unraveling%20the%20complexity%20of%20black%20boxes%0Aby%20providing%20elucidating%20explanations.%20Among%20the%20various%20XAI%20approaches%2C%0Afeature%20attribution/importance%20stands%20out%20for%20its%20capacity%20to%20delineate%20the%0Asignificance%20of%20input%20features%20in%20the%20prediction%20process.%20However%2C%20most%0Aexisting%20attribution%20methods%20have%20limitations%2C%20such%20as%20instability%2C%20when%0Adivergent%20explanations%20may%20result%20from%20similar%20or%20even%20the%20same%20instance.%20This%0Awork%20introduces%20T-Explainer%2C%20a%20novel%20local%20additive%20attribution%20explainer%20based%0Aon%20Taylor%20expansion.%20It%20has%20desirable%20properties%2C%20such%20as%20local%20accuracy%20and%0Aconsistency%2C%20making%20T-Explainer%20stable%20over%20multiple%20runs.%20We%20demonstrate%0AT-Explainer%27s%20effectiveness%20in%20quantitative%20benchmark%20experiments%20against%0Awell-known%20attribution%20methods.%20Additionally%2C%20we%20provide%20several%20tools%20to%0Aevaluate%20and%20visualize%20explanations%2C%20turning%20T-Explainer%20into%20a%20comprehensive%0AXAI%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16495v2&entry.124074799=Read"},
{"title": "When a Relation Tells More Than a Concept: Exploring and Evaluating\n  Classifier Decisions with CoReX", "author": "Bettina Finzel and Patrick Hilme and Johannes Rabold and Ute Schmid", "abstract": "  Explanations for Convolutional Neural Networks (CNNs) based on relevance of\ninput pixels might be too unspecific to evaluate which and how input features\nimpact model decisions. Especially in complex real-world domains like biology,\nthe presence of specific concepts and of relations between concepts might be\ndiscriminating between classes. Pixel relevance is not expressive enough to\nconvey this type of information. In consequence, model evaluation is limited\nand relevant aspects present in the data and influencing the model decisions\nmight be overlooked. This work presents a novel method to explain and evaluate\nCNN models, which uses a concept- and relation-based explainer (CoReX). It\nexplains the predictive behavior of a model on a set of images by masking\n(ir-)relevant concepts from the decision-making process and by constraining\nrelations in a learned interpretable surrogate model. We test our approach with\nseveral image data sets and CNN architectures. Results show that CoReX\nexplanations are faithful to the CNN model in terms of predictive outcomes. We\nfurther demonstrate through a human evaluation that CoReX is a suitable tool\nfor generating combined explanations that help assessing the classification\nquality of CNNs. We further show that CoReX supports the identification and\nre-classification of incorrect or ambiguous classifications.\n", "link": "http://arxiv.org/abs/2405.01661v3", "date": "2024-08-06", "relevancy": 1.8941, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4785}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4759}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20a%20Relation%20Tells%20More%20Than%20a%20Concept%3A%20Exploring%20and%20Evaluating%0A%20%20Classifier%20Decisions%20with%20CoReX&body=Title%3A%20When%20a%20Relation%20Tells%20More%20Than%20a%20Concept%3A%20Exploring%20and%20Evaluating%0A%20%20Classifier%20Decisions%20with%20CoReX%0AAuthor%3A%20Bettina%20Finzel%20and%20Patrick%20Hilme%20and%20Johannes%20Rabold%20and%20Ute%20Schmid%0AAbstract%3A%20%20%20Explanations%20for%20Convolutional%20Neural%20Networks%20%28CNNs%29%20based%20on%20relevance%20of%0Ainput%20pixels%20might%20be%20too%20unspecific%20to%20evaluate%20which%20and%20how%20input%20features%0Aimpact%20model%20decisions.%20Especially%20in%20complex%20real-world%20domains%20like%20biology%2C%0Athe%20presence%20of%20specific%20concepts%20and%20of%20relations%20between%20concepts%20might%20be%0Adiscriminating%20between%20classes.%20Pixel%20relevance%20is%20not%20expressive%20enough%20to%0Aconvey%20this%20type%20of%20information.%20In%20consequence%2C%20model%20evaluation%20is%20limited%0Aand%20relevant%20aspects%20present%20in%20the%20data%20and%20influencing%20the%20model%20decisions%0Amight%20be%20overlooked.%20This%20work%20presents%20a%20novel%20method%20to%20explain%20and%20evaluate%0ACNN%20models%2C%20which%20uses%20a%20concept-%20and%20relation-based%20explainer%20%28CoReX%29.%20It%0Aexplains%20the%20predictive%20behavior%20of%20a%20model%20on%20a%20set%20of%20images%20by%20masking%0A%28ir-%29relevant%20concepts%20from%20the%20decision-making%20process%20and%20by%20constraining%0Arelations%20in%20a%20learned%20interpretable%20surrogate%20model.%20We%20test%20our%20approach%20with%0Aseveral%20image%20data%20sets%20and%20CNN%20architectures.%20Results%20show%20that%20CoReX%0Aexplanations%20are%20faithful%20to%20the%20CNN%20model%20in%20terms%20of%20predictive%20outcomes.%20We%0Afurther%20demonstrate%20through%20a%20human%20evaluation%20that%20CoReX%20is%20a%20suitable%20tool%0Afor%20generating%20combined%20explanations%20that%20help%20assessing%20the%20classification%0Aquality%20of%20CNNs.%20We%20further%20show%20that%20CoReX%20supports%20the%20identification%20and%0Are-classification%20of%20incorrect%20or%20ambiguous%20classifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01661v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520a%2520Relation%2520Tells%2520More%2520Than%2520a%2520Concept%253A%2520Exploring%2520and%2520Evaluating%250A%2520%2520Classifier%2520Decisions%2520with%2520CoReX%26entry.906535625%3DBettina%2520Finzel%2520and%2520Patrick%2520Hilme%2520and%2520Johannes%2520Rabold%2520and%2520Ute%2520Schmid%26entry.1292438233%3D%2520%2520Explanations%2520for%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520based%2520on%2520relevance%2520of%250Ainput%2520pixels%2520might%2520be%2520too%2520unspecific%2520to%2520evaluate%2520which%2520and%2520how%2520input%2520features%250Aimpact%2520model%2520decisions.%2520Especially%2520in%2520complex%2520real-world%2520domains%2520like%2520biology%252C%250Athe%2520presence%2520of%2520specific%2520concepts%2520and%2520of%2520relations%2520between%2520concepts%2520might%2520be%250Adiscriminating%2520between%2520classes.%2520Pixel%2520relevance%2520is%2520not%2520expressive%2520enough%2520to%250Aconvey%2520this%2520type%2520of%2520information.%2520In%2520consequence%252C%2520model%2520evaluation%2520is%2520limited%250Aand%2520relevant%2520aspects%2520present%2520in%2520the%2520data%2520and%2520influencing%2520the%2520model%2520decisions%250Amight%2520be%2520overlooked.%2520This%2520work%2520presents%2520a%2520novel%2520method%2520to%2520explain%2520and%2520evaluate%250ACNN%2520models%252C%2520which%2520uses%2520a%2520concept-%2520and%2520relation-based%2520explainer%2520%2528CoReX%2529.%2520It%250Aexplains%2520the%2520predictive%2520behavior%2520of%2520a%2520model%2520on%2520a%2520set%2520of%2520images%2520by%2520masking%250A%2528ir-%2529relevant%2520concepts%2520from%2520the%2520decision-making%2520process%2520and%2520by%2520constraining%250Arelations%2520in%2520a%2520learned%2520interpretable%2520surrogate%2520model.%2520We%2520test%2520our%2520approach%2520with%250Aseveral%2520image%2520data%2520sets%2520and%2520CNN%2520architectures.%2520Results%2520show%2520that%2520CoReX%250Aexplanations%2520are%2520faithful%2520to%2520the%2520CNN%2520model%2520in%2520terms%2520of%2520predictive%2520outcomes.%2520We%250Afurther%2520demonstrate%2520through%2520a%2520human%2520evaluation%2520that%2520CoReX%2520is%2520a%2520suitable%2520tool%250Afor%2520generating%2520combined%2520explanations%2520that%2520help%2520assessing%2520the%2520classification%250Aquality%2520of%2520CNNs.%2520We%2520further%2520show%2520that%2520CoReX%2520supports%2520the%2520identification%2520and%250Are-classification%2520of%2520incorrect%2520or%2520ambiguous%2520classifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01661v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20a%20Relation%20Tells%20More%20Than%20a%20Concept%3A%20Exploring%20and%20Evaluating%0A%20%20Classifier%20Decisions%20with%20CoReX&entry.906535625=Bettina%20Finzel%20and%20Patrick%20Hilme%20and%20Johannes%20Rabold%20and%20Ute%20Schmid&entry.1292438233=%20%20Explanations%20for%20Convolutional%20Neural%20Networks%20%28CNNs%29%20based%20on%20relevance%20of%0Ainput%20pixels%20might%20be%20too%20unspecific%20to%20evaluate%20which%20and%20how%20input%20features%0Aimpact%20model%20decisions.%20Especially%20in%20complex%20real-world%20domains%20like%20biology%2C%0Athe%20presence%20of%20specific%20concepts%20and%20of%20relations%20between%20concepts%20might%20be%0Adiscriminating%20between%20classes.%20Pixel%20relevance%20is%20not%20expressive%20enough%20to%0Aconvey%20this%20type%20of%20information.%20In%20consequence%2C%20model%20evaluation%20is%20limited%0Aand%20relevant%20aspects%20present%20in%20the%20data%20and%20influencing%20the%20model%20decisions%0Amight%20be%20overlooked.%20This%20work%20presents%20a%20novel%20method%20to%20explain%20and%20evaluate%0ACNN%20models%2C%20which%20uses%20a%20concept-%20and%20relation-based%20explainer%20%28CoReX%29.%20It%0Aexplains%20the%20predictive%20behavior%20of%20a%20model%20on%20a%20set%20of%20images%20by%20masking%0A%28ir-%29relevant%20concepts%20from%20the%20decision-making%20process%20and%20by%20constraining%0Arelations%20in%20a%20learned%20interpretable%20surrogate%20model.%20We%20test%20our%20approach%20with%0Aseveral%20image%20data%20sets%20and%20CNN%20architectures.%20Results%20show%20that%20CoReX%0Aexplanations%20are%20faithful%20to%20the%20CNN%20model%20in%20terms%20of%20predictive%20outcomes.%20We%0Afurther%20demonstrate%20through%20a%20human%20evaluation%20that%20CoReX%20is%20a%20suitable%20tool%0Afor%20generating%20combined%20explanations%20that%20help%20assessing%20the%20classification%0Aquality%20of%20CNNs.%20We%20further%20show%20that%20CoReX%20supports%20the%20identification%20and%0Are-classification%20of%20incorrect%20or%20ambiguous%20classifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01661v3&entry.124074799=Read"},
{"title": "DiffX: Guide Your Layout to Cross-Modal Generative Modeling", "author": "Zeyu Wang and Jingyu Lin and Yifei Qian and Yi Huang and Shicen Tian and Bosong Chai and Juncan Deng and Lan Du and Cunjian Chen and Yufei Guo and Kejie Huang", "abstract": "  Diffusion models have made significant strides in language-driven and\nlayout-driven image generation. However, most diffusion models are limited to\nvisible RGB image generation. In fact, human perception of the world is\nenriched by diverse viewpoints, such as chromatic contrast, thermal\nillumination, and depth information. In this paper, we introduce a novel\ndiffusion model for general layout-guided cross-modal generation, called DiffX.\nNotably, DiffX presents a simple yet effective cross-modal generative modeling\npipeline, which conducts diffusion and denoising processes in the\nmodality-shared latent space. Moreover, we introduce the Joint-Modality\nEmbedder (JME) to enhance interaction between layout and text conditions by\nincorporating a gated attention mechanism. Meanwhile, the advanced Long-CLIP is\nemployed for long caption embedding for user instruction. To facilitate the\nuser-instructed generative training, we construct the cross-modal image\ndatasets with detailed text captions assisted by the Large-Multimodal Model\n(LMM). Through extensive experiments, DiffX demonstrates robustness in\ncross-modal generation across three ``RGB+X'' datasets: FLIR, MFNet, and\nCOME15K, guided by various layout conditions. It also shows the potential for\nthe adaptive generation of ``RGB+X+Y+Z'' images or more diverse modalities on\nCOME15K and MCXFace datasets. Our code and constructed cross-modal image\ndatasets are available at https://github.com/zeyuwang-zju/DiffX.\n", "link": "http://arxiv.org/abs/2407.15488v3", "date": "2024-08-06", "relevancy": 1.88, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6578}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6495}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffX%3A%20Guide%20Your%20Layout%20to%20Cross-Modal%20Generative%20Modeling&body=Title%3A%20DiffX%3A%20Guide%20Your%20Layout%20to%20Cross-Modal%20Generative%20Modeling%0AAuthor%3A%20Zeyu%20Wang%20and%20Jingyu%20Lin%20and%20Yifei%20Qian%20and%20Yi%20Huang%20and%20Shicen%20Tian%20and%20Bosong%20Chai%20and%20Juncan%20Deng%20and%20Lan%20Du%20and%20Cunjian%20Chen%20and%20Yufei%20Guo%20and%20Kejie%20Huang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20made%20significant%20strides%20in%20language-driven%20and%0Alayout-driven%20image%20generation.%20However%2C%20most%20diffusion%20models%20are%20limited%20to%0Avisible%20RGB%20image%20generation.%20In%20fact%2C%20human%20perception%20of%20the%20world%20is%0Aenriched%20by%20diverse%20viewpoints%2C%20such%20as%20chromatic%20contrast%2C%20thermal%0Aillumination%2C%20and%20depth%20information.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Adiffusion%20model%20for%20general%20layout-guided%20cross-modal%20generation%2C%20called%20DiffX.%0ANotably%2C%20DiffX%20presents%20a%20simple%20yet%20effective%20cross-modal%20generative%20modeling%0Apipeline%2C%20which%20conducts%20diffusion%20and%20denoising%20processes%20in%20the%0Amodality-shared%20latent%20space.%20Moreover%2C%20we%20introduce%20the%20Joint-Modality%0AEmbedder%20%28JME%29%20to%20enhance%20interaction%20between%20layout%20and%20text%20conditions%20by%0Aincorporating%20a%20gated%20attention%20mechanism.%20Meanwhile%2C%20the%20advanced%20Long-CLIP%20is%0Aemployed%20for%20long%20caption%20embedding%20for%20user%20instruction.%20To%20facilitate%20the%0Auser-instructed%20generative%20training%2C%20we%20construct%20the%20cross-modal%20image%0Adatasets%20with%20detailed%20text%20captions%20assisted%20by%20the%20Large-Multimodal%20Model%0A%28LMM%29.%20Through%20extensive%20experiments%2C%20DiffX%20demonstrates%20robustness%20in%0Across-modal%20generation%20across%20three%20%60%60RGB%2BX%27%27%20datasets%3A%20FLIR%2C%20MFNet%2C%20and%0ACOME15K%2C%20guided%20by%20various%20layout%20conditions.%20It%20also%20shows%20the%20potential%20for%0Athe%20adaptive%20generation%20of%20%60%60RGB%2BX%2BY%2BZ%27%27%20images%20or%20more%20diverse%20modalities%20on%0ACOME15K%20and%20MCXFace%20datasets.%20Our%20code%20and%20constructed%20cross-modal%20image%0Adatasets%20are%20available%20at%20https%3A//github.com/zeyuwang-zju/DiffX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15488v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffX%253A%2520Guide%2520Your%2520Layout%2520to%2520Cross-Modal%2520Generative%2520Modeling%26entry.906535625%3DZeyu%2520Wang%2520and%2520Jingyu%2520Lin%2520and%2520Yifei%2520Qian%2520and%2520Yi%2520Huang%2520and%2520Shicen%2520Tian%2520and%2520Bosong%2520Chai%2520and%2520Juncan%2520Deng%2520and%2520Lan%2520Du%2520and%2520Cunjian%2520Chen%2520and%2520Yufei%2520Guo%2520and%2520Kejie%2520Huang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520made%2520significant%2520strides%2520in%2520language-driven%2520and%250Alayout-driven%2520image%2520generation.%2520However%252C%2520most%2520diffusion%2520models%2520are%2520limited%2520to%250Avisible%2520RGB%2520image%2520generation.%2520In%2520fact%252C%2520human%2520perception%2520of%2520the%2520world%2520is%250Aenriched%2520by%2520diverse%2520viewpoints%252C%2520such%2520as%2520chromatic%2520contrast%252C%2520thermal%250Aillumination%252C%2520and%2520depth%2520information.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Adiffusion%2520model%2520for%2520general%2520layout-guided%2520cross-modal%2520generation%252C%2520called%2520DiffX.%250ANotably%252C%2520DiffX%2520presents%2520a%2520simple%2520yet%2520effective%2520cross-modal%2520generative%2520modeling%250Apipeline%252C%2520which%2520conducts%2520diffusion%2520and%2520denoising%2520processes%2520in%2520the%250Amodality-shared%2520latent%2520space.%2520Moreover%252C%2520we%2520introduce%2520the%2520Joint-Modality%250AEmbedder%2520%2528JME%2529%2520to%2520enhance%2520interaction%2520between%2520layout%2520and%2520text%2520conditions%2520by%250Aincorporating%2520a%2520gated%2520attention%2520mechanism.%2520Meanwhile%252C%2520the%2520advanced%2520Long-CLIP%2520is%250Aemployed%2520for%2520long%2520caption%2520embedding%2520for%2520user%2520instruction.%2520To%2520facilitate%2520the%250Auser-instructed%2520generative%2520training%252C%2520we%2520construct%2520the%2520cross-modal%2520image%250Adatasets%2520with%2520detailed%2520text%2520captions%2520assisted%2520by%2520the%2520Large-Multimodal%2520Model%250A%2528LMM%2529.%2520Through%2520extensive%2520experiments%252C%2520DiffX%2520demonstrates%2520robustness%2520in%250Across-modal%2520generation%2520across%2520three%2520%2560%2560RGB%252BX%2527%2527%2520datasets%253A%2520FLIR%252C%2520MFNet%252C%2520and%250ACOME15K%252C%2520guided%2520by%2520various%2520layout%2520conditions.%2520It%2520also%2520shows%2520the%2520potential%2520for%250Athe%2520adaptive%2520generation%2520of%2520%2560%2560RGB%252BX%252BY%252BZ%2527%2527%2520images%2520or%2520more%2520diverse%2520modalities%2520on%250ACOME15K%2520and%2520MCXFace%2520datasets.%2520Our%2520code%2520and%2520constructed%2520cross-modal%2520image%250Adatasets%2520are%2520available%2520at%2520https%253A//github.com/zeyuwang-zju/DiffX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15488v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffX%3A%20Guide%20Your%20Layout%20to%20Cross-Modal%20Generative%20Modeling&entry.906535625=Zeyu%20Wang%20and%20Jingyu%20Lin%20and%20Yifei%20Qian%20and%20Yi%20Huang%20and%20Shicen%20Tian%20and%20Bosong%20Chai%20and%20Juncan%20Deng%20and%20Lan%20Du%20and%20Cunjian%20Chen%20and%20Yufei%20Guo%20and%20Kejie%20Huang&entry.1292438233=%20%20Diffusion%20models%20have%20made%20significant%20strides%20in%20language-driven%20and%0Alayout-driven%20image%20generation.%20However%2C%20most%20diffusion%20models%20are%20limited%20to%0Avisible%20RGB%20image%20generation.%20In%20fact%2C%20human%20perception%20of%20the%20world%20is%0Aenriched%20by%20diverse%20viewpoints%2C%20such%20as%20chromatic%20contrast%2C%20thermal%0Aillumination%2C%20and%20depth%20information.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Adiffusion%20model%20for%20general%20layout-guided%20cross-modal%20generation%2C%20called%20DiffX.%0ANotably%2C%20DiffX%20presents%20a%20simple%20yet%20effective%20cross-modal%20generative%20modeling%0Apipeline%2C%20which%20conducts%20diffusion%20and%20denoising%20processes%20in%20the%0Amodality-shared%20latent%20space.%20Moreover%2C%20we%20introduce%20the%20Joint-Modality%0AEmbedder%20%28JME%29%20to%20enhance%20interaction%20between%20layout%20and%20text%20conditions%20by%0Aincorporating%20a%20gated%20attention%20mechanism.%20Meanwhile%2C%20the%20advanced%20Long-CLIP%20is%0Aemployed%20for%20long%20caption%20embedding%20for%20user%20instruction.%20To%20facilitate%20the%0Auser-instructed%20generative%20training%2C%20we%20construct%20the%20cross-modal%20image%0Adatasets%20with%20detailed%20text%20captions%20assisted%20by%20the%20Large-Multimodal%20Model%0A%28LMM%29.%20Through%20extensive%20experiments%2C%20DiffX%20demonstrates%20robustness%20in%0Across-modal%20generation%20across%20three%20%60%60RGB%2BX%27%27%20datasets%3A%20FLIR%2C%20MFNet%2C%20and%0ACOME15K%2C%20guided%20by%20various%20layout%20conditions.%20It%20also%20shows%20the%20potential%20for%0Athe%20adaptive%20generation%20of%20%60%60RGB%2BX%2BY%2BZ%27%27%20images%20or%20more%20diverse%20modalities%20on%0ACOME15K%20and%20MCXFace%20datasets.%20Our%20code%20and%20constructed%20cross-modal%20image%0Adatasets%20are%20available%20at%20https%3A//github.com/zeyuwang-zju/DiffX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15488v3&entry.124074799=Read"},
{"title": "MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture\n  Generation", "author": "Xiaofeng Mao and Zhengkai Jiang and Qilin Wang and Chencan Fu and Jiangning Zhang and Jiafu Wu and Yabiao Wang and Chengjie Wang and Wei Li and Mingmin Chi", "abstract": "  Recent advancements in the field of Diffusion Transformers have substantially\nimproved the generation of high-quality 2D images, 3D videos, and 3D shapes.\nHowever, the effectiveness of the Transformer architecture in the domain of\nco-speech gesture generation remains relatively unexplored, as prior\nmethodologies have predominantly employed the Convolutional Neural Network\n(CNNs) or simple a few transformer layers. In an attempt to bridge this\nresearch gap, we introduce a novel Masked Diffusion Transformer for co-speech\ngesture generation, referred to as MDT-A2G, which directly implements the\ndenoising process on gesture sequences. To enhance the contextual reasoning\ncapability of temporally aligned speech-driven gestures, we incorporate a novel\nMasked Diffusion Transformer. This model employs a mask modeling scheme\nspecifically designed to strengthen temporal relation learning among sequence\ngestures, thereby expediting the learning process and leading to coherent and\nrealistic motions. Apart from audio, Our MDT-A2G model also integrates\nmulti-modal information, encompassing text, emotion, and identity. Furthermore,\nwe propose an efficient inference strategy that diminishes the denoising\ncomputation by leveraging previously calculated results, thereby achieving a\nspeedup with negligible performance degradation. Experimental results\ndemonstrate that MDT-A2G excels in gesture generation, boasting a learning\nspeed that is over 6$\\times$ faster than traditional diffusion transformers and\nan inference speed that is 5.7$\\times$ than the standard diffusion model.\n", "link": "http://arxiv.org/abs/2408.03312v1", "date": "2024-08-06", "relevancy": 1.878, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6572}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6229}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDT-A2G%3A%20Exploring%20Masked%20Diffusion%20Transformers%20for%20Co-Speech%20Gesture%0A%20%20Generation&body=Title%3A%20MDT-A2G%3A%20Exploring%20Masked%20Diffusion%20Transformers%20for%20Co-Speech%20Gesture%0A%20%20Generation%0AAuthor%3A%20Xiaofeng%20Mao%20and%20Zhengkai%20Jiang%20and%20Qilin%20Wang%20and%20Chencan%20Fu%20and%20Jiangning%20Zhang%20and%20Jiafu%20Wu%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Wei%20Li%20and%20Mingmin%20Chi%0AAbstract%3A%20%20%20Recent%20advancements%20in%20the%20field%20of%20Diffusion%20Transformers%20have%20substantially%0Aimproved%20the%20generation%20of%20high-quality%202D%20images%2C%203D%20videos%2C%20and%203D%20shapes.%0AHowever%2C%20the%20effectiveness%20of%20the%20Transformer%20architecture%20in%20the%20domain%20of%0Aco-speech%20gesture%20generation%20remains%20relatively%20unexplored%2C%20as%20prior%0Amethodologies%20have%20predominantly%20employed%20the%20Convolutional%20Neural%20Network%0A%28CNNs%29%20or%20simple%20a%20few%20transformer%20layers.%20In%20an%20attempt%20to%20bridge%20this%0Aresearch%20gap%2C%20we%20introduce%20a%20novel%20Masked%20Diffusion%20Transformer%20for%20co-speech%0Agesture%20generation%2C%20referred%20to%20as%20MDT-A2G%2C%20which%20directly%20implements%20the%0Adenoising%20process%20on%20gesture%20sequences.%20To%20enhance%20the%20contextual%20reasoning%0Acapability%20of%20temporally%20aligned%20speech-driven%20gestures%2C%20we%20incorporate%20a%20novel%0AMasked%20Diffusion%20Transformer.%20This%20model%20employs%20a%20mask%20modeling%20scheme%0Aspecifically%20designed%20to%20strengthen%20temporal%20relation%20learning%20among%20sequence%0Agestures%2C%20thereby%20expediting%20the%20learning%20process%20and%20leading%20to%20coherent%20and%0Arealistic%20motions.%20Apart%20from%20audio%2C%20Our%20MDT-A2G%20model%20also%20integrates%0Amulti-modal%20information%2C%20encompassing%20text%2C%20emotion%2C%20and%20identity.%20Furthermore%2C%0Awe%20propose%20an%20efficient%20inference%20strategy%20that%20diminishes%20the%20denoising%0Acomputation%20by%20leveraging%20previously%20calculated%20results%2C%20thereby%20achieving%20a%0Aspeedup%20with%20negligible%20performance%20degradation.%20Experimental%20results%0Ademonstrate%20that%20MDT-A2G%20excels%20in%20gesture%20generation%2C%20boasting%20a%20learning%0Aspeed%20that%20is%20over%206%24%5Ctimes%24%20faster%20than%20traditional%20diffusion%20transformers%20and%0Aan%20inference%20speed%20that%20is%205.7%24%5Ctimes%24%20than%20the%20standard%20diffusion%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDT-A2G%253A%2520Exploring%2520Masked%2520Diffusion%2520Transformers%2520for%2520Co-Speech%2520Gesture%250A%2520%2520Generation%26entry.906535625%3DXiaofeng%2520Mao%2520and%2520Zhengkai%2520Jiang%2520and%2520Qilin%2520Wang%2520and%2520Chencan%2520Fu%2520and%2520Jiangning%2520Zhang%2520and%2520Jiafu%2520Wu%2520and%2520Yabiao%2520Wang%2520and%2520Chengjie%2520Wang%2520and%2520Wei%2520Li%2520and%2520Mingmin%2520Chi%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520the%2520field%2520of%2520Diffusion%2520Transformers%2520have%2520substantially%250Aimproved%2520the%2520generation%2520of%2520high-quality%25202D%2520images%252C%25203D%2520videos%252C%2520and%25203D%2520shapes.%250AHowever%252C%2520the%2520effectiveness%2520of%2520the%2520Transformer%2520architecture%2520in%2520the%2520domain%2520of%250Aco-speech%2520gesture%2520generation%2520remains%2520relatively%2520unexplored%252C%2520as%2520prior%250Amethodologies%2520have%2520predominantly%2520employed%2520the%2520Convolutional%2520Neural%2520Network%250A%2528CNNs%2529%2520or%2520simple%2520a%2520few%2520transformer%2520layers.%2520In%2520an%2520attempt%2520to%2520bridge%2520this%250Aresearch%2520gap%252C%2520we%2520introduce%2520a%2520novel%2520Masked%2520Diffusion%2520Transformer%2520for%2520co-speech%250Agesture%2520generation%252C%2520referred%2520to%2520as%2520MDT-A2G%252C%2520which%2520directly%2520implements%2520the%250Adenoising%2520process%2520on%2520gesture%2520sequences.%2520To%2520enhance%2520the%2520contextual%2520reasoning%250Acapability%2520of%2520temporally%2520aligned%2520speech-driven%2520gestures%252C%2520we%2520incorporate%2520a%2520novel%250AMasked%2520Diffusion%2520Transformer.%2520This%2520model%2520employs%2520a%2520mask%2520modeling%2520scheme%250Aspecifically%2520designed%2520to%2520strengthen%2520temporal%2520relation%2520learning%2520among%2520sequence%250Agestures%252C%2520thereby%2520expediting%2520the%2520learning%2520process%2520and%2520leading%2520to%2520coherent%2520and%250Arealistic%2520motions.%2520Apart%2520from%2520audio%252C%2520Our%2520MDT-A2G%2520model%2520also%2520integrates%250Amulti-modal%2520information%252C%2520encompassing%2520text%252C%2520emotion%252C%2520and%2520identity.%2520Furthermore%252C%250Awe%2520propose%2520an%2520efficient%2520inference%2520strategy%2520that%2520diminishes%2520the%2520denoising%250Acomputation%2520by%2520leveraging%2520previously%2520calculated%2520results%252C%2520thereby%2520achieving%2520a%250Aspeedup%2520with%2520negligible%2520performance%2520degradation.%2520Experimental%2520results%250Ademonstrate%2520that%2520MDT-A2G%2520excels%2520in%2520gesture%2520generation%252C%2520boasting%2520a%2520learning%250Aspeed%2520that%2520is%2520over%25206%2524%255Ctimes%2524%2520faster%2520than%2520traditional%2520diffusion%2520transformers%2520and%250Aan%2520inference%2520speed%2520that%2520is%25205.7%2524%255Ctimes%2524%2520than%2520the%2520standard%2520diffusion%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDT-A2G%3A%20Exploring%20Masked%20Diffusion%20Transformers%20for%20Co-Speech%20Gesture%0A%20%20Generation&entry.906535625=Xiaofeng%20Mao%20and%20Zhengkai%20Jiang%20and%20Qilin%20Wang%20and%20Chencan%20Fu%20and%20Jiangning%20Zhang%20and%20Jiafu%20Wu%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Wei%20Li%20and%20Mingmin%20Chi&entry.1292438233=%20%20Recent%20advancements%20in%20the%20field%20of%20Diffusion%20Transformers%20have%20substantially%0Aimproved%20the%20generation%20of%20high-quality%202D%20images%2C%203D%20videos%2C%20and%203D%20shapes.%0AHowever%2C%20the%20effectiveness%20of%20the%20Transformer%20architecture%20in%20the%20domain%20of%0Aco-speech%20gesture%20generation%20remains%20relatively%20unexplored%2C%20as%20prior%0Amethodologies%20have%20predominantly%20employed%20the%20Convolutional%20Neural%20Network%0A%28CNNs%29%20or%20simple%20a%20few%20transformer%20layers.%20In%20an%20attempt%20to%20bridge%20this%0Aresearch%20gap%2C%20we%20introduce%20a%20novel%20Masked%20Diffusion%20Transformer%20for%20co-speech%0Agesture%20generation%2C%20referred%20to%20as%20MDT-A2G%2C%20which%20directly%20implements%20the%0Adenoising%20process%20on%20gesture%20sequences.%20To%20enhance%20the%20contextual%20reasoning%0Acapability%20of%20temporally%20aligned%20speech-driven%20gestures%2C%20we%20incorporate%20a%20novel%0AMasked%20Diffusion%20Transformer.%20This%20model%20employs%20a%20mask%20modeling%20scheme%0Aspecifically%20designed%20to%20strengthen%20temporal%20relation%20learning%20among%20sequence%0Agestures%2C%20thereby%20expediting%20the%20learning%20process%20and%20leading%20to%20coherent%20and%0Arealistic%20motions.%20Apart%20from%20audio%2C%20Our%20MDT-A2G%20model%20also%20integrates%0Amulti-modal%20information%2C%20encompassing%20text%2C%20emotion%2C%20and%20identity.%20Furthermore%2C%0Awe%20propose%20an%20efficient%20inference%20strategy%20that%20diminishes%20the%20denoising%0Acomputation%20by%20leveraging%20previously%20calculated%20results%2C%20thereby%20achieving%20a%0Aspeedup%20with%20negligible%20performance%20degradation.%20Experimental%20results%0Ademonstrate%20that%20MDT-A2G%20excels%20in%20gesture%20generation%2C%20boasting%20a%20learning%0Aspeed%20that%20is%20over%206%24%5Ctimes%24%20faster%20than%20traditional%20diffusion%20transformers%20and%0Aan%20inference%20speed%20that%20is%205.7%24%5Ctimes%24%20than%20the%20standard%20diffusion%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03312v1&entry.124074799=Read"},
{"title": "DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for\n  Fingerprint Presentation Attack Detection", "author": "Anuj Rai and Parsheel Kumar Tiwari and Jyotishna Baishya and Ram Prakash Sharma and Somnath Dey", "abstract": "  Automatic fingerprint recognition systems suffer from the threat of\npresentation attacks due to their wide range of deployment in areas including\nnational borders and commercial applications. A presentation attack can be\nperformed by creating a spoof of a user's fingerprint with or without their\nconsent. This paper presents a dynamic ensemble of deep CNN and handcrafted\nfeatures to detect presentation attacks in known-material and unknown-material\nprotocols of the livness detection competition. The proposed presentation\nattack detection model, in this way, utilizes the capabilities of both deep CNN\nand handcrafted features techniques and exhibits better performance than their\nindividual performances. The proposed method is validated using benchmark\ndatabases from the Liveness Detection Competition in 2015, 2017, and 2019,\nyielding overall accuracy of 96.10\\%, 96.49\\%, and 94.99\\% on them,\nrespectively. The proposed method outperforms state-of-the-art methods in terms\nof classification accuracy.\n", "link": "http://arxiv.org/abs/2308.10015v2", "date": "2024-08-06", "relevancy": 1.878, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4893}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4675}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyFFPAD%3A%20Dynamic%20Fusion%20of%20Convolutional%20and%20Handcrafted%20Features%20for%0A%20%20Fingerprint%20Presentation%20Attack%20Detection&body=Title%3A%20DyFFPAD%3A%20Dynamic%20Fusion%20of%20Convolutional%20and%20Handcrafted%20Features%20for%0A%20%20Fingerprint%20Presentation%20Attack%20Detection%0AAuthor%3A%20Anuj%20Rai%20and%20Parsheel%20Kumar%20Tiwari%20and%20Jyotishna%20Baishya%20and%20Ram%20Prakash%20Sharma%20and%20Somnath%20Dey%0AAbstract%3A%20%20%20Automatic%20fingerprint%20recognition%20systems%20suffer%20from%20the%20threat%20of%0Apresentation%20attacks%20due%20to%20their%20wide%20range%20of%20deployment%20in%20areas%20including%0Anational%20borders%20and%20commercial%20applications.%20A%20presentation%20attack%20can%20be%0Aperformed%20by%20creating%20a%20spoof%20of%20a%20user%27s%20fingerprint%20with%20or%20without%20their%0Aconsent.%20This%20paper%20presents%20a%20dynamic%20ensemble%20of%20deep%20CNN%20and%20handcrafted%0Afeatures%20to%20detect%20presentation%20attacks%20in%20known-material%20and%20unknown-material%0Aprotocols%20of%20the%20livness%20detection%20competition.%20The%20proposed%20presentation%0Aattack%20detection%20model%2C%20in%20this%20way%2C%20utilizes%20the%20capabilities%20of%20both%20deep%20CNN%0Aand%20handcrafted%20features%20techniques%20and%20exhibits%20better%20performance%20than%20their%0Aindividual%20performances.%20The%20proposed%20method%20is%20validated%20using%20benchmark%0Adatabases%20from%20the%20Liveness%20Detection%20Competition%20in%202015%2C%202017%2C%20and%202019%2C%0Ayielding%20overall%20accuracy%20of%2096.10%5C%25%2C%2096.49%5C%25%2C%20and%2094.99%5C%25%20on%20them%2C%0Arespectively.%20The%20proposed%20method%20outperforms%20state-of-the-art%20methods%20in%20terms%0Aof%20classification%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.10015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyFFPAD%253A%2520Dynamic%2520Fusion%2520of%2520Convolutional%2520and%2520Handcrafted%2520Features%2520for%250A%2520%2520Fingerprint%2520Presentation%2520Attack%2520Detection%26entry.906535625%3DAnuj%2520Rai%2520and%2520Parsheel%2520Kumar%2520Tiwari%2520and%2520Jyotishna%2520Baishya%2520and%2520Ram%2520Prakash%2520Sharma%2520and%2520Somnath%2520Dey%26entry.1292438233%3D%2520%2520Automatic%2520fingerprint%2520recognition%2520systems%2520suffer%2520from%2520the%2520threat%2520of%250Apresentation%2520attacks%2520due%2520to%2520their%2520wide%2520range%2520of%2520deployment%2520in%2520areas%2520including%250Anational%2520borders%2520and%2520commercial%2520applications.%2520A%2520presentation%2520attack%2520can%2520be%250Aperformed%2520by%2520creating%2520a%2520spoof%2520of%2520a%2520user%2527s%2520fingerprint%2520with%2520or%2520without%2520their%250Aconsent.%2520This%2520paper%2520presents%2520a%2520dynamic%2520ensemble%2520of%2520deep%2520CNN%2520and%2520handcrafted%250Afeatures%2520to%2520detect%2520presentation%2520attacks%2520in%2520known-material%2520and%2520unknown-material%250Aprotocols%2520of%2520the%2520livness%2520detection%2520competition.%2520The%2520proposed%2520presentation%250Aattack%2520detection%2520model%252C%2520in%2520this%2520way%252C%2520utilizes%2520the%2520capabilities%2520of%2520both%2520deep%2520CNN%250Aand%2520handcrafted%2520features%2520techniques%2520and%2520exhibits%2520better%2520performance%2520than%2520their%250Aindividual%2520performances.%2520The%2520proposed%2520method%2520is%2520validated%2520using%2520benchmark%250Adatabases%2520from%2520the%2520Liveness%2520Detection%2520Competition%2520in%25202015%252C%25202017%252C%2520and%25202019%252C%250Ayielding%2520overall%2520accuracy%2520of%252096.10%255C%2525%252C%252096.49%255C%2525%252C%2520and%252094.99%255C%2525%2520on%2520them%252C%250Arespectively.%2520The%2520proposed%2520method%2520outperforms%2520state-of-the-art%2520methods%2520in%2520terms%250Aof%2520classification%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.10015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyFFPAD%3A%20Dynamic%20Fusion%20of%20Convolutional%20and%20Handcrafted%20Features%20for%0A%20%20Fingerprint%20Presentation%20Attack%20Detection&entry.906535625=Anuj%20Rai%20and%20Parsheel%20Kumar%20Tiwari%20and%20Jyotishna%20Baishya%20and%20Ram%20Prakash%20Sharma%20and%20Somnath%20Dey&entry.1292438233=%20%20Automatic%20fingerprint%20recognition%20systems%20suffer%20from%20the%20threat%20of%0Apresentation%20attacks%20due%20to%20their%20wide%20range%20of%20deployment%20in%20areas%20including%0Anational%20borders%20and%20commercial%20applications.%20A%20presentation%20attack%20can%20be%0Aperformed%20by%20creating%20a%20spoof%20of%20a%20user%27s%20fingerprint%20with%20or%20without%20their%0Aconsent.%20This%20paper%20presents%20a%20dynamic%20ensemble%20of%20deep%20CNN%20and%20handcrafted%0Afeatures%20to%20detect%20presentation%20attacks%20in%20known-material%20and%20unknown-material%0Aprotocols%20of%20the%20livness%20detection%20competition.%20The%20proposed%20presentation%0Aattack%20detection%20model%2C%20in%20this%20way%2C%20utilizes%20the%20capabilities%20of%20both%20deep%20CNN%0Aand%20handcrafted%20features%20techniques%20and%20exhibits%20better%20performance%20than%20their%0Aindividual%20performances.%20The%20proposed%20method%20is%20validated%20using%20benchmark%0Adatabases%20from%20the%20Liveness%20Detection%20Competition%20in%202015%2C%202017%2C%20and%202019%2C%0Ayielding%20overall%20accuracy%20of%2096.10%5C%25%2C%2096.49%5C%25%2C%20and%2094.99%5C%25%20on%20them%2C%0Arespectively.%20The%20proposed%20method%20outperforms%20state-of-the-art%20methods%20in%20terms%0Aof%20classification%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10015v2&entry.124074799=Read"},
{"title": "Self-Evaluation as a Defense Against Adversarial Attacks on LLMs", "author": "Hannah Brown and Leon Lin and Kenji Kawaguchi and Michael Shieh", "abstract": "  We introduce a defense against adversarial attacks on LLMs utilizing\nself-evaluation. Our method requires no model fine-tuning, instead using\npre-trained models to evaluate the inputs and outputs of a generator model,\nsignificantly reducing the cost of implementation in comparison to other,\nfinetuning-based methods. Our method can significantly reduce the attack\nsuccess rate of attacks on both open and closed-source LLMs, beyond the\nreductions demonstrated by Llama-Guard2 and commonly used content moderation\nAPIs. We present an analysis of the effectiveness of our method, including\nattempts to attack the evaluator in various settings, demonstrating that it is\nalso more resilient to attacks than existing methods. Code and data will be\nmade available at https://github.com/Linlt-leon/self-eval.\n", "link": "http://arxiv.org/abs/2407.03234v3", "date": "2024-08-06", "relevancy": 1.8677, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4821}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4667}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Evaluation%20as%20a%20Defense%20Against%20Adversarial%20Attacks%20on%20LLMs&body=Title%3A%20Self-Evaluation%20as%20a%20Defense%20Against%20Adversarial%20Attacks%20on%20LLMs%0AAuthor%3A%20Hannah%20Brown%20and%20Leon%20Lin%20and%20Kenji%20Kawaguchi%20and%20Michael%20Shieh%0AAbstract%3A%20%20%20We%20introduce%20a%20defense%20against%20adversarial%20attacks%20on%20LLMs%20utilizing%0Aself-evaluation.%20Our%20method%20requires%20no%20model%20fine-tuning%2C%20instead%20using%0Apre-trained%20models%20to%20evaluate%20the%20inputs%20and%20outputs%20of%20a%20generator%20model%2C%0Asignificantly%20reducing%20the%20cost%20of%20implementation%20in%20comparison%20to%20other%2C%0Afinetuning-based%20methods.%20Our%20method%20can%20significantly%20reduce%20the%20attack%0Asuccess%20rate%20of%20attacks%20on%20both%20open%20and%20closed-source%20LLMs%2C%20beyond%20the%0Areductions%20demonstrated%20by%20Llama-Guard2%20and%20commonly%20used%20content%20moderation%0AAPIs.%20We%20present%20an%20analysis%20of%20the%20effectiveness%20of%20our%20method%2C%20including%0Aattempts%20to%20attack%20the%20evaluator%20in%20various%20settings%2C%20demonstrating%20that%20it%20is%0Aalso%20more%20resilient%20to%20attacks%20than%20existing%20methods.%20Code%20and%20data%20will%20be%0Amade%20available%20at%20https%3A//github.com/Linlt-leon/self-eval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03234v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Evaluation%2520as%2520a%2520Defense%2520Against%2520Adversarial%2520Attacks%2520on%2520LLMs%26entry.906535625%3DHannah%2520Brown%2520and%2520Leon%2520Lin%2520and%2520Kenji%2520Kawaguchi%2520and%2520Michael%2520Shieh%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520defense%2520against%2520adversarial%2520attacks%2520on%2520LLMs%2520utilizing%250Aself-evaluation.%2520Our%2520method%2520requires%2520no%2520model%2520fine-tuning%252C%2520instead%2520using%250Apre-trained%2520models%2520to%2520evaluate%2520the%2520inputs%2520and%2520outputs%2520of%2520a%2520generator%2520model%252C%250Asignificantly%2520reducing%2520the%2520cost%2520of%2520implementation%2520in%2520comparison%2520to%2520other%252C%250Afinetuning-based%2520methods.%2520Our%2520method%2520can%2520significantly%2520reduce%2520the%2520attack%250Asuccess%2520rate%2520of%2520attacks%2520on%2520both%2520open%2520and%2520closed-source%2520LLMs%252C%2520beyond%2520the%250Areductions%2520demonstrated%2520by%2520Llama-Guard2%2520and%2520commonly%2520used%2520content%2520moderation%250AAPIs.%2520We%2520present%2520an%2520analysis%2520of%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520including%250Aattempts%2520to%2520attack%2520the%2520evaluator%2520in%2520various%2520settings%252C%2520demonstrating%2520that%2520it%2520is%250Aalso%2520more%2520resilient%2520to%2520attacks%2520than%2520existing%2520methods.%2520Code%2520and%2520data%2520will%2520be%250Amade%2520available%2520at%2520https%253A//github.com/Linlt-leon/self-eval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03234v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Evaluation%20as%20a%20Defense%20Against%20Adversarial%20Attacks%20on%20LLMs&entry.906535625=Hannah%20Brown%20and%20Leon%20Lin%20and%20Kenji%20Kawaguchi%20and%20Michael%20Shieh&entry.1292438233=%20%20We%20introduce%20a%20defense%20against%20adversarial%20attacks%20on%20LLMs%20utilizing%0Aself-evaluation.%20Our%20method%20requires%20no%20model%20fine-tuning%2C%20instead%20using%0Apre-trained%20models%20to%20evaluate%20the%20inputs%20and%20outputs%20of%20a%20generator%20model%2C%0Asignificantly%20reducing%20the%20cost%20of%20implementation%20in%20comparison%20to%20other%2C%0Afinetuning-based%20methods.%20Our%20method%20can%20significantly%20reduce%20the%20attack%0Asuccess%20rate%20of%20attacks%20on%20both%20open%20and%20closed-source%20LLMs%2C%20beyond%20the%0Areductions%20demonstrated%20by%20Llama-Guard2%20and%20commonly%20used%20content%20moderation%0AAPIs.%20We%20present%20an%20analysis%20of%20the%20effectiveness%20of%20our%20method%2C%20including%0Aattempts%20to%20attack%20the%20evaluator%20in%20various%20settings%2C%20demonstrating%20that%20it%20is%0Aalso%20more%20resilient%20to%20attacks%20than%20existing%20methods.%20Code%20and%20data%20will%20be%0Amade%20available%20at%20https%3A//github.com/Linlt-leon/self-eval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03234v3&entry.124074799=Read"},
{"title": "COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework", "author": "Rajvee Sheth and Shubh Nisar and Heenaben Prajapati and Himanshu Beniwal and Mayank Singh", "abstract": "  As the NLP community increasingly addresses challenges associated with\nmultilingualism, robust annotation tools are essential to handle multilingual\ndatasets efficiently. In this paper, we introduce a code-mixed multilingual\ntext annotation framework, COMMENTATOR, specifically designed for annotating\ncode-mixed text. The tool demonstrates its effectiveness in token-level and\nsentence-level language annotation tasks for Hinglish text. We perform robust\nqualitative human-based evaluations to showcase COMMENTATOR led to 5x faster\nannotations than the best baseline. Our code is publicly available at\n\\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is\navailable at \\url{https://bit.ly/commentator_video}.\n", "link": "http://arxiv.org/abs/2408.03125v1", "date": "2024-08-06", "relevancy": 1.8645, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4806}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4688}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMMENTATOR%3A%20A%20Code-mixed%20Multilingual%20Text%20Annotation%20Framework&body=Title%3A%20COMMENTATOR%3A%20A%20Code-mixed%20Multilingual%20Text%20Annotation%20Framework%0AAuthor%3A%20Rajvee%20Sheth%20and%20Shubh%20Nisar%20and%20Heenaben%20Prajapati%20and%20Himanshu%20Beniwal%20and%20Mayank%20Singh%0AAbstract%3A%20%20%20As%20the%20NLP%20community%20increasingly%20addresses%20challenges%20associated%20with%0Amultilingualism%2C%20robust%20annotation%20tools%20are%20essential%20to%20handle%20multilingual%0Adatasets%20efficiently.%20In%20this%20paper%2C%20we%20introduce%20a%20code-mixed%20multilingual%0Atext%20annotation%20framework%2C%20COMMENTATOR%2C%20specifically%20designed%20for%20annotating%0Acode-mixed%20text.%20The%20tool%20demonstrates%20its%20effectiveness%20in%20token-level%20and%0Asentence-level%20language%20annotation%20tasks%20for%20Hinglish%20text.%20We%20perform%20robust%0Aqualitative%20human-based%20evaluations%20to%20showcase%20COMMENTATOR%20led%20to%205x%20faster%0Aannotations%20than%20the%20best%20baseline.%20Our%20code%20is%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/lingo-iitgn/commentator%7D.%20The%20demonstration%20video%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//bit.ly/commentator_video%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMMENTATOR%253A%2520A%2520Code-mixed%2520Multilingual%2520Text%2520Annotation%2520Framework%26entry.906535625%3DRajvee%2520Sheth%2520and%2520Shubh%2520Nisar%2520and%2520Heenaben%2520Prajapati%2520and%2520Himanshu%2520Beniwal%2520and%2520Mayank%2520Singh%26entry.1292438233%3D%2520%2520As%2520the%2520NLP%2520community%2520increasingly%2520addresses%2520challenges%2520associated%2520with%250Amultilingualism%252C%2520robust%2520annotation%2520tools%2520are%2520essential%2520to%2520handle%2520multilingual%250Adatasets%2520efficiently.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520code-mixed%2520multilingual%250Atext%2520annotation%2520framework%252C%2520COMMENTATOR%252C%2520specifically%2520designed%2520for%2520annotating%250Acode-mixed%2520text.%2520The%2520tool%2520demonstrates%2520its%2520effectiveness%2520in%2520token-level%2520and%250Asentence-level%2520language%2520annotation%2520tasks%2520for%2520Hinglish%2520text.%2520We%2520perform%2520robust%250Aqualitative%2520human-based%2520evaluations%2520to%2520showcase%2520COMMENTATOR%2520led%2520to%25205x%2520faster%250Aannotations%2520than%2520the%2520best%2520baseline.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/lingo-iitgn/commentator%257D.%2520The%2520demonstration%2520video%2520is%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//bit.ly/commentator_video%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMMENTATOR%3A%20A%20Code-mixed%20Multilingual%20Text%20Annotation%20Framework&entry.906535625=Rajvee%20Sheth%20and%20Shubh%20Nisar%20and%20Heenaben%20Prajapati%20and%20Himanshu%20Beniwal%20and%20Mayank%20Singh&entry.1292438233=%20%20As%20the%20NLP%20community%20increasingly%20addresses%20challenges%20associated%20with%0Amultilingualism%2C%20robust%20annotation%20tools%20are%20essential%20to%20handle%20multilingual%0Adatasets%20efficiently.%20In%20this%20paper%2C%20we%20introduce%20a%20code-mixed%20multilingual%0Atext%20annotation%20framework%2C%20COMMENTATOR%2C%20specifically%20designed%20for%20annotating%0Acode-mixed%20text.%20The%20tool%20demonstrates%20its%20effectiveness%20in%20token-level%20and%0Asentence-level%20language%20annotation%20tasks%20for%20Hinglish%20text.%20We%20perform%20robust%0Aqualitative%20human-based%20evaluations%20to%20showcase%20COMMENTATOR%20led%20to%205x%20faster%0Aannotations%20than%20the%20best%20baseline.%20Our%20code%20is%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/lingo-iitgn/commentator%7D.%20The%20demonstration%20video%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//bit.ly/commentator_video%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03125v1&entry.124074799=Read"},
{"title": "Stability-Informed Initialization of Neural Ordinary Differential\n  Equations", "author": "Theodor Westny and Arman Mohammadi and Daniel Jung and Erik Frisk", "abstract": "  This paper addresses the training of Neural Ordinary Differential Equations\n(neural ODEs), and in particular explores the interplay between numerical\nintegration techniques, stability regions, step size, and initialization\ntechniques. It is shown how the choice of integration technique implicitly\nregularizes the learned model, and how the solver's corresponding stability\nregion affects training and prediction performance. From this analysis, a\nstability-informed parameter initialization technique is introduced. The\neffectiveness of the initialization method is displayed across several learning\nbenchmarks and industrial applications.\n", "link": "http://arxiv.org/abs/2311.15890v3", "date": "2024-08-06", "relevancy": 1.845, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4983}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4539}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stability-Informed%20Initialization%20of%20Neural%20Ordinary%20Differential%0A%20%20Equations&body=Title%3A%20Stability-Informed%20Initialization%20of%20Neural%20Ordinary%20Differential%0A%20%20Equations%0AAuthor%3A%20Theodor%20Westny%20and%20Arman%20Mohammadi%20and%20Daniel%20Jung%20and%20Erik%20Frisk%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20training%20of%20Neural%20Ordinary%20Differential%20Equations%0A%28neural%20ODEs%29%2C%20and%20in%20particular%20explores%20the%20interplay%20between%20numerical%0Aintegration%20techniques%2C%20stability%20regions%2C%20step%20size%2C%20and%20initialization%0Atechniques.%20It%20is%20shown%20how%20the%20choice%20of%20integration%20technique%20implicitly%0Aregularizes%20the%20learned%20model%2C%20and%20how%20the%20solver%27s%20corresponding%20stability%0Aregion%20affects%20training%20and%20prediction%20performance.%20From%20this%20analysis%2C%20a%0Astability-informed%20parameter%20initialization%20technique%20is%20introduced.%20The%0Aeffectiveness%20of%20the%20initialization%20method%20is%20displayed%20across%20several%20learning%0Abenchmarks%20and%20industrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15890v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStability-Informed%2520Initialization%2520of%2520Neural%2520Ordinary%2520Differential%250A%2520%2520Equations%26entry.906535625%3DTheodor%2520Westny%2520and%2520Arman%2520Mohammadi%2520and%2520Daniel%2520Jung%2520and%2520Erik%2520Frisk%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520training%2520of%2520Neural%2520Ordinary%2520Differential%2520Equations%250A%2528neural%2520ODEs%2529%252C%2520and%2520in%2520particular%2520explores%2520the%2520interplay%2520between%2520numerical%250Aintegration%2520techniques%252C%2520stability%2520regions%252C%2520step%2520size%252C%2520and%2520initialization%250Atechniques.%2520It%2520is%2520shown%2520how%2520the%2520choice%2520of%2520integration%2520technique%2520implicitly%250Aregularizes%2520the%2520learned%2520model%252C%2520and%2520how%2520the%2520solver%2527s%2520corresponding%2520stability%250Aregion%2520affects%2520training%2520and%2520prediction%2520performance.%2520From%2520this%2520analysis%252C%2520a%250Astability-informed%2520parameter%2520initialization%2520technique%2520is%2520introduced.%2520The%250Aeffectiveness%2520of%2520the%2520initialization%2520method%2520is%2520displayed%2520across%2520several%2520learning%250Abenchmarks%2520and%2520industrial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15890v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stability-Informed%20Initialization%20of%20Neural%20Ordinary%20Differential%0A%20%20Equations&entry.906535625=Theodor%20Westny%20and%20Arman%20Mohammadi%20and%20Daniel%20Jung%20and%20Erik%20Frisk&entry.1292438233=%20%20This%20paper%20addresses%20the%20training%20of%20Neural%20Ordinary%20Differential%20Equations%0A%28neural%20ODEs%29%2C%20and%20in%20particular%20explores%20the%20interplay%20between%20numerical%0Aintegration%20techniques%2C%20stability%20regions%2C%20step%20size%2C%20and%20initialization%0Atechniques.%20It%20is%20shown%20how%20the%20choice%20of%20integration%20technique%20implicitly%0Aregularizes%20the%20learned%20model%2C%20and%20how%20the%20solver%27s%20corresponding%20stability%0Aregion%20affects%20training%20and%20prediction%20performance.%20From%20this%20analysis%2C%20a%0Astability-informed%20parameter%20initialization%20technique%20is%20introduced.%20The%0Aeffectiveness%20of%20the%20initialization%20method%20is%20displayed%20across%20several%20learning%0Abenchmarks%20and%20industrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15890v3&entry.124074799=Read"},
{"title": "Enhancing AI-based Generation of Software Exploits with Contextual\n  Information", "author": "Pietro Liguori and Cristina Improta and Roberto Natella and Bojan Cukic and Domenico Cotroneo", "abstract": "  This practical experience report explores Neural Machine Translation (NMT)\nmodels' capability to generate offensive security code from natural language\n(NL) descriptions, highlighting the significance of contextual understanding\nand its impact on model performance. Our study employs a dataset comprising\nreal shellcodes to evaluate the models across various scenarios, including\nmissing information, necessary context, and unnecessary context. The\nexperiments are designed to assess the models' resilience against incomplete\ndescriptions, their proficiency in leveraging context for enhanced accuracy,\nand their ability to discern irrelevant information. The findings reveal that\nthe introduction of contextual data significantly improves performance.\nHowever, the benefits of additional context diminish beyond a certain point,\nindicating an optimal level of contextual information for model training.\nMoreover, the models demonstrate an ability to filter out unnecessary context,\nmaintaining high levels of accuracy in the generation of offensive security\ncode. This study paves the way for future research on optimizing context use in\nAI-driven code generation, particularly for applications requiring a high\ndegree of technical precision such as the generation of offensive code.\n", "link": "http://arxiv.org/abs/2408.02402v2", "date": "2024-08-06", "relevancy": 1.8444, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4729}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4623}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20AI-based%20Generation%20of%20Software%20Exploits%20with%20Contextual%0A%20%20Information&body=Title%3A%20Enhancing%20AI-based%20Generation%20of%20Software%20Exploits%20with%20Contextual%0A%20%20Information%0AAuthor%3A%20Pietro%20Liguori%20and%20Cristina%20Improta%20and%20Roberto%20Natella%20and%20Bojan%20Cukic%20and%20Domenico%20Cotroneo%0AAbstract%3A%20%20%20This%20practical%20experience%20report%20explores%20Neural%20Machine%20Translation%20%28NMT%29%0Amodels%27%20capability%20to%20generate%20offensive%20security%20code%20from%20natural%20language%0A%28NL%29%20descriptions%2C%20highlighting%20the%20significance%20of%20contextual%20understanding%0Aand%20its%20impact%20on%20model%20performance.%20Our%20study%20employs%20a%20dataset%20comprising%0Areal%20shellcodes%20to%20evaluate%20the%20models%20across%20various%20scenarios%2C%20including%0Amissing%20information%2C%20necessary%20context%2C%20and%20unnecessary%20context.%20The%0Aexperiments%20are%20designed%20to%20assess%20the%20models%27%20resilience%20against%20incomplete%0Adescriptions%2C%20their%20proficiency%20in%20leveraging%20context%20for%20enhanced%20accuracy%2C%0Aand%20their%20ability%20to%20discern%20irrelevant%20information.%20The%20findings%20reveal%20that%0Athe%20introduction%20of%20contextual%20data%20significantly%20improves%20performance.%0AHowever%2C%20the%20benefits%20of%20additional%20context%20diminish%20beyond%20a%20certain%20point%2C%0Aindicating%20an%20optimal%20level%20of%20contextual%20information%20for%20model%20training.%0AMoreover%2C%20the%20models%20demonstrate%20an%20ability%20to%20filter%20out%20unnecessary%20context%2C%0Amaintaining%20high%20levels%20of%20accuracy%20in%20the%20generation%20of%20offensive%20security%0Acode.%20This%20study%20paves%20the%20way%20for%20future%20research%20on%20optimizing%20context%20use%20in%0AAI-driven%20code%20generation%2C%20particularly%20for%20applications%20requiring%20a%20high%0Adegree%20of%20technical%20precision%20such%20as%20the%20generation%20of%20offensive%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02402v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520AI-based%2520Generation%2520of%2520Software%2520Exploits%2520with%2520Contextual%250A%2520%2520Information%26entry.906535625%3DPietro%2520Liguori%2520and%2520Cristina%2520Improta%2520and%2520Roberto%2520Natella%2520and%2520Bojan%2520Cukic%2520and%2520Domenico%2520Cotroneo%26entry.1292438233%3D%2520%2520This%2520practical%2520experience%2520report%2520explores%2520Neural%2520Machine%2520Translation%2520%2528NMT%2529%250Amodels%2527%2520capability%2520to%2520generate%2520offensive%2520security%2520code%2520from%2520natural%2520language%250A%2528NL%2529%2520descriptions%252C%2520highlighting%2520the%2520significance%2520of%2520contextual%2520understanding%250Aand%2520its%2520impact%2520on%2520model%2520performance.%2520Our%2520study%2520employs%2520a%2520dataset%2520comprising%250Areal%2520shellcodes%2520to%2520evaluate%2520the%2520models%2520across%2520various%2520scenarios%252C%2520including%250Amissing%2520information%252C%2520necessary%2520context%252C%2520and%2520unnecessary%2520context.%2520The%250Aexperiments%2520are%2520designed%2520to%2520assess%2520the%2520models%2527%2520resilience%2520against%2520incomplete%250Adescriptions%252C%2520their%2520proficiency%2520in%2520leveraging%2520context%2520for%2520enhanced%2520accuracy%252C%250Aand%2520their%2520ability%2520to%2520discern%2520irrelevant%2520information.%2520The%2520findings%2520reveal%2520that%250Athe%2520introduction%2520of%2520contextual%2520data%2520significantly%2520improves%2520performance.%250AHowever%252C%2520the%2520benefits%2520of%2520additional%2520context%2520diminish%2520beyond%2520a%2520certain%2520point%252C%250Aindicating%2520an%2520optimal%2520level%2520of%2520contextual%2520information%2520for%2520model%2520training.%250AMoreover%252C%2520the%2520models%2520demonstrate%2520an%2520ability%2520to%2520filter%2520out%2520unnecessary%2520context%252C%250Amaintaining%2520high%2520levels%2520of%2520accuracy%2520in%2520the%2520generation%2520of%2520offensive%2520security%250Acode.%2520This%2520study%2520paves%2520the%2520way%2520for%2520future%2520research%2520on%2520optimizing%2520context%2520use%2520in%250AAI-driven%2520code%2520generation%252C%2520particularly%2520for%2520applications%2520requiring%2520a%2520high%250Adegree%2520of%2520technical%2520precision%2520such%2520as%2520the%2520generation%2520of%2520offensive%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02402v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20AI-based%20Generation%20of%20Software%20Exploits%20with%20Contextual%0A%20%20Information&entry.906535625=Pietro%20Liguori%20and%20Cristina%20Improta%20and%20Roberto%20Natella%20and%20Bojan%20Cukic%20and%20Domenico%20Cotroneo&entry.1292438233=%20%20This%20practical%20experience%20report%20explores%20Neural%20Machine%20Translation%20%28NMT%29%0Amodels%27%20capability%20to%20generate%20offensive%20security%20code%20from%20natural%20language%0A%28NL%29%20descriptions%2C%20highlighting%20the%20significance%20of%20contextual%20understanding%0Aand%20its%20impact%20on%20model%20performance.%20Our%20study%20employs%20a%20dataset%20comprising%0Areal%20shellcodes%20to%20evaluate%20the%20models%20across%20various%20scenarios%2C%20including%0Amissing%20information%2C%20necessary%20context%2C%20and%20unnecessary%20context.%20The%0Aexperiments%20are%20designed%20to%20assess%20the%20models%27%20resilience%20against%20incomplete%0Adescriptions%2C%20their%20proficiency%20in%20leveraging%20context%20for%20enhanced%20accuracy%2C%0Aand%20their%20ability%20to%20discern%20irrelevant%20information.%20The%20findings%20reveal%20that%0Athe%20introduction%20of%20contextual%20data%20significantly%20improves%20performance.%0AHowever%2C%20the%20benefits%20of%20additional%20context%20diminish%20beyond%20a%20certain%20point%2C%0Aindicating%20an%20optimal%20level%20of%20contextual%20information%20for%20model%20training.%0AMoreover%2C%20the%20models%20demonstrate%20an%20ability%20to%20filter%20out%20unnecessary%20context%2C%0Amaintaining%20high%20levels%20of%20accuracy%20in%20the%20generation%20of%20offensive%20security%0Acode.%20This%20study%20paves%20the%20way%20for%20future%20research%20on%20optimizing%20context%20use%20in%0AAI-driven%20code%20generation%2C%20particularly%20for%20applications%20requiring%20a%20high%0Adegree%20of%20technical%20precision%20such%20as%20the%20generation%20of%20offensive%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02402v2&entry.124074799=Read"},
{"title": "Malicious Internet Entity Detection Using Local Graph Inference", "author": "Simon Mandlik and Tomas Pevny and Vaclav Smidl and Lukas Bajer", "abstract": "  Detection of malicious behavior in a large network is a challenging problem\nfor machine learning in computer security, since it requires a model with high\nexpressive power and scalable inference. Existing solutions struggle to achieve\nthis feat -- current cybersec-tailored approaches are still limited in\nexpressivity, and methods successful in other domains do not scale well for\nlarge volumes of data, rendering frequent retraining impossible. This work\nproposes a new perspective for learning from graph data that is modeling\nnetwork entity interactions as a large heterogeneous graph. High expressivity\nof the method is achieved with neural network architecture HMILnet that\nnaturally models this type of data and provides theoretical guarantees. The\nscalability is achieved by pursuing local graph inference, i.e., classifying\nindividual vertices and their neighborhood as independent samples. Our\nexperiments exhibit improvement over the state-of-the-art Probabilistic Threat\nPropagation (PTP) algorithm, show a further threefold accuracy improvement when\nadditional data is used, which is not possible with the PTP algorithm, and\ndemonstrate the generalization capabilities of the method to new, previously\nunseen entities.\n", "link": "http://arxiv.org/abs/2408.03287v1", "date": "2024-08-06", "relevancy": 1.8368, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.474}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.466}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Malicious%20Internet%20Entity%20Detection%20Using%20Local%20Graph%20Inference&body=Title%3A%20Malicious%20Internet%20Entity%20Detection%20Using%20Local%20Graph%20Inference%0AAuthor%3A%20Simon%20Mandlik%20and%20Tomas%20Pevny%20and%20Vaclav%20Smidl%20and%20Lukas%20Bajer%0AAbstract%3A%20%20%20Detection%20of%20malicious%20behavior%20in%20a%20large%20network%20is%20a%20challenging%20problem%0Afor%20machine%20learning%20in%20computer%20security%2C%20since%20it%20requires%20a%20model%20with%20high%0Aexpressive%20power%20and%20scalable%20inference.%20Existing%20solutions%20struggle%20to%20achieve%0Athis%20feat%20--%20current%20cybersec-tailored%20approaches%20are%20still%20limited%20in%0Aexpressivity%2C%20and%20methods%20successful%20in%20other%20domains%20do%20not%20scale%20well%20for%0Alarge%20volumes%20of%20data%2C%20rendering%20frequent%20retraining%20impossible.%20This%20work%0Aproposes%20a%20new%20perspective%20for%20learning%20from%20graph%20data%20that%20is%20modeling%0Anetwork%20entity%20interactions%20as%20a%20large%20heterogeneous%20graph.%20High%20expressivity%0Aof%20the%20method%20is%20achieved%20with%20neural%20network%20architecture%20HMILnet%20that%0Anaturally%20models%20this%20type%20of%20data%20and%20provides%20theoretical%20guarantees.%20The%0Ascalability%20is%20achieved%20by%20pursuing%20local%20graph%20inference%2C%20i.e.%2C%20classifying%0Aindividual%20vertices%20and%20their%20neighborhood%20as%20independent%20samples.%20Our%0Aexperiments%20exhibit%20improvement%20over%20the%20state-of-the-art%20Probabilistic%20Threat%0APropagation%20%28PTP%29%20algorithm%2C%20show%20a%20further%20threefold%20accuracy%20improvement%20when%0Aadditional%20data%20is%20used%2C%20which%20is%20not%20possible%20with%20the%20PTP%20algorithm%2C%20and%0Ademonstrate%20the%20generalization%20capabilities%20of%20the%20method%20to%20new%2C%20previously%0Aunseen%20entities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMalicious%2520Internet%2520Entity%2520Detection%2520Using%2520Local%2520Graph%2520Inference%26entry.906535625%3DSimon%2520Mandlik%2520and%2520Tomas%2520Pevny%2520and%2520Vaclav%2520Smidl%2520and%2520Lukas%2520Bajer%26entry.1292438233%3D%2520%2520Detection%2520of%2520malicious%2520behavior%2520in%2520a%2520large%2520network%2520is%2520a%2520challenging%2520problem%250Afor%2520machine%2520learning%2520in%2520computer%2520security%252C%2520since%2520it%2520requires%2520a%2520model%2520with%2520high%250Aexpressive%2520power%2520and%2520scalable%2520inference.%2520Existing%2520solutions%2520struggle%2520to%2520achieve%250Athis%2520feat%2520--%2520current%2520cybersec-tailored%2520approaches%2520are%2520still%2520limited%2520in%250Aexpressivity%252C%2520and%2520methods%2520successful%2520in%2520other%2520domains%2520do%2520not%2520scale%2520well%2520for%250Alarge%2520volumes%2520of%2520data%252C%2520rendering%2520frequent%2520retraining%2520impossible.%2520This%2520work%250Aproposes%2520a%2520new%2520perspective%2520for%2520learning%2520from%2520graph%2520data%2520that%2520is%2520modeling%250Anetwork%2520entity%2520interactions%2520as%2520a%2520large%2520heterogeneous%2520graph.%2520High%2520expressivity%250Aof%2520the%2520method%2520is%2520achieved%2520with%2520neural%2520network%2520architecture%2520HMILnet%2520that%250Anaturally%2520models%2520this%2520type%2520of%2520data%2520and%2520provides%2520theoretical%2520guarantees.%2520The%250Ascalability%2520is%2520achieved%2520by%2520pursuing%2520local%2520graph%2520inference%252C%2520i.e.%252C%2520classifying%250Aindividual%2520vertices%2520and%2520their%2520neighborhood%2520as%2520independent%2520samples.%2520Our%250Aexperiments%2520exhibit%2520improvement%2520over%2520the%2520state-of-the-art%2520Probabilistic%2520Threat%250APropagation%2520%2528PTP%2529%2520algorithm%252C%2520show%2520a%2520further%2520threefold%2520accuracy%2520improvement%2520when%250Aadditional%2520data%2520is%2520used%252C%2520which%2520is%2520not%2520possible%2520with%2520the%2520PTP%2520algorithm%252C%2520and%250Ademonstrate%2520the%2520generalization%2520capabilities%2520of%2520the%2520method%2520to%2520new%252C%2520previously%250Aunseen%2520entities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Malicious%20Internet%20Entity%20Detection%20Using%20Local%20Graph%20Inference&entry.906535625=Simon%20Mandlik%20and%20Tomas%20Pevny%20and%20Vaclav%20Smidl%20and%20Lukas%20Bajer&entry.1292438233=%20%20Detection%20of%20malicious%20behavior%20in%20a%20large%20network%20is%20a%20challenging%20problem%0Afor%20machine%20learning%20in%20computer%20security%2C%20since%20it%20requires%20a%20model%20with%20high%0Aexpressive%20power%20and%20scalable%20inference.%20Existing%20solutions%20struggle%20to%20achieve%0Athis%20feat%20--%20current%20cybersec-tailored%20approaches%20are%20still%20limited%20in%0Aexpressivity%2C%20and%20methods%20successful%20in%20other%20domains%20do%20not%20scale%20well%20for%0Alarge%20volumes%20of%20data%2C%20rendering%20frequent%20retraining%20impossible.%20This%20work%0Aproposes%20a%20new%20perspective%20for%20learning%20from%20graph%20data%20that%20is%20modeling%0Anetwork%20entity%20interactions%20as%20a%20large%20heterogeneous%20graph.%20High%20expressivity%0Aof%20the%20method%20is%20achieved%20with%20neural%20network%20architecture%20HMILnet%20that%0Anaturally%20models%20this%20type%20of%20data%20and%20provides%20theoretical%20guarantees.%20The%0Ascalability%20is%20achieved%20by%20pursuing%20local%20graph%20inference%2C%20i.e.%2C%20classifying%0Aindividual%20vertices%20and%20their%20neighborhood%20as%20independent%20samples.%20Our%0Aexperiments%20exhibit%20improvement%20over%20the%20state-of-the-art%20Probabilistic%20Threat%0APropagation%20%28PTP%29%20algorithm%2C%20show%20a%20further%20threefold%20accuracy%20improvement%20when%0Aadditional%20data%20is%20used%2C%20which%20is%20not%20possible%20with%20the%20PTP%20algorithm%2C%20and%0Ademonstrate%20the%20generalization%20capabilities%20of%20the%20method%20to%20new%2C%20previously%0Aunseen%20entities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03287v1&entry.124074799=Read"},
{"title": "CaloQVAE : Simulating high-energy particle-calorimeter interactions\n  using hybrid quantum-classical generative models", "author": "Sehmimul Hoque and Hao Jia and Abhishek Abhishek and Mojde Fadaie and J. Quetzalcoatl Toledo-Mar\u00edn and Tiago Vale and Roger G. Melko and Maximilian Swiatlowski and Wojciech T. Fedorko", "abstract": "  The Large Hadron Collider's high luminosity era presents major computational\nchallenges in the analysis of collision events. Large amounts of Monte Carlo\n(MC) simulation will be required to constrain the statistical uncertainties of\nthe simulated datasets below these of the experimental data. Modelling of\nhigh-energy particles propagating through the calorimeter section of the\ndetector is the most computationally intensive MC simulation task. We introduce\na technique combining recent advancements in generative models and quantum\nannealing for fast and efficient simulation of high-energy particle-calorimeter\ninteractions.\n", "link": "http://arxiv.org/abs/2312.03179v4", "date": "2024-08-06", "relevancy": 1.8082, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4571}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4571}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaloQVAE%20%3A%20Simulating%20high-energy%20particle-calorimeter%20interactions%0A%20%20using%20hybrid%20quantum-classical%20generative%20models&body=Title%3A%20CaloQVAE%20%3A%20Simulating%20high-energy%20particle-calorimeter%20interactions%0A%20%20using%20hybrid%20quantum-classical%20generative%20models%0AAuthor%3A%20Sehmimul%20Hoque%20and%20Hao%20Jia%20and%20Abhishek%20Abhishek%20and%20Mojde%20Fadaie%20and%20J.%20Quetzalcoatl%20Toledo-Mar%C3%ADn%20and%20Tiago%20Vale%20and%20Roger%20G.%20Melko%20and%20Maximilian%20Swiatlowski%20and%20Wojciech%20T.%20Fedorko%0AAbstract%3A%20%20%20The%20Large%20Hadron%20Collider%27s%20high%20luminosity%20era%20presents%20major%20computational%0Achallenges%20in%20the%20analysis%20of%20collision%20events.%20Large%20amounts%20of%20Monte%20Carlo%0A%28MC%29%20simulation%20will%20be%20required%20to%20constrain%20the%20statistical%20uncertainties%20of%0Athe%20simulated%20datasets%20below%20these%20of%20the%20experimental%20data.%20Modelling%20of%0Ahigh-energy%20particles%20propagating%20through%20the%20calorimeter%20section%20of%20the%0Adetector%20is%20the%20most%20computationally%20intensive%20MC%20simulation%20task.%20We%20introduce%0Aa%20technique%20combining%20recent%20advancements%20in%20generative%20models%20and%20quantum%0Aannealing%20for%20fast%20and%20efficient%20simulation%20of%20high-energy%20particle-calorimeter%0Ainteractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03179v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaloQVAE%2520%253A%2520Simulating%2520high-energy%2520particle-calorimeter%2520interactions%250A%2520%2520using%2520hybrid%2520quantum-classical%2520generative%2520models%26entry.906535625%3DSehmimul%2520Hoque%2520and%2520Hao%2520Jia%2520and%2520Abhishek%2520Abhishek%2520and%2520Mojde%2520Fadaie%2520and%2520J.%2520Quetzalcoatl%2520Toledo-Mar%25C3%25ADn%2520and%2520Tiago%2520Vale%2520and%2520Roger%2520G.%2520Melko%2520and%2520Maximilian%2520Swiatlowski%2520and%2520Wojciech%2520T.%2520Fedorko%26entry.1292438233%3D%2520%2520The%2520Large%2520Hadron%2520Collider%2527s%2520high%2520luminosity%2520era%2520presents%2520major%2520computational%250Achallenges%2520in%2520the%2520analysis%2520of%2520collision%2520events.%2520Large%2520amounts%2520of%2520Monte%2520Carlo%250A%2528MC%2529%2520simulation%2520will%2520be%2520required%2520to%2520constrain%2520the%2520statistical%2520uncertainties%2520of%250Athe%2520simulated%2520datasets%2520below%2520these%2520of%2520the%2520experimental%2520data.%2520Modelling%2520of%250Ahigh-energy%2520particles%2520propagating%2520through%2520the%2520calorimeter%2520section%2520of%2520the%250Adetector%2520is%2520the%2520most%2520computationally%2520intensive%2520MC%2520simulation%2520task.%2520We%2520introduce%250Aa%2520technique%2520combining%2520recent%2520advancements%2520in%2520generative%2520models%2520and%2520quantum%250Aannealing%2520for%2520fast%2520and%2520efficient%2520simulation%2520of%2520high-energy%2520particle-calorimeter%250Ainteractions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03179v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaloQVAE%20%3A%20Simulating%20high-energy%20particle-calorimeter%20interactions%0A%20%20using%20hybrid%20quantum-classical%20generative%20models&entry.906535625=Sehmimul%20Hoque%20and%20Hao%20Jia%20and%20Abhishek%20Abhishek%20and%20Mojde%20Fadaie%20and%20J.%20Quetzalcoatl%20Toledo-Mar%C3%ADn%20and%20Tiago%20Vale%20and%20Roger%20G.%20Melko%20and%20Maximilian%20Swiatlowski%20and%20Wojciech%20T.%20Fedorko&entry.1292438233=%20%20The%20Large%20Hadron%20Collider%27s%20high%20luminosity%20era%20presents%20major%20computational%0Achallenges%20in%20the%20analysis%20of%20collision%20events.%20Large%20amounts%20of%20Monte%20Carlo%0A%28MC%29%20simulation%20will%20be%20required%20to%20constrain%20the%20statistical%20uncertainties%20of%0Athe%20simulated%20datasets%20below%20these%20of%20the%20experimental%20data.%20Modelling%20of%0Ahigh-energy%20particles%20propagating%20through%20the%20calorimeter%20section%20of%20the%0Adetector%20is%20the%20most%20computationally%20intensive%20MC%20simulation%20task.%20We%20introduce%0Aa%20technique%20combining%20recent%20advancements%20in%20generative%20models%20and%20quantum%0Aannealing%20for%20fast%20and%20efficient%20simulation%20of%20high-energy%20particle-calorimeter%0Ainteractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03179v4&entry.124074799=Read"},
{"title": "SARA: Singular-Value Based Adaptive Low-Rank Adaption", "author": "Jihao Gu and Shuai Chen and Zelin Wang and Yibo Zhang and Ping Gong", "abstract": "  With the increasing number of parameters in large pre-trained models, LoRA as\na parameter-efficient fine-tuning(PEFT) method is widely used for not adding\ninference overhead. The LoRA method assumes that weight changes during\nfine-tuning can be approximated by low-rank matrices. However, the rank values\nneed to be manually verified to match different downstream tasks, and they\ncannot accommodate the varying importance of different layers in the model. In\nthis work, we first analyze the relationship between the performance of\ndifferent layers and their ranks using SVD. Based on this, we design the\nSingular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds\nthe rank during initialization by performing SVD on the pre-trained weights.\nAdditionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly\nreduces the number of parameters by fine-tuning only multiple parallel sets of\nsingular values controlled by a router. Extensive experiments on various\ncomplex tasks demonstrate the simplicity and parameter efficiency of our\nmethods. They can effectively and adaptively find the most suitable rank for\neach layer of each model.\n", "link": "http://arxiv.org/abs/2408.03290v1", "date": "2024-08-06", "relevancy": 1.7783, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4466}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4456}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SARA%3A%20Singular-Value%20Based%20Adaptive%20Low-Rank%20Adaption&body=Title%3A%20SARA%3A%20Singular-Value%20Based%20Adaptive%20Low-Rank%20Adaption%0AAuthor%3A%20Jihao%20Gu%20and%20Shuai%20Chen%20and%20Zelin%20Wang%20and%20Yibo%20Zhang%20and%20Ping%20Gong%0AAbstract%3A%20%20%20With%20the%20increasing%20number%20of%20parameters%20in%20large%20pre-trained%20models%2C%20LoRA%20as%0Aa%20parameter-efficient%20fine-tuning%28PEFT%29%20method%20is%20widely%20used%20for%20not%20adding%0Ainference%20overhead.%20The%20LoRA%20method%20assumes%20that%20weight%20changes%20during%0Afine-tuning%20can%20be%20approximated%20by%20low-rank%20matrices.%20However%2C%20the%20rank%20values%0Aneed%20to%20be%20manually%20verified%20to%20match%20different%20downstream%20tasks%2C%20and%20they%0Acannot%20accommodate%20the%20varying%20importance%20of%20different%20layers%20in%20the%20model.%20In%0Athis%20work%2C%20we%20first%20analyze%20the%20relationship%20between%20the%20performance%20of%0Adifferent%20layers%20and%20their%20ranks%20using%20SVD.%20Based%20on%20this%2C%20we%20design%20the%0ASingular-Value%20Based%20Adaptive%20Low-Rank%20Adaption%28SARA%29%2C%20which%20adaptively%20finds%0Athe%20rank%20during%20initialization%20by%20performing%20SVD%20on%20the%20pre-trained%20weights.%0AAdditionally%2C%20we%20explore%20the%20Mixture-of-SARA%28Mo-SARA%29%2C%20which%20significantly%0Areduces%20the%20number%20of%20parameters%20by%20fine-tuning%20only%20multiple%20parallel%20sets%20of%0Asingular%20values%20controlled%20by%20a%20router.%20Extensive%20experiments%20on%20various%0Acomplex%20tasks%20demonstrate%20the%20simplicity%20and%20parameter%20efficiency%20of%20our%0Amethods.%20They%20can%20effectively%20and%20adaptively%20find%20the%20most%20suitable%20rank%20for%0Aeach%20layer%20of%20each%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSARA%253A%2520Singular-Value%2520Based%2520Adaptive%2520Low-Rank%2520Adaption%26entry.906535625%3DJihao%2520Gu%2520and%2520Shuai%2520Chen%2520and%2520Zelin%2520Wang%2520and%2520Yibo%2520Zhang%2520and%2520Ping%2520Gong%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520number%2520of%2520parameters%2520in%2520large%2520pre-trained%2520models%252C%2520LoRA%2520as%250Aa%2520parameter-efficient%2520fine-tuning%2528PEFT%2529%2520method%2520is%2520widely%2520used%2520for%2520not%2520adding%250Ainference%2520overhead.%2520The%2520LoRA%2520method%2520assumes%2520that%2520weight%2520changes%2520during%250Afine-tuning%2520can%2520be%2520approximated%2520by%2520low-rank%2520matrices.%2520However%252C%2520the%2520rank%2520values%250Aneed%2520to%2520be%2520manually%2520verified%2520to%2520match%2520different%2520downstream%2520tasks%252C%2520and%2520they%250Acannot%2520accommodate%2520the%2520varying%2520importance%2520of%2520different%2520layers%2520in%2520the%2520model.%2520In%250Athis%2520work%252C%2520we%2520first%2520analyze%2520the%2520relationship%2520between%2520the%2520performance%2520of%250Adifferent%2520layers%2520and%2520their%2520ranks%2520using%2520SVD.%2520Based%2520on%2520this%252C%2520we%2520design%2520the%250ASingular-Value%2520Based%2520Adaptive%2520Low-Rank%2520Adaption%2528SARA%2529%252C%2520which%2520adaptively%2520finds%250Athe%2520rank%2520during%2520initialization%2520by%2520performing%2520SVD%2520on%2520the%2520pre-trained%2520weights.%250AAdditionally%252C%2520we%2520explore%2520the%2520Mixture-of-SARA%2528Mo-SARA%2529%252C%2520which%2520significantly%250Areduces%2520the%2520number%2520of%2520parameters%2520by%2520fine-tuning%2520only%2520multiple%2520parallel%2520sets%2520of%250Asingular%2520values%2520controlled%2520by%2520a%2520router.%2520Extensive%2520experiments%2520on%2520various%250Acomplex%2520tasks%2520demonstrate%2520the%2520simplicity%2520and%2520parameter%2520efficiency%2520of%2520our%250Amethods.%2520They%2520can%2520effectively%2520and%2520adaptively%2520find%2520the%2520most%2520suitable%2520rank%2520for%250Aeach%2520layer%2520of%2520each%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SARA%3A%20Singular-Value%20Based%20Adaptive%20Low-Rank%20Adaption&entry.906535625=Jihao%20Gu%20and%20Shuai%20Chen%20and%20Zelin%20Wang%20and%20Yibo%20Zhang%20and%20Ping%20Gong&entry.1292438233=%20%20With%20the%20increasing%20number%20of%20parameters%20in%20large%20pre-trained%20models%2C%20LoRA%20as%0Aa%20parameter-efficient%20fine-tuning%28PEFT%29%20method%20is%20widely%20used%20for%20not%20adding%0Ainference%20overhead.%20The%20LoRA%20method%20assumes%20that%20weight%20changes%20during%0Afine-tuning%20can%20be%20approximated%20by%20low-rank%20matrices.%20However%2C%20the%20rank%20values%0Aneed%20to%20be%20manually%20verified%20to%20match%20different%20downstream%20tasks%2C%20and%20they%0Acannot%20accommodate%20the%20varying%20importance%20of%20different%20layers%20in%20the%20model.%20In%0Athis%20work%2C%20we%20first%20analyze%20the%20relationship%20between%20the%20performance%20of%0Adifferent%20layers%20and%20their%20ranks%20using%20SVD.%20Based%20on%20this%2C%20we%20design%20the%0ASingular-Value%20Based%20Adaptive%20Low-Rank%20Adaption%28SARA%29%2C%20which%20adaptively%20finds%0Athe%20rank%20during%20initialization%20by%20performing%20SVD%20on%20the%20pre-trained%20weights.%0AAdditionally%2C%20we%20explore%20the%20Mixture-of-SARA%28Mo-SARA%29%2C%20which%20significantly%0Areduces%20the%20number%20of%20parameters%20by%20fine-tuning%20only%20multiple%20parallel%20sets%20of%0Asingular%20values%20controlled%20by%20a%20router.%20Extensive%20experiments%20on%20various%0Acomplex%20tasks%20demonstrate%20the%20simplicity%20and%20parameter%20efficiency%20of%20our%0Amethods.%20They%20can%20effectively%20and%20adaptively%20find%20the%20most%20suitable%20rank%20for%0Aeach%20layer%20of%20each%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03290v1&entry.124074799=Read"},
{"title": "Training LLMs to Recognize Hedges in Spontaneous Narratives", "author": "Amie J. Paige and Adil Soubki and John Murzaku and Owen Rambow and Susan E. Brennan", "abstract": "  Hedges allow speakers to mark utterances as provisional, whether to signal\nnon-prototypicality or \"fuzziness\", to indicate a lack of commitment to an\nutterance, to attribute responsibility for a statement to someone else, to\ninvite input from a partner, or to soften critical feedback in the service of\nface-management needs. Here we focus on hedges in an experimentally\nparameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced\nfrom memory by 21 speakers for co-present addressees, transcribed to text\n(Galati and Brennan, 2010). We created a gold standard of hedges annotated by\nhuman coders (the Roadrunner-Hedge corpus) and compared three LLM-based\napproaches for hedge detection: fine-tuning BERT, and zero and few-shot\nprompting with GPT-4o and LLaMA-3. The best-performing approach was a\nfine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on\nthe top performing approaches, we used an LLM-in-the-Loop approach to improve\nthe gold standard coding, as well as to highlight cases in which hedges are\nambiguous in linguistically interesting ways that will guide future research.\nThis is the first step in our research program to train LLMs to interpret and\ngenerate collateral signals appropriately and meaningfully in conversation.\n", "link": "http://arxiv.org/abs/2408.03319v1", "date": "2024-08-06", "relevancy": 1.7753, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4549}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.442}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20LLMs%20to%20Recognize%20Hedges%20in%20Spontaneous%20Narratives&body=Title%3A%20Training%20LLMs%20to%20Recognize%20Hedges%20in%20Spontaneous%20Narratives%0AAuthor%3A%20Amie%20J.%20Paige%20and%20Adil%20Soubki%20and%20John%20Murzaku%20and%20Owen%20Rambow%20and%20Susan%20E.%20Brennan%0AAbstract%3A%20%20%20Hedges%20allow%20speakers%20to%20mark%20utterances%20as%20provisional%2C%20whether%20to%20signal%0Anon-prototypicality%20or%20%22fuzziness%22%2C%20to%20indicate%20a%20lack%20of%20commitment%20to%20an%0Autterance%2C%20to%20attribute%20responsibility%20for%20a%20statement%20to%20someone%20else%2C%20to%0Ainvite%20input%20from%20a%20partner%2C%20or%20to%20soften%20critical%20feedback%20in%20the%20service%20of%0Aface-management%20needs.%20Here%20we%20focus%20on%20hedges%20in%20an%20experimentally%0Aparameterized%20corpus%20of%2063%20Roadrunner%20cartoon%20narratives%20spontaneously%20produced%0Afrom%20memory%20by%2021%20speakers%20for%20co-present%20addressees%2C%20transcribed%20to%20text%0A%28Galati%20and%20Brennan%2C%202010%29.%20We%20created%20a%20gold%20standard%20of%20hedges%20annotated%20by%0Ahuman%20coders%20%28the%20Roadrunner-Hedge%20corpus%29%20and%20compared%20three%20LLM-based%0Aapproaches%20for%20hedge%20detection%3A%20fine-tuning%20BERT%2C%20and%20zero%20and%20few-shot%0Aprompting%20with%20GPT-4o%20and%20LLaMA-3.%20The%20best-performing%20approach%20was%20a%0Afine-tuned%20BERT%20model%2C%20followed%20by%20few-shot%20GPT-4o.%20After%20an%20error%20analysis%20on%0Athe%20top%20performing%20approaches%2C%20we%20used%20an%20LLM-in-the-Loop%20approach%20to%20improve%0Athe%20gold%20standard%20coding%2C%20as%20well%20as%20to%20highlight%20cases%20in%20which%20hedges%20are%0Aambiguous%20in%20linguistically%20interesting%20ways%20that%20will%20guide%20future%20research.%0AThis%20is%20the%20first%20step%20in%20our%20research%20program%20to%20train%20LLMs%20to%20interpret%20and%0Agenerate%20collateral%20signals%20appropriately%20and%20meaningfully%20in%20conversation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520LLMs%2520to%2520Recognize%2520Hedges%2520in%2520Spontaneous%2520Narratives%26entry.906535625%3DAmie%2520J.%2520Paige%2520and%2520Adil%2520Soubki%2520and%2520John%2520Murzaku%2520and%2520Owen%2520Rambow%2520and%2520Susan%2520E.%2520Brennan%26entry.1292438233%3D%2520%2520Hedges%2520allow%2520speakers%2520to%2520mark%2520utterances%2520as%2520provisional%252C%2520whether%2520to%2520signal%250Anon-prototypicality%2520or%2520%2522fuzziness%2522%252C%2520to%2520indicate%2520a%2520lack%2520of%2520commitment%2520to%2520an%250Autterance%252C%2520to%2520attribute%2520responsibility%2520for%2520a%2520statement%2520to%2520someone%2520else%252C%2520to%250Ainvite%2520input%2520from%2520a%2520partner%252C%2520or%2520to%2520soften%2520critical%2520feedback%2520in%2520the%2520service%2520of%250Aface-management%2520needs.%2520Here%2520we%2520focus%2520on%2520hedges%2520in%2520an%2520experimentally%250Aparameterized%2520corpus%2520of%252063%2520Roadrunner%2520cartoon%2520narratives%2520spontaneously%2520produced%250Afrom%2520memory%2520by%252021%2520speakers%2520for%2520co-present%2520addressees%252C%2520transcribed%2520to%2520text%250A%2528Galati%2520and%2520Brennan%252C%25202010%2529.%2520We%2520created%2520a%2520gold%2520standard%2520of%2520hedges%2520annotated%2520by%250Ahuman%2520coders%2520%2528the%2520Roadrunner-Hedge%2520corpus%2529%2520and%2520compared%2520three%2520LLM-based%250Aapproaches%2520for%2520hedge%2520detection%253A%2520fine-tuning%2520BERT%252C%2520and%2520zero%2520and%2520few-shot%250Aprompting%2520with%2520GPT-4o%2520and%2520LLaMA-3.%2520The%2520best-performing%2520approach%2520was%2520a%250Afine-tuned%2520BERT%2520model%252C%2520followed%2520by%2520few-shot%2520GPT-4o.%2520After%2520an%2520error%2520analysis%2520on%250Athe%2520top%2520performing%2520approaches%252C%2520we%2520used%2520an%2520LLM-in-the-Loop%2520approach%2520to%2520improve%250Athe%2520gold%2520standard%2520coding%252C%2520as%2520well%2520as%2520to%2520highlight%2520cases%2520in%2520which%2520hedges%2520are%250Aambiguous%2520in%2520linguistically%2520interesting%2520ways%2520that%2520will%2520guide%2520future%2520research.%250AThis%2520is%2520the%2520first%2520step%2520in%2520our%2520research%2520program%2520to%2520train%2520LLMs%2520to%2520interpret%2520and%250Agenerate%2520collateral%2520signals%2520appropriately%2520and%2520meaningfully%2520in%2520conversation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20LLMs%20to%20Recognize%20Hedges%20in%20Spontaneous%20Narratives&entry.906535625=Amie%20J.%20Paige%20and%20Adil%20Soubki%20and%20John%20Murzaku%20and%20Owen%20Rambow%20and%20Susan%20E.%20Brennan&entry.1292438233=%20%20Hedges%20allow%20speakers%20to%20mark%20utterances%20as%20provisional%2C%20whether%20to%20signal%0Anon-prototypicality%20or%20%22fuzziness%22%2C%20to%20indicate%20a%20lack%20of%20commitment%20to%20an%0Autterance%2C%20to%20attribute%20responsibility%20for%20a%20statement%20to%20someone%20else%2C%20to%0Ainvite%20input%20from%20a%20partner%2C%20or%20to%20soften%20critical%20feedback%20in%20the%20service%20of%0Aface-management%20needs.%20Here%20we%20focus%20on%20hedges%20in%20an%20experimentally%0Aparameterized%20corpus%20of%2063%20Roadrunner%20cartoon%20narratives%20spontaneously%20produced%0Afrom%20memory%20by%2021%20speakers%20for%20co-present%20addressees%2C%20transcribed%20to%20text%0A%28Galati%20and%20Brennan%2C%202010%29.%20We%20created%20a%20gold%20standard%20of%20hedges%20annotated%20by%0Ahuman%20coders%20%28the%20Roadrunner-Hedge%20corpus%29%20and%20compared%20three%20LLM-based%0Aapproaches%20for%20hedge%20detection%3A%20fine-tuning%20BERT%2C%20and%20zero%20and%20few-shot%0Aprompting%20with%20GPT-4o%20and%20LLaMA-3.%20The%20best-performing%20approach%20was%20a%0Afine-tuned%20BERT%20model%2C%20followed%20by%20few-shot%20GPT-4o.%20After%20an%20error%20analysis%20on%0Athe%20top%20performing%20approaches%2C%20we%20used%20an%20LLM-in-the-Loop%20approach%20to%20improve%0Athe%20gold%20standard%20coding%2C%20as%20well%20as%20to%20highlight%20cases%20in%20which%20hedges%20are%0Aambiguous%20in%20linguistically%20interesting%20ways%20that%20will%20guide%20future%20research.%0AThis%20is%20the%20first%20step%20in%20our%20research%20program%20to%20train%20LLMs%20to%20interpret%20and%0Agenerate%20collateral%20signals%20appropriately%20and%20meaningfully%20in%20conversation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03319v1&entry.124074799=Read"},
{"title": "An Adaptive Second-order Method for a Class of Nonconvex Nonsmooth\n  Composite Optimization", "author": "Hao Wang and Xiangyu Yang and Yichen Zhu", "abstract": "  This paper explores a specific type of nonconvex sparsity-promoting\nregularization problems, namely those involving $\\ell_p$-norm regularization,\nin conjunction with a twice continuously differentiable loss function. We\npropose a novel second-order algorithm designed to effectively address this\nclass of challenging nonconvex and nonsmooth problems, showcasing several\ninnovative features: (i) The use of an alternating strategy to solve a\nreweighted $\\ell_1$ regularized subproblem and the subspace approximate Newton\nstep. (ii) The reweighted $\\ell_1$ regularized subproblem relies on a convex\napproximation to the nonconvex regularization term, enabling a closed-form\nsolution characterized by the soft-thresholding operator. This feature allows\nour method to be applied to various nonconvex regularization problems. (iii)\nOur algorithm ensures that the iterates maintain their sign values and that\nnonzero components are kept away from 0 for a sufficient number of iterations,\neventually transitioning to a perturbed Newton method. (iv) We provide\ntheoretical guarantees of global convergence, local superlinear convergence in\nthe presence of the Kurdyka-\\L ojasiewicz (KL) property, and local quadratic\nconvergence when employing the exact Newton step in our algorithm. We also\nshowcase the effectiveness of our approach through experiments on a diverse set\nof model prediction problems.\n", "link": "http://arxiv.org/abs/2407.17216v2", "date": "2024-08-06", "relevancy": 1.7713, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4484}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4463}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20Second-order%20Method%20for%20a%20Class%20of%20Nonconvex%20Nonsmooth%0A%20%20Composite%20Optimization&body=Title%3A%20An%20Adaptive%20Second-order%20Method%20for%20a%20Class%20of%20Nonconvex%20Nonsmooth%0A%20%20Composite%20Optimization%0AAuthor%3A%20Hao%20Wang%20and%20Xiangyu%20Yang%20and%20Yichen%20Zhu%0AAbstract%3A%20%20%20This%20paper%20explores%20a%20specific%20type%20of%20nonconvex%20sparsity-promoting%0Aregularization%20problems%2C%20namely%20those%20involving%20%24%5Cell_p%24-norm%20regularization%2C%0Ain%20conjunction%20with%20a%20twice%20continuously%20differentiable%20loss%20function.%20We%0Apropose%20a%20novel%20second-order%20algorithm%20designed%20to%20effectively%20address%20this%0Aclass%20of%20challenging%20nonconvex%20and%20nonsmooth%20problems%2C%20showcasing%20several%0Ainnovative%20features%3A%20%28i%29%20The%20use%20of%20an%20alternating%20strategy%20to%20solve%20a%0Areweighted%20%24%5Cell_1%24%20regularized%20subproblem%20and%20the%20subspace%20approximate%20Newton%0Astep.%20%28ii%29%20The%20reweighted%20%24%5Cell_1%24%20regularized%20subproblem%20relies%20on%20a%20convex%0Aapproximation%20to%20the%20nonconvex%20regularization%20term%2C%20enabling%20a%20closed-form%0Asolution%20characterized%20by%20the%20soft-thresholding%20operator.%20This%20feature%20allows%0Aour%20method%20to%20be%20applied%20to%20various%20nonconvex%20regularization%20problems.%20%28iii%29%0AOur%20algorithm%20ensures%20that%20the%20iterates%20maintain%20their%20sign%20values%20and%20that%0Anonzero%20components%20are%20kept%20away%20from%200%20for%20a%20sufficient%20number%20of%20iterations%2C%0Aeventually%20transitioning%20to%20a%20perturbed%20Newton%20method.%20%28iv%29%20We%20provide%0Atheoretical%20guarantees%20of%20global%20convergence%2C%20local%20superlinear%20convergence%20in%0Athe%20presence%20of%20the%20Kurdyka-%5CL%20ojasiewicz%20%28KL%29%20property%2C%20and%20local%20quadratic%0Aconvergence%20when%20employing%20the%20exact%20Newton%20step%20in%20our%20algorithm.%20We%20also%0Ashowcase%20the%20effectiveness%20of%20our%20approach%20through%20experiments%20on%20a%20diverse%20set%0Aof%20model%20prediction%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520Second-order%2520Method%2520for%2520a%2520Class%2520of%2520Nonconvex%2520Nonsmooth%250A%2520%2520Composite%2520Optimization%26entry.906535625%3DHao%2520Wang%2520and%2520Xiangyu%2520Yang%2520and%2520Yichen%2520Zhu%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520a%2520specific%2520type%2520of%2520nonconvex%2520sparsity-promoting%250Aregularization%2520problems%252C%2520namely%2520those%2520involving%2520%2524%255Cell_p%2524-norm%2520regularization%252C%250Ain%2520conjunction%2520with%2520a%2520twice%2520continuously%2520differentiable%2520loss%2520function.%2520We%250Apropose%2520a%2520novel%2520second-order%2520algorithm%2520designed%2520to%2520effectively%2520address%2520this%250Aclass%2520of%2520challenging%2520nonconvex%2520and%2520nonsmooth%2520problems%252C%2520showcasing%2520several%250Ainnovative%2520features%253A%2520%2528i%2529%2520The%2520use%2520of%2520an%2520alternating%2520strategy%2520to%2520solve%2520a%250Areweighted%2520%2524%255Cell_1%2524%2520regularized%2520subproblem%2520and%2520the%2520subspace%2520approximate%2520Newton%250Astep.%2520%2528ii%2529%2520The%2520reweighted%2520%2524%255Cell_1%2524%2520regularized%2520subproblem%2520relies%2520on%2520a%2520convex%250Aapproximation%2520to%2520the%2520nonconvex%2520regularization%2520term%252C%2520enabling%2520a%2520closed-form%250Asolution%2520characterized%2520by%2520the%2520soft-thresholding%2520operator.%2520This%2520feature%2520allows%250Aour%2520method%2520to%2520be%2520applied%2520to%2520various%2520nonconvex%2520regularization%2520problems.%2520%2528iii%2529%250AOur%2520algorithm%2520ensures%2520that%2520the%2520iterates%2520maintain%2520their%2520sign%2520values%2520and%2520that%250Anonzero%2520components%2520are%2520kept%2520away%2520from%25200%2520for%2520a%2520sufficient%2520number%2520of%2520iterations%252C%250Aeventually%2520transitioning%2520to%2520a%2520perturbed%2520Newton%2520method.%2520%2528iv%2529%2520We%2520provide%250Atheoretical%2520guarantees%2520of%2520global%2520convergence%252C%2520local%2520superlinear%2520convergence%2520in%250Athe%2520presence%2520of%2520the%2520Kurdyka-%255CL%2520ojasiewicz%2520%2528KL%2529%2520property%252C%2520and%2520local%2520quadratic%250Aconvergence%2520when%2520employing%2520the%2520exact%2520Newton%2520step%2520in%2520our%2520algorithm.%2520We%2520also%250Ashowcase%2520the%2520effectiveness%2520of%2520our%2520approach%2520through%2520experiments%2520on%2520a%2520diverse%2520set%250Aof%2520model%2520prediction%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20Second-order%20Method%20for%20a%20Class%20of%20Nonconvex%20Nonsmooth%0A%20%20Composite%20Optimization&entry.906535625=Hao%20Wang%20and%20Xiangyu%20Yang%20and%20Yichen%20Zhu&entry.1292438233=%20%20This%20paper%20explores%20a%20specific%20type%20of%20nonconvex%20sparsity-promoting%0Aregularization%20problems%2C%20namely%20those%20involving%20%24%5Cell_p%24-norm%20regularization%2C%0Ain%20conjunction%20with%20a%20twice%20continuously%20differentiable%20loss%20function.%20We%0Apropose%20a%20novel%20second-order%20algorithm%20designed%20to%20effectively%20address%20this%0Aclass%20of%20challenging%20nonconvex%20and%20nonsmooth%20problems%2C%20showcasing%20several%0Ainnovative%20features%3A%20%28i%29%20The%20use%20of%20an%20alternating%20strategy%20to%20solve%20a%0Areweighted%20%24%5Cell_1%24%20regularized%20subproblem%20and%20the%20subspace%20approximate%20Newton%0Astep.%20%28ii%29%20The%20reweighted%20%24%5Cell_1%24%20regularized%20subproblem%20relies%20on%20a%20convex%0Aapproximation%20to%20the%20nonconvex%20regularization%20term%2C%20enabling%20a%20closed-form%0Asolution%20characterized%20by%20the%20soft-thresholding%20operator.%20This%20feature%20allows%0Aour%20method%20to%20be%20applied%20to%20various%20nonconvex%20regularization%20problems.%20%28iii%29%0AOur%20algorithm%20ensures%20that%20the%20iterates%20maintain%20their%20sign%20values%20and%20that%0Anonzero%20components%20are%20kept%20away%20from%200%20for%20a%20sufficient%20number%20of%20iterations%2C%0Aeventually%20transitioning%20to%20a%20perturbed%20Newton%20method.%20%28iv%29%20We%20provide%0Atheoretical%20guarantees%20of%20global%20convergence%2C%20local%20superlinear%20convergence%20in%0Athe%20presence%20of%20the%20Kurdyka-%5CL%20ojasiewicz%20%28KL%29%20property%2C%20and%20local%20quadratic%0Aconvergence%20when%20employing%20the%20exact%20Newton%20step%20in%20our%20algorithm.%20We%20also%0Ashowcase%20the%20effectiveness%20of%20our%20approach%20through%20experiments%20on%20a%20diverse%20set%0Aof%20model%20prediction%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17216v2&entry.124074799=Read"},
{"title": "IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning\n  using Instruct Prompts", "author": "Ciara Rowles and Shimon Vainer and Dante De Nigris and Slava Elizarov and Konstantin Kutsy and Simon Donn\u00e9", "abstract": "  Diffusion models continuously push the boundary of state-of-the-art image\ngeneration, but the process is hard to control with any nuance: practice proves\nthat textual prompts are inadequate for accurately describing image style or\nfine structural details (such as faces). ControlNet and IPAdapter address this\nshortcoming by conditioning the generative process on imagery instead, but each\nindividual instance is limited to modeling a single conditional posterior: for\npractical use-cases, where multiple different posteriors are desired within the\nsame workflow, training and using multiple adapters is cumbersome. We propose\nIPAdapter-Instruct, which combines natural-image conditioning with ``Instruct''\nprompts to swap between interpretations for the same conditioning image: style\ntransfer, object extraction, both, or something else still? IPAdapterInstruct\nefficiently learns multiple tasks with minimal loss in quality compared to\ndedicated per-task models.\n", "link": "http://arxiv.org/abs/2408.03209v1", "date": "2024-08-06", "relevancy": 1.7353, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6262}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.608}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IPAdapter-Instruct%3A%20Resolving%20Ambiguity%20in%20Image-based%20Conditioning%0A%20%20using%20Instruct%20Prompts&body=Title%3A%20IPAdapter-Instruct%3A%20Resolving%20Ambiguity%20in%20Image-based%20Conditioning%0A%20%20using%20Instruct%20Prompts%0AAuthor%3A%20Ciara%20Rowles%20and%20Shimon%20Vainer%20and%20Dante%20De%20Nigris%20and%20Slava%20Elizarov%20and%20Konstantin%20Kutsy%20and%20Simon%20Donn%C3%A9%0AAbstract%3A%20%20%20Diffusion%20models%20continuously%20push%20the%20boundary%20of%20state-of-the-art%20image%0Ageneration%2C%20but%20the%20process%20is%20hard%20to%20control%20with%20any%20nuance%3A%20practice%20proves%0Athat%20textual%20prompts%20are%20inadequate%20for%20accurately%20describing%20image%20style%20or%0Afine%20structural%20details%20%28such%20as%20faces%29.%20ControlNet%20and%20IPAdapter%20address%20this%0Ashortcoming%20by%20conditioning%20the%20generative%20process%20on%20imagery%20instead%2C%20but%20each%0Aindividual%20instance%20is%20limited%20to%20modeling%20a%20single%20conditional%20posterior%3A%20for%0Apractical%20use-cases%2C%20where%20multiple%20different%20posteriors%20are%20desired%20within%20the%0Asame%20workflow%2C%20training%20and%20using%20multiple%20adapters%20is%20cumbersome.%20We%20propose%0AIPAdapter-Instruct%2C%20which%20combines%20natural-image%20conditioning%20with%20%60%60Instruct%27%27%0Aprompts%20to%20swap%20between%20interpretations%20for%20the%20same%20conditioning%20image%3A%20style%0Atransfer%2C%20object%20extraction%2C%20both%2C%20or%20something%20else%20still%3F%20IPAdapterInstruct%0Aefficiently%20learns%20multiple%20tasks%20with%20minimal%20loss%20in%20quality%20compared%20to%0Adedicated%20per-task%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIPAdapter-Instruct%253A%2520Resolving%2520Ambiguity%2520in%2520Image-based%2520Conditioning%250A%2520%2520using%2520Instruct%2520Prompts%26entry.906535625%3DCiara%2520Rowles%2520and%2520Shimon%2520Vainer%2520and%2520Dante%2520De%2520Nigris%2520and%2520Slava%2520Elizarov%2520and%2520Konstantin%2520Kutsy%2520and%2520Simon%2520Donn%25C3%25A9%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520continuously%2520push%2520the%2520boundary%2520of%2520state-of-the-art%2520image%250Ageneration%252C%2520but%2520the%2520process%2520is%2520hard%2520to%2520control%2520with%2520any%2520nuance%253A%2520practice%2520proves%250Athat%2520textual%2520prompts%2520are%2520inadequate%2520for%2520accurately%2520describing%2520image%2520style%2520or%250Afine%2520structural%2520details%2520%2528such%2520as%2520faces%2529.%2520ControlNet%2520and%2520IPAdapter%2520address%2520this%250Ashortcoming%2520by%2520conditioning%2520the%2520generative%2520process%2520on%2520imagery%2520instead%252C%2520but%2520each%250Aindividual%2520instance%2520is%2520limited%2520to%2520modeling%2520a%2520single%2520conditional%2520posterior%253A%2520for%250Apractical%2520use-cases%252C%2520where%2520multiple%2520different%2520posteriors%2520are%2520desired%2520within%2520the%250Asame%2520workflow%252C%2520training%2520and%2520using%2520multiple%2520adapters%2520is%2520cumbersome.%2520We%2520propose%250AIPAdapter-Instruct%252C%2520which%2520combines%2520natural-image%2520conditioning%2520with%2520%2560%2560Instruct%2527%2527%250Aprompts%2520to%2520swap%2520between%2520interpretations%2520for%2520the%2520same%2520conditioning%2520image%253A%2520style%250Atransfer%252C%2520object%2520extraction%252C%2520both%252C%2520or%2520something%2520else%2520still%253F%2520IPAdapterInstruct%250Aefficiently%2520learns%2520multiple%2520tasks%2520with%2520minimal%2520loss%2520in%2520quality%2520compared%2520to%250Adedicated%2520per-task%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IPAdapter-Instruct%3A%20Resolving%20Ambiguity%20in%20Image-based%20Conditioning%0A%20%20using%20Instruct%20Prompts&entry.906535625=Ciara%20Rowles%20and%20Shimon%20Vainer%20and%20Dante%20De%20Nigris%20and%20Slava%20Elizarov%20and%20Konstantin%20Kutsy%20and%20Simon%20Donn%C3%A9&entry.1292438233=%20%20Diffusion%20models%20continuously%20push%20the%20boundary%20of%20state-of-the-art%20image%0Ageneration%2C%20but%20the%20process%20is%20hard%20to%20control%20with%20any%20nuance%3A%20practice%20proves%0Athat%20textual%20prompts%20are%20inadequate%20for%20accurately%20describing%20image%20style%20or%0Afine%20structural%20details%20%28such%20as%20faces%29.%20ControlNet%20and%20IPAdapter%20address%20this%0Ashortcoming%20by%20conditioning%20the%20generative%20process%20on%20imagery%20instead%2C%20but%20each%0Aindividual%20instance%20is%20limited%20to%20modeling%20a%20single%20conditional%20posterior%3A%20for%0Apractical%20use-cases%2C%20where%20multiple%20different%20posteriors%20are%20desired%20within%20the%0Asame%20workflow%2C%20training%20and%20using%20multiple%20adapters%20is%20cumbersome.%20We%20propose%0AIPAdapter-Instruct%2C%20which%20combines%20natural-image%20conditioning%20with%20%60%60Instruct%27%27%0Aprompts%20to%20swap%20between%20interpretations%20for%20the%20same%20conditioning%20image%3A%20style%0Atransfer%2C%20object%20extraction%2C%20both%2C%20or%20something%20else%20still%3F%20IPAdapterInstruct%0Aefficiently%20learns%20multiple%20tasks%20with%20minimal%20loss%20in%20quality%20compared%20to%0Adedicated%20per-task%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03209v1&entry.124074799=Read"},
{"title": "Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data", "author": "Tim Baumg\u00e4rtner and Yang Gao and Dana Alon and Donald Metzler", "abstract": "  Reinforcement Learning from Human Feedback (RLHF) is a popular method for\naligning Language Models (LM) with human values and preferences. RLHF requires\na large number of preference pairs as training data, which are often used in\nboth the Supervised Fine-Tuning and Reward Model training and therefore\npublicly available datasets are commonly used. In this work, we study to what\nextent a malicious actor can manipulate the LMs generations by poisoning the\npreferences, i.e., injecting poisonous preference pairs into these datasets and\nthe RLHF training process. We propose strategies to build poisonous preference\npairs and test their performance by poisoning two widely used preference\ndatasets. Our results show that preference poisoning is highly effective:\ninjecting a small amount of poisonous data (1-5\\% of the original dataset), we\ncan effectively manipulate the LM to generate a target entity in a target\nsentiment (positive or negative). The findings from our experiments also shed\nlight on strategies to defend against the preference poisoning attack.\n", "link": "http://arxiv.org/abs/2404.05530v2", "date": "2024-08-06", "relevancy": 1.7259, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4624}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4099}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Best-of-Venom%3A%20Attacking%20RLHF%20by%20Injecting%20Poisoned%20Preference%20Data&body=Title%3A%20Best-of-Venom%3A%20Attacking%20RLHF%20by%20Injecting%20Poisoned%20Preference%20Data%0AAuthor%3A%20Tim%20Baumg%C3%A4rtner%20and%20Yang%20Gao%20and%20Dana%20Alon%20and%20Donald%20Metzler%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20a%20popular%20method%20for%0Aaligning%20Language%20Models%20%28LM%29%20with%20human%20values%20and%20preferences.%20RLHF%20requires%0Aa%20large%20number%20of%20preference%20pairs%20as%20training%20data%2C%20which%20are%20often%20used%20in%0Aboth%20the%20Supervised%20Fine-Tuning%20and%20Reward%20Model%20training%20and%20therefore%0Apublicly%20available%20datasets%20are%20commonly%20used.%20In%20this%20work%2C%20we%20study%20to%20what%0Aextent%20a%20malicious%20actor%20can%20manipulate%20the%20LMs%20generations%20by%20poisoning%20the%0Apreferences%2C%20i.e.%2C%20injecting%20poisonous%20preference%20pairs%20into%20these%20datasets%20and%0Athe%20RLHF%20training%20process.%20We%20propose%20strategies%20to%20build%20poisonous%20preference%0Apairs%20and%20test%20their%20performance%20by%20poisoning%20two%20widely%20used%20preference%0Adatasets.%20Our%20results%20show%20that%20preference%20poisoning%20is%20highly%20effective%3A%0Ainjecting%20a%20small%20amount%20of%20poisonous%20data%20%281-5%5C%25%20of%20the%20original%20dataset%29%2C%20we%0Acan%20effectively%20manipulate%20the%20LM%20to%20generate%20a%20target%20entity%20in%20a%20target%0Asentiment%20%28positive%20or%20negative%29.%20The%20findings%20from%20our%20experiments%20also%20shed%0Alight%20on%20strategies%20to%20defend%20against%20the%20preference%20poisoning%20attack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05530v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBest-of-Venom%253A%2520Attacking%2520RLHF%2520by%2520Injecting%2520Poisoned%2520Preference%2520Data%26entry.906535625%3DTim%2520Baumg%25C3%25A4rtner%2520and%2520Yang%2520Gao%2520and%2520Dana%2520Alon%2520and%2520Donald%2520Metzler%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520is%2520a%2520popular%2520method%2520for%250Aaligning%2520Language%2520Models%2520%2528LM%2529%2520with%2520human%2520values%2520and%2520preferences.%2520RLHF%2520requires%250Aa%2520large%2520number%2520of%2520preference%2520pairs%2520as%2520training%2520data%252C%2520which%2520are%2520often%2520used%2520in%250Aboth%2520the%2520Supervised%2520Fine-Tuning%2520and%2520Reward%2520Model%2520training%2520and%2520therefore%250Apublicly%2520available%2520datasets%2520are%2520commonly%2520used.%2520In%2520this%2520work%252C%2520we%2520study%2520to%2520what%250Aextent%2520a%2520malicious%2520actor%2520can%2520manipulate%2520the%2520LMs%2520generations%2520by%2520poisoning%2520the%250Apreferences%252C%2520i.e.%252C%2520injecting%2520poisonous%2520preference%2520pairs%2520into%2520these%2520datasets%2520and%250Athe%2520RLHF%2520training%2520process.%2520We%2520propose%2520strategies%2520to%2520build%2520poisonous%2520preference%250Apairs%2520and%2520test%2520their%2520performance%2520by%2520poisoning%2520two%2520widely%2520used%2520preference%250Adatasets.%2520Our%2520results%2520show%2520that%2520preference%2520poisoning%2520is%2520highly%2520effective%253A%250Ainjecting%2520a%2520small%2520amount%2520of%2520poisonous%2520data%2520%25281-5%255C%2525%2520of%2520the%2520original%2520dataset%2529%252C%2520we%250Acan%2520effectively%2520manipulate%2520the%2520LM%2520to%2520generate%2520a%2520target%2520entity%2520in%2520a%2520target%250Asentiment%2520%2528positive%2520or%2520negative%2529.%2520The%2520findings%2520from%2520our%2520experiments%2520also%2520shed%250Alight%2520on%2520strategies%2520to%2520defend%2520against%2520the%2520preference%2520poisoning%2520attack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05530v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Best-of-Venom%3A%20Attacking%20RLHF%20by%20Injecting%20Poisoned%20Preference%20Data&entry.906535625=Tim%20Baumg%C3%A4rtner%20and%20Yang%20Gao%20and%20Dana%20Alon%20and%20Donald%20Metzler&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20a%20popular%20method%20for%0Aaligning%20Language%20Models%20%28LM%29%20with%20human%20values%20and%20preferences.%20RLHF%20requires%0Aa%20large%20number%20of%20preference%20pairs%20as%20training%20data%2C%20which%20are%20often%20used%20in%0Aboth%20the%20Supervised%20Fine-Tuning%20and%20Reward%20Model%20training%20and%20therefore%0Apublicly%20available%20datasets%20are%20commonly%20used.%20In%20this%20work%2C%20we%20study%20to%20what%0Aextent%20a%20malicious%20actor%20can%20manipulate%20the%20LMs%20generations%20by%20poisoning%20the%0Apreferences%2C%20i.e.%2C%20injecting%20poisonous%20preference%20pairs%20into%20these%20datasets%20and%0Athe%20RLHF%20training%20process.%20We%20propose%20strategies%20to%20build%20poisonous%20preference%0Apairs%20and%20test%20their%20performance%20by%20poisoning%20two%20widely%20used%20preference%0Adatasets.%20Our%20results%20show%20that%20preference%20poisoning%20is%20highly%20effective%3A%0Ainjecting%20a%20small%20amount%20of%20poisonous%20data%20%281-5%5C%25%20of%20the%20original%20dataset%29%2C%20we%0Acan%20effectively%20manipulate%20the%20LM%20to%20generate%20a%20target%20entity%20in%20a%20target%0Asentiment%20%28positive%20or%20negative%29.%20The%20findings%20from%20our%20experiments%20also%20shed%0Alight%20on%20strategies%20to%20defend%20against%20the%20preference%20poisoning%20attack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05530v2&entry.124074799=Read"},
{"title": "Dilated Convolution with Learnable Spacings makes visual models more\n  aligned with humans: a Grad-CAM study", "author": "Rabih Chamas and Ismail Khalfaoui-Hassani and Timothee Masquelier", "abstract": "  Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced\nconvolution method that allows enlarging the receptive fields (RF) without\nincreasing the number of parameters, like the dilated convolution, yet without\nimposing a regular grid. DCLS has been shown to outperform the standard and\ndilated convolutions on several computer vision benchmarks. Here, we show that,\nin addition, DCLS increases the models' interpretability, defined as the\nalignment with human visual strategies. To quantify it, we use the Spearman\ncorrelation between the models' GradCAM heatmaps and the ClickMe dataset\nheatmaps, which reflect human visual attention. We took eight reference models\n- ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and\n36) - and drop-in replaced the standard convolution layers with DCLS ones. This\nimproved the interpretability score in seven of them. Moreover, we observed\nthat Grad-CAM generated random heatmaps for two models in our study: CAFormer\nand ConvFormer models, leading to low interpretability scores. We addressed\nthis issue by introducing Threshold-Grad-CAM, a modification built on top of\nGrad-CAM that enhanced interpretability across nearly all models. The code and\ncheckpoints to reproduce this study are available at:\nhttps://github.com/rabihchamas/DCLS-GradCAM-Eval.\n", "link": "http://arxiv.org/abs/2408.03164v1", "date": "2024-08-06", "relevancy": 1.6995, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.573}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5684}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dilated%20Convolution%20with%20Learnable%20Spacings%20makes%20visual%20models%20more%0A%20%20aligned%20with%20humans%3A%20a%20Grad-CAM%20study&body=Title%3A%20Dilated%20Convolution%20with%20Learnable%20Spacings%20makes%20visual%20models%20more%0A%20%20aligned%20with%20humans%3A%20a%20Grad-CAM%20study%0AAuthor%3A%20Rabih%20Chamas%20and%20Ismail%20Khalfaoui-Hassani%20and%20Timothee%20Masquelier%0AAbstract%3A%20%20%20Dilated%20Convolution%20with%20Learnable%20Spacing%20%28DCLS%29%20is%20a%20recent%20advanced%0Aconvolution%20method%20that%20allows%20enlarging%20the%20receptive%20fields%20%28RF%29%20without%0Aincreasing%20the%20number%20of%20parameters%2C%20like%20the%20dilated%20convolution%2C%20yet%20without%0Aimposing%20a%20regular%20grid.%20DCLS%20has%20been%20shown%20to%20outperform%20the%20standard%20and%0Adilated%20convolutions%20on%20several%20computer%20vision%20benchmarks.%20Here%2C%20we%20show%20that%2C%0Ain%20addition%2C%20DCLS%20increases%20the%20models%27%20interpretability%2C%20defined%20as%20the%0Aalignment%20with%20human%20visual%20strategies.%20To%20quantify%20it%2C%20we%20use%20the%20Spearman%0Acorrelation%20between%20the%20models%27%20GradCAM%20heatmaps%20and%20the%20ClickMe%20dataset%0Aheatmaps%2C%20which%20reflect%20human%20visual%20attention.%20We%20took%20eight%20reference%20models%0A-%20ResNet50%2C%20ConvNeXt%20%28T%2C%20S%20and%20B%29%2C%20CAFormer%2C%20ConvFormer%2C%20and%20FastViT%20%28sa%2024%20and%0A36%29%20-%20and%20drop-in%20replaced%20the%20standard%20convolution%20layers%20with%20DCLS%20ones.%20This%0Aimproved%20the%20interpretability%20score%20in%20seven%20of%20them.%20Moreover%2C%20we%20observed%0Athat%20Grad-CAM%20generated%20random%20heatmaps%20for%20two%20models%20in%20our%20study%3A%20CAFormer%0Aand%20ConvFormer%20models%2C%20leading%20to%20low%20interpretability%20scores.%20We%20addressed%0Athis%20issue%20by%20introducing%20Threshold-Grad-CAM%2C%20a%20modification%20built%20on%20top%20of%0AGrad-CAM%20that%20enhanced%20interpretability%20across%20nearly%20all%20models.%20The%20code%20and%0Acheckpoints%20to%20reproduce%20this%20study%20are%20available%20at%3A%0Ahttps%3A//github.com/rabihchamas/DCLS-GradCAM-Eval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDilated%2520Convolution%2520with%2520Learnable%2520Spacings%2520makes%2520visual%2520models%2520more%250A%2520%2520aligned%2520with%2520humans%253A%2520a%2520Grad-CAM%2520study%26entry.906535625%3DRabih%2520Chamas%2520and%2520Ismail%2520Khalfaoui-Hassani%2520and%2520Timothee%2520Masquelier%26entry.1292438233%3D%2520%2520Dilated%2520Convolution%2520with%2520Learnable%2520Spacing%2520%2528DCLS%2529%2520is%2520a%2520recent%2520advanced%250Aconvolution%2520method%2520that%2520allows%2520enlarging%2520the%2520receptive%2520fields%2520%2528RF%2529%2520without%250Aincreasing%2520the%2520number%2520of%2520parameters%252C%2520like%2520the%2520dilated%2520convolution%252C%2520yet%2520without%250Aimposing%2520a%2520regular%2520grid.%2520DCLS%2520has%2520been%2520shown%2520to%2520outperform%2520the%2520standard%2520and%250Adilated%2520convolutions%2520on%2520several%2520computer%2520vision%2520benchmarks.%2520Here%252C%2520we%2520show%2520that%252C%250Ain%2520addition%252C%2520DCLS%2520increases%2520the%2520models%2527%2520interpretability%252C%2520defined%2520as%2520the%250Aalignment%2520with%2520human%2520visual%2520strategies.%2520To%2520quantify%2520it%252C%2520we%2520use%2520the%2520Spearman%250Acorrelation%2520between%2520the%2520models%2527%2520GradCAM%2520heatmaps%2520and%2520the%2520ClickMe%2520dataset%250Aheatmaps%252C%2520which%2520reflect%2520human%2520visual%2520attention.%2520We%2520took%2520eight%2520reference%2520models%250A-%2520ResNet50%252C%2520ConvNeXt%2520%2528T%252C%2520S%2520and%2520B%2529%252C%2520CAFormer%252C%2520ConvFormer%252C%2520and%2520FastViT%2520%2528sa%252024%2520and%250A36%2529%2520-%2520and%2520drop-in%2520replaced%2520the%2520standard%2520convolution%2520layers%2520with%2520DCLS%2520ones.%2520This%250Aimproved%2520the%2520interpretability%2520score%2520in%2520seven%2520of%2520them.%2520Moreover%252C%2520we%2520observed%250Athat%2520Grad-CAM%2520generated%2520random%2520heatmaps%2520for%2520two%2520models%2520in%2520our%2520study%253A%2520CAFormer%250Aand%2520ConvFormer%2520models%252C%2520leading%2520to%2520low%2520interpretability%2520scores.%2520We%2520addressed%250Athis%2520issue%2520by%2520introducing%2520Threshold-Grad-CAM%252C%2520a%2520modification%2520built%2520on%2520top%2520of%250AGrad-CAM%2520that%2520enhanced%2520interpretability%2520across%2520nearly%2520all%2520models.%2520The%2520code%2520and%250Acheckpoints%2520to%2520reproduce%2520this%2520study%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/rabihchamas/DCLS-GradCAM-Eval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dilated%20Convolution%20with%20Learnable%20Spacings%20makes%20visual%20models%20more%0A%20%20aligned%20with%20humans%3A%20a%20Grad-CAM%20study&entry.906535625=Rabih%20Chamas%20and%20Ismail%20Khalfaoui-Hassani%20and%20Timothee%20Masquelier&entry.1292438233=%20%20Dilated%20Convolution%20with%20Learnable%20Spacing%20%28DCLS%29%20is%20a%20recent%20advanced%0Aconvolution%20method%20that%20allows%20enlarging%20the%20receptive%20fields%20%28RF%29%20without%0Aincreasing%20the%20number%20of%20parameters%2C%20like%20the%20dilated%20convolution%2C%20yet%20without%0Aimposing%20a%20regular%20grid.%20DCLS%20has%20been%20shown%20to%20outperform%20the%20standard%20and%0Adilated%20convolutions%20on%20several%20computer%20vision%20benchmarks.%20Here%2C%20we%20show%20that%2C%0Ain%20addition%2C%20DCLS%20increases%20the%20models%27%20interpretability%2C%20defined%20as%20the%0Aalignment%20with%20human%20visual%20strategies.%20To%20quantify%20it%2C%20we%20use%20the%20Spearman%0Acorrelation%20between%20the%20models%27%20GradCAM%20heatmaps%20and%20the%20ClickMe%20dataset%0Aheatmaps%2C%20which%20reflect%20human%20visual%20attention.%20We%20took%20eight%20reference%20models%0A-%20ResNet50%2C%20ConvNeXt%20%28T%2C%20S%20and%20B%29%2C%20CAFormer%2C%20ConvFormer%2C%20and%20FastViT%20%28sa%2024%20and%0A36%29%20-%20and%20drop-in%20replaced%20the%20standard%20convolution%20layers%20with%20DCLS%20ones.%20This%0Aimproved%20the%20interpretability%20score%20in%20seven%20of%20them.%20Moreover%2C%20we%20observed%0Athat%20Grad-CAM%20generated%20random%20heatmaps%20for%20two%20models%20in%20our%20study%3A%20CAFormer%0Aand%20ConvFormer%20models%2C%20leading%20to%20low%20interpretability%20scores.%20We%20addressed%0Athis%20issue%20by%20introducing%20Threshold-Grad-CAM%2C%20a%20modification%20built%20on%20top%20of%0AGrad-CAM%20that%20enhanced%20interpretability%20across%20nearly%20all%20models.%20The%20code%20and%0Acheckpoints%20to%20reproduce%20this%20study%20are%20available%20at%3A%0Ahttps%3A//github.com/rabihchamas/DCLS-GradCAM-Eval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03164v1&entry.124074799=Read"},
{"title": "Variance reduction techniques for stochastic proximal point algorithms", "author": "Cheik Traor\u00e9 and Vassilis Apidopoulos and Saverio Salzo and Silvia Villa", "abstract": "  In the context of finite sums minimization, variance reduction techniques are\nwidely used to improve the performance of state-of-the-art stochastic gradient\nmethods. Their practical impact is clear, as well as their theoretical\nproperties. Stochastic proximal point algorithms have been studied as an\nalternative to stochastic gradient algorithms since they are more stable with\nrespect to the choice of the step size. However, their variance-reduced\nversions are not as well studied as the gradient ones. In this work, we propose\nthe first unified study of variance reduction techniques for stochastic\nproximal point algorithms. We introduce a generic stochastic proximal-based\nalgorithm that can be specified to give the proximal version of SVRG, SAGA, and\nsome of their variants. For this algorithm, in the smooth setting, we provide\nseveral convergence rates for the iterates and the objective function values,\nwhich are faster than those of the vanilla stochastic proximal point algorithm.\nMore specifically, for convex functions, we prove a sublinear convergence rate\nof $O(1/k)$. In addition, under the Polyak-{\\L}ojasiewicz (PL) condition, we\nobtain linear convergence rates. Finally, our numerical experiments demonstrate\nthe advantages of the proximal variance reduction methods over their gradient\ncounterparts in terms of the stability with respect to the choice of the step\nsize in most cases, especially for difficult problems.\n", "link": "http://arxiv.org/abs/2308.09310v3", "date": "2024-08-06", "relevancy": 1.6891, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4531}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4168}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variance%20reduction%20techniques%20for%20stochastic%20proximal%20point%20algorithms&body=Title%3A%20Variance%20reduction%20techniques%20for%20stochastic%20proximal%20point%20algorithms%0AAuthor%3A%20Cheik%20Traor%C3%A9%20and%20Vassilis%20Apidopoulos%20and%20Saverio%20Salzo%20and%20Silvia%20Villa%0AAbstract%3A%20%20%20In%20the%20context%20of%20finite%20sums%20minimization%2C%20variance%20reduction%20techniques%20are%0Awidely%20used%20to%20improve%20the%20performance%20of%20state-of-the-art%20stochastic%20gradient%0Amethods.%20Their%20practical%20impact%20is%20clear%2C%20as%20well%20as%20their%20theoretical%0Aproperties.%20Stochastic%20proximal%20point%20algorithms%20have%20been%20studied%20as%20an%0Aalternative%20to%20stochastic%20gradient%20algorithms%20since%20they%20are%20more%20stable%20with%0Arespect%20to%20the%20choice%20of%20the%20step%20size.%20However%2C%20their%20variance-reduced%0Aversions%20are%20not%20as%20well%20studied%20as%20the%20gradient%20ones.%20In%20this%20work%2C%20we%20propose%0Athe%20first%20unified%20study%20of%20variance%20reduction%20techniques%20for%20stochastic%0Aproximal%20point%20algorithms.%20We%20introduce%20a%20generic%20stochastic%20proximal-based%0Aalgorithm%20that%20can%20be%20specified%20to%20give%20the%20proximal%20version%20of%20SVRG%2C%20SAGA%2C%20and%0Asome%20of%20their%20variants.%20For%20this%20algorithm%2C%20in%20the%20smooth%20setting%2C%20we%20provide%0Aseveral%20convergence%20rates%20for%20the%20iterates%20and%20the%20objective%20function%20values%2C%0Awhich%20are%20faster%20than%20those%20of%20the%20vanilla%20stochastic%20proximal%20point%20algorithm.%0AMore%20specifically%2C%20for%20convex%20functions%2C%20we%20prove%20a%20sublinear%20convergence%20rate%0Aof%20%24O%281/k%29%24.%20In%20addition%2C%20under%20the%20Polyak-%7B%5CL%7Dojasiewicz%20%28PL%29%20condition%2C%20we%0Aobtain%20linear%20convergence%20rates.%20Finally%2C%20our%20numerical%20experiments%20demonstrate%0Athe%20advantages%20of%20the%20proximal%20variance%20reduction%20methods%20over%20their%20gradient%0Acounterparts%20in%20terms%20of%20the%20stability%20with%20respect%20to%20the%20choice%20of%20the%20step%0Asize%20in%20most%20cases%2C%20especially%20for%20difficult%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09310v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariance%2520reduction%2520techniques%2520for%2520stochastic%2520proximal%2520point%2520algorithms%26entry.906535625%3DCheik%2520Traor%25C3%25A9%2520and%2520Vassilis%2520Apidopoulos%2520and%2520Saverio%2520Salzo%2520and%2520Silvia%2520Villa%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520finite%2520sums%2520minimization%252C%2520variance%2520reduction%2520techniques%2520are%250Awidely%2520used%2520to%2520improve%2520the%2520performance%2520of%2520state-of-the-art%2520stochastic%2520gradient%250Amethods.%2520Their%2520practical%2520impact%2520is%2520clear%252C%2520as%2520well%2520as%2520their%2520theoretical%250Aproperties.%2520Stochastic%2520proximal%2520point%2520algorithms%2520have%2520been%2520studied%2520as%2520an%250Aalternative%2520to%2520stochastic%2520gradient%2520algorithms%2520since%2520they%2520are%2520more%2520stable%2520with%250Arespect%2520to%2520the%2520choice%2520of%2520the%2520step%2520size.%2520However%252C%2520their%2520variance-reduced%250Aversions%2520are%2520not%2520as%2520well%2520studied%2520as%2520the%2520gradient%2520ones.%2520In%2520this%2520work%252C%2520we%2520propose%250Athe%2520first%2520unified%2520study%2520of%2520variance%2520reduction%2520techniques%2520for%2520stochastic%250Aproximal%2520point%2520algorithms.%2520We%2520introduce%2520a%2520generic%2520stochastic%2520proximal-based%250Aalgorithm%2520that%2520can%2520be%2520specified%2520to%2520give%2520the%2520proximal%2520version%2520of%2520SVRG%252C%2520SAGA%252C%2520and%250Asome%2520of%2520their%2520variants.%2520For%2520this%2520algorithm%252C%2520in%2520the%2520smooth%2520setting%252C%2520we%2520provide%250Aseveral%2520convergence%2520rates%2520for%2520the%2520iterates%2520and%2520the%2520objective%2520function%2520values%252C%250Awhich%2520are%2520faster%2520than%2520those%2520of%2520the%2520vanilla%2520stochastic%2520proximal%2520point%2520algorithm.%250AMore%2520specifically%252C%2520for%2520convex%2520functions%252C%2520we%2520prove%2520a%2520sublinear%2520convergence%2520rate%250Aof%2520%2524O%25281/k%2529%2524.%2520In%2520addition%252C%2520under%2520the%2520Polyak-%257B%255CL%257Dojasiewicz%2520%2528PL%2529%2520condition%252C%2520we%250Aobtain%2520linear%2520convergence%2520rates.%2520Finally%252C%2520our%2520numerical%2520experiments%2520demonstrate%250Athe%2520advantages%2520of%2520the%2520proximal%2520variance%2520reduction%2520methods%2520over%2520their%2520gradient%250Acounterparts%2520in%2520terms%2520of%2520the%2520stability%2520with%2520respect%2520to%2520the%2520choice%2520of%2520the%2520step%250Asize%2520in%2520most%2520cases%252C%2520especially%2520for%2520difficult%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.09310v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variance%20reduction%20techniques%20for%20stochastic%20proximal%20point%20algorithms&entry.906535625=Cheik%20Traor%C3%A9%20and%20Vassilis%20Apidopoulos%20and%20Saverio%20Salzo%20and%20Silvia%20Villa&entry.1292438233=%20%20In%20the%20context%20of%20finite%20sums%20minimization%2C%20variance%20reduction%20techniques%20are%0Awidely%20used%20to%20improve%20the%20performance%20of%20state-of-the-art%20stochastic%20gradient%0Amethods.%20Their%20practical%20impact%20is%20clear%2C%20as%20well%20as%20their%20theoretical%0Aproperties.%20Stochastic%20proximal%20point%20algorithms%20have%20been%20studied%20as%20an%0Aalternative%20to%20stochastic%20gradient%20algorithms%20since%20they%20are%20more%20stable%20with%0Arespect%20to%20the%20choice%20of%20the%20step%20size.%20However%2C%20their%20variance-reduced%0Aversions%20are%20not%20as%20well%20studied%20as%20the%20gradient%20ones.%20In%20this%20work%2C%20we%20propose%0Athe%20first%20unified%20study%20of%20variance%20reduction%20techniques%20for%20stochastic%0Aproximal%20point%20algorithms.%20We%20introduce%20a%20generic%20stochastic%20proximal-based%0Aalgorithm%20that%20can%20be%20specified%20to%20give%20the%20proximal%20version%20of%20SVRG%2C%20SAGA%2C%20and%0Asome%20of%20their%20variants.%20For%20this%20algorithm%2C%20in%20the%20smooth%20setting%2C%20we%20provide%0Aseveral%20convergence%20rates%20for%20the%20iterates%20and%20the%20objective%20function%20values%2C%0Awhich%20are%20faster%20than%20those%20of%20the%20vanilla%20stochastic%20proximal%20point%20algorithm.%0AMore%20specifically%2C%20for%20convex%20functions%2C%20we%20prove%20a%20sublinear%20convergence%20rate%0Aof%20%24O%281/k%29%24.%20In%20addition%2C%20under%20the%20Polyak-%7B%5CL%7Dojasiewicz%20%28PL%29%20condition%2C%20we%0Aobtain%20linear%20convergence%20rates.%20Finally%2C%20our%20numerical%20experiments%20demonstrate%0Athe%20advantages%20of%20the%20proximal%20variance%20reduction%20methods%20over%20their%20gradient%0Acounterparts%20in%20terms%20of%20the%20stability%20with%20respect%20to%20the%20choice%20of%20the%20step%0Asize%20in%20most%20cases%2C%20especially%20for%20difficult%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09310v3&entry.124074799=Read"},
{"title": "Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized\n  Least-Squares Algorithm", "author": "Zhu Li and Dimitri Meunier and Mattes Mollenhauer and Arthur Gretton", "abstract": "  We present the first optimal rates for infinite-dimensional vector-valued\nridge regression on a continuous scale of norms that interpolate between $L_2$\nand the hypothesis space, which we consider as a vector-valued reproducing\nkernel Hilbert space. These rates allow to treat the misspecified case in which\nthe true regression function is not contained in the hypothesis space. We\ncombine standard assumptions on the capacity of the hypothesis space with a\nnovel tensor product construction of vector-valued interpolation spaces in\norder to characterize the smoothness of the regression function. Our upper\nbound not only attains the same rate as real-valued kernel ridge regression,\nbut also removes the assumption that the target regression function is bounded.\nFor the lower bound, we reduce the problem to the scalar setting using a\nprojection argument. We show that these rates are optimal in most cases and\nindependent of the dimension of the output space. We illustrate our results for\nthe special case of vector-valued Sobolev spaces.\n", "link": "http://arxiv.org/abs/2312.07186v5", "date": "2024-08-06", "relevancy": 1.6803, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4327}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4178}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Optimal%20Sobolev%20Norm%20Rates%20for%20the%20Vector-Valued%20Regularized%0A%20%20Least-Squares%20Algorithm&body=Title%3A%20Towards%20Optimal%20Sobolev%20Norm%20Rates%20for%20the%20Vector-Valued%20Regularized%0A%20%20Least-Squares%20Algorithm%0AAuthor%3A%20Zhu%20Li%20and%20Dimitri%20Meunier%20and%20Mattes%20Mollenhauer%20and%20Arthur%20Gretton%0AAbstract%3A%20%20%20We%20present%20the%20first%20optimal%20rates%20for%20infinite-dimensional%20vector-valued%0Aridge%20regression%20on%20a%20continuous%20scale%20of%20norms%20that%20interpolate%20between%20%24L_2%24%0Aand%20the%20hypothesis%20space%2C%20which%20we%20consider%20as%20a%20vector-valued%20reproducing%0Akernel%20Hilbert%20space.%20These%20rates%20allow%20to%20treat%20the%20misspecified%20case%20in%20which%0Athe%20true%20regression%20function%20is%20not%20contained%20in%20the%20hypothesis%20space.%20We%0Acombine%20standard%20assumptions%20on%20the%20capacity%20of%20the%20hypothesis%20space%20with%20a%0Anovel%20tensor%20product%20construction%20of%20vector-valued%20interpolation%20spaces%20in%0Aorder%20to%20characterize%20the%20smoothness%20of%20the%20regression%20function.%20Our%20upper%0Abound%20not%20only%20attains%20the%20same%20rate%20as%20real-valued%20kernel%20ridge%20regression%2C%0Abut%20also%20removes%20the%20assumption%20that%20the%20target%20regression%20function%20is%20bounded.%0AFor%20the%20lower%20bound%2C%20we%20reduce%20the%20problem%20to%20the%20scalar%20setting%20using%20a%0Aprojection%20argument.%20We%20show%20that%20these%20rates%20are%20optimal%20in%20most%20cases%20and%0Aindependent%20of%20the%20dimension%20of%20the%20output%20space.%20We%20illustrate%20our%20results%20for%0Athe%20special%20case%20of%20vector-valued%20Sobolev%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07186v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Optimal%2520Sobolev%2520Norm%2520Rates%2520for%2520the%2520Vector-Valued%2520Regularized%250A%2520%2520Least-Squares%2520Algorithm%26entry.906535625%3DZhu%2520Li%2520and%2520Dimitri%2520Meunier%2520and%2520Mattes%2520Mollenhauer%2520and%2520Arthur%2520Gretton%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520optimal%2520rates%2520for%2520infinite-dimensional%2520vector-valued%250Aridge%2520regression%2520on%2520a%2520continuous%2520scale%2520of%2520norms%2520that%2520interpolate%2520between%2520%2524L_2%2524%250Aand%2520the%2520hypothesis%2520space%252C%2520which%2520we%2520consider%2520as%2520a%2520vector-valued%2520reproducing%250Akernel%2520Hilbert%2520space.%2520These%2520rates%2520allow%2520to%2520treat%2520the%2520misspecified%2520case%2520in%2520which%250Athe%2520true%2520regression%2520function%2520is%2520not%2520contained%2520in%2520the%2520hypothesis%2520space.%2520We%250Acombine%2520standard%2520assumptions%2520on%2520the%2520capacity%2520of%2520the%2520hypothesis%2520space%2520with%2520a%250Anovel%2520tensor%2520product%2520construction%2520of%2520vector-valued%2520interpolation%2520spaces%2520in%250Aorder%2520to%2520characterize%2520the%2520smoothness%2520of%2520the%2520regression%2520function.%2520Our%2520upper%250Abound%2520not%2520only%2520attains%2520the%2520same%2520rate%2520as%2520real-valued%2520kernel%2520ridge%2520regression%252C%250Abut%2520also%2520removes%2520the%2520assumption%2520that%2520the%2520target%2520regression%2520function%2520is%2520bounded.%250AFor%2520the%2520lower%2520bound%252C%2520we%2520reduce%2520the%2520problem%2520to%2520the%2520scalar%2520setting%2520using%2520a%250Aprojection%2520argument.%2520We%2520show%2520that%2520these%2520rates%2520are%2520optimal%2520in%2520most%2520cases%2520and%250Aindependent%2520of%2520the%2520dimension%2520of%2520the%2520output%2520space.%2520We%2520illustrate%2520our%2520results%2520for%250Athe%2520special%2520case%2520of%2520vector-valued%2520Sobolev%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07186v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Optimal%20Sobolev%20Norm%20Rates%20for%20the%20Vector-Valued%20Regularized%0A%20%20Least-Squares%20Algorithm&entry.906535625=Zhu%20Li%20and%20Dimitri%20Meunier%20and%20Mattes%20Mollenhauer%20and%20Arthur%20Gretton&entry.1292438233=%20%20We%20present%20the%20first%20optimal%20rates%20for%20infinite-dimensional%20vector-valued%0Aridge%20regression%20on%20a%20continuous%20scale%20of%20norms%20that%20interpolate%20between%20%24L_2%24%0Aand%20the%20hypothesis%20space%2C%20which%20we%20consider%20as%20a%20vector-valued%20reproducing%0Akernel%20Hilbert%20space.%20These%20rates%20allow%20to%20treat%20the%20misspecified%20case%20in%20which%0Athe%20true%20regression%20function%20is%20not%20contained%20in%20the%20hypothesis%20space.%20We%0Acombine%20standard%20assumptions%20on%20the%20capacity%20of%20the%20hypothesis%20space%20with%20a%0Anovel%20tensor%20product%20construction%20of%20vector-valued%20interpolation%20spaces%20in%0Aorder%20to%20characterize%20the%20smoothness%20of%20the%20regression%20function.%20Our%20upper%0Abound%20not%20only%20attains%20the%20same%20rate%20as%20real-valued%20kernel%20ridge%20regression%2C%0Abut%20also%20removes%20the%20assumption%20that%20the%20target%20regression%20function%20is%20bounded.%0AFor%20the%20lower%20bound%2C%20we%20reduce%20the%20problem%20to%20the%20scalar%20setting%20using%20a%0Aprojection%20argument.%20We%20show%20that%20these%20rates%20are%20optimal%20in%20most%20cases%20and%0Aindependent%20of%20the%20dimension%20of%20the%20output%20space.%20We%20illustrate%20our%20results%20for%0Athe%20special%20case%20of%20vector-valued%20Sobolev%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07186v5&entry.124074799=Read"},
{"title": "Training Compute Thresholds: Features and Functions in AI Regulation", "author": "Lennart Heim and Leonie Koessler", "abstract": "  Regulators in the US and EU are using thresholds based on training\ncompute--the number of computational operations used in training--to identify\ngeneral-purpose artificial intelligence (GPAI) models that may pose risks of\nlarge-scale societal harm. We argue that training compute currently is the most\nsuitable metric to identify GPAI models that deserve regulatory oversight and\nfurther scrutiny. Training compute correlates with model capabilities and\nrisks, is quantifiable, can be measured early in the AI lifecycle, and can be\nverified by external actors, among other advantageous features. These features\nmake compute thresholds considerably more suitable than other proposed metrics\nto serve as an initial filter to trigger additional regulatory requirements and\nscrutiny. However, training compute is an imperfect proxy for risk. As such,\ncompute thresholds should not be used in isolation to determine appropriate\nmitigation measures. Instead, they should be used to detect potentially risky\nGPAI models that warrant regulatory oversight, such as through notification\nrequirements, and further scrutiny, such as via model evaluations and risk\nassessments, the results of which may inform which mitigation measures are\nappropriate. In fact, this appears largely consistent with how compute\nthresholds are used today. As GPAI technology and market structures evolve,\nregulators should update compute thresholds and complement them with other\nmetrics into regulatory review processes.\n", "link": "http://arxiv.org/abs/2405.10799v2", "date": "2024-08-06", "relevancy": 1.6795, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4434}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4192}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Compute%20Thresholds%3A%20Features%20and%20Functions%20in%20AI%20Regulation&body=Title%3A%20Training%20Compute%20Thresholds%3A%20Features%20and%20Functions%20in%20AI%20Regulation%0AAuthor%3A%20Lennart%20Heim%20and%20Leonie%20Koessler%0AAbstract%3A%20%20%20Regulators%20in%20the%20US%20and%20EU%20are%20using%20thresholds%20based%20on%20training%0Acompute--the%20number%20of%20computational%20operations%20used%20in%20training--to%20identify%0Ageneral-purpose%20artificial%20intelligence%20%28GPAI%29%20models%20that%20may%20pose%20risks%20of%0Alarge-scale%20societal%20harm.%20We%20argue%20that%20training%20compute%20currently%20is%20the%20most%0Asuitable%20metric%20to%20identify%20GPAI%20models%20that%20deserve%20regulatory%20oversight%20and%0Afurther%20scrutiny.%20Training%20compute%20correlates%20with%20model%20capabilities%20and%0Arisks%2C%20is%20quantifiable%2C%20can%20be%20measured%20early%20in%20the%20AI%20lifecycle%2C%20and%20can%20be%0Averified%20by%20external%20actors%2C%20among%20other%20advantageous%20features.%20These%20features%0Amake%20compute%20thresholds%20considerably%20more%20suitable%20than%20other%20proposed%20metrics%0Ato%20serve%20as%20an%20initial%20filter%20to%20trigger%20additional%20regulatory%20requirements%20and%0Ascrutiny.%20However%2C%20training%20compute%20is%20an%20imperfect%20proxy%20for%20risk.%20As%20such%2C%0Acompute%20thresholds%20should%20not%20be%20used%20in%20isolation%20to%20determine%20appropriate%0Amitigation%20measures.%20Instead%2C%20they%20should%20be%20used%20to%20detect%20potentially%20risky%0AGPAI%20models%20that%20warrant%20regulatory%20oversight%2C%20such%20as%20through%20notification%0Arequirements%2C%20and%20further%20scrutiny%2C%20such%20as%20via%20model%20evaluations%20and%20risk%0Aassessments%2C%20the%20results%20of%20which%20may%20inform%20which%20mitigation%20measures%20are%0Aappropriate.%20In%20fact%2C%20this%20appears%20largely%20consistent%20with%20how%20compute%0Athresholds%20are%20used%20today.%20As%20GPAI%20technology%20and%20market%20structures%20evolve%2C%0Aregulators%20should%20update%20compute%20thresholds%20and%20complement%20them%20with%20other%0Ametrics%20into%20regulatory%20review%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10799v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Compute%2520Thresholds%253A%2520Features%2520and%2520Functions%2520in%2520AI%2520Regulation%26entry.906535625%3DLennart%2520Heim%2520and%2520Leonie%2520Koessler%26entry.1292438233%3D%2520%2520Regulators%2520in%2520the%2520US%2520and%2520EU%2520are%2520using%2520thresholds%2520based%2520on%2520training%250Acompute--the%2520number%2520of%2520computational%2520operations%2520used%2520in%2520training--to%2520identify%250Ageneral-purpose%2520artificial%2520intelligence%2520%2528GPAI%2529%2520models%2520that%2520may%2520pose%2520risks%2520of%250Alarge-scale%2520societal%2520harm.%2520We%2520argue%2520that%2520training%2520compute%2520currently%2520is%2520the%2520most%250Asuitable%2520metric%2520to%2520identify%2520GPAI%2520models%2520that%2520deserve%2520regulatory%2520oversight%2520and%250Afurther%2520scrutiny.%2520Training%2520compute%2520correlates%2520with%2520model%2520capabilities%2520and%250Arisks%252C%2520is%2520quantifiable%252C%2520can%2520be%2520measured%2520early%2520in%2520the%2520AI%2520lifecycle%252C%2520and%2520can%2520be%250Averified%2520by%2520external%2520actors%252C%2520among%2520other%2520advantageous%2520features.%2520These%2520features%250Amake%2520compute%2520thresholds%2520considerably%2520more%2520suitable%2520than%2520other%2520proposed%2520metrics%250Ato%2520serve%2520as%2520an%2520initial%2520filter%2520to%2520trigger%2520additional%2520regulatory%2520requirements%2520and%250Ascrutiny.%2520However%252C%2520training%2520compute%2520is%2520an%2520imperfect%2520proxy%2520for%2520risk.%2520As%2520such%252C%250Acompute%2520thresholds%2520should%2520not%2520be%2520used%2520in%2520isolation%2520to%2520determine%2520appropriate%250Amitigation%2520measures.%2520Instead%252C%2520they%2520should%2520be%2520used%2520to%2520detect%2520potentially%2520risky%250AGPAI%2520models%2520that%2520warrant%2520regulatory%2520oversight%252C%2520such%2520as%2520through%2520notification%250Arequirements%252C%2520and%2520further%2520scrutiny%252C%2520such%2520as%2520via%2520model%2520evaluations%2520and%2520risk%250Aassessments%252C%2520the%2520results%2520of%2520which%2520may%2520inform%2520which%2520mitigation%2520measures%2520are%250Aappropriate.%2520In%2520fact%252C%2520this%2520appears%2520largely%2520consistent%2520with%2520how%2520compute%250Athresholds%2520are%2520used%2520today.%2520As%2520GPAI%2520technology%2520and%2520market%2520structures%2520evolve%252C%250Aregulators%2520should%2520update%2520compute%2520thresholds%2520and%2520complement%2520them%2520with%2520other%250Ametrics%2520into%2520regulatory%2520review%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10799v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Compute%20Thresholds%3A%20Features%20and%20Functions%20in%20AI%20Regulation&entry.906535625=Lennart%20Heim%20and%20Leonie%20Koessler&entry.1292438233=%20%20Regulators%20in%20the%20US%20and%20EU%20are%20using%20thresholds%20based%20on%20training%0Acompute--the%20number%20of%20computational%20operations%20used%20in%20training--to%20identify%0Ageneral-purpose%20artificial%20intelligence%20%28GPAI%29%20models%20that%20may%20pose%20risks%20of%0Alarge-scale%20societal%20harm.%20We%20argue%20that%20training%20compute%20currently%20is%20the%20most%0Asuitable%20metric%20to%20identify%20GPAI%20models%20that%20deserve%20regulatory%20oversight%20and%0Afurther%20scrutiny.%20Training%20compute%20correlates%20with%20model%20capabilities%20and%0Arisks%2C%20is%20quantifiable%2C%20can%20be%20measured%20early%20in%20the%20AI%20lifecycle%2C%20and%20can%20be%0Averified%20by%20external%20actors%2C%20among%20other%20advantageous%20features.%20These%20features%0Amake%20compute%20thresholds%20considerably%20more%20suitable%20than%20other%20proposed%20metrics%0Ato%20serve%20as%20an%20initial%20filter%20to%20trigger%20additional%20regulatory%20requirements%20and%0Ascrutiny.%20However%2C%20training%20compute%20is%20an%20imperfect%20proxy%20for%20risk.%20As%20such%2C%0Acompute%20thresholds%20should%20not%20be%20used%20in%20isolation%20to%20determine%20appropriate%0Amitigation%20measures.%20Instead%2C%20they%20should%20be%20used%20to%20detect%20potentially%20risky%0AGPAI%20models%20that%20warrant%20regulatory%20oversight%2C%20such%20as%20through%20notification%0Arequirements%2C%20and%20further%20scrutiny%2C%20such%20as%20via%20model%20evaluations%20and%20risk%0Aassessments%2C%20the%20results%20of%20which%20may%20inform%20which%20mitigation%20measures%20are%0Aappropriate.%20In%20fact%2C%20this%20appears%20largely%20consistent%20with%20how%20compute%0Athresholds%20are%20used%20today.%20As%20GPAI%20technology%20and%20market%20structures%20evolve%2C%0Aregulators%20should%20update%20compute%20thresholds%20and%20complement%20them%20with%20other%0Ametrics%20into%20regulatory%20review%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10799v2&entry.124074799=Read"},
{"title": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware\n  Experts", "author": "Xi Victoria Lin and Akshat Shrivastava and Liang Luo and Srinivasan Iyer and Mike Lewis and Gargi Gosh and Luke Zettlemoyer and Armen Aghajanyan", "abstract": "  We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems.\n", "link": "http://arxiv.org/abs/2407.21770v2", "date": "2024-08-06", "relevancy": 1.6775, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.583}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5409}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoMa%3A%20Efficient%20Early-Fusion%20Pre-training%20with%20Mixture%20of%20Modality-Aware%0A%20%20Experts&body=Title%3A%20MoMa%3A%20Efficient%20Early-Fusion%20Pre-training%20with%20Mixture%20of%20Modality-Aware%0A%20%20Experts%0AAuthor%3A%20Xi%20Victoria%20Lin%20and%20Akshat%20Shrivastava%20and%20Liang%20Luo%20and%20Srinivasan%20Iyer%20and%20Mike%20Lewis%20and%20Gargi%20Gosh%20and%20Luke%20Zettlemoyer%20and%20Armen%20Aghajanyan%0AAbstract%3A%20%20%20We%20introduce%20MoMa%2C%20a%20novel%20modality-aware%20mixture-of-experts%20%28MoE%29%0Aarchitecture%20designed%20for%20pre-training%20mixed-modal%2C%20early-fusion%20language%0Amodels.%20MoMa%20processes%20images%20and%20text%20in%20arbitrary%20sequences%20by%20dividing%0Aexpert%20modules%20into%20modality-specific%20groups.%20These%20groups%20exclusively%20process%0Adesignated%20tokens%20while%20employing%20learned%20routing%20within%20each%20group%20to%20maintain%0Asemantically%20informed%20adaptivity.%20Our%20empirical%20results%20reveal%20substantial%0Apre-training%20efficiency%20gains%20through%20this%20modality-specific%20parameter%0Aallocation.%20Under%20a%201-trillion-token%20training%20budget%2C%20the%20MoMa%201.4B%20model%2C%0Afeaturing%204%20text%20experts%20and%204%20image%20experts%2C%20achieves%20impressive%20FLOPs%0Asavings%3A%203.7x%20overall%2C%20with%202.6x%20for%20text%20and%205.2x%20for%20image%20processing%0Acompared%20to%20a%20compute-equivalent%20dense%20baseline%2C%20measured%20by%20pre-training%20loss.%0AThis%20outperforms%20the%20standard%20expert-choice%20MoE%20with%208%20mixed-modal%20experts%2C%0Awhich%20achieves%203x%20overall%20FLOPs%20savings%20%283x%20for%20text%2C%202.8x%20for%20image%29.%0ACombining%20MoMa%20with%20mixture-of-depths%20%28MoD%29%20further%20improves%20pre-training%20FLOPs%0Asavings%20to%204.2x%20overall%20%28text%3A%203.4x%2C%20image%3A%205.3x%29%2C%20although%20this%20combination%0Ahurts%20performance%20in%20causal%20inference%20due%20to%20increased%20sensitivity%20to%20router%0Aaccuracy.%20These%20results%20demonstrate%20MoMa%27s%20potential%20to%20significantly%20advance%0Athe%20efficiency%20of%20mixed-modal%2C%20early-fusion%20language%20model%20pre-training%2C%20paving%0Athe%20way%20for%20more%20resource-efficient%20and%20capable%20multimodal%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21770v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoMa%253A%2520Efficient%2520Early-Fusion%2520Pre-training%2520with%2520Mixture%2520of%2520Modality-Aware%250A%2520%2520Experts%26entry.906535625%3DXi%2520Victoria%2520Lin%2520and%2520Akshat%2520Shrivastava%2520and%2520Liang%2520Luo%2520and%2520Srinivasan%2520Iyer%2520and%2520Mike%2520Lewis%2520and%2520Gargi%2520Gosh%2520and%2520Luke%2520Zettlemoyer%2520and%2520Armen%2520Aghajanyan%26entry.1292438233%3D%2520%2520We%2520introduce%2520MoMa%252C%2520a%2520novel%2520modality-aware%2520mixture-of-experts%2520%2528MoE%2529%250Aarchitecture%2520designed%2520for%2520pre-training%2520mixed-modal%252C%2520early-fusion%2520language%250Amodels.%2520MoMa%2520processes%2520images%2520and%2520text%2520in%2520arbitrary%2520sequences%2520by%2520dividing%250Aexpert%2520modules%2520into%2520modality-specific%2520groups.%2520These%2520groups%2520exclusively%2520process%250Adesignated%2520tokens%2520while%2520employing%2520learned%2520routing%2520within%2520each%2520group%2520to%2520maintain%250Asemantically%2520informed%2520adaptivity.%2520Our%2520empirical%2520results%2520reveal%2520substantial%250Apre-training%2520efficiency%2520gains%2520through%2520this%2520modality-specific%2520parameter%250Aallocation.%2520Under%2520a%25201-trillion-token%2520training%2520budget%252C%2520the%2520MoMa%25201.4B%2520model%252C%250Afeaturing%25204%2520text%2520experts%2520and%25204%2520image%2520experts%252C%2520achieves%2520impressive%2520FLOPs%250Asavings%253A%25203.7x%2520overall%252C%2520with%25202.6x%2520for%2520text%2520and%25205.2x%2520for%2520image%2520processing%250Acompared%2520to%2520a%2520compute-equivalent%2520dense%2520baseline%252C%2520measured%2520by%2520pre-training%2520loss.%250AThis%2520outperforms%2520the%2520standard%2520expert-choice%2520MoE%2520with%25208%2520mixed-modal%2520experts%252C%250Awhich%2520achieves%25203x%2520overall%2520FLOPs%2520savings%2520%25283x%2520for%2520text%252C%25202.8x%2520for%2520image%2529.%250ACombining%2520MoMa%2520with%2520mixture-of-depths%2520%2528MoD%2529%2520further%2520improves%2520pre-training%2520FLOPs%250Asavings%2520to%25204.2x%2520overall%2520%2528text%253A%25203.4x%252C%2520image%253A%25205.3x%2529%252C%2520although%2520this%2520combination%250Ahurts%2520performance%2520in%2520causal%2520inference%2520due%2520to%2520increased%2520sensitivity%2520to%2520router%250Aaccuracy.%2520These%2520results%2520demonstrate%2520MoMa%2527s%2520potential%2520to%2520significantly%2520advance%250Athe%2520efficiency%2520of%2520mixed-modal%252C%2520early-fusion%2520language%2520model%2520pre-training%252C%2520paving%250Athe%2520way%2520for%2520more%2520resource-efficient%2520and%2520capable%2520multimodal%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21770v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoMa%3A%20Efficient%20Early-Fusion%20Pre-training%20with%20Mixture%20of%20Modality-Aware%0A%20%20Experts&entry.906535625=Xi%20Victoria%20Lin%20and%20Akshat%20Shrivastava%20and%20Liang%20Luo%20and%20Srinivasan%20Iyer%20and%20Mike%20Lewis%20and%20Gargi%20Gosh%20and%20Luke%20Zettlemoyer%20and%20Armen%20Aghajanyan&entry.1292438233=%20%20We%20introduce%20MoMa%2C%20a%20novel%20modality-aware%20mixture-of-experts%20%28MoE%29%0Aarchitecture%20designed%20for%20pre-training%20mixed-modal%2C%20early-fusion%20language%0Amodels.%20MoMa%20processes%20images%20and%20text%20in%20arbitrary%20sequences%20by%20dividing%0Aexpert%20modules%20into%20modality-specific%20groups.%20These%20groups%20exclusively%20process%0Adesignated%20tokens%20while%20employing%20learned%20routing%20within%20each%20group%20to%20maintain%0Asemantically%20informed%20adaptivity.%20Our%20empirical%20results%20reveal%20substantial%0Apre-training%20efficiency%20gains%20through%20this%20modality-specific%20parameter%0Aallocation.%20Under%20a%201-trillion-token%20training%20budget%2C%20the%20MoMa%201.4B%20model%2C%0Afeaturing%204%20text%20experts%20and%204%20image%20experts%2C%20achieves%20impressive%20FLOPs%0Asavings%3A%203.7x%20overall%2C%20with%202.6x%20for%20text%20and%205.2x%20for%20image%20processing%0Acompared%20to%20a%20compute-equivalent%20dense%20baseline%2C%20measured%20by%20pre-training%20loss.%0AThis%20outperforms%20the%20standard%20expert-choice%20MoE%20with%208%20mixed-modal%20experts%2C%0Awhich%20achieves%203x%20overall%20FLOPs%20savings%20%283x%20for%20text%2C%202.8x%20for%20image%29.%0ACombining%20MoMa%20with%20mixture-of-depths%20%28MoD%29%20further%20improves%20pre-training%20FLOPs%0Asavings%20to%204.2x%20overall%20%28text%3A%203.4x%2C%20image%3A%205.3x%29%2C%20although%20this%20combination%0Ahurts%20performance%20in%20causal%20inference%20due%20to%20increased%20sensitivity%20to%20router%0Aaccuracy.%20These%20results%20demonstrate%20MoMa%27s%20potential%20to%20significantly%20advance%0Athe%20efficiency%20of%20mixed-modal%2C%20early-fusion%20language%20model%20pre-training%2C%20paving%0Athe%20way%20for%20more%20resource-efficient%20and%20capable%20multimodal%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21770v2&entry.124074799=Read"},
{"title": "LLaVA-OneVision: Easy Visual Task Transfer", "author": "Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li", "abstract": "  We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.\n", "link": "http://arxiv.org/abs/2408.03326v1", "date": "2024-08-06", "relevancy": 1.6668, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5711}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.542}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA-OneVision%3A%20Easy%20Visual%20Task%20Transfer&body=Title%3A%20LLaVA-OneVision%3A%20Easy%20Visual%20Task%20Transfer%0AAuthor%3A%20Bo%20Li%20and%20Yuanhan%20Zhang%20and%20Dong%20Guo%20and%20Renrui%20Zhang%20and%20Feng%20Li%20and%20Hao%20Zhang%20and%20Kaichen%20Zhang%20and%20Yanwei%20Li%20and%20Ziwei%20Liu%20and%20Chunyuan%20Li%0AAbstract%3A%20%20%20We%20present%20LLaVA-OneVision%2C%20a%20family%20of%20open%20large%20multimodal%20models%20%28LMMs%29%0Adeveloped%20by%20consolidating%20our%20insights%20into%20data%2C%20models%2C%20and%20visual%0Arepresentations%20in%20the%20LLaVA-NeXT%20blog%20series.%20Our%20experimental%20results%0Ademonstrate%20that%20LLaVA-OneVision%20is%20the%20first%20single%20model%20that%20can%0Asimultaneously%20push%20the%20performance%20boundaries%20of%20open%20LMMs%20in%20three%20important%0Acomputer%20vision%20scenarios%3A%20single-image%2C%20multi-image%2C%20and%20video%20scenarios.%0AImportantly%2C%20the%20design%20of%20LLaVA-OneVision%20allows%20strong%20transfer%20learning%0Aacross%20different%20modalities/scenarios%2C%20yielding%20new%20emerging%20capabilities.%20In%0Aparticular%2C%20strong%20video%20understanding%20and%20cross-scenario%20capabilities%20are%0Ademonstrated%20through%20task%20transfer%20from%20images%20to%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA-OneVision%253A%2520Easy%2520Visual%2520Task%2520Transfer%26entry.906535625%3DBo%2520Li%2520and%2520Yuanhan%2520Zhang%2520and%2520Dong%2520Guo%2520and%2520Renrui%2520Zhang%2520and%2520Feng%2520Li%2520and%2520Hao%2520Zhang%2520and%2520Kaichen%2520Zhang%2520and%2520Yanwei%2520Li%2520and%2520Ziwei%2520Liu%2520and%2520Chunyuan%2520Li%26entry.1292438233%3D%2520%2520We%2520present%2520LLaVA-OneVision%252C%2520a%2520family%2520of%2520open%2520large%2520multimodal%2520models%2520%2528LMMs%2529%250Adeveloped%2520by%2520consolidating%2520our%2520insights%2520into%2520data%252C%2520models%252C%2520and%2520visual%250Arepresentations%2520in%2520the%2520LLaVA-NeXT%2520blog%2520series.%2520Our%2520experimental%2520results%250Ademonstrate%2520that%2520LLaVA-OneVision%2520is%2520the%2520first%2520single%2520model%2520that%2520can%250Asimultaneously%2520push%2520the%2520performance%2520boundaries%2520of%2520open%2520LMMs%2520in%2520three%2520important%250Acomputer%2520vision%2520scenarios%253A%2520single-image%252C%2520multi-image%252C%2520and%2520video%2520scenarios.%250AImportantly%252C%2520the%2520design%2520of%2520LLaVA-OneVision%2520allows%2520strong%2520transfer%2520learning%250Aacross%2520different%2520modalities/scenarios%252C%2520yielding%2520new%2520emerging%2520capabilities.%2520In%250Aparticular%252C%2520strong%2520video%2520understanding%2520and%2520cross-scenario%2520capabilities%2520are%250Ademonstrated%2520through%2520task%2520transfer%2520from%2520images%2520to%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-OneVision%3A%20Easy%20Visual%20Task%20Transfer&entry.906535625=Bo%20Li%20and%20Yuanhan%20Zhang%20and%20Dong%20Guo%20and%20Renrui%20Zhang%20and%20Feng%20Li%20and%20Hao%20Zhang%20and%20Kaichen%20Zhang%20and%20Yanwei%20Li%20and%20Ziwei%20Liu%20and%20Chunyuan%20Li&entry.1292438233=%20%20We%20present%20LLaVA-OneVision%2C%20a%20family%20of%20open%20large%20multimodal%20models%20%28LMMs%29%0Adeveloped%20by%20consolidating%20our%20insights%20into%20data%2C%20models%2C%20and%20visual%0Arepresentations%20in%20the%20LLaVA-NeXT%20blog%20series.%20Our%20experimental%20results%0Ademonstrate%20that%20LLaVA-OneVision%20is%20the%20first%20single%20model%20that%20can%0Asimultaneously%20push%20the%20performance%20boundaries%20of%20open%20LMMs%20in%20three%20important%0Acomputer%20vision%20scenarios%3A%20single-image%2C%20multi-image%2C%20and%20video%20scenarios.%0AImportantly%2C%20the%20design%20of%20LLaVA-OneVision%20allows%20strong%20transfer%20learning%0Aacross%20different%20modalities/scenarios%2C%20yielding%20new%20emerging%20capabilities.%20In%0Aparticular%2C%20strong%20video%20understanding%20and%20cross-scenario%20capabilities%20are%0Ademonstrated%20through%20task%20transfer%20from%20images%20to%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03326v1&entry.124074799=Read"},
{"title": "Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity\n  with Free-Flying Robots", "author": "Holly Dinkel and Julia Di and Jamie Santos and Keenan Albee and Paulo Borges and Marina Moreira and Oleg Alexandrov and Brian Coltin and Trey Smith", "abstract": "  Assistive free-flyer robots autonomously caring for future crewed outposts --\nsuch as NASA's Astrobee robots on the International Space Station (ISS) -- must\nbe able to detect day-to-day interior changes to track inventory, detect and\ndiagnose faults, and monitor the outpost status. This work presents a framework\nfor multi-agent cooperative mapping and change detection to enable robotic\nmaintenance of space outposts. One agent is used to reconstruct a 3D model of\nthe environment from sequences of images and corresponding depth information.\nAnother agent is used to periodically scan the environment for inconsistencies\nagainst the 3D model. Change detection is validated after completing the\nsurveys using real image and pose data collected by Astrobee robots in a ground\ntesting environment and from microgravity aboard the ISS. This work outlines\nthe objectives, requirements, and algorithmic modules for the multi-agent\nreconstruction system, including recommendations for its use by assistive\nfree-flyers aboard future microgravity outposts.\n  *Denotes Equal Contribution\n", "link": "http://arxiv.org/abs/2311.02558v3", "date": "2024-08-06", "relevancy": 1.6459, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5897}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5827}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%203D%20Map%20Reconstruction%20and%20Change%20Detection%20in%20Microgravity%0A%20%20with%20Free-Flying%20Robots&body=Title%3A%20Multi-Agent%203D%20Map%20Reconstruction%20and%20Change%20Detection%20in%20Microgravity%0A%20%20with%20Free-Flying%20Robots%0AAuthor%3A%20Holly%20Dinkel%20and%20Julia%20Di%20and%20Jamie%20Santos%20and%20Keenan%20Albee%20and%20Paulo%20Borges%20and%20Marina%20Moreira%20and%20Oleg%20Alexandrov%20and%20Brian%20Coltin%20and%20Trey%20Smith%0AAbstract%3A%20%20%20Assistive%20free-flyer%20robots%20autonomously%20caring%20for%20future%20crewed%20outposts%20--%0Asuch%20as%20NASA%27s%20Astrobee%20robots%20on%20the%20International%20Space%20Station%20%28ISS%29%20--%20must%0Abe%20able%20to%20detect%20day-to-day%20interior%20changes%20to%20track%20inventory%2C%20detect%20and%0Adiagnose%20faults%2C%20and%20monitor%20the%20outpost%20status.%20This%20work%20presents%20a%20framework%0Afor%20multi-agent%20cooperative%20mapping%20and%20change%20detection%20to%20enable%20robotic%0Amaintenance%20of%20space%20outposts.%20One%20agent%20is%20used%20to%20reconstruct%20a%203D%20model%20of%0Athe%20environment%20from%20sequences%20of%20images%20and%20corresponding%20depth%20information.%0AAnother%20agent%20is%20used%20to%20periodically%20scan%20the%20environment%20for%20inconsistencies%0Aagainst%20the%203D%20model.%20Change%20detection%20is%20validated%20after%20completing%20the%0Asurveys%20using%20real%20image%20and%20pose%20data%20collected%20by%20Astrobee%20robots%20in%20a%20ground%0Atesting%20environment%20and%20from%20microgravity%20aboard%20the%20ISS.%20This%20work%20outlines%0Athe%20objectives%2C%20requirements%2C%20and%20algorithmic%20modules%20for%20the%20multi-agent%0Areconstruction%20system%2C%20including%20recommendations%20for%20its%20use%20by%20assistive%0Afree-flyers%20aboard%20future%20microgravity%20outposts.%0A%20%20%2ADenotes%20Equal%20Contribution%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02558v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%25203D%2520Map%2520Reconstruction%2520and%2520Change%2520Detection%2520in%2520Microgravity%250A%2520%2520with%2520Free-Flying%2520Robots%26entry.906535625%3DHolly%2520Dinkel%2520and%2520Julia%2520Di%2520and%2520Jamie%2520Santos%2520and%2520Keenan%2520Albee%2520and%2520Paulo%2520Borges%2520and%2520Marina%2520Moreira%2520and%2520Oleg%2520Alexandrov%2520and%2520Brian%2520Coltin%2520and%2520Trey%2520Smith%26entry.1292438233%3D%2520%2520Assistive%2520free-flyer%2520robots%2520autonomously%2520caring%2520for%2520future%2520crewed%2520outposts%2520--%250Asuch%2520as%2520NASA%2527s%2520Astrobee%2520robots%2520on%2520the%2520International%2520Space%2520Station%2520%2528ISS%2529%2520--%2520must%250Abe%2520able%2520to%2520detect%2520day-to-day%2520interior%2520changes%2520to%2520track%2520inventory%252C%2520detect%2520and%250Adiagnose%2520faults%252C%2520and%2520monitor%2520the%2520outpost%2520status.%2520This%2520work%2520presents%2520a%2520framework%250Afor%2520multi-agent%2520cooperative%2520mapping%2520and%2520change%2520detection%2520to%2520enable%2520robotic%250Amaintenance%2520of%2520space%2520outposts.%2520One%2520agent%2520is%2520used%2520to%2520reconstruct%2520a%25203D%2520model%2520of%250Athe%2520environment%2520from%2520sequences%2520of%2520images%2520and%2520corresponding%2520depth%2520information.%250AAnother%2520agent%2520is%2520used%2520to%2520periodically%2520scan%2520the%2520environment%2520for%2520inconsistencies%250Aagainst%2520the%25203D%2520model.%2520Change%2520detection%2520is%2520validated%2520after%2520completing%2520the%250Asurveys%2520using%2520real%2520image%2520and%2520pose%2520data%2520collected%2520by%2520Astrobee%2520robots%2520in%2520a%2520ground%250Atesting%2520environment%2520and%2520from%2520microgravity%2520aboard%2520the%2520ISS.%2520This%2520work%2520outlines%250Athe%2520objectives%252C%2520requirements%252C%2520and%2520algorithmic%2520modules%2520for%2520the%2520multi-agent%250Areconstruction%2520system%252C%2520including%2520recommendations%2520for%2520its%2520use%2520by%2520assistive%250Afree-flyers%2520aboard%2520future%2520microgravity%2520outposts.%250A%2520%2520%252ADenotes%2520Equal%2520Contribution%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.02558v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%203D%20Map%20Reconstruction%20and%20Change%20Detection%20in%20Microgravity%0A%20%20with%20Free-Flying%20Robots&entry.906535625=Holly%20Dinkel%20and%20Julia%20Di%20and%20Jamie%20Santos%20and%20Keenan%20Albee%20and%20Paulo%20Borges%20and%20Marina%20Moreira%20and%20Oleg%20Alexandrov%20and%20Brian%20Coltin%20and%20Trey%20Smith&entry.1292438233=%20%20Assistive%20free-flyer%20robots%20autonomously%20caring%20for%20future%20crewed%20outposts%20--%0Asuch%20as%20NASA%27s%20Astrobee%20robots%20on%20the%20International%20Space%20Station%20%28ISS%29%20--%20must%0Abe%20able%20to%20detect%20day-to-day%20interior%20changes%20to%20track%20inventory%2C%20detect%20and%0Adiagnose%20faults%2C%20and%20monitor%20the%20outpost%20status.%20This%20work%20presents%20a%20framework%0Afor%20multi-agent%20cooperative%20mapping%20and%20change%20detection%20to%20enable%20robotic%0Amaintenance%20of%20space%20outposts.%20One%20agent%20is%20used%20to%20reconstruct%20a%203D%20model%20of%0Athe%20environment%20from%20sequences%20of%20images%20and%20corresponding%20depth%20information.%0AAnother%20agent%20is%20used%20to%20periodically%20scan%20the%20environment%20for%20inconsistencies%0Aagainst%20the%203D%20model.%20Change%20detection%20is%20validated%20after%20completing%20the%0Asurveys%20using%20real%20image%20and%20pose%20data%20collected%20by%20Astrobee%20robots%20in%20a%20ground%0Atesting%20environment%20and%20from%20microgravity%20aboard%20the%20ISS.%20This%20work%20outlines%0Athe%20objectives%2C%20requirements%2C%20and%20algorithmic%20modules%20for%20the%20multi-agent%0Areconstruction%20system%2C%20including%20recommendations%20for%20its%20use%20by%20assistive%0Afree-flyers%20aboard%20future%20microgravity%20outposts.%0A%20%20%2ADenotes%20Equal%20Contribution%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02558v3&entry.124074799=Read"},
{"title": "Physics-guided Active Sample Reweighting for Urban Flow Prediction", "author": "Wei Jiang and Tong Chen and Guanhua Ye and Wentao Zhang and Lizhen Cui and Zi Huang and Hongzhi Yin", "abstract": "  Urban flow prediction is a spatio-temporal modeling task that estimates the\nthroughput of transportation services like buses, taxis, and ride-sharing,\nwhere data-driven models have become the most popular solution in the past\ndecade. Meanwhile, the implicitly learned mapping between historical\nobservations to the prediction targets tend to over-simplify the dynamics of\nreal-world urban flows, leading to suboptimal predictions. Some recent\nspatio-temporal prediction solutions bring remedies with the notion of\nphysics-guided machine learning (PGML), which describes spatio-temporal data\nwith nuanced and principled physics laws, thus enhancing both the prediction\naccuracy and interpretability. However, these spatio-temporal PGML methods are\nbuilt upon a strong assumption that the observed data fully conforms to the\ndifferential equations that define the physical system, which can quickly\nbecome ill-posed in urban flow prediction tasks. The observed urban flow data,\nespecially when sliced into time-dependent snapshots to facilitate predictions,\nis typically incomplete and sparse, and prone to inherent noise incurred in the\ncollection process. As a result, such physical inconsistency between the data\nand PGML model significantly limits the predictive power and robustness of the\nsolution. Moreover, due to the interval-based predictions and intermittent\nnature of data filing in many transportation services, the instantaneous\ndynamics of urban flows can hardly be captured, rendering differential\nequation-based continuous modeling a loose fit for this setting. To overcome\nthe challenges, we develop a discretized physics-guided network (PN), and\npropose a data-aware framework Physics-guided Active Sample Reweighting\n(P-GASR) to enhance PN. Experimental results in four real-world datasets\ndemonstrate that our method achieves state-of-the-art performance with a\ndemonstrable improvement in robustness.\n", "link": "http://arxiv.org/abs/2407.13605v2", "date": "2024-08-06", "relevancy": 1.6369, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.568}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5608}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-guided%20Active%20Sample%20Reweighting%20for%20Urban%20Flow%20Prediction&body=Title%3A%20Physics-guided%20Active%20Sample%20Reweighting%20for%20Urban%20Flow%20Prediction%0AAuthor%3A%20Wei%20Jiang%20and%20Tong%20Chen%20and%20Guanhua%20Ye%20and%20Wentao%20Zhang%20and%20Lizhen%20Cui%20and%20Zi%20Huang%20and%20Hongzhi%20Yin%0AAbstract%3A%20%20%20Urban%20flow%20prediction%20is%20a%20spatio-temporal%20modeling%20task%20that%20estimates%20the%0Athroughput%20of%20transportation%20services%20like%20buses%2C%20taxis%2C%20and%20ride-sharing%2C%0Awhere%20data-driven%20models%20have%20become%20the%20most%20popular%20solution%20in%20the%20past%0Adecade.%20Meanwhile%2C%20the%20implicitly%20learned%20mapping%20between%20historical%0Aobservations%20to%20the%20prediction%20targets%20tend%20to%20over-simplify%20the%20dynamics%20of%0Areal-world%20urban%20flows%2C%20leading%20to%20suboptimal%20predictions.%20Some%20recent%0Aspatio-temporal%20prediction%20solutions%20bring%20remedies%20with%20the%20notion%20of%0Aphysics-guided%20machine%20learning%20%28PGML%29%2C%20which%20describes%20spatio-temporal%20data%0Awith%20nuanced%20and%20principled%20physics%20laws%2C%20thus%20enhancing%20both%20the%20prediction%0Aaccuracy%20and%20interpretability.%20However%2C%20these%20spatio-temporal%20PGML%20methods%20are%0Abuilt%20upon%20a%20strong%20assumption%20that%20the%20observed%20data%20fully%20conforms%20to%20the%0Adifferential%20equations%20that%20define%20the%20physical%20system%2C%20which%20can%20quickly%0Abecome%20ill-posed%20in%20urban%20flow%20prediction%20tasks.%20The%20observed%20urban%20flow%20data%2C%0Aespecially%20when%20sliced%20into%20time-dependent%20snapshots%20to%20facilitate%20predictions%2C%0Ais%20typically%20incomplete%20and%20sparse%2C%20and%20prone%20to%20inherent%20noise%20incurred%20in%20the%0Acollection%20process.%20As%20a%20result%2C%20such%20physical%20inconsistency%20between%20the%20data%0Aand%20PGML%20model%20significantly%20limits%20the%20predictive%20power%20and%20robustness%20of%20the%0Asolution.%20Moreover%2C%20due%20to%20the%20interval-based%20predictions%20and%20intermittent%0Anature%20of%20data%20filing%20in%20many%20transportation%20services%2C%20the%20instantaneous%0Adynamics%20of%20urban%20flows%20can%20hardly%20be%20captured%2C%20rendering%20differential%0Aequation-based%20continuous%20modeling%20a%20loose%20fit%20for%20this%20setting.%20To%20overcome%0Athe%20challenges%2C%20we%20develop%20a%20discretized%20physics-guided%20network%20%28PN%29%2C%20and%0Apropose%20a%20data-aware%20framework%20Physics-guided%20Active%20Sample%20Reweighting%0A%28P-GASR%29%20to%20enhance%20PN.%20Experimental%20results%20in%20four%20real-world%20datasets%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20with%20a%0Ademonstrable%20improvement%20in%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13605v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-guided%2520Active%2520Sample%2520Reweighting%2520for%2520Urban%2520Flow%2520Prediction%26entry.906535625%3DWei%2520Jiang%2520and%2520Tong%2520Chen%2520and%2520Guanhua%2520Ye%2520and%2520Wentao%2520Zhang%2520and%2520Lizhen%2520Cui%2520and%2520Zi%2520Huang%2520and%2520Hongzhi%2520Yin%26entry.1292438233%3D%2520%2520Urban%2520flow%2520prediction%2520is%2520a%2520spatio-temporal%2520modeling%2520task%2520that%2520estimates%2520the%250Athroughput%2520of%2520transportation%2520services%2520like%2520buses%252C%2520taxis%252C%2520and%2520ride-sharing%252C%250Awhere%2520data-driven%2520models%2520have%2520become%2520the%2520most%2520popular%2520solution%2520in%2520the%2520past%250Adecade.%2520Meanwhile%252C%2520the%2520implicitly%2520learned%2520mapping%2520between%2520historical%250Aobservations%2520to%2520the%2520prediction%2520targets%2520tend%2520to%2520over-simplify%2520the%2520dynamics%2520of%250Areal-world%2520urban%2520flows%252C%2520leading%2520to%2520suboptimal%2520predictions.%2520Some%2520recent%250Aspatio-temporal%2520prediction%2520solutions%2520bring%2520remedies%2520with%2520the%2520notion%2520of%250Aphysics-guided%2520machine%2520learning%2520%2528PGML%2529%252C%2520which%2520describes%2520spatio-temporal%2520data%250Awith%2520nuanced%2520and%2520principled%2520physics%2520laws%252C%2520thus%2520enhancing%2520both%2520the%2520prediction%250Aaccuracy%2520and%2520interpretability.%2520However%252C%2520these%2520spatio-temporal%2520PGML%2520methods%2520are%250Abuilt%2520upon%2520a%2520strong%2520assumption%2520that%2520the%2520observed%2520data%2520fully%2520conforms%2520to%2520the%250Adifferential%2520equations%2520that%2520define%2520the%2520physical%2520system%252C%2520which%2520can%2520quickly%250Abecome%2520ill-posed%2520in%2520urban%2520flow%2520prediction%2520tasks.%2520The%2520observed%2520urban%2520flow%2520data%252C%250Aespecially%2520when%2520sliced%2520into%2520time-dependent%2520snapshots%2520to%2520facilitate%2520predictions%252C%250Ais%2520typically%2520incomplete%2520and%2520sparse%252C%2520and%2520prone%2520to%2520inherent%2520noise%2520incurred%2520in%2520the%250Acollection%2520process.%2520As%2520a%2520result%252C%2520such%2520physical%2520inconsistency%2520between%2520the%2520data%250Aand%2520PGML%2520model%2520significantly%2520limits%2520the%2520predictive%2520power%2520and%2520robustness%2520of%2520the%250Asolution.%2520Moreover%252C%2520due%2520to%2520the%2520interval-based%2520predictions%2520and%2520intermittent%250Anature%2520of%2520data%2520filing%2520in%2520many%2520transportation%2520services%252C%2520the%2520instantaneous%250Adynamics%2520of%2520urban%2520flows%2520can%2520hardly%2520be%2520captured%252C%2520rendering%2520differential%250Aequation-based%2520continuous%2520modeling%2520a%2520loose%2520fit%2520for%2520this%2520setting.%2520To%2520overcome%250Athe%2520challenges%252C%2520we%2520develop%2520a%2520discretized%2520physics-guided%2520network%2520%2528PN%2529%252C%2520and%250Apropose%2520a%2520data-aware%2520framework%2520Physics-guided%2520Active%2520Sample%2520Reweighting%250A%2528P-GASR%2529%2520to%2520enhance%2520PN.%2520Experimental%2520results%2520in%2520four%2520real-world%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520with%2520a%250Ademonstrable%2520improvement%2520in%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13605v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-guided%20Active%20Sample%20Reweighting%20for%20Urban%20Flow%20Prediction&entry.906535625=Wei%20Jiang%20and%20Tong%20Chen%20and%20Guanhua%20Ye%20and%20Wentao%20Zhang%20and%20Lizhen%20Cui%20and%20Zi%20Huang%20and%20Hongzhi%20Yin&entry.1292438233=%20%20Urban%20flow%20prediction%20is%20a%20spatio-temporal%20modeling%20task%20that%20estimates%20the%0Athroughput%20of%20transportation%20services%20like%20buses%2C%20taxis%2C%20and%20ride-sharing%2C%0Awhere%20data-driven%20models%20have%20become%20the%20most%20popular%20solution%20in%20the%20past%0Adecade.%20Meanwhile%2C%20the%20implicitly%20learned%20mapping%20between%20historical%0Aobservations%20to%20the%20prediction%20targets%20tend%20to%20over-simplify%20the%20dynamics%20of%0Areal-world%20urban%20flows%2C%20leading%20to%20suboptimal%20predictions.%20Some%20recent%0Aspatio-temporal%20prediction%20solutions%20bring%20remedies%20with%20the%20notion%20of%0Aphysics-guided%20machine%20learning%20%28PGML%29%2C%20which%20describes%20spatio-temporal%20data%0Awith%20nuanced%20and%20principled%20physics%20laws%2C%20thus%20enhancing%20both%20the%20prediction%0Aaccuracy%20and%20interpretability.%20However%2C%20these%20spatio-temporal%20PGML%20methods%20are%0Abuilt%20upon%20a%20strong%20assumption%20that%20the%20observed%20data%20fully%20conforms%20to%20the%0Adifferential%20equations%20that%20define%20the%20physical%20system%2C%20which%20can%20quickly%0Abecome%20ill-posed%20in%20urban%20flow%20prediction%20tasks.%20The%20observed%20urban%20flow%20data%2C%0Aespecially%20when%20sliced%20into%20time-dependent%20snapshots%20to%20facilitate%20predictions%2C%0Ais%20typically%20incomplete%20and%20sparse%2C%20and%20prone%20to%20inherent%20noise%20incurred%20in%20the%0Acollection%20process.%20As%20a%20result%2C%20such%20physical%20inconsistency%20between%20the%20data%0Aand%20PGML%20model%20significantly%20limits%20the%20predictive%20power%20and%20robustness%20of%20the%0Asolution.%20Moreover%2C%20due%20to%20the%20interval-based%20predictions%20and%20intermittent%0Anature%20of%20data%20filing%20in%20many%20transportation%20services%2C%20the%20instantaneous%0Adynamics%20of%20urban%20flows%20can%20hardly%20be%20captured%2C%20rendering%20differential%0Aequation-based%20continuous%20modeling%20a%20loose%20fit%20for%20this%20setting.%20To%20overcome%0Athe%20challenges%2C%20we%20develop%20a%20discretized%20physics-guided%20network%20%28PN%29%2C%20and%0Apropose%20a%20data-aware%20framework%20Physics-guided%20Active%20Sample%20Reweighting%0A%28P-GASR%29%20to%20enhance%20PN.%20Experimental%20results%20in%20four%20real-world%20datasets%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20with%20a%0Ademonstrable%20improvement%20in%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13605v2&entry.124074799=Read"},
{"title": "Analysis of Partially-Calibrated Sparse Subarrays for Direction Finding\n  with Extended Degrees of Freedom", "author": "W. S. Leite and R. C. de Lamare", "abstract": "  This paper investigates the problem of direction-of-arrival (DOA) estimation\nusing multiple partially-calibrated sparse subarrays. In particular, we present\nthe Generalized Coarray Multiple Signal Classification (GCA-MUSIC) DOA\nestimation algorithm to scenarios with partially-calibrated sparse subarrays.\nThe proposed GCA-MUSIC algorithm exploits the difference coarray for each\nsubarray, followed by a specific pseudo-spectrum merging rule that is based on\nthe intersection of the signal subspaces associated to each subarray. This rule\nassumes that there is no a priori knowledge about the cross-covariance between\nsubarrays. In that way, only the second-order statistics of each subarray are\nused to estimate the directions with increased degrees of freedom, i.e., the\nestimation procedure preserves the coarray Multiple Signal Classification and\nsparse arrays properties to estimate more sources than the number of physical\nsensors in each subarray. Numerical simulations show that the proposed\nGCA-MUSIC has better performance than other similar strategies.\n", "link": "http://arxiv.org/abs/2408.03236v1", "date": "2024-08-06", "relevancy": 1.6244, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4198}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4114}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Partially-Calibrated%20Sparse%20Subarrays%20for%20Direction%20Finding%0A%20%20with%20Extended%20Degrees%20of%20Freedom&body=Title%3A%20Analysis%20of%20Partially-Calibrated%20Sparse%20Subarrays%20for%20Direction%20Finding%0A%20%20with%20Extended%20Degrees%20of%20Freedom%0AAuthor%3A%20W.%20S.%20Leite%20and%20R.%20C.%20de%20Lamare%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20problem%20of%20direction-of-arrival%20%28DOA%29%20estimation%0Ausing%20multiple%20partially-calibrated%20sparse%20subarrays.%20In%20particular%2C%20we%20present%0Athe%20Generalized%20Coarray%20Multiple%20Signal%20Classification%20%28GCA-MUSIC%29%20DOA%0Aestimation%20algorithm%20to%20scenarios%20with%20partially-calibrated%20sparse%20subarrays.%0AThe%20proposed%20GCA-MUSIC%20algorithm%20exploits%20the%20difference%20coarray%20for%20each%0Asubarray%2C%20followed%20by%20a%20specific%20pseudo-spectrum%20merging%20rule%20that%20is%20based%20on%0Athe%20intersection%20of%20the%20signal%20subspaces%20associated%20to%20each%20subarray.%20This%20rule%0Aassumes%20that%20there%20is%20no%20a%20priori%20knowledge%20about%20the%20cross-covariance%20between%0Asubarrays.%20In%20that%20way%2C%20only%20the%20second-order%20statistics%20of%20each%20subarray%20are%0Aused%20to%20estimate%20the%20directions%20with%20increased%20degrees%20of%20freedom%2C%20i.e.%2C%20the%0Aestimation%20procedure%20preserves%20the%20coarray%20Multiple%20Signal%20Classification%20and%0Asparse%20arrays%20properties%20to%20estimate%20more%20sources%20than%20the%20number%20of%20physical%0Asensors%20in%20each%20subarray.%20Numerical%20simulations%20show%20that%20the%20proposed%0AGCA-MUSIC%20has%20better%20performance%20than%20other%20similar%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Partially-Calibrated%2520Sparse%2520Subarrays%2520for%2520Direction%2520Finding%250A%2520%2520with%2520Extended%2520Degrees%2520of%2520Freedom%26entry.906535625%3DW.%2520S.%2520Leite%2520and%2520R.%2520C.%2520de%2520Lamare%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520problem%2520of%2520direction-of-arrival%2520%2528DOA%2529%2520estimation%250Ausing%2520multiple%2520partially-calibrated%2520sparse%2520subarrays.%2520In%2520particular%252C%2520we%2520present%250Athe%2520Generalized%2520Coarray%2520Multiple%2520Signal%2520Classification%2520%2528GCA-MUSIC%2529%2520DOA%250Aestimation%2520algorithm%2520to%2520scenarios%2520with%2520partially-calibrated%2520sparse%2520subarrays.%250AThe%2520proposed%2520GCA-MUSIC%2520algorithm%2520exploits%2520the%2520difference%2520coarray%2520for%2520each%250Asubarray%252C%2520followed%2520by%2520a%2520specific%2520pseudo-spectrum%2520merging%2520rule%2520that%2520is%2520based%2520on%250Athe%2520intersection%2520of%2520the%2520signal%2520subspaces%2520associated%2520to%2520each%2520subarray.%2520This%2520rule%250Aassumes%2520that%2520there%2520is%2520no%2520a%2520priori%2520knowledge%2520about%2520the%2520cross-covariance%2520between%250Asubarrays.%2520In%2520that%2520way%252C%2520only%2520the%2520second-order%2520statistics%2520of%2520each%2520subarray%2520are%250Aused%2520to%2520estimate%2520the%2520directions%2520with%2520increased%2520degrees%2520of%2520freedom%252C%2520i.e.%252C%2520the%250Aestimation%2520procedure%2520preserves%2520the%2520coarray%2520Multiple%2520Signal%2520Classification%2520and%250Asparse%2520arrays%2520properties%2520to%2520estimate%2520more%2520sources%2520than%2520the%2520number%2520of%2520physical%250Asensors%2520in%2520each%2520subarray.%2520Numerical%2520simulations%2520show%2520that%2520the%2520proposed%250AGCA-MUSIC%2520has%2520better%2520performance%2520than%2520other%2520similar%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Partially-Calibrated%20Sparse%20Subarrays%20for%20Direction%20Finding%0A%20%20with%20Extended%20Degrees%20of%20Freedom&entry.906535625=W.%20S.%20Leite%20and%20R.%20C.%20de%20Lamare&entry.1292438233=%20%20This%20paper%20investigates%20the%20problem%20of%20direction-of-arrival%20%28DOA%29%20estimation%0Ausing%20multiple%20partially-calibrated%20sparse%20subarrays.%20In%20particular%2C%20we%20present%0Athe%20Generalized%20Coarray%20Multiple%20Signal%20Classification%20%28GCA-MUSIC%29%20DOA%0Aestimation%20algorithm%20to%20scenarios%20with%20partially-calibrated%20sparse%20subarrays.%0AThe%20proposed%20GCA-MUSIC%20algorithm%20exploits%20the%20difference%20coarray%20for%20each%0Asubarray%2C%20followed%20by%20a%20specific%20pseudo-spectrum%20merging%20rule%20that%20is%20based%20on%0Athe%20intersection%20of%20the%20signal%20subspaces%20associated%20to%20each%20subarray.%20This%20rule%0Aassumes%20that%20there%20is%20no%20a%20priori%20knowledge%20about%20the%20cross-covariance%20between%0Asubarrays.%20In%20that%20way%2C%20only%20the%20second-order%20statistics%20of%20each%20subarray%20are%0Aused%20to%20estimate%20the%20directions%20with%20increased%20degrees%20of%20freedom%2C%20i.e.%2C%20the%0Aestimation%20procedure%20preserves%20the%20coarray%20Multiple%20Signal%20Classification%20and%0Asparse%20arrays%20properties%20to%20estimate%20more%20sources%20than%20the%20number%20of%20physical%0Asensors%20in%20each%20subarray.%20Numerical%20simulations%20show%20that%20the%20proposed%0AGCA-MUSIC%20has%20better%20performance%20than%20other%20similar%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03236v1&entry.124074799=Read"},
{"title": "Training on the Fly: On-device Self-supervised Learning aboard\n  Nano-drones within 20 mW", "author": "Elia Cereda and Alessandro Giusti and Daniele Palossi", "abstract": "  Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning\n(TinyML), such as nano-drones, are becoming an increasingly attractive\ntechnology. Their small form factor (i.e., ~10cm diameter) ensures vast\napplicability, ranging from the exploration of narrow disaster scenarios to\nsafe human-robot interaction. Simple electronics make these CPSes inexpensive,\nbut strongly limit the computational, memory, and sensing resources available\non board. In real-world applications, these limitations are further exacerbated\nby domain shift. This fundamental machine learning problem implies that model\nperception performance drops when moving from the training domain to a\ndifferent deployment one. To cope with and mitigate this general problem, we\npresent a novel on-device fine-tuning approach that relies only on the limited\nultra-low power resources available aboard nano-drones. Then, to overcome the\nlack of ground-truth training labels aboard our CPS, we also employ a\nself-supervised method based on ego-motion consistency. Albeit our work builds\non top of a specific real-world vision-based human pose estimation task, it is\nwidely applicable for many embedded TinyML use cases. Our 512-image on-device\ntraining procedure is fully deployed aboard an ultra-low power GWT GAP9\nSystem-on-Chip and requires only 1MB of memory while consuming as low as 19mW\nor running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our\non-device learning approach by field-testing our closed-loop CPS, showing a\nreduction in horizontal position error of up to 26% vs. a non-fine-tuned\nstate-of-the-art baseline. In the most challenging never-seen-before\nenvironment, our on-device learning procedure makes the difference between\nsucceeding or failing the mission.\n", "link": "http://arxiv.org/abs/2408.03168v1", "date": "2024-08-06", "relevancy": 1.6231, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5485}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5411}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20on%20the%20Fly%3A%20On-device%20Self-supervised%20Learning%20aboard%0A%20%20Nano-drones%20within%2020%20mW&body=Title%3A%20Training%20on%20the%20Fly%3A%20On-device%20Self-supervised%20Learning%20aboard%0A%20%20Nano-drones%20within%2020%20mW%0AAuthor%3A%20Elia%20Cereda%20and%20Alessandro%20Giusti%20and%20Daniele%20Palossi%0AAbstract%3A%20%20%20Miniaturized%20cyber-physical%20systems%20%28CPSes%29%20powered%20by%20tiny%20machine%20learning%0A%28TinyML%29%2C%20such%20as%20nano-drones%2C%20are%20becoming%20an%20increasingly%20attractive%0Atechnology.%20Their%20small%20form%20factor%20%28i.e.%2C%20~10cm%20diameter%29%20ensures%20vast%0Aapplicability%2C%20ranging%20from%20the%20exploration%20of%20narrow%20disaster%20scenarios%20to%0Asafe%20human-robot%20interaction.%20Simple%20electronics%20make%20these%20CPSes%20inexpensive%2C%0Abut%20strongly%20limit%20the%20computational%2C%20memory%2C%20and%20sensing%20resources%20available%0Aon%20board.%20In%20real-world%20applications%2C%20these%20limitations%20are%20further%20exacerbated%0Aby%20domain%20shift.%20This%20fundamental%20machine%20learning%20problem%20implies%20that%20model%0Aperception%20performance%20drops%20when%20moving%20from%20the%20training%20domain%20to%20a%0Adifferent%20deployment%20one.%20To%20cope%20with%20and%20mitigate%20this%20general%20problem%2C%20we%0Apresent%20a%20novel%20on-device%20fine-tuning%20approach%20that%20relies%20only%20on%20the%20limited%0Aultra-low%20power%20resources%20available%20aboard%20nano-drones.%20Then%2C%20to%20overcome%20the%0Alack%20of%20ground-truth%20training%20labels%20aboard%20our%20CPS%2C%20we%20also%20employ%20a%0Aself-supervised%20method%20based%20on%20ego-motion%20consistency.%20Albeit%20our%20work%20builds%0Aon%20top%20of%20a%20specific%20real-world%20vision-based%20human%20pose%20estimation%20task%2C%20it%20is%0Awidely%20applicable%20for%20many%20embedded%20TinyML%20use%20cases.%20Our%20512-image%20on-device%0Atraining%20procedure%20is%20fully%20deployed%20aboard%20an%20ultra-low%20power%20GWT%20GAP9%0ASystem-on-Chip%20and%20requires%20only%201MB%20of%20memory%20while%20consuming%20as%20low%20as%2019mW%0Aor%20running%20in%20just%20510ms%20%28at%2038mW%29.%20Finally%2C%20we%20demonstrate%20the%20benefits%20of%20our%0Aon-device%20learning%20approach%20by%20field-testing%20our%20closed-loop%20CPS%2C%20showing%20a%0Areduction%20in%20horizontal%20position%20error%20of%20up%20to%2026%25%20vs.%20a%20non-fine-tuned%0Astate-of-the-art%20baseline.%20In%20the%20most%20challenging%20never-seen-before%0Aenvironment%2C%20our%20on-device%20learning%20procedure%20makes%20the%20difference%20between%0Asucceeding%20or%20failing%20the%20mission.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520on%2520the%2520Fly%253A%2520On-device%2520Self-supervised%2520Learning%2520aboard%250A%2520%2520Nano-drones%2520within%252020%2520mW%26entry.906535625%3DElia%2520Cereda%2520and%2520Alessandro%2520Giusti%2520and%2520Daniele%2520Palossi%26entry.1292438233%3D%2520%2520Miniaturized%2520cyber-physical%2520systems%2520%2528CPSes%2529%2520powered%2520by%2520tiny%2520machine%2520learning%250A%2528TinyML%2529%252C%2520such%2520as%2520nano-drones%252C%2520are%2520becoming%2520an%2520increasingly%2520attractive%250Atechnology.%2520Their%2520small%2520form%2520factor%2520%2528i.e.%252C%2520~10cm%2520diameter%2529%2520ensures%2520vast%250Aapplicability%252C%2520ranging%2520from%2520the%2520exploration%2520of%2520narrow%2520disaster%2520scenarios%2520to%250Asafe%2520human-robot%2520interaction.%2520Simple%2520electronics%2520make%2520these%2520CPSes%2520inexpensive%252C%250Abut%2520strongly%2520limit%2520the%2520computational%252C%2520memory%252C%2520and%2520sensing%2520resources%2520available%250Aon%2520board.%2520In%2520real-world%2520applications%252C%2520these%2520limitations%2520are%2520further%2520exacerbated%250Aby%2520domain%2520shift.%2520This%2520fundamental%2520machine%2520learning%2520problem%2520implies%2520that%2520model%250Aperception%2520performance%2520drops%2520when%2520moving%2520from%2520the%2520training%2520domain%2520to%2520a%250Adifferent%2520deployment%2520one.%2520To%2520cope%2520with%2520and%2520mitigate%2520this%2520general%2520problem%252C%2520we%250Apresent%2520a%2520novel%2520on-device%2520fine-tuning%2520approach%2520that%2520relies%2520only%2520on%2520the%2520limited%250Aultra-low%2520power%2520resources%2520available%2520aboard%2520nano-drones.%2520Then%252C%2520to%2520overcome%2520the%250Alack%2520of%2520ground-truth%2520training%2520labels%2520aboard%2520our%2520CPS%252C%2520we%2520also%2520employ%2520a%250Aself-supervised%2520method%2520based%2520on%2520ego-motion%2520consistency.%2520Albeit%2520our%2520work%2520builds%250Aon%2520top%2520of%2520a%2520specific%2520real-world%2520vision-based%2520human%2520pose%2520estimation%2520task%252C%2520it%2520is%250Awidely%2520applicable%2520for%2520many%2520embedded%2520TinyML%2520use%2520cases.%2520Our%2520512-image%2520on-device%250Atraining%2520procedure%2520is%2520fully%2520deployed%2520aboard%2520an%2520ultra-low%2520power%2520GWT%2520GAP9%250ASystem-on-Chip%2520and%2520requires%2520only%25201MB%2520of%2520memory%2520while%2520consuming%2520as%2520low%2520as%252019mW%250Aor%2520running%2520in%2520just%2520510ms%2520%2528at%252038mW%2529.%2520Finally%252C%2520we%2520demonstrate%2520the%2520benefits%2520of%2520our%250Aon-device%2520learning%2520approach%2520by%2520field-testing%2520our%2520closed-loop%2520CPS%252C%2520showing%2520a%250Areduction%2520in%2520horizontal%2520position%2520error%2520of%2520up%2520to%252026%2525%2520vs.%2520a%2520non-fine-tuned%250Astate-of-the-art%2520baseline.%2520In%2520the%2520most%2520challenging%2520never-seen-before%250Aenvironment%252C%2520our%2520on-device%2520learning%2520procedure%2520makes%2520the%2520difference%2520between%250Asucceeding%2520or%2520failing%2520the%2520mission.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20on%20the%20Fly%3A%20On-device%20Self-supervised%20Learning%20aboard%0A%20%20Nano-drones%20within%2020%20mW&entry.906535625=Elia%20Cereda%20and%20Alessandro%20Giusti%20and%20Daniele%20Palossi&entry.1292438233=%20%20Miniaturized%20cyber-physical%20systems%20%28CPSes%29%20powered%20by%20tiny%20machine%20learning%0A%28TinyML%29%2C%20such%20as%20nano-drones%2C%20are%20becoming%20an%20increasingly%20attractive%0Atechnology.%20Their%20small%20form%20factor%20%28i.e.%2C%20~10cm%20diameter%29%20ensures%20vast%0Aapplicability%2C%20ranging%20from%20the%20exploration%20of%20narrow%20disaster%20scenarios%20to%0Asafe%20human-robot%20interaction.%20Simple%20electronics%20make%20these%20CPSes%20inexpensive%2C%0Abut%20strongly%20limit%20the%20computational%2C%20memory%2C%20and%20sensing%20resources%20available%0Aon%20board.%20In%20real-world%20applications%2C%20these%20limitations%20are%20further%20exacerbated%0Aby%20domain%20shift.%20This%20fundamental%20machine%20learning%20problem%20implies%20that%20model%0Aperception%20performance%20drops%20when%20moving%20from%20the%20training%20domain%20to%20a%0Adifferent%20deployment%20one.%20To%20cope%20with%20and%20mitigate%20this%20general%20problem%2C%20we%0Apresent%20a%20novel%20on-device%20fine-tuning%20approach%20that%20relies%20only%20on%20the%20limited%0Aultra-low%20power%20resources%20available%20aboard%20nano-drones.%20Then%2C%20to%20overcome%20the%0Alack%20of%20ground-truth%20training%20labels%20aboard%20our%20CPS%2C%20we%20also%20employ%20a%0Aself-supervised%20method%20based%20on%20ego-motion%20consistency.%20Albeit%20our%20work%20builds%0Aon%20top%20of%20a%20specific%20real-world%20vision-based%20human%20pose%20estimation%20task%2C%20it%20is%0Awidely%20applicable%20for%20many%20embedded%20TinyML%20use%20cases.%20Our%20512-image%20on-device%0Atraining%20procedure%20is%20fully%20deployed%20aboard%20an%20ultra-low%20power%20GWT%20GAP9%0ASystem-on-Chip%20and%20requires%20only%201MB%20of%20memory%20while%20consuming%20as%20low%20as%2019mW%0Aor%20running%20in%20just%20510ms%20%28at%2038mW%29.%20Finally%2C%20we%20demonstrate%20the%20benefits%20of%20our%0Aon-device%20learning%20approach%20by%20field-testing%20our%20closed-loop%20CPS%2C%20showing%20a%0Areduction%20in%20horizontal%20position%20error%20of%20up%20to%2026%25%20vs.%20a%20non-fine-tuned%0Astate-of-the-art%20baseline.%20In%20the%20most%20challenging%20never-seen-before%0Aenvironment%2C%20our%20on-device%20learning%20procedure%20makes%20the%20difference%20between%0Asucceeding%20or%20failing%20the%20mission.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03168v1&entry.124074799=Read"},
{"title": "Learning Provably Robust Policies in Uncertain Parametric Environments", "author": "Yannik Schnitzer and Alessandro Abate and David Parker", "abstract": "  We present a data-driven approach for learning MDP policies that are robust\nacross stochastic environments whose transition probabilities are defined by\nparameters with an unknown distribution. We produce probably approximately\ncorrect (PAC) guarantees for the performance of these learned policies in a\nnew, unseen environment over the unknown distribution. Our approach is based on\nfinite samples of the MDP environments, for each of which we build an\napproximation of the model as an interval MDP, by exploring a set of generated\ntrajectories. We use the built approximations to synthesise a single policy\nthat performs well (meets given requirements) across the sampled environments,\nand furthermore bound its risk (of not meeting the given requirements) when\ndeployed in an unseen environment. Our procedure offers a trade-off between the\nguaranteed performance of the learned policy and the risk of not meeting the\nguarantee in an unseen environment. Our approach exploits knowledge of the\nenvironment's state space and graph structure, and we show how additional\nknowledge of its parametric structure can be leveraged to optimize learning and\nto obtain tighter guarantees from less samples. We evaluate our approach on a\ndiverse range of established benchmarks, demonstrating that we can generate\nhighly performing and robust policies, along with guarantees that tightly\nquantify their performance and the associated risk.\n", "link": "http://arxiv.org/abs/2408.03093v1", "date": "2024-08-06", "relevancy": 1.6209, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5564}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5454}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Provably%20Robust%20Policies%20in%20Uncertain%20Parametric%20Environments&body=Title%3A%20Learning%20Provably%20Robust%20Policies%20in%20Uncertain%20Parametric%20Environments%0AAuthor%3A%20Yannik%20Schnitzer%20and%20Alessandro%20Abate%20and%20David%20Parker%0AAbstract%3A%20%20%20We%20present%20a%20data-driven%20approach%20for%20learning%20MDP%20policies%20that%20are%20robust%0Aacross%20stochastic%20environments%20whose%20transition%20probabilities%20are%20defined%20by%0Aparameters%20with%20an%20unknown%20distribution.%20We%20produce%20probably%20approximately%0Acorrect%20%28PAC%29%20guarantees%20for%20the%20performance%20of%20these%20learned%20policies%20in%20a%0Anew%2C%20unseen%20environment%20over%20the%20unknown%20distribution.%20Our%20approach%20is%20based%20on%0Afinite%20samples%20of%20the%20MDP%20environments%2C%20for%20each%20of%20which%20we%20build%20an%0Aapproximation%20of%20the%20model%20as%20an%20interval%20MDP%2C%20by%20exploring%20a%20set%20of%20generated%0Atrajectories.%20We%20use%20the%20built%20approximations%20to%20synthesise%20a%20single%20policy%0Athat%20performs%20well%20%28meets%20given%20requirements%29%20across%20the%20sampled%20environments%2C%0Aand%20furthermore%20bound%20its%20risk%20%28of%20not%20meeting%20the%20given%20requirements%29%20when%0Adeployed%20in%20an%20unseen%20environment.%20Our%20procedure%20offers%20a%20trade-off%20between%20the%0Aguaranteed%20performance%20of%20the%20learned%20policy%20and%20the%20risk%20of%20not%20meeting%20the%0Aguarantee%20in%20an%20unseen%20environment.%20Our%20approach%20exploits%20knowledge%20of%20the%0Aenvironment%27s%20state%20space%20and%20graph%20structure%2C%20and%20we%20show%20how%20additional%0Aknowledge%20of%20its%20parametric%20structure%20can%20be%20leveraged%20to%20optimize%20learning%20and%0Ato%20obtain%20tighter%20guarantees%20from%20less%20samples.%20We%20evaluate%20our%20approach%20on%20a%0Adiverse%20range%20of%20established%20benchmarks%2C%20demonstrating%20that%20we%20can%20generate%0Ahighly%20performing%20and%20robust%20policies%2C%20along%20with%20guarantees%20that%20tightly%0Aquantify%20their%20performance%20and%20the%20associated%20risk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Provably%2520Robust%2520Policies%2520in%2520Uncertain%2520Parametric%2520Environments%26entry.906535625%3DYannik%2520Schnitzer%2520and%2520Alessandro%2520Abate%2520and%2520David%2520Parker%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520data-driven%2520approach%2520for%2520learning%2520MDP%2520policies%2520that%2520are%2520robust%250Aacross%2520stochastic%2520environments%2520whose%2520transition%2520probabilities%2520are%2520defined%2520by%250Aparameters%2520with%2520an%2520unknown%2520distribution.%2520We%2520produce%2520probably%2520approximately%250Acorrect%2520%2528PAC%2529%2520guarantees%2520for%2520the%2520performance%2520of%2520these%2520learned%2520policies%2520in%2520a%250Anew%252C%2520unseen%2520environment%2520over%2520the%2520unknown%2520distribution.%2520Our%2520approach%2520is%2520based%2520on%250Afinite%2520samples%2520of%2520the%2520MDP%2520environments%252C%2520for%2520each%2520of%2520which%2520we%2520build%2520an%250Aapproximation%2520of%2520the%2520model%2520as%2520an%2520interval%2520MDP%252C%2520by%2520exploring%2520a%2520set%2520of%2520generated%250Atrajectories.%2520We%2520use%2520the%2520built%2520approximations%2520to%2520synthesise%2520a%2520single%2520policy%250Athat%2520performs%2520well%2520%2528meets%2520given%2520requirements%2529%2520across%2520the%2520sampled%2520environments%252C%250Aand%2520furthermore%2520bound%2520its%2520risk%2520%2528of%2520not%2520meeting%2520the%2520given%2520requirements%2529%2520when%250Adeployed%2520in%2520an%2520unseen%2520environment.%2520Our%2520procedure%2520offers%2520a%2520trade-off%2520between%2520the%250Aguaranteed%2520performance%2520of%2520the%2520learned%2520policy%2520and%2520the%2520risk%2520of%2520not%2520meeting%2520the%250Aguarantee%2520in%2520an%2520unseen%2520environment.%2520Our%2520approach%2520exploits%2520knowledge%2520of%2520the%250Aenvironment%2527s%2520state%2520space%2520and%2520graph%2520structure%252C%2520and%2520we%2520show%2520how%2520additional%250Aknowledge%2520of%2520its%2520parametric%2520structure%2520can%2520be%2520leveraged%2520to%2520optimize%2520learning%2520and%250Ato%2520obtain%2520tighter%2520guarantees%2520from%2520less%2520samples.%2520We%2520evaluate%2520our%2520approach%2520on%2520a%250Adiverse%2520range%2520of%2520established%2520benchmarks%252C%2520demonstrating%2520that%2520we%2520can%2520generate%250Ahighly%2520performing%2520and%2520robust%2520policies%252C%2520along%2520with%2520guarantees%2520that%2520tightly%250Aquantify%2520their%2520performance%2520and%2520the%2520associated%2520risk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Provably%20Robust%20Policies%20in%20Uncertain%20Parametric%20Environments&entry.906535625=Yannik%20Schnitzer%20and%20Alessandro%20Abate%20and%20David%20Parker&entry.1292438233=%20%20We%20present%20a%20data-driven%20approach%20for%20learning%20MDP%20policies%20that%20are%20robust%0Aacross%20stochastic%20environments%20whose%20transition%20probabilities%20are%20defined%20by%0Aparameters%20with%20an%20unknown%20distribution.%20We%20produce%20probably%20approximately%0Acorrect%20%28PAC%29%20guarantees%20for%20the%20performance%20of%20these%20learned%20policies%20in%20a%0Anew%2C%20unseen%20environment%20over%20the%20unknown%20distribution.%20Our%20approach%20is%20based%20on%0Afinite%20samples%20of%20the%20MDP%20environments%2C%20for%20each%20of%20which%20we%20build%20an%0Aapproximation%20of%20the%20model%20as%20an%20interval%20MDP%2C%20by%20exploring%20a%20set%20of%20generated%0Atrajectories.%20We%20use%20the%20built%20approximations%20to%20synthesise%20a%20single%20policy%0Athat%20performs%20well%20%28meets%20given%20requirements%29%20across%20the%20sampled%20environments%2C%0Aand%20furthermore%20bound%20its%20risk%20%28of%20not%20meeting%20the%20given%20requirements%29%20when%0Adeployed%20in%20an%20unseen%20environment.%20Our%20procedure%20offers%20a%20trade-off%20between%20the%0Aguaranteed%20performance%20of%20the%20learned%20policy%20and%20the%20risk%20of%20not%20meeting%20the%0Aguarantee%20in%20an%20unseen%20environment.%20Our%20approach%20exploits%20knowledge%20of%20the%0Aenvironment%27s%20state%20space%20and%20graph%20structure%2C%20and%20we%20show%20how%20additional%0Aknowledge%20of%20its%20parametric%20structure%20can%20be%20leveraged%20to%20optimize%20learning%20and%0Ato%20obtain%20tighter%20guarantees%20from%20less%20samples.%20We%20evaluate%20our%20approach%20on%20a%0Adiverse%20range%20of%20established%20benchmarks%2C%20demonstrating%20that%20we%20can%20generate%0Ahighly%20performing%20and%20robust%20policies%2C%20along%20with%20guarantees%20that%20tightly%0Aquantify%20their%20performance%20and%20the%20associated%20risk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03093v1&entry.124074799=Read"},
{"title": "Rethinking Jailbreaking through the Lens of Representation Engineering", "author": "Tianlong Li and Shihan Dou and Wenhao Liu and Muling Wu and Changze Lv and Rui Zheng and Xiaoqing Zheng and Xuanjing Huang", "abstract": "  The recent surge in jailbreaking methods has revealed the vulnerability of\nLarge Language Models (LLMs) to malicious inputs. While earlier research has\nprimarily concentrated on increasing the success rates of jailbreaking attacks,\nthe underlying mechanism for safeguarding LLMs remains underexplored. This\nstudy investigates the vulnerability of safety-aligned LLMs by uncovering\nspecific activity patterns within the representation space generated by LLMs.\nSuch ``safety patterns'' can be identified with only a few pairs of contrastive\nqueries in a simple method and function as ``keys'' (used as a metaphor for\nsecurity defense capability) that can be used to open or lock Pandora's Box of\nLLMs. Extensive experiments demonstrate that the robustness of LLMs against\njailbreaking can be lessened or augmented by attenuating or strengthening the\nidentified safety patterns. These findings deepen our understanding of\njailbreaking phenomena and call for the LLM community to address the potential\nmisuse of open-source LLMs.\n", "link": "http://arxiv.org/abs/2401.06824v3", "date": "2024-08-06", "relevancy": 1.6139, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4163}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3956}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Jailbreaking%20through%20the%20Lens%20of%20Representation%20Engineering&body=Title%3A%20Rethinking%20Jailbreaking%20through%20the%20Lens%20of%20Representation%20Engineering%0AAuthor%3A%20Tianlong%20Li%20and%20Shihan%20Dou%20and%20Wenhao%20Liu%20and%20Muling%20Wu%20and%20Changze%20Lv%20and%20Rui%20Zheng%20and%20Xiaoqing%20Zheng%20and%20Xuanjing%20Huang%0AAbstract%3A%20%20%20The%20recent%20surge%20in%20jailbreaking%20methods%20has%20revealed%20the%20vulnerability%20of%0ALarge%20Language%20Models%20%28LLMs%29%20to%20malicious%20inputs.%20While%20earlier%20research%20has%0Aprimarily%20concentrated%20on%20increasing%20the%20success%20rates%20of%20jailbreaking%20attacks%2C%0Athe%20underlying%20mechanism%20for%20safeguarding%20LLMs%20remains%20underexplored.%20This%0Astudy%20investigates%20the%20vulnerability%20of%20safety-aligned%20LLMs%20by%20uncovering%0Aspecific%20activity%20patterns%20within%20the%20representation%20space%20generated%20by%20LLMs.%0ASuch%20%60%60safety%20patterns%27%27%20can%20be%20identified%20with%20only%20a%20few%20pairs%20of%20contrastive%0Aqueries%20in%20a%20simple%20method%20and%20function%20as%20%60%60keys%27%27%20%28used%20as%20a%20metaphor%20for%0Asecurity%20defense%20capability%29%20that%20can%20be%20used%20to%20open%20or%20lock%20Pandora%27s%20Box%20of%0ALLMs.%20Extensive%20experiments%20demonstrate%20that%20the%20robustness%20of%20LLMs%20against%0Ajailbreaking%20can%20be%20lessened%20or%20augmented%20by%20attenuating%20or%20strengthening%20the%0Aidentified%20safety%20patterns.%20These%20findings%20deepen%20our%20understanding%20of%0Ajailbreaking%20phenomena%20and%20call%20for%20the%20LLM%20community%20to%20address%20the%20potential%0Amisuse%20of%20open-source%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06824v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Jailbreaking%2520through%2520the%2520Lens%2520of%2520Representation%2520Engineering%26entry.906535625%3DTianlong%2520Li%2520and%2520Shihan%2520Dou%2520and%2520Wenhao%2520Liu%2520and%2520Muling%2520Wu%2520and%2520Changze%2520Lv%2520and%2520Rui%2520Zheng%2520and%2520Xiaoqing%2520Zheng%2520and%2520Xuanjing%2520Huang%26entry.1292438233%3D%2520%2520The%2520recent%2520surge%2520in%2520jailbreaking%2520methods%2520has%2520revealed%2520the%2520vulnerability%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520malicious%2520inputs.%2520While%2520earlier%2520research%2520has%250Aprimarily%2520concentrated%2520on%2520increasing%2520the%2520success%2520rates%2520of%2520jailbreaking%2520attacks%252C%250Athe%2520underlying%2520mechanism%2520for%2520safeguarding%2520LLMs%2520remains%2520underexplored.%2520This%250Astudy%2520investigates%2520the%2520vulnerability%2520of%2520safety-aligned%2520LLMs%2520by%2520uncovering%250Aspecific%2520activity%2520patterns%2520within%2520the%2520representation%2520space%2520generated%2520by%2520LLMs.%250ASuch%2520%2560%2560safety%2520patterns%2527%2527%2520can%2520be%2520identified%2520with%2520only%2520a%2520few%2520pairs%2520of%2520contrastive%250Aqueries%2520in%2520a%2520simple%2520method%2520and%2520function%2520as%2520%2560%2560keys%2527%2527%2520%2528used%2520as%2520a%2520metaphor%2520for%250Asecurity%2520defense%2520capability%2529%2520that%2520can%2520be%2520used%2520to%2520open%2520or%2520lock%2520Pandora%2527s%2520Box%2520of%250ALLMs.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520robustness%2520of%2520LLMs%2520against%250Ajailbreaking%2520can%2520be%2520lessened%2520or%2520augmented%2520by%2520attenuating%2520or%2520strengthening%2520the%250Aidentified%2520safety%2520patterns.%2520These%2520findings%2520deepen%2520our%2520understanding%2520of%250Ajailbreaking%2520phenomena%2520and%2520call%2520for%2520the%2520LLM%2520community%2520to%2520address%2520the%2520potential%250Amisuse%2520of%2520open-source%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06824v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Jailbreaking%20through%20the%20Lens%20of%20Representation%20Engineering&entry.906535625=Tianlong%20Li%20and%20Shihan%20Dou%20and%20Wenhao%20Liu%20and%20Muling%20Wu%20and%20Changze%20Lv%20and%20Rui%20Zheng%20and%20Xiaoqing%20Zheng%20and%20Xuanjing%20Huang&entry.1292438233=%20%20The%20recent%20surge%20in%20jailbreaking%20methods%20has%20revealed%20the%20vulnerability%20of%0ALarge%20Language%20Models%20%28LLMs%29%20to%20malicious%20inputs.%20While%20earlier%20research%20has%0Aprimarily%20concentrated%20on%20increasing%20the%20success%20rates%20of%20jailbreaking%20attacks%2C%0Athe%20underlying%20mechanism%20for%20safeguarding%20LLMs%20remains%20underexplored.%20This%0Astudy%20investigates%20the%20vulnerability%20of%20safety-aligned%20LLMs%20by%20uncovering%0Aspecific%20activity%20patterns%20within%20the%20representation%20space%20generated%20by%20LLMs.%0ASuch%20%60%60safety%20patterns%27%27%20can%20be%20identified%20with%20only%20a%20few%20pairs%20of%20contrastive%0Aqueries%20in%20a%20simple%20method%20and%20function%20as%20%60%60keys%27%27%20%28used%20as%20a%20metaphor%20for%0Asecurity%20defense%20capability%29%20that%20can%20be%20used%20to%20open%20or%20lock%20Pandora%27s%20Box%20of%0ALLMs.%20Extensive%20experiments%20demonstrate%20that%20the%20robustness%20of%20LLMs%20against%0Ajailbreaking%20can%20be%20lessened%20or%20augmented%20by%20attenuating%20or%20strengthening%20the%0Aidentified%20safety%20patterns.%20These%20findings%20deepen%20our%20understanding%20of%0Ajailbreaking%20phenomena%20and%20call%20for%20the%20LLM%20community%20to%20address%20the%20potential%0Amisuse%20of%20open-source%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06824v3&entry.124074799=Read"},
{"title": "Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons", "author": "Yifei Wang and Yuheng Chen and Wanting Wen and Yu Sheng and Linjing Li and Daniel Dajun Zeng", "abstract": "  In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.\n", "link": "http://arxiv.org/abs/2408.03247v1", "date": "2024-08-06", "relevancy": 0.9001, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4541}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.448}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Factual%20Recall%20Behaviors%20of%20Large%20Language%20Models%20through%0A%20%20Knowledge%20Neurons&body=Title%3A%20Unveiling%20Factual%20Recall%20Behaviors%20of%20Large%20Language%20Models%20through%0A%20%20Knowledge%20Neurons%0AAuthor%3A%20Yifei%20Wang%20and%20Yuheng%20Chen%20and%20Wanting%20Wen%20and%20Yu%20Sheng%20and%20Linjing%20Li%20and%20Daniel%20Dajun%20Zeng%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20whether%20Large%20Language%20Models%20%28LLMs%29%20actively%0Arecall%20or%20retrieve%20their%20internal%20repositories%20of%20factual%20knowledge%20when%20faced%0Awith%20reasoning%20tasks.%20Through%20an%20analysis%20of%20LLMs%27%20internal%20factual%20recall%20at%0Aeach%20reasoning%20step%20via%20Knowledge%20Neurons%2C%20we%20reveal%20that%20LLMs%20fail%20to%20harness%0Athe%20critical%20factual%20associations%20under%20certain%20circumstances.%20Instead%2C%20they%0Atend%20to%20opt%20for%20alternative%2C%20shortcut-like%20pathways%20to%20answer%20reasoning%0Aquestions.%20By%20manually%20manipulating%20the%20recall%20process%20of%20parametric%20knowledge%0Ain%20LLMs%2C%20we%20demonstrate%20that%20enhancing%20this%20recall%20process%20directly%20improves%0Areasoning%20performance%20whereas%20suppressing%20it%20leads%20to%20notable%20degradation.%0AFurthermore%2C%20we%20assess%20the%20effect%20of%20Chain-of-Thought%20%28CoT%29%20prompting%2C%20a%0Apowerful%20technique%20for%20addressing%20complex%20reasoning%20tasks.%20Our%20findings%0Aindicate%20that%20CoT%20can%20intensify%20the%20recall%20of%20factual%20knowledge%20by%20encouraging%0ALLMs%20to%20engage%20in%20orderly%20and%20reliable%20reasoning.%20Furthermore%2C%20we%20explored%20how%0Acontextual%20conflicts%20affect%20the%20retrieval%20of%20facts%20during%20the%20reasoning%20process%0Ato%20gain%20a%20comprehensive%20understanding%20of%20the%20factual%20recall%20behaviors%20of%20LLMs.%0ACode%20and%20data%20will%20be%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Factual%2520Recall%2520Behaviors%2520of%2520Large%2520Language%2520Models%2520through%250A%2520%2520Knowledge%2520Neurons%26entry.906535625%3DYifei%2520Wang%2520and%2520Yuheng%2520Chen%2520and%2520Wanting%2520Wen%2520and%2520Yu%2520Sheng%2520and%2520Linjing%2520Li%2520and%2520Daniel%2520Dajun%2520Zeng%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520whether%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520actively%250Arecall%2520or%2520retrieve%2520their%2520internal%2520repositories%2520of%2520factual%2520knowledge%2520when%2520faced%250Awith%2520reasoning%2520tasks.%2520Through%2520an%2520analysis%2520of%2520LLMs%2527%2520internal%2520factual%2520recall%2520at%250Aeach%2520reasoning%2520step%2520via%2520Knowledge%2520Neurons%252C%2520we%2520reveal%2520that%2520LLMs%2520fail%2520to%2520harness%250Athe%2520critical%2520factual%2520associations%2520under%2520certain%2520circumstances.%2520Instead%252C%2520they%250Atend%2520to%2520opt%2520for%2520alternative%252C%2520shortcut-like%2520pathways%2520to%2520answer%2520reasoning%250Aquestions.%2520By%2520manually%2520manipulating%2520the%2520recall%2520process%2520of%2520parametric%2520knowledge%250Ain%2520LLMs%252C%2520we%2520demonstrate%2520that%2520enhancing%2520this%2520recall%2520process%2520directly%2520improves%250Areasoning%2520performance%2520whereas%2520suppressing%2520it%2520leads%2520to%2520notable%2520degradation.%250AFurthermore%252C%2520we%2520assess%2520the%2520effect%2520of%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%252C%2520a%250Apowerful%2520technique%2520for%2520addressing%2520complex%2520reasoning%2520tasks.%2520Our%2520findings%250Aindicate%2520that%2520CoT%2520can%2520intensify%2520the%2520recall%2520of%2520factual%2520knowledge%2520by%2520encouraging%250ALLMs%2520to%2520engage%2520in%2520orderly%2520and%2520reliable%2520reasoning.%2520Furthermore%252C%2520we%2520explored%2520how%250Acontextual%2520conflicts%2520affect%2520the%2520retrieval%2520of%2520facts%2520during%2520the%2520reasoning%2520process%250Ato%2520gain%2520a%2520comprehensive%2520understanding%2520of%2520the%2520factual%2520recall%2520behaviors%2520of%2520LLMs.%250ACode%2520and%2520data%2520will%2520be%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Factual%20Recall%20Behaviors%20of%20Large%20Language%20Models%20through%0A%20%20Knowledge%20Neurons&entry.906535625=Yifei%20Wang%20and%20Yuheng%20Chen%20and%20Wanting%20Wen%20and%20Yu%20Sheng%20and%20Linjing%20Li%20and%20Daniel%20Dajun%20Zeng&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20whether%20Large%20Language%20Models%20%28LLMs%29%20actively%0Arecall%20or%20retrieve%20their%20internal%20repositories%20of%20factual%20knowledge%20when%20faced%0Awith%20reasoning%20tasks.%20Through%20an%20analysis%20of%20LLMs%27%20internal%20factual%20recall%20at%0Aeach%20reasoning%20step%20via%20Knowledge%20Neurons%2C%20we%20reveal%20that%20LLMs%20fail%20to%20harness%0Athe%20critical%20factual%20associations%20under%20certain%20circumstances.%20Instead%2C%20they%0Atend%20to%20opt%20for%20alternative%2C%20shortcut-like%20pathways%20to%20answer%20reasoning%0Aquestions.%20By%20manually%20manipulating%20the%20recall%20process%20of%20parametric%20knowledge%0Ain%20LLMs%2C%20we%20demonstrate%20that%20enhancing%20this%20recall%20process%20directly%20improves%0Areasoning%20performance%20whereas%20suppressing%20it%20leads%20to%20notable%20degradation.%0AFurthermore%2C%20we%20assess%20the%20effect%20of%20Chain-of-Thought%20%28CoT%29%20prompting%2C%20a%0Apowerful%20technique%20for%20addressing%20complex%20reasoning%20tasks.%20Our%20findings%0Aindicate%20that%20CoT%20can%20intensify%20the%20recall%20of%20factual%20knowledge%20by%20encouraging%0ALLMs%20to%20engage%20in%20orderly%20and%20reliable%20reasoning.%20Furthermore%2C%20we%20explored%20how%0Acontextual%20conflicts%20affect%20the%20retrieval%20of%20facts%20during%20the%20reasoning%20process%0Ato%20gain%20a%20comprehensive%20understanding%20of%20the%20factual%20recall%20behaviors%20of%20LLMs.%0ACode%20and%20data%20will%20be%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03247v1&entry.124074799=Read"},
{"title": "Think Fast, Think Slow, Think Critical: Designing an Automated\n  Propaganda Detection Tool", "author": "Liudmila Zavolokina and Kilian Sprenkamp and Zoya Katashinskaya and Daniel Gordon Jones and Gerhard Schwabe", "abstract": "  In today's digital age, characterized by rapid news consumption and\nincreasing vulnerability to propaganda, fostering citizens' critical thinking\nis crucial for stable democracies. This paper introduces the design of\nClarifAI, a novel automated propaganda detection tool designed to nudge readers\ntowards more critical news consumption by activating the analytical mode of\nthinking, following Kahneman's dual-system theory of cognition. Using Large\nLanguage Models, ClarifAI detects propaganda in news articles and provides\ncontext-rich explanations, enhancing users' understanding and critical\nthinking. Our contribution is threefold: first, we propose the design of\nClarifAI; second, in an online experiment, we demonstrate that this design\neffectively encourages news readers to engage in more critical reading; and\nthird, we emphasize the value of explanations for fostering critical thinking.\nThe study thus offers both a practical tool and useful design knowledge for\nmitigating propaganda in digital news.\n", "link": "http://arxiv.org/abs/2402.19135v2", "date": "2024-08-06", "relevancy": 1.3274, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4464}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.44}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20Fast%2C%20Think%20Slow%2C%20Think%20Critical%3A%20Designing%20an%20Automated%0A%20%20Propaganda%20Detection%20Tool&body=Title%3A%20Think%20Fast%2C%20Think%20Slow%2C%20Think%20Critical%3A%20Designing%20an%20Automated%0A%20%20Propaganda%20Detection%20Tool%0AAuthor%3A%20Liudmila%20Zavolokina%20and%20Kilian%20Sprenkamp%20and%20Zoya%20Katashinskaya%20and%20Daniel%20Gordon%20Jones%20and%20Gerhard%20Schwabe%0AAbstract%3A%20%20%20In%20today%27s%20digital%20age%2C%20characterized%20by%20rapid%20news%20consumption%20and%0Aincreasing%20vulnerability%20to%20propaganda%2C%20fostering%20citizens%27%20critical%20thinking%0Ais%20crucial%20for%20stable%20democracies.%20This%20paper%20introduces%20the%20design%20of%0AClarifAI%2C%20a%20novel%20automated%20propaganda%20detection%20tool%20designed%20to%20nudge%20readers%0Atowards%20more%20critical%20news%20consumption%20by%20activating%20the%20analytical%20mode%20of%0Athinking%2C%20following%20Kahneman%27s%20dual-system%20theory%20of%20cognition.%20Using%20Large%0ALanguage%20Models%2C%20ClarifAI%20detects%20propaganda%20in%20news%20articles%20and%20provides%0Acontext-rich%20explanations%2C%20enhancing%20users%27%20understanding%20and%20critical%0Athinking.%20Our%20contribution%20is%20threefold%3A%20first%2C%20we%20propose%20the%20design%20of%0AClarifAI%3B%20second%2C%20in%20an%20online%20experiment%2C%20we%20demonstrate%20that%20this%20design%0Aeffectively%20encourages%20news%20readers%20to%20engage%20in%20more%20critical%20reading%3B%20and%0Athird%2C%20we%20emphasize%20the%20value%20of%20explanations%20for%20fostering%20critical%20thinking.%0AThe%20study%20thus%20offers%20both%20a%20practical%20tool%20and%20useful%20design%20knowledge%20for%0Amitigating%20propaganda%20in%20digital%20news.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19135v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520Fast%252C%2520Think%2520Slow%252C%2520Think%2520Critical%253A%2520Designing%2520an%2520Automated%250A%2520%2520Propaganda%2520Detection%2520Tool%26entry.906535625%3DLiudmila%2520Zavolokina%2520and%2520Kilian%2520Sprenkamp%2520and%2520Zoya%2520Katashinskaya%2520and%2520Daniel%2520Gordon%2520Jones%2520and%2520Gerhard%2520Schwabe%26entry.1292438233%3D%2520%2520In%2520today%2527s%2520digital%2520age%252C%2520characterized%2520by%2520rapid%2520news%2520consumption%2520and%250Aincreasing%2520vulnerability%2520to%2520propaganda%252C%2520fostering%2520citizens%2527%2520critical%2520thinking%250Ais%2520crucial%2520for%2520stable%2520democracies.%2520This%2520paper%2520introduces%2520the%2520design%2520of%250AClarifAI%252C%2520a%2520novel%2520automated%2520propaganda%2520detection%2520tool%2520designed%2520to%2520nudge%2520readers%250Atowards%2520more%2520critical%2520news%2520consumption%2520by%2520activating%2520the%2520analytical%2520mode%2520of%250Athinking%252C%2520following%2520Kahneman%2527s%2520dual-system%2520theory%2520of%2520cognition.%2520Using%2520Large%250ALanguage%2520Models%252C%2520ClarifAI%2520detects%2520propaganda%2520in%2520news%2520articles%2520and%2520provides%250Acontext-rich%2520explanations%252C%2520enhancing%2520users%2527%2520understanding%2520and%2520critical%250Athinking.%2520Our%2520contribution%2520is%2520threefold%253A%2520first%252C%2520we%2520propose%2520the%2520design%2520of%250AClarifAI%253B%2520second%252C%2520in%2520an%2520online%2520experiment%252C%2520we%2520demonstrate%2520that%2520this%2520design%250Aeffectively%2520encourages%2520news%2520readers%2520to%2520engage%2520in%2520more%2520critical%2520reading%253B%2520and%250Athird%252C%2520we%2520emphasize%2520the%2520value%2520of%2520explanations%2520for%2520fostering%2520critical%2520thinking.%250AThe%2520study%2520thus%2520offers%2520both%2520a%2520practical%2520tool%2520and%2520useful%2520design%2520knowledge%2520for%250Amitigating%2520propaganda%2520in%2520digital%2520news.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19135v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20Fast%2C%20Think%20Slow%2C%20Think%20Critical%3A%20Designing%20an%20Automated%0A%20%20Propaganda%20Detection%20Tool&entry.906535625=Liudmila%20Zavolokina%20and%20Kilian%20Sprenkamp%20and%20Zoya%20Katashinskaya%20and%20Daniel%20Gordon%20Jones%20and%20Gerhard%20Schwabe&entry.1292438233=%20%20In%20today%27s%20digital%20age%2C%20characterized%20by%20rapid%20news%20consumption%20and%0Aincreasing%20vulnerability%20to%20propaganda%2C%20fostering%20citizens%27%20critical%20thinking%0Ais%20crucial%20for%20stable%20democracies.%20This%20paper%20introduces%20the%20design%20of%0AClarifAI%2C%20a%20novel%20automated%20propaganda%20detection%20tool%20designed%20to%20nudge%20readers%0Atowards%20more%20critical%20news%20consumption%20by%20activating%20the%20analytical%20mode%20of%0Athinking%2C%20following%20Kahneman%27s%20dual-system%20theory%20of%20cognition.%20Using%20Large%0ALanguage%20Models%2C%20ClarifAI%20detects%20propaganda%20in%20news%20articles%20and%20provides%0Acontext-rich%20explanations%2C%20enhancing%20users%27%20understanding%20and%20critical%0Athinking.%20Our%20contribution%20is%20threefold%3A%20first%2C%20we%20propose%20the%20design%20of%0AClarifAI%3B%20second%2C%20in%20an%20online%20experiment%2C%20we%20demonstrate%20that%20this%20design%0Aeffectively%20encourages%20news%20readers%20to%20engage%20in%20more%20critical%20reading%3B%20and%0Athird%2C%20we%20emphasize%20the%20value%20of%20explanations%20for%20fostering%20critical%20thinking.%0AThe%20study%20thus%20offers%20both%20a%20practical%20tool%20and%20useful%20design%20knowledge%20for%0Amitigating%20propaganda%20in%20digital%20news.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19135v2&entry.124074799=Read"},
{"title": "Enhancing Complex Causality Extraction via Improved Subtask Interaction\n  and Knowledge Fusion", "author": "Jinglong Gao and Chen Lu and Xiao Ding and Zhongyang Li and Ting Liu and Bing Qin", "abstract": "  Event Causality Extraction (ECE) aims at extracting causal event pairs from\ntexts. Despite ChatGPT's recent success, fine-tuning small models remains the\nbest approach for the ECE task. However, existing fine-tuning based ECE methods\ncannot address all three key challenges in ECE simultaneously: 1) Complex\nCausality Extraction, where multiple causal-effect pairs occur within a single\nsentence; 2) Subtask~ Interaction, which involves modeling the mutual\ndependence between the two subtasks of ECE, i.e., extracting events and\nidentifying the causal relationship between extracted events; and 3) Knowledge\nFusion, which requires effectively fusing the knowledge in two modalities,\ni.e., the expressive pretrained language models and the structured knowledge\ngraphs. In this paper, we propose a unified ECE framework (UniCE to address all\nthree issues in ECE simultaneously. Specifically, we design a subtask\ninteraction mechanism to enable mutual interaction between the two ECE\nsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in\nthe two modalities. Furthermore, we employ separate decoders for each subtask\nto facilitate complex causality extraction. Experiments on three benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance and\noutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,\nour model can also be used to effectively improve the ECE performance of\nChatGPT via in-context learning.\n", "link": "http://arxiv.org/abs/2408.03079v1", "date": "2024-08-06", "relevancy": 1.4374, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.489}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4767}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Complex%20Causality%20Extraction%20via%20Improved%20Subtask%20Interaction%0A%20%20and%20Knowledge%20Fusion&body=Title%3A%20Enhancing%20Complex%20Causality%20Extraction%20via%20Improved%20Subtask%20Interaction%0A%20%20and%20Knowledge%20Fusion%0AAuthor%3A%20Jinglong%20Gao%20and%20Chen%20Lu%20and%20Xiao%20Ding%20and%20Zhongyang%20Li%20and%20Ting%20Liu%20and%20Bing%20Qin%0AAbstract%3A%20%20%20Event%20Causality%20Extraction%20%28ECE%29%20aims%20at%20extracting%20causal%20event%20pairs%20from%0Atexts.%20Despite%20ChatGPT%27s%20recent%20success%2C%20fine-tuning%20small%20models%20remains%20the%0Abest%20approach%20for%20the%20ECE%20task.%20However%2C%20existing%20fine-tuning%20based%20ECE%20methods%0Acannot%20address%20all%20three%20key%20challenges%20in%20ECE%20simultaneously%3A%201%29%20Complex%0ACausality%20Extraction%2C%20where%20multiple%20causal-effect%20pairs%20occur%20within%20a%20single%0Asentence%3B%202%29%20Subtask~%20Interaction%2C%20which%20involves%20modeling%20the%20mutual%0Adependence%20between%20the%20two%20subtasks%20of%20ECE%2C%20i.e.%2C%20extracting%20events%20and%0Aidentifying%20the%20causal%20relationship%20between%20extracted%20events%3B%20and%203%29%20Knowledge%0AFusion%2C%20which%20requires%20effectively%20fusing%20the%20knowledge%20in%20two%20modalities%2C%0Ai.e.%2C%20the%20expressive%20pretrained%20language%20models%20and%20the%20structured%20knowledge%0Agraphs.%20In%20this%20paper%2C%20we%20propose%20a%20unified%20ECE%20framework%20%28UniCE%20to%20address%20all%0Athree%20issues%20in%20ECE%20simultaneously.%20Specifically%2C%20we%20design%20a%20subtask%0Ainteraction%20mechanism%20to%20enable%20mutual%20interaction%20between%20the%20two%20ECE%0Asubtasks.%20Besides%2C%20we%20design%20a%20knowledge%20fusion%20mechanism%20to%20fuse%20knowledge%20in%0Athe%20two%20modalities.%20Furthermore%2C%20we%20employ%20separate%20decoders%20for%20each%20subtask%0Ato%20facilitate%20complex%20causality%20extraction.%20Experiments%20on%20three%20benchmark%0Adatasets%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20and%0Aoutperforms%20ChatGPT%20with%20a%20margin%20of%20at%20least%2030%25%20F1-score.%20More%20importantly%2C%0Aour%20model%20can%20also%20be%20used%20to%20effectively%20improve%20the%20ECE%20performance%20of%0AChatGPT%20via%20in-context%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Complex%2520Causality%2520Extraction%2520via%2520Improved%2520Subtask%2520Interaction%250A%2520%2520and%2520Knowledge%2520Fusion%26entry.906535625%3DJinglong%2520Gao%2520and%2520Chen%2520Lu%2520and%2520Xiao%2520Ding%2520and%2520Zhongyang%2520Li%2520and%2520Ting%2520Liu%2520and%2520Bing%2520Qin%26entry.1292438233%3D%2520%2520Event%2520Causality%2520Extraction%2520%2528ECE%2529%2520aims%2520at%2520extracting%2520causal%2520event%2520pairs%2520from%250Atexts.%2520Despite%2520ChatGPT%2527s%2520recent%2520success%252C%2520fine-tuning%2520small%2520models%2520remains%2520the%250Abest%2520approach%2520for%2520the%2520ECE%2520task.%2520However%252C%2520existing%2520fine-tuning%2520based%2520ECE%2520methods%250Acannot%2520address%2520all%2520three%2520key%2520challenges%2520in%2520ECE%2520simultaneously%253A%25201%2529%2520Complex%250ACausality%2520Extraction%252C%2520where%2520multiple%2520causal-effect%2520pairs%2520occur%2520within%2520a%2520single%250Asentence%253B%25202%2529%2520Subtask~%2520Interaction%252C%2520which%2520involves%2520modeling%2520the%2520mutual%250Adependence%2520between%2520the%2520two%2520subtasks%2520of%2520ECE%252C%2520i.e.%252C%2520extracting%2520events%2520and%250Aidentifying%2520the%2520causal%2520relationship%2520between%2520extracted%2520events%253B%2520and%25203%2529%2520Knowledge%250AFusion%252C%2520which%2520requires%2520effectively%2520fusing%2520the%2520knowledge%2520in%2520two%2520modalities%252C%250Ai.e.%252C%2520the%2520expressive%2520pretrained%2520language%2520models%2520and%2520the%2520structured%2520knowledge%250Agraphs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520unified%2520ECE%2520framework%2520%2528UniCE%2520to%2520address%2520all%250Athree%2520issues%2520in%2520ECE%2520simultaneously.%2520Specifically%252C%2520we%2520design%2520a%2520subtask%250Ainteraction%2520mechanism%2520to%2520enable%2520mutual%2520interaction%2520between%2520the%2520two%2520ECE%250Asubtasks.%2520Besides%252C%2520we%2520design%2520a%2520knowledge%2520fusion%2520mechanism%2520to%2520fuse%2520knowledge%2520in%250Athe%2520two%2520modalities.%2520Furthermore%252C%2520we%2520employ%2520separate%2520decoders%2520for%2520each%2520subtask%250Ato%2520facilitate%2520complex%2520causality%2520extraction.%2520Experiments%2520on%2520three%2520benchmark%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520and%250Aoutperforms%2520ChatGPT%2520with%2520a%2520margin%2520of%2520at%2520least%252030%2525%2520F1-score.%2520More%2520importantly%252C%250Aour%2520model%2520can%2520also%2520be%2520used%2520to%2520effectively%2520improve%2520the%2520ECE%2520performance%2520of%250AChatGPT%2520via%2520in-context%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Complex%20Causality%20Extraction%20via%20Improved%20Subtask%20Interaction%0A%20%20and%20Knowledge%20Fusion&entry.906535625=Jinglong%20Gao%20and%20Chen%20Lu%20and%20Xiao%20Ding%20and%20Zhongyang%20Li%20and%20Ting%20Liu%20and%20Bing%20Qin&entry.1292438233=%20%20Event%20Causality%20Extraction%20%28ECE%29%20aims%20at%20extracting%20causal%20event%20pairs%20from%0Atexts.%20Despite%20ChatGPT%27s%20recent%20success%2C%20fine-tuning%20small%20models%20remains%20the%0Abest%20approach%20for%20the%20ECE%20task.%20However%2C%20existing%20fine-tuning%20based%20ECE%20methods%0Acannot%20address%20all%20three%20key%20challenges%20in%20ECE%20simultaneously%3A%201%29%20Complex%0ACausality%20Extraction%2C%20where%20multiple%20causal-effect%20pairs%20occur%20within%20a%20single%0Asentence%3B%202%29%20Subtask~%20Interaction%2C%20which%20involves%20modeling%20the%20mutual%0Adependence%20between%20the%20two%20subtasks%20of%20ECE%2C%20i.e.%2C%20extracting%20events%20and%0Aidentifying%20the%20causal%20relationship%20between%20extracted%20events%3B%20and%203%29%20Knowledge%0AFusion%2C%20which%20requires%20effectively%20fusing%20the%20knowledge%20in%20two%20modalities%2C%0Ai.e.%2C%20the%20expressive%20pretrained%20language%20models%20and%20the%20structured%20knowledge%0Agraphs.%20In%20this%20paper%2C%20we%20propose%20a%20unified%20ECE%20framework%20%28UniCE%20to%20address%20all%0Athree%20issues%20in%20ECE%20simultaneously.%20Specifically%2C%20we%20design%20a%20subtask%0Ainteraction%20mechanism%20to%20enable%20mutual%20interaction%20between%20the%20two%20ECE%0Asubtasks.%20Besides%2C%20we%20design%20a%20knowledge%20fusion%20mechanism%20to%20fuse%20knowledge%20in%0Athe%20two%20modalities.%20Furthermore%2C%20we%20employ%20separate%20decoders%20for%20each%20subtask%0Ato%20facilitate%20complex%20causality%20extraction.%20Experiments%20on%20three%20benchmark%0Adatasets%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20and%0Aoutperforms%20ChatGPT%20with%20a%20margin%20of%20at%20least%2030%25%20F1-score.%20More%20importantly%2C%0Aour%20model%20can%20also%20be%20used%20to%20effectively%20improve%20the%20ECE%20performance%20of%0AChatGPT%20via%20in-context%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03079v1&entry.124074799=Read"},
{"title": "Matrix Multiplication on Quantum Computer", "author": "Jiaqi Yao and Ding Liu", "abstract": "  This paper introduces an innovative and practical approach to universal\nquantum matrix multiplication. We designed optimized quantum adders and\nmultipliers based on Quantum Fourier Transform (QFT), which significantly\nreduced the number of gates used compared to classical adders and multipliers.\nSubsequently, we construct a basic universal quantum matrix multiplication and\nextend it to the Strassen algorithm. We conduct comparative experiments to\nanalyze the performance of the quantum matrix multiplication and evaluate the\nacceleration provided by the optimized quantum adder and multiplier.\nFurthermore, we investigate the advantages and disadvantages of the quantum\nStrassen algorithm compared to basic quantum matrix multiplication.\n", "link": "http://arxiv.org/abs/2408.03085v1", "date": "2024-08-06", "relevancy": 1.0329, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.353}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3449}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matrix%20Multiplication%20on%20Quantum%20Computer&body=Title%3A%20Matrix%20Multiplication%20on%20Quantum%20Computer%0AAuthor%3A%20Jiaqi%20Yao%20and%20Ding%20Liu%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20innovative%20and%20practical%20approach%20to%20universal%0Aquantum%20matrix%20multiplication.%20We%20designed%20optimized%20quantum%20adders%20and%0Amultipliers%20based%20on%20Quantum%20Fourier%20Transform%20%28QFT%29%2C%20which%20significantly%0Areduced%20the%20number%20of%20gates%20used%20compared%20to%20classical%20adders%20and%20multipliers.%0ASubsequently%2C%20we%20construct%20a%20basic%20universal%20quantum%20matrix%20multiplication%20and%0Aextend%20it%20to%20the%20Strassen%20algorithm.%20We%20conduct%20comparative%20experiments%20to%0Aanalyze%20the%20performance%20of%20the%20quantum%20matrix%20multiplication%20and%20evaluate%20the%0Aacceleration%20provided%20by%20the%20optimized%20quantum%20adder%20and%20multiplier.%0AFurthermore%2C%20we%20investigate%20the%20advantages%20and%20disadvantages%20of%20the%20quantum%0AStrassen%20algorithm%20compared%20to%20basic%20quantum%20matrix%20multiplication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatrix%2520Multiplication%2520on%2520Quantum%2520Computer%26entry.906535625%3DJiaqi%2520Yao%2520and%2520Ding%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520innovative%2520and%2520practical%2520approach%2520to%2520universal%250Aquantum%2520matrix%2520multiplication.%2520We%2520designed%2520optimized%2520quantum%2520adders%2520and%250Amultipliers%2520based%2520on%2520Quantum%2520Fourier%2520Transform%2520%2528QFT%2529%252C%2520which%2520significantly%250Areduced%2520the%2520number%2520of%2520gates%2520used%2520compared%2520to%2520classical%2520adders%2520and%2520multipliers.%250ASubsequently%252C%2520we%2520construct%2520a%2520basic%2520universal%2520quantum%2520matrix%2520multiplication%2520and%250Aextend%2520it%2520to%2520the%2520Strassen%2520algorithm.%2520We%2520conduct%2520comparative%2520experiments%2520to%250Aanalyze%2520the%2520performance%2520of%2520the%2520quantum%2520matrix%2520multiplication%2520and%2520evaluate%2520the%250Aacceleration%2520provided%2520by%2520the%2520optimized%2520quantum%2520adder%2520and%2520multiplier.%250AFurthermore%252C%2520we%2520investigate%2520the%2520advantages%2520and%2520disadvantages%2520of%2520the%2520quantum%250AStrassen%2520algorithm%2520compared%2520to%2520basic%2520quantum%2520matrix%2520multiplication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matrix%20Multiplication%20on%20Quantum%20Computer&entry.906535625=Jiaqi%20Yao%20and%20Ding%20Liu&entry.1292438233=%20%20This%20paper%20introduces%20an%20innovative%20and%20practical%20approach%20to%20universal%0Aquantum%20matrix%20multiplication.%20We%20designed%20optimized%20quantum%20adders%20and%0Amultipliers%20based%20on%20Quantum%20Fourier%20Transform%20%28QFT%29%2C%20which%20significantly%0Areduced%20the%20number%20of%20gates%20used%20compared%20to%20classical%20adders%20and%20multipliers.%0ASubsequently%2C%20we%20construct%20a%20basic%20universal%20quantum%20matrix%20multiplication%20and%0Aextend%20it%20to%20the%20Strassen%20algorithm.%20We%20conduct%20comparative%20experiments%20to%0Aanalyze%20the%20performance%20of%20the%20quantum%20matrix%20multiplication%20and%20evaluate%20the%0Aacceleration%20provided%20by%20the%20optimized%20quantum%20adder%20and%20multiplier.%0AFurthermore%2C%20we%20investigate%20the%20advantages%20and%20disadvantages%20of%20the%20quantum%0AStrassen%20algorithm%20compared%20to%20basic%20quantum%20matrix%20multiplication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03085v1&entry.124074799=Read"},
{"title": "Research on Autonomous Driving Decision-making Strategies based Deep\n  Reinforcement Learning", "author": "Zixiang Wang and Hao Yan and Changsong Wei and Junyu Wang and Shi Bo and Minheng Xiao", "abstract": "  The behavior decision-making subsystem is a key component of the autonomous\ndriving system, which reflects the decision-making ability of the vehicle and\nthe driver, and is an important symbol of the high-level intelligence of the\nvehicle. However, the existing rule-based decision-making schemes are limited\nby the prior knowledge of designers, and it is difficult to cope with complex\nand changeable traffic scenarios. In this work, an advanced deep reinforcement\nlearning model is adopted, which can autonomously learn and optimize driving\nstrategies in a complex and changeable traffic environment by modeling the\ndriving decision-making process as a reinforcement learning problem.\nSpecifically, we used Deep Q-Network (DQN) and Proximal Policy Optimization\n(PPO) for comparative experiments. DQN guides the agent to choose the best\naction by approximating the state-action value function, while PPO improves the\ndecision-making quality by optimizing the policy function. We also introduce\nimprovements in the design of the reward function to promote the robustness and\nadaptability of the model in real-world driving situations. Experimental\nresults show that the decision-making strategy based on deep reinforcement\nlearning has better performance than the traditional rule-based method in a\nvariety of driving tasks.\n", "link": "http://arxiv.org/abs/2408.03084v1", "date": "2024-08-06", "relevancy": 1.5296, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.518}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.502}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20on%20Autonomous%20Driving%20Decision-making%20Strategies%20based%20Deep%0A%20%20Reinforcement%20Learning&body=Title%3A%20Research%20on%20Autonomous%20Driving%20Decision-making%20Strategies%20based%20Deep%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Zixiang%20Wang%20and%20Hao%20Yan%20and%20Changsong%20Wei%20and%20Junyu%20Wang%20and%20Shi%20Bo%20and%20Minheng%20Xiao%0AAbstract%3A%20%20%20The%20behavior%20decision-making%20subsystem%20is%20a%20key%20component%20of%20the%20autonomous%0Adriving%20system%2C%20which%20reflects%20the%20decision-making%20ability%20of%20the%20vehicle%20and%0Athe%20driver%2C%20and%20is%20an%20important%20symbol%20of%20the%20high-level%20intelligence%20of%20the%0Avehicle.%20However%2C%20the%20existing%20rule-based%20decision-making%20schemes%20are%20limited%0Aby%20the%20prior%20knowledge%20of%20designers%2C%20and%20it%20is%20difficult%20to%20cope%20with%20complex%0Aand%20changeable%20traffic%20scenarios.%20In%20this%20work%2C%20an%20advanced%20deep%20reinforcement%0Alearning%20model%20is%20adopted%2C%20which%20can%20autonomously%20learn%20and%20optimize%20driving%0Astrategies%20in%20a%20complex%20and%20changeable%20traffic%20environment%20by%20modeling%20the%0Adriving%20decision-making%20process%20as%20a%20reinforcement%20learning%20problem.%0ASpecifically%2C%20we%20used%20Deep%20Q-Network%20%28DQN%29%20and%20Proximal%20Policy%20Optimization%0A%28PPO%29%20for%20comparative%20experiments.%20DQN%20guides%20the%20agent%20to%20choose%20the%20best%0Aaction%20by%20approximating%20the%20state-action%20value%20function%2C%20while%20PPO%20improves%20the%0Adecision-making%20quality%20by%20optimizing%20the%20policy%20function.%20We%20also%20introduce%0Aimprovements%20in%20the%20design%20of%20the%20reward%20function%20to%20promote%20the%20robustness%20and%0Aadaptability%20of%20the%20model%20in%20real-world%20driving%20situations.%20Experimental%0Aresults%20show%20that%20the%20decision-making%20strategy%20based%20on%20deep%20reinforcement%0Alearning%20has%20better%20performance%20than%20the%20traditional%20rule-based%20method%20in%20a%0Avariety%20of%20driving%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520on%2520Autonomous%2520Driving%2520Decision-making%2520Strategies%2520based%2520Deep%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DZixiang%2520Wang%2520and%2520Hao%2520Yan%2520and%2520Changsong%2520Wei%2520and%2520Junyu%2520Wang%2520and%2520Shi%2520Bo%2520and%2520Minheng%2520Xiao%26entry.1292438233%3D%2520%2520The%2520behavior%2520decision-making%2520subsystem%2520is%2520a%2520key%2520component%2520of%2520the%2520autonomous%250Adriving%2520system%252C%2520which%2520reflects%2520the%2520decision-making%2520ability%2520of%2520the%2520vehicle%2520and%250Athe%2520driver%252C%2520and%2520is%2520an%2520important%2520symbol%2520of%2520the%2520high-level%2520intelligence%2520of%2520the%250Avehicle.%2520However%252C%2520the%2520existing%2520rule-based%2520decision-making%2520schemes%2520are%2520limited%250Aby%2520the%2520prior%2520knowledge%2520of%2520designers%252C%2520and%2520it%2520is%2520difficult%2520to%2520cope%2520with%2520complex%250Aand%2520changeable%2520traffic%2520scenarios.%2520In%2520this%2520work%252C%2520an%2520advanced%2520deep%2520reinforcement%250Alearning%2520model%2520is%2520adopted%252C%2520which%2520can%2520autonomously%2520learn%2520and%2520optimize%2520driving%250Astrategies%2520in%2520a%2520complex%2520and%2520changeable%2520traffic%2520environment%2520by%2520modeling%2520the%250Adriving%2520decision-making%2520process%2520as%2520a%2520reinforcement%2520learning%2520problem.%250ASpecifically%252C%2520we%2520used%2520Deep%2520Q-Network%2520%2528DQN%2529%2520and%2520Proximal%2520Policy%2520Optimization%250A%2528PPO%2529%2520for%2520comparative%2520experiments.%2520DQN%2520guides%2520the%2520agent%2520to%2520choose%2520the%2520best%250Aaction%2520by%2520approximating%2520the%2520state-action%2520value%2520function%252C%2520while%2520PPO%2520improves%2520the%250Adecision-making%2520quality%2520by%2520optimizing%2520the%2520policy%2520function.%2520We%2520also%2520introduce%250Aimprovements%2520in%2520the%2520design%2520of%2520the%2520reward%2520function%2520to%2520promote%2520the%2520robustness%2520and%250Aadaptability%2520of%2520the%2520model%2520in%2520real-world%2520driving%2520situations.%2520Experimental%250Aresults%2520show%2520that%2520the%2520decision-making%2520strategy%2520based%2520on%2520deep%2520reinforcement%250Alearning%2520has%2520better%2520performance%2520than%2520the%2520traditional%2520rule-based%2520method%2520in%2520a%250Avariety%2520of%2520driving%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20Autonomous%20Driving%20Decision-making%20Strategies%20based%20Deep%0A%20%20Reinforcement%20Learning&entry.906535625=Zixiang%20Wang%20and%20Hao%20Yan%20and%20Changsong%20Wei%20and%20Junyu%20Wang%20and%20Shi%20Bo%20and%20Minheng%20Xiao&entry.1292438233=%20%20The%20behavior%20decision-making%20subsystem%20is%20a%20key%20component%20of%20the%20autonomous%0Adriving%20system%2C%20which%20reflects%20the%20decision-making%20ability%20of%20the%20vehicle%20and%0Athe%20driver%2C%20and%20is%20an%20important%20symbol%20of%20the%20high-level%20intelligence%20of%20the%0Avehicle.%20However%2C%20the%20existing%20rule-based%20decision-making%20schemes%20are%20limited%0Aby%20the%20prior%20knowledge%20of%20designers%2C%20and%20it%20is%20difficult%20to%20cope%20with%20complex%0Aand%20changeable%20traffic%20scenarios.%20In%20this%20work%2C%20an%20advanced%20deep%20reinforcement%0Alearning%20model%20is%20adopted%2C%20which%20can%20autonomously%20learn%20and%20optimize%20driving%0Astrategies%20in%20a%20complex%20and%20changeable%20traffic%20environment%20by%20modeling%20the%0Adriving%20decision-making%20process%20as%20a%20reinforcement%20learning%20problem.%0ASpecifically%2C%20we%20used%20Deep%20Q-Network%20%28DQN%29%20and%20Proximal%20Policy%20Optimization%0A%28PPO%29%20for%20comparative%20experiments.%20DQN%20guides%20the%20agent%20to%20choose%20the%20best%0Aaction%20by%20approximating%20the%20state-action%20value%20function%2C%20while%20PPO%20improves%20the%0Adecision-making%20quality%20by%20optimizing%20the%20policy%20function.%20We%20also%20introduce%0Aimprovements%20in%20the%20design%20of%20the%20reward%20function%20to%20promote%20the%20robustness%20and%0Aadaptability%20of%20the%20model%20in%20real-world%20driving%20situations.%20Experimental%0Aresults%20show%20that%20the%20decision-making%20strategy%20based%20on%20deep%20reinforcement%0Alearning%20has%20better%20performance%20than%20the%20traditional%20rule-based%20method%20in%20a%0Avariety%20of%20driving%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03084v1&entry.124074799=Read"},
{"title": "Biomedical SAM 2: Segment Anything in Biomedical Images and Videos", "author": "Zhiling Yan and Weixiang Sun and Rong Zhou and Zhengqing Yuan and Kai Zhang and Yiwei Li and Tianming Liu and Quanzheng Li and Xiang Li and Lifang He and Lichao Sun", "abstract": "  Medical image segmentation and video object segmentation are essential for\ndiagnosing and analyzing diseases by identifying and measuring biological\nstructures. Recent advances in natural domain have been driven by foundation\nmodels like the Segment Anything Model 2 (SAM 2). To explore the performance of\nSAM 2 in biomedical applications, we designed two evaluation pipelines for\nsingle-frame image segmentation and multi-frame video segmentation with varied\nprompt designs, revealing SAM 2's limitations in medical contexts.\nConsequently, we developed BioSAM 2, an enhanced foundation model optimized for\nbiomedical data based on SAM 2. Our experiments show that BioSAM 2 not only\nsurpasses the performance of existing state-of-the-art foundation models but\nalso matches or even exceeds specialist models, demonstrating its efficacy and\npotential in the medical domain.\n", "link": "http://arxiv.org/abs/2408.03286v1", "date": "2024-08-06", "relevancy": 1.5459, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4873}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biomedical%20SAM%202%3A%20Segment%20Anything%20in%20Biomedical%20Images%20and%20Videos&body=Title%3A%20Biomedical%20SAM%202%3A%20Segment%20Anything%20in%20Biomedical%20Images%20and%20Videos%0AAuthor%3A%20Zhiling%20Yan%20and%20Weixiang%20Sun%20and%20Rong%20Zhou%20and%20Zhengqing%20Yuan%20and%20Kai%20Zhang%20and%20Yiwei%20Li%20and%20Tianming%20Liu%20and%20Quanzheng%20Li%20and%20Xiang%20Li%20and%20Lifang%20He%20and%20Lichao%20Sun%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20and%20video%20object%20segmentation%20are%20essential%20for%0Adiagnosing%20and%20analyzing%20diseases%20by%20identifying%20and%20measuring%20biological%0Astructures.%20Recent%20advances%20in%20natural%20domain%20have%20been%20driven%20by%20foundation%0Amodels%20like%20the%20Segment%20Anything%20Model%202%20%28SAM%202%29.%20To%20explore%20the%20performance%20of%0ASAM%202%20in%20biomedical%20applications%2C%20we%20designed%20two%20evaluation%20pipelines%20for%0Asingle-frame%20image%20segmentation%20and%20multi-frame%20video%20segmentation%20with%20varied%0Aprompt%20designs%2C%20revealing%20SAM%202%27s%20limitations%20in%20medical%20contexts.%0AConsequently%2C%20we%20developed%20BioSAM%202%2C%20an%20enhanced%20foundation%20model%20optimized%20for%0Abiomedical%20data%20based%20on%20SAM%202.%20Our%20experiments%20show%20that%20BioSAM%202%20not%20only%0Asurpasses%20the%20performance%20of%20existing%20state-of-the-art%20foundation%20models%20but%0Aalso%20matches%20or%20even%20exceeds%20specialist%20models%2C%20demonstrating%20its%20efficacy%20and%0Apotential%20in%20the%20medical%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiomedical%2520SAM%25202%253A%2520Segment%2520Anything%2520in%2520Biomedical%2520Images%2520and%2520Videos%26entry.906535625%3DZhiling%2520Yan%2520and%2520Weixiang%2520Sun%2520and%2520Rong%2520Zhou%2520and%2520Zhengqing%2520Yuan%2520and%2520Kai%2520Zhang%2520and%2520Yiwei%2520Li%2520and%2520Tianming%2520Liu%2520and%2520Quanzheng%2520Li%2520and%2520Xiang%2520Li%2520and%2520Lifang%2520He%2520and%2520Lichao%2520Sun%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520and%2520video%2520object%2520segmentation%2520are%2520essential%2520for%250Adiagnosing%2520and%2520analyzing%2520diseases%2520by%2520identifying%2520and%2520measuring%2520biological%250Astructures.%2520Recent%2520advances%2520in%2520natural%2520domain%2520have%2520been%2520driven%2520by%2520foundation%250Amodels%2520like%2520the%2520Segment%2520Anything%2520Model%25202%2520%2528SAM%25202%2529.%2520To%2520explore%2520the%2520performance%2520of%250ASAM%25202%2520in%2520biomedical%2520applications%252C%2520we%2520designed%2520two%2520evaluation%2520pipelines%2520for%250Asingle-frame%2520image%2520segmentation%2520and%2520multi-frame%2520video%2520segmentation%2520with%2520varied%250Aprompt%2520designs%252C%2520revealing%2520SAM%25202%2527s%2520limitations%2520in%2520medical%2520contexts.%250AConsequently%252C%2520we%2520developed%2520BioSAM%25202%252C%2520an%2520enhanced%2520foundation%2520model%2520optimized%2520for%250Abiomedical%2520data%2520based%2520on%2520SAM%25202.%2520Our%2520experiments%2520show%2520that%2520BioSAM%25202%2520not%2520only%250Asurpasses%2520the%2520performance%2520of%2520existing%2520state-of-the-art%2520foundation%2520models%2520but%250Aalso%2520matches%2520or%2520even%2520exceeds%2520specialist%2520models%252C%2520demonstrating%2520its%2520efficacy%2520and%250Apotential%2520in%2520the%2520medical%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biomedical%20SAM%202%3A%20Segment%20Anything%20in%20Biomedical%20Images%20and%20Videos&entry.906535625=Zhiling%20Yan%20and%20Weixiang%20Sun%20and%20Rong%20Zhou%20and%20Zhengqing%20Yuan%20and%20Kai%20Zhang%20and%20Yiwei%20Li%20and%20Tianming%20Liu%20and%20Quanzheng%20Li%20and%20Xiang%20Li%20and%20Lifang%20He%20and%20Lichao%20Sun&entry.1292438233=%20%20Medical%20image%20segmentation%20and%20video%20object%20segmentation%20are%20essential%20for%0Adiagnosing%20and%20analyzing%20diseases%20by%20identifying%20and%20measuring%20biological%0Astructures.%20Recent%20advances%20in%20natural%20domain%20have%20been%20driven%20by%20foundation%0Amodels%20like%20the%20Segment%20Anything%20Model%202%20%28SAM%202%29.%20To%20explore%20the%20performance%20of%0ASAM%202%20in%20biomedical%20applications%2C%20we%20designed%20two%20evaluation%20pipelines%20for%0Asingle-frame%20image%20segmentation%20and%20multi-frame%20video%20segmentation%20with%20varied%0Aprompt%20designs%2C%20revealing%20SAM%202%27s%20limitations%20in%20medical%20contexts.%0AConsequently%2C%20we%20developed%20BioSAM%202%2C%20an%20enhanced%20foundation%20model%20optimized%20for%0Abiomedical%20data%20based%20on%20SAM%202.%20Our%20experiments%20show%20that%20BioSAM%202%20not%20only%0Asurpasses%20the%20performance%20of%20existing%20state-of-the-art%20foundation%20models%20but%0Aalso%20matches%20or%20even%20exceeds%20specialist%20models%2C%20demonstrating%20its%20efficacy%20and%0Apotential%20in%20the%20medical%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03286v1&entry.124074799=Read"},
{"title": "Tool Learning with Foundation Models", "author": "Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Yi Ren Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shihao Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bowen Li and Ziwei Tang and Jing Yi and Yuzhang Zhu and Zhenning Dai and Lan Yan and Xin Cong and Yaxi Lu and Weilin Zhao and Yuxiang Huang and Junxi Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun", "abstract": "  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n", "link": "http://arxiv.org/abs/2304.08354v3", "date": "2024-08-06", "relevancy": 1.4925, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5105}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4819}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tool%20Learning%20with%20Foundation%20Models&body=Title%3A%20Tool%20Learning%20with%20Foundation%20Models%0AAuthor%3A%20Yujia%20Qin%20and%20Shengding%20Hu%20and%20Yankai%20Lin%20and%20Weize%20Chen%20and%20Ning%20Ding%20and%20Ganqu%20Cui%20and%20Zheni%20Zeng%20and%20Yufei%20Huang%20and%20Chaojun%20Xiao%20and%20Chi%20Han%20and%20Yi%20Ren%20Fung%20and%20Yusheng%20Su%20and%20Huadong%20Wang%20and%20Cheng%20Qian%20and%20Runchu%20Tian%20and%20Kunlun%20Zhu%20and%20Shihao%20Liang%20and%20Xingyu%20Shen%20and%20Bokai%20Xu%20and%20Zhen%20Zhang%20and%20Yining%20Ye%20and%20Bowen%20Li%20and%20Ziwei%20Tang%20and%20Jing%20Yi%20and%20Yuzhang%20Zhu%20and%20Zhenning%20Dai%20and%20Lan%20Yan%20and%20Xin%20Cong%20and%20Yaxi%20Lu%20and%20Weilin%20Zhao%20and%20Yuxiang%20Huang%20and%20Junxi%20Yan%20and%20Xu%20Han%20and%20Xian%20Sun%20and%20Dahai%20Li%20and%20Jason%20Phang%20and%20Cheng%20Yang%20and%20Tongshuang%20Wu%20and%20Heng%20Ji%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Humans%20possess%20an%20extraordinary%20ability%20to%20create%20and%20utilize%20tools%2C%20allowing%0Athem%20to%20overcome%20physical%20limitations%20and%20explore%20new%20frontiers.%20With%20the%0Aadvent%20of%20foundation%20models%2C%20AI%20systems%20have%20the%20potential%20to%20be%20equally%20adept%0Ain%20tool%20use%20as%20humans.%20This%20paradigm%2C%20i.e.%2C%20tool%20learning%20with%20foundation%0Amodels%2C%20combines%20the%20strengths%20of%20specialized%20tools%20and%20foundation%20models%20to%0Aachieve%20enhanced%20accuracy%2C%20efficiency%2C%20and%20automation%20in%20problem-solving.%0ADespite%20its%20immense%20potential%2C%20there%20is%20still%20a%20lack%20of%20a%20comprehensive%0Aunderstanding%20of%20key%20challenges%2C%20opportunities%2C%20and%20future%20endeavors%20in%20this%0Afield.%20To%20this%20end%2C%20we%20present%20a%20systematic%20investigation%20of%20tool%20learning%20in%0Athis%20paper.%20We%20first%20introduce%20the%20background%20of%20tool%20learning%2C%20including%20its%0Acognitive%20origins%2C%20the%20paradigm%20shift%20of%20foundation%20models%2C%20and%20the%0Acomplementary%20roles%20of%20tools%20and%20models.%20Then%20we%20recapitulate%20existing%20tool%0Alearning%20research%20into%20tool-augmented%20and%20tool-oriented%20learning.%20We%20formulate%0Aa%20general%20tool%20learning%20framework%3A%20starting%20from%20understanding%20the%20user%0Ainstruction%2C%20models%20should%20learn%20to%20decompose%20a%20complex%20task%20into%20several%0Asubtasks%2C%20dynamically%20adjust%20their%20plan%20through%20reasoning%2C%20and%20effectively%0Aconquer%20each%20sub-task%20by%20selecting%20appropriate%20tools.%20We%20also%20discuss%20how%20to%0Atrain%20models%20for%20improved%20tool-use%20capabilities%20and%20facilitate%20the%0Ageneralization%20in%20tool%20learning.%20Considering%20the%20lack%20of%20a%20systematic%20tool%0Alearning%20evaluation%20in%20prior%20works%2C%20we%20experiment%20with%2018%20representative%20tools%0Aand%20show%20the%20potential%20of%20current%20foundation%20models%20in%20skillfully%20utilizing%0Atools.%20Finally%2C%20we%20discuss%20several%20open%20problems%20that%20require%20further%0Ainvestigation%20for%20tool%20learning.%20In%20general%2C%20we%20hope%20this%20paper%20could%20inspire%0Afuture%20research%20in%20integrating%20tools%20with%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.08354v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTool%2520Learning%2520with%2520Foundation%2520Models%26entry.906535625%3DYujia%2520Qin%2520and%2520Shengding%2520Hu%2520and%2520Yankai%2520Lin%2520and%2520Weize%2520Chen%2520and%2520Ning%2520Ding%2520and%2520Ganqu%2520Cui%2520and%2520Zheni%2520Zeng%2520and%2520Yufei%2520Huang%2520and%2520Chaojun%2520Xiao%2520and%2520Chi%2520Han%2520and%2520Yi%2520Ren%2520Fung%2520and%2520Yusheng%2520Su%2520and%2520Huadong%2520Wang%2520and%2520Cheng%2520Qian%2520and%2520Runchu%2520Tian%2520and%2520Kunlun%2520Zhu%2520and%2520Shihao%2520Liang%2520and%2520Xingyu%2520Shen%2520and%2520Bokai%2520Xu%2520and%2520Zhen%2520Zhang%2520and%2520Yining%2520Ye%2520and%2520Bowen%2520Li%2520and%2520Ziwei%2520Tang%2520and%2520Jing%2520Yi%2520and%2520Yuzhang%2520Zhu%2520and%2520Zhenning%2520Dai%2520and%2520Lan%2520Yan%2520and%2520Xin%2520Cong%2520and%2520Yaxi%2520Lu%2520and%2520Weilin%2520Zhao%2520and%2520Yuxiang%2520Huang%2520and%2520Junxi%2520Yan%2520and%2520Xu%2520Han%2520and%2520Xian%2520Sun%2520and%2520Dahai%2520Li%2520and%2520Jason%2520Phang%2520and%2520Cheng%2520Yang%2520and%2520Tongshuang%2520Wu%2520and%2520Heng%2520Ji%2520and%2520Zhiyuan%2520Liu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Humans%2520possess%2520an%2520extraordinary%2520ability%2520to%2520create%2520and%2520utilize%2520tools%252C%2520allowing%250Athem%2520to%2520overcome%2520physical%2520limitations%2520and%2520explore%2520new%2520frontiers.%2520With%2520the%250Aadvent%2520of%2520foundation%2520models%252C%2520AI%2520systems%2520have%2520the%2520potential%2520to%2520be%2520equally%2520adept%250Ain%2520tool%2520use%2520as%2520humans.%2520This%2520paradigm%252C%2520i.e.%252C%2520tool%2520learning%2520with%2520foundation%250Amodels%252C%2520combines%2520the%2520strengths%2520of%2520specialized%2520tools%2520and%2520foundation%2520models%2520to%250Aachieve%2520enhanced%2520accuracy%252C%2520efficiency%252C%2520and%2520automation%2520in%2520problem-solving.%250ADespite%2520its%2520immense%2520potential%252C%2520there%2520is%2520still%2520a%2520lack%2520of%2520a%2520comprehensive%250Aunderstanding%2520of%2520key%2520challenges%252C%2520opportunities%252C%2520and%2520future%2520endeavors%2520in%2520this%250Afield.%2520To%2520this%2520end%252C%2520we%2520present%2520a%2520systematic%2520investigation%2520of%2520tool%2520learning%2520in%250Athis%2520paper.%2520We%2520first%2520introduce%2520the%2520background%2520of%2520tool%2520learning%252C%2520including%2520its%250Acognitive%2520origins%252C%2520the%2520paradigm%2520shift%2520of%2520foundation%2520models%252C%2520and%2520the%250Acomplementary%2520roles%2520of%2520tools%2520and%2520models.%2520Then%2520we%2520recapitulate%2520existing%2520tool%250Alearning%2520research%2520into%2520tool-augmented%2520and%2520tool-oriented%2520learning.%2520We%2520formulate%250Aa%2520general%2520tool%2520learning%2520framework%253A%2520starting%2520from%2520understanding%2520the%2520user%250Ainstruction%252C%2520models%2520should%2520learn%2520to%2520decompose%2520a%2520complex%2520task%2520into%2520several%250Asubtasks%252C%2520dynamically%2520adjust%2520their%2520plan%2520through%2520reasoning%252C%2520and%2520effectively%250Aconquer%2520each%2520sub-task%2520by%2520selecting%2520appropriate%2520tools.%2520We%2520also%2520discuss%2520how%2520to%250Atrain%2520models%2520for%2520improved%2520tool-use%2520capabilities%2520and%2520facilitate%2520the%250Ageneralization%2520in%2520tool%2520learning.%2520Considering%2520the%2520lack%2520of%2520a%2520systematic%2520tool%250Alearning%2520evaluation%2520in%2520prior%2520works%252C%2520we%2520experiment%2520with%252018%2520representative%2520tools%250Aand%2520show%2520the%2520potential%2520of%2520current%2520foundation%2520models%2520in%2520skillfully%2520utilizing%250Atools.%2520Finally%252C%2520we%2520discuss%2520several%2520open%2520problems%2520that%2520require%2520further%250Ainvestigation%2520for%2520tool%2520learning.%2520In%2520general%252C%2520we%2520hope%2520this%2520paper%2520could%2520inspire%250Afuture%2520research%2520in%2520integrating%2520tools%2520with%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.08354v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tool%20Learning%20with%20Foundation%20Models&entry.906535625=Yujia%20Qin%20and%20Shengding%20Hu%20and%20Yankai%20Lin%20and%20Weize%20Chen%20and%20Ning%20Ding%20and%20Ganqu%20Cui%20and%20Zheni%20Zeng%20and%20Yufei%20Huang%20and%20Chaojun%20Xiao%20and%20Chi%20Han%20and%20Yi%20Ren%20Fung%20and%20Yusheng%20Su%20and%20Huadong%20Wang%20and%20Cheng%20Qian%20and%20Runchu%20Tian%20and%20Kunlun%20Zhu%20and%20Shihao%20Liang%20and%20Xingyu%20Shen%20and%20Bokai%20Xu%20and%20Zhen%20Zhang%20and%20Yining%20Ye%20and%20Bowen%20Li%20and%20Ziwei%20Tang%20and%20Jing%20Yi%20and%20Yuzhang%20Zhu%20and%20Zhenning%20Dai%20and%20Lan%20Yan%20and%20Xin%20Cong%20and%20Yaxi%20Lu%20and%20Weilin%20Zhao%20and%20Yuxiang%20Huang%20and%20Junxi%20Yan%20and%20Xu%20Han%20and%20Xian%20Sun%20and%20Dahai%20Li%20and%20Jason%20Phang%20and%20Cheng%20Yang%20and%20Tongshuang%20Wu%20and%20Heng%20Ji%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Humans%20possess%20an%20extraordinary%20ability%20to%20create%20and%20utilize%20tools%2C%20allowing%0Athem%20to%20overcome%20physical%20limitations%20and%20explore%20new%20frontiers.%20With%20the%0Aadvent%20of%20foundation%20models%2C%20AI%20systems%20have%20the%20potential%20to%20be%20equally%20adept%0Ain%20tool%20use%20as%20humans.%20This%20paradigm%2C%20i.e.%2C%20tool%20learning%20with%20foundation%0Amodels%2C%20combines%20the%20strengths%20of%20specialized%20tools%20and%20foundation%20models%20to%0Aachieve%20enhanced%20accuracy%2C%20efficiency%2C%20and%20automation%20in%20problem-solving.%0ADespite%20its%20immense%20potential%2C%20there%20is%20still%20a%20lack%20of%20a%20comprehensive%0Aunderstanding%20of%20key%20challenges%2C%20opportunities%2C%20and%20future%20endeavors%20in%20this%0Afield.%20To%20this%20end%2C%20we%20present%20a%20systematic%20investigation%20of%20tool%20learning%20in%0Athis%20paper.%20We%20first%20introduce%20the%20background%20of%20tool%20learning%2C%20including%20its%0Acognitive%20origins%2C%20the%20paradigm%20shift%20of%20foundation%20models%2C%20and%20the%0Acomplementary%20roles%20of%20tools%20and%20models.%20Then%20we%20recapitulate%20existing%20tool%0Alearning%20research%20into%20tool-augmented%20and%20tool-oriented%20learning.%20We%20formulate%0Aa%20general%20tool%20learning%20framework%3A%20starting%20from%20understanding%20the%20user%0Ainstruction%2C%20models%20should%20learn%20to%20decompose%20a%20complex%20task%20into%20several%0Asubtasks%2C%20dynamically%20adjust%20their%20plan%20through%20reasoning%2C%20and%20effectively%0Aconquer%20each%20sub-task%20by%20selecting%20appropriate%20tools.%20We%20also%20discuss%20how%20to%0Atrain%20models%20for%20improved%20tool-use%20capabilities%20and%20facilitate%20the%0Ageneralization%20in%20tool%20learning.%20Considering%20the%20lack%20of%20a%20systematic%20tool%0Alearning%20evaluation%20in%20prior%20works%2C%20we%20experiment%20with%2018%20representative%20tools%0Aand%20show%20the%20potential%20of%20current%20foundation%20models%20in%20skillfully%20utilizing%0Atools.%20Finally%2C%20we%20discuss%20several%20open%20problems%20that%20require%20further%0Ainvestigation%20for%20tool%20learning.%20In%20general%2C%20we%20hope%20this%20paper%20could%20inspire%0Afuture%20research%20in%20integrating%20tools%20with%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.08354v3&entry.124074799=Read"},
{"title": "Deep-learning Assisted Detection and Quantification of (oo)cysts of\n  Giardia and Cryptosporidium on Smartphone Microscopy Images", "author": "Suprim Nakarmi and Sanam Pudasaini and Safal Thapaliya and Pratima Upretee and Retina Shrestha and Basant Giri and Bhanu Bhakta Neupane and Bishesh Khanal", "abstract": "  The consumption of microbial-contaminated food and water is responsible for\nthe deaths of millions of people annually. Smartphone-based microscopy systems\nare portable, low-cost, and more accessible alternatives for the detection of\nGiardia and Cryptosporidium than traditional brightfield microscopes. However,\nthe images from smartphone microscopes are noisier and require manual cyst\nidentification by trained technicians, usually unavailable in resource-limited\nsettings. Automatic detection of (oo)cysts using deep-learning-based object\ndetection could offer a solution for this limitation. We evaluate the\nperformance of four state-of-the-art object detectors to detect (oo)cysts of\nGiardia and Cryptosporidium on a custom dataset that includes both smartphone\nand brightfield microscopic images from vegetable samples. Faster RCNN,\nRetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer\n(Deformable DETR) deep-learning models were employed to explore their efficacy\nand limitations. Our results show that while the deep-learning models perform\nbetter with the brightfield microscopy image dataset than the smartphone\nmicroscopy image dataset, the smartphone microscopy predictions are still\ncomparable to the prediction performance of non-experts. Also, we publicly\nrelease brightfield and smartphone microscopy datasets with the benchmark\nresults for the detection of Giardia and Cryptosporidium, independently\ncaptured on reference (or standard lab setting) and vegetable samples. Our code\nand dataset are available at\nhttps://github.com/naamiinepal/smartphone_microscopy and\nhttps://doi.org/10.5281/zenodo.7813183, respectively.\n", "link": "http://arxiv.org/abs/2304.05339v2", "date": "2024-08-06", "relevancy": 1.374, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4614}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4572}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep-learning%20Assisted%20Detection%20and%20Quantification%20of%20%28oo%29cysts%20of%0A%20%20Giardia%20and%20Cryptosporidium%20on%20Smartphone%20Microscopy%20Images&body=Title%3A%20Deep-learning%20Assisted%20Detection%20and%20Quantification%20of%20%28oo%29cysts%20of%0A%20%20Giardia%20and%20Cryptosporidium%20on%20Smartphone%20Microscopy%20Images%0AAuthor%3A%20Suprim%20Nakarmi%20and%20Sanam%20Pudasaini%20and%20Safal%20Thapaliya%20and%20Pratima%20Upretee%20and%20Retina%20Shrestha%20and%20Basant%20Giri%20and%20Bhanu%20Bhakta%20Neupane%20and%20Bishesh%20Khanal%0AAbstract%3A%20%20%20The%20consumption%20of%20microbial-contaminated%20food%20and%20water%20is%20responsible%20for%0Athe%20deaths%20of%20millions%20of%20people%20annually.%20Smartphone-based%20microscopy%20systems%0Aare%20portable%2C%20low-cost%2C%20and%20more%20accessible%20alternatives%20for%20the%20detection%20of%0AGiardia%20and%20Cryptosporidium%20than%20traditional%20brightfield%20microscopes.%20However%2C%0Athe%20images%20from%20smartphone%20microscopes%20are%20noisier%20and%20require%20manual%20cyst%0Aidentification%20by%20trained%20technicians%2C%20usually%20unavailable%20in%20resource-limited%0Asettings.%20Automatic%20detection%20of%20%28oo%29cysts%20using%20deep-learning-based%20object%0Adetection%20could%20offer%20a%20solution%20for%20this%20limitation.%20We%20evaluate%20the%0Aperformance%20of%20four%20state-of-the-art%20object%20detectors%20to%20detect%20%28oo%29cysts%20of%0AGiardia%20and%20Cryptosporidium%20on%20a%20custom%20dataset%20that%20includes%20both%20smartphone%0Aand%20brightfield%20microscopic%20images%20from%20vegetable%20samples.%20Faster%20RCNN%2C%0ARetinaNet%2C%20You%20Only%20Look%20Once%20%28YOLOv8s%29%2C%20and%20Deformable%20Detection%20Transformer%0A%28Deformable%20DETR%29%20deep-learning%20models%20were%20employed%20to%20explore%20their%20efficacy%0Aand%20limitations.%20Our%20results%20show%20that%20while%20the%20deep-learning%20models%20perform%0Abetter%20with%20the%20brightfield%20microscopy%20image%20dataset%20than%20the%20smartphone%0Amicroscopy%20image%20dataset%2C%20the%20smartphone%20microscopy%20predictions%20are%20still%0Acomparable%20to%20the%20prediction%20performance%20of%20non-experts.%20Also%2C%20we%20publicly%0Arelease%20brightfield%20and%20smartphone%20microscopy%20datasets%20with%20the%20benchmark%0Aresults%20for%20the%20detection%20of%20Giardia%20and%20Cryptosporidium%2C%20independently%0Acaptured%20on%20reference%20%28or%20standard%20lab%20setting%29%20and%20vegetable%20samples.%20Our%20code%0Aand%20dataset%20are%20available%20at%0Ahttps%3A//github.com/naamiinepal/smartphone_microscopy%20and%0Ahttps%3A//doi.org/10.5281/zenodo.7813183%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.05339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep-learning%2520Assisted%2520Detection%2520and%2520Quantification%2520of%2520%2528oo%2529cysts%2520of%250A%2520%2520Giardia%2520and%2520Cryptosporidium%2520on%2520Smartphone%2520Microscopy%2520Images%26entry.906535625%3DSuprim%2520Nakarmi%2520and%2520Sanam%2520Pudasaini%2520and%2520Safal%2520Thapaliya%2520and%2520Pratima%2520Upretee%2520and%2520Retina%2520Shrestha%2520and%2520Basant%2520Giri%2520and%2520Bhanu%2520Bhakta%2520Neupane%2520and%2520Bishesh%2520Khanal%26entry.1292438233%3D%2520%2520The%2520consumption%2520of%2520microbial-contaminated%2520food%2520and%2520water%2520is%2520responsible%2520for%250Athe%2520deaths%2520of%2520millions%2520of%2520people%2520annually.%2520Smartphone-based%2520microscopy%2520systems%250Aare%2520portable%252C%2520low-cost%252C%2520and%2520more%2520accessible%2520alternatives%2520for%2520the%2520detection%2520of%250AGiardia%2520and%2520Cryptosporidium%2520than%2520traditional%2520brightfield%2520microscopes.%2520However%252C%250Athe%2520images%2520from%2520smartphone%2520microscopes%2520are%2520noisier%2520and%2520require%2520manual%2520cyst%250Aidentification%2520by%2520trained%2520technicians%252C%2520usually%2520unavailable%2520in%2520resource-limited%250Asettings.%2520Automatic%2520detection%2520of%2520%2528oo%2529cysts%2520using%2520deep-learning-based%2520object%250Adetection%2520could%2520offer%2520a%2520solution%2520for%2520this%2520limitation.%2520We%2520evaluate%2520the%250Aperformance%2520of%2520four%2520state-of-the-art%2520object%2520detectors%2520to%2520detect%2520%2528oo%2529cysts%2520of%250AGiardia%2520and%2520Cryptosporidium%2520on%2520a%2520custom%2520dataset%2520that%2520includes%2520both%2520smartphone%250Aand%2520brightfield%2520microscopic%2520images%2520from%2520vegetable%2520samples.%2520Faster%2520RCNN%252C%250ARetinaNet%252C%2520You%2520Only%2520Look%2520Once%2520%2528YOLOv8s%2529%252C%2520and%2520Deformable%2520Detection%2520Transformer%250A%2528Deformable%2520DETR%2529%2520deep-learning%2520models%2520were%2520employed%2520to%2520explore%2520their%2520efficacy%250Aand%2520limitations.%2520Our%2520results%2520show%2520that%2520while%2520the%2520deep-learning%2520models%2520perform%250Abetter%2520with%2520the%2520brightfield%2520microscopy%2520image%2520dataset%2520than%2520the%2520smartphone%250Amicroscopy%2520image%2520dataset%252C%2520the%2520smartphone%2520microscopy%2520predictions%2520are%2520still%250Acomparable%2520to%2520the%2520prediction%2520performance%2520of%2520non-experts.%2520Also%252C%2520we%2520publicly%250Arelease%2520brightfield%2520and%2520smartphone%2520microscopy%2520datasets%2520with%2520the%2520benchmark%250Aresults%2520for%2520the%2520detection%2520of%2520Giardia%2520and%2520Cryptosporidium%252C%2520independently%250Acaptured%2520on%2520reference%2520%2528or%2520standard%2520lab%2520setting%2529%2520and%2520vegetable%2520samples.%2520Our%2520code%250Aand%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/naamiinepal/smartphone_microscopy%2520and%250Ahttps%253A//doi.org/10.5281/zenodo.7813183%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.05339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-learning%20Assisted%20Detection%20and%20Quantification%20of%20%28oo%29cysts%20of%0A%20%20Giardia%20and%20Cryptosporidium%20on%20Smartphone%20Microscopy%20Images&entry.906535625=Suprim%20Nakarmi%20and%20Sanam%20Pudasaini%20and%20Safal%20Thapaliya%20and%20Pratima%20Upretee%20and%20Retina%20Shrestha%20and%20Basant%20Giri%20and%20Bhanu%20Bhakta%20Neupane%20and%20Bishesh%20Khanal&entry.1292438233=%20%20The%20consumption%20of%20microbial-contaminated%20food%20and%20water%20is%20responsible%20for%0Athe%20deaths%20of%20millions%20of%20people%20annually.%20Smartphone-based%20microscopy%20systems%0Aare%20portable%2C%20low-cost%2C%20and%20more%20accessible%20alternatives%20for%20the%20detection%20of%0AGiardia%20and%20Cryptosporidium%20than%20traditional%20brightfield%20microscopes.%20However%2C%0Athe%20images%20from%20smartphone%20microscopes%20are%20noisier%20and%20require%20manual%20cyst%0Aidentification%20by%20trained%20technicians%2C%20usually%20unavailable%20in%20resource-limited%0Asettings.%20Automatic%20detection%20of%20%28oo%29cysts%20using%20deep-learning-based%20object%0Adetection%20could%20offer%20a%20solution%20for%20this%20limitation.%20We%20evaluate%20the%0Aperformance%20of%20four%20state-of-the-art%20object%20detectors%20to%20detect%20%28oo%29cysts%20of%0AGiardia%20and%20Cryptosporidium%20on%20a%20custom%20dataset%20that%20includes%20both%20smartphone%0Aand%20brightfield%20microscopic%20images%20from%20vegetable%20samples.%20Faster%20RCNN%2C%0ARetinaNet%2C%20You%20Only%20Look%20Once%20%28YOLOv8s%29%2C%20and%20Deformable%20Detection%20Transformer%0A%28Deformable%20DETR%29%20deep-learning%20models%20were%20employed%20to%20explore%20their%20efficacy%0Aand%20limitations.%20Our%20results%20show%20that%20while%20the%20deep-learning%20models%20perform%0Abetter%20with%20the%20brightfield%20microscopy%20image%20dataset%20than%20the%20smartphone%0Amicroscopy%20image%20dataset%2C%20the%20smartphone%20microscopy%20predictions%20are%20still%0Acomparable%20to%20the%20prediction%20performance%20of%20non-experts.%20Also%2C%20we%20publicly%0Arelease%20brightfield%20and%20smartphone%20microscopy%20datasets%20with%20the%20benchmark%0Aresults%20for%20the%20detection%20of%20Giardia%20and%20Cryptosporidium%2C%20independently%0Acaptured%20on%20reference%20%28or%20standard%20lab%20setting%29%20and%20vegetable%20samples.%20Our%20code%0Aand%20dataset%20are%20available%20at%0Ahttps%3A//github.com/naamiinepal/smartphone_microscopy%20and%0Ahttps%3A//doi.org/10.5281/zenodo.7813183%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.05339v2&entry.124074799=Read"},
{"title": "A finite element-based physics-informed operator learning framework for\n  spatiotemporal partial differential equations on arbitrary domains", "author": "Yusuke Yamazaki and Ali Harandi and Mayu Muramatsu and Alexandre Viardin and Markus Apel and Tim Brepols and Stefanie Reese and Shahed Rezaei", "abstract": "  We propose a novel finite element-based physics-informed operator learning\nframework that allows for predicting spatiotemporal dynamics governed by\npartial differential equations (PDEs). The proposed framework employs a loss\nfunction inspired by the finite element method (FEM) with the implicit Euler\ntime integration scheme. A transient thermal conduction problem is considered\nto benchmark the performance. The proposed operator learning framework takes a\ntemperature field at the current time step as input and predicts a temperature\nfield at the next time step. The Galerkin discretized weak formulation of the\nheat equation is employed to incorporate physics into the loss function, which\nis coined finite operator learning (FOL). Upon training, the networks\nsuccessfully predict the temperature evolution over time for any initial\ntemperature field at high accuracy compared to the FEM solution. The framework\nis also confirmed to be applicable to a heterogeneous thermal conductivity and\narbitrary geometry. The advantages of FOL can be summarized as follows: First,\nthe training is performed in an unsupervised manner, avoiding the need for a\nlarge data set prepared from costly simulations or experiments. Instead, random\ntemperature patterns generated by the Gaussian random process and the Fourier\nseries, combined with constant temperature fields, are used as training data to\ncover possible temperature cases. Second, shape functions and backward\ndifference approximation are exploited for the domain discretization, resulting\nin a purely algebraic equation. This enhances training efficiency, as one\navoids time-consuming automatic differentiation when optimizing weights and\nbiases while accepting possible discretization errors. Finally, thanks to the\ninterpolation power of FEM, any arbitrary geometry can be handled with FOL,\nwhich is crucial to addressing various engineering application scenarios.\n", "link": "http://arxiv.org/abs/2405.12465v3", "date": "2024-08-06", "relevancy": 1.348, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4729}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4463}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20finite%20element-based%20physics-informed%20operator%20learning%20framework%20for%0A%20%20spatiotemporal%20partial%20differential%20equations%20on%20arbitrary%20domains&body=Title%3A%20A%20finite%20element-based%20physics-informed%20operator%20learning%20framework%20for%0A%20%20spatiotemporal%20partial%20differential%20equations%20on%20arbitrary%20domains%0AAuthor%3A%20Yusuke%20Yamazaki%20and%20Ali%20Harandi%20and%20Mayu%20Muramatsu%20and%20Alexandre%20Viardin%20and%20Markus%20Apel%20and%20Tim%20Brepols%20and%20Stefanie%20Reese%20and%20Shahed%20Rezaei%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20finite%20element-based%20physics-informed%20operator%20learning%0Aframework%20that%20allows%20for%20predicting%20spatiotemporal%20dynamics%20governed%20by%0Apartial%20differential%20equations%20%28PDEs%29.%20The%20proposed%20framework%20employs%20a%20loss%0Afunction%20inspired%20by%20the%20finite%20element%20method%20%28FEM%29%20with%20the%20implicit%20Euler%0Atime%20integration%20scheme.%20A%20transient%20thermal%20conduction%20problem%20is%20considered%0Ato%20benchmark%20the%20performance.%20The%20proposed%20operator%20learning%20framework%20takes%20a%0Atemperature%20field%20at%20the%20current%20time%20step%20as%20input%20and%20predicts%20a%20temperature%0Afield%20at%20the%20next%20time%20step.%20The%20Galerkin%20discretized%20weak%20formulation%20of%20the%0Aheat%20equation%20is%20employed%20to%20incorporate%20physics%20into%20the%20loss%20function%2C%20which%0Ais%20coined%20finite%20operator%20learning%20%28FOL%29.%20Upon%20training%2C%20the%20networks%0Asuccessfully%20predict%20the%20temperature%20evolution%20over%20time%20for%20any%20initial%0Atemperature%20field%20at%20high%20accuracy%20compared%20to%20the%20FEM%20solution.%20The%20framework%0Ais%20also%20confirmed%20to%20be%20applicable%20to%20a%20heterogeneous%20thermal%20conductivity%20and%0Aarbitrary%20geometry.%20The%20advantages%20of%20FOL%20can%20be%20summarized%20as%20follows%3A%20First%2C%0Athe%20training%20is%20performed%20in%20an%20unsupervised%20manner%2C%20avoiding%20the%20need%20for%20a%0Alarge%20data%20set%20prepared%20from%20costly%20simulations%20or%20experiments.%20Instead%2C%20random%0Atemperature%20patterns%20generated%20by%20the%20Gaussian%20random%20process%20and%20the%20Fourier%0Aseries%2C%20combined%20with%20constant%20temperature%20fields%2C%20are%20used%20as%20training%20data%20to%0Acover%20possible%20temperature%20cases.%20Second%2C%20shape%20functions%20and%20backward%0Adifference%20approximation%20are%20exploited%20for%20the%20domain%20discretization%2C%20resulting%0Ain%20a%20purely%20algebraic%20equation.%20This%20enhances%20training%20efficiency%2C%20as%20one%0Aavoids%20time-consuming%20automatic%20differentiation%20when%20optimizing%20weights%20and%0Abiases%20while%20accepting%20possible%20discretization%20errors.%20Finally%2C%20thanks%20to%20the%0Ainterpolation%20power%20of%20FEM%2C%20any%20arbitrary%20geometry%20can%20be%20handled%20with%20FOL%2C%0Awhich%20is%20crucial%20to%20addressing%20various%20engineering%20application%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12465v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520finite%2520element-based%2520physics-informed%2520operator%2520learning%2520framework%2520for%250A%2520%2520spatiotemporal%2520partial%2520differential%2520equations%2520on%2520arbitrary%2520domains%26entry.906535625%3DYusuke%2520Yamazaki%2520and%2520Ali%2520Harandi%2520and%2520Mayu%2520Muramatsu%2520and%2520Alexandre%2520Viardin%2520and%2520Markus%2520Apel%2520and%2520Tim%2520Brepols%2520and%2520Stefanie%2520Reese%2520and%2520Shahed%2520Rezaei%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520finite%2520element-based%2520physics-informed%2520operator%2520learning%250Aframework%2520that%2520allows%2520for%2520predicting%2520spatiotemporal%2520dynamics%2520governed%2520by%250Apartial%2520differential%2520equations%2520%2528PDEs%2529.%2520The%2520proposed%2520framework%2520employs%2520a%2520loss%250Afunction%2520inspired%2520by%2520the%2520finite%2520element%2520method%2520%2528FEM%2529%2520with%2520the%2520implicit%2520Euler%250Atime%2520integration%2520scheme.%2520A%2520transient%2520thermal%2520conduction%2520problem%2520is%2520considered%250Ato%2520benchmark%2520the%2520performance.%2520The%2520proposed%2520operator%2520learning%2520framework%2520takes%2520a%250Atemperature%2520field%2520at%2520the%2520current%2520time%2520step%2520as%2520input%2520and%2520predicts%2520a%2520temperature%250Afield%2520at%2520the%2520next%2520time%2520step.%2520The%2520Galerkin%2520discretized%2520weak%2520formulation%2520of%2520the%250Aheat%2520equation%2520is%2520employed%2520to%2520incorporate%2520physics%2520into%2520the%2520loss%2520function%252C%2520which%250Ais%2520coined%2520finite%2520operator%2520learning%2520%2528FOL%2529.%2520Upon%2520training%252C%2520the%2520networks%250Asuccessfully%2520predict%2520the%2520temperature%2520evolution%2520over%2520time%2520for%2520any%2520initial%250Atemperature%2520field%2520at%2520high%2520accuracy%2520compared%2520to%2520the%2520FEM%2520solution.%2520The%2520framework%250Ais%2520also%2520confirmed%2520to%2520be%2520applicable%2520to%2520a%2520heterogeneous%2520thermal%2520conductivity%2520and%250Aarbitrary%2520geometry.%2520The%2520advantages%2520of%2520FOL%2520can%2520be%2520summarized%2520as%2520follows%253A%2520First%252C%250Athe%2520training%2520is%2520performed%2520in%2520an%2520unsupervised%2520manner%252C%2520avoiding%2520the%2520need%2520for%2520a%250Alarge%2520data%2520set%2520prepared%2520from%2520costly%2520simulations%2520or%2520experiments.%2520Instead%252C%2520random%250Atemperature%2520patterns%2520generated%2520by%2520the%2520Gaussian%2520random%2520process%2520and%2520the%2520Fourier%250Aseries%252C%2520combined%2520with%2520constant%2520temperature%2520fields%252C%2520are%2520used%2520as%2520training%2520data%2520to%250Acover%2520possible%2520temperature%2520cases.%2520Second%252C%2520shape%2520functions%2520and%2520backward%250Adifference%2520approximation%2520are%2520exploited%2520for%2520the%2520domain%2520discretization%252C%2520resulting%250Ain%2520a%2520purely%2520algebraic%2520equation.%2520This%2520enhances%2520training%2520efficiency%252C%2520as%2520one%250Aavoids%2520time-consuming%2520automatic%2520differentiation%2520when%2520optimizing%2520weights%2520and%250Abiases%2520while%2520accepting%2520possible%2520discretization%2520errors.%2520Finally%252C%2520thanks%2520to%2520the%250Ainterpolation%2520power%2520of%2520FEM%252C%2520any%2520arbitrary%2520geometry%2520can%2520be%2520handled%2520with%2520FOL%252C%250Awhich%2520is%2520crucial%2520to%2520addressing%2520various%2520engineering%2520application%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12465v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20finite%20element-based%20physics-informed%20operator%20learning%20framework%20for%0A%20%20spatiotemporal%20partial%20differential%20equations%20on%20arbitrary%20domains&entry.906535625=Yusuke%20Yamazaki%20and%20Ali%20Harandi%20and%20Mayu%20Muramatsu%20and%20Alexandre%20Viardin%20and%20Markus%20Apel%20and%20Tim%20Brepols%20and%20Stefanie%20Reese%20and%20Shahed%20Rezaei&entry.1292438233=%20%20We%20propose%20a%20novel%20finite%20element-based%20physics-informed%20operator%20learning%0Aframework%20that%20allows%20for%20predicting%20spatiotemporal%20dynamics%20governed%20by%0Apartial%20differential%20equations%20%28PDEs%29.%20The%20proposed%20framework%20employs%20a%20loss%0Afunction%20inspired%20by%20the%20finite%20element%20method%20%28FEM%29%20with%20the%20implicit%20Euler%0Atime%20integration%20scheme.%20A%20transient%20thermal%20conduction%20problem%20is%20considered%0Ato%20benchmark%20the%20performance.%20The%20proposed%20operator%20learning%20framework%20takes%20a%0Atemperature%20field%20at%20the%20current%20time%20step%20as%20input%20and%20predicts%20a%20temperature%0Afield%20at%20the%20next%20time%20step.%20The%20Galerkin%20discretized%20weak%20formulation%20of%20the%0Aheat%20equation%20is%20employed%20to%20incorporate%20physics%20into%20the%20loss%20function%2C%20which%0Ais%20coined%20finite%20operator%20learning%20%28FOL%29.%20Upon%20training%2C%20the%20networks%0Asuccessfully%20predict%20the%20temperature%20evolution%20over%20time%20for%20any%20initial%0Atemperature%20field%20at%20high%20accuracy%20compared%20to%20the%20FEM%20solution.%20The%20framework%0Ais%20also%20confirmed%20to%20be%20applicable%20to%20a%20heterogeneous%20thermal%20conductivity%20and%0Aarbitrary%20geometry.%20The%20advantages%20of%20FOL%20can%20be%20summarized%20as%20follows%3A%20First%2C%0Athe%20training%20is%20performed%20in%20an%20unsupervised%20manner%2C%20avoiding%20the%20need%20for%20a%0Alarge%20data%20set%20prepared%20from%20costly%20simulations%20or%20experiments.%20Instead%2C%20random%0Atemperature%20patterns%20generated%20by%20the%20Gaussian%20random%20process%20and%20the%20Fourier%0Aseries%2C%20combined%20with%20constant%20temperature%20fields%2C%20are%20used%20as%20training%20data%20to%0Acover%20possible%20temperature%20cases.%20Second%2C%20shape%20functions%20and%20backward%0Adifference%20approximation%20are%20exploited%20for%20the%20domain%20discretization%2C%20resulting%0Ain%20a%20purely%20algebraic%20equation.%20This%20enhances%20training%20efficiency%2C%20as%20one%0Aavoids%20time-consuming%20automatic%20differentiation%20when%20optimizing%20weights%20and%0Abiases%20while%20accepting%20possible%20discretization%20errors.%20Finally%2C%20thanks%20to%20the%0Ainterpolation%20power%20of%20FEM%2C%20any%20arbitrary%20geometry%20can%20be%20handled%20with%20FOL%2C%0Awhich%20is%20crucial%20to%20addressing%20various%20engineering%20application%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12465v3&entry.124074799=Read"},
{"title": "Stochastic Trajectory Optimization for Demonstration Imitation", "author": "Chenlin Ming and Zitong Wang and Boxuan Zhang and Xiaoming Duan and Jianping He", "abstract": "  Humans often learn new skills by imitating the experts and gradually\ndeveloping their proficiency. In this work, we introduce Stochastic Trajectory\nOptimization for Demonstration Imitation (STODI), a trajectory optimization\nframework for robots to imitate the shape of demonstration trajectories with\nimproved dynamic performance. Consistent with the human learning process,\ndemonstration imitation serves as an initial step, while trajectory\noptimization aims to enhance robot motion performance. By generating random\nnoise and constructing proper cost functions, the STODI effectively explores\nand exploits generated noisy trajectories while preserving the demonstration\nshape characteristics. We employ three metrics to measure the similarity of\ntrajectories in both the time and frequency domains to help with demonstration\nimitation. Theoretical analysis reveals relationships among these metrics,\nemphasizing the benefits of frequency-domain analysis for specific tasks.\nExperiments on a 7-DOF robotic arm in the PyBullet simulator validate the\nefficacy of the STODI framework, showcasing the improved optimization\nperformance and stability compared to previous methods.\n", "link": "http://arxiv.org/abs/2408.03131v1", "date": "2024-08-06", "relevancy": 1.5502, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5499}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5269}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Trajectory%20Optimization%20for%20Demonstration%20Imitation&body=Title%3A%20Stochastic%20Trajectory%20Optimization%20for%20Demonstration%20Imitation%0AAuthor%3A%20Chenlin%20Ming%20and%20Zitong%20Wang%20and%20Boxuan%20Zhang%20and%20Xiaoming%20Duan%20and%20Jianping%20He%0AAbstract%3A%20%20%20Humans%20often%20learn%20new%20skills%20by%20imitating%20the%20experts%20and%20gradually%0Adeveloping%20their%20proficiency.%20In%20this%20work%2C%20we%20introduce%20Stochastic%20Trajectory%0AOptimization%20for%20Demonstration%20Imitation%20%28STODI%29%2C%20a%20trajectory%20optimization%0Aframework%20for%20robots%20to%20imitate%20the%20shape%20of%20demonstration%20trajectories%20with%0Aimproved%20dynamic%20performance.%20Consistent%20with%20the%20human%20learning%20process%2C%0Ademonstration%20imitation%20serves%20as%20an%20initial%20step%2C%20while%20trajectory%0Aoptimization%20aims%20to%20enhance%20robot%20motion%20performance.%20By%20generating%20random%0Anoise%20and%20constructing%20proper%20cost%20functions%2C%20the%20STODI%20effectively%20explores%0Aand%20exploits%20generated%20noisy%20trajectories%20while%20preserving%20the%20demonstration%0Ashape%20characteristics.%20We%20employ%20three%20metrics%20to%20measure%20the%20similarity%20of%0Atrajectories%20in%20both%20the%20time%20and%20frequency%20domains%20to%20help%20with%20demonstration%0Aimitation.%20Theoretical%20analysis%20reveals%20relationships%20among%20these%20metrics%2C%0Aemphasizing%20the%20benefits%20of%20frequency-domain%20analysis%20for%20specific%20tasks.%0AExperiments%20on%20a%207-DOF%20robotic%20arm%20in%20the%20PyBullet%20simulator%20validate%20the%0Aefficacy%20of%20the%20STODI%20framework%2C%20showcasing%20the%20improved%20optimization%0Aperformance%20and%20stability%20compared%20to%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Trajectory%2520Optimization%2520for%2520Demonstration%2520Imitation%26entry.906535625%3DChenlin%2520Ming%2520and%2520Zitong%2520Wang%2520and%2520Boxuan%2520Zhang%2520and%2520Xiaoming%2520Duan%2520and%2520Jianping%2520He%26entry.1292438233%3D%2520%2520Humans%2520often%2520learn%2520new%2520skills%2520by%2520imitating%2520the%2520experts%2520and%2520gradually%250Adeveloping%2520their%2520proficiency.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Stochastic%2520Trajectory%250AOptimization%2520for%2520Demonstration%2520Imitation%2520%2528STODI%2529%252C%2520a%2520trajectory%2520optimization%250Aframework%2520for%2520robots%2520to%2520imitate%2520the%2520shape%2520of%2520demonstration%2520trajectories%2520with%250Aimproved%2520dynamic%2520performance.%2520Consistent%2520with%2520the%2520human%2520learning%2520process%252C%250Ademonstration%2520imitation%2520serves%2520as%2520an%2520initial%2520step%252C%2520while%2520trajectory%250Aoptimization%2520aims%2520to%2520enhance%2520robot%2520motion%2520performance.%2520By%2520generating%2520random%250Anoise%2520and%2520constructing%2520proper%2520cost%2520functions%252C%2520the%2520STODI%2520effectively%2520explores%250Aand%2520exploits%2520generated%2520noisy%2520trajectories%2520while%2520preserving%2520the%2520demonstration%250Ashape%2520characteristics.%2520We%2520employ%2520three%2520metrics%2520to%2520measure%2520the%2520similarity%2520of%250Atrajectories%2520in%2520both%2520the%2520time%2520and%2520frequency%2520domains%2520to%2520help%2520with%2520demonstration%250Aimitation.%2520Theoretical%2520analysis%2520reveals%2520relationships%2520among%2520these%2520metrics%252C%250Aemphasizing%2520the%2520benefits%2520of%2520frequency-domain%2520analysis%2520for%2520specific%2520tasks.%250AExperiments%2520on%2520a%25207-DOF%2520robotic%2520arm%2520in%2520the%2520PyBullet%2520simulator%2520validate%2520the%250Aefficacy%2520of%2520the%2520STODI%2520framework%252C%2520showcasing%2520the%2520improved%2520optimization%250Aperformance%2520and%2520stability%2520compared%2520to%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Trajectory%20Optimization%20for%20Demonstration%20Imitation&entry.906535625=Chenlin%20Ming%20and%20Zitong%20Wang%20and%20Boxuan%20Zhang%20and%20Xiaoming%20Duan%20and%20Jianping%20He&entry.1292438233=%20%20Humans%20often%20learn%20new%20skills%20by%20imitating%20the%20experts%20and%20gradually%0Adeveloping%20their%20proficiency.%20In%20this%20work%2C%20we%20introduce%20Stochastic%20Trajectory%0AOptimization%20for%20Demonstration%20Imitation%20%28STODI%29%2C%20a%20trajectory%20optimization%0Aframework%20for%20robots%20to%20imitate%20the%20shape%20of%20demonstration%20trajectories%20with%0Aimproved%20dynamic%20performance.%20Consistent%20with%20the%20human%20learning%20process%2C%0Ademonstration%20imitation%20serves%20as%20an%20initial%20step%2C%20while%20trajectory%0Aoptimization%20aims%20to%20enhance%20robot%20motion%20performance.%20By%20generating%20random%0Anoise%20and%20constructing%20proper%20cost%20functions%2C%20the%20STODI%20effectively%20explores%0Aand%20exploits%20generated%20noisy%20trajectories%20while%20preserving%20the%20demonstration%0Ashape%20characteristics.%20We%20employ%20three%20metrics%20to%20measure%20the%20similarity%20of%0Atrajectories%20in%20both%20the%20time%20and%20frequency%20domains%20to%20help%20with%20demonstration%0Aimitation.%20Theoretical%20analysis%20reveals%20relationships%20among%20these%20metrics%2C%0Aemphasizing%20the%20benefits%20of%20frequency-domain%20analysis%20for%20specific%20tasks.%0AExperiments%20on%20a%207-DOF%20robotic%20arm%20in%20the%20PyBullet%20simulator%20validate%20the%0Aefficacy%20of%20the%20STODI%20framework%2C%20showcasing%20the%20improved%20optimization%0Aperformance%20and%20stability%20compared%20to%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03131v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


