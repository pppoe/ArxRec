<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250708.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity\n  Animatable Face Avatars", "author": "Gent Serifi and Marcel C. B\u00fchler", "abstract": "  We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for\nhigh-quality animatable face avatars. Creating such detailed face avatars from\nvideos is a challenging problem and has numerous applications in augmented and\nvirtual reality. While tremendous successes have been achieved for static\nfaces, animatable avatars from monocular videos still fall in the uncanny\nvalley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face\nthrough a collection of 3D Gaussian primitives. 3DGS excels at rendering static\nfaces, but the state-of-the-art still struggles with nonlinear deformations,\ncomplex lighting effects, and fine details. While most related works focus on\npredicting better Gaussian parameters from expression codes, we rethink the 3D\nGaussian representation itself and how to make it more expressive. Our insights\nlead to a novel extension of 3D Gaussians to high-dimensional multivariate\nGaussians, dubbed 'HyperGaussians'. The higher dimensionality increases\nexpressivity through conditioning on a learnable local embedding. However,\nsplatting HyperGaussians is computationally expensive because it requires\ninverting a high-dimensional covariance matrix. We solve this by\nreparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.\nThis trick boosts the efficiency so that HyperGaussians can be seamlessly\nintegrated into existing models. To demonstrate this, we plug in HyperGaussians\ninto the state-of-the-art in fast monocular face avatars: FlashAvatar. Our\nevaluation on 19 subjects from 4 face datasets shows that HyperGaussians\noutperform 3DGS numerically and visually, particularly for high-frequency\ndetails like eyeglass frames, teeth, complex facial movements, and specular\nreflections.\n", "link": "http://arxiv.org/abs/2507.02803v2", "date": "2025-07-08", "relevancy": 3.7182, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7549}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7549}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperGaussians%3A%20High-Dimensional%20Gaussian%20Splatting%20for%20High-Fidelity%0A%20%20Animatable%20Face%20Avatars&body=Title%3A%20HyperGaussians%3A%20High-Dimensional%20Gaussian%20Splatting%20for%20High-Fidelity%0A%20%20Animatable%20Face%20Avatars%0AAuthor%3A%20Gent%20Serifi%20and%20Marcel%20C.%20B%C3%BChler%0AAbstract%3A%20%20%20We%20introduce%20HyperGaussians%2C%20a%20novel%20extension%20of%203D%20Gaussian%20Splatting%20for%0Ahigh-quality%20animatable%20face%20avatars.%20Creating%20such%20detailed%20face%20avatars%20from%0Avideos%20is%20a%20challenging%20problem%20and%20has%20numerous%20applications%20in%20augmented%20and%0Avirtual%20reality.%20While%20tremendous%20successes%20have%20been%20achieved%20for%20static%0Afaces%2C%20animatable%20avatars%20from%20monocular%20videos%20still%20fall%20in%20the%20uncanny%0Avalley.%20The%20de%20facto%20standard%2C%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20represents%20a%20face%0Athrough%20a%20collection%20of%203D%20Gaussian%20primitives.%203DGS%20excels%20at%20rendering%20static%0Afaces%2C%20but%20the%20state-of-the-art%20still%20struggles%20with%20nonlinear%20deformations%2C%0Acomplex%20lighting%20effects%2C%20and%20fine%20details.%20While%20most%20related%20works%20focus%20on%0Apredicting%20better%20Gaussian%20parameters%20from%20expression%20codes%2C%20we%20rethink%20the%203D%0AGaussian%20representation%20itself%20and%20how%20to%20make%20it%20more%20expressive.%20Our%20insights%0Alead%20to%20a%20novel%20extension%20of%203D%20Gaussians%20to%20high-dimensional%20multivariate%0AGaussians%2C%20dubbed%20%27HyperGaussians%27.%20The%20higher%20dimensionality%20increases%0Aexpressivity%20through%20conditioning%20on%20a%20learnable%20local%20embedding.%20However%2C%0Asplatting%20HyperGaussians%20is%20computationally%20expensive%20because%20it%20requires%0Ainverting%20a%20high-dimensional%20covariance%20matrix.%20We%20solve%20this%20by%0Areparameterizing%20the%20covariance%20matrix%2C%20dubbed%20the%20%27inverse%20covariance%20trick%27.%0AThis%20trick%20boosts%20the%20efficiency%20so%20that%20HyperGaussians%20can%20be%20seamlessly%0Aintegrated%20into%20existing%20models.%20To%20demonstrate%20this%2C%20we%20plug%20in%20HyperGaussians%0Ainto%20the%20state-of-the-art%20in%20fast%20monocular%20face%20avatars%3A%20FlashAvatar.%20Our%0Aevaluation%20on%2019%20subjects%20from%204%20face%20datasets%20shows%20that%20HyperGaussians%0Aoutperform%203DGS%20numerically%20and%20visually%2C%20particularly%20for%20high-frequency%0Adetails%20like%20eyeglass%20frames%2C%20teeth%2C%20complex%20facial%20movements%2C%20and%20specular%0Areflections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02803v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperGaussians%253A%2520High-Dimensional%2520Gaussian%2520Splatting%2520for%2520High-Fidelity%250A%2520%2520Animatable%2520Face%2520Avatars%26entry.906535625%3DGent%2520Serifi%2520and%2520Marcel%2520C.%2520B%25C3%25BChler%26entry.1292438233%3D%2520%2520We%2520introduce%2520HyperGaussians%252C%2520a%2520novel%2520extension%2520of%25203D%2520Gaussian%2520Splatting%2520for%250Ahigh-quality%2520animatable%2520face%2520avatars.%2520Creating%2520such%2520detailed%2520face%2520avatars%2520from%250Avideos%2520is%2520a%2520challenging%2520problem%2520and%2520has%2520numerous%2520applications%2520in%2520augmented%2520and%250Avirtual%2520reality.%2520While%2520tremendous%2520successes%2520have%2520been%2520achieved%2520for%2520static%250Afaces%252C%2520animatable%2520avatars%2520from%2520monocular%2520videos%2520still%2520fall%2520in%2520the%2520uncanny%250Avalley.%2520The%2520de%2520facto%2520standard%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520represents%2520a%2520face%250Athrough%2520a%2520collection%2520of%25203D%2520Gaussian%2520primitives.%25203DGS%2520excels%2520at%2520rendering%2520static%250Afaces%252C%2520but%2520the%2520state-of-the-art%2520still%2520struggles%2520with%2520nonlinear%2520deformations%252C%250Acomplex%2520lighting%2520effects%252C%2520and%2520fine%2520details.%2520While%2520most%2520related%2520works%2520focus%2520on%250Apredicting%2520better%2520Gaussian%2520parameters%2520from%2520expression%2520codes%252C%2520we%2520rethink%2520the%25203D%250AGaussian%2520representation%2520itself%2520and%2520how%2520to%2520make%2520it%2520more%2520expressive.%2520Our%2520insights%250Alead%2520to%2520a%2520novel%2520extension%2520of%25203D%2520Gaussians%2520to%2520high-dimensional%2520multivariate%250AGaussians%252C%2520dubbed%2520%2527HyperGaussians%2527.%2520The%2520higher%2520dimensionality%2520increases%250Aexpressivity%2520through%2520conditioning%2520on%2520a%2520learnable%2520local%2520embedding.%2520However%252C%250Asplatting%2520HyperGaussians%2520is%2520computationally%2520expensive%2520because%2520it%2520requires%250Ainverting%2520a%2520high-dimensional%2520covariance%2520matrix.%2520We%2520solve%2520this%2520by%250Areparameterizing%2520the%2520covariance%2520matrix%252C%2520dubbed%2520the%2520%2527inverse%2520covariance%2520trick%2527.%250AThis%2520trick%2520boosts%2520the%2520efficiency%2520so%2520that%2520HyperGaussians%2520can%2520be%2520seamlessly%250Aintegrated%2520into%2520existing%2520models.%2520To%2520demonstrate%2520this%252C%2520we%2520plug%2520in%2520HyperGaussians%250Ainto%2520the%2520state-of-the-art%2520in%2520fast%2520monocular%2520face%2520avatars%253A%2520FlashAvatar.%2520Our%250Aevaluation%2520on%252019%2520subjects%2520from%25204%2520face%2520datasets%2520shows%2520that%2520HyperGaussians%250Aoutperform%25203DGS%2520numerically%2520and%2520visually%252C%2520particularly%2520for%2520high-frequency%250Adetails%2520like%2520eyeglass%2520frames%252C%2520teeth%252C%2520complex%2520facial%2520movements%252C%2520and%2520specular%250Areflections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02803v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperGaussians%3A%20High-Dimensional%20Gaussian%20Splatting%20for%20High-Fidelity%0A%20%20Animatable%20Face%20Avatars&entry.906535625=Gent%20Serifi%20and%20Marcel%20C.%20B%C3%BChler&entry.1292438233=%20%20We%20introduce%20HyperGaussians%2C%20a%20novel%20extension%20of%203D%20Gaussian%20Splatting%20for%0Ahigh-quality%20animatable%20face%20avatars.%20Creating%20such%20detailed%20face%20avatars%20from%0Avideos%20is%20a%20challenging%20problem%20and%20has%20numerous%20applications%20in%20augmented%20and%0Avirtual%20reality.%20While%20tremendous%20successes%20have%20been%20achieved%20for%20static%0Afaces%2C%20animatable%20avatars%20from%20monocular%20videos%20still%20fall%20in%20the%20uncanny%0Avalley.%20The%20de%20facto%20standard%2C%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20represents%20a%20face%0Athrough%20a%20collection%20of%203D%20Gaussian%20primitives.%203DGS%20excels%20at%20rendering%20static%0Afaces%2C%20but%20the%20state-of-the-art%20still%20struggles%20with%20nonlinear%20deformations%2C%0Acomplex%20lighting%20effects%2C%20and%20fine%20details.%20While%20most%20related%20works%20focus%20on%0Apredicting%20better%20Gaussian%20parameters%20from%20expression%20codes%2C%20we%20rethink%20the%203D%0AGaussian%20representation%20itself%20and%20how%20to%20make%20it%20more%20expressive.%20Our%20insights%0Alead%20to%20a%20novel%20extension%20of%203D%20Gaussians%20to%20high-dimensional%20multivariate%0AGaussians%2C%20dubbed%20%27HyperGaussians%27.%20The%20higher%20dimensionality%20increases%0Aexpressivity%20through%20conditioning%20on%20a%20learnable%20local%20embedding.%20However%2C%0Asplatting%20HyperGaussians%20is%20computationally%20expensive%20because%20it%20requires%0Ainverting%20a%20high-dimensional%20covariance%20matrix.%20We%20solve%20this%20by%0Areparameterizing%20the%20covariance%20matrix%2C%20dubbed%20the%20%27inverse%20covariance%20trick%27.%0AThis%20trick%20boosts%20the%20efficiency%20so%20that%20HyperGaussians%20can%20be%20seamlessly%0Aintegrated%20into%20existing%20models.%20To%20demonstrate%20this%2C%20we%20plug%20in%20HyperGaussians%0Ainto%20the%20state-of-the-art%20in%20fast%20monocular%20face%20avatars%3A%20FlashAvatar.%20Our%0Aevaluation%20on%2019%20subjects%20from%204%20face%20datasets%20shows%20that%20HyperGaussians%0Aoutperform%203DGS%20numerically%20and%20visually%2C%20particularly%20for%20high-frequency%0Adetails%20like%20eyeglass%20frames%2C%20teeth%2C%20complex%20facial%20movements%2C%20and%20specular%0Areflections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02803v2&entry.124074799=Read"},
{"title": "Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D\n  Gaussian Splatting for Photorealistic Scenes Rendering", "author": "Jiayi Song and Zihan Ye and Qingyuan Zhou and Weidong Yang and Ben Fei and Jingyi Xu and Ying He and Wanli Ouyang", "abstract": "  Accurately rendering scenes with reflective surfaces remains a significant\nchallenge in novel view synthesis, as existing methods like Neural Radiance\nFields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections\nas physical geometry, resulting in degraded reconstructions. Previous methods\nrely on incomplete and non-generalizable geometric constraints, leading to\nmisalignment between the positions of Gaussian splats and the actual scene\ngeometry. When dealing with real-world scenes containing complex geometry, the\naccumulation of Gaussians further exacerbates surface artifacts and results in\nblurred reconstructions. To address these limitations, in this work, we propose\nRef-Unlock, a novel geometry-aware reflection modeling framework based on 3D\nGaussian Splatting, which explicitly disentangles transmitted and reflected\ncomponents to better capture complex reflections and enhance geometric\nconsistency in real-world scenes. Our approach employs a dual-branch\nrepresentation with high-order spherical harmonics to capture high-frequency\nreflective details, alongside a reflection removal module providing pseudo\nreflection-free supervision to guide clean decomposition. Additionally, we\nincorporate pseudo-depth maps and a geometry-aware bilateral smoothness\nconstraint to enhance 3D geometric consistency and stability in decomposition.\nExtensive experiments demonstrate that Ref-Unlock significantly outperforms\nclassical GS-based reflection methods and achieves competitive results with\nNeRF-based models, while enabling flexible vision foundation models (VFMs)\ndriven reflection editing. Our method thus offers an efficient and\ngeneralizable solution for realistic rendering of reflective scenes. Our code\nis available at https://ref-unlock.github.io/.\n", "link": "http://arxiv.org/abs/2507.06103v1", "date": "2025-07-08", "relevancy": 3.2975, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6928}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6695}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reflections%20Unlock%3A%20Geometry-Aware%20Reflection%20Disentanglement%20in%203D%0A%20%20Gaussian%20Splatting%20for%20Photorealistic%20Scenes%20Rendering&body=Title%3A%20Reflections%20Unlock%3A%20Geometry-Aware%20Reflection%20Disentanglement%20in%203D%0A%20%20Gaussian%20Splatting%20for%20Photorealistic%20Scenes%20Rendering%0AAuthor%3A%20Jiayi%20Song%20and%20Zihan%20Ye%20and%20Qingyuan%20Zhou%20and%20Weidong%20Yang%20and%20Ben%20Fei%20and%20Jingyi%20Xu%20and%20Ying%20He%20and%20Wanli%20Ouyang%0AAbstract%3A%20%20%20Accurately%20rendering%20scenes%20with%20reflective%20surfaces%20remains%20a%20significant%0Achallenge%20in%20novel%20view%20synthesis%2C%20as%20existing%20methods%20like%20Neural%20Radiance%0AFields%20%28NeRF%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29%20often%20misinterpret%20reflections%0Aas%20physical%20geometry%2C%20resulting%20in%20degraded%20reconstructions.%20Previous%20methods%0Arely%20on%20incomplete%20and%20non-generalizable%20geometric%20constraints%2C%20leading%20to%0Amisalignment%20between%20the%20positions%20of%20Gaussian%20splats%20and%20the%20actual%20scene%0Ageometry.%20When%20dealing%20with%20real-world%20scenes%20containing%20complex%20geometry%2C%20the%0Aaccumulation%20of%20Gaussians%20further%20exacerbates%20surface%20artifacts%20and%20results%20in%0Ablurred%20reconstructions.%20To%20address%20these%20limitations%2C%20in%20this%20work%2C%20we%20propose%0ARef-Unlock%2C%20a%20novel%20geometry-aware%20reflection%20modeling%20framework%20based%20on%203D%0AGaussian%20Splatting%2C%20which%20explicitly%20disentangles%20transmitted%20and%20reflected%0Acomponents%20to%20better%20capture%20complex%20reflections%20and%20enhance%20geometric%0Aconsistency%20in%20real-world%20scenes.%20Our%20approach%20employs%20a%20dual-branch%0Arepresentation%20with%20high-order%20spherical%20harmonics%20to%20capture%20high-frequency%0Areflective%20details%2C%20alongside%20a%20reflection%20removal%20module%20providing%20pseudo%0Areflection-free%20supervision%20to%20guide%20clean%20decomposition.%20Additionally%2C%20we%0Aincorporate%20pseudo-depth%20maps%20and%20a%20geometry-aware%20bilateral%20smoothness%0Aconstraint%20to%20enhance%203D%20geometric%20consistency%20and%20stability%20in%20decomposition.%0AExtensive%20experiments%20demonstrate%20that%20Ref-Unlock%20significantly%20outperforms%0Aclassical%20GS-based%20reflection%20methods%20and%20achieves%20competitive%20results%20with%0ANeRF-based%20models%2C%20while%20enabling%20flexible%20vision%20foundation%20models%20%28VFMs%29%0Adriven%20reflection%20editing.%20Our%20method%20thus%20offers%20an%20efficient%20and%0Ageneralizable%20solution%20for%20realistic%20rendering%20of%20reflective%20scenes.%20Our%20code%0Ais%20available%20at%20https%3A//ref-unlock.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReflections%2520Unlock%253A%2520Geometry-Aware%2520Reflection%2520Disentanglement%2520in%25203D%250A%2520%2520Gaussian%2520Splatting%2520for%2520Photorealistic%2520Scenes%2520Rendering%26entry.906535625%3DJiayi%2520Song%2520and%2520Zihan%2520Ye%2520and%2520Qingyuan%2520Zhou%2520and%2520Weidong%2520Yang%2520and%2520Ben%2520Fei%2520and%2520Jingyi%2520Xu%2520and%2520Ying%2520He%2520and%2520Wanli%2520Ouyang%26entry.1292438233%3D%2520%2520Accurately%2520rendering%2520scenes%2520with%2520reflective%2520surfaces%2520remains%2520a%2520significant%250Achallenge%2520in%2520novel%2520view%2520synthesis%252C%2520as%2520existing%2520methods%2520like%2520Neural%2520Radiance%250AFields%2520%2528NeRF%2529%2520and%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520often%2520misinterpret%2520reflections%250Aas%2520physical%2520geometry%252C%2520resulting%2520in%2520degraded%2520reconstructions.%2520Previous%2520methods%250Arely%2520on%2520incomplete%2520and%2520non-generalizable%2520geometric%2520constraints%252C%2520leading%2520to%250Amisalignment%2520between%2520the%2520positions%2520of%2520Gaussian%2520splats%2520and%2520the%2520actual%2520scene%250Ageometry.%2520When%2520dealing%2520with%2520real-world%2520scenes%2520containing%2520complex%2520geometry%252C%2520the%250Aaccumulation%2520of%2520Gaussians%2520further%2520exacerbates%2520surface%2520artifacts%2520and%2520results%2520in%250Ablurred%2520reconstructions.%2520To%2520address%2520these%2520limitations%252C%2520in%2520this%2520work%252C%2520we%2520propose%250ARef-Unlock%252C%2520a%2520novel%2520geometry-aware%2520reflection%2520modeling%2520framework%2520based%2520on%25203D%250AGaussian%2520Splatting%252C%2520which%2520explicitly%2520disentangles%2520transmitted%2520and%2520reflected%250Acomponents%2520to%2520better%2520capture%2520complex%2520reflections%2520and%2520enhance%2520geometric%250Aconsistency%2520in%2520real-world%2520scenes.%2520Our%2520approach%2520employs%2520a%2520dual-branch%250Arepresentation%2520with%2520high-order%2520spherical%2520harmonics%2520to%2520capture%2520high-frequency%250Areflective%2520details%252C%2520alongside%2520a%2520reflection%2520removal%2520module%2520providing%2520pseudo%250Areflection-free%2520supervision%2520to%2520guide%2520clean%2520decomposition.%2520Additionally%252C%2520we%250Aincorporate%2520pseudo-depth%2520maps%2520and%2520a%2520geometry-aware%2520bilateral%2520smoothness%250Aconstraint%2520to%2520enhance%25203D%2520geometric%2520consistency%2520and%2520stability%2520in%2520decomposition.%250AExtensive%2520experiments%2520demonstrate%2520that%2520Ref-Unlock%2520significantly%2520outperforms%250Aclassical%2520GS-based%2520reflection%2520methods%2520and%2520achieves%2520competitive%2520results%2520with%250ANeRF-based%2520models%252C%2520while%2520enabling%2520flexible%2520vision%2520foundation%2520models%2520%2528VFMs%2529%250Adriven%2520reflection%2520editing.%2520Our%2520method%2520thus%2520offers%2520an%2520efficient%2520and%250Ageneralizable%2520solution%2520for%2520realistic%2520rendering%2520of%2520reflective%2520scenes.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//ref-unlock.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reflections%20Unlock%3A%20Geometry-Aware%20Reflection%20Disentanglement%20in%203D%0A%20%20Gaussian%20Splatting%20for%20Photorealistic%20Scenes%20Rendering&entry.906535625=Jiayi%20Song%20and%20Zihan%20Ye%20and%20Qingyuan%20Zhou%20and%20Weidong%20Yang%20and%20Ben%20Fei%20and%20Jingyi%20Xu%20and%20Ying%20He%20and%20Wanli%20Ouyang&entry.1292438233=%20%20Accurately%20rendering%20scenes%20with%20reflective%20surfaces%20remains%20a%20significant%0Achallenge%20in%20novel%20view%20synthesis%2C%20as%20existing%20methods%20like%20Neural%20Radiance%0AFields%20%28NeRF%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29%20often%20misinterpret%20reflections%0Aas%20physical%20geometry%2C%20resulting%20in%20degraded%20reconstructions.%20Previous%20methods%0Arely%20on%20incomplete%20and%20non-generalizable%20geometric%20constraints%2C%20leading%20to%0Amisalignment%20between%20the%20positions%20of%20Gaussian%20splats%20and%20the%20actual%20scene%0Ageometry.%20When%20dealing%20with%20real-world%20scenes%20containing%20complex%20geometry%2C%20the%0Aaccumulation%20of%20Gaussians%20further%20exacerbates%20surface%20artifacts%20and%20results%20in%0Ablurred%20reconstructions.%20To%20address%20these%20limitations%2C%20in%20this%20work%2C%20we%20propose%0ARef-Unlock%2C%20a%20novel%20geometry-aware%20reflection%20modeling%20framework%20based%20on%203D%0AGaussian%20Splatting%2C%20which%20explicitly%20disentangles%20transmitted%20and%20reflected%0Acomponents%20to%20better%20capture%20complex%20reflections%20and%20enhance%20geometric%0Aconsistency%20in%20real-world%20scenes.%20Our%20approach%20employs%20a%20dual-branch%0Arepresentation%20with%20high-order%20spherical%20harmonics%20to%20capture%20high-frequency%0Areflective%20details%2C%20alongside%20a%20reflection%20removal%20module%20providing%20pseudo%0Areflection-free%20supervision%20to%20guide%20clean%20decomposition.%20Additionally%2C%20we%0Aincorporate%20pseudo-depth%20maps%20and%20a%20geometry-aware%20bilateral%20smoothness%0Aconstraint%20to%20enhance%203D%20geometric%20consistency%20and%20stability%20in%20decomposition.%0AExtensive%20experiments%20demonstrate%20that%20Ref-Unlock%20significantly%20outperforms%0Aclassical%20GS-based%20reflection%20methods%20and%20achieves%20competitive%20results%20with%0ANeRF-based%20models%2C%20while%20enabling%20flexible%20vision%20foundation%20models%20%28VFMs%29%0Adriven%20reflection%20editing.%20Our%20method%20thus%20offers%20an%20efficient%20and%0Ageneralizable%20solution%20for%20realistic%20rendering%20of%20reflective%20scenes.%20Our%20code%0Ais%20available%20at%20https%3A//ref-unlock.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06103v1&entry.124074799=Read"},
{"title": "Omni-Video: Democratizing Unified Video Understanding and Generation", "author": "Zhiyu Tan and Hao Yang and Luozheng Qin and Jia Gong and Mengping Yang and Hao Li", "abstract": "  Notable breakthroughs in unified understanding and generation modeling have\nled to remarkable advancements in image understanding, reasoning, production\nand editing, yet current foundational models predominantly focus on processing\nimages, creating a gap in the development of unified models for video\nunderstanding and generation. This report presents Omni-Video, an efficient and\neffective unified framework for video understanding, generation, as well as\ninstruction-based editing. Our key insight is to teach existing multimodal\nlarge language models (MLLMs) to produce continuous visual clues that are used\nas the input of diffusion decoders, which produce high-quality videos\nconditioned on these visual clues. To fully unlock the potential of our system\nfor unified video modeling, we integrate several technical improvements: 1) a\nlightweight architectural design that respectively attaches a vision head on\nthe top of MLLMs and a adapter before the input of diffusion decoders, the\nformer produce visual tokens for the latter, which adapts these visual tokens\nto the conditional space of diffusion decoders; and 2) an efficient multi-stage\ntraining scheme that facilitates a fast connection between MLLMs and diffusion\ndecoders with limited data and computational resources. We empirically\ndemonstrate that our model exhibits satisfactory generalization abilities\nacross video generation, editing and understanding tasks.\n", "link": "http://arxiv.org/abs/2507.06119v1", "date": "2025-07-08", "relevancy": 3.2666, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6675}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-Video%3A%20Democratizing%20Unified%20Video%20Understanding%20and%20Generation&body=Title%3A%20Omni-Video%3A%20Democratizing%20Unified%20Video%20Understanding%20and%20Generation%0AAuthor%3A%20Zhiyu%20Tan%20and%20Hao%20Yang%20and%20Luozheng%20Qin%20and%20Jia%20Gong%20and%20Mengping%20Yang%20and%20Hao%20Li%0AAbstract%3A%20%20%20Notable%20breakthroughs%20in%20unified%20understanding%20and%20generation%20modeling%20have%0Aled%20to%20remarkable%20advancements%20in%20image%20understanding%2C%20reasoning%2C%20production%0Aand%20editing%2C%20yet%20current%20foundational%20models%20predominantly%20focus%20on%20processing%0Aimages%2C%20creating%20a%20gap%20in%20the%20development%20of%20unified%20models%20for%20video%0Aunderstanding%20and%20generation.%20This%20report%20presents%20Omni-Video%2C%20an%20efficient%20and%0Aeffective%20unified%20framework%20for%20video%20understanding%2C%20generation%2C%20as%20well%20as%0Ainstruction-based%20editing.%20Our%20key%20insight%20is%20to%20teach%20existing%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20to%20produce%20continuous%20visual%20clues%20that%20are%20used%0Aas%20the%20input%20of%20diffusion%20decoders%2C%20which%20produce%20high-quality%20videos%0Aconditioned%20on%20these%20visual%20clues.%20To%20fully%20unlock%20the%20potential%20of%20our%20system%0Afor%20unified%20video%20modeling%2C%20we%20integrate%20several%20technical%20improvements%3A%201%29%20a%0Alightweight%20architectural%20design%20that%20respectively%20attaches%20a%20vision%20head%20on%0Athe%20top%20of%20MLLMs%20and%20a%20adapter%20before%20the%20input%20of%20diffusion%20decoders%2C%20the%0Aformer%20produce%20visual%20tokens%20for%20the%20latter%2C%20which%20adapts%20these%20visual%20tokens%0Ato%20the%20conditional%20space%20of%20diffusion%20decoders%3B%20and%202%29%20an%20efficient%20multi-stage%0Atraining%20scheme%20that%20facilitates%20a%20fast%20connection%20between%20MLLMs%20and%20diffusion%0Adecoders%20with%20limited%20data%20and%20computational%20resources.%20We%20empirically%0Ademonstrate%20that%20our%20model%20exhibits%20satisfactory%20generalization%20abilities%0Aacross%20video%20generation%2C%20editing%20and%20understanding%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-Video%253A%2520Democratizing%2520Unified%2520Video%2520Understanding%2520and%2520Generation%26entry.906535625%3DZhiyu%2520Tan%2520and%2520Hao%2520Yang%2520and%2520Luozheng%2520Qin%2520and%2520Jia%2520Gong%2520and%2520Mengping%2520Yang%2520and%2520Hao%2520Li%26entry.1292438233%3D%2520%2520Notable%2520breakthroughs%2520in%2520unified%2520understanding%2520and%2520generation%2520modeling%2520have%250Aled%2520to%2520remarkable%2520advancements%2520in%2520image%2520understanding%252C%2520reasoning%252C%2520production%250Aand%2520editing%252C%2520yet%2520current%2520foundational%2520models%2520predominantly%2520focus%2520on%2520processing%250Aimages%252C%2520creating%2520a%2520gap%2520in%2520the%2520development%2520of%2520unified%2520models%2520for%2520video%250Aunderstanding%2520and%2520generation.%2520This%2520report%2520presents%2520Omni-Video%252C%2520an%2520efficient%2520and%250Aeffective%2520unified%2520framework%2520for%2520video%2520understanding%252C%2520generation%252C%2520as%2520well%2520as%250Ainstruction-based%2520editing.%2520Our%2520key%2520insight%2520is%2520to%2520teach%2520existing%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%2520to%2520produce%2520continuous%2520visual%2520clues%2520that%2520are%2520used%250Aas%2520the%2520input%2520of%2520diffusion%2520decoders%252C%2520which%2520produce%2520high-quality%2520videos%250Aconditioned%2520on%2520these%2520visual%2520clues.%2520To%2520fully%2520unlock%2520the%2520potential%2520of%2520our%2520system%250Afor%2520unified%2520video%2520modeling%252C%2520we%2520integrate%2520several%2520technical%2520improvements%253A%25201%2529%2520a%250Alightweight%2520architectural%2520design%2520that%2520respectively%2520attaches%2520a%2520vision%2520head%2520on%250Athe%2520top%2520of%2520MLLMs%2520and%2520a%2520adapter%2520before%2520the%2520input%2520of%2520diffusion%2520decoders%252C%2520the%250Aformer%2520produce%2520visual%2520tokens%2520for%2520the%2520latter%252C%2520which%2520adapts%2520these%2520visual%2520tokens%250Ato%2520the%2520conditional%2520space%2520of%2520diffusion%2520decoders%253B%2520and%25202%2529%2520an%2520efficient%2520multi-stage%250Atraining%2520scheme%2520that%2520facilitates%2520a%2520fast%2520connection%2520between%2520MLLMs%2520and%2520diffusion%250Adecoders%2520with%2520limited%2520data%2520and%2520computational%2520resources.%2520We%2520empirically%250Ademonstrate%2520that%2520our%2520model%2520exhibits%2520satisfactory%2520generalization%2520abilities%250Aacross%2520video%2520generation%252C%2520editing%2520and%2520understanding%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-Video%3A%20Democratizing%20Unified%20Video%20Understanding%20and%20Generation&entry.906535625=Zhiyu%20Tan%20and%20Hao%20Yang%20and%20Luozheng%20Qin%20and%20Jia%20Gong%20and%20Mengping%20Yang%20and%20Hao%20Li&entry.1292438233=%20%20Notable%20breakthroughs%20in%20unified%20understanding%20and%20generation%20modeling%20have%0Aled%20to%20remarkable%20advancements%20in%20image%20understanding%2C%20reasoning%2C%20production%0Aand%20editing%2C%20yet%20current%20foundational%20models%20predominantly%20focus%20on%20processing%0Aimages%2C%20creating%20a%20gap%20in%20the%20development%20of%20unified%20models%20for%20video%0Aunderstanding%20and%20generation.%20This%20report%20presents%20Omni-Video%2C%20an%20efficient%20and%0Aeffective%20unified%20framework%20for%20video%20understanding%2C%20generation%2C%20as%20well%20as%0Ainstruction-based%20editing.%20Our%20key%20insight%20is%20to%20teach%20existing%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20to%20produce%20continuous%20visual%20clues%20that%20are%20used%0Aas%20the%20input%20of%20diffusion%20decoders%2C%20which%20produce%20high-quality%20videos%0Aconditioned%20on%20these%20visual%20clues.%20To%20fully%20unlock%20the%20potential%20of%20our%20system%0Afor%20unified%20video%20modeling%2C%20we%20integrate%20several%20technical%20improvements%3A%201%29%20a%0Alightweight%20architectural%20design%20that%20respectively%20attaches%20a%20vision%20head%20on%0Athe%20top%20of%20MLLMs%20and%20a%20adapter%20before%20the%20input%20of%20diffusion%20decoders%2C%20the%0Aformer%20produce%20visual%20tokens%20for%20the%20latter%2C%20which%20adapts%20these%20visual%20tokens%0Ato%20the%20conditional%20space%20of%20diffusion%20decoders%3B%20and%202%29%20an%20efficient%20multi-stage%0Atraining%20scheme%20that%20facilitates%20a%20fast%20connection%20between%20MLLMs%20and%20diffusion%0Adecoders%20with%20limited%20data%20and%20computational%20resources.%20We%20empirically%0Ademonstrate%20that%20our%20model%20exhibits%20satisfactory%20generalization%20abilities%0Aacross%20video%20generation%2C%20editing%20and%20understanding%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06119v1&entry.124074799=Read"},
{"title": "LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for\n  Panorama-Style Mobile Captures", "author": "Seungoh Han and Jaehoon Jang and Hyunsu Kim and Jaeheung Surh and Junhyung Kwak and Hyowon Ha and Kyungdon Joo", "abstract": "  Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel\nview synthesis (NVS) with impressive quality in indoor scenes. However,\nachieving high-fidelity rendering requires meticulously captured images\ncovering the entire scene, limiting accessibility for general users. We aim to\ndevelop a practical 3DGS-based NVS framework using simple panorama-style motion\nwith a handheld camera (e.g., mobile device). While convenient, this\nrotation-dominant motion and narrow baseline make accurate camera pose and 3D\npoint estimation challenging, especially in textureless indoor scenes. To\naddress these challenges, we propose LighthouseGS, a novel framework inspired\nby the lighthouse-like sweeping motion of panoramic views. LighthouseGS\nleverages rough geometric priors, such as mobile device camera poses and\nmonocular depth estimation, and utilizes the planar structures often found in\nindoor environments. We present a new initialization method called plane\nscaffold assembly to generate consistent 3D points on these structures,\nfollowed by a stable pruning strategy to enhance geometry and optimization\nstability. Additionally, we introduce geometric and photometric corrections to\nresolve inconsistencies from motion drift and auto-exposure in mobile devices.\nTested on collected real and synthetic indoor scenes, LighthouseGS delivers\nphotorealistic rendering, surpassing state-of-the-art methods and demonstrating\nthe potential for panoramic view synthesis and object placement.\n", "link": "http://arxiv.org/abs/2507.06109v1", "date": "2025-07-08", "relevancy": 3.2325, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6797}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6319}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LighthouseGS%3A%20Indoor%20Structure-aware%203D%20Gaussian%20Splatting%20for%0A%20%20Panorama-Style%20Mobile%20Captures&body=Title%3A%20LighthouseGS%3A%20Indoor%20Structure-aware%203D%20Gaussian%20Splatting%20for%0A%20%20Panorama-Style%20Mobile%20Captures%0AAuthor%3A%20Seungoh%20Han%20and%20Jaehoon%20Jang%20and%20Hyunsu%20Kim%20and%20Jaeheung%20Surh%20and%20Junhyung%20Kwak%20and%20Hyowon%20Ha%20and%20Kyungdon%20Joo%0AAbstract%3A%20%20%20Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20enabled%20real-time%20novel%0Aview%20synthesis%20%28NVS%29%20with%20impressive%20quality%20in%20indoor%20scenes.%20However%2C%0Aachieving%20high-fidelity%20rendering%20requires%20meticulously%20captured%20images%0Acovering%20the%20entire%20scene%2C%20limiting%20accessibility%20for%20general%20users.%20We%20aim%20to%0Adevelop%20a%20practical%203DGS-based%20NVS%20framework%20using%20simple%20panorama-style%20motion%0Awith%20a%20handheld%20camera%20%28e.g.%2C%20mobile%20device%29.%20While%20convenient%2C%20this%0Arotation-dominant%20motion%20and%20narrow%20baseline%20make%20accurate%20camera%20pose%20and%203D%0Apoint%20estimation%20challenging%2C%20especially%20in%20textureless%20indoor%20scenes.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20LighthouseGS%2C%20a%20novel%20framework%20inspired%0Aby%20the%20lighthouse-like%20sweeping%20motion%20of%20panoramic%20views.%20LighthouseGS%0Aleverages%20rough%20geometric%20priors%2C%20such%20as%20mobile%20device%20camera%20poses%20and%0Amonocular%20depth%20estimation%2C%20and%20utilizes%20the%20planar%20structures%20often%20found%20in%0Aindoor%20environments.%20We%20present%20a%20new%20initialization%20method%20called%20plane%0Ascaffold%20assembly%20to%20generate%20consistent%203D%20points%20on%20these%20structures%2C%0Afollowed%20by%20a%20stable%20pruning%20strategy%20to%20enhance%20geometry%20and%20optimization%0Astability.%20Additionally%2C%20we%20introduce%20geometric%20and%20photometric%20corrections%20to%0Aresolve%20inconsistencies%20from%20motion%20drift%20and%20auto-exposure%20in%20mobile%20devices.%0ATested%20on%20collected%20real%20and%20synthetic%20indoor%20scenes%2C%20LighthouseGS%20delivers%0Aphotorealistic%20rendering%2C%20surpassing%20state-of-the-art%20methods%20and%20demonstrating%0Athe%20potential%20for%20panoramic%20view%20synthesis%20and%20object%20placement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLighthouseGS%253A%2520Indoor%2520Structure-aware%25203D%2520Gaussian%2520Splatting%2520for%250A%2520%2520Panorama-Style%2520Mobile%2520Captures%26entry.906535625%3DSeungoh%2520Han%2520and%2520Jaehoon%2520Jang%2520and%2520Hyunsu%2520Kim%2520and%2520Jaeheung%2520Surh%2520and%2520Junhyung%2520Kwak%2520and%2520Hyowon%2520Ha%2520and%2520Kyungdon%2520Joo%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520enabled%2520real-time%2520novel%250Aview%2520synthesis%2520%2528NVS%2529%2520with%2520impressive%2520quality%2520in%2520indoor%2520scenes.%2520However%252C%250Aachieving%2520high-fidelity%2520rendering%2520requires%2520meticulously%2520captured%2520images%250Acovering%2520the%2520entire%2520scene%252C%2520limiting%2520accessibility%2520for%2520general%2520users.%2520We%2520aim%2520to%250Adevelop%2520a%2520practical%25203DGS-based%2520NVS%2520framework%2520using%2520simple%2520panorama-style%2520motion%250Awith%2520a%2520handheld%2520camera%2520%2528e.g.%252C%2520mobile%2520device%2529.%2520While%2520convenient%252C%2520this%250Arotation-dominant%2520motion%2520and%2520narrow%2520baseline%2520make%2520accurate%2520camera%2520pose%2520and%25203D%250Apoint%2520estimation%2520challenging%252C%2520especially%2520in%2520textureless%2520indoor%2520scenes.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520LighthouseGS%252C%2520a%2520novel%2520framework%2520inspired%250Aby%2520the%2520lighthouse-like%2520sweeping%2520motion%2520of%2520panoramic%2520views.%2520LighthouseGS%250Aleverages%2520rough%2520geometric%2520priors%252C%2520such%2520as%2520mobile%2520device%2520camera%2520poses%2520and%250Amonocular%2520depth%2520estimation%252C%2520and%2520utilizes%2520the%2520planar%2520structures%2520often%2520found%2520in%250Aindoor%2520environments.%2520We%2520present%2520a%2520new%2520initialization%2520method%2520called%2520plane%250Ascaffold%2520assembly%2520to%2520generate%2520consistent%25203D%2520points%2520on%2520these%2520structures%252C%250Afollowed%2520by%2520a%2520stable%2520pruning%2520strategy%2520to%2520enhance%2520geometry%2520and%2520optimization%250Astability.%2520Additionally%252C%2520we%2520introduce%2520geometric%2520and%2520photometric%2520corrections%2520to%250Aresolve%2520inconsistencies%2520from%2520motion%2520drift%2520and%2520auto-exposure%2520in%2520mobile%2520devices.%250ATested%2520on%2520collected%2520real%2520and%2520synthetic%2520indoor%2520scenes%252C%2520LighthouseGS%2520delivers%250Aphotorealistic%2520rendering%252C%2520surpassing%2520state-of-the-art%2520methods%2520and%2520demonstrating%250Athe%2520potential%2520for%2520panoramic%2520view%2520synthesis%2520and%2520object%2520placement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LighthouseGS%3A%20Indoor%20Structure-aware%203D%20Gaussian%20Splatting%20for%0A%20%20Panorama-Style%20Mobile%20Captures&entry.906535625=Seungoh%20Han%20and%20Jaehoon%20Jang%20and%20Hyunsu%20Kim%20and%20Jaeheung%20Surh%20and%20Junhyung%20Kwak%20and%20Hyowon%20Ha%20and%20Kyungdon%20Joo&entry.1292438233=%20%20Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20enabled%20real-time%20novel%0Aview%20synthesis%20%28NVS%29%20with%20impressive%20quality%20in%20indoor%20scenes.%20However%2C%0Aachieving%20high-fidelity%20rendering%20requires%20meticulously%20captured%20images%0Acovering%20the%20entire%20scene%2C%20limiting%20accessibility%20for%20general%20users.%20We%20aim%20to%0Adevelop%20a%20practical%203DGS-based%20NVS%20framework%20using%20simple%20panorama-style%20motion%0Awith%20a%20handheld%20camera%20%28e.g.%2C%20mobile%20device%29.%20While%20convenient%2C%20this%0Arotation-dominant%20motion%20and%20narrow%20baseline%20make%20accurate%20camera%20pose%20and%203D%0Apoint%20estimation%20challenging%2C%20especially%20in%20textureless%20indoor%20scenes.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20LighthouseGS%2C%20a%20novel%20framework%20inspired%0Aby%20the%20lighthouse-like%20sweeping%20motion%20of%20panoramic%20views.%20LighthouseGS%0Aleverages%20rough%20geometric%20priors%2C%20such%20as%20mobile%20device%20camera%20poses%20and%0Amonocular%20depth%20estimation%2C%20and%20utilizes%20the%20planar%20structures%20often%20found%20in%0Aindoor%20environments.%20We%20present%20a%20new%20initialization%20method%20called%20plane%0Ascaffold%20assembly%20to%20generate%20consistent%203D%20points%20on%20these%20structures%2C%0Afollowed%20by%20a%20stable%20pruning%20strategy%20to%20enhance%20geometry%20and%20optimization%0Astability.%20Additionally%2C%20we%20introduce%20geometric%20and%20photometric%20corrections%20to%0Aresolve%20inconsistencies%20from%20motion%20drift%20and%20auto-exposure%20in%20mobile%20devices.%0ATested%20on%20collected%20real%20and%20synthetic%20indoor%20scenes%2C%20LighthouseGS%20delivers%0Aphotorealistic%20rendering%2C%20surpassing%20state-of-the-art%20methods%20and%20demonstrating%0Athe%20potential%20for%20panoramic%20view%20synthesis%20and%20object%20placement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06109v1&entry.124074799=Read"},
{"title": "VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis", "author": "Alexandre Symeonidis-Herzig and \u00d6zge Mercano\u011flu Sincan and Richard Bowden", "abstract": "  Realistic, high-fidelity 3D facial animations are crucial for expressive\navatar systems in human-computer interaction and accessibility. Although prior\nmethods show promising quality, their reliance on the mesh domain limits their\nability to fully leverage the rapid visual innovations seen in 2D computer\nvision and graphics. We propose VisualSpeaker, a novel method that bridges this\ngap using photorealistic differentiable rendering, supervised by visual speech\nrecognition, for improved 3D facial animation. Our contribution is a perceptual\nlip-reading loss, derived by passing photorealistic 3D Gaussian Splatting\navatar renders through a pre-trained Visual Automatic Speech Recognition model\nduring training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker\nimproves both the standard Lip Vertex Error metric by 56.1% and the perceptual\nquality of the generated animations, while retaining the controllability of\nmesh-driven animation. This perceptual focus naturally supports accurate\nmouthings, essential cues that disambiguate similar manual signs in sign\nlanguage avatars.\n", "link": "http://arxiv.org/abs/2507.06060v1", "date": "2025-07-08", "relevancy": 3.1291, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6419}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6419}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisualSpeaker%3A%20Visually-Guided%203D%20Avatar%20Lip%20Synthesis&body=Title%3A%20VisualSpeaker%3A%20Visually-Guided%203D%20Avatar%20Lip%20Synthesis%0AAuthor%3A%20Alexandre%20Symeonidis-Herzig%20and%20%C3%96zge%20Mercano%C4%9Flu%20Sincan%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Realistic%2C%20high-fidelity%203D%20facial%20animations%20are%20crucial%20for%20expressive%0Aavatar%20systems%20in%20human-computer%20interaction%20and%20accessibility.%20Although%20prior%0Amethods%20show%20promising%20quality%2C%20their%20reliance%20on%20the%20mesh%20domain%20limits%20their%0Aability%20to%20fully%20leverage%20the%20rapid%20visual%20innovations%20seen%20in%202D%20computer%0Avision%20and%20graphics.%20We%20propose%20VisualSpeaker%2C%20a%20novel%20method%20that%20bridges%20this%0Agap%20using%20photorealistic%20differentiable%20rendering%2C%20supervised%20by%20visual%20speech%0Arecognition%2C%20for%20improved%203D%20facial%20animation.%20Our%20contribution%20is%20a%20perceptual%0Alip-reading%20loss%2C%20derived%20by%20passing%20photorealistic%203D%20Gaussian%20Splatting%0Aavatar%20renders%20through%20a%20pre-trained%20Visual%20Automatic%20Speech%20Recognition%20model%0Aduring%20training.%20Evaluation%20on%20the%20MEAD%20dataset%20demonstrates%20that%20VisualSpeaker%0Aimproves%20both%20the%20standard%20Lip%20Vertex%20Error%20metric%20by%2056.1%25%20and%20the%20perceptual%0Aquality%20of%20the%20generated%20animations%2C%20while%20retaining%20the%20controllability%20of%0Amesh-driven%20animation.%20This%20perceptual%20focus%20naturally%20supports%20accurate%0Amouthings%2C%20essential%20cues%20that%20disambiguate%20similar%20manual%20signs%20in%20sign%0Alanguage%20avatars.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualSpeaker%253A%2520Visually-Guided%25203D%2520Avatar%2520Lip%2520Synthesis%26entry.906535625%3DAlexandre%2520Symeonidis-Herzig%2520and%2520%25C3%2596zge%2520Mercano%25C4%259Flu%2520Sincan%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Realistic%252C%2520high-fidelity%25203D%2520facial%2520animations%2520are%2520crucial%2520for%2520expressive%250Aavatar%2520systems%2520in%2520human-computer%2520interaction%2520and%2520accessibility.%2520Although%2520prior%250Amethods%2520show%2520promising%2520quality%252C%2520their%2520reliance%2520on%2520the%2520mesh%2520domain%2520limits%2520their%250Aability%2520to%2520fully%2520leverage%2520the%2520rapid%2520visual%2520innovations%2520seen%2520in%25202D%2520computer%250Avision%2520and%2520graphics.%2520We%2520propose%2520VisualSpeaker%252C%2520a%2520novel%2520method%2520that%2520bridges%2520this%250Agap%2520using%2520photorealistic%2520differentiable%2520rendering%252C%2520supervised%2520by%2520visual%2520speech%250Arecognition%252C%2520for%2520improved%25203D%2520facial%2520animation.%2520Our%2520contribution%2520is%2520a%2520perceptual%250Alip-reading%2520loss%252C%2520derived%2520by%2520passing%2520photorealistic%25203D%2520Gaussian%2520Splatting%250Aavatar%2520renders%2520through%2520a%2520pre-trained%2520Visual%2520Automatic%2520Speech%2520Recognition%2520model%250Aduring%2520training.%2520Evaluation%2520on%2520the%2520MEAD%2520dataset%2520demonstrates%2520that%2520VisualSpeaker%250Aimproves%2520both%2520the%2520standard%2520Lip%2520Vertex%2520Error%2520metric%2520by%252056.1%2525%2520and%2520the%2520perceptual%250Aquality%2520of%2520the%2520generated%2520animations%252C%2520while%2520retaining%2520the%2520controllability%2520of%250Amesh-driven%2520animation.%2520This%2520perceptual%2520focus%2520naturally%2520supports%2520accurate%250Amouthings%252C%2520essential%2520cues%2520that%2520disambiguate%2520similar%2520manual%2520signs%2520in%2520sign%250Alanguage%2520avatars.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisualSpeaker%3A%20Visually-Guided%203D%20Avatar%20Lip%20Synthesis&entry.906535625=Alexandre%20Symeonidis-Herzig%20and%20%C3%96zge%20Mercano%C4%9Flu%20Sincan%20and%20Richard%20Bowden&entry.1292438233=%20%20Realistic%2C%20high-fidelity%203D%20facial%20animations%20are%20crucial%20for%20expressive%0Aavatar%20systems%20in%20human-computer%20interaction%20and%20accessibility.%20Although%20prior%0Amethods%20show%20promising%20quality%2C%20their%20reliance%20on%20the%20mesh%20domain%20limits%20their%0Aability%20to%20fully%20leverage%20the%20rapid%20visual%20innovations%20seen%20in%202D%20computer%0Avision%20and%20graphics.%20We%20propose%20VisualSpeaker%2C%20a%20novel%20method%20that%20bridges%20this%0Agap%20using%20photorealistic%20differentiable%20rendering%2C%20supervised%20by%20visual%20speech%0Arecognition%2C%20for%20improved%203D%20facial%20animation.%20Our%20contribution%20is%20a%20perceptual%0Alip-reading%20loss%2C%20derived%20by%20passing%20photorealistic%203D%20Gaussian%20Splatting%0Aavatar%20renders%20through%20a%20pre-trained%20Visual%20Automatic%20Speech%20Recognition%20model%0Aduring%20training.%20Evaluation%20on%20the%20MEAD%20dataset%20demonstrates%20that%20VisualSpeaker%0Aimproves%20both%20the%20standard%20Lip%20Vertex%20Error%20metric%20by%2056.1%25%20and%20the%20perceptual%0Aquality%20of%20the%20generated%20animations%2C%20while%20retaining%20the%20controllability%20of%0Amesh-driven%20animation.%20This%20perceptual%20focus%20naturally%20supports%20accurate%0Amouthings%2C%20essential%20cues%20that%20disambiguate%20similar%20manual%20signs%20in%20sign%0Alanguage%20avatars.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06060v1&entry.124074799=Read"},
{"title": "TDRI: Two-Phase Dialogue Refinement and Co-Adaptation for Interactive\n  Image Generation", "author": "Yuheng Feng and Jianhui Wang and Kun Li and Sida Li and Tianyu Shi and Haoyue Han and Miao Zhang and Xueqian Wang", "abstract": "  Although text-to-image generation technologies have made significant\nadvancements, they still face challenges when dealing with ambiguous prompts\nand aligning outputs with user intent.Our proposed framework, TDRI (Two-Phase\nDialogue Refinement and Co-Adaptation), addresses these issues by enhancing\nimage generation through iterative user interaction. It consists of two phases:\nthe Initial Generation Phase, which creates base images based on user prompts,\nand the Interactive Refinement Phase, which integrates user feedback through\nthree key modules. The Dialogue-to-Prompt (D2P) module ensures that user\nfeedback is effectively transformed into actionable prompts, which improves the\nalignment between user intent and model input. By evaluating generated outputs\nagainst user expectations, the Feedback-Reflection (FR) module identifies\ndiscrepancies and facilitates improvements. In an effort to ensure consistently\nhigh-quality results, the Adaptive Optimization (AO) module fine-tunes the\ngeneration process by balancing user preferences and maintaining prompt\nfidelity. Experimental results show that TDRI outperforms existing methods by\nachieving 33.6% human preference, compared to 6.2% for GPT-4 augmentation, and\nthe highest CLIP and BLIP alignment scores (0.338 and 0.336, respectively). In\niterative feedback tasks, user satisfaction increased to 88% after 8 rounds,\nwith diminishing returns beyond 6 rounds. Furthermore, TDRI has been found to\nreduce the number of iterations and improve personalization in the creation of\nfashion products. TDRI exhibits a strong potential for a wide range of\napplications in the creative and industrial domains, as it streamlines the\ncreative process and improves alignment with user preferences\n", "link": "http://arxiv.org/abs/2503.17669v3", "date": "2025-07-08", "relevancy": 3.0026, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.614}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5939}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TDRI%3A%20Two-Phase%20Dialogue%20Refinement%20and%20Co-Adaptation%20for%20Interactive%0A%20%20Image%20Generation&body=Title%3A%20TDRI%3A%20Two-Phase%20Dialogue%20Refinement%20and%20Co-Adaptation%20for%20Interactive%0A%20%20Image%20Generation%0AAuthor%3A%20Yuheng%20Feng%20and%20Jianhui%20Wang%20and%20Kun%20Li%20and%20Sida%20Li%20and%20Tianyu%20Shi%20and%20Haoyue%20Han%20and%20Miao%20Zhang%20and%20Xueqian%20Wang%0AAbstract%3A%20%20%20Although%20text-to-image%20generation%20technologies%20have%20made%20significant%0Aadvancements%2C%20they%20still%20face%20challenges%20when%20dealing%20with%20ambiguous%20prompts%0Aand%20aligning%20outputs%20with%20user%20intent.Our%20proposed%20framework%2C%20TDRI%20%28Two-Phase%0ADialogue%20Refinement%20and%20Co-Adaptation%29%2C%20addresses%20these%20issues%20by%20enhancing%0Aimage%20generation%20through%20iterative%20user%20interaction.%20It%20consists%20of%20two%20phases%3A%0Athe%20Initial%20Generation%20Phase%2C%20which%20creates%20base%20images%20based%20on%20user%20prompts%2C%0Aand%20the%20Interactive%20Refinement%20Phase%2C%20which%20integrates%20user%20feedback%20through%0Athree%20key%20modules.%20The%20Dialogue-to-Prompt%20%28D2P%29%20module%20ensures%20that%20user%0Afeedback%20is%20effectively%20transformed%20into%20actionable%20prompts%2C%20which%20improves%20the%0Aalignment%20between%20user%20intent%20and%20model%20input.%20By%20evaluating%20generated%20outputs%0Aagainst%20user%20expectations%2C%20the%20Feedback-Reflection%20%28FR%29%20module%20identifies%0Adiscrepancies%20and%20facilitates%20improvements.%20In%20an%20effort%20to%20ensure%20consistently%0Ahigh-quality%20results%2C%20the%20Adaptive%20Optimization%20%28AO%29%20module%20fine-tunes%20the%0Ageneration%20process%20by%20balancing%20user%20preferences%20and%20maintaining%20prompt%0Afidelity.%20Experimental%20results%20show%20that%20TDRI%20outperforms%20existing%20methods%20by%0Aachieving%2033.6%25%20human%20preference%2C%20compared%20to%206.2%25%20for%20GPT-4%20augmentation%2C%20and%0Athe%20highest%20CLIP%20and%20BLIP%20alignment%20scores%20%280.338%20and%200.336%2C%20respectively%29.%20In%0Aiterative%20feedback%20tasks%2C%20user%20satisfaction%20increased%20to%2088%25%20after%208%20rounds%2C%0Awith%20diminishing%20returns%20beyond%206%20rounds.%20Furthermore%2C%20TDRI%20has%20been%20found%20to%0Areduce%20the%20number%20of%20iterations%20and%20improve%20personalization%20in%20the%20creation%20of%0Afashion%20products.%20TDRI%20exhibits%20a%20strong%20potential%20for%20a%20wide%20range%20of%0Aapplications%20in%20the%20creative%20and%20industrial%20domains%2C%20as%20it%20streamlines%20the%0Acreative%20process%20and%20improves%20alignment%20with%20user%20preferences%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17669v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTDRI%253A%2520Two-Phase%2520Dialogue%2520Refinement%2520and%2520Co-Adaptation%2520for%2520Interactive%250A%2520%2520Image%2520Generation%26entry.906535625%3DYuheng%2520Feng%2520and%2520Jianhui%2520Wang%2520and%2520Kun%2520Li%2520and%2520Sida%2520Li%2520and%2520Tianyu%2520Shi%2520and%2520Haoyue%2520Han%2520and%2520Miao%2520Zhang%2520and%2520Xueqian%2520Wang%26entry.1292438233%3D%2520%2520Although%2520text-to-image%2520generation%2520technologies%2520have%2520made%2520significant%250Aadvancements%252C%2520they%2520still%2520face%2520challenges%2520when%2520dealing%2520with%2520ambiguous%2520prompts%250Aand%2520aligning%2520outputs%2520with%2520user%2520intent.Our%2520proposed%2520framework%252C%2520TDRI%2520%2528Two-Phase%250ADialogue%2520Refinement%2520and%2520Co-Adaptation%2529%252C%2520addresses%2520these%2520issues%2520by%2520enhancing%250Aimage%2520generation%2520through%2520iterative%2520user%2520interaction.%2520It%2520consists%2520of%2520two%2520phases%253A%250Athe%2520Initial%2520Generation%2520Phase%252C%2520which%2520creates%2520base%2520images%2520based%2520on%2520user%2520prompts%252C%250Aand%2520the%2520Interactive%2520Refinement%2520Phase%252C%2520which%2520integrates%2520user%2520feedback%2520through%250Athree%2520key%2520modules.%2520The%2520Dialogue-to-Prompt%2520%2528D2P%2529%2520module%2520ensures%2520that%2520user%250Afeedback%2520is%2520effectively%2520transformed%2520into%2520actionable%2520prompts%252C%2520which%2520improves%2520the%250Aalignment%2520between%2520user%2520intent%2520and%2520model%2520input.%2520By%2520evaluating%2520generated%2520outputs%250Aagainst%2520user%2520expectations%252C%2520the%2520Feedback-Reflection%2520%2528FR%2529%2520module%2520identifies%250Adiscrepancies%2520and%2520facilitates%2520improvements.%2520In%2520an%2520effort%2520to%2520ensure%2520consistently%250Ahigh-quality%2520results%252C%2520the%2520Adaptive%2520Optimization%2520%2528AO%2529%2520module%2520fine-tunes%2520the%250Ageneration%2520process%2520by%2520balancing%2520user%2520preferences%2520and%2520maintaining%2520prompt%250Afidelity.%2520Experimental%2520results%2520show%2520that%2520TDRI%2520outperforms%2520existing%2520methods%2520by%250Aachieving%252033.6%2525%2520human%2520preference%252C%2520compared%2520to%25206.2%2525%2520for%2520GPT-4%2520augmentation%252C%2520and%250Athe%2520highest%2520CLIP%2520and%2520BLIP%2520alignment%2520scores%2520%25280.338%2520and%25200.336%252C%2520respectively%2529.%2520In%250Aiterative%2520feedback%2520tasks%252C%2520user%2520satisfaction%2520increased%2520to%252088%2525%2520after%25208%2520rounds%252C%250Awith%2520diminishing%2520returns%2520beyond%25206%2520rounds.%2520Furthermore%252C%2520TDRI%2520has%2520been%2520found%2520to%250Areduce%2520the%2520number%2520of%2520iterations%2520and%2520improve%2520personalization%2520in%2520the%2520creation%2520of%250Afashion%2520products.%2520TDRI%2520exhibits%2520a%2520strong%2520potential%2520for%2520a%2520wide%2520range%2520of%250Aapplications%2520in%2520the%2520creative%2520and%2520industrial%2520domains%252C%2520as%2520it%2520streamlines%2520the%250Acreative%2520process%2520and%2520improves%2520alignment%2520with%2520user%2520preferences%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17669v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TDRI%3A%20Two-Phase%20Dialogue%20Refinement%20and%20Co-Adaptation%20for%20Interactive%0A%20%20Image%20Generation&entry.906535625=Yuheng%20Feng%20and%20Jianhui%20Wang%20and%20Kun%20Li%20and%20Sida%20Li%20and%20Tianyu%20Shi%20and%20Haoyue%20Han%20and%20Miao%20Zhang%20and%20Xueqian%20Wang&entry.1292438233=%20%20Although%20text-to-image%20generation%20technologies%20have%20made%20significant%0Aadvancements%2C%20they%20still%20face%20challenges%20when%20dealing%20with%20ambiguous%20prompts%0Aand%20aligning%20outputs%20with%20user%20intent.Our%20proposed%20framework%2C%20TDRI%20%28Two-Phase%0ADialogue%20Refinement%20and%20Co-Adaptation%29%2C%20addresses%20these%20issues%20by%20enhancing%0Aimage%20generation%20through%20iterative%20user%20interaction.%20It%20consists%20of%20two%20phases%3A%0Athe%20Initial%20Generation%20Phase%2C%20which%20creates%20base%20images%20based%20on%20user%20prompts%2C%0Aand%20the%20Interactive%20Refinement%20Phase%2C%20which%20integrates%20user%20feedback%20through%0Athree%20key%20modules.%20The%20Dialogue-to-Prompt%20%28D2P%29%20module%20ensures%20that%20user%0Afeedback%20is%20effectively%20transformed%20into%20actionable%20prompts%2C%20which%20improves%20the%0Aalignment%20between%20user%20intent%20and%20model%20input.%20By%20evaluating%20generated%20outputs%0Aagainst%20user%20expectations%2C%20the%20Feedback-Reflection%20%28FR%29%20module%20identifies%0Adiscrepancies%20and%20facilitates%20improvements.%20In%20an%20effort%20to%20ensure%20consistently%0Ahigh-quality%20results%2C%20the%20Adaptive%20Optimization%20%28AO%29%20module%20fine-tunes%20the%0Ageneration%20process%20by%20balancing%20user%20preferences%20and%20maintaining%20prompt%0Afidelity.%20Experimental%20results%20show%20that%20TDRI%20outperforms%20existing%20methods%20by%0Aachieving%2033.6%25%20human%20preference%2C%20compared%20to%206.2%25%20for%20GPT-4%20augmentation%2C%20and%0Athe%20highest%20CLIP%20and%20BLIP%20alignment%20scores%20%280.338%20and%200.336%2C%20respectively%29.%20In%0Aiterative%20feedback%20tasks%2C%20user%20satisfaction%20increased%20to%2088%25%20after%208%20rounds%2C%0Awith%20diminishing%20returns%20beyond%206%20rounds.%20Furthermore%2C%20TDRI%20has%20been%20found%20to%0Areduce%20the%20number%20of%20iterations%20and%20improve%20personalization%20in%20the%20creation%20of%0Afashion%20products.%20TDRI%20exhibits%20a%20strong%20potential%20for%20a%20wide%20range%20of%0Aapplications%20in%20the%20creative%20and%20industrial%20domains%2C%20as%20it%20streamlines%20the%0Acreative%20process%20and%20improves%20alignment%20with%20user%20preferences%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17669v3&entry.124074799=Read"},
{"title": "Learning to Track Any Points from Human Motion", "author": "In\u00e8s Hyeonsu Kim and Seokju Cho and Jahyeok Koo and Junghyun Park and Jiahui Huang and Joon-Young Lee and Seungryong Kim", "abstract": "  Human motion, with its inherent complexities, such as non-rigid deformations,\narticulated movements, clothing distortions, and frequent occlusions caused by\nlimbs or other individuals, provides a rich and challenging source of\nsupervision that is crucial for training robust and generalizable point\ntrackers. Despite the suitability of human motion, acquiring extensive training\ndata for point tracking remains difficult due to laborious manual annotation.\nOur proposed pipeline, AnthroTAP, addresses this by proposing an automated\npipeline to generate pseudo-labeled training data, leveraging the Skinned\nMulti-Person Linear (SMPL) model. We first fit the SMPL model to detected\nhumans in video frames, project the resulting 3D mesh vertices onto 2D image\nplanes to generate pseudo-trajectories, handle occlusions using ray-casting,\nand filter out unreliable tracks based on optical flow consistency. A point\ntracking model trained on AnthroTAP annotated dataset achieves state-of-the-art\nperformance on the TAP-Vid benchmark, surpassing other models trained on real\nvideos while using 10,000 times less data and only 1 day in 4 GPUs, compared to\n256 GPUs used in recent state-of-the-art.\n", "link": "http://arxiv.org/abs/2507.06233v1", "date": "2025-07-08", "relevancy": 2.9721, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.634}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5778}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Track%20Any%20Points%20from%20Human%20Motion&body=Title%3A%20Learning%20to%20Track%20Any%20Points%20from%20Human%20Motion%0AAuthor%3A%20In%C3%A8s%20Hyeonsu%20Kim%20and%20Seokju%20Cho%20and%20Jahyeok%20Koo%20and%20Junghyun%20Park%20and%20Jiahui%20Huang%20and%20Joon-Young%20Lee%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20Human%20motion%2C%20with%20its%20inherent%20complexities%2C%20such%20as%20non-rigid%20deformations%2C%0Aarticulated%20movements%2C%20clothing%20distortions%2C%20and%20frequent%20occlusions%20caused%20by%0Alimbs%20or%20other%20individuals%2C%20provides%20a%20rich%20and%20challenging%20source%20of%0Asupervision%20that%20is%20crucial%20for%20training%20robust%20and%20generalizable%20point%0Atrackers.%20Despite%20the%20suitability%20of%20human%20motion%2C%20acquiring%20extensive%20training%0Adata%20for%20point%20tracking%20remains%20difficult%20due%20to%20laborious%20manual%20annotation.%0AOur%20proposed%20pipeline%2C%20AnthroTAP%2C%20addresses%20this%20by%20proposing%20an%20automated%0Apipeline%20to%20generate%20pseudo-labeled%20training%20data%2C%20leveraging%20the%20Skinned%0AMulti-Person%20Linear%20%28SMPL%29%20model.%20We%20first%20fit%20the%20SMPL%20model%20to%20detected%0Ahumans%20in%20video%20frames%2C%20project%20the%20resulting%203D%20mesh%20vertices%20onto%202D%20image%0Aplanes%20to%20generate%20pseudo-trajectories%2C%20handle%20occlusions%20using%20ray-casting%2C%0Aand%20filter%20out%20unreliable%20tracks%20based%20on%20optical%20flow%20consistency.%20A%20point%0Atracking%20model%20trained%20on%20AnthroTAP%20annotated%20dataset%20achieves%20state-of-the-art%0Aperformance%20on%20the%20TAP-Vid%20benchmark%2C%20surpassing%20other%20models%20trained%20on%20real%0Avideos%20while%20using%2010%2C000%20times%20less%20data%20and%20only%201%20day%20in%204%20GPUs%2C%20compared%20to%0A256%20GPUs%20used%20in%20recent%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Track%2520Any%2520Points%2520from%2520Human%2520Motion%26entry.906535625%3DIn%25C3%25A8s%2520Hyeonsu%2520Kim%2520and%2520Seokju%2520Cho%2520and%2520Jahyeok%2520Koo%2520and%2520Junghyun%2520Park%2520and%2520Jiahui%2520Huang%2520and%2520Joon-Young%2520Lee%2520and%2520Seungryong%2520Kim%26entry.1292438233%3D%2520%2520Human%2520motion%252C%2520with%2520its%2520inherent%2520complexities%252C%2520such%2520as%2520non-rigid%2520deformations%252C%250Aarticulated%2520movements%252C%2520clothing%2520distortions%252C%2520and%2520frequent%2520occlusions%2520caused%2520by%250Alimbs%2520or%2520other%2520individuals%252C%2520provides%2520a%2520rich%2520and%2520challenging%2520source%2520of%250Asupervision%2520that%2520is%2520crucial%2520for%2520training%2520robust%2520and%2520generalizable%2520point%250Atrackers.%2520Despite%2520the%2520suitability%2520of%2520human%2520motion%252C%2520acquiring%2520extensive%2520training%250Adata%2520for%2520point%2520tracking%2520remains%2520difficult%2520due%2520to%2520laborious%2520manual%2520annotation.%250AOur%2520proposed%2520pipeline%252C%2520AnthroTAP%252C%2520addresses%2520this%2520by%2520proposing%2520an%2520automated%250Apipeline%2520to%2520generate%2520pseudo-labeled%2520training%2520data%252C%2520leveraging%2520the%2520Skinned%250AMulti-Person%2520Linear%2520%2528SMPL%2529%2520model.%2520We%2520first%2520fit%2520the%2520SMPL%2520model%2520to%2520detected%250Ahumans%2520in%2520video%2520frames%252C%2520project%2520the%2520resulting%25203D%2520mesh%2520vertices%2520onto%25202D%2520image%250Aplanes%2520to%2520generate%2520pseudo-trajectories%252C%2520handle%2520occlusions%2520using%2520ray-casting%252C%250Aand%2520filter%2520out%2520unreliable%2520tracks%2520based%2520on%2520optical%2520flow%2520consistency.%2520A%2520point%250Atracking%2520model%2520trained%2520on%2520AnthroTAP%2520annotated%2520dataset%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520the%2520TAP-Vid%2520benchmark%252C%2520surpassing%2520other%2520models%2520trained%2520on%2520real%250Avideos%2520while%2520using%252010%252C000%2520times%2520less%2520data%2520and%2520only%25201%2520day%2520in%25204%2520GPUs%252C%2520compared%2520to%250A256%2520GPUs%2520used%2520in%2520recent%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Track%20Any%20Points%20from%20Human%20Motion&entry.906535625=In%C3%A8s%20Hyeonsu%20Kim%20and%20Seokju%20Cho%20and%20Jahyeok%20Koo%20and%20Junghyun%20Park%20and%20Jiahui%20Huang%20and%20Joon-Young%20Lee%20and%20Seungryong%20Kim&entry.1292438233=%20%20Human%20motion%2C%20with%20its%20inherent%20complexities%2C%20such%20as%20non-rigid%20deformations%2C%0Aarticulated%20movements%2C%20clothing%20distortions%2C%20and%20frequent%20occlusions%20caused%20by%0Alimbs%20or%20other%20individuals%2C%20provides%20a%20rich%20and%20challenging%20source%20of%0Asupervision%20that%20is%20crucial%20for%20training%20robust%20and%20generalizable%20point%0Atrackers.%20Despite%20the%20suitability%20of%20human%20motion%2C%20acquiring%20extensive%20training%0Adata%20for%20point%20tracking%20remains%20difficult%20due%20to%20laborious%20manual%20annotation.%0AOur%20proposed%20pipeline%2C%20AnthroTAP%2C%20addresses%20this%20by%20proposing%20an%20automated%0Apipeline%20to%20generate%20pseudo-labeled%20training%20data%2C%20leveraging%20the%20Skinned%0AMulti-Person%20Linear%20%28SMPL%29%20model.%20We%20first%20fit%20the%20SMPL%20model%20to%20detected%0Ahumans%20in%20video%20frames%2C%20project%20the%20resulting%203D%20mesh%20vertices%20onto%202D%20image%0Aplanes%20to%20generate%20pseudo-trajectories%2C%20handle%20occlusions%20using%20ray-casting%2C%0Aand%20filter%20out%20unreliable%20tracks%20based%20on%20optical%20flow%20consistency.%20A%20point%0Atracking%20model%20trained%20on%20AnthroTAP%20annotated%20dataset%20achieves%20state-of-the-art%0Aperformance%20on%20the%20TAP-Vid%20benchmark%2C%20surpassing%20other%20models%20trained%20on%20real%0Avideos%20while%20using%2010%2C000%20times%20less%20data%20and%20only%201%20day%20in%204%20GPUs%2C%20compared%20to%0A256%20GPUs%20used%20in%20recent%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06233v1&entry.124074799=Read"},
{"title": "MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions\n  by Disentangled Embedding", "author": "Chang Liu and Ye Pan and Chenyang Ding and Susanto Rahardja and Xiaokang Yang", "abstract": "  Audio-driven emotional 3D facial animation aims to generate synchronized lip\nmovements and vivid facial expressions. However, most existing approaches focus\non static and predefined emotion labels, limiting their diversity and\nnaturalness. To address these challenges, we propose MEDTalk, a novel framework\nfor fine-grained and dynamic emotional talking head generation. Our approach\nfirst disentangles content and emotion embedding spaces from motion sequences\nusing a carefully designed cross-reconstruction process, enabling independent\ncontrol over lip movements and facial expressions. Beyond conventional\naudio-driven lip synchronization, we integrate audio and speech text,\npredicting frame-wise intensity variations and dynamically adjusting static\nemotion features to generate realistic emotional expressions. Furthermore, to\nenhance control and personalization, we incorporate multimodal inputs-including\ntext descriptions and reference expression images-to guide the generation of\nuser-specified facial expressions. With MetaHuman as the priority, our\ngenerated results can be conveniently integrated into the industrial production\npipeline.\n", "link": "http://arxiv.org/abs/2507.06071v1", "date": "2025-07-08", "relevancy": 2.9526, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6415}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.565}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEDTalk%3A%20Multimodal%20Controlled%203D%20Facial%20Animation%20with%20Dynamic%20Emotions%0A%20%20by%20Disentangled%20Embedding&body=Title%3A%20MEDTalk%3A%20Multimodal%20Controlled%203D%20Facial%20Animation%20with%20Dynamic%20Emotions%0A%20%20by%20Disentangled%20Embedding%0AAuthor%3A%20Chang%20Liu%20and%20Ye%20Pan%20and%20Chenyang%20Ding%20and%20Susanto%20Rahardja%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20Audio-driven%20emotional%203D%20facial%20animation%20aims%20to%20generate%20synchronized%20lip%0Amovements%20and%20vivid%20facial%20expressions.%20However%2C%20most%20existing%20approaches%20focus%0Aon%20static%20and%20predefined%20emotion%20labels%2C%20limiting%20their%20diversity%20and%0Anaturalness.%20To%20address%20these%20challenges%2C%20we%20propose%20MEDTalk%2C%20a%20novel%20framework%0Afor%20fine-grained%20and%20dynamic%20emotional%20talking%20head%20generation.%20Our%20approach%0Afirst%20disentangles%20content%20and%20emotion%20embedding%20spaces%20from%20motion%20sequences%0Ausing%20a%20carefully%20designed%20cross-reconstruction%20process%2C%20enabling%20independent%0Acontrol%20over%20lip%20movements%20and%20facial%20expressions.%20Beyond%20conventional%0Aaudio-driven%20lip%20synchronization%2C%20we%20integrate%20audio%20and%20speech%20text%2C%0Apredicting%20frame-wise%20intensity%20variations%20and%20dynamically%20adjusting%20static%0Aemotion%20features%20to%20generate%20realistic%20emotional%20expressions.%20Furthermore%2C%20to%0Aenhance%20control%20and%20personalization%2C%20we%20incorporate%20multimodal%20inputs-including%0Atext%20descriptions%20and%20reference%20expression%20images-to%20guide%20the%20generation%20of%0Auser-specified%20facial%20expressions.%20With%20MetaHuman%20as%20the%20priority%2C%20our%0Agenerated%20results%20can%20be%20conveniently%20integrated%20into%20the%20industrial%20production%0Apipeline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEDTalk%253A%2520Multimodal%2520Controlled%25203D%2520Facial%2520Animation%2520with%2520Dynamic%2520Emotions%250A%2520%2520by%2520Disentangled%2520Embedding%26entry.906535625%3DChang%2520Liu%2520and%2520Ye%2520Pan%2520and%2520Chenyang%2520Ding%2520and%2520Susanto%2520Rahardja%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520Audio-driven%2520emotional%25203D%2520facial%2520animation%2520aims%2520to%2520generate%2520synchronized%2520lip%250Amovements%2520and%2520vivid%2520facial%2520expressions.%2520However%252C%2520most%2520existing%2520approaches%2520focus%250Aon%2520static%2520and%2520predefined%2520emotion%2520labels%252C%2520limiting%2520their%2520diversity%2520and%250Anaturalness.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520MEDTalk%252C%2520a%2520novel%2520framework%250Afor%2520fine-grained%2520and%2520dynamic%2520emotional%2520talking%2520head%2520generation.%2520Our%2520approach%250Afirst%2520disentangles%2520content%2520and%2520emotion%2520embedding%2520spaces%2520from%2520motion%2520sequences%250Ausing%2520a%2520carefully%2520designed%2520cross-reconstruction%2520process%252C%2520enabling%2520independent%250Acontrol%2520over%2520lip%2520movements%2520and%2520facial%2520expressions.%2520Beyond%2520conventional%250Aaudio-driven%2520lip%2520synchronization%252C%2520we%2520integrate%2520audio%2520and%2520speech%2520text%252C%250Apredicting%2520frame-wise%2520intensity%2520variations%2520and%2520dynamically%2520adjusting%2520static%250Aemotion%2520features%2520to%2520generate%2520realistic%2520emotional%2520expressions.%2520Furthermore%252C%2520to%250Aenhance%2520control%2520and%2520personalization%252C%2520we%2520incorporate%2520multimodal%2520inputs-including%250Atext%2520descriptions%2520and%2520reference%2520expression%2520images-to%2520guide%2520the%2520generation%2520of%250Auser-specified%2520facial%2520expressions.%2520With%2520MetaHuman%2520as%2520the%2520priority%252C%2520our%250Agenerated%2520results%2520can%2520be%2520conveniently%2520integrated%2520into%2520the%2520industrial%2520production%250Apipeline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEDTalk%3A%20Multimodal%20Controlled%203D%20Facial%20Animation%20with%20Dynamic%20Emotions%0A%20%20by%20Disentangled%20Embedding&entry.906535625=Chang%20Liu%20and%20Ye%20Pan%20and%20Chenyang%20Ding%20and%20Susanto%20Rahardja%20and%20Xiaokang%20Yang&entry.1292438233=%20%20Audio-driven%20emotional%203D%20facial%20animation%20aims%20to%20generate%20synchronized%20lip%0Amovements%20and%20vivid%20facial%20expressions.%20However%2C%20most%20existing%20approaches%20focus%0Aon%20static%20and%20predefined%20emotion%20labels%2C%20limiting%20their%20diversity%20and%0Anaturalness.%20To%20address%20these%20challenges%2C%20we%20propose%20MEDTalk%2C%20a%20novel%20framework%0Afor%20fine-grained%20and%20dynamic%20emotional%20talking%20head%20generation.%20Our%20approach%0Afirst%20disentangles%20content%20and%20emotion%20embedding%20spaces%20from%20motion%20sequences%0Ausing%20a%20carefully%20designed%20cross-reconstruction%20process%2C%20enabling%20independent%0Acontrol%20over%20lip%20movements%20and%20facial%20expressions.%20Beyond%20conventional%0Aaudio-driven%20lip%20synchronization%2C%20we%20integrate%20audio%20and%20speech%20text%2C%0Apredicting%20frame-wise%20intensity%20variations%20and%20dynamically%20adjusting%20static%0Aemotion%20features%20to%20generate%20realistic%20emotional%20expressions.%20Furthermore%2C%20to%0Aenhance%20control%20and%20personalization%2C%20we%20incorporate%20multimodal%20inputs-including%0Atext%20descriptions%20and%20reference%20expression%20images-to%20guide%20the%20generation%20of%0Auser-specified%20facial%20expressions.%20With%20MetaHuman%20as%20the%20priority%2C%20our%0Agenerated%20results%20can%20be%20conveniently%20integrated%20into%20the%20industrial%20production%0Apipeline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06071v1&entry.124074799=Read"},
{"title": "Enhancing Scientific Visual Question Answering through Multimodal\n  Reasoning and Ensemble Modeling", "author": "Prahitha Movva and Naga Harshita Marupaka", "abstract": "  Technical reports and articles often contain valuable information in the form\nof semi-structured data like charts, and figures. Interpreting these and using\nthe information from them is essential for downstream tasks such as question\nanswering (QA). Current approaches to visual question answering often struggle\nwith the precision required for scientific data interpretation, particularly in\nhandling numerical values, multi-step reasoning over visual elements, and\nmaintaining consistency between visual observation and textual reasoning. We\npresent our approach to the SciVQA 2025 shared task, focusing on answering\nvisual and non-visual questions grounded in scientific figures from scholarly\narticles.\n  We conducted a series of experiments using models with 5B to 8B parameters.\nOur strongest individual model, InternVL3, achieved ROUGE-1 and ROUGE-L F1\nscores of \\textbf{0.740} and a BERTScore of \\textbf{0.983} on the SciVQA test\nsplit. We also developed an ensemble model with multiple vision language models\n(VLMs). Through error analysis on the validation split, our ensemble approach\nimproved performance compared to most individual models, though InternVL3\nremained the strongest standalone performer. Our findings underscore the\neffectiveness of prompt optimization, chain-of-thought reasoning and ensemble\nmodeling in improving the model's ability in visual question answering.\n", "link": "http://arxiv.org/abs/2507.06183v1", "date": "2025-07-08", "relevancy": 2.9135, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6129}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6129}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Scientific%20Visual%20Question%20Answering%20through%20Multimodal%0A%20%20Reasoning%20and%20Ensemble%20Modeling&body=Title%3A%20Enhancing%20Scientific%20Visual%20Question%20Answering%20through%20Multimodal%0A%20%20Reasoning%20and%20Ensemble%20Modeling%0AAuthor%3A%20Prahitha%20Movva%20and%20Naga%20Harshita%20Marupaka%0AAbstract%3A%20%20%20Technical%20reports%20and%20articles%20often%20contain%20valuable%20information%20in%20the%20form%0Aof%20semi-structured%20data%20like%20charts%2C%20and%20figures.%20Interpreting%20these%20and%20using%0Athe%20information%20from%20them%20is%20essential%20for%20downstream%20tasks%20such%20as%20question%0Aanswering%20%28QA%29.%20Current%20approaches%20to%20visual%20question%20answering%20often%20struggle%0Awith%20the%20precision%20required%20for%20scientific%20data%20interpretation%2C%20particularly%20in%0Ahandling%20numerical%20values%2C%20multi-step%20reasoning%20over%20visual%20elements%2C%20and%0Amaintaining%20consistency%20between%20visual%20observation%20and%20textual%20reasoning.%20We%0Apresent%20our%20approach%20to%20the%20SciVQA%202025%20shared%20task%2C%20focusing%20on%20answering%0Avisual%20and%20non-visual%20questions%20grounded%20in%20scientific%20figures%20from%20scholarly%0Aarticles.%0A%20%20We%20conducted%20a%20series%20of%20experiments%20using%20models%20with%205B%20to%208B%20parameters.%0AOur%20strongest%20individual%20model%2C%20InternVL3%2C%20achieved%20ROUGE-1%20and%20ROUGE-L%20F1%0Ascores%20of%20%5Ctextbf%7B0.740%7D%20and%20a%20BERTScore%20of%20%5Ctextbf%7B0.983%7D%20on%20the%20SciVQA%20test%0Asplit.%20We%20also%20developed%20an%20ensemble%20model%20with%20multiple%20vision%20language%20models%0A%28VLMs%29.%20Through%20error%20analysis%20on%20the%20validation%20split%2C%20our%20ensemble%20approach%0Aimproved%20performance%20compared%20to%20most%20individual%20models%2C%20though%20InternVL3%0Aremained%20the%20strongest%20standalone%20performer.%20Our%20findings%20underscore%20the%0Aeffectiveness%20of%20prompt%20optimization%2C%20chain-of-thought%20reasoning%20and%20ensemble%0Amodeling%20in%20improving%20the%20model%27s%20ability%20in%20visual%20question%20answering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Scientific%2520Visual%2520Question%2520Answering%2520through%2520Multimodal%250A%2520%2520Reasoning%2520and%2520Ensemble%2520Modeling%26entry.906535625%3DPrahitha%2520Movva%2520and%2520Naga%2520Harshita%2520Marupaka%26entry.1292438233%3D%2520%2520Technical%2520reports%2520and%2520articles%2520often%2520contain%2520valuable%2520information%2520in%2520the%2520form%250Aof%2520semi-structured%2520data%2520like%2520charts%252C%2520and%2520figures.%2520Interpreting%2520these%2520and%2520using%250Athe%2520information%2520from%2520them%2520is%2520essential%2520for%2520downstream%2520tasks%2520such%2520as%2520question%250Aanswering%2520%2528QA%2529.%2520Current%2520approaches%2520to%2520visual%2520question%2520answering%2520often%2520struggle%250Awith%2520the%2520precision%2520required%2520for%2520scientific%2520data%2520interpretation%252C%2520particularly%2520in%250Ahandling%2520numerical%2520values%252C%2520multi-step%2520reasoning%2520over%2520visual%2520elements%252C%2520and%250Amaintaining%2520consistency%2520between%2520visual%2520observation%2520and%2520textual%2520reasoning.%2520We%250Apresent%2520our%2520approach%2520to%2520the%2520SciVQA%25202025%2520shared%2520task%252C%2520focusing%2520on%2520answering%250Avisual%2520and%2520non-visual%2520questions%2520grounded%2520in%2520scientific%2520figures%2520from%2520scholarly%250Aarticles.%250A%2520%2520We%2520conducted%2520a%2520series%2520of%2520experiments%2520using%2520models%2520with%25205B%2520to%25208B%2520parameters.%250AOur%2520strongest%2520individual%2520model%252C%2520InternVL3%252C%2520achieved%2520ROUGE-1%2520and%2520ROUGE-L%2520F1%250Ascores%2520of%2520%255Ctextbf%257B0.740%257D%2520and%2520a%2520BERTScore%2520of%2520%255Ctextbf%257B0.983%257D%2520on%2520the%2520SciVQA%2520test%250Asplit.%2520We%2520also%2520developed%2520an%2520ensemble%2520model%2520with%2520multiple%2520vision%2520language%2520models%250A%2528VLMs%2529.%2520Through%2520error%2520analysis%2520on%2520the%2520validation%2520split%252C%2520our%2520ensemble%2520approach%250Aimproved%2520performance%2520compared%2520to%2520most%2520individual%2520models%252C%2520though%2520InternVL3%250Aremained%2520the%2520strongest%2520standalone%2520performer.%2520Our%2520findings%2520underscore%2520the%250Aeffectiveness%2520of%2520prompt%2520optimization%252C%2520chain-of-thought%2520reasoning%2520and%2520ensemble%250Amodeling%2520in%2520improving%2520the%2520model%2527s%2520ability%2520in%2520visual%2520question%2520answering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Scientific%20Visual%20Question%20Answering%20through%20Multimodal%0A%20%20Reasoning%20and%20Ensemble%20Modeling&entry.906535625=Prahitha%20Movva%20and%20Naga%20Harshita%20Marupaka&entry.1292438233=%20%20Technical%20reports%20and%20articles%20often%20contain%20valuable%20information%20in%20the%20form%0Aof%20semi-structured%20data%20like%20charts%2C%20and%20figures.%20Interpreting%20these%20and%20using%0Athe%20information%20from%20them%20is%20essential%20for%20downstream%20tasks%20such%20as%20question%0Aanswering%20%28QA%29.%20Current%20approaches%20to%20visual%20question%20answering%20often%20struggle%0Awith%20the%20precision%20required%20for%20scientific%20data%20interpretation%2C%20particularly%20in%0Ahandling%20numerical%20values%2C%20multi-step%20reasoning%20over%20visual%20elements%2C%20and%0Amaintaining%20consistency%20between%20visual%20observation%20and%20textual%20reasoning.%20We%0Apresent%20our%20approach%20to%20the%20SciVQA%202025%20shared%20task%2C%20focusing%20on%20answering%0Avisual%20and%20non-visual%20questions%20grounded%20in%20scientific%20figures%20from%20scholarly%0Aarticles.%0A%20%20We%20conducted%20a%20series%20of%20experiments%20using%20models%20with%205B%20to%208B%20parameters.%0AOur%20strongest%20individual%20model%2C%20InternVL3%2C%20achieved%20ROUGE-1%20and%20ROUGE-L%20F1%0Ascores%20of%20%5Ctextbf%7B0.740%7D%20and%20a%20BERTScore%20of%20%5Ctextbf%7B0.983%7D%20on%20the%20SciVQA%20test%0Asplit.%20We%20also%20developed%20an%20ensemble%20model%20with%20multiple%20vision%20language%20models%0A%28VLMs%29.%20Through%20error%20analysis%20on%20the%20validation%20split%2C%20our%20ensemble%20approach%0Aimproved%20performance%20compared%20to%20most%20individual%20models%2C%20though%20InternVL3%0Aremained%20the%20strongest%20standalone%20performer.%20Our%20findings%20underscore%20the%0Aeffectiveness%20of%20prompt%20optimization%2C%20chain-of-thought%20reasoning%20and%20ensemble%0Amodeling%20in%20improving%20the%20model%27s%20ability%20in%20visual%20question%20answering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06183v1&entry.124074799=Read"},
{"title": "CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic\n  Images and Contextualized Captions", "author": "Yuchen Huang and Zhiyuan Fan and Zhitao He and Sandeep Polisetty and Wenyan Li and Yi R. Fung", "abstract": "  Pretrained vision-language models (VLMs) such as CLIP excel in multimodal\nunderstanding but struggle with contextually relevant fine-grained visual\nfeatures, making it difficult to distinguish visually similar yet culturally\ndistinct concepts. This limitation stems from the scarcity of high-quality\nculture-specific datasets, the lack of integrated contextual knowledge, and the\nabsence of hard negatives highlighting subtle distinctions. To address these\nchallenges, we first design a data curation pipeline that leverages\nopen-sourced VLMs and text-to-image diffusion models to construct CulTwin, a\nsynthetic cultural dataset. This dataset consists of paired\nconcept-caption-image triplets, where concepts visually resemble each other but\nrepresent different cultural contexts. Then, we fine-tune CLIP on CulTwin to\ncreate CultureCLIP, which aligns cultural concepts with contextually enhanced\ncaptions and synthetic images through customized contrastive learning, enabling\nfiner cultural differentiation while preserving generalization capabilities.\nExperiments on culturally relevant benchmarks show that CultureCLIP outperforms\nthe base CLIP, achieving up to a notable 5.49% improvement in fine-grained\nconcept recognition on certain tasks, while preserving CLIP's original\ngeneralization ability, validating the effectiveness of our data synthesis and\nVLM backbone training paradigm in capturing subtle cultural distinctions.\n", "link": "http://arxiv.org/abs/2507.06210v1", "date": "2025-07-08", "relevancy": 2.9003, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CultureCLIP%3A%20Empowering%20CLIP%20with%20Cultural%20Awareness%20through%20Synthetic%0A%20%20Images%20and%20Contextualized%20Captions&body=Title%3A%20CultureCLIP%3A%20Empowering%20CLIP%20with%20Cultural%20Awareness%20through%20Synthetic%0A%20%20Images%20and%20Contextualized%20Captions%0AAuthor%3A%20Yuchen%20Huang%20and%20Zhiyuan%20Fan%20and%20Zhitao%20He%20and%20Sandeep%20Polisetty%20and%20Wenyan%20Li%20and%20Yi%20R.%20Fung%0AAbstract%3A%20%20%20Pretrained%20vision-language%20models%20%28VLMs%29%20such%20as%20CLIP%20excel%20in%20multimodal%0Aunderstanding%20but%20struggle%20with%20contextually%20relevant%20fine-grained%20visual%0Afeatures%2C%20making%20it%20difficult%20to%20distinguish%20visually%20similar%20yet%20culturally%0Adistinct%20concepts.%20This%20limitation%20stems%20from%20the%20scarcity%20of%20high-quality%0Aculture-specific%20datasets%2C%20the%20lack%20of%20integrated%20contextual%20knowledge%2C%20and%20the%0Aabsence%20of%20hard%20negatives%20highlighting%20subtle%20distinctions.%20To%20address%20these%0Achallenges%2C%20we%20first%20design%20a%20data%20curation%20pipeline%20that%20leverages%0Aopen-sourced%20VLMs%20and%20text-to-image%20diffusion%20models%20to%20construct%20CulTwin%2C%20a%0Asynthetic%20cultural%20dataset.%20This%20dataset%20consists%20of%20paired%0Aconcept-caption-image%20triplets%2C%20where%20concepts%20visually%20resemble%20each%20other%20but%0Arepresent%20different%20cultural%20contexts.%20Then%2C%20we%20fine-tune%20CLIP%20on%20CulTwin%20to%0Acreate%20CultureCLIP%2C%20which%20aligns%20cultural%20concepts%20with%20contextually%20enhanced%0Acaptions%20and%20synthetic%20images%20through%20customized%20contrastive%20learning%2C%20enabling%0Afiner%20cultural%20differentiation%20while%20preserving%20generalization%20capabilities.%0AExperiments%20on%20culturally%20relevant%20benchmarks%20show%20that%20CultureCLIP%20outperforms%0Athe%20base%20CLIP%2C%20achieving%20up%20to%20a%20notable%205.49%25%20improvement%20in%20fine-grained%0Aconcept%20recognition%20on%20certain%20tasks%2C%20while%20preserving%20CLIP%27s%20original%0Ageneralization%20ability%2C%20validating%20the%20effectiveness%20of%20our%20data%20synthesis%20and%0AVLM%20backbone%20training%20paradigm%20in%20capturing%20subtle%20cultural%20distinctions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCultureCLIP%253A%2520Empowering%2520CLIP%2520with%2520Cultural%2520Awareness%2520through%2520Synthetic%250A%2520%2520Images%2520and%2520Contextualized%2520Captions%26entry.906535625%3DYuchen%2520Huang%2520and%2520Zhiyuan%2520Fan%2520and%2520Zhitao%2520He%2520and%2520Sandeep%2520Polisetty%2520and%2520Wenyan%2520Li%2520and%2520Yi%2520R.%2520Fung%26entry.1292438233%3D%2520%2520Pretrained%2520vision-language%2520models%2520%2528VLMs%2529%2520such%2520as%2520CLIP%2520excel%2520in%2520multimodal%250Aunderstanding%2520but%2520struggle%2520with%2520contextually%2520relevant%2520fine-grained%2520visual%250Afeatures%252C%2520making%2520it%2520difficult%2520to%2520distinguish%2520visually%2520similar%2520yet%2520culturally%250Adistinct%2520concepts.%2520This%2520limitation%2520stems%2520from%2520the%2520scarcity%2520of%2520high-quality%250Aculture-specific%2520datasets%252C%2520the%2520lack%2520of%2520integrated%2520contextual%2520knowledge%252C%2520and%2520the%250Aabsence%2520of%2520hard%2520negatives%2520highlighting%2520subtle%2520distinctions.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520first%2520design%2520a%2520data%2520curation%2520pipeline%2520that%2520leverages%250Aopen-sourced%2520VLMs%2520and%2520text-to-image%2520diffusion%2520models%2520to%2520construct%2520CulTwin%252C%2520a%250Asynthetic%2520cultural%2520dataset.%2520This%2520dataset%2520consists%2520of%2520paired%250Aconcept-caption-image%2520triplets%252C%2520where%2520concepts%2520visually%2520resemble%2520each%2520other%2520but%250Arepresent%2520different%2520cultural%2520contexts.%2520Then%252C%2520we%2520fine-tune%2520CLIP%2520on%2520CulTwin%2520to%250Acreate%2520CultureCLIP%252C%2520which%2520aligns%2520cultural%2520concepts%2520with%2520contextually%2520enhanced%250Acaptions%2520and%2520synthetic%2520images%2520through%2520customized%2520contrastive%2520learning%252C%2520enabling%250Afiner%2520cultural%2520differentiation%2520while%2520preserving%2520generalization%2520capabilities.%250AExperiments%2520on%2520culturally%2520relevant%2520benchmarks%2520show%2520that%2520CultureCLIP%2520outperforms%250Athe%2520base%2520CLIP%252C%2520achieving%2520up%2520to%2520a%2520notable%25205.49%2525%2520improvement%2520in%2520fine-grained%250Aconcept%2520recognition%2520on%2520certain%2520tasks%252C%2520while%2520preserving%2520CLIP%2527s%2520original%250Ageneralization%2520ability%252C%2520validating%2520the%2520effectiveness%2520of%2520our%2520data%2520synthesis%2520and%250AVLM%2520backbone%2520training%2520paradigm%2520in%2520capturing%2520subtle%2520cultural%2520distinctions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CultureCLIP%3A%20Empowering%20CLIP%20with%20Cultural%20Awareness%20through%20Synthetic%0A%20%20Images%20and%20Contextualized%20Captions&entry.906535625=Yuchen%20Huang%20and%20Zhiyuan%20Fan%20and%20Zhitao%20He%20and%20Sandeep%20Polisetty%20and%20Wenyan%20Li%20and%20Yi%20R.%20Fung&entry.1292438233=%20%20Pretrained%20vision-language%20models%20%28VLMs%29%20such%20as%20CLIP%20excel%20in%20multimodal%0Aunderstanding%20but%20struggle%20with%20contextually%20relevant%20fine-grained%20visual%0Afeatures%2C%20making%20it%20difficult%20to%20distinguish%20visually%20similar%20yet%20culturally%0Adistinct%20concepts.%20This%20limitation%20stems%20from%20the%20scarcity%20of%20high-quality%0Aculture-specific%20datasets%2C%20the%20lack%20of%20integrated%20contextual%20knowledge%2C%20and%20the%0Aabsence%20of%20hard%20negatives%20highlighting%20subtle%20distinctions.%20To%20address%20these%0Achallenges%2C%20we%20first%20design%20a%20data%20curation%20pipeline%20that%20leverages%0Aopen-sourced%20VLMs%20and%20text-to-image%20diffusion%20models%20to%20construct%20CulTwin%2C%20a%0Asynthetic%20cultural%20dataset.%20This%20dataset%20consists%20of%20paired%0Aconcept-caption-image%20triplets%2C%20where%20concepts%20visually%20resemble%20each%20other%20but%0Arepresent%20different%20cultural%20contexts.%20Then%2C%20we%20fine-tune%20CLIP%20on%20CulTwin%20to%0Acreate%20CultureCLIP%2C%20which%20aligns%20cultural%20concepts%20with%20contextually%20enhanced%0Acaptions%20and%20synthetic%20images%20through%20customized%20contrastive%20learning%2C%20enabling%0Afiner%20cultural%20differentiation%20while%20preserving%20generalization%20capabilities.%0AExperiments%20on%20culturally%20relevant%20benchmarks%20show%20that%20CultureCLIP%20outperforms%0Athe%20base%20CLIP%2C%20achieving%20up%20to%20a%20notable%205.49%25%20improvement%20in%20fine-grained%0Aconcept%20recognition%20on%20certain%20tasks%2C%20while%20preserving%20CLIP%27s%20original%0Ageneralization%20ability%2C%20validating%20the%20effectiveness%20of%20our%20data%20synthesis%20and%0AVLM%20backbone%20training%20paradigm%20in%20capturing%20subtle%20cultural%20distinctions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06210v1&entry.124074799=Read"},
{"title": "Multimodal Integration Challenges in Emotionally Expressive Child\n  Avatars for Training Applications", "author": "Pegah Salehi and Sajad Amouei Sheshkal and Vajira Thambawita and Michael A. Riegler and P\u00e5l Halvorsen", "abstract": "  Dynamic facial emotion is essential for believable AI-generated avatars, yet\nmost systems remain visually static, limiting their use in simulations like\nvirtual training for investigative interviews with abused children. We present\na real-time architecture combining Unreal Engine 5 MetaHuman rendering with\nNVIDIA Omniverse Audio2Face to generate facial expressions from vocal prosody\nin photorealistic child avatars. Due to limited TTS options, both avatars were\nvoiced using young adult female models from two systems to better fit character\nprofiles, introducing a voice-age mismatch. This confound may affect\naudiovisual alignment. We used a two-PC setup to decouple speech generation\nfrom GPU-intensive rendering, enabling low-latency interaction in desktop and\nVR. A between-subjects study (N=70) compared audio+visual vs. visual-only\nconditions as participants rated emotional clarity, facial realism, and empathy\nfor avatars expressing joy, sadness, and anger. While emotions were generally\nrecognized - especially sadness and joy - anger was harder to detect without\naudio, highlighting the role of voice in high-arousal expressions.\nInterestingly, silencing clips improved perceived realism by removing\nmismatches between voice and animation, especially when tone or age felt\nincongruent. These results emphasize the importance of audiovisual congruence:\nmismatched voice undermines expression, while a good match can enhance weaker\nvisuals - posing challenges for emotionally coherent avatars in sensitive\ncontexts.\n", "link": "http://arxiv.org/abs/2506.13477v2", "date": "2025-07-08", "relevancy": 2.8346, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5883}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5883}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Integration%20Challenges%20in%20Emotionally%20Expressive%20Child%0A%20%20Avatars%20for%20Training%20Applications&body=Title%3A%20Multimodal%20Integration%20Challenges%20in%20Emotionally%20Expressive%20Child%0A%20%20Avatars%20for%20Training%20Applications%0AAuthor%3A%20Pegah%20Salehi%20and%20Sajad%20Amouei%20Sheshkal%20and%20Vajira%20Thambawita%20and%20Michael%20A.%20Riegler%20and%20P%C3%A5l%20Halvorsen%0AAbstract%3A%20%20%20Dynamic%20facial%20emotion%20is%20essential%20for%20believable%20AI-generated%20avatars%2C%20yet%0Amost%20systems%20remain%20visually%20static%2C%20limiting%20their%20use%20in%20simulations%20like%0Avirtual%20training%20for%20investigative%20interviews%20with%20abused%20children.%20We%20present%0Aa%20real-time%20architecture%20combining%20Unreal%20Engine%205%20MetaHuman%20rendering%20with%0ANVIDIA%20Omniverse%20Audio2Face%20to%20generate%20facial%20expressions%20from%20vocal%20prosody%0Ain%20photorealistic%20child%20avatars.%20Due%20to%20limited%20TTS%20options%2C%20both%20avatars%20were%0Avoiced%20using%20young%20adult%20female%20models%20from%20two%20systems%20to%20better%20fit%20character%0Aprofiles%2C%20introducing%20a%20voice-age%20mismatch.%20This%20confound%20may%20affect%0Aaudiovisual%20alignment.%20We%20used%20a%20two-PC%20setup%20to%20decouple%20speech%20generation%0Afrom%20GPU-intensive%20rendering%2C%20enabling%20low-latency%20interaction%20in%20desktop%20and%0AVR.%20A%20between-subjects%20study%20%28N%3D70%29%20compared%20audio%2Bvisual%20vs.%20visual-only%0Aconditions%20as%20participants%20rated%20emotional%20clarity%2C%20facial%20realism%2C%20and%20empathy%0Afor%20avatars%20expressing%20joy%2C%20sadness%2C%20and%20anger.%20While%20emotions%20were%20generally%0Arecognized%20-%20especially%20sadness%20and%20joy%20-%20anger%20was%20harder%20to%20detect%20without%0Aaudio%2C%20highlighting%20the%20role%20of%20voice%20in%20high-arousal%20expressions.%0AInterestingly%2C%20silencing%20clips%20improved%20perceived%20realism%20by%20removing%0Amismatches%20between%20voice%20and%20animation%2C%20especially%20when%20tone%20or%20age%20felt%0Aincongruent.%20These%20results%20emphasize%20the%20importance%20of%20audiovisual%20congruence%3A%0Amismatched%20voice%20undermines%20expression%2C%20while%20a%20good%20match%20can%20enhance%20weaker%0Avisuals%20-%20posing%20challenges%20for%20emotionally%20coherent%20avatars%20in%20sensitive%0Acontexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Integration%2520Challenges%2520in%2520Emotionally%2520Expressive%2520Child%250A%2520%2520Avatars%2520for%2520Training%2520Applications%26entry.906535625%3DPegah%2520Salehi%2520and%2520Sajad%2520Amouei%2520Sheshkal%2520and%2520Vajira%2520Thambawita%2520and%2520Michael%2520A.%2520Riegler%2520and%2520P%25C3%25A5l%2520Halvorsen%26entry.1292438233%3D%2520%2520Dynamic%2520facial%2520emotion%2520is%2520essential%2520for%2520believable%2520AI-generated%2520avatars%252C%2520yet%250Amost%2520systems%2520remain%2520visually%2520static%252C%2520limiting%2520their%2520use%2520in%2520simulations%2520like%250Avirtual%2520training%2520for%2520investigative%2520interviews%2520with%2520abused%2520children.%2520We%2520present%250Aa%2520real-time%2520architecture%2520combining%2520Unreal%2520Engine%25205%2520MetaHuman%2520rendering%2520with%250ANVIDIA%2520Omniverse%2520Audio2Face%2520to%2520generate%2520facial%2520expressions%2520from%2520vocal%2520prosody%250Ain%2520photorealistic%2520child%2520avatars.%2520Due%2520to%2520limited%2520TTS%2520options%252C%2520both%2520avatars%2520were%250Avoiced%2520using%2520young%2520adult%2520female%2520models%2520from%2520two%2520systems%2520to%2520better%2520fit%2520character%250Aprofiles%252C%2520introducing%2520a%2520voice-age%2520mismatch.%2520This%2520confound%2520may%2520affect%250Aaudiovisual%2520alignment.%2520We%2520used%2520a%2520two-PC%2520setup%2520to%2520decouple%2520speech%2520generation%250Afrom%2520GPU-intensive%2520rendering%252C%2520enabling%2520low-latency%2520interaction%2520in%2520desktop%2520and%250AVR.%2520A%2520between-subjects%2520study%2520%2528N%253D70%2529%2520compared%2520audio%252Bvisual%2520vs.%2520visual-only%250Aconditions%2520as%2520participants%2520rated%2520emotional%2520clarity%252C%2520facial%2520realism%252C%2520and%2520empathy%250Afor%2520avatars%2520expressing%2520joy%252C%2520sadness%252C%2520and%2520anger.%2520While%2520emotions%2520were%2520generally%250Arecognized%2520-%2520especially%2520sadness%2520and%2520joy%2520-%2520anger%2520was%2520harder%2520to%2520detect%2520without%250Aaudio%252C%2520highlighting%2520the%2520role%2520of%2520voice%2520in%2520high-arousal%2520expressions.%250AInterestingly%252C%2520silencing%2520clips%2520improved%2520perceived%2520realism%2520by%2520removing%250Amismatches%2520between%2520voice%2520and%2520animation%252C%2520especially%2520when%2520tone%2520or%2520age%2520felt%250Aincongruent.%2520These%2520results%2520emphasize%2520the%2520importance%2520of%2520audiovisual%2520congruence%253A%250Amismatched%2520voice%2520undermines%2520expression%252C%2520while%2520a%2520good%2520match%2520can%2520enhance%2520weaker%250Avisuals%2520-%2520posing%2520challenges%2520for%2520emotionally%2520coherent%2520avatars%2520in%2520sensitive%250Acontexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Integration%20Challenges%20in%20Emotionally%20Expressive%20Child%0A%20%20Avatars%20for%20Training%20Applications&entry.906535625=Pegah%20Salehi%20and%20Sajad%20Amouei%20Sheshkal%20and%20Vajira%20Thambawita%20and%20Michael%20A.%20Riegler%20and%20P%C3%A5l%20Halvorsen&entry.1292438233=%20%20Dynamic%20facial%20emotion%20is%20essential%20for%20believable%20AI-generated%20avatars%2C%20yet%0Amost%20systems%20remain%20visually%20static%2C%20limiting%20their%20use%20in%20simulations%20like%0Avirtual%20training%20for%20investigative%20interviews%20with%20abused%20children.%20We%20present%0Aa%20real-time%20architecture%20combining%20Unreal%20Engine%205%20MetaHuman%20rendering%20with%0ANVIDIA%20Omniverse%20Audio2Face%20to%20generate%20facial%20expressions%20from%20vocal%20prosody%0Ain%20photorealistic%20child%20avatars.%20Due%20to%20limited%20TTS%20options%2C%20both%20avatars%20were%0Avoiced%20using%20young%20adult%20female%20models%20from%20two%20systems%20to%20better%20fit%20character%0Aprofiles%2C%20introducing%20a%20voice-age%20mismatch.%20This%20confound%20may%20affect%0Aaudiovisual%20alignment.%20We%20used%20a%20two-PC%20setup%20to%20decouple%20speech%20generation%0Afrom%20GPU-intensive%20rendering%2C%20enabling%20low-latency%20interaction%20in%20desktop%20and%0AVR.%20A%20between-subjects%20study%20%28N%3D70%29%20compared%20audio%2Bvisual%20vs.%20visual-only%0Aconditions%20as%20participants%20rated%20emotional%20clarity%2C%20facial%20realism%2C%20and%20empathy%0Afor%20avatars%20expressing%20joy%2C%20sadness%2C%20and%20anger.%20While%20emotions%20were%20generally%0Arecognized%20-%20especially%20sadness%20and%20joy%20-%20anger%20was%20harder%20to%20detect%20without%0Aaudio%2C%20highlighting%20the%20role%20of%20voice%20in%20high-arousal%20expressions.%0AInterestingly%2C%20silencing%20clips%20improved%20perceived%20realism%20by%20removing%0Amismatches%20between%20voice%20and%20animation%2C%20especially%20when%20tone%20or%20age%20felt%0Aincongruent.%20These%20results%20emphasize%20the%20importance%20of%20audiovisual%20congruence%3A%0Amismatched%20voice%20undermines%20expression%2C%20while%20a%20good%20match%20can%20enhance%20weaker%0Avisuals%20-%20posing%20challenges%20for%20emotionally%20coherent%20avatars%20in%20sensitive%0Acontexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13477v2&entry.124074799=Read"},
{"title": "Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images\n  without GNSS", "author": "Xinyu Wang and Muhammad Ibrahim and Atif Mansoor and Ajmal Mian", "abstract": "  Accurate geo-registration of LiDAR point clouds presents significant\nchallenges in GNSS signal denied urban areas with high-rise buildings and\nbridges. Existing methods typically rely on real-time GNSS and IMU data, that\nrequire pre-calibration and assume stable positioning during data collection.\nHowever, this assumption often fails in dense urban areas, resulting in\nlocalization errors. To address this, we propose a structured geo-registration\nand spatial correction method that aligns 3D point clouds with satellite\nimages, enabling frame-wise recovery of GNSS information and reconstruction of\ncity scale 3D maps without relying on prior localization. The proposed approach\nemploys a pre-trained Point Transformer model to segment the road points and\nthen extracts the road skeleton and intersection points from the point cloud as\nwell as the target map for alignment. Global rigid alignment of the two is\nperformed using the intersection points, followed by local refinement using\nradial basis function (RBF) interpolation. Elevation correction is then applied\nto the point cloud based on terrain information from SRTM dataset to resolve\nvertical discrepancies. The proposed method was tested on the popular KITTI\nbenchmark and a locally collected Perth (Western Australia) CBD dataset. On the\nKITTI dataset, our method achieved an average planimetric alignment standard\ndeviation (STD) of 0.84~m across sequences with intersections, representing a\n55.3\\% improvement over the original dataset. On the Perth dataset, which lacks\nGNSS information, our method achieved an average STD of 0.96~m compared to the\nGPS data extracted from Google Maps API. This corresponds to a 77.4\\%\nimprovement from the initial alignment. Our method also resulted in elevation\ncorrelation gains of 30.5\\% on the KITTI dataset and 50.4\\% on the Perth\ndataset.\n", "link": "http://arxiv.org/abs/2507.05999v1", "date": "2025-07-08", "relevancy": 2.8309, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5885}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5582}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geo-Registration%20of%20Terrestrial%20LiDAR%20Point%20Clouds%20with%20Satellite%20Images%0A%20%20without%20GNSS&body=Title%3A%20Geo-Registration%20of%20Terrestrial%20LiDAR%20Point%20Clouds%20with%20Satellite%20Images%0A%20%20without%20GNSS%0AAuthor%3A%20Xinyu%20Wang%20and%20Muhammad%20Ibrahim%20and%20Atif%20Mansoor%20and%20Ajmal%20Mian%0AAbstract%3A%20%20%20Accurate%20geo-registration%20of%20LiDAR%20point%20clouds%20presents%20significant%0Achallenges%20in%20GNSS%20signal%20denied%20urban%20areas%20with%20high-rise%20buildings%20and%0Abridges.%20Existing%20methods%20typically%20rely%20on%20real-time%20GNSS%20and%20IMU%20data%2C%20that%0Arequire%20pre-calibration%20and%20assume%20stable%20positioning%20during%20data%20collection.%0AHowever%2C%20this%20assumption%20often%20fails%20in%20dense%20urban%20areas%2C%20resulting%20in%0Alocalization%20errors.%20To%20address%20this%2C%20we%20propose%20a%20structured%20geo-registration%0Aand%20spatial%20correction%20method%20that%20aligns%203D%20point%20clouds%20with%20satellite%0Aimages%2C%20enabling%20frame-wise%20recovery%20of%20GNSS%20information%20and%20reconstruction%20of%0Acity%20scale%203D%20maps%20without%20relying%20on%20prior%20localization.%20The%20proposed%20approach%0Aemploys%20a%20pre-trained%20Point%20Transformer%20model%20to%20segment%20the%20road%20points%20and%0Athen%20extracts%20the%20road%20skeleton%20and%20intersection%20points%20from%20the%20point%20cloud%20as%0Awell%20as%20the%20target%20map%20for%20alignment.%20Global%20rigid%20alignment%20of%20the%20two%20is%0Aperformed%20using%20the%20intersection%20points%2C%20followed%20by%20local%20refinement%20using%0Aradial%20basis%20function%20%28RBF%29%20interpolation.%20Elevation%20correction%20is%20then%20applied%0Ato%20the%20point%20cloud%20based%20on%20terrain%20information%20from%20SRTM%20dataset%20to%20resolve%0Avertical%20discrepancies.%20The%20proposed%20method%20was%20tested%20on%20the%20popular%20KITTI%0Abenchmark%20and%20a%20locally%20collected%20Perth%20%28Western%20Australia%29%20CBD%20dataset.%20On%20the%0AKITTI%20dataset%2C%20our%20method%20achieved%20an%20average%20planimetric%20alignment%20standard%0Adeviation%20%28STD%29%20of%200.84~m%20across%20sequences%20with%20intersections%2C%20representing%20a%0A55.3%5C%25%20improvement%20over%20the%20original%20dataset.%20On%20the%20Perth%20dataset%2C%20which%20lacks%0AGNSS%20information%2C%20our%20method%20achieved%20an%20average%20STD%20of%200.96~m%20compared%20to%20the%0AGPS%20data%20extracted%20from%20Google%20Maps%20API.%20This%20corresponds%20to%20a%2077.4%5C%25%0Aimprovement%20from%20the%20initial%20alignment.%20Our%20method%20also%20resulted%20in%20elevation%0Acorrelation%20gains%20of%2030.5%5C%25%20on%20the%20KITTI%20dataset%20and%2050.4%5C%25%20on%20the%20Perth%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeo-Registration%2520of%2520Terrestrial%2520LiDAR%2520Point%2520Clouds%2520with%2520Satellite%2520Images%250A%2520%2520without%2520GNSS%26entry.906535625%3DXinyu%2520Wang%2520and%2520Muhammad%2520Ibrahim%2520and%2520Atif%2520Mansoor%2520and%2520Ajmal%2520Mian%26entry.1292438233%3D%2520%2520Accurate%2520geo-registration%2520of%2520LiDAR%2520point%2520clouds%2520presents%2520significant%250Achallenges%2520in%2520GNSS%2520signal%2520denied%2520urban%2520areas%2520with%2520high-rise%2520buildings%2520and%250Abridges.%2520Existing%2520methods%2520typically%2520rely%2520on%2520real-time%2520GNSS%2520and%2520IMU%2520data%252C%2520that%250Arequire%2520pre-calibration%2520and%2520assume%2520stable%2520positioning%2520during%2520data%2520collection.%250AHowever%252C%2520this%2520assumption%2520often%2520fails%2520in%2520dense%2520urban%2520areas%252C%2520resulting%2520in%250Alocalization%2520errors.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520structured%2520geo-registration%250Aand%2520spatial%2520correction%2520method%2520that%2520aligns%25203D%2520point%2520clouds%2520with%2520satellite%250Aimages%252C%2520enabling%2520frame-wise%2520recovery%2520of%2520GNSS%2520information%2520and%2520reconstruction%2520of%250Acity%2520scale%25203D%2520maps%2520without%2520relying%2520on%2520prior%2520localization.%2520The%2520proposed%2520approach%250Aemploys%2520a%2520pre-trained%2520Point%2520Transformer%2520model%2520to%2520segment%2520the%2520road%2520points%2520and%250Athen%2520extracts%2520the%2520road%2520skeleton%2520and%2520intersection%2520points%2520from%2520the%2520point%2520cloud%2520as%250Awell%2520as%2520the%2520target%2520map%2520for%2520alignment.%2520Global%2520rigid%2520alignment%2520of%2520the%2520two%2520is%250Aperformed%2520using%2520the%2520intersection%2520points%252C%2520followed%2520by%2520local%2520refinement%2520using%250Aradial%2520basis%2520function%2520%2528RBF%2529%2520interpolation.%2520Elevation%2520correction%2520is%2520then%2520applied%250Ato%2520the%2520point%2520cloud%2520based%2520on%2520terrain%2520information%2520from%2520SRTM%2520dataset%2520to%2520resolve%250Avertical%2520discrepancies.%2520The%2520proposed%2520method%2520was%2520tested%2520on%2520the%2520popular%2520KITTI%250Abenchmark%2520and%2520a%2520locally%2520collected%2520Perth%2520%2528Western%2520Australia%2529%2520CBD%2520dataset.%2520On%2520the%250AKITTI%2520dataset%252C%2520our%2520method%2520achieved%2520an%2520average%2520planimetric%2520alignment%2520standard%250Adeviation%2520%2528STD%2529%2520of%25200.84~m%2520across%2520sequences%2520with%2520intersections%252C%2520representing%2520a%250A55.3%255C%2525%2520improvement%2520over%2520the%2520original%2520dataset.%2520On%2520the%2520Perth%2520dataset%252C%2520which%2520lacks%250AGNSS%2520information%252C%2520our%2520method%2520achieved%2520an%2520average%2520STD%2520of%25200.96~m%2520compared%2520to%2520the%250AGPS%2520data%2520extracted%2520from%2520Google%2520Maps%2520API.%2520This%2520corresponds%2520to%2520a%252077.4%255C%2525%250Aimprovement%2520from%2520the%2520initial%2520alignment.%2520Our%2520method%2520also%2520resulted%2520in%2520elevation%250Acorrelation%2520gains%2520of%252030.5%255C%2525%2520on%2520the%2520KITTI%2520dataset%2520and%252050.4%255C%2525%2520on%2520the%2520Perth%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geo-Registration%20of%20Terrestrial%20LiDAR%20Point%20Clouds%20with%20Satellite%20Images%0A%20%20without%20GNSS&entry.906535625=Xinyu%20Wang%20and%20Muhammad%20Ibrahim%20and%20Atif%20Mansoor%20and%20Ajmal%20Mian&entry.1292438233=%20%20Accurate%20geo-registration%20of%20LiDAR%20point%20clouds%20presents%20significant%0Achallenges%20in%20GNSS%20signal%20denied%20urban%20areas%20with%20high-rise%20buildings%20and%0Abridges.%20Existing%20methods%20typically%20rely%20on%20real-time%20GNSS%20and%20IMU%20data%2C%20that%0Arequire%20pre-calibration%20and%20assume%20stable%20positioning%20during%20data%20collection.%0AHowever%2C%20this%20assumption%20often%20fails%20in%20dense%20urban%20areas%2C%20resulting%20in%0Alocalization%20errors.%20To%20address%20this%2C%20we%20propose%20a%20structured%20geo-registration%0Aand%20spatial%20correction%20method%20that%20aligns%203D%20point%20clouds%20with%20satellite%0Aimages%2C%20enabling%20frame-wise%20recovery%20of%20GNSS%20information%20and%20reconstruction%20of%0Acity%20scale%203D%20maps%20without%20relying%20on%20prior%20localization.%20The%20proposed%20approach%0Aemploys%20a%20pre-trained%20Point%20Transformer%20model%20to%20segment%20the%20road%20points%20and%0Athen%20extracts%20the%20road%20skeleton%20and%20intersection%20points%20from%20the%20point%20cloud%20as%0Awell%20as%20the%20target%20map%20for%20alignment.%20Global%20rigid%20alignment%20of%20the%20two%20is%0Aperformed%20using%20the%20intersection%20points%2C%20followed%20by%20local%20refinement%20using%0Aradial%20basis%20function%20%28RBF%29%20interpolation.%20Elevation%20correction%20is%20then%20applied%0Ato%20the%20point%20cloud%20based%20on%20terrain%20information%20from%20SRTM%20dataset%20to%20resolve%0Avertical%20discrepancies.%20The%20proposed%20method%20was%20tested%20on%20the%20popular%20KITTI%0Abenchmark%20and%20a%20locally%20collected%20Perth%20%28Western%20Australia%29%20CBD%20dataset.%20On%20the%0AKITTI%20dataset%2C%20our%20method%20achieved%20an%20average%20planimetric%20alignment%20standard%0Adeviation%20%28STD%29%20of%200.84~m%20across%20sequences%20with%20intersections%2C%20representing%20a%0A55.3%5C%25%20improvement%20over%20the%20original%20dataset.%20On%20the%20Perth%20dataset%2C%20which%20lacks%0AGNSS%20information%2C%20our%20method%20achieved%20an%20average%20STD%20of%200.96~m%20compared%20to%20the%0AGPS%20data%20extracted%20from%20Google%20Maps%20API.%20This%20corresponds%20to%20a%2077.4%5C%25%0Aimprovement%20from%20the%20initial%20alignment.%20Our%20method%20also%20resulted%20in%20elevation%0Acorrelation%20gains%20of%2030.5%5C%25%20on%20the%20KITTI%20dataset%20and%2050.4%5C%25%20on%20the%20Perth%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05999v1&entry.124074799=Read"},
{"title": "Hita: Holistic Tokenizer for Autoregressive Image Generation", "author": "Anlin Zheng and Haochen Wang and Yucheng Zhao and Weipeng Deng and Tiancai Wang and Xiangyu Zhang and Xiaojuan Qi", "abstract": "  Vanilla autoregressive image generation models generate visual tokens\nstep-by-step, limiting their ability to capture holistic relationships among\ntoken sequences. Moreover, because most visual tokenizers map local image\npatches into latent tokens, global information is limited. To address this, we\nintroduce \\textit{Hita}, a novel image tokenizer for autoregressive (AR) image\ngeneration. It introduces a holistic-to-local tokenization scheme with\nlearnable holistic queries and local patch tokens. Hita incorporates two key\nstrategies to better align with the AR generation process: 1) {arranging} a\nsequential structure with holistic tokens at the beginning, followed by\npatch-level tokens, and using causal attention to maintain awareness of\nprevious tokens; and 2) adopting a lightweight fusion module before feeding the\nde-quantized tokens into the decoder to control information flow and prioritize\nholistic tokens. Extensive experiments show that Hita accelerates the training\nspeed of AR generators and outperforms those trained with vanilla tokenizers,\nachieving \\textbf{2.59 FID} and \\textbf{281.9 IS} on the ImageNet benchmark.\nDetailed analysis of the holistic representation highlights its ability to\ncapture global image properties, such as textures, materials, and shapes.\nAdditionally, Hita also demonstrates effectiveness in zero-shot style transfer\nand image in-painting. The code is available at\n\\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}.\n", "link": "http://arxiv.org/abs/2507.02358v3", "date": "2025-07-08", "relevancy": 2.7909, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6082}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5395}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hita%3A%20Holistic%20Tokenizer%20for%20Autoregressive%20Image%20Generation&body=Title%3A%20Hita%3A%20Holistic%20Tokenizer%20for%20Autoregressive%20Image%20Generation%0AAuthor%3A%20Anlin%20Zheng%20and%20Haochen%20Wang%20and%20Yucheng%20Zhao%20and%20Weipeng%20Deng%20and%20Tiancai%20Wang%20and%20Xiangyu%20Zhang%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20Vanilla%20autoregressive%20image%20generation%20models%20generate%20visual%20tokens%0Astep-by-step%2C%20limiting%20their%20ability%20to%20capture%20holistic%20relationships%20among%0Atoken%20sequences.%20Moreover%2C%20because%20most%20visual%20tokenizers%20map%20local%20image%0Apatches%20into%20latent%20tokens%2C%20global%20information%20is%20limited.%20To%20address%20this%2C%20we%0Aintroduce%20%5Ctextit%7BHita%7D%2C%20a%20novel%20image%20tokenizer%20for%20autoregressive%20%28AR%29%20image%0Ageneration.%20It%20introduces%20a%20holistic-to-local%20tokenization%20scheme%20with%0Alearnable%20holistic%20queries%20and%20local%20patch%20tokens.%20Hita%20incorporates%20two%20key%0Astrategies%20to%20better%20align%20with%20the%20AR%20generation%20process%3A%201%29%20%7Barranging%7D%20a%0Asequential%20structure%20with%20holistic%20tokens%20at%20the%20beginning%2C%20followed%20by%0Apatch-level%20tokens%2C%20and%20using%20causal%20attention%20to%20maintain%20awareness%20of%0Aprevious%20tokens%3B%20and%202%29%20adopting%20a%20lightweight%20fusion%20module%20before%20feeding%20the%0Ade-quantized%20tokens%20into%20the%20decoder%20to%20control%20information%20flow%20and%20prioritize%0Aholistic%20tokens.%20Extensive%20experiments%20show%20that%20Hita%20accelerates%20the%20training%0Aspeed%20of%20AR%20generators%20and%20outperforms%20those%20trained%20with%20vanilla%20tokenizers%2C%0Aachieving%20%5Ctextbf%7B2.59%20FID%7D%20and%20%5Ctextbf%7B281.9%20IS%7D%20on%20the%20ImageNet%20benchmark.%0ADetailed%20analysis%20of%20the%20holistic%20representation%20highlights%20its%20ability%20to%0Acapture%20global%20image%20properties%2C%20such%20as%20textures%2C%20materials%2C%20and%20shapes.%0AAdditionally%2C%20Hita%20also%20demonstrates%20effectiveness%20in%20zero-shot%20style%20transfer%0Aand%20image%20in-painting.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02358v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHita%253A%2520Holistic%2520Tokenizer%2520for%2520Autoregressive%2520Image%2520Generation%26entry.906535625%3DAnlin%2520Zheng%2520and%2520Haochen%2520Wang%2520and%2520Yucheng%2520Zhao%2520and%2520Weipeng%2520Deng%2520and%2520Tiancai%2520Wang%2520and%2520Xiangyu%2520Zhang%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520Vanilla%2520autoregressive%2520image%2520generation%2520models%2520generate%2520visual%2520tokens%250Astep-by-step%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520holistic%2520relationships%2520among%250Atoken%2520sequences.%2520Moreover%252C%2520because%2520most%2520visual%2520tokenizers%2520map%2520local%2520image%250Apatches%2520into%2520latent%2520tokens%252C%2520global%2520information%2520is%2520limited.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520%255Ctextit%257BHita%257D%252C%2520a%2520novel%2520image%2520tokenizer%2520for%2520autoregressive%2520%2528AR%2529%2520image%250Ageneration.%2520It%2520introduces%2520a%2520holistic-to-local%2520tokenization%2520scheme%2520with%250Alearnable%2520holistic%2520queries%2520and%2520local%2520patch%2520tokens.%2520Hita%2520incorporates%2520two%2520key%250Astrategies%2520to%2520better%2520align%2520with%2520the%2520AR%2520generation%2520process%253A%25201%2529%2520%257Barranging%257D%2520a%250Asequential%2520structure%2520with%2520holistic%2520tokens%2520at%2520the%2520beginning%252C%2520followed%2520by%250Apatch-level%2520tokens%252C%2520and%2520using%2520causal%2520attention%2520to%2520maintain%2520awareness%2520of%250Aprevious%2520tokens%253B%2520and%25202%2529%2520adopting%2520a%2520lightweight%2520fusion%2520module%2520before%2520feeding%2520the%250Ade-quantized%2520tokens%2520into%2520the%2520decoder%2520to%2520control%2520information%2520flow%2520and%2520prioritize%250Aholistic%2520tokens.%2520Extensive%2520experiments%2520show%2520that%2520Hita%2520accelerates%2520the%2520training%250Aspeed%2520of%2520AR%2520generators%2520and%2520outperforms%2520those%2520trained%2520with%2520vanilla%2520tokenizers%252C%250Aachieving%2520%255Ctextbf%257B2.59%2520FID%257D%2520and%2520%255Ctextbf%257B281.9%2520IS%257D%2520on%2520the%2520ImageNet%2520benchmark.%250ADetailed%2520analysis%2520of%2520the%2520holistic%2520representation%2520highlights%2520its%2520ability%2520to%250Acapture%2520global%2520image%2520properties%252C%2520such%2520as%2520textures%252C%2520materials%252C%2520and%2520shapes.%250AAdditionally%252C%2520Hita%2520also%2520demonstrates%2520effectiveness%2520in%2520zero-shot%2520style%2520transfer%250Aand%2520image%2520in-painting.%2520The%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/CVMI-Lab/Hita%257D%257Bhttps%253A//github.com/CVMI-Lab/Hita%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02358v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hita%3A%20Holistic%20Tokenizer%20for%20Autoregressive%20Image%20Generation&entry.906535625=Anlin%20Zheng%20and%20Haochen%20Wang%20and%20Yucheng%20Zhao%20and%20Weipeng%20Deng%20and%20Tiancai%20Wang%20and%20Xiangyu%20Zhang%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20Vanilla%20autoregressive%20image%20generation%20models%20generate%20visual%20tokens%0Astep-by-step%2C%20limiting%20their%20ability%20to%20capture%20holistic%20relationships%20among%0Atoken%20sequences.%20Moreover%2C%20because%20most%20visual%20tokenizers%20map%20local%20image%0Apatches%20into%20latent%20tokens%2C%20global%20information%20is%20limited.%20To%20address%20this%2C%20we%0Aintroduce%20%5Ctextit%7BHita%7D%2C%20a%20novel%20image%20tokenizer%20for%20autoregressive%20%28AR%29%20image%0Ageneration.%20It%20introduces%20a%20holistic-to-local%20tokenization%20scheme%20with%0Alearnable%20holistic%20queries%20and%20local%20patch%20tokens.%20Hita%20incorporates%20two%20key%0Astrategies%20to%20better%20align%20with%20the%20AR%20generation%20process%3A%201%29%20%7Barranging%7D%20a%0Asequential%20structure%20with%20holistic%20tokens%20at%20the%20beginning%2C%20followed%20by%0Apatch-level%20tokens%2C%20and%20using%20causal%20attention%20to%20maintain%20awareness%20of%0Aprevious%20tokens%3B%20and%202%29%20adopting%20a%20lightweight%20fusion%20module%20before%20feeding%20the%0Ade-quantized%20tokens%20into%20the%20decoder%20to%20control%20information%20flow%20and%20prioritize%0Aholistic%20tokens.%20Extensive%20experiments%20show%20that%20Hita%20accelerates%20the%20training%0Aspeed%20of%20AR%20generators%20and%20outperforms%20those%20trained%20with%20vanilla%20tokenizers%2C%0Aachieving%20%5Ctextbf%7B2.59%20FID%7D%20and%20%5Ctextbf%7B281.9%20IS%7D%20on%20the%20ImageNet%20benchmark.%0ADetailed%20analysis%20of%20the%20holistic%20representation%20highlights%20its%20ability%20to%0Acapture%20global%20image%20properties%2C%20such%20as%20textures%2C%20materials%2C%20and%20shapes.%0AAdditionally%2C%20Hita%20also%20demonstrates%20effectiveness%20in%20zero-shot%20style%20transfer%0Aand%20image%20in-painting.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02358v3&entry.124074799=Read"},
{"title": "Enhancing Synthetic CT from CBCT via Multimodal Fusion and End-To-End\n  Registration", "author": "Maximilian Tschuchnig and Lukas Lamminger and Philipp Steininger and Michael Gadermayr", "abstract": "  Cone-Beam Computed Tomography (CBCT) is widely used for intraoperative\nimaging due to its rapid acquisition and low radiation dose. However, CBCT\nimages typically suffer from artifacts and lower visual quality compared to\nconventional Computed Tomography (CT). A promising solution is synthetic CT\n(sCT) generation, where CBCT volumes are translated into the CT domain. In this\nwork, we enhance sCT generation through multimodal learning by jointly\nleveraging intraoperative CBCT and preoperative CT data. To overcome the\ninherent misalignment between modalities, we introduce an end-to-end learnable\nregistration module within the sCT pipeline. This model is evaluated on a\ncontrolled synthetic dataset, allowing precise manipulation of data quality and\nalignment parameters. Further, we validate its robustness and generalizability\non two real-world clinical datasets. Experimental results demonstrate that\nintegrating registration in multimodal sCT generation improves sCT quality,\noutperforming baseline multimodal methods in 79 out of 90 evaluation settings.\nNotably, the improvement is most significant in cases where CBCT quality is low\nand the preoperative CT is moderately misaligned.\n", "link": "http://arxiv.org/abs/2507.06067v1", "date": "2025-07-08", "relevancy": 2.7797, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5565}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5557}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Synthetic%20CT%20from%20CBCT%20via%20Multimodal%20Fusion%20and%20End-To-End%0A%20%20Registration&body=Title%3A%20Enhancing%20Synthetic%20CT%20from%20CBCT%20via%20Multimodal%20Fusion%20and%20End-To-End%0A%20%20Registration%0AAuthor%3A%20Maximilian%20Tschuchnig%20and%20Lukas%20Lamminger%20and%20Philipp%20Steininger%20and%20Michael%20Gadermayr%0AAbstract%3A%20%20%20Cone-Beam%20Computed%20Tomography%20%28CBCT%29%20is%20widely%20used%20for%20intraoperative%0Aimaging%20due%20to%20its%20rapid%20acquisition%20and%20low%20radiation%20dose.%20However%2C%20CBCT%0Aimages%20typically%20suffer%20from%20artifacts%20and%20lower%20visual%20quality%20compared%20to%0Aconventional%20Computed%20Tomography%20%28CT%29.%20A%20promising%20solution%20is%20synthetic%20CT%0A%28sCT%29%20generation%2C%20where%20CBCT%20volumes%20are%20translated%20into%20the%20CT%20domain.%20In%20this%0Awork%2C%20we%20enhance%20sCT%20generation%20through%20multimodal%20learning%20by%20jointly%0Aleveraging%20intraoperative%20CBCT%20and%20preoperative%20CT%20data.%20To%20overcome%20the%0Ainherent%20misalignment%20between%20modalities%2C%20we%20introduce%20an%20end-to-end%20learnable%0Aregistration%20module%20within%20the%20sCT%20pipeline.%20This%20model%20is%20evaluated%20on%20a%0Acontrolled%20synthetic%20dataset%2C%20allowing%20precise%20manipulation%20of%20data%20quality%20and%0Aalignment%20parameters.%20Further%2C%20we%20validate%20its%20robustness%20and%20generalizability%0Aon%20two%20real-world%20clinical%20datasets.%20Experimental%20results%20demonstrate%20that%0Aintegrating%20registration%20in%20multimodal%20sCT%20generation%20improves%20sCT%20quality%2C%0Aoutperforming%20baseline%20multimodal%20methods%20in%2079%20out%20of%2090%20evaluation%20settings.%0ANotably%2C%20the%20improvement%20is%20most%20significant%20in%20cases%20where%20CBCT%20quality%20is%20low%0Aand%20the%20preoperative%20CT%20is%20moderately%20misaligned.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Synthetic%2520CT%2520from%2520CBCT%2520via%2520Multimodal%2520Fusion%2520and%2520End-To-End%250A%2520%2520Registration%26entry.906535625%3DMaximilian%2520Tschuchnig%2520and%2520Lukas%2520Lamminger%2520and%2520Philipp%2520Steininger%2520and%2520Michael%2520Gadermayr%26entry.1292438233%3D%2520%2520Cone-Beam%2520Computed%2520Tomography%2520%2528CBCT%2529%2520is%2520widely%2520used%2520for%2520intraoperative%250Aimaging%2520due%2520to%2520its%2520rapid%2520acquisition%2520and%2520low%2520radiation%2520dose.%2520However%252C%2520CBCT%250Aimages%2520typically%2520suffer%2520from%2520artifacts%2520and%2520lower%2520visual%2520quality%2520compared%2520to%250Aconventional%2520Computed%2520Tomography%2520%2528CT%2529.%2520A%2520promising%2520solution%2520is%2520synthetic%2520CT%250A%2528sCT%2529%2520generation%252C%2520where%2520CBCT%2520volumes%2520are%2520translated%2520into%2520the%2520CT%2520domain.%2520In%2520this%250Awork%252C%2520we%2520enhance%2520sCT%2520generation%2520through%2520multimodal%2520learning%2520by%2520jointly%250Aleveraging%2520intraoperative%2520CBCT%2520and%2520preoperative%2520CT%2520data.%2520To%2520overcome%2520the%250Ainherent%2520misalignment%2520between%2520modalities%252C%2520we%2520introduce%2520an%2520end-to-end%2520learnable%250Aregistration%2520module%2520within%2520the%2520sCT%2520pipeline.%2520This%2520model%2520is%2520evaluated%2520on%2520a%250Acontrolled%2520synthetic%2520dataset%252C%2520allowing%2520precise%2520manipulation%2520of%2520data%2520quality%2520and%250Aalignment%2520parameters.%2520Further%252C%2520we%2520validate%2520its%2520robustness%2520and%2520generalizability%250Aon%2520two%2520real-world%2520clinical%2520datasets.%2520Experimental%2520results%2520demonstrate%2520that%250Aintegrating%2520registration%2520in%2520multimodal%2520sCT%2520generation%2520improves%2520sCT%2520quality%252C%250Aoutperforming%2520baseline%2520multimodal%2520methods%2520in%252079%2520out%2520of%252090%2520evaluation%2520settings.%250ANotably%252C%2520the%2520improvement%2520is%2520most%2520significant%2520in%2520cases%2520where%2520CBCT%2520quality%2520is%2520low%250Aand%2520the%2520preoperative%2520CT%2520is%2520moderately%2520misaligned.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Synthetic%20CT%20from%20CBCT%20via%20Multimodal%20Fusion%20and%20End-To-End%0A%20%20Registration&entry.906535625=Maximilian%20Tschuchnig%20and%20Lukas%20Lamminger%20and%20Philipp%20Steininger%20and%20Michael%20Gadermayr&entry.1292438233=%20%20Cone-Beam%20Computed%20Tomography%20%28CBCT%29%20is%20widely%20used%20for%20intraoperative%0Aimaging%20due%20to%20its%20rapid%20acquisition%20and%20low%20radiation%20dose.%20However%2C%20CBCT%0Aimages%20typically%20suffer%20from%20artifacts%20and%20lower%20visual%20quality%20compared%20to%0Aconventional%20Computed%20Tomography%20%28CT%29.%20A%20promising%20solution%20is%20synthetic%20CT%0A%28sCT%29%20generation%2C%20where%20CBCT%20volumes%20are%20translated%20into%20the%20CT%20domain.%20In%20this%0Awork%2C%20we%20enhance%20sCT%20generation%20through%20multimodal%20learning%20by%20jointly%0Aleveraging%20intraoperative%20CBCT%20and%20preoperative%20CT%20data.%20To%20overcome%20the%0Ainherent%20misalignment%20between%20modalities%2C%20we%20introduce%20an%20end-to-end%20learnable%0Aregistration%20module%20within%20the%20sCT%20pipeline.%20This%20model%20is%20evaluated%20on%20a%0Acontrolled%20synthetic%20dataset%2C%20allowing%20precise%20manipulation%20of%20data%20quality%20and%0Aalignment%20parameters.%20Further%2C%20we%20validate%20its%20robustness%20and%20generalizability%0Aon%20two%20real-world%20clinical%20datasets.%20Experimental%20results%20demonstrate%20that%0Aintegrating%20registration%20in%20multimodal%20sCT%20generation%20improves%20sCT%20quality%2C%0Aoutperforming%20baseline%20multimodal%20methods%20in%2079%20out%20of%2090%20evaluation%20settings.%0ANotably%2C%20the%20improvement%20is%20most%20significant%20in%20cases%20where%20CBCT%20quality%20is%20low%0Aand%20the%20preoperative%20CT%20is%20moderately%20misaligned.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06067v1&entry.124074799=Read"},
{"title": "FA: Forced Prompt Learning of Vision-Language Models for\n  Out-of-Distribution Detection", "author": "Xinhua Lu and Runhe Lai and Yanqi Wu and Kanghao Chen and Wei-Shi Zheng and Ruixuan Wang", "abstract": "  Pre-trained vision-language models (VLMs) have advanced out-of-distribution\n(OOD) detection recently. However, existing CLIP-based methods often focus on\nlearning OOD-related knowledge to improve OOD detection, showing limited\ngeneralization or reliance on external large-scale auxiliary datasets. In this\nstudy, instead of delving into the intricate OOD-related knowledge, we propose\nan innovative CLIP-based framework based on Forced prompt leArning (FA),\ndesigned to make full use of the In-Distribution (ID) knowledge and ultimately\nboost the effectiveness of OOD detection. Our key insight is to learn a prompt\n(i.e., forced prompt) that contains more diversified and richer descriptions of\nthe ID classes beyond the textual semantics of class labels. Specifically, it\npromotes better discernment for ID images, by forcing more notable semantic\nsimilarity between ID images and the learnable forced prompt. Moreover, we\nintroduce a forced coefficient, encouraging the forced prompt to learn more\ncomprehensive and nuanced descriptions of the ID classes. In this way, FA is\ncapable of achieving notable improvements in OOD detection, even when trained\nwithout any external auxiliary datasets, while maintaining an identical number\nof trainable parameters as CoOp. Extensive empirical evaluations confirm our\nmethod consistently outperforms current state-of-the-art methods. Code is\navailable at https://github.com/0xFAFA/FA.\n", "link": "http://arxiv.org/abs/2507.04511v2", "date": "2025-07-08", "relevancy": 2.7346, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5507}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FA%3A%20Forced%20Prompt%20Learning%20of%20Vision-Language%20Models%20for%0A%20%20Out-of-Distribution%20Detection&body=Title%3A%20FA%3A%20Forced%20Prompt%20Learning%20of%20Vision-Language%20Models%20for%0A%20%20Out-of-Distribution%20Detection%0AAuthor%3A%20Xinhua%20Lu%20and%20Runhe%20Lai%20and%20Yanqi%20Wu%20and%20Kanghao%20Chen%20and%20Wei-Shi%20Zheng%20and%20Ruixuan%20Wang%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%20have%20advanced%20out-of-distribution%0A%28OOD%29%20detection%20recently.%20However%2C%20existing%20CLIP-based%20methods%20often%20focus%20on%0Alearning%20OOD-related%20knowledge%20to%20improve%20OOD%20detection%2C%20showing%20limited%0Ageneralization%20or%20reliance%20on%20external%20large-scale%20auxiliary%20datasets.%20In%20this%0Astudy%2C%20instead%20of%20delving%20into%20the%20intricate%20OOD-related%20knowledge%2C%20we%20propose%0Aan%20innovative%20CLIP-based%20framework%20based%20on%20Forced%20prompt%20leArning%20%28FA%29%2C%0Adesigned%20to%20make%20full%20use%20of%20the%20In-Distribution%20%28ID%29%20knowledge%20and%20ultimately%0Aboost%20the%20effectiveness%20of%20OOD%20detection.%20Our%20key%20insight%20is%20to%20learn%20a%20prompt%0A%28i.e.%2C%20forced%20prompt%29%20that%20contains%20more%20diversified%20and%20richer%20descriptions%20of%0Athe%20ID%20classes%20beyond%20the%20textual%20semantics%20of%20class%20labels.%20Specifically%2C%20it%0Apromotes%20better%20discernment%20for%20ID%20images%2C%20by%20forcing%20more%20notable%20semantic%0Asimilarity%20between%20ID%20images%20and%20the%20learnable%20forced%20prompt.%20Moreover%2C%20we%0Aintroduce%20a%20forced%20coefficient%2C%20encouraging%20the%20forced%20prompt%20to%20learn%20more%0Acomprehensive%20and%20nuanced%20descriptions%20of%20the%20ID%20classes.%20In%20this%20way%2C%20FA%20is%0Acapable%20of%20achieving%20notable%20improvements%20in%20OOD%20detection%2C%20even%20when%20trained%0Awithout%20any%20external%20auxiliary%20datasets%2C%20while%20maintaining%20an%20identical%20number%0Aof%20trainable%20parameters%20as%20CoOp.%20Extensive%20empirical%20evaluations%20confirm%20our%0Amethod%20consistently%20outperforms%20current%20state-of-the-art%20methods.%20Code%20is%0Aavailable%20at%20https%3A//github.com/0xFAFA/FA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFA%253A%2520Forced%2520Prompt%2520Learning%2520of%2520Vision-Language%2520Models%2520for%250A%2520%2520Out-of-Distribution%2520Detection%26entry.906535625%3DXinhua%2520Lu%2520and%2520Runhe%2520Lai%2520and%2520Yanqi%2520Wu%2520and%2520Kanghao%2520Chen%2520and%2520Wei-Shi%2520Zheng%2520and%2520Ruixuan%2520Wang%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520advanced%2520out-of-distribution%250A%2528OOD%2529%2520detection%2520recently.%2520However%252C%2520existing%2520CLIP-based%2520methods%2520often%2520focus%2520on%250Alearning%2520OOD-related%2520knowledge%2520to%2520improve%2520OOD%2520detection%252C%2520showing%2520limited%250Ageneralization%2520or%2520reliance%2520on%2520external%2520large-scale%2520auxiliary%2520datasets.%2520In%2520this%250Astudy%252C%2520instead%2520of%2520delving%2520into%2520the%2520intricate%2520OOD-related%2520knowledge%252C%2520we%2520propose%250Aan%2520innovative%2520CLIP-based%2520framework%2520based%2520on%2520Forced%2520prompt%2520leArning%2520%2528FA%2529%252C%250Adesigned%2520to%2520make%2520full%2520use%2520of%2520the%2520In-Distribution%2520%2528ID%2529%2520knowledge%2520and%2520ultimately%250Aboost%2520the%2520effectiveness%2520of%2520OOD%2520detection.%2520Our%2520key%2520insight%2520is%2520to%2520learn%2520a%2520prompt%250A%2528i.e.%252C%2520forced%2520prompt%2529%2520that%2520contains%2520more%2520diversified%2520and%2520richer%2520descriptions%2520of%250Athe%2520ID%2520classes%2520beyond%2520the%2520textual%2520semantics%2520of%2520class%2520labels.%2520Specifically%252C%2520it%250Apromotes%2520better%2520discernment%2520for%2520ID%2520images%252C%2520by%2520forcing%2520more%2520notable%2520semantic%250Asimilarity%2520between%2520ID%2520images%2520and%2520the%2520learnable%2520forced%2520prompt.%2520Moreover%252C%2520we%250Aintroduce%2520a%2520forced%2520coefficient%252C%2520encouraging%2520the%2520forced%2520prompt%2520to%2520learn%2520more%250Acomprehensive%2520and%2520nuanced%2520descriptions%2520of%2520the%2520ID%2520classes.%2520In%2520this%2520way%252C%2520FA%2520is%250Acapable%2520of%2520achieving%2520notable%2520improvements%2520in%2520OOD%2520detection%252C%2520even%2520when%2520trained%250Awithout%2520any%2520external%2520auxiliary%2520datasets%252C%2520while%2520maintaining%2520an%2520identical%2520number%250Aof%2520trainable%2520parameters%2520as%2520CoOp.%2520Extensive%2520empirical%2520evaluations%2520confirm%2520our%250Amethod%2520consistently%2520outperforms%2520current%2520state-of-the-art%2520methods.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/0xFAFA/FA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FA%3A%20Forced%20Prompt%20Learning%20of%20Vision-Language%20Models%20for%0A%20%20Out-of-Distribution%20Detection&entry.906535625=Xinhua%20Lu%20and%20Runhe%20Lai%20and%20Yanqi%20Wu%20and%20Kanghao%20Chen%20and%20Wei-Shi%20Zheng%20and%20Ruixuan%20Wang&entry.1292438233=%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%20have%20advanced%20out-of-distribution%0A%28OOD%29%20detection%20recently.%20However%2C%20existing%20CLIP-based%20methods%20often%20focus%20on%0Alearning%20OOD-related%20knowledge%20to%20improve%20OOD%20detection%2C%20showing%20limited%0Ageneralization%20or%20reliance%20on%20external%20large-scale%20auxiliary%20datasets.%20In%20this%0Astudy%2C%20instead%20of%20delving%20into%20the%20intricate%20OOD-related%20knowledge%2C%20we%20propose%0Aan%20innovative%20CLIP-based%20framework%20based%20on%20Forced%20prompt%20leArning%20%28FA%29%2C%0Adesigned%20to%20make%20full%20use%20of%20the%20In-Distribution%20%28ID%29%20knowledge%20and%20ultimately%0Aboost%20the%20effectiveness%20of%20OOD%20detection.%20Our%20key%20insight%20is%20to%20learn%20a%20prompt%0A%28i.e.%2C%20forced%20prompt%29%20that%20contains%20more%20diversified%20and%20richer%20descriptions%20of%0Athe%20ID%20classes%20beyond%20the%20textual%20semantics%20of%20class%20labels.%20Specifically%2C%20it%0Apromotes%20better%20discernment%20for%20ID%20images%2C%20by%20forcing%20more%20notable%20semantic%0Asimilarity%20between%20ID%20images%20and%20the%20learnable%20forced%20prompt.%20Moreover%2C%20we%0Aintroduce%20a%20forced%20coefficient%2C%20encouraging%20the%20forced%20prompt%20to%20learn%20more%0Acomprehensive%20and%20nuanced%20descriptions%20of%20the%20ID%20classes.%20In%20this%20way%2C%20FA%20is%0Acapable%20of%20achieving%20notable%20improvements%20in%20OOD%20detection%2C%20even%20when%20trained%0Awithout%20any%20external%20auxiliary%20datasets%2C%20while%20maintaining%20an%20identical%20number%0Aof%20trainable%20parameters%20as%20CoOp.%20Extensive%20empirical%20evaluations%20confirm%20our%0Amethod%20consistently%20outperforms%20current%20state-of-the-art%20methods.%20Code%20is%0Aavailable%20at%20https%3A//github.com/0xFAFA/FA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04511v2&entry.124074799=Read"},
{"title": "Multi-view mid fusion: a universal approach for learning in an HDLSS\n  setting", "author": "Lynn Houthuys", "abstract": "  The high-dimensional low-sample-size (HDLSS) setting presents significant\nchallenges in various applications where the feature dimension far exceeds the\nnumber of available samples. This paper introduces a universal approach for\nlearning in HDLSS setting using multi-view mid fusion techniques. It shows how\nexisting mid fusion multi-view methods perform well in an HDLSS setting even if\nno inherent views are provided. Three view construction methods are proposed\nthat split the high-dimensional feature vectors into smaller subsets, each\nrepresenting a different view. Extensive experimental validation across\nmodel-types and learning tasks confirm the effectiveness and generalization of\nthe approach. We believe the work in this paper lays the foundation for further\nresearch into the universal benefits of multi-view mid fusion learning.\n", "link": "http://arxiv.org/abs/2507.06026v1", "date": "2025-07-08", "relevancy": 2.7164, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20mid%20fusion%3A%20a%20universal%20approach%20for%20learning%20in%20an%20HDLSS%0A%20%20setting&body=Title%3A%20Multi-view%20mid%20fusion%3A%20a%20universal%20approach%20for%20learning%20in%20an%20HDLSS%0A%20%20setting%0AAuthor%3A%20Lynn%20Houthuys%0AAbstract%3A%20%20%20The%20high-dimensional%20low-sample-size%20%28HDLSS%29%20setting%20presents%20significant%0Achallenges%20in%20various%20applications%20where%20the%20feature%20dimension%20far%20exceeds%20the%0Anumber%20of%20available%20samples.%20This%20paper%20introduces%20a%20universal%20approach%20for%0Alearning%20in%20HDLSS%20setting%20using%20multi-view%20mid%20fusion%20techniques.%20It%20shows%20how%0Aexisting%20mid%20fusion%20multi-view%20methods%20perform%20well%20in%20an%20HDLSS%20setting%20even%20if%0Ano%20inherent%20views%20are%20provided.%20Three%20view%20construction%20methods%20are%20proposed%0Athat%20split%20the%20high-dimensional%20feature%20vectors%20into%20smaller%20subsets%2C%20each%0Arepresenting%20a%20different%20view.%20Extensive%20experimental%20validation%20across%0Amodel-types%20and%20learning%20tasks%20confirm%20the%20effectiveness%20and%20generalization%20of%0Athe%20approach.%20We%20believe%20the%20work%20in%20this%20paper%20lays%20the%20foundation%20for%20further%0Aresearch%20into%20the%20universal%20benefits%20of%20multi-view%20mid%20fusion%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520mid%2520fusion%253A%2520a%2520universal%2520approach%2520for%2520learning%2520in%2520an%2520HDLSS%250A%2520%2520setting%26entry.906535625%3DLynn%2520Houthuys%26entry.1292438233%3D%2520%2520The%2520high-dimensional%2520low-sample-size%2520%2528HDLSS%2529%2520setting%2520presents%2520significant%250Achallenges%2520in%2520various%2520applications%2520where%2520the%2520feature%2520dimension%2520far%2520exceeds%2520the%250Anumber%2520of%2520available%2520samples.%2520This%2520paper%2520introduces%2520a%2520universal%2520approach%2520for%250Alearning%2520in%2520HDLSS%2520setting%2520using%2520multi-view%2520mid%2520fusion%2520techniques.%2520It%2520shows%2520how%250Aexisting%2520mid%2520fusion%2520multi-view%2520methods%2520perform%2520well%2520in%2520an%2520HDLSS%2520setting%2520even%2520if%250Ano%2520inherent%2520views%2520are%2520provided.%2520Three%2520view%2520construction%2520methods%2520are%2520proposed%250Athat%2520split%2520the%2520high-dimensional%2520feature%2520vectors%2520into%2520smaller%2520subsets%252C%2520each%250Arepresenting%2520a%2520different%2520view.%2520Extensive%2520experimental%2520validation%2520across%250Amodel-types%2520and%2520learning%2520tasks%2520confirm%2520the%2520effectiveness%2520and%2520generalization%2520of%250Athe%2520approach.%2520We%2520believe%2520the%2520work%2520in%2520this%2520paper%2520lays%2520the%2520foundation%2520for%2520further%250Aresearch%2520into%2520the%2520universal%2520benefits%2520of%2520multi-view%2520mid%2520fusion%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20mid%20fusion%3A%20a%20universal%20approach%20for%20learning%20in%20an%20HDLSS%0A%20%20setting&entry.906535625=Lynn%20Houthuys&entry.1292438233=%20%20The%20high-dimensional%20low-sample-size%20%28HDLSS%29%20setting%20presents%20significant%0Achallenges%20in%20various%20applications%20where%20the%20feature%20dimension%20far%20exceeds%20the%0Anumber%20of%20available%20samples.%20This%20paper%20introduces%20a%20universal%20approach%20for%0Alearning%20in%20HDLSS%20setting%20using%20multi-view%20mid%20fusion%20techniques.%20It%20shows%20how%0Aexisting%20mid%20fusion%20multi-view%20methods%20perform%20well%20in%20an%20HDLSS%20setting%20even%20if%0Ano%20inherent%20views%20are%20provided.%20Three%20view%20construction%20methods%20are%20proposed%0Athat%20split%20the%20high-dimensional%20feature%20vectors%20into%20smaller%20subsets%2C%20each%0Arepresenting%20a%20different%20view.%20Extensive%20experimental%20validation%20across%0Amodel-types%20and%20learning%20tasks%20confirm%20the%20effectiveness%20and%20generalization%20of%0Athe%20approach.%20We%20believe%20the%20work%20in%20this%20paper%20lays%20the%20foundation%20for%20further%0Aresearch%20into%20the%20universal%20benefits%20of%20multi-view%20mid%20fusion%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06026v1&entry.124074799=Read"},
{"title": "MaSS13K: A Matting-level Semantic Segmentation Benchmark", "author": "Chenxi Xie and Minghan Li and Hui Zeng and Jun Luo and Lei Zhang", "abstract": "  High-resolution semantic segmentation is essential for applications such as\nimage editing, bokeh imaging, AR/VR, etc. Unfortunately, existing datasets\noften have limited resolution and lack precise mask details and boundaries. In\nthis work, we build a large-scale, matting-level semantic segmentation dataset,\nnamed MaSS13K, which consists of 13,348 real-world images, all at 4K\nresolution. MaSS13K provides high-quality mask annotations of a number of\nobjects, which are categorized into seven categories: human, vegetation,\nground, sky, water, building, and others. MaSS13K features precise masks, with\nan average mask complexity 20-50 times higher than existing semantic\nsegmentation datasets. We consequently present a method specifically designed\nfor high-resolution semantic segmentation, namely MaSSFormer, which employs an\nefficient pixel decoder that aggregates high-level semantic features and\nlow-level texture features across three stages, aiming to produce\nhigh-resolution masks with minimal computational cost. Finally, we propose a\nnew learning paradigm, which integrates the high-quality masks of the seven\ngiven categories with pseudo labels from new classes, enabling MaSSFormer to\ntransfer its accurate segmentation capability to other classes of objects. Our\nproposed MaSSFormer is comprehensively evaluated on the MaSS13K benchmark\ntogether with 14 representative segmentation models. We expect that our\nmeticulously annotated MaSS13K dataset and the MaSSFormer model can facilitate\nthe research of high-resolution and high-quality semantic segmentation.\nDatasets and codes can be found at https://github.com/xiechenxi99/MaSS13K.\n", "link": "http://arxiv.org/abs/2503.18364v2", "date": "2025-07-08", "relevancy": 2.6524, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaSS13K%3A%20A%20Matting-level%20Semantic%20Segmentation%20Benchmark&body=Title%3A%20MaSS13K%3A%20A%20Matting-level%20Semantic%20Segmentation%20Benchmark%0AAuthor%3A%20Chenxi%20Xie%20and%20Minghan%20Li%20and%20Hui%20Zeng%20and%20Jun%20Luo%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20High-resolution%20semantic%20segmentation%20is%20essential%20for%20applications%20such%20as%0Aimage%20editing%2C%20bokeh%20imaging%2C%20AR/VR%2C%20etc.%20Unfortunately%2C%20existing%20datasets%0Aoften%20have%20limited%20resolution%20and%20lack%20precise%20mask%20details%20and%20boundaries.%20In%0Athis%20work%2C%20we%20build%20a%20large-scale%2C%20matting-level%20semantic%20segmentation%20dataset%2C%0Anamed%20MaSS13K%2C%20which%20consists%20of%2013%2C348%20real-world%20images%2C%20all%20at%204K%0Aresolution.%20MaSS13K%20provides%20high-quality%20mask%20annotations%20of%20a%20number%20of%0Aobjects%2C%20which%20are%20categorized%20into%20seven%20categories%3A%20human%2C%20vegetation%2C%0Aground%2C%20sky%2C%20water%2C%20building%2C%20and%20others.%20MaSS13K%20features%20precise%20masks%2C%20with%0Aan%20average%20mask%20complexity%2020-50%20times%20higher%20than%20existing%20semantic%0Asegmentation%20datasets.%20We%20consequently%20present%20a%20method%20specifically%20designed%0Afor%20high-resolution%20semantic%20segmentation%2C%20namely%20MaSSFormer%2C%20which%20employs%20an%0Aefficient%20pixel%20decoder%20that%20aggregates%20high-level%20semantic%20features%20and%0Alow-level%20texture%20features%20across%20three%20stages%2C%20aiming%20to%20produce%0Ahigh-resolution%20masks%20with%20minimal%20computational%20cost.%20Finally%2C%20we%20propose%20a%0Anew%20learning%20paradigm%2C%20which%20integrates%20the%20high-quality%20masks%20of%20the%20seven%0Agiven%20categories%20with%20pseudo%20labels%20from%20new%20classes%2C%20enabling%20MaSSFormer%20to%0Atransfer%20its%20accurate%20segmentation%20capability%20to%20other%20classes%20of%20objects.%20Our%0Aproposed%20MaSSFormer%20is%20comprehensively%20evaluated%20on%20the%20MaSS13K%20benchmark%0Atogether%20with%2014%20representative%20segmentation%20models.%20We%20expect%20that%20our%0Ameticulously%20annotated%20MaSS13K%20dataset%20and%20the%20MaSSFormer%20model%20can%20facilitate%0Athe%20research%20of%20high-resolution%20and%20high-quality%20semantic%20segmentation.%0ADatasets%20and%20codes%20can%20be%20found%20at%20https%3A//github.com/xiechenxi99/MaSS13K.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18364v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaSS13K%253A%2520A%2520Matting-level%2520Semantic%2520Segmentation%2520Benchmark%26entry.906535625%3DChenxi%2520Xie%2520and%2520Minghan%2520Li%2520and%2520Hui%2520Zeng%2520and%2520Jun%2520Luo%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520High-resolution%2520semantic%2520segmentation%2520is%2520essential%2520for%2520applications%2520such%2520as%250Aimage%2520editing%252C%2520bokeh%2520imaging%252C%2520AR/VR%252C%2520etc.%2520Unfortunately%252C%2520existing%2520datasets%250Aoften%2520have%2520limited%2520resolution%2520and%2520lack%2520precise%2520mask%2520details%2520and%2520boundaries.%2520In%250Athis%2520work%252C%2520we%2520build%2520a%2520large-scale%252C%2520matting-level%2520semantic%2520segmentation%2520dataset%252C%250Anamed%2520MaSS13K%252C%2520which%2520consists%2520of%252013%252C348%2520real-world%2520images%252C%2520all%2520at%25204K%250Aresolution.%2520MaSS13K%2520provides%2520high-quality%2520mask%2520annotations%2520of%2520a%2520number%2520of%250Aobjects%252C%2520which%2520are%2520categorized%2520into%2520seven%2520categories%253A%2520human%252C%2520vegetation%252C%250Aground%252C%2520sky%252C%2520water%252C%2520building%252C%2520and%2520others.%2520MaSS13K%2520features%2520precise%2520masks%252C%2520with%250Aan%2520average%2520mask%2520complexity%252020-50%2520times%2520higher%2520than%2520existing%2520semantic%250Asegmentation%2520datasets.%2520We%2520consequently%2520present%2520a%2520method%2520specifically%2520designed%250Afor%2520high-resolution%2520semantic%2520segmentation%252C%2520namely%2520MaSSFormer%252C%2520which%2520employs%2520an%250Aefficient%2520pixel%2520decoder%2520that%2520aggregates%2520high-level%2520semantic%2520features%2520and%250Alow-level%2520texture%2520features%2520across%2520three%2520stages%252C%2520aiming%2520to%2520produce%250Ahigh-resolution%2520masks%2520with%2520minimal%2520computational%2520cost.%2520Finally%252C%2520we%2520propose%2520a%250Anew%2520learning%2520paradigm%252C%2520which%2520integrates%2520the%2520high-quality%2520masks%2520of%2520the%2520seven%250Agiven%2520categories%2520with%2520pseudo%2520labels%2520from%2520new%2520classes%252C%2520enabling%2520MaSSFormer%2520to%250Atransfer%2520its%2520accurate%2520segmentation%2520capability%2520to%2520other%2520classes%2520of%2520objects.%2520Our%250Aproposed%2520MaSSFormer%2520is%2520comprehensively%2520evaluated%2520on%2520the%2520MaSS13K%2520benchmark%250Atogether%2520with%252014%2520representative%2520segmentation%2520models.%2520We%2520expect%2520that%2520our%250Ameticulously%2520annotated%2520MaSS13K%2520dataset%2520and%2520the%2520MaSSFormer%2520model%2520can%2520facilitate%250Athe%2520research%2520of%2520high-resolution%2520and%2520high-quality%2520semantic%2520segmentation.%250ADatasets%2520and%2520codes%2520can%2520be%2520found%2520at%2520https%253A//github.com/xiechenxi99/MaSS13K.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18364v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaSS13K%3A%20A%20Matting-level%20Semantic%20Segmentation%20Benchmark&entry.906535625=Chenxi%20Xie%20and%20Minghan%20Li%20and%20Hui%20Zeng%20and%20Jun%20Luo%20and%20Lei%20Zhang&entry.1292438233=%20%20High-resolution%20semantic%20segmentation%20is%20essential%20for%20applications%20such%20as%0Aimage%20editing%2C%20bokeh%20imaging%2C%20AR/VR%2C%20etc.%20Unfortunately%2C%20existing%20datasets%0Aoften%20have%20limited%20resolution%20and%20lack%20precise%20mask%20details%20and%20boundaries.%20In%0Athis%20work%2C%20we%20build%20a%20large-scale%2C%20matting-level%20semantic%20segmentation%20dataset%2C%0Anamed%20MaSS13K%2C%20which%20consists%20of%2013%2C348%20real-world%20images%2C%20all%20at%204K%0Aresolution.%20MaSS13K%20provides%20high-quality%20mask%20annotations%20of%20a%20number%20of%0Aobjects%2C%20which%20are%20categorized%20into%20seven%20categories%3A%20human%2C%20vegetation%2C%0Aground%2C%20sky%2C%20water%2C%20building%2C%20and%20others.%20MaSS13K%20features%20precise%20masks%2C%20with%0Aan%20average%20mask%20complexity%2020-50%20times%20higher%20than%20existing%20semantic%0Asegmentation%20datasets.%20We%20consequently%20present%20a%20method%20specifically%20designed%0Afor%20high-resolution%20semantic%20segmentation%2C%20namely%20MaSSFormer%2C%20which%20employs%20an%0Aefficient%20pixel%20decoder%20that%20aggregates%20high-level%20semantic%20features%20and%0Alow-level%20texture%20features%20across%20three%20stages%2C%20aiming%20to%20produce%0Ahigh-resolution%20masks%20with%20minimal%20computational%20cost.%20Finally%2C%20we%20propose%20a%0Anew%20learning%20paradigm%2C%20which%20integrates%20the%20high-quality%20masks%20of%20the%20seven%0Agiven%20categories%20with%20pseudo%20labels%20from%20new%20classes%2C%20enabling%20MaSSFormer%20to%0Atransfer%20its%20accurate%20segmentation%20capability%20to%20other%20classes%20of%20objects.%20Our%0Aproposed%20MaSSFormer%20is%20comprehensively%20evaluated%20on%20the%20MaSS13K%20benchmark%0Atogether%20with%2014%20representative%20segmentation%20models.%20We%20expect%20that%20our%0Ameticulously%20annotated%20MaSS13K%20dataset%20and%20the%20MaSSFormer%20model%20can%20facilitate%0Athe%20research%20of%20high-resolution%20and%20high-quality%20semantic%20segmentation.%0ADatasets%20and%20codes%20can%20be%20found%20at%20https%3A//github.com/xiechenxi99/MaSS13K.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18364v2&entry.124074799=Read"},
{"title": "Coding Triangle: How Does Large Language Model Understand Code?", "author": "Taolin Zhang and Zihan Ma and Maosong Cao and Junnan Liu and Songyang Zhang and Kai Chen", "abstract": "  Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models.\n", "link": "http://arxiv.org/abs/2507.06138v1", "date": "2025-07-08", "relevancy": 2.6349, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coding%20Triangle%3A%20How%20Does%20Large%20Language%20Model%20Understand%20Code%3F&body=Title%3A%20Coding%20Triangle%3A%20How%20Does%20Large%20Language%20Model%20Understand%20Code%3F%0AAuthor%3A%20Taolin%20Zhang%20and%20Zihan%20Ma%20and%20Maosong%20Cao%20and%20Junnan%20Liu%20and%20Songyang%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20in%20code%0Ageneration%2C%20yet%20their%20true%20programming%20competence%20remains%20underexplored.%20We%0Aintroduce%20the%20Code%20Triangle%20framework%2C%20which%20systematically%20evaluates%20LLMs%0Aacross%20three%20fundamental%20dimensions%3A%20editorial%20analysis%2C%20code%20implementation%2C%0Aand%20test%20case%20generation.%20Through%20extensive%20experiments%20on%20competitive%0Aprogramming%20benchmarks%2C%20we%20reveal%20that%20while%20LLMs%20can%20form%20a%20self-consistent%0Asystem%20across%20these%20dimensions%2C%20their%20solutions%20often%20lack%20the%20diversity%20and%0Arobustness%20of%20human%20programmers.%20We%20identify%20a%20significant%20distribution%20shift%0Abetween%20model%20cognition%20and%20human%20expertise%2C%20with%20model%20errors%20tending%20to%0Acluster%20due%20to%20training%20data%20biases%20and%20limited%20reasoning%20transfer.%20Our%20study%0Ademonstrates%20that%20incorporating%20human-generated%20editorials%2C%20solutions%2C%20and%0Adiverse%20test%20cases%2C%20as%20well%20as%20leveraging%20model%20mixtures%2C%20can%20substantially%0Aenhance%20both%20the%20performance%20and%20robustness%20of%20LLMs.%20Furthermore%2C%20we%20reveal%0Aboth%20the%20consistency%20and%20inconsistency%20in%20the%20cognition%20of%20LLMs%20that%20may%0Afacilitate%20self-reflection%20and%20self-improvement%2C%20providing%20a%20potential%0Adirection%20for%20developing%20more%20powerful%20coding%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06138v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoding%2520Triangle%253A%2520How%2520Does%2520Large%2520Language%2520Model%2520Understand%2520Code%253F%26entry.906535625%3DTaolin%2520Zhang%2520and%2520Zihan%2520Ma%2520and%2520Maosong%2520Cao%2520and%2520Junnan%2520Liu%2520and%2520Songyang%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520in%2520code%250Ageneration%252C%2520yet%2520their%2520true%2520programming%2520competence%2520remains%2520underexplored.%2520We%250Aintroduce%2520the%2520Code%2520Triangle%2520framework%252C%2520which%2520systematically%2520evaluates%2520LLMs%250Aacross%2520three%2520fundamental%2520dimensions%253A%2520editorial%2520analysis%252C%2520code%2520implementation%252C%250Aand%2520test%2520case%2520generation.%2520Through%2520extensive%2520experiments%2520on%2520competitive%250Aprogramming%2520benchmarks%252C%2520we%2520reveal%2520that%2520while%2520LLMs%2520can%2520form%2520a%2520self-consistent%250Asystem%2520across%2520these%2520dimensions%252C%2520their%2520solutions%2520often%2520lack%2520the%2520diversity%2520and%250Arobustness%2520of%2520human%2520programmers.%2520We%2520identify%2520a%2520significant%2520distribution%2520shift%250Abetween%2520model%2520cognition%2520and%2520human%2520expertise%252C%2520with%2520model%2520errors%2520tending%2520to%250Acluster%2520due%2520to%2520training%2520data%2520biases%2520and%2520limited%2520reasoning%2520transfer.%2520Our%2520study%250Ademonstrates%2520that%2520incorporating%2520human-generated%2520editorials%252C%2520solutions%252C%2520and%250Adiverse%2520test%2520cases%252C%2520as%2520well%2520as%2520leveraging%2520model%2520mixtures%252C%2520can%2520substantially%250Aenhance%2520both%2520the%2520performance%2520and%2520robustness%2520of%2520LLMs.%2520Furthermore%252C%2520we%2520reveal%250Aboth%2520the%2520consistency%2520and%2520inconsistency%2520in%2520the%2520cognition%2520of%2520LLMs%2520that%2520may%250Afacilitate%2520self-reflection%2520and%2520self-improvement%252C%2520providing%2520a%2520potential%250Adirection%2520for%2520developing%2520more%2520powerful%2520coding%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06138v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coding%20Triangle%3A%20How%20Does%20Large%20Language%20Model%20Understand%20Code%3F&entry.906535625=Taolin%20Zhang%20and%20Zihan%20Ma%20and%20Maosong%20Cao%20and%20Junnan%20Liu%20and%20Songyang%20Zhang%20and%20Kai%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20in%20code%0Ageneration%2C%20yet%20their%20true%20programming%20competence%20remains%20underexplored.%20We%0Aintroduce%20the%20Code%20Triangle%20framework%2C%20which%20systematically%20evaluates%20LLMs%0Aacross%20three%20fundamental%20dimensions%3A%20editorial%20analysis%2C%20code%20implementation%2C%0Aand%20test%20case%20generation.%20Through%20extensive%20experiments%20on%20competitive%0Aprogramming%20benchmarks%2C%20we%20reveal%20that%20while%20LLMs%20can%20form%20a%20self-consistent%0Asystem%20across%20these%20dimensions%2C%20their%20solutions%20often%20lack%20the%20diversity%20and%0Arobustness%20of%20human%20programmers.%20We%20identify%20a%20significant%20distribution%20shift%0Abetween%20model%20cognition%20and%20human%20expertise%2C%20with%20model%20errors%20tending%20to%0Acluster%20due%20to%20training%20data%20biases%20and%20limited%20reasoning%20transfer.%20Our%20study%0Ademonstrates%20that%20incorporating%20human-generated%20editorials%2C%20solutions%2C%20and%0Adiverse%20test%20cases%2C%20as%20well%20as%20leveraging%20model%20mixtures%2C%20can%20substantially%0Aenhance%20both%20the%20performance%20and%20robustness%20of%20LLMs.%20Furthermore%2C%20we%20reveal%0Aboth%20the%20consistency%20and%20inconsistency%20in%20the%20cognition%20of%20LLMs%20that%20may%0Afacilitate%20self-reflection%20and%20self-improvement%2C%20providing%20a%20potential%0Adirection%20for%20developing%20more%20powerful%20coding%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06138v1&entry.124074799=Read"},
{"title": "Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action\n  Recognition via Learning Temporal-Frequency Dynamics", "author": "Naichuan Zheng and Yuchen Du and Hailun Xia and Zeyu Liang", "abstract": "  For multimodal skeleton-based action recognition, Graph Convolutional\nNetworks (GCNs) are effective models. Still, their reliance on floating-point\ncomputations leads to high energy consumption, limiting their applicability in\nbattery-powered devices. While energy-efficient, Spiking Neural Networks (SNNs)\nstruggle to model skeleton dynamics, leading to suboptimal solutions. We\npropose Signal-SGN (Spiking Graph Convolutional Network), which utilizes the\ntemporal dimension of skeleton sequences as the spike time steps and represents\nfeatures as multi-dimensional discrete stochastic signals for\ntemporal-frequency domain feature extraction. It combines the 1D Spiking Graph\nConvolution (1D-SGC) module and the Frequency Spiking Convolution (FSC) module\nto extract features from the skeleton represented as spiking form.\nAdditionally, the Multi-Scale Wavelet Transform Feature Fusion (MWTF) module is\nproposed to extract dynamic spiking features and capture frequency-specific\ncharacteristics, enhancing classification performance. Experiments across three\nlarge-scale datasets reveal Signal-SGN exceeding state-of-the-art SNN-based\nmethods in accuracy and computational efficiency while attaining comparable\nperformance with GCN methods and significantly reducing theoretical energy\nconsumption.\n", "link": "http://arxiv.org/abs/2408.01701v4", "date": "2025-07-08", "relevancy": 2.5769, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5179}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5143}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Signal-SGN%3A%20A%20Spiking%20Graph%20Convolutional%20Network%20for%20Skeletal%20Action%0A%20%20Recognition%20via%20Learning%20Temporal-Frequency%20Dynamics&body=Title%3A%20Signal-SGN%3A%20A%20Spiking%20Graph%20Convolutional%20Network%20for%20Skeletal%20Action%0A%20%20Recognition%20via%20Learning%20Temporal-Frequency%20Dynamics%0AAuthor%3A%20Naichuan%20Zheng%20and%20Yuchen%20Du%20and%20Hailun%20Xia%20and%20Zeyu%20Liang%0AAbstract%3A%20%20%20For%20multimodal%20skeleton-based%20action%20recognition%2C%20Graph%20Convolutional%0ANetworks%20%28GCNs%29%20are%20effective%20models.%20Still%2C%20their%20reliance%20on%20floating-point%0Acomputations%20leads%20to%20high%20energy%20consumption%2C%20limiting%20their%20applicability%20in%0Abattery-powered%20devices.%20While%20energy-efficient%2C%20Spiking%20Neural%20Networks%20%28SNNs%29%0Astruggle%20to%20model%20skeleton%20dynamics%2C%20leading%20to%20suboptimal%20solutions.%20We%0Apropose%20Signal-SGN%20%28Spiking%20Graph%20Convolutional%20Network%29%2C%20which%20utilizes%20the%0Atemporal%20dimension%20of%20skeleton%20sequences%20as%20the%20spike%20time%20steps%20and%20represents%0Afeatures%20as%20multi-dimensional%20discrete%20stochastic%20signals%20for%0Atemporal-frequency%20domain%20feature%20extraction.%20It%20combines%20the%201D%20Spiking%20Graph%0AConvolution%20%281D-SGC%29%20module%20and%20the%20Frequency%20Spiking%20Convolution%20%28FSC%29%20module%0Ato%20extract%20features%20from%20the%20skeleton%20represented%20as%20spiking%20form.%0AAdditionally%2C%20the%20Multi-Scale%20Wavelet%20Transform%20Feature%20Fusion%20%28MWTF%29%20module%20is%0Aproposed%20to%20extract%20dynamic%20spiking%20features%20and%20capture%20frequency-specific%0Acharacteristics%2C%20enhancing%20classification%20performance.%20Experiments%20across%20three%0Alarge-scale%20datasets%20reveal%20Signal-SGN%20exceeding%20state-of-the-art%20SNN-based%0Amethods%20in%20accuracy%20and%20computational%20efficiency%20while%20attaining%20comparable%0Aperformance%20with%20GCN%20methods%20and%20significantly%20reducing%20theoretical%20energy%0Aconsumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01701v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignal-SGN%253A%2520A%2520Spiking%2520Graph%2520Convolutional%2520Network%2520for%2520Skeletal%2520Action%250A%2520%2520Recognition%2520via%2520Learning%2520Temporal-Frequency%2520Dynamics%26entry.906535625%3DNaichuan%2520Zheng%2520and%2520Yuchen%2520Du%2520and%2520Hailun%2520Xia%2520and%2520Zeyu%2520Liang%26entry.1292438233%3D%2520%2520For%2520multimodal%2520skeleton-based%2520action%2520recognition%252C%2520Graph%2520Convolutional%250ANetworks%2520%2528GCNs%2529%2520are%2520effective%2520models.%2520Still%252C%2520their%2520reliance%2520on%2520floating-point%250Acomputations%2520leads%2520to%2520high%2520energy%2520consumption%252C%2520limiting%2520their%2520applicability%2520in%250Abattery-powered%2520devices.%2520While%2520energy-efficient%252C%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%250Astruggle%2520to%2520model%2520skeleton%2520dynamics%252C%2520leading%2520to%2520suboptimal%2520solutions.%2520We%250Apropose%2520Signal-SGN%2520%2528Spiking%2520Graph%2520Convolutional%2520Network%2529%252C%2520which%2520utilizes%2520the%250Atemporal%2520dimension%2520of%2520skeleton%2520sequences%2520as%2520the%2520spike%2520time%2520steps%2520and%2520represents%250Afeatures%2520as%2520multi-dimensional%2520discrete%2520stochastic%2520signals%2520for%250Atemporal-frequency%2520domain%2520feature%2520extraction.%2520It%2520combines%2520the%25201D%2520Spiking%2520Graph%250AConvolution%2520%25281D-SGC%2529%2520module%2520and%2520the%2520Frequency%2520Spiking%2520Convolution%2520%2528FSC%2529%2520module%250Ato%2520extract%2520features%2520from%2520the%2520skeleton%2520represented%2520as%2520spiking%2520form.%250AAdditionally%252C%2520the%2520Multi-Scale%2520Wavelet%2520Transform%2520Feature%2520Fusion%2520%2528MWTF%2529%2520module%2520is%250Aproposed%2520to%2520extract%2520dynamic%2520spiking%2520features%2520and%2520capture%2520frequency-specific%250Acharacteristics%252C%2520enhancing%2520classification%2520performance.%2520Experiments%2520across%2520three%250Alarge-scale%2520datasets%2520reveal%2520Signal-SGN%2520exceeding%2520state-of-the-art%2520SNN-based%250Amethods%2520in%2520accuracy%2520and%2520computational%2520efficiency%2520while%2520attaining%2520comparable%250Aperformance%2520with%2520GCN%2520methods%2520and%2520significantly%2520reducing%2520theoretical%2520energy%250Aconsumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01701v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signal-SGN%3A%20A%20Spiking%20Graph%20Convolutional%20Network%20for%20Skeletal%20Action%0A%20%20Recognition%20via%20Learning%20Temporal-Frequency%20Dynamics&entry.906535625=Naichuan%20Zheng%20and%20Yuchen%20Du%20and%20Hailun%20Xia%20and%20Zeyu%20Liang&entry.1292438233=%20%20For%20multimodal%20skeleton-based%20action%20recognition%2C%20Graph%20Convolutional%0ANetworks%20%28GCNs%29%20are%20effective%20models.%20Still%2C%20their%20reliance%20on%20floating-point%0Acomputations%20leads%20to%20high%20energy%20consumption%2C%20limiting%20their%20applicability%20in%0Abattery-powered%20devices.%20While%20energy-efficient%2C%20Spiking%20Neural%20Networks%20%28SNNs%29%0Astruggle%20to%20model%20skeleton%20dynamics%2C%20leading%20to%20suboptimal%20solutions.%20We%0Apropose%20Signal-SGN%20%28Spiking%20Graph%20Convolutional%20Network%29%2C%20which%20utilizes%20the%0Atemporal%20dimension%20of%20skeleton%20sequences%20as%20the%20spike%20time%20steps%20and%20represents%0Afeatures%20as%20multi-dimensional%20discrete%20stochastic%20signals%20for%0Atemporal-frequency%20domain%20feature%20extraction.%20It%20combines%20the%201D%20Spiking%20Graph%0AConvolution%20%281D-SGC%29%20module%20and%20the%20Frequency%20Spiking%20Convolution%20%28FSC%29%20module%0Ato%20extract%20features%20from%20the%20skeleton%20represented%20as%20spiking%20form.%0AAdditionally%2C%20the%20Multi-Scale%20Wavelet%20Transform%20Feature%20Fusion%20%28MWTF%29%20module%20is%0Aproposed%20to%20extract%20dynamic%20spiking%20features%20and%20capture%20frequency-specific%0Acharacteristics%2C%20enhancing%20classification%20performance.%20Experiments%20across%20three%0Alarge-scale%20datasets%20reveal%20Signal-SGN%20exceeding%20state-of-the-art%20SNN-based%0Amethods%20in%20accuracy%20and%20computational%20efficiency%20while%20attaining%20comparable%0Aperformance%20with%20GCN%20methods%20and%20significantly%20reducing%20theoretical%20energy%0Aconsumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01701v4&entry.124074799=Read"},
{"title": "cuVSLAM: CUDA accelerated visual odometry and mapping", "author": "Alexander Korovko and Dmitry Slepichev and Alexander Efitorov and Aigul Dzhumamuratova and Viktor Kuznetsov and Hesam Rabeti and Joydeep Biswas and Soha Pouya", "abstract": "  Accurate and robust pose estimation is a key requirement for any autonomous\nrobot. We present cuVSLAM, a state-of-the-art solution for visual simultaneous\nlocalization and mapping, which can operate with a variety of visual-inertial\nsensor suites, including multiple RGB and depth cameras, and inertial\nmeasurement units. cuVSLAM supports operation with as few as one RGB camera to\nas many as 32 cameras, in arbitrary geometric configurations, thus supporting a\nwide range of robotic setups. cuVSLAM is specifically optimized using CUDA to\ndeploy in real-time applications with minimal computational overhead on\nedge-computing devices such as the NVIDIA Jetson. We present the design and\nimplementation of cuVSLAM, example use cases, and empirical results on several\nstate-of-the-art benchmarks demonstrating the best-in-class performance of\ncuVSLAM.\n", "link": "http://arxiv.org/abs/2506.04359v3", "date": "2025-07-08", "relevancy": 2.4891, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.636}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6253}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20cuVSLAM%3A%20CUDA%20accelerated%20visual%20odometry%20and%20mapping&body=Title%3A%20cuVSLAM%3A%20CUDA%20accelerated%20visual%20odometry%20and%20mapping%0AAuthor%3A%20Alexander%20Korovko%20and%20Dmitry%20Slepichev%20and%20Alexander%20Efitorov%20and%20Aigul%20Dzhumamuratova%20and%20Viktor%20Kuznetsov%20and%20Hesam%20Rabeti%20and%20Joydeep%20Biswas%20and%20Soha%20Pouya%0AAbstract%3A%20%20%20Accurate%20and%20robust%20pose%20estimation%20is%20a%20key%20requirement%20for%20any%20autonomous%0Arobot.%20We%20present%20cuVSLAM%2C%20a%20state-of-the-art%20solution%20for%20visual%20simultaneous%0Alocalization%20and%20mapping%2C%20which%20can%20operate%20with%20a%20variety%20of%20visual-inertial%0Asensor%20suites%2C%20including%20multiple%20RGB%20and%20depth%20cameras%2C%20and%20inertial%0Ameasurement%20units.%20cuVSLAM%20supports%20operation%20with%20as%20few%20as%20one%20RGB%20camera%20to%0Aas%20many%20as%2032%20cameras%2C%20in%20arbitrary%20geometric%20configurations%2C%20thus%20supporting%20a%0Awide%20range%20of%20robotic%20setups.%20cuVSLAM%20is%20specifically%20optimized%20using%20CUDA%20to%0Adeploy%20in%20real-time%20applications%20with%20minimal%20computational%20overhead%20on%0Aedge-computing%20devices%20such%20as%20the%20NVIDIA%20Jetson.%20We%20present%20the%20design%20and%0Aimplementation%20of%20cuVSLAM%2C%20example%20use%20cases%2C%20and%20empirical%20results%20on%20several%0Astate-of-the-art%20benchmarks%20demonstrating%20the%20best-in-class%20performance%20of%0AcuVSLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04359v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DcuVSLAM%253A%2520CUDA%2520accelerated%2520visual%2520odometry%2520and%2520mapping%26entry.906535625%3DAlexander%2520Korovko%2520and%2520Dmitry%2520Slepichev%2520and%2520Alexander%2520Efitorov%2520and%2520Aigul%2520Dzhumamuratova%2520and%2520Viktor%2520Kuznetsov%2520and%2520Hesam%2520Rabeti%2520and%2520Joydeep%2520Biswas%2520and%2520Soha%2520Pouya%26entry.1292438233%3D%2520%2520Accurate%2520and%2520robust%2520pose%2520estimation%2520is%2520a%2520key%2520requirement%2520for%2520any%2520autonomous%250Arobot.%2520We%2520present%2520cuVSLAM%252C%2520a%2520state-of-the-art%2520solution%2520for%2520visual%2520simultaneous%250Alocalization%2520and%2520mapping%252C%2520which%2520can%2520operate%2520with%2520a%2520variety%2520of%2520visual-inertial%250Asensor%2520suites%252C%2520including%2520multiple%2520RGB%2520and%2520depth%2520cameras%252C%2520and%2520inertial%250Ameasurement%2520units.%2520cuVSLAM%2520supports%2520operation%2520with%2520as%2520few%2520as%2520one%2520RGB%2520camera%2520to%250Aas%2520many%2520as%252032%2520cameras%252C%2520in%2520arbitrary%2520geometric%2520configurations%252C%2520thus%2520supporting%2520a%250Awide%2520range%2520of%2520robotic%2520setups.%2520cuVSLAM%2520is%2520specifically%2520optimized%2520using%2520CUDA%2520to%250Adeploy%2520in%2520real-time%2520applications%2520with%2520minimal%2520computational%2520overhead%2520on%250Aedge-computing%2520devices%2520such%2520as%2520the%2520NVIDIA%2520Jetson.%2520We%2520present%2520the%2520design%2520and%250Aimplementation%2520of%2520cuVSLAM%252C%2520example%2520use%2520cases%252C%2520and%2520empirical%2520results%2520on%2520several%250Astate-of-the-art%2520benchmarks%2520demonstrating%2520the%2520best-in-class%2520performance%2520of%250AcuVSLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04359v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=cuVSLAM%3A%20CUDA%20accelerated%20visual%20odometry%20and%20mapping&entry.906535625=Alexander%20Korovko%20and%20Dmitry%20Slepichev%20and%20Alexander%20Efitorov%20and%20Aigul%20Dzhumamuratova%20and%20Viktor%20Kuznetsov%20and%20Hesam%20Rabeti%20and%20Joydeep%20Biswas%20and%20Soha%20Pouya&entry.1292438233=%20%20Accurate%20and%20robust%20pose%20estimation%20is%20a%20key%20requirement%20for%20any%20autonomous%0Arobot.%20We%20present%20cuVSLAM%2C%20a%20state-of-the-art%20solution%20for%20visual%20simultaneous%0Alocalization%20and%20mapping%2C%20which%20can%20operate%20with%20a%20variety%20of%20visual-inertial%0Asensor%20suites%2C%20including%20multiple%20RGB%20and%20depth%20cameras%2C%20and%20inertial%0Ameasurement%20units.%20cuVSLAM%20supports%20operation%20with%20as%20few%20as%20one%20RGB%20camera%20to%0Aas%20many%20as%2032%20cameras%2C%20in%20arbitrary%20geometric%20configurations%2C%20thus%20supporting%20a%0Awide%20range%20of%20robotic%20setups.%20cuVSLAM%20is%20specifically%20optimized%20using%20CUDA%20to%0Adeploy%20in%20real-time%20applications%20with%20minimal%20computational%20overhead%20on%0Aedge-computing%20devices%20such%20as%20the%20NVIDIA%20Jetson.%20We%20present%20the%20design%20and%0Aimplementation%20of%20cuVSLAM%2C%20example%20use%20cases%2C%20and%20empirical%20results%20on%20several%0Astate-of-the-art%20benchmarks%20demonstrating%20the%20best-in-class%20performance%20of%0AcuVSLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04359v3&entry.124074799=Read"},
{"title": "RichControl: Structure- and Appearance-Rich Training-Free Spatial\n  Control for Text-to-Image Generation", "author": "Liheng Zhang and Lexi Pang and Hang Ye and Xiaoxuan Ma and Yizhou Wang", "abstract": "  Text-to-image (T2I) diffusion models have shown remarkable success in\ngenerating high-quality images from text prompts. Recent efforts extend these\nmodels to incorporate conditional images (e.g., depth or pose maps) for\nfine-grained spatial control. Among them, feature injection methods have\nemerged as a training-free alternative to traditional fine-tuning approaches.\nHowever, they often suffer from structural misalignment, condition leakage, and\nvisual artifacts, especially when the condition image diverges significantly\nfrom natural RGB distributions. By revisiting existing methods, we identify a\ncore limitation: the synchronous injection of condition features fails to\naccount for the trade-off between domain alignment and structural preservation\nduring denoising. Inspired by this observation, we propose a flexible feature\ninjection framework that decouples the injection timestep from the denoising\nprocess. At its core is a structure-rich injection module, which enables the\nmodel to better adapt to the evolving interplay between alignment and structure\npreservation throughout the diffusion steps, resulting in more faithful\nstructural generation. In addition, we introduce appearance-rich prompting and\na restart refinement strategy to further enhance appearance control and visual\nquality. Together, these designs enable training-free generation that is both\nstructure-rich and appearance-rich. Extensive experiments show that our\napproach achieves state-of-the-art performance across diverse zero-shot\nconditioning scenarios.\n", "link": "http://arxiv.org/abs/2507.02792v2", "date": "2025-07-08", "relevancy": 2.4783, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6363}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6179}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RichControl%3A%20Structure-%20and%20Appearance-Rich%20Training-Free%20Spatial%0A%20%20Control%20for%20Text-to-Image%20Generation&body=Title%3A%20RichControl%3A%20Structure-%20and%20Appearance-Rich%20Training-Free%20Spatial%0A%20%20Control%20for%20Text-to-Image%20Generation%0AAuthor%3A%20Liheng%20Zhang%20and%20Lexi%20Pang%20and%20Hang%20Ye%20and%20Xiaoxuan%20Ma%20and%20Yizhou%20Wang%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20shown%20remarkable%20success%20in%0Agenerating%20high-quality%20images%20from%20text%20prompts.%20Recent%20efforts%20extend%20these%0Amodels%20to%20incorporate%20conditional%20images%20%28e.g.%2C%20depth%20or%20pose%20maps%29%20for%0Afine-grained%20spatial%20control.%20Among%20them%2C%20feature%20injection%20methods%20have%0Aemerged%20as%20a%20training-free%20alternative%20to%20traditional%20fine-tuning%20approaches.%0AHowever%2C%20they%20often%20suffer%20from%20structural%20misalignment%2C%20condition%20leakage%2C%20and%0Avisual%20artifacts%2C%20especially%20when%20the%20condition%20image%20diverges%20significantly%0Afrom%20natural%20RGB%20distributions.%20By%20revisiting%20existing%20methods%2C%20we%20identify%20a%0Acore%20limitation%3A%20the%20synchronous%20injection%20of%20condition%20features%20fails%20to%0Aaccount%20for%20the%20trade-off%20between%20domain%20alignment%20and%20structural%20preservation%0Aduring%20denoising.%20Inspired%20by%20this%20observation%2C%20we%20propose%20a%20flexible%20feature%0Ainjection%20framework%20that%20decouples%20the%20injection%20timestep%20from%20the%20denoising%0Aprocess.%20At%20its%20core%20is%20a%20structure-rich%20injection%20module%2C%20which%20enables%20the%0Amodel%20to%20better%20adapt%20to%20the%20evolving%20interplay%20between%20alignment%20and%20structure%0Apreservation%20throughout%20the%20diffusion%20steps%2C%20resulting%20in%20more%20faithful%0Astructural%20generation.%20In%20addition%2C%20we%20introduce%20appearance-rich%20prompting%20and%0Aa%20restart%20refinement%20strategy%20to%20further%20enhance%20appearance%20control%20and%20visual%0Aquality.%20Together%2C%20these%20designs%20enable%20training-free%20generation%20that%20is%20both%0Astructure-rich%20and%20appearance-rich.%20Extensive%20experiments%20show%20that%20our%0Aapproach%20achieves%20state-of-the-art%20performance%20across%20diverse%20zero-shot%0Aconditioning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRichControl%253A%2520Structure-%2520and%2520Appearance-Rich%2520Training-Free%2520Spatial%250A%2520%2520Control%2520for%2520Text-to-Image%2520Generation%26entry.906535625%3DLiheng%2520Zhang%2520and%2520Lexi%2520Pang%2520and%2520Hang%2520Ye%2520and%2520Xiaoxuan%2520Ma%2520and%2520Yizhou%2520Wang%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520have%2520shown%2520remarkable%2520success%2520in%250Agenerating%2520high-quality%2520images%2520from%2520text%2520prompts.%2520Recent%2520efforts%2520extend%2520these%250Amodels%2520to%2520incorporate%2520conditional%2520images%2520%2528e.g.%252C%2520depth%2520or%2520pose%2520maps%2529%2520for%250Afine-grained%2520spatial%2520control.%2520Among%2520them%252C%2520feature%2520injection%2520methods%2520have%250Aemerged%2520as%2520a%2520training-free%2520alternative%2520to%2520traditional%2520fine-tuning%2520approaches.%250AHowever%252C%2520they%2520often%2520suffer%2520from%2520structural%2520misalignment%252C%2520condition%2520leakage%252C%2520and%250Avisual%2520artifacts%252C%2520especially%2520when%2520the%2520condition%2520image%2520diverges%2520significantly%250Afrom%2520natural%2520RGB%2520distributions.%2520By%2520revisiting%2520existing%2520methods%252C%2520we%2520identify%2520a%250Acore%2520limitation%253A%2520the%2520synchronous%2520injection%2520of%2520condition%2520features%2520fails%2520to%250Aaccount%2520for%2520the%2520trade-off%2520between%2520domain%2520alignment%2520and%2520structural%2520preservation%250Aduring%2520denoising.%2520Inspired%2520by%2520this%2520observation%252C%2520we%2520propose%2520a%2520flexible%2520feature%250Ainjection%2520framework%2520that%2520decouples%2520the%2520injection%2520timestep%2520from%2520the%2520denoising%250Aprocess.%2520At%2520its%2520core%2520is%2520a%2520structure-rich%2520injection%2520module%252C%2520which%2520enables%2520the%250Amodel%2520to%2520better%2520adapt%2520to%2520the%2520evolving%2520interplay%2520between%2520alignment%2520and%2520structure%250Apreservation%2520throughout%2520the%2520diffusion%2520steps%252C%2520resulting%2520in%2520more%2520faithful%250Astructural%2520generation.%2520In%2520addition%252C%2520we%2520introduce%2520appearance-rich%2520prompting%2520and%250Aa%2520restart%2520refinement%2520strategy%2520to%2520further%2520enhance%2520appearance%2520control%2520and%2520visual%250Aquality.%2520Together%252C%2520these%2520designs%2520enable%2520training-free%2520generation%2520that%2520is%2520both%250Astructure-rich%2520and%2520appearance-rich.%2520Extensive%2520experiments%2520show%2520that%2520our%250Aapproach%2520achieves%2520state-of-the-art%2520performance%2520across%2520diverse%2520zero-shot%250Aconditioning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RichControl%3A%20Structure-%20and%20Appearance-Rich%20Training-Free%20Spatial%0A%20%20Control%20for%20Text-to-Image%20Generation&entry.906535625=Liheng%20Zhang%20and%20Lexi%20Pang%20and%20Hang%20Ye%20and%20Xiaoxuan%20Ma%20and%20Yizhou%20Wang&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20shown%20remarkable%20success%20in%0Agenerating%20high-quality%20images%20from%20text%20prompts.%20Recent%20efforts%20extend%20these%0Amodels%20to%20incorporate%20conditional%20images%20%28e.g.%2C%20depth%20or%20pose%20maps%29%20for%0Afine-grained%20spatial%20control.%20Among%20them%2C%20feature%20injection%20methods%20have%0Aemerged%20as%20a%20training-free%20alternative%20to%20traditional%20fine-tuning%20approaches.%0AHowever%2C%20they%20often%20suffer%20from%20structural%20misalignment%2C%20condition%20leakage%2C%20and%0Avisual%20artifacts%2C%20especially%20when%20the%20condition%20image%20diverges%20significantly%0Afrom%20natural%20RGB%20distributions.%20By%20revisiting%20existing%20methods%2C%20we%20identify%20a%0Acore%20limitation%3A%20the%20synchronous%20injection%20of%20condition%20features%20fails%20to%0Aaccount%20for%20the%20trade-off%20between%20domain%20alignment%20and%20structural%20preservation%0Aduring%20denoising.%20Inspired%20by%20this%20observation%2C%20we%20propose%20a%20flexible%20feature%0Ainjection%20framework%20that%20decouples%20the%20injection%20timestep%20from%20the%20denoising%0Aprocess.%20At%20its%20core%20is%20a%20structure-rich%20injection%20module%2C%20which%20enables%20the%0Amodel%20to%20better%20adapt%20to%20the%20evolving%20interplay%20between%20alignment%20and%20structure%0Apreservation%20throughout%20the%20diffusion%20steps%2C%20resulting%20in%20more%20faithful%0Astructural%20generation.%20In%20addition%2C%20we%20introduce%20appearance-rich%20prompting%20and%0Aa%20restart%20refinement%20strategy%20to%20further%20enhance%20appearance%20control%20and%20visual%0Aquality.%20Together%2C%20these%20designs%20enable%20training-free%20generation%20that%20is%20both%0Astructure-rich%20and%20appearance-rich.%20Extensive%20experiments%20show%20that%20our%0Aapproach%20achieves%20state-of-the-art%20performance%20across%20diverse%20zero-shot%0Aconditioning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02792v2&entry.124074799=Read"},
{"title": "Embedding Atlas: Low-Friction, Interactive Embedding Visualization", "author": "Donghao Ren and Fred Hohman and Halden Lin and Dominik Moritz", "abstract": "  Embedding projections are popular for visualizing large datasets and models.\nHowever, people often encounter \"friction\" when using embedding visualization\ntools: (1) barriers to adoption, e.g., tedious data wrangling and loading,\nscalability limits, no integration of results into existing workflows, and (2)\nlimitations in possible analyses, without integration with external tools to\nadditionally show coordinated views of metadata. In this paper, we present\nEmbedding Atlas, a scalable, interactive visualization tool designed to make\ninteracting with large embeddings as easy as possible. Embedding Atlas uses\nmodern web technologies and advanced algorithms -- including density-based\nclustering, and automated labeling -- to provide a fast and rich data analysis\nexperience at scale. We evaluate Embedding Atlas with a competitive analysis\nagainst other popular embedding tools, showing that Embedding Atlas's feature\nset specifically helps reduce friction, and report a benchmark on its real-time\nrendering performance with millions of points. Embedding Atlas is available as\nopen source to support future work in embedding-based analysis.\n", "link": "http://arxiv.org/abs/2505.06386v2", "date": "2025-07-08", "relevancy": 2.4583, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4926}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4926}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding%20Atlas%3A%20Low-Friction%2C%20Interactive%20Embedding%20Visualization&body=Title%3A%20Embedding%20Atlas%3A%20Low-Friction%2C%20Interactive%20Embedding%20Visualization%0AAuthor%3A%20Donghao%20Ren%20and%20Fred%20Hohman%20and%20Halden%20Lin%20and%20Dominik%20Moritz%0AAbstract%3A%20%20%20Embedding%20projections%20are%20popular%20for%20visualizing%20large%20datasets%20and%20models.%0AHowever%2C%20people%20often%20encounter%20%22friction%22%20when%20using%20embedding%20visualization%0Atools%3A%20%281%29%20barriers%20to%20adoption%2C%20e.g.%2C%20tedious%20data%20wrangling%20and%20loading%2C%0Ascalability%20limits%2C%20no%20integration%20of%20results%20into%20existing%20workflows%2C%20and%20%282%29%0Alimitations%20in%20possible%20analyses%2C%20without%20integration%20with%20external%20tools%20to%0Aadditionally%20show%20coordinated%20views%20of%20metadata.%20In%20this%20paper%2C%20we%20present%0AEmbedding%20Atlas%2C%20a%20scalable%2C%20interactive%20visualization%20tool%20designed%20to%20make%0Ainteracting%20with%20large%20embeddings%20as%20easy%20as%20possible.%20Embedding%20Atlas%20uses%0Amodern%20web%20technologies%20and%20advanced%20algorithms%20--%20including%20density-based%0Aclustering%2C%20and%20automated%20labeling%20--%20to%20provide%20a%20fast%20and%20rich%20data%20analysis%0Aexperience%20at%20scale.%20We%20evaluate%20Embedding%20Atlas%20with%20a%20competitive%20analysis%0Aagainst%20other%20popular%20embedding%20tools%2C%20showing%20that%20Embedding%20Atlas%27s%20feature%0Aset%20specifically%20helps%20reduce%20friction%2C%20and%20report%20a%20benchmark%20on%20its%20real-time%0Arendering%20performance%20with%20millions%20of%20points.%20Embedding%20Atlas%20is%20available%20as%0Aopen%20source%20to%20support%20future%20work%20in%20embedding-based%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06386v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding%2520Atlas%253A%2520Low-Friction%252C%2520Interactive%2520Embedding%2520Visualization%26entry.906535625%3DDonghao%2520Ren%2520and%2520Fred%2520Hohman%2520and%2520Halden%2520Lin%2520and%2520Dominik%2520Moritz%26entry.1292438233%3D%2520%2520Embedding%2520projections%2520are%2520popular%2520for%2520visualizing%2520large%2520datasets%2520and%2520models.%250AHowever%252C%2520people%2520often%2520encounter%2520%2522friction%2522%2520when%2520using%2520embedding%2520visualization%250Atools%253A%2520%25281%2529%2520barriers%2520to%2520adoption%252C%2520e.g.%252C%2520tedious%2520data%2520wrangling%2520and%2520loading%252C%250Ascalability%2520limits%252C%2520no%2520integration%2520of%2520results%2520into%2520existing%2520workflows%252C%2520and%2520%25282%2529%250Alimitations%2520in%2520possible%2520analyses%252C%2520without%2520integration%2520with%2520external%2520tools%2520to%250Aadditionally%2520show%2520coordinated%2520views%2520of%2520metadata.%2520In%2520this%2520paper%252C%2520we%2520present%250AEmbedding%2520Atlas%252C%2520a%2520scalable%252C%2520interactive%2520visualization%2520tool%2520designed%2520to%2520make%250Ainteracting%2520with%2520large%2520embeddings%2520as%2520easy%2520as%2520possible.%2520Embedding%2520Atlas%2520uses%250Amodern%2520web%2520technologies%2520and%2520advanced%2520algorithms%2520--%2520including%2520density-based%250Aclustering%252C%2520and%2520automated%2520labeling%2520--%2520to%2520provide%2520a%2520fast%2520and%2520rich%2520data%2520analysis%250Aexperience%2520at%2520scale.%2520We%2520evaluate%2520Embedding%2520Atlas%2520with%2520a%2520competitive%2520analysis%250Aagainst%2520other%2520popular%2520embedding%2520tools%252C%2520showing%2520that%2520Embedding%2520Atlas%2527s%2520feature%250Aset%2520specifically%2520helps%2520reduce%2520friction%252C%2520and%2520report%2520a%2520benchmark%2520on%2520its%2520real-time%250Arendering%2520performance%2520with%2520millions%2520of%2520points.%2520Embedding%2520Atlas%2520is%2520available%2520as%250Aopen%2520source%2520to%2520support%2520future%2520work%2520in%2520embedding-based%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06386v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20Atlas%3A%20Low-Friction%2C%20Interactive%20Embedding%20Visualization&entry.906535625=Donghao%20Ren%20and%20Fred%20Hohman%20and%20Halden%20Lin%20and%20Dominik%20Moritz&entry.1292438233=%20%20Embedding%20projections%20are%20popular%20for%20visualizing%20large%20datasets%20and%20models.%0AHowever%2C%20people%20often%20encounter%20%22friction%22%20when%20using%20embedding%20visualization%0Atools%3A%20%281%29%20barriers%20to%20adoption%2C%20e.g.%2C%20tedious%20data%20wrangling%20and%20loading%2C%0Ascalability%20limits%2C%20no%20integration%20of%20results%20into%20existing%20workflows%2C%20and%20%282%29%0Alimitations%20in%20possible%20analyses%2C%20without%20integration%20with%20external%20tools%20to%0Aadditionally%20show%20coordinated%20views%20of%20metadata.%20In%20this%20paper%2C%20we%20present%0AEmbedding%20Atlas%2C%20a%20scalable%2C%20interactive%20visualization%20tool%20designed%20to%20make%0Ainteracting%20with%20large%20embeddings%20as%20easy%20as%20possible.%20Embedding%20Atlas%20uses%0Amodern%20web%20technologies%20and%20advanced%20algorithms%20--%20including%20density-based%0Aclustering%2C%20and%20automated%20labeling%20--%20to%20provide%20a%20fast%20and%20rich%20data%20analysis%0Aexperience%20at%20scale.%20We%20evaluate%20Embedding%20Atlas%20with%20a%20competitive%20analysis%0Aagainst%20other%20popular%20embedding%20tools%2C%20showing%20that%20Embedding%20Atlas%27s%20feature%0Aset%20specifically%20helps%20reduce%20friction%2C%20and%20report%20a%20benchmark%20on%20its%20real-time%0Arendering%20performance%20with%20millions%20of%20points.%20Embedding%20Atlas%20is%20available%20as%0Aopen%20source%20to%20support%20future%20work%20in%20embedding-based%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06386v2&entry.124074799=Read"},
{"title": "The GenAI Generation: Student Views of Awareness, Preparedness, and\n  Concern", "author": "Micaela Siraj and Jon Duke and Thomas Pl\u00f6tz", "abstract": "  Generative Artificial Intelligence (GenAI) is revolutionizing education and\nworkforce development, profoundly shaping how students learn, engage, and\nprepare for their future. Outpacing the development of uniform policies and\nstructures, GenAI has heralded a unique era and given rise to the GenAI\nGeneration. We define the GenAI Generation as a cohort of students whose\neducation has been increasingly shaped by the opportunities and challenges\nGenAI presents during its widespread adoption within society. This study\nexamines students' perceptions of GenAI through a concise survey with optional\nopen-ended questions, focusing on their awareness, preparedness, and concerns.\nNotably, readiness appears increasingly tied to exposure to GenAI through one's\ncoursework. Students with greater curricular exposure to GenAI tend to feel\nmore prepared, while those without it more often express vulnerability and\nuncertainty, highlighting a new and growing divide in readiness that goes\nbeyond traditional disciplinary boundaries. Evaluation of more than 250\nresponses, with over 40% providing detailed qualitative feedback, reveals a\ncore dual sentiment: while most students express enthusiasm for GenAI, an even\ngreater proportion voice a spectrum of concerns about ethics, job displacement,\nand the adequacy of educational structures given the highly transformative\ntechnology. These findings offer critical insights into how students view the\npotential and pitfalls of GenAI for future career impacts. The challenge ahead\ninvolves implementing associated recommendations for educational institutions,\nmoving beyond the baseline of access toward more informed guidance on the use\nof these tools, while preserving critical thinking, ethical reasoning, and\nadaptive learning.\n", "link": "http://arxiv.org/abs/2505.02230v2", "date": "2025-07-08", "relevancy": 2.4474, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5384}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4768}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20GenAI%20Generation%3A%20Student%20Views%20of%20Awareness%2C%20Preparedness%2C%20and%0A%20%20Concern&body=Title%3A%20The%20GenAI%20Generation%3A%20Student%20Views%20of%20Awareness%2C%20Preparedness%2C%20and%0A%20%20Concern%0AAuthor%3A%20Micaela%20Siraj%20and%20Jon%20Duke%20and%20Thomas%20Pl%C3%B6tz%0AAbstract%3A%20%20%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20is%20revolutionizing%20education%20and%0Aworkforce%20development%2C%20profoundly%20shaping%20how%20students%20learn%2C%20engage%2C%20and%0Aprepare%20for%20their%20future.%20Outpacing%20the%20development%20of%20uniform%20policies%20and%0Astructures%2C%20GenAI%20has%20heralded%20a%20unique%20era%20and%20given%20rise%20to%20the%20GenAI%0AGeneration.%20We%20define%20the%20GenAI%20Generation%20as%20a%20cohort%20of%20students%20whose%0Aeducation%20has%20been%20increasingly%20shaped%20by%20the%20opportunities%20and%20challenges%0AGenAI%20presents%20during%20its%20widespread%20adoption%20within%20society.%20This%20study%0Aexamines%20students%27%20perceptions%20of%20GenAI%20through%20a%20concise%20survey%20with%20optional%0Aopen-ended%20questions%2C%20focusing%20on%20their%20awareness%2C%20preparedness%2C%20and%20concerns.%0ANotably%2C%20readiness%20appears%20increasingly%20tied%20to%20exposure%20to%20GenAI%20through%20one%27s%0Acoursework.%20Students%20with%20greater%20curricular%20exposure%20to%20GenAI%20tend%20to%20feel%0Amore%20prepared%2C%20while%20those%20without%20it%20more%20often%20express%20vulnerability%20and%0Auncertainty%2C%20highlighting%20a%20new%20and%20growing%20divide%20in%20readiness%20that%20goes%0Abeyond%20traditional%20disciplinary%20boundaries.%20Evaluation%20of%20more%20than%20250%0Aresponses%2C%20with%20over%2040%25%20providing%20detailed%20qualitative%20feedback%2C%20reveals%20a%0Acore%20dual%20sentiment%3A%20while%20most%20students%20express%20enthusiasm%20for%20GenAI%2C%20an%20even%0Agreater%20proportion%20voice%20a%20spectrum%20of%20concerns%20about%20ethics%2C%20job%20displacement%2C%0Aand%20the%20adequacy%20of%20educational%20structures%20given%20the%20highly%20transformative%0Atechnology.%20These%20findings%20offer%20critical%20insights%20into%20how%20students%20view%20the%0Apotential%20and%20pitfalls%20of%20GenAI%20for%20future%20career%20impacts.%20The%20challenge%20ahead%0Ainvolves%20implementing%20associated%20recommendations%20for%20educational%20institutions%2C%0Amoving%20beyond%20the%20baseline%20of%20access%20toward%20more%20informed%20guidance%20on%20the%20use%0Aof%20these%20tools%2C%20while%20preserving%20critical%20thinking%2C%20ethical%20reasoning%2C%20and%0Aadaptive%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02230v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520GenAI%2520Generation%253A%2520Student%2520Views%2520of%2520Awareness%252C%2520Preparedness%252C%2520and%250A%2520%2520Concern%26entry.906535625%3DMicaela%2520Siraj%2520and%2520Jon%2520Duke%2520and%2520Thomas%2520Pl%25C3%25B6tz%26entry.1292438233%3D%2520%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520is%2520revolutionizing%2520education%2520and%250Aworkforce%2520development%252C%2520profoundly%2520shaping%2520how%2520students%2520learn%252C%2520engage%252C%2520and%250Aprepare%2520for%2520their%2520future.%2520Outpacing%2520the%2520development%2520of%2520uniform%2520policies%2520and%250Astructures%252C%2520GenAI%2520has%2520heralded%2520a%2520unique%2520era%2520and%2520given%2520rise%2520to%2520the%2520GenAI%250AGeneration.%2520We%2520define%2520the%2520GenAI%2520Generation%2520as%2520a%2520cohort%2520of%2520students%2520whose%250Aeducation%2520has%2520been%2520increasingly%2520shaped%2520by%2520the%2520opportunities%2520and%2520challenges%250AGenAI%2520presents%2520during%2520its%2520widespread%2520adoption%2520within%2520society.%2520This%2520study%250Aexamines%2520students%2527%2520perceptions%2520of%2520GenAI%2520through%2520a%2520concise%2520survey%2520with%2520optional%250Aopen-ended%2520questions%252C%2520focusing%2520on%2520their%2520awareness%252C%2520preparedness%252C%2520and%2520concerns.%250ANotably%252C%2520readiness%2520appears%2520increasingly%2520tied%2520to%2520exposure%2520to%2520GenAI%2520through%2520one%2527s%250Acoursework.%2520Students%2520with%2520greater%2520curricular%2520exposure%2520to%2520GenAI%2520tend%2520to%2520feel%250Amore%2520prepared%252C%2520while%2520those%2520without%2520it%2520more%2520often%2520express%2520vulnerability%2520and%250Auncertainty%252C%2520highlighting%2520a%2520new%2520and%2520growing%2520divide%2520in%2520readiness%2520that%2520goes%250Abeyond%2520traditional%2520disciplinary%2520boundaries.%2520Evaluation%2520of%2520more%2520than%2520250%250Aresponses%252C%2520with%2520over%252040%2525%2520providing%2520detailed%2520qualitative%2520feedback%252C%2520reveals%2520a%250Acore%2520dual%2520sentiment%253A%2520while%2520most%2520students%2520express%2520enthusiasm%2520for%2520GenAI%252C%2520an%2520even%250Agreater%2520proportion%2520voice%2520a%2520spectrum%2520of%2520concerns%2520about%2520ethics%252C%2520job%2520displacement%252C%250Aand%2520the%2520adequacy%2520of%2520educational%2520structures%2520given%2520the%2520highly%2520transformative%250Atechnology.%2520These%2520findings%2520offer%2520critical%2520insights%2520into%2520how%2520students%2520view%2520the%250Apotential%2520and%2520pitfalls%2520of%2520GenAI%2520for%2520future%2520career%2520impacts.%2520The%2520challenge%2520ahead%250Ainvolves%2520implementing%2520associated%2520recommendations%2520for%2520educational%2520institutions%252C%250Amoving%2520beyond%2520the%2520baseline%2520of%2520access%2520toward%2520more%2520informed%2520guidance%2520on%2520the%2520use%250Aof%2520these%2520tools%252C%2520while%2520preserving%2520critical%2520thinking%252C%2520ethical%2520reasoning%252C%2520and%250Aadaptive%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02230v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20GenAI%20Generation%3A%20Student%20Views%20of%20Awareness%2C%20Preparedness%2C%20and%0A%20%20Concern&entry.906535625=Micaela%20Siraj%20and%20Jon%20Duke%20and%20Thomas%20Pl%C3%B6tz&entry.1292438233=%20%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20is%20revolutionizing%20education%20and%0Aworkforce%20development%2C%20profoundly%20shaping%20how%20students%20learn%2C%20engage%2C%20and%0Aprepare%20for%20their%20future.%20Outpacing%20the%20development%20of%20uniform%20policies%20and%0Astructures%2C%20GenAI%20has%20heralded%20a%20unique%20era%20and%20given%20rise%20to%20the%20GenAI%0AGeneration.%20We%20define%20the%20GenAI%20Generation%20as%20a%20cohort%20of%20students%20whose%0Aeducation%20has%20been%20increasingly%20shaped%20by%20the%20opportunities%20and%20challenges%0AGenAI%20presents%20during%20its%20widespread%20adoption%20within%20society.%20This%20study%0Aexamines%20students%27%20perceptions%20of%20GenAI%20through%20a%20concise%20survey%20with%20optional%0Aopen-ended%20questions%2C%20focusing%20on%20their%20awareness%2C%20preparedness%2C%20and%20concerns.%0ANotably%2C%20readiness%20appears%20increasingly%20tied%20to%20exposure%20to%20GenAI%20through%20one%27s%0Acoursework.%20Students%20with%20greater%20curricular%20exposure%20to%20GenAI%20tend%20to%20feel%0Amore%20prepared%2C%20while%20those%20without%20it%20more%20often%20express%20vulnerability%20and%0Auncertainty%2C%20highlighting%20a%20new%20and%20growing%20divide%20in%20readiness%20that%20goes%0Abeyond%20traditional%20disciplinary%20boundaries.%20Evaluation%20of%20more%20than%20250%0Aresponses%2C%20with%20over%2040%25%20providing%20detailed%20qualitative%20feedback%2C%20reveals%20a%0Acore%20dual%20sentiment%3A%20while%20most%20students%20express%20enthusiasm%20for%20GenAI%2C%20an%20even%0Agreater%20proportion%20voice%20a%20spectrum%20of%20concerns%20about%20ethics%2C%20job%20displacement%2C%0Aand%20the%20adequacy%20of%20educational%20structures%20given%20the%20highly%20transformative%0Atechnology.%20These%20findings%20offer%20critical%20insights%20into%20how%20students%20view%20the%0Apotential%20and%20pitfalls%20of%20GenAI%20for%20future%20career%20impacts.%20The%20challenge%20ahead%0Ainvolves%20implementing%20associated%20recommendations%20for%20educational%20institutions%2C%0Amoving%20beyond%20the%20baseline%20of%20access%20toward%20more%20informed%20guidance%20on%20the%20use%0Aof%20these%20tools%2C%20while%20preserving%20critical%20thinking%2C%20ethical%20reasoning%2C%20and%0Aadaptive%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02230v2&entry.124074799=Read"},
{"title": "EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG\n  Alignment via Large Language Model and Contrastive Learning on ChineseEEG", "author": "Jacky Tai-Yu Lu and Jung Chiang and Chi-Sheng Chen and Anna Nai-Yun Tung and Hsiang Wei Hu and Yuan Chiao Cheng", "abstract": "  We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese.\n", "link": "http://arxiv.org/abs/2506.00854v3", "date": "2025-07-08", "relevancy": 2.4328, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4958}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4958}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG2TEXT-CN%3A%20An%20Exploratory%20Study%20of%20Open-Vocabulary%20Chinese%20Text-EEG%0A%20%20Alignment%20via%20Large%20Language%20Model%20and%20Contrastive%20Learning%20on%20ChineseEEG&body=Title%3A%20EEG2TEXT-CN%3A%20An%20Exploratory%20Study%20of%20Open-Vocabulary%20Chinese%20Text-EEG%0A%20%20Alignment%20via%20Large%20Language%20Model%20and%20Contrastive%20Learning%20on%20ChineseEEG%0AAuthor%3A%20Jacky%20Tai-Yu%20Lu%20and%20Jung%20Chiang%20and%20Chi-Sheng%20Chen%20and%20Anna%20Nai-Yun%20Tung%20and%20Hsiang%20Wei%20Hu%20and%20Yuan%20Chiao%20Cheng%0AAbstract%3A%20%20%20We%20propose%20EEG2TEXT-CN%2C%20which%2C%20to%20the%20best%20of%20our%20knowledge%2C%20represents%20one%0Aof%20the%20earliest%20open-vocabulary%20EEG-to-text%20generation%20frameworks%20tailored%20for%0AChinese.%20Built%20on%20a%20biologically%20grounded%20EEG%20encoder%20%28NICE-EEG%29%20and%20a%20compact%0Apretrained%20language%20model%20%28MiniLM%29%2C%20our%20architecture%20aligns%20multichannel%20brain%0Asignals%20with%20natural%20language%20representations%20via%20masked%20pretraining%20and%0Acontrastive%20learning.%20Using%20a%20subset%20of%20the%20ChineseEEG%20dataset%2C%20where%20each%0Asentence%20contains%20approximately%20ten%20Chinese%20characters%20aligned%20with%20128-channel%0AEEG%20recorded%20at%20256%20Hz%2C%20we%20segment%20EEG%20into%20per-character%20embeddings%20and%0Apredict%20full%20sentences%20in%20a%20zero-shot%20setting.%20The%20decoder%20is%20trained%20with%0Ateacher%20forcing%20and%20padding%20masks%20to%20accommodate%20variable-length%20sequences.%0AEvaluation%20on%20over%201%2C500%20training-validation%20sentences%20and%20300%20held-out%20test%0Asamples%20shows%20promising%20lexical%20alignment%2C%20with%20a%20best%20BLEU-1%20score%20of%206.38%5C%25.%0AWhile%20syntactic%20fluency%20remains%20a%20challenge%2C%20our%20findings%20demonstrate%20the%0Afeasibility%20of%20non-phonetic%2C%20cross-modal%20language%20decoding%20from%20EEG.%20This%20work%0Aopens%20a%20new%20direction%20in%20multilingual%20brain-to-text%20research%20and%20lays%20the%0Afoundation%20for%20future%20cognitive-language%20interfaces%20in%20Chinese.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00854v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG2TEXT-CN%253A%2520An%2520Exploratory%2520Study%2520of%2520Open-Vocabulary%2520Chinese%2520Text-EEG%250A%2520%2520Alignment%2520via%2520Large%2520Language%2520Model%2520and%2520Contrastive%2520Learning%2520on%2520ChineseEEG%26entry.906535625%3DJacky%2520Tai-Yu%2520Lu%2520and%2520Jung%2520Chiang%2520and%2520Chi-Sheng%2520Chen%2520and%2520Anna%2520Nai-Yun%2520Tung%2520and%2520Hsiang%2520Wei%2520Hu%2520and%2520Yuan%2520Chiao%2520Cheng%26entry.1292438233%3D%2520%2520We%2520propose%2520EEG2TEXT-CN%252C%2520which%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520represents%2520one%250Aof%2520the%2520earliest%2520open-vocabulary%2520EEG-to-text%2520generation%2520frameworks%2520tailored%2520for%250AChinese.%2520Built%2520on%2520a%2520biologically%2520grounded%2520EEG%2520encoder%2520%2528NICE-EEG%2529%2520and%2520a%2520compact%250Apretrained%2520language%2520model%2520%2528MiniLM%2529%252C%2520our%2520architecture%2520aligns%2520multichannel%2520brain%250Asignals%2520with%2520natural%2520language%2520representations%2520via%2520masked%2520pretraining%2520and%250Acontrastive%2520learning.%2520Using%2520a%2520subset%2520of%2520the%2520ChineseEEG%2520dataset%252C%2520where%2520each%250Asentence%2520contains%2520approximately%2520ten%2520Chinese%2520characters%2520aligned%2520with%2520128-channel%250AEEG%2520recorded%2520at%2520256%2520Hz%252C%2520we%2520segment%2520EEG%2520into%2520per-character%2520embeddings%2520and%250Apredict%2520full%2520sentences%2520in%2520a%2520zero-shot%2520setting.%2520The%2520decoder%2520is%2520trained%2520with%250Ateacher%2520forcing%2520and%2520padding%2520masks%2520to%2520accommodate%2520variable-length%2520sequences.%250AEvaluation%2520on%2520over%25201%252C500%2520training-validation%2520sentences%2520and%2520300%2520held-out%2520test%250Asamples%2520shows%2520promising%2520lexical%2520alignment%252C%2520with%2520a%2520best%2520BLEU-1%2520score%2520of%25206.38%255C%2525.%250AWhile%2520syntactic%2520fluency%2520remains%2520a%2520challenge%252C%2520our%2520findings%2520demonstrate%2520the%250Afeasibility%2520of%2520non-phonetic%252C%2520cross-modal%2520language%2520decoding%2520from%2520EEG.%2520This%2520work%250Aopens%2520a%2520new%2520direction%2520in%2520multilingual%2520brain-to-text%2520research%2520and%2520lays%2520the%250Afoundation%2520for%2520future%2520cognitive-language%2520interfaces%2520in%2520Chinese.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00854v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG2TEXT-CN%3A%20An%20Exploratory%20Study%20of%20Open-Vocabulary%20Chinese%20Text-EEG%0A%20%20Alignment%20via%20Large%20Language%20Model%20and%20Contrastive%20Learning%20on%20ChineseEEG&entry.906535625=Jacky%20Tai-Yu%20Lu%20and%20Jung%20Chiang%20and%20Chi-Sheng%20Chen%20and%20Anna%20Nai-Yun%20Tung%20and%20Hsiang%20Wei%20Hu%20and%20Yuan%20Chiao%20Cheng&entry.1292438233=%20%20We%20propose%20EEG2TEXT-CN%2C%20which%2C%20to%20the%20best%20of%20our%20knowledge%2C%20represents%20one%0Aof%20the%20earliest%20open-vocabulary%20EEG-to-text%20generation%20frameworks%20tailored%20for%0AChinese.%20Built%20on%20a%20biologically%20grounded%20EEG%20encoder%20%28NICE-EEG%29%20and%20a%20compact%0Apretrained%20language%20model%20%28MiniLM%29%2C%20our%20architecture%20aligns%20multichannel%20brain%0Asignals%20with%20natural%20language%20representations%20via%20masked%20pretraining%20and%0Acontrastive%20learning.%20Using%20a%20subset%20of%20the%20ChineseEEG%20dataset%2C%20where%20each%0Asentence%20contains%20approximately%20ten%20Chinese%20characters%20aligned%20with%20128-channel%0AEEG%20recorded%20at%20256%20Hz%2C%20we%20segment%20EEG%20into%20per-character%20embeddings%20and%0Apredict%20full%20sentences%20in%20a%20zero-shot%20setting.%20The%20decoder%20is%20trained%20with%0Ateacher%20forcing%20and%20padding%20masks%20to%20accommodate%20variable-length%20sequences.%0AEvaluation%20on%20over%201%2C500%20training-validation%20sentences%20and%20300%20held-out%20test%0Asamples%20shows%20promising%20lexical%20alignment%2C%20with%20a%20best%20BLEU-1%20score%20of%206.38%5C%25.%0AWhile%20syntactic%20fluency%20remains%20a%20challenge%2C%20our%20findings%20demonstrate%20the%0Afeasibility%20of%20non-phonetic%2C%20cross-modal%20language%20decoding%20from%20EEG.%20This%20work%0Aopens%20a%20new%20direction%20in%20multilingual%20brain-to-text%20research%20and%20lays%20the%0Afoundation%20for%20future%20cognitive-language%20interfaces%20in%20Chinese.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00854v3&entry.124074799=Read"},
{"title": "Visual Imitation Enables Contextual Humanoid Control", "author": "Arthur Allshire and Hongsuk Choi and Junyi Zhang and David McAllister and Anthony Zhang and Chung Min Kim and Trevor Darrell and Pieter Abbeel and Jitendra Malik and Angjoo Kanazawa", "abstract": "  How can we teach humanoids to climb staircases and sit on chairs using the\nsurrounding environment context? Arguably, the simplest way is to just show\nthem-casually capture a human motion video and feed it to humanoids. We\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\nvideos, jointly reconstructs the humans and the environment, and produces\nwhole-body control policies for humanoid robots that perform the corresponding\nskills. We demonstrate the results of our pipeline on real humanoid robots,\nshowing robust, repeatable contextual control such as staircase ascents and\ndescents, sitting and standing from chairs and benches, as well as other\ndynamic whole-body skills-all from a single policy, conditioned on the\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\nteaching humanoids to operate in diverse real-world environments.\n", "link": "http://arxiv.org/abs/2505.03729v4", "date": "2025-07-08", "relevancy": 2.4311, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6172}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6154}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Imitation%20Enables%20Contextual%20Humanoid%20Control&body=Title%3A%20Visual%20Imitation%20Enables%20Contextual%20Humanoid%20Control%0AAuthor%3A%20Arthur%20Allshire%20and%20Hongsuk%20Choi%20and%20Junyi%20Zhang%20and%20David%20McAllister%20and%20Anthony%20Zhang%20and%20Chung%20Min%20Kim%20and%20Trevor%20Darrell%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20How%20can%20we%20teach%20humanoids%20to%20climb%20staircases%20and%20sit%20on%20chairs%20using%20the%0Asurrounding%20environment%20context%3F%20Arguably%2C%20the%20simplest%20way%20is%20to%20just%20show%0Athem-casually%20capture%20a%20human%20motion%20video%20and%20feed%20it%20to%20humanoids.%20We%0Aintroduce%20VIDEOMIMIC%2C%20a%20real-to-sim-to-real%20pipeline%20that%20mines%20everyday%0Avideos%2C%20jointly%20reconstructs%20the%20humans%20and%20the%20environment%2C%20and%20produces%0Awhole-body%20control%20policies%20for%20humanoid%20robots%20that%20perform%20the%20corresponding%0Askills.%20We%20demonstrate%20the%20results%20of%20our%20pipeline%20on%20real%20humanoid%20robots%2C%0Ashowing%20robust%2C%20repeatable%20contextual%20control%20such%20as%20staircase%20ascents%20and%0Adescents%2C%20sitting%20and%20standing%20from%20chairs%20and%20benches%2C%20as%20well%20as%20other%0Adynamic%20whole-body%20skills-all%20from%20a%20single%20policy%2C%20conditioned%20on%20the%0Aenvironment%20and%20global%20root%20commands.%20VIDEOMIMIC%20offers%20a%20scalable%20path%20towards%0Ateaching%20humanoids%20to%20operate%20in%20diverse%20real-world%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03729v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Imitation%2520Enables%2520Contextual%2520Humanoid%2520Control%26entry.906535625%3DArthur%2520Allshire%2520and%2520Hongsuk%2520Choi%2520and%2520Junyi%2520Zhang%2520and%2520David%2520McAllister%2520and%2520Anthony%2520Zhang%2520and%2520Chung%2520Min%2520Kim%2520and%2520Trevor%2520Darrell%2520and%2520Pieter%2520Abbeel%2520and%2520Jitendra%2520Malik%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520How%2520can%2520we%2520teach%2520humanoids%2520to%2520climb%2520staircases%2520and%2520sit%2520on%2520chairs%2520using%2520the%250Asurrounding%2520environment%2520context%253F%2520Arguably%252C%2520the%2520simplest%2520way%2520is%2520to%2520just%2520show%250Athem-casually%2520capture%2520a%2520human%2520motion%2520video%2520and%2520feed%2520it%2520to%2520humanoids.%2520We%250Aintroduce%2520VIDEOMIMIC%252C%2520a%2520real-to-sim-to-real%2520pipeline%2520that%2520mines%2520everyday%250Avideos%252C%2520jointly%2520reconstructs%2520the%2520humans%2520and%2520the%2520environment%252C%2520and%2520produces%250Awhole-body%2520control%2520policies%2520for%2520humanoid%2520robots%2520that%2520perform%2520the%2520corresponding%250Askills.%2520We%2520demonstrate%2520the%2520results%2520of%2520our%2520pipeline%2520on%2520real%2520humanoid%2520robots%252C%250Ashowing%2520robust%252C%2520repeatable%2520contextual%2520control%2520such%2520as%2520staircase%2520ascents%2520and%250Adescents%252C%2520sitting%2520and%2520standing%2520from%2520chairs%2520and%2520benches%252C%2520as%2520well%2520as%2520other%250Adynamic%2520whole-body%2520skills-all%2520from%2520a%2520single%2520policy%252C%2520conditioned%2520on%2520the%250Aenvironment%2520and%2520global%2520root%2520commands.%2520VIDEOMIMIC%2520offers%2520a%2520scalable%2520path%2520towards%250Ateaching%2520humanoids%2520to%2520operate%2520in%2520diverse%2520real-world%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03729v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Imitation%20Enables%20Contextual%20Humanoid%20Control&entry.906535625=Arthur%20Allshire%20and%20Hongsuk%20Choi%20and%20Junyi%20Zhang%20and%20David%20McAllister%20and%20Anthony%20Zhang%20and%20Chung%20Min%20Kim%20and%20Trevor%20Darrell%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20How%20can%20we%20teach%20humanoids%20to%20climb%20staircases%20and%20sit%20on%20chairs%20using%20the%0Asurrounding%20environment%20context%3F%20Arguably%2C%20the%20simplest%20way%20is%20to%20just%20show%0Athem-casually%20capture%20a%20human%20motion%20video%20and%20feed%20it%20to%20humanoids.%20We%0Aintroduce%20VIDEOMIMIC%2C%20a%20real-to-sim-to-real%20pipeline%20that%20mines%20everyday%0Avideos%2C%20jointly%20reconstructs%20the%20humans%20and%20the%20environment%2C%20and%20produces%0Awhole-body%20control%20policies%20for%20humanoid%20robots%20that%20perform%20the%20corresponding%0Askills.%20We%20demonstrate%20the%20results%20of%20our%20pipeline%20on%20real%20humanoid%20robots%2C%0Ashowing%20robust%2C%20repeatable%20contextual%20control%20such%20as%20staircase%20ascents%20and%0Adescents%2C%20sitting%20and%20standing%20from%20chairs%20and%20benches%2C%20as%20well%20as%20other%0Adynamic%20whole-body%20skills-all%20from%20a%20single%20policy%2C%20conditioned%20on%20the%0Aenvironment%20and%20global%20root%20commands.%20VIDEOMIMIC%20offers%20a%20scalable%20path%20towards%0Ateaching%20humanoids%20to%20operate%20in%20diverse%20real-world%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03729v4&entry.124074799=Read"},
{"title": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion", "author": "Aleksandar Jevti\u0107 and Christoph Reich and Felix Wimbauer and Oliver Hahn and Christian Rupprecht and Stefan Roth and Daniel Cremers", "abstract": "  Semantic scene completion (SSC) aims to infer both the 3D geometry and\nsemantics of a scene from single images. In contrast to prior work on SSC that\nheavily relies on expensive ground-truth annotations, we approach SSC in an\nunsupervised setting. Our novel method, SceneDINO, adapts techniques from\nself-supervised representation learning and 2D unsupervised scene understanding\nto SSC. Our training exclusively utilizes multi-view consistency\nself-supervision without any form of semantic or geometric ground truth. Given\na single input image, SceneDINO infers the 3D geometry and expressive 3D DINO\nfeatures in a feed-forward manner. Through a novel 3D feature distillation\napproach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised\nscene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.\nLinear probing our 3D features matches the segmentation accuracy of a current\nsupervised SSC approach. Additionally, we showcase the domain generalization\nand multi-view consistency of SceneDINO, taking the first steps towards a\nstrong foundation for single image 3D scene understanding.\n", "link": "http://arxiv.org/abs/2507.06230v1", "date": "2025-07-08", "relevancy": 2.4241, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6108}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feed-Forward%20SceneDINO%20for%20Unsupervised%20Semantic%20Scene%20Completion&body=Title%3A%20Feed-Forward%20SceneDINO%20for%20Unsupervised%20Semantic%20Scene%20Completion%0AAuthor%3A%20Aleksandar%20Jevti%C4%87%20and%20Christoph%20Reich%20and%20Felix%20Wimbauer%20and%20Oliver%20Hahn%20and%20Christian%20Rupprecht%20and%20Stefan%20Roth%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Semantic%20scene%20completion%20%28SSC%29%20aims%20to%20infer%20both%20the%203D%20geometry%20and%0Asemantics%20of%20a%20scene%20from%20single%20images.%20In%20contrast%20to%20prior%20work%20on%20SSC%20that%0Aheavily%20relies%20on%20expensive%20ground-truth%20annotations%2C%20we%20approach%20SSC%20in%20an%0Aunsupervised%20setting.%20Our%20novel%20method%2C%20SceneDINO%2C%20adapts%20techniques%20from%0Aself-supervised%20representation%20learning%20and%202D%20unsupervised%20scene%20understanding%0Ato%20SSC.%20Our%20training%20exclusively%20utilizes%20multi-view%20consistency%0Aself-supervision%20without%20any%20form%20of%20semantic%20or%20geometric%20ground%20truth.%20Given%0Aa%20single%20input%20image%2C%20SceneDINO%20infers%20the%203D%20geometry%20and%20expressive%203D%20DINO%0Afeatures%20in%20a%20feed-forward%20manner.%20Through%20a%20novel%203D%20feature%20distillation%0Aapproach%2C%20we%20obtain%20unsupervised%203D%20semantics.%20In%20both%203D%20and%202D%20unsupervised%0Ascene%20understanding%2C%20SceneDINO%20reaches%20state-of-the-art%20segmentation%20accuracy.%0ALinear%20probing%20our%203D%20features%20matches%20the%20segmentation%20accuracy%20of%20a%20current%0Asupervised%20SSC%20approach.%20Additionally%2C%20we%20showcase%20the%20domain%20generalization%0Aand%20multi-view%20consistency%20of%20SceneDINO%2C%20taking%20the%20first%20steps%20towards%20a%0Astrong%20foundation%20for%20single%20image%203D%20scene%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeed-Forward%2520SceneDINO%2520for%2520Unsupervised%2520Semantic%2520Scene%2520Completion%26entry.906535625%3DAleksandar%2520Jevti%25C4%2587%2520and%2520Christoph%2520Reich%2520and%2520Felix%2520Wimbauer%2520and%2520Oliver%2520Hahn%2520and%2520Christian%2520Rupprecht%2520and%2520Stefan%2520Roth%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Semantic%2520scene%2520completion%2520%2528SSC%2529%2520aims%2520to%2520infer%2520both%2520the%25203D%2520geometry%2520and%250Asemantics%2520of%2520a%2520scene%2520from%2520single%2520images.%2520In%2520contrast%2520to%2520prior%2520work%2520on%2520SSC%2520that%250Aheavily%2520relies%2520on%2520expensive%2520ground-truth%2520annotations%252C%2520we%2520approach%2520SSC%2520in%2520an%250Aunsupervised%2520setting.%2520Our%2520novel%2520method%252C%2520SceneDINO%252C%2520adapts%2520techniques%2520from%250Aself-supervised%2520representation%2520learning%2520and%25202D%2520unsupervised%2520scene%2520understanding%250Ato%2520SSC.%2520Our%2520training%2520exclusively%2520utilizes%2520multi-view%2520consistency%250Aself-supervision%2520without%2520any%2520form%2520of%2520semantic%2520or%2520geometric%2520ground%2520truth.%2520Given%250Aa%2520single%2520input%2520image%252C%2520SceneDINO%2520infers%2520the%25203D%2520geometry%2520and%2520expressive%25203D%2520DINO%250Afeatures%2520in%2520a%2520feed-forward%2520manner.%2520Through%2520a%2520novel%25203D%2520feature%2520distillation%250Aapproach%252C%2520we%2520obtain%2520unsupervised%25203D%2520semantics.%2520In%2520both%25203D%2520and%25202D%2520unsupervised%250Ascene%2520understanding%252C%2520SceneDINO%2520reaches%2520state-of-the-art%2520segmentation%2520accuracy.%250ALinear%2520probing%2520our%25203D%2520features%2520matches%2520the%2520segmentation%2520accuracy%2520of%2520a%2520current%250Asupervised%2520SSC%2520approach.%2520Additionally%252C%2520we%2520showcase%2520the%2520domain%2520generalization%250Aand%2520multi-view%2520consistency%2520of%2520SceneDINO%252C%2520taking%2520the%2520first%2520steps%2520towards%2520a%250Astrong%2520foundation%2520for%2520single%2520image%25203D%2520scene%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feed-Forward%20SceneDINO%20for%20Unsupervised%20Semantic%20Scene%20Completion&entry.906535625=Aleksandar%20Jevti%C4%87%20and%20Christoph%20Reich%20and%20Felix%20Wimbauer%20and%20Oliver%20Hahn%20and%20Christian%20Rupprecht%20and%20Stefan%20Roth%20and%20Daniel%20Cremers&entry.1292438233=%20%20Semantic%20scene%20completion%20%28SSC%29%20aims%20to%20infer%20both%20the%203D%20geometry%20and%0Asemantics%20of%20a%20scene%20from%20single%20images.%20In%20contrast%20to%20prior%20work%20on%20SSC%20that%0Aheavily%20relies%20on%20expensive%20ground-truth%20annotations%2C%20we%20approach%20SSC%20in%20an%0Aunsupervised%20setting.%20Our%20novel%20method%2C%20SceneDINO%2C%20adapts%20techniques%20from%0Aself-supervised%20representation%20learning%20and%202D%20unsupervised%20scene%20understanding%0Ato%20SSC.%20Our%20training%20exclusively%20utilizes%20multi-view%20consistency%0Aself-supervision%20without%20any%20form%20of%20semantic%20or%20geometric%20ground%20truth.%20Given%0Aa%20single%20input%20image%2C%20SceneDINO%20infers%20the%203D%20geometry%20and%20expressive%203D%20DINO%0Afeatures%20in%20a%20feed-forward%20manner.%20Through%20a%20novel%203D%20feature%20distillation%0Aapproach%2C%20we%20obtain%20unsupervised%203D%20semantics.%20In%20both%203D%20and%202D%20unsupervised%0Ascene%20understanding%2C%20SceneDINO%20reaches%20state-of-the-art%20segmentation%20accuracy.%0ALinear%20probing%20our%203D%20features%20matches%20the%20segmentation%20accuracy%20of%20a%20current%0Asupervised%20SSC%20approach.%20Additionally%2C%20we%20showcase%20the%20domain%20generalization%0Aand%20multi-view%20consistency%20of%20SceneDINO%2C%20taking%20the%20first%20steps%20towards%20a%0Astrong%20foundation%20for%20single%20image%203D%20scene%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06230v1&entry.124074799=Read"},
{"title": "OmniPart: Part-Aware 3D Generation with Semantic Decoupling and\n  Structural Cohesion", "author": "Yunhan Yang and Yufan Zhou and Yuan-Chen Guo and Zi-Xin Zou and Yukun Huang and Ying-Tian Liu and Hao Xu and Ding Liang and Yan-Pei Cao and Xihui Liu", "abstract": "  The creation of 3D assets with explicit, editable part structures is crucial\nfor advancing interactive applications, yet most generative methods produce\nonly monolithic shapes, limiting their utility. We introduce OmniPart, a novel\nframework for part-aware 3D object generation designed to achieve high semantic\ndecoupling among components while maintaining robust structural cohesion.\nOmniPart uniquely decouples this complex task into two synergistic stages: (1)\nan autoregressive structure planning module generates a controllable,\nvariable-length sequence of 3D part bounding boxes, critically guided by\nflexible 2D part masks that allow for intuitive control over part decomposition\nwithout requiring direct correspondences or semantic labels; and (2) a\nspatially-conditioned rectified flow model, efficiently adapted from a\npre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and\nconsistently within the planned layout. Our approach supports user-defined part\ngranularity, precise localization, and enables diverse downstream applications.\nExtensive experiments demonstrate that OmniPart achieves state-of-the-art\nperformance, paving the way for more interpretable, editable, and versatile 3D\ncontent.\n", "link": "http://arxiv.org/abs/2507.06165v1", "date": "2025-07-08", "relevancy": 2.4177, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6195}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6014}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniPart%3A%20Part-Aware%203D%20Generation%20with%20Semantic%20Decoupling%20and%0A%20%20Structural%20Cohesion&body=Title%3A%20OmniPart%3A%20Part-Aware%203D%20Generation%20with%20Semantic%20Decoupling%20and%0A%20%20Structural%20Cohesion%0AAuthor%3A%20Yunhan%20Yang%20and%20Yufan%20Zhou%20and%20Yuan-Chen%20Guo%20and%20Zi-Xin%20Zou%20and%20Yukun%20Huang%20and%20Ying-Tian%20Liu%20and%20Hao%20Xu%20and%20Ding%20Liang%20and%20Yan-Pei%20Cao%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20The%20creation%20of%203D%20assets%20with%20explicit%2C%20editable%20part%20structures%20is%20crucial%0Afor%20advancing%20interactive%20applications%2C%20yet%20most%20generative%20methods%20produce%0Aonly%20monolithic%20shapes%2C%20limiting%20their%20utility.%20We%20introduce%20OmniPart%2C%20a%20novel%0Aframework%20for%20part-aware%203D%20object%20generation%20designed%20to%20achieve%20high%20semantic%0Adecoupling%20among%20components%20while%20maintaining%20robust%20structural%20cohesion.%0AOmniPart%20uniquely%20decouples%20this%20complex%20task%20into%20two%20synergistic%20stages%3A%20%281%29%0Aan%20autoregressive%20structure%20planning%20module%20generates%20a%20controllable%2C%0Avariable-length%20sequence%20of%203D%20part%20bounding%20boxes%2C%20critically%20guided%20by%0Aflexible%202D%20part%20masks%20that%20allow%20for%20intuitive%20control%20over%20part%20decomposition%0Awithout%20requiring%20direct%20correspondences%20or%20semantic%20labels%3B%20and%20%282%29%20a%0Aspatially-conditioned%20rectified%20flow%20model%2C%20efficiently%20adapted%20from%20a%0Apre-trained%20holistic%203D%20generator%2C%20synthesizes%20all%203D%20parts%20simultaneously%20and%0Aconsistently%20within%20the%20planned%20layout.%20Our%20approach%20supports%20user-defined%20part%0Agranularity%2C%20precise%20localization%2C%20and%20enables%20diverse%20downstream%20applications.%0AExtensive%20experiments%20demonstrate%20that%20OmniPart%20achieves%20state-of-the-art%0Aperformance%2C%20paving%20the%20way%20for%20more%20interpretable%2C%20editable%2C%20and%20versatile%203D%0Acontent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniPart%253A%2520Part-Aware%25203D%2520Generation%2520with%2520Semantic%2520Decoupling%2520and%250A%2520%2520Structural%2520Cohesion%26entry.906535625%3DYunhan%2520Yang%2520and%2520Yufan%2520Zhou%2520and%2520Yuan-Chen%2520Guo%2520and%2520Zi-Xin%2520Zou%2520and%2520Yukun%2520Huang%2520and%2520Ying-Tian%2520Liu%2520and%2520Hao%2520Xu%2520and%2520Ding%2520Liang%2520and%2520Yan-Pei%2520Cao%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520The%2520creation%2520of%25203D%2520assets%2520with%2520explicit%252C%2520editable%2520part%2520structures%2520is%2520crucial%250Afor%2520advancing%2520interactive%2520applications%252C%2520yet%2520most%2520generative%2520methods%2520produce%250Aonly%2520monolithic%2520shapes%252C%2520limiting%2520their%2520utility.%2520We%2520introduce%2520OmniPart%252C%2520a%2520novel%250Aframework%2520for%2520part-aware%25203D%2520object%2520generation%2520designed%2520to%2520achieve%2520high%2520semantic%250Adecoupling%2520among%2520components%2520while%2520maintaining%2520robust%2520structural%2520cohesion.%250AOmniPart%2520uniquely%2520decouples%2520this%2520complex%2520task%2520into%2520two%2520synergistic%2520stages%253A%2520%25281%2529%250Aan%2520autoregressive%2520structure%2520planning%2520module%2520generates%2520a%2520controllable%252C%250Avariable-length%2520sequence%2520of%25203D%2520part%2520bounding%2520boxes%252C%2520critically%2520guided%2520by%250Aflexible%25202D%2520part%2520masks%2520that%2520allow%2520for%2520intuitive%2520control%2520over%2520part%2520decomposition%250Awithout%2520requiring%2520direct%2520correspondences%2520or%2520semantic%2520labels%253B%2520and%2520%25282%2529%2520a%250Aspatially-conditioned%2520rectified%2520flow%2520model%252C%2520efficiently%2520adapted%2520from%2520a%250Apre-trained%2520holistic%25203D%2520generator%252C%2520synthesizes%2520all%25203D%2520parts%2520simultaneously%2520and%250Aconsistently%2520within%2520the%2520planned%2520layout.%2520Our%2520approach%2520supports%2520user-defined%2520part%250Agranularity%252C%2520precise%2520localization%252C%2520and%2520enables%2520diverse%2520downstream%2520applications.%250AExtensive%2520experiments%2520demonstrate%2520that%2520OmniPart%2520achieves%2520state-of-the-art%250Aperformance%252C%2520paving%2520the%2520way%2520for%2520more%2520interpretable%252C%2520editable%252C%2520and%2520versatile%25203D%250Acontent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniPart%3A%20Part-Aware%203D%20Generation%20with%20Semantic%20Decoupling%20and%0A%20%20Structural%20Cohesion&entry.906535625=Yunhan%20Yang%20and%20Yufan%20Zhou%20and%20Yuan-Chen%20Guo%20and%20Zi-Xin%20Zou%20and%20Yukun%20Huang%20and%20Ying-Tian%20Liu%20and%20Hao%20Xu%20and%20Ding%20Liang%20and%20Yan-Pei%20Cao%20and%20Xihui%20Liu&entry.1292438233=%20%20The%20creation%20of%203D%20assets%20with%20explicit%2C%20editable%20part%20structures%20is%20crucial%0Afor%20advancing%20interactive%20applications%2C%20yet%20most%20generative%20methods%20produce%0Aonly%20monolithic%20shapes%2C%20limiting%20their%20utility.%20We%20introduce%20OmniPart%2C%20a%20novel%0Aframework%20for%20part-aware%203D%20object%20generation%20designed%20to%20achieve%20high%20semantic%0Adecoupling%20among%20components%20while%20maintaining%20robust%20structural%20cohesion.%0AOmniPart%20uniquely%20decouples%20this%20complex%20task%20into%20two%20synergistic%20stages%3A%20%281%29%0Aan%20autoregressive%20structure%20planning%20module%20generates%20a%20controllable%2C%0Avariable-length%20sequence%20of%203D%20part%20bounding%20boxes%2C%20critically%20guided%20by%0Aflexible%202D%20part%20masks%20that%20allow%20for%20intuitive%20control%20over%20part%20decomposition%0Awithout%20requiring%20direct%20correspondences%20or%20semantic%20labels%3B%20and%20%282%29%20a%0Aspatially-conditioned%20rectified%20flow%20model%2C%20efficiently%20adapted%20from%20a%0Apre-trained%20holistic%203D%20generator%2C%20synthesizes%20all%203D%20parts%20simultaneously%20and%0Aconsistently%20within%20the%20planned%20layout.%20Our%20approach%20supports%20user-defined%20part%0Agranularity%2C%20precise%20localization%2C%20and%20enables%20diverse%20downstream%20applications.%0AExtensive%20experiments%20demonstrate%20that%20OmniPart%20achieves%20state-of-the-art%0Aperformance%2C%20paving%20the%20way%20for%20more%20interpretable%2C%20editable%2C%20and%20versatile%203D%0Acontent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06165v1&entry.124074799=Read"},
{"title": "RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with\n  Foundation Models", "author": "Keyan Chen and Chenyang Liu and Bowen Chen and Jiafan Zhang and Zhengxia Zou and Zhenwei Shi", "abstract": "  Referring Remote Sensing Image Segmentation provides a flexible and\nfine-grained framework for remote sensing scene analysis via vision-language\ncollaborative interpretation. Current approaches predominantly utilize a\nthree-stage pipeline encompassing dual-modal encoding, cross-modal interaction,\nand pixel decoding. These methods demonstrate significant limitations in\nmanaging complex semantic relationships and achieving precise cross-modal\nalignment, largely due to their coupled processing mechanism that conflates\ntarget localization with boundary delineation. This architectural coupling\namplifies error propagation under semantic ambiguity while restricting model\ngeneralizability and interpretability. To address these issues, we propose\nRSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow\ninto a collaborative dual-stage framework: coarse localization followed by fine\nsegmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with\nSAM's segmentation generalizability through strategic foundation model\ncollaboration. Specifically, CLIP is employed as the dual-modal encoder to\nactivate target features within its pre-aligned semantic space and generate\nlocalization prompts. To mitigate CLIP's misactivation challenges in\nmulti-entity scenarios described by referring texts, a cascaded second-order\nprompter is devised, which enhances precision through implicit reasoning via\ndecomposition of text embeddings into complementary semantic subspaces. These\noptimized semantic prompts subsequently direct the SAM to generate pixel-level\nrefined masks, thereby completing the semantic transmission pipeline. Extensive\nexperiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2\nsurpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex\nsemantic interpretation. Code is available at:\nhttps://github.com/KyanChen/RSRefSeg2.\n", "link": "http://arxiv.org/abs/2507.06231v1", "date": "2025-07-08", "relevancy": 2.4149, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.609}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSRefSeg%202%3A%20Decoupling%20Referring%20Remote%20Sensing%20Image%20Segmentation%20with%0A%20%20Foundation%20Models&body=Title%3A%20RSRefSeg%202%3A%20Decoupling%20Referring%20Remote%20Sensing%20Image%20Segmentation%20with%0A%20%20Foundation%20Models%0AAuthor%3A%20Keyan%20Chen%20and%20Chenyang%20Liu%20and%20Bowen%20Chen%20and%20Jiafan%20Zhang%20and%20Zhengxia%20Zou%20and%20Zhenwei%20Shi%0AAbstract%3A%20%20%20Referring%20Remote%20Sensing%20Image%20Segmentation%20provides%20a%20flexible%20and%0Afine-grained%20framework%20for%20remote%20sensing%20scene%20analysis%20via%20vision-language%0Acollaborative%20interpretation.%20Current%20approaches%20predominantly%20utilize%20a%0Athree-stage%20pipeline%20encompassing%20dual-modal%20encoding%2C%20cross-modal%20interaction%2C%0Aand%20pixel%20decoding.%20These%20methods%20demonstrate%20significant%20limitations%20in%0Amanaging%20complex%20semantic%20relationships%20and%20achieving%20precise%20cross-modal%0Aalignment%2C%20largely%20due%20to%20their%20coupled%20processing%20mechanism%20that%20conflates%0Atarget%20localization%20with%20boundary%20delineation.%20This%20architectural%20coupling%0Aamplifies%20error%20propagation%20under%20semantic%20ambiguity%20while%20restricting%20model%0Ageneralizability%20and%20interpretability.%20To%20address%20these%20issues%2C%20we%20propose%0ARSRefSeg%202%2C%20a%20decoupling%20paradigm%20that%20reformulates%20the%20conventional%20workflow%0Ainto%20a%20collaborative%20dual-stage%20framework%3A%20coarse%20localization%20followed%20by%20fine%0Asegmentation.%20RSRefSeg%202%20integrates%20CLIP%27s%20cross-modal%20alignment%20strength%20with%0ASAM%27s%20segmentation%20generalizability%20through%20strategic%20foundation%20model%0Acollaboration.%20Specifically%2C%20CLIP%20is%20employed%20as%20the%20dual-modal%20encoder%20to%0Aactivate%20target%20features%20within%20its%20pre-aligned%20semantic%20space%20and%20generate%0Alocalization%20prompts.%20To%20mitigate%20CLIP%27s%20misactivation%20challenges%20in%0Amulti-entity%20scenarios%20described%20by%20referring%20texts%2C%20a%20cascaded%20second-order%0Aprompter%20is%20devised%2C%20which%20enhances%20precision%20through%20implicit%20reasoning%20via%0Adecomposition%20of%20text%20embeddings%20into%20complementary%20semantic%20subspaces.%20These%0Aoptimized%20semantic%20prompts%20subsequently%20direct%20the%20SAM%20to%20generate%20pixel-level%0Arefined%20masks%2C%20thereby%20completing%20the%20semantic%20transmission%20pipeline.%20Extensive%0Aexperiments%20%28RefSegRS%2C%20RRSIS-D%2C%20and%20RISBench%29%20demonstrate%20that%20RSRefSeg%202%0Asurpasses%20contemporary%20methods%20in%20segmentation%20accuracy%20%28%2B~3%25%20gIoU%29%20and%20complex%0Asemantic%20interpretation.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/KyanChen/RSRefSeg2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSRefSeg%25202%253A%2520Decoupling%2520Referring%2520Remote%2520Sensing%2520Image%2520Segmentation%2520with%250A%2520%2520Foundation%2520Models%26entry.906535625%3DKeyan%2520Chen%2520and%2520Chenyang%2520Liu%2520and%2520Bowen%2520Chen%2520and%2520Jiafan%2520Zhang%2520and%2520Zhengxia%2520Zou%2520and%2520Zhenwei%2520Shi%26entry.1292438233%3D%2520%2520Referring%2520Remote%2520Sensing%2520Image%2520Segmentation%2520provides%2520a%2520flexible%2520and%250Afine-grained%2520framework%2520for%2520remote%2520sensing%2520scene%2520analysis%2520via%2520vision-language%250Acollaborative%2520interpretation.%2520Current%2520approaches%2520predominantly%2520utilize%2520a%250Athree-stage%2520pipeline%2520encompassing%2520dual-modal%2520encoding%252C%2520cross-modal%2520interaction%252C%250Aand%2520pixel%2520decoding.%2520These%2520methods%2520demonstrate%2520significant%2520limitations%2520in%250Amanaging%2520complex%2520semantic%2520relationships%2520and%2520achieving%2520precise%2520cross-modal%250Aalignment%252C%2520largely%2520due%2520to%2520their%2520coupled%2520processing%2520mechanism%2520that%2520conflates%250Atarget%2520localization%2520with%2520boundary%2520delineation.%2520This%2520architectural%2520coupling%250Aamplifies%2520error%2520propagation%2520under%2520semantic%2520ambiguity%2520while%2520restricting%2520model%250Ageneralizability%2520and%2520interpretability.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250ARSRefSeg%25202%252C%2520a%2520decoupling%2520paradigm%2520that%2520reformulates%2520the%2520conventional%2520workflow%250Ainto%2520a%2520collaborative%2520dual-stage%2520framework%253A%2520coarse%2520localization%2520followed%2520by%2520fine%250Asegmentation.%2520RSRefSeg%25202%2520integrates%2520CLIP%2527s%2520cross-modal%2520alignment%2520strength%2520with%250ASAM%2527s%2520segmentation%2520generalizability%2520through%2520strategic%2520foundation%2520model%250Acollaboration.%2520Specifically%252C%2520CLIP%2520is%2520employed%2520as%2520the%2520dual-modal%2520encoder%2520to%250Aactivate%2520target%2520features%2520within%2520its%2520pre-aligned%2520semantic%2520space%2520and%2520generate%250Alocalization%2520prompts.%2520To%2520mitigate%2520CLIP%2527s%2520misactivation%2520challenges%2520in%250Amulti-entity%2520scenarios%2520described%2520by%2520referring%2520texts%252C%2520a%2520cascaded%2520second-order%250Aprompter%2520is%2520devised%252C%2520which%2520enhances%2520precision%2520through%2520implicit%2520reasoning%2520via%250Adecomposition%2520of%2520text%2520embeddings%2520into%2520complementary%2520semantic%2520subspaces.%2520These%250Aoptimized%2520semantic%2520prompts%2520subsequently%2520direct%2520the%2520SAM%2520to%2520generate%2520pixel-level%250Arefined%2520masks%252C%2520thereby%2520completing%2520the%2520semantic%2520transmission%2520pipeline.%2520Extensive%250Aexperiments%2520%2528RefSegRS%252C%2520RRSIS-D%252C%2520and%2520RISBench%2529%2520demonstrate%2520that%2520RSRefSeg%25202%250Asurpasses%2520contemporary%2520methods%2520in%2520segmentation%2520accuracy%2520%2528%252B~3%2525%2520gIoU%2529%2520and%2520complex%250Asemantic%2520interpretation.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/KyanChen/RSRefSeg2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSRefSeg%202%3A%20Decoupling%20Referring%20Remote%20Sensing%20Image%20Segmentation%20with%0A%20%20Foundation%20Models&entry.906535625=Keyan%20Chen%20and%20Chenyang%20Liu%20and%20Bowen%20Chen%20and%20Jiafan%20Zhang%20and%20Zhengxia%20Zou%20and%20Zhenwei%20Shi&entry.1292438233=%20%20Referring%20Remote%20Sensing%20Image%20Segmentation%20provides%20a%20flexible%20and%0Afine-grained%20framework%20for%20remote%20sensing%20scene%20analysis%20via%20vision-language%0Acollaborative%20interpretation.%20Current%20approaches%20predominantly%20utilize%20a%0Athree-stage%20pipeline%20encompassing%20dual-modal%20encoding%2C%20cross-modal%20interaction%2C%0Aand%20pixel%20decoding.%20These%20methods%20demonstrate%20significant%20limitations%20in%0Amanaging%20complex%20semantic%20relationships%20and%20achieving%20precise%20cross-modal%0Aalignment%2C%20largely%20due%20to%20their%20coupled%20processing%20mechanism%20that%20conflates%0Atarget%20localization%20with%20boundary%20delineation.%20This%20architectural%20coupling%0Aamplifies%20error%20propagation%20under%20semantic%20ambiguity%20while%20restricting%20model%0Ageneralizability%20and%20interpretability.%20To%20address%20these%20issues%2C%20we%20propose%0ARSRefSeg%202%2C%20a%20decoupling%20paradigm%20that%20reformulates%20the%20conventional%20workflow%0Ainto%20a%20collaborative%20dual-stage%20framework%3A%20coarse%20localization%20followed%20by%20fine%0Asegmentation.%20RSRefSeg%202%20integrates%20CLIP%27s%20cross-modal%20alignment%20strength%20with%0ASAM%27s%20segmentation%20generalizability%20through%20strategic%20foundation%20model%0Acollaboration.%20Specifically%2C%20CLIP%20is%20employed%20as%20the%20dual-modal%20encoder%20to%0Aactivate%20target%20features%20within%20its%20pre-aligned%20semantic%20space%20and%20generate%0Alocalization%20prompts.%20To%20mitigate%20CLIP%27s%20misactivation%20challenges%20in%0Amulti-entity%20scenarios%20described%20by%20referring%20texts%2C%20a%20cascaded%20second-order%0Aprompter%20is%20devised%2C%20which%20enhances%20precision%20through%20implicit%20reasoning%20via%0Adecomposition%20of%20text%20embeddings%20into%20complementary%20semantic%20subspaces.%20These%0Aoptimized%20semantic%20prompts%20subsequently%20direct%20the%20SAM%20to%20generate%20pixel-level%0Arefined%20masks%2C%20thereby%20completing%20the%20semantic%20transmission%20pipeline.%20Extensive%0Aexperiments%20%28RefSegRS%2C%20RRSIS-D%2C%20and%20RISBench%29%20demonstrate%20that%20RSRefSeg%202%0Asurpasses%20contemporary%20methods%20in%20segmentation%20accuracy%20%28%2B~3%25%20gIoU%29%20and%20complex%0Asemantic%20interpretation.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/KyanChen/RSRefSeg2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06231v1&entry.124074799=Read"},
{"title": "Efficient Implementation of Gaussian Process Regression Accelerated\n  Saddle Point Searches with Application to Molecular Reactions", "author": "Rohit Goswami and Maxim Masterov and Satish Kamath and Alejandro Pe\u00f1a-Torres and Hannes J\u00f3nsson", "abstract": "  The task of locating first order saddle points on high-dimensional surfaces\ndescribing the variation of energy as a function of atomic coordinates is an\nessential step for identifying the mechanism and estimating the rate of\nthermally activated events within the harmonic approximation of transition\nstate theory. When combined directly with electronic structure calculations,\nthe number of energy and atomic force evaluations needed for convergence is a\nprimary issue. Here, we describe an efficient implementation of Gaussian\nprocess regression (GPR) acceleration of the minimum mode following method\nwhere a dimer is used to estimate the lowest eigenmode of the Hessian. A\nsurrogate energy surface is constructed and updated after each electronic\nstructure calculation. The method is applied to a test set of 500 molecular\nreactions previously generated by Hermez and coworkers [J. Chem. Theory Comput.\n18, 6974 (2022)]. An order of magnitude reduction in the number of electronic\nstructure calculations needed to reach the saddle point configurations is\nobtained by using the GPR compared to the dimer method. Despite the wide range\nin stiffness of the molecular degrees of freedom, the calculations are carried\nout using Cartesian coordinates and are found to require similar number of\nelectronic structure calculations as an elaborate internal coordinate method\nimplemented in the Sella software package. The present implementation of the\nGPR surrogate model in C++ is efficient enough for the wall time of the saddle\npoint searches to be reduced in 3 out of 4 cases even though the calculations\nare carried out at a low Hartree-Fock level.\n", "link": "http://arxiv.org/abs/2505.12519v2", "date": "2025-07-08", "relevancy": 2.3841, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4933}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4912}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Implementation%20of%20Gaussian%20Process%20Regression%20Accelerated%0A%20%20Saddle%20Point%20Searches%20with%20Application%20to%20Molecular%20Reactions&body=Title%3A%20Efficient%20Implementation%20of%20Gaussian%20Process%20Regression%20Accelerated%0A%20%20Saddle%20Point%20Searches%20with%20Application%20to%20Molecular%20Reactions%0AAuthor%3A%20Rohit%20Goswami%20and%20Maxim%20Masterov%20and%20Satish%20Kamath%20and%20Alejandro%20Pe%C3%B1a-Torres%20and%20Hannes%20J%C3%B3nsson%0AAbstract%3A%20%20%20The%20task%20of%20locating%20first%20order%20saddle%20points%20on%20high-dimensional%20surfaces%0Adescribing%20the%20variation%20of%20energy%20as%20a%20function%20of%20atomic%20coordinates%20is%20an%0Aessential%20step%20for%20identifying%20the%20mechanism%20and%20estimating%20the%20rate%20of%0Athermally%20activated%20events%20within%20the%20harmonic%20approximation%20of%20transition%0Astate%20theory.%20When%20combined%20directly%20with%20electronic%20structure%20calculations%2C%0Athe%20number%20of%20energy%20and%20atomic%20force%20evaluations%20needed%20for%20convergence%20is%20a%0Aprimary%20issue.%20Here%2C%20we%20describe%20an%20efficient%20implementation%20of%20Gaussian%0Aprocess%20regression%20%28GPR%29%20acceleration%20of%20the%20minimum%20mode%20following%20method%0Awhere%20a%20dimer%20is%20used%20to%20estimate%20the%20lowest%20eigenmode%20of%20the%20Hessian.%20A%0Asurrogate%20energy%20surface%20is%20constructed%20and%20updated%20after%20each%20electronic%0Astructure%20calculation.%20The%20method%20is%20applied%20to%20a%20test%20set%20of%20500%20molecular%0Areactions%20previously%20generated%20by%20Hermez%20and%20coworkers%20%5BJ.%20Chem.%20Theory%20Comput.%0A18%2C%206974%20%282022%29%5D.%20An%20order%20of%20magnitude%20reduction%20in%20the%20number%20of%20electronic%0Astructure%20calculations%20needed%20to%20reach%20the%20saddle%20point%20configurations%20is%0Aobtained%20by%20using%20the%20GPR%20compared%20to%20the%20dimer%20method.%20Despite%20the%20wide%20range%0Ain%20stiffness%20of%20the%20molecular%20degrees%20of%20freedom%2C%20the%20calculations%20are%20carried%0Aout%20using%20Cartesian%20coordinates%20and%20are%20found%20to%20require%20similar%20number%20of%0Aelectronic%20structure%20calculations%20as%20an%20elaborate%20internal%20coordinate%20method%0Aimplemented%20in%20the%20Sella%20software%20package.%20The%20present%20implementation%20of%20the%0AGPR%20surrogate%20model%20in%20C%2B%2B%20is%20efficient%20enough%20for%20the%20wall%20time%20of%20the%20saddle%0Apoint%20searches%20to%20be%20reduced%20in%203%20out%20of%204%20cases%20even%20though%20the%20calculations%0Aare%20carried%20out%20at%20a%20low%20Hartree-Fock%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12519v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Implementation%2520of%2520Gaussian%2520Process%2520Regression%2520Accelerated%250A%2520%2520Saddle%2520Point%2520Searches%2520with%2520Application%2520to%2520Molecular%2520Reactions%26entry.906535625%3DRohit%2520Goswami%2520and%2520Maxim%2520Masterov%2520and%2520Satish%2520Kamath%2520and%2520Alejandro%2520Pe%25C3%25B1a-Torres%2520and%2520Hannes%2520J%25C3%25B3nsson%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520locating%2520first%2520order%2520saddle%2520points%2520on%2520high-dimensional%2520surfaces%250Adescribing%2520the%2520variation%2520of%2520energy%2520as%2520a%2520function%2520of%2520atomic%2520coordinates%2520is%2520an%250Aessential%2520step%2520for%2520identifying%2520the%2520mechanism%2520and%2520estimating%2520the%2520rate%2520of%250Athermally%2520activated%2520events%2520within%2520the%2520harmonic%2520approximation%2520of%2520transition%250Astate%2520theory.%2520When%2520combined%2520directly%2520with%2520electronic%2520structure%2520calculations%252C%250Athe%2520number%2520of%2520energy%2520and%2520atomic%2520force%2520evaluations%2520needed%2520for%2520convergence%2520is%2520a%250Aprimary%2520issue.%2520Here%252C%2520we%2520describe%2520an%2520efficient%2520implementation%2520of%2520Gaussian%250Aprocess%2520regression%2520%2528GPR%2529%2520acceleration%2520of%2520the%2520minimum%2520mode%2520following%2520method%250Awhere%2520a%2520dimer%2520is%2520used%2520to%2520estimate%2520the%2520lowest%2520eigenmode%2520of%2520the%2520Hessian.%2520A%250Asurrogate%2520energy%2520surface%2520is%2520constructed%2520and%2520updated%2520after%2520each%2520electronic%250Astructure%2520calculation.%2520The%2520method%2520is%2520applied%2520to%2520a%2520test%2520set%2520of%2520500%2520molecular%250Areactions%2520previously%2520generated%2520by%2520Hermez%2520and%2520coworkers%2520%255BJ.%2520Chem.%2520Theory%2520Comput.%250A18%252C%25206974%2520%25282022%2529%255D.%2520An%2520order%2520of%2520magnitude%2520reduction%2520in%2520the%2520number%2520of%2520electronic%250Astructure%2520calculations%2520needed%2520to%2520reach%2520the%2520saddle%2520point%2520configurations%2520is%250Aobtained%2520by%2520using%2520the%2520GPR%2520compared%2520to%2520the%2520dimer%2520method.%2520Despite%2520the%2520wide%2520range%250Ain%2520stiffness%2520of%2520the%2520molecular%2520degrees%2520of%2520freedom%252C%2520the%2520calculations%2520are%2520carried%250Aout%2520using%2520Cartesian%2520coordinates%2520and%2520are%2520found%2520to%2520require%2520similar%2520number%2520of%250Aelectronic%2520structure%2520calculations%2520as%2520an%2520elaborate%2520internal%2520coordinate%2520method%250Aimplemented%2520in%2520the%2520Sella%2520software%2520package.%2520The%2520present%2520implementation%2520of%2520the%250AGPR%2520surrogate%2520model%2520in%2520C%252B%252B%2520is%2520efficient%2520enough%2520for%2520the%2520wall%2520time%2520of%2520the%2520saddle%250Apoint%2520searches%2520to%2520be%2520reduced%2520in%25203%2520out%2520of%25204%2520cases%2520even%2520though%2520the%2520calculations%250Aare%2520carried%2520out%2520at%2520a%2520low%2520Hartree-Fock%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12519v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Implementation%20of%20Gaussian%20Process%20Regression%20Accelerated%0A%20%20Saddle%20Point%20Searches%20with%20Application%20to%20Molecular%20Reactions&entry.906535625=Rohit%20Goswami%20and%20Maxim%20Masterov%20and%20Satish%20Kamath%20and%20Alejandro%20Pe%C3%B1a-Torres%20and%20Hannes%20J%C3%B3nsson&entry.1292438233=%20%20The%20task%20of%20locating%20first%20order%20saddle%20points%20on%20high-dimensional%20surfaces%0Adescribing%20the%20variation%20of%20energy%20as%20a%20function%20of%20atomic%20coordinates%20is%20an%0Aessential%20step%20for%20identifying%20the%20mechanism%20and%20estimating%20the%20rate%20of%0Athermally%20activated%20events%20within%20the%20harmonic%20approximation%20of%20transition%0Astate%20theory.%20When%20combined%20directly%20with%20electronic%20structure%20calculations%2C%0Athe%20number%20of%20energy%20and%20atomic%20force%20evaluations%20needed%20for%20convergence%20is%20a%0Aprimary%20issue.%20Here%2C%20we%20describe%20an%20efficient%20implementation%20of%20Gaussian%0Aprocess%20regression%20%28GPR%29%20acceleration%20of%20the%20minimum%20mode%20following%20method%0Awhere%20a%20dimer%20is%20used%20to%20estimate%20the%20lowest%20eigenmode%20of%20the%20Hessian.%20A%0Asurrogate%20energy%20surface%20is%20constructed%20and%20updated%20after%20each%20electronic%0Astructure%20calculation.%20The%20method%20is%20applied%20to%20a%20test%20set%20of%20500%20molecular%0Areactions%20previously%20generated%20by%20Hermez%20and%20coworkers%20%5BJ.%20Chem.%20Theory%20Comput.%0A18%2C%206974%20%282022%29%5D.%20An%20order%20of%20magnitude%20reduction%20in%20the%20number%20of%20electronic%0Astructure%20calculations%20needed%20to%20reach%20the%20saddle%20point%20configurations%20is%0Aobtained%20by%20using%20the%20GPR%20compared%20to%20the%20dimer%20method.%20Despite%20the%20wide%20range%0Ain%20stiffness%20of%20the%20molecular%20degrees%20of%20freedom%2C%20the%20calculations%20are%20carried%0Aout%20using%20Cartesian%20coordinates%20and%20are%20found%20to%20require%20similar%20number%20of%0Aelectronic%20structure%20calculations%20as%20an%20elaborate%20internal%20coordinate%20method%0Aimplemented%20in%20the%20Sella%20software%20package.%20The%20present%20implementation%20of%20the%0AGPR%20surrogate%20model%20in%20C%2B%2B%20is%20efficient%20enough%20for%20the%20wall%20time%20of%20the%20saddle%0Apoint%20searches%20to%20be%20reduced%20in%203%20out%20of%204%20cases%20even%20though%20the%20calculations%0Aare%20carried%20out%20at%20a%20low%20Hartree-Fock%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12519v2&entry.124074799=Read"},
{"title": "CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs", "author": "Haoxi Li and Sikai Bai and Jie Zhang and Song Guo", "abstract": "  Large reasoning models (LRMs) have demonstrated impressive capabilities in\ndomains like mathematics and program synthesis. Despite their strong\nperformance, LRMs often exhibit overthinking -- excessive and redundant\nreasoning steps that introduce inefficiencies during inference. This phenomenon\nraises an important question for LRM self-evaluation: How can a model\nautonomously assess the correctness of its own reasoning trajectory without\nexternal labels? To address this, we propose Chain-of-Reasoning Embedding\n(CoRE), a series of hidden states in latent space to enable label-free\nself-evaluation on intermediate reasoning steps of LRMs, so as to enhance\nmetacognition abilities for improved reasoning efficiency. By analyzing the\ngeometric properties of the CoRE trajectories, we reveal that redundant\nreasoning usually presents cyclical fluctuations, which correspond to\nrepetitive and unconscious reflection/exploration. Leveraging this insight, we\nfurther introduce a training-free, label-free self-evaluation framework,\nCoRE-Eval, to detect such patterns and dynamically determine whether to\nterminate reasoning early. Extensive experiments on mathematical reasoning\nbenchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B\ndemonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2%\nwhile improving answer accuracy by around 10%, achieving 70.0% accuracy on the\nchallenging AIME benchmark with the 32B model.\n", "link": "http://arxiv.org/abs/2507.06087v1", "date": "2025-07-08", "relevancy": 2.3737, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoRE%3A%20Enhancing%20Metacognition%20with%20Label-free%20Self-evaluation%20in%20LRMs&body=Title%3A%20CoRE%3A%20Enhancing%20Metacognition%20with%20Label-free%20Self-evaluation%20in%20LRMs%0AAuthor%3A%20Haoxi%20Li%20and%20Sikai%20Bai%20and%20Jie%20Zhang%20and%20Song%20Guo%0AAbstract%3A%20%20%20Large%20reasoning%20models%20%28LRMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Adomains%20like%20mathematics%20and%20program%20synthesis.%20Despite%20their%20strong%0Aperformance%2C%20LRMs%20often%20exhibit%20overthinking%20--%20excessive%20and%20redundant%0Areasoning%20steps%20that%20introduce%20inefficiencies%20during%20inference.%20This%20phenomenon%0Araises%20an%20important%20question%20for%20LRM%20self-evaluation%3A%20How%20can%20a%20model%0Aautonomously%20assess%20the%20correctness%20of%20its%20own%20reasoning%20trajectory%20without%0Aexternal%20labels%3F%20To%20address%20this%2C%20we%20propose%20Chain-of-Reasoning%20Embedding%0A%28CoRE%29%2C%20a%20series%20of%20hidden%20states%20in%20latent%20space%20to%20enable%20label-free%0Aself-evaluation%20on%20intermediate%20reasoning%20steps%20of%20LRMs%2C%20so%20as%20to%20enhance%0Ametacognition%20abilities%20for%20improved%20reasoning%20efficiency.%20By%20analyzing%20the%0Ageometric%20properties%20of%20the%20CoRE%20trajectories%2C%20we%20reveal%20that%20redundant%0Areasoning%20usually%20presents%20cyclical%20fluctuations%2C%20which%20correspond%20to%0Arepetitive%20and%20unconscious%20reflection/exploration.%20Leveraging%20this%20insight%2C%20we%0Afurther%20introduce%20a%20training-free%2C%20label-free%20self-evaluation%20framework%2C%0ACoRE-Eval%2C%20to%20detect%20such%20patterns%20and%20dynamically%20determine%20whether%20to%0Aterminate%20reasoning%20early.%20Extensive%20experiments%20on%20mathematical%20reasoning%0Abenchmarks%20%28GSM8K%2C%20MATH-500%2C%20and%20AIME%29%20and%20across%20model%20sizes%20from%207B%20to%2032B%0Ademonstrate%20that%20CoRE-Eval%20reduces%20chain-of-thought%20length%20by%2013.7%25%20to%2033.2%25%0Awhile%20improving%20answer%20accuracy%20by%20around%2010%25%2C%20achieving%2070.0%25%20accuracy%20on%20the%0Achallenging%20AIME%20benchmark%20with%20the%2032B%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoRE%253A%2520Enhancing%2520Metacognition%2520with%2520Label-free%2520Self-evaluation%2520in%2520LRMs%26entry.906535625%3DHaoxi%2520Li%2520and%2520Sikai%2520Bai%2520and%2520Jie%2520Zhang%2520and%2520Song%2520Guo%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520%2528LRMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%250Adomains%2520like%2520mathematics%2520and%2520program%2520synthesis.%2520Despite%2520their%2520strong%250Aperformance%252C%2520LRMs%2520often%2520exhibit%2520overthinking%2520--%2520excessive%2520and%2520redundant%250Areasoning%2520steps%2520that%2520introduce%2520inefficiencies%2520during%2520inference.%2520This%2520phenomenon%250Araises%2520an%2520important%2520question%2520for%2520LRM%2520self-evaluation%253A%2520How%2520can%2520a%2520model%250Aautonomously%2520assess%2520the%2520correctness%2520of%2520its%2520own%2520reasoning%2520trajectory%2520without%250Aexternal%2520labels%253F%2520To%2520address%2520this%252C%2520we%2520propose%2520Chain-of-Reasoning%2520Embedding%250A%2528CoRE%2529%252C%2520a%2520series%2520of%2520hidden%2520states%2520in%2520latent%2520space%2520to%2520enable%2520label-free%250Aself-evaluation%2520on%2520intermediate%2520reasoning%2520steps%2520of%2520LRMs%252C%2520so%2520as%2520to%2520enhance%250Ametacognition%2520abilities%2520for%2520improved%2520reasoning%2520efficiency.%2520By%2520analyzing%2520the%250Ageometric%2520properties%2520of%2520the%2520CoRE%2520trajectories%252C%2520we%2520reveal%2520that%2520redundant%250Areasoning%2520usually%2520presents%2520cyclical%2520fluctuations%252C%2520which%2520correspond%2520to%250Arepetitive%2520and%2520unconscious%2520reflection/exploration.%2520Leveraging%2520this%2520insight%252C%2520we%250Afurther%2520introduce%2520a%2520training-free%252C%2520label-free%2520self-evaluation%2520framework%252C%250ACoRE-Eval%252C%2520to%2520detect%2520such%2520patterns%2520and%2520dynamically%2520determine%2520whether%2520to%250Aterminate%2520reasoning%2520early.%2520Extensive%2520experiments%2520on%2520mathematical%2520reasoning%250Abenchmarks%2520%2528GSM8K%252C%2520MATH-500%252C%2520and%2520AIME%2529%2520and%2520across%2520model%2520sizes%2520from%25207B%2520to%252032B%250Ademonstrate%2520that%2520CoRE-Eval%2520reduces%2520chain-of-thought%2520length%2520by%252013.7%2525%2520to%252033.2%2525%250Awhile%2520improving%2520answer%2520accuracy%2520by%2520around%252010%2525%252C%2520achieving%252070.0%2525%2520accuracy%2520on%2520the%250Achallenging%2520AIME%2520benchmark%2520with%2520the%252032B%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoRE%3A%20Enhancing%20Metacognition%20with%20Label-free%20Self-evaluation%20in%20LRMs&entry.906535625=Haoxi%20Li%20and%20Sikai%20Bai%20and%20Jie%20Zhang%20and%20Song%20Guo&entry.1292438233=%20%20Large%20reasoning%20models%20%28LRMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Adomains%20like%20mathematics%20and%20program%20synthesis.%20Despite%20their%20strong%0Aperformance%2C%20LRMs%20often%20exhibit%20overthinking%20--%20excessive%20and%20redundant%0Areasoning%20steps%20that%20introduce%20inefficiencies%20during%20inference.%20This%20phenomenon%0Araises%20an%20important%20question%20for%20LRM%20self-evaluation%3A%20How%20can%20a%20model%0Aautonomously%20assess%20the%20correctness%20of%20its%20own%20reasoning%20trajectory%20without%0Aexternal%20labels%3F%20To%20address%20this%2C%20we%20propose%20Chain-of-Reasoning%20Embedding%0A%28CoRE%29%2C%20a%20series%20of%20hidden%20states%20in%20latent%20space%20to%20enable%20label-free%0Aself-evaluation%20on%20intermediate%20reasoning%20steps%20of%20LRMs%2C%20so%20as%20to%20enhance%0Ametacognition%20abilities%20for%20improved%20reasoning%20efficiency.%20By%20analyzing%20the%0Ageometric%20properties%20of%20the%20CoRE%20trajectories%2C%20we%20reveal%20that%20redundant%0Areasoning%20usually%20presents%20cyclical%20fluctuations%2C%20which%20correspond%20to%0Arepetitive%20and%20unconscious%20reflection/exploration.%20Leveraging%20this%20insight%2C%20we%0Afurther%20introduce%20a%20training-free%2C%20label-free%20self-evaluation%20framework%2C%0ACoRE-Eval%2C%20to%20detect%20such%20patterns%20and%20dynamically%20determine%20whether%20to%0Aterminate%20reasoning%20early.%20Extensive%20experiments%20on%20mathematical%20reasoning%0Abenchmarks%20%28GSM8K%2C%20MATH-500%2C%20and%20AIME%29%20and%20across%20model%20sizes%20from%207B%20to%2032B%0Ademonstrate%20that%20CoRE-Eval%20reduces%20chain-of-thought%20length%20by%2013.7%25%20to%2033.2%25%0Awhile%20improving%20answer%20accuracy%20by%20around%2010%25%2C%20achieving%2070.0%25%20accuracy%20on%20the%0Achallenging%20AIME%20benchmark%20with%20the%2032B%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06087v1&entry.124074799=Read"},
{"title": "OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech\n  Language Model", "author": "Chen Wang and Tianyu Peng and Wen Yang and Yinan Bai and Guangfu Wang and Jun Lin and Lanpeng Jia and Lingxiang Wu and Jinqiao Wang and Chengqing Zong and Jiajun Zhang", "abstract": "  Empathetic interaction is a cornerstone of human-machine communication, due\nto the need for understanding speech enriched with paralinguistic cues and\ngenerating emotional and expressive responses. However, the most powerful\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\nthe architecture, data and development opaque to researchers. Given the\ncritical need for transparent research into the LSLMs and empathetic behavior,\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\ndesigned to enable empathetic speech interactions. Based on our empathetic\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\ndecoding architecture to achieve low-latency speech generation. To facilitate\nend-to-end training, OpenS2S incorporates an automated data construction\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\nlow cost. By leveraging large language models to generate empathetic content\nand controllable text-to-speech systems to introduce speaker and emotional\nvariation, we construct a scalable training corpus with rich paralinguistic\ndiversity and minimal human supervision. We release the fully open-source\nOpenS2S model, including the dataset, model weights, pre-training and\nfine-tuning codes, to empower the broader research community and accelerate\ninnovation in empathetic speech systems. The project webpage can be accessed at\nhttps://casia-lm.github.io/OpenS2S\n", "link": "http://arxiv.org/abs/2507.05177v2", "date": "2025-07-08", "relevancy": 2.3636, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenS2S%3A%20Advancing%20Fully%20Open-Source%20End-to-End%20Empathetic%20Large%20Speech%0A%20%20Language%20Model&body=Title%3A%20OpenS2S%3A%20Advancing%20Fully%20Open-Source%20End-to-End%20Empathetic%20Large%20Speech%0A%20%20Language%20Model%0AAuthor%3A%20Chen%20Wang%20and%20Tianyu%20Peng%20and%20Wen%20Yang%20and%20Yinan%20Bai%20and%20Guangfu%20Wang%20and%20Jun%20Lin%20and%20Lanpeng%20Jia%20and%20Lingxiang%20Wu%20and%20Jinqiao%20Wang%20and%20Chengqing%20Zong%20and%20Jiajun%20Zhang%0AAbstract%3A%20%20%20Empathetic%20interaction%20is%20a%20cornerstone%20of%20human-machine%20communication%2C%20due%0Ato%20the%20need%20for%20understanding%20speech%20enriched%20with%20paralinguistic%20cues%20and%0Agenerating%20emotional%20and%20expressive%20responses.%20However%2C%20the%20most%20powerful%0Aempathetic%20LSLMs%20are%20increasingly%20closed%20off%2C%20leaving%20the%20crucial%20details%20about%0Athe%20architecture%2C%20data%20and%20development%20opaque%20to%20researchers.%20Given%20the%0Acritical%20need%20for%20transparent%20research%20into%20the%20LSLMs%20and%20empathetic%20behavior%2C%0Awe%20present%20OpenS2S%2C%20a%20fully%20open-source%2C%20transparent%20and%20end-to-end%20LSLM%0Adesigned%20to%20enable%20empathetic%20speech%20interactions.%20Based%20on%20our%20empathetic%0Aspeech-to-text%20model%20BLSP-Emo%2C%20OpenS2S%20further%20employs%20a%20streaming%20interleaved%0Adecoding%20architecture%20to%20achieve%20low-latency%20speech%20generation.%20To%20facilitate%0Aend-to-end%20training%2C%20OpenS2S%20incorporates%20an%20automated%20data%20construction%0Apipeline%20that%20synthesizes%20diverse%2C%20high-quality%20empathetic%20speech%20dialogues%20at%0Alow%20cost.%20By%20leveraging%20large%20language%20models%20to%20generate%20empathetic%20content%0Aand%20controllable%20text-to-speech%20systems%20to%20introduce%20speaker%20and%20emotional%0Avariation%2C%20we%20construct%20a%20scalable%20training%20corpus%20with%20rich%20paralinguistic%0Adiversity%20and%20minimal%20human%20supervision.%20We%20release%20the%20fully%20open-source%0AOpenS2S%20model%2C%20including%20the%20dataset%2C%20model%20weights%2C%20pre-training%20and%0Afine-tuning%20codes%2C%20to%20empower%20the%20broader%20research%20community%20and%20accelerate%0Ainnovation%20in%20empathetic%20speech%20systems.%20The%20project%20webpage%20can%20be%20accessed%20at%0Ahttps%3A//casia-lm.github.io/OpenS2S%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05177v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenS2S%253A%2520Advancing%2520Fully%2520Open-Source%2520End-to-End%2520Empathetic%2520Large%2520Speech%250A%2520%2520Language%2520Model%26entry.906535625%3DChen%2520Wang%2520and%2520Tianyu%2520Peng%2520and%2520Wen%2520Yang%2520and%2520Yinan%2520Bai%2520and%2520Guangfu%2520Wang%2520and%2520Jun%2520Lin%2520and%2520Lanpeng%2520Jia%2520and%2520Lingxiang%2520Wu%2520and%2520Jinqiao%2520Wang%2520and%2520Chengqing%2520Zong%2520and%2520Jiajun%2520Zhang%26entry.1292438233%3D%2520%2520Empathetic%2520interaction%2520is%2520a%2520cornerstone%2520of%2520human-machine%2520communication%252C%2520due%250Ato%2520the%2520need%2520for%2520understanding%2520speech%2520enriched%2520with%2520paralinguistic%2520cues%2520and%250Agenerating%2520emotional%2520and%2520expressive%2520responses.%2520However%252C%2520the%2520most%2520powerful%250Aempathetic%2520LSLMs%2520are%2520increasingly%2520closed%2520off%252C%2520leaving%2520the%2520crucial%2520details%2520about%250Athe%2520architecture%252C%2520data%2520and%2520development%2520opaque%2520to%2520researchers.%2520Given%2520the%250Acritical%2520need%2520for%2520transparent%2520research%2520into%2520the%2520LSLMs%2520and%2520empathetic%2520behavior%252C%250Awe%2520present%2520OpenS2S%252C%2520a%2520fully%2520open-source%252C%2520transparent%2520and%2520end-to-end%2520LSLM%250Adesigned%2520to%2520enable%2520empathetic%2520speech%2520interactions.%2520Based%2520on%2520our%2520empathetic%250Aspeech-to-text%2520model%2520BLSP-Emo%252C%2520OpenS2S%2520further%2520employs%2520a%2520streaming%2520interleaved%250Adecoding%2520architecture%2520to%2520achieve%2520low-latency%2520speech%2520generation.%2520To%2520facilitate%250Aend-to-end%2520training%252C%2520OpenS2S%2520incorporates%2520an%2520automated%2520data%2520construction%250Apipeline%2520that%2520synthesizes%2520diverse%252C%2520high-quality%2520empathetic%2520speech%2520dialogues%2520at%250Alow%2520cost.%2520By%2520leveraging%2520large%2520language%2520models%2520to%2520generate%2520empathetic%2520content%250Aand%2520controllable%2520text-to-speech%2520systems%2520to%2520introduce%2520speaker%2520and%2520emotional%250Avariation%252C%2520we%2520construct%2520a%2520scalable%2520training%2520corpus%2520with%2520rich%2520paralinguistic%250Adiversity%2520and%2520minimal%2520human%2520supervision.%2520We%2520release%2520the%2520fully%2520open-source%250AOpenS2S%2520model%252C%2520including%2520the%2520dataset%252C%2520model%2520weights%252C%2520pre-training%2520and%250Afine-tuning%2520codes%252C%2520to%2520empower%2520the%2520broader%2520research%2520community%2520and%2520accelerate%250Ainnovation%2520in%2520empathetic%2520speech%2520systems.%2520The%2520project%2520webpage%2520can%2520be%2520accessed%2520at%250Ahttps%253A//casia-lm.github.io/OpenS2S%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05177v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenS2S%3A%20Advancing%20Fully%20Open-Source%20End-to-End%20Empathetic%20Large%20Speech%0A%20%20Language%20Model&entry.906535625=Chen%20Wang%20and%20Tianyu%20Peng%20and%20Wen%20Yang%20and%20Yinan%20Bai%20and%20Guangfu%20Wang%20and%20Jun%20Lin%20and%20Lanpeng%20Jia%20and%20Lingxiang%20Wu%20and%20Jinqiao%20Wang%20and%20Chengqing%20Zong%20and%20Jiajun%20Zhang&entry.1292438233=%20%20Empathetic%20interaction%20is%20a%20cornerstone%20of%20human-machine%20communication%2C%20due%0Ato%20the%20need%20for%20understanding%20speech%20enriched%20with%20paralinguistic%20cues%20and%0Agenerating%20emotional%20and%20expressive%20responses.%20However%2C%20the%20most%20powerful%0Aempathetic%20LSLMs%20are%20increasingly%20closed%20off%2C%20leaving%20the%20crucial%20details%20about%0Athe%20architecture%2C%20data%20and%20development%20opaque%20to%20researchers.%20Given%20the%0Acritical%20need%20for%20transparent%20research%20into%20the%20LSLMs%20and%20empathetic%20behavior%2C%0Awe%20present%20OpenS2S%2C%20a%20fully%20open-source%2C%20transparent%20and%20end-to-end%20LSLM%0Adesigned%20to%20enable%20empathetic%20speech%20interactions.%20Based%20on%20our%20empathetic%0Aspeech-to-text%20model%20BLSP-Emo%2C%20OpenS2S%20further%20employs%20a%20streaming%20interleaved%0Adecoding%20architecture%20to%20achieve%20low-latency%20speech%20generation.%20To%20facilitate%0Aend-to-end%20training%2C%20OpenS2S%20incorporates%20an%20automated%20data%20construction%0Apipeline%20that%20synthesizes%20diverse%2C%20high-quality%20empathetic%20speech%20dialogues%20at%0Alow%20cost.%20By%20leveraging%20large%20language%20models%20to%20generate%20empathetic%20content%0Aand%20controllable%20text-to-speech%20systems%20to%20introduce%20speaker%20and%20emotional%0Avariation%2C%20we%20construct%20a%20scalable%20training%20corpus%20with%20rich%20paralinguistic%0Adiversity%20and%20minimal%20human%20supervision.%20We%20release%20the%20fully%20open-source%0AOpenS2S%20model%2C%20including%20the%20dataset%2C%20model%20weights%2C%20pre-training%20and%0Afine-tuning%20codes%2C%20to%20empower%20the%20broader%20research%20community%20and%20accelerate%0Ainnovation%20in%20empathetic%20speech%20systems.%20The%20project%20webpage%20can%20be%20accessed%20at%0Ahttps%3A//casia-lm.github.io/OpenS2S%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05177v2&entry.124074799=Read"},
{"title": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in\n  LLMs", "author": "Yizhan Huang and Zhe Yang and Meifang Chen and Jianping Zhang and Michael R. Lyu", "abstract": "  Large Language Models (LLMs) are known to memorize portions of their training\ndata, sometimes reproducing content verbatim when prompted appropriately. In\nthis work, we investigate a fundamental yet under-explored question in the\ndomain of memorization: How to characterize memorization difficulty of training\ndata in LLMs? Through empirical experiments on OLMo, a family of open models,\nwe present the Entropy-Memorization Law. It suggests that data entropy is\nlinearly correlated with memorization score. Moreover, in a case study of\nmemorizing highly randomized strings, or \"gibberish\", we observe that such\nsequences, despite their apparent randomness, exhibit unexpectedly low\nempirical entropy compared to the broader training corpus. Adopting the same\nstrategy to discover Entropy-Memorization Law, we derive a simple yet effective\napproach to distinguish training and testing data, enabling Dataset Inference\n(DI).\n", "link": "http://arxiv.org/abs/2507.06056v1", "date": "2025-07-08", "relevancy": 2.3622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4757}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy-Memorization%20Law%3A%20Evaluating%20Memorization%20Difficulty%20of%20Data%20in%0A%20%20LLMs&body=Title%3A%20Entropy-Memorization%20Law%3A%20Evaluating%20Memorization%20Difficulty%20of%20Data%20in%0A%20%20LLMs%0AAuthor%3A%20Yizhan%20Huang%20and%20Zhe%20Yang%20and%20Meifang%20Chen%20and%20Jianping%20Zhang%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20known%20to%20memorize%20portions%20of%20their%20training%0Adata%2C%20sometimes%20reproducing%20content%20verbatim%20when%20prompted%20appropriately.%20In%0Athis%20work%2C%20we%20investigate%20a%20fundamental%20yet%20under-explored%20question%20in%20the%0Adomain%20of%20memorization%3A%20How%20to%20characterize%20memorization%20difficulty%20of%20training%0Adata%20in%20LLMs%3F%20Through%20empirical%20experiments%20on%20OLMo%2C%20a%20family%20of%20open%20models%2C%0Awe%20present%20the%20Entropy-Memorization%20Law.%20It%20suggests%20that%20data%20entropy%20is%0Alinearly%20correlated%20with%20memorization%20score.%20Moreover%2C%20in%20a%20case%20study%20of%0Amemorizing%20highly%20randomized%20strings%2C%20or%20%22gibberish%22%2C%20we%20observe%20that%20such%0Asequences%2C%20despite%20their%20apparent%20randomness%2C%20exhibit%20unexpectedly%20low%0Aempirical%20entropy%20compared%20to%20the%20broader%20training%20corpus.%20Adopting%20the%20same%0Astrategy%20to%20discover%20Entropy-Memorization%20Law%2C%20we%20derive%20a%20simple%20yet%20effective%0Aapproach%20to%20distinguish%20training%20and%20testing%20data%2C%20enabling%20Dataset%20Inference%0A%28DI%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy-Memorization%2520Law%253A%2520Evaluating%2520Memorization%2520Difficulty%2520of%2520Data%2520in%250A%2520%2520LLMs%26entry.906535625%3DYizhan%2520Huang%2520and%2520Zhe%2520Yang%2520and%2520Meifang%2520Chen%2520and%2520Jianping%2520Zhang%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520known%2520to%2520memorize%2520portions%2520of%2520their%2520training%250Adata%252C%2520sometimes%2520reproducing%2520content%2520verbatim%2520when%2520prompted%2520appropriately.%2520In%250Athis%2520work%252C%2520we%2520investigate%2520a%2520fundamental%2520yet%2520under-explored%2520question%2520in%2520the%250Adomain%2520of%2520memorization%253A%2520How%2520to%2520characterize%2520memorization%2520difficulty%2520of%2520training%250Adata%2520in%2520LLMs%253F%2520Through%2520empirical%2520experiments%2520on%2520OLMo%252C%2520a%2520family%2520of%2520open%2520models%252C%250Awe%2520present%2520the%2520Entropy-Memorization%2520Law.%2520It%2520suggests%2520that%2520data%2520entropy%2520is%250Alinearly%2520correlated%2520with%2520memorization%2520score.%2520Moreover%252C%2520in%2520a%2520case%2520study%2520of%250Amemorizing%2520highly%2520randomized%2520strings%252C%2520or%2520%2522gibberish%2522%252C%2520we%2520observe%2520that%2520such%250Asequences%252C%2520despite%2520their%2520apparent%2520randomness%252C%2520exhibit%2520unexpectedly%2520low%250Aempirical%2520entropy%2520compared%2520to%2520the%2520broader%2520training%2520corpus.%2520Adopting%2520the%2520same%250Astrategy%2520to%2520discover%2520Entropy-Memorization%2520Law%252C%2520we%2520derive%2520a%2520simple%2520yet%2520effective%250Aapproach%2520to%2520distinguish%2520training%2520and%2520testing%2520data%252C%2520enabling%2520Dataset%2520Inference%250A%2528DI%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy-Memorization%20Law%3A%20Evaluating%20Memorization%20Difficulty%20of%20Data%20in%0A%20%20LLMs&entry.906535625=Yizhan%20Huang%20and%20Zhe%20Yang%20and%20Meifang%20Chen%20and%20Jianping%20Zhang%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20known%20to%20memorize%20portions%20of%20their%20training%0Adata%2C%20sometimes%20reproducing%20content%20verbatim%20when%20prompted%20appropriately.%20In%0Athis%20work%2C%20we%20investigate%20a%20fundamental%20yet%20under-explored%20question%20in%20the%0Adomain%20of%20memorization%3A%20How%20to%20characterize%20memorization%20difficulty%20of%20training%0Adata%20in%20LLMs%3F%20Through%20empirical%20experiments%20on%20OLMo%2C%20a%20family%20of%20open%20models%2C%0Awe%20present%20the%20Entropy-Memorization%20Law.%20It%20suggests%20that%20data%20entropy%20is%0Alinearly%20correlated%20with%20memorization%20score.%20Moreover%2C%20in%20a%20case%20study%20of%0Amemorizing%20highly%20randomized%20strings%2C%20or%20%22gibberish%22%2C%20we%20observe%20that%20such%0Asequences%2C%20despite%20their%20apparent%20randomness%2C%20exhibit%20unexpectedly%20low%0Aempirical%20entropy%20compared%20to%20the%20broader%20training%20corpus.%20Adopting%20the%20same%0Astrategy%20to%20discover%20Entropy-Memorization%20Law%2C%20we%20derive%20a%20simple%20yet%20effective%0Aapproach%20to%20distinguish%20training%20and%20testing%20data%2C%20enabling%20Dataset%20Inference%0A%28DI%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06056v1&entry.124074799=Read"},
{"title": "Eyes on the Environment: AI-Driven Analysis for Fire and Smoke\n  Classification, Segmentation, and Detection", "author": "Sayed Pedram Haeri Boroujeni and Niloufar Mehrabi and Fatemeh Afghah and Connor Peter McGrath and Danish Bhatkar and Mithilesh Anil Biradar and Abolfazl Razi", "abstract": "  Fire and smoke phenomena pose a significant threat to the natural\nenvironment, ecosystems, and global economy, as well as human lives and\nwildlife. In this particular circumstance, there is a demand for more\nsophisticated and advanced technologies to implement an effective strategy for\nearly detection, real-time monitoring, and minimizing the overall impacts of\nfires on ecological balance and public safety. Recently, the rapid advancement\nof Artificial Intelligence (AI) and Computer Vision (CV) frameworks has\nsubstantially revolutionized the momentum for developing efficient fire\nmanagement systems. However, these systems extensively rely on the availability\nof adequate and high-quality fire and smoke data to create proficient Machine\nLearning (ML) methods for various tasks, such as detection and monitoring.\nAlthough fire and smoke datasets play a critical role in training, evaluating,\nand testing advanced Deep Learning (DL) models, a comprehensive review of the\nexisting datasets is still unexplored. For this purpose, we provide an in-depth\nreview to systematically analyze and evaluate fire and smoke datasets collected\nover the past 20 years. We investigate the characteristics of each dataset,\nincluding type, size, format, collection methods, and geographical diversities.\nWe also review and highlight the unique features of each dataset, such as\nimaging modalities (RGB, thermal, infrared) and their applicability for\ndifferent fire management tasks (classification, segmentation, detection).\nFurthermore, we summarize the strengths and weaknesses of each dataset and\ndiscuss their potential for advancing research and technology in fire\nmanagement. Ultimately, we conduct extensive experimental analyses across\ndifferent datasets using several state-of-the-art algorithms, such as\nResNet-50, DeepLab-V3, and YoloV8.\n", "link": "http://arxiv.org/abs/2503.14552v2", "date": "2025-07-08", "relevancy": 2.355, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eyes%20on%20the%20Environment%3A%20AI-Driven%20Analysis%20for%20Fire%20and%20Smoke%0A%20%20Classification%2C%20Segmentation%2C%20and%20Detection&body=Title%3A%20Eyes%20on%20the%20Environment%3A%20AI-Driven%20Analysis%20for%20Fire%20and%20Smoke%0A%20%20Classification%2C%20Segmentation%2C%20and%20Detection%0AAuthor%3A%20Sayed%20Pedram%20Haeri%20Boroujeni%20and%20Niloufar%20Mehrabi%20and%20Fatemeh%20Afghah%20and%20Connor%20Peter%20McGrath%20and%20Danish%20Bhatkar%20and%20Mithilesh%20Anil%20Biradar%20and%20Abolfazl%20Razi%0AAbstract%3A%20%20%20Fire%20and%20smoke%20phenomena%20pose%20a%20significant%20threat%20to%20the%20natural%0Aenvironment%2C%20ecosystems%2C%20and%20global%20economy%2C%20as%20well%20as%20human%20lives%20and%0Awildlife.%20In%20this%20particular%20circumstance%2C%20there%20is%20a%20demand%20for%20more%0Asophisticated%20and%20advanced%20technologies%20to%20implement%20an%20effective%20strategy%20for%0Aearly%20detection%2C%20real-time%20monitoring%2C%20and%20minimizing%20the%20overall%20impacts%20of%0Afires%20on%20ecological%20balance%20and%20public%20safety.%20Recently%2C%20the%20rapid%20advancement%0Aof%20Artificial%20Intelligence%20%28AI%29%20and%20Computer%20Vision%20%28CV%29%20frameworks%20has%0Asubstantially%20revolutionized%20the%20momentum%20for%20developing%20efficient%20fire%0Amanagement%20systems.%20However%2C%20these%20systems%20extensively%20rely%20on%20the%20availability%0Aof%20adequate%20and%20high-quality%20fire%20and%20smoke%20data%20to%20create%20proficient%20Machine%0ALearning%20%28ML%29%20methods%20for%20various%20tasks%2C%20such%20as%20detection%20and%20monitoring.%0AAlthough%20fire%20and%20smoke%20datasets%20play%20a%20critical%20role%20in%20training%2C%20evaluating%2C%0Aand%20testing%20advanced%20Deep%20Learning%20%28DL%29%20models%2C%20a%20comprehensive%20review%20of%20the%0Aexisting%20datasets%20is%20still%20unexplored.%20For%20this%20purpose%2C%20we%20provide%20an%20in-depth%0Areview%20to%20systematically%20analyze%20and%20evaluate%20fire%20and%20smoke%20datasets%20collected%0Aover%20the%20past%2020%20years.%20We%20investigate%20the%20characteristics%20of%20each%20dataset%2C%0Aincluding%20type%2C%20size%2C%20format%2C%20collection%20methods%2C%20and%20geographical%20diversities.%0AWe%20also%20review%20and%20highlight%20the%20unique%20features%20of%20each%20dataset%2C%20such%20as%0Aimaging%20modalities%20%28RGB%2C%20thermal%2C%20infrared%29%20and%20their%20applicability%20for%0Adifferent%20fire%20management%20tasks%20%28classification%2C%20segmentation%2C%20detection%29.%0AFurthermore%2C%20we%20summarize%20the%20strengths%20and%20weaknesses%20of%20each%20dataset%20and%0Adiscuss%20their%20potential%20for%20advancing%20research%20and%20technology%20in%20fire%0Amanagement.%20Ultimately%2C%20we%20conduct%20extensive%20experimental%20analyses%20across%0Adifferent%20datasets%20using%20several%20state-of-the-art%20algorithms%2C%20such%20as%0AResNet-50%2C%20DeepLab-V3%2C%20and%20YoloV8.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14552v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyes%2520on%2520the%2520Environment%253A%2520AI-Driven%2520Analysis%2520for%2520Fire%2520and%2520Smoke%250A%2520%2520Classification%252C%2520Segmentation%252C%2520and%2520Detection%26entry.906535625%3DSayed%2520Pedram%2520Haeri%2520Boroujeni%2520and%2520Niloufar%2520Mehrabi%2520and%2520Fatemeh%2520Afghah%2520and%2520Connor%2520Peter%2520McGrath%2520and%2520Danish%2520Bhatkar%2520and%2520Mithilesh%2520Anil%2520Biradar%2520and%2520Abolfazl%2520Razi%26entry.1292438233%3D%2520%2520Fire%2520and%2520smoke%2520phenomena%2520pose%2520a%2520significant%2520threat%2520to%2520the%2520natural%250Aenvironment%252C%2520ecosystems%252C%2520and%2520global%2520economy%252C%2520as%2520well%2520as%2520human%2520lives%2520and%250Awildlife.%2520In%2520this%2520particular%2520circumstance%252C%2520there%2520is%2520a%2520demand%2520for%2520more%250Asophisticated%2520and%2520advanced%2520technologies%2520to%2520implement%2520an%2520effective%2520strategy%2520for%250Aearly%2520detection%252C%2520real-time%2520monitoring%252C%2520and%2520minimizing%2520the%2520overall%2520impacts%2520of%250Afires%2520on%2520ecological%2520balance%2520and%2520public%2520safety.%2520Recently%252C%2520the%2520rapid%2520advancement%250Aof%2520Artificial%2520Intelligence%2520%2528AI%2529%2520and%2520Computer%2520Vision%2520%2528CV%2529%2520frameworks%2520has%250Asubstantially%2520revolutionized%2520the%2520momentum%2520for%2520developing%2520efficient%2520fire%250Amanagement%2520systems.%2520However%252C%2520these%2520systems%2520extensively%2520rely%2520on%2520the%2520availability%250Aof%2520adequate%2520and%2520high-quality%2520fire%2520and%2520smoke%2520data%2520to%2520create%2520proficient%2520Machine%250ALearning%2520%2528ML%2529%2520methods%2520for%2520various%2520tasks%252C%2520such%2520as%2520detection%2520and%2520monitoring.%250AAlthough%2520fire%2520and%2520smoke%2520datasets%2520play%2520a%2520critical%2520role%2520in%2520training%252C%2520evaluating%252C%250Aand%2520testing%2520advanced%2520Deep%2520Learning%2520%2528DL%2529%2520models%252C%2520a%2520comprehensive%2520review%2520of%2520the%250Aexisting%2520datasets%2520is%2520still%2520unexplored.%2520For%2520this%2520purpose%252C%2520we%2520provide%2520an%2520in-depth%250Areview%2520to%2520systematically%2520analyze%2520and%2520evaluate%2520fire%2520and%2520smoke%2520datasets%2520collected%250Aover%2520the%2520past%252020%2520years.%2520We%2520investigate%2520the%2520characteristics%2520of%2520each%2520dataset%252C%250Aincluding%2520type%252C%2520size%252C%2520format%252C%2520collection%2520methods%252C%2520and%2520geographical%2520diversities.%250AWe%2520also%2520review%2520and%2520highlight%2520the%2520unique%2520features%2520of%2520each%2520dataset%252C%2520such%2520as%250Aimaging%2520modalities%2520%2528RGB%252C%2520thermal%252C%2520infrared%2529%2520and%2520their%2520applicability%2520for%250Adifferent%2520fire%2520management%2520tasks%2520%2528classification%252C%2520segmentation%252C%2520detection%2529.%250AFurthermore%252C%2520we%2520summarize%2520the%2520strengths%2520and%2520weaknesses%2520of%2520each%2520dataset%2520and%250Adiscuss%2520their%2520potential%2520for%2520advancing%2520research%2520and%2520technology%2520in%2520fire%250Amanagement.%2520Ultimately%252C%2520we%2520conduct%2520extensive%2520experimental%2520analyses%2520across%250Adifferent%2520datasets%2520using%2520several%2520state-of-the-art%2520algorithms%252C%2520such%2520as%250AResNet-50%252C%2520DeepLab-V3%252C%2520and%2520YoloV8.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14552v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eyes%20on%20the%20Environment%3A%20AI-Driven%20Analysis%20for%20Fire%20and%20Smoke%0A%20%20Classification%2C%20Segmentation%2C%20and%20Detection&entry.906535625=Sayed%20Pedram%20Haeri%20Boroujeni%20and%20Niloufar%20Mehrabi%20and%20Fatemeh%20Afghah%20and%20Connor%20Peter%20McGrath%20and%20Danish%20Bhatkar%20and%20Mithilesh%20Anil%20Biradar%20and%20Abolfazl%20Razi&entry.1292438233=%20%20Fire%20and%20smoke%20phenomena%20pose%20a%20significant%20threat%20to%20the%20natural%0Aenvironment%2C%20ecosystems%2C%20and%20global%20economy%2C%20as%20well%20as%20human%20lives%20and%0Awildlife.%20In%20this%20particular%20circumstance%2C%20there%20is%20a%20demand%20for%20more%0Asophisticated%20and%20advanced%20technologies%20to%20implement%20an%20effective%20strategy%20for%0Aearly%20detection%2C%20real-time%20monitoring%2C%20and%20minimizing%20the%20overall%20impacts%20of%0Afires%20on%20ecological%20balance%20and%20public%20safety.%20Recently%2C%20the%20rapid%20advancement%0Aof%20Artificial%20Intelligence%20%28AI%29%20and%20Computer%20Vision%20%28CV%29%20frameworks%20has%0Asubstantially%20revolutionized%20the%20momentum%20for%20developing%20efficient%20fire%0Amanagement%20systems.%20However%2C%20these%20systems%20extensively%20rely%20on%20the%20availability%0Aof%20adequate%20and%20high-quality%20fire%20and%20smoke%20data%20to%20create%20proficient%20Machine%0ALearning%20%28ML%29%20methods%20for%20various%20tasks%2C%20such%20as%20detection%20and%20monitoring.%0AAlthough%20fire%20and%20smoke%20datasets%20play%20a%20critical%20role%20in%20training%2C%20evaluating%2C%0Aand%20testing%20advanced%20Deep%20Learning%20%28DL%29%20models%2C%20a%20comprehensive%20review%20of%20the%0Aexisting%20datasets%20is%20still%20unexplored.%20For%20this%20purpose%2C%20we%20provide%20an%20in-depth%0Areview%20to%20systematically%20analyze%20and%20evaluate%20fire%20and%20smoke%20datasets%20collected%0Aover%20the%20past%2020%20years.%20We%20investigate%20the%20characteristics%20of%20each%20dataset%2C%0Aincluding%20type%2C%20size%2C%20format%2C%20collection%20methods%2C%20and%20geographical%20diversities.%0AWe%20also%20review%20and%20highlight%20the%20unique%20features%20of%20each%20dataset%2C%20such%20as%0Aimaging%20modalities%20%28RGB%2C%20thermal%2C%20infrared%29%20and%20their%20applicability%20for%0Adifferent%20fire%20management%20tasks%20%28classification%2C%20segmentation%2C%20detection%29.%0AFurthermore%2C%20we%20summarize%20the%20strengths%20and%20weaknesses%20of%20each%20dataset%20and%0Adiscuss%20their%20potential%20for%20advancing%20research%20and%20technology%20in%20fire%0Amanagement.%20Ultimately%2C%20we%20conduct%20extensive%20experimental%20analyses%20across%0Adifferent%20datasets%20using%20several%20state-of-the-art%20algorithms%2C%20such%20as%0AResNet-50%2C%20DeepLab-V3%2C%20and%20YoloV8.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14552v2&entry.124074799=Read"},
{"title": "Skywork-R1V3 Technical Report", "author": "Wei Shen and Jiangbo Pei and Yi Peng and Xuchen Song and Yang Liu and Jian Peng and Haofeng Sun and Yunzhuo Hao and Peiyu Wang and Yahui Zhou", "abstract": "  We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.\n", "link": "http://arxiv.org/abs/2507.06167v1", "date": "2025-07-08", "relevancy": 2.2999, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skywork-R1V3%20Technical%20Report&body=Title%3A%20Skywork-R1V3%20Technical%20Report%0AAuthor%3A%20Wei%20Shen%20and%20Jiangbo%20Pei%20and%20Yi%20Peng%20and%20Xuchen%20Song%20and%20Yang%20Liu%20and%20Jian%20Peng%20and%20Haofeng%20Sun%20and%20Yunzhuo%20Hao%20and%20Peiyu%20Wang%20and%20Yahui%20Zhou%0AAbstract%3A%20%20%20We%20introduce%20Skywork-R1V3%2C%20an%20advanced%2C%20open-source%20vision-language%20model%0A%28VLM%29%20that%20pioneers%20a%20new%20approach%20to%20visual%20reasoning.%20Its%20key%20innovation%20lies%0Ain%20effectively%20transferring%20reasoning%20skills%20from%20text-only%20Large%20Language%0AModels%20%28LLMs%29%20to%20visual%20tasks.%20The%20strong%20performance%20of%20Skywork-R1V3%20primarily%0Astems%20from%20our%20elaborate%20post-training%20RL%20framework%2C%20which%20effectively%0Aactivates%20and%20enhances%20the%20model%27s%20reasoning%20ability%2C%20without%20the%20need%20for%0Aadditional%20continue%20pre-training.%20Through%20this%20framework%2C%20we%20further%20uncover%0Athe%20fundamental%20role%20of%20the%20connector%20module%20in%20achieving%20robust%20cross-modal%0Aalignment%20for%20multimodal%20reasoning%20models.%20In%20addition%2C%20we%20introduce%20a%20unique%0Aindicator%20of%20reasoning%20capability%2C%20the%20entropy%20of%20critical%20reasoning%20tokens%2C%0Awhich%20has%20proven%20highly%20effective%20for%20checkpoint%20selection%20during%20RL%20training.%0ASkywork-R1V3%20achieves%20state-of-the-art%20results%20on%20MMMU%2C%20significantly%20improving%0Afrom%2064.3%25%20to%2076.0%25.%20This%20performance%20matches%20entry-level%20human%20capabilities.%0ARemarkably%2C%20our%20RL-powered%20post-training%20approach%20enables%20even%20the%2038B%0Aparameter%20model%20to%20rival%20top%20closed-source%20VLMs.%20The%20implementation%0Asuccessfully%20transfers%20mathematical%20reasoning%20to%20other%20subject-related%0Areasoning%20tasks.%20We%20also%20include%20an%20analysis%20of%20curriculum%20learning%20and%0Areinforcement%20finetuning%20strategies%2C%20along%20with%20a%20broader%20discussion%20on%0Amultimodal%20reasoning.%20Skywork-R1V3%20represents%20a%20significant%20leap%20in%20multimodal%0Areasoning%2C%20showcasing%20RL%20as%20a%20powerful%20engine%20for%20advancing%20open-source%20VLM%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkywork-R1V3%2520Technical%2520Report%26entry.906535625%3DWei%2520Shen%2520and%2520Jiangbo%2520Pei%2520and%2520Yi%2520Peng%2520and%2520Xuchen%2520Song%2520and%2520Yang%2520Liu%2520and%2520Jian%2520Peng%2520and%2520Haofeng%2520Sun%2520and%2520Yunzhuo%2520Hao%2520and%2520Peiyu%2520Wang%2520and%2520Yahui%2520Zhou%26entry.1292438233%3D%2520%2520We%2520introduce%2520Skywork-R1V3%252C%2520an%2520advanced%252C%2520open-source%2520vision-language%2520model%250A%2528VLM%2529%2520that%2520pioneers%2520a%2520new%2520approach%2520to%2520visual%2520reasoning.%2520Its%2520key%2520innovation%2520lies%250Ain%2520effectively%2520transferring%2520reasoning%2520skills%2520from%2520text-only%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520to%2520visual%2520tasks.%2520The%2520strong%2520performance%2520of%2520Skywork-R1V3%2520primarily%250Astems%2520from%2520our%2520elaborate%2520post-training%2520RL%2520framework%252C%2520which%2520effectively%250Aactivates%2520and%2520enhances%2520the%2520model%2527s%2520reasoning%2520ability%252C%2520without%2520the%2520need%2520for%250Aadditional%2520continue%2520pre-training.%2520Through%2520this%2520framework%252C%2520we%2520further%2520uncover%250Athe%2520fundamental%2520role%2520of%2520the%2520connector%2520module%2520in%2520achieving%2520robust%2520cross-modal%250Aalignment%2520for%2520multimodal%2520reasoning%2520models.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520unique%250Aindicator%2520of%2520reasoning%2520capability%252C%2520the%2520entropy%2520of%2520critical%2520reasoning%2520tokens%252C%250Awhich%2520has%2520proven%2520highly%2520effective%2520for%2520checkpoint%2520selection%2520during%2520RL%2520training.%250ASkywork-R1V3%2520achieves%2520state-of-the-art%2520results%2520on%2520MMMU%252C%2520significantly%2520improving%250Afrom%252064.3%2525%2520to%252076.0%2525.%2520This%2520performance%2520matches%2520entry-level%2520human%2520capabilities.%250ARemarkably%252C%2520our%2520RL-powered%2520post-training%2520approach%2520enables%2520even%2520the%252038B%250Aparameter%2520model%2520to%2520rival%2520top%2520closed-source%2520VLMs.%2520The%2520implementation%250Asuccessfully%2520transfers%2520mathematical%2520reasoning%2520to%2520other%2520subject-related%250Areasoning%2520tasks.%2520We%2520also%2520include%2520an%2520analysis%2520of%2520curriculum%2520learning%2520and%250Areinforcement%2520finetuning%2520strategies%252C%2520along%2520with%2520a%2520broader%2520discussion%2520on%250Amultimodal%2520reasoning.%2520Skywork-R1V3%2520represents%2520a%2520significant%2520leap%2520in%2520multimodal%250Areasoning%252C%2520showcasing%2520RL%2520as%2520a%2520powerful%2520engine%2520for%2520advancing%2520open-source%2520VLM%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skywork-R1V3%20Technical%20Report&entry.906535625=Wei%20Shen%20and%20Jiangbo%20Pei%20and%20Yi%20Peng%20and%20Xuchen%20Song%20and%20Yang%20Liu%20and%20Jian%20Peng%20and%20Haofeng%20Sun%20and%20Yunzhuo%20Hao%20and%20Peiyu%20Wang%20and%20Yahui%20Zhou&entry.1292438233=%20%20We%20introduce%20Skywork-R1V3%2C%20an%20advanced%2C%20open-source%20vision-language%20model%0A%28VLM%29%20that%20pioneers%20a%20new%20approach%20to%20visual%20reasoning.%20Its%20key%20innovation%20lies%0Ain%20effectively%20transferring%20reasoning%20skills%20from%20text-only%20Large%20Language%0AModels%20%28LLMs%29%20to%20visual%20tasks.%20The%20strong%20performance%20of%20Skywork-R1V3%20primarily%0Astems%20from%20our%20elaborate%20post-training%20RL%20framework%2C%20which%20effectively%0Aactivates%20and%20enhances%20the%20model%27s%20reasoning%20ability%2C%20without%20the%20need%20for%0Aadditional%20continue%20pre-training.%20Through%20this%20framework%2C%20we%20further%20uncover%0Athe%20fundamental%20role%20of%20the%20connector%20module%20in%20achieving%20robust%20cross-modal%0Aalignment%20for%20multimodal%20reasoning%20models.%20In%20addition%2C%20we%20introduce%20a%20unique%0Aindicator%20of%20reasoning%20capability%2C%20the%20entropy%20of%20critical%20reasoning%20tokens%2C%0Awhich%20has%20proven%20highly%20effective%20for%20checkpoint%20selection%20during%20RL%20training.%0ASkywork-R1V3%20achieves%20state-of-the-art%20results%20on%20MMMU%2C%20significantly%20improving%0Afrom%2064.3%25%20to%2076.0%25.%20This%20performance%20matches%20entry-level%20human%20capabilities.%0ARemarkably%2C%20our%20RL-powered%20post-training%20approach%20enables%20even%20the%2038B%0Aparameter%20model%20to%20rival%20top%20closed-source%20VLMs.%20The%20implementation%0Asuccessfully%20transfers%20mathematical%20reasoning%20to%20other%20subject-related%0Areasoning%20tasks.%20We%20also%20include%20an%20analysis%20of%20curriculum%20learning%20and%0Areinforcement%20finetuning%20strategies%2C%20along%20with%20a%20broader%20discussion%20on%0Amultimodal%20reasoning.%20Skywork-R1V3%20represents%20a%20significant%20leap%20in%20multimodal%0Areasoning%2C%20showcasing%20RL%20as%20a%20powerful%20engine%20for%20advancing%20open-source%20VLM%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06167v1&entry.124074799=Read"},
{"title": "Benchmarking the CoW with the TopCoW Challenge: Topology-Aware\n  Anatomical Segmentation of the Circle of Willis for CTA and MRA", "author": "Kaiyuan Yang and Fabio Musio and Yihui Ma and Norman Juchler and Johannes C. Paetzold and Rami Al-Maskari and Luciano H\u00f6her and Hongwei Bran Li and Ibrahim Ethem Hamamci and Anjany Sekuboyina and Suprosanna Shit and Houjing Huang and Chinmay Prabhakar and Ezequiel de la Rosa and Bastian Wittmann and Diana Waldmannstetter and Florian Kofler and Fernando Navarro and Martin Menten and Ivan Ezhov and Daniel Rueckert and Iris N. Vos and Ynte M. Ruigrok and Birgitta K. Velthuis and Hugo J. Kuijf and Pengcheng Shi and Wei Liu and Ting Ma and Maximilian R. Rokuss and Yannick Kirchhoff and Fabian Isensee and Klaus Maier-Hein and Chengcheng Zhu and Huilin Zhao and Philippe Bijlenga and Julien H\u00e4mmerli and Catherine Wurster and Laura Westphal and Jeroen Bisschop and Elisa Colombo and Hakim Baazaoui and Hannah-Lea Handelsmann and Andrew Makmur and James Hallinan and Amrish Soundararajan and Bene Wiestler and Jan S. Kirschke and Roland Wiest and Emmanuel Montagnon and Laurent Letourneau-Guillon and Kwanseok Oh and Dahye Lee and Adam Hilbert and Orhun Utku Aydin and Dimitrios Rallios and Jana Rieger and Satoru Tanioka and Alexander Koch and Dietmar Frey and Abdul Qayyum and Moona Mazher and Steven Niederer and Nico Disch and Julius Holzschuh and Dominic LaBella and Francesco Galati and Daniele Falcetta and Maria A. Zuluaga and Chaolong Lin and Haoran Zhao and Zehan Zhang and Minghui Zhang and Xin You and Hanxiao Zhang and Guang-Zhong Yang and Yun Gu and Sinyoung Ra and Jongyun Hwang and Hyunjin Park and Junqiang Chen and Marek Wodzinski and Henning M\u00fcller and Nesrin Mansouri and Florent Autrusseau and Cansu Yal\u00e7in and Rachika E. Hamadache and Clara Lisazo and Joaquim Salvi and Adri\u00e0 Casamitjana and Xavier Llad\u00f3 and Uma Maria Lal-Trehan Estrada and Valeriia Abramova and Luca Giancardo and Arnau Oliver and Paula Casademunt and Adrian Galdran and Matteo Delucchi and Jialu Liu and Haibin Huang and Yue Cui and Zehang Lin and Yusheng Liu and Shunzhi Zhu and Tatsat R. Patel and Adnan H. Siddiqui and Vincent M. Tutino and Maysam Orouskhani and Huayu Wang and Mahmud Mossa-Basha and Yuki Sato and Sven Hirsch and Susanne Wegener and Bjoern Menze", "abstract": "  The Circle of Willis (CoW) is an important network of arteries connecting\nmajor circulations of the brain. Its vascular architecture is believed to\naffect the risk, severity, and clinical outcome of serious neurovascular\ndiseases. However, characterizing the highly variable CoW anatomy is still a\nmanual and time-consuming expert task. The CoW is usually imaged by two\nnon-invasive angiographic imaging modalities, magnetic resonance angiography\n(MRA) and computed tomography angiography (CTA), but there exist limited\ndatasets with annotations on CoW anatomy, especially for CTA. Therefore, we\norganized the TopCoW challenge with the release of an annotated CoW dataset.\nThe TopCoW dataset is the first public dataset with voxel-level annotations for\n13 CoW vessel components, enabled by virtual reality technology. It is also the\nfirst large dataset using 200 pairs of MRA and CTA from the same patients. As\npart of the benchmark, we invited submissions worldwide and attracted over 250\nregistered participants from six continents. The submissions were evaluated on\nboth internal and external test datasets of 226 scans from over five centers.\nThe top performing teams achieved over 90% Dice scores at segmenting the CoW\ncomponents, over 80% F1 scores at detecting key CoW components, and over 70%\nbalanced accuracy at classifying CoW variants for nearly all test sets. The\nbest algorithms also showed clinical potential in classifying fetal-type\nposterior cerebral artery and locating aneurysms with CoW anatomy. TopCoW\ndemonstrated the utility and versatility of CoW segmentation algorithms for a\nwide range of downstream clinical applications with explainability. The\nannotated datasets and best performing algorithms have been released as public\nZenodo records to foster further methodological development and clinical tool\nbuilding.\n", "link": "http://arxiv.org/abs/2312.17670v4", "date": "2025-07-08", "relevancy": 2.2653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20the%20CoW%20with%20the%20TopCoW%20Challenge%3A%20Topology-Aware%0A%20%20Anatomical%20Segmentation%20of%20the%20Circle%20of%20Willis%20for%20CTA%20and%20MRA&body=Title%3A%20Benchmarking%20the%20CoW%20with%20the%20TopCoW%20Challenge%3A%20Topology-Aware%0A%20%20Anatomical%20Segmentation%20of%20the%20Circle%20of%20Willis%20for%20CTA%20and%20MRA%0AAuthor%3A%20Kaiyuan%20Yang%20and%20Fabio%20Musio%20and%20Yihui%20Ma%20and%20Norman%20Juchler%20and%20Johannes%20C.%20Paetzold%20and%20Rami%20Al-Maskari%20and%20Luciano%20H%C3%B6her%20and%20Hongwei%20Bran%20Li%20and%20Ibrahim%20Ethem%20Hamamci%20and%20Anjany%20Sekuboyina%20and%20Suprosanna%20Shit%20and%20Houjing%20Huang%20and%20Chinmay%20Prabhakar%20and%20Ezequiel%20de%20la%20Rosa%20and%20Bastian%20Wittmann%20and%20Diana%20Waldmannstetter%20and%20Florian%20Kofler%20and%20Fernando%20Navarro%20and%20Martin%20Menten%20and%20Ivan%20Ezhov%20and%20Daniel%20Rueckert%20and%20Iris%20N.%20Vos%20and%20Ynte%20M.%20Ruigrok%20and%20Birgitta%20K.%20Velthuis%20and%20Hugo%20J.%20Kuijf%20and%20Pengcheng%20Shi%20and%20Wei%20Liu%20and%20Ting%20Ma%20and%20Maximilian%20R.%20Rokuss%20and%20Yannick%20Kirchhoff%20and%20Fabian%20Isensee%20and%20Klaus%20Maier-Hein%20and%20Chengcheng%20Zhu%20and%20Huilin%20Zhao%20and%20Philippe%20Bijlenga%20and%20Julien%20H%C3%A4mmerli%20and%20Catherine%20Wurster%20and%20Laura%20Westphal%20and%20Jeroen%20Bisschop%20and%20Elisa%20Colombo%20and%20Hakim%20Baazaoui%20and%20Hannah-Lea%20Handelsmann%20and%20Andrew%20Makmur%20and%20James%20Hallinan%20and%20Amrish%20Soundararajan%20and%20Bene%20Wiestler%20and%20Jan%20S.%20Kirschke%20and%20Roland%20Wiest%20and%20Emmanuel%20Montagnon%20and%20Laurent%20Letourneau-Guillon%20and%20Kwanseok%20Oh%20and%20Dahye%20Lee%20and%20Adam%20Hilbert%20and%20Orhun%20Utku%20Aydin%20and%20Dimitrios%20Rallios%20and%20Jana%20Rieger%20and%20Satoru%20Tanioka%20and%20Alexander%20Koch%20and%20Dietmar%20Frey%20and%20Abdul%20Qayyum%20and%20Moona%20Mazher%20and%20Steven%20Niederer%20and%20Nico%20Disch%20and%20Julius%20Holzschuh%20and%20Dominic%20LaBella%20and%20Francesco%20Galati%20and%20Daniele%20Falcetta%20and%20Maria%20A.%20Zuluaga%20and%20Chaolong%20Lin%20and%20Haoran%20Zhao%20and%20Zehan%20Zhang%20and%20Minghui%20Zhang%20and%20Xin%20You%20and%20Hanxiao%20Zhang%20and%20Guang-Zhong%20Yang%20and%20Yun%20Gu%20and%20Sinyoung%20Ra%20and%20Jongyun%20Hwang%20and%20Hyunjin%20Park%20and%20Junqiang%20Chen%20and%20Marek%20Wodzinski%20and%20Henning%20M%C3%BCller%20and%20Nesrin%20Mansouri%20and%20Florent%20Autrusseau%20and%20Cansu%20Yal%C3%A7in%20and%20Rachika%20E.%20Hamadache%20and%20Clara%20Lisazo%20and%20Joaquim%20Salvi%20and%20Adri%C3%A0%20Casamitjana%20and%20Xavier%20Llad%C3%B3%20and%20Uma%20Maria%20Lal-Trehan%20Estrada%20and%20Valeriia%20Abramova%20and%20Luca%20Giancardo%20and%20Arnau%20Oliver%20and%20Paula%20Casademunt%20and%20Adrian%20Galdran%20and%20Matteo%20Delucchi%20and%20Jialu%20Liu%20and%20Haibin%20Huang%20and%20Yue%20Cui%20and%20Zehang%20Lin%20and%20Yusheng%20Liu%20and%20Shunzhi%20Zhu%20and%20Tatsat%20R.%20Patel%20and%20Adnan%20H.%20Siddiqui%20and%20Vincent%20M.%20Tutino%20and%20Maysam%20Orouskhani%20and%20Huayu%20Wang%20and%20Mahmud%20Mossa-Basha%20and%20Yuki%20Sato%20and%20Sven%20Hirsch%20and%20Susanne%20Wegener%20and%20Bjoern%20Menze%0AAbstract%3A%20%20%20The%20Circle%20of%20Willis%20%28CoW%29%20is%20an%20important%20network%20of%20arteries%20connecting%0Amajor%20circulations%20of%20the%20brain.%20Its%20vascular%20architecture%20is%20believed%20to%0Aaffect%20the%20risk%2C%20severity%2C%20and%20clinical%20outcome%20of%20serious%20neurovascular%0Adiseases.%20However%2C%20characterizing%20the%20highly%20variable%20CoW%20anatomy%20is%20still%20a%0Amanual%20and%20time-consuming%20expert%20task.%20The%20CoW%20is%20usually%20imaged%20by%20two%0Anon-invasive%20angiographic%20imaging%20modalities%2C%20magnetic%20resonance%20angiography%0A%28MRA%29%20and%20computed%20tomography%20angiography%20%28CTA%29%2C%20but%20there%20exist%20limited%0Adatasets%20with%20annotations%20on%20CoW%20anatomy%2C%20especially%20for%20CTA.%20Therefore%2C%20we%0Aorganized%20the%20TopCoW%20challenge%20with%20the%20release%20of%20an%20annotated%20CoW%20dataset.%0AThe%20TopCoW%20dataset%20is%20the%20first%20public%20dataset%20with%20voxel-level%20annotations%20for%0A13%20CoW%20vessel%20components%2C%20enabled%20by%20virtual%20reality%20technology.%20It%20is%20also%20the%0Afirst%20large%20dataset%20using%20200%20pairs%20of%20MRA%20and%20CTA%20from%20the%20same%20patients.%20As%0Apart%20of%20the%20benchmark%2C%20we%20invited%20submissions%20worldwide%20and%20attracted%20over%20250%0Aregistered%20participants%20from%20six%20continents.%20The%20submissions%20were%20evaluated%20on%0Aboth%20internal%20and%20external%20test%20datasets%20of%20226%20scans%20from%20over%20five%20centers.%0AThe%20top%20performing%20teams%20achieved%20over%2090%25%20Dice%20scores%20at%20segmenting%20the%20CoW%0Acomponents%2C%20over%2080%25%20F1%20scores%20at%20detecting%20key%20CoW%20components%2C%20and%20over%2070%25%0Abalanced%20accuracy%20at%20classifying%20CoW%20variants%20for%20nearly%20all%20test%20sets.%20The%0Abest%20algorithms%20also%20showed%20clinical%20potential%20in%20classifying%20fetal-type%0Aposterior%20cerebral%20artery%20and%20locating%20aneurysms%20with%20CoW%20anatomy.%20TopCoW%0Ademonstrated%20the%20utility%20and%20versatility%20of%20CoW%20segmentation%20algorithms%20for%20a%0Awide%20range%20of%20downstream%20clinical%20applications%20with%20explainability.%20The%0Aannotated%20datasets%20and%20best%20performing%20algorithms%20have%20been%20released%20as%20public%0AZenodo%20records%20to%20foster%20further%20methodological%20development%20and%20clinical%20tool%0Abuilding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17670v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520the%2520CoW%2520with%2520the%2520TopCoW%2520Challenge%253A%2520Topology-Aware%250A%2520%2520Anatomical%2520Segmentation%2520of%2520the%2520Circle%2520of%2520Willis%2520for%2520CTA%2520and%2520MRA%26entry.906535625%3DKaiyuan%2520Yang%2520and%2520Fabio%2520Musio%2520and%2520Yihui%2520Ma%2520and%2520Norman%2520Juchler%2520and%2520Johannes%2520C.%2520Paetzold%2520and%2520Rami%2520Al-Maskari%2520and%2520Luciano%2520H%25C3%25B6her%2520and%2520Hongwei%2520Bran%2520Li%2520and%2520Ibrahim%2520Ethem%2520Hamamci%2520and%2520Anjany%2520Sekuboyina%2520and%2520Suprosanna%2520Shit%2520and%2520Houjing%2520Huang%2520and%2520Chinmay%2520Prabhakar%2520and%2520Ezequiel%2520de%2520la%2520Rosa%2520and%2520Bastian%2520Wittmann%2520and%2520Diana%2520Waldmannstetter%2520and%2520Florian%2520Kofler%2520and%2520Fernando%2520Navarro%2520and%2520Martin%2520Menten%2520and%2520Ivan%2520Ezhov%2520and%2520Daniel%2520Rueckert%2520and%2520Iris%2520N.%2520Vos%2520and%2520Ynte%2520M.%2520Ruigrok%2520and%2520Birgitta%2520K.%2520Velthuis%2520and%2520Hugo%2520J.%2520Kuijf%2520and%2520Pengcheng%2520Shi%2520and%2520Wei%2520Liu%2520and%2520Ting%2520Ma%2520and%2520Maximilian%2520R.%2520Rokuss%2520and%2520Yannick%2520Kirchhoff%2520and%2520Fabian%2520Isensee%2520and%2520Klaus%2520Maier-Hein%2520and%2520Chengcheng%2520Zhu%2520and%2520Huilin%2520Zhao%2520and%2520Philippe%2520Bijlenga%2520and%2520Julien%2520H%25C3%25A4mmerli%2520and%2520Catherine%2520Wurster%2520and%2520Laura%2520Westphal%2520and%2520Jeroen%2520Bisschop%2520and%2520Elisa%2520Colombo%2520and%2520Hakim%2520Baazaoui%2520and%2520Hannah-Lea%2520Handelsmann%2520and%2520Andrew%2520Makmur%2520and%2520James%2520Hallinan%2520and%2520Amrish%2520Soundararajan%2520and%2520Bene%2520Wiestler%2520and%2520Jan%2520S.%2520Kirschke%2520and%2520Roland%2520Wiest%2520and%2520Emmanuel%2520Montagnon%2520and%2520Laurent%2520Letourneau-Guillon%2520and%2520Kwanseok%2520Oh%2520and%2520Dahye%2520Lee%2520and%2520Adam%2520Hilbert%2520and%2520Orhun%2520Utku%2520Aydin%2520and%2520Dimitrios%2520Rallios%2520and%2520Jana%2520Rieger%2520and%2520Satoru%2520Tanioka%2520and%2520Alexander%2520Koch%2520and%2520Dietmar%2520Frey%2520and%2520Abdul%2520Qayyum%2520and%2520Moona%2520Mazher%2520and%2520Steven%2520Niederer%2520and%2520Nico%2520Disch%2520and%2520Julius%2520Holzschuh%2520and%2520Dominic%2520LaBella%2520and%2520Francesco%2520Galati%2520and%2520Daniele%2520Falcetta%2520and%2520Maria%2520A.%2520Zuluaga%2520and%2520Chaolong%2520Lin%2520and%2520Haoran%2520Zhao%2520and%2520Zehan%2520Zhang%2520and%2520Minghui%2520Zhang%2520and%2520Xin%2520You%2520and%2520Hanxiao%2520Zhang%2520and%2520Guang-Zhong%2520Yang%2520and%2520Yun%2520Gu%2520and%2520Sinyoung%2520Ra%2520and%2520Jongyun%2520Hwang%2520and%2520Hyunjin%2520Park%2520and%2520Junqiang%2520Chen%2520and%2520Marek%2520Wodzinski%2520and%2520Henning%2520M%25C3%25BCller%2520and%2520Nesrin%2520Mansouri%2520and%2520Florent%2520Autrusseau%2520and%2520Cansu%2520Yal%25C3%25A7in%2520and%2520Rachika%2520E.%2520Hamadache%2520and%2520Clara%2520Lisazo%2520and%2520Joaquim%2520Salvi%2520and%2520Adri%25C3%25A0%2520Casamitjana%2520and%2520Xavier%2520Llad%25C3%25B3%2520and%2520Uma%2520Maria%2520Lal-Trehan%2520Estrada%2520and%2520Valeriia%2520Abramova%2520and%2520Luca%2520Giancardo%2520and%2520Arnau%2520Oliver%2520and%2520Paula%2520Casademunt%2520and%2520Adrian%2520Galdran%2520and%2520Matteo%2520Delucchi%2520and%2520Jialu%2520Liu%2520and%2520Haibin%2520Huang%2520and%2520Yue%2520Cui%2520and%2520Zehang%2520Lin%2520and%2520Yusheng%2520Liu%2520and%2520Shunzhi%2520Zhu%2520and%2520Tatsat%2520R.%2520Patel%2520and%2520Adnan%2520H.%2520Siddiqui%2520and%2520Vincent%2520M.%2520Tutino%2520and%2520Maysam%2520Orouskhani%2520and%2520Huayu%2520Wang%2520and%2520Mahmud%2520Mossa-Basha%2520and%2520Yuki%2520Sato%2520and%2520Sven%2520Hirsch%2520and%2520Susanne%2520Wegener%2520and%2520Bjoern%2520Menze%26entry.1292438233%3D%2520%2520The%2520Circle%2520of%2520Willis%2520%2528CoW%2529%2520is%2520an%2520important%2520network%2520of%2520arteries%2520connecting%250Amajor%2520circulations%2520of%2520the%2520brain.%2520Its%2520vascular%2520architecture%2520is%2520believed%2520to%250Aaffect%2520the%2520risk%252C%2520severity%252C%2520and%2520clinical%2520outcome%2520of%2520serious%2520neurovascular%250Adiseases.%2520However%252C%2520characterizing%2520the%2520highly%2520variable%2520CoW%2520anatomy%2520is%2520still%2520a%250Amanual%2520and%2520time-consuming%2520expert%2520task.%2520The%2520CoW%2520is%2520usually%2520imaged%2520by%2520two%250Anon-invasive%2520angiographic%2520imaging%2520modalities%252C%2520magnetic%2520resonance%2520angiography%250A%2528MRA%2529%2520and%2520computed%2520tomography%2520angiography%2520%2528CTA%2529%252C%2520but%2520there%2520exist%2520limited%250Adatasets%2520with%2520annotations%2520on%2520CoW%2520anatomy%252C%2520especially%2520for%2520CTA.%2520Therefore%252C%2520we%250Aorganized%2520the%2520TopCoW%2520challenge%2520with%2520the%2520release%2520of%2520an%2520annotated%2520CoW%2520dataset.%250AThe%2520TopCoW%2520dataset%2520is%2520the%2520first%2520public%2520dataset%2520with%2520voxel-level%2520annotations%2520for%250A13%2520CoW%2520vessel%2520components%252C%2520enabled%2520by%2520virtual%2520reality%2520technology.%2520It%2520is%2520also%2520the%250Afirst%2520large%2520dataset%2520using%2520200%2520pairs%2520of%2520MRA%2520and%2520CTA%2520from%2520the%2520same%2520patients.%2520As%250Apart%2520of%2520the%2520benchmark%252C%2520we%2520invited%2520submissions%2520worldwide%2520and%2520attracted%2520over%2520250%250Aregistered%2520participants%2520from%2520six%2520continents.%2520The%2520submissions%2520were%2520evaluated%2520on%250Aboth%2520internal%2520and%2520external%2520test%2520datasets%2520of%2520226%2520scans%2520from%2520over%2520five%2520centers.%250AThe%2520top%2520performing%2520teams%2520achieved%2520over%252090%2525%2520Dice%2520scores%2520at%2520segmenting%2520the%2520CoW%250Acomponents%252C%2520over%252080%2525%2520F1%2520scores%2520at%2520detecting%2520key%2520CoW%2520components%252C%2520and%2520over%252070%2525%250Abalanced%2520accuracy%2520at%2520classifying%2520CoW%2520variants%2520for%2520nearly%2520all%2520test%2520sets.%2520The%250Abest%2520algorithms%2520also%2520showed%2520clinical%2520potential%2520in%2520classifying%2520fetal-type%250Aposterior%2520cerebral%2520artery%2520and%2520locating%2520aneurysms%2520with%2520CoW%2520anatomy.%2520TopCoW%250Ademonstrated%2520the%2520utility%2520and%2520versatility%2520of%2520CoW%2520segmentation%2520algorithms%2520for%2520a%250Awide%2520range%2520of%2520downstream%2520clinical%2520applications%2520with%2520explainability.%2520The%250Aannotated%2520datasets%2520and%2520best%2520performing%2520algorithms%2520have%2520been%2520released%2520as%2520public%250AZenodo%2520records%2520to%2520foster%2520further%2520methodological%2520development%2520and%2520clinical%2520tool%250Abuilding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.17670v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20the%20CoW%20with%20the%20TopCoW%20Challenge%3A%20Topology-Aware%0A%20%20Anatomical%20Segmentation%20of%20the%20Circle%20of%20Willis%20for%20CTA%20and%20MRA&entry.906535625=Kaiyuan%20Yang%20and%20Fabio%20Musio%20and%20Yihui%20Ma%20and%20Norman%20Juchler%20and%20Johannes%20C.%20Paetzold%20and%20Rami%20Al-Maskari%20and%20Luciano%20H%C3%B6her%20and%20Hongwei%20Bran%20Li%20and%20Ibrahim%20Ethem%20Hamamci%20and%20Anjany%20Sekuboyina%20and%20Suprosanna%20Shit%20and%20Houjing%20Huang%20and%20Chinmay%20Prabhakar%20and%20Ezequiel%20de%20la%20Rosa%20and%20Bastian%20Wittmann%20and%20Diana%20Waldmannstetter%20and%20Florian%20Kofler%20and%20Fernando%20Navarro%20and%20Martin%20Menten%20and%20Ivan%20Ezhov%20and%20Daniel%20Rueckert%20and%20Iris%20N.%20Vos%20and%20Ynte%20M.%20Ruigrok%20and%20Birgitta%20K.%20Velthuis%20and%20Hugo%20J.%20Kuijf%20and%20Pengcheng%20Shi%20and%20Wei%20Liu%20and%20Ting%20Ma%20and%20Maximilian%20R.%20Rokuss%20and%20Yannick%20Kirchhoff%20and%20Fabian%20Isensee%20and%20Klaus%20Maier-Hein%20and%20Chengcheng%20Zhu%20and%20Huilin%20Zhao%20and%20Philippe%20Bijlenga%20and%20Julien%20H%C3%A4mmerli%20and%20Catherine%20Wurster%20and%20Laura%20Westphal%20and%20Jeroen%20Bisschop%20and%20Elisa%20Colombo%20and%20Hakim%20Baazaoui%20and%20Hannah-Lea%20Handelsmann%20and%20Andrew%20Makmur%20and%20James%20Hallinan%20and%20Amrish%20Soundararajan%20and%20Bene%20Wiestler%20and%20Jan%20S.%20Kirschke%20and%20Roland%20Wiest%20and%20Emmanuel%20Montagnon%20and%20Laurent%20Letourneau-Guillon%20and%20Kwanseok%20Oh%20and%20Dahye%20Lee%20and%20Adam%20Hilbert%20and%20Orhun%20Utku%20Aydin%20and%20Dimitrios%20Rallios%20and%20Jana%20Rieger%20and%20Satoru%20Tanioka%20and%20Alexander%20Koch%20and%20Dietmar%20Frey%20and%20Abdul%20Qayyum%20and%20Moona%20Mazher%20and%20Steven%20Niederer%20and%20Nico%20Disch%20and%20Julius%20Holzschuh%20and%20Dominic%20LaBella%20and%20Francesco%20Galati%20and%20Daniele%20Falcetta%20and%20Maria%20A.%20Zuluaga%20and%20Chaolong%20Lin%20and%20Haoran%20Zhao%20and%20Zehan%20Zhang%20and%20Minghui%20Zhang%20and%20Xin%20You%20and%20Hanxiao%20Zhang%20and%20Guang-Zhong%20Yang%20and%20Yun%20Gu%20and%20Sinyoung%20Ra%20and%20Jongyun%20Hwang%20and%20Hyunjin%20Park%20and%20Junqiang%20Chen%20and%20Marek%20Wodzinski%20and%20Henning%20M%C3%BCller%20and%20Nesrin%20Mansouri%20and%20Florent%20Autrusseau%20and%20Cansu%20Yal%C3%A7in%20and%20Rachika%20E.%20Hamadache%20and%20Clara%20Lisazo%20and%20Joaquim%20Salvi%20and%20Adri%C3%A0%20Casamitjana%20and%20Xavier%20Llad%C3%B3%20and%20Uma%20Maria%20Lal-Trehan%20Estrada%20and%20Valeriia%20Abramova%20and%20Luca%20Giancardo%20and%20Arnau%20Oliver%20and%20Paula%20Casademunt%20and%20Adrian%20Galdran%20and%20Matteo%20Delucchi%20and%20Jialu%20Liu%20and%20Haibin%20Huang%20and%20Yue%20Cui%20and%20Zehang%20Lin%20and%20Yusheng%20Liu%20and%20Shunzhi%20Zhu%20and%20Tatsat%20R.%20Patel%20and%20Adnan%20H.%20Siddiqui%20and%20Vincent%20M.%20Tutino%20and%20Maysam%20Orouskhani%20and%20Huayu%20Wang%20and%20Mahmud%20Mossa-Basha%20and%20Yuki%20Sato%20and%20Sven%20Hirsch%20and%20Susanne%20Wegener%20and%20Bjoern%20Menze&entry.1292438233=%20%20The%20Circle%20of%20Willis%20%28CoW%29%20is%20an%20important%20network%20of%20arteries%20connecting%0Amajor%20circulations%20of%20the%20brain.%20Its%20vascular%20architecture%20is%20believed%20to%0Aaffect%20the%20risk%2C%20severity%2C%20and%20clinical%20outcome%20of%20serious%20neurovascular%0Adiseases.%20However%2C%20characterizing%20the%20highly%20variable%20CoW%20anatomy%20is%20still%20a%0Amanual%20and%20time-consuming%20expert%20task.%20The%20CoW%20is%20usually%20imaged%20by%20two%0Anon-invasive%20angiographic%20imaging%20modalities%2C%20magnetic%20resonance%20angiography%0A%28MRA%29%20and%20computed%20tomography%20angiography%20%28CTA%29%2C%20but%20there%20exist%20limited%0Adatasets%20with%20annotations%20on%20CoW%20anatomy%2C%20especially%20for%20CTA.%20Therefore%2C%20we%0Aorganized%20the%20TopCoW%20challenge%20with%20the%20release%20of%20an%20annotated%20CoW%20dataset.%0AThe%20TopCoW%20dataset%20is%20the%20first%20public%20dataset%20with%20voxel-level%20annotations%20for%0A13%20CoW%20vessel%20components%2C%20enabled%20by%20virtual%20reality%20technology.%20It%20is%20also%20the%0Afirst%20large%20dataset%20using%20200%20pairs%20of%20MRA%20and%20CTA%20from%20the%20same%20patients.%20As%0Apart%20of%20the%20benchmark%2C%20we%20invited%20submissions%20worldwide%20and%20attracted%20over%20250%0Aregistered%20participants%20from%20six%20continents.%20The%20submissions%20were%20evaluated%20on%0Aboth%20internal%20and%20external%20test%20datasets%20of%20226%20scans%20from%20over%20five%20centers.%0AThe%20top%20performing%20teams%20achieved%20over%2090%25%20Dice%20scores%20at%20segmenting%20the%20CoW%0Acomponents%2C%20over%2080%25%20F1%20scores%20at%20detecting%20key%20CoW%20components%2C%20and%20over%2070%25%0Abalanced%20accuracy%20at%20classifying%20CoW%20variants%20for%20nearly%20all%20test%20sets.%20The%0Abest%20algorithms%20also%20showed%20clinical%20potential%20in%20classifying%20fetal-type%0Aposterior%20cerebral%20artery%20and%20locating%20aneurysms%20with%20CoW%20anatomy.%20TopCoW%0Ademonstrated%20the%20utility%20and%20versatility%20of%20CoW%20segmentation%20algorithms%20for%20a%0Awide%20range%20of%20downstream%20clinical%20applications%20with%20explainability.%20The%0Aannotated%20datasets%20and%20best%20performing%20algorithms%20have%20been%20released%20as%20public%0AZenodo%20records%20to%20foster%20further%20methodological%20development%20and%20clinical%20tool%0Abuilding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17670v4&entry.124074799=Read"},
{"title": "LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising\n  with Vision-language Models", "author": "Zhihao Chen and Tao Chen and Chenhui Wang and Qi Gao and Huidong Xie and Chuang Niu and Ge Wang and Hongming Shan", "abstract": "  Low-dose computed tomography (LDCT) reduces radiation exposure but often\ndegrades image quality, potentially compromising diagnostic accuracy. Existing\ndeep learning-based denoising methods focus primarily on pixel-level mappings,\noverlooking the potential benefits of high-level semantic guidance. Recent\nadvances in vision-language models (VLMs) suggest that language can serve as a\npowerful tool for capturing structured semantic information, offering new\nopportunities to improve LDCT reconstruction. In this paper, we introduce\nLangMamba, a Language-driven Mamba framework for LDCT denoising that leverages\nVLM-derived representations to enhance supervision from normal-dose CT (NDCT).\nLangMamba follows a two-stage learning strategy. First, we pre-train a\nLanguage-guided AutoEncoder (LangAE) that leverages frozen VLMs to map NDCT\nimages into a semantic space enriched with anatomical information. Second, we\nsynergize LangAE with two key components to guide LDCT denoising:\nSemantic-Enhanced Efficient Denoiser (SEED), which enhances NDCT-relevant local\nsemantic while capturing global features with efficient Mamba mechanism, and\nLanguage-engaged Dual-space Alignment (LangDA) Loss, which ensures that\ndenoised images align with NDCT in both perceptual and semantic spaces.\nExtensive experiments on two public datasets demonstrate that LangMamba\noutperforms conventional state-of-the-art methods, significantly improving\ndetail preservation and visual fidelity. Remarkably, LangAE exhibits strong\ngeneralizability to unseen datasets, thereby reducing training costs.\nFurthermore, LangDA loss improves explainability by integrating language-guided\ninsights into image reconstruction and offers a plug-and-play fashion. Our\nfindings shed new light on the potential of language as a supervisory signal to\nadvance LDCT denoising. The code is publicly available on\nhttps://github.com/hao1635/LangMamba.\n", "link": "http://arxiv.org/abs/2507.06140v1", "date": "2025-07-08", "relevancy": 2.2482, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5639}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5627}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LangMamba%3A%20A%20Language-driven%20Mamba%20Framework%20for%20Low-dose%20CT%20Denoising%0A%20%20with%20Vision-language%20Models&body=Title%3A%20LangMamba%3A%20A%20Language-driven%20Mamba%20Framework%20for%20Low-dose%20CT%20Denoising%0A%20%20with%20Vision-language%20Models%0AAuthor%3A%20Zhihao%20Chen%20and%20Tao%20Chen%20and%20Chenhui%20Wang%20and%20Qi%20Gao%20and%20Huidong%20Xie%20and%20Chuang%20Niu%20and%20Ge%20Wang%20and%20Hongming%20Shan%0AAbstract%3A%20%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20reduces%20radiation%20exposure%20but%20often%0Adegrades%20image%20quality%2C%20potentially%20compromising%20diagnostic%20accuracy.%20Existing%0Adeep%20learning-based%20denoising%20methods%20focus%20primarily%20on%20pixel-level%20mappings%2C%0Aoverlooking%20the%20potential%20benefits%20of%20high-level%20semantic%20guidance.%20Recent%0Aadvances%20in%20vision-language%20models%20%28VLMs%29%20suggest%20that%20language%20can%20serve%20as%20a%0Apowerful%20tool%20for%20capturing%20structured%20semantic%20information%2C%20offering%20new%0Aopportunities%20to%20improve%20LDCT%20reconstruction.%20In%20this%20paper%2C%20we%20introduce%0ALangMamba%2C%20a%20Language-driven%20Mamba%20framework%20for%20LDCT%20denoising%20that%20leverages%0AVLM-derived%20representations%20to%20enhance%20supervision%20from%20normal-dose%20CT%20%28NDCT%29.%0ALangMamba%20follows%20a%20two-stage%20learning%20strategy.%20First%2C%20we%20pre-train%20a%0ALanguage-guided%20AutoEncoder%20%28LangAE%29%20that%20leverages%20frozen%20VLMs%20to%20map%20NDCT%0Aimages%20into%20a%20semantic%20space%20enriched%20with%20anatomical%20information.%20Second%2C%20we%0Asynergize%20LangAE%20with%20two%20key%20components%20to%20guide%20LDCT%20denoising%3A%0ASemantic-Enhanced%20Efficient%20Denoiser%20%28SEED%29%2C%20which%20enhances%20NDCT-relevant%20local%0Asemantic%20while%20capturing%20global%20features%20with%20efficient%20Mamba%20mechanism%2C%20and%0ALanguage-engaged%20Dual-space%20Alignment%20%28LangDA%29%20Loss%2C%20which%20ensures%20that%0Adenoised%20images%20align%20with%20NDCT%20in%20both%20perceptual%20and%20semantic%20spaces.%0AExtensive%20experiments%20on%20two%20public%20datasets%20demonstrate%20that%20LangMamba%0Aoutperforms%20conventional%20state-of-the-art%20methods%2C%20significantly%20improving%0Adetail%20preservation%20and%20visual%20fidelity.%20Remarkably%2C%20LangAE%20exhibits%20strong%0Ageneralizability%20to%20unseen%20datasets%2C%20thereby%20reducing%20training%20costs.%0AFurthermore%2C%20LangDA%20loss%20improves%20explainability%20by%20integrating%20language-guided%0Ainsights%20into%20image%20reconstruction%20and%20offers%20a%20plug-and-play%20fashion.%20Our%0Afindings%20shed%20new%20light%20on%20the%20potential%20of%20language%20as%20a%20supervisory%20signal%20to%0Aadvance%20LDCT%20denoising.%20The%20code%20is%20publicly%20available%20on%0Ahttps%3A//github.com/hao1635/LangMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangMamba%253A%2520A%2520Language-driven%2520Mamba%2520Framework%2520for%2520Low-dose%2520CT%2520Denoising%250A%2520%2520with%2520Vision-language%2520Models%26entry.906535625%3DZhihao%2520Chen%2520and%2520Tao%2520Chen%2520and%2520Chenhui%2520Wang%2520and%2520Qi%2520Gao%2520and%2520Huidong%2520Xie%2520and%2520Chuang%2520Niu%2520and%2520Ge%2520Wang%2520and%2520Hongming%2520Shan%26entry.1292438233%3D%2520%2520Low-dose%2520computed%2520tomography%2520%2528LDCT%2529%2520reduces%2520radiation%2520exposure%2520but%2520often%250Adegrades%2520image%2520quality%252C%2520potentially%2520compromising%2520diagnostic%2520accuracy.%2520Existing%250Adeep%2520learning-based%2520denoising%2520methods%2520focus%2520primarily%2520on%2520pixel-level%2520mappings%252C%250Aoverlooking%2520the%2520potential%2520benefits%2520of%2520high-level%2520semantic%2520guidance.%2520Recent%250Aadvances%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520suggest%2520that%2520language%2520can%2520serve%2520as%2520a%250Apowerful%2520tool%2520for%2520capturing%2520structured%2520semantic%2520information%252C%2520offering%2520new%250Aopportunities%2520to%2520improve%2520LDCT%2520reconstruction.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ALangMamba%252C%2520a%2520Language-driven%2520Mamba%2520framework%2520for%2520LDCT%2520denoising%2520that%2520leverages%250AVLM-derived%2520representations%2520to%2520enhance%2520supervision%2520from%2520normal-dose%2520CT%2520%2528NDCT%2529.%250ALangMamba%2520follows%2520a%2520two-stage%2520learning%2520strategy.%2520First%252C%2520we%2520pre-train%2520a%250ALanguage-guided%2520AutoEncoder%2520%2528LangAE%2529%2520that%2520leverages%2520frozen%2520VLMs%2520to%2520map%2520NDCT%250Aimages%2520into%2520a%2520semantic%2520space%2520enriched%2520with%2520anatomical%2520information.%2520Second%252C%2520we%250Asynergize%2520LangAE%2520with%2520two%2520key%2520components%2520to%2520guide%2520LDCT%2520denoising%253A%250ASemantic-Enhanced%2520Efficient%2520Denoiser%2520%2528SEED%2529%252C%2520which%2520enhances%2520NDCT-relevant%2520local%250Asemantic%2520while%2520capturing%2520global%2520features%2520with%2520efficient%2520Mamba%2520mechanism%252C%2520and%250ALanguage-engaged%2520Dual-space%2520Alignment%2520%2528LangDA%2529%2520Loss%252C%2520which%2520ensures%2520that%250Adenoised%2520images%2520align%2520with%2520NDCT%2520in%2520both%2520perceptual%2520and%2520semantic%2520spaces.%250AExtensive%2520experiments%2520on%2520two%2520public%2520datasets%2520demonstrate%2520that%2520LangMamba%250Aoutperforms%2520conventional%2520state-of-the-art%2520methods%252C%2520significantly%2520improving%250Adetail%2520preservation%2520and%2520visual%2520fidelity.%2520Remarkably%252C%2520LangAE%2520exhibits%2520strong%250Ageneralizability%2520to%2520unseen%2520datasets%252C%2520thereby%2520reducing%2520training%2520costs.%250AFurthermore%252C%2520LangDA%2520loss%2520improves%2520explainability%2520by%2520integrating%2520language-guided%250Ainsights%2520into%2520image%2520reconstruction%2520and%2520offers%2520a%2520plug-and-play%2520fashion.%2520Our%250Afindings%2520shed%2520new%2520light%2520on%2520the%2520potential%2520of%2520language%2520as%2520a%2520supervisory%2520signal%2520to%250Aadvance%2520LDCT%2520denoising.%2520The%2520code%2520is%2520publicly%2520available%2520on%250Ahttps%253A//github.com/hao1635/LangMamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangMamba%3A%20A%20Language-driven%20Mamba%20Framework%20for%20Low-dose%20CT%20Denoising%0A%20%20with%20Vision-language%20Models&entry.906535625=Zhihao%20Chen%20and%20Tao%20Chen%20and%20Chenhui%20Wang%20and%20Qi%20Gao%20and%20Huidong%20Xie%20and%20Chuang%20Niu%20and%20Ge%20Wang%20and%20Hongming%20Shan&entry.1292438233=%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20reduces%20radiation%20exposure%20but%20often%0Adegrades%20image%20quality%2C%20potentially%20compromising%20diagnostic%20accuracy.%20Existing%0Adeep%20learning-based%20denoising%20methods%20focus%20primarily%20on%20pixel-level%20mappings%2C%0Aoverlooking%20the%20potential%20benefits%20of%20high-level%20semantic%20guidance.%20Recent%0Aadvances%20in%20vision-language%20models%20%28VLMs%29%20suggest%20that%20language%20can%20serve%20as%20a%0Apowerful%20tool%20for%20capturing%20structured%20semantic%20information%2C%20offering%20new%0Aopportunities%20to%20improve%20LDCT%20reconstruction.%20In%20this%20paper%2C%20we%20introduce%0ALangMamba%2C%20a%20Language-driven%20Mamba%20framework%20for%20LDCT%20denoising%20that%20leverages%0AVLM-derived%20representations%20to%20enhance%20supervision%20from%20normal-dose%20CT%20%28NDCT%29.%0ALangMamba%20follows%20a%20two-stage%20learning%20strategy.%20First%2C%20we%20pre-train%20a%0ALanguage-guided%20AutoEncoder%20%28LangAE%29%20that%20leverages%20frozen%20VLMs%20to%20map%20NDCT%0Aimages%20into%20a%20semantic%20space%20enriched%20with%20anatomical%20information.%20Second%2C%20we%0Asynergize%20LangAE%20with%20two%20key%20components%20to%20guide%20LDCT%20denoising%3A%0ASemantic-Enhanced%20Efficient%20Denoiser%20%28SEED%29%2C%20which%20enhances%20NDCT-relevant%20local%0Asemantic%20while%20capturing%20global%20features%20with%20efficient%20Mamba%20mechanism%2C%20and%0ALanguage-engaged%20Dual-space%20Alignment%20%28LangDA%29%20Loss%2C%20which%20ensures%20that%0Adenoised%20images%20align%20with%20NDCT%20in%20both%20perceptual%20and%20semantic%20spaces.%0AExtensive%20experiments%20on%20two%20public%20datasets%20demonstrate%20that%20LangMamba%0Aoutperforms%20conventional%20state-of-the-art%20methods%2C%20significantly%20improving%0Adetail%20preservation%20and%20visual%20fidelity.%20Remarkably%2C%20LangAE%20exhibits%20strong%0Ageneralizability%20to%20unseen%20datasets%2C%20thereby%20reducing%20training%20costs.%0AFurthermore%2C%20LangDA%20loss%20improves%20explainability%20by%20integrating%20language-guided%0Ainsights%20into%20image%20reconstruction%20and%20offers%20a%20plug-and-play%20fashion.%20Our%0Afindings%20shed%20new%20light%20on%20the%20potential%20of%20language%20as%20a%20supervisory%20signal%20to%0Aadvance%20LDCT%20denoising.%20The%20code%20is%20publicly%20available%20on%0Ahttps%3A//github.com/hao1635/LangMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06140v1&entry.124074799=Read"},
{"title": "What's Making That Sound Right Now? Video-centric Audio-Visual\n  Localization", "author": "Hahyeon Choi and Junhoo Lee and Nojun Kwak", "abstract": "  Audio-Visual Localization (AVL) aims to identify sound-emitting sources\nwithin a visual scene. However, existing studies focus on image-level\naudio-visual associations, failing to capture temporal dynamics. Moreover, they\nassume simplified scenarios where sound sources are always visible and involve\nonly a single object. To address these limitations, we propose AVATAR, a\nvideo-centric AVL benchmark that incorporates high-resolution temporal\ninformation. AVATAR introduces four distinct scenarios -- Single-sound,\nMixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive\nevaluation of AVL models. Additionally, we present TAVLO, a novel video-centric\nAVL model that explicitly integrates temporal information. Experimental results\nshow that conventional methods struggle to track temporal variations due to\ntheir reliance on global audio features and frame-level mappings. In contrast,\nTAVLO achieves robust and precise audio-visual alignment by leveraging\nhigh-resolution temporal modeling. Our work empirically demonstrates the\nimportance of temporal dynamics in AVL and establishes a new standard for\nvideo-centric audio-visual localization.\n", "link": "http://arxiv.org/abs/2507.04667v2", "date": "2025-07-08", "relevancy": 2.23, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%27s%20Making%20That%20Sound%20Right%20Now%3F%20Video-centric%20Audio-Visual%0A%20%20Localization&body=Title%3A%20What%27s%20Making%20That%20Sound%20Right%20Now%3F%20Video-centric%20Audio-Visual%0A%20%20Localization%0AAuthor%3A%20Hahyeon%20Choi%20and%20Junhoo%20Lee%20and%20Nojun%20Kwak%0AAbstract%3A%20%20%20Audio-Visual%20Localization%20%28AVL%29%20aims%20to%20identify%20sound-emitting%20sources%0Awithin%20a%20visual%20scene.%20However%2C%20existing%20studies%20focus%20on%20image-level%0Aaudio-visual%20associations%2C%20failing%20to%20capture%20temporal%20dynamics.%20Moreover%2C%20they%0Aassume%20simplified%20scenarios%20where%20sound%20sources%20are%20always%20visible%20and%20involve%0Aonly%20a%20single%20object.%20To%20address%20these%20limitations%2C%20we%20propose%20AVATAR%2C%20a%0Avideo-centric%20AVL%20benchmark%20that%20incorporates%20high-resolution%20temporal%0Ainformation.%20AVATAR%20introduces%20four%20distinct%20scenarios%20--%20Single-sound%2C%0AMixed-sound%2C%20Multi-entity%2C%20and%20Off-screen%20--%20enabling%20a%20more%20comprehensive%0Aevaluation%20of%20AVL%20models.%20Additionally%2C%20we%20present%20TAVLO%2C%20a%20novel%20video-centric%0AAVL%20model%20that%20explicitly%20integrates%20temporal%20information.%20Experimental%20results%0Ashow%20that%20conventional%20methods%20struggle%20to%20track%20temporal%20variations%20due%20to%0Atheir%20reliance%20on%20global%20audio%20features%20and%20frame-level%20mappings.%20In%20contrast%2C%0ATAVLO%20achieves%20robust%20and%20precise%20audio-visual%20alignment%20by%20leveraging%0Ahigh-resolution%20temporal%20modeling.%20Our%20work%20empirically%20demonstrates%20the%0Aimportance%20of%20temporal%20dynamics%20in%20AVL%20and%20establishes%20a%20new%20standard%20for%0Avideo-centric%20audio-visual%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04667v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2527s%2520Making%2520That%2520Sound%2520Right%2520Now%253F%2520Video-centric%2520Audio-Visual%250A%2520%2520Localization%26entry.906535625%3DHahyeon%2520Choi%2520and%2520Junhoo%2520Lee%2520and%2520Nojun%2520Kwak%26entry.1292438233%3D%2520%2520Audio-Visual%2520Localization%2520%2528AVL%2529%2520aims%2520to%2520identify%2520sound-emitting%2520sources%250Awithin%2520a%2520visual%2520scene.%2520However%252C%2520existing%2520studies%2520focus%2520on%2520image-level%250Aaudio-visual%2520associations%252C%2520failing%2520to%2520capture%2520temporal%2520dynamics.%2520Moreover%252C%2520they%250Aassume%2520simplified%2520scenarios%2520where%2520sound%2520sources%2520are%2520always%2520visible%2520and%2520involve%250Aonly%2520a%2520single%2520object.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520AVATAR%252C%2520a%250Avideo-centric%2520AVL%2520benchmark%2520that%2520incorporates%2520high-resolution%2520temporal%250Ainformation.%2520AVATAR%2520introduces%2520four%2520distinct%2520scenarios%2520--%2520Single-sound%252C%250AMixed-sound%252C%2520Multi-entity%252C%2520and%2520Off-screen%2520--%2520enabling%2520a%2520more%2520comprehensive%250Aevaluation%2520of%2520AVL%2520models.%2520Additionally%252C%2520we%2520present%2520TAVLO%252C%2520a%2520novel%2520video-centric%250AAVL%2520model%2520that%2520explicitly%2520integrates%2520temporal%2520information.%2520Experimental%2520results%250Ashow%2520that%2520conventional%2520methods%2520struggle%2520to%2520track%2520temporal%2520variations%2520due%2520to%250Atheir%2520reliance%2520on%2520global%2520audio%2520features%2520and%2520frame-level%2520mappings.%2520In%2520contrast%252C%250ATAVLO%2520achieves%2520robust%2520and%2520precise%2520audio-visual%2520alignment%2520by%2520leveraging%250Ahigh-resolution%2520temporal%2520modeling.%2520Our%2520work%2520empirically%2520demonstrates%2520the%250Aimportance%2520of%2520temporal%2520dynamics%2520in%2520AVL%2520and%2520establishes%2520a%2520new%2520standard%2520for%250Avideo-centric%2520audio-visual%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04667v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%27s%20Making%20That%20Sound%20Right%20Now%3F%20Video-centric%20Audio-Visual%0A%20%20Localization&entry.906535625=Hahyeon%20Choi%20and%20Junhoo%20Lee%20and%20Nojun%20Kwak&entry.1292438233=%20%20Audio-Visual%20Localization%20%28AVL%29%20aims%20to%20identify%20sound-emitting%20sources%0Awithin%20a%20visual%20scene.%20However%2C%20existing%20studies%20focus%20on%20image-level%0Aaudio-visual%20associations%2C%20failing%20to%20capture%20temporal%20dynamics.%20Moreover%2C%20they%0Aassume%20simplified%20scenarios%20where%20sound%20sources%20are%20always%20visible%20and%20involve%0Aonly%20a%20single%20object.%20To%20address%20these%20limitations%2C%20we%20propose%20AVATAR%2C%20a%0Avideo-centric%20AVL%20benchmark%20that%20incorporates%20high-resolution%20temporal%0Ainformation.%20AVATAR%20introduces%20four%20distinct%20scenarios%20--%20Single-sound%2C%0AMixed-sound%2C%20Multi-entity%2C%20and%20Off-screen%20--%20enabling%20a%20more%20comprehensive%0Aevaluation%20of%20AVL%20models.%20Additionally%2C%20we%20present%20TAVLO%2C%20a%20novel%20video-centric%0AAVL%20model%20that%20explicitly%20integrates%20temporal%20information.%20Experimental%20results%0Ashow%20that%20conventional%20methods%20struggle%20to%20track%20temporal%20variations%20due%20to%0Atheir%20reliance%20on%20global%20audio%20features%20and%20frame-level%20mappings.%20In%20contrast%2C%0ATAVLO%20achieves%20robust%20and%20precise%20audio-visual%20alignment%20by%20leveraging%0Ahigh-resolution%20temporal%20modeling.%20Our%20work%20empirically%20demonstrates%20the%0Aimportance%20of%20temporal%20dynamics%20in%20AVL%20and%20establishes%20a%20new%20standard%20for%0Avideo-centric%20audio-visual%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04667v2&entry.124074799=Read"},
{"title": "Discontinuity-aware Normal Integration for Generic Central Camera Models", "author": "Francesco Milano and Manuel L\u00f3pez-Antequera and Naina Dhingra and Roland Siegwart and Robert Thiel", "abstract": "  Recovering a 3D surface from its surface normal map, a problem known as\nnormal integration, is a key component for photometric shape reconstruction\ntechniques such as shape-from-shading and photometric stereo. The vast majority\nof existing approaches for normal integration handle only implicitly the\npresence of depth discontinuities and are limited to orthographic or ideal\npinhole cameras. In this paper, we propose a novel formulation that allows\nmodeling discontinuities explicitly and handling generic central cameras. Our\nkey idea is based on a local planarity assumption, that we model through\nconstraints between surface normals and ray directions. Compared to existing\nmethods, our approach more accurately approximates the relation between depth\nand surface normals, achieves state-of-the-art results on the standard normal\nintegration benchmark, and is the first to directly handle generic central\ncamera models.\n", "link": "http://arxiv.org/abs/2507.06075v1", "date": "2025-07-08", "relevancy": 2.224, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5981}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5476}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discontinuity-aware%20Normal%20Integration%20for%20Generic%20Central%20Camera%20Models&body=Title%3A%20Discontinuity-aware%20Normal%20Integration%20for%20Generic%20Central%20Camera%20Models%0AAuthor%3A%20Francesco%20Milano%20and%20Manuel%20L%C3%B3pez-Antequera%20and%20Naina%20Dhingra%20and%20Roland%20Siegwart%20and%20Robert%20Thiel%0AAbstract%3A%20%20%20Recovering%20a%203D%20surface%20from%20its%20surface%20normal%20map%2C%20a%20problem%20known%20as%0Anormal%20integration%2C%20is%20a%20key%20component%20for%20photometric%20shape%20reconstruction%0Atechniques%20such%20as%20shape-from-shading%20and%20photometric%20stereo.%20The%20vast%20majority%0Aof%20existing%20approaches%20for%20normal%20integration%20handle%20only%20implicitly%20the%0Apresence%20of%20depth%20discontinuities%20and%20are%20limited%20to%20orthographic%20or%20ideal%0Apinhole%20cameras.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20formulation%20that%20allows%0Amodeling%20discontinuities%20explicitly%20and%20handling%20generic%20central%20cameras.%20Our%0Akey%20idea%20is%20based%20on%20a%20local%20planarity%20assumption%2C%20that%20we%20model%20through%0Aconstraints%20between%20surface%20normals%20and%20ray%20directions.%20Compared%20to%20existing%0Amethods%2C%20our%20approach%20more%20accurately%20approximates%20the%20relation%20between%20depth%0Aand%20surface%20normals%2C%20achieves%20state-of-the-art%20results%20on%20the%20standard%20normal%0Aintegration%20benchmark%2C%20and%20is%20the%20first%20to%20directly%20handle%20generic%20central%0Acamera%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscontinuity-aware%2520Normal%2520Integration%2520for%2520Generic%2520Central%2520Camera%2520Models%26entry.906535625%3DFrancesco%2520Milano%2520and%2520Manuel%2520L%25C3%25B3pez-Antequera%2520and%2520Naina%2520Dhingra%2520and%2520Roland%2520Siegwart%2520and%2520Robert%2520Thiel%26entry.1292438233%3D%2520%2520Recovering%2520a%25203D%2520surface%2520from%2520its%2520surface%2520normal%2520map%252C%2520a%2520problem%2520known%2520as%250Anormal%2520integration%252C%2520is%2520a%2520key%2520component%2520for%2520photometric%2520shape%2520reconstruction%250Atechniques%2520such%2520as%2520shape-from-shading%2520and%2520photometric%2520stereo.%2520The%2520vast%2520majority%250Aof%2520existing%2520approaches%2520for%2520normal%2520integration%2520handle%2520only%2520implicitly%2520the%250Apresence%2520of%2520depth%2520discontinuities%2520and%2520are%2520limited%2520to%2520orthographic%2520or%2520ideal%250Apinhole%2520cameras.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520formulation%2520that%2520allows%250Amodeling%2520discontinuities%2520explicitly%2520and%2520handling%2520generic%2520central%2520cameras.%2520Our%250Akey%2520idea%2520is%2520based%2520on%2520a%2520local%2520planarity%2520assumption%252C%2520that%2520we%2520model%2520through%250Aconstraints%2520between%2520surface%2520normals%2520and%2520ray%2520directions.%2520Compared%2520to%2520existing%250Amethods%252C%2520our%2520approach%2520more%2520accurately%2520approximates%2520the%2520relation%2520between%2520depth%250Aand%2520surface%2520normals%252C%2520achieves%2520state-of-the-art%2520results%2520on%2520the%2520standard%2520normal%250Aintegration%2520benchmark%252C%2520and%2520is%2520the%2520first%2520to%2520directly%2520handle%2520generic%2520central%250Acamera%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discontinuity-aware%20Normal%20Integration%20for%20Generic%20Central%20Camera%20Models&entry.906535625=Francesco%20Milano%20and%20Manuel%20L%C3%B3pez-Antequera%20and%20Naina%20Dhingra%20and%20Roland%20Siegwart%20and%20Robert%20Thiel&entry.1292438233=%20%20Recovering%20a%203D%20surface%20from%20its%20surface%20normal%20map%2C%20a%20problem%20known%20as%0Anormal%20integration%2C%20is%20a%20key%20component%20for%20photometric%20shape%20reconstruction%0Atechniques%20such%20as%20shape-from-shading%20and%20photometric%20stereo.%20The%20vast%20majority%0Aof%20existing%20approaches%20for%20normal%20integration%20handle%20only%20implicitly%20the%0Apresence%20of%20depth%20discontinuities%20and%20are%20limited%20to%20orthographic%20or%20ideal%0Apinhole%20cameras.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20formulation%20that%20allows%0Amodeling%20discontinuities%20explicitly%20and%20handling%20generic%20central%20cameras.%20Our%0Akey%20idea%20is%20based%20on%20a%20local%20planarity%20assumption%2C%20that%20we%20model%20through%0Aconstraints%20between%20surface%20normals%20and%20ray%20directions.%20Compared%20to%20existing%0Amethods%2C%20our%20approach%20more%20accurately%20approximates%20the%20relation%20between%20depth%0Aand%20surface%20normals%2C%20achieves%20state-of-the-art%20results%20on%20the%20standard%20normal%0Aintegration%20benchmark%2C%20and%20is%20the%20first%20to%20directly%20handle%20generic%20central%0Acamera%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06075v1&entry.124074799=Read"},
{"title": "Neural-Network solver of ideal MHD equilibria", "author": "Timo Thun and Andrea Merlo and Rory Conlin and Dario Panici and Daniel B\u00f6ckenhoff", "abstract": "  We present a novel approach to compute three-dimensional Magnetohydrodynamic\nequilibria by parametrizing Fourier modes with artificial neural networks and\ncompare it to equilibria computed by conventional solvers. The full nonlinear\nglobal force residual across the volume in real space is then minimized with\nfirst order optimizers. Already,we observe competitive computational cost to\narrive at the same minimum residuals computed by existing codes. With increased\ncomputational cost,lower minima of the residual are achieved by the neural\nnetworks,establishing a new lower bound for the force residual. We use\nminimally complex neural networks,and we expect significant improvements for\nsolving not only single equilibria with neural networks,but also for computing\nneural network models valid over continuous distributions of equilibria.\n", "link": "http://arxiv.org/abs/2507.03119v2", "date": "2025-07-08", "relevancy": 2.2232, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4538}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4487}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural-Network%20solver%20of%20ideal%20MHD%20equilibria&body=Title%3A%20Neural-Network%20solver%20of%20ideal%20MHD%20equilibria%0AAuthor%3A%20Timo%20Thun%20and%20Andrea%20Merlo%20and%20Rory%20Conlin%20and%20Dario%20Panici%20and%20Daniel%20B%C3%B6ckenhoff%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20compute%20three-dimensional%20Magnetohydrodynamic%0Aequilibria%20by%20parametrizing%20Fourier%20modes%20with%20artificial%20neural%20networks%20and%0Acompare%20it%20to%20equilibria%20computed%20by%20conventional%20solvers.%20The%20full%20nonlinear%0Aglobal%20force%20residual%20across%20the%20volume%20in%20real%20space%20is%20then%20minimized%20with%0Afirst%20order%20optimizers.%20Already%2Cwe%20observe%20competitive%20computational%20cost%20to%0Aarrive%20at%20the%20same%20minimum%20residuals%20computed%20by%20existing%20codes.%20With%20increased%0Acomputational%20cost%2Clower%20minima%20of%20the%20residual%20are%20achieved%20by%20the%20neural%0Anetworks%2Cestablishing%20a%20new%20lower%20bound%20for%20the%20force%20residual.%20We%20use%0Aminimally%20complex%20neural%20networks%2Cand%20we%20expect%20significant%20improvements%20for%0Asolving%20not%20only%20single%20equilibria%20with%20neural%20networks%2Cbut%20also%20for%20computing%0Aneural%20network%20models%20valid%20over%20continuous%20distributions%20of%20equilibria.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03119v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural-Network%2520solver%2520of%2520ideal%2520MHD%2520equilibria%26entry.906535625%3DTimo%2520Thun%2520and%2520Andrea%2520Merlo%2520and%2520Rory%2520Conlin%2520and%2520Dario%2520Panici%2520and%2520Daniel%2520B%25C3%25B6ckenhoff%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520compute%2520three-dimensional%2520Magnetohydrodynamic%250Aequilibria%2520by%2520parametrizing%2520Fourier%2520modes%2520with%2520artificial%2520neural%2520networks%2520and%250Acompare%2520it%2520to%2520equilibria%2520computed%2520by%2520conventional%2520solvers.%2520The%2520full%2520nonlinear%250Aglobal%2520force%2520residual%2520across%2520the%2520volume%2520in%2520real%2520space%2520is%2520then%2520minimized%2520with%250Afirst%2520order%2520optimizers.%2520Already%252Cwe%2520observe%2520competitive%2520computational%2520cost%2520to%250Aarrive%2520at%2520the%2520same%2520minimum%2520residuals%2520computed%2520by%2520existing%2520codes.%2520With%2520increased%250Acomputational%2520cost%252Clower%2520minima%2520of%2520the%2520residual%2520are%2520achieved%2520by%2520the%2520neural%250Anetworks%252Cestablishing%2520a%2520new%2520lower%2520bound%2520for%2520the%2520force%2520residual.%2520We%2520use%250Aminimally%2520complex%2520neural%2520networks%252Cand%2520we%2520expect%2520significant%2520improvements%2520for%250Asolving%2520not%2520only%2520single%2520equilibria%2520with%2520neural%2520networks%252Cbut%2520also%2520for%2520computing%250Aneural%2520network%2520models%2520valid%2520over%2520continuous%2520distributions%2520of%2520equilibria.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03119v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural-Network%20solver%20of%20ideal%20MHD%20equilibria&entry.906535625=Timo%20Thun%20and%20Andrea%20Merlo%20and%20Rory%20Conlin%20and%20Dario%20Panici%20and%20Daniel%20B%C3%B6ckenhoff&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20compute%20three-dimensional%20Magnetohydrodynamic%0Aequilibria%20by%20parametrizing%20Fourier%20modes%20with%20artificial%20neural%20networks%20and%0Acompare%20it%20to%20equilibria%20computed%20by%20conventional%20solvers.%20The%20full%20nonlinear%0Aglobal%20force%20residual%20across%20the%20volume%20in%20real%20space%20is%20then%20minimized%20with%0Afirst%20order%20optimizers.%20Already%2Cwe%20observe%20competitive%20computational%20cost%20to%0Aarrive%20at%20the%20same%20minimum%20residuals%20computed%20by%20existing%20codes.%20With%20increased%0Acomputational%20cost%2Clower%20minima%20of%20the%20residual%20are%20achieved%20by%20the%20neural%0Anetworks%2Cestablishing%20a%20new%20lower%20bound%20for%20the%20force%20residual.%20We%20use%0Aminimally%20complex%20neural%20networks%2Cand%20we%20expect%20significant%20improvements%20for%0Asolving%20not%20only%20single%20equilibria%20with%20neural%20networks%2Cbut%20also%20for%20computing%0Aneural%20network%20models%20valid%20over%20continuous%20distributions%20of%20equilibria.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03119v2&entry.124074799=Read"},
{"title": "KnowIt: Deep Time Series Modeling and Interpretation", "author": "M. W. Theunissen and R. Rabe and M. H. Davel", "abstract": "  KnowIt (Knowledge discovery in time series data) is a flexible framework for\nbuilding deep time series models and interpreting them. It is implemented as a\nPython toolkit, with source code and documentation available from\nhttps://must-deep-learning.github.io/KnowIt. It imposes minimal assumptions\nabout task specifications and decouples the definition of dataset, deep neural\nnetwork architecture, and interpretability technique through well defined\ninterfaces. This ensures the ease of importing new datasets, custom\narchitectures, and the definition of different interpretability paradigms while\nmaintaining on-the-fly modeling and interpretation of different aspects of a\nuser's own time series data. KnowIt aims to provide an environment where users\ncan perform knowledge discovery on their own complex time series data through\nbuilding powerful deep learning models and explaining their behavior. With\nongoing development, collaboration and application our goal is to make this a\nplatform to progress this underexplored field and produce a trusted tool for\ndeep time series modeling.\n", "link": "http://arxiv.org/abs/2507.06009v1", "date": "2025-07-08", "relevancy": 2.2174, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4481}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KnowIt%3A%20Deep%20Time%20Series%20Modeling%20and%20Interpretation&body=Title%3A%20KnowIt%3A%20Deep%20Time%20Series%20Modeling%20and%20Interpretation%0AAuthor%3A%20M.%20W.%20Theunissen%20and%20R.%20Rabe%20and%20M.%20H.%20Davel%0AAbstract%3A%20%20%20KnowIt%20%28Knowledge%20discovery%20in%20time%20series%20data%29%20is%20a%20flexible%20framework%20for%0Abuilding%20deep%20time%20series%20models%20and%20interpreting%20them.%20It%20is%20implemented%20as%20a%0APython%20toolkit%2C%20with%20source%20code%20and%20documentation%20available%20from%0Ahttps%3A//must-deep-learning.github.io/KnowIt.%20It%20imposes%20minimal%20assumptions%0Aabout%20task%20specifications%20and%20decouples%20the%20definition%20of%20dataset%2C%20deep%20neural%0Anetwork%20architecture%2C%20and%20interpretability%20technique%20through%20well%20defined%0Ainterfaces.%20This%20ensures%20the%20ease%20of%20importing%20new%20datasets%2C%20custom%0Aarchitectures%2C%20and%20the%20definition%20of%20different%20interpretability%20paradigms%20while%0Amaintaining%20on-the-fly%20modeling%20and%20interpretation%20of%20different%20aspects%20of%20a%0Auser%27s%20own%20time%20series%20data.%20KnowIt%20aims%20to%20provide%20an%20environment%20where%20users%0Acan%20perform%20knowledge%20discovery%20on%20their%20own%20complex%20time%20series%20data%20through%0Abuilding%20powerful%20deep%20learning%20models%20and%20explaining%20their%20behavior.%20With%0Aongoing%20development%2C%20collaboration%20and%20application%20our%20goal%20is%20to%20make%20this%20a%0Aplatform%20to%20progress%20this%20underexplored%20field%20and%20produce%20a%20trusted%20tool%20for%0Adeep%20time%20series%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowIt%253A%2520Deep%2520Time%2520Series%2520Modeling%2520and%2520Interpretation%26entry.906535625%3DM.%2520W.%2520Theunissen%2520and%2520R.%2520Rabe%2520and%2520M.%2520H.%2520Davel%26entry.1292438233%3D%2520%2520KnowIt%2520%2528Knowledge%2520discovery%2520in%2520time%2520series%2520data%2529%2520is%2520a%2520flexible%2520framework%2520for%250Abuilding%2520deep%2520time%2520series%2520models%2520and%2520interpreting%2520them.%2520It%2520is%2520implemented%2520as%2520a%250APython%2520toolkit%252C%2520with%2520source%2520code%2520and%2520documentation%2520available%2520from%250Ahttps%253A//must-deep-learning.github.io/KnowIt.%2520It%2520imposes%2520minimal%2520assumptions%250Aabout%2520task%2520specifications%2520and%2520decouples%2520the%2520definition%2520of%2520dataset%252C%2520deep%2520neural%250Anetwork%2520architecture%252C%2520and%2520interpretability%2520technique%2520through%2520well%2520defined%250Ainterfaces.%2520This%2520ensures%2520the%2520ease%2520of%2520importing%2520new%2520datasets%252C%2520custom%250Aarchitectures%252C%2520and%2520the%2520definition%2520of%2520different%2520interpretability%2520paradigms%2520while%250Amaintaining%2520on-the-fly%2520modeling%2520and%2520interpretation%2520of%2520different%2520aspects%2520of%2520a%250Auser%2527s%2520own%2520time%2520series%2520data.%2520KnowIt%2520aims%2520to%2520provide%2520an%2520environment%2520where%2520users%250Acan%2520perform%2520knowledge%2520discovery%2520on%2520their%2520own%2520complex%2520time%2520series%2520data%2520through%250Abuilding%2520powerful%2520deep%2520learning%2520models%2520and%2520explaining%2520their%2520behavior.%2520With%250Aongoing%2520development%252C%2520collaboration%2520and%2520application%2520our%2520goal%2520is%2520to%2520make%2520this%2520a%250Aplatform%2520to%2520progress%2520this%2520underexplored%2520field%2520and%2520produce%2520a%2520trusted%2520tool%2520for%250Adeep%2520time%2520series%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KnowIt%3A%20Deep%20Time%20Series%20Modeling%20and%20Interpretation&entry.906535625=M.%20W.%20Theunissen%20and%20R.%20Rabe%20and%20M.%20H.%20Davel&entry.1292438233=%20%20KnowIt%20%28Knowledge%20discovery%20in%20time%20series%20data%29%20is%20a%20flexible%20framework%20for%0Abuilding%20deep%20time%20series%20models%20and%20interpreting%20them.%20It%20is%20implemented%20as%20a%0APython%20toolkit%2C%20with%20source%20code%20and%20documentation%20available%20from%0Ahttps%3A//must-deep-learning.github.io/KnowIt.%20It%20imposes%20minimal%20assumptions%0Aabout%20task%20specifications%20and%20decouples%20the%20definition%20of%20dataset%2C%20deep%20neural%0Anetwork%20architecture%2C%20and%20interpretability%20technique%20through%20well%20defined%0Ainterfaces.%20This%20ensures%20the%20ease%20of%20importing%20new%20datasets%2C%20custom%0Aarchitectures%2C%20and%20the%20definition%20of%20different%20interpretability%20paradigms%20while%0Amaintaining%20on-the-fly%20modeling%20and%20interpretation%20of%20different%20aspects%20of%20a%0Auser%27s%20own%20time%20series%20data.%20KnowIt%20aims%20to%20provide%20an%20environment%20where%20users%0Acan%20perform%20knowledge%20discovery%20on%20their%20own%20complex%20time%20series%20data%20through%0Abuilding%20powerful%20deep%20learning%20models%20and%20explaining%20their%20behavior.%20With%0Aongoing%20development%2C%20collaboration%20and%20application%20our%20goal%20is%20to%20make%20this%20a%0Aplatform%20to%20progress%20this%20underexplored%20field%20and%20produce%20a%20trusted%20tool%20for%0Adeep%20time%20series%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06009v1&entry.124074799=Read"},
{"title": "BoundMatch: Boundary detection applied to semi-supervised segmentation\n  for urban-driving scenes", "author": "Haruya Ishikawa and Yoshimitsu Aoki", "abstract": "  Semi-supervised semantic segmentation (SS-SS) aims to mitigate the heavy\nannotation burden of dense pixel labeling by leveraging abundant unlabeled\nimages alongside a small labeled set. While current consistency regularization\nmethods achieve strong results, they often overlook a critical challenge: the\nprecise delineation of object boundaries. In this paper, we propose BoundMatch,\na novel multi-task SS-SS framework that explicitly integrates semantic boundary\ndetection into a teacher-student consistency regularization pipeline. Our core\nmechanism, Boundary Consistency Regularized Multi-Task Learning (BCRM),\nenforces prediction agreement between teacher and student models on both\nsegmentation masks and detailed semantic boundaries. To further enhance\nperformance and sharpen boundaries, BoundMatch incorporates two lightweight\nfusion modules: Boundary-Semantic Fusion (BSF) injects learned boundary cues\ninto the segmentation decoder, while Spatial Gradient Fusion (SGF) refines\nboundary predictions using mask gradients, leading to higher-quality boundary\npseudo-labels. This framework is built upon SAMTH, a strong teacher-student\nbaseline featuring a Harmonious Batch Normalization (HBN) update strategy for\nimproved stability. Extensive experiments on diverse urban-driving scene\ndatasets including Cityscapes, BDD100K, and SYNTHIA show that BoundMatch\nachieves competitive performance against current state-of-the-art methods. Our\napproach achieves state-of-the-art results on the new benchmark with DINOv2\nfoundation model. We further validate our approach's generalizability on Pascal\nVOC and ADE20K datasets. Ablation studies highlight BoundMatch's ability to\nimprove boundary-specific evaluation metrics, its effectiveness in realistic\nlarge-scale unlabeled data scenarios, and applicability to lightweight\narchitectures for mobile deployment.\n", "link": "http://arxiv.org/abs/2503.23519v2", "date": "2025-07-08", "relevancy": 2.2047, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5681}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5562}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoundMatch%3A%20Boundary%20detection%20applied%20to%20semi-supervised%20segmentation%0A%20%20for%20urban-driving%20scenes&body=Title%3A%20BoundMatch%3A%20Boundary%20detection%20applied%20to%20semi-supervised%20segmentation%0A%20%20for%20urban-driving%20scenes%0AAuthor%3A%20Haruya%20Ishikawa%20and%20Yoshimitsu%20Aoki%0AAbstract%3A%20%20%20Semi-supervised%20semantic%20segmentation%20%28SS-SS%29%20aims%20to%20mitigate%20the%20heavy%0Aannotation%20burden%20of%20dense%20pixel%20labeling%20by%20leveraging%20abundant%20unlabeled%0Aimages%20alongside%20a%20small%20labeled%20set.%20While%20current%20consistency%20regularization%0Amethods%20achieve%20strong%20results%2C%20they%20often%20overlook%20a%20critical%20challenge%3A%20the%0Aprecise%20delineation%20of%20object%20boundaries.%20In%20this%20paper%2C%20we%20propose%20BoundMatch%2C%0Aa%20novel%20multi-task%20SS-SS%20framework%20that%20explicitly%20integrates%20semantic%20boundary%0Adetection%20into%20a%20teacher-student%20consistency%20regularization%20pipeline.%20Our%20core%0Amechanism%2C%20Boundary%20Consistency%20Regularized%20Multi-Task%20Learning%20%28BCRM%29%2C%0Aenforces%20prediction%20agreement%20between%20teacher%20and%20student%20models%20on%20both%0Asegmentation%20masks%20and%20detailed%20semantic%20boundaries.%20To%20further%20enhance%0Aperformance%20and%20sharpen%20boundaries%2C%20BoundMatch%20incorporates%20two%20lightweight%0Afusion%20modules%3A%20Boundary-Semantic%20Fusion%20%28BSF%29%20injects%20learned%20boundary%20cues%0Ainto%20the%20segmentation%20decoder%2C%20while%20Spatial%20Gradient%20Fusion%20%28SGF%29%20refines%0Aboundary%20predictions%20using%20mask%20gradients%2C%20leading%20to%20higher-quality%20boundary%0Apseudo-labels.%20This%20framework%20is%20built%20upon%20SAMTH%2C%20a%20strong%20teacher-student%0Abaseline%20featuring%20a%20Harmonious%20Batch%20Normalization%20%28HBN%29%20update%20strategy%20for%0Aimproved%20stability.%20Extensive%20experiments%20on%20diverse%20urban-driving%20scene%0Adatasets%20including%20Cityscapes%2C%20BDD100K%2C%20and%20SYNTHIA%20show%20that%20BoundMatch%0Aachieves%20competitive%20performance%20against%20current%20state-of-the-art%20methods.%20Our%0Aapproach%20achieves%20state-of-the-art%20results%20on%20the%20new%20benchmark%20with%20DINOv2%0Afoundation%20model.%20We%20further%20validate%20our%20approach%27s%20generalizability%20on%20Pascal%0AVOC%20and%20ADE20K%20datasets.%20Ablation%20studies%20highlight%20BoundMatch%27s%20ability%20to%0Aimprove%20boundary-specific%20evaluation%20metrics%2C%20its%20effectiveness%20in%20realistic%0Alarge-scale%20unlabeled%20data%20scenarios%2C%20and%20applicability%20to%20lightweight%0Aarchitectures%20for%20mobile%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23519v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoundMatch%253A%2520Boundary%2520detection%2520applied%2520to%2520semi-supervised%2520segmentation%250A%2520%2520for%2520urban-driving%2520scenes%26entry.906535625%3DHaruya%2520Ishikawa%2520and%2520Yoshimitsu%2520Aoki%26entry.1292438233%3D%2520%2520Semi-supervised%2520semantic%2520segmentation%2520%2528SS-SS%2529%2520aims%2520to%2520mitigate%2520the%2520heavy%250Aannotation%2520burden%2520of%2520dense%2520pixel%2520labeling%2520by%2520leveraging%2520abundant%2520unlabeled%250Aimages%2520alongside%2520a%2520small%2520labeled%2520set.%2520While%2520current%2520consistency%2520regularization%250Amethods%2520achieve%2520strong%2520results%252C%2520they%2520often%2520overlook%2520a%2520critical%2520challenge%253A%2520the%250Aprecise%2520delineation%2520of%2520object%2520boundaries.%2520In%2520this%2520paper%252C%2520we%2520propose%2520BoundMatch%252C%250Aa%2520novel%2520multi-task%2520SS-SS%2520framework%2520that%2520explicitly%2520integrates%2520semantic%2520boundary%250Adetection%2520into%2520a%2520teacher-student%2520consistency%2520regularization%2520pipeline.%2520Our%2520core%250Amechanism%252C%2520Boundary%2520Consistency%2520Regularized%2520Multi-Task%2520Learning%2520%2528BCRM%2529%252C%250Aenforces%2520prediction%2520agreement%2520between%2520teacher%2520and%2520student%2520models%2520on%2520both%250Asegmentation%2520masks%2520and%2520detailed%2520semantic%2520boundaries.%2520To%2520further%2520enhance%250Aperformance%2520and%2520sharpen%2520boundaries%252C%2520BoundMatch%2520incorporates%2520two%2520lightweight%250Afusion%2520modules%253A%2520Boundary-Semantic%2520Fusion%2520%2528BSF%2529%2520injects%2520learned%2520boundary%2520cues%250Ainto%2520the%2520segmentation%2520decoder%252C%2520while%2520Spatial%2520Gradient%2520Fusion%2520%2528SGF%2529%2520refines%250Aboundary%2520predictions%2520using%2520mask%2520gradients%252C%2520leading%2520to%2520higher-quality%2520boundary%250Apseudo-labels.%2520This%2520framework%2520is%2520built%2520upon%2520SAMTH%252C%2520a%2520strong%2520teacher-student%250Abaseline%2520featuring%2520a%2520Harmonious%2520Batch%2520Normalization%2520%2528HBN%2529%2520update%2520strategy%2520for%250Aimproved%2520stability.%2520Extensive%2520experiments%2520on%2520diverse%2520urban-driving%2520scene%250Adatasets%2520including%2520Cityscapes%252C%2520BDD100K%252C%2520and%2520SYNTHIA%2520show%2520that%2520BoundMatch%250Aachieves%2520competitive%2520performance%2520against%2520current%2520state-of-the-art%2520methods.%2520Our%250Aapproach%2520achieves%2520state-of-the-art%2520results%2520on%2520the%2520new%2520benchmark%2520with%2520DINOv2%250Afoundation%2520model.%2520We%2520further%2520validate%2520our%2520approach%2527s%2520generalizability%2520on%2520Pascal%250AVOC%2520and%2520ADE20K%2520datasets.%2520Ablation%2520studies%2520highlight%2520BoundMatch%2527s%2520ability%2520to%250Aimprove%2520boundary-specific%2520evaluation%2520metrics%252C%2520its%2520effectiveness%2520in%2520realistic%250Alarge-scale%2520unlabeled%2520data%2520scenarios%252C%2520and%2520applicability%2520to%2520lightweight%250Aarchitectures%2520for%2520mobile%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23519v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoundMatch%3A%20Boundary%20detection%20applied%20to%20semi-supervised%20segmentation%0A%20%20for%20urban-driving%20scenes&entry.906535625=Haruya%20Ishikawa%20and%20Yoshimitsu%20Aoki&entry.1292438233=%20%20Semi-supervised%20semantic%20segmentation%20%28SS-SS%29%20aims%20to%20mitigate%20the%20heavy%0Aannotation%20burden%20of%20dense%20pixel%20labeling%20by%20leveraging%20abundant%20unlabeled%0Aimages%20alongside%20a%20small%20labeled%20set.%20While%20current%20consistency%20regularization%0Amethods%20achieve%20strong%20results%2C%20they%20often%20overlook%20a%20critical%20challenge%3A%20the%0Aprecise%20delineation%20of%20object%20boundaries.%20In%20this%20paper%2C%20we%20propose%20BoundMatch%2C%0Aa%20novel%20multi-task%20SS-SS%20framework%20that%20explicitly%20integrates%20semantic%20boundary%0Adetection%20into%20a%20teacher-student%20consistency%20regularization%20pipeline.%20Our%20core%0Amechanism%2C%20Boundary%20Consistency%20Regularized%20Multi-Task%20Learning%20%28BCRM%29%2C%0Aenforces%20prediction%20agreement%20between%20teacher%20and%20student%20models%20on%20both%0Asegmentation%20masks%20and%20detailed%20semantic%20boundaries.%20To%20further%20enhance%0Aperformance%20and%20sharpen%20boundaries%2C%20BoundMatch%20incorporates%20two%20lightweight%0Afusion%20modules%3A%20Boundary-Semantic%20Fusion%20%28BSF%29%20injects%20learned%20boundary%20cues%0Ainto%20the%20segmentation%20decoder%2C%20while%20Spatial%20Gradient%20Fusion%20%28SGF%29%20refines%0Aboundary%20predictions%20using%20mask%20gradients%2C%20leading%20to%20higher-quality%20boundary%0Apseudo-labels.%20This%20framework%20is%20built%20upon%20SAMTH%2C%20a%20strong%20teacher-student%0Abaseline%20featuring%20a%20Harmonious%20Batch%20Normalization%20%28HBN%29%20update%20strategy%20for%0Aimproved%20stability.%20Extensive%20experiments%20on%20diverse%20urban-driving%20scene%0Adatasets%20including%20Cityscapes%2C%20BDD100K%2C%20and%20SYNTHIA%20show%20that%20BoundMatch%0Aachieves%20competitive%20performance%20against%20current%20state-of-the-art%20methods.%20Our%0Aapproach%20achieves%20state-of-the-art%20results%20on%20the%20new%20benchmark%20with%20DINOv2%0Afoundation%20model.%20We%20further%20validate%20our%20approach%27s%20generalizability%20on%20Pascal%0AVOC%20and%20ADE20K%20datasets.%20Ablation%20studies%20highlight%20BoundMatch%27s%20ability%20to%0Aimprove%20boundary-specific%20evaluation%20metrics%2C%20its%20effectiveness%20in%20realistic%0Alarge-scale%20unlabeled%20data%20scenarios%2C%20and%20applicability%20to%20lightweight%0Aarchitectures%20for%20mobile%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23519v2&entry.124074799=Read"},
{"title": "An Optimal Transport Perspective on Unpaired Image Super-Resolution", "author": "Milena Gazdieva and Petr Mokrov and Litu Rout and Alexander Korotin and Andrey Kravchenko and Alexander Filippov and Evgeny Burnaev", "abstract": "  Real-world image super-resolution (SR) tasks often do not have paired\ndatasets, which limits the application of supervised techniques. As a result,\nthe tasks are usually approached by unpaired techniques based on Generative\nAdversarial Networks (GANs), which yield complex training losses with several\nregularization terms, e.g., content or identity losses. While GANs usually\nprovide good practical performance, they are used heuristically, i.e.,\ntheoretical understanding of their behaviour is yet rather limited. We\ntheoretically investigate optimization problems which arise in such models and\nfind two surprising observations. First, the learned SR map is always an\noptimal transport (OT) map. Second, we theoretically prove and empirically show\nthat the learned map is biased, i.e., it does not actually transform the\ndistribution of low-resolution images to high-resolution ones. Inspired by\nthese findings, we investigate recent advances in neural OT field to resolve\nthe bias issue. We establish an intriguing connection between regularized GANs\nand neural OT approaches. We show that unlike the existing GAN-based\nalternatives, these algorithms aim to learn an unbiased OT map. We empirically\ndemonstrate our findings via a series of synthetic and real-world unpaired SR\nexperiments. Our source code is publicly available at\nhttps://github.com/milenagazdieva/OT-Super-Resolution.\n", "link": "http://arxiv.org/abs/2202.01116v3", "date": "2025-07-08", "relevancy": 2.1817, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5543}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.549}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Optimal%20Transport%20Perspective%20on%20Unpaired%20Image%20Super-Resolution&body=Title%3A%20An%20Optimal%20Transport%20Perspective%20on%20Unpaired%20Image%20Super-Resolution%0AAuthor%3A%20Milena%20Gazdieva%20and%20Petr%20Mokrov%20and%20Litu%20Rout%20and%20Alexander%20Korotin%20and%20Andrey%20Kravchenko%20and%20Alexander%20Filippov%20and%20Evgeny%20Burnaev%0AAbstract%3A%20%20%20Real-world%20image%20super-resolution%20%28SR%29%20tasks%20often%20do%20not%20have%20paired%0Adatasets%2C%20which%20limits%20the%20application%20of%20supervised%20techniques.%20As%20a%20result%2C%0Athe%20tasks%20are%20usually%20approached%20by%20unpaired%20techniques%20based%20on%20Generative%0AAdversarial%20Networks%20%28GANs%29%2C%20which%20yield%20complex%20training%20losses%20with%20several%0Aregularization%20terms%2C%20e.g.%2C%20content%20or%20identity%20losses.%20While%20GANs%20usually%0Aprovide%20good%20practical%20performance%2C%20they%20are%20used%20heuristically%2C%20i.e.%2C%0Atheoretical%20understanding%20of%20their%20behaviour%20is%20yet%20rather%20limited.%20We%0Atheoretically%20investigate%20optimization%20problems%20which%20arise%20in%20such%20models%20and%0Afind%20two%20surprising%20observations.%20First%2C%20the%20learned%20SR%20map%20is%20always%20an%0Aoptimal%20transport%20%28OT%29%20map.%20Second%2C%20we%20theoretically%20prove%20and%20empirically%20show%0Athat%20the%20learned%20map%20is%20biased%2C%20i.e.%2C%20it%20does%20not%20actually%20transform%20the%0Adistribution%20of%20low-resolution%20images%20to%20high-resolution%20ones.%20Inspired%20by%0Athese%20findings%2C%20we%20investigate%20recent%20advances%20in%20neural%20OT%20field%20to%20resolve%0Athe%20bias%20issue.%20We%20establish%20an%20intriguing%20connection%20between%20regularized%20GANs%0Aand%20neural%20OT%20approaches.%20We%20show%20that%20unlike%20the%20existing%20GAN-based%0Aalternatives%2C%20these%20algorithms%20aim%20to%20learn%20an%20unbiased%20OT%20map.%20We%20empirically%0Ademonstrate%20our%20findings%20via%20a%20series%20of%20synthetic%20and%20real-world%20unpaired%20SR%0Aexperiments.%20Our%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/milenagazdieva/OT-Super-Resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.01116v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Optimal%2520Transport%2520Perspective%2520on%2520Unpaired%2520Image%2520Super-Resolution%26entry.906535625%3DMilena%2520Gazdieva%2520and%2520Petr%2520Mokrov%2520and%2520Litu%2520Rout%2520and%2520Alexander%2520Korotin%2520and%2520Andrey%2520Kravchenko%2520and%2520Alexander%2520Filippov%2520and%2520Evgeny%2520Burnaev%26entry.1292438233%3D%2520%2520Real-world%2520image%2520super-resolution%2520%2528SR%2529%2520tasks%2520often%2520do%2520not%2520have%2520paired%250Adatasets%252C%2520which%2520limits%2520the%2520application%2520of%2520supervised%2520techniques.%2520As%2520a%2520result%252C%250Athe%2520tasks%2520are%2520usually%2520approached%2520by%2520unpaired%2520techniques%2520based%2520on%2520Generative%250AAdversarial%2520Networks%2520%2528GANs%2529%252C%2520which%2520yield%2520complex%2520training%2520losses%2520with%2520several%250Aregularization%2520terms%252C%2520e.g.%252C%2520content%2520or%2520identity%2520losses.%2520While%2520GANs%2520usually%250Aprovide%2520good%2520practical%2520performance%252C%2520they%2520are%2520used%2520heuristically%252C%2520i.e.%252C%250Atheoretical%2520understanding%2520of%2520their%2520behaviour%2520is%2520yet%2520rather%2520limited.%2520We%250Atheoretically%2520investigate%2520optimization%2520problems%2520which%2520arise%2520in%2520such%2520models%2520and%250Afind%2520two%2520surprising%2520observations.%2520First%252C%2520the%2520learned%2520SR%2520map%2520is%2520always%2520an%250Aoptimal%2520transport%2520%2528OT%2529%2520map.%2520Second%252C%2520we%2520theoretically%2520prove%2520and%2520empirically%2520show%250Athat%2520the%2520learned%2520map%2520is%2520biased%252C%2520i.e.%252C%2520it%2520does%2520not%2520actually%2520transform%2520the%250Adistribution%2520of%2520low-resolution%2520images%2520to%2520high-resolution%2520ones.%2520Inspired%2520by%250Athese%2520findings%252C%2520we%2520investigate%2520recent%2520advances%2520in%2520neural%2520OT%2520field%2520to%2520resolve%250Athe%2520bias%2520issue.%2520We%2520establish%2520an%2520intriguing%2520connection%2520between%2520regularized%2520GANs%250Aand%2520neural%2520OT%2520approaches.%2520We%2520show%2520that%2520unlike%2520the%2520existing%2520GAN-based%250Aalternatives%252C%2520these%2520algorithms%2520aim%2520to%2520learn%2520an%2520unbiased%2520OT%2520map.%2520We%2520empirically%250Ademonstrate%2520our%2520findings%2520via%2520a%2520series%2520of%2520synthetic%2520and%2520real-world%2520unpaired%2520SR%250Aexperiments.%2520Our%2520source%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/milenagazdieva/OT-Super-Resolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.01116v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Optimal%20Transport%20Perspective%20on%20Unpaired%20Image%20Super-Resolution&entry.906535625=Milena%20Gazdieva%20and%20Petr%20Mokrov%20and%20Litu%20Rout%20and%20Alexander%20Korotin%20and%20Andrey%20Kravchenko%20and%20Alexander%20Filippov%20and%20Evgeny%20Burnaev&entry.1292438233=%20%20Real-world%20image%20super-resolution%20%28SR%29%20tasks%20often%20do%20not%20have%20paired%0Adatasets%2C%20which%20limits%20the%20application%20of%20supervised%20techniques.%20As%20a%20result%2C%0Athe%20tasks%20are%20usually%20approached%20by%20unpaired%20techniques%20based%20on%20Generative%0AAdversarial%20Networks%20%28GANs%29%2C%20which%20yield%20complex%20training%20losses%20with%20several%0Aregularization%20terms%2C%20e.g.%2C%20content%20or%20identity%20losses.%20While%20GANs%20usually%0Aprovide%20good%20practical%20performance%2C%20they%20are%20used%20heuristically%2C%20i.e.%2C%0Atheoretical%20understanding%20of%20their%20behaviour%20is%20yet%20rather%20limited.%20We%0Atheoretically%20investigate%20optimization%20problems%20which%20arise%20in%20such%20models%20and%0Afind%20two%20surprising%20observations.%20First%2C%20the%20learned%20SR%20map%20is%20always%20an%0Aoptimal%20transport%20%28OT%29%20map.%20Second%2C%20we%20theoretically%20prove%20and%20empirically%20show%0Athat%20the%20learned%20map%20is%20biased%2C%20i.e.%2C%20it%20does%20not%20actually%20transform%20the%0Adistribution%20of%20low-resolution%20images%20to%20high-resolution%20ones.%20Inspired%20by%0Athese%20findings%2C%20we%20investigate%20recent%20advances%20in%20neural%20OT%20field%20to%20resolve%0Athe%20bias%20issue.%20We%20establish%20an%20intriguing%20connection%20between%20regularized%20GANs%0Aand%20neural%20OT%20approaches.%20We%20show%20that%20unlike%20the%20existing%20GAN-based%0Aalternatives%2C%20these%20algorithms%20aim%20to%20learn%20an%20unbiased%20OT%20map.%20We%20empirically%0Ademonstrate%20our%20findings%20via%20a%20series%20of%20synthetic%20and%20real-world%20unpaired%20SR%0Aexperiments.%20Our%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/milenagazdieva/OT-Super-Resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.01116v3&entry.124074799=Read"},
{"title": "Fast and Accurate Collision Probability Estimation for Autonomous\n  Vehicles using Adaptive Sigma-Point Sampling", "author": "Charles Champagne Cossette and Taylor Scott Clawson and Andrew Feit", "abstract": "  A novel algorithm is presented for the estimation of collision probabilities\nbetween dynamic objects with uncertain trajectories, where the trajectories are\ngiven as a sequence of poses with Gaussian distributions. We propose an\nadaptive sigma-point sampling scheme, which ultimately produces a fast, simple\nalgorithm capable of estimating the collision probability with a median error\nof 3.5%, and a median runtime of 0.21ms, when measured on an Intel Xeon Gold\n6226R Processor. Importantly, the algorithm explicitly accounts for the\ncollision probability's temporal dependence, which is often neglected in prior\nwork and otherwise leads to an overestimation of the collision probability.\nFinally, the method is tested on a diverse set of relevant real-world\nscenarios, consisting of 400 6-second snippets of autonomous vehicle logs,\nwhere the accuracy and latency is rigorously evaluated.\n", "link": "http://arxiv.org/abs/2507.06149v1", "date": "2025-07-08", "relevancy": 2.1708, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.572}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5378}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Accurate%20Collision%20Probability%20Estimation%20for%20Autonomous%0A%20%20Vehicles%20using%20Adaptive%20Sigma-Point%20Sampling&body=Title%3A%20Fast%20and%20Accurate%20Collision%20Probability%20Estimation%20for%20Autonomous%0A%20%20Vehicles%20using%20Adaptive%20Sigma-Point%20Sampling%0AAuthor%3A%20Charles%20Champagne%20Cossette%20and%20Taylor%20Scott%20Clawson%20and%20Andrew%20Feit%0AAbstract%3A%20%20%20A%20novel%20algorithm%20is%20presented%20for%20the%20estimation%20of%20collision%20probabilities%0Abetween%20dynamic%20objects%20with%20uncertain%20trajectories%2C%20where%20the%20trajectories%20are%0Agiven%20as%20a%20sequence%20of%20poses%20with%20Gaussian%20distributions.%20We%20propose%20an%0Aadaptive%20sigma-point%20sampling%20scheme%2C%20which%20ultimately%20produces%20a%20fast%2C%20simple%0Aalgorithm%20capable%20of%20estimating%20the%20collision%20probability%20with%20a%20median%20error%0Aof%203.5%25%2C%20and%20a%20median%20runtime%20of%200.21ms%2C%20when%20measured%20on%20an%20Intel%20Xeon%20Gold%0A6226R%20Processor.%20Importantly%2C%20the%20algorithm%20explicitly%20accounts%20for%20the%0Acollision%20probability%27s%20temporal%20dependence%2C%20which%20is%20often%20neglected%20in%20prior%0Awork%20and%20otherwise%20leads%20to%20an%20overestimation%20of%20the%20collision%20probability.%0AFinally%2C%20the%20method%20is%20tested%20on%20a%20diverse%20set%20of%20relevant%20real-world%0Ascenarios%2C%20consisting%20of%20400%206-second%20snippets%20of%20autonomous%20vehicle%20logs%2C%0Awhere%20the%20accuracy%20and%20latency%20is%20rigorously%20evaluated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Accurate%2520Collision%2520Probability%2520Estimation%2520for%2520Autonomous%250A%2520%2520Vehicles%2520using%2520Adaptive%2520Sigma-Point%2520Sampling%26entry.906535625%3DCharles%2520Champagne%2520Cossette%2520and%2520Taylor%2520Scott%2520Clawson%2520and%2520Andrew%2520Feit%26entry.1292438233%3D%2520%2520A%2520novel%2520algorithm%2520is%2520presented%2520for%2520the%2520estimation%2520of%2520collision%2520probabilities%250Abetween%2520dynamic%2520objects%2520with%2520uncertain%2520trajectories%252C%2520where%2520the%2520trajectories%2520are%250Agiven%2520as%2520a%2520sequence%2520of%2520poses%2520with%2520Gaussian%2520distributions.%2520We%2520propose%2520an%250Aadaptive%2520sigma-point%2520sampling%2520scheme%252C%2520which%2520ultimately%2520produces%2520a%2520fast%252C%2520simple%250Aalgorithm%2520capable%2520of%2520estimating%2520the%2520collision%2520probability%2520with%2520a%2520median%2520error%250Aof%25203.5%2525%252C%2520and%2520a%2520median%2520runtime%2520of%25200.21ms%252C%2520when%2520measured%2520on%2520an%2520Intel%2520Xeon%2520Gold%250A6226R%2520Processor.%2520Importantly%252C%2520the%2520algorithm%2520explicitly%2520accounts%2520for%2520the%250Acollision%2520probability%2527s%2520temporal%2520dependence%252C%2520which%2520is%2520often%2520neglected%2520in%2520prior%250Awork%2520and%2520otherwise%2520leads%2520to%2520an%2520overestimation%2520of%2520the%2520collision%2520probability.%250AFinally%252C%2520the%2520method%2520is%2520tested%2520on%2520a%2520diverse%2520set%2520of%2520relevant%2520real-world%250Ascenarios%252C%2520consisting%2520of%2520400%25206-second%2520snippets%2520of%2520autonomous%2520vehicle%2520logs%252C%250Awhere%2520the%2520accuracy%2520and%2520latency%2520is%2520rigorously%2520evaluated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Accurate%20Collision%20Probability%20Estimation%20for%20Autonomous%0A%20%20Vehicles%20using%20Adaptive%20Sigma-Point%20Sampling&entry.906535625=Charles%20Champagne%20Cossette%20and%20Taylor%20Scott%20Clawson%20and%20Andrew%20Feit&entry.1292438233=%20%20A%20novel%20algorithm%20is%20presented%20for%20the%20estimation%20of%20collision%20probabilities%0Abetween%20dynamic%20objects%20with%20uncertain%20trajectories%2C%20where%20the%20trajectories%20are%0Agiven%20as%20a%20sequence%20of%20poses%20with%20Gaussian%20distributions.%20We%20propose%20an%0Aadaptive%20sigma-point%20sampling%20scheme%2C%20which%20ultimately%20produces%20a%20fast%2C%20simple%0Aalgorithm%20capable%20of%20estimating%20the%20collision%20probability%20with%20a%20median%20error%0Aof%203.5%25%2C%20and%20a%20median%20runtime%20of%200.21ms%2C%20when%20measured%20on%20an%20Intel%20Xeon%20Gold%0A6226R%20Processor.%20Importantly%2C%20the%20algorithm%20explicitly%20accounts%20for%20the%0Acollision%20probability%27s%20temporal%20dependence%2C%20which%20is%20often%20neglected%20in%20prior%0Awork%20and%20otherwise%20leads%20to%20an%20overestimation%20of%20the%20collision%20probability.%0AFinally%2C%20the%20method%20is%20tested%20on%20a%20diverse%20set%20of%20relevant%20real-world%0Ascenarios%2C%20consisting%20of%20400%206-second%20snippets%20of%20autonomous%20vehicle%20logs%2C%0Awhere%20the%20accuracy%20and%20latency%20is%20rigorously%20evaluated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06149v1&entry.124074799=Read"},
{"title": "Taming Data Challenges in ML-based Security Tasks: Lessons from\n  Integrating Generative AI", "author": "Shravya Kanchi and Neal Mangaokar and Aravind Cheruvu and Sifat Muhammad Abdullah and Shirin Nilizadeh and Atul Prakash and Bimal Viswanath", "abstract": "  Machine learning-based supervised classifiers are widely used for security\ntasks, and their improvement has been largely focused on algorithmic\nadvancements. We argue that data challenges that negatively impact the\nperformance of these classifiers have received limited attention. We address\nthe following research question: Can developments in Generative AI (GenAI)\naddress these data challenges and improve classifier performance? We propose\naugmenting training datasets with synthetic data generated using GenAI\ntechniques to improve classifier generalization. We evaluate this approach\nacross 7 diverse security tasks using 6 state-of-the-art GenAI methods and\nintroduce a novel GenAI scheme called Nimai that enables highly controlled data\nsynthesis. We find that GenAI techniques can significantly improve the\nperformance of security classifiers, achieving improvements of up to 32.6% even\nin severely data-constrained settings (only ~180 training samples).\nFurthermore, we demonstrate that GenAI can facilitate rapid adaptation to\nconcept drift post-deployment, requiring minimal labeling in the adjustment\nprocess. Despite successes, our study finds that some GenAI schemes struggle to\ninitialize (train and produce data) on certain security tasks. We also identify\ncharacteristics of specific tasks, such as noisy labels, overlapping class\ndistributions, and sparse feature vectors, which hinder performance boost using\nGenAI. We believe that our study will drive the development of future GenAI\ntools designed for security tasks.\n", "link": "http://arxiv.org/abs/2507.06092v1", "date": "2025-07-08", "relevancy": 2.1667, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5744}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.521}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20Data%20Challenges%20in%20ML-based%20Security%20Tasks%3A%20Lessons%20from%0A%20%20Integrating%20Generative%20AI&body=Title%3A%20Taming%20Data%20Challenges%20in%20ML-based%20Security%20Tasks%3A%20Lessons%20from%0A%20%20Integrating%20Generative%20AI%0AAuthor%3A%20Shravya%20Kanchi%20and%20Neal%20Mangaokar%20and%20Aravind%20Cheruvu%20and%20Sifat%20Muhammad%20Abdullah%20and%20Shirin%20Nilizadeh%20and%20Atul%20Prakash%20and%20Bimal%20Viswanath%0AAbstract%3A%20%20%20Machine%20learning-based%20supervised%20classifiers%20are%20widely%20used%20for%20security%0Atasks%2C%20and%20their%20improvement%20has%20been%20largely%20focused%20on%20algorithmic%0Aadvancements.%20We%20argue%20that%20data%20challenges%20that%20negatively%20impact%20the%0Aperformance%20of%20these%20classifiers%20have%20received%20limited%20attention.%20We%20address%0Athe%20following%20research%20question%3A%20Can%20developments%20in%20Generative%20AI%20%28GenAI%29%0Aaddress%20these%20data%20challenges%20and%20improve%20classifier%20performance%3F%20We%20propose%0Aaugmenting%20training%20datasets%20with%20synthetic%20data%20generated%20using%20GenAI%0Atechniques%20to%20improve%20classifier%20generalization.%20We%20evaluate%20this%20approach%0Aacross%207%20diverse%20security%20tasks%20using%206%20state-of-the-art%20GenAI%20methods%20and%0Aintroduce%20a%20novel%20GenAI%20scheme%20called%20Nimai%20that%20enables%20highly%20controlled%20data%0Asynthesis.%20We%20find%20that%20GenAI%20techniques%20can%20significantly%20improve%20the%0Aperformance%20of%20security%20classifiers%2C%20achieving%20improvements%20of%20up%20to%2032.6%25%20even%0Ain%20severely%20data-constrained%20settings%20%28only%20~180%20training%20samples%29.%0AFurthermore%2C%20we%20demonstrate%20that%20GenAI%20can%20facilitate%20rapid%20adaptation%20to%0Aconcept%20drift%20post-deployment%2C%20requiring%20minimal%20labeling%20in%20the%20adjustment%0Aprocess.%20Despite%20successes%2C%20our%20study%20finds%20that%20some%20GenAI%20schemes%20struggle%20to%0Ainitialize%20%28train%20and%20produce%20data%29%20on%20certain%20security%20tasks.%20We%20also%20identify%0Acharacteristics%20of%20specific%20tasks%2C%20such%20as%20noisy%20labels%2C%20overlapping%20class%0Adistributions%2C%20and%20sparse%20feature%20vectors%2C%20which%20hinder%20performance%20boost%20using%0AGenAI.%20We%20believe%20that%20our%20study%20will%20drive%20the%20development%20of%20future%20GenAI%0Atools%20designed%20for%20security%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520Data%2520Challenges%2520in%2520ML-based%2520Security%2520Tasks%253A%2520Lessons%2520from%250A%2520%2520Integrating%2520Generative%2520AI%26entry.906535625%3DShravya%2520Kanchi%2520and%2520Neal%2520Mangaokar%2520and%2520Aravind%2520Cheruvu%2520and%2520Sifat%2520Muhammad%2520Abdullah%2520and%2520Shirin%2520Nilizadeh%2520and%2520Atul%2520Prakash%2520and%2520Bimal%2520Viswanath%26entry.1292438233%3D%2520%2520Machine%2520learning-based%2520supervised%2520classifiers%2520are%2520widely%2520used%2520for%2520security%250Atasks%252C%2520and%2520their%2520improvement%2520has%2520been%2520largely%2520focused%2520on%2520algorithmic%250Aadvancements.%2520We%2520argue%2520that%2520data%2520challenges%2520that%2520negatively%2520impact%2520the%250Aperformance%2520of%2520these%2520classifiers%2520have%2520received%2520limited%2520attention.%2520We%2520address%250Athe%2520following%2520research%2520question%253A%2520Can%2520developments%2520in%2520Generative%2520AI%2520%2528GenAI%2529%250Aaddress%2520these%2520data%2520challenges%2520and%2520improve%2520classifier%2520performance%253F%2520We%2520propose%250Aaugmenting%2520training%2520datasets%2520with%2520synthetic%2520data%2520generated%2520using%2520GenAI%250Atechniques%2520to%2520improve%2520classifier%2520generalization.%2520We%2520evaluate%2520this%2520approach%250Aacross%25207%2520diverse%2520security%2520tasks%2520using%25206%2520state-of-the-art%2520GenAI%2520methods%2520and%250Aintroduce%2520a%2520novel%2520GenAI%2520scheme%2520called%2520Nimai%2520that%2520enables%2520highly%2520controlled%2520data%250Asynthesis.%2520We%2520find%2520that%2520GenAI%2520techniques%2520can%2520significantly%2520improve%2520the%250Aperformance%2520of%2520security%2520classifiers%252C%2520achieving%2520improvements%2520of%2520up%2520to%252032.6%2525%2520even%250Ain%2520severely%2520data-constrained%2520settings%2520%2528only%2520~180%2520training%2520samples%2529.%250AFurthermore%252C%2520we%2520demonstrate%2520that%2520GenAI%2520can%2520facilitate%2520rapid%2520adaptation%2520to%250Aconcept%2520drift%2520post-deployment%252C%2520requiring%2520minimal%2520labeling%2520in%2520the%2520adjustment%250Aprocess.%2520Despite%2520successes%252C%2520our%2520study%2520finds%2520that%2520some%2520GenAI%2520schemes%2520struggle%2520to%250Ainitialize%2520%2528train%2520and%2520produce%2520data%2529%2520on%2520certain%2520security%2520tasks.%2520We%2520also%2520identify%250Acharacteristics%2520of%2520specific%2520tasks%252C%2520such%2520as%2520noisy%2520labels%252C%2520overlapping%2520class%250Adistributions%252C%2520and%2520sparse%2520feature%2520vectors%252C%2520which%2520hinder%2520performance%2520boost%2520using%250AGenAI.%2520We%2520believe%2520that%2520our%2520study%2520will%2520drive%2520the%2520development%2520of%2520future%2520GenAI%250Atools%2520designed%2520for%2520security%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Data%20Challenges%20in%20ML-based%20Security%20Tasks%3A%20Lessons%20from%0A%20%20Integrating%20Generative%20AI&entry.906535625=Shravya%20Kanchi%20and%20Neal%20Mangaokar%20and%20Aravind%20Cheruvu%20and%20Sifat%20Muhammad%20Abdullah%20and%20Shirin%20Nilizadeh%20and%20Atul%20Prakash%20and%20Bimal%20Viswanath&entry.1292438233=%20%20Machine%20learning-based%20supervised%20classifiers%20are%20widely%20used%20for%20security%0Atasks%2C%20and%20their%20improvement%20has%20been%20largely%20focused%20on%20algorithmic%0Aadvancements.%20We%20argue%20that%20data%20challenges%20that%20negatively%20impact%20the%0Aperformance%20of%20these%20classifiers%20have%20received%20limited%20attention.%20We%20address%0Athe%20following%20research%20question%3A%20Can%20developments%20in%20Generative%20AI%20%28GenAI%29%0Aaddress%20these%20data%20challenges%20and%20improve%20classifier%20performance%3F%20We%20propose%0Aaugmenting%20training%20datasets%20with%20synthetic%20data%20generated%20using%20GenAI%0Atechniques%20to%20improve%20classifier%20generalization.%20We%20evaluate%20this%20approach%0Aacross%207%20diverse%20security%20tasks%20using%206%20state-of-the-art%20GenAI%20methods%20and%0Aintroduce%20a%20novel%20GenAI%20scheme%20called%20Nimai%20that%20enables%20highly%20controlled%20data%0Asynthesis.%20We%20find%20that%20GenAI%20techniques%20can%20significantly%20improve%20the%0Aperformance%20of%20security%20classifiers%2C%20achieving%20improvements%20of%20up%20to%2032.6%25%20even%0Ain%20severely%20data-constrained%20settings%20%28only%20~180%20training%20samples%29.%0AFurthermore%2C%20we%20demonstrate%20that%20GenAI%20can%20facilitate%20rapid%20adaptation%20to%0Aconcept%20drift%20post-deployment%2C%20requiring%20minimal%20labeling%20in%20the%20adjustment%0Aprocess.%20Despite%20successes%2C%20our%20study%20finds%20that%20some%20GenAI%20schemes%20struggle%20to%0Ainitialize%20%28train%20and%20produce%20data%29%20on%20certain%20security%20tasks.%20We%20also%20identify%0Acharacteristics%20of%20specific%20tasks%2C%20such%20as%20noisy%20labels%2C%20overlapping%20class%0Adistributions%2C%20and%20sparse%20feature%20vectors%2C%20which%20hinder%20performance%20boost%20using%0AGenAI.%20We%20believe%20that%20our%20study%20will%20drive%20the%20development%20of%20future%20GenAI%0Atools%20designed%20for%20security%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06092v1&entry.124074799=Read"},
{"title": "NeoBabel: A Multilingual Open Tower for Visual Generation", "author": "Mohammad Mahdi Derakhshani and Dheeraj Varghese and Marzieh Fadaee and Cees G. M. Snoek", "abstract": "  Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI.\n", "link": "http://arxiv.org/abs/2507.06137v1", "date": "2025-07-08", "relevancy": 2.1658, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5534}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeoBabel%3A%20A%20Multilingual%20Open%20Tower%20for%20Visual%20Generation&body=Title%3A%20NeoBabel%3A%20A%20Multilingual%20Open%20Tower%20for%20Visual%20Generation%0AAuthor%3A%20Mohammad%20Mahdi%20Derakhshani%20and%20Dheeraj%20Varghese%20and%20Marzieh%20Fadaee%20and%20Cees%20G.%20M.%20Snoek%0AAbstract%3A%20%20%20Text-to-image%20generation%20advancements%20have%20been%20predominantly%0AEnglish-centric%2C%20creating%20barriers%20for%20non-English%20speakers%20and%20perpetuating%0Adigital%20inequities.%20While%20existing%20systems%20rely%20on%20translation%20pipelines%2C%20these%0Aintroduce%20semantic%20drift%2C%20computational%20overhead%2C%20and%20cultural%20misalignment.%20We%0Aintroduce%20NeoBabel%2C%20a%20novel%20multilingual%20image%20generation%20framework%20that%20sets%20a%0Anew%20Pareto%20frontier%20in%20performance%2C%20efficiency%20and%20inclusivity%2C%20supporting%20six%0Alanguages%3A%20English%2C%20Chinese%2C%20Dutch%2C%20French%2C%20Hindi%2C%20and%20Persian.%20The%20model%20is%0Atrained%20using%20a%20combination%20of%20large-scale%20multilingual%20pretraining%20and%0Ahigh-resolution%20instruction%20tuning.%20To%20evaluate%20its%20capabilities%2C%20we%20expand%20two%0AEnglish-only%20benchmarks%20to%20multilingual%20equivalents%3A%20m-GenEval%20and%20m-DPG.%0ANeoBabel%20achieves%20state-of-the-art%20multilingual%20performance%20while%20retaining%0Astrong%20English%20capability%2C%20scoring%200.75%20on%20m-GenEval%20and%200.68%20on%20m-DPG.%0ANotably%2C%20it%20performs%20on%20par%20with%20leading%20models%20on%20English%20tasks%20while%0Aoutperforming%20them%20by%20%2B0.11%20and%20%2B0.09%20on%20multilingual%20benchmarks%2C%20even%20though%0Athese%20models%20are%20built%20on%20multilingual%20base%20LLMs.%20This%20demonstrates%20the%0Aeffectiveness%20of%20our%20targeted%20alignment%20training%20for%20preserving%20and%20extending%0Acrosslingual%20generalization.%20We%20further%20introduce%20two%20new%20metrics%20to%20rigorously%0Aassess%20multilingual%20alignment%20and%20robustness%20to%20code-mixed%20prompts.%20Notably%2C%0ANeoBabel%20matches%20or%20exceeds%20English-only%20models%20while%20being%202-4x%20smaller.%20We%0Arelease%20an%20open%20toolkit%2C%20including%20all%20code%2C%20model%20checkpoints%2C%20a%20curated%0Adataset%20of%20124M%20multilingual%20text-image%20pairs%2C%20and%20standardized%20multilingual%0Aevaluation%20protocols%2C%20to%20advance%20inclusive%20AI%20research.%20Our%20work%20demonstrates%0Athat%20multilingual%20capability%20is%20not%20a%20trade-off%20but%20a%20catalyst%20for%20improved%0Arobustness%2C%20efficiency%2C%20and%20cultural%20fidelity%20in%20generative%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeoBabel%253A%2520A%2520Multilingual%2520Open%2520Tower%2520for%2520Visual%2520Generation%26entry.906535625%3DMohammad%2520Mahdi%2520Derakhshani%2520and%2520Dheeraj%2520Varghese%2520and%2520Marzieh%2520Fadaee%2520and%2520Cees%2520G.%2520M.%2520Snoek%26entry.1292438233%3D%2520%2520Text-to-image%2520generation%2520advancements%2520have%2520been%2520predominantly%250AEnglish-centric%252C%2520creating%2520barriers%2520for%2520non-English%2520speakers%2520and%2520perpetuating%250Adigital%2520inequities.%2520While%2520existing%2520systems%2520rely%2520on%2520translation%2520pipelines%252C%2520these%250Aintroduce%2520semantic%2520drift%252C%2520computational%2520overhead%252C%2520and%2520cultural%2520misalignment.%2520We%250Aintroduce%2520NeoBabel%252C%2520a%2520novel%2520multilingual%2520image%2520generation%2520framework%2520that%2520sets%2520a%250Anew%2520Pareto%2520frontier%2520in%2520performance%252C%2520efficiency%2520and%2520inclusivity%252C%2520supporting%2520six%250Alanguages%253A%2520English%252C%2520Chinese%252C%2520Dutch%252C%2520French%252C%2520Hindi%252C%2520and%2520Persian.%2520The%2520model%2520is%250Atrained%2520using%2520a%2520combination%2520of%2520large-scale%2520multilingual%2520pretraining%2520and%250Ahigh-resolution%2520instruction%2520tuning.%2520To%2520evaluate%2520its%2520capabilities%252C%2520we%2520expand%2520two%250AEnglish-only%2520benchmarks%2520to%2520multilingual%2520equivalents%253A%2520m-GenEval%2520and%2520m-DPG.%250ANeoBabel%2520achieves%2520state-of-the-art%2520multilingual%2520performance%2520while%2520retaining%250Astrong%2520English%2520capability%252C%2520scoring%25200.75%2520on%2520m-GenEval%2520and%25200.68%2520on%2520m-DPG.%250ANotably%252C%2520it%2520performs%2520on%2520par%2520with%2520leading%2520models%2520on%2520English%2520tasks%2520while%250Aoutperforming%2520them%2520by%2520%252B0.11%2520and%2520%252B0.09%2520on%2520multilingual%2520benchmarks%252C%2520even%2520though%250Athese%2520models%2520are%2520built%2520on%2520multilingual%2520base%2520LLMs.%2520This%2520demonstrates%2520the%250Aeffectiveness%2520of%2520our%2520targeted%2520alignment%2520training%2520for%2520preserving%2520and%2520extending%250Acrosslingual%2520generalization.%2520We%2520further%2520introduce%2520two%2520new%2520metrics%2520to%2520rigorously%250Aassess%2520multilingual%2520alignment%2520and%2520robustness%2520to%2520code-mixed%2520prompts.%2520Notably%252C%250ANeoBabel%2520matches%2520or%2520exceeds%2520English-only%2520models%2520while%2520being%25202-4x%2520smaller.%2520We%250Arelease%2520an%2520open%2520toolkit%252C%2520including%2520all%2520code%252C%2520model%2520checkpoints%252C%2520a%2520curated%250Adataset%2520of%2520124M%2520multilingual%2520text-image%2520pairs%252C%2520and%2520standardized%2520multilingual%250Aevaluation%2520protocols%252C%2520to%2520advance%2520inclusive%2520AI%2520research.%2520Our%2520work%2520demonstrates%250Athat%2520multilingual%2520capability%2520is%2520not%2520a%2520trade-off%2520but%2520a%2520catalyst%2520for%2520improved%250Arobustness%252C%2520efficiency%252C%2520and%2520cultural%2520fidelity%2520in%2520generative%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeoBabel%3A%20A%20Multilingual%20Open%20Tower%20for%20Visual%20Generation&entry.906535625=Mohammad%20Mahdi%20Derakhshani%20and%20Dheeraj%20Varghese%20and%20Marzieh%20Fadaee%20and%20Cees%20G.%20M.%20Snoek&entry.1292438233=%20%20Text-to-image%20generation%20advancements%20have%20been%20predominantly%0AEnglish-centric%2C%20creating%20barriers%20for%20non-English%20speakers%20and%20perpetuating%0Adigital%20inequities.%20While%20existing%20systems%20rely%20on%20translation%20pipelines%2C%20these%0Aintroduce%20semantic%20drift%2C%20computational%20overhead%2C%20and%20cultural%20misalignment.%20We%0Aintroduce%20NeoBabel%2C%20a%20novel%20multilingual%20image%20generation%20framework%20that%20sets%20a%0Anew%20Pareto%20frontier%20in%20performance%2C%20efficiency%20and%20inclusivity%2C%20supporting%20six%0Alanguages%3A%20English%2C%20Chinese%2C%20Dutch%2C%20French%2C%20Hindi%2C%20and%20Persian.%20The%20model%20is%0Atrained%20using%20a%20combination%20of%20large-scale%20multilingual%20pretraining%20and%0Ahigh-resolution%20instruction%20tuning.%20To%20evaluate%20its%20capabilities%2C%20we%20expand%20two%0AEnglish-only%20benchmarks%20to%20multilingual%20equivalents%3A%20m-GenEval%20and%20m-DPG.%0ANeoBabel%20achieves%20state-of-the-art%20multilingual%20performance%20while%20retaining%0Astrong%20English%20capability%2C%20scoring%200.75%20on%20m-GenEval%20and%200.68%20on%20m-DPG.%0ANotably%2C%20it%20performs%20on%20par%20with%20leading%20models%20on%20English%20tasks%20while%0Aoutperforming%20them%20by%20%2B0.11%20and%20%2B0.09%20on%20multilingual%20benchmarks%2C%20even%20though%0Athese%20models%20are%20built%20on%20multilingual%20base%20LLMs.%20This%20demonstrates%20the%0Aeffectiveness%20of%20our%20targeted%20alignment%20training%20for%20preserving%20and%20extending%0Acrosslingual%20generalization.%20We%20further%20introduce%20two%20new%20metrics%20to%20rigorously%0Aassess%20multilingual%20alignment%20and%20robustness%20to%20code-mixed%20prompts.%20Notably%2C%0ANeoBabel%20matches%20or%20exceeds%20English-only%20models%20while%20being%202-4x%20smaller.%20We%0Arelease%20an%20open%20toolkit%2C%20including%20all%20code%2C%20model%20checkpoints%2C%20a%20curated%0Adataset%20of%20124M%20multilingual%20text-image%20pairs%2C%20and%20standardized%20multilingual%0Aevaluation%20protocols%2C%20to%20advance%20inclusive%20AI%20research.%20Our%20work%20demonstrates%0Athat%20multilingual%20capability%20is%20not%20a%20trade-off%20but%20a%20catalyst%20for%20improved%0Arobustness%2C%20efficiency%2C%20and%20cultural%20fidelity%20in%20generative%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06137v1&entry.124074799=Read"},
{"title": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging\n  Control Integrating Large Language Models", "author": "Miao Zhang and Zhenlong Fang and Tianyi Wang and Qian Zhang and Shuai Lu and Junfeng Jiao and Tianyu Shi", "abstract": "  Traditional Reinforcement Learning (RL) suffers from replicating human-like\nbehaviors, generalizing effectively in multi-agent scenarios, and overcoming\ninherent interpretability issues.These tasks are compounded when deep\nenvironment understanding, agent coordination and dynamic optimization are\nrequired. While Large Language Model (LLM) enhanced methods have shown promise\nin generalization and interoperability, they often neglect necessary\nmulti-agent coordination. Therefore, we introduce the Cascading Cooperative\nMulti-agent (CCMA) framework, integrating RL for individual interactions, a\nfine-tuned LLM for regional cooperation, a reward function for global\noptimization, and the Retrieval-augmented Generation mechanism to dynamically\noptimize decision-making across complex driving scenarios. Our experiments\ndemonstrate that the CCMA outperforms existing RL methods, demonstrating\nsignificant improvements in both micro and macro-level performance in complex\ndriving environments.\n", "link": "http://arxiv.org/abs/2503.08199v2", "date": "2025-07-08", "relevancy": 2.1576, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5446}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5423}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Cascading%20Cooperative%20Multi-agent%20Framework%20for%20On-ramp%20Merging%0A%20%20Control%20Integrating%20Large%20Language%20Models&body=Title%3A%20A%20Cascading%20Cooperative%20Multi-agent%20Framework%20for%20On-ramp%20Merging%0A%20%20Control%20Integrating%20Large%20Language%20Models%0AAuthor%3A%20Miao%20Zhang%20and%20Zhenlong%20Fang%20and%20Tianyi%20Wang%20and%20Qian%20Zhang%20and%20Shuai%20Lu%20and%20Junfeng%20Jiao%20and%20Tianyu%20Shi%0AAbstract%3A%20%20%20Traditional%20Reinforcement%20Learning%20%28RL%29%20suffers%20from%20replicating%20human-like%0Abehaviors%2C%20generalizing%20effectively%20in%20multi-agent%20scenarios%2C%20and%20overcoming%0Ainherent%20interpretability%20issues.These%20tasks%20are%20compounded%20when%20deep%0Aenvironment%20understanding%2C%20agent%20coordination%20and%20dynamic%20optimization%20are%0Arequired.%20While%20Large%20Language%20Model%20%28LLM%29%20enhanced%20methods%20have%20shown%20promise%0Ain%20generalization%20and%20interoperability%2C%20they%20often%20neglect%20necessary%0Amulti-agent%20coordination.%20Therefore%2C%20we%20introduce%20the%20Cascading%20Cooperative%0AMulti-agent%20%28CCMA%29%20framework%2C%20integrating%20RL%20for%20individual%20interactions%2C%20a%0Afine-tuned%20LLM%20for%20regional%20cooperation%2C%20a%20reward%20function%20for%20global%0Aoptimization%2C%20and%20the%20Retrieval-augmented%20Generation%20mechanism%20to%20dynamically%0Aoptimize%20decision-making%20across%20complex%20driving%20scenarios.%20Our%20experiments%0Ademonstrate%20that%20the%20CCMA%20outperforms%20existing%20RL%20methods%2C%20demonstrating%0Asignificant%20improvements%20in%20both%20micro%20and%20macro-level%20performance%20in%20complex%0Adriving%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08199v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Cascading%2520Cooperative%2520Multi-agent%2520Framework%2520for%2520On-ramp%2520Merging%250A%2520%2520Control%2520Integrating%2520Large%2520Language%2520Models%26entry.906535625%3DMiao%2520Zhang%2520and%2520Zhenlong%2520Fang%2520and%2520Tianyi%2520Wang%2520and%2520Qian%2520Zhang%2520and%2520Shuai%2520Lu%2520and%2520Junfeng%2520Jiao%2520and%2520Tianyu%2520Shi%26entry.1292438233%3D%2520%2520Traditional%2520Reinforcement%2520Learning%2520%2528RL%2529%2520suffers%2520from%2520replicating%2520human-like%250Abehaviors%252C%2520generalizing%2520effectively%2520in%2520multi-agent%2520scenarios%252C%2520and%2520overcoming%250Ainherent%2520interpretability%2520issues.These%2520tasks%2520are%2520compounded%2520when%2520deep%250Aenvironment%2520understanding%252C%2520agent%2520coordination%2520and%2520dynamic%2520optimization%2520are%250Arequired.%2520While%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520enhanced%2520methods%2520have%2520shown%2520promise%250Ain%2520generalization%2520and%2520interoperability%252C%2520they%2520often%2520neglect%2520necessary%250Amulti-agent%2520coordination.%2520Therefore%252C%2520we%2520introduce%2520the%2520Cascading%2520Cooperative%250AMulti-agent%2520%2528CCMA%2529%2520framework%252C%2520integrating%2520RL%2520for%2520individual%2520interactions%252C%2520a%250Afine-tuned%2520LLM%2520for%2520regional%2520cooperation%252C%2520a%2520reward%2520function%2520for%2520global%250Aoptimization%252C%2520and%2520the%2520Retrieval-augmented%2520Generation%2520mechanism%2520to%2520dynamically%250Aoptimize%2520decision-making%2520across%2520complex%2520driving%2520scenarios.%2520Our%2520experiments%250Ademonstrate%2520that%2520the%2520CCMA%2520outperforms%2520existing%2520RL%2520methods%252C%2520demonstrating%250Asignificant%2520improvements%2520in%2520both%2520micro%2520and%2520macro-level%2520performance%2520in%2520complex%250Adriving%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08199v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Cascading%20Cooperative%20Multi-agent%20Framework%20for%20On-ramp%20Merging%0A%20%20Control%20Integrating%20Large%20Language%20Models&entry.906535625=Miao%20Zhang%20and%20Zhenlong%20Fang%20and%20Tianyi%20Wang%20and%20Qian%20Zhang%20and%20Shuai%20Lu%20and%20Junfeng%20Jiao%20and%20Tianyu%20Shi&entry.1292438233=%20%20Traditional%20Reinforcement%20Learning%20%28RL%29%20suffers%20from%20replicating%20human-like%0Abehaviors%2C%20generalizing%20effectively%20in%20multi-agent%20scenarios%2C%20and%20overcoming%0Ainherent%20interpretability%20issues.These%20tasks%20are%20compounded%20when%20deep%0Aenvironment%20understanding%2C%20agent%20coordination%20and%20dynamic%20optimization%20are%0Arequired.%20While%20Large%20Language%20Model%20%28LLM%29%20enhanced%20methods%20have%20shown%20promise%0Ain%20generalization%20and%20interoperability%2C%20they%20often%20neglect%20necessary%0Amulti-agent%20coordination.%20Therefore%2C%20we%20introduce%20the%20Cascading%20Cooperative%0AMulti-agent%20%28CCMA%29%20framework%2C%20integrating%20RL%20for%20individual%20interactions%2C%20a%0Afine-tuned%20LLM%20for%20regional%20cooperation%2C%20a%20reward%20function%20for%20global%0Aoptimization%2C%20and%20the%20Retrieval-augmented%20Generation%20mechanism%20to%20dynamically%0Aoptimize%20decision-making%20across%20complex%20driving%20scenarios.%20Our%20experiments%0Ademonstrate%20that%20the%20CCMA%20outperforms%20existing%20RL%20methods%2C%20demonstrating%0Asignificant%20improvements%20in%20both%20micro%20and%20macro-level%20performance%20in%20complex%0Adriving%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08199v2&entry.124074799=Read"},
{"title": "Future Slot Prediction for Unsupervised Object Discovery in Surgical\n  Video", "author": "Guiqiu Liao and Matjaz Jogan and Marcel Hussing and Edward Zhang and Eric Eaton and Daniel A. Hashimoto", "abstract": "  Object-centric slot attention is an emerging paradigm for unsupervised\nlearning of structured, interpretable object-centric representations (slots).\nThis enables effective reasoning about objects and events at a low\ncomputational cost and is thus applicable to critical healthcare applications,\nsuch as real-time interpretation of surgical video. The heterogeneous scenes in\nreal-world applications like surgery are, however, difficult to parse into a\nmeaningful set of slots. Current approaches with an adaptive slot count perform\nwell on images, but their performance on surgical videos is low. To address\nthis challenge, we propose a dynamic temporal slot transformer (DTST) module\nthat is trained both for temporal reasoning and for predicting the optimal\nfuture slot initialization. The model achieves state-of-the-art performance on\nmultiple surgical databases, demonstrating that unsupervised object-centric\nmethods can be applied to real-world data and become part of the common arsenal\nin healthcare applications.\n", "link": "http://arxiv.org/abs/2507.01882v2", "date": "2025-07-08", "relevancy": 2.1346, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5417}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5301}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Future%20Slot%20Prediction%20for%20Unsupervised%20Object%20Discovery%20in%20Surgical%0A%20%20Video&body=Title%3A%20Future%20Slot%20Prediction%20for%20Unsupervised%20Object%20Discovery%20in%20Surgical%0A%20%20Video%0AAuthor%3A%20Guiqiu%20Liao%20and%20Matjaz%20Jogan%20and%20Marcel%20Hussing%20and%20Edward%20Zhang%20and%20Eric%20Eaton%20and%20Daniel%20A.%20Hashimoto%0AAbstract%3A%20%20%20Object-centric%20slot%20attention%20is%20an%20emerging%20paradigm%20for%20unsupervised%0Alearning%20of%20structured%2C%20interpretable%20object-centric%20representations%20%28slots%29.%0AThis%20enables%20effective%20reasoning%20about%20objects%20and%20events%20at%20a%20low%0Acomputational%20cost%20and%20is%20thus%20applicable%20to%20critical%20healthcare%20applications%2C%0Asuch%20as%20real-time%20interpretation%20of%20surgical%20video.%20The%20heterogeneous%20scenes%20in%0Areal-world%20applications%20like%20surgery%20are%2C%20however%2C%20difficult%20to%20parse%20into%20a%0Ameaningful%20set%20of%20slots.%20Current%20approaches%20with%20an%20adaptive%20slot%20count%20perform%0Awell%20on%20images%2C%20but%20their%20performance%20on%20surgical%20videos%20is%20low.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20dynamic%20temporal%20slot%20transformer%20%28DTST%29%20module%0Athat%20is%20trained%20both%20for%20temporal%20reasoning%20and%20for%20predicting%20the%20optimal%0Afuture%20slot%20initialization.%20The%20model%20achieves%20state-of-the-art%20performance%20on%0Amultiple%20surgical%20databases%2C%20demonstrating%20that%20unsupervised%20object-centric%0Amethods%20can%20be%20applied%20to%20real-world%20data%20and%20become%20part%20of%20the%20common%20arsenal%0Ain%20healthcare%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01882v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuture%2520Slot%2520Prediction%2520for%2520Unsupervised%2520Object%2520Discovery%2520in%2520Surgical%250A%2520%2520Video%26entry.906535625%3DGuiqiu%2520Liao%2520and%2520Matjaz%2520Jogan%2520and%2520Marcel%2520Hussing%2520and%2520Edward%2520Zhang%2520and%2520Eric%2520Eaton%2520and%2520Daniel%2520A.%2520Hashimoto%26entry.1292438233%3D%2520%2520Object-centric%2520slot%2520attention%2520is%2520an%2520emerging%2520paradigm%2520for%2520unsupervised%250Alearning%2520of%2520structured%252C%2520interpretable%2520object-centric%2520representations%2520%2528slots%2529.%250AThis%2520enables%2520effective%2520reasoning%2520about%2520objects%2520and%2520events%2520at%2520a%2520low%250Acomputational%2520cost%2520and%2520is%2520thus%2520applicable%2520to%2520critical%2520healthcare%2520applications%252C%250Asuch%2520as%2520real-time%2520interpretation%2520of%2520surgical%2520video.%2520The%2520heterogeneous%2520scenes%2520in%250Areal-world%2520applications%2520like%2520surgery%2520are%252C%2520however%252C%2520difficult%2520to%2520parse%2520into%2520a%250Ameaningful%2520set%2520of%2520slots.%2520Current%2520approaches%2520with%2520an%2520adaptive%2520slot%2520count%2520perform%250Awell%2520on%2520images%252C%2520but%2520their%2520performance%2520on%2520surgical%2520videos%2520is%2520low.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520a%2520dynamic%2520temporal%2520slot%2520transformer%2520%2528DTST%2529%2520module%250Athat%2520is%2520trained%2520both%2520for%2520temporal%2520reasoning%2520and%2520for%2520predicting%2520the%2520optimal%250Afuture%2520slot%2520initialization.%2520The%2520model%2520achieves%2520state-of-the-art%2520performance%2520on%250Amultiple%2520surgical%2520databases%252C%2520demonstrating%2520that%2520unsupervised%2520object-centric%250Amethods%2520can%2520be%2520applied%2520to%2520real-world%2520data%2520and%2520become%2520part%2520of%2520the%2520common%2520arsenal%250Ain%2520healthcare%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01882v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Future%20Slot%20Prediction%20for%20Unsupervised%20Object%20Discovery%20in%20Surgical%0A%20%20Video&entry.906535625=Guiqiu%20Liao%20and%20Matjaz%20Jogan%20and%20Marcel%20Hussing%20and%20Edward%20Zhang%20and%20Eric%20Eaton%20and%20Daniel%20A.%20Hashimoto&entry.1292438233=%20%20Object-centric%20slot%20attention%20is%20an%20emerging%20paradigm%20for%20unsupervised%0Alearning%20of%20structured%2C%20interpretable%20object-centric%20representations%20%28slots%29.%0AThis%20enables%20effective%20reasoning%20about%20objects%20and%20events%20at%20a%20low%0Acomputational%20cost%20and%20is%20thus%20applicable%20to%20critical%20healthcare%20applications%2C%0Asuch%20as%20real-time%20interpretation%20of%20surgical%20video.%20The%20heterogeneous%20scenes%20in%0Areal-world%20applications%20like%20surgery%20are%2C%20however%2C%20difficult%20to%20parse%20into%20a%0Ameaningful%20set%20of%20slots.%20Current%20approaches%20with%20an%20adaptive%20slot%20count%20perform%0Awell%20on%20images%2C%20but%20their%20performance%20on%20surgical%20videos%20is%20low.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20dynamic%20temporal%20slot%20transformer%20%28DTST%29%20module%0Athat%20is%20trained%20both%20for%20temporal%20reasoning%20and%20for%20predicting%20the%20optimal%0Afuture%20slot%20initialization.%20The%20model%20achieves%20state-of-the-art%20performance%20on%0Amultiple%20surgical%20databases%2C%20demonstrating%20that%20unsupervised%20object-centric%0Amethods%20can%20be%20applied%20to%20real-world%20data%20and%20become%20part%20of%20the%20common%20arsenal%0Ain%20healthcare%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01882v2&entry.124074799=Read"},
{"title": "AURA-CVC: Autonomous Ultrasound-guided Robotic Assistance for Central\n  Venous Catheterization", "author": "Deepak Raina and Lidia Al-Zogbi and Brian Teixeira and Vivek Singh and Ankur Kapoor and Thorsten Fleiter and Muyinatu A. Lediju Bell and Vinciya Pandian and Axel Krieger", "abstract": "  Purpose: Central venous catheterization (CVC) is a critical medical procedure\nfor vascular access, hemodynamic monitoring, and life-saving interventions. Its\nsuccess remains challenging due to the need for continuous ultrasound-guided\nvisualization of a target vessel and approaching needle, which is further\ncomplicated by anatomical variability and operator dependency. Errors in needle\nplacement can lead to life-threatening complications. While robotic systems\noffer a potential solution, achieving full autonomy remains challenging. In\nthis work, we propose an end-to-end robotic-ultrasound-guided CVC pipeline,\nfrom scan initialization to needle insertion. Methods: We introduce a\ndeep-learning model to identify clinically relevant anatomical landmarks from a\ndepth image of the patient's neck, obtained using RGB-D camera, to autonomously\ndefine the scanning region and paths. Then, a robot motion planning framework\nis proposed to scan, segment, reconstruct, and localize vessels (veins and\narteries), followed by the identification of the optimal insertion zone.\nFinally, a needle guidance module plans the insertion under ultrasound guidance\nwith operator's feedback. This pipeline was validated on a high-fidelity\ncommercial phantom across 10 simulated clinical scenarios. Results: The\nproposed pipeline achieved 10 out of 10 successful needle placements on the\nfirst attempt. Vessels were reconstructed with a mean error of 2.15\n\\textit{mm}, and autonomous needle insertion was performed with an error less\nthan or close to 1 \\textit{mm}. Conclusion: To our knowledge, this is the first\nrobotic CVC system demonstrated on a high-fidelity phantom with integrated\nplanning, scanning, and insertion. Experimental results show its potential for\nclinical translation.\n", "link": "http://arxiv.org/abs/2507.05979v1", "date": "2025-07-08", "relevancy": 2.1331, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5741}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5076}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AURA-CVC%3A%20Autonomous%20Ultrasound-guided%20Robotic%20Assistance%20for%20Central%0A%20%20Venous%20Catheterization&body=Title%3A%20AURA-CVC%3A%20Autonomous%20Ultrasound-guided%20Robotic%20Assistance%20for%20Central%0A%20%20Venous%20Catheterization%0AAuthor%3A%20Deepak%20Raina%20and%20Lidia%20Al-Zogbi%20and%20Brian%20Teixeira%20and%20Vivek%20Singh%20and%20Ankur%20Kapoor%20and%20Thorsten%20Fleiter%20and%20Muyinatu%20A.%20Lediju%20Bell%20and%20Vinciya%20Pandian%20and%20Axel%20Krieger%0AAbstract%3A%20%20%20Purpose%3A%20Central%20venous%20catheterization%20%28CVC%29%20is%20a%20critical%20medical%20procedure%0Afor%20vascular%20access%2C%20hemodynamic%20monitoring%2C%20and%20life-saving%20interventions.%20Its%0Asuccess%20remains%20challenging%20due%20to%20the%20need%20for%20continuous%20ultrasound-guided%0Avisualization%20of%20a%20target%20vessel%20and%20approaching%20needle%2C%20which%20is%20further%0Acomplicated%20by%20anatomical%20variability%20and%20operator%20dependency.%20Errors%20in%20needle%0Aplacement%20can%20lead%20to%20life-threatening%20complications.%20While%20robotic%20systems%0Aoffer%20a%20potential%20solution%2C%20achieving%20full%20autonomy%20remains%20challenging.%20In%0Athis%20work%2C%20we%20propose%20an%20end-to-end%20robotic-ultrasound-guided%20CVC%20pipeline%2C%0Afrom%20scan%20initialization%20to%20needle%20insertion.%20Methods%3A%20We%20introduce%20a%0Adeep-learning%20model%20to%20identify%20clinically%20relevant%20anatomical%20landmarks%20from%20a%0Adepth%20image%20of%20the%20patient%27s%20neck%2C%20obtained%20using%20RGB-D%20camera%2C%20to%20autonomously%0Adefine%20the%20scanning%20region%20and%20paths.%20Then%2C%20a%20robot%20motion%20planning%20framework%0Ais%20proposed%20to%20scan%2C%20segment%2C%20reconstruct%2C%20and%20localize%20vessels%20%28veins%20and%0Aarteries%29%2C%20followed%20by%20the%20identification%20of%20the%20optimal%20insertion%20zone.%0AFinally%2C%20a%20needle%20guidance%20module%20plans%20the%20insertion%20under%20ultrasound%20guidance%0Awith%20operator%27s%20feedback.%20This%20pipeline%20was%20validated%20on%20a%20high-fidelity%0Acommercial%20phantom%20across%2010%20simulated%20clinical%20scenarios.%20Results%3A%20The%0Aproposed%20pipeline%20achieved%2010%20out%20of%2010%20successful%20needle%20placements%20on%20the%0Afirst%20attempt.%20Vessels%20were%20reconstructed%20with%20a%20mean%20error%20of%202.15%0A%5Ctextit%7Bmm%7D%2C%20and%20autonomous%20needle%20insertion%20was%20performed%20with%20an%20error%20less%0Athan%20or%20close%20to%201%20%5Ctextit%7Bmm%7D.%20Conclusion%3A%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Arobotic%20CVC%20system%20demonstrated%20on%20a%20high-fidelity%20phantom%20with%20integrated%0Aplanning%2C%20scanning%2C%20and%20insertion.%20Experimental%20results%20show%20its%20potential%20for%0Aclinical%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAURA-CVC%253A%2520Autonomous%2520Ultrasound-guided%2520Robotic%2520Assistance%2520for%2520Central%250A%2520%2520Venous%2520Catheterization%26entry.906535625%3DDeepak%2520Raina%2520and%2520Lidia%2520Al-Zogbi%2520and%2520Brian%2520Teixeira%2520and%2520Vivek%2520Singh%2520and%2520Ankur%2520Kapoor%2520and%2520Thorsten%2520Fleiter%2520and%2520Muyinatu%2520A.%2520Lediju%2520Bell%2520and%2520Vinciya%2520Pandian%2520and%2520Axel%2520Krieger%26entry.1292438233%3D%2520%2520Purpose%253A%2520Central%2520venous%2520catheterization%2520%2528CVC%2529%2520is%2520a%2520critical%2520medical%2520procedure%250Afor%2520vascular%2520access%252C%2520hemodynamic%2520monitoring%252C%2520and%2520life-saving%2520interventions.%2520Its%250Asuccess%2520remains%2520challenging%2520due%2520to%2520the%2520need%2520for%2520continuous%2520ultrasound-guided%250Avisualization%2520of%2520a%2520target%2520vessel%2520and%2520approaching%2520needle%252C%2520which%2520is%2520further%250Acomplicated%2520by%2520anatomical%2520variability%2520and%2520operator%2520dependency.%2520Errors%2520in%2520needle%250Aplacement%2520can%2520lead%2520to%2520life-threatening%2520complications.%2520While%2520robotic%2520systems%250Aoffer%2520a%2520potential%2520solution%252C%2520achieving%2520full%2520autonomy%2520remains%2520challenging.%2520In%250Athis%2520work%252C%2520we%2520propose%2520an%2520end-to-end%2520robotic-ultrasound-guided%2520CVC%2520pipeline%252C%250Afrom%2520scan%2520initialization%2520to%2520needle%2520insertion.%2520Methods%253A%2520We%2520introduce%2520a%250Adeep-learning%2520model%2520to%2520identify%2520clinically%2520relevant%2520anatomical%2520landmarks%2520from%2520a%250Adepth%2520image%2520of%2520the%2520patient%2527s%2520neck%252C%2520obtained%2520using%2520RGB-D%2520camera%252C%2520to%2520autonomously%250Adefine%2520the%2520scanning%2520region%2520and%2520paths.%2520Then%252C%2520a%2520robot%2520motion%2520planning%2520framework%250Ais%2520proposed%2520to%2520scan%252C%2520segment%252C%2520reconstruct%252C%2520and%2520localize%2520vessels%2520%2528veins%2520and%250Aarteries%2529%252C%2520followed%2520by%2520the%2520identification%2520of%2520the%2520optimal%2520insertion%2520zone.%250AFinally%252C%2520a%2520needle%2520guidance%2520module%2520plans%2520the%2520insertion%2520under%2520ultrasound%2520guidance%250Awith%2520operator%2527s%2520feedback.%2520This%2520pipeline%2520was%2520validated%2520on%2520a%2520high-fidelity%250Acommercial%2520phantom%2520across%252010%2520simulated%2520clinical%2520scenarios.%2520Results%253A%2520The%250Aproposed%2520pipeline%2520achieved%252010%2520out%2520of%252010%2520successful%2520needle%2520placements%2520on%2520the%250Afirst%2520attempt.%2520Vessels%2520were%2520reconstructed%2520with%2520a%2520mean%2520error%2520of%25202.15%250A%255Ctextit%257Bmm%257D%252C%2520and%2520autonomous%2520needle%2520insertion%2520was%2520performed%2520with%2520an%2520error%2520less%250Athan%2520or%2520close%2520to%25201%2520%255Ctextit%257Bmm%257D.%2520Conclusion%253A%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Arobotic%2520CVC%2520system%2520demonstrated%2520on%2520a%2520high-fidelity%2520phantom%2520with%2520integrated%250Aplanning%252C%2520scanning%252C%2520and%2520insertion.%2520Experimental%2520results%2520show%2520its%2520potential%2520for%250Aclinical%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AURA-CVC%3A%20Autonomous%20Ultrasound-guided%20Robotic%20Assistance%20for%20Central%0A%20%20Venous%20Catheterization&entry.906535625=Deepak%20Raina%20and%20Lidia%20Al-Zogbi%20and%20Brian%20Teixeira%20and%20Vivek%20Singh%20and%20Ankur%20Kapoor%20and%20Thorsten%20Fleiter%20and%20Muyinatu%20A.%20Lediju%20Bell%20and%20Vinciya%20Pandian%20and%20Axel%20Krieger&entry.1292438233=%20%20Purpose%3A%20Central%20venous%20catheterization%20%28CVC%29%20is%20a%20critical%20medical%20procedure%0Afor%20vascular%20access%2C%20hemodynamic%20monitoring%2C%20and%20life-saving%20interventions.%20Its%0Asuccess%20remains%20challenging%20due%20to%20the%20need%20for%20continuous%20ultrasound-guided%0Avisualization%20of%20a%20target%20vessel%20and%20approaching%20needle%2C%20which%20is%20further%0Acomplicated%20by%20anatomical%20variability%20and%20operator%20dependency.%20Errors%20in%20needle%0Aplacement%20can%20lead%20to%20life-threatening%20complications.%20While%20robotic%20systems%0Aoffer%20a%20potential%20solution%2C%20achieving%20full%20autonomy%20remains%20challenging.%20In%0Athis%20work%2C%20we%20propose%20an%20end-to-end%20robotic-ultrasound-guided%20CVC%20pipeline%2C%0Afrom%20scan%20initialization%20to%20needle%20insertion.%20Methods%3A%20We%20introduce%20a%0Adeep-learning%20model%20to%20identify%20clinically%20relevant%20anatomical%20landmarks%20from%20a%0Adepth%20image%20of%20the%20patient%27s%20neck%2C%20obtained%20using%20RGB-D%20camera%2C%20to%20autonomously%0Adefine%20the%20scanning%20region%20and%20paths.%20Then%2C%20a%20robot%20motion%20planning%20framework%0Ais%20proposed%20to%20scan%2C%20segment%2C%20reconstruct%2C%20and%20localize%20vessels%20%28veins%20and%0Aarteries%29%2C%20followed%20by%20the%20identification%20of%20the%20optimal%20insertion%20zone.%0AFinally%2C%20a%20needle%20guidance%20module%20plans%20the%20insertion%20under%20ultrasound%20guidance%0Awith%20operator%27s%20feedback.%20This%20pipeline%20was%20validated%20on%20a%20high-fidelity%0Acommercial%20phantom%20across%2010%20simulated%20clinical%20scenarios.%20Results%3A%20The%0Aproposed%20pipeline%20achieved%2010%20out%20of%2010%20successful%20needle%20placements%20on%20the%0Afirst%20attempt.%20Vessels%20were%20reconstructed%20with%20a%20mean%20error%20of%202.15%0A%5Ctextit%7Bmm%7D%2C%20and%20autonomous%20needle%20insertion%20was%20performed%20with%20an%20error%20less%0Athan%20or%20close%20to%201%20%5Ctextit%7Bmm%7D.%20Conclusion%3A%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Arobotic%20CVC%20system%20demonstrated%20on%20a%20high-fidelity%20phantom%20with%20integrated%0Aplanning%2C%20scanning%2C%20and%20insertion.%20Experimental%20results%20show%20its%20potential%20for%0Aclinical%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05979v1&entry.124074799=Read"},
{"title": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large\n  Language Models", "author": "Bo Pang and Yalu Ouyang and Hangfei Xu and Ziqi Jia and Panpan Li and Shengzhao Wen and Lu Wang and Shiyong Li and Yanpeng Wang", "abstract": "  Advancements in reasoning for large language models (LLMs) have lead to\nsignificant performance improvements for LLMs in various fields such as\nmathematics and programming. However, research applying these advances to the\nfinancial domain, where considerable domain-specific knowledge is necessary to\ncomplete tasks, remains limited. To address this gap, we introduce FEVO\n(Financial Evolution), a multi-stage enhancement framework developed to enhance\nLLM performance in the financial domain. FEVO systemically enhances LLM\nperformance by using continued pre-training (CPT) to expand financial domain\nknowledge, supervised fine-tuning (SFT) to instill structured, elaborate\nreasoning patterns, and reinforcement learning (RL) to further integrate the\nexpanded financial domain knowledge with the learned structured reasoning. To\nensure effective and efficient training, we leverage frontier reasoning models\nand rule-based filtering to curate FEVO-Train, high-quality datasets\nspecifically designed for the different post-training phases. Using our\nframework, we train the FEVO series of models -- C32B, S32B, R32B -- from\nQwen2.5-32B and evaluate them on seven benchmarks to assess financial and\ngeneral capabilities, with results showing that FEVO-R32B achieves\nstate-of-the-art performance on five financial benchmarks against much larger\nmodels as well as specialist models. More significantly, FEVO-R32B demonstrates\nmarkedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct\nusing only RL), thus validating the effectiveness of financial domain knowledge\nexpansion and structured, logical reasoning distillation\n", "link": "http://arxiv.org/abs/2507.06057v1", "date": "2025-07-08", "relevancy": 2.1278, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5409}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5409}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FEVO%3A%20Financial%20Knowledge%20Expansion%20and%20Reasoning%20Evolution%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20FEVO%3A%20Financial%20Knowledge%20Expansion%20and%20Reasoning%20Evolution%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Bo%20Pang%20and%20Yalu%20Ouyang%20and%20Hangfei%20Xu%20and%20Ziqi%20Jia%20and%20Panpan%20Li%20and%20Shengzhao%20Wen%20and%20Lu%20Wang%20and%20Shiyong%20Li%20and%20Yanpeng%20Wang%0AAbstract%3A%20%20%20Advancements%20in%20reasoning%20for%20large%20language%20models%20%28LLMs%29%20have%20lead%20to%0Asignificant%20performance%20improvements%20for%20LLMs%20in%20various%20fields%20such%20as%0Amathematics%20and%20programming.%20However%2C%20research%20applying%20these%20advances%20to%20the%0Afinancial%20domain%2C%20where%20considerable%20domain-specific%20knowledge%20is%20necessary%20to%0Acomplete%20tasks%2C%20remains%20limited.%20To%20address%20this%20gap%2C%20we%20introduce%20FEVO%0A%28Financial%20Evolution%29%2C%20a%20multi-stage%20enhancement%20framework%20developed%20to%20enhance%0ALLM%20performance%20in%20the%20financial%20domain.%20FEVO%20systemically%20enhances%20LLM%0Aperformance%20by%20using%20continued%20pre-training%20%28CPT%29%20to%20expand%20financial%20domain%0Aknowledge%2C%20supervised%20fine-tuning%20%28SFT%29%20to%20instill%20structured%2C%20elaborate%0Areasoning%20patterns%2C%20and%20reinforcement%20learning%20%28RL%29%20to%20further%20integrate%20the%0Aexpanded%20financial%20domain%20knowledge%20with%20the%20learned%20structured%20reasoning.%20To%0Aensure%20effective%20and%20efficient%20training%2C%20we%20leverage%20frontier%20reasoning%20models%0Aand%20rule-based%20filtering%20to%20curate%20FEVO-Train%2C%20high-quality%20datasets%0Aspecifically%20designed%20for%20the%20different%20post-training%20phases.%20Using%20our%0Aframework%2C%20we%20train%20the%20FEVO%20series%20of%20models%20--%20C32B%2C%20S32B%2C%20R32B%20--%20from%0AQwen2.5-32B%20and%20evaluate%20them%20on%20seven%20benchmarks%20to%20assess%20financial%20and%0Ageneral%20capabilities%2C%20with%20results%20showing%20that%20FEVO-R32B%20achieves%0Astate-of-the-art%20performance%20on%20five%20financial%20benchmarks%20against%20much%20larger%0Amodels%20as%20well%20as%20specialist%20models.%20More%20significantly%2C%20FEVO-R32B%20demonstrates%0Amarkedly%20better%20performance%20than%20FEVO-R32B-0%20%28trained%20from%20Qwen2.5-32B-Instruct%0Ausing%20only%20RL%29%2C%20thus%20validating%20the%20effectiveness%20of%20financial%20domain%20knowledge%0Aexpansion%20and%20structured%2C%20logical%20reasoning%20distillation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFEVO%253A%2520Financial%2520Knowledge%2520Expansion%2520and%2520Reasoning%2520Evolution%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DBo%2520Pang%2520and%2520Yalu%2520Ouyang%2520and%2520Hangfei%2520Xu%2520and%2520Ziqi%2520Jia%2520and%2520Panpan%2520Li%2520and%2520Shengzhao%2520Wen%2520and%2520Lu%2520Wang%2520and%2520Shiyong%2520Li%2520and%2520Yanpeng%2520Wang%26entry.1292438233%3D%2520%2520Advancements%2520in%2520reasoning%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520lead%2520to%250Asignificant%2520performance%2520improvements%2520for%2520LLMs%2520in%2520various%2520fields%2520such%2520as%250Amathematics%2520and%2520programming.%2520However%252C%2520research%2520applying%2520these%2520advances%2520to%2520the%250Afinancial%2520domain%252C%2520where%2520considerable%2520domain-specific%2520knowledge%2520is%2520necessary%2520to%250Acomplete%2520tasks%252C%2520remains%2520limited.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520FEVO%250A%2528Financial%2520Evolution%2529%252C%2520a%2520multi-stage%2520enhancement%2520framework%2520developed%2520to%2520enhance%250ALLM%2520performance%2520in%2520the%2520financial%2520domain.%2520FEVO%2520systemically%2520enhances%2520LLM%250Aperformance%2520by%2520using%2520continued%2520pre-training%2520%2528CPT%2529%2520to%2520expand%2520financial%2520domain%250Aknowledge%252C%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520to%2520instill%2520structured%252C%2520elaborate%250Areasoning%2520patterns%252C%2520and%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520further%2520integrate%2520the%250Aexpanded%2520financial%2520domain%2520knowledge%2520with%2520the%2520learned%2520structured%2520reasoning.%2520To%250Aensure%2520effective%2520and%2520efficient%2520training%252C%2520we%2520leverage%2520frontier%2520reasoning%2520models%250Aand%2520rule-based%2520filtering%2520to%2520curate%2520FEVO-Train%252C%2520high-quality%2520datasets%250Aspecifically%2520designed%2520for%2520the%2520different%2520post-training%2520phases.%2520Using%2520our%250Aframework%252C%2520we%2520train%2520the%2520FEVO%2520series%2520of%2520models%2520--%2520C32B%252C%2520S32B%252C%2520R32B%2520--%2520from%250AQwen2.5-32B%2520and%2520evaluate%2520them%2520on%2520seven%2520benchmarks%2520to%2520assess%2520financial%2520and%250Ageneral%2520capabilities%252C%2520with%2520results%2520showing%2520that%2520FEVO-R32B%2520achieves%250Astate-of-the-art%2520performance%2520on%2520five%2520financial%2520benchmarks%2520against%2520much%2520larger%250Amodels%2520as%2520well%2520as%2520specialist%2520models.%2520More%2520significantly%252C%2520FEVO-R32B%2520demonstrates%250Amarkedly%2520better%2520performance%2520than%2520FEVO-R32B-0%2520%2528trained%2520from%2520Qwen2.5-32B-Instruct%250Ausing%2520only%2520RL%2529%252C%2520thus%2520validating%2520the%2520effectiveness%2520of%2520financial%2520domain%2520knowledge%250Aexpansion%2520and%2520structured%252C%2520logical%2520reasoning%2520distillation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FEVO%3A%20Financial%20Knowledge%20Expansion%20and%20Reasoning%20Evolution%20for%20Large%0A%20%20Language%20Models&entry.906535625=Bo%20Pang%20and%20Yalu%20Ouyang%20and%20Hangfei%20Xu%20and%20Ziqi%20Jia%20and%20Panpan%20Li%20and%20Shengzhao%20Wen%20and%20Lu%20Wang%20and%20Shiyong%20Li%20and%20Yanpeng%20Wang&entry.1292438233=%20%20Advancements%20in%20reasoning%20for%20large%20language%20models%20%28LLMs%29%20have%20lead%20to%0Asignificant%20performance%20improvements%20for%20LLMs%20in%20various%20fields%20such%20as%0Amathematics%20and%20programming.%20However%2C%20research%20applying%20these%20advances%20to%20the%0Afinancial%20domain%2C%20where%20considerable%20domain-specific%20knowledge%20is%20necessary%20to%0Acomplete%20tasks%2C%20remains%20limited.%20To%20address%20this%20gap%2C%20we%20introduce%20FEVO%0A%28Financial%20Evolution%29%2C%20a%20multi-stage%20enhancement%20framework%20developed%20to%20enhance%0ALLM%20performance%20in%20the%20financial%20domain.%20FEVO%20systemically%20enhances%20LLM%0Aperformance%20by%20using%20continued%20pre-training%20%28CPT%29%20to%20expand%20financial%20domain%0Aknowledge%2C%20supervised%20fine-tuning%20%28SFT%29%20to%20instill%20structured%2C%20elaborate%0Areasoning%20patterns%2C%20and%20reinforcement%20learning%20%28RL%29%20to%20further%20integrate%20the%0Aexpanded%20financial%20domain%20knowledge%20with%20the%20learned%20structured%20reasoning.%20To%0Aensure%20effective%20and%20efficient%20training%2C%20we%20leverage%20frontier%20reasoning%20models%0Aand%20rule-based%20filtering%20to%20curate%20FEVO-Train%2C%20high-quality%20datasets%0Aspecifically%20designed%20for%20the%20different%20post-training%20phases.%20Using%20our%0Aframework%2C%20we%20train%20the%20FEVO%20series%20of%20models%20--%20C32B%2C%20S32B%2C%20R32B%20--%20from%0AQwen2.5-32B%20and%20evaluate%20them%20on%20seven%20benchmarks%20to%20assess%20financial%20and%0Ageneral%20capabilities%2C%20with%20results%20showing%20that%20FEVO-R32B%20achieves%0Astate-of-the-art%20performance%20on%20five%20financial%20benchmarks%20against%20much%20larger%0Amodels%20as%20well%20as%20specialist%20models.%20More%20significantly%2C%20FEVO-R32B%20demonstrates%0Amarkedly%20better%20performance%20than%20FEVO-R32B-0%20%28trained%20from%20Qwen2.5-32B-Instruct%0Ausing%20only%20RL%29%2C%20thus%20validating%20the%20effectiveness%20of%20financial%20domain%20knowledge%0Aexpansion%20and%20structured%2C%20logical%20reasoning%20distillation%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06057v1&entry.124074799=Read"},
{"title": "Fast Bilateral Teleoperation and Imitation Learning Using Sensorless\n  Force Control via Accurate Dynamics Model", "author": "Koki Yamane and Yunhan Li and Masashi Konosu and Koki Inami and Junji Oaki and Sho Sakaino and Toshiaki Tsuji", "abstract": "  In recent years, the advancement of imitation learning has led to increased\ninterest in teleoperating low-cost manipulators to collect demonstration data.\nHowever, most existing systems rely on unilateral control, which only transmits\ntarget position values. While this approach is easy to implement and suitable\nfor slow, non-contact tasks, it struggles with fast or contact-rich operations\ndue to the absence of force feedback. This work demonstrates that fast\nteleoperation with force feedback is feasible even with force-sensorless,\nlow-cost manipulators by leveraging 4-channel bilateral control. Based on\naccurately identified manipulator dynamics, our method integrates nonlinear\nterms compensation, velocity and external force estimation, and variable gain\ncorresponding to inertial variation. Furthermore, using data collected by\n4-channel bilateral control, we show that incorporating force information into\nboth the input and output of learned policies improves performance in imitation\nlearning. These results highlight the practical effectiveness of our system for\nhigh-fidelity teleoperation and data collection on affordable hardware.\n", "link": "http://arxiv.org/abs/2507.06174v1", "date": "2025-07-08", "relevancy": 2.1225, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5628}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5301}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Bilateral%20Teleoperation%20and%20Imitation%20Learning%20Using%20Sensorless%0A%20%20Force%20Control%20via%20Accurate%20Dynamics%20Model&body=Title%3A%20Fast%20Bilateral%20Teleoperation%20and%20Imitation%20Learning%20Using%20Sensorless%0A%20%20Force%20Control%20via%20Accurate%20Dynamics%20Model%0AAuthor%3A%20Koki%20Yamane%20and%20Yunhan%20Li%20and%20Masashi%20Konosu%20and%20Koki%20Inami%20and%20Junji%20Oaki%20and%20Sho%20Sakaino%20and%20Toshiaki%20Tsuji%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20advancement%20of%20imitation%20learning%20has%20led%20to%20increased%0Ainterest%20in%20teleoperating%20low-cost%20manipulators%20to%20collect%20demonstration%20data.%0AHowever%2C%20most%20existing%20systems%20rely%20on%20unilateral%20control%2C%20which%20only%20transmits%0Atarget%20position%20values.%20While%20this%20approach%20is%20easy%20to%20implement%20and%20suitable%0Afor%20slow%2C%20non-contact%20tasks%2C%20it%20struggles%20with%20fast%20or%20contact-rich%20operations%0Adue%20to%20the%20absence%20of%20force%20feedback.%20This%20work%20demonstrates%20that%20fast%0Ateleoperation%20with%20force%20feedback%20is%20feasible%20even%20with%20force-sensorless%2C%0Alow-cost%20manipulators%20by%20leveraging%204-channel%20bilateral%20control.%20Based%20on%0Aaccurately%20identified%20manipulator%20dynamics%2C%20our%20method%20integrates%20nonlinear%0Aterms%20compensation%2C%20velocity%20and%20external%20force%20estimation%2C%20and%20variable%20gain%0Acorresponding%20to%20inertial%20variation.%20Furthermore%2C%20using%20data%20collected%20by%0A4-channel%20bilateral%20control%2C%20we%20show%20that%20incorporating%20force%20information%20into%0Aboth%20the%20input%20and%20output%20of%20learned%20policies%20improves%20performance%20in%20imitation%0Alearning.%20These%20results%20highlight%20the%20practical%20effectiveness%20of%20our%20system%20for%0Ahigh-fidelity%20teleoperation%20and%20data%20collection%20on%20affordable%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Bilateral%2520Teleoperation%2520and%2520Imitation%2520Learning%2520Using%2520Sensorless%250A%2520%2520Force%2520Control%2520via%2520Accurate%2520Dynamics%2520Model%26entry.906535625%3DKoki%2520Yamane%2520and%2520Yunhan%2520Li%2520and%2520Masashi%2520Konosu%2520and%2520Koki%2520Inami%2520and%2520Junji%2520Oaki%2520and%2520Sho%2520Sakaino%2520and%2520Toshiaki%2520Tsuji%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520advancement%2520of%2520imitation%2520learning%2520has%2520led%2520to%2520increased%250Ainterest%2520in%2520teleoperating%2520low-cost%2520manipulators%2520to%2520collect%2520demonstration%2520data.%250AHowever%252C%2520most%2520existing%2520systems%2520rely%2520on%2520unilateral%2520control%252C%2520which%2520only%2520transmits%250Atarget%2520position%2520values.%2520While%2520this%2520approach%2520is%2520easy%2520to%2520implement%2520and%2520suitable%250Afor%2520slow%252C%2520non-contact%2520tasks%252C%2520it%2520struggles%2520with%2520fast%2520or%2520contact-rich%2520operations%250Adue%2520to%2520the%2520absence%2520of%2520force%2520feedback.%2520This%2520work%2520demonstrates%2520that%2520fast%250Ateleoperation%2520with%2520force%2520feedback%2520is%2520feasible%2520even%2520with%2520force-sensorless%252C%250Alow-cost%2520manipulators%2520by%2520leveraging%25204-channel%2520bilateral%2520control.%2520Based%2520on%250Aaccurately%2520identified%2520manipulator%2520dynamics%252C%2520our%2520method%2520integrates%2520nonlinear%250Aterms%2520compensation%252C%2520velocity%2520and%2520external%2520force%2520estimation%252C%2520and%2520variable%2520gain%250Acorresponding%2520to%2520inertial%2520variation.%2520Furthermore%252C%2520using%2520data%2520collected%2520by%250A4-channel%2520bilateral%2520control%252C%2520we%2520show%2520that%2520incorporating%2520force%2520information%2520into%250Aboth%2520the%2520input%2520and%2520output%2520of%2520learned%2520policies%2520improves%2520performance%2520in%2520imitation%250Alearning.%2520These%2520results%2520highlight%2520the%2520practical%2520effectiveness%2520of%2520our%2520system%2520for%250Ahigh-fidelity%2520teleoperation%2520and%2520data%2520collection%2520on%2520affordable%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Bilateral%20Teleoperation%20and%20Imitation%20Learning%20Using%20Sensorless%0A%20%20Force%20Control%20via%20Accurate%20Dynamics%20Model&entry.906535625=Koki%20Yamane%20and%20Yunhan%20Li%20and%20Masashi%20Konosu%20and%20Koki%20Inami%20and%20Junji%20Oaki%20and%20Sho%20Sakaino%20and%20Toshiaki%20Tsuji&entry.1292438233=%20%20In%20recent%20years%2C%20the%20advancement%20of%20imitation%20learning%20has%20led%20to%20increased%0Ainterest%20in%20teleoperating%20low-cost%20manipulators%20to%20collect%20demonstration%20data.%0AHowever%2C%20most%20existing%20systems%20rely%20on%20unilateral%20control%2C%20which%20only%20transmits%0Atarget%20position%20values.%20While%20this%20approach%20is%20easy%20to%20implement%20and%20suitable%0Afor%20slow%2C%20non-contact%20tasks%2C%20it%20struggles%20with%20fast%20or%20contact-rich%20operations%0Adue%20to%20the%20absence%20of%20force%20feedback.%20This%20work%20demonstrates%20that%20fast%0Ateleoperation%20with%20force%20feedback%20is%20feasible%20even%20with%20force-sensorless%2C%0Alow-cost%20manipulators%20by%20leveraging%204-channel%20bilateral%20control.%20Based%20on%0Aaccurately%20identified%20manipulator%20dynamics%2C%20our%20method%20integrates%20nonlinear%0Aterms%20compensation%2C%20velocity%20and%20external%20force%20estimation%2C%20and%20variable%20gain%0Acorresponding%20to%20inertial%20variation.%20Furthermore%2C%20using%20data%20collected%20by%0A4-channel%20bilateral%20control%2C%20we%20show%20that%20incorporating%20force%20information%20into%0Aboth%20the%20input%20and%20output%20of%20learned%20policies%20improves%20performance%20in%20imitation%0Alearning.%20These%20results%20highlight%20the%20practical%20effectiveness%20of%20our%20system%20for%0Ahigh-fidelity%20teleoperation%20and%20data%20collection%20on%20affordable%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06174v1&entry.124074799=Read"},
{"title": "Critical Nodes Identification in Complex Networks: A Survey", "author": "Duxin Chen and Jiawen Chen and Xiaoyu Zhang and Qinghan Jia and Xiaolu Liu and Ye Sun and Linyuan Lv and Wenwu Yu", "abstract": "  Complex networks have become essential tools for understanding diverse\nphenomena in social systems, traffic systems, biomolecular systems, and\nfinancial systems. Identifying critical nodes is a central theme in\ncontemporary research, serving as a vital bridge between theoretical\nfoundations and practical applications. Nevertheless, the intrinsic complexity\nand structural heterogeneity characterizing real-world networks, with\nparticular emphasis on dynamic and higher-order networks, present substantial\nobstacles to the development of universal frameworks for critical node\nidentification. This paper provides a comprehensive review of critical node\nidentification techniques, categorizing them into seven main classes:\ncentrality, critical nodes deletion problem, influence maximization, network\ncontrol, artificial intelligence, higher-order and dynamic methods. Our review\nbridges the gaps in existing surveys by systematically classifying methods\nbased on their methodological foundations and practical implications, and by\nhighlighting their strengths, limitations, and applicability across different\nnetwork types. Our work enhances the understanding of critical node research by\nidentifying key challenges, such as algorithmic universality, real-time\nevaluation in dynamic networks, analysis of higher-order structures, and\ncomputational efficiency in large-scale networks. The structured synthesis\nconsolidates current progress and highlights open questions, particularly in\nmodeling temporal dynamics, advancing efficient algorithms, integrating machine\nlearning approaches, and developing scalable and interpretable metrics for\ncomplex systems.\n", "link": "http://arxiv.org/abs/2507.06164v1", "date": "2025-07-08", "relevancy": 2.1225, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.438}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Critical%20Nodes%20Identification%20in%20Complex%20Networks%3A%20A%20Survey&body=Title%3A%20Critical%20Nodes%20Identification%20in%20Complex%20Networks%3A%20A%20Survey%0AAuthor%3A%20Duxin%20Chen%20and%20Jiawen%20Chen%20and%20Xiaoyu%20Zhang%20and%20Qinghan%20Jia%20and%20Xiaolu%20Liu%20and%20Ye%20Sun%20and%20Linyuan%20Lv%20and%20Wenwu%20Yu%0AAbstract%3A%20%20%20Complex%20networks%20have%20become%20essential%20tools%20for%20understanding%20diverse%0Aphenomena%20in%20social%20systems%2C%20traffic%20systems%2C%20biomolecular%20systems%2C%20and%0Afinancial%20systems.%20Identifying%20critical%20nodes%20is%20a%20central%20theme%20in%0Acontemporary%20research%2C%20serving%20as%20a%20vital%20bridge%20between%20theoretical%0Afoundations%20and%20practical%20applications.%20Nevertheless%2C%20the%20intrinsic%20complexity%0Aand%20structural%20heterogeneity%20characterizing%20real-world%20networks%2C%20with%0Aparticular%20emphasis%20on%20dynamic%20and%20higher-order%20networks%2C%20present%20substantial%0Aobstacles%20to%20the%20development%20of%20universal%20frameworks%20for%20critical%20node%0Aidentification.%20This%20paper%20provides%20a%20comprehensive%20review%20of%20critical%20node%0Aidentification%20techniques%2C%20categorizing%20them%20into%20seven%20main%20classes%3A%0Acentrality%2C%20critical%20nodes%20deletion%20problem%2C%20influence%20maximization%2C%20network%0Acontrol%2C%20artificial%20intelligence%2C%20higher-order%20and%20dynamic%20methods.%20Our%20review%0Abridges%20the%20gaps%20in%20existing%20surveys%20by%20systematically%20classifying%20methods%0Abased%20on%20their%20methodological%20foundations%20and%20practical%20implications%2C%20and%20by%0Ahighlighting%20their%20strengths%2C%20limitations%2C%20and%20applicability%20across%20different%0Anetwork%20types.%20Our%20work%20enhances%20the%20understanding%20of%20critical%20node%20research%20by%0Aidentifying%20key%20challenges%2C%20such%20as%20algorithmic%20universality%2C%20real-time%0Aevaluation%20in%20dynamic%20networks%2C%20analysis%20of%20higher-order%20structures%2C%20and%0Acomputational%20efficiency%20in%20large-scale%20networks.%20The%20structured%20synthesis%0Aconsolidates%20current%20progress%20and%20highlights%20open%20questions%2C%20particularly%20in%0Amodeling%20temporal%20dynamics%2C%20advancing%20efficient%20algorithms%2C%20integrating%20machine%0Alearning%20approaches%2C%20and%20developing%20scalable%20and%20interpretable%20metrics%20for%0Acomplex%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCritical%2520Nodes%2520Identification%2520in%2520Complex%2520Networks%253A%2520A%2520Survey%26entry.906535625%3DDuxin%2520Chen%2520and%2520Jiawen%2520Chen%2520and%2520Xiaoyu%2520Zhang%2520and%2520Qinghan%2520Jia%2520and%2520Xiaolu%2520Liu%2520and%2520Ye%2520Sun%2520and%2520Linyuan%2520Lv%2520and%2520Wenwu%2520Yu%26entry.1292438233%3D%2520%2520Complex%2520networks%2520have%2520become%2520essential%2520tools%2520for%2520understanding%2520diverse%250Aphenomena%2520in%2520social%2520systems%252C%2520traffic%2520systems%252C%2520biomolecular%2520systems%252C%2520and%250Afinancial%2520systems.%2520Identifying%2520critical%2520nodes%2520is%2520a%2520central%2520theme%2520in%250Acontemporary%2520research%252C%2520serving%2520as%2520a%2520vital%2520bridge%2520between%2520theoretical%250Afoundations%2520and%2520practical%2520applications.%2520Nevertheless%252C%2520the%2520intrinsic%2520complexity%250Aand%2520structural%2520heterogeneity%2520characterizing%2520real-world%2520networks%252C%2520with%250Aparticular%2520emphasis%2520on%2520dynamic%2520and%2520higher-order%2520networks%252C%2520present%2520substantial%250Aobstacles%2520to%2520the%2520development%2520of%2520universal%2520frameworks%2520for%2520critical%2520node%250Aidentification.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520review%2520of%2520critical%2520node%250Aidentification%2520techniques%252C%2520categorizing%2520them%2520into%2520seven%2520main%2520classes%253A%250Acentrality%252C%2520critical%2520nodes%2520deletion%2520problem%252C%2520influence%2520maximization%252C%2520network%250Acontrol%252C%2520artificial%2520intelligence%252C%2520higher-order%2520and%2520dynamic%2520methods.%2520Our%2520review%250Abridges%2520the%2520gaps%2520in%2520existing%2520surveys%2520by%2520systematically%2520classifying%2520methods%250Abased%2520on%2520their%2520methodological%2520foundations%2520and%2520practical%2520implications%252C%2520and%2520by%250Ahighlighting%2520their%2520strengths%252C%2520limitations%252C%2520and%2520applicability%2520across%2520different%250Anetwork%2520types.%2520Our%2520work%2520enhances%2520the%2520understanding%2520of%2520critical%2520node%2520research%2520by%250Aidentifying%2520key%2520challenges%252C%2520such%2520as%2520algorithmic%2520universality%252C%2520real-time%250Aevaluation%2520in%2520dynamic%2520networks%252C%2520analysis%2520of%2520higher-order%2520structures%252C%2520and%250Acomputational%2520efficiency%2520in%2520large-scale%2520networks.%2520The%2520structured%2520synthesis%250Aconsolidates%2520current%2520progress%2520and%2520highlights%2520open%2520questions%252C%2520particularly%2520in%250Amodeling%2520temporal%2520dynamics%252C%2520advancing%2520efficient%2520algorithms%252C%2520integrating%2520machine%250Alearning%2520approaches%252C%2520and%2520developing%2520scalable%2520and%2520interpretable%2520metrics%2520for%250Acomplex%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Critical%20Nodes%20Identification%20in%20Complex%20Networks%3A%20A%20Survey&entry.906535625=Duxin%20Chen%20and%20Jiawen%20Chen%20and%20Xiaoyu%20Zhang%20and%20Qinghan%20Jia%20and%20Xiaolu%20Liu%20and%20Ye%20Sun%20and%20Linyuan%20Lv%20and%20Wenwu%20Yu&entry.1292438233=%20%20Complex%20networks%20have%20become%20essential%20tools%20for%20understanding%20diverse%0Aphenomena%20in%20social%20systems%2C%20traffic%20systems%2C%20biomolecular%20systems%2C%20and%0Afinancial%20systems.%20Identifying%20critical%20nodes%20is%20a%20central%20theme%20in%0Acontemporary%20research%2C%20serving%20as%20a%20vital%20bridge%20between%20theoretical%0Afoundations%20and%20practical%20applications.%20Nevertheless%2C%20the%20intrinsic%20complexity%0Aand%20structural%20heterogeneity%20characterizing%20real-world%20networks%2C%20with%0Aparticular%20emphasis%20on%20dynamic%20and%20higher-order%20networks%2C%20present%20substantial%0Aobstacles%20to%20the%20development%20of%20universal%20frameworks%20for%20critical%20node%0Aidentification.%20This%20paper%20provides%20a%20comprehensive%20review%20of%20critical%20node%0Aidentification%20techniques%2C%20categorizing%20them%20into%20seven%20main%20classes%3A%0Acentrality%2C%20critical%20nodes%20deletion%20problem%2C%20influence%20maximization%2C%20network%0Acontrol%2C%20artificial%20intelligence%2C%20higher-order%20and%20dynamic%20methods.%20Our%20review%0Abridges%20the%20gaps%20in%20existing%20surveys%20by%20systematically%20classifying%20methods%0Abased%20on%20their%20methodological%20foundations%20and%20practical%20implications%2C%20and%20by%0Ahighlighting%20their%20strengths%2C%20limitations%2C%20and%20applicability%20across%20different%0Anetwork%20types.%20Our%20work%20enhances%20the%20understanding%20of%20critical%20node%20research%20by%0Aidentifying%20key%20challenges%2C%20such%20as%20algorithmic%20universality%2C%20real-time%0Aevaluation%20in%20dynamic%20networks%2C%20analysis%20of%20higher-order%20structures%2C%20and%0Acomputational%20efficiency%20in%20large-scale%20networks.%20The%20structured%20synthesis%0Aconsolidates%20current%20progress%20and%20highlights%20open%20questions%2C%20particularly%20in%0Amodeling%20temporal%20dynamics%2C%20advancing%20efficient%20algorithms%2C%20integrating%20machine%0Alearning%20approaches%2C%20and%20developing%20scalable%20and%20interpretable%20metrics%20for%0Acomplex%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06164v1&entry.124074799=Read"},
{"title": "MedGemma Technical Report", "author": "Andrew Sellergren and Sahar Kazemzadeh and Tiam Jaroensri and Atilla Kiraly and Madeleine Traverse and Timo Kohlberger and Shawn Xu and Fayaz Jamil and C\u00edan Hughes and Charles Lau and Justin Chen and Fereshteh Mahvar and Liron Yatziv and Tiffany Chen and Bram Sterling and Stefanie Anna Baby and Susanna Maria Baby and Jeremy Lai and Samuel Schmidgall and Lu Yang and Kejia Chen and Per Bjornsson and Shashir Reddy and Ryan Brush and Kenneth Philbrick and Howard Hu and Howard Yang and Richa Tiwari and Sunny Jansen and Preeti Singh and Yun Liu and Shekoofeh Azizi and Aishwarya Kamath and Johan Ferret and Shreya Pathak and Nino Vieillard and Ramona Merhej and Sarah Perrin and Tatiana Matejovicova and Alexandre Ram\u00e9 and Morgane Riviere and Louis Rouillard and Thomas Mesnard and Geoffrey Cideron and Jean-bastien Grill and Sabela Ramos and Edouard Yvinec and Michelle Casbon and Elena Buchatskaya and Jean-Baptiste Alayrac and Dmitry Lepikhin and Vlad Feinberg and Sebastian Borgeaud and Alek Andreev and Cassidy Hardin and Robert Dadashi and L\u00e9onard Hussenot and Armand Joulin and Olivier Bachem and Yossi Matias and Katherine Chou and Avinatan Hassidim and Kavi Goel and Clement Farabet and Joelle Barral and Tris Warkentin and Jonathon Shlens and David Fleet and Victor Cotruta and Omar Sanseviero and Gus Martins and Phoebe Kirk and Anand Rao and Shravya Shetty and David F. Steiner and Can Kirmizibayrak and Rory Pilgrim and Daniel Golden and Lin Yang", "abstract": "  Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma.\n", "link": "http://arxiv.org/abs/2507.05201v2", "date": "2025-07-08", "relevancy": 2.1159, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedGemma%20Technical%20Report&body=Title%3A%20MedGemma%20Technical%20Report%0AAuthor%3A%20Andrew%20Sellergren%20and%20Sahar%20Kazemzadeh%20and%20Tiam%20Jaroensri%20and%20Atilla%20Kiraly%20and%20Madeleine%20Traverse%20and%20Timo%20Kohlberger%20and%20Shawn%20Xu%20and%20Fayaz%20Jamil%20and%20C%C3%ADan%20Hughes%20and%20Charles%20Lau%20and%20Justin%20Chen%20and%20Fereshteh%20Mahvar%20and%20Liron%20Yatziv%20and%20Tiffany%20Chen%20and%20Bram%20Sterling%20and%20Stefanie%20Anna%20Baby%20and%20Susanna%20Maria%20Baby%20and%20Jeremy%20Lai%20and%20Samuel%20Schmidgall%20and%20Lu%20Yang%20and%20Kejia%20Chen%20and%20Per%20Bjornsson%20and%20Shashir%20Reddy%20and%20Ryan%20Brush%20and%20Kenneth%20Philbrick%20and%20Howard%20Hu%20and%20Howard%20Yang%20and%20Richa%20Tiwari%20and%20Sunny%20Jansen%20and%20Preeti%20Singh%20and%20Yun%20Liu%20and%20Shekoofeh%20Azizi%20and%20Aishwarya%20Kamath%20and%20Johan%20Ferret%20and%20Shreya%20Pathak%20and%20Nino%20Vieillard%20and%20Ramona%20Merhej%20and%20Sarah%20Perrin%20and%20Tatiana%20Matejovicova%20and%20Alexandre%20Ram%C3%A9%20and%20Morgane%20Riviere%20and%20Louis%20Rouillard%20and%20Thomas%20Mesnard%20and%20Geoffrey%20Cideron%20and%20Jean-bastien%20Grill%20and%20Sabela%20Ramos%20and%20Edouard%20Yvinec%20and%20Michelle%20Casbon%20and%20Elena%20Buchatskaya%20and%20Jean-Baptiste%20Alayrac%20and%20Dmitry%20Lepikhin%20and%20Vlad%20Feinberg%20and%20Sebastian%20Borgeaud%20and%20Alek%20Andreev%20and%20Cassidy%20Hardin%20and%20Robert%20Dadashi%20and%20L%C3%A9onard%20Hussenot%20and%20Armand%20Joulin%20and%20Olivier%20Bachem%20and%20Yossi%20Matias%20and%20Katherine%20Chou%20and%20Avinatan%20Hassidim%20and%20Kavi%20Goel%20and%20Clement%20Farabet%20and%20Joelle%20Barral%20and%20Tris%20Warkentin%20and%20Jonathon%20Shlens%20and%20David%20Fleet%20and%20Victor%20Cotruta%20and%20Omar%20Sanseviero%20and%20Gus%20Martins%20and%20Phoebe%20Kirk%20and%20Anand%20Rao%20and%20Shravya%20Shetty%20and%20David%20F.%20Steiner%20and%20Can%20Kirmizibayrak%20and%20Rory%20Pilgrim%20and%20Daniel%20Golden%20and%20Lin%20Yang%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20has%20significant%20potential%20in%20healthcare%0Aapplications%2C%20but%20its%20training%20and%20deployment%20faces%20challenges%20due%20to%0Ahealthcare%27s%20diverse%20data%2C%20complex%20tasks%2C%20and%20the%20need%20to%20preserve%20privacy.%0AFoundation%20models%20that%20perform%20well%20on%20medical%20tasks%20and%20require%20less%0Atask-specific%20tuning%20data%20are%20critical%20to%20accelerate%20the%20development%20of%0Ahealthcare%20AI%20applications.%20We%20introduce%20MedGemma%2C%20a%20collection%20of%20medical%0Avision-language%20foundation%20models%20based%20on%20Gemma%203%204B%20and%2027B.%20MedGemma%0Ademonstrates%20advanced%20medical%20understanding%20and%20reasoning%20on%20images%20and%20text%2C%0Asignificantly%20exceeding%20the%20performance%20of%20similar-sized%20generative%20models%20and%0Aapproaching%20the%20performance%20of%20task-specific%20models%2C%20while%20maintaining%20the%0Ageneral%20capabilities%20of%20the%20Gemma%203%20base%20models.%20For%20out-of-distribution%20tasks%2C%0AMedGemma%20achieves%202.6-10%25%20improvement%20on%20medical%20multimodal%20question%20answering%2C%0A15.5-18.1%25%20improvement%20on%20chest%20X-ray%20finding%20classification%2C%20and%2010.8%25%0Aimprovement%20on%20agentic%20evaluations%20compared%20to%20the%20base%20models.%20Fine-tuning%0AMedGemma%20further%20improves%20performance%20in%20subdomains%2C%20reducing%20errors%20in%0Aelectronic%20health%20record%20information%20retrieval%20by%2050%25%20and%20reaching%20comparable%0Aperformance%20to%20existing%20specialized%20state-of-the-art%20methods%20for%20pneumothorax%0Aclassification%20and%20histopathology%20patch%20classification.%20We%20additionally%0Aintroduce%20MedSigLIP%2C%20a%20medically-tuned%20vision%20encoder%20derived%20from%20SigLIP.%0AMedSigLIP%20powers%20the%20visual%20understanding%20capabilities%20of%20MedGemma%20and%20as%20an%0Aencoder%20achieves%20comparable%20or%20better%20performance%20than%20specialized%20medical%0Aimage%20encoders.%20Taken%20together%2C%20the%20MedGemma%20collection%20provides%20a%20strong%0Afoundation%20of%20medical%20image%20and%20text%20capabilities%2C%20with%20potential%20to%0Asignificantly%20accelerate%20medical%20research%20and%20development%20of%20downstream%0Aapplications.%20The%20MedGemma%20collection%2C%20including%20tutorials%20and%20model%20weights%2C%0Acan%20be%20found%20at%20https%3A//goo.gle/medgemma.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05201v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedGemma%2520Technical%2520Report%26entry.906535625%3DAndrew%2520Sellergren%2520and%2520Sahar%2520Kazemzadeh%2520and%2520Tiam%2520Jaroensri%2520and%2520Atilla%2520Kiraly%2520and%2520Madeleine%2520Traverse%2520and%2520Timo%2520Kohlberger%2520and%2520Shawn%2520Xu%2520and%2520Fayaz%2520Jamil%2520and%2520C%25C3%25ADan%2520Hughes%2520and%2520Charles%2520Lau%2520and%2520Justin%2520Chen%2520and%2520Fereshteh%2520Mahvar%2520and%2520Liron%2520Yatziv%2520and%2520Tiffany%2520Chen%2520and%2520Bram%2520Sterling%2520and%2520Stefanie%2520Anna%2520Baby%2520and%2520Susanna%2520Maria%2520Baby%2520and%2520Jeremy%2520Lai%2520and%2520Samuel%2520Schmidgall%2520and%2520Lu%2520Yang%2520and%2520Kejia%2520Chen%2520and%2520Per%2520Bjornsson%2520and%2520Shashir%2520Reddy%2520and%2520Ryan%2520Brush%2520and%2520Kenneth%2520Philbrick%2520and%2520Howard%2520Hu%2520and%2520Howard%2520Yang%2520and%2520Richa%2520Tiwari%2520and%2520Sunny%2520Jansen%2520and%2520Preeti%2520Singh%2520and%2520Yun%2520Liu%2520and%2520Shekoofeh%2520Azizi%2520and%2520Aishwarya%2520Kamath%2520and%2520Johan%2520Ferret%2520and%2520Shreya%2520Pathak%2520and%2520Nino%2520Vieillard%2520and%2520Ramona%2520Merhej%2520and%2520Sarah%2520Perrin%2520and%2520Tatiana%2520Matejovicova%2520and%2520Alexandre%2520Ram%25C3%25A9%2520and%2520Morgane%2520Riviere%2520and%2520Louis%2520Rouillard%2520and%2520Thomas%2520Mesnard%2520and%2520Geoffrey%2520Cideron%2520and%2520Jean-bastien%2520Grill%2520and%2520Sabela%2520Ramos%2520and%2520Edouard%2520Yvinec%2520and%2520Michelle%2520Casbon%2520and%2520Elena%2520Buchatskaya%2520and%2520Jean-Baptiste%2520Alayrac%2520and%2520Dmitry%2520Lepikhin%2520and%2520Vlad%2520Feinberg%2520and%2520Sebastian%2520Borgeaud%2520and%2520Alek%2520Andreev%2520and%2520Cassidy%2520Hardin%2520and%2520Robert%2520Dadashi%2520and%2520L%25C3%25A9onard%2520Hussenot%2520and%2520Armand%2520Joulin%2520and%2520Olivier%2520Bachem%2520and%2520Yossi%2520Matias%2520and%2520Katherine%2520Chou%2520and%2520Avinatan%2520Hassidim%2520and%2520Kavi%2520Goel%2520and%2520Clement%2520Farabet%2520and%2520Joelle%2520Barral%2520and%2520Tris%2520Warkentin%2520and%2520Jonathon%2520Shlens%2520and%2520David%2520Fleet%2520and%2520Victor%2520Cotruta%2520and%2520Omar%2520Sanseviero%2520and%2520Gus%2520Martins%2520and%2520Phoebe%2520Kirk%2520and%2520Anand%2520Rao%2520and%2520Shravya%2520Shetty%2520and%2520David%2520F.%2520Steiner%2520and%2520Can%2520Kirmizibayrak%2520and%2520Rory%2520Pilgrim%2520and%2520Daniel%2520Golden%2520and%2520Lin%2520Yang%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520has%2520significant%2520potential%2520in%2520healthcare%250Aapplications%252C%2520but%2520its%2520training%2520and%2520deployment%2520faces%2520challenges%2520due%2520to%250Ahealthcare%2527s%2520diverse%2520data%252C%2520complex%2520tasks%252C%2520and%2520the%2520need%2520to%2520preserve%2520privacy.%250AFoundation%2520models%2520that%2520perform%2520well%2520on%2520medical%2520tasks%2520and%2520require%2520less%250Atask-specific%2520tuning%2520data%2520are%2520critical%2520to%2520accelerate%2520the%2520development%2520of%250Ahealthcare%2520AI%2520applications.%2520We%2520introduce%2520MedGemma%252C%2520a%2520collection%2520of%2520medical%250Avision-language%2520foundation%2520models%2520based%2520on%2520Gemma%25203%25204B%2520and%252027B.%2520MedGemma%250Ademonstrates%2520advanced%2520medical%2520understanding%2520and%2520reasoning%2520on%2520images%2520and%2520text%252C%250Asignificantly%2520exceeding%2520the%2520performance%2520of%2520similar-sized%2520generative%2520models%2520and%250Aapproaching%2520the%2520performance%2520of%2520task-specific%2520models%252C%2520while%2520maintaining%2520the%250Ageneral%2520capabilities%2520of%2520the%2520Gemma%25203%2520base%2520models.%2520For%2520out-of-distribution%2520tasks%252C%250AMedGemma%2520achieves%25202.6-10%2525%2520improvement%2520on%2520medical%2520multimodal%2520question%2520answering%252C%250A15.5-18.1%2525%2520improvement%2520on%2520chest%2520X-ray%2520finding%2520classification%252C%2520and%252010.8%2525%250Aimprovement%2520on%2520agentic%2520evaluations%2520compared%2520to%2520the%2520base%2520models.%2520Fine-tuning%250AMedGemma%2520further%2520improves%2520performance%2520in%2520subdomains%252C%2520reducing%2520errors%2520in%250Aelectronic%2520health%2520record%2520information%2520retrieval%2520by%252050%2525%2520and%2520reaching%2520comparable%250Aperformance%2520to%2520existing%2520specialized%2520state-of-the-art%2520methods%2520for%2520pneumothorax%250Aclassification%2520and%2520histopathology%2520patch%2520classification.%2520We%2520additionally%250Aintroduce%2520MedSigLIP%252C%2520a%2520medically-tuned%2520vision%2520encoder%2520derived%2520from%2520SigLIP.%250AMedSigLIP%2520powers%2520the%2520visual%2520understanding%2520capabilities%2520of%2520MedGemma%2520and%2520as%2520an%250Aencoder%2520achieves%2520comparable%2520or%2520better%2520performance%2520than%2520specialized%2520medical%250Aimage%2520encoders.%2520Taken%2520together%252C%2520the%2520MedGemma%2520collection%2520provides%2520a%2520strong%250Afoundation%2520of%2520medical%2520image%2520and%2520text%2520capabilities%252C%2520with%2520potential%2520to%250Asignificantly%2520accelerate%2520medical%2520research%2520and%2520development%2520of%2520downstream%250Aapplications.%2520The%2520MedGemma%2520collection%252C%2520including%2520tutorials%2520and%2520model%2520weights%252C%250Acan%2520be%2520found%2520at%2520https%253A//goo.gle/medgemma.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05201v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedGemma%20Technical%20Report&entry.906535625=Andrew%20Sellergren%20and%20Sahar%20Kazemzadeh%20and%20Tiam%20Jaroensri%20and%20Atilla%20Kiraly%20and%20Madeleine%20Traverse%20and%20Timo%20Kohlberger%20and%20Shawn%20Xu%20and%20Fayaz%20Jamil%20and%20C%C3%ADan%20Hughes%20and%20Charles%20Lau%20and%20Justin%20Chen%20and%20Fereshteh%20Mahvar%20and%20Liron%20Yatziv%20and%20Tiffany%20Chen%20and%20Bram%20Sterling%20and%20Stefanie%20Anna%20Baby%20and%20Susanna%20Maria%20Baby%20and%20Jeremy%20Lai%20and%20Samuel%20Schmidgall%20and%20Lu%20Yang%20and%20Kejia%20Chen%20and%20Per%20Bjornsson%20and%20Shashir%20Reddy%20and%20Ryan%20Brush%20and%20Kenneth%20Philbrick%20and%20Howard%20Hu%20and%20Howard%20Yang%20and%20Richa%20Tiwari%20and%20Sunny%20Jansen%20and%20Preeti%20Singh%20and%20Yun%20Liu%20and%20Shekoofeh%20Azizi%20and%20Aishwarya%20Kamath%20and%20Johan%20Ferret%20and%20Shreya%20Pathak%20and%20Nino%20Vieillard%20and%20Ramona%20Merhej%20and%20Sarah%20Perrin%20and%20Tatiana%20Matejovicova%20and%20Alexandre%20Ram%C3%A9%20and%20Morgane%20Riviere%20and%20Louis%20Rouillard%20and%20Thomas%20Mesnard%20and%20Geoffrey%20Cideron%20and%20Jean-bastien%20Grill%20and%20Sabela%20Ramos%20and%20Edouard%20Yvinec%20and%20Michelle%20Casbon%20and%20Elena%20Buchatskaya%20and%20Jean-Baptiste%20Alayrac%20and%20Dmitry%20Lepikhin%20and%20Vlad%20Feinberg%20and%20Sebastian%20Borgeaud%20and%20Alek%20Andreev%20and%20Cassidy%20Hardin%20and%20Robert%20Dadashi%20and%20L%C3%A9onard%20Hussenot%20and%20Armand%20Joulin%20and%20Olivier%20Bachem%20and%20Yossi%20Matias%20and%20Katherine%20Chou%20and%20Avinatan%20Hassidim%20and%20Kavi%20Goel%20and%20Clement%20Farabet%20and%20Joelle%20Barral%20and%20Tris%20Warkentin%20and%20Jonathon%20Shlens%20and%20David%20Fleet%20and%20Victor%20Cotruta%20and%20Omar%20Sanseviero%20and%20Gus%20Martins%20and%20Phoebe%20Kirk%20and%20Anand%20Rao%20and%20Shravya%20Shetty%20and%20David%20F.%20Steiner%20and%20Can%20Kirmizibayrak%20and%20Rory%20Pilgrim%20and%20Daniel%20Golden%20and%20Lin%20Yang&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20has%20significant%20potential%20in%20healthcare%0Aapplications%2C%20but%20its%20training%20and%20deployment%20faces%20challenges%20due%20to%0Ahealthcare%27s%20diverse%20data%2C%20complex%20tasks%2C%20and%20the%20need%20to%20preserve%20privacy.%0AFoundation%20models%20that%20perform%20well%20on%20medical%20tasks%20and%20require%20less%0Atask-specific%20tuning%20data%20are%20critical%20to%20accelerate%20the%20development%20of%0Ahealthcare%20AI%20applications.%20We%20introduce%20MedGemma%2C%20a%20collection%20of%20medical%0Avision-language%20foundation%20models%20based%20on%20Gemma%203%204B%20and%2027B.%20MedGemma%0Ademonstrates%20advanced%20medical%20understanding%20and%20reasoning%20on%20images%20and%20text%2C%0Asignificantly%20exceeding%20the%20performance%20of%20similar-sized%20generative%20models%20and%0Aapproaching%20the%20performance%20of%20task-specific%20models%2C%20while%20maintaining%20the%0Ageneral%20capabilities%20of%20the%20Gemma%203%20base%20models.%20For%20out-of-distribution%20tasks%2C%0AMedGemma%20achieves%202.6-10%25%20improvement%20on%20medical%20multimodal%20question%20answering%2C%0A15.5-18.1%25%20improvement%20on%20chest%20X-ray%20finding%20classification%2C%20and%2010.8%25%0Aimprovement%20on%20agentic%20evaluations%20compared%20to%20the%20base%20models.%20Fine-tuning%0AMedGemma%20further%20improves%20performance%20in%20subdomains%2C%20reducing%20errors%20in%0Aelectronic%20health%20record%20information%20retrieval%20by%2050%25%20and%20reaching%20comparable%0Aperformance%20to%20existing%20specialized%20state-of-the-art%20methods%20for%20pneumothorax%0Aclassification%20and%20histopathology%20patch%20classification.%20We%20additionally%0Aintroduce%20MedSigLIP%2C%20a%20medically-tuned%20vision%20encoder%20derived%20from%20SigLIP.%0AMedSigLIP%20powers%20the%20visual%20understanding%20capabilities%20of%20MedGemma%20and%20as%20an%0Aencoder%20achieves%20comparable%20or%20better%20performance%20than%20specialized%20medical%0Aimage%20encoders.%20Taken%20together%2C%20the%20MedGemma%20collection%20provides%20a%20strong%0Afoundation%20of%20medical%20image%20and%20text%20capabilities%2C%20with%20potential%20to%0Asignificantly%20accelerate%20medical%20research%20and%20development%20of%20downstream%0Aapplications.%20The%20MedGemma%20collection%2C%20including%20tutorials%20and%20model%20weights%2C%0Acan%20be%20found%20at%20https%3A//goo.gle/medgemma.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05201v2&entry.124074799=Read"},
{"title": "What ZTF Saw Where Rubin Looked: Anomaly Hunting in DR23", "author": "Maria V. Pruzhinskaya and Anastasia D. Lavrukhina and Timofey A. Semenikhi and Alina A. Volnova and Sreevarsha Sreejith and Vadim V. Krushinsky and Emmanuel Gangler and Emille E. O. Ishida and Matwey V. Kornilov and Konstantin L. Malanchev", "abstract": "  We present results from the SNAD VIII Workshop, during which we conducted the\nfirst systematic anomaly search in the ZTF fields also observed by LSSTComCam\nduring Rubin Scientific Pipeline commissioning. Using the PineForest active\nanomaly detection algorithm, we analysed four selected fields (two galactic and\ntwo extragalactic) and visually inspected 400 candidates. As a result, we\ndiscovered six previously uncatalogued variable stars, including RS~CVn, BY\nDraconis, ellipsoidal, and solar-type variables, and refined classifications\nand periods for six known objects. These results demonstrate the effectiveness\nof the SNAD anomaly detection pipeline and provide a preview of the discovery\npotential in the upcoming LSST data.\n", "link": "http://arxiv.org/abs/2507.06217v1", "date": "2025-07-08", "relevancy": 2.0975, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4305}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.414}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20ZTF%20Saw%20Where%20Rubin%20Looked%3A%20Anomaly%20Hunting%20in%20DR23&body=Title%3A%20What%20ZTF%20Saw%20Where%20Rubin%20Looked%3A%20Anomaly%20Hunting%20in%20DR23%0AAuthor%3A%20Maria%20V.%20Pruzhinskaya%20and%20Anastasia%20D.%20Lavrukhina%20and%20Timofey%20A.%20Semenikhi%20and%20Alina%20A.%20Volnova%20and%20Sreevarsha%20Sreejith%20and%20Vadim%20V.%20Krushinsky%20and%20Emmanuel%20Gangler%20and%20Emille%20E.%20O.%20Ishida%20and%20Matwey%20V.%20Kornilov%20and%20Konstantin%20L.%20Malanchev%0AAbstract%3A%20%20%20We%20present%20results%20from%20the%20SNAD%20VIII%20Workshop%2C%20during%20which%20we%20conducted%20the%0Afirst%20systematic%20anomaly%20search%20in%20the%20ZTF%20fields%20also%20observed%20by%20LSSTComCam%0Aduring%20Rubin%20Scientific%20Pipeline%20commissioning.%20Using%20the%20PineForest%20active%0Aanomaly%20detection%20algorithm%2C%20we%20analysed%20four%20selected%20fields%20%28two%20galactic%20and%0Atwo%20extragalactic%29%20and%20visually%20inspected%20400%20candidates.%20As%20a%20result%2C%20we%0Adiscovered%20six%20previously%20uncatalogued%20variable%20stars%2C%20including%20RS~CVn%2C%20BY%0ADraconis%2C%20ellipsoidal%2C%20and%20solar-type%20variables%2C%20and%20refined%20classifications%0Aand%20periods%20for%20six%20known%20objects.%20These%20results%20demonstrate%20the%20effectiveness%0Aof%20the%20SNAD%20anomaly%20detection%20pipeline%20and%20provide%20a%20preview%20of%20the%20discovery%0Apotential%20in%20the%20upcoming%20LSST%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520ZTF%2520Saw%2520Where%2520Rubin%2520Looked%253A%2520Anomaly%2520Hunting%2520in%2520DR23%26entry.906535625%3DMaria%2520V.%2520Pruzhinskaya%2520and%2520Anastasia%2520D.%2520Lavrukhina%2520and%2520Timofey%2520A.%2520Semenikhi%2520and%2520Alina%2520A.%2520Volnova%2520and%2520Sreevarsha%2520Sreejith%2520and%2520Vadim%2520V.%2520Krushinsky%2520and%2520Emmanuel%2520Gangler%2520and%2520Emille%2520E.%2520O.%2520Ishida%2520and%2520Matwey%2520V.%2520Kornilov%2520and%2520Konstantin%2520L.%2520Malanchev%26entry.1292438233%3D%2520%2520We%2520present%2520results%2520from%2520the%2520SNAD%2520VIII%2520Workshop%252C%2520during%2520which%2520we%2520conducted%2520the%250Afirst%2520systematic%2520anomaly%2520search%2520in%2520the%2520ZTF%2520fields%2520also%2520observed%2520by%2520LSSTComCam%250Aduring%2520Rubin%2520Scientific%2520Pipeline%2520commissioning.%2520Using%2520the%2520PineForest%2520active%250Aanomaly%2520detection%2520algorithm%252C%2520we%2520analysed%2520four%2520selected%2520fields%2520%2528two%2520galactic%2520and%250Atwo%2520extragalactic%2529%2520and%2520visually%2520inspected%2520400%2520candidates.%2520As%2520a%2520result%252C%2520we%250Adiscovered%2520six%2520previously%2520uncatalogued%2520variable%2520stars%252C%2520including%2520RS~CVn%252C%2520BY%250ADraconis%252C%2520ellipsoidal%252C%2520and%2520solar-type%2520variables%252C%2520and%2520refined%2520classifications%250Aand%2520periods%2520for%2520six%2520known%2520objects.%2520These%2520results%2520demonstrate%2520the%2520effectiveness%250Aof%2520the%2520SNAD%2520anomaly%2520detection%2520pipeline%2520and%2520provide%2520a%2520preview%2520of%2520the%2520discovery%250Apotential%2520in%2520the%2520upcoming%2520LSST%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20ZTF%20Saw%20Where%20Rubin%20Looked%3A%20Anomaly%20Hunting%20in%20DR23&entry.906535625=Maria%20V.%20Pruzhinskaya%20and%20Anastasia%20D.%20Lavrukhina%20and%20Timofey%20A.%20Semenikhi%20and%20Alina%20A.%20Volnova%20and%20Sreevarsha%20Sreejith%20and%20Vadim%20V.%20Krushinsky%20and%20Emmanuel%20Gangler%20and%20Emille%20E.%20O.%20Ishida%20and%20Matwey%20V.%20Kornilov%20and%20Konstantin%20L.%20Malanchev&entry.1292438233=%20%20We%20present%20results%20from%20the%20SNAD%20VIII%20Workshop%2C%20during%20which%20we%20conducted%20the%0Afirst%20systematic%20anomaly%20search%20in%20the%20ZTF%20fields%20also%20observed%20by%20LSSTComCam%0Aduring%20Rubin%20Scientific%20Pipeline%20commissioning.%20Using%20the%20PineForest%20active%0Aanomaly%20detection%20algorithm%2C%20we%20analysed%20four%20selected%20fields%20%28two%20galactic%20and%0Atwo%20extragalactic%29%20and%20visually%20inspected%20400%20candidates.%20As%20a%20result%2C%20we%0Adiscovered%20six%20previously%20uncatalogued%20variable%20stars%2C%20including%20RS~CVn%2C%20BY%0ADraconis%2C%20ellipsoidal%2C%20and%20solar-type%20variables%2C%20and%20refined%20classifications%0Aand%20periods%20for%20six%20known%20objects.%20These%20results%20demonstrate%20the%20effectiveness%0Aof%20the%20SNAD%20anomaly%20detection%20pipeline%20and%20provide%20a%20preview%20of%20the%20discovery%0Apotential%20in%20the%20upcoming%20LSST%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06217v1&entry.124074799=Read"},
{"title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I.\n  X-Master as Foundation: Can We Lead on Humanity's Last Exam?", "author": "Jingyi Chai and Shuo Tang and Rui Ye and Yuwen Du and Xinyu Zhu and Mengcheng Zhou and Yanfeng Wang and Weinan E and Yuzhi Zhang and Linfeng Zhang and Siheng Chen", "abstract": "  The rapid advancements of AI agents have ignited the long-held ambition of\nleveraging them to accelerate scientific discovery. Achieving this goal\nrequires a deep understanding of the frontiers of human knowledge. As such,\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\nevaluating scientific AI agents. In this work, we aim to construct the\nfoundational architecture for general-purpose agents and validate the\ncapabilities through leading performance on HLE. To achieve this, we introduce\nX-Master, a tool-augmented reasoning agent designed to emulate human\nresearchers by interacting flexibly with external tools during its reasoning\nprocess. This agent, guided by the conceptualization of code as an interaction\nlanguage, can flexibly leverage built-in Python libraries and our customized\ntools to augment the reasoning. We further scale its capabilities through\nX-Masters, a scattered-and-stacked agentic workflow that systematically\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\ncomplex task-solving and accumulates valuable experience that can inform future\nadvancements, guiding subsequent model training.\n", "link": "http://arxiv.org/abs/2507.05241v2", "date": "2025-07-08", "relevancy": 2.0952, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5518}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SciMaster%3A%20Towards%20General-Purpose%20Scientific%20AI%20Agents%2C%20Part%20I.%0A%20%20X-Master%20as%20Foundation%3A%20Can%20We%20Lead%20on%20Humanity%27s%20Last%20Exam%3F&body=Title%3A%20SciMaster%3A%20Towards%20General-Purpose%20Scientific%20AI%20Agents%2C%20Part%20I.%0A%20%20X-Master%20as%20Foundation%3A%20Can%20We%20Lead%20on%20Humanity%27s%20Last%20Exam%3F%0AAuthor%3A%20Jingyi%20Chai%20and%20Shuo%20Tang%20and%20Rui%20Ye%20and%20Yuwen%20Du%20and%20Xinyu%20Zhu%20and%20Mengcheng%20Zhou%20and%20Yanfeng%20Wang%20and%20Weinan%20E%20and%20Yuzhi%20Zhang%20and%20Linfeng%20Zhang%20and%20Siheng%20Chen%0AAbstract%3A%20%20%20The%20rapid%20advancements%20of%20AI%20agents%20have%20ignited%20the%20long-held%20ambition%20of%0Aleveraging%20them%20to%20accelerate%20scientific%20discovery.%20Achieving%20this%20goal%0Arequires%20a%20deep%20understanding%20of%20the%20frontiers%20of%20human%20knowledge.%20As%20such%2C%0AHumanity%27s%20Last%20Exam%20%28HLE%29%20provides%20an%20exceptionally%20challenging%20touchstone%20for%0Aevaluating%20scientific%20AI%20agents.%20In%20this%20work%2C%20we%20aim%20to%20construct%20the%0Afoundational%20architecture%20for%20general-purpose%20agents%20and%20validate%20the%0Acapabilities%20through%20leading%20performance%20on%20HLE.%20To%20achieve%20this%2C%20we%20introduce%0AX-Master%2C%20a%20tool-augmented%20reasoning%20agent%20designed%20to%20emulate%20human%0Aresearchers%20by%20interacting%20flexibly%20with%20external%20tools%20during%20its%20reasoning%0Aprocess.%20This%20agent%2C%20guided%20by%20the%20conceptualization%20of%20code%20as%20an%20interaction%0Alanguage%2C%20can%20flexibly%20leverage%20built-in%20Python%20libraries%20and%20our%20customized%0Atools%20to%20augment%20the%20reasoning.%20We%20further%20scale%20its%20capabilities%20through%0AX-Masters%2C%20a%20scattered-and-stacked%20agentic%20workflow%20that%20systematically%0Aenhances%20breadth%20and%20depth%20of%20reasoning.%20Our%20open-source%20solution%2C%20X-Masters%2C%0Asets%20a%20new%20state-of-the-art%20record%20on%20HLE%20with%20a%20score%20of%2032.1%25%2C%20surpassing%0AOpenAI%27s%20and%20Google%27s%20Deep%20Research%20%2826.6%25%20and%2026.9%25%29%20and%20becoming%20the%20first%20to%0Aexceed%20the%2030%25%20threshold.%20This%20work%20allows%20us%20to%20gain%20a%20deeper%20understanding%20of%0Acomplex%20task-solving%20and%20accumulates%20valuable%20experience%20that%20can%20inform%20future%0Aadvancements%2C%20guiding%20subsequent%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSciMaster%253A%2520Towards%2520General-Purpose%2520Scientific%2520AI%2520Agents%252C%2520Part%2520I.%250A%2520%2520X-Master%2520as%2520Foundation%253A%2520Can%2520We%2520Lead%2520on%2520Humanity%2527s%2520Last%2520Exam%253F%26entry.906535625%3DJingyi%2520Chai%2520and%2520Shuo%2520Tang%2520and%2520Rui%2520Ye%2520and%2520Yuwen%2520Du%2520and%2520Xinyu%2520Zhu%2520and%2520Mengcheng%2520Zhou%2520and%2520Yanfeng%2520Wang%2520and%2520Weinan%2520E%2520and%2520Yuzhi%2520Zhang%2520and%2520Linfeng%2520Zhang%2520and%2520Siheng%2520Chen%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancements%2520of%2520AI%2520agents%2520have%2520ignited%2520the%2520long-held%2520ambition%2520of%250Aleveraging%2520them%2520to%2520accelerate%2520scientific%2520discovery.%2520Achieving%2520this%2520goal%250Arequires%2520a%2520deep%2520understanding%2520of%2520the%2520frontiers%2520of%2520human%2520knowledge.%2520As%2520such%252C%250AHumanity%2527s%2520Last%2520Exam%2520%2528HLE%2529%2520provides%2520an%2520exceptionally%2520challenging%2520touchstone%2520for%250Aevaluating%2520scientific%2520AI%2520agents.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520construct%2520the%250Afoundational%2520architecture%2520for%2520general-purpose%2520agents%2520and%2520validate%2520the%250Acapabilities%2520through%2520leading%2520performance%2520on%2520HLE.%2520To%2520achieve%2520this%252C%2520we%2520introduce%250AX-Master%252C%2520a%2520tool-augmented%2520reasoning%2520agent%2520designed%2520to%2520emulate%2520human%250Aresearchers%2520by%2520interacting%2520flexibly%2520with%2520external%2520tools%2520during%2520its%2520reasoning%250Aprocess.%2520This%2520agent%252C%2520guided%2520by%2520the%2520conceptualization%2520of%2520code%2520as%2520an%2520interaction%250Alanguage%252C%2520can%2520flexibly%2520leverage%2520built-in%2520Python%2520libraries%2520and%2520our%2520customized%250Atools%2520to%2520augment%2520the%2520reasoning.%2520We%2520further%2520scale%2520its%2520capabilities%2520through%250AX-Masters%252C%2520a%2520scattered-and-stacked%2520agentic%2520workflow%2520that%2520systematically%250Aenhances%2520breadth%2520and%2520depth%2520of%2520reasoning.%2520Our%2520open-source%2520solution%252C%2520X-Masters%252C%250Asets%2520a%2520new%2520state-of-the-art%2520record%2520on%2520HLE%2520with%2520a%2520score%2520of%252032.1%2525%252C%2520surpassing%250AOpenAI%2527s%2520and%2520Google%2527s%2520Deep%2520Research%2520%252826.6%2525%2520and%252026.9%2525%2529%2520and%2520becoming%2520the%2520first%2520to%250Aexceed%2520the%252030%2525%2520threshold.%2520This%2520work%2520allows%2520us%2520to%2520gain%2520a%2520deeper%2520understanding%2520of%250Acomplex%2520task-solving%2520and%2520accumulates%2520valuable%2520experience%2520that%2520can%2520inform%2520future%250Aadvancements%252C%2520guiding%2520subsequent%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SciMaster%3A%20Towards%20General-Purpose%20Scientific%20AI%20Agents%2C%20Part%20I.%0A%20%20X-Master%20as%20Foundation%3A%20Can%20We%20Lead%20on%20Humanity%27s%20Last%20Exam%3F&entry.906535625=Jingyi%20Chai%20and%20Shuo%20Tang%20and%20Rui%20Ye%20and%20Yuwen%20Du%20and%20Xinyu%20Zhu%20and%20Mengcheng%20Zhou%20and%20Yanfeng%20Wang%20and%20Weinan%20E%20and%20Yuzhi%20Zhang%20and%20Linfeng%20Zhang%20and%20Siheng%20Chen&entry.1292438233=%20%20The%20rapid%20advancements%20of%20AI%20agents%20have%20ignited%20the%20long-held%20ambition%20of%0Aleveraging%20them%20to%20accelerate%20scientific%20discovery.%20Achieving%20this%20goal%0Arequires%20a%20deep%20understanding%20of%20the%20frontiers%20of%20human%20knowledge.%20As%20such%2C%0AHumanity%27s%20Last%20Exam%20%28HLE%29%20provides%20an%20exceptionally%20challenging%20touchstone%20for%0Aevaluating%20scientific%20AI%20agents.%20In%20this%20work%2C%20we%20aim%20to%20construct%20the%0Afoundational%20architecture%20for%20general-purpose%20agents%20and%20validate%20the%0Acapabilities%20through%20leading%20performance%20on%20HLE.%20To%20achieve%20this%2C%20we%20introduce%0AX-Master%2C%20a%20tool-augmented%20reasoning%20agent%20designed%20to%20emulate%20human%0Aresearchers%20by%20interacting%20flexibly%20with%20external%20tools%20during%20its%20reasoning%0Aprocess.%20This%20agent%2C%20guided%20by%20the%20conceptualization%20of%20code%20as%20an%20interaction%0Alanguage%2C%20can%20flexibly%20leverage%20built-in%20Python%20libraries%20and%20our%20customized%0Atools%20to%20augment%20the%20reasoning.%20We%20further%20scale%20its%20capabilities%20through%0AX-Masters%2C%20a%20scattered-and-stacked%20agentic%20workflow%20that%20systematically%0Aenhances%20breadth%20and%20depth%20of%20reasoning.%20Our%20open-source%20solution%2C%20X-Masters%2C%0Asets%20a%20new%20state-of-the-art%20record%20on%20HLE%20with%20a%20score%20of%2032.1%25%2C%20surpassing%0AOpenAI%27s%20and%20Google%27s%20Deep%20Research%20%2826.6%25%20and%2026.9%25%29%20and%20becoming%20the%20first%20to%0Aexceed%20the%2030%25%20threshold.%20This%20work%20allows%20us%20to%20gain%20a%20deeper%20understanding%20of%0Acomplex%20task-solving%20and%20accumulates%20valuable%20experience%20that%20can%20inform%20future%0Aadvancements%2C%20guiding%20subsequent%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05241v2&entry.124074799=Read"},
{"title": "Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot\n  Multi-Species Plant Identification", "author": "Murilo Gustineli and Anthony Miyaguchi and Adrian Cheung and Divyansh Khattak", "abstract": "  We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on\nmulti-species plant identification in vegetation quadrat images. Our pipeline\ncombines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level\ninference, (ii) a 4x4 tiling strategy that aligns patch size with the network's\n518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +\nK-Means visual clustering and geolocation filtering. Tile predictions are\naggregated by majority vote and re-weighted with cluster-specific Bayesian\npriors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while\nrequiring no additional training. All code, configuration files, and\nreproducibility scripts are publicly available at\nhttps://github.com/dsgt-arc/plantclef-2025.\n", "link": "http://arxiv.org/abs/2507.06093v1", "date": "2025-07-08", "relevancy": 2.0787, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5372}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5083}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tile-Based%20ViT%20Inference%20with%20Visual-Cluster%20Priors%20for%20Zero-Shot%0A%20%20Multi-Species%20Plant%20Identification&body=Title%3A%20Tile-Based%20ViT%20Inference%20with%20Visual-Cluster%20Priors%20for%20Zero-Shot%0A%20%20Multi-Species%20Plant%20Identification%0AAuthor%3A%20Murilo%20Gustineli%20and%20Anthony%20Miyaguchi%20and%20Adrian%20Cheung%20and%20Divyansh%20Khattak%0AAbstract%3A%20%20%20We%20describe%20DS%40GT%27s%20second-place%20solution%20to%20the%20PlantCLEF%202025%20challenge%20on%0Amulti-species%20plant%20identification%20in%20vegetation%20quadrat%20images.%20Our%20pipeline%0Acombines%20%28i%29%20a%20fine-tuned%20Vision%20Transformer%20ViTD2PC24All%20for%20patch-level%0Ainference%2C%20%28ii%29%20a%204x4%20tiling%20strategy%20that%20aligns%20patch%20size%20with%20the%20network%27s%0A518x518%20receptive%20field%2C%20and%20%28iii%29%20domain-prior%20adaptation%20through%20PaCMAP%20%2B%0AK-Means%20visual%20clustering%20and%20geolocation%20filtering.%20Tile%20predictions%20are%0Aaggregated%20by%20majority%20vote%20and%20re-weighted%20with%20cluster-specific%20Bayesian%0Apriors%2C%20yielding%20a%20macro-averaged%20F1%20of%200.348%20%28private%20leaderboard%29%20while%0Arequiring%20no%20additional%20training.%20All%20code%2C%20configuration%20files%2C%20and%0Areproducibility%20scripts%20are%20publicly%20available%20at%0Ahttps%3A//github.com/dsgt-arc/plantclef-2025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTile-Based%2520ViT%2520Inference%2520with%2520Visual-Cluster%2520Priors%2520for%2520Zero-Shot%250A%2520%2520Multi-Species%2520Plant%2520Identification%26entry.906535625%3DMurilo%2520Gustineli%2520and%2520Anthony%2520Miyaguchi%2520and%2520Adrian%2520Cheung%2520and%2520Divyansh%2520Khattak%26entry.1292438233%3D%2520%2520We%2520describe%2520DS%2540GT%2527s%2520second-place%2520solution%2520to%2520the%2520PlantCLEF%25202025%2520challenge%2520on%250Amulti-species%2520plant%2520identification%2520in%2520vegetation%2520quadrat%2520images.%2520Our%2520pipeline%250Acombines%2520%2528i%2529%2520a%2520fine-tuned%2520Vision%2520Transformer%2520ViTD2PC24All%2520for%2520patch-level%250Ainference%252C%2520%2528ii%2529%2520a%25204x4%2520tiling%2520strategy%2520that%2520aligns%2520patch%2520size%2520with%2520the%2520network%2527s%250A518x518%2520receptive%2520field%252C%2520and%2520%2528iii%2529%2520domain-prior%2520adaptation%2520through%2520PaCMAP%2520%252B%250AK-Means%2520visual%2520clustering%2520and%2520geolocation%2520filtering.%2520Tile%2520predictions%2520are%250Aaggregated%2520by%2520majority%2520vote%2520and%2520re-weighted%2520with%2520cluster-specific%2520Bayesian%250Apriors%252C%2520yielding%2520a%2520macro-averaged%2520F1%2520of%25200.348%2520%2528private%2520leaderboard%2529%2520while%250Arequiring%2520no%2520additional%2520training.%2520All%2520code%252C%2520configuration%2520files%252C%2520and%250Areproducibility%2520scripts%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/dsgt-arc/plantclef-2025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tile-Based%20ViT%20Inference%20with%20Visual-Cluster%20Priors%20for%20Zero-Shot%0A%20%20Multi-Species%20Plant%20Identification&entry.906535625=Murilo%20Gustineli%20and%20Anthony%20Miyaguchi%20and%20Adrian%20Cheung%20and%20Divyansh%20Khattak&entry.1292438233=%20%20We%20describe%20DS%40GT%27s%20second-place%20solution%20to%20the%20PlantCLEF%202025%20challenge%20on%0Amulti-species%20plant%20identification%20in%20vegetation%20quadrat%20images.%20Our%20pipeline%0Acombines%20%28i%29%20a%20fine-tuned%20Vision%20Transformer%20ViTD2PC24All%20for%20patch-level%0Ainference%2C%20%28ii%29%20a%204x4%20tiling%20strategy%20that%20aligns%20patch%20size%20with%20the%20network%27s%0A518x518%20receptive%20field%2C%20and%20%28iii%29%20domain-prior%20adaptation%20through%20PaCMAP%20%2B%0AK-Means%20visual%20clustering%20and%20geolocation%20filtering.%20Tile%20predictions%20are%0Aaggregated%20by%20majority%20vote%20and%20re-weighted%20with%20cluster-specific%20Bayesian%0Apriors%2C%20yielding%20a%20macro-averaged%20F1%20of%200.348%20%28private%20leaderboard%29%20while%0Arequiring%20no%20additional%20training.%20All%20code%2C%20configuration%20files%2C%20and%0Areproducibility%20scripts%20are%20publicly%20available%20at%0Ahttps%3A//github.com/dsgt-arc/plantclef-2025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06093v1&entry.124074799=Read"},
{"title": "DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning\n  and Corrective Data Augmentation", "author": "Maximilian Heil and Dionne Bang", "abstract": "  This paper presents our submission to Task 1, Subjectivity Detection, of the\nCheckThat! Lab at CLEF 2025. We investigate the effectiveness of\ntransfer-learning and stylistic data augmentation to improve classification of\nsubjective and objective sentences in English news text. Our approach contrasts\nfine-tuning of pre-trained encoders and transfer-learning of fine-tuned\ntransformer on related tasks. We also introduce a controlled augmentation\npipeline using GPT-4o to generate paraphrases in predefined subjectivity\nstyles. To ensure label and style consistency, we employ the same model to\ncorrect and refine the generated samples. Results show that transfer-learning\nof specified encoders outperforms fine-tuning general-purpose ones, and that\ncarefully curated augmentation significantly enhances model robustness,\nespecially in detecting subjective content. Our official submission placed us\n$16^{th}$ of 24 participants. Overall, our findings underscore the value of\ncombining encoder specialization with label-consistent augmentation for\nimproved subjectivity detection. Our code is available at\nhttps://github.com/dsgt-arc/checkthat-2025-subject.\n", "link": "http://arxiv.org/abs/2507.06189v1", "date": "2025-07-08", "relevancy": 2.0668, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.53}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5082}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DS%40GT%20at%20CheckThat%21%202025%3A%20Detecting%20Subjectivity%20via%20Transfer-Learning%0A%20%20and%20Corrective%20Data%20Augmentation&body=Title%3A%20DS%40GT%20at%20CheckThat%21%202025%3A%20Detecting%20Subjectivity%20via%20Transfer-Learning%0A%20%20and%20Corrective%20Data%20Augmentation%0AAuthor%3A%20Maximilian%20Heil%20and%20Dionne%20Bang%0AAbstract%3A%20%20%20This%20paper%20presents%20our%20submission%20to%20Task%201%2C%20Subjectivity%20Detection%2C%20of%20the%0ACheckThat%21%20Lab%20at%20CLEF%202025.%20We%20investigate%20the%20effectiveness%20of%0Atransfer-learning%20and%20stylistic%20data%20augmentation%20to%20improve%20classification%20of%0Asubjective%20and%20objective%20sentences%20in%20English%20news%20text.%20Our%20approach%20contrasts%0Afine-tuning%20of%20pre-trained%20encoders%20and%20transfer-learning%20of%20fine-tuned%0Atransformer%20on%20related%20tasks.%20We%20also%20introduce%20a%20controlled%20augmentation%0Apipeline%20using%20GPT-4o%20to%20generate%20paraphrases%20in%20predefined%20subjectivity%0Astyles.%20To%20ensure%20label%20and%20style%20consistency%2C%20we%20employ%20the%20same%20model%20to%0Acorrect%20and%20refine%20the%20generated%20samples.%20Results%20show%20that%20transfer-learning%0Aof%20specified%20encoders%20outperforms%20fine-tuning%20general-purpose%20ones%2C%20and%20that%0Acarefully%20curated%20augmentation%20significantly%20enhances%20model%20robustness%2C%0Aespecially%20in%20detecting%20subjective%20content.%20Our%20official%20submission%20placed%20us%0A%2416%5E%7Bth%7D%24%20of%2024%20participants.%20Overall%2C%20our%20findings%20underscore%20the%20value%20of%0Acombining%20encoder%20specialization%20with%20label-consistent%20augmentation%20for%0Aimproved%20subjectivity%20detection.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/dsgt-arc/checkthat-2025-subject.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDS%2540GT%2520at%2520CheckThat%2521%25202025%253A%2520Detecting%2520Subjectivity%2520via%2520Transfer-Learning%250A%2520%2520and%2520Corrective%2520Data%2520Augmentation%26entry.906535625%3DMaximilian%2520Heil%2520and%2520Dionne%2520Bang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520our%2520submission%2520to%2520Task%25201%252C%2520Subjectivity%2520Detection%252C%2520of%2520the%250ACheckThat%2521%2520Lab%2520at%2520CLEF%25202025.%2520We%2520investigate%2520the%2520effectiveness%2520of%250Atransfer-learning%2520and%2520stylistic%2520data%2520augmentation%2520to%2520improve%2520classification%2520of%250Asubjective%2520and%2520objective%2520sentences%2520in%2520English%2520news%2520text.%2520Our%2520approach%2520contrasts%250Afine-tuning%2520of%2520pre-trained%2520encoders%2520and%2520transfer-learning%2520of%2520fine-tuned%250Atransformer%2520on%2520related%2520tasks.%2520We%2520also%2520introduce%2520a%2520controlled%2520augmentation%250Apipeline%2520using%2520GPT-4o%2520to%2520generate%2520paraphrases%2520in%2520predefined%2520subjectivity%250Astyles.%2520To%2520ensure%2520label%2520and%2520style%2520consistency%252C%2520we%2520employ%2520the%2520same%2520model%2520to%250Acorrect%2520and%2520refine%2520the%2520generated%2520samples.%2520Results%2520show%2520that%2520transfer-learning%250Aof%2520specified%2520encoders%2520outperforms%2520fine-tuning%2520general-purpose%2520ones%252C%2520and%2520that%250Acarefully%2520curated%2520augmentation%2520significantly%2520enhances%2520model%2520robustness%252C%250Aespecially%2520in%2520detecting%2520subjective%2520content.%2520Our%2520official%2520submission%2520placed%2520us%250A%252416%255E%257Bth%257D%2524%2520of%252024%2520participants.%2520Overall%252C%2520our%2520findings%2520underscore%2520the%2520value%2520of%250Acombining%2520encoder%2520specialization%2520with%2520label-consistent%2520augmentation%2520for%250Aimproved%2520subjectivity%2520detection.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/dsgt-arc/checkthat-2025-subject.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DS%40GT%20at%20CheckThat%21%202025%3A%20Detecting%20Subjectivity%20via%20Transfer-Learning%0A%20%20and%20Corrective%20Data%20Augmentation&entry.906535625=Maximilian%20Heil%20and%20Dionne%20Bang&entry.1292438233=%20%20This%20paper%20presents%20our%20submission%20to%20Task%201%2C%20Subjectivity%20Detection%2C%20of%20the%0ACheckThat%21%20Lab%20at%20CLEF%202025.%20We%20investigate%20the%20effectiveness%20of%0Atransfer-learning%20and%20stylistic%20data%20augmentation%20to%20improve%20classification%20of%0Asubjective%20and%20objective%20sentences%20in%20English%20news%20text.%20Our%20approach%20contrasts%0Afine-tuning%20of%20pre-trained%20encoders%20and%20transfer-learning%20of%20fine-tuned%0Atransformer%20on%20related%20tasks.%20We%20also%20introduce%20a%20controlled%20augmentation%0Apipeline%20using%20GPT-4o%20to%20generate%20paraphrases%20in%20predefined%20subjectivity%0Astyles.%20To%20ensure%20label%20and%20style%20consistency%2C%20we%20employ%20the%20same%20model%20to%0Acorrect%20and%20refine%20the%20generated%20samples.%20Results%20show%20that%20transfer-learning%0Aof%20specified%20encoders%20outperforms%20fine-tuning%20general-purpose%20ones%2C%20and%20that%0Acarefully%20curated%20augmentation%20significantly%20enhances%20model%20robustness%2C%0Aespecially%20in%20detecting%20subjective%20content.%20Our%20official%20submission%20placed%20us%0A%2416%5E%7Bth%7D%24%20of%2024%20participants.%20Overall%2C%20our%20findings%20underscore%20the%20value%20of%0Acombining%20encoder%20specialization%20with%20label-consistent%20augmentation%20for%0Aimproved%20subjectivity%20detection.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/dsgt-arc/checkthat-2025-subject.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06189v1&entry.124074799=Read"},
{"title": "Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural\n  Control Barrier Functions", "author": "Ji Yin and Oswin So and Eric Yang Yu and Chuchu Fan and Panagiotis Tsiotras", "abstract": "  A common problem when using model predictive control (MPC) in practice is the\nsatisfaction of safety specifications beyond the prediction horizon. While\ntheoretical works have shown that safety can be guaranteed by enforcing a\nsuitable terminal set constraint or a sufficiently long prediction horizon,\nthese techniques are difficult to apply and thus are rarely used by\npractitioners, especially in the case of general nonlinear dynamics. To solve\nthis problem, we impose a tradeoff between exact recursive feasibility,\ncomputational tractability, and applicability to ``black-box'' dynamics by\nlearning an approximate discrete-time control barrier function and\nincorporating it into a variational inference MPC (VIMPC), a sampling-based MPC\nparadigm. To handle the resulting state constraints, we further propose a new\nsampling strategy that greatly reduces the variance of the estimated optimal\ncontrol, improving the sample efficiency, and enabling real-time planning on a\nCPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial\nsafety improvements compared to existing sampling-based MPC controllers, even\nunder badly designed cost functions. We validate our approach in both\nsimulation and real-world hardware experiments. Project website:\nhttps://mit-realm.github.io/ns-vimpc/.\n", "link": "http://arxiv.org/abs/2502.15006v2", "date": "2025-07-08", "relevancy": 2.055, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5537}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5266}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Beyond%20the%20Horizon%3A%20Efficient%20Sampling-based%20MPC%20with%20Neural%0A%20%20Control%20Barrier%20Functions&body=Title%3A%20Safe%20Beyond%20the%20Horizon%3A%20Efficient%20Sampling-based%20MPC%20with%20Neural%0A%20%20Control%20Barrier%20Functions%0AAuthor%3A%20Ji%20Yin%20and%20Oswin%20So%20and%20Eric%20Yang%20Yu%20and%20Chuchu%20Fan%20and%20Panagiotis%20Tsiotras%0AAbstract%3A%20%20%20A%20common%20problem%20when%20using%20model%20predictive%20control%20%28MPC%29%20in%20practice%20is%20the%0Asatisfaction%20of%20safety%20specifications%20beyond%20the%20prediction%20horizon.%20While%0Atheoretical%20works%20have%20shown%20that%20safety%20can%20be%20guaranteed%20by%20enforcing%20a%0Asuitable%20terminal%20set%20constraint%20or%20a%20sufficiently%20long%20prediction%20horizon%2C%0Athese%20techniques%20are%20difficult%20to%20apply%20and%20thus%20are%20rarely%20used%20by%0Apractitioners%2C%20especially%20in%20the%20case%20of%20general%20nonlinear%20dynamics.%20To%20solve%0Athis%20problem%2C%20we%20impose%20a%20tradeoff%20between%20exact%20recursive%20feasibility%2C%0Acomputational%20tractability%2C%20and%20applicability%20to%20%60%60black-box%27%27%20dynamics%20by%0Alearning%20an%20approximate%20discrete-time%20control%20barrier%20function%20and%0Aincorporating%20it%20into%20a%20variational%20inference%20MPC%20%28VIMPC%29%2C%20a%20sampling-based%20MPC%0Aparadigm.%20To%20handle%20the%20resulting%20state%20constraints%2C%20we%20further%20propose%20a%20new%0Asampling%20strategy%20that%20greatly%20reduces%20the%20variance%20of%20the%20estimated%20optimal%0Acontrol%2C%20improving%20the%20sample%20efficiency%2C%20and%20enabling%20real-time%20planning%20on%20a%0ACPU.%20The%20resulting%20Neural%20Shield-VIMPC%20%28NS-VIMPC%29%20controller%20yields%20substantial%0Asafety%20improvements%20compared%20to%20existing%20sampling-based%20MPC%20controllers%2C%20even%0Aunder%20badly%20designed%20cost%20functions.%20We%20validate%20our%20approach%20in%20both%0Asimulation%20and%20real-world%20hardware%20experiments.%20Project%20website%3A%0Ahttps%3A//mit-realm.github.io/ns-vimpc/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Beyond%2520the%2520Horizon%253A%2520Efficient%2520Sampling-based%2520MPC%2520with%2520Neural%250A%2520%2520Control%2520Barrier%2520Functions%26entry.906535625%3DJi%2520Yin%2520and%2520Oswin%2520So%2520and%2520Eric%2520Yang%2520Yu%2520and%2520Chuchu%2520Fan%2520and%2520Panagiotis%2520Tsiotras%26entry.1292438233%3D%2520%2520A%2520common%2520problem%2520when%2520using%2520model%2520predictive%2520control%2520%2528MPC%2529%2520in%2520practice%2520is%2520the%250Asatisfaction%2520of%2520safety%2520specifications%2520beyond%2520the%2520prediction%2520horizon.%2520While%250Atheoretical%2520works%2520have%2520shown%2520that%2520safety%2520can%2520be%2520guaranteed%2520by%2520enforcing%2520a%250Asuitable%2520terminal%2520set%2520constraint%2520or%2520a%2520sufficiently%2520long%2520prediction%2520horizon%252C%250Athese%2520techniques%2520are%2520difficult%2520to%2520apply%2520and%2520thus%2520are%2520rarely%2520used%2520by%250Apractitioners%252C%2520especially%2520in%2520the%2520case%2520of%2520general%2520nonlinear%2520dynamics.%2520To%2520solve%250Athis%2520problem%252C%2520we%2520impose%2520a%2520tradeoff%2520between%2520exact%2520recursive%2520feasibility%252C%250Acomputational%2520tractability%252C%2520and%2520applicability%2520to%2520%2560%2560black-box%2527%2527%2520dynamics%2520by%250Alearning%2520an%2520approximate%2520discrete-time%2520control%2520barrier%2520function%2520and%250Aincorporating%2520it%2520into%2520a%2520variational%2520inference%2520MPC%2520%2528VIMPC%2529%252C%2520a%2520sampling-based%2520MPC%250Aparadigm.%2520To%2520handle%2520the%2520resulting%2520state%2520constraints%252C%2520we%2520further%2520propose%2520a%2520new%250Asampling%2520strategy%2520that%2520greatly%2520reduces%2520the%2520variance%2520of%2520the%2520estimated%2520optimal%250Acontrol%252C%2520improving%2520the%2520sample%2520efficiency%252C%2520and%2520enabling%2520real-time%2520planning%2520on%2520a%250ACPU.%2520The%2520resulting%2520Neural%2520Shield-VIMPC%2520%2528NS-VIMPC%2529%2520controller%2520yields%2520substantial%250Asafety%2520improvements%2520compared%2520to%2520existing%2520sampling-based%2520MPC%2520controllers%252C%2520even%250Aunder%2520badly%2520designed%2520cost%2520functions.%2520We%2520validate%2520our%2520approach%2520in%2520both%250Asimulation%2520and%2520real-world%2520hardware%2520experiments.%2520Project%2520website%253A%250Ahttps%253A//mit-realm.github.io/ns-vimpc/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Beyond%20the%20Horizon%3A%20Efficient%20Sampling-based%20MPC%20with%20Neural%0A%20%20Control%20Barrier%20Functions&entry.906535625=Ji%20Yin%20and%20Oswin%20So%20and%20Eric%20Yang%20Yu%20and%20Chuchu%20Fan%20and%20Panagiotis%20Tsiotras&entry.1292438233=%20%20A%20common%20problem%20when%20using%20model%20predictive%20control%20%28MPC%29%20in%20practice%20is%20the%0Asatisfaction%20of%20safety%20specifications%20beyond%20the%20prediction%20horizon.%20While%0Atheoretical%20works%20have%20shown%20that%20safety%20can%20be%20guaranteed%20by%20enforcing%20a%0Asuitable%20terminal%20set%20constraint%20or%20a%20sufficiently%20long%20prediction%20horizon%2C%0Athese%20techniques%20are%20difficult%20to%20apply%20and%20thus%20are%20rarely%20used%20by%0Apractitioners%2C%20especially%20in%20the%20case%20of%20general%20nonlinear%20dynamics.%20To%20solve%0Athis%20problem%2C%20we%20impose%20a%20tradeoff%20between%20exact%20recursive%20feasibility%2C%0Acomputational%20tractability%2C%20and%20applicability%20to%20%60%60black-box%27%27%20dynamics%20by%0Alearning%20an%20approximate%20discrete-time%20control%20barrier%20function%20and%0Aincorporating%20it%20into%20a%20variational%20inference%20MPC%20%28VIMPC%29%2C%20a%20sampling-based%20MPC%0Aparadigm.%20To%20handle%20the%20resulting%20state%20constraints%2C%20we%20further%20propose%20a%20new%0Asampling%20strategy%20that%20greatly%20reduces%20the%20variance%20of%20the%20estimated%20optimal%0Acontrol%2C%20improving%20the%20sample%20efficiency%2C%20and%20enabling%20real-time%20planning%20on%20a%0ACPU.%20The%20resulting%20Neural%20Shield-VIMPC%20%28NS-VIMPC%29%20controller%20yields%20substantial%0Asafety%20improvements%20compared%20to%20existing%20sampling-based%20MPC%20controllers%2C%20even%0Aunder%20badly%20designed%20cost%20functions.%20We%20validate%20our%20approach%20in%20both%0Asimulation%20and%20real-world%20hardware%20experiments.%20Project%20website%3A%0Ahttps%3A//mit-realm.github.io/ns-vimpc/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15006v2&entry.124074799=Read"},
{"title": "Speech Quality Assessment Model Based on Mixture of Experts:\n  System-Level Performance Enhancement and Utterance-Level Challenge Analysis", "author": "Xintong Hu and Yixuan Chen and Rui Yang and Wenxiang Guo and Changhao Pan", "abstract": "  Automatic speech quality assessment plays a crucial role in the development\nof speech synthesis systems, but existing models exhibit significant\nperformance variations across different granularity levels of prediction tasks.\nThis paper proposes an enhanced MOS prediction system based on self-supervised\nlearning speech models, incorporating a Mixture of Experts (MoE) classification\nhead and utilizing synthetic data from multiple commercial generation models\nfor data augmentation. Our method builds upon existing self-supervised models\nsuch as wav2vec2, designing a specialized MoE architecture to address different\ntypes of speech quality assessment tasks. We also collected a large-scale\nsynthetic speech dataset encompassing the latest text-to-speech, speech\nconversion, and speech enhancement systems. However, despite the adoption of\nthe MoE architecture and expanded dataset, the model's performance improvements\nin sentence-level prediction tasks remain limited. Our work reveals the\nlimitations of current methods in handling sentence-level quality assessment,\nprovides new technical pathways for the field of automatic speech quality\nassessment, and also delves into the fundamental causes of performance\ndifferences across different assessment granularities.\n", "link": "http://arxiv.org/abs/2507.06116v1", "date": "2025-07-08", "relevancy": 2.0477, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speech%20Quality%20Assessment%20Model%20Based%20on%20Mixture%20of%20Experts%3A%0A%20%20System-Level%20Performance%20Enhancement%20and%20Utterance-Level%20Challenge%20Analysis&body=Title%3A%20Speech%20Quality%20Assessment%20Model%20Based%20on%20Mixture%20of%20Experts%3A%0A%20%20System-Level%20Performance%20Enhancement%20and%20Utterance-Level%20Challenge%20Analysis%0AAuthor%3A%20Xintong%20Hu%20and%20Yixuan%20Chen%20and%20Rui%20Yang%20and%20Wenxiang%20Guo%20and%20Changhao%20Pan%0AAbstract%3A%20%20%20Automatic%20speech%20quality%20assessment%20plays%20a%20crucial%20role%20in%20the%20development%0Aof%20speech%20synthesis%20systems%2C%20but%20existing%20models%20exhibit%20significant%0Aperformance%20variations%20across%20different%20granularity%20levels%20of%20prediction%20tasks.%0AThis%20paper%20proposes%20an%20enhanced%20MOS%20prediction%20system%20based%20on%20self-supervised%0Alearning%20speech%20models%2C%20incorporating%20a%20Mixture%20of%20Experts%20%28MoE%29%20classification%0Ahead%20and%20utilizing%20synthetic%20data%20from%20multiple%20commercial%20generation%20models%0Afor%20data%20augmentation.%20Our%20method%20builds%20upon%20existing%20self-supervised%20models%0Asuch%20as%20wav2vec2%2C%20designing%20a%20specialized%20MoE%20architecture%20to%20address%20different%0Atypes%20of%20speech%20quality%20assessment%20tasks.%20We%20also%20collected%20a%20large-scale%0Asynthetic%20speech%20dataset%20encompassing%20the%20latest%20text-to-speech%2C%20speech%0Aconversion%2C%20and%20speech%20enhancement%20systems.%20However%2C%20despite%20the%20adoption%20of%0Athe%20MoE%20architecture%20and%20expanded%20dataset%2C%20the%20model%27s%20performance%20improvements%0Ain%20sentence-level%20prediction%20tasks%20remain%20limited.%20Our%20work%20reveals%20the%0Alimitations%20of%20current%20methods%20in%20handling%20sentence-level%20quality%20assessment%2C%0Aprovides%20new%20technical%20pathways%20for%20the%20field%20of%20automatic%20speech%20quality%0Aassessment%2C%20and%20also%20delves%20into%20the%20fundamental%20causes%20of%20performance%0Adifferences%20across%20different%20assessment%20granularities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeech%2520Quality%2520Assessment%2520Model%2520Based%2520on%2520Mixture%2520of%2520Experts%253A%250A%2520%2520System-Level%2520Performance%2520Enhancement%2520and%2520Utterance-Level%2520Challenge%2520Analysis%26entry.906535625%3DXintong%2520Hu%2520and%2520Yixuan%2520Chen%2520and%2520Rui%2520Yang%2520and%2520Wenxiang%2520Guo%2520and%2520Changhao%2520Pan%26entry.1292438233%3D%2520%2520Automatic%2520speech%2520quality%2520assessment%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520development%250Aof%2520speech%2520synthesis%2520systems%252C%2520but%2520existing%2520models%2520exhibit%2520significant%250Aperformance%2520variations%2520across%2520different%2520granularity%2520levels%2520of%2520prediction%2520tasks.%250AThis%2520paper%2520proposes%2520an%2520enhanced%2520MOS%2520prediction%2520system%2520based%2520on%2520self-supervised%250Alearning%2520speech%2520models%252C%2520incorporating%2520a%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520classification%250Ahead%2520and%2520utilizing%2520synthetic%2520data%2520from%2520multiple%2520commercial%2520generation%2520models%250Afor%2520data%2520augmentation.%2520Our%2520method%2520builds%2520upon%2520existing%2520self-supervised%2520models%250Asuch%2520as%2520wav2vec2%252C%2520designing%2520a%2520specialized%2520MoE%2520architecture%2520to%2520address%2520different%250Atypes%2520of%2520speech%2520quality%2520assessment%2520tasks.%2520We%2520also%2520collected%2520a%2520large-scale%250Asynthetic%2520speech%2520dataset%2520encompassing%2520the%2520latest%2520text-to-speech%252C%2520speech%250Aconversion%252C%2520and%2520speech%2520enhancement%2520systems.%2520However%252C%2520despite%2520the%2520adoption%2520of%250Athe%2520MoE%2520architecture%2520and%2520expanded%2520dataset%252C%2520the%2520model%2527s%2520performance%2520improvements%250Ain%2520sentence-level%2520prediction%2520tasks%2520remain%2520limited.%2520Our%2520work%2520reveals%2520the%250Alimitations%2520of%2520current%2520methods%2520in%2520handling%2520sentence-level%2520quality%2520assessment%252C%250Aprovides%2520new%2520technical%2520pathways%2520for%2520the%2520field%2520of%2520automatic%2520speech%2520quality%250Aassessment%252C%2520and%2520also%2520delves%2520into%2520the%2520fundamental%2520causes%2520of%2520performance%250Adifferences%2520across%2520different%2520assessment%2520granularities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speech%20Quality%20Assessment%20Model%20Based%20on%20Mixture%20of%20Experts%3A%0A%20%20System-Level%20Performance%20Enhancement%20and%20Utterance-Level%20Challenge%20Analysis&entry.906535625=Xintong%20Hu%20and%20Yixuan%20Chen%20and%20Rui%20Yang%20and%20Wenxiang%20Guo%20and%20Changhao%20Pan&entry.1292438233=%20%20Automatic%20speech%20quality%20assessment%20plays%20a%20crucial%20role%20in%20the%20development%0Aof%20speech%20synthesis%20systems%2C%20but%20existing%20models%20exhibit%20significant%0Aperformance%20variations%20across%20different%20granularity%20levels%20of%20prediction%20tasks.%0AThis%20paper%20proposes%20an%20enhanced%20MOS%20prediction%20system%20based%20on%20self-supervised%0Alearning%20speech%20models%2C%20incorporating%20a%20Mixture%20of%20Experts%20%28MoE%29%20classification%0Ahead%20and%20utilizing%20synthetic%20data%20from%20multiple%20commercial%20generation%20models%0Afor%20data%20augmentation.%20Our%20method%20builds%20upon%20existing%20self-supervised%20models%0Asuch%20as%20wav2vec2%2C%20designing%20a%20specialized%20MoE%20architecture%20to%20address%20different%0Atypes%20of%20speech%20quality%20assessment%20tasks.%20We%20also%20collected%20a%20large-scale%0Asynthetic%20speech%20dataset%20encompassing%20the%20latest%20text-to-speech%2C%20speech%0Aconversion%2C%20and%20speech%20enhancement%20systems.%20However%2C%20despite%20the%20adoption%20of%0Athe%20MoE%20architecture%20and%20expanded%20dataset%2C%20the%20model%27s%20performance%20improvements%0Ain%20sentence-level%20prediction%20tasks%20remain%20limited.%20Our%20work%20reveals%20the%0Alimitations%20of%20current%20methods%20in%20handling%20sentence-level%20quality%20assessment%2C%0Aprovides%20new%20technical%20pathways%20for%20the%20field%20of%20automatic%20speech%20quality%0Aassessment%2C%20and%20also%20delves%20into%20the%20fundamental%20causes%20of%20performance%0Adifferences%20across%20different%20assessment%20granularities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06116v1&entry.124074799=Read"},
{"title": "Overcoming Data Scarcity in Generative Language Modelling for\n  Low-Resource Languages: A Systematic Review", "author": "Josh McGiff and Nikola S. Nikolov", "abstract": "  Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies.\n", "link": "http://arxiv.org/abs/2505.04531v2", "date": "2025-07-08", "relevancy": 2.0382, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5221}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5087}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20Data%20Scarcity%20in%20Generative%20Language%20Modelling%20for%0A%20%20Low-Resource%20Languages%3A%20A%20Systematic%20Review&body=Title%3A%20Overcoming%20Data%20Scarcity%20in%20Generative%20Language%20Modelling%20for%0A%20%20Low-Resource%20Languages%3A%20A%20Systematic%20Review%0AAuthor%3A%20Josh%20McGiff%20and%20Nikola%20S.%20Nikolov%0AAbstract%3A%20%20%20Generative%20language%20modelling%20has%20surged%20in%20popularity%20with%20the%20emergence%20of%0Aservices%20such%20as%20ChatGPT%20and%20Google%20Gemini.%20While%20these%20models%20have%0Ademonstrated%20transformative%20potential%20in%20productivity%20and%20communication%2C%20they%0Aoverwhelmingly%20cater%20to%20high-resource%20languages%20like%20English.%20This%20has%0Aamplified%20concerns%20over%20linguistic%20inequality%20in%20natural%20language%20processing%0A%28NLP%29.%20This%20paper%20presents%20the%20first%20systematic%20review%20focused%20specifically%20on%0Astrategies%20to%20address%20data%20scarcity%20in%20generative%20language%20modelling%20for%0Alow-resource%20languages%20%28LRL%29.%20Drawing%20from%2054%20studies%2C%20we%20identify%2C%20categorise%0Aand%20evaluate%20technical%20approaches%2C%20including%20monolingual%20data%20augmentation%2C%0Aback-translation%2C%20multilingual%20training%2C%20and%20prompt%20engineering%2C%20across%0Agenerative%20tasks.%20We%20also%20analyse%20trends%20in%20architecture%20choices%2C%20language%0Afamily%20representation%2C%20and%20evaluation%20methods.%20Our%20findings%20highlight%20a%20strong%0Areliance%20on%20transformer-based%20models%2C%20a%20concentration%20on%20a%20small%20subset%20of%0ALRLs%2C%20and%20a%20lack%20of%20consistent%20evaluation%20across%20studies.%20We%20conclude%20with%0Arecommendations%20for%20extending%20these%20methods%20to%20a%20wider%20range%20of%20LRLs%20and%0Aoutline%20open%20challenges%20in%20building%20equitable%20generative%20language%20systems.%0AUltimately%2C%20this%20review%20aims%20to%20support%20researchers%20and%20developers%20in%20building%0Ainclusive%20AI%20tools%20for%20underrepresented%20languages%2C%20a%20necessary%20step%20toward%0Aempowering%20LRL%20speakers%20and%20the%20preservation%20of%20linguistic%20diversity%20in%20a%20world%0Aincreasingly%20shaped%20by%20large-scale%20language%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04531v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520Data%2520Scarcity%2520in%2520Generative%2520Language%2520Modelling%2520for%250A%2520%2520Low-Resource%2520Languages%253A%2520A%2520Systematic%2520Review%26entry.906535625%3DJosh%2520McGiff%2520and%2520Nikola%2520S.%2520Nikolov%26entry.1292438233%3D%2520%2520Generative%2520language%2520modelling%2520has%2520surged%2520in%2520popularity%2520with%2520the%2520emergence%2520of%250Aservices%2520such%2520as%2520ChatGPT%2520and%2520Google%2520Gemini.%2520While%2520these%2520models%2520have%250Ademonstrated%2520transformative%2520potential%2520in%2520productivity%2520and%2520communication%252C%2520they%250Aoverwhelmingly%2520cater%2520to%2520high-resource%2520languages%2520like%2520English.%2520This%2520has%250Aamplified%2520concerns%2520over%2520linguistic%2520inequality%2520in%2520natural%2520language%2520processing%250A%2528NLP%2529.%2520This%2520paper%2520presents%2520the%2520first%2520systematic%2520review%2520focused%2520specifically%2520on%250Astrategies%2520to%2520address%2520data%2520scarcity%2520in%2520generative%2520language%2520modelling%2520for%250Alow-resource%2520languages%2520%2528LRL%2529.%2520Drawing%2520from%252054%2520studies%252C%2520we%2520identify%252C%2520categorise%250Aand%2520evaluate%2520technical%2520approaches%252C%2520including%2520monolingual%2520data%2520augmentation%252C%250Aback-translation%252C%2520multilingual%2520training%252C%2520and%2520prompt%2520engineering%252C%2520across%250Agenerative%2520tasks.%2520We%2520also%2520analyse%2520trends%2520in%2520architecture%2520choices%252C%2520language%250Afamily%2520representation%252C%2520and%2520evaluation%2520methods.%2520Our%2520findings%2520highlight%2520a%2520strong%250Areliance%2520on%2520transformer-based%2520models%252C%2520a%2520concentration%2520on%2520a%2520small%2520subset%2520of%250ALRLs%252C%2520and%2520a%2520lack%2520of%2520consistent%2520evaluation%2520across%2520studies.%2520We%2520conclude%2520with%250Arecommendations%2520for%2520extending%2520these%2520methods%2520to%2520a%2520wider%2520range%2520of%2520LRLs%2520and%250Aoutline%2520open%2520challenges%2520in%2520building%2520equitable%2520generative%2520language%2520systems.%250AUltimately%252C%2520this%2520review%2520aims%2520to%2520support%2520researchers%2520and%2520developers%2520in%2520building%250Ainclusive%2520AI%2520tools%2520for%2520underrepresented%2520languages%252C%2520a%2520necessary%2520step%2520toward%250Aempowering%2520LRL%2520speakers%2520and%2520the%2520preservation%2520of%2520linguistic%2520diversity%2520in%2520a%2520world%250Aincreasingly%2520shaped%2520by%2520large-scale%2520language%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04531v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Data%20Scarcity%20in%20Generative%20Language%20Modelling%20for%0A%20%20Low-Resource%20Languages%3A%20A%20Systematic%20Review&entry.906535625=Josh%20McGiff%20and%20Nikola%20S.%20Nikolov&entry.1292438233=%20%20Generative%20language%20modelling%20has%20surged%20in%20popularity%20with%20the%20emergence%20of%0Aservices%20such%20as%20ChatGPT%20and%20Google%20Gemini.%20While%20these%20models%20have%0Ademonstrated%20transformative%20potential%20in%20productivity%20and%20communication%2C%20they%0Aoverwhelmingly%20cater%20to%20high-resource%20languages%20like%20English.%20This%20has%0Aamplified%20concerns%20over%20linguistic%20inequality%20in%20natural%20language%20processing%0A%28NLP%29.%20This%20paper%20presents%20the%20first%20systematic%20review%20focused%20specifically%20on%0Astrategies%20to%20address%20data%20scarcity%20in%20generative%20language%20modelling%20for%0Alow-resource%20languages%20%28LRL%29.%20Drawing%20from%2054%20studies%2C%20we%20identify%2C%20categorise%0Aand%20evaluate%20technical%20approaches%2C%20including%20monolingual%20data%20augmentation%2C%0Aback-translation%2C%20multilingual%20training%2C%20and%20prompt%20engineering%2C%20across%0Agenerative%20tasks.%20We%20also%20analyse%20trends%20in%20architecture%20choices%2C%20language%0Afamily%20representation%2C%20and%20evaluation%20methods.%20Our%20findings%20highlight%20a%20strong%0Areliance%20on%20transformer-based%20models%2C%20a%20concentration%20on%20a%20small%20subset%20of%0ALRLs%2C%20and%20a%20lack%20of%20consistent%20evaluation%20across%20studies.%20We%20conclude%20with%0Arecommendations%20for%20extending%20these%20methods%20to%20a%20wider%20range%20of%20LRLs%20and%0Aoutline%20open%20challenges%20in%20building%20equitable%20generative%20language%20systems.%0AUltimately%2C%20this%20review%20aims%20to%20support%20researchers%20and%20developers%20in%20building%0Ainclusive%20AI%20tools%20for%20underrepresented%20languages%2C%20a%20necessary%20step%20toward%0Aempowering%20LRL%20speakers%20and%20the%20preservation%20of%20linguistic%20diversity%20in%20a%20world%0Aincreasingly%20shaped%20by%20large-scale%20language%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04531v2&entry.124074799=Read"},
{"title": "MARS: Radio Map Super-resolution and Reconstruction Method under Sparse\n  Channel Measurements", "author": "Chuyun Deng and Na Liu and Wei Xie and Lianming Xu and Li Wang", "abstract": "  Radio maps reflect the spatial distribution of signal strength and are\nessential for applications like smart cities, IoT, and wireless network\nplanning. However, reconstructing accurate radio maps from sparse measurements\nremains challenging. Traditional interpolation and inpainting methods lack\nenvironmental awareness, while many deep learning approaches depend on detailed\nscene data, limiting generalization. To address this, we propose MARS, a\nMulti-scale Aware Radiomap Super-resolution method that combines CNNs and\nTransformers with multi-scale feature fusion and residual connections. MARS\nfocuses on both global and local feature extraction, enhancing feature\nrepresentation across different receptive fields and improving reconstruction\naccuracy. Experiments across different scenes and antenna locations show that\nMARS outperforms baseline models in both MSE and SSIM, while maintaining low\ncomputational cost, demonstrating strong practical potential.\n", "link": "http://arxiv.org/abs/2506.04682v3", "date": "2025-07-08", "relevancy": 2.0191, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5211}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4992}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARS%3A%20Radio%20Map%20Super-resolution%20and%20Reconstruction%20Method%20under%20Sparse%0A%20%20Channel%20Measurements&body=Title%3A%20MARS%3A%20Radio%20Map%20Super-resolution%20and%20Reconstruction%20Method%20under%20Sparse%0A%20%20Channel%20Measurements%0AAuthor%3A%20Chuyun%20Deng%20and%20Na%20Liu%20and%20Wei%20Xie%20and%20Lianming%20Xu%20and%20Li%20Wang%0AAbstract%3A%20%20%20Radio%20maps%20reflect%20the%20spatial%20distribution%20of%20signal%20strength%20and%20are%0Aessential%20for%20applications%20like%20smart%20cities%2C%20IoT%2C%20and%20wireless%20network%0Aplanning.%20However%2C%20reconstructing%20accurate%20radio%20maps%20from%20sparse%20measurements%0Aremains%20challenging.%20Traditional%20interpolation%20and%20inpainting%20methods%20lack%0Aenvironmental%20awareness%2C%20while%20many%20deep%20learning%20approaches%20depend%20on%20detailed%0Ascene%20data%2C%20limiting%20generalization.%20To%20address%20this%2C%20we%20propose%20MARS%2C%20a%0AMulti-scale%20Aware%20Radiomap%20Super-resolution%20method%20that%20combines%20CNNs%20and%0ATransformers%20with%20multi-scale%20feature%20fusion%20and%20residual%20connections.%20MARS%0Afocuses%20on%20both%20global%20and%20local%20feature%20extraction%2C%20enhancing%20feature%0Arepresentation%20across%20different%20receptive%20fields%20and%20improving%20reconstruction%0Aaccuracy.%20Experiments%20across%20different%20scenes%20and%20antenna%20locations%20show%20that%0AMARS%20outperforms%20baseline%20models%20in%20both%20MSE%20and%20SSIM%2C%20while%20maintaining%20low%0Acomputational%20cost%2C%20demonstrating%20strong%20practical%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04682v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARS%253A%2520Radio%2520Map%2520Super-resolution%2520and%2520Reconstruction%2520Method%2520under%2520Sparse%250A%2520%2520Channel%2520Measurements%26entry.906535625%3DChuyun%2520Deng%2520and%2520Na%2520Liu%2520and%2520Wei%2520Xie%2520and%2520Lianming%2520Xu%2520and%2520Li%2520Wang%26entry.1292438233%3D%2520%2520Radio%2520maps%2520reflect%2520the%2520spatial%2520distribution%2520of%2520signal%2520strength%2520and%2520are%250Aessential%2520for%2520applications%2520like%2520smart%2520cities%252C%2520IoT%252C%2520and%2520wireless%2520network%250Aplanning.%2520However%252C%2520reconstructing%2520accurate%2520radio%2520maps%2520from%2520sparse%2520measurements%250Aremains%2520challenging.%2520Traditional%2520interpolation%2520and%2520inpainting%2520methods%2520lack%250Aenvironmental%2520awareness%252C%2520while%2520many%2520deep%2520learning%2520approaches%2520depend%2520on%2520detailed%250Ascene%2520data%252C%2520limiting%2520generalization.%2520To%2520address%2520this%252C%2520we%2520propose%2520MARS%252C%2520a%250AMulti-scale%2520Aware%2520Radiomap%2520Super-resolution%2520method%2520that%2520combines%2520CNNs%2520and%250ATransformers%2520with%2520multi-scale%2520feature%2520fusion%2520and%2520residual%2520connections.%2520MARS%250Afocuses%2520on%2520both%2520global%2520and%2520local%2520feature%2520extraction%252C%2520enhancing%2520feature%250Arepresentation%2520across%2520different%2520receptive%2520fields%2520and%2520improving%2520reconstruction%250Aaccuracy.%2520Experiments%2520across%2520different%2520scenes%2520and%2520antenna%2520locations%2520show%2520that%250AMARS%2520outperforms%2520baseline%2520models%2520in%2520both%2520MSE%2520and%2520SSIM%252C%2520while%2520maintaining%2520low%250Acomputational%2520cost%252C%2520demonstrating%2520strong%2520practical%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04682v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARS%3A%20Radio%20Map%20Super-resolution%20and%20Reconstruction%20Method%20under%20Sparse%0A%20%20Channel%20Measurements&entry.906535625=Chuyun%20Deng%20and%20Na%20Liu%20and%20Wei%20Xie%20and%20Lianming%20Xu%20and%20Li%20Wang&entry.1292438233=%20%20Radio%20maps%20reflect%20the%20spatial%20distribution%20of%20signal%20strength%20and%20are%0Aessential%20for%20applications%20like%20smart%20cities%2C%20IoT%2C%20and%20wireless%20network%0Aplanning.%20However%2C%20reconstructing%20accurate%20radio%20maps%20from%20sparse%20measurements%0Aremains%20challenging.%20Traditional%20interpolation%20and%20inpainting%20methods%20lack%0Aenvironmental%20awareness%2C%20while%20many%20deep%20learning%20approaches%20depend%20on%20detailed%0Ascene%20data%2C%20limiting%20generalization.%20To%20address%20this%2C%20we%20propose%20MARS%2C%20a%0AMulti-scale%20Aware%20Radiomap%20Super-resolution%20method%20that%20combines%20CNNs%20and%0ATransformers%20with%20multi-scale%20feature%20fusion%20and%20residual%20connections.%20MARS%0Afocuses%20on%20both%20global%20and%20local%20feature%20extraction%2C%20enhancing%20feature%0Arepresentation%20across%20different%20receptive%20fields%20and%20improving%20reconstruction%0Aaccuracy.%20Experiments%20across%20different%20scenes%20and%20antenna%20locations%20show%20that%0AMARS%20outperforms%20baseline%20models%20in%20both%20MSE%20and%20SSIM%2C%20while%20maintaining%20low%0Acomputational%20cost%2C%20demonstrating%20strong%20practical%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04682v3&entry.124074799=Read"},
{"title": "UQLM: A Python Package for Uncertainty Quantification in Large Language\n  Models", "author": "Dylan Bouchard and Mohit Singh Chauhan and David Skarbrevik and Ho-Kyeong Ra and Viren Bajaj and Zeya Ahmad", "abstract": "  Hallucinations, defined as instances where Large Language Models (LLMs)\ngenerate false or misleading content, pose a significant challenge that impacts\nthe safety and trust of downstream applications. We introduce UQLM, a Python\npackage for LLM hallucination detection using state-of-the-art uncertainty\nquantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers\nthat compute response-level confidence scores ranging from 0 to 1. This library\nprovides an off-the-shelf solution for UQ-based hallucination detection that\ncan be easily integrated to enhance the reliability of LLM outputs.\n", "link": "http://arxiv.org/abs/2507.06196v1", "date": "2025-07-08", "relevancy": 2.0189, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5594}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5354}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UQLM%3A%20A%20Python%20Package%20for%20Uncertainty%20Quantification%20in%20Large%20Language%0A%20%20Models&body=Title%3A%20UQLM%3A%20A%20Python%20Package%20for%20Uncertainty%20Quantification%20in%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Dylan%20Bouchard%20and%20Mohit%20Singh%20Chauhan%20and%20David%20Skarbrevik%20and%20Ho-Kyeong%20Ra%20and%20Viren%20Bajaj%20and%20Zeya%20Ahmad%0AAbstract%3A%20%20%20Hallucinations%2C%20defined%20as%20instances%20where%20Large%20Language%20Models%20%28LLMs%29%0Agenerate%20false%20or%20misleading%20content%2C%20pose%20a%20significant%20challenge%20that%20impacts%0Athe%20safety%20and%20trust%20of%20downstream%20applications.%20We%20introduce%20UQLM%2C%20a%20Python%0Apackage%20for%20LLM%20hallucination%20detection%20using%20state-of-the-art%20uncertainty%0Aquantification%20%28UQ%29%20techniques.%20This%20toolkit%20offers%20a%20suite%20of%20UQ-based%20scorers%0Athat%20compute%20response-level%20confidence%20scores%20ranging%20from%200%20to%201.%20This%20library%0Aprovides%20an%20off-the-shelf%20solution%20for%20UQ-based%20hallucination%20detection%20that%0Acan%20be%20easily%20integrated%20to%20enhance%20the%20reliability%20of%20LLM%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUQLM%253A%2520A%2520Python%2520Package%2520for%2520Uncertainty%2520Quantification%2520in%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DDylan%2520Bouchard%2520and%2520Mohit%2520Singh%2520Chauhan%2520and%2520David%2520Skarbrevik%2520and%2520Ho-Kyeong%2520Ra%2520and%2520Viren%2520Bajaj%2520and%2520Zeya%2520Ahmad%26entry.1292438233%3D%2520%2520Hallucinations%252C%2520defined%2520as%2520instances%2520where%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Agenerate%2520false%2520or%2520misleading%2520content%252C%2520pose%2520a%2520significant%2520challenge%2520that%2520impacts%250Athe%2520safety%2520and%2520trust%2520of%2520downstream%2520applications.%2520We%2520introduce%2520UQLM%252C%2520a%2520Python%250Apackage%2520for%2520LLM%2520hallucination%2520detection%2520using%2520state-of-the-art%2520uncertainty%250Aquantification%2520%2528UQ%2529%2520techniques.%2520This%2520toolkit%2520offers%2520a%2520suite%2520of%2520UQ-based%2520scorers%250Athat%2520compute%2520response-level%2520confidence%2520scores%2520ranging%2520from%25200%2520to%25201.%2520This%2520library%250Aprovides%2520an%2520off-the-shelf%2520solution%2520for%2520UQ-based%2520hallucination%2520detection%2520that%250Acan%2520be%2520easily%2520integrated%2520to%2520enhance%2520the%2520reliability%2520of%2520LLM%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UQLM%3A%20A%20Python%20Package%20for%20Uncertainty%20Quantification%20in%20Large%20Language%0A%20%20Models&entry.906535625=Dylan%20Bouchard%20and%20Mohit%20Singh%20Chauhan%20and%20David%20Skarbrevik%20and%20Ho-Kyeong%20Ra%20and%20Viren%20Bajaj%20and%20Zeya%20Ahmad&entry.1292438233=%20%20Hallucinations%2C%20defined%20as%20instances%20where%20Large%20Language%20Models%20%28LLMs%29%0Agenerate%20false%20or%20misleading%20content%2C%20pose%20a%20significant%20challenge%20that%20impacts%0Athe%20safety%20and%20trust%20of%20downstream%20applications.%20We%20introduce%20UQLM%2C%20a%20Python%0Apackage%20for%20LLM%20hallucination%20detection%20using%20state-of-the-art%20uncertainty%0Aquantification%20%28UQ%29%20techniques.%20This%20toolkit%20offers%20a%20suite%20of%20UQ-based%20scorers%0Athat%20compute%20response-level%20confidence%20scores%20ranging%20from%200%20to%201.%20This%20library%0Aprovides%20an%20off-the-shelf%20solution%20for%20UQ-based%20hallucination%20detection%20that%0Acan%20be%20easily%20integrated%20to%20enhance%20the%20reliability%20of%20LLM%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06196v1&entry.124074799=Read"},
{"title": "The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield\n  Strong Gains", "author": "Scott Geng and Hamish Ivison and Chun-Liang Li and Maarten Sap and Jerry Li and Ranjay Krishna and Pang Wei Koh", "abstract": "  Improvements in language models are often driven by improving the quality of\nthe data we train them on, which can be limiting when strong supervision is\nscarce. In this work, we show that paired preference data consisting of\nindividually weak data points can enable gains beyond the strength of each\nindividual data point. We formulate the delta learning hypothesis to explain\nthis phenomenon, positing that the relative quality delta between points\nsuffices to drive learning via preference tuning--even when supervised\nfinetuning on the weak data hurts. We validate our hypothesis in controlled\nexperiments and at scale, where we post-train 8B models on preference data\ngenerated by pairing a small 3B model's responses with outputs from an even\nsmaller 1.5B model to create a meaningful delta. Strikingly, on a standard\n11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the\nperformance of Tulu 3, a state-of-the-art open model tuned from the same base\nmodel while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta\nlearning enables simpler and cheaper open recipes for state-of-the-art\npost-training. To better understand delta learning, we prove in logistic\nregression that the performance gap between two weak teacher models provides\nuseful signal for improving a stronger student. Overall, our work shows that\nmodels can learn surprisingly well from paired data that might typically be\nconsidered weak.\n", "link": "http://arxiv.org/abs/2507.06187v1", "date": "2025-07-08", "relevancy": 2.012, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5133}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5013}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Delta%20Learning%20Hypothesis%3A%20Preference%20Tuning%20on%20Weak%20Data%20can%20Yield%0A%20%20Strong%20Gains&body=Title%3A%20The%20Delta%20Learning%20Hypothesis%3A%20Preference%20Tuning%20on%20Weak%20Data%20can%20Yield%0A%20%20Strong%20Gains%0AAuthor%3A%20Scott%20Geng%20and%20Hamish%20Ivison%20and%20Chun-Liang%20Li%20and%20Maarten%20Sap%20and%20Jerry%20Li%20and%20Ranjay%20Krishna%20and%20Pang%20Wei%20Koh%0AAbstract%3A%20%20%20Improvements%20in%20language%20models%20are%20often%20driven%20by%20improving%20the%20quality%20of%0Athe%20data%20we%20train%20them%20on%2C%20which%20can%20be%20limiting%20when%20strong%20supervision%20is%0Ascarce.%20In%20this%20work%2C%20we%20show%20that%20paired%20preference%20data%20consisting%20of%0Aindividually%20weak%20data%20points%20can%20enable%20gains%20beyond%20the%20strength%20of%20each%0Aindividual%20data%20point.%20We%20formulate%20the%20delta%20learning%20hypothesis%20to%20explain%0Athis%20phenomenon%2C%20positing%20that%20the%20relative%20quality%20delta%20between%20points%0Asuffices%20to%20drive%20learning%20via%20preference%20tuning--even%20when%20supervised%0Afinetuning%20on%20the%20weak%20data%20hurts.%20We%20validate%20our%20hypothesis%20in%20controlled%0Aexperiments%20and%20at%20scale%2C%20where%20we%20post-train%208B%20models%20on%20preference%20data%0Agenerated%20by%20pairing%20a%20small%203B%20model%27s%20responses%20with%20outputs%20from%20an%20even%0Asmaller%201.5B%20model%20to%20create%20a%20meaningful%20delta.%20Strikingly%2C%20on%20a%20standard%0A11-benchmark%20evaluation%20suite%20%28MATH%2C%20MMLU%2C%20etc.%29%2C%20our%20simple%20recipe%20matches%20the%0Aperformance%20of%20Tulu%203%2C%20a%20state-of-the-art%20open%20model%20tuned%20from%20the%20same%20base%0Amodel%20while%20relying%20on%20much%20stronger%20supervisors%20%28e.g.%2C%20GPT-4o%29.%20Thus%2C%20delta%0Alearning%20enables%20simpler%20and%20cheaper%20open%20recipes%20for%20state-of-the-art%0Apost-training.%20To%20better%20understand%20delta%20learning%2C%20we%20prove%20in%20logistic%0Aregression%20that%20the%20performance%20gap%20between%20two%20weak%20teacher%20models%20provides%0Auseful%20signal%20for%20improving%20a%20stronger%20student.%20Overall%2C%20our%20work%20shows%20that%0Amodels%20can%20learn%20surprisingly%20well%20from%20paired%20data%20that%20might%20typically%20be%0Aconsidered%20weak.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Delta%2520Learning%2520Hypothesis%253A%2520Preference%2520Tuning%2520on%2520Weak%2520Data%2520can%2520Yield%250A%2520%2520Strong%2520Gains%26entry.906535625%3DScott%2520Geng%2520and%2520Hamish%2520Ivison%2520and%2520Chun-Liang%2520Li%2520and%2520Maarten%2520Sap%2520and%2520Jerry%2520Li%2520and%2520Ranjay%2520Krishna%2520and%2520Pang%2520Wei%2520Koh%26entry.1292438233%3D%2520%2520Improvements%2520in%2520language%2520models%2520are%2520often%2520driven%2520by%2520improving%2520the%2520quality%2520of%250Athe%2520data%2520we%2520train%2520them%2520on%252C%2520which%2520can%2520be%2520limiting%2520when%2520strong%2520supervision%2520is%250Ascarce.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520paired%2520preference%2520data%2520consisting%2520of%250Aindividually%2520weak%2520data%2520points%2520can%2520enable%2520gains%2520beyond%2520the%2520strength%2520of%2520each%250Aindividual%2520data%2520point.%2520We%2520formulate%2520the%2520delta%2520learning%2520hypothesis%2520to%2520explain%250Athis%2520phenomenon%252C%2520positing%2520that%2520the%2520relative%2520quality%2520delta%2520between%2520points%250Asuffices%2520to%2520drive%2520learning%2520via%2520preference%2520tuning--even%2520when%2520supervised%250Afinetuning%2520on%2520the%2520weak%2520data%2520hurts.%2520We%2520validate%2520our%2520hypothesis%2520in%2520controlled%250Aexperiments%2520and%2520at%2520scale%252C%2520where%2520we%2520post-train%25208B%2520models%2520on%2520preference%2520data%250Agenerated%2520by%2520pairing%2520a%2520small%25203B%2520model%2527s%2520responses%2520with%2520outputs%2520from%2520an%2520even%250Asmaller%25201.5B%2520model%2520to%2520create%2520a%2520meaningful%2520delta.%2520Strikingly%252C%2520on%2520a%2520standard%250A11-benchmark%2520evaluation%2520suite%2520%2528MATH%252C%2520MMLU%252C%2520etc.%2529%252C%2520our%2520simple%2520recipe%2520matches%2520the%250Aperformance%2520of%2520Tulu%25203%252C%2520a%2520state-of-the-art%2520open%2520model%2520tuned%2520from%2520the%2520same%2520base%250Amodel%2520while%2520relying%2520on%2520much%2520stronger%2520supervisors%2520%2528e.g.%252C%2520GPT-4o%2529.%2520Thus%252C%2520delta%250Alearning%2520enables%2520simpler%2520and%2520cheaper%2520open%2520recipes%2520for%2520state-of-the-art%250Apost-training.%2520To%2520better%2520understand%2520delta%2520learning%252C%2520we%2520prove%2520in%2520logistic%250Aregression%2520that%2520the%2520performance%2520gap%2520between%2520two%2520weak%2520teacher%2520models%2520provides%250Auseful%2520signal%2520for%2520improving%2520a%2520stronger%2520student.%2520Overall%252C%2520our%2520work%2520shows%2520that%250Amodels%2520can%2520learn%2520surprisingly%2520well%2520from%2520paired%2520data%2520that%2520might%2520typically%2520be%250Aconsidered%2520weak.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Delta%20Learning%20Hypothesis%3A%20Preference%20Tuning%20on%20Weak%20Data%20can%20Yield%0A%20%20Strong%20Gains&entry.906535625=Scott%20Geng%20and%20Hamish%20Ivison%20and%20Chun-Liang%20Li%20and%20Maarten%20Sap%20and%20Jerry%20Li%20and%20Ranjay%20Krishna%20and%20Pang%20Wei%20Koh&entry.1292438233=%20%20Improvements%20in%20language%20models%20are%20often%20driven%20by%20improving%20the%20quality%20of%0Athe%20data%20we%20train%20them%20on%2C%20which%20can%20be%20limiting%20when%20strong%20supervision%20is%0Ascarce.%20In%20this%20work%2C%20we%20show%20that%20paired%20preference%20data%20consisting%20of%0Aindividually%20weak%20data%20points%20can%20enable%20gains%20beyond%20the%20strength%20of%20each%0Aindividual%20data%20point.%20We%20formulate%20the%20delta%20learning%20hypothesis%20to%20explain%0Athis%20phenomenon%2C%20positing%20that%20the%20relative%20quality%20delta%20between%20points%0Asuffices%20to%20drive%20learning%20via%20preference%20tuning--even%20when%20supervised%0Afinetuning%20on%20the%20weak%20data%20hurts.%20We%20validate%20our%20hypothesis%20in%20controlled%0Aexperiments%20and%20at%20scale%2C%20where%20we%20post-train%208B%20models%20on%20preference%20data%0Agenerated%20by%20pairing%20a%20small%203B%20model%27s%20responses%20with%20outputs%20from%20an%20even%0Asmaller%201.5B%20model%20to%20create%20a%20meaningful%20delta.%20Strikingly%2C%20on%20a%20standard%0A11-benchmark%20evaluation%20suite%20%28MATH%2C%20MMLU%2C%20etc.%29%2C%20our%20simple%20recipe%20matches%20the%0Aperformance%20of%20Tulu%203%2C%20a%20state-of-the-art%20open%20model%20tuned%20from%20the%20same%20base%0Amodel%20while%20relying%20on%20much%20stronger%20supervisors%20%28e.g.%2C%20GPT-4o%29.%20Thus%2C%20delta%0Alearning%20enables%20simpler%20and%20cheaper%20open%20recipes%20for%20state-of-the-art%0Apost-training.%20To%20better%20understand%20delta%20learning%2C%20we%20prove%20in%20logistic%0Aregression%20that%20the%20performance%20gap%20between%20two%20weak%20teacher%20models%20provides%0Auseful%20signal%20for%20improving%20a%20stronger%20student.%20Overall%2C%20our%20work%20shows%20that%0Amodels%20can%20learn%20surprisingly%20well%20from%20paired%20data%20that%20might%20typically%20be%0Aconsidered%20weak.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06187v1&entry.124074799=Read"},
{"title": "Entropy stable conservative flux form neural networks", "author": "Lizuo Liu and Tongtong Li and Anne Gelb and Yoonsang Lee", "abstract": "  We propose an entropy-stable conservative flux form neural network (CFN) that\nintegrates classical numerical conservation laws into a data-driven framework\nusing the entropy-stable, second-order, and non-oscillatory Kurganov-Tadmor\n(KT) scheme. The proposed entropy-stable CFN uses slope limiting as a denoising\nmechanism, ensuring accurate predictions in both noisy and sparse observation\nenvironments, as well as in both smooth and discontinuous regions. Numerical\nexperiments demonstrate that the entropy-stable CFN achieves both stability and\nconservation while maintaining accuracy over extended time domains.\nFurthermore, it successfully predicts shock propagation speeds in long-term\nsimulations, {\\it without} oracle knowledge of later-time profiles in the\ntraining data.\n", "link": "http://arxiv.org/abs/2411.01746v2", "date": "2025-07-08", "relevancy": 1.987, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5142}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy%20stable%20conservative%20flux%20form%20neural%20networks&body=Title%3A%20Entropy%20stable%20conservative%20flux%20form%20neural%20networks%0AAuthor%3A%20Lizuo%20Liu%20and%20Tongtong%20Li%20and%20Anne%20Gelb%20and%20Yoonsang%20Lee%0AAbstract%3A%20%20%20We%20propose%20an%20entropy-stable%20conservative%20flux%20form%20neural%20network%20%28CFN%29%20that%0Aintegrates%20classical%20numerical%20conservation%20laws%20into%20a%20data-driven%20framework%0Ausing%20the%20entropy-stable%2C%20second-order%2C%20and%20non-oscillatory%20Kurganov-Tadmor%0A%28KT%29%20scheme.%20The%20proposed%20entropy-stable%20CFN%20uses%20slope%20limiting%20as%20a%20denoising%0Amechanism%2C%20ensuring%20accurate%20predictions%20in%20both%20noisy%20and%20sparse%20observation%0Aenvironments%2C%20as%20well%20as%20in%20both%20smooth%20and%20discontinuous%20regions.%20Numerical%0Aexperiments%20demonstrate%20that%20the%20entropy-stable%20CFN%20achieves%20both%20stability%20and%0Aconservation%20while%20maintaining%20accuracy%20over%20extended%20time%20domains.%0AFurthermore%2C%20it%20successfully%20predicts%20shock%20propagation%20speeds%20in%20long-term%0Asimulations%2C%20%7B%5Cit%20without%7D%20oracle%20knowledge%20of%20later-time%20profiles%20in%20the%0Atraining%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01746v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy%2520stable%2520conservative%2520flux%2520form%2520neural%2520networks%26entry.906535625%3DLizuo%2520Liu%2520and%2520Tongtong%2520Li%2520and%2520Anne%2520Gelb%2520and%2520Yoonsang%2520Lee%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520entropy-stable%2520conservative%2520flux%2520form%2520neural%2520network%2520%2528CFN%2529%2520that%250Aintegrates%2520classical%2520numerical%2520conservation%2520laws%2520into%2520a%2520data-driven%2520framework%250Ausing%2520the%2520entropy-stable%252C%2520second-order%252C%2520and%2520non-oscillatory%2520Kurganov-Tadmor%250A%2528KT%2529%2520scheme.%2520The%2520proposed%2520entropy-stable%2520CFN%2520uses%2520slope%2520limiting%2520as%2520a%2520denoising%250Amechanism%252C%2520ensuring%2520accurate%2520predictions%2520in%2520both%2520noisy%2520and%2520sparse%2520observation%250Aenvironments%252C%2520as%2520well%2520as%2520in%2520both%2520smooth%2520and%2520discontinuous%2520regions.%2520Numerical%250Aexperiments%2520demonstrate%2520that%2520the%2520entropy-stable%2520CFN%2520achieves%2520both%2520stability%2520and%250Aconservation%2520while%2520maintaining%2520accuracy%2520over%2520extended%2520time%2520domains.%250AFurthermore%252C%2520it%2520successfully%2520predicts%2520shock%2520propagation%2520speeds%2520in%2520long-term%250Asimulations%252C%2520%257B%255Cit%2520without%257D%2520oracle%2520knowledge%2520of%2520later-time%2520profiles%2520in%2520the%250Atraining%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01746v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy%20stable%20conservative%20flux%20form%20neural%20networks&entry.906535625=Lizuo%20Liu%20and%20Tongtong%20Li%20and%20Anne%20Gelb%20and%20Yoonsang%20Lee&entry.1292438233=%20%20We%20propose%20an%20entropy-stable%20conservative%20flux%20form%20neural%20network%20%28CFN%29%20that%0Aintegrates%20classical%20numerical%20conservation%20laws%20into%20a%20data-driven%20framework%0Ausing%20the%20entropy-stable%2C%20second-order%2C%20and%20non-oscillatory%20Kurganov-Tadmor%0A%28KT%29%20scheme.%20The%20proposed%20entropy-stable%20CFN%20uses%20slope%20limiting%20as%20a%20denoising%0Amechanism%2C%20ensuring%20accurate%20predictions%20in%20both%20noisy%20and%20sparse%20observation%0Aenvironments%2C%20as%20well%20as%20in%20both%20smooth%20and%20discontinuous%20regions.%20Numerical%0Aexperiments%20demonstrate%20that%20the%20entropy-stable%20CFN%20achieves%20both%20stability%20and%0Aconservation%20while%20maintaining%20accuracy%20over%20extended%20time%20domains.%0AFurthermore%2C%20it%20successfully%20predicts%20shock%20propagation%20speeds%20in%20long-term%0Asimulations%2C%20%7B%5Cit%20without%7D%20oracle%20knowledge%20of%20later-time%20profiles%20in%20the%0Atraining%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01746v2&entry.124074799=Read"},
{"title": "CURVE: CLIP-Utilized Reinforcement Learning for Visual Image Enhancement\n  via Simple Image Processing", "author": "Yuka Ogino and Takahiro Toizumi and Atsushi Ito", "abstract": "  Low-Light Image Enhancement (LLIE) is crucial for improving both human\nperception and computer vision tasks. This paper addresses two challenges in\nzero-reference LLIE: obtaining perceptually 'good' images using the Contrastive\nLanguage-Image Pre-Training (CLIP) model and maintaining computational\nefficiency for high-resolution images. We propose CLIP-Utilized Reinforcement\nlearning-based Visual image Enhancement (CURVE). CURVE employs a simple image\nprocessing module which adjusts global image tone based on B\\'ezier curve and\nestimates its processing parameters iteratively. The estimator is trained by\nreinforcement learning with rewards designed using CLIP text embeddings.\nExperiments on low-light and multi-exposure datasets demonstrate the\nperformance of CURVE in terms of enhancement quality and processing speed\ncompared to conventional methods.\n", "link": "http://arxiv.org/abs/2505.23102v2", "date": "2025-07-08", "relevancy": 1.9759, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5015}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CURVE%3A%20CLIP-Utilized%20Reinforcement%20Learning%20for%20Visual%20Image%20Enhancement%0A%20%20via%20Simple%20Image%20Processing&body=Title%3A%20CURVE%3A%20CLIP-Utilized%20Reinforcement%20Learning%20for%20Visual%20Image%20Enhancement%0A%20%20via%20Simple%20Image%20Processing%0AAuthor%3A%20Yuka%20Ogino%20and%20Takahiro%20Toizumi%20and%20Atsushi%20Ito%0AAbstract%3A%20%20%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20is%20crucial%20for%20improving%20both%20human%0Aperception%20and%20computer%20vision%20tasks.%20This%20paper%20addresses%20two%20challenges%20in%0Azero-reference%20LLIE%3A%20obtaining%20perceptually%20%27good%27%20images%20using%20the%20Contrastive%0ALanguage-Image%20Pre-Training%20%28CLIP%29%20model%20and%20maintaining%20computational%0Aefficiency%20for%20high-resolution%20images.%20We%20propose%20CLIP-Utilized%20Reinforcement%0Alearning-based%20Visual%20image%20Enhancement%20%28CURVE%29.%20CURVE%20employs%20a%20simple%20image%0Aprocessing%20module%20which%20adjusts%20global%20image%20tone%20based%20on%20B%5C%27ezier%20curve%20and%0Aestimates%20its%20processing%20parameters%20iteratively.%20The%20estimator%20is%20trained%20by%0Areinforcement%20learning%20with%20rewards%20designed%20using%20CLIP%20text%20embeddings.%0AExperiments%20on%20low-light%20and%20multi-exposure%20datasets%20demonstrate%20the%0Aperformance%20of%20CURVE%20in%20terms%20of%20enhancement%20quality%20and%20processing%20speed%0Acompared%20to%20conventional%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23102v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCURVE%253A%2520CLIP-Utilized%2520Reinforcement%2520Learning%2520for%2520Visual%2520Image%2520Enhancement%250A%2520%2520via%2520Simple%2520Image%2520Processing%26entry.906535625%3DYuka%2520Ogino%2520and%2520Takahiro%2520Toizumi%2520and%2520Atsushi%2520Ito%26entry.1292438233%3D%2520%2520Low-Light%2520Image%2520Enhancement%2520%2528LLIE%2529%2520is%2520crucial%2520for%2520improving%2520both%2520human%250Aperception%2520and%2520computer%2520vision%2520tasks.%2520This%2520paper%2520addresses%2520two%2520challenges%2520in%250Azero-reference%2520LLIE%253A%2520obtaining%2520perceptually%2520%2527good%2527%2520images%2520using%2520the%2520Contrastive%250ALanguage-Image%2520Pre-Training%2520%2528CLIP%2529%2520model%2520and%2520maintaining%2520computational%250Aefficiency%2520for%2520high-resolution%2520images.%2520We%2520propose%2520CLIP-Utilized%2520Reinforcement%250Alearning-based%2520Visual%2520image%2520Enhancement%2520%2528CURVE%2529.%2520CURVE%2520employs%2520a%2520simple%2520image%250Aprocessing%2520module%2520which%2520adjusts%2520global%2520image%2520tone%2520based%2520on%2520B%255C%2527ezier%2520curve%2520and%250Aestimates%2520its%2520processing%2520parameters%2520iteratively.%2520The%2520estimator%2520is%2520trained%2520by%250Areinforcement%2520learning%2520with%2520rewards%2520designed%2520using%2520CLIP%2520text%2520embeddings.%250AExperiments%2520on%2520low-light%2520and%2520multi-exposure%2520datasets%2520demonstrate%2520the%250Aperformance%2520of%2520CURVE%2520in%2520terms%2520of%2520enhancement%2520quality%2520and%2520processing%2520speed%250Acompared%2520to%2520conventional%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23102v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CURVE%3A%20CLIP-Utilized%20Reinforcement%20Learning%20for%20Visual%20Image%20Enhancement%0A%20%20via%20Simple%20Image%20Processing&entry.906535625=Yuka%20Ogino%20and%20Takahiro%20Toizumi%20and%20Atsushi%20Ito&entry.1292438233=%20%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20is%20crucial%20for%20improving%20both%20human%0Aperception%20and%20computer%20vision%20tasks.%20This%20paper%20addresses%20two%20challenges%20in%0Azero-reference%20LLIE%3A%20obtaining%20perceptually%20%27good%27%20images%20using%20the%20Contrastive%0ALanguage-Image%20Pre-Training%20%28CLIP%29%20model%20and%20maintaining%20computational%0Aefficiency%20for%20high-resolution%20images.%20We%20propose%20CLIP-Utilized%20Reinforcement%0Alearning-based%20Visual%20image%20Enhancement%20%28CURVE%29.%20CURVE%20employs%20a%20simple%20image%0Aprocessing%20module%20which%20adjusts%20global%20image%20tone%20based%20on%20B%5C%27ezier%20curve%20and%0Aestimates%20its%20processing%20parameters%20iteratively.%20The%20estimator%20is%20trained%20by%0Areinforcement%20learning%20with%20rewards%20designed%20using%20CLIP%20text%20embeddings.%0AExperiments%20on%20low-light%20and%20multi-exposure%20datasets%20demonstrate%20the%0Aperformance%20of%20CURVE%20in%20terms%20of%20enhancement%20quality%20and%20processing%20speed%0Acompared%20to%20conventional%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23102v2&entry.124074799=Read"},
{"title": "CoDy: Counterfactual Explainers for Dynamic Graphs", "author": "Zhan Qu and Daniel Gomm and Michael F\u00e4rber", "abstract": "  Temporal Graph Neural Networks (TGNNs) are widely used to model dynamic\nsystems where relationships and features evolve over time. Although TGNNs\ndemonstrate strong predictive capabilities in these domains, their complex\narchitectures pose significant challenges for explainability. Counterfactual\nexplanation methods provide a promising solution by illustrating how\nmodifications to input graphs can influence model predictions. To address this\nchallenge, we present CoDy, Counterfactual Explainer for Dynamic Graphs, a\nmodel-agnostic, instance-level explanation approach that identifies\ncounterfactual subgraphs to interpret TGNN predictions. CoDy employs a search\nalgorithm that combines Monte Carlo Tree Search with heuristic selection\npolicies, efficiently exploring a vast search space of potential explanatory\nsubgraphs by leveraging spatial, temporal, and local event impact information.\nExtensive experiments against state-of-the-art factual and counterfactual\nbaselines demonstrate CoDy's effectiveness, with improvements of 16% in AUFSC+\nover the strongest baseline.\n", "link": "http://arxiv.org/abs/2403.16846v2", "date": "2025-07-08", "relevancy": 1.9625, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5138}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4844}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoDy%3A%20Counterfactual%20Explainers%20for%20Dynamic%20Graphs&body=Title%3A%20CoDy%3A%20Counterfactual%20Explainers%20for%20Dynamic%20Graphs%0AAuthor%3A%20Zhan%20Qu%20and%20Daniel%20Gomm%20and%20Michael%20F%C3%A4rber%0AAbstract%3A%20%20%20Temporal%20Graph%20Neural%20Networks%20%28TGNNs%29%20are%20widely%20used%20to%20model%20dynamic%0Asystems%20where%20relationships%20and%20features%20evolve%20over%20time.%20Although%20TGNNs%0Ademonstrate%20strong%20predictive%20capabilities%20in%20these%20domains%2C%20their%20complex%0Aarchitectures%20pose%20significant%20challenges%20for%20explainability.%20Counterfactual%0Aexplanation%20methods%20provide%20a%20promising%20solution%20by%20illustrating%20how%0Amodifications%20to%20input%20graphs%20can%20influence%20model%20predictions.%20To%20address%20this%0Achallenge%2C%20we%20present%20CoDy%2C%20Counterfactual%20Explainer%20for%20Dynamic%20Graphs%2C%20a%0Amodel-agnostic%2C%20instance-level%20explanation%20approach%20that%20identifies%0Acounterfactual%20subgraphs%20to%20interpret%20TGNN%20predictions.%20CoDy%20employs%20a%20search%0Aalgorithm%20that%20combines%20Monte%20Carlo%20Tree%20Search%20with%20heuristic%20selection%0Apolicies%2C%20efficiently%20exploring%20a%20vast%20search%20space%20of%20potential%20explanatory%0Asubgraphs%20by%20leveraging%20spatial%2C%20temporal%2C%20and%20local%20event%20impact%20information.%0AExtensive%20experiments%20against%20state-of-the-art%20factual%20and%20counterfactual%0Abaselines%20demonstrate%20CoDy%27s%20effectiveness%2C%20with%20improvements%20of%2016%25%20in%20AUFSC%2B%0Aover%20the%20strongest%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoDy%253A%2520Counterfactual%2520Explainers%2520for%2520Dynamic%2520Graphs%26entry.906535625%3DZhan%2520Qu%2520and%2520Daniel%2520Gomm%2520and%2520Michael%2520F%25C3%25A4rber%26entry.1292438233%3D%2520%2520Temporal%2520Graph%2520Neural%2520Networks%2520%2528TGNNs%2529%2520are%2520widely%2520used%2520to%2520model%2520dynamic%250Asystems%2520where%2520relationships%2520and%2520features%2520evolve%2520over%2520time.%2520Although%2520TGNNs%250Ademonstrate%2520strong%2520predictive%2520capabilities%2520in%2520these%2520domains%252C%2520their%2520complex%250Aarchitectures%2520pose%2520significant%2520challenges%2520for%2520explainability.%2520Counterfactual%250Aexplanation%2520methods%2520provide%2520a%2520promising%2520solution%2520by%2520illustrating%2520how%250Amodifications%2520to%2520input%2520graphs%2520can%2520influence%2520model%2520predictions.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520present%2520CoDy%252C%2520Counterfactual%2520Explainer%2520for%2520Dynamic%2520Graphs%252C%2520a%250Amodel-agnostic%252C%2520instance-level%2520explanation%2520approach%2520that%2520identifies%250Acounterfactual%2520subgraphs%2520to%2520interpret%2520TGNN%2520predictions.%2520CoDy%2520employs%2520a%2520search%250Aalgorithm%2520that%2520combines%2520Monte%2520Carlo%2520Tree%2520Search%2520with%2520heuristic%2520selection%250Apolicies%252C%2520efficiently%2520exploring%2520a%2520vast%2520search%2520space%2520of%2520potential%2520explanatory%250Asubgraphs%2520by%2520leveraging%2520spatial%252C%2520temporal%252C%2520and%2520local%2520event%2520impact%2520information.%250AExtensive%2520experiments%2520against%2520state-of-the-art%2520factual%2520and%2520counterfactual%250Abaselines%2520demonstrate%2520CoDy%2527s%2520effectiveness%252C%2520with%2520improvements%2520of%252016%2525%2520in%2520AUFSC%252B%250Aover%2520the%2520strongest%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoDy%3A%20Counterfactual%20Explainers%20for%20Dynamic%20Graphs&entry.906535625=Zhan%20Qu%20and%20Daniel%20Gomm%20and%20Michael%20F%C3%A4rber&entry.1292438233=%20%20Temporal%20Graph%20Neural%20Networks%20%28TGNNs%29%20are%20widely%20used%20to%20model%20dynamic%0Asystems%20where%20relationships%20and%20features%20evolve%20over%20time.%20Although%20TGNNs%0Ademonstrate%20strong%20predictive%20capabilities%20in%20these%20domains%2C%20their%20complex%0Aarchitectures%20pose%20significant%20challenges%20for%20explainability.%20Counterfactual%0Aexplanation%20methods%20provide%20a%20promising%20solution%20by%20illustrating%20how%0Amodifications%20to%20input%20graphs%20can%20influence%20model%20predictions.%20To%20address%20this%0Achallenge%2C%20we%20present%20CoDy%2C%20Counterfactual%20Explainer%20for%20Dynamic%20Graphs%2C%20a%0Amodel-agnostic%2C%20instance-level%20explanation%20approach%20that%20identifies%0Acounterfactual%20subgraphs%20to%20interpret%20TGNN%20predictions.%20CoDy%20employs%20a%20search%0Aalgorithm%20that%20combines%20Monte%20Carlo%20Tree%20Search%20with%20heuristic%20selection%0Apolicies%2C%20efficiently%20exploring%20a%20vast%20search%20space%20of%20potential%20explanatory%0Asubgraphs%20by%20leveraging%20spatial%2C%20temporal%2C%20and%20local%20event%20impact%20information.%0AExtensive%20experiments%20against%20state-of-the-art%20factual%20and%20counterfactual%0Abaselines%20demonstrate%20CoDy%27s%20effectiveness%2C%20with%20improvements%20of%2016%25%20in%20AUFSC%2B%0Aover%20the%20strongest%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16846v2&entry.124074799=Read"},
{"title": "Contrastive and Transfer Learning for Effective Audio Fingerprinting\n  through a Real-World Evaluation Protocol", "author": "Christos Nikou and Theodoros Giannakopoulos", "abstract": "  Recent advances in song identification leverage deep neural networks to learn\ncompact audio fingerprints directly from raw waveforms. While these methods\nperform well under controlled conditions, their accuracy drops significantly in\nreal-world scenarios where the audio is captured via mobile devices in noisy\nenvironments. In this paper, we introduce a novel evaluation protocol designed\nto better reflect such real-world conditions. We generate three recordings of\nthe same audio, each with increasing levels of noise, captured using a mobile\ndevice's microphone. Our results reveal a substantial performance drop for two\nstate-of-the-art CNN-based models under this protocol, compared to previously\nreported benchmarks. Additionally, we highlight the critical role of the\naugmentation pipeline during training with contrastive loss. By introduction\nlow pass and high pass filters in the augmentation pipeline we significantly\nincrease the performance of both systems in our proposed evaluation.\nFurthermore, we develop a transformer-based model with a tailored projection\nmodule and demonstrate that transferring knowledge from a semantically relevant\ndomain yields a more robust solution. The transformer architecture outperforms\nCNN-based models across all noise levels, and query durations. In low noise\nconditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in\nfinding the correct song, surpassing by 14%, and by 18.5% the second-best\nperforming model, respectively, Under heavy noise levels, we achieve a\ndetection rate 56.5% for 15-second query duration. All experiments are\nconducted on public large-scale dataset of over 100K songs, with queries\nmatched against a database of 56 million vectors.\n", "link": "http://arxiv.org/abs/2507.06070v1", "date": "2025-07-08", "relevancy": 1.958, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4991}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4947}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20and%20Transfer%20Learning%20for%20Effective%20Audio%20Fingerprinting%0A%20%20through%20a%20Real-World%20Evaluation%20Protocol&body=Title%3A%20Contrastive%20and%20Transfer%20Learning%20for%20Effective%20Audio%20Fingerprinting%0A%20%20through%20a%20Real-World%20Evaluation%20Protocol%0AAuthor%3A%20Christos%20Nikou%20and%20Theodoros%20Giannakopoulos%0AAbstract%3A%20%20%20Recent%20advances%20in%20song%20identification%20leverage%20deep%20neural%20networks%20to%20learn%0Acompact%20audio%20fingerprints%20directly%20from%20raw%20waveforms.%20While%20these%20methods%0Aperform%20well%20under%20controlled%20conditions%2C%20their%20accuracy%20drops%20significantly%20in%0Areal-world%20scenarios%20where%20the%20audio%20is%20captured%20via%20mobile%20devices%20in%20noisy%0Aenvironments.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20evaluation%20protocol%20designed%0Ato%20better%20reflect%20such%20real-world%20conditions.%20We%20generate%20three%20recordings%20of%0Athe%20same%20audio%2C%20each%20with%20increasing%20levels%20of%20noise%2C%20captured%20using%20a%20mobile%0Adevice%27s%20microphone.%20Our%20results%20reveal%20a%20substantial%20performance%20drop%20for%20two%0Astate-of-the-art%20CNN-based%20models%20under%20this%20protocol%2C%20compared%20to%20previously%0Areported%20benchmarks.%20Additionally%2C%20we%20highlight%20the%20critical%20role%20of%20the%0Aaugmentation%20pipeline%20during%20training%20with%20contrastive%20loss.%20By%20introduction%0Alow%20pass%20and%20high%20pass%20filters%20in%20the%20augmentation%20pipeline%20we%20significantly%0Aincrease%20the%20performance%20of%20both%20systems%20in%20our%20proposed%20evaluation.%0AFurthermore%2C%20we%20develop%20a%20transformer-based%20model%20with%20a%20tailored%20projection%0Amodule%20and%20demonstrate%20that%20transferring%20knowledge%20from%20a%20semantically%20relevant%0Adomain%20yields%20a%20more%20robust%20solution.%20The%20transformer%20architecture%20outperforms%0ACNN-based%20models%20across%20all%20noise%20levels%2C%20and%20query%20durations.%20In%20low%20noise%0Aconditions%20it%20achieves%2047.99%25%20for%201-sec%20queries%2C%20and%2097%25%20for%2010-sec%20queries%20in%0Afinding%20the%20correct%20song%2C%20surpassing%20by%2014%25%2C%20and%20by%2018.5%25%20the%20second-best%0Aperforming%20model%2C%20respectively%2C%20Under%20heavy%20noise%20levels%2C%20we%20achieve%20a%0Adetection%20rate%2056.5%25%20for%2015-second%20query%20duration.%20All%20experiments%20are%0Aconducted%20on%20public%20large-scale%20dataset%20of%20over%20100K%20songs%2C%20with%20queries%0Amatched%20against%20a%20database%20of%2056%20million%20vectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520and%2520Transfer%2520Learning%2520for%2520Effective%2520Audio%2520Fingerprinting%250A%2520%2520through%2520a%2520Real-World%2520Evaluation%2520Protocol%26entry.906535625%3DChristos%2520Nikou%2520and%2520Theodoros%2520Giannakopoulos%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520song%2520identification%2520leverage%2520deep%2520neural%2520networks%2520to%2520learn%250Acompact%2520audio%2520fingerprints%2520directly%2520from%2520raw%2520waveforms.%2520While%2520these%2520methods%250Aperform%2520well%2520under%2520controlled%2520conditions%252C%2520their%2520accuracy%2520drops%2520significantly%2520in%250Areal-world%2520scenarios%2520where%2520the%2520audio%2520is%2520captured%2520via%2520mobile%2520devices%2520in%2520noisy%250Aenvironments.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520evaluation%2520protocol%2520designed%250Ato%2520better%2520reflect%2520such%2520real-world%2520conditions.%2520We%2520generate%2520three%2520recordings%2520of%250Athe%2520same%2520audio%252C%2520each%2520with%2520increasing%2520levels%2520of%2520noise%252C%2520captured%2520using%2520a%2520mobile%250Adevice%2527s%2520microphone.%2520Our%2520results%2520reveal%2520a%2520substantial%2520performance%2520drop%2520for%2520two%250Astate-of-the-art%2520CNN-based%2520models%2520under%2520this%2520protocol%252C%2520compared%2520to%2520previously%250Areported%2520benchmarks.%2520Additionally%252C%2520we%2520highlight%2520the%2520critical%2520role%2520of%2520the%250Aaugmentation%2520pipeline%2520during%2520training%2520with%2520contrastive%2520loss.%2520By%2520introduction%250Alow%2520pass%2520and%2520high%2520pass%2520filters%2520in%2520the%2520augmentation%2520pipeline%2520we%2520significantly%250Aincrease%2520the%2520performance%2520of%2520both%2520systems%2520in%2520our%2520proposed%2520evaluation.%250AFurthermore%252C%2520we%2520develop%2520a%2520transformer-based%2520model%2520with%2520a%2520tailored%2520projection%250Amodule%2520and%2520demonstrate%2520that%2520transferring%2520knowledge%2520from%2520a%2520semantically%2520relevant%250Adomain%2520yields%2520a%2520more%2520robust%2520solution.%2520The%2520transformer%2520architecture%2520outperforms%250ACNN-based%2520models%2520across%2520all%2520noise%2520levels%252C%2520and%2520query%2520durations.%2520In%2520low%2520noise%250Aconditions%2520it%2520achieves%252047.99%2525%2520for%25201-sec%2520queries%252C%2520and%252097%2525%2520for%252010-sec%2520queries%2520in%250Afinding%2520the%2520correct%2520song%252C%2520surpassing%2520by%252014%2525%252C%2520and%2520by%252018.5%2525%2520the%2520second-best%250Aperforming%2520model%252C%2520respectively%252C%2520Under%2520heavy%2520noise%2520levels%252C%2520we%2520achieve%2520a%250Adetection%2520rate%252056.5%2525%2520for%252015-second%2520query%2520duration.%2520All%2520experiments%2520are%250Aconducted%2520on%2520public%2520large-scale%2520dataset%2520of%2520over%2520100K%2520songs%252C%2520with%2520queries%250Amatched%2520against%2520a%2520database%2520of%252056%2520million%2520vectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20and%20Transfer%20Learning%20for%20Effective%20Audio%20Fingerprinting%0A%20%20through%20a%20Real-World%20Evaluation%20Protocol&entry.906535625=Christos%20Nikou%20and%20Theodoros%20Giannakopoulos&entry.1292438233=%20%20Recent%20advances%20in%20song%20identification%20leverage%20deep%20neural%20networks%20to%20learn%0Acompact%20audio%20fingerprints%20directly%20from%20raw%20waveforms.%20While%20these%20methods%0Aperform%20well%20under%20controlled%20conditions%2C%20their%20accuracy%20drops%20significantly%20in%0Areal-world%20scenarios%20where%20the%20audio%20is%20captured%20via%20mobile%20devices%20in%20noisy%0Aenvironments.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20evaluation%20protocol%20designed%0Ato%20better%20reflect%20such%20real-world%20conditions.%20We%20generate%20three%20recordings%20of%0Athe%20same%20audio%2C%20each%20with%20increasing%20levels%20of%20noise%2C%20captured%20using%20a%20mobile%0Adevice%27s%20microphone.%20Our%20results%20reveal%20a%20substantial%20performance%20drop%20for%20two%0Astate-of-the-art%20CNN-based%20models%20under%20this%20protocol%2C%20compared%20to%20previously%0Areported%20benchmarks.%20Additionally%2C%20we%20highlight%20the%20critical%20role%20of%20the%0Aaugmentation%20pipeline%20during%20training%20with%20contrastive%20loss.%20By%20introduction%0Alow%20pass%20and%20high%20pass%20filters%20in%20the%20augmentation%20pipeline%20we%20significantly%0Aincrease%20the%20performance%20of%20both%20systems%20in%20our%20proposed%20evaluation.%0AFurthermore%2C%20we%20develop%20a%20transformer-based%20model%20with%20a%20tailored%20projection%0Amodule%20and%20demonstrate%20that%20transferring%20knowledge%20from%20a%20semantically%20relevant%0Adomain%20yields%20a%20more%20robust%20solution.%20The%20transformer%20architecture%20outperforms%0ACNN-based%20models%20across%20all%20noise%20levels%2C%20and%20query%20durations.%20In%20low%20noise%0Aconditions%20it%20achieves%2047.99%25%20for%201-sec%20queries%2C%20and%2097%25%20for%2010-sec%20queries%20in%0Afinding%20the%20correct%20song%2C%20surpassing%20by%2014%25%2C%20and%20by%2018.5%25%20the%20second-best%0Aperforming%20model%2C%20respectively%2C%20Under%20heavy%20noise%20levels%2C%20we%20achieve%20a%0Adetection%20rate%2056.5%25%20for%2015-second%20query%20duration.%20All%20experiments%20are%0Aconducted%20on%20public%20large-scale%20dataset%20of%20over%20100K%20songs%2C%20with%20queries%0Amatched%20against%20a%20database%20of%2056%20million%20vectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06070v1&entry.124074799=Read"},
{"title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the\n  Oracle", "author": "Hui Dai and Ryan Teehan and Mengye Ren", "abstract": "  Many existing evaluation benchmarks for Large Language Models (LLMs) quickly\nbecome outdated due to the emergence of new models and training data. These\nbenchmarks also fall short in assessing how LLM performance changes over time,\nas they consist of a static set of questions without a temporal dimension. To\naddress these limitations, we propose using future event prediction as a\ncontinuous evaluation method to assess LLMs' temporal generalization and\nforecasting abilities. Our benchmark, Daily Oracle, automatically generates\nquestion-answer (QA) pairs from daily news, challenging LLMs to predict\n\"future\" event outcomes. Our findings reveal that as pre-training data becomes\noutdated, LLM performance degrades over time. While Retrieval Augmented\nGeneration (RAG) has the potential to enhance prediction accuracy, the\nperformance degradation pattern persists, highlighting the need for continuous\nmodel updates. Code and data are available at\nhttps://agenticlearning.ai/daily-oracle.\n", "link": "http://arxiv.org/abs/2411.08324v2", "date": "2025-07-08", "relevancy": 1.9579, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20LLMs%20Prescient%3F%20A%20Continuous%20Evaluation%20using%20Daily%20News%20as%20the%0A%20%20Oracle&body=Title%3A%20Are%20LLMs%20Prescient%3F%20A%20Continuous%20Evaluation%20using%20Daily%20News%20as%20the%0A%20%20Oracle%0AAuthor%3A%20Hui%20Dai%20and%20Ryan%20Teehan%20and%20Mengye%20Ren%0AAbstract%3A%20%20%20Many%20existing%20evaluation%20benchmarks%20for%20Large%20Language%20Models%20%28LLMs%29%20quickly%0Abecome%20outdated%20due%20to%20the%20emergence%20of%20new%20models%20and%20training%20data.%20These%0Abenchmarks%20also%20fall%20short%20in%20assessing%20how%20LLM%20performance%20changes%20over%20time%2C%0Aas%20they%20consist%20of%20a%20static%20set%20of%20questions%20without%20a%20temporal%20dimension.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20using%20future%20event%20prediction%20as%20a%0Acontinuous%20evaluation%20method%20to%20assess%20LLMs%27%20temporal%20generalization%20and%0Aforecasting%20abilities.%20Our%20benchmark%2C%20Daily%20Oracle%2C%20automatically%20generates%0Aquestion-answer%20%28QA%29%20pairs%20from%20daily%20news%2C%20challenging%20LLMs%20to%20predict%0A%22future%22%20event%20outcomes.%20Our%20findings%20reveal%20that%20as%20pre-training%20data%20becomes%0Aoutdated%2C%20LLM%20performance%20degrades%20over%20time.%20While%20Retrieval%20Augmented%0AGeneration%20%28RAG%29%20has%20the%20potential%20to%20enhance%20prediction%20accuracy%2C%20the%0Aperformance%20degradation%20pattern%20persists%2C%20highlighting%20the%20need%20for%20continuous%0Amodel%20updates.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//agenticlearning.ai/daily-oracle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08324v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520LLMs%2520Prescient%253F%2520A%2520Continuous%2520Evaluation%2520using%2520Daily%2520News%2520as%2520the%250A%2520%2520Oracle%26entry.906535625%3DHui%2520Dai%2520and%2520Ryan%2520Teehan%2520and%2520Mengye%2520Ren%26entry.1292438233%3D%2520%2520Many%2520existing%2520evaluation%2520benchmarks%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520quickly%250Abecome%2520outdated%2520due%2520to%2520the%2520emergence%2520of%2520new%2520models%2520and%2520training%2520data.%2520These%250Abenchmarks%2520also%2520fall%2520short%2520in%2520assessing%2520how%2520LLM%2520performance%2520changes%2520over%2520time%252C%250Aas%2520they%2520consist%2520of%2520a%2520static%2520set%2520of%2520questions%2520without%2520a%2520temporal%2520dimension.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520using%2520future%2520event%2520prediction%2520as%2520a%250Acontinuous%2520evaluation%2520method%2520to%2520assess%2520LLMs%2527%2520temporal%2520generalization%2520and%250Aforecasting%2520abilities.%2520Our%2520benchmark%252C%2520Daily%2520Oracle%252C%2520automatically%2520generates%250Aquestion-answer%2520%2528QA%2529%2520pairs%2520from%2520daily%2520news%252C%2520challenging%2520LLMs%2520to%2520predict%250A%2522future%2522%2520event%2520outcomes.%2520Our%2520findings%2520reveal%2520that%2520as%2520pre-training%2520data%2520becomes%250Aoutdated%252C%2520LLM%2520performance%2520degrades%2520over%2520time.%2520While%2520Retrieval%2520Augmented%250AGeneration%2520%2528RAG%2529%2520has%2520the%2520potential%2520to%2520enhance%2520prediction%2520accuracy%252C%2520the%250Aperformance%2520degradation%2520pattern%2520persists%252C%2520highlighting%2520the%2520need%2520for%2520continuous%250Amodel%2520updates.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//agenticlearning.ai/daily-oracle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08324v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20LLMs%20Prescient%3F%20A%20Continuous%20Evaluation%20using%20Daily%20News%20as%20the%0A%20%20Oracle&entry.906535625=Hui%20Dai%20and%20Ryan%20Teehan%20and%20Mengye%20Ren&entry.1292438233=%20%20Many%20existing%20evaluation%20benchmarks%20for%20Large%20Language%20Models%20%28LLMs%29%20quickly%0Abecome%20outdated%20due%20to%20the%20emergence%20of%20new%20models%20and%20training%20data.%20These%0Abenchmarks%20also%20fall%20short%20in%20assessing%20how%20LLM%20performance%20changes%20over%20time%2C%0Aas%20they%20consist%20of%20a%20static%20set%20of%20questions%20without%20a%20temporal%20dimension.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20using%20future%20event%20prediction%20as%20a%0Acontinuous%20evaluation%20method%20to%20assess%20LLMs%27%20temporal%20generalization%20and%0Aforecasting%20abilities.%20Our%20benchmark%2C%20Daily%20Oracle%2C%20automatically%20generates%0Aquestion-answer%20%28QA%29%20pairs%20from%20daily%20news%2C%20challenging%20LLMs%20to%20predict%0A%22future%22%20event%20outcomes.%20Our%20findings%20reveal%20that%20as%20pre-training%20data%20becomes%0Aoutdated%2C%20LLM%20performance%20degrades%20over%20time.%20While%20Retrieval%20Augmented%0AGeneration%20%28RAG%29%20has%20the%20potential%20to%20enhance%20prediction%20accuracy%2C%20the%0Aperformance%20degradation%20pattern%20persists%2C%20highlighting%20the%20need%20for%20continuous%0Amodel%20updates.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//agenticlearning.ai/daily-oracle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08324v2&entry.124074799=Read"},
{"title": "Agents Are All You Need for LLM Unlearning", "author": "Debdeep Sanyal and Murari Mandal", "abstract": "  Information removal or suppression in large language models (LLMs) is a\ndesired functionality, useful in AI regulation, legal compliance, safety, and\nprivacy. LLM unlearning methods aim to remove information on demand from LLMs.\nCurrent LLM unlearning methods struggle to balance the unlearning efficacy and\nutility due to the competing nature of these objectives. Keeping the unlearning\nprocess computationally feasible without assuming access to the model weights\nis an overlooked area. In this work we show that \\textit{agents might be all we\nneed for effective and practical inference-time LLM unlearning}. We present the\nfirst agentic LLM unlearning (\\texttt{ALU}) method, a multi-agent,\nretrain-free, model-agnostic approach to LLM unlearning that achieves effective\nunlearning while preserving the utility. Our \\texttt{ALU} framework unlearns by\ninvolving multiple LLM agents, each designed for a specific step in the\nunlearning process, without the need to update model weights for any of the\nagents in the framework. Users can easily request any set of unlearning\ninstances in any sequence, and \\texttt{ALU} seamlessly adapts in real time.\nThis is facilitated without requiring any changes in the underlying LLM model.\nThrough extensive experiments on established benchmarks (TOFU, WMDP, WPU) and\njailbreaking techniques (many shot, target masking, other languages), we\ndemonstrate that \\texttt{ALU} consistently stands out as the most robust\ninference-time LLM unlearning framework among current state-of-the-art methods\nwhile incurring time cost that remains effectively constant regardless of the\nnumber of unlearning targets. We further highlight \\texttt{ALU}'s superior\nperformance compared to existing methods when evaluated at scale. Specifically,\n\\texttt{ALU} is assessed on up to 1000 unlearning targets, exceeding the\nevaluation scope of all previously proposed LLM unlearning methods.\n", "link": "http://arxiv.org/abs/2502.00406v2", "date": "2025-07-08", "relevancy": 1.9563, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.509}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5085}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agents%20Are%20All%20You%20Need%20for%20LLM%20Unlearning&body=Title%3A%20Agents%20Are%20All%20You%20Need%20for%20LLM%20Unlearning%0AAuthor%3A%20Debdeep%20Sanyal%20and%20Murari%20Mandal%0AAbstract%3A%20%20%20Information%20removal%20or%20suppression%20in%20large%20language%20models%20%28LLMs%29%20is%20a%0Adesired%20functionality%2C%20useful%20in%20AI%20regulation%2C%20legal%20compliance%2C%20safety%2C%20and%0Aprivacy.%20LLM%20unlearning%20methods%20aim%20to%20remove%20information%20on%20demand%20from%20LLMs.%0ACurrent%20LLM%20unlearning%20methods%20struggle%20to%20balance%20the%20unlearning%20efficacy%20and%0Autility%20due%20to%20the%20competing%20nature%20of%20these%20objectives.%20Keeping%20the%20unlearning%0Aprocess%20computationally%20feasible%20without%20assuming%20access%20to%20the%20model%20weights%0Ais%20an%20overlooked%20area.%20In%20this%20work%20we%20show%20that%20%5Ctextit%7Bagents%20might%20be%20all%20we%0Aneed%20for%20effective%20and%20practical%20inference-time%20LLM%20unlearning%7D.%20We%20present%20the%0Afirst%20agentic%20LLM%20unlearning%20%28%5Ctexttt%7BALU%7D%29%20method%2C%20a%20multi-agent%2C%0Aretrain-free%2C%20model-agnostic%20approach%20to%20LLM%20unlearning%20that%20achieves%20effective%0Aunlearning%20while%20preserving%20the%20utility.%20Our%20%5Ctexttt%7BALU%7D%20framework%20unlearns%20by%0Ainvolving%20multiple%20LLM%20agents%2C%20each%20designed%20for%20a%20specific%20step%20in%20the%0Aunlearning%20process%2C%20without%20the%20need%20to%20update%20model%20weights%20for%20any%20of%20the%0Aagents%20in%20the%20framework.%20Users%20can%20easily%20request%20any%20set%20of%20unlearning%0Ainstances%20in%20any%20sequence%2C%20and%20%5Ctexttt%7BALU%7D%20seamlessly%20adapts%20in%20real%20time.%0AThis%20is%20facilitated%20without%20requiring%20any%20changes%20in%20the%20underlying%20LLM%20model.%0AThrough%20extensive%20experiments%20on%20established%20benchmarks%20%28TOFU%2C%20WMDP%2C%20WPU%29%20and%0Ajailbreaking%20techniques%20%28many%20shot%2C%20target%20masking%2C%20other%20languages%29%2C%20we%0Ademonstrate%20that%20%5Ctexttt%7BALU%7D%20consistently%20stands%20out%20as%20the%20most%20robust%0Ainference-time%20LLM%20unlearning%20framework%20among%20current%20state-of-the-art%20methods%0Awhile%20incurring%20time%20cost%20that%20remains%20effectively%20constant%20regardless%20of%20the%0Anumber%20of%20unlearning%20targets.%20We%20further%20highlight%20%5Ctexttt%7BALU%7D%27s%20superior%0Aperformance%20compared%20to%20existing%20methods%20when%20evaluated%20at%20scale.%20Specifically%2C%0A%5Ctexttt%7BALU%7D%20is%20assessed%20on%20up%20to%201000%20unlearning%20targets%2C%20exceeding%20the%0Aevaluation%20scope%20of%20all%20previously%20proposed%20LLM%20unlearning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00406v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgents%2520Are%2520All%2520You%2520Need%2520for%2520LLM%2520Unlearning%26entry.906535625%3DDebdeep%2520Sanyal%2520and%2520Murari%2520Mandal%26entry.1292438233%3D%2520%2520Information%2520removal%2520or%2520suppression%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520a%250Adesired%2520functionality%252C%2520useful%2520in%2520AI%2520regulation%252C%2520legal%2520compliance%252C%2520safety%252C%2520and%250Aprivacy.%2520LLM%2520unlearning%2520methods%2520aim%2520to%2520remove%2520information%2520on%2520demand%2520from%2520LLMs.%250ACurrent%2520LLM%2520unlearning%2520methods%2520struggle%2520to%2520balance%2520the%2520unlearning%2520efficacy%2520and%250Autility%2520due%2520to%2520the%2520competing%2520nature%2520of%2520these%2520objectives.%2520Keeping%2520the%2520unlearning%250Aprocess%2520computationally%2520feasible%2520without%2520assuming%2520access%2520to%2520the%2520model%2520weights%250Ais%2520an%2520overlooked%2520area.%2520In%2520this%2520work%2520we%2520show%2520that%2520%255Ctextit%257Bagents%2520might%2520be%2520all%2520we%250Aneed%2520for%2520effective%2520and%2520practical%2520inference-time%2520LLM%2520unlearning%257D.%2520We%2520present%2520the%250Afirst%2520agentic%2520LLM%2520unlearning%2520%2528%255Ctexttt%257BALU%257D%2529%2520method%252C%2520a%2520multi-agent%252C%250Aretrain-free%252C%2520model-agnostic%2520approach%2520to%2520LLM%2520unlearning%2520that%2520achieves%2520effective%250Aunlearning%2520while%2520preserving%2520the%2520utility.%2520Our%2520%255Ctexttt%257BALU%257D%2520framework%2520unlearns%2520by%250Ainvolving%2520multiple%2520LLM%2520agents%252C%2520each%2520designed%2520for%2520a%2520specific%2520step%2520in%2520the%250Aunlearning%2520process%252C%2520without%2520the%2520need%2520to%2520update%2520model%2520weights%2520for%2520any%2520of%2520the%250Aagents%2520in%2520the%2520framework.%2520Users%2520can%2520easily%2520request%2520any%2520set%2520of%2520unlearning%250Ainstances%2520in%2520any%2520sequence%252C%2520and%2520%255Ctexttt%257BALU%257D%2520seamlessly%2520adapts%2520in%2520real%2520time.%250AThis%2520is%2520facilitated%2520without%2520requiring%2520any%2520changes%2520in%2520the%2520underlying%2520LLM%2520model.%250AThrough%2520extensive%2520experiments%2520on%2520established%2520benchmarks%2520%2528TOFU%252C%2520WMDP%252C%2520WPU%2529%2520and%250Ajailbreaking%2520techniques%2520%2528many%2520shot%252C%2520target%2520masking%252C%2520other%2520languages%2529%252C%2520we%250Ademonstrate%2520that%2520%255Ctexttt%257BALU%257D%2520consistently%2520stands%2520out%2520as%2520the%2520most%2520robust%250Ainference-time%2520LLM%2520unlearning%2520framework%2520among%2520current%2520state-of-the-art%2520methods%250Awhile%2520incurring%2520time%2520cost%2520that%2520remains%2520effectively%2520constant%2520regardless%2520of%2520the%250Anumber%2520of%2520unlearning%2520targets.%2520We%2520further%2520highlight%2520%255Ctexttt%257BALU%257D%2527s%2520superior%250Aperformance%2520compared%2520to%2520existing%2520methods%2520when%2520evaluated%2520at%2520scale.%2520Specifically%252C%250A%255Ctexttt%257BALU%257D%2520is%2520assessed%2520on%2520up%2520to%25201000%2520unlearning%2520targets%252C%2520exceeding%2520the%250Aevaluation%2520scope%2520of%2520all%2520previously%2520proposed%2520LLM%2520unlearning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00406v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agents%20Are%20All%20You%20Need%20for%20LLM%20Unlearning&entry.906535625=Debdeep%20Sanyal%20and%20Murari%20Mandal&entry.1292438233=%20%20Information%20removal%20or%20suppression%20in%20large%20language%20models%20%28LLMs%29%20is%20a%0Adesired%20functionality%2C%20useful%20in%20AI%20regulation%2C%20legal%20compliance%2C%20safety%2C%20and%0Aprivacy.%20LLM%20unlearning%20methods%20aim%20to%20remove%20information%20on%20demand%20from%20LLMs.%0ACurrent%20LLM%20unlearning%20methods%20struggle%20to%20balance%20the%20unlearning%20efficacy%20and%0Autility%20due%20to%20the%20competing%20nature%20of%20these%20objectives.%20Keeping%20the%20unlearning%0Aprocess%20computationally%20feasible%20without%20assuming%20access%20to%20the%20model%20weights%0Ais%20an%20overlooked%20area.%20In%20this%20work%20we%20show%20that%20%5Ctextit%7Bagents%20might%20be%20all%20we%0Aneed%20for%20effective%20and%20practical%20inference-time%20LLM%20unlearning%7D.%20We%20present%20the%0Afirst%20agentic%20LLM%20unlearning%20%28%5Ctexttt%7BALU%7D%29%20method%2C%20a%20multi-agent%2C%0Aretrain-free%2C%20model-agnostic%20approach%20to%20LLM%20unlearning%20that%20achieves%20effective%0Aunlearning%20while%20preserving%20the%20utility.%20Our%20%5Ctexttt%7BALU%7D%20framework%20unlearns%20by%0Ainvolving%20multiple%20LLM%20agents%2C%20each%20designed%20for%20a%20specific%20step%20in%20the%0Aunlearning%20process%2C%20without%20the%20need%20to%20update%20model%20weights%20for%20any%20of%20the%0Aagents%20in%20the%20framework.%20Users%20can%20easily%20request%20any%20set%20of%20unlearning%0Ainstances%20in%20any%20sequence%2C%20and%20%5Ctexttt%7BALU%7D%20seamlessly%20adapts%20in%20real%20time.%0AThis%20is%20facilitated%20without%20requiring%20any%20changes%20in%20the%20underlying%20LLM%20model.%0AThrough%20extensive%20experiments%20on%20established%20benchmarks%20%28TOFU%2C%20WMDP%2C%20WPU%29%20and%0Ajailbreaking%20techniques%20%28many%20shot%2C%20target%20masking%2C%20other%20languages%29%2C%20we%0Ademonstrate%20that%20%5Ctexttt%7BALU%7D%20consistently%20stands%20out%20as%20the%20most%20robust%0Ainference-time%20LLM%20unlearning%20framework%20among%20current%20state-of-the-art%20methods%0Awhile%20incurring%20time%20cost%20that%20remains%20effectively%20constant%20regardless%20of%20the%0Anumber%20of%20unlearning%20targets.%20We%20further%20highlight%20%5Ctexttt%7BALU%7D%27s%20superior%0Aperformance%20compared%20to%20existing%20methods%20when%20evaluated%20at%20scale.%20Specifically%2C%0A%5Ctexttt%7BALU%7D%20is%20assessed%20on%20up%20to%201000%20unlearning%20targets%2C%20exceeding%20the%0Aevaluation%20scope%20of%20all%20previously%20proposed%20LLM%20unlearning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00406v2&entry.124074799=Read"},
{"title": "Efficient Federated Learning with Timely Update Dissemination", "author": "Juncheng Jia and Ji Liu and Chao Huo and Yihui Shen and Yang Zhou and Huaiyu Dai and Dejing Dou", "abstract": "  Federated Learning (FL) has emerged as a compelling methodology for the\nmanagement of distributed data, marked by significant advancements in recent\nyears. In this paper, we propose an efficient FL approach that capitalizes on\nadditional downlink bandwidth resources to ensure timely update dissemination.\nInitially, we implement this strategy within an asynchronous framework,\nintroducing the Asynchronous Staleness-aware Model Update (FedASMU), which\nintegrates both server-side and device-side methodologies. On the server side,\nwe present an asynchronous FL system model that employs a dynamic model\naggregation technique, which harmonizes local model updates with the global\nmodel to enhance both accuracy and efficiency. Concurrently, on the device\nside, we propose an adaptive model adjustment mechanism that integrates the\nlatest global model with local models during training to further elevate\naccuracy. Subsequently, we extend this approach to a synchronous context,\nreferred to as FedSSMU. Theoretical analyses substantiate the convergence of\nour proposed methodologies. Extensive experiments, encompassing six models and\nfive public datasets, demonstrate that FedASMU and FedSSMU significantly\nsurpass baseline methods in terms of both accuracy (up to 145.87%) and\nefficiency (up to 97.59%).\n", "link": "http://arxiv.org/abs/2507.06031v1", "date": "2025-07-08", "relevancy": 1.9551, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4939}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4938}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Federated%20Learning%20with%20Timely%20Update%20Dissemination&body=Title%3A%20Efficient%20Federated%20Learning%20with%20Timely%20Update%20Dissemination%0AAuthor%3A%20Juncheng%20Jia%20and%20Ji%20Liu%20and%20Chao%20Huo%20and%20Yihui%20Shen%20and%20Yang%20Zhou%20and%20Huaiyu%20Dai%20and%20Dejing%20Dou%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20compelling%20methodology%20for%20the%0Amanagement%20of%20distributed%20data%2C%20marked%20by%20significant%20advancements%20in%20recent%0Ayears.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20FL%20approach%20that%20capitalizes%20on%0Aadditional%20downlink%20bandwidth%20resources%20to%20ensure%20timely%20update%20dissemination.%0AInitially%2C%20we%20implement%20this%20strategy%20within%20an%20asynchronous%20framework%2C%0Aintroducing%20the%20Asynchronous%20Staleness-aware%20Model%20Update%20%28FedASMU%29%2C%20which%0Aintegrates%20both%20server-side%20and%20device-side%20methodologies.%20On%20the%20server%20side%2C%0Awe%20present%20an%20asynchronous%20FL%20system%20model%20that%20employs%20a%20dynamic%20model%0Aaggregation%20technique%2C%20which%20harmonizes%20local%20model%20updates%20with%20the%20global%0Amodel%20to%20enhance%20both%20accuracy%20and%20efficiency.%20Concurrently%2C%20on%20the%20device%0Aside%2C%20we%20propose%20an%20adaptive%20model%20adjustment%20mechanism%20that%20integrates%20the%0Alatest%20global%20model%20with%20local%20models%20during%20training%20to%20further%20elevate%0Aaccuracy.%20Subsequently%2C%20we%20extend%20this%20approach%20to%20a%20synchronous%20context%2C%0Areferred%20to%20as%20FedSSMU.%20Theoretical%20analyses%20substantiate%20the%20convergence%20of%0Aour%20proposed%20methodologies.%20Extensive%20experiments%2C%20encompassing%20six%20models%20and%0Afive%20public%20datasets%2C%20demonstrate%20that%20FedASMU%20and%20FedSSMU%20significantly%0Asurpass%20baseline%20methods%20in%20terms%20of%20both%20accuracy%20%28up%20to%20145.87%25%29%20and%0Aefficiency%20%28up%20to%2097.59%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Federated%2520Learning%2520with%2520Timely%2520Update%2520Dissemination%26entry.906535625%3DJuncheng%2520Jia%2520and%2520Ji%2520Liu%2520and%2520Chao%2520Huo%2520and%2520Yihui%2520Shen%2520and%2520Yang%2520Zhou%2520and%2520Huaiyu%2520Dai%2520and%2520Dejing%2520Dou%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520compelling%2520methodology%2520for%2520the%250Amanagement%2520of%2520distributed%2520data%252C%2520marked%2520by%2520significant%2520advancements%2520in%2520recent%250Ayears.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520efficient%2520FL%2520approach%2520that%2520capitalizes%2520on%250Aadditional%2520downlink%2520bandwidth%2520resources%2520to%2520ensure%2520timely%2520update%2520dissemination.%250AInitially%252C%2520we%2520implement%2520this%2520strategy%2520within%2520an%2520asynchronous%2520framework%252C%250Aintroducing%2520the%2520Asynchronous%2520Staleness-aware%2520Model%2520Update%2520%2528FedASMU%2529%252C%2520which%250Aintegrates%2520both%2520server-side%2520and%2520device-side%2520methodologies.%2520On%2520the%2520server%2520side%252C%250Awe%2520present%2520an%2520asynchronous%2520FL%2520system%2520model%2520that%2520employs%2520a%2520dynamic%2520model%250Aaggregation%2520technique%252C%2520which%2520harmonizes%2520local%2520model%2520updates%2520with%2520the%2520global%250Amodel%2520to%2520enhance%2520both%2520accuracy%2520and%2520efficiency.%2520Concurrently%252C%2520on%2520the%2520device%250Aside%252C%2520we%2520propose%2520an%2520adaptive%2520model%2520adjustment%2520mechanism%2520that%2520integrates%2520the%250Alatest%2520global%2520model%2520with%2520local%2520models%2520during%2520training%2520to%2520further%2520elevate%250Aaccuracy.%2520Subsequently%252C%2520we%2520extend%2520this%2520approach%2520to%2520a%2520synchronous%2520context%252C%250Areferred%2520to%2520as%2520FedSSMU.%2520Theoretical%2520analyses%2520substantiate%2520the%2520convergence%2520of%250Aour%2520proposed%2520methodologies.%2520Extensive%2520experiments%252C%2520encompassing%2520six%2520models%2520and%250Afive%2520public%2520datasets%252C%2520demonstrate%2520that%2520FedASMU%2520and%2520FedSSMU%2520significantly%250Asurpass%2520baseline%2520methods%2520in%2520terms%2520of%2520both%2520accuracy%2520%2528up%2520to%2520145.87%2525%2529%2520and%250Aefficiency%2520%2528up%2520to%252097.59%2525%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Federated%20Learning%20with%20Timely%20Update%20Dissemination&entry.906535625=Juncheng%20Jia%20and%20Ji%20Liu%20and%20Chao%20Huo%20and%20Yihui%20Shen%20and%20Yang%20Zhou%20and%20Huaiyu%20Dai%20and%20Dejing%20Dou&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20compelling%20methodology%20for%20the%0Amanagement%20of%20distributed%20data%2C%20marked%20by%20significant%20advancements%20in%20recent%0Ayears.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20FL%20approach%20that%20capitalizes%20on%0Aadditional%20downlink%20bandwidth%20resources%20to%20ensure%20timely%20update%20dissemination.%0AInitially%2C%20we%20implement%20this%20strategy%20within%20an%20asynchronous%20framework%2C%0Aintroducing%20the%20Asynchronous%20Staleness-aware%20Model%20Update%20%28FedASMU%29%2C%20which%0Aintegrates%20both%20server-side%20and%20device-side%20methodologies.%20On%20the%20server%20side%2C%0Awe%20present%20an%20asynchronous%20FL%20system%20model%20that%20employs%20a%20dynamic%20model%0Aaggregation%20technique%2C%20which%20harmonizes%20local%20model%20updates%20with%20the%20global%0Amodel%20to%20enhance%20both%20accuracy%20and%20efficiency.%20Concurrently%2C%20on%20the%20device%0Aside%2C%20we%20propose%20an%20adaptive%20model%20adjustment%20mechanism%20that%20integrates%20the%0Alatest%20global%20model%20with%20local%20models%20during%20training%20to%20further%20elevate%0Aaccuracy.%20Subsequently%2C%20we%20extend%20this%20approach%20to%20a%20synchronous%20context%2C%0Areferred%20to%20as%20FedSSMU.%20Theoretical%20analyses%20substantiate%20the%20convergence%20of%0Aour%20proposed%20methodologies.%20Extensive%20experiments%2C%20encompassing%20six%20models%20and%0Afive%20public%20datasets%2C%20demonstrate%20that%20FedASMU%20and%20FedSSMU%20significantly%0Asurpass%20baseline%20methods%20in%20terms%20of%20both%20accuracy%20%28up%20to%20145.87%25%29%20and%0Aefficiency%20%28up%20to%2097.59%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06031v1&entry.124074799=Read"},
{"title": "TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and\n  OCR-Guided Supervision", "author": "Syeda Anshrah Gillani and Mirza Samad Ahmed Baig and Osama Ahmed Khan and Shahid Munir Shah and Umema Mujeeb and Maheen Ali", "abstract": "  The modern text-to-image diffusion models boom has opened a new era in\ndigital content production as it has proven the previously unseen ability to\nproduce photorealistic and stylistically diverse imagery based on the semantics\nof natural-language descriptions. However, the consistent disadvantage of these\nmodels is that they cannot generate readable, meaningful, and correctly spelled\ntext in generated images, which significantly limits the use of practical\npurposes like advertising, learning, and creative design. This paper introduces\na new framework, namely Glyph-Conditioned Diffusion with Character-Aware\nAttention (GCDA), using which a typical diffusion backbone is extended by three\nwell-designed modules. To begin with, the model has a dual-stream text encoder\nthat encodes both semantic contextual information and explicit glyph\nrepresentations, resulting in a character-aware representation of the input\ntext that is rich in nature. Second, an attention mechanism that is aware of\nthe character is proposed with a new attention segregation loss that aims to\nlimit the attention distribution of each character independently in order to\navoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning\nphase, where a full text perceptual loss, directly optimises models to be\nlegible and accurately spell. Large scale experiments to benchmark datasets,\nsuch as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new\nstate-of-the-art on all metrics, with better character based metrics on text\nrendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error\nRate: 0.15 vs 0.25), human perception, and comparable image synthesis quality\non high-fidelity (FID: 14.3).\n", "link": "http://arxiv.org/abs/2507.06033v1", "date": "2025-07-08", "relevancy": 1.9517, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6573}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6569}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextPixs%3A%20Glyph-Conditioned%20Diffusion%20with%20Character-Aware%20Attention%20and%0A%20%20OCR-Guided%20Supervision&body=Title%3A%20TextPixs%3A%20Glyph-Conditioned%20Diffusion%20with%20Character-Aware%20Attention%20and%0A%20%20OCR-Guided%20Supervision%0AAuthor%3A%20Syeda%20Anshrah%20Gillani%20and%20Mirza%20Samad%20Ahmed%20Baig%20and%20Osama%20Ahmed%20Khan%20and%20Shahid%20Munir%20Shah%20and%20Umema%20Mujeeb%20and%20Maheen%20Ali%0AAbstract%3A%20%20%20The%20modern%20text-to-image%20diffusion%20models%20boom%20has%20opened%20a%20new%20era%20in%0Adigital%20content%20production%20as%20it%20has%20proven%20the%20previously%20unseen%20ability%20to%0Aproduce%20photorealistic%20and%20stylistically%20diverse%20imagery%20based%20on%20the%20semantics%0Aof%20natural-language%20descriptions.%20However%2C%20the%20consistent%20disadvantage%20of%20these%0Amodels%20is%20that%20they%20cannot%20generate%20readable%2C%20meaningful%2C%20and%20correctly%20spelled%0Atext%20in%20generated%20images%2C%20which%20significantly%20limits%20the%20use%20of%20practical%0Apurposes%20like%20advertising%2C%20learning%2C%20and%20creative%20design.%20This%20paper%20introduces%0Aa%20new%20framework%2C%20namely%20Glyph-Conditioned%20Diffusion%20with%20Character-Aware%0AAttention%20%28GCDA%29%2C%20using%20which%20a%20typical%20diffusion%20backbone%20is%20extended%20by%20three%0Awell-designed%20modules.%20To%20begin%20with%2C%20the%20model%20has%20a%20dual-stream%20text%20encoder%0Athat%20encodes%20both%20semantic%20contextual%20information%20and%20explicit%20glyph%0Arepresentations%2C%20resulting%20in%20a%20character-aware%20representation%20of%20the%20input%0Atext%20that%20is%20rich%20in%20nature.%20Second%2C%20an%20attention%20mechanism%20that%20is%20aware%20of%0Athe%20character%20is%20proposed%20with%20a%20new%20attention%20segregation%20loss%20that%20aims%20to%0Alimit%20the%20attention%20distribution%20of%20each%20character%20independently%20in%20order%20to%0Aavoid%20distortion%20artifacts.%20Lastly%2C%20GCDA%20has%20an%20OCR-in-the-loop%20fine-tuning%0Aphase%2C%20where%20a%20full%20text%20perceptual%20loss%2C%20directly%20optimises%20models%20to%20be%0Alegible%20and%20accurately%20spell.%20Large%20scale%20experiments%20to%20benchmark%20datasets%2C%0Asuch%20as%20MARIO-10M%20and%20T2I-CompBench%2C%20reveal%20that%20GCDA%20sets%20a%20new%0Astate-of-the-art%20on%20all%20metrics%2C%20with%20better%20character%20based%20metrics%20on%20text%0Arendering%20%28Character%20Error%20Rate%3A%200.08%20vs%200.21%20for%20the%20previous%20best%3B%20Word%20Error%0ARate%3A%200.15%20vs%200.25%29%2C%20human%20perception%2C%20and%20comparable%20image%20synthesis%20quality%0Aon%20high-fidelity%20%28FID%3A%2014.3%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextPixs%253A%2520Glyph-Conditioned%2520Diffusion%2520with%2520Character-Aware%2520Attention%2520and%250A%2520%2520OCR-Guided%2520Supervision%26entry.906535625%3DSyeda%2520Anshrah%2520Gillani%2520and%2520Mirza%2520Samad%2520Ahmed%2520Baig%2520and%2520Osama%2520Ahmed%2520Khan%2520and%2520Shahid%2520Munir%2520Shah%2520and%2520Umema%2520Mujeeb%2520and%2520Maheen%2520Ali%26entry.1292438233%3D%2520%2520The%2520modern%2520text-to-image%2520diffusion%2520models%2520boom%2520has%2520opened%2520a%2520new%2520era%2520in%250Adigital%2520content%2520production%2520as%2520it%2520has%2520proven%2520the%2520previously%2520unseen%2520ability%2520to%250Aproduce%2520photorealistic%2520and%2520stylistically%2520diverse%2520imagery%2520based%2520on%2520the%2520semantics%250Aof%2520natural-language%2520descriptions.%2520However%252C%2520the%2520consistent%2520disadvantage%2520of%2520these%250Amodels%2520is%2520that%2520they%2520cannot%2520generate%2520readable%252C%2520meaningful%252C%2520and%2520correctly%2520spelled%250Atext%2520in%2520generated%2520images%252C%2520which%2520significantly%2520limits%2520the%2520use%2520of%2520practical%250Apurposes%2520like%2520advertising%252C%2520learning%252C%2520and%2520creative%2520design.%2520This%2520paper%2520introduces%250Aa%2520new%2520framework%252C%2520namely%2520Glyph-Conditioned%2520Diffusion%2520with%2520Character-Aware%250AAttention%2520%2528GCDA%2529%252C%2520using%2520which%2520a%2520typical%2520diffusion%2520backbone%2520is%2520extended%2520by%2520three%250Awell-designed%2520modules.%2520To%2520begin%2520with%252C%2520the%2520model%2520has%2520a%2520dual-stream%2520text%2520encoder%250Athat%2520encodes%2520both%2520semantic%2520contextual%2520information%2520and%2520explicit%2520glyph%250Arepresentations%252C%2520resulting%2520in%2520a%2520character-aware%2520representation%2520of%2520the%2520input%250Atext%2520that%2520is%2520rich%2520in%2520nature.%2520Second%252C%2520an%2520attention%2520mechanism%2520that%2520is%2520aware%2520of%250Athe%2520character%2520is%2520proposed%2520with%2520a%2520new%2520attention%2520segregation%2520loss%2520that%2520aims%2520to%250Alimit%2520the%2520attention%2520distribution%2520of%2520each%2520character%2520independently%2520in%2520order%2520to%250Aavoid%2520distortion%2520artifacts.%2520Lastly%252C%2520GCDA%2520has%2520an%2520OCR-in-the-loop%2520fine-tuning%250Aphase%252C%2520where%2520a%2520full%2520text%2520perceptual%2520loss%252C%2520directly%2520optimises%2520models%2520to%2520be%250Alegible%2520and%2520accurately%2520spell.%2520Large%2520scale%2520experiments%2520to%2520benchmark%2520datasets%252C%250Asuch%2520as%2520MARIO-10M%2520and%2520T2I-CompBench%252C%2520reveal%2520that%2520GCDA%2520sets%2520a%2520new%250Astate-of-the-art%2520on%2520all%2520metrics%252C%2520with%2520better%2520character%2520based%2520metrics%2520on%2520text%250Arendering%2520%2528Character%2520Error%2520Rate%253A%25200.08%2520vs%25200.21%2520for%2520the%2520previous%2520best%253B%2520Word%2520Error%250ARate%253A%25200.15%2520vs%25200.25%2529%252C%2520human%2520perception%252C%2520and%2520comparable%2520image%2520synthesis%2520quality%250Aon%2520high-fidelity%2520%2528FID%253A%252014.3%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextPixs%3A%20Glyph-Conditioned%20Diffusion%20with%20Character-Aware%20Attention%20and%0A%20%20OCR-Guided%20Supervision&entry.906535625=Syeda%20Anshrah%20Gillani%20and%20Mirza%20Samad%20Ahmed%20Baig%20and%20Osama%20Ahmed%20Khan%20and%20Shahid%20Munir%20Shah%20and%20Umema%20Mujeeb%20and%20Maheen%20Ali&entry.1292438233=%20%20The%20modern%20text-to-image%20diffusion%20models%20boom%20has%20opened%20a%20new%20era%20in%0Adigital%20content%20production%20as%20it%20has%20proven%20the%20previously%20unseen%20ability%20to%0Aproduce%20photorealistic%20and%20stylistically%20diverse%20imagery%20based%20on%20the%20semantics%0Aof%20natural-language%20descriptions.%20However%2C%20the%20consistent%20disadvantage%20of%20these%0Amodels%20is%20that%20they%20cannot%20generate%20readable%2C%20meaningful%2C%20and%20correctly%20spelled%0Atext%20in%20generated%20images%2C%20which%20significantly%20limits%20the%20use%20of%20practical%0Apurposes%20like%20advertising%2C%20learning%2C%20and%20creative%20design.%20This%20paper%20introduces%0Aa%20new%20framework%2C%20namely%20Glyph-Conditioned%20Diffusion%20with%20Character-Aware%0AAttention%20%28GCDA%29%2C%20using%20which%20a%20typical%20diffusion%20backbone%20is%20extended%20by%20three%0Awell-designed%20modules.%20To%20begin%20with%2C%20the%20model%20has%20a%20dual-stream%20text%20encoder%0Athat%20encodes%20both%20semantic%20contextual%20information%20and%20explicit%20glyph%0Arepresentations%2C%20resulting%20in%20a%20character-aware%20representation%20of%20the%20input%0Atext%20that%20is%20rich%20in%20nature.%20Second%2C%20an%20attention%20mechanism%20that%20is%20aware%20of%0Athe%20character%20is%20proposed%20with%20a%20new%20attention%20segregation%20loss%20that%20aims%20to%0Alimit%20the%20attention%20distribution%20of%20each%20character%20independently%20in%20order%20to%0Aavoid%20distortion%20artifacts.%20Lastly%2C%20GCDA%20has%20an%20OCR-in-the-loop%20fine-tuning%0Aphase%2C%20where%20a%20full%20text%20perceptual%20loss%2C%20directly%20optimises%20models%20to%20be%0Alegible%20and%20accurately%20spell.%20Large%20scale%20experiments%20to%20benchmark%20datasets%2C%0Asuch%20as%20MARIO-10M%20and%20T2I-CompBench%2C%20reveal%20that%20GCDA%20sets%20a%20new%0Astate-of-the-art%20on%20all%20metrics%2C%20with%20better%20character%20based%20metrics%20on%20text%0Arendering%20%28Character%20Error%20Rate%3A%200.08%20vs%200.21%20for%20the%20previous%20best%3B%20Word%20Error%0ARate%3A%200.15%20vs%200.25%29%2C%20human%20perception%2C%20and%20comparable%20image%20synthesis%20quality%0Aon%20high-fidelity%20%28FID%3A%2014.3%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06033v1&entry.124074799=Read"},
{"title": "A Method for Optimizing Connections in Differentiable Logic Gate\n  Networks", "author": "Wout Mommen and Lars Keuninckx and Matthias Hartmann and Piet Wambacq", "abstract": "  We introduce a novel method for partial optimization of the connections in\nDeep Differentiable Logic Gate Networks (LGNs). Our training method utilizes a\nprobability distribution over a subset of connections per gate input, selecting\nthe connection with highest merit, after which the gate-types are selected. We\nshow that the connection-optimized LGNs outperform standard fixed-connection\nLGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only\na fraction of the number of logic gates. When training all connections, we\ndemonstrate that 8000 simple logic gates are sufficient to achieve over 98% on\nthe MNIST data set. Additionally, we show that our network has 24 times fewer\ngates, while performing better on the MNIST data set compared to standard fully\nconnected LGNs. As such, our work shows a pathway towards fully trainable\nBoolean logic.\n", "link": "http://arxiv.org/abs/2507.06173v1", "date": "2025-07-08", "relevancy": 1.9402, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5123}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4708}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Method%20for%20Optimizing%20Connections%20in%20Differentiable%20Logic%20Gate%0A%20%20Networks&body=Title%3A%20A%20Method%20for%20Optimizing%20Connections%20in%20Differentiable%20Logic%20Gate%0A%20%20Networks%0AAuthor%3A%20Wout%20Mommen%20and%20Lars%20Keuninckx%20and%20Matthias%20Hartmann%20and%20Piet%20Wambacq%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20method%20for%20partial%20optimization%20of%20the%20connections%20in%0ADeep%20Differentiable%20Logic%20Gate%20Networks%20%28LGNs%29.%20Our%20training%20method%20utilizes%20a%0Aprobability%20distribution%20over%20a%20subset%20of%20connections%20per%20gate%20input%2C%20selecting%0Athe%20connection%20with%20highest%20merit%2C%20after%20which%20the%20gate-types%20are%20selected.%20We%0Ashow%20that%20the%20connection-optimized%20LGNs%20outperform%20standard%20fixed-connection%0ALGNs%20on%20the%20Yin-Yang%2C%20MNIST%20and%20Fashion-MNIST%20benchmarks%2C%20while%20requiring%20only%0Aa%20fraction%20of%20the%20number%20of%20logic%20gates.%20When%20training%20all%20connections%2C%20we%0Ademonstrate%20that%208000%20simple%20logic%20gates%20are%20sufficient%20to%20achieve%20over%2098%25%20on%0Athe%20MNIST%20data%20set.%20Additionally%2C%20we%20show%20that%20our%20network%20has%2024%20times%20fewer%0Agates%2C%20while%20performing%20better%20on%20the%20MNIST%20data%20set%20compared%20to%20standard%20fully%0Aconnected%20LGNs.%20As%20such%2C%20our%20work%20shows%20a%20pathway%20towards%20fully%20trainable%0ABoolean%20logic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Method%2520for%2520Optimizing%2520Connections%2520in%2520Differentiable%2520Logic%2520Gate%250A%2520%2520Networks%26entry.906535625%3DWout%2520Mommen%2520and%2520Lars%2520Keuninckx%2520and%2520Matthias%2520Hartmann%2520and%2520Piet%2520Wambacq%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520method%2520for%2520partial%2520optimization%2520of%2520the%2520connections%2520in%250ADeep%2520Differentiable%2520Logic%2520Gate%2520Networks%2520%2528LGNs%2529.%2520Our%2520training%2520method%2520utilizes%2520a%250Aprobability%2520distribution%2520over%2520a%2520subset%2520of%2520connections%2520per%2520gate%2520input%252C%2520selecting%250Athe%2520connection%2520with%2520highest%2520merit%252C%2520after%2520which%2520the%2520gate-types%2520are%2520selected.%2520We%250Ashow%2520that%2520the%2520connection-optimized%2520LGNs%2520outperform%2520standard%2520fixed-connection%250ALGNs%2520on%2520the%2520Yin-Yang%252C%2520MNIST%2520and%2520Fashion-MNIST%2520benchmarks%252C%2520while%2520requiring%2520only%250Aa%2520fraction%2520of%2520the%2520number%2520of%2520logic%2520gates.%2520When%2520training%2520all%2520connections%252C%2520we%250Ademonstrate%2520that%25208000%2520simple%2520logic%2520gates%2520are%2520sufficient%2520to%2520achieve%2520over%252098%2525%2520on%250Athe%2520MNIST%2520data%2520set.%2520Additionally%252C%2520we%2520show%2520that%2520our%2520network%2520has%252024%2520times%2520fewer%250Agates%252C%2520while%2520performing%2520better%2520on%2520the%2520MNIST%2520data%2520set%2520compared%2520to%2520standard%2520fully%250Aconnected%2520LGNs.%2520As%2520such%252C%2520our%2520work%2520shows%2520a%2520pathway%2520towards%2520fully%2520trainable%250ABoolean%2520logic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Method%20for%20Optimizing%20Connections%20in%20Differentiable%20Logic%20Gate%0A%20%20Networks&entry.906535625=Wout%20Mommen%20and%20Lars%20Keuninckx%20and%20Matthias%20Hartmann%20and%20Piet%20Wambacq&entry.1292438233=%20%20We%20introduce%20a%20novel%20method%20for%20partial%20optimization%20of%20the%20connections%20in%0ADeep%20Differentiable%20Logic%20Gate%20Networks%20%28LGNs%29.%20Our%20training%20method%20utilizes%20a%0Aprobability%20distribution%20over%20a%20subset%20of%20connections%20per%20gate%20input%2C%20selecting%0Athe%20connection%20with%20highest%20merit%2C%20after%20which%20the%20gate-types%20are%20selected.%20We%0Ashow%20that%20the%20connection-optimized%20LGNs%20outperform%20standard%20fixed-connection%0ALGNs%20on%20the%20Yin-Yang%2C%20MNIST%20and%20Fashion-MNIST%20benchmarks%2C%20while%20requiring%20only%0Aa%20fraction%20of%20the%20number%20of%20logic%20gates.%20When%20training%20all%20connections%2C%20we%0Ademonstrate%20that%208000%20simple%20logic%20gates%20are%20sufficient%20to%20achieve%20over%2098%25%20on%0Athe%20MNIST%20data%20set.%20Additionally%2C%20we%20show%20that%20our%20network%20has%2024%20times%20fewer%0Agates%2C%20while%20performing%20better%20on%20the%20MNIST%20data%20set%20compared%20to%20standard%20fully%0Aconnected%20LGNs.%20As%20such%2C%20our%20work%20shows%20a%20pathway%20towards%20fully%20trainable%0ABoolean%20logic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06173v1&entry.124074799=Read"},
{"title": "StreamDiffusion: A Pipeline-level Solution for Real-time Interactive\n  Generation", "author": "Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Masayoshi Tomizuka and Kurt Keutzer", "abstract": "  We introduce StreamDiffusion, a real-time diffusion pipeline designed for\ninteractive image generation. Existing diffusion models are adept at creating\nimages from text or image prompts, yet they often fall short in real-time\ninteraction. This limitation becomes particularly evident in scenarios\ninvolving continuous input, such as Metaverse, live video streaming, and\nbroadcasting, where high throughput is imperative. To address this, we present\na novel approach that transforms the original sequential denoising into the\nbatching denoising process. Stream Batch eliminates the conventional\nwait-and-interact approach and enables fluid and high throughput streams. To\nhandle the frequency disparity between data input and model throughput, we\ndesign a novel input-output queue for parallelizing the streaming process.\nMoreover, the existing diffusion pipeline uses classifier-free guidance(CFG),\nwhich requires additional U-Net computation. To mitigate the redundant\ncomputations, we propose a novel residual classifier-free guidance (RCFG)\nalgorithm that reduces the number of negative conditional denoising steps to\nonly one or even zero. Besides, we introduce a stochastic similarity\nfilter(SSF) to optimize power consumption. Our Stream Batch achieves around\n1.5x speedup compared to the sequential denoising method at different denoising\nlevels. The proposed RCFG leads to speeds up to 2.05x higher than the\nconventional CFG. Combining the proposed strategies and existing mature\nacceleration tools makes the image-to-image generation achieve up-to 91.07fps\non one RTX4090, improving the throughputs of AutoPipline developed by Diffusers\nover 59.56x. Furthermore, our proposed StreamDiffusion also significantly\nreduces the energy consumption by 2.39x on one RTX3060 and 1.99x on one\nRTX4090, respectively.\n", "link": "http://arxiv.org/abs/2312.12491v2", "date": "2025-07-08", "relevancy": 1.9284, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6983}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6459}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamDiffusion%3A%20A%20Pipeline-level%20Solution%20for%20Real-time%20Interactive%0A%20%20Generation&body=Title%3A%20StreamDiffusion%3A%20A%20Pipeline-level%20Solution%20for%20Real-time%20Interactive%0A%20%20Generation%0AAuthor%3A%20Akio%20Kodaira%20and%20Chenfeng%20Xu%20and%20Toshiki%20Hazama%20and%20Takanori%20Yoshimoto%20and%20Kohei%20Ohno%20and%20Shogo%20Mitsuhori%20and%20Soichi%20Sugano%20and%20Hanying%20Cho%20and%20Zhijian%20Liu%20and%20Masayoshi%20Tomizuka%20and%20Kurt%20Keutzer%0AAbstract%3A%20%20%20We%20introduce%20StreamDiffusion%2C%20a%20real-time%20diffusion%20pipeline%20designed%20for%0Ainteractive%20image%20generation.%20Existing%20diffusion%20models%20are%20adept%20at%20creating%0Aimages%20from%20text%20or%20image%20prompts%2C%20yet%20they%20often%20fall%20short%20in%20real-time%0Ainteraction.%20This%20limitation%20becomes%20particularly%20evident%20in%20scenarios%0Ainvolving%20continuous%20input%2C%20such%20as%20Metaverse%2C%20live%20video%20streaming%2C%20and%0Abroadcasting%2C%20where%20high%20throughput%20is%20imperative.%20To%20address%20this%2C%20we%20present%0Aa%20novel%20approach%20that%20transforms%20the%20original%20sequential%20denoising%20into%20the%0Abatching%20denoising%20process.%20Stream%20Batch%20eliminates%20the%20conventional%0Await-and-interact%20approach%20and%20enables%20fluid%20and%20high%20throughput%20streams.%20To%0Ahandle%20the%20frequency%20disparity%20between%20data%20input%20and%20model%20throughput%2C%20we%0Adesign%20a%20novel%20input-output%20queue%20for%20parallelizing%20the%20streaming%20process.%0AMoreover%2C%20the%20existing%20diffusion%20pipeline%20uses%20classifier-free%20guidance%28CFG%29%2C%0Awhich%20requires%20additional%20U-Net%20computation.%20To%20mitigate%20the%20redundant%0Acomputations%2C%20we%20propose%20a%20novel%20residual%20classifier-free%20guidance%20%28RCFG%29%0Aalgorithm%20that%20reduces%20the%20number%20of%20negative%20conditional%20denoising%20steps%20to%0Aonly%20one%20or%20even%20zero.%20Besides%2C%20we%20introduce%20a%20stochastic%20similarity%0Afilter%28SSF%29%20to%20optimize%20power%20consumption.%20Our%20Stream%20Batch%20achieves%20around%0A1.5x%20speedup%20compared%20to%20the%20sequential%20denoising%20method%20at%20different%20denoising%0Alevels.%20The%20proposed%20RCFG%20leads%20to%20speeds%20up%20to%202.05x%20higher%20than%20the%0Aconventional%20CFG.%20Combining%20the%20proposed%20strategies%20and%20existing%20mature%0Aacceleration%20tools%20makes%20the%20image-to-image%20generation%20achieve%20up-to%2091.07fps%0Aon%20one%20RTX4090%2C%20improving%20the%20throughputs%20of%20AutoPipline%20developed%20by%20Diffusers%0Aover%2059.56x.%20Furthermore%2C%20our%20proposed%20StreamDiffusion%20also%20significantly%0Areduces%20the%20energy%20consumption%20by%202.39x%20on%20one%20RTX3060%20and%201.99x%20on%20one%0ARTX4090%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12491v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamDiffusion%253A%2520A%2520Pipeline-level%2520Solution%2520for%2520Real-time%2520Interactive%250A%2520%2520Generation%26entry.906535625%3DAkio%2520Kodaira%2520and%2520Chenfeng%2520Xu%2520and%2520Toshiki%2520Hazama%2520and%2520Takanori%2520Yoshimoto%2520and%2520Kohei%2520Ohno%2520and%2520Shogo%2520Mitsuhori%2520and%2520Soichi%2520Sugano%2520and%2520Hanying%2520Cho%2520and%2520Zhijian%2520Liu%2520and%2520Masayoshi%2520Tomizuka%2520and%2520Kurt%2520Keutzer%26entry.1292438233%3D%2520%2520We%2520introduce%2520StreamDiffusion%252C%2520a%2520real-time%2520diffusion%2520pipeline%2520designed%2520for%250Ainteractive%2520image%2520generation.%2520Existing%2520diffusion%2520models%2520are%2520adept%2520at%2520creating%250Aimages%2520from%2520text%2520or%2520image%2520prompts%252C%2520yet%2520they%2520often%2520fall%2520short%2520in%2520real-time%250Ainteraction.%2520This%2520limitation%2520becomes%2520particularly%2520evident%2520in%2520scenarios%250Ainvolving%2520continuous%2520input%252C%2520such%2520as%2520Metaverse%252C%2520live%2520video%2520streaming%252C%2520and%250Abroadcasting%252C%2520where%2520high%2520throughput%2520is%2520imperative.%2520To%2520address%2520this%252C%2520we%2520present%250Aa%2520novel%2520approach%2520that%2520transforms%2520the%2520original%2520sequential%2520denoising%2520into%2520the%250Abatching%2520denoising%2520process.%2520Stream%2520Batch%2520eliminates%2520the%2520conventional%250Await-and-interact%2520approach%2520and%2520enables%2520fluid%2520and%2520high%2520throughput%2520streams.%2520To%250Ahandle%2520the%2520frequency%2520disparity%2520between%2520data%2520input%2520and%2520model%2520throughput%252C%2520we%250Adesign%2520a%2520novel%2520input-output%2520queue%2520for%2520parallelizing%2520the%2520streaming%2520process.%250AMoreover%252C%2520the%2520existing%2520diffusion%2520pipeline%2520uses%2520classifier-free%2520guidance%2528CFG%2529%252C%250Awhich%2520requires%2520additional%2520U-Net%2520computation.%2520To%2520mitigate%2520the%2520redundant%250Acomputations%252C%2520we%2520propose%2520a%2520novel%2520residual%2520classifier-free%2520guidance%2520%2528RCFG%2529%250Aalgorithm%2520that%2520reduces%2520the%2520number%2520of%2520negative%2520conditional%2520denoising%2520steps%2520to%250Aonly%2520one%2520or%2520even%2520zero.%2520Besides%252C%2520we%2520introduce%2520a%2520stochastic%2520similarity%250Afilter%2528SSF%2529%2520to%2520optimize%2520power%2520consumption.%2520Our%2520Stream%2520Batch%2520achieves%2520around%250A1.5x%2520speedup%2520compared%2520to%2520the%2520sequential%2520denoising%2520method%2520at%2520different%2520denoising%250Alevels.%2520The%2520proposed%2520RCFG%2520leads%2520to%2520speeds%2520up%2520to%25202.05x%2520higher%2520than%2520the%250Aconventional%2520CFG.%2520Combining%2520the%2520proposed%2520strategies%2520and%2520existing%2520mature%250Aacceleration%2520tools%2520makes%2520the%2520image-to-image%2520generation%2520achieve%2520up-to%252091.07fps%250Aon%2520one%2520RTX4090%252C%2520improving%2520the%2520throughputs%2520of%2520AutoPipline%2520developed%2520by%2520Diffusers%250Aover%252059.56x.%2520Furthermore%252C%2520our%2520proposed%2520StreamDiffusion%2520also%2520significantly%250Areduces%2520the%2520energy%2520consumption%2520by%25202.39x%2520on%2520one%2520RTX3060%2520and%25201.99x%2520on%2520one%250ARTX4090%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12491v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamDiffusion%3A%20A%20Pipeline-level%20Solution%20for%20Real-time%20Interactive%0A%20%20Generation&entry.906535625=Akio%20Kodaira%20and%20Chenfeng%20Xu%20and%20Toshiki%20Hazama%20and%20Takanori%20Yoshimoto%20and%20Kohei%20Ohno%20and%20Shogo%20Mitsuhori%20and%20Soichi%20Sugano%20and%20Hanying%20Cho%20and%20Zhijian%20Liu%20and%20Masayoshi%20Tomizuka%20and%20Kurt%20Keutzer&entry.1292438233=%20%20We%20introduce%20StreamDiffusion%2C%20a%20real-time%20diffusion%20pipeline%20designed%20for%0Ainteractive%20image%20generation.%20Existing%20diffusion%20models%20are%20adept%20at%20creating%0Aimages%20from%20text%20or%20image%20prompts%2C%20yet%20they%20often%20fall%20short%20in%20real-time%0Ainteraction.%20This%20limitation%20becomes%20particularly%20evident%20in%20scenarios%0Ainvolving%20continuous%20input%2C%20such%20as%20Metaverse%2C%20live%20video%20streaming%2C%20and%0Abroadcasting%2C%20where%20high%20throughput%20is%20imperative.%20To%20address%20this%2C%20we%20present%0Aa%20novel%20approach%20that%20transforms%20the%20original%20sequential%20denoising%20into%20the%0Abatching%20denoising%20process.%20Stream%20Batch%20eliminates%20the%20conventional%0Await-and-interact%20approach%20and%20enables%20fluid%20and%20high%20throughput%20streams.%20To%0Ahandle%20the%20frequency%20disparity%20between%20data%20input%20and%20model%20throughput%2C%20we%0Adesign%20a%20novel%20input-output%20queue%20for%20parallelizing%20the%20streaming%20process.%0AMoreover%2C%20the%20existing%20diffusion%20pipeline%20uses%20classifier-free%20guidance%28CFG%29%2C%0Awhich%20requires%20additional%20U-Net%20computation.%20To%20mitigate%20the%20redundant%0Acomputations%2C%20we%20propose%20a%20novel%20residual%20classifier-free%20guidance%20%28RCFG%29%0Aalgorithm%20that%20reduces%20the%20number%20of%20negative%20conditional%20denoising%20steps%20to%0Aonly%20one%20or%20even%20zero.%20Besides%2C%20we%20introduce%20a%20stochastic%20similarity%0Afilter%28SSF%29%20to%20optimize%20power%20consumption.%20Our%20Stream%20Batch%20achieves%20around%0A1.5x%20speedup%20compared%20to%20the%20sequential%20denoising%20method%20at%20different%20denoising%0Alevels.%20The%20proposed%20RCFG%20leads%20to%20speeds%20up%20to%202.05x%20higher%20than%20the%0Aconventional%20CFG.%20Combining%20the%20proposed%20strategies%20and%20existing%20mature%0Aacceleration%20tools%20makes%20the%20image-to-image%20generation%20achieve%20up-to%2091.07fps%0Aon%20one%20RTX4090%2C%20improving%20the%20throughputs%20of%20AutoPipline%20developed%20by%20Diffusers%0Aover%2059.56x.%20Furthermore%2C%20our%20proposed%20StreamDiffusion%20also%20significantly%0Areduces%20the%20energy%20consumption%20by%202.39x%20on%20one%20RTX3060%20and%201.99x%20on%20one%0ARTX4090%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12491v2&entry.124074799=Read"},
{"title": "Instruction Following by Boosting Attention of Large Language Models", "author": "Vitoria Guardieiro and Adam Stein and Avishree Khare and Eric Wong", "abstract": "  Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering.\n", "link": "http://arxiv.org/abs/2506.13734v2", "date": "2025-07-08", "relevancy": 1.9229, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5032}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4653}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction%20Following%20by%20Boosting%20Attention%20of%20Large%20Language%20Models&body=Title%3A%20Instruction%20Following%20by%20Boosting%20Attention%20of%20Large%20Language%20Models%0AAuthor%3A%20Vitoria%20Guardieiro%20and%20Adam%20Stein%20and%20Avishree%20Khare%20and%20Eric%20Wong%0AAbstract%3A%20%20%20Controlling%20the%20generation%20of%20large%20language%20models%20%28LLMs%29%20remains%20a%20central%0Achallenge%20to%20ensure%20their%20safe%20and%20reliable%20deployment.%20While%20prompt%0Aengineering%20and%20finetuning%20are%20common%20approaches%2C%20recent%20work%20has%20explored%0Alatent%20steering%2C%20a%20lightweight%20technique%20that%20alters%20LLM%20internal%20activations%0Ato%20guide%20generation.%20However%2C%20subsequent%20studies%20revealed%20latent%20steering%27s%0Aeffectiveness%20to%20be%20limited%2C%20often%20underperforming%20simple%20instruction%0Aprompting.%20To%20address%20this%20limitation%2C%20we%20first%20establish%20a%20benchmark%20across%0Adiverse%20behaviors%20for%20standardized%20evaluation%20of%20steering%20techniques.%20Building%0Aon%20insights%20from%20this%20benchmark%2C%20we%20introduce%20Instruction%20Attention%20Boosting%0A%28InstABoost%29%2C%20a%20latent%20steering%20method%20that%20boosts%20the%20strength%20of%20instruction%0Aprompting%20by%20altering%20the%20model%27s%20attention%20during%20generation.%20InstABoost%0Acombines%20the%20strengths%20of%20existing%20approaches%20and%20is%20theoretically%20supported%20by%0Aprior%20work%20that%20suggests%20that%20in-context%20rule%20following%20in%20transformer-based%0Amodels%20can%20be%20controlled%20by%20manipulating%20attention%20on%20instructions.%0AEmpirically%2C%20InstABoost%20demonstrates%20superior%20control%20success%20compared%20to%20both%0Atraditional%20prompting%20and%20latent%20steering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction%2520Following%2520by%2520Boosting%2520Attention%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DVitoria%2520Guardieiro%2520and%2520Adam%2520Stein%2520and%2520Avishree%2520Khare%2520and%2520Eric%2520Wong%26entry.1292438233%3D%2520%2520Controlling%2520the%2520generation%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520remains%2520a%2520central%250Achallenge%2520to%2520ensure%2520their%2520safe%2520and%2520reliable%2520deployment.%2520While%2520prompt%250Aengineering%2520and%2520finetuning%2520are%2520common%2520approaches%252C%2520recent%2520work%2520has%2520explored%250Alatent%2520steering%252C%2520a%2520lightweight%2520technique%2520that%2520alters%2520LLM%2520internal%2520activations%250Ato%2520guide%2520generation.%2520However%252C%2520subsequent%2520studies%2520revealed%2520latent%2520steering%2527s%250Aeffectiveness%2520to%2520be%2520limited%252C%2520often%2520underperforming%2520simple%2520instruction%250Aprompting.%2520To%2520address%2520this%2520limitation%252C%2520we%2520first%2520establish%2520a%2520benchmark%2520across%250Adiverse%2520behaviors%2520for%2520standardized%2520evaluation%2520of%2520steering%2520techniques.%2520Building%250Aon%2520insights%2520from%2520this%2520benchmark%252C%2520we%2520introduce%2520Instruction%2520Attention%2520Boosting%250A%2528InstABoost%2529%252C%2520a%2520latent%2520steering%2520method%2520that%2520boosts%2520the%2520strength%2520of%2520instruction%250Aprompting%2520by%2520altering%2520the%2520model%2527s%2520attention%2520during%2520generation.%2520InstABoost%250Acombines%2520the%2520strengths%2520of%2520existing%2520approaches%2520and%2520is%2520theoretically%2520supported%2520by%250Aprior%2520work%2520that%2520suggests%2520that%2520in-context%2520rule%2520following%2520in%2520transformer-based%250Amodels%2520can%2520be%2520controlled%2520by%2520manipulating%2520attention%2520on%2520instructions.%250AEmpirically%252C%2520InstABoost%2520demonstrates%2520superior%2520control%2520success%2520compared%2520to%2520both%250Atraditional%2520prompting%2520and%2520latent%2520steering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction%20Following%20by%20Boosting%20Attention%20of%20Large%20Language%20Models&entry.906535625=Vitoria%20Guardieiro%20and%20Adam%20Stein%20and%20Avishree%20Khare%20and%20Eric%20Wong&entry.1292438233=%20%20Controlling%20the%20generation%20of%20large%20language%20models%20%28LLMs%29%20remains%20a%20central%0Achallenge%20to%20ensure%20their%20safe%20and%20reliable%20deployment.%20While%20prompt%0Aengineering%20and%20finetuning%20are%20common%20approaches%2C%20recent%20work%20has%20explored%0Alatent%20steering%2C%20a%20lightweight%20technique%20that%20alters%20LLM%20internal%20activations%0Ato%20guide%20generation.%20However%2C%20subsequent%20studies%20revealed%20latent%20steering%27s%0Aeffectiveness%20to%20be%20limited%2C%20often%20underperforming%20simple%20instruction%0Aprompting.%20To%20address%20this%20limitation%2C%20we%20first%20establish%20a%20benchmark%20across%0Adiverse%20behaviors%20for%20standardized%20evaluation%20of%20steering%20techniques.%20Building%0Aon%20insights%20from%20this%20benchmark%2C%20we%20introduce%20Instruction%20Attention%20Boosting%0A%28InstABoost%29%2C%20a%20latent%20steering%20method%20that%20boosts%20the%20strength%20of%20instruction%0Aprompting%20by%20altering%20the%20model%27s%20attention%20during%20generation.%20InstABoost%0Acombines%20the%20strengths%20of%20existing%20approaches%20and%20is%20theoretically%20supported%20by%0Aprior%20work%20that%20suggests%20that%20in-context%20rule%20following%20in%20transformer-based%0Amodels%20can%20be%20controlled%20by%20manipulating%20attention%20on%20instructions.%0AEmpirically%2C%20InstABoost%20demonstrates%20superior%20control%20success%20compared%20to%20both%0Atraditional%20prompting%20and%20latent%20steering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13734v2&entry.124074799=Read"},
{"title": "Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model\n  Predictions", "author": "Courtney Ford and Mark T. Keane", "abstract": "  Explainable AI (XAI) methods often struggle to generate clear, interpretable\noutputs for users without domain expertise. We introduce Feature-Guided\nNeighbor Selection (FGNS), a post hoc method that enhances interpretability by\nselecting class-representative examples using both local and global feature\nimportance. In a user study (N = 98) evaluating Kannada script classifications,\nFGNS significantly improved non-experts' ability to identify model errors while\nmaintaining appropriate agreement with correct predictions. Participants made\nfaster and more accurate decisions compared to those given traditional k-NN\nexplanations. Quantitative analysis shows that FGNS selects neighbors that\nbetter reflect class characteristics rather than merely minimizing\nfeature-space distance, leading to more consistent selection and tighter\nclustering around class prototypes. These results support FGNS as a step toward\nmore human-aligned model assessment, although further work is needed to address\nthe gap between explanation quality and perceived trust.\n", "link": "http://arxiv.org/abs/2507.06029v1", "date": "2025-07-08", "relevancy": 1.9199, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4841}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4784}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature-Guided%20Neighbor%20Selection%20for%20Non-Expert%20Evaluation%20of%20Model%0A%20%20Predictions&body=Title%3A%20Feature-Guided%20Neighbor%20Selection%20for%20Non-Expert%20Evaluation%20of%20Model%0A%20%20Predictions%0AAuthor%3A%20Courtney%20Ford%20and%20Mark%20T.%20Keane%0AAbstract%3A%20%20%20Explainable%20AI%20%28XAI%29%20methods%20often%20struggle%20to%20generate%20clear%2C%20interpretable%0Aoutputs%20for%20users%20without%20domain%20expertise.%20We%20introduce%20Feature-Guided%0ANeighbor%20Selection%20%28FGNS%29%2C%20a%20post%20hoc%20method%20that%20enhances%20interpretability%20by%0Aselecting%20class-representative%20examples%20using%20both%20local%20and%20global%20feature%0Aimportance.%20In%20a%20user%20study%20%28N%20%3D%2098%29%20evaluating%20Kannada%20script%20classifications%2C%0AFGNS%20significantly%20improved%20non-experts%27%20ability%20to%20identify%20model%20errors%20while%0Amaintaining%20appropriate%20agreement%20with%20correct%20predictions.%20Participants%20made%0Afaster%20and%20more%20accurate%20decisions%20compared%20to%20those%20given%20traditional%20k-NN%0Aexplanations.%20Quantitative%20analysis%20shows%20that%20FGNS%20selects%20neighbors%20that%0Abetter%20reflect%20class%20characteristics%20rather%20than%20merely%20minimizing%0Afeature-space%20distance%2C%20leading%20to%20more%20consistent%20selection%20and%20tighter%0Aclustering%20around%20class%20prototypes.%20These%20results%20support%20FGNS%20as%20a%20step%20toward%0Amore%20human-aligned%20model%20assessment%2C%20although%20further%20work%20is%20needed%20to%20address%0Athe%20gap%20between%20explanation%20quality%20and%20perceived%20trust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature-Guided%2520Neighbor%2520Selection%2520for%2520Non-Expert%2520Evaluation%2520of%2520Model%250A%2520%2520Predictions%26entry.906535625%3DCourtney%2520Ford%2520and%2520Mark%2520T.%2520Keane%26entry.1292438233%3D%2520%2520Explainable%2520AI%2520%2528XAI%2529%2520methods%2520often%2520struggle%2520to%2520generate%2520clear%252C%2520interpretable%250Aoutputs%2520for%2520users%2520without%2520domain%2520expertise.%2520We%2520introduce%2520Feature-Guided%250ANeighbor%2520Selection%2520%2528FGNS%2529%252C%2520a%2520post%2520hoc%2520method%2520that%2520enhances%2520interpretability%2520by%250Aselecting%2520class-representative%2520examples%2520using%2520both%2520local%2520and%2520global%2520feature%250Aimportance.%2520In%2520a%2520user%2520study%2520%2528N%2520%253D%252098%2529%2520evaluating%2520Kannada%2520script%2520classifications%252C%250AFGNS%2520significantly%2520improved%2520non-experts%2527%2520ability%2520to%2520identify%2520model%2520errors%2520while%250Amaintaining%2520appropriate%2520agreement%2520with%2520correct%2520predictions.%2520Participants%2520made%250Afaster%2520and%2520more%2520accurate%2520decisions%2520compared%2520to%2520those%2520given%2520traditional%2520k-NN%250Aexplanations.%2520Quantitative%2520analysis%2520shows%2520that%2520FGNS%2520selects%2520neighbors%2520that%250Abetter%2520reflect%2520class%2520characteristics%2520rather%2520than%2520merely%2520minimizing%250Afeature-space%2520distance%252C%2520leading%2520to%2520more%2520consistent%2520selection%2520and%2520tighter%250Aclustering%2520around%2520class%2520prototypes.%2520These%2520results%2520support%2520FGNS%2520as%2520a%2520step%2520toward%250Amore%2520human-aligned%2520model%2520assessment%252C%2520although%2520further%2520work%2520is%2520needed%2520to%2520address%250Athe%2520gap%2520between%2520explanation%2520quality%2520and%2520perceived%2520trust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-Guided%20Neighbor%20Selection%20for%20Non-Expert%20Evaluation%20of%20Model%0A%20%20Predictions&entry.906535625=Courtney%20Ford%20and%20Mark%20T.%20Keane&entry.1292438233=%20%20Explainable%20AI%20%28XAI%29%20methods%20often%20struggle%20to%20generate%20clear%2C%20interpretable%0Aoutputs%20for%20users%20without%20domain%20expertise.%20We%20introduce%20Feature-Guided%0ANeighbor%20Selection%20%28FGNS%29%2C%20a%20post%20hoc%20method%20that%20enhances%20interpretability%20by%0Aselecting%20class-representative%20examples%20using%20both%20local%20and%20global%20feature%0Aimportance.%20In%20a%20user%20study%20%28N%20%3D%2098%29%20evaluating%20Kannada%20script%20classifications%2C%0AFGNS%20significantly%20improved%20non-experts%27%20ability%20to%20identify%20model%20errors%20while%0Amaintaining%20appropriate%20agreement%20with%20correct%20predictions.%20Participants%20made%0Afaster%20and%20more%20accurate%20decisions%20compared%20to%20those%20given%20traditional%20k-NN%0Aexplanations.%20Quantitative%20analysis%20shows%20that%20FGNS%20selects%20neighbors%20that%0Abetter%20reflect%20class%20characteristics%20rather%20than%20merely%20minimizing%0Afeature-space%20distance%2C%20leading%20to%20more%20consistent%20selection%20and%20tighter%0Aclustering%20around%20class%20prototypes.%20These%20results%20support%20FGNS%20as%20a%20step%20toward%0Amore%20human-aligned%20model%20assessment%2C%20although%20further%20work%20is%20needed%20to%20address%0Athe%20gap%20between%20explanation%20quality%20and%20perceived%20trust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06029v1&entry.124074799=Read"},
{"title": "Empirical evidence of Large Language Model's influence on human spoken\n  communication", "author": "Hiromu Yakura and Ezequiel Lopez-Lopez and Levin Brinkmann and Ignacio Serna and Prateek Gupta and Ivan Soraperra and Iyad Rahwan", "abstract": "  From the invention of writing and the printing press, to television and\nsocial media, human history is punctuated by major innovations in communication\ntechnology, which fundamentally altered how ideas spread and reshaped our\nculture. Recent chatbots powered by generative artificial intelligence\nconstitute a novel medium that encodes cultural patterns in their neural\nrepresentations and disseminates them in conversations with hundreds of\nmillions of people. Understanding whether these patterns transmit into human\nlanguage, and ultimately shape human culture, is a fundamental question. While\nfully quantifying the causal impact of a chatbot like ChatGPT on human culture\nis very challenging, lexicographic shift in human spoken communication may\noffer an early indicator of such broad phenomenon. Here, we apply econometric\ncausal inference techniques to 740,249 hours of human discourse from 360,445\nYouTube academic talks and 771,591 conversational podcast episodes across\nmultiple disciplines. We detect a measurable and abrupt increase in the use of\nwords preferentially generated by ChatGPT, such as delve, comprehend, boast,\nswift, and meticulous, after its release. These findings suggest a scenario\nwhere machines, originally trained on human data and subsequently exhibiting\ntheir own cultural traits, can, in turn, measurably reshape human culture. This\nmarks the beginning of a closed cultural feedback loop in which cultural traits\ncirculate bidirectionally between humans and machines. Our results motivate\nfurther research into the evolution of human-machine culture, and raise\nconcerns over the erosion of linguistic and cultural diversity, and the risks\nof scalable manipulation.\n", "link": "http://arxiv.org/abs/2409.01754v3", "date": "2025-07-08", "relevancy": 1.9084, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4872}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4814}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empirical%20evidence%20of%20Large%20Language%20Model%27s%20influence%20on%20human%20spoken%0A%20%20communication&body=Title%3A%20Empirical%20evidence%20of%20Large%20Language%20Model%27s%20influence%20on%20human%20spoken%0A%20%20communication%0AAuthor%3A%20Hiromu%20Yakura%20and%20Ezequiel%20Lopez-Lopez%20and%20Levin%20Brinkmann%20and%20Ignacio%20Serna%20and%20Prateek%20Gupta%20and%20Ivan%20Soraperra%20and%20Iyad%20Rahwan%0AAbstract%3A%20%20%20From%20the%20invention%20of%20writing%20and%20the%20printing%20press%2C%20to%20television%20and%0Asocial%20media%2C%20human%20history%20is%20punctuated%20by%20major%20innovations%20in%20communication%0Atechnology%2C%20which%20fundamentally%20altered%20how%20ideas%20spread%20and%20reshaped%20our%0Aculture.%20Recent%20chatbots%20powered%20by%20generative%20artificial%20intelligence%0Aconstitute%20a%20novel%20medium%20that%20encodes%20cultural%20patterns%20in%20their%20neural%0Arepresentations%20and%20disseminates%20them%20in%20conversations%20with%20hundreds%20of%0Amillions%20of%20people.%20Understanding%20whether%20these%20patterns%20transmit%20into%20human%0Alanguage%2C%20and%20ultimately%20shape%20human%20culture%2C%20is%20a%20fundamental%20question.%20While%0Afully%20quantifying%20the%20causal%20impact%20of%20a%20chatbot%20like%20ChatGPT%20on%20human%20culture%0Ais%20very%20challenging%2C%20lexicographic%20shift%20in%20human%20spoken%20communication%20may%0Aoffer%20an%20early%20indicator%20of%20such%20broad%20phenomenon.%20Here%2C%20we%20apply%20econometric%0Acausal%20inference%20techniques%20to%20740%2C249%20hours%20of%20human%20discourse%20from%20360%2C445%0AYouTube%20academic%20talks%20and%20771%2C591%20conversational%20podcast%20episodes%20across%0Amultiple%20disciplines.%20We%20detect%20a%20measurable%20and%20abrupt%20increase%20in%20the%20use%20of%0Awords%20preferentially%20generated%20by%20ChatGPT%2C%20such%20as%20delve%2C%20comprehend%2C%20boast%2C%0Aswift%2C%20and%20meticulous%2C%20after%20its%20release.%20These%20findings%20suggest%20a%20scenario%0Awhere%20machines%2C%20originally%20trained%20on%20human%20data%20and%20subsequently%20exhibiting%0Atheir%20own%20cultural%20traits%2C%20can%2C%20in%20turn%2C%20measurably%20reshape%20human%20culture.%20This%0Amarks%20the%20beginning%20of%20a%20closed%20cultural%20feedback%20loop%20in%20which%20cultural%20traits%0Acirculate%20bidirectionally%20between%20humans%20and%20machines.%20Our%20results%20motivate%0Afurther%20research%20into%20the%20evolution%20of%20human-machine%20culture%2C%20and%20raise%0Aconcerns%20over%20the%20erosion%20of%20linguistic%20and%20cultural%20diversity%2C%20and%20the%20risks%0Aof%20scalable%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01754v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpirical%2520evidence%2520of%2520Large%2520Language%2520Model%2527s%2520influence%2520on%2520human%2520spoken%250A%2520%2520communication%26entry.906535625%3DHiromu%2520Yakura%2520and%2520Ezequiel%2520Lopez-Lopez%2520and%2520Levin%2520Brinkmann%2520and%2520Ignacio%2520Serna%2520and%2520Prateek%2520Gupta%2520and%2520Ivan%2520Soraperra%2520and%2520Iyad%2520Rahwan%26entry.1292438233%3D%2520%2520From%2520the%2520invention%2520of%2520writing%2520and%2520the%2520printing%2520press%252C%2520to%2520television%2520and%250Asocial%2520media%252C%2520human%2520history%2520is%2520punctuated%2520by%2520major%2520innovations%2520in%2520communication%250Atechnology%252C%2520which%2520fundamentally%2520altered%2520how%2520ideas%2520spread%2520and%2520reshaped%2520our%250Aculture.%2520Recent%2520chatbots%2520powered%2520by%2520generative%2520artificial%2520intelligence%250Aconstitute%2520a%2520novel%2520medium%2520that%2520encodes%2520cultural%2520patterns%2520in%2520their%2520neural%250Arepresentations%2520and%2520disseminates%2520them%2520in%2520conversations%2520with%2520hundreds%2520of%250Amillions%2520of%2520people.%2520Understanding%2520whether%2520these%2520patterns%2520transmit%2520into%2520human%250Alanguage%252C%2520and%2520ultimately%2520shape%2520human%2520culture%252C%2520is%2520a%2520fundamental%2520question.%2520While%250Afully%2520quantifying%2520the%2520causal%2520impact%2520of%2520a%2520chatbot%2520like%2520ChatGPT%2520on%2520human%2520culture%250Ais%2520very%2520challenging%252C%2520lexicographic%2520shift%2520in%2520human%2520spoken%2520communication%2520may%250Aoffer%2520an%2520early%2520indicator%2520of%2520such%2520broad%2520phenomenon.%2520Here%252C%2520we%2520apply%2520econometric%250Acausal%2520inference%2520techniques%2520to%2520740%252C249%2520hours%2520of%2520human%2520discourse%2520from%2520360%252C445%250AYouTube%2520academic%2520talks%2520and%2520771%252C591%2520conversational%2520podcast%2520episodes%2520across%250Amultiple%2520disciplines.%2520We%2520detect%2520a%2520measurable%2520and%2520abrupt%2520increase%2520in%2520the%2520use%2520of%250Awords%2520preferentially%2520generated%2520by%2520ChatGPT%252C%2520such%2520as%2520delve%252C%2520comprehend%252C%2520boast%252C%250Aswift%252C%2520and%2520meticulous%252C%2520after%2520its%2520release.%2520These%2520findings%2520suggest%2520a%2520scenario%250Awhere%2520machines%252C%2520originally%2520trained%2520on%2520human%2520data%2520and%2520subsequently%2520exhibiting%250Atheir%2520own%2520cultural%2520traits%252C%2520can%252C%2520in%2520turn%252C%2520measurably%2520reshape%2520human%2520culture.%2520This%250Amarks%2520the%2520beginning%2520of%2520a%2520closed%2520cultural%2520feedback%2520loop%2520in%2520which%2520cultural%2520traits%250Acirculate%2520bidirectionally%2520between%2520humans%2520and%2520machines.%2520Our%2520results%2520motivate%250Afurther%2520research%2520into%2520the%2520evolution%2520of%2520human-machine%2520culture%252C%2520and%2520raise%250Aconcerns%2520over%2520the%2520erosion%2520of%2520linguistic%2520and%2520cultural%2520diversity%252C%2520and%2520the%2520risks%250Aof%2520scalable%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01754v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empirical%20evidence%20of%20Large%20Language%20Model%27s%20influence%20on%20human%20spoken%0A%20%20communication&entry.906535625=Hiromu%20Yakura%20and%20Ezequiel%20Lopez-Lopez%20and%20Levin%20Brinkmann%20and%20Ignacio%20Serna%20and%20Prateek%20Gupta%20and%20Ivan%20Soraperra%20and%20Iyad%20Rahwan&entry.1292438233=%20%20From%20the%20invention%20of%20writing%20and%20the%20printing%20press%2C%20to%20television%20and%0Asocial%20media%2C%20human%20history%20is%20punctuated%20by%20major%20innovations%20in%20communication%0Atechnology%2C%20which%20fundamentally%20altered%20how%20ideas%20spread%20and%20reshaped%20our%0Aculture.%20Recent%20chatbots%20powered%20by%20generative%20artificial%20intelligence%0Aconstitute%20a%20novel%20medium%20that%20encodes%20cultural%20patterns%20in%20their%20neural%0Arepresentations%20and%20disseminates%20them%20in%20conversations%20with%20hundreds%20of%0Amillions%20of%20people.%20Understanding%20whether%20these%20patterns%20transmit%20into%20human%0Alanguage%2C%20and%20ultimately%20shape%20human%20culture%2C%20is%20a%20fundamental%20question.%20While%0Afully%20quantifying%20the%20causal%20impact%20of%20a%20chatbot%20like%20ChatGPT%20on%20human%20culture%0Ais%20very%20challenging%2C%20lexicographic%20shift%20in%20human%20spoken%20communication%20may%0Aoffer%20an%20early%20indicator%20of%20such%20broad%20phenomenon.%20Here%2C%20we%20apply%20econometric%0Acausal%20inference%20techniques%20to%20740%2C249%20hours%20of%20human%20discourse%20from%20360%2C445%0AYouTube%20academic%20talks%20and%20771%2C591%20conversational%20podcast%20episodes%20across%0Amultiple%20disciplines.%20We%20detect%20a%20measurable%20and%20abrupt%20increase%20in%20the%20use%20of%0Awords%20preferentially%20generated%20by%20ChatGPT%2C%20such%20as%20delve%2C%20comprehend%2C%20boast%2C%0Aswift%2C%20and%20meticulous%2C%20after%20its%20release.%20These%20findings%20suggest%20a%20scenario%0Awhere%20machines%2C%20originally%20trained%20on%20human%20data%20and%20subsequently%20exhibiting%0Atheir%20own%20cultural%20traits%2C%20can%2C%20in%20turn%2C%20measurably%20reshape%20human%20culture.%20This%0Amarks%20the%20beginning%20of%20a%20closed%20cultural%20feedback%20loop%20in%20which%20cultural%20traits%0Acirculate%20bidirectionally%20between%20humans%20and%20machines.%20Our%20results%20motivate%0Afurther%20research%20into%20the%20evolution%20of%20human-machine%20culture%2C%20and%20raise%0Aconcerns%20over%20the%20erosion%20of%20linguistic%20and%20cultural%20diversity%2C%20and%20the%20risks%0Aof%20scalable%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01754v3&entry.124074799=Read"},
{"title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative\n  Adversarial Attacks on their Internal Representations", "author": "Xiaohu Li and Yunfeng Ning and Zepeng Bao and Mayi Xu and Jianhao Chen and Tieyun Qian", "abstract": "  Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN.\n", "link": "http://arxiv.org/abs/2507.06043v1", "date": "2025-07-08", "relevancy": 1.9042, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4925}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4666}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAVGAN%3A%20Unifying%20Jailbreak%20and%20Defense%20of%20LLMs%20via%20Generative%0A%20%20Adversarial%20Attacks%20on%20their%20Internal%20Representations&body=Title%3A%20CAVGAN%3A%20Unifying%20Jailbreak%20and%20Defense%20of%20LLMs%20via%20Generative%0A%20%20Adversarial%20Attacks%20on%20their%20Internal%20Representations%0AAuthor%3A%20Xiaohu%20Li%20and%20Yunfeng%20Ning%20and%20Zepeng%20Bao%20and%20Mayi%20Xu%20and%20Jianhao%20Chen%20and%20Tieyun%20Qian%0AAbstract%3A%20%20%20Security%20alignment%20enables%20the%20Large%20Language%20Model%20%28LLM%29%20to%20gain%20the%0Aprotection%20against%20malicious%20queries%2C%20but%20various%20jailbreak%20attack%20methods%0Areveal%20the%20vulnerability%20of%20this%20security%20mechanism.%20Previous%20studies%20have%0Aisolated%20LLM%20jailbreak%20attacks%20and%20defenses.%20We%20analyze%20the%20security%20protection%0Amechanism%20of%20the%20LLM%2C%20and%20propose%20a%20framework%20that%20combines%20attack%20and%20defense.%0AOur%20method%20is%20based%20on%20the%20linearly%20separable%20property%20of%20LLM%20intermediate%0Alayer%20embedding%2C%20as%20well%20as%20the%20essence%20of%20jailbreak%20attack%2C%20which%20aims%20to%0Aembed%20harmful%20problems%20and%20transfer%20them%20to%20the%20safe%20area.%20We%20utilize%0Agenerative%20adversarial%20network%20%28GAN%29%20to%20learn%20the%20security%20judgment%20boundary%0Ainside%20the%20LLM%20to%20achieve%20efficient%20jailbreak%20attack%20and%20defense.%20The%0Aexperimental%20results%20indicate%20that%20our%20method%20achieves%20an%20average%20jailbreak%0Asuccess%20rate%20of%2088.85%5C%25%20across%20three%20popular%20LLMs%2C%20while%20the%20defense%20success%0Arate%20on%20the%20state-of-the-art%20jailbreak%20dataset%20reaches%20an%20average%20of%2084.17%5C%25.%0AThis%20not%20only%20validates%20the%20effectiveness%20of%20our%20approach%20but%20also%20sheds%20light%0Aon%20the%20internal%20security%20mechanisms%20of%20LLMs%2C%20offering%20new%20insights%20for%0Aenhancing%20model%20security%20The%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/NLPGM/CAVGAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAVGAN%253A%2520Unifying%2520Jailbreak%2520and%2520Defense%2520of%2520LLMs%2520via%2520Generative%250A%2520%2520Adversarial%2520Attacks%2520on%2520their%2520Internal%2520Representations%26entry.906535625%3DXiaohu%2520Li%2520and%2520Yunfeng%2520Ning%2520and%2520Zepeng%2520Bao%2520and%2520Mayi%2520Xu%2520and%2520Jianhao%2520Chen%2520and%2520Tieyun%2520Qian%26entry.1292438233%3D%2520%2520Security%2520alignment%2520enables%2520the%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520gain%2520the%250Aprotection%2520against%2520malicious%2520queries%252C%2520but%2520various%2520jailbreak%2520attack%2520methods%250Areveal%2520the%2520vulnerability%2520of%2520this%2520security%2520mechanism.%2520Previous%2520studies%2520have%250Aisolated%2520LLM%2520jailbreak%2520attacks%2520and%2520defenses.%2520We%2520analyze%2520the%2520security%2520protection%250Amechanism%2520of%2520the%2520LLM%252C%2520and%2520propose%2520a%2520framework%2520that%2520combines%2520attack%2520and%2520defense.%250AOur%2520method%2520is%2520based%2520on%2520the%2520linearly%2520separable%2520property%2520of%2520LLM%2520intermediate%250Alayer%2520embedding%252C%2520as%2520well%2520as%2520the%2520essence%2520of%2520jailbreak%2520attack%252C%2520which%2520aims%2520to%250Aembed%2520harmful%2520problems%2520and%2520transfer%2520them%2520to%2520the%2520safe%2520area.%2520We%2520utilize%250Agenerative%2520adversarial%2520network%2520%2528GAN%2529%2520to%2520learn%2520the%2520security%2520judgment%2520boundary%250Ainside%2520the%2520LLM%2520to%2520achieve%2520efficient%2520jailbreak%2520attack%2520and%2520defense.%2520The%250Aexperimental%2520results%2520indicate%2520that%2520our%2520method%2520achieves%2520an%2520average%2520jailbreak%250Asuccess%2520rate%2520of%252088.85%255C%2525%2520across%2520three%2520popular%2520LLMs%252C%2520while%2520the%2520defense%2520success%250Arate%2520on%2520the%2520state-of-the-art%2520jailbreak%2520dataset%2520reaches%2520an%2520average%2520of%252084.17%255C%2525.%250AThis%2520not%2520only%2520validates%2520the%2520effectiveness%2520of%2520our%2520approach%2520but%2520also%2520sheds%2520light%250Aon%2520the%2520internal%2520security%2520mechanisms%2520of%2520LLMs%252C%2520offering%2520new%2520insights%2520for%250Aenhancing%2520model%2520security%2520The%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/NLPGM/CAVGAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAVGAN%3A%20Unifying%20Jailbreak%20and%20Defense%20of%20LLMs%20via%20Generative%0A%20%20Adversarial%20Attacks%20on%20their%20Internal%20Representations&entry.906535625=Xiaohu%20Li%20and%20Yunfeng%20Ning%20and%20Zepeng%20Bao%20and%20Mayi%20Xu%20and%20Jianhao%20Chen%20and%20Tieyun%20Qian&entry.1292438233=%20%20Security%20alignment%20enables%20the%20Large%20Language%20Model%20%28LLM%29%20to%20gain%20the%0Aprotection%20against%20malicious%20queries%2C%20but%20various%20jailbreak%20attack%20methods%0Areveal%20the%20vulnerability%20of%20this%20security%20mechanism.%20Previous%20studies%20have%0Aisolated%20LLM%20jailbreak%20attacks%20and%20defenses.%20We%20analyze%20the%20security%20protection%0Amechanism%20of%20the%20LLM%2C%20and%20propose%20a%20framework%20that%20combines%20attack%20and%20defense.%0AOur%20method%20is%20based%20on%20the%20linearly%20separable%20property%20of%20LLM%20intermediate%0Alayer%20embedding%2C%20as%20well%20as%20the%20essence%20of%20jailbreak%20attack%2C%20which%20aims%20to%0Aembed%20harmful%20problems%20and%20transfer%20them%20to%20the%20safe%20area.%20We%20utilize%0Agenerative%20adversarial%20network%20%28GAN%29%20to%20learn%20the%20security%20judgment%20boundary%0Ainside%20the%20LLM%20to%20achieve%20efficient%20jailbreak%20attack%20and%20defense.%20The%0Aexperimental%20results%20indicate%20that%20our%20method%20achieves%20an%20average%20jailbreak%0Asuccess%20rate%20of%2088.85%5C%25%20across%20three%20popular%20LLMs%2C%20while%20the%20defense%20success%0Arate%20on%20the%20state-of-the-art%20jailbreak%20dataset%20reaches%20an%20average%20of%2084.17%5C%25.%0AThis%20not%20only%20validates%20the%20effectiveness%20of%20our%20approach%20but%20also%20sheds%20light%0Aon%20the%20internal%20security%20mechanisms%20of%20LLMs%2C%20offering%20new%20insights%20for%0Aenhancing%20model%20security%20The%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/NLPGM/CAVGAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06043v1&entry.124074799=Read"},
{"title": "The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing\n  Learning and Social Interaction for Children with Autism Spectrum Disorders:\n  A Systematic Review", "author": "Biplov Paneru", "abstract": "  The emergence of large language models (LLMs), augmented reality (AR), and\nuser interface/user experience (UI/UX) design in therapies for children,\nespecially with disorders like autism spectrum disorder (ASD), is studied in\ndetail in this review study. 150 publications were collected by a thorough\nliterature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google\nScholar; 60 of them were chosen based on their methodological rigor and\nrelevance to the focus area. Three of the primary areas are studied and covered\nin this review: how AR can improve social and learning results, how LLMs can\nsupport communication, and how UI/UX design affects how effective these\ntechnologies can be. Results show that while LLMs can provide individualized\nlearning and communication support, AR has shown promise in enhancing social\nskills, motivation, and attention. For children with ASD, accessible and\nengaging interventions rely heavily on effective UI/UX design, but there is\nstill a significant lack of robotics-based education and therapeutic programs\nspecifically tailored for autistic children. To optimize the benefits of these\ntechnologies in ASD therapies and immersive education, the study emphasizes the\nneed for additional research to address difficulties related to customization,\naccessibility, and integration.\n", "link": "http://arxiv.org/abs/2409.18162v3", "date": "2025-07-08", "relevancy": 1.9039, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4887}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Nexus%20of%20AR/VR%2C%20AI%2C%20UI/UX%2C%20and%20Robotics%20Technologies%20in%20Enhancing%0A%20%20Learning%20and%20Social%20Interaction%20for%20Children%20with%20Autism%20Spectrum%20Disorders%3A%0A%20%20A%20Systematic%20Review&body=Title%3A%20The%20Nexus%20of%20AR/VR%2C%20AI%2C%20UI/UX%2C%20and%20Robotics%20Technologies%20in%20Enhancing%0A%20%20Learning%20and%20Social%20Interaction%20for%20Children%20with%20Autism%20Spectrum%20Disorders%3A%0A%20%20A%20Systematic%20Review%0AAuthor%3A%20Biplov%20Paneru%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20language%20models%20%28LLMs%29%2C%20augmented%20reality%20%28AR%29%2C%20and%0Auser%20interface/user%20experience%20%28UI/UX%29%20design%20in%20therapies%20for%20children%2C%0Aespecially%20with%20disorders%20like%20autism%20spectrum%20disorder%20%28ASD%29%2C%20is%20studied%20in%0Adetail%20in%20this%20review%20study.%20150%20publications%20were%20collected%20by%20a%20thorough%0Aliterature%20search%20throughout%20PubMed%2C%20ACM%2C%20IEEE%20Xplore%2C%20Elsevier%2C%20and%20Google%0AScholar%3B%2060%20of%20them%20were%20chosen%20based%20on%20their%20methodological%20rigor%20and%0Arelevance%20to%20the%20focus%20area.%20Three%20of%20the%20primary%20areas%20are%20studied%20and%20covered%0Ain%20this%20review%3A%20how%20AR%20can%20improve%20social%20and%20learning%20results%2C%20how%20LLMs%20can%0Asupport%20communication%2C%20and%20how%20UI/UX%20design%20affects%20how%20effective%20these%0Atechnologies%20can%20be.%20Results%20show%20that%20while%20LLMs%20can%20provide%20individualized%0Alearning%20and%20communication%20support%2C%20AR%20has%20shown%20promise%20in%20enhancing%20social%0Askills%2C%20motivation%2C%20and%20attention.%20For%20children%20with%20ASD%2C%20accessible%20and%0Aengaging%20interventions%20rely%20heavily%20on%20effective%20UI/UX%20design%2C%20but%20there%20is%0Astill%20a%20significant%20lack%20of%20robotics-based%20education%20and%20therapeutic%20programs%0Aspecifically%20tailored%20for%20autistic%20children.%20To%20optimize%20the%20benefits%20of%20these%0Atechnologies%20in%20ASD%20therapies%20and%20immersive%20education%2C%20the%20study%20emphasizes%20the%0Aneed%20for%20additional%20research%20to%20address%20difficulties%20related%20to%20customization%2C%0Aaccessibility%2C%20and%20integration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18162v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Nexus%2520of%2520AR/VR%252C%2520AI%252C%2520UI/UX%252C%2520and%2520Robotics%2520Technologies%2520in%2520Enhancing%250A%2520%2520Learning%2520and%2520Social%2520Interaction%2520for%2520Children%2520with%2520Autism%2520Spectrum%2520Disorders%253A%250A%2520%2520A%2520Systematic%2520Review%26entry.906535625%3DBiplov%2520Paneru%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520augmented%2520reality%2520%2528AR%2529%252C%2520and%250Auser%2520interface/user%2520experience%2520%2528UI/UX%2529%2520design%2520in%2520therapies%2520for%2520children%252C%250Aespecially%2520with%2520disorders%2520like%2520autism%2520spectrum%2520disorder%2520%2528ASD%2529%252C%2520is%2520studied%2520in%250Adetail%2520in%2520this%2520review%2520study.%2520150%2520publications%2520were%2520collected%2520by%2520a%2520thorough%250Aliterature%2520search%2520throughout%2520PubMed%252C%2520ACM%252C%2520IEEE%2520Xplore%252C%2520Elsevier%252C%2520and%2520Google%250AScholar%253B%252060%2520of%2520them%2520were%2520chosen%2520based%2520on%2520their%2520methodological%2520rigor%2520and%250Arelevance%2520to%2520the%2520focus%2520area.%2520Three%2520of%2520the%2520primary%2520areas%2520are%2520studied%2520and%2520covered%250Ain%2520this%2520review%253A%2520how%2520AR%2520can%2520improve%2520social%2520and%2520learning%2520results%252C%2520how%2520LLMs%2520can%250Asupport%2520communication%252C%2520and%2520how%2520UI/UX%2520design%2520affects%2520how%2520effective%2520these%250Atechnologies%2520can%2520be.%2520Results%2520show%2520that%2520while%2520LLMs%2520can%2520provide%2520individualized%250Alearning%2520and%2520communication%2520support%252C%2520AR%2520has%2520shown%2520promise%2520in%2520enhancing%2520social%250Askills%252C%2520motivation%252C%2520and%2520attention.%2520For%2520children%2520with%2520ASD%252C%2520accessible%2520and%250Aengaging%2520interventions%2520rely%2520heavily%2520on%2520effective%2520UI/UX%2520design%252C%2520but%2520there%2520is%250Astill%2520a%2520significant%2520lack%2520of%2520robotics-based%2520education%2520and%2520therapeutic%2520programs%250Aspecifically%2520tailored%2520for%2520autistic%2520children.%2520To%2520optimize%2520the%2520benefits%2520of%2520these%250Atechnologies%2520in%2520ASD%2520therapies%2520and%2520immersive%2520education%252C%2520the%2520study%2520emphasizes%2520the%250Aneed%2520for%2520additional%2520research%2520to%2520address%2520difficulties%2520related%2520to%2520customization%252C%250Aaccessibility%252C%2520and%2520integration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18162v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Nexus%20of%20AR/VR%2C%20AI%2C%20UI/UX%2C%20and%20Robotics%20Technologies%20in%20Enhancing%0A%20%20Learning%20and%20Social%20Interaction%20for%20Children%20with%20Autism%20Spectrum%20Disorders%3A%0A%20%20A%20Systematic%20Review&entry.906535625=Biplov%20Paneru&entry.1292438233=%20%20The%20emergence%20of%20large%20language%20models%20%28LLMs%29%2C%20augmented%20reality%20%28AR%29%2C%20and%0Auser%20interface/user%20experience%20%28UI/UX%29%20design%20in%20therapies%20for%20children%2C%0Aespecially%20with%20disorders%20like%20autism%20spectrum%20disorder%20%28ASD%29%2C%20is%20studied%20in%0Adetail%20in%20this%20review%20study.%20150%20publications%20were%20collected%20by%20a%20thorough%0Aliterature%20search%20throughout%20PubMed%2C%20ACM%2C%20IEEE%20Xplore%2C%20Elsevier%2C%20and%20Google%0AScholar%3B%2060%20of%20them%20were%20chosen%20based%20on%20their%20methodological%20rigor%20and%0Arelevance%20to%20the%20focus%20area.%20Three%20of%20the%20primary%20areas%20are%20studied%20and%20covered%0Ain%20this%20review%3A%20how%20AR%20can%20improve%20social%20and%20learning%20results%2C%20how%20LLMs%20can%0Asupport%20communication%2C%20and%20how%20UI/UX%20design%20affects%20how%20effective%20these%0Atechnologies%20can%20be.%20Results%20show%20that%20while%20LLMs%20can%20provide%20individualized%0Alearning%20and%20communication%20support%2C%20AR%20has%20shown%20promise%20in%20enhancing%20social%0Askills%2C%20motivation%2C%20and%20attention.%20For%20children%20with%20ASD%2C%20accessible%20and%0Aengaging%20interventions%20rely%20heavily%20on%20effective%20UI/UX%20design%2C%20but%20there%20is%0Astill%20a%20significant%20lack%20of%20robotics-based%20education%20and%20therapeutic%20programs%0Aspecifically%20tailored%20for%20autistic%20children.%20To%20optimize%20the%20benefits%20of%20these%0Atechnologies%20in%20ASD%20therapies%20and%20immersive%20education%2C%20the%20study%20emphasizes%20the%0Aneed%20for%20additional%20research%20to%20address%20difficulties%20related%20to%20customization%2C%0Aaccessibility%2C%20and%20integration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18162v3&entry.124074799=Read"},
{"title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager", "author": "Lucie Galland and Catherine Pelachaud and Florian Pecune", "abstract": "  In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals.\n", "link": "http://arxiv.org/abs/2506.19652v2", "date": "2025-07-08", "relevancy": 1.8962, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4872}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailored%20Conversations%20beyond%20LLMs%3A%20A%20RL-Based%20Dialogue%20Manager&body=Title%3A%20Tailored%20Conversations%20beyond%20LLMs%3A%20A%20RL-Based%20Dialogue%20Manager%0AAuthor%3A%20Lucie%20Galland%20and%20Catherine%20Pelachaud%20and%20Florian%20Pecune%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%20integrates%20large%20language%0Amodels%20%28LLMs%29%20with%20an%20RL-based%20dialogue%20manager%20for%20open-ended%20dialogue%20with%20a%0Aspecific%20goal.%20By%20leveraging%20hierarchical%20reinforcement%20learning%20to%20model%20the%0Astructured%20phases%20of%20dialogue%20and%20employ%20meta-learning%20to%20enhance%20adaptability%0Aacross%20diverse%20user%20profiles%2C%20our%20approach%20enhances%20adaptability%20and%0Aefficiency%2C%20enabling%20the%20system%20to%20learn%20from%20limited%20data%2C%20transition%20fluidly%0Abetween%20dialogue%20phases%2C%20and%20personalize%20responses%20to%20heterogeneous%20patient%0Aneeds.%20We%20apply%20our%20framework%20to%20Motivational%20Interviews%2C%20aiming%20to%20foster%0Abehavior%20change%2C%20and%20demonstrate%20that%20the%20proposed%20dialogue%20manager%20outperforms%0Aa%20state-of-the-art%20LLM%20baseline%20in%20terms%20of%20reward%2C%20showing%20a%20potential%20benefit%0Aof%20conditioning%20LLMs%20to%20create%20open-ended%20dialogue%20systems%20with%20specific%20goals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19652v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailored%2520Conversations%2520beyond%2520LLMs%253A%2520A%2520RL-Based%2520Dialogue%2520Manager%26entry.906535625%3DLucie%2520Galland%2520and%2520Catherine%2520Pelachaud%2520and%2520Florian%2520Pecune%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520integrates%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520with%2520an%2520RL-based%2520dialogue%2520manager%2520for%2520open-ended%2520dialogue%2520with%2520a%250Aspecific%2520goal.%2520By%2520leveraging%2520hierarchical%2520reinforcement%2520learning%2520to%2520model%2520the%250Astructured%2520phases%2520of%2520dialogue%2520and%2520employ%2520meta-learning%2520to%2520enhance%2520adaptability%250Aacross%2520diverse%2520user%2520profiles%252C%2520our%2520approach%2520enhances%2520adaptability%2520and%250Aefficiency%252C%2520enabling%2520the%2520system%2520to%2520learn%2520from%2520limited%2520data%252C%2520transition%2520fluidly%250Abetween%2520dialogue%2520phases%252C%2520and%2520personalize%2520responses%2520to%2520heterogeneous%2520patient%250Aneeds.%2520We%2520apply%2520our%2520framework%2520to%2520Motivational%2520Interviews%252C%2520aiming%2520to%2520foster%250Abehavior%2520change%252C%2520and%2520demonstrate%2520that%2520the%2520proposed%2520dialogue%2520manager%2520outperforms%250Aa%2520state-of-the-art%2520LLM%2520baseline%2520in%2520terms%2520of%2520reward%252C%2520showing%2520a%2520potential%2520benefit%250Aof%2520conditioning%2520LLMs%2520to%2520create%2520open-ended%2520dialogue%2520systems%2520with%2520specific%2520goals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19652v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailored%20Conversations%20beyond%20LLMs%3A%20A%20RL-Based%20Dialogue%20Manager&entry.906535625=Lucie%20Galland%20and%20Catherine%20Pelachaud%20and%20Florian%20Pecune&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%20integrates%20large%20language%0Amodels%20%28LLMs%29%20with%20an%20RL-based%20dialogue%20manager%20for%20open-ended%20dialogue%20with%20a%0Aspecific%20goal.%20By%20leveraging%20hierarchical%20reinforcement%20learning%20to%20model%20the%0Astructured%20phases%20of%20dialogue%20and%20employ%20meta-learning%20to%20enhance%20adaptability%0Aacross%20diverse%20user%20profiles%2C%20our%20approach%20enhances%20adaptability%20and%0Aefficiency%2C%20enabling%20the%20system%20to%20learn%20from%20limited%20data%2C%20transition%20fluidly%0Abetween%20dialogue%20phases%2C%20and%20personalize%20responses%20to%20heterogeneous%20patient%0Aneeds.%20We%20apply%20our%20framework%20to%20Motivational%20Interviews%2C%20aiming%20to%20foster%0Abehavior%20change%2C%20and%20demonstrate%20that%20the%20proposed%20dialogue%20manager%20outperforms%0Aa%20state-of-the-art%20LLM%20baseline%20in%20terms%20of%20reward%2C%20showing%20a%20potential%20benefit%0Aof%20conditioning%20LLMs%20to%20create%20open-ended%20dialogue%20systems%20with%20specific%20goals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19652v2&entry.124074799=Read"},
{"title": "CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL\n  Generation", "author": "Kushal Gajjar and Harshit Sikchi and Arpit Singh Gautam and Marc Hammons and Saurabh Jha", "abstract": "  Translating natural language into SQL (Text-to-SQL) remains a core challenge\nat the intersection of language understanding and structured data access.\nAlthough large language models (LLMs) have improved fluency, generating correct\nand executable SQL, especially for complex queries, continues to be\nchallenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)\nframework and model that produces accurate SQL using a lightweight reward\nsignal based on execution correctness and format-tag compliance. By avoiding\nintermediate supervision, hybrid pipelines and complex reward shaping, our\nmethod encourages stable learning and stronger alignment with the ultimate task\nobjective-producing executable programs. CogniSQL-R1-Zero achieves\nstate-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,\noutperforming prior supervised and instruction-tuned baselines including SFT\nCodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a\nsignificantly smaller 7B backbone. This result underscores the scalability and\nefficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs\n(40 GB VRAM each). To support further research in efficient and interpretable\nText-to-SQL modeling, we release two curated datasets: (i) a collection of\n5,024 reasoning traces with varying context lengths, and (ii) a\npositive-sampled corpus of 36,356 corpus of weakly supervised queries, each\nannotated with six semantically diverse reasoning paths. Together, these\ncontributions advance scalable, execution-aligned Text-to-SQL generation.\n", "link": "http://arxiv.org/abs/2507.06013v1", "date": "2025-07-08", "relevancy": 1.8841, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogniSQL-R1-Zero%3A%20Lightweight%20Reinforced%20Reasoning%20for%20Efficient%20SQL%0A%20%20Generation&body=Title%3A%20CogniSQL-R1-Zero%3A%20Lightweight%20Reinforced%20Reasoning%20for%20Efficient%20SQL%0A%20%20Generation%0AAuthor%3A%20Kushal%20Gajjar%20and%20Harshit%20Sikchi%20and%20Arpit%20Singh%20Gautam%20and%20Marc%20Hammons%20and%20Saurabh%20Jha%0AAbstract%3A%20%20%20Translating%20natural%20language%20into%20SQL%20%28Text-to-SQL%29%20remains%20a%20core%20challenge%0Aat%20the%20intersection%20of%20language%20understanding%20and%20structured%20data%20access.%0AAlthough%20large%20language%20models%20%28LLMs%29%20have%20improved%20fluency%2C%20generating%20correct%0Aand%20executable%20SQL%2C%20especially%20for%20complex%20queries%2C%20continues%20to%20be%0Achallenging.%20We%20introduce%20CogniSQL-R1-Zero%2C%20a%20reinforcement%20learning%20%28RL%29%0Aframework%20and%20model%20that%20produces%20accurate%20SQL%20using%20a%20lightweight%20reward%0Asignal%20based%20on%20execution%20correctness%20and%20format-tag%20compliance.%20By%20avoiding%0Aintermediate%20supervision%2C%20hybrid%20pipelines%20and%20complex%20reward%20shaping%2C%20our%0Amethod%20encourages%20stable%20learning%20and%20stronger%20alignment%20with%20the%20ultimate%20task%0Aobjective-producing%20executable%20programs.%20CogniSQL-R1-Zero%20achieves%0Astate-of-the-art%20execution%20accuracy%20on%20Text2SQL%20benchmark%3B%20BIRD%20bench%2C%0Aoutperforming%20prior%20supervised%20and%20instruction-tuned%20baselines%20including%20SFT%0ACodeS-7B%2C%20DeepSeek-Coder%20236B%2C%20and%20Mistral%20123B-despite%20being%20trained%20on%20a%0Asignificantly%20smaller%207B%20backbone.%20This%20result%20underscores%20the%20scalability%20and%0Aefficiency%20of%20our%20RL-based%20approach%20when%20trained%20on%20just%20four%20NVIDIA%20A100%20GPUs%0A%2840%20GB%20VRAM%20each%29.%20To%20support%20further%20research%20in%20efficient%20and%20interpretable%0AText-to-SQL%20modeling%2C%20we%20release%20two%20curated%20datasets%3A%20%28i%29%20a%20collection%20of%0A5%2C024%20reasoning%20traces%20with%20varying%20context%20lengths%2C%20and%20%28ii%29%20a%0Apositive-sampled%20corpus%20of%2036%2C356%20corpus%20of%20weakly%20supervised%20queries%2C%20each%0Aannotated%20with%20six%20semantically%20diverse%20reasoning%20paths.%20Together%2C%20these%0Acontributions%20advance%20scalable%2C%20execution-aligned%20Text-to-SQL%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogniSQL-R1-Zero%253A%2520Lightweight%2520Reinforced%2520Reasoning%2520for%2520Efficient%2520SQL%250A%2520%2520Generation%26entry.906535625%3DKushal%2520Gajjar%2520and%2520Harshit%2520Sikchi%2520and%2520Arpit%2520Singh%2520Gautam%2520and%2520Marc%2520Hammons%2520and%2520Saurabh%2520Jha%26entry.1292438233%3D%2520%2520Translating%2520natural%2520language%2520into%2520SQL%2520%2528Text-to-SQL%2529%2520remains%2520a%2520core%2520challenge%250Aat%2520the%2520intersection%2520of%2520language%2520understanding%2520and%2520structured%2520data%2520access.%250AAlthough%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520improved%2520fluency%252C%2520generating%2520correct%250Aand%2520executable%2520SQL%252C%2520especially%2520for%2520complex%2520queries%252C%2520continues%2520to%2520be%250Achallenging.%2520We%2520introduce%2520CogniSQL-R1-Zero%252C%2520a%2520reinforcement%2520learning%2520%2528RL%2529%250Aframework%2520and%2520model%2520that%2520produces%2520accurate%2520SQL%2520using%2520a%2520lightweight%2520reward%250Asignal%2520based%2520on%2520execution%2520correctness%2520and%2520format-tag%2520compliance.%2520By%2520avoiding%250Aintermediate%2520supervision%252C%2520hybrid%2520pipelines%2520and%2520complex%2520reward%2520shaping%252C%2520our%250Amethod%2520encourages%2520stable%2520learning%2520and%2520stronger%2520alignment%2520with%2520the%2520ultimate%2520task%250Aobjective-producing%2520executable%2520programs.%2520CogniSQL-R1-Zero%2520achieves%250Astate-of-the-art%2520execution%2520accuracy%2520on%2520Text2SQL%2520benchmark%253B%2520BIRD%2520bench%252C%250Aoutperforming%2520prior%2520supervised%2520and%2520instruction-tuned%2520baselines%2520including%2520SFT%250ACodeS-7B%252C%2520DeepSeek-Coder%2520236B%252C%2520and%2520Mistral%2520123B-despite%2520being%2520trained%2520on%2520a%250Asignificantly%2520smaller%25207B%2520backbone.%2520This%2520result%2520underscores%2520the%2520scalability%2520and%250Aefficiency%2520of%2520our%2520RL-based%2520approach%2520when%2520trained%2520on%2520just%2520four%2520NVIDIA%2520A100%2520GPUs%250A%252840%2520GB%2520VRAM%2520each%2529.%2520To%2520support%2520further%2520research%2520in%2520efficient%2520and%2520interpretable%250AText-to-SQL%2520modeling%252C%2520we%2520release%2520two%2520curated%2520datasets%253A%2520%2528i%2529%2520a%2520collection%2520of%250A5%252C024%2520reasoning%2520traces%2520with%2520varying%2520context%2520lengths%252C%2520and%2520%2528ii%2529%2520a%250Apositive-sampled%2520corpus%2520of%252036%252C356%2520corpus%2520of%2520weakly%2520supervised%2520queries%252C%2520each%250Aannotated%2520with%2520six%2520semantically%2520diverse%2520reasoning%2520paths.%2520Together%252C%2520these%250Acontributions%2520advance%2520scalable%252C%2520execution-aligned%2520Text-to-SQL%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogniSQL-R1-Zero%3A%20Lightweight%20Reinforced%20Reasoning%20for%20Efficient%20SQL%0A%20%20Generation&entry.906535625=Kushal%20Gajjar%20and%20Harshit%20Sikchi%20and%20Arpit%20Singh%20Gautam%20and%20Marc%20Hammons%20and%20Saurabh%20Jha&entry.1292438233=%20%20Translating%20natural%20language%20into%20SQL%20%28Text-to-SQL%29%20remains%20a%20core%20challenge%0Aat%20the%20intersection%20of%20language%20understanding%20and%20structured%20data%20access.%0AAlthough%20large%20language%20models%20%28LLMs%29%20have%20improved%20fluency%2C%20generating%20correct%0Aand%20executable%20SQL%2C%20especially%20for%20complex%20queries%2C%20continues%20to%20be%0Achallenging.%20We%20introduce%20CogniSQL-R1-Zero%2C%20a%20reinforcement%20learning%20%28RL%29%0Aframework%20and%20model%20that%20produces%20accurate%20SQL%20using%20a%20lightweight%20reward%0Asignal%20based%20on%20execution%20correctness%20and%20format-tag%20compliance.%20By%20avoiding%0Aintermediate%20supervision%2C%20hybrid%20pipelines%20and%20complex%20reward%20shaping%2C%20our%0Amethod%20encourages%20stable%20learning%20and%20stronger%20alignment%20with%20the%20ultimate%20task%0Aobjective-producing%20executable%20programs.%20CogniSQL-R1-Zero%20achieves%0Astate-of-the-art%20execution%20accuracy%20on%20Text2SQL%20benchmark%3B%20BIRD%20bench%2C%0Aoutperforming%20prior%20supervised%20and%20instruction-tuned%20baselines%20including%20SFT%0ACodeS-7B%2C%20DeepSeek-Coder%20236B%2C%20and%20Mistral%20123B-despite%20being%20trained%20on%20a%0Asignificantly%20smaller%207B%20backbone.%20This%20result%20underscores%20the%20scalability%20and%0Aefficiency%20of%20our%20RL-based%20approach%20when%20trained%20on%20just%20four%20NVIDIA%20A100%20GPUs%0A%2840%20GB%20VRAM%20each%29.%20To%20support%20further%20research%20in%20efficient%20and%20interpretable%0AText-to-SQL%20modeling%2C%20we%20release%20two%20curated%20datasets%3A%20%28i%29%20a%20collection%20of%0A5%2C024%20reasoning%20traces%20with%20varying%20context%20lengths%2C%20and%20%28ii%29%20a%0Apositive-sampled%20corpus%20of%2036%2C356%20corpus%20of%20weakly%20supervised%20queries%2C%20each%0Aannotated%20with%20six%20semantically%20diverse%20reasoning%20paths.%20Together%2C%20these%0Acontributions%20advance%20scalable%2C%20execution-aligned%20Text-to-SQL%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06013v1&entry.124074799=Read"},
{"title": "Conservative approximation-based feedforward neural network for WENO\n  schemes", "author": "Kwanghyuk Park and Jiaxi Gu and Jae-Hun Jung", "abstract": "  In this work, we present the feedforward neural network based on the\nconservative approximation to the derivative from point values, for the\nweighted essentially non-oscillatory (WENO) schemes in solving hyperbolic\nconservation laws. The feedforward neural network, whose inputs are point\nvalues from the three-point stencil and outputs are two nonlinear weights,\ntakes the place of the classical WENO weighting procedure. For the training\nphase, we employ the supervised learning and create a new labeled dataset for\none-dimensional conservative approximation, where we construct a numerical flux\nfunction from the given point values such that the flux difference approximates\nthe derivative to high-order accuracy. The symmetric-balancing term is\nintroduced for the loss function so that it propels the neural network to match\nthe conservative approximation to the derivative and satisfy the symmetric\nproperty that WENO3-JS and WENO3-Z have in common. The consequent WENO schemes,\nWENO3-CADNNs, demonstrate robust generalization across various benchmark\nscenarios and resolutions, where they outperform WENO3-Z and achieve accuracy\ncomparable to WENO5-JS.\n", "link": "http://arxiv.org/abs/2507.06190v1", "date": "2025-07-08", "relevancy": 1.8687, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4852}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4619}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conservative%20approximation-based%20feedforward%20neural%20network%20for%20WENO%0A%20%20schemes&body=Title%3A%20Conservative%20approximation-based%20feedforward%20neural%20network%20for%20WENO%0A%20%20schemes%0AAuthor%3A%20Kwanghyuk%20Park%20and%20Jiaxi%20Gu%20and%20Jae-Hun%20Jung%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20the%20feedforward%20neural%20network%20based%20on%20the%0Aconservative%20approximation%20to%20the%20derivative%20from%20point%20values%2C%20for%20the%0Aweighted%20essentially%20non-oscillatory%20%28WENO%29%20schemes%20in%20solving%20hyperbolic%0Aconservation%20laws.%20The%20feedforward%20neural%20network%2C%20whose%20inputs%20are%20point%0Avalues%20from%20the%20three-point%20stencil%20and%20outputs%20are%20two%20nonlinear%20weights%2C%0Atakes%20the%20place%20of%20the%20classical%20WENO%20weighting%20procedure.%20For%20the%20training%0Aphase%2C%20we%20employ%20the%20supervised%20learning%20and%20create%20a%20new%20labeled%20dataset%20for%0Aone-dimensional%20conservative%20approximation%2C%20where%20we%20construct%20a%20numerical%20flux%0Afunction%20from%20the%20given%20point%20values%20such%20that%20the%20flux%20difference%20approximates%0Athe%20derivative%20to%20high-order%20accuracy.%20The%20symmetric-balancing%20term%20is%0Aintroduced%20for%20the%20loss%20function%20so%20that%20it%20propels%20the%20neural%20network%20to%20match%0Athe%20conservative%20approximation%20to%20the%20derivative%20and%20satisfy%20the%20symmetric%0Aproperty%20that%20WENO3-JS%20and%20WENO3-Z%20have%20in%20common.%20The%20consequent%20WENO%20schemes%2C%0AWENO3-CADNNs%2C%20demonstrate%20robust%20generalization%20across%20various%20benchmark%0Ascenarios%20and%20resolutions%2C%20where%20they%20outperform%20WENO3-Z%20and%20achieve%20accuracy%0Acomparable%20to%20WENO5-JS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConservative%2520approximation-based%2520feedforward%2520neural%2520network%2520for%2520WENO%250A%2520%2520schemes%26entry.906535625%3DKwanghyuk%2520Park%2520and%2520Jiaxi%2520Gu%2520and%2520Jae-Hun%2520Jung%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520feedforward%2520neural%2520network%2520based%2520on%2520the%250Aconservative%2520approximation%2520to%2520the%2520derivative%2520from%2520point%2520values%252C%2520for%2520the%250Aweighted%2520essentially%2520non-oscillatory%2520%2528WENO%2529%2520schemes%2520in%2520solving%2520hyperbolic%250Aconservation%2520laws.%2520The%2520feedforward%2520neural%2520network%252C%2520whose%2520inputs%2520are%2520point%250Avalues%2520from%2520the%2520three-point%2520stencil%2520and%2520outputs%2520are%2520two%2520nonlinear%2520weights%252C%250Atakes%2520the%2520place%2520of%2520the%2520classical%2520WENO%2520weighting%2520procedure.%2520For%2520the%2520training%250Aphase%252C%2520we%2520employ%2520the%2520supervised%2520learning%2520and%2520create%2520a%2520new%2520labeled%2520dataset%2520for%250Aone-dimensional%2520conservative%2520approximation%252C%2520where%2520we%2520construct%2520a%2520numerical%2520flux%250Afunction%2520from%2520the%2520given%2520point%2520values%2520such%2520that%2520the%2520flux%2520difference%2520approximates%250Athe%2520derivative%2520to%2520high-order%2520accuracy.%2520The%2520symmetric-balancing%2520term%2520is%250Aintroduced%2520for%2520the%2520loss%2520function%2520so%2520that%2520it%2520propels%2520the%2520neural%2520network%2520to%2520match%250Athe%2520conservative%2520approximation%2520to%2520the%2520derivative%2520and%2520satisfy%2520the%2520symmetric%250Aproperty%2520that%2520WENO3-JS%2520and%2520WENO3-Z%2520have%2520in%2520common.%2520The%2520consequent%2520WENO%2520schemes%252C%250AWENO3-CADNNs%252C%2520demonstrate%2520robust%2520generalization%2520across%2520various%2520benchmark%250Ascenarios%2520and%2520resolutions%252C%2520where%2520they%2520outperform%2520WENO3-Z%2520and%2520achieve%2520accuracy%250Acomparable%2520to%2520WENO5-JS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conservative%20approximation-based%20feedforward%20neural%20network%20for%20WENO%0A%20%20schemes&entry.906535625=Kwanghyuk%20Park%20and%20Jiaxi%20Gu%20and%20Jae-Hun%20Jung&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20the%20feedforward%20neural%20network%20based%20on%20the%0Aconservative%20approximation%20to%20the%20derivative%20from%20point%20values%2C%20for%20the%0Aweighted%20essentially%20non-oscillatory%20%28WENO%29%20schemes%20in%20solving%20hyperbolic%0Aconservation%20laws.%20The%20feedforward%20neural%20network%2C%20whose%20inputs%20are%20point%0Avalues%20from%20the%20three-point%20stencil%20and%20outputs%20are%20two%20nonlinear%20weights%2C%0Atakes%20the%20place%20of%20the%20classical%20WENO%20weighting%20procedure.%20For%20the%20training%0Aphase%2C%20we%20employ%20the%20supervised%20learning%20and%20create%20a%20new%20labeled%20dataset%20for%0Aone-dimensional%20conservative%20approximation%2C%20where%20we%20construct%20a%20numerical%20flux%0Afunction%20from%20the%20given%20point%20values%20such%20that%20the%20flux%20difference%20approximates%0Athe%20derivative%20to%20high-order%20accuracy.%20The%20symmetric-balancing%20term%20is%0Aintroduced%20for%20the%20loss%20function%20so%20that%20it%20propels%20the%20neural%20network%20to%20match%0Athe%20conservative%20approximation%20to%20the%20derivative%20and%20satisfy%20the%20symmetric%0Aproperty%20that%20WENO3-JS%20and%20WENO3-Z%20have%20in%20common.%20The%20consequent%20WENO%20schemes%2C%0AWENO3-CADNNs%2C%20demonstrate%20robust%20generalization%20across%20various%20benchmark%0Ascenarios%20and%20resolutions%2C%20where%20they%20outperform%20WENO3-Z%20and%20achieve%20accuracy%0Acomparable%20to%20WENO5-JS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06190v1&entry.124074799=Read"},
{"title": "Classification of autoimmune diseases from Peripheral blood TCR\n  repertoires by multimodal multi-instance learning", "author": "Ruihao Zhang and Fei Ye and Dandan Meng and Yixuan Huang and  Maochen and Xiao Liu", "abstract": "  T cell receptor (TCR) repertoires encode critical immunological signatures\nfor autoimmune diseases, yet their clinical application remains limited by\nsequence sparsity and low witness rates. We developed EAMil, a multi-instance\ndeep learning framework that leverages TCR sequencing data to diagnose systemic\nlupus erythematosus (SLE) and rheumatoid arthritis (RA) with exceptional\naccuracy. By integrating PrimeSeq feature extraction with ESMonehot encoding\nand enhanced gate attention mechanisms, our model achieved state-of-the-art\nperformance with AUCs of 98.95% for SLE and 97.76% for RA. EAMil successfully\nidentified disease-associated genes with over 90% concordance with established\ndifferential analyses and effectively distinguished disease-specific TCR genes.\nThe model demonstrated robustness in classifying multiple disease categories,\nutilizing the SLEDAI score to stratify SLE patients by disease severity as well\nas to diagnose the site of damage in SLE patients, and effectively controlling\nfor confounding factors such as age and gender. This interpretable framework\nfor immune receptor analysis provides new insights for autoimmune disease\ndetection and classification with broad potential clinical applications across\nimmune-mediated conditions.\n", "link": "http://arxiv.org/abs/2507.04981v2", "date": "2025-07-08", "relevancy": 1.8645, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4708}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4658}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20autoimmune%20diseases%20from%20Peripheral%20blood%20TCR%0A%20%20repertoires%20by%20multimodal%20multi-instance%20learning&body=Title%3A%20Classification%20of%20autoimmune%20diseases%20from%20Peripheral%20blood%20TCR%0A%20%20repertoires%20by%20multimodal%20multi-instance%20learning%0AAuthor%3A%20Ruihao%20Zhang%20and%20Fei%20Ye%20and%20Dandan%20Meng%20and%20Yixuan%20Huang%20and%20%20Maochen%20and%20Xiao%20Liu%0AAbstract%3A%20%20%20T%20cell%20receptor%20%28TCR%29%20repertoires%20encode%20critical%20immunological%20signatures%0Afor%20autoimmune%20diseases%2C%20yet%20their%20clinical%20application%20remains%20limited%20by%0Asequence%20sparsity%20and%20low%20witness%20rates.%20We%20developed%20EAMil%2C%20a%20multi-instance%0Adeep%20learning%20framework%20that%20leverages%20TCR%20sequencing%20data%20to%20diagnose%20systemic%0Alupus%20erythematosus%20%28SLE%29%20and%20rheumatoid%20arthritis%20%28RA%29%20with%20exceptional%0Aaccuracy.%20By%20integrating%20PrimeSeq%20feature%20extraction%20with%20ESMonehot%20encoding%0Aand%20enhanced%20gate%20attention%20mechanisms%2C%20our%20model%20achieved%20state-of-the-art%0Aperformance%20with%20AUCs%20of%2098.95%25%20for%20SLE%20and%2097.76%25%20for%20RA.%20EAMil%20successfully%0Aidentified%20disease-associated%20genes%20with%20over%2090%25%20concordance%20with%20established%0Adifferential%20analyses%20and%20effectively%20distinguished%20disease-specific%20TCR%20genes.%0AThe%20model%20demonstrated%20robustness%20in%20classifying%20multiple%20disease%20categories%2C%0Autilizing%20the%20SLEDAI%20score%20to%20stratify%20SLE%20patients%20by%20disease%20severity%20as%20well%0Aas%20to%20diagnose%20the%20site%20of%20damage%20in%20SLE%20patients%2C%20and%20effectively%20controlling%0Afor%20confounding%20factors%20such%20as%20age%20and%20gender.%20This%20interpretable%20framework%0Afor%20immune%20receptor%20analysis%20provides%20new%20insights%20for%20autoimmune%20disease%0Adetection%20and%20classification%20with%20broad%20potential%20clinical%20applications%20across%0Aimmune-mediated%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04981v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520autoimmune%2520diseases%2520from%2520Peripheral%2520blood%2520TCR%250A%2520%2520repertoires%2520by%2520multimodal%2520multi-instance%2520learning%26entry.906535625%3DRuihao%2520Zhang%2520and%2520Fei%2520Ye%2520and%2520Dandan%2520Meng%2520and%2520Yixuan%2520Huang%2520and%2520%2520Maochen%2520and%2520Xiao%2520Liu%26entry.1292438233%3D%2520%2520T%2520cell%2520receptor%2520%2528TCR%2529%2520repertoires%2520encode%2520critical%2520immunological%2520signatures%250Afor%2520autoimmune%2520diseases%252C%2520yet%2520their%2520clinical%2520application%2520remains%2520limited%2520by%250Asequence%2520sparsity%2520and%2520low%2520witness%2520rates.%2520We%2520developed%2520EAMil%252C%2520a%2520multi-instance%250Adeep%2520learning%2520framework%2520that%2520leverages%2520TCR%2520sequencing%2520data%2520to%2520diagnose%2520systemic%250Alupus%2520erythematosus%2520%2528SLE%2529%2520and%2520rheumatoid%2520arthritis%2520%2528RA%2529%2520with%2520exceptional%250Aaccuracy.%2520By%2520integrating%2520PrimeSeq%2520feature%2520extraction%2520with%2520ESMonehot%2520encoding%250Aand%2520enhanced%2520gate%2520attention%2520mechanisms%252C%2520our%2520model%2520achieved%2520state-of-the-art%250Aperformance%2520with%2520AUCs%2520of%252098.95%2525%2520for%2520SLE%2520and%252097.76%2525%2520for%2520RA.%2520EAMil%2520successfully%250Aidentified%2520disease-associated%2520genes%2520with%2520over%252090%2525%2520concordance%2520with%2520established%250Adifferential%2520analyses%2520and%2520effectively%2520distinguished%2520disease-specific%2520TCR%2520genes.%250AThe%2520model%2520demonstrated%2520robustness%2520in%2520classifying%2520multiple%2520disease%2520categories%252C%250Autilizing%2520the%2520SLEDAI%2520score%2520to%2520stratify%2520SLE%2520patients%2520by%2520disease%2520severity%2520as%2520well%250Aas%2520to%2520diagnose%2520the%2520site%2520of%2520damage%2520in%2520SLE%2520patients%252C%2520and%2520effectively%2520controlling%250Afor%2520confounding%2520factors%2520such%2520as%2520age%2520and%2520gender.%2520This%2520interpretable%2520framework%250Afor%2520immune%2520receptor%2520analysis%2520provides%2520new%2520insights%2520for%2520autoimmune%2520disease%250Adetection%2520and%2520classification%2520with%2520broad%2520potential%2520clinical%2520applications%2520across%250Aimmune-mediated%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04981v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20autoimmune%20diseases%20from%20Peripheral%20blood%20TCR%0A%20%20repertoires%20by%20multimodal%20multi-instance%20learning&entry.906535625=Ruihao%20Zhang%20and%20Fei%20Ye%20and%20Dandan%20Meng%20and%20Yixuan%20Huang%20and%20%20Maochen%20and%20Xiao%20Liu&entry.1292438233=%20%20T%20cell%20receptor%20%28TCR%29%20repertoires%20encode%20critical%20immunological%20signatures%0Afor%20autoimmune%20diseases%2C%20yet%20their%20clinical%20application%20remains%20limited%20by%0Asequence%20sparsity%20and%20low%20witness%20rates.%20We%20developed%20EAMil%2C%20a%20multi-instance%0Adeep%20learning%20framework%20that%20leverages%20TCR%20sequencing%20data%20to%20diagnose%20systemic%0Alupus%20erythematosus%20%28SLE%29%20and%20rheumatoid%20arthritis%20%28RA%29%20with%20exceptional%0Aaccuracy.%20By%20integrating%20PrimeSeq%20feature%20extraction%20with%20ESMonehot%20encoding%0Aand%20enhanced%20gate%20attention%20mechanisms%2C%20our%20model%20achieved%20state-of-the-art%0Aperformance%20with%20AUCs%20of%2098.95%25%20for%20SLE%20and%2097.76%25%20for%20RA.%20EAMil%20successfully%0Aidentified%20disease-associated%20genes%20with%20over%2090%25%20concordance%20with%20established%0Adifferential%20analyses%20and%20effectively%20distinguished%20disease-specific%20TCR%20genes.%0AThe%20model%20demonstrated%20robustness%20in%20classifying%20multiple%20disease%20categories%2C%0Autilizing%20the%20SLEDAI%20score%20to%20stratify%20SLE%20patients%20by%20disease%20severity%20as%20well%0Aas%20to%20diagnose%20the%20site%20of%20damage%20in%20SLE%20patients%2C%20and%20effectively%20controlling%0Afor%20confounding%20factors%20such%20as%20age%20and%20gender.%20This%20interpretable%20framework%0Afor%20immune%20receptor%20analysis%20provides%20new%20insights%20for%20autoimmune%20disease%0Adetection%20and%20classification%20with%20broad%20potential%20clinical%20applications%20across%0Aimmune-mediated%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04981v2&entry.124074799=Read"},
{"title": "GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit\n  Geometry and Mitigating Barren Plateaus", "author": "Marwan Ait Haddou and Mohamed Bennai", "abstract": "  Variational Quantum Algorithms (VQAs) offer potential for near-term quantum\nadvantage but face challenges from barren plateaus, where gradients vanish, and\npoorly conditioned optimization landscapes. We introduce GuiderNet, a\nmeta-learning framework that conditions Parameterized Quantum Circuits (PQCs)\nusing data-dependent parameter shifts aimed at minimizing the log condition\nnumber of the Fubini-Study metric tensor. Implemented as a classical neural\nnetwork, GuiderNet is meta-trained to guide PQC parameters into geometrically\nfavorable regions and is embedded within hybrid quantum-classical pipelines to\nsteer both initialization and adaptive modulation during training.\n  Applied to the Kaggle Diabetes classification task, GuiderNet reduces\ncumulative training loss by over 5x, improves test accuracy from 75.3% to\n98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also\nsuppresses gradient explosion and stabilizes parameter updates, enabling\nsmoother and more robust optimization. These results demonstrate that geometric\nmeta-conditioning can mitigate barren plateaus and ill-conditioning, providing\na scalable approach to enhance trainability and generalization in quantum\nmachine learning.\n", "link": "http://arxiv.org/abs/2506.21940v2", "date": "2025-07-08", "relevancy": 1.8618, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4676}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4663}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GuiderNet%3A%20A%20Meta-Learning%20Framework%20for%20Optimizing%20Quantum%20Circuit%0A%20%20Geometry%20and%20Mitigating%20Barren%20Plateaus&body=Title%3A%20GuiderNet%3A%20A%20Meta-Learning%20Framework%20for%20Optimizing%20Quantum%20Circuit%0A%20%20Geometry%20and%20Mitigating%20Barren%20Plateaus%0AAuthor%3A%20Marwan%20Ait%20Haddou%20and%20Mohamed%20Bennai%0AAbstract%3A%20%20%20Variational%20Quantum%20Algorithms%20%28VQAs%29%20offer%20potential%20for%20near-term%20quantum%0Aadvantage%20but%20face%20challenges%20from%20barren%20plateaus%2C%20where%20gradients%20vanish%2C%20and%0Apoorly%20conditioned%20optimization%20landscapes.%20We%20introduce%20GuiderNet%2C%20a%0Ameta-learning%20framework%20that%20conditions%20Parameterized%20Quantum%20Circuits%20%28PQCs%29%0Ausing%20data-dependent%20parameter%20shifts%20aimed%20at%20minimizing%20the%20log%20condition%0Anumber%20of%20the%20Fubini-Study%20metric%20tensor.%20Implemented%20as%20a%20classical%20neural%0Anetwork%2C%20GuiderNet%20is%20meta-trained%20to%20guide%20PQC%20parameters%20into%20geometrically%0Afavorable%20regions%20and%20is%20embedded%20within%20hybrid%20quantum-classical%20pipelines%20to%0Asteer%20both%20initialization%20and%20adaptive%20modulation%20during%20training.%0A%20%20Applied%20to%20the%20Kaggle%20Diabetes%20classification%20task%2C%20GuiderNet%20reduces%0Acumulative%20training%20loss%20by%20over%205x%2C%20improves%20test%20accuracy%20from%2075.3%25%20to%0A98.6%25%2C%20and%20increases%20the%20minority-class%20F1%20score%20from%200.67%20to%200.95.%20It%20also%0Asuppresses%20gradient%20explosion%20and%20stabilizes%20parameter%20updates%2C%20enabling%0Asmoother%20and%20more%20robust%20optimization.%20These%20results%20demonstrate%20that%20geometric%0Ameta-conditioning%20can%20mitigate%20barren%20plateaus%20and%20ill-conditioning%2C%20providing%0Aa%20scalable%20approach%20to%20enhance%20trainability%20and%20generalization%20in%20quantum%0Amachine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21940v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuiderNet%253A%2520A%2520Meta-Learning%2520Framework%2520for%2520Optimizing%2520Quantum%2520Circuit%250A%2520%2520Geometry%2520and%2520Mitigating%2520Barren%2520Plateaus%26entry.906535625%3DMarwan%2520Ait%2520Haddou%2520and%2520Mohamed%2520Bennai%26entry.1292438233%3D%2520%2520Variational%2520Quantum%2520Algorithms%2520%2528VQAs%2529%2520offer%2520potential%2520for%2520near-term%2520quantum%250Aadvantage%2520but%2520face%2520challenges%2520from%2520barren%2520plateaus%252C%2520where%2520gradients%2520vanish%252C%2520and%250Apoorly%2520conditioned%2520optimization%2520landscapes.%2520We%2520introduce%2520GuiderNet%252C%2520a%250Ameta-learning%2520framework%2520that%2520conditions%2520Parameterized%2520Quantum%2520Circuits%2520%2528PQCs%2529%250Ausing%2520data-dependent%2520parameter%2520shifts%2520aimed%2520at%2520minimizing%2520the%2520log%2520condition%250Anumber%2520of%2520the%2520Fubini-Study%2520metric%2520tensor.%2520Implemented%2520as%2520a%2520classical%2520neural%250Anetwork%252C%2520GuiderNet%2520is%2520meta-trained%2520to%2520guide%2520PQC%2520parameters%2520into%2520geometrically%250Afavorable%2520regions%2520and%2520is%2520embedded%2520within%2520hybrid%2520quantum-classical%2520pipelines%2520to%250Asteer%2520both%2520initialization%2520and%2520adaptive%2520modulation%2520during%2520training.%250A%2520%2520Applied%2520to%2520the%2520Kaggle%2520Diabetes%2520classification%2520task%252C%2520GuiderNet%2520reduces%250Acumulative%2520training%2520loss%2520by%2520over%25205x%252C%2520improves%2520test%2520accuracy%2520from%252075.3%2525%2520to%250A98.6%2525%252C%2520and%2520increases%2520the%2520minority-class%2520F1%2520score%2520from%25200.67%2520to%25200.95.%2520It%2520also%250Asuppresses%2520gradient%2520explosion%2520and%2520stabilizes%2520parameter%2520updates%252C%2520enabling%250Asmoother%2520and%2520more%2520robust%2520optimization.%2520These%2520results%2520demonstrate%2520that%2520geometric%250Ameta-conditioning%2520can%2520mitigate%2520barren%2520plateaus%2520and%2520ill-conditioning%252C%2520providing%250Aa%2520scalable%2520approach%2520to%2520enhance%2520trainability%2520and%2520generalization%2520in%2520quantum%250Amachine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21940v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GuiderNet%3A%20A%20Meta-Learning%20Framework%20for%20Optimizing%20Quantum%20Circuit%0A%20%20Geometry%20and%20Mitigating%20Barren%20Plateaus&entry.906535625=Marwan%20Ait%20Haddou%20and%20Mohamed%20Bennai&entry.1292438233=%20%20Variational%20Quantum%20Algorithms%20%28VQAs%29%20offer%20potential%20for%20near-term%20quantum%0Aadvantage%20but%20face%20challenges%20from%20barren%20plateaus%2C%20where%20gradients%20vanish%2C%20and%0Apoorly%20conditioned%20optimization%20landscapes.%20We%20introduce%20GuiderNet%2C%20a%0Ameta-learning%20framework%20that%20conditions%20Parameterized%20Quantum%20Circuits%20%28PQCs%29%0Ausing%20data-dependent%20parameter%20shifts%20aimed%20at%20minimizing%20the%20log%20condition%0Anumber%20of%20the%20Fubini-Study%20metric%20tensor.%20Implemented%20as%20a%20classical%20neural%0Anetwork%2C%20GuiderNet%20is%20meta-trained%20to%20guide%20PQC%20parameters%20into%20geometrically%0Afavorable%20regions%20and%20is%20embedded%20within%20hybrid%20quantum-classical%20pipelines%20to%0Asteer%20both%20initialization%20and%20adaptive%20modulation%20during%20training.%0A%20%20Applied%20to%20the%20Kaggle%20Diabetes%20classification%20task%2C%20GuiderNet%20reduces%0Acumulative%20training%20loss%20by%20over%205x%2C%20improves%20test%20accuracy%20from%2075.3%25%20to%0A98.6%25%2C%20and%20increases%20the%20minority-class%20F1%20score%20from%200.67%20to%200.95.%20It%20also%0Asuppresses%20gradient%20explosion%20and%20stabilizes%20parameter%20updates%2C%20enabling%0Asmoother%20and%20more%20robust%20optimization.%20These%20results%20demonstrate%20that%20geometric%0Ameta-conditioning%20can%20mitigate%20barren%20plateaus%20and%20ill-conditioning%2C%20providing%0Aa%20scalable%20approach%20to%20enhance%20trainability%20and%20generalization%20in%20quantum%0Amachine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21940v2&entry.124074799=Read"},
{"title": "OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion\n  Models for Improved Intent Understanding", "author": "Kun Li and Jianhui Wang and Miao Zhang and Xueqian Wang", "abstract": "  Generative AI has significantly advanced text-driven image generation, but it\nstill faces challenges in producing outputs that consistently align with\nevolving user preferences and intents, particularly in multi-turn dialogue\nscenarios. In this research, We present a Visual Co-Adaptation (VCA) framework\nthat incorporates human-in-the-loop feedback, utilizing a well-trained reward\nmodel specifically designed to closely align with human preferences. Using a\ndiverse multi-turn dialogue dataset, the framework applies multiple reward\nfunctions (such as diversity, consistency, and preference feedback) to refine\nthe diffusion model through LoRA, effectively optimizing image generation based\non user input. We also constructed multi-round dialogue datasets with prompts\nand image pairs that well-fit user intent. Experiments show the model achieves\n508 wins in human evaluation, outperforming DALL-E 3 (463 wins) and others. It\nalso achieves 3.4 rounds in dialogue efficiency (vs. 13.7 for DALL-E 3) and\nexcels in metrics like LPIPS (0.15) and BLIP (0.59). Various experiments\ndemonstrate the effectiveness of the proposed method over state-of-the-art\nbaselines, with significant improvements in image consistency and alignment\nwith user intent.\n", "link": "http://arxiv.org/abs/2503.17660v3", "date": "2025-07-08", "relevancy": 1.8491, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6324}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6178}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OMR-Diffusion%3AOptimizing%20Multi-Round%20Enhanced%20Training%20in%20Diffusion%0A%20%20Models%20for%20Improved%20Intent%20Understanding&body=Title%3A%20OMR-Diffusion%3AOptimizing%20Multi-Round%20Enhanced%20Training%20in%20Diffusion%0A%20%20Models%20for%20Improved%20Intent%20Understanding%0AAuthor%3A%20Kun%20Li%20and%20Jianhui%20Wang%20and%20Miao%20Zhang%20and%20Xueqian%20Wang%0AAbstract%3A%20%20%20Generative%20AI%20has%20significantly%20advanced%20text-driven%20image%20generation%2C%20but%20it%0Astill%20faces%20challenges%20in%20producing%20outputs%20that%20consistently%20align%20with%0Aevolving%20user%20preferences%20and%20intents%2C%20particularly%20in%20multi-turn%20dialogue%0Ascenarios.%20In%20this%20research%2C%20We%20present%20a%20Visual%20Co-Adaptation%20%28VCA%29%20framework%0Athat%20incorporates%20human-in-the-loop%20feedback%2C%20utilizing%20a%20well-trained%20reward%0Amodel%20specifically%20designed%20to%20closely%20align%20with%20human%20preferences.%20Using%20a%0Adiverse%20multi-turn%20dialogue%20dataset%2C%20the%20framework%20applies%20multiple%20reward%0Afunctions%20%28such%20as%20diversity%2C%20consistency%2C%20and%20preference%20feedback%29%20to%20refine%0Athe%20diffusion%20model%20through%20LoRA%2C%20effectively%20optimizing%20image%20generation%20based%0Aon%20user%20input.%20We%20also%20constructed%20multi-round%20dialogue%20datasets%20with%20prompts%0Aand%20image%20pairs%20that%20well-fit%20user%20intent.%20Experiments%20show%20the%20model%20achieves%0A508%20wins%20in%20human%20evaluation%2C%20outperforming%20DALL-E%203%20%28463%20wins%29%20and%20others.%20It%0Aalso%20achieves%203.4%20rounds%20in%20dialogue%20efficiency%20%28vs.%2013.7%20for%20DALL-E%203%29%20and%0Aexcels%20in%20metrics%20like%20LPIPS%20%280.15%29%20and%20BLIP%20%280.59%29.%20Various%20experiments%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20over%20state-of-the-art%0Abaselines%2C%20with%20significant%20improvements%20in%20image%20consistency%20and%20alignment%0Awith%20user%20intent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17660v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOMR-Diffusion%253AOptimizing%2520Multi-Round%2520Enhanced%2520Training%2520in%2520Diffusion%250A%2520%2520Models%2520for%2520Improved%2520Intent%2520Understanding%26entry.906535625%3DKun%2520Li%2520and%2520Jianhui%2520Wang%2520and%2520Miao%2520Zhang%2520and%2520Xueqian%2520Wang%26entry.1292438233%3D%2520%2520Generative%2520AI%2520has%2520significantly%2520advanced%2520text-driven%2520image%2520generation%252C%2520but%2520it%250Astill%2520faces%2520challenges%2520in%2520producing%2520outputs%2520that%2520consistently%2520align%2520with%250Aevolving%2520user%2520preferences%2520and%2520intents%252C%2520particularly%2520in%2520multi-turn%2520dialogue%250Ascenarios.%2520In%2520this%2520research%252C%2520We%2520present%2520a%2520Visual%2520Co-Adaptation%2520%2528VCA%2529%2520framework%250Athat%2520incorporates%2520human-in-the-loop%2520feedback%252C%2520utilizing%2520a%2520well-trained%2520reward%250Amodel%2520specifically%2520designed%2520to%2520closely%2520align%2520with%2520human%2520preferences.%2520Using%2520a%250Adiverse%2520multi-turn%2520dialogue%2520dataset%252C%2520the%2520framework%2520applies%2520multiple%2520reward%250Afunctions%2520%2528such%2520as%2520diversity%252C%2520consistency%252C%2520and%2520preference%2520feedback%2529%2520to%2520refine%250Athe%2520diffusion%2520model%2520through%2520LoRA%252C%2520effectively%2520optimizing%2520image%2520generation%2520based%250Aon%2520user%2520input.%2520We%2520also%2520constructed%2520multi-round%2520dialogue%2520datasets%2520with%2520prompts%250Aand%2520image%2520pairs%2520that%2520well-fit%2520user%2520intent.%2520Experiments%2520show%2520the%2520model%2520achieves%250A508%2520wins%2520in%2520human%2520evaluation%252C%2520outperforming%2520DALL-E%25203%2520%2528463%2520wins%2529%2520and%2520others.%2520It%250Aalso%2520achieves%25203.4%2520rounds%2520in%2520dialogue%2520efficiency%2520%2528vs.%252013.7%2520for%2520DALL-E%25203%2529%2520and%250Aexcels%2520in%2520metrics%2520like%2520LPIPS%2520%25280.15%2529%2520and%2520BLIP%2520%25280.59%2529.%2520Various%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520over%2520state-of-the-art%250Abaselines%252C%2520with%2520significant%2520improvements%2520in%2520image%2520consistency%2520and%2520alignment%250Awith%2520user%2520intent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17660v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMR-Diffusion%3AOptimizing%20Multi-Round%20Enhanced%20Training%20in%20Diffusion%0A%20%20Models%20for%20Improved%20Intent%20Understanding&entry.906535625=Kun%20Li%20and%20Jianhui%20Wang%20and%20Miao%20Zhang%20and%20Xueqian%20Wang&entry.1292438233=%20%20Generative%20AI%20has%20significantly%20advanced%20text-driven%20image%20generation%2C%20but%20it%0Astill%20faces%20challenges%20in%20producing%20outputs%20that%20consistently%20align%20with%0Aevolving%20user%20preferences%20and%20intents%2C%20particularly%20in%20multi-turn%20dialogue%0Ascenarios.%20In%20this%20research%2C%20We%20present%20a%20Visual%20Co-Adaptation%20%28VCA%29%20framework%0Athat%20incorporates%20human-in-the-loop%20feedback%2C%20utilizing%20a%20well-trained%20reward%0Amodel%20specifically%20designed%20to%20closely%20align%20with%20human%20preferences.%20Using%20a%0Adiverse%20multi-turn%20dialogue%20dataset%2C%20the%20framework%20applies%20multiple%20reward%0Afunctions%20%28such%20as%20diversity%2C%20consistency%2C%20and%20preference%20feedback%29%20to%20refine%0Athe%20diffusion%20model%20through%20LoRA%2C%20effectively%20optimizing%20image%20generation%20based%0Aon%20user%20input.%20We%20also%20constructed%20multi-round%20dialogue%20datasets%20with%20prompts%0Aand%20image%20pairs%20that%20well-fit%20user%20intent.%20Experiments%20show%20the%20model%20achieves%0A508%20wins%20in%20human%20evaluation%2C%20outperforming%20DALL-E%203%20%28463%20wins%29%20and%20others.%20It%0Aalso%20achieves%203.4%20rounds%20in%20dialogue%20efficiency%20%28vs.%2013.7%20for%20DALL-E%203%29%20and%0Aexcels%20in%20metrics%20like%20LPIPS%20%280.15%29%20and%20BLIP%20%280.59%29.%20Various%20experiments%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20over%20state-of-the-art%0Abaselines%2C%20with%20significant%20improvements%20in%20image%20consistency%20and%20alignment%0Awith%20user%20intent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17660v3&entry.124074799=Read"},
{"title": "Hierarchical Vision-Language Planning for Multi-Step Humanoid\n  Manipulation", "author": "Andr\u00e9 Schakkal and Ben Zandonati and Zhutian Yang and Navid Azizan", "abstract": "  Enabling humanoid robots to reliably execute complex multi-step manipulation\ntasks is crucial for their effective deployment in industrial and household\nenvironments. This paper presents a hierarchical planning and control framework\ndesigned to achieve reliable multi-step humanoid manipulation. The proposed\nsystem comprises three layers: (1) a low-level RL-based controller responsible\nfor tracking whole-body motion targets; (2) a mid-level set of skill policies\ntrained via imitation learning that produce motion targets for different steps\nof a task; and (3) a high-level vision-language planning module that determines\nwhich skills should be executed and also monitors their completion in real-time\nusing pretrained vision-language models (VLMs). Experimental validation is\nperformed on a Unitree G1 humanoid robot executing a non-prehensile\npick-and-place task. Over 40 real-world trials, the hierarchical system\nachieved a 73% success rate in completing the full manipulation sequence. These\nexperiments confirm the feasibility of the proposed hierarchical system,\nhighlighting the benefits of VLM-based skill planning and monitoring for\nmulti-step manipulation scenarios. See https://vlp-humanoid.github.io/ for\nvideo demonstrations of the policy rollout.\n", "link": "http://arxiv.org/abs/2506.22827v2", "date": "2025-07-08", "relevancy": 1.8446, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.615}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6148}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Vision-Language%20Planning%20for%20Multi-Step%20Humanoid%0A%20%20Manipulation&body=Title%3A%20Hierarchical%20Vision-Language%20Planning%20for%20Multi-Step%20Humanoid%0A%20%20Manipulation%0AAuthor%3A%20Andr%C3%A9%20Schakkal%20and%20Ben%20Zandonati%20and%20Zhutian%20Yang%20and%20Navid%20Azizan%0AAbstract%3A%20%20%20Enabling%20humanoid%20robots%20to%20reliably%20execute%20complex%20multi-step%20manipulation%0Atasks%20is%20crucial%20for%20their%20effective%20deployment%20in%20industrial%20and%20household%0Aenvironments.%20This%20paper%20presents%20a%20hierarchical%20planning%20and%20control%20framework%0Adesigned%20to%20achieve%20reliable%20multi-step%20humanoid%20manipulation.%20The%20proposed%0Asystem%20comprises%20three%20layers%3A%20%281%29%20a%20low-level%20RL-based%20controller%20responsible%0Afor%20tracking%20whole-body%20motion%20targets%3B%20%282%29%20a%20mid-level%20set%20of%20skill%20policies%0Atrained%20via%20imitation%20learning%20that%20produce%20motion%20targets%20for%20different%20steps%0Aof%20a%20task%3B%20and%20%283%29%20a%20high-level%20vision-language%20planning%20module%20that%20determines%0Awhich%20skills%20should%20be%20executed%20and%20also%20monitors%20their%20completion%20in%20real-time%0Ausing%20pretrained%20vision-language%20models%20%28VLMs%29.%20Experimental%20validation%20is%0Aperformed%20on%20a%20Unitree%20G1%20humanoid%20robot%20executing%20a%20non-prehensile%0Apick-and-place%20task.%20Over%2040%20real-world%20trials%2C%20the%20hierarchical%20system%0Aachieved%20a%2073%25%20success%20rate%20in%20completing%20the%20full%20manipulation%20sequence.%20These%0Aexperiments%20confirm%20the%20feasibility%20of%20the%20proposed%20hierarchical%20system%2C%0Ahighlighting%20the%20benefits%20of%20VLM-based%20skill%20planning%20and%20monitoring%20for%0Amulti-step%20manipulation%20scenarios.%20See%20https%3A//vlp-humanoid.github.io/%20for%0Avideo%20demonstrations%20of%20the%20policy%20rollout.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.22827v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Vision-Language%2520Planning%2520for%2520Multi-Step%2520Humanoid%250A%2520%2520Manipulation%26entry.906535625%3DAndr%25C3%25A9%2520Schakkal%2520and%2520Ben%2520Zandonati%2520and%2520Zhutian%2520Yang%2520and%2520Navid%2520Azizan%26entry.1292438233%3D%2520%2520Enabling%2520humanoid%2520robots%2520to%2520reliably%2520execute%2520complex%2520multi-step%2520manipulation%250Atasks%2520is%2520crucial%2520for%2520their%2520effective%2520deployment%2520in%2520industrial%2520and%2520household%250Aenvironments.%2520This%2520paper%2520presents%2520a%2520hierarchical%2520planning%2520and%2520control%2520framework%250Adesigned%2520to%2520achieve%2520reliable%2520multi-step%2520humanoid%2520manipulation.%2520The%2520proposed%250Asystem%2520comprises%2520three%2520layers%253A%2520%25281%2529%2520a%2520low-level%2520RL-based%2520controller%2520responsible%250Afor%2520tracking%2520whole-body%2520motion%2520targets%253B%2520%25282%2529%2520a%2520mid-level%2520set%2520of%2520skill%2520policies%250Atrained%2520via%2520imitation%2520learning%2520that%2520produce%2520motion%2520targets%2520for%2520different%2520steps%250Aof%2520a%2520task%253B%2520and%2520%25283%2529%2520a%2520high-level%2520vision-language%2520planning%2520module%2520that%2520determines%250Awhich%2520skills%2520should%2520be%2520executed%2520and%2520also%2520monitors%2520their%2520completion%2520in%2520real-time%250Ausing%2520pretrained%2520vision-language%2520models%2520%2528VLMs%2529.%2520Experimental%2520validation%2520is%250Aperformed%2520on%2520a%2520Unitree%2520G1%2520humanoid%2520robot%2520executing%2520a%2520non-prehensile%250Apick-and-place%2520task.%2520Over%252040%2520real-world%2520trials%252C%2520the%2520hierarchical%2520system%250Aachieved%2520a%252073%2525%2520success%2520rate%2520in%2520completing%2520the%2520full%2520manipulation%2520sequence.%2520These%250Aexperiments%2520confirm%2520the%2520feasibility%2520of%2520the%2520proposed%2520hierarchical%2520system%252C%250Ahighlighting%2520the%2520benefits%2520of%2520VLM-based%2520skill%2520planning%2520and%2520monitoring%2520for%250Amulti-step%2520manipulation%2520scenarios.%2520See%2520https%253A//vlp-humanoid.github.io/%2520for%250Avideo%2520demonstrations%2520of%2520the%2520policy%2520rollout.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22827v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Vision-Language%20Planning%20for%20Multi-Step%20Humanoid%0A%20%20Manipulation&entry.906535625=Andr%C3%A9%20Schakkal%20and%20Ben%20Zandonati%20and%20Zhutian%20Yang%20and%20Navid%20Azizan&entry.1292438233=%20%20Enabling%20humanoid%20robots%20to%20reliably%20execute%20complex%20multi-step%20manipulation%0Atasks%20is%20crucial%20for%20their%20effective%20deployment%20in%20industrial%20and%20household%0Aenvironments.%20This%20paper%20presents%20a%20hierarchical%20planning%20and%20control%20framework%0Adesigned%20to%20achieve%20reliable%20multi-step%20humanoid%20manipulation.%20The%20proposed%0Asystem%20comprises%20three%20layers%3A%20%281%29%20a%20low-level%20RL-based%20controller%20responsible%0Afor%20tracking%20whole-body%20motion%20targets%3B%20%282%29%20a%20mid-level%20set%20of%20skill%20policies%0Atrained%20via%20imitation%20learning%20that%20produce%20motion%20targets%20for%20different%20steps%0Aof%20a%20task%3B%20and%20%283%29%20a%20high-level%20vision-language%20planning%20module%20that%20determines%0Awhich%20skills%20should%20be%20executed%20and%20also%20monitors%20their%20completion%20in%20real-time%0Ausing%20pretrained%20vision-language%20models%20%28VLMs%29.%20Experimental%20validation%20is%0Aperformed%20on%20a%20Unitree%20G1%20humanoid%20robot%20executing%20a%20non-prehensile%0Apick-and-place%20task.%20Over%2040%20real-world%20trials%2C%20the%20hierarchical%20system%0Aachieved%20a%2073%25%20success%20rate%20in%20completing%20the%20full%20manipulation%20sequence.%20These%0Aexperiments%20confirm%20the%20feasibility%20of%20the%20proposed%20hierarchical%20system%2C%0Ahighlighting%20the%20benefits%20of%20VLM-based%20skill%20planning%20and%20monitoring%20for%0Amulti-step%20manipulation%20scenarios.%20See%20https%3A//vlp-humanoid.github.io/%20for%0Avideo%20demonstrations%20of%20the%20policy%20rollout.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.22827v2&entry.124074799=Read"},
{"title": "FineGrasp: Towards Robust Grasping for Delicate Objects", "author": "Yun Du and Mengao Zhao and Tianwei Lin and Yiwei Jin and Chaodong Huang and Zhizhong Su", "abstract": "  Recent advancements in robotic grasping have led to its integration as a core\nmodule in many manipulation systems. For instance, language-driven semantic\nsegmentation enables the grasping of any designated object or object part.\nHowever, existing methods often struggle to generate feasible grasp poses for\nsmall objects or delicate components, potentially causing the entire pipeline\nto fail. To address this issue, we propose a novel grasping method, FineGrasp,\nwhich introduces improvements in three key aspects. First, we introduce\nmultiple network modifications to enhance the ability of to handle delicate\nregions. Second, we address the issue of label imbalance and propose a refined\ngraspness label normalization strategy. Third, we introduce a new simulated\ngrasp dataset and show that mixed sim-to-real training further improves grasp\nperformance. Experimental results show significant improvements, especially in\ngrasping small objects, and confirm the effectiveness of our system in semantic\ngrasping.\n", "link": "http://arxiv.org/abs/2507.05978v1", "date": "2025-07-08", "relevancy": 1.8444, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6643}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.561}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FineGrasp%3A%20Towards%20Robust%20Grasping%20for%20Delicate%20Objects&body=Title%3A%20FineGrasp%3A%20Towards%20Robust%20Grasping%20for%20Delicate%20Objects%0AAuthor%3A%20Yun%20Du%20and%20Mengao%20Zhao%20and%20Tianwei%20Lin%20and%20Yiwei%20Jin%20and%20Chaodong%20Huang%20and%20Zhizhong%20Su%0AAbstract%3A%20%20%20Recent%20advancements%20in%20robotic%20grasping%20have%20led%20to%20its%20integration%20as%20a%20core%0Amodule%20in%20many%20manipulation%20systems.%20For%20instance%2C%20language-driven%20semantic%0Asegmentation%20enables%20the%20grasping%20of%20any%20designated%20object%20or%20object%20part.%0AHowever%2C%20existing%20methods%20often%20struggle%20to%20generate%20feasible%20grasp%20poses%20for%0Asmall%20objects%20or%20delicate%20components%2C%20potentially%20causing%20the%20entire%20pipeline%0Ato%20fail.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20grasping%20method%2C%20FineGrasp%2C%0Awhich%20introduces%20improvements%20in%20three%20key%20aspects.%20First%2C%20we%20introduce%0Amultiple%20network%20modifications%20to%20enhance%20the%20ability%20of%20to%20handle%20delicate%0Aregions.%20Second%2C%20we%20address%20the%20issue%20of%20label%20imbalance%20and%20propose%20a%20refined%0Agraspness%20label%20normalization%20strategy.%20Third%2C%20we%20introduce%20a%20new%20simulated%0Agrasp%20dataset%20and%20show%20that%20mixed%20sim-to-real%20training%20further%20improves%20grasp%0Aperformance.%20Experimental%20results%20show%20significant%20improvements%2C%20especially%20in%0Agrasping%20small%20objects%2C%20and%20confirm%20the%20effectiveness%20of%20our%20system%20in%20semantic%0Agrasping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFineGrasp%253A%2520Towards%2520Robust%2520Grasping%2520for%2520Delicate%2520Objects%26entry.906535625%3DYun%2520Du%2520and%2520Mengao%2520Zhao%2520and%2520Tianwei%2520Lin%2520and%2520Yiwei%2520Jin%2520and%2520Chaodong%2520Huang%2520and%2520Zhizhong%2520Su%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520robotic%2520grasping%2520have%2520led%2520to%2520its%2520integration%2520as%2520a%2520core%250Amodule%2520in%2520many%2520manipulation%2520systems.%2520For%2520instance%252C%2520language-driven%2520semantic%250Asegmentation%2520enables%2520the%2520grasping%2520of%2520any%2520designated%2520object%2520or%2520object%2520part.%250AHowever%252C%2520existing%2520methods%2520often%2520struggle%2520to%2520generate%2520feasible%2520grasp%2520poses%2520for%250Asmall%2520objects%2520or%2520delicate%2520components%252C%2520potentially%2520causing%2520the%2520entire%2520pipeline%250Ato%2520fail.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520grasping%2520method%252C%2520FineGrasp%252C%250Awhich%2520introduces%2520improvements%2520in%2520three%2520key%2520aspects.%2520First%252C%2520we%2520introduce%250Amultiple%2520network%2520modifications%2520to%2520enhance%2520the%2520ability%2520of%2520to%2520handle%2520delicate%250Aregions.%2520Second%252C%2520we%2520address%2520the%2520issue%2520of%2520label%2520imbalance%2520and%2520propose%2520a%2520refined%250Agraspness%2520label%2520normalization%2520strategy.%2520Third%252C%2520we%2520introduce%2520a%2520new%2520simulated%250Agrasp%2520dataset%2520and%2520show%2520that%2520mixed%2520sim-to-real%2520training%2520further%2520improves%2520grasp%250Aperformance.%2520Experimental%2520results%2520show%2520significant%2520improvements%252C%2520especially%2520in%250Agrasping%2520small%2520objects%252C%2520and%2520confirm%2520the%2520effectiveness%2520of%2520our%2520system%2520in%2520semantic%250Agrasping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FineGrasp%3A%20Towards%20Robust%20Grasping%20for%20Delicate%20Objects&entry.906535625=Yun%20Du%20and%20Mengao%20Zhao%20and%20Tianwei%20Lin%20and%20Yiwei%20Jin%20and%20Chaodong%20Huang%20and%20Zhizhong%20Su&entry.1292438233=%20%20Recent%20advancements%20in%20robotic%20grasping%20have%20led%20to%20its%20integration%20as%20a%20core%0Amodule%20in%20many%20manipulation%20systems.%20For%20instance%2C%20language-driven%20semantic%0Asegmentation%20enables%20the%20grasping%20of%20any%20designated%20object%20or%20object%20part.%0AHowever%2C%20existing%20methods%20often%20struggle%20to%20generate%20feasible%20grasp%20poses%20for%0Asmall%20objects%20or%20delicate%20components%2C%20potentially%20causing%20the%20entire%20pipeline%0Ato%20fail.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20grasping%20method%2C%20FineGrasp%2C%0Awhich%20introduces%20improvements%20in%20three%20key%20aspects.%20First%2C%20we%20introduce%0Amultiple%20network%20modifications%20to%20enhance%20the%20ability%20of%20to%20handle%20delicate%0Aregions.%20Second%2C%20we%20address%20the%20issue%20of%20label%20imbalance%20and%20propose%20a%20refined%0Agraspness%20label%20normalization%20strategy.%20Third%2C%20we%20introduce%20a%20new%20simulated%0Agrasp%20dataset%20and%20show%20that%20mixed%20sim-to-real%20training%20further%20improves%20grasp%0Aperformance.%20Experimental%20results%20show%20significant%20improvements%2C%20especially%20in%0Agrasping%20small%20objects%2C%20and%20confirm%20the%20effectiveness%20of%20our%20system%20in%20semantic%0Agrasping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05978v1&entry.124074799=Read"},
{"title": "UniCombine: Unified Multi-Conditional Combination with Diffusion\n  Transformer", "author": "Haoxuan Wang and Jinlong Peng and Qingdong He and Hao Yang and Ying Jin and Jiafu Wu and Xiaobin Hu and Yanjie Pan and Zhenye Gan and Mingmin Chi and Bo Peng and Yabiao Wang", "abstract": "  With the rapid development of diffusion models in image generation, the\ndemand for more powerful and flexible controllable frameworks is increasing.\nAlthough existing methods can guide generation beyond text prompts, the\nchallenge of effectively combining multiple conditional inputs while\nmaintaining consistency with all of them remains unsolved. To address this, we\nintroduce UniCombine, a DiT-based multi-conditional controllable generative\nframework capable of handling any combination of conditions, including but not\nlimited to text prompts, spatial maps, and subject images. Specifically, we\nintroduce a novel Conditional MMDiT Attention mechanism and incorporate a\ntrainable LoRA module to build both the training-free and training-based\nversions. Additionally, we propose a new pipeline to construct\nSubjectSpatial200K, the first dataset designed for multi-conditional generative\ntasks covering both the subject-driven and spatially-aligned conditions.\nExtensive experimental results on multi-conditional generation demonstrate the\noutstanding universality and powerful capability of our approach with\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2503.09277v2", "date": "2025-07-08", "relevancy": 1.8418, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.648}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6326}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniCombine%3A%20Unified%20Multi-Conditional%20Combination%20with%20Diffusion%0A%20%20Transformer&body=Title%3A%20UniCombine%3A%20Unified%20Multi-Conditional%20Combination%20with%20Diffusion%0A%20%20Transformer%0AAuthor%3A%20Haoxuan%20Wang%20and%20Jinlong%20Peng%20and%20Qingdong%20He%20and%20Hao%20Yang%20and%20Ying%20Jin%20and%20Jiafu%20Wu%20and%20Xiaobin%20Hu%20and%20Yanjie%20Pan%20and%20Zhenye%20Gan%20and%20Mingmin%20Chi%20and%20Bo%20Peng%20and%20Yabiao%20Wang%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20diffusion%20models%20in%20image%20generation%2C%20the%0Ademand%20for%20more%20powerful%20and%20flexible%20controllable%20frameworks%20is%20increasing.%0AAlthough%20existing%20methods%20can%20guide%20generation%20beyond%20text%20prompts%2C%20the%0Achallenge%20of%20effectively%20combining%20multiple%20conditional%20inputs%20while%0Amaintaining%20consistency%20with%20all%20of%20them%20remains%20unsolved.%20To%20address%20this%2C%20we%0Aintroduce%20UniCombine%2C%20a%20DiT-based%20multi-conditional%20controllable%20generative%0Aframework%20capable%20of%20handling%20any%20combination%20of%20conditions%2C%20including%20but%20not%0Alimited%20to%20text%20prompts%2C%20spatial%20maps%2C%20and%20subject%20images.%20Specifically%2C%20we%0Aintroduce%20a%20novel%20Conditional%20MMDiT%20Attention%20mechanism%20and%20incorporate%20a%0Atrainable%20LoRA%20module%20to%20build%20both%20the%20training-free%20and%20training-based%0Aversions.%20Additionally%2C%20we%20propose%20a%20new%20pipeline%20to%20construct%0ASubjectSpatial200K%2C%20the%20first%20dataset%20designed%20for%20multi-conditional%20generative%0Atasks%20covering%20both%20the%20subject-driven%20and%20spatially-aligned%20conditions.%0AExtensive%20experimental%20results%20on%20multi-conditional%20generation%20demonstrate%20the%0Aoutstanding%20universality%20and%20powerful%20capability%20of%20our%20approach%20with%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniCombine%253A%2520Unified%2520Multi-Conditional%2520Combination%2520with%2520Diffusion%250A%2520%2520Transformer%26entry.906535625%3DHaoxuan%2520Wang%2520and%2520Jinlong%2520Peng%2520and%2520Qingdong%2520He%2520and%2520Hao%2520Yang%2520and%2520Ying%2520Jin%2520and%2520Jiafu%2520Wu%2520and%2520Xiaobin%2520Hu%2520and%2520Yanjie%2520Pan%2520and%2520Zhenye%2520Gan%2520and%2520Mingmin%2520Chi%2520and%2520Bo%2520Peng%2520and%2520Yabiao%2520Wang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520diffusion%2520models%2520in%2520image%2520generation%252C%2520the%250Ademand%2520for%2520more%2520powerful%2520and%2520flexible%2520controllable%2520frameworks%2520is%2520increasing.%250AAlthough%2520existing%2520methods%2520can%2520guide%2520generation%2520beyond%2520text%2520prompts%252C%2520the%250Achallenge%2520of%2520effectively%2520combining%2520multiple%2520conditional%2520inputs%2520while%250Amaintaining%2520consistency%2520with%2520all%2520of%2520them%2520remains%2520unsolved.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520UniCombine%252C%2520a%2520DiT-based%2520multi-conditional%2520controllable%2520generative%250Aframework%2520capable%2520of%2520handling%2520any%2520combination%2520of%2520conditions%252C%2520including%2520but%2520not%250Alimited%2520to%2520text%2520prompts%252C%2520spatial%2520maps%252C%2520and%2520subject%2520images.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520novel%2520Conditional%2520MMDiT%2520Attention%2520mechanism%2520and%2520incorporate%2520a%250Atrainable%2520LoRA%2520module%2520to%2520build%2520both%2520the%2520training-free%2520and%2520training-based%250Aversions.%2520Additionally%252C%2520we%2520propose%2520a%2520new%2520pipeline%2520to%2520construct%250ASubjectSpatial200K%252C%2520the%2520first%2520dataset%2520designed%2520for%2520multi-conditional%2520generative%250Atasks%2520covering%2520both%2520the%2520subject-driven%2520and%2520spatially-aligned%2520conditions.%250AExtensive%2520experimental%2520results%2520on%2520multi-conditional%2520generation%2520demonstrate%2520the%250Aoutstanding%2520universality%2520and%2520powerful%2520capability%2520of%2520our%2520approach%2520with%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniCombine%3A%20Unified%20Multi-Conditional%20Combination%20with%20Diffusion%0A%20%20Transformer&entry.906535625=Haoxuan%20Wang%20and%20Jinlong%20Peng%20and%20Qingdong%20He%20and%20Hao%20Yang%20and%20Ying%20Jin%20and%20Jiafu%20Wu%20and%20Xiaobin%20Hu%20and%20Yanjie%20Pan%20and%20Zhenye%20Gan%20and%20Mingmin%20Chi%20and%20Bo%20Peng%20and%20Yabiao%20Wang&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20diffusion%20models%20in%20image%20generation%2C%20the%0Ademand%20for%20more%20powerful%20and%20flexible%20controllable%20frameworks%20is%20increasing.%0AAlthough%20existing%20methods%20can%20guide%20generation%20beyond%20text%20prompts%2C%20the%0Achallenge%20of%20effectively%20combining%20multiple%20conditional%20inputs%20while%0Amaintaining%20consistency%20with%20all%20of%20them%20remains%20unsolved.%20To%20address%20this%2C%20we%0Aintroduce%20UniCombine%2C%20a%20DiT-based%20multi-conditional%20controllable%20generative%0Aframework%20capable%20of%20handling%20any%20combination%20of%20conditions%2C%20including%20but%20not%0Alimited%20to%20text%20prompts%2C%20spatial%20maps%2C%20and%20subject%20images.%20Specifically%2C%20we%0Aintroduce%20a%20novel%20Conditional%20MMDiT%20Attention%20mechanism%20and%20incorporate%20a%0Atrainable%20LoRA%20module%20to%20build%20both%20the%20training-free%20and%20training-based%0Aversions.%20Additionally%2C%20we%20propose%20a%20new%20pipeline%20to%20construct%0ASubjectSpatial200K%2C%20the%20first%20dataset%20designed%20for%20multi-conditional%20generative%0Atasks%20covering%20both%20the%20subject-driven%20and%20spatially-aligned%20conditions.%0AExtensive%20experimental%20results%20on%20multi-conditional%20generation%20demonstrate%20the%0Aoutstanding%20universality%20and%20powerful%20capability%20of%20our%20approach%20with%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09277v2&entry.124074799=Read"},
{"title": "Driving View Synthesis on Free-form Trajectories with Generative Prior", "author": "Zeyu Yang and Zijie Pan and Yuankun Yang and Xiatian Zhu and Li Zhang", "abstract": "  Driving view synthesis along free-form trajectories is essential for\nrealistic driving simulations, enabling closed-loop evaluation of end-to-end\ndriving policies. Existing methods excel at view interpolation along recorded\npaths but struggle to generalize to novel trajectories due to limited\nviewpoints in driving videos. To tackle this challenge, we propose DriveX, a\nnovel free-form driving view synthesis framework, that progressively distills\ngenerative prior into the 3D Gaussian model during its optimization. Within\nthis framework, we utilize a video diffusion model to refine the degraded novel\ntrajectory renderings from the in-training Gaussian model, while the restored\nvideos in turn serve as additional supervision for optimizing the 3D Gaussian.\nConcretely, we craft an inpainting-based video restoration task, which can\ndisentangle the identification of degraded regions from the generative\ncapability of the diffusion model and remove the need of simulating specific\ndegraded pattern in the training of the diffusion model. To further enhance the\nconsistency and fidelity of generated contents, the pseudo ground truth is\nprogressively updated with gradually improved novel trajectory rendering,\nallowing both components to co-adapt and reinforce each other while minimizing\nthe disruption on the optimization. By tightly integrating 3D scene\nrepresentation with generative prior, DriveX achieves high-quality view\nsynthesis beyond recorded trajectories in real time--unlocking new\npossibilities for flexible and realistic driving simulations on free-form\ntrajectories.\n", "link": "http://arxiv.org/abs/2412.01717v3", "date": "2025-07-08", "relevancy": 1.8399, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6311}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.594}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Driving%20View%20Synthesis%20on%20Free-form%20Trajectories%20with%20Generative%20Prior&body=Title%3A%20Driving%20View%20Synthesis%20on%20Free-form%20Trajectories%20with%20Generative%20Prior%0AAuthor%3A%20Zeyu%20Yang%20and%20Zijie%20Pan%20and%20Yuankun%20Yang%20and%20Xiatian%20Zhu%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Driving%20view%20synthesis%20along%20free-form%20trajectories%20is%20essential%20for%0Arealistic%20driving%20simulations%2C%20enabling%20closed-loop%20evaluation%20of%20end-to-end%0Adriving%20policies.%20Existing%20methods%20excel%20at%20view%20interpolation%20along%20recorded%0Apaths%20but%20struggle%20to%20generalize%20to%20novel%20trajectories%20due%20to%20limited%0Aviewpoints%20in%20driving%20videos.%20To%20tackle%20this%20challenge%2C%20we%20propose%20DriveX%2C%20a%0Anovel%20free-form%20driving%20view%20synthesis%20framework%2C%20that%20progressively%20distills%0Agenerative%20prior%20into%20the%203D%20Gaussian%20model%20during%20its%20optimization.%20Within%0Athis%20framework%2C%20we%20utilize%20a%20video%20diffusion%20model%20to%20refine%20the%20degraded%20novel%0Atrajectory%20renderings%20from%20the%20in-training%20Gaussian%20model%2C%20while%20the%20restored%0Avideos%20in%20turn%20serve%20as%20additional%20supervision%20for%20optimizing%20the%203D%20Gaussian.%0AConcretely%2C%20we%20craft%20an%20inpainting-based%20video%20restoration%20task%2C%20which%20can%0Adisentangle%20the%20identification%20of%20degraded%20regions%20from%20the%20generative%0Acapability%20of%20the%20diffusion%20model%20and%20remove%20the%20need%20of%20simulating%20specific%0Adegraded%20pattern%20in%20the%20training%20of%20the%20diffusion%20model.%20To%20further%20enhance%20the%0Aconsistency%20and%20fidelity%20of%20generated%20contents%2C%20the%20pseudo%20ground%20truth%20is%0Aprogressively%20updated%20with%20gradually%20improved%20novel%20trajectory%20rendering%2C%0Aallowing%20both%20components%20to%20co-adapt%20and%20reinforce%20each%20other%20while%20minimizing%0Athe%20disruption%20on%20the%20optimization.%20By%20tightly%20integrating%203D%20scene%0Arepresentation%20with%20generative%20prior%2C%20DriveX%20achieves%20high-quality%20view%0Asynthesis%20beyond%20recorded%20trajectories%20in%20real%20time--unlocking%20new%0Apossibilities%20for%20flexible%20and%20realistic%20driving%20simulations%20on%20free-form%0Atrajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01717v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriving%2520View%2520Synthesis%2520on%2520Free-form%2520Trajectories%2520with%2520Generative%2520Prior%26entry.906535625%3DZeyu%2520Yang%2520and%2520Zijie%2520Pan%2520and%2520Yuankun%2520Yang%2520and%2520Xiatian%2520Zhu%2520and%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520Driving%2520view%2520synthesis%2520along%2520free-form%2520trajectories%2520is%2520essential%2520for%250Arealistic%2520driving%2520simulations%252C%2520enabling%2520closed-loop%2520evaluation%2520of%2520end-to-end%250Adriving%2520policies.%2520Existing%2520methods%2520excel%2520at%2520view%2520interpolation%2520along%2520recorded%250Apaths%2520but%2520struggle%2520to%2520generalize%2520to%2520novel%2520trajectories%2520due%2520to%2520limited%250Aviewpoints%2520in%2520driving%2520videos.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520DriveX%252C%2520a%250Anovel%2520free-form%2520driving%2520view%2520synthesis%2520framework%252C%2520that%2520progressively%2520distills%250Agenerative%2520prior%2520into%2520the%25203D%2520Gaussian%2520model%2520during%2520its%2520optimization.%2520Within%250Athis%2520framework%252C%2520we%2520utilize%2520a%2520video%2520diffusion%2520model%2520to%2520refine%2520the%2520degraded%2520novel%250Atrajectory%2520renderings%2520from%2520the%2520in-training%2520Gaussian%2520model%252C%2520while%2520the%2520restored%250Avideos%2520in%2520turn%2520serve%2520as%2520additional%2520supervision%2520for%2520optimizing%2520the%25203D%2520Gaussian.%250AConcretely%252C%2520we%2520craft%2520an%2520inpainting-based%2520video%2520restoration%2520task%252C%2520which%2520can%250Adisentangle%2520the%2520identification%2520of%2520degraded%2520regions%2520from%2520the%2520generative%250Acapability%2520of%2520the%2520diffusion%2520model%2520and%2520remove%2520the%2520need%2520of%2520simulating%2520specific%250Adegraded%2520pattern%2520in%2520the%2520training%2520of%2520the%2520diffusion%2520model.%2520To%2520further%2520enhance%2520the%250Aconsistency%2520and%2520fidelity%2520of%2520generated%2520contents%252C%2520the%2520pseudo%2520ground%2520truth%2520is%250Aprogressively%2520updated%2520with%2520gradually%2520improved%2520novel%2520trajectory%2520rendering%252C%250Aallowing%2520both%2520components%2520to%2520co-adapt%2520and%2520reinforce%2520each%2520other%2520while%2520minimizing%250Athe%2520disruption%2520on%2520the%2520optimization.%2520By%2520tightly%2520integrating%25203D%2520scene%250Arepresentation%2520with%2520generative%2520prior%252C%2520DriveX%2520achieves%2520high-quality%2520view%250Asynthesis%2520beyond%2520recorded%2520trajectories%2520in%2520real%2520time--unlocking%2520new%250Apossibilities%2520for%2520flexible%2520and%2520realistic%2520driving%2520simulations%2520on%2520free-form%250Atrajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01717v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Driving%20View%20Synthesis%20on%20Free-form%20Trajectories%20with%20Generative%20Prior&entry.906535625=Zeyu%20Yang%20and%20Zijie%20Pan%20and%20Yuankun%20Yang%20and%20Xiatian%20Zhu%20and%20Li%20Zhang&entry.1292438233=%20%20Driving%20view%20synthesis%20along%20free-form%20trajectories%20is%20essential%20for%0Arealistic%20driving%20simulations%2C%20enabling%20closed-loop%20evaluation%20of%20end-to-end%0Adriving%20policies.%20Existing%20methods%20excel%20at%20view%20interpolation%20along%20recorded%0Apaths%20but%20struggle%20to%20generalize%20to%20novel%20trajectories%20due%20to%20limited%0Aviewpoints%20in%20driving%20videos.%20To%20tackle%20this%20challenge%2C%20we%20propose%20DriveX%2C%20a%0Anovel%20free-form%20driving%20view%20synthesis%20framework%2C%20that%20progressively%20distills%0Agenerative%20prior%20into%20the%203D%20Gaussian%20model%20during%20its%20optimization.%20Within%0Athis%20framework%2C%20we%20utilize%20a%20video%20diffusion%20model%20to%20refine%20the%20degraded%20novel%0Atrajectory%20renderings%20from%20the%20in-training%20Gaussian%20model%2C%20while%20the%20restored%0Avideos%20in%20turn%20serve%20as%20additional%20supervision%20for%20optimizing%20the%203D%20Gaussian.%0AConcretely%2C%20we%20craft%20an%20inpainting-based%20video%20restoration%20task%2C%20which%20can%0Adisentangle%20the%20identification%20of%20degraded%20regions%20from%20the%20generative%0Acapability%20of%20the%20diffusion%20model%20and%20remove%20the%20need%20of%20simulating%20specific%0Adegraded%20pattern%20in%20the%20training%20of%20the%20diffusion%20model.%20To%20further%20enhance%20the%0Aconsistency%20and%20fidelity%20of%20generated%20contents%2C%20the%20pseudo%20ground%20truth%20is%0Aprogressively%20updated%20with%20gradually%20improved%20novel%20trajectory%20rendering%2C%0Aallowing%20both%20components%20to%20co-adapt%20and%20reinforce%20each%20other%20while%20minimizing%0Athe%20disruption%20on%20the%20optimization.%20By%20tightly%20integrating%203D%20scene%0Arepresentation%20with%20generative%20prior%2C%20DriveX%20achieves%20high-quality%20view%0Asynthesis%20beyond%20recorded%20trajectories%20in%20real%20time--unlocking%20new%0Apossibilities%20for%20flexible%20and%20realistic%20driving%20simulations%20on%20free-form%0Atrajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01717v3&entry.124074799=Read"},
{"title": "Beating the Best Constant Rebalancing Portfolio in Long-Term Investment:\n  A Generalization of the Kelly Criterion and Universal Learning Algorithm for\n  Markets with Serial Dependence", "author": "Duy Khanh Lam", "abstract": "  In the online portfolio optimization framework, existing learning algorithms\ngenerate strategies that yield significantly poorer cumulative wealth compared\nto the best constant rebalancing portfolio in hindsight, despite being\nconsistent in asymptotic growth rate. While this unappealing performance can be\nimproved by incorporating more side information, it raises difficulties in\nfeature selection and high-dimensional settings. Instead, the inherent serial\ndependence of assets' returns, such as day-of-the-week and other calendar\neffects, can be leveraged. Although latent serial dependence patterns are\ncommonly detected using large training datasets, this paper proposes an\nalgorithm that learns such dependence using only gradually revealed data,\nwithout any assumption on their distribution, to form a strategy that\neventually exceeds the cumulative wealth of the best constant rebalancing\nportfolio.\n  Moreover, the classical Kelly criterion, which requires independent assets'\nreturns, is generalized to accommodate serial dependence in a market modeled as\nan independent and identically distributed process of random matrices. In such\na stochastic market, where existing learning algorithms designed for stationary\nprocesses fail to apply, the proposed learning algorithm still generates a\nstrategy that asymptotically grows to the highest rate among all strategies,\nmatching that of the optimal strategy constructed under the generalized Kelly\ncriterion. The experimental results with real market data demonstrate the\ntheoretical guarantees of the algorithm and its performance as expected, as\nlong as serial dependence is significant, regardless of the validity of the\ngeneralized Kelly criterion in the experimental market. This further affirms\nthe broad applicability of the algorithm in general contexts.\n", "link": "http://arxiv.org/abs/2507.05994v1", "date": "2025-07-08", "relevancy": 1.8362, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4696}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.462}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beating%20the%20Best%20Constant%20Rebalancing%20Portfolio%20in%20Long-Term%20Investment%3A%0A%20%20A%20Generalization%20of%20the%20Kelly%20Criterion%20and%20Universal%20Learning%20Algorithm%20for%0A%20%20Markets%20with%20Serial%20Dependence&body=Title%3A%20Beating%20the%20Best%20Constant%20Rebalancing%20Portfolio%20in%20Long-Term%20Investment%3A%0A%20%20A%20Generalization%20of%20the%20Kelly%20Criterion%20and%20Universal%20Learning%20Algorithm%20for%0A%20%20Markets%20with%20Serial%20Dependence%0AAuthor%3A%20Duy%20Khanh%20Lam%0AAbstract%3A%20%20%20In%20the%20online%20portfolio%20optimization%20framework%2C%20existing%20learning%20algorithms%0Agenerate%20strategies%20that%20yield%20significantly%20poorer%20cumulative%20wealth%20compared%0Ato%20the%20best%20constant%20rebalancing%20portfolio%20in%20hindsight%2C%20despite%20being%0Aconsistent%20in%20asymptotic%20growth%20rate.%20While%20this%20unappealing%20performance%20can%20be%0Aimproved%20by%20incorporating%20more%20side%20information%2C%20it%20raises%20difficulties%20in%0Afeature%20selection%20and%20high-dimensional%20settings.%20Instead%2C%20the%20inherent%20serial%0Adependence%20of%20assets%27%20returns%2C%20such%20as%20day-of-the-week%20and%20other%20calendar%0Aeffects%2C%20can%20be%20leveraged.%20Although%20latent%20serial%20dependence%20patterns%20are%0Acommonly%20detected%20using%20large%20training%20datasets%2C%20this%20paper%20proposes%20an%0Aalgorithm%20that%20learns%20such%20dependence%20using%20only%20gradually%20revealed%20data%2C%0Awithout%20any%20assumption%20on%20their%20distribution%2C%20to%20form%20a%20strategy%20that%0Aeventually%20exceeds%20the%20cumulative%20wealth%20of%20the%20best%20constant%20rebalancing%0Aportfolio.%0A%20%20Moreover%2C%20the%20classical%20Kelly%20criterion%2C%20which%20requires%20independent%20assets%27%0Areturns%2C%20is%20generalized%20to%20accommodate%20serial%20dependence%20in%20a%20market%20modeled%20as%0Aan%20independent%20and%20identically%20distributed%20process%20of%20random%20matrices.%20In%20such%0Aa%20stochastic%20market%2C%20where%20existing%20learning%20algorithms%20designed%20for%20stationary%0Aprocesses%20fail%20to%20apply%2C%20the%20proposed%20learning%20algorithm%20still%20generates%20a%0Astrategy%20that%20asymptotically%20grows%20to%20the%20highest%20rate%20among%20all%20strategies%2C%0Amatching%20that%20of%20the%20optimal%20strategy%20constructed%20under%20the%20generalized%20Kelly%0Acriterion.%20The%20experimental%20results%20with%20real%20market%20data%20demonstrate%20the%0Atheoretical%20guarantees%20of%20the%20algorithm%20and%20its%20performance%20as%20expected%2C%20as%0Along%20as%20serial%20dependence%20is%20significant%2C%20regardless%20of%20the%20validity%20of%20the%0Ageneralized%20Kelly%20criterion%20in%20the%20experimental%20market.%20This%20further%20affirms%0Athe%20broad%20applicability%20of%20the%20algorithm%20in%20general%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeating%2520the%2520Best%2520Constant%2520Rebalancing%2520Portfolio%2520in%2520Long-Term%2520Investment%253A%250A%2520%2520A%2520Generalization%2520of%2520the%2520Kelly%2520Criterion%2520and%2520Universal%2520Learning%2520Algorithm%2520for%250A%2520%2520Markets%2520with%2520Serial%2520Dependence%26entry.906535625%3DDuy%2520Khanh%2520Lam%26entry.1292438233%3D%2520%2520In%2520the%2520online%2520portfolio%2520optimization%2520framework%252C%2520existing%2520learning%2520algorithms%250Agenerate%2520strategies%2520that%2520yield%2520significantly%2520poorer%2520cumulative%2520wealth%2520compared%250Ato%2520the%2520best%2520constant%2520rebalancing%2520portfolio%2520in%2520hindsight%252C%2520despite%2520being%250Aconsistent%2520in%2520asymptotic%2520growth%2520rate.%2520While%2520this%2520unappealing%2520performance%2520can%2520be%250Aimproved%2520by%2520incorporating%2520more%2520side%2520information%252C%2520it%2520raises%2520difficulties%2520in%250Afeature%2520selection%2520and%2520high-dimensional%2520settings.%2520Instead%252C%2520the%2520inherent%2520serial%250Adependence%2520of%2520assets%2527%2520returns%252C%2520such%2520as%2520day-of-the-week%2520and%2520other%2520calendar%250Aeffects%252C%2520can%2520be%2520leveraged.%2520Although%2520latent%2520serial%2520dependence%2520patterns%2520are%250Acommonly%2520detected%2520using%2520large%2520training%2520datasets%252C%2520this%2520paper%2520proposes%2520an%250Aalgorithm%2520that%2520learns%2520such%2520dependence%2520using%2520only%2520gradually%2520revealed%2520data%252C%250Awithout%2520any%2520assumption%2520on%2520their%2520distribution%252C%2520to%2520form%2520a%2520strategy%2520that%250Aeventually%2520exceeds%2520the%2520cumulative%2520wealth%2520of%2520the%2520best%2520constant%2520rebalancing%250Aportfolio.%250A%2520%2520Moreover%252C%2520the%2520classical%2520Kelly%2520criterion%252C%2520which%2520requires%2520independent%2520assets%2527%250Areturns%252C%2520is%2520generalized%2520to%2520accommodate%2520serial%2520dependence%2520in%2520a%2520market%2520modeled%2520as%250Aan%2520independent%2520and%2520identically%2520distributed%2520process%2520of%2520random%2520matrices.%2520In%2520such%250Aa%2520stochastic%2520market%252C%2520where%2520existing%2520learning%2520algorithms%2520designed%2520for%2520stationary%250Aprocesses%2520fail%2520to%2520apply%252C%2520the%2520proposed%2520learning%2520algorithm%2520still%2520generates%2520a%250Astrategy%2520that%2520asymptotically%2520grows%2520to%2520the%2520highest%2520rate%2520among%2520all%2520strategies%252C%250Amatching%2520that%2520of%2520the%2520optimal%2520strategy%2520constructed%2520under%2520the%2520generalized%2520Kelly%250Acriterion.%2520The%2520experimental%2520results%2520with%2520real%2520market%2520data%2520demonstrate%2520the%250Atheoretical%2520guarantees%2520of%2520the%2520algorithm%2520and%2520its%2520performance%2520as%2520expected%252C%2520as%250Along%2520as%2520serial%2520dependence%2520is%2520significant%252C%2520regardless%2520of%2520the%2520validity%2520of%2520the%250Ageneralized%2520Kelly%2520criterion%2520in%2520the%2520experimental%2520market.%2520This%2520further%2520affirms%250Athe%2520broad%2520applicability%2520of%2520the%2520algorithm%2520in%2520general%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beating%20the%20Best%20Constant%20Rebalancing%20Portfolio%20in%20Long-Term%20Investment%3A%0A%20%20A%20Generalization%20of%20the%20Kelly%20Criterion%20and%20Universal%20Learning%20Algorithm%20for%0A%20%20Markets%20with%20Serial%20Dependence&entry.906535625=Duy%20Khanh%20Lam&entry.1292438233=%20%20In%20the%20online%20portfolio%20optimization%20framework%2C%20existing%20learning%20algorithms%0Agenerate%20strategies%20that%20yield%20significantly%20poorer%20cumulative%20wealth%20compared%0Ato%20the%20best%20constant%20rebalancing%20portfolio%20in%20hindsight%2C%20despite%20being%0Aconsistent%20in%20asymptotic%20growth%20rate.%20While%20this%20unappealing%20performance%20can%20be%0Aimproved%20by%20incorporating%20more%20side%20information%2C%20it%20raises%20difficulties%20in%0Afeature%20selection%20and%20high-dimensional%20settings.%20Instead%2C%20the%20inherent%20serial%0Adependence%20of%20assets%27%20returns%2C%20such%20as%20day-of-the-week%20and%20other%20calendar%0Aeffects%2C%20can%20be%20leveraged.%20Although%20latent%20serial%20dependence%20patterns%20are%0Acommonly%20detected%20using%20large%20training%20datasets%2C%20this%20paper%20proposes%20an%0Aalgorithm%20that%20learns%20such%20dependence%20using%20only%20gradually%20revealed%20data%2C%0Awithout%20any%20assumption%20on%20their%20distribution%2C%20to%20form%20a%20strategy%20that%0Aeventually%20exceeds%20the%20cumulative%20wealth%20of%20the%20best%20constant%20rebalancing%0Aportfolio.%0A%20%20Moreover%2C%20the%20classical%20Kelly%20criterion%2C%20which%20requires%20independent%20assets%27%0Areturns%2C%20is%20generalized%20to%20accommodate%20serial%20dependence%20in%20a%20market%20modeled%20as%0Aan%20independent%20and%20identically%20distributed%20process%20of%20random%20matrices.%20In%20such%0Aa%20stochastic%20market%2C%20where%20existing%20learning%20algorithms%20designed%20for%20stationary%0Aprocesses%20fail%20to%20apply%2C%20the%20proposed%20learning%20algorithm%20still%20generates%20a%0Astrategy%20that%20asymptotically%20grows%20to%20the%20highest%20rate%20among%20all%20strategies%2C%0Amatching%20that%20of%20the%20optimal%20strategy%20constructed%20under%20the%20generalized%20Kelly%0Acriterion.%20The%20experimental%20results%20with%20real%20market%20data%20demonstrate%20the%0Atheoretical%20guarantees%20of%20the%20algorithm%20and%20its%20performance%20as%20expected%2C%20as%0Along%20as%20serial%20dependence%20is%20significant%2C%20regardless%20of%20the%20validity%20of%20the%0Ageneralized%20Kelly%20criterion%20in%20the%20experimental%20market.%20This%20further%20affirms%0Athe%20broad%20applicability%20of%20the%20algorithm%20in%20general%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05994v1&entry.124074799=Read"},
{"title": "SoftReMish: A Novel Activation Function for Enhanced Convolutional\n  Neural Networks for Visual Recognition Performance", "author": "Mustafa Bayram G\u00fccen", "abstract": "  In this study, SoftReMish, a new activation function designed to improve the\nperformance of convolutional neural networks (CNNs) in image classification\ntasks, is proposed. Using the MNIST dataset, a standard CNN architecture\nconsisting of two convolutional layers, max pooling, and fully connected layers\nwas implemented. SoftReMish was evaluated against popular activation functions\nincluding ReLU, Tanh, and Mish by replacing the activation function in all\ntrainable layers. The model performance was assessed in terms of minimum\ntraining loss and maximum validation accuracy. Results showed that SoftReMish\nachieved a minimum loss (3.14e-8) and a validation accuracy (99.41%),\noutperforming all other functions tested. These findings demonstrate that\nSoftReMish offers better convergence behavior and generalization capability,\nmaking it a promising candidate for visual recognition tasks.\n", "link": "http://arxiv.org/abs/2507.06148v1", "date": "2025-07-08", "relevancy": 1.8303, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4846}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4536}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoftReMish%3A%20A%20Novel%20Activation%20Function%20for%20Enhanced%20Convolutional%0A%20%20Neural%20Networks%20for%20Visual%20Recognition%20Performance&body=Title%3A%20SoftReMish%3A%20A%20Novel%20Activation%20Function%20for%20Enhanced%20Convolutional%0A%20%20Neural%20Networks%20for%20Visual%20Recognition%20Performance%0AAuthor%3A%20Mustafa%20Bayram%20G%C3%BCcen%0AAbstract%3A%20%20%20In%20this%20study%2C%20SoftReMish%2C%20a%20new%20activation%20function%20designed%20to%20improve%20the%0Aperformance%20of%20convolutional%20neural%20networks%20%28CNNs%29%20in%20image%20classification%0Atasks%2C%20is%20proposed.%20Using%20the%20MNIST%20dataset%2C%20a%20standard%20CNN%20architecture%0Aconsisting%20of%20two%20convolutional%20layers%2C%20max%20pooling%2C%20and%20fully%20connected%20layers%0Awas%20implemented.%20SoftReMish%20was%20evaluated%20against%20popular%20activation%20functions%0Aincluding%20ReLU%2C%20Tanh%2C%20and%20Mish%20by%20replacing%20the%20activation%20function%20in%20all%0Atrainable%20layers.%20The%20model%20performance%20was%20assessed%20in%20terms%20of%20minimum%0Atraining%20loss%20and%20maximum%20validation%20accuracy.%20Results%20showed%20that%20SoftReMish%0Aachieved%20a%20minimum%20loss%20%283.14e-8%29%20and%20a%20validation%20accuracy%20%2899.41%25%29%2C%0Aoutperforming%20all%20other%20functions%20tested.%20These%20findings%20demonstrate%20that%0ASoftReMish%20offers%20better%20convergence%20behavior%20and%20generalization%20capability%2C%0Amaking%20it%20a%20promising%20candidate%20for%20visual%20recognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoftReMish%253A%2520A%2520Novel%2520Activation%2520Function%2520for%2520Enhanced%2520Convolutional%250A%2520%2520Neural%2520Networks%2520for%2520Visual%2520Recognition%2520Performance%26entry.906535625%3DMustafa%2520Bayram%2520G%25C3%25BCcen%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520SoftReMish%252C%2520a%2520new%2520activation%2520function%2520designed%2520to%2520improve%2520the%250Aperformance%2520of%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520in%2520image%2520classification%250Atasks%252C%2520is%2520proposed.%2520Using%2520the%2520MNIST%2520dataset%252C%2520a%2520standard%2520CNN%2520architecture%250Aconsisting%2520of%2520two%2520convolutional%2520layers%252C%2520max%2520pooling%252C%2520and%2520fully%2520connected%2520layers%250Awas%2520implemented.%2520SoftReMish%2520was%2520evaluated%2520against%2520popular%2520activation%2520functions%250Aincluding%2520ReLU%252C%2520Tanh%252C%2520and%2520Mish%2520by%2520replacing%2520the%2520activation%2520function%2520in%2520all%250Atrainable%2520layers.%2520The%2520model%2520performance%2520was%2520assessed%2520in%2520terms%2520of%2520minimum%250Atraining%2520loss%2520and%2520maximum%2520validation%2520accuracy.%2520Results%2520showed%2520that%2520SoftReMish%250Aachieved%2520a%2520minimum%2520loss%2520%25283.14e-8%2529%2520and%2520a%2520validation%2520accuracy%2520%252899.41%2525%2529%252C%250Aoutperforming%2520all%2520other%2520functions%2520tested.%2520These%2520findings%2520demonstrate%2520that%250ASoftReMish%2520offers%2520better%2520convergence%2520behavior%2520and%2520generalization%2520capability%252C%250Amaking%2520it%2520a%2520promising%2520candidate%2520for%2520visual%2520recognition%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoftReMish%3A%20A%20Novel%20Activation%20Function%20for%20Enhanced%20Convolutional%0A%20%20Neural%20Networks%20for%20Visual%20Recognition%20Performance&entry.906535625=Mustafa%20Bayram%20G%C3%BCcen&entry.1292438233=%20%20In%20this%20study%2C%20SoftReMish%2C%20a%20new%20activation%20function%20designed%20to%20improve%20the%0Aperformance%20of%20convolutional%20neural%20networks%20%28CNNs%29%20in%20image%20classification%0Atasks%2C%20is%20proposed.%20Using%20the%20MNIST%20dataset%2C%20a%20standard%20CNN%20architecture%0Aconsisting%20of%20two%20convolutional%20layers%2C%20max%20pooling%2C%20and%20fully%20connected%20layers%0Awas%20implemented.%20SoftReMish%20was%20evaluated%20against%20popular%20activation%20functions%0Aincluding%20ReLU%2C%20Tanh%2C%20and%20Mish%20by%20replacing%20the%20activation%20function%20in%20all%0Atrainable%20layers.%20The%20model%20performance%20was%20assessed%20in%20terms%20of%20minimum%0Atraining%20loss%20and%20maximum%20validation%20accuracy.%20Results%20showed%20that%20SoftReMish%0Aachieved%20a%20minimum%20loss%20%283.14e-8%29%20and%20a%20validation%20accuracy%20%2899.41%25%29%2C%0Aoutperforming%20all%20other%20functions%20tested.%20These%20findings%20demonstrate%20that%0ASoftReMish%20offers%20better%20convergence%20behavior%20and%20generalization%20capability%2C%0Amaking%20it%20a%20promising%20candidate%20for%20visual%20recognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06148v1&entry.124074799=Read"},
{"title": "Analysis and experiments of the dissipative Twistcar: direction reversal\n  and asymptotic approximations", "author": "Rom Levy and Ari Dantus and Zitao Yu and Yizhar Or", "abstract": "  Underactuated wheeled vehicles are commonly studied as nonholonomic systems\nwith periodic actuation. Twistcar is a classical example inspired by a riding\ntoy, which has been analyzed using a planar model of a dynamical system with\nnonholonomic constraints. Most of the previous analyses did not account for\nenergy dissipation due to frictional resistance. In this work, we study a\ntheoretical two-link model of the Twistcar while incorporating dissipation due\nto rolling resistance. We obtain asymptotic expressions for the system's\nsmall-amplitude steady-state periodic dynamics, which reveals the possibility\nof reversing the direction of motion upon varying the geometric and mass\nproperties of the vehicle. Next, we design and construct a robotic prototype of\nthe Twistcar whose center-of-mass position can be shifted by adding and\nremoving a massive block, enabling experimental demonstration of the Twistcar's\ndirection reversal phenomenon. We also conduct parameter fitting for the\nfrictional resistance in order to improve agreement with experiments.\n", "link": "http://arxiv.org/abs/2506.19112v2", "date": "2025-07-08", "relevancy": 1.8213, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4762}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4741}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20and%20experiments%20of%20the%20dissipative%20Twistcar%3A%20direction%20reversal%0A%20%20and%20asymptotic%20approximations&body=Title%3A%20Analysis%20and%20experiments%20of%20the%20dissipative%20Twistcar%3A%20direction%20reversal%0A%20%20and%20asymptotic%20approximations%0AAuthor%3A%20Rom%20Levy%20and%20Ari%20Dantus%20and%20Zitao%20Yu%20and%20Yizhar%20Or%0AAbstract%3A%20%20%20Underactuated%20wheeled%20vehicles%20are%20commonly%20studied%20as%20nonholonomic%20systems%0Awith%20periodic%20actuation.%20Twistcar%20is%20a%20classical%20example%20inspired%20by%20a%20riding%0Atoy%2C%20which%20has%20been%20analyzed%20using%20a%20planar%20model%20of%20a%20dynamical%20system%20with%0Anonholonomic%20constraints.%20Most%20of%20the%20previous%20analyses%20did%20not%20account%20for%0Aenergy%20dissipation%20due%20to%20frictional%20resistance.%20In%20this%20work%2C%20we%20study%20a%0Atheoretical%20two-link%20model%20of%20the%20Twistcar%20while%20incorporating%20dissipation%20due%0Ato%20rolling%20resistance.%20We%20obtain%20asymptotic%20expressions%20for%20the%20system%27s%0Asmall-amplitude%20steady-state%20periodic%20dynamics%2C%20which%20reveals%20the%20possibility%0Aof%20reversing%20the%20direction%20of%20motion%20upon%20varying%20the%20geometric%20and%20mass%0Aproperties%20of%20the%20vehicle.%20Next%2C%20we%20design%20and%20construct%20a%20robotic%20prototype%20of%0Athe%20Twistcar%20whose%20center-of-mass%20position%20can%20be%20shifted%20by%20adding%20and%0Aremoving%20a%20massive%20block%2C%20enabling%20experimental%20demonstration%20of%20the%20Twistcar%27s%0Adirection%20reversal%20phenomenon.%20We%20also%20conduct%20parameter%20fitting%20for%20the%0Africtional%20resistance%20in%20order%20to%20improve%20agreement%20with%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19112v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520and%2520experiments%2520of%2520the%2520dissipative%2520Twistcar%253A%2520direction%2520reversal%250A%2520%2520and%2520asymptotic%2520approximations%26entry.906535625%3DRom%2520Levy%2520and%2520Ari%2520Dantus%2520and%2520Zitao%2520Yu%2520and%2520Yizhar%2520Or%26entry.1292438233%3D%2520%2520Underactuated%2520wheeled%2520vehicles%2520are%2520commonly%2520studied%2520as%2520nonholonomic%2520systems%250Awith%2520periodic%2520actuation.%2520Twistcar%2520is%2520a%2520classical%2520example%2520inspired%2520by%2520a%2520riding%250Atoy%252C%2520which%2520has%2520been%2520analyzed%2520using%2520a%2520planar%2520model%2520of%2520a%2520dynamical%2520system%2520with%250Anonholonomic%2520constraints.%2520Most%2520of%2520the%2520previous%2520analyses%2520did%2520not%2520account%2520for%250Aenergy%2520dissipation%2520due%2520to%2520frictional%2520resistance.%2520In%2520this%2520work%252C%2520we%2520study%2520a%250Atheoretical%2520two-link%2520model%2520of%2520the%2520Twistcar%2520while%2520incorporating%2520dissipation%2520due%250Ato%2520rolling%2520resistance.%2520We%2520obtain%2520asymptotic%2520expressions%2520for%2520the%2520system%2527s%250Asmall-amplitude%2520steady-state%2520periodic%2520dynamics%252C%2520which%2520reveals%2520the%2520possibility%250Aof%2520reversing%2520the%2520direction%2520of%2520motion%2520upon%2520varying%2520the%2520geometric%2520and%2520mass%250Aproperties%2520of%2520the%2520vehicle.%2520Next%252C%2520we%2520design%2520and%2520construct%2520a%2520robotic%2520prototype%2520of%250Athe%2520Twistcar%2520whose%2520center-of-mass%2520position%2520can%2520be%2520shifted%2520by%2520adding%2520and%250Aremoving%2520a%2520massive%2520block%252C%2520enabling%2520experimental%2520demonstration%2520of%2520the%2520Twistcar%2527s%250Adirection%2520reversal%2520phenomenon.%2520We%2520also%2520conduct%2520parameter%2520fitting%2520for%2520the%250Africtional%2520resistance%2520in%2520order%2520to%2520improve%2520agreement%2520with%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19112v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20and%20experiments%20of%20the%20dissipative%20Twistcar%3A%20direction%20reversal%0A%20%20and%20asymptotic%20approximations&entry.906535625=Rom%20Levy%20and%20Ari%20Dantus%20and%20Zitao%20Yu%20and%20Yizhar%20Or&entry.1292438233=%20%20Underactuated%20wheeled%20vehicles%20are%20commonly%20studied%20as%20nonholonomic%20systems%0Awith%20periodic%20actuation.%20Twistcar%20is%20a%20classical%20example%20inspired%20by%20a%20riding%0Atoy%2C%20which%20has%20been%20analyzed%20using%20a%20planar%20model%20of%20a%20dynamical%20system%20with%0Anonholonomic%20constraints.%20Most%20of%20the%20previous%20analyses%20did%20not%20account%20for%0Aenergy%20dissipation%20due%20to%20frictional%20resistance.%20In%20this%20work%2C%20we%20study%20a%0Atheoretical%20two-link%20model%20of%20the%20Twistcar%20while%20incorporating%20dissipation%20due%0Ato%20rolling%20resistance.%20We%20obtain%20asymptotic%20expressions%20for%20the%20system%27s%0Asmall-amplitude%20steady-state%20periodic%20dynamics%2C%20which%20reveals%20the%20possibility%0Aof%20reversing%20the%20direction%20of%20motion%20upon%20varying%20the%20geometric%20and%20mass%0Aproperties%20of%20the%20vehicle.%20Next%2C%20we%20design%20and%20construct%20a%20robotic%20prototype%20of%0Athe%20Twistcar%20whose%20center-of-mass%20position%20can%20be%20shifted%20by%20adding%20and%0Aremoving%20a%20massive%20block%2C%20enabling%20experimental%20demonstration%20of%20the%20Twistcar%27s%0Adirection%20reversal%20phenomenon.%20We%20also%20conduct%20parameter%20fitting%20for%20the%0Africtional%20resistance%20in%20order%20to%20improve%20agreement%20with%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19112v2&entry.124074799=Read"},
{"title": "RabakBench: Scaling Human Annotations to Construct Localized\n  Multilingual Safety Benchmarks for Low-Resource Languages", "author": "Gabriel Chua and Leanne Tan and Ziyu Ge and Roy Ka-Wei Lee", "abstract": "  Large language models (LLMs) and their safety classifiers often perform\npoorly on low-resource languages due to limited training data and evaluation\nbenchmarks. This paper introduces RabakBench, a new multilingual safety\nbenchmark localized to Singapore's unique linguistic context, covering\nSinglish, Chinese, Malay, and Tamil. RabakBench is constructed through a\nscalable three-stage pipeline: (i) Generate - adversarial example generation by\naugmenting real Singlish web content with LLM-driven red teaming; (ii) Label -\nsemi-automated multi-label safety annotation using majority-voted LLM labelers\naligned with human judgments; and (iii) Translate - high-fidelity translation\npreserving linguistic nuance and toxicity across languages. The final dataset\ncomprises over 5,000 safety-labeled examples across four languages and six\nfine-grained safety categories with severity levels. Evaluations of 11 popular\nopen-source and closed-source guardrail classifiers reveal significant\nperformance degradation. RabakBench not only enables robust safety evaluation\nin Southeast Asian multilingual settings but also offers a reproducible\nframework for building localized safety datasets in low-resource environments.\nThe benchmark dataset, including the human-verified translations, and\nevaluation code are publicly available.\n", "link": "http://arxiv.org/abs/2507.05980v1", "date": "2025-07-08", "relevancy": 1.4107, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4785}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4783}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RabakBench%3A%20Scaling%20Human%20Annotations%20to%20Construct%20Localized%0A%20%20Multilingual%20Safety%20Benchmarks%20for%20Low-Resource%20Languages&body=Title%3A%20RabakBench%3A%20Scaling%20Human%20Annotations%20to%20Construct%20Localized%0A%20%20Multilingual%20Safety%20Benchmarks%20for%20Low-Resource%20Languages%0AAuthor%3A%20Gabriel%20Chua%20and%20Leanne%20Tan%20and%20Ziyu%20Ge%20and%20Roy%20Ka-Wei%20Lee%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20and%20their%20safety%20classifiers%20often%20perform%0Apoorly%20on%20low-resource%20languages%20due%20to%20limited%20training%20data%20and%20evaluation%0Abenchmarks.%20This%20paper%20introduces%20RabakBench%2C%20a%20new%20multilingual%20safety%0Abenchmark%20localized%20to%20Singapore%27s%20unique%20linguistic%20context%2C%20covering%0ASinglish%2C%20Chinese%2C%20Malay%2C%20and%20Tamil.%20RabakBench%20is%20constructed%20through%20a%0Ascalable%20three-stage%20pipeline%3A%20%28i%29%20Generate%20-%20adversarial%20example%20generation%20by%0Aaugmenting%20real%20Singlish%20web%20content%20with%20LLM-driven%20red%20teaming%3B%20%28ii%29%20Label%20-%0Asemi-automated%20multi-label%20safety%20annotation%20using%20majority-voted%20LLM%20labelers%0Aaligned%20with%20human%20judgments%3B%20and%20%28iii%29%20Translate%20-%20high-fidelity%20translation%0Apreserving%20linguistic%20nuance%20and%20toxicity%20across%20languages.%20The%20final%20dataset%0Acomprises%20over%205%2C000%20safety-labeled%20examples%20across%20four%20languages%20and%20six%0Afine-grained%20safety%20categories%20with%20severity%20levels.%20Evaluations%20of%2011%20popular%0Aopen-source%20and%20closed-source%20guardrail%20classifiers%20reveal%20significant%0Aperformance%20degradation.%20RabakBench%20not%20only%20enables%20robust%20safety%20evaluation%0Ain%20Southeast%20Asian%20multilingual%20settings%20but%20also%20offers%20a%20reproducible%0Aframework%20for%20building%20localized%20safety%20datasets%20in%20low-resource%20environments.%0AThe%20benchmark%20dataset%2C%20including%20the%20human-verified%20translations%2C%20and%0Aevaluation%20code%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRabakBench%253A%2520Scaling%2520Human%2520Annotations%2520to%2520Construct%2520Localized%250A%2520%2520Multilingual%2520Safety%2520Benchmarks%2520for%2520Low-Resource%2520Languages%26entry.906535625%3DGabriel%2520Chua%2520and%2520Leanne%2520Tan%2520and%2520Ziyu%2520Ge%2520and%2520Roy%2520Ka-Wei%2520Lee%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520and%2520their%2520safety%2520classifiers%2520often%2520perform%250Apoorly%2520on%2520low-resource%2520languages%2520due%2520to%2520limited%2520training%2520data%2520and%2520evaluation%250Abenchmarks.%2520This%2520paper%2520introduces%2520RabakBench%252C%2520a%2520new%2520multilingual%2520safety%250Abenchmark%2520localized%2520to%2520Singapore%2527s%2520unique%2520linguistic%2520context%252C%2520covering%250ASinglish%252C%2520Chinese%252C%2520Malay%252C%2520and%2520Tamil.%2520RabakBench%2520is%2520constructed%2520through%2520a%250Ascalable%2520three-stage%2520pipeline%253A%2520%2528i%2529%2520Generate%2520-%2520adversarial%2520example%2520generation%2520by%250Aaugmenting%2520real%2520Singlish%2520web%2520content%2520with%2520LLM-driven%2520red%2520teaming%253B%2520%2528ii%2529%2520Label%2520-%250Asemi-automated%2520multi-label%2520safety%2520annotation%2520using%2520majority-voted%2520LLM%2520labelers%250Aaligned%2520with%2520human%2520judgments%253B%2520and%2520%2528iii%2529%2520Translate%2520-%2520high-fidelity%2520translation%250Apreserving%2520linguistic%2520nuance%2520and%2520toxicity%2520across%2520languages.%2520The%2520final%2520dataset%250Acomprises%2520over%25205%252C000%2520safety-labeled%2520examples%2520across%2520four%2520languages%2520and%2520six%250Afine-grained%2520safety%2520categories%2520with%2520severity%2520levels.%2520Evaluations%2520of%252011%2520popular%250Aopen-source%2520and%2520closed-source%2520guardrail%2520classifiers%2520reveal%2520significant%250Aperformance%2520degradation.%2520RabakBench%2520not%2520only%2520enables%2520robust%2520safety%2520evaluation%250Ain%2520Southeast%2520Asian%2520multilingual%2520settings%2520but%2520also%2520offers%2520a%2520reproducible%250Aframework%2520for%2520building%2520localized%2520safety%2520datasets%2520in%2520low-resource%2520environments.%250AThe%2520benchmark%2520dataset%252C%2520including%2520the%2520human-verified%2520translations%252C%2520and%250Aevaluation%2520code%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RabakBench%3A%20Scaling%20Human%20Annotations%20to%20Construct%20Localized%0A%20%20Multilingual%20Safety%20Benchmarks%20for%20Low-Resource%20Languages&entry.906535625=Gabriel%20Chua%20and%20Leanne%20Tan%20and%20Ziyu%20Ge%20and%20Roy%20Ka-Wei%20Lee&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20and%20their%20safety%20classifiers%20often%20perform%0Apoorly%20on%20low-resource%20languages%20due%20to%20limited%20training%20data%20and%20evaluation%0Abenchmarks.%20This%20paper%20introduces%20RabakBench%2C%20a%20new%20multilingual%20safety%0Abenchmark%20localized%20to%20Singapore%27s%20unique%20linguistic%20context%2C%20covering%0ASinglish%2C%20Chinese%2C%20Malay%2C%20and%20Tamil.%20RabakBench%20is%20constructed%20through%20a%0Ascalable%20three-stage%20pipeline%3A%20%28i%29%20Generate%20-%20adversarial%20example%20generation%20by%0Aaugmenting%20real%20Singlish%20web%20content%20with%20LLM-driven%20red%20teaming%3B%20%28ii%29%20Label%20-%0Asemi-automated%20multi-label%20safety%20annotation%20using%20majority-voted%20LLM%20labelers%0Aaligned%20with%20human%20judgments%3B%20and%20%28iii%29%20Translate%20-%20high-fidelity%20translation%0Apreserving%20linguistic%20nuance%20and%20toxicity%20across%20languages.%20The%20final%20dataset%0Acomprises%20over%205%2C000%20safety-labeled%20examples%20across%20four%20languages%20and%20six%0Afine-grained%20safety%20categories%20with%20severity%20levels.%20Evaluations%20of%2011%20popular%0Aopen-source%20and%20closed-source%20guardrail%20classifiers%20reveal%20significant%0Aperformance%20degradation.%20RabakBench%20not%20only%20enables%20robust%20safety%20evaluation%0Ain%20Southeast%20Asian%20multilingual%20settings%20but%20also%20offers%20a%20reproducible%0Aframework%20for%20building%20localized%20safety%20datasets%20in%20low-resource%20environments.%0AThe%20benchmark%20dataset%2C%20including%20the%20human-verified%20translations%2C%20and%0Aevaluation%20code%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05980v1&entry.124074799=Read"},
{"title": "Is Diversity All You Need for Scalable Robotic Manipulation?", "author": "Modi Shi and Li Chen and Jin Chen and Yuxiang Lu and Chiming Liu and Guanghui Ren and Ping Luo and Di Huang and Maoqing Yao and Hongyang Li", "abstract": "  Data scaling has driven remarkable success in foundation models for Natural\nLanguage Processing (NLP) and Computer Vision (CV), yet the principles of\neffective data scaling in robotic manipulation remain insufficiently\nunderstood. In this work, we investigate the nuanced role of data diversity in\nrobot learning by examining three critical dimensions-task (what to do),\nembodiment (which robot to use), and expert (who demonstrates)-challenging the\nconventional intuition of \"more diverse is better\". Throughout extensive\nexperiments on various robot platforms, we reveal that (1) task diversity\nproves more critical than per-task demonstration quantity, benefiting transfer\nfrom diverse pre-training tasks to novel downstream scenarios; (2)\nmulti-embodiment pre-training data is optional for cross-embodiment\ntransfer-models trained on high-quality single-embodiment data can efficiently\ntransfer to different platforms, showing more desirable scaling property during\nfine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,\narising from individual operational preferences and stochastic variations in\nhuman demonstrations, can be confounding to policy learning, with velocity\nmultimodality emerging as a key contributing factor. Based on this insight, we\npropose a distribution debiasing method to mitigate velocity ambiguity, the\nyielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to\nusing 2.5 times pre-training data. Collectively, these findings provide new\nperspectives and offer practical guidance on how to scale robotic manipulation\ndatasets effectively.\n", "link": "http://arxiv.org/abs/2507.06219v1", "date": "2025-07-08", "relevancy": 1.7069, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5799}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5775}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Diversity%20All%20You%20Need%20for%20Scalable%20Robotic%20Manipulation%3F&body=Title%3A%20Is%20Diversity%20All%20You%20Need%20for%20Scalable%20Robotic%20Manipulation%3F%0AAuthor%3A%20Modi%20Shi%20and%20Li%20Chen%20and%20Jin%20Chen%20and%20Yuxiang%20Lu%20and%20Chiming%20Liu%20and%20Guanghui%20Ren%20and%20Ping%20Luo%20and%20Di%20Huang%20and%20Maoqing%20Yao%20and%20Hongyang%20Li%0AAbstract%3A%20%20%20Data%20scaling%20has%20driven%20remarkable%20success%20in%20foundation%20models%20for%20Natural%0ALanguage%20Processing%20%28NLP%29%20and%20Computer%20Vision%20%28CV%29%2C%20yet%20the%20principles%20of%0Aeffective%20data%20scaling%20in%20robotic%20manipulation%20remain%20insufficiently%0Aunderstood.%20In%20this%20work%2C%20we%20investigate%20the%20nuanced%20role%20of%20data%20diversity%20in%0Arobot%20learning%20by%20examining%20three%20critical%20dimensions-task%20%28what%20to%20do%29%2C%0Aembodiment%20%28which%20robot%20to%20use%29%2C%20and%20expert%20%28who%20demonstrates%29-challenging%20the%0Aconventional%20intuition%20of%20%22more%20diverse%20is%20better%22.%20Throughout%20extensive%0Aexperiments%20on%20various%20robot%20platforms%2C%20we%20reveal%20that%20%281%29%20task%20diversity%0Aproves%20more%20critical%20than%20per-task%20demonstration%20quantity%2C%20benefiting%20transfer%0Afrom%20diverse%20pre-training%20tasks%20to%20novel%20downstream%20scenarios%3B%20%282%29%0Amulti-embodiment%20pre-training%20data%20is%20optional%20for%20cross-embodiment%0Atransfer-models%20trained%20on%20high-quality%20single-embodiment%20data%20can%20efficiently%0Atransfer%20to%20different%20platforms%2C%20showing%20more%20desirable%20scaling%20property%20during%0Afine-tuning%20than%20multi-embodiment%20pre-trained%20models%3B%20and%20%283%29%20expert%20diversity%2C%0Aarising%20from%20individual%20operational%20preferences%20and%20stochastic%20variations%20in%0Ahuman%20demonstrations%2C%20can%20be%20confounding%20to%20policy%20learning%2C%20with%20velocity%0Amultimodality%20emerging%20as%20a%20key%20contributing%20factor.%20Based%20on%20this%20insight%2C%20we%0Apropose%20a%20distribution%20debiasing%20method%20to%20mitigate%20velocity%20ambiguity%2C%20the%0Ayielding%20GO-1-Pro%20achieves%20substantial%20performance%20gains%20of%2015%25%2C%20equivalent%20to%0Ausing%202.5%20times%20pre-training%20data.%20Collectively%2C%20these%20findings%20provide%20new%0Aperspectives%20and%20offer%20practical%20guidance%20on%20how%20to%20scale%20robotic%20manipulation%0Adatasets%20effectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Diversity%2520All%2520You%2520Need%2520for%2520Scalable%2520Robotic%2520Manipulation%253F%26entry.906535625%3DModi%2520Shi%2520and%2520Li%2520Chen%2520and%2520Jin%2520Chen%2520and%2520Yuxiang%2520Lu%2520and%2520Chiming%2520Liu%2520and%2520Guanghui%2520Ren%2520and%2520Ping%2520Luo%2520and%2520Di%2520Huang%2520and%2520Maoqing%2520Yao%2520and%2520Hongyang%2520Li%26entry.1292438233%3D%2520%2520Data%2520scaling%2520has%2520driven%2520remarkable%2520success%2520in%2520foundation%2520models%2520for%2520Natural%250ALanguage%2520Processing%2520%2528NLP%2529%2520and%2520Computer%2520Vision%2520%2528CV%2529%252C%2520yet%2520the%2520principles%2520of%250Aeffective%2520data%2520scaling%2520in%2520robotic%2520manipulation%2520remain%2520insufficiently%250Aunderstood.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520nuanced%2520role%2520of%2520data%2520diversity%2520in%250Arobot%2520learning%2520by%2520examining%2520three%2520critical%2520dimensions-task%2520%2528what%2520to%2520do%2529%252C%250Aembodiment%2520%2528which%2520robot%2520to%2520use%2529%252C%2520and%2520expert%2520%2528who%2520demonstrates%2529-challenging%2520the%250Aconventional%2520intuition%2520of%2520%2522more%2520diverse%2520is%2520better%2522.%2520Throughout%2520extensive%250Aexperiments%2520on%2520various%2520robot%2520platforms%252C%2520we%2520reveal%2520that%2520%25281%2529%2520task%2520diversity%250Aproves%2520more%2520critical%2520than%2520per-task%2520demonstration%2520quantity%252C%2520benefiting%2520transfer%250Afrom%2520diverse%2520pre-training%2520tasks%2520to%2520novel%2520downstream%2520scenarios%253B%2520%25282%2529%250Amulti-embodiment%2520pre-training%2520data%2520is%2520optional%2520for%2520cross-embodiment%250Atransfer-models%2520trained%2520on%2520high-quality%2520single-embodiment%2520data%2520can%2520efficiently%250Atransfer%2520to%2520different%2520platforms%252C%2520showing%2520more%2520desirable%2520scaling%2520property%2520during%250Afine-tuning%2520than%2520multi-embodiment%2520pre-trained%2520models%253B%2520and%2520%25283%2529%2520expert%2520diversity%252C%250Aarising%2520from%2520individual%2520operational%2520preferences%2520and%2520stochastic%2520variations%2520in%250Ahuman%2520demonstrations%252C%2520can%2520be%2520confounding%2520to%2520policy%2520learning%252C%2520with%2520velocity%250Amultimodality%2520emerging%2520as%2520a%2520key%2520contributing%2520factor.%2520Based%2520on%2520this%2520insight%252C%2520we%250Apropose%2520a%2520distribution%2520debiasing%2520method%2520to%2520mitigate%2520velocity%2520ambiguity%252C%2520the%250Ayielding%2520GO-1-Pro%2520achieves%2520substantial%2520performance%2520gains%2520of%252015%2525%252C%2520equivalent%2520to%250Ausing%25202.5%2520times%2520pre-training%2520data.%2520Collectively%252C%2520these%2520findings%2520provide%2520new%250Aperspectives%2520and%2520offer%2520practical%2520guidance%2520on%2520how%2520to%2520scale%2520robotic%2520manipulation%250Adatasets%2520effectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Diversity%20All%20You%20Need%20for%20Scalable%20Robotic%20Manipulation%3F&entry.906535625=Modi%20Shi%20and%20Li%20Chen%20and%20Jin%20Chen%20and%20Yuxiang%20Lu%20and%20Chiming%20Liu%20and%20Guanghui%20Ren%20and%20Ping%20Luo%20and%20Di%20Huang%20and%20Maoqing%20Yao%20and%20Hongyang%20Li&entry.1292438233=%20%20Data%20scaling%20has%20driven%20remarkable%20success%20in%20foundation%20models%20for%20Natural%0ALanguage%20Processing%20%28NLP%29%20and%20Computer%20Vision%20%28CV%29%2C%20yet%20the%20principles%20of%0Aeffective%20data%20scaling%20in%20robotic%20manipulation%20remain%20insufficiently%0Aunderstood.%20In%20this%20work%2C%20we%20investigate%20the%20nuanced%20role%20of%20data%20diversity%20in%0Arobot%20learning%20by%20examining%20three%20critical%20dimensions-task%20%28what%20to%20do%29%2C%0Aembodiment%20%28which%20robot%20to%20use%29%2C%20and%20expert%20%28who%20demonstrates%29-challenging%20the%0Aconventional%20intuition%20of%20%22more%20diverse%20is%20better%22.%20Throughout%20extensive%0Aexperiments%20on%20various%20robot%20platforms%2C%20we%20reveal%20that%20%281%29%20task%20diversity%0Aproves%20more%20critical%20than%20per-task%20demonstration%20quantity%2C%20benefiting%20transfer%0Afrom%20diverse%20pre-training%20tasks%20to%20novel%20downstream%20scenarios%3B%20%282%29%0Amulti-embodiment%20pre-training%20data%20is%20optional%20for%20cross-embodiment%0Atransfer-models%20trained%20on%20high-quality%20single-embodiment%20data%20can%20efficiently%0Atransfer%20to%20different%20platforms%2C%20showing%20more%20desirable%20scaling%20property%20during%0Afine-tuning%20than%20multi-embodiment%20pre-trained%20models%3B%20and%20%283%29%20expert%20diversity%2C%0Aarising%20from%20individual%20operational%20preferences%20and%20stochastic%20variations%20in%0Ahuman%20demonstrations%2C%20can%20be%20confounding%20to%20policy%20learning%2C%20with%20velocity%0Amultimodality%20emerging%20as%20a%20key%20contributing%20factor.%20Based%20on%20this%20insight%2C%20we%0Apropose%20a%20distribution%20debiasing%20method%20to%20mitigate%20velocity%20ambiguity%2C%20the%0Ayielding%20GO-1-Pro%20achieves%20substantial%20performance%20gains%20of%2015%25%2C%20equivalent%20to%0Ausing%202.5%20times%20pre-training%20data.%20Collectively%2C%20these%20findings%20provide%20new%0Aperspectives%20and%20offer%20practical%20guidance%20on%20how%20to%20scale%20robotic%20manipulation%0Adatasets%20effectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06219v1&entry.124074799=Read"},
{"title": "DRAN: A Distribution and Relation Adaptive Network for Spatio-temporal\n  Forecasting", "author": "Xiaobei Zou and Luolin Xiong and Kexuan Zhang and Cesare Alippi and Yang Tang", "abstract": "  Accurate predictions of spatio-temporal systems are crucial for tasks such as\nsystem management, control, and crisis prevention. However, the inherent time\nvariance of many spatio-temporal systems poses challenges to achieving accurate\npredictions whenever stationarity is not granted. In order to address\nnon-stationarity, we propose a Distribution and Relation Adaptive Network\n(DRAN) capable of dynamically adapting to relation and distribution changes\nover time. While temporal normalization and de-normalization are frequently\nused techniques to adapt to distribution shifts, this operation is not suitable\nfor the spatio-temporal context as temporal normalization scales the time\nseries of nodes and possibly disrupts the spatial relations among nodes. In\norder to address this problem, a Spatial Factor Learner (SFL) module is\ndeveloped that enables the normalization and de-normalization process. To adapt\nto dynamic changes in spatial relationships among sensors, we propose a\nDynamic-Static Fusion Learner (DSFL) module that effectively integrates\nfeatures learned from both dynamic and static relations through an adaptive\nfusion ratio mechanism. Furthermore, we introduce a Stochastic Learner to\ncapture the noisy components of spatio-temporal representations. Our approach\noutperforms state-of-the-art methods on weather prediction and traffic flow\nforecasting tasks.Experimental results show that our SFL efficiently preserves\nspatial relationships across various temporal normalization operations.\nVisualizations of the learned dynamic and static relations demonstrate that\nDSFL can capture both local and distant relationships between nodes.\n", "link": "http://arxiv.org/abs/2504.01531v2", "date": "2025-07-08", "relevancy": 1.5033, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5182}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5027}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRAN%3A%20A%20Distribution%20and%20Relation%20Adaptive%20Network%20for%20Spatio-temporal%0A%20%20Forecasting&body=Title%3A%20DRAN%3A%20A%20Distribution%20and%20Relation%20Adaptive%20Network%20for%20Spatio-temporal%0A%20%20Forecasting%0AAuthor%3A%20Xiaobei%20Zou%20and%20Luolin%20Xiong%20and%20Kexuan%20Zhang%20and%20Cesare%20Alippi%20and%20Yang%20Tang%0AAbstract%3A%20%20%20Accurate%20predictions%20of%20spatio-temporal%20systems%20are%20crucial%20for%20tasks%20such%20as%0Asystem%20management%2C%20control%2C%20and%20crisis%20prevention.%20However%2C%20the%20inherent%20time%0Avariance%20of%20many%20spatio-temporal%20systems%20poses%20challenges%20to%20achieving%20accurate%0Apredictions%20whenever%20stationarity%20is%20not%20granted.%20In%20order%20to%20address%0Anon-stationarity%2C%20we%20propose%20a%20Distribution%20and%20Relation%20Adaptive%20Network%0A%28DRAN%29%20capable%20of%20dynamically%20adapting%20to%20relation%20and%20distribution%20changes%0Aover%20time.%20While%20temporal%20normalization%20and%20de-normalization%20are%20frequently%0Aused%20techniques%20to%20adapt%20to%20distribution%20shifts%2C%20this%20operation%20is%20not%20suitable%0Afor%20the%20spatio-temporal%20context%20as%20temporal%20normalization%20scales%20the%20time%0Aseries%20of%20nodes%20and%20possibly%20disrupts%20the%20spatial%20relations%20among%20nodes.%20In%0Aorder%20to%20address%20this%20problem%2C%20a%20Spatial%20Factor%20Learner%20%28SFL%29%20module%20is%0Adeveloped%20that%20enables%20the%20normalization%20and%20de-normalization%20process.%20To%20adapt%0Ato%20dynamic%20changes%20in%20spatial%20relationships%20among%20sensors%2C%20we%20propose%20a%0ADynamic-Static%20Fusion%20Learner%20%28DSFL%29%20module%20that%20effectively%20integrates%0Afeatures%20learned%20from%20both%20dynamic%20and%20static%20relations%20through%20an%20adaptive%0Afusion%20ratio%20mechanism.%20Furthermore%2C%20we%20introduce%20a%20Stochastic%20Learner%20to%0Acapture%20the%20noisy%20components%20of%20spatio-temporal%20representations.%20Our%20approach%0Aoutperforms%20state-of-the-art%20methods%20on%20weather%20prediction%20and%20traffic%20flow%0Aforecasting%20tasks.Experimental%20results%20show%20that%20our%20SFL%20efficiently%20preserves%0Aspatial%20relationships%20across%20various%20temporal%20normalization%20operations.%0AVisualizations%20of%20the%20learned%20dynamic%20and%20static%20relations%20demonstrate%20that%0ADSFL%20can%20capture%20both%20local%20and%20distant%20relationships%20between%20nodes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01531v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRAN%253A%2520A%2520Distribution%2520and%2520Relation%2520Adaptive%2520Network%2520for%2520Spatio-temporal%250A%2520%2520Forecasting%26entry.906535625%3DXiaobei%2520Zou%2520and%2520Luolin%2520Xiong%2520and%2520Kexuan%2520Zhang%2520and%2520Cesare%2520Alippi%2520and%2520Yang%2520Tang%26entry.1292438233%3D%2520%2520Accurate%2520predictions%2520of%2520spatio-temporal%2520systems%2520are%2520crucial%2520for%2520tasks%2520such%2520as%250Asystem%2520management%252C%2520control%252C%2520and%2520crisis%2520prevention.%2520However%252C%2520the%2520inherent%2520time%250Avariance%2520of%2520many%2520spatio-temporal%2520systems%2520poses%2520challenges%2520to%2520achieving%2520accurate%250Apredictions%2520whenever%2520stationarity%2520is%2520not%2520granted.%2520In%2520order%2520to%2520address%250Anon-stationarity%252C%2520we%2520propose%2520a%2520Distribution%2520and%2520Relation%2520Adaptive%2520Network%250A%2528DRAN%2529%2520capable%2520of%2520dynamically%2520adapting%2520to%2520relation%2520and%2520distribution%2520changes%250Aover%2520time.%2520While%2520temporal%2520normalization%2520and%2520de-normalization%2520are%2520frequently%250Aused%2520techniques%2520to%2520adapt%2520to%2520distribution%2520shifts%252C%2520this%2520operation%2520is%2520not%2520suitable%250Afor%2520the%2520spatio-temporal%2520context%2520as%2520temporal%2520normalization%2520scales%2520the%2520time%250Aseries%2520of%2520nodes%2520and%2520possibly%2520disrupts%2520the%2520spatial%2520relations%2520among%2520nodes.%2520In%250Aorder%2520to%2520address%2520this%2520problem%252C%2520a%2520Spatial%2520Factor%2520Learner%2520%2528SFL%2529%2520module%2520is%250Adeveloped%2520that%2520enables%2520the%2520normalization%2520and%2520de-normalization%2520process.%2520To%2520adapt%250Ato%2520dynamic%2520changes%2520in%2520spatial%2520relationships%2520among%2520sensors%252C%2520we%2520propose%2520a%250ADynamic-Static%2520Fusion%2520Learner%2520%2528DSFL%2529%2520module%2520that%2520effectively%2520integrates%250Afeatures%2520learned%2520from%2520both%2520dynamic%2520and%2520static%2520relations%2520through%2520an%2520adaptive%250Afusion%2520ratio%2520mechanism.%2520Furthermore%252C%2520we%2520introduce%2520a%2520Stochastic%2520Learner%2520to%250Acapture%2520the%2520noisy%2520components%2520of%2520spatio-temporal%2520representations.%2520Our%2520approach%250Aoutperforms%2520state-of-the-art%2520methods%2520on%2520weather%2520prediction%2520and%2520traffic%2520flow%250Aforecasting%2520tasks.Experimental%2520results%2520show%2520that%2520our%2520SFL%2520efficiently%2520preserves%250Aspatial%2520relationships%2520across%2520various%2520temporal%2520normalization%2520operations.%250AVisualizations%2520of%2520the%2520learned%2520dynamic%2520and%2520static%2520relations%2520demonstrate%2520that%250ADSFL%2520can%2520capture%2520both%2520local%2520and%2520distant%2520relationships%2520between%2520nodes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01531v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRAN%3A%20A%20Distribution%20and%20Relation%20Adaptive%20Network%20for%20Spatio-temporal%0A%20%20Forecasting&entry.906535625=Xiaobei%20Zou%20and%20Luolin%20Xiong%20and%20Kexuan%20Zhang%20and%20Cesare%20Alippi%20and%20Yang%20Tang&entry.1292438233=%20%20Accurate%20predictions%20of%20spatio-temporal%20systems%20are%20crucial%20for%20tasks%20such%20as%0Asystem%20management%2C%20control%2C%20and%20crisis%20prevention.%20However%2C%20the%20inherent%20time%0Avariance%20of%20many%20spatio-temporal%20systems%20poses%20challenges%20to%20achieving%20accurate%0Apredictions%20whenever%20stationarity%20is%20not%20granted.%20In%20order%20to%20address%0Anon-stationarity%2C%20we%20propose%20a%20Distribution%20and%20Relation%20Adaptive%20Network%0A%28DRAN%29%20capable%20of%20dynamically%20adapting%20to%20relation%20and%20distribution%20changes%0Aover%20time.%20While%20temporal%20normalization%20and%20de-normalization%20are%20frequently%0Aused%20techniques%20to%20adapt%20to%20distribution%20shifts%2C%20this%20operation%20is%20not%20suitable%0Afor%20the%20spatio-temporal%20context%20as%20temporal%20normalization%20scales%20the%20time%0Aseries%20of%20nodes%20and%20possibly%20disrupts%20the%20spatial%20relations%20among%20nodes.%20In%0Aorder%20to%20address%20this%20problem%2C%20a%20Spatial%20Factor%20Learner%20%28SFL%29%20module%20is%0Adeveloped%20that%20enables%20the%20normalization%20and%20de-normalization%20process.%20To%20adapt%0Ato%20dynamic%20changes%20in%20spatial%20relationships%20among%20sensors%2C%20we%20propose%20a%0ADynamic-Static%20Fusion%20Learner%20%28DSFL%29%20module%20that%20effectively%20integrates%0Afeatures%20learned%20from%20both%20dynamic%20and%20static%20relations%20through%20an%20adaptive%0Afusion%20ratio%20mechanism.%20Furthermore%2C%20we%20introduce%20a%20Stochastic%20Learner%20to%0Acapture%20the%20noisy%20components%20of%20spatio-temporal%20representations.%20Our%20approach%0Aoutperforms%20state-of-the-art%20methods%20on%20weather%20prediction%20and%20traffic%20flow%0Aforecasting%20tasks.Experimental%20results%20show%20that%20our%20SFL%20efficiently%20preserves%0Aspatial%20relationships%20across%20various%20temporal%20normalization%20operations.%0AVisualizations%20of%20the%20learned%20dynamic%20and%20static%20relations%20demonstrate%20that%0ADSFL%20can%20capture%20both%20local%20and%20distant%20relationships%20between%20nodes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01531v2&entry.124074799=Read"},
{"title": "Transformers Simulate MLE for Sequence Generation in Bayesian Networks", "author": "Yuan Cao and Yihan He and Dennis Wu and Hong-Yu Chen and Jianqing Fan and Han Liu", "abstract": "  Transformers have achieved significant success in various fields, notably\nexcelling in tasks involving sequential data like natural language processing.\nDespite these achievements, the theoretical understanding of transformers'\ncapabilities remains limited. In this paper, we investigate the theoretical\ncapabilities of transformers to autoregressively generate sequences in Bayesian\nnetworks based on in-context maximum likelihood estimation (MLE). Specifically,\nwe consider a setting where a context is formed by a set of independent\nsequences generated according to a Bayesian network. We demonstrate that there\nexists a simple transformer model that can (i) estimate the conditional\nprobabilities of the Bayesian network according to the context, and (ii)\nautoregressively generate a new sample according to the Bayesian network with\nestimated conditional probabilities. We further demonstrate in extensive\nexperiments that such a transformer does not only exist in theory, but can also\nbe effectively obtained through training. Our analysis highlights the potential\nof transformers to learn complex probabilistic models and contributes to a\nbetter understanding of large language models as a powerful class of sequence\ngenerators.\n", "link": "http://arxiv.org/abs/2501.02547v2", "date": "2025-07-08", "relevancy": 1.5817, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5377}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5347}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20Simulate%20MLE%20for%20Sequence%20Generation%20in%20Bayesian%20Networks&body=Title%3A%20Transformers%20Simulate%20MLE%20for%20Sequence%20Generation%20in%20Bayesian%20Networks%0AAuthor%3A%20Yuan%20Cao%20and%20Yihan%20He%20and%20Dennis%20Wu%20and%20Hong-Yu%20Chen%20and%20Jianqing%20Fan%20and%20Han%20Liu%0AAbstract%3A%20%20%20Transformers%20have%20achieved%20significant%20success%20in%20various%20fields%2C%20notably%0Aexcelling%20in%20tasks%20involving%20sequential%20data%20like%20natural%20language%20processing.%0ADespite%20these%20achievements%2C%20the%20theoretical%20understanding%20of%20transformers%27%0Acapabilities%20remains%20limited.%20In%20this%20paper%2C%20we%20investigate%20the%20theoretical%0Acapabilities%20of%20transformers%20to%20autoregressively%20generate%20sequences%20in%20Bayesian%0Anetworks%20based%20on%20in-context%20maximum%20likelihood%20estimation%20%28MLE%29.%20Specifically%2C%0Awe%20consider%20a%20setting%20where%20a%20context%20is%20formed%20by%20a%20set%20of%20independent%0Asequences%20generated%20according%20to%20a%20Bayesian%20network.%20We%20demonstrate%20that%20there%0Aexists%20a%20simple%20transformer%20model%20that%20can%20%28i%29%20estimate%20the%20conditional%0Aprobabilities%20of%20the%20Bayesian%20network%20according%20to%20the%20context%2C%20and%20%28ii%29%0Aautoregressively%20generate%20a%20new%20sample%20according%20to%20the%20Bayesian%20network%20with%0Aestimated%20conditional%20probabilities.%20We%20further%20demonstrate%20in%20extensive%0Aexperiments%20that%20such%20a%20transformer%20does%20not%20only%20exist%20in%20theory%2C%20but%20can%20also%0Abe%20effectively%20obtained%20through%20training.%20Our%20analysis%20highlights%20the%20potential%0Aof%20transformers%20to%20learn%20complex%20probabilistic%20models%20and%20contributes%20to%20a%0Abetter%20understanding%20of%20large%20language%20models%20as%20a%20powerful%20class%20of%20sequence%0Agenerators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520Simulate%2520MLE%2520for%2520Sequence%2520Generation%2520in%2520Bayesian%2520Networks%26entry.906535625%3DYuan%2520Cao%2520and%2520Yihan%2520He%2520and%2520Dennis%2520Wu%2520and%2520Hong-Yu%2520Chen%2520and%2520Jianqing%2520Fan%2520and%2520Han%2520Liu%26entry.1292438233%3D%2520%2520Transformers%2520have%2520achieved%2520significant%2520success%2520in%2520various%2520fields%252C%2520notably%250Aexcelling%2520in%2520tasks%2520involving%2520sequential%2520data%2520like%2520natural%2520language%2520processing.%250ADespite%2520these%2520achievements%252C%2520the%2520theoretical%2520understanding%2520of%2520transformers%2527%250Acapabilities%2520remains%2520limited.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520theoretical%250Acapabilities%2520of%2520transformers%2520to%2520autoregressively%2520generate%2520sequences%2520in%2520Bayesian%250Anetworks%2520based%2520on%2520in-context%2520maximum%2520likelihood%2520estimation%2520%2528MLE%2529.%2520Specifically%252C%250Awe%2520consider%2520a%2520setting%2520where%2520a%2520context%2520is%2520formed%2520by%2520a%2520set%2520of%2520independent%250Asequences%2520generated%2520according%2520to%2520a%2520Bayesian%2520network.%2520We%2520demonstrate%2520that%2520there%250Aexists%2520a%2520simple%2520transformer%2520model%2520that%2520can%2520%2528i%2529%2520estimate%2520the%2520conditional%250Aprobabilities%2520of%2520the%2520Bayesian%2520network%2520according%2520to%2520the%2520context%252C%2520and%2520%2528ii%2529%250Aautoregressively%2520generate%2520a%2520new%2520sample%2520according%2520to%2520the%2520Bayesian%2520network%2520with%250Aestimated%2520conditional%2520probabilities.%2520We%2520further%2520demonstrate%2520in%2520extensive%250Aexperiments%2520that%2520such%2520a%2520transformer%2520does%2520not%2520only%2520exist%2520in%2520theory%252C%2520but%2520can%2520also%250Abe%2520effectively%2520obtained%2520through%2520training.%2520Our%2520analysis%2520highlights%2520the%2520potential%250Aof%2520transformers%2520to%2520learn%2520complex%2520probabilistic%2520models%2520and%2520contributes%2520to%2520a%250Abetter%2520understanding%2520of%2520large%2520language%2520models%2520as%2520a%2520powerful%2520class%2520of%2520sequence%250Agenerators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20Simulate%20MLE%20for%20Sequence%20Generation%20in%20Bayesian%20Networks&entry.906535625=Yuan%20Cao%20and%20Yihan%20He%20and%20Dennis%20Wu%20and%20Hong-Yu%20Chen%20and%20Jianqing%20Fan%20and%20Han%20Liu&entry.1292438233=%20%20Transformers%20have%20achieved%20significant%20success%20in%20various%20fields%2C%20notably%0Aexcelling%20in%20tasks%20involving%20sequential%20data%20like%20natural%20language%20processing.%0ADespite%20these%20achievements%2C%20the%20theoretical%20understanding%20of%20transformers%27%0Acapabilities%20remains%20limited.%20In%20this%20paper%2C%20we%20investigate%20the%20theoretical%0Acapabilities%20of%20transformers%20to%20autoregressively%20generate%20sequences%20in%20Bayesian%0Anetworks%20based%20on%20in-context%20maximum%20likelihood%20estimation%20%28MLE%29.%20Specifically%2C%0Awe%20consider%20a%20setting%20where%20a%20context%20is%20formed%20by%20a%20set%20of%20independent%0Asequences%20generated%20according%20to%20a%20Bayesian%20network.%20We%20demonstrate%20that%20there%0Aexists%20a%20simple%20transformer%20model%20that%20can%20%28i%29%20estimate%20the%20conditional%0Aprobabilities%20of%20the%20Bayesian%20network%20according%20to%20the%20context%2C%20and%20%28ii%29%0Aautoregressively%20generate%20a%20new%20sample%20according%20to%20the%20Bayesian%20network%20with%0Aestimated%20conditional%20probabilities.%20We%20further%20demonstrate%20in%20extensive%0Aexperiments%20that%20such%20a%20transformer%20does%20not%20only%20exist%20in%20theory%2C%20but%20can%20also%0Abe%20effectively%20obtained%20through%20training.%20Our%20analysis%20highlights%20the%20potential%0Aof%20transformers%20to%20learn%20complex%20probabilistic%20models%20and%20contributes%20to%20a%0Abetter%20understanding%20of%20large%20language%20models%20as%20a%20powerful%20class%20of%20sequence%0Agenerators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02547v2&entry.124074799=Read"},
{"title": "Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution\n  Detection and Policy Adaptation", "author": "Mohamad H. Danesh and Maxime Wabartha and Stanley Wu and Joelle Pineau and Hsiu-Chin Lin", "abstract": "  Deploying reinforcement learning (RL) policies in real-world involves\nsignificant challenges, including distribution shifts, safety concerns, and the\nimpracticality of direct interactions during policy refinement. Existing\nmethods, such as domain randomization (DR) and off-dynamics RL, enhance policy\nrobustness by direct interaction with the target domain, an inherently unsafe\npractice. We propose Uncertainty-Aware RL (UARL), a novel framework that\nprioritizes safety during training by addressing Out-Of-Distribution (OOD)\ndetection and policy adaptation without requiring direct interactions in target\ndomain. UARL employs an ensemble of critics to quantify policy uncertainty and\nincorporates progressive environmental randomization to prepare the policy for\ndiverse real-world conditions. By iteratively refining over high-uncertainty\nregions of the state space in simulated environments, UARL enhances robust\ngeneralization to the target domain without explicitly training on it. We\nevaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its\neffectiveness in reliable OOD detection, improved performance, and enhanced\nsample efficiency compared to baselines.\n", "link": "http://arxiv.org/abs/2507.06111v1", "date": "2025-07-08", "relevancy": 1.6675, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.573}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5549}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Domain%20Randomization%20via%20Uncertainty-Aware%20Out-of-Distribution%0A%20%20Detection%20and%20Policy%20Adaptation&body=Title%3A%20Safe%20Domain%20Randomization%20via%20Uncertainty-Aware%20Out-of-Distribution%0A%20%20Detection%20and%20Policy%20Adaptation%0AAuthor%3A%20Mohamad%20H.%20Danesh%20and%20Maxime%20Wabartha%20and%20Stanley%20Wu%20and%20Joelle%20Pineau%20and%20Hsiu-Chin%20Lin%0AAbstract%3A%20%20%20Deploying%20reinforcement%20learning%20%28RL%29%20policies%20in%20real-world%20involves%0Asignificant%20challenges%2C%20including%20distribution%20shifts%2C%20safety%20concerns%2C%20and%20the%0Aimpracticality%20of%20direct%20interactions%20during%20policy%20refinement.%20Existing%0Amethods%2C%20such%20as%20domain%20randomization%20%28DR%29%20and%20off-dynamics%20RL%2C%20enhance%20policy%0Arobustness%20by%20direct%20interaction%20with%20the%20target%20domain%2C%20an%20inherently%20unsafe%0Apractice.%20We%20propose%20Uncertainty-Aware%20RL%20%28UARL%29%2C%20a%20novel%20framework%20that%0Aprioritizes%20safety%20during%20training%20by%20addressing%20Out-Of-Distribution%20%28OOD%29%0Adetection%20and%20policy%20adaptation%20without%20requiring%20direct%20interactions%20in%20target%0Adomain.%20UARL%20employs%20an%20ensemble%20of%20critics%20to%20quantify%20policy%20uncertainty%20and%0Aincorporates%20progressive%20environmental%20randomization%20to%20prepare%20the%20policy%20for%0Adiverse%20real-world%20conditions.%20By%20iteratively%20refining%20over%20high-uncertainty%0Aregions%20of%20the%20state%20space%20in%20simulated%20environments%2C%20UARL%20enhances%20robust%0Ageneralization%20to%20the%20target%20domain%20without%20explicitly%20training%20on%20it.%20We%0Aevaluate%20UARL%20on%20MuJoCo%20benchmarks%20and%20a%20quadrupedal%20robot%2C%20demonstrating%20its%0Aeffectiveness%20in%20reliable%20OOD%20detection%2C%20improved%20performance%2C%20and%20enhanced%0Asample%20efficiency%20compared%20to%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Domain%2520Randomization%2520via%2520Uncertainty-Aware%2520Out-of-Distribution%250A%2520%2520Detection%2520and%2520Policy%2520Adaptation%26entry.906535625%3DMohamad%2520H.%2520Danesh%2520and%2520Maxime%2520Wabartha%2520and%2520Stanley%2520Wu%2520and%2520Joelle%2520Pineau%2520and%2520Hsiu-Chin%2520Lin%26entry.1292438233%3D%2520%2520Deploying%2520reinforcement%2520learning%2520%2528RL%2529%2520policies%2520in%2520real-world%2520involves%250Asignificant%2520challenges%252C%2520including%2520distribution%2520shifts%252C%2520safety%2520concerns%252C%2520and%2520the%250Aimpracticality%2520of%2520direct%2520interactions%2520during%2520policy%2520refinement.%2520Existing%250Amethods%252C%2520such%2520as%2520domain%2520randomization%2520%2528DR%2529%2520and%2520off-dynamics%2520RL%252C%2520enhance%2520policy%250Arobustness%2520by%2520direct%2520interaction%2520with%2520the%2520target%2520domain%252C%2520an%2520inherently%2520unsafe%250Apractice.%2520We%2520propose%2520Uncertainty-Aware%2520RL%2520%2528UARL%2529%252C%2520a%2520novel%2520framework%2520that%250Aprioritizes%2520safety%2520during%2520training%2520by%2520addressing%2520Out-Of-Distribution%2520%2528OOD%2529%250Adetection%2520and%2520policy%2520adaptation%2520without%2520requiring%2520direct%2520interactions%2520in%2520target%250Adomain.%2520UARL%2520employs%2520an%2520ensemble%2520of%2520critics%2520to%2520quantify%2520policy%2520uncertainty%2520and%250Aincorporates%2520progressive%2520environmental%2520randomization%2520to%2520prepare%2520the%2520policy%2520for%250Adiverse%2520real-world%2520conditions.%2520By%2520iteratively%2520refining%2520over%2520high-uncertainty%250Aregions%2520of%2520the%2520state%2520space%2520in%2520simulated%2520environments%252C%2520UARL%2520enhances%2520robust%250Ageneralization%2520to%2520the%2520target%2520domain%2520without%2520explicitly%2520training%2520on%2520it.%2520We%250Aevaluate%2520UARL%2520on%2520MuJoCo%2520benchmarks%2520and%2520a%2520quadrupedal%2520robot%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520reliable%2520OOD%2520detection%252C%2520improved%2520performance%252C%2520and%2520enhanced%250Asample%2520efficiency%2520compared%2520to%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Domain%20Randomization%20via%20Uncertainty-Aware%20Out-of-Distribution%0A%20%20Detection%20and%20Policy%20Adaptation&entry.906535625=Mohamad%20H.%20Danesh%20and%20Maxime%20Wabartha%20and%20Stanley%20Wu%20and%20Joelle%20Pineau%20and%20Hsiu-Chin%20Lin&entry.1292438233=%20%20Deploying%20reinforcement%20learning%20%28RL%29%20policies%20in%20real-world%20involves%0Asignificant%20challenges%2C%20including%20distribution%20shifts%2C%20safety%20concerns%2C%20and%20the%0Aimpracticality%20of%20direct%20interactions%20during%20policy%20refinement.%20Existing%0Amethods%2C%20such%20as%20domain%20randomization%20%28DR%29%20and%20off-dynamics%20RL%2C%20enhance%20policy%0Arobustness%20by%20direct%20interaction%20with%20the%20target%20domain%2C%20an%20inherently%20unsafe%0Apractice.%20We%20propose%20Uncertainty-Aware%20RL%20%28UARL%29%2C%20a%20novel%20framework%20that%0Aprioritizes%20safety%20during%20training%20by%20addressing%20Out-Of-Distribution%20%28OOD%29%0Adetection%20and%20policy%20adaptation%20without%20requiring%20direct%20interactions%20in%20target%0Adomain.%20UARL%20employs%20an%20ensemble%20of%20critics%20to%20quantify%20policy%20uncertainty%20and%0Aincorporates%20progressive%20environmental%20randomization%20to%20prepare%20the%20policy%20for%0Adiverse%20real-world%20conditions.%20By%20iteratively%20refining%20over%20high-uncertainty%0Aregions%20of%20the%20state%20space%20in%20simulated%20environments%2C%20UARL%20enhances%20robust%0Ageneralization%20to%20the%20target%20domain%20without%20explicitly%20training%20on%20it.%20We%0Aevaluate%20UARL%20on%20MuJoCo%20benchmarks%20and%20a%20quadrupedal%20robot%2C%20demonstrating%20its%0Aeffectiveness%20in%20reliable%20OOD%20detection%2C%20improved%20performance%2C%20and%20enhanced%0Asample%20efficiency%20compared%20to%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06111v1&entry.124074799=Read"},
{"title": "Normalizing Diffusion Kernels with Optimal Transport", "author": "Nathan Kessler and Robin Magnet and Jean Feydy", "abstract": "  Smoothing a signal based on local neighborhoods is a core operation in\nmachine learning and geometry processing. On well-structured domains such as\nvector spaces and manifolds, the Laplace operator derived from differential\ngeometry offers a principled approach to smoothing via heat diffusion, with\nstrong theoretical guarantees. However, constructing such Laplacians requires a\ncarefully defined domain structure, which is not always available. Most\npractitioners thus rely on simple convolution kernels and message-passing\nlayers, which are biased against the boundaries of the domain. We bridge this\ngap by introducing a broad class of smoothing operators, derived from general\nsimilarity or adjacency matrices, and demonstrate that they can be normalized\ninto diffusion-like operators that inherit desirable properties from\nLaplacians. Our approach relies on a symmetric variant of the Sinkhorn\nalgorithm, which rescales positive smoothing operators to match the structural\nbehavior of heat diffusion. This construction enables Laplacian-like smoothing\nand processing of irregular data such as point clouds, sparse voxel grids or\nmixture of Gaussians. We show that the resulting operators not only approximate\nheat diffusion but also retain spectral information from the Laplacian itself,\nwith applications to shape analysis and matching.\n", "link": "http://arxiv.org/abs/2507.06161v1", "date": "2025-07-08", "relevancy": 1.5226, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5267}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5104}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normalizing%20Diffusion%20Kernels%20with%20Optimal%20Transport&body=Title%3A%20Normalizing%20Diffusion%20Kernels%20with%20Optimal%20Transport%0AAuthor%3A%20Nathan%20Kessler%20and%20Robin%20Magnet%20and%20Jean%20Feydy%0AAbstract%3A%20%20%20Smoothing%20a%20signal%20based%20on%20local%20neighborhoods%20is%20a%20core%20operation%20in%0Amachine%20learning%20and%20geometry%20processing.%20On%20well-structured%20domains%20such%20as%0Avector%20spaces%20and%20manifolds%2C%20the%20Laplace%20operator%20derived%20from%20differential%0Ageometry%20offers%20a%20principled%20approach%20to%20smoothing%20via%20heat%20diffusion%2C%20with%0Astrong%20theoretical%20guarantees.%20However%2C%20constructing%20such%20Laplacians%20requires%20a%0Acarefully%20defined%20domain%20structure%2C%20which%20is%20not%20always%20available.%20Most%0Apractitioners%20thus%20rely%20on%20simple%20convolution%20kernels%20and%20message-passing%0Alayers%2C%20which%20are%20biased%20against%20the%20boundaries%20of%20the%20domain.%20We%20bridge%20this%0Agap%20by%20introducing%20a%20broad%20class%20of%20smoothing%20operators%2C%20derived%20from%20general%0Asimilarity%20or%20adjacency%20matrices%2C%20and%20demonstrate%20that%20they%20can%20be%20normalized%0Ainto%20diffusion-like%20operators%20that%20inherit%20desirable%20properties%20from%0ALaplacians.%20Our%20approach%20relies%20on%20a%20symmetric%20variant%20of%20the%20Sinkhorn%0Aalgorithm%2C%20which%20rescales%20positive%20smoothing%20operators%20to%20match%20the%20structural%0Abehavior%20of%20heat%20diffusion.%20This%20construction%20enables%20Laplacian-like%20smoothing%0Aand%20processing%20of%20irregular%20data%20such%20as%20point%20clouds%2C%20sparse%20voxel%20grids%20or%0Amixture%20of%20Gaussians.%20We%20show%20that%20the%20resulting%20operators%20not%20only%20approximate%0Aheat%20diffusion%20but%20also%20retain%20spectral%20information%20from%20the%20Laplacian%20itself%2C%0Awith%20applications%20to%20shape%20analysis%20and%20matching.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormalizing%2520Diffusion%2520Kernels%2520with%2520Optimal%2520Transport%26entry.906535625%3DNathan%2520Kessler%2520and%2520Robin%2520Magnet%2520and%2520Jean%2520Feydy%26entry.1292438233%3D%2520%2520Smoothing%2520a%2520signal%2520based%2520on%2520local%2520neighborhoods%2520is%2520a%2520core%2520operation%2520in%250Amachine%2520learning%2520and%2520geometry%2520processing.%2520On%2520well-structured%2520domains%2520such%2520as%250Avector%2520spaces%2520and%2520manifolds%252C%2520the%2520Laplace%2520operator%2520derived%2520from%2520differential%250Ageometry%2520offers%2520a%2520principled%2520approach%2520to%2520smoothing%2520via%2520heat%2520diffusion%252C%2520with%250Astrong%2520theoretical%2520guarantees.%2520However%252C%2520constructing%2520such%2520Laplacians%2520requires%2520a%250Acarefully%2520defined%2520domain%2520structure%252C%2520which%2520is%2520not%2520always%2520available.%2520Most%250Apractitioners%2520thus%2520rely%2520on%2520simple%2520convolution%2520kernels%2520and%2520message-passing%250Alayers%252C%2520which%2520are%2520biased%2520against%2520the%2520boundaries%2520of%2520the%2520domain.%2520We%2520bridge%2520this%250Agap%2520by%2520introducing%2520a%2520broad%2520class%2520of%2520smoothing%2520operators%252C%2520derived%2520from%2520general%250Asimilarity%2520or%2520adjacency%2520matrices%252C%2520and%2520demonstrate%2520that%2520they%2520can%2520be%2520normalized%250Ainto%2520diffusion-like%2520operators%2520that%2520inherit%2520desirable%2520properties%2520from%250ALaplacians.%2520Our%2520approach%2520relies%2520on%2520a%2520symmetric%2520variant%2520of%2520the%2520Sinkhorn%250Aalgorithm%252C%2520which%2520rescales%2520positive%2520smoothing%2520operators%2520to%2520match%2520the%2520structural%250Abehavior%2520of%2520heat%2520diffusion.%2520This%2520construction%2520enables%2520Laplacian-like%2520smoothing%250Aand%2520processing%2520of%2520irregular%2520data%2520such%2520as%2520point%2520clouds%252C%2520sparse%2520voxel%2520grids%2520or%250Amixture%2520of%2520Gaussians.%2520We%2520show%2520that%2520the%2520resulting%2520operators%2520not%2520only%2520approximate%250Aheat%2520diffusion%2520but%2520also%2520retain%2520spectral%2520information%2520from%2520the%2520Laplacian%2520itself%252C%250Awith%2520applications%2520to%2520shape%2520analysis%2520and%2520matching.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normalizing%20Diffusion%20Kernels%20with%20Optimal%20Transport&entry.906535625=Nathan%20Kessler%20and%20Robin%20Magnet%20and%20Jean%20Feydy&entry.1292438233=%20%20Smoothing%20a%20signal%20based%20on%20local%20neighborhoods%20is%20a%20core%20operation%20in%0Amachine%20learning%20and%20geometry%20processing.%20On%20well-structured%20domains%20such%20as%0Avector%20spaces%20and%20manifolds%2C%20the%20Laplace%20operator%20derived%20from%20differential%0Ageometry%20offers%20a%20principled%20approach%20to%20smoothing%20via%20heat%20diffusion%2C%20with%0Astrong%20theoretical%20guarantees.%20However%2C%20constructing%20such%20Laplacians%20requires%20a%0Acarefully%20defined%20domain%20structure%2C%20which%20is%20not%20always%20available.%20Most%0Apractitioners%20thus%20rely%20on%20simple%20convolution%20kernels%20and%20message-passing%0Alayers%2C%20which%20are%20biased%20against%20the%20boundaries%20of%20the%20domain.%20We%20bridge%20this%0Agap%20by%20introducing%20a%20broad%20class%20of%20smoothing%20operators%2C%20derived%20from%20general%0Asimilarity%20or%20adjacency%20matrices%2C%20and%20demonstrate%20that%20they%20can%20be%20normalized%0Ainto%20diffusion-like%20operators%20that%20inherit%20desirable%20properties%20from%0ALaplacians.%20Our%20approach%20relies%20on%20a%20symmetric%20variant%20of%20the%20Sinkhorn%0Aalgorithm%2C%20which%20rescales%20positive%20smoothing%20operators%20to%20match%20the%20structural%0Abehavior%20of%20heat%20diffusion.%20This%20construction%20enables%20Laplacian-like%20smoothing%0Aand%20processing%20of%20irregular%20data%20such%20as%20point%20clouds%2C%20sparse%20voxel%20grids%20or%0Amixture%20of%20Gaussians.%20We%20show%20that%20the%20resulting%20operators%20not%20only%20approximate%0Aheat%20diffusion%20but%20also%20retain%20spectral%20information%20from%20the%20Laplacian%20itself%2C%0Awith%20applications%20to%20shape%20analysis%20and%20matching.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06161v1&entry.124074799=Read"},
{"title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review", "author": "Zhicheng Lin", "abstract": "  In July 2025, 18 academic manuscripts on the preprint website arXiv were\nfound to contain hidden instructions known as prompts designed to manipulate\nAI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\"\nwere concealed using techniques like white-colored text. Author responses\nvaried: one planned to withdraw the affected paper, while another defended the\npractice as legitimate testing of reviewer compliance. This commentary analyzes\nthis practice as a novel form of research misconduct. We examine the technique\nof prompt injection in large language models (LLMs), revealing four types of\nhidden prompts, ranging from simple positive review commands to detailed\nevaluation frameworks. The defense that prompts served as \"honeypots\" to detect\nreviewers improperly using AI fails under examination--the consistently\nself-serving nature of prompt instructions indicates intent to manipulate.\nPublishers maintain inconsistent policies: Elsevier prohibits AI use in peer\nreview entirely, while Springer Nature permits limited use with disclosure\nrequirements. The incident exposes systematic vulnerabilities extending beyond\npeer review to any automated system processing scholarly texts, including\nplagiarism detection and citation indexing. Our analysis underscores the need\nfor coordinated technical screening at submission portals and harmonized\npolicies governing generative AI (GenAI) use in academic evaluation.\n", "link": "http://arxiv.org/abs/2507.06185v1", "date": "2025-07-08", "relevancy": 1.1425, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3833}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3829}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hidden%20Prompts%20in%20Manuscripts%20Exploit%20AI-Assisted%20Peer%20Review&body=Title%3A%20Hidden%20Prompts%20in%20Manuscripts%20Exploit%20AI-Assisted%20Peer%20Review%0AAuthor%3A%20Zhicheng%20Lin%0AAbstract%3A%20%20%20In%20July%202025%2C%2018%20academic%20manuscripts%20on%20the%20preprint%20website%20arXiv%20were%0Afound%20to%20contain%20hidden%20instructions%20known%20as%20prompts%20designed%20to%20manipulate%0AAI-assisted%20peer%20review.%20Instructions%20such%20as%20%22GIVE%20A%20POSITIVE%20REVIEW%20ONLY%22%0Awere%20concealed%20using%20techniques%20like%20white-colored%20text.%20Author%20responses%0Avaried%3A%20one%20planned%20to%20withdraw%20the%20affected%20paper%2C%20while%20another%20defended%20the%0Apractice%20as%20legitimate%20testing%20of%20reviewer%20compliance.%20This%20commentary%20analyzes%0Athis%20practice%20as%20a%20novel%20form%20of%20research%20misconduct.%20We%20examine%20the%20technique%0Aof%20prompt%20injection%20in%20large%20language%20models%20%28LLMs%29%2C%20revealing%20four%20types%20of%0Ahidden%20prompts%2C%20ranging%20from%20simple%20positive%20review%20commands%20to%20detailed%0Aevaluation%20frameworks.%20The%20defense%20that%20prompts%20served%20as%20%22honeypots%22%20to%20detect%0Areviewers%20improperly%20using%20AI%20fails%20under%20examination--the%20consistently%0Aself-serving%20nature%20of%20prompt%20instructions%20indicates%20intent%20to%20manipulate.%0APublishers%20maintain%20inconsistent%20policies%3A%20Elsevier%20prohibits%20AI%20use%20in%20peer%0Areview%20entirely%2C%20while%20Springer%20Nature%20permits%20limited%20use%20with%20disclosure%0Arequirements.%20The%20incident%20exposes%20systematic%20vulnerabilities%20extending%20beyond%0Apeer%20review%20to%20any%20automated%20system%20processing%20scholarly%20texts%2C%20including%0Aplagiarism%20detection%20and%20citation%20indexing.%20Our%20analysis%20underscores%20the%20need%0Afor%20coordinated%20technical%20screening%20at%20submission%20portals%20and%20harmonized%0Apolicies%20governing%20generative%20AI%20%28GenAI%29%20use%20in%20academic%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHidden%2520Prompts%2520in%2520Manuscripts%2520Exploit%2520AI-Assisted%2520Peer%2520Review%26entry.906535625%3DZhicheng%2520Lin%26entry.1292438233%3D%2520%2520In%2520July%25202025%252C%252018%2520academic%2520manuscripts%2520on%2520the%2520preprint%2520website%2520arXiv%2520were%250Afound%2520to%2520contain%2520hidden%2520instructions%2520known%2520as%2520prompts%2520designed%2520to%2520manipulate%250AAI-assisted%2520peer%2520review.%2520Instructions%2520such%2520as%2520%2522GIVE%2520A%2520POSITIVE%2520REVIEW%2520ONLY%2522%250Awere%2520concealed%2520using%2520techniques%2520like%2520white-colored%2520text.%2520Author%2520responses%250Avaried%253A%2520one%2520planned%2520to%2520withdraw%2520the%2520affected%2520paper%252C%2520while%2520another%2520defended%2520the%250Apractice%2520as%2520legitimate%2520testing%2520of%2520reviewer%2520compliance.%2520This%2520commentary%2520analyzes%250Athis%2520practice%2520as%2520a%2520novel%2520form%2520of%2520research%2520misconduct.%2520We%2520examine%2520the%2520technique%250Aof%2520prompt%2520injection%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520revealing%2520four%2520types%2520of%250Ahidden%2520prompts%252C%2520ranging%2520from%2520simple%2520positive%2520review%2520commands%2520to%2520detailed%250Aevaluation%2520frameworks.%2520The%2520defense%2520that%2520prompts%2520served%2520as%2520%2522honeypots%2522%2520to%2520detect%250Areviewers%2520improperly%2520using%2520AI%2520fails%2520under%2520examination--the%2520consistently%250Aself-serving%2520nature%2520of%2520prompt%2520instructions%2520indicates%2520intent%2520to%2520manipulate.%250APublishers%2520maintain%2520inconsistent%2520policies%253A%2520Elsevier%2520prohibits%2520AI%2520use%2520in%2520peer%250Areview%2520entirely%252C%2520while%2520Springer%2520Nature%2520permits%2520limited%2520use%2520with%2520disclosure%250Arequirements.%2520The%2520incident%2520exposes%2520systematic%2520vulnerabilities%2520extending%2520beyond%250Apeer%2520review%2520to%2520any%2520automated%2520system%2520processing%2520scholarly%2520texts%252C%2520including%250Aplagiarism%2520detection%2520and%2520citation%2520indexing.%2520Our%2520analysis%2520underscores%2520the%2520need%250Afor%2520coordinated%2520technical%2520screening%2520at%2520submission%2520portals%2520and%2520harmonized%250Apolicies%2520governing%2520generative%2520AI%2520%2528GenAI%2529%2520use%2520in%2520academic%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hidden%20Prompts%20in%20Manuscripts%20Exploit%20AI-Assisted%20Peer%20Review&entry.906535625=Zhicheng%20Lin&entry.1292438233=%20%20In%20July%202025%2C%2018%20academic%20manuscripts%20on%20the%20preprint%20website%20arXiv%20were%0Afound%20to%20contain%20hidden%20instructions%20known%20as%20prompts%20designed%20to%20manipulate%0AAI-assisted%20peer%20review.%20Instructions%20such%20as%20%22GIVE%20A%20POSITIVE%20REVIEW%20ONLY%22%0Awere%20concealed%20using%20techniques%20like%20white-colored%20text.%20Author%20responses%0Avaried%3A%20one%20planned%20to%20withdraw%20the%20affected%20paper%2C%20while%20another%20defended%20the%0Apractice%20as%20legitimate%20testing%20of%20reviewer%20compliance.%20This%20commentary%20analyzes%0Athis%20practice%20as%20a%20novel%20form%20of%20research%20misconduct.%20We%20examine%20the%20technique%0Aof%20prompt%20injection%20in%20large%20language%20models%20%28LLMs%29%2C%20revealing%20four%20types%20of%0Ahidden%20prompts%2C%20ranging%20from%20simple%20positive%20review%20commands%20to%20detailed%0Aevaluation%20frameworks.%20The%20defense%20that%20prompts%20served%20as%20%22honeypots%22%20to%20detect%0Areviewers%20improperly%20using%20AI%20fails%20under%20examination--the%20consistently%0Aself-serving%20nature%20of%20prompt%20instructions%20indicates%20intent%20to%20manipulate.%0APublishers%20maintain%20inconsistent%20policies%3A%20Elsevier%20prohibits%20AI%20use%20in%20peer%0Areview%20entirely%2C%20while%20Springer%20Nature%20permits%20limited%20use%20with%20disclosure%0Arequirements.%20The%20incident%20exposes%20systematic%20vulnerabilities%20extending%20beyond%0Apeer%20review%20to%20any%20automated%20system%20processing%20scholarly%20texts%2C%20including%0Aplagiarism%20detection%20and%20citation%20indexing.%20Our%20analysis%20underscores%20the%20need%0Afor%20coordinated%20technical%20screening%20at%20submission%20portals%20and%20harmonized%0Apolicies%20governing%20generative%20AI%20%28GenAI%29%20use%20in%20academic%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06185v1&entry.124074799=Read"},
{"title": "Subspace-based Approximate Hessian Method for Zeroth-Order Optimization", "author": "Dongyoon Kim and Sungjae Lee and Wonjin Lee and Kwang In Kim", "abstract": "  Zeroth-order optimization addresses problems where gradient information is\ninaccessible or impractical to compute. While most existing methods rely on\nfirst-order approximations, incorporating second-order (curvature) information\ncan, in principle, significantly accelerate convergence. However, the high cost\nof function evaluations required to estimate Hessian matrices often limits\npractical applicability. We present the subspace-based approximate Hessian\n(ZO-SAH) method, a zeroth-order optimization algorithm that mitigates these\ncosts by focusing on randomly selected two-dimensional subspaces. Within each\nsubspace, ZO-SAH estimates the Hessian by fitting a quadratic polynomial to the\nobjective function and extracting its second-order coefficients. To further\nreduce function-query costs, ZO-SAH employs a periodic subspace-switching\nstrategy that reuses function evaluations across optimization steps.\nExperiments on eight benchmark datasets, including logistic regression and deep\nneural network training tasks, demonstrate that ZO-SAH achieves significantly\nfaster convergence than existing zeroth-order methods.\n", "link": "http://arxiv.org/abs/2507.06125v1", "date": "2025-07-08", "relevancy": 1.2768, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4283}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4264}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subspace-based%20Approximate%20Hessian%20Method%20for%20Zeroth-Order%20Optimization&body=Title%3A%20Subspace-based%20Approximate%20Hessian%20Method%20for%20Zeroth-Order%20Optimization%0AAuthor%3A%20Dongyoon%20Kim%20and%20Sungjae%20Lee%20and%20Wonjin%20Lee%20and%20Kwang%20In%20Kim%0AAbstract%3A%20%20%20Zeroth-order%20optimization%20addresses%20problems%20where%20gradient%20information%20is%0Ainaccessible%20or%20impractical%20to%20compute.%20While%20most%20existing%20methods%20rely%20on%0Afirst-order%20approximations%2C%20incorporating%20second-order%20%28curvature%29%20information%0Acan%2C%20in%20principle%2C%20significantly%20accelerate%20convergence.%20However%2C%20the%20high%20cost%0Aof%20function%20evaluations%20required%20to%20estimate%20Hessian%20matrices%20often%20limits%0Apractical%20applicability.%20We%20present%20the%20subspace-based%20approximate%20Hessian%0A%28ZO-SAH%29%20method%2C%20a%20zeroth-order%20optimization%20algorithm%20that%20mitigates%20these%0Acosts%20by%20focusing%20on%20randomly%20selected%20two-dimensional%20subspaces.%20Within%20each%0Asubspace%2C%20ZO-SAH%20estimates%20the%20Hessian%20by%20fitting%20a%20quadratic%20polynomial%20to%20the%0Aobjective%20function%20and%20extracting%20its%20second-order%20coefficients.%20To%20further%0Areduce%20function-query%20costs%2C%20ZO-SAH%20employs%20a%20periodic%20subspace-switching%0Astrategy%20that%20reuses%20function%20evaluations%20across%20optimization%20steps.%0AExperiments%20on%20eight%20benchmark%20datasets%2C%20including%20logistic%20regression%20and%20deep%0Aneural%20network%20training%20tasks%2C%20demonstrate%20that%20ZO-SAH%20achieves%20significantly%0Afaster%20convergence%20than%20existing%20zeroth-order%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubspace-based%2520Approximate%2520Hessian%2520Method%2520for%2520Zeroth-Order%2520Optimization%26entry.906535625%3DDongyoon%2520Kim%2520and%2520Sungjae%2520Lee%2520and%2520Wonjin%2520Lee%2520and%2520Kwang%2520In%2520Kim%26entry.1292438233%3D%2520%2520Zeroth-order%2520optimization%2520addresses%2520problems%2520where%2520gradient%2520information%2520is%250Ainaccessible%2520or%2520impractical%2520to%2520compute.%2520While%2520most%2520existing%2520methods%2520rely%2520on%250Afirst-order%2520approximations%252C%2520incorporating%2520second-order%2520%2528curvature%2529%2520information%250Acan%252C%2520in%2520principle%252C%2520significantly%2520accelerate%2520convergence.%2520However%252C%2520the%2520high%2520cost%250Aof%2520function%2520evaluations%2520required%2520to%2520estimate%2520Hessian%2520matrices%2520often%2520limits%250Apractical%2520applicability.%2520We%2520present%2520the%2520subspace-based%2520approximate%2520Hessian%250A%2528ZO-SAH%2529%2520method%252C%2520a%2520zeroth-order%2520optimization%2520algorithm%2520that%2520mitigates%2520these%250Acosts%2520by%2520focusing%2520on%2520randomly%2520selected%2520two-dimensional%2520subspaces.%2520Within%2520each%250Asubspace%252C%2520ZO-SAH%2520estimates%2520the%2520Hessian%2520by%2520fitting%2520a%2520quadratic%2520polynomial%2520to%2520the%250Aobjective%2520function%2520and%2520extracting%2520its%2520second-order%2520coefficients.%2520To%2520further%250Areduce%2520function-query%2520costs%252C%2520ZO-SAH%2520employs%2520a%2520periodic%2520subspace-switching%250Astrategy%2520that%2520reuses%2520function%2520evaluations%2520across%2520optimization%2520steps.%250AExperiments%2520on%2520eight%2520benchmark%2520datasets%252C%2520including%2520logistic%2520regression%2520and%2520deep%250Aneural%2520network%2520training%2520tasks%252C%2520demonstrate%2520that%2520ZO-SAH%2520achieves%2520significantly%250Afaster%2520convergence%2520than%2520existing%2520zeroth-order%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subspace-based%20Approximate%20Hessian%20Method%20for%20Zeroth-Order%20Optimization&entry.906535625=Dongyoon%20Kim%20and%20Sungjae%20Lee%20and%20Wonjin%20Lee%20and%20Kwang%20In%20Kim&entry.1292438233=%20%20Zeroth-order%20optimization%20addresses%20problems%20where%20gradient%20information%20is%0Ainaccessible%20or%20impractical%20to%20compute.%20While%20most%20existing%20methods%20rely%20on%0Afirst-order%20approximations%2C%20incorporating%20second-order%20%28curvature%29%20information%0Acan%2C%20in%20principle%2C%20significantly%20accelerate%20convergence.%20However%2C%20the%20high%20cost%0Aof%20function%20evaluations%20required%20to%20estimate%20Hessian%20matrices%20often%20limits%0Apractical%20applicability.%20We%20present%20the%20subspace-based%20approximate%20Hessian%0A%28ZO-SAH%29%20method%2C%20a%20zeroth-order%20optimization%20algorithm%20that%20mitigates%20these%0Acosts%20by%20focusing%20on%20randomly%20selected%20two-dimensional%20subspaces.%20Within%20each%0Asubspace%2C%20ZO-SAH%20estimates%20the%20Hessian%20by%20fitting%20a%20quadratic%20polynomial%20to%20the%0Aobjective%20function%20and%20extracting%20its%20second-order%20coefficients.%20To%20further%0Areduce%20function-query%20costs%2C%20ZO-SAH%20employs%20a%20periodic%20subspace-switching%0Astrategy%20that%20reuses%20function%20evaluations%20across%20optimization%20steps.%0AExperiments%20on%20eight%20benchmark%20datasets%2C%20including%20logistic%20regression%20and%20deep%0Aneural%20network%20training%20tasks%2C%20demonstrate%20that%20ZO-SAH%20achieves%20significantly%0Afaster%20convergence%20than%20existing%20zeroth-order%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06125v1&entry.124074799=Read"},
{"title": "ScoreAdv: Score-based Targeted Generation of Natural Adversarial\n  Examples via Diffusion Models", "author": "Chihan Huang and Hao Tang", "abstract": "  Despite the success of deep learning across various domains, it remains\nvulnerable to adversarial attacks. Although many existing adversarial attack\nmethods achieve high success rates, they typically rely on $\\ell_{p}$-norm\nperturbation constraints, which do not align with human perceptual\ncapabilities. Consequently, researchers have shifted their focus toward\ngenerating natural, unrestricted adversarial examples (UAEs). GAN-based\napproaches suffer from inherent limitations, such as poor image quality due to\ninstability and mode collapse. Meanwhile, diffusion models have been employed\nfor UAE generation, but they still rely on iterative PGD perturbation\ninjection, without fully leveraging their central denoising capabilities. In\nthis paper, we introduce a novel approach for generating UAEs based on\ndiffusion models, named ScoreAdv. This method incorporates an interpretable\nadversarial guidance mechanism to gradually shift the sampling distribution\ntowards the adversarial distribution, while using an interpretable saliency map\nto inject the visual information of a reference image into the generated\nsamples. Notably, our method is capable of generating an unlimited number of\nnatural adversarial examples and can attack not only classification models but\nalso retrieval models. We conduct extensive experiments on ImageNet and CelebA\ndatasets, validating the performance of ScoreAdv across ten target models in\nboth black-box and white-box settings. Our results demonstrate that ScoreAdv\nachieves state-of-the-art attack success rates and image quality. Furthermore,\nthe dynamic balance between denoising and adversarial perturbation enables\nScoreAdv to remain robust even under defensive measures.\n", "link": "http://arxiv.org/abs/2507.06078v1", "date": "2025-07-08", "relevancy": 1.6608, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5593}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5524}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScoreAdv%3A%20Score-based%20Targeted%20Generation%20of%20Natural%20Adversarial%0A%20%20Examples%20via%20Diffusion%20Models&body=Title%3A%20ScoreAdv%3A%20Score-based%20Targeted%20Generation%20of%20Natural%20Adversarial%0A%20%20Examples%20via%20Diffusion%20Models%0AAuthor%3A%20Chihan%20Huang%20and%20Hao%20Tang%0AAbstract%3A%20%20%20Despite%20the%20success%20of%20deep%20learning%20across%20various%20domains%2C%20it%20remains%0Avulnerable%20to%20adversarial%20attacks.%20Although%20many%20existing%20adversarial%20attack%0Amethods%20achieve%20high%20success%20rates%2C%20they%20typically%20rely%20on%20%24%5Cell_%7Bp%7D%24-norm%0Aperturbation%20constraints%2C%20which%20do%20not%20align%20with%20human%20perceptual%0Acapabilities.%20Consequently%2C%20researchers%20have%20shifted%20their%20focus%20toward%0Agenerating%20natural%2C%20unrestricted%20adversarial%20examples%20%28UAEs%29.%20GAN-based%0Aapproaches%20suffer%20from%20inherent%20limitations%2C%20such%20as%20poor%20image%20quality%20due%20to%0Ainstability%20and%20mode%20collapse.%20Meanwhile%2C%20diffusion%20models%20have%20been%20employed%0Afor%20UAE%20generation%2C%20but%20they%20still%20rely%20on%20iterative%20PGD%20perturbation%0Ainjection%2C%20without%20fully%20leveraging%20their%20central%20denoising%20capabilities.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20approach%20for%20generating%20UAEs%20based%20on%0Adiffusion%20models%2C%20named%20ScoreAdv.%20This%20method%20incorporates%20an%20interpretable%0Aadversarial%20guidance%20mechanism%20to%20gradually%20shift%20the%20sampling%20distribution%0Atowards%20the%20adversarial%20distribution%2C%20while%20using%20an%20interpretable%20saliency%20map%0Ato%20inject%20the%20visual%20information%20of%20a%20reference%20image%20into%20the%20generated%0Asamples.%20Notably%2C%20our%20method%20is%20capable%20of%20generating%20an%20unlimited%20number%20of%0Anatural%20adversarial%20examples%20and%20can%20attack%20not%20only%20classification%20models%20but%0Aalso%20retrieval%20models.%20We%20conduct%20extensive%20experiments%20on%20ImageNet%20and%20CelebA%0Adatasets%2C%20validating%20the%20performance%20of%20ScoreAdv%20across%20ten%20target%20models%20in%0Aboth%20black-box%20and%20white-box%20settings.%20Our%20results%20demonstrate%20that%20ScoreAdv%0Aachieves%20state-of-the-art%20attack%20success%20rates%20and%20image%20quality.%20Furthermore%2C%0Athe%20dynamic%20balance%20between%20denoising%20and%20adversarial%20perturbation%20enables%0AScoreAdv%20to%20remain%20robust%20even%20under%20defensive%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScoreAdv%253A%2520Score-based%2520Targeted%2520Generation%2520of%2520Natural%2520Adversarial%250A%2520%2520Examples%2520via%2520Diffusion%2520Models%26entry.906535625%3DChihan%2520Huang%2520and%2520Hao%2520Tang%26entry.1292438233%3D%2520%2520Despite%2520the%2520success%2520of%2520deep%2520learning%2520across%2520various%2520domains%252C%2520it%2520remains%250Avulnerable%2520to%2520adversarial%2520attacks.%2520Although%2520many%2520existing%2520adversarial%2520attack%250Amethods%2520achieve%2520high%2520success%2520rates%252C%2520they%2520typically%2520rely%2520on%2520%2524%255Cell_%257Bp%257D%2524-norm%250Aperturbation%2520constraints%252C%2520which%2520do%2520not%2520align%2520with%2520human%2520perceptual%250Acapabilities.%2520Consequently%252C%2520researchers%2520have%2520shifted%2520their%2520focus%2520toward%250Agenerating%2520natural%252C%2520unrestricted%2520adversarial%2520examples%2520%2528UAEs%2529.%2520GAN-based%250Aapproaches%2520suffer%2520from%2520inherent%2520limitations%252C%2520such%2520as%2520poor%2520image%2520quality%2520due%2520to%250Ainstability%2520and%2520mode%2520collapse.%2520Meanwhile%252C%2520diffusion%2520models%2520have%2520been%2520employed%250Afor%2520UAE%2520generation%252C%2520but%2520they%2520still%2520rely%2520on%2520iterative%2520PGD%2520perturbation%250Ainjection%252C%2520without%2520fully%2520leveraging%2520their%2520central%2520denoising%2520capabilities.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520for%2520generating%2520UAEs%2520based%2520on%250Adiffusion%2520models%252C%2520named%2520ScoreAdv.%2520This%2520method%2520incorporates%2520an%2520interpretable%250Aadversarial%2520guidance%2520mechanism%2520to%2520gradually%2520shift%2520the%2520sampling%2520distribution%250Atowards%2520the%2520adversarial%2520distribution%252C%2520while%2520using%2520an%2520interpretable%2520saliency%2520map%250Ato%2520inject%2520the%2520visual%2520information%2520of%2520a%2520reference%2520image%2520into%2520the%2520generated%250Asamples.%2520Notably%252C%2520our%2520method%2520is%2520capable%2520of%2520generating%2520an%2520unlimited%2520number%2520of%250Anatural%2520adversarial%2520examples%2520and%2520can%2520attack%2520not%2520only%2520classification%2520models%2520but%250Aalso%2520retrieval%2520models.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520ImageNet%2520and%2520CelebA%250Adatasets%252C%2520validating%2520the%2520performance%2520of%2520ScoreAdv%2520across%2520ten%2520target%2520models%2520in%250Aboth%2520black-box%2520and%2520white-box%2520settings.%2520Our%2520results%2520demonstrate%2520that%2520ScoreAdv%250Aachieves%2520state-of-the-art%2520attack%2520success%2520rates%2520and%2520image%2520quality.%2520Furthermore%252C%250Athe%2520dynamic%2520balance%2520between%2520denoising%2520and%2520adversarial%2520perturbation%2520enables%250AScoreAdv%2520to%2520remain%2520robust%2520even%2520under%2520defensive%2520measures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScoreAdv%3A%20Score-based%20Targeted%20Generation%20of%20Natural%20Adversarial%0A%20%20Examples%20via%20Diffusion%20Models&entry.906535625=Chihan%20Huang%20and%20Hao%20Tang&entry.1292438233=%20%20Despite%20the%20success%20of%20deep%20learning%20across%20various%20domains%2C%20it%20remains%0Avulnerable%20to%20adversarial%20attacks.%20Although%20many%20existing%20adversarial%20attack%0Amethods%20achieve%20high%20success%20rates%2C%20they%20typically%20rely%20on%20%24%5Cell_%7Bp%7D%24-norm%0Aperturbation%20constraints%2C%20which%20do%20not%20align%20with%20human%20perceptual%0Acapabilities.%20Consequently%2C%20researchers%20have%20shifted%20their%20focus%20toward%0Agenerating%20natural%2C%20unrestricted%20adversarial%20examples%20%28UAEs%29.%20GAN-based%0Aapproaches%20suffer%20from%20inherent%20limitations%2C%20such%20as%20poor%20image%20quality%20due%20to%0Ainstability%20and%20mode%20collapse.%20Meanwhile%2C%20diffusion%20models%20have%20been%20employed%0Afor%20UAE%20generation%2C%20but%20they%20still%20rely%20on%20iterative%20PGD%20perturbation%0Ainjection%2C%20without%20fully%20leveraging%20their%20central%20denoising%20capabilities.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20approach%20for%20generating%20UAEs%20based%20on%0Adiffusion%20models%2C%20named%20ScoreAdv.%20This%20method%20incorporates%20an%20interpretable%0Aadversarial%20guidance%20mechanism%20to%20gradually%20shift%20the%20sampling%20distribution%0Atowards%20the%20adversarial%20distribution%2C%20while%20using%20an%20interpretable%20saliency%20map%0Ato%20inject%20the%20visual%20information%20of%20a%20reference%20image%20into%20the%20generated%0Asamples.%20Notably%2C%20our%20method%20is%20capable%20of%20generating%20an%20unlimited%20number%20of%0Anatural%20adversarial%20examples%20and%20can%20attack%20not%20only%20classification%20models%20but%0Aalso%20retrieval%20models.%20We%20conduct%20extensive%20experiments%20on%20ImageNet%20and%20CelebA%0Adatasets%2C%20validating%20the%20performance%20of%20ScoreAdv%20across%20ten%20target%20models%20in%0Aboth%20black-box%20and%20white-box%20settings.%20Our%20results%20demonstrate%20that%20ScoreAdv%0Aachieves%20state-of-the-art%20attack%20success%20rates%20and%20image%20quality.%20Furthermore%2C%0Athe%20dynamic%20balance%20between%20denoising%20and%20adversarial%20perturbation%20enables%0AScoreAdv%20to%20remain%20robust%20even%20under%20defensive%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06078v1&entry.124074799=Read"},
{"title": "Counterfactual Inference under Thompson Sampling", "author": "Olivier Jeunen", "abstract": "  Recommender systems exemplify sequential decision-making under uncertainty,\nstrategically deciding what content to serve to users, to optimise a range of\npotential objectives. To balance the explore-exploit trade-off successfully,\nThompson sampling provides a natural and widespread paradigm to\nprobabilistically select which action to take. Questions of causal and\ncounterfactual inference, which underpin use-cases like offline evaluation, are\nnot straightforward to answer in these contexts. Specifically, whilst most\nexisting estimators rely on action propensities, these are not readily\navailable under Thompson sampling procedures.\n  We derive exact and efficiently computable expressions for action\npropensities under a variety of parameter and outcome distributions, enabling\nthe use of off-policy estimators in Thompson sampling scenarios. This opens up\na range of practical use-cases where counterfactual inference is crucial,\nincluding unbiased offline evaluation of recommender systems, as well as\ngeneral applications of causal inference in online advertising,\npersonalisation, and beyond.\n", "link": "http://arxiv.org/abs/2504.08773v2", "date": "2025-07-08", "relevancy": 1.677, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4807}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4136}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Inference%20under%20Thompson%20Sampling&body=Title%3A%20Counterfactual%20Inference%20under%20Thompson%20Sampling%0AAuthor%3A%20Olivier%20Jeunen%0AAbstract%3A%20%20%20Recommender%20systems%20exemplify%20sequential%20decision-making%20under%20uncertainty%2C%0Astrategically%20deciding%20what%20content%20to%20serve%20to%20users%2C%20to%20optimise%20a%20range%20of%0Apotential%20objectives.%20To%20balance%20the%20explore-exploit%20trade-off%20successfully%2C%0AThompson%20sampling%20provides%20a%20natural%20and%20widespread%20paradigm%20to%0Aprobabilistically%20select%20which%20action%20to%20take.%20Questions%20of%20causal%20and%0Acounterfactual%20inference%2C%20which%20underpin%20use-cases%20like%20offline%20evaluation%2C%20are%0Anot%20straightforward%20to%20answer%20in%20these%20contexts.%20Specifically%2C%20whilst%20most%0Aexisting%20estimators%20rely%20on%20action%20propensities%2C%20these%20are%20not%20readily%0Aavailable%20under%20Thompson%20sampling%20procedures.%0A%20%20We%20derive%20exact%20and%20efficiently%20computable%20expressions%20for%20action%0Apropensities%20under%20a%20variety%20of%20parameter%20and%20outcome%20distributions%2C%20enabling%0Athe%20use%20of%20off-policy%20estimators%20in%20Thompson%20sampling%20scenarios.%20This%20opens%20up%0Aa%20range%20of%20practical%20use-cases%20where%20counterfactual%20inference%20is%20crucial%2C%0Aincluding%20unbiased%20offline%20evaluation%20of%20recommender%20systems%2C%20as%20well%20as%0Ageneral%20applications%20of%20causal%20inference%20in%20online%20advertising%2C%0Apersonalisation%2C%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08773v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520Inference%2520under%2520Thompson%2520Sampling%26entry.906535625%3DOlivier%2520Jeunen%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520exemplify%2520sequential%2520decision-making%2520under%2520uncertainty%252C%250Astrategically%2520deciding%2520what%2520content%2520to%2520serve%2520to%2520users%252C%2520to%2520optimise%2520a%2520range%2520of%250Apotential%2520objectives.%2520To%2520balance%2520the%2520explore-exploit%2520trade-off%2520successfully%252C%250AThompson%2520sampling%2520provides%2520a%2520natural%2520and%2520widespread%2520paradigm%2520to%250Aprobabilistically%2520select%2520which%2520action%2520to%2520take.%2520Questions%2520of%2520causal%2520and%250Acounterfactual%2520inference%252C%2520which%2520underpin%2520use-cases%2520like%2520offline%2520evaluation%252C%2520are%250Anot%2520straightforward%2520to%2520answer%2520in%2520these%2520contexts.%2520Specifically%252C%2520whilst%2520most%250Aexisting%2520estimators%2520rely%2520on%2520action%2520propensities%252C%2520these%2520are%2520not%2520readily%250Aavailable%2520under%2520Thompson%2520sampling%2520procedures.%250A%2520%2520We%2520derive%2520exact%2520and%2520efficiently%2520computable%2520expressions%2520for%2520action%250Apropensities%2520under%2520a%2520variety%2520of%2520parameter%2520and%2520outcome%2520distributions%252C%2520enabling%250Athe%2520use%2520of%2520off-policy%2520estimators%2520in%2520Thompson%2520sampling%2520scenarios.%2520This%2520opens%2520up%250Aa%2520range%2520of%2520practical%2520use-cases%2520where%2520counterfactual%2520inference%2520is%2520crucial%252C%250Aincluding%2520unbiased%2520offline%2520evaluation%2520of%2520recommender%2520systems%252C%2520as%2520well%2520as%250Ageneral%2520applications%2520of%2520causal%2520inference%2520in%2520online%2520advertising%252C%250Apersonalisation%252C%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08773v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Inference%20under%20Thompson%20Sampling&entry.906535625=Olivier%20Jeunen&entry.1292438233=%20%20Recommender%20systems%20exemplify%20sequential%20decision-making%20under%20uncertainty%2C%0Astrategically%20deciding%20what%20content%20to%20serve%20to%20users%2C%20to%20optimise%20a%20range%20of%0Apotential%20objectives.%20To%20balance%20the%20explore-exploit%20trade-off%20successfully%2C%0AThompson%20sampling%20provides%20a%20natural%20and%20widespread%20paradigm%20to%0Aprobabilistically%20select%20which%20action%20to%20take.%20Questions%20of%20causal%20and%0Acounterfactual%20inference%2C%20which%20underpin%20use-cases%20like%20offline%20evaluation%2C%20are%0Anot%20straightforward%20to%20answer%20in%20these%20contexts.%20Specifically%2C%20whilst%20most%0Aexisting%20estimators%20rely%20on%20action%20propensities%2C%20these%20are%20not%20readily%0Aavailable%20under%20Thompson%20sampling%20procedures.%0A%20%20We%20derive%20exact%20and%20efficiently%20computable%20expressions%20for%20action%0Apropensities%20under%20a%20variety%20of%20parameter%20and%20outcome%20distributions%2C%20enabling%0Athe%20use%20of%20off-policy%20estimators%20in%20Thompson%20sampling%20scenarios.%20This%20opens%20up%0Aa%20range%20of%20practical%20use-cases%20where%20counterfactual%20inference%20is%20crucial%2C%0Aincluding%20unbiased%20offline%20evaluation%20of%20recommender%20systems%2C%20as%20well%20as%0Ageneral%20applications%20of%20causal%20inference%20in%20online%20advertising%2C%0Apersonalisation%2C%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08773v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


