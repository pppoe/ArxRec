<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260217.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory", "author": "Felix Windisch and Thomas K\u00f6hler and Lukas Radl and Mattia D'Urso and Michael Steiner and Dieter Schmalstieg and Markus Steinberger", "abstract": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.", "link": "http://arxiv.org/abs/2507.01110v4", "date": "2026-02-17", "relevancy": 3.3708, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7153}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6964}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20LoD%20of%20Gaussians%3A%20Unified%20Training%20and%20Rendering%20for%20Ultra-Large%20Scale%20Reconstruction%20with%20External%20Memory&body=Title%3A%20A%20LoD%20of%20Gaussians%3A%20Unified%20Training%20and%20Rendering%20for%20Ultra-Large%20Scale%20Reconstruction%20with%20External%20Memory%0AAuthor%3A%20Felix%20Windisch%20and%20Thomas%20K%C3%B6hler%20and%20Lukas%20Radl%20and%20Mattia%20D%27Urso%20and%20Michael%20Steiner%20and%20Dieter%20Schmalstieg%20and%20Markus%20Steinberger%0AAbstract%3A%20Gaussian%20Splatting%20has%20emerged%20as%20a%20high-performance%20technique%20for%20novel%20view%20synthesis%2C%20enabling%20real-time%20rendering%20and%20high-quality%20reconstruction%20of%20small%20scenes.%20However%2C%20scaling%20to%20larger%20environments%20has%20so%20far%20relied%20on%20partitioning%20the%20scene%20into%20chunks%20--%20a%20strategy%20that%20introduces%20artifacts%20at%20chunk%20boundaries%2C%20complicates%20training%20across%20varying%20scales%2C%20and%20is%20poorly%20suited%20to%20unstructured%20scenarios%20such%20as%20city-scale%20flyovers%20combined%20with%20street-level%20views.%20Moreover%2C%20rendering%20remains%20fundamentally%20limited%20by%20GPU%20memory%2C%20as%20all%20visible%20chunks%20must%20reside%20in%20VRAM%20simultaneously.%20We%20introduce%20A%20LoD%20of%20Gaussians%2C%20a%20framework%20for%20training%20and%20rendering%20ultra-large-scale%20Gaussian%20scenes%20on%20a%20single%20consumer-grade%20GPU%20--%20without%20partitioning.%20Our%20method%20stores%20the%20full%20scene%20out-of-core%20%28e.g.%2C%20in%20CPU%20memory%29%20and%20trains%20a%20Level-of-Detail%20%28LoD%29%20representation%20directly%2C%20dynamically%20streaming%20only%20the%20relevant%20Gaussians.%20A%20hybrid%20data%20structure%20combining%20Gaussian%20hierarchies%20with%20Sequential%20Point%20Trees%20enables%20efficient%2C%20view-dependent%20LoD%20selection%2C%20while%20a%20lightweight%20caching%20and%20view%20scheduling%20system%20exploits%20temporal%20coherence%20to%20support%20real-time%20streaming%20and%20rendering.%20Together%2C%20these%20innovations%20enable%20seamless%20multi-scale%20reconstruction%20and%20interactive%20visualization%20of%20complex%20scenes%20--%20from%20broad%20aerial%20views%20to%20fine-grained%20ground-level%20details.%0ALink%3A%20http%3A//arxiv.org/abs/2507.01110v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520LoD%2520of%2520Gaussians%253A%2520Unified%2520Training%2520and%2520Rendering%2520for%2520Ultra-Large%2520Scale%2520Reconstruction%2520with%2520External%2520Memory%26entry.906535625%3DFelix%2520Windisch%2520and%2520Thomas%2520K%25C3%25B6hler%2520and%2520Lukas%2520Radl%2520and%2520Mattia%2520D%2527Urso%2520and%2520Michael%2520Steiner%2520and%2520Dieter%2520Schmalstieg%2520and%2520Markus%2520Steinberger%26entry.1292438233%3DGaussian%2520Splatting%2520has%2520emerged%2520as%2520a%2520high-performance%2520technique%2520for%2520novel%2520view%2520synthesis%252C%2520enabling%2520real-time%2520rendering%2520and%2520high-quality%2520reconstruction%2520of%2520small%2520scenes.%2520However%252C%2520scaling%2520to%2520larger%2520environments%2520has%2520so%2520far%2520relied%2520on%2520partitioning%2520the%2520scene%2520into%2520chunks%2520--%2520a%2520strategy%2520that%2520introduces%2520artifacts%2520at%2520chunk%2520boundaries%252C%2520complicates%2520training%2520across%2520varying%2520scales%252C%2520and%2520is%2520poorly%2520suited%2520to%2520unstructured%2520scenarios%2520such%2520as%2520city-scale%2520flyovers%2520combined%2520with%2520street-level%2520views.%2520Moreover%252C%2520rendering%2520remains%2520fundamentally%2520limited%2520by%2520GPU%2520memory%252C%2520as%2520all%2520visible%2520chunks%2520must%2520reside%2520in%2520VRAM%2520simultaneously.%2520We%2520introduce%2520A%2520LoD%2520of%2520Gaussians%252C%2520a%2520framework%2520for%2520training%2520and%2520rendering%2520ultra-large-scale%2520Gaussian%2520scenes%2520on%2520a%2520single%2520consumer-grade%2520GPU%2520--%2520without%2520partitioning.%2520Our%2520method%2520stores%2520the%2520full%2520scene%2520out-of-core%2520%2528e.g.%252C%2520in%2520CPU%2520memory%2529%2520and%2520trains%2520a%2520Level-of-Detail%2520%2528LoD%2529%2520representation%2520directly%252C%2520dynamically%2520streaming%2520only%2520the%2520relevant%2520Gaussians.%2520A%2520hybrid%2520data%2520structure%2520combining%2520Gaussian%2520hierarchies%2520with%2520Sequential%2520Point%2520Trees%2520enables%2520efficient%252C%2520view-dependent%2520LoD%2520selection%252C%2520while%2520a%2520lightweight%2520caching%2520and%2520view%2520scheduling%2520system%2520exploits%2520temporal%2520coherence%2520to%2520support%2520real-time%2520streaming%2520and%2520rendering.%2520Together%252C%2520these%2520innovations%2520enable%2520seamless%2520multi-scale%2520reconstruction%2520and%2520interactive%2520visualization%2520of%2520complex%2520scenes%2520--%2520from%2520broad%2520aerial%2520views%2520to%2520fine-grained%2520ground-level%2520details.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01110v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20LoD%20of%20Gaussians%3A%20Unified%20Training%20and%20Rendering%20for%20Ultra-Large%20Scale%20Reconstruction%20with%20External%20Memory&entry.906535625=Felix%20Windisch%20and%20Thomas%20K%C3%B6hler%20and%20Lukas%20Radl%20and%20Mattia%20D%27Urso%20and%20Michael%20Steiner%20and%20Dieter%20Schmalstieg%20and%20Markus%20Steinberger&entry.1292438233=Gaussian%20Splatting%20has%20emerged%20as%20a%20high-performance%20technique%20for%20novel%20view%20synthesis%2C%20enabling%20real-time%20rendering%20and%20high-quality%20reconstruction%20of%20small%20scenes.%20However%2C%20scaling%20to%20larger%20environments%20has%20so%20far%20relied%20on%20partitioning%20the%20scene%20into%20chunks%20--%20a%20strategy%20that%20introduces%20artifacts%20at%20chunk%20boundaries%2C%20complicates%20training%20across%20varying%20scales%2C%20and%20is%20poorly%20suited%20to%20unstructured%20scenarios%20such%20as%20city-scale%20flyovers%20combined%20with%20street-level%20views.%20Moreover%2C%20rendering%20remains%20fundamentally%20limited%20by%20GPU%20memory%2C%20as%20all%20visible%20chunks%20must%20reside%20in%20VRAM%20simultaneously.%20We%20introduce%20A%20LoD%20of%20Gaussians%2C%20a%20framework%20for%20training%20and%20rendering%20ultra-large-scale%20Gaussian%20scenes%20on%20a%20single%20consumer-grade%20GPU%20--%20without%20partitioning.%20Our%20method%20stores%20the%20full%20scene%20out-of-core%20%28e.g.%2C%20in%20CPU%20memory%29%20and%20trains%20a%20Level-of-Detail%20%28LoD%29%20representation%20directly%2C%20dynamically%20streaming%20only%20the%20relevant%20Gaussians.%20A%20hybrid%20data%20structure%20combining%20Gaussian%20hierarchies%20with%20Sequential%20Point%20Trees%20enables%20efficient%2C%20view-dependent%20LoD%20selection%2C%20while%20a%20lightweight%20caching%20and%20view%20scheduling%20system%20exploits%20temporal%20coherence%20to%20support%20real-time%20streaming%20and%20rendering.%20Together%2C%20these%20innovations%20enable%20seamless%20multi-scale%20reconstruction%20and%20interactive%20visualization%20of%20complex%20scenes%20--%20from%20broad%20aerial%20views%20to%20fine-grained%20ground-level%20details.&entry.1838667208=http%3A//arxiv.org/abs/2507.01110v4&entry.124074799=Read"},
{"title": "Semantic-Guided 3D Gaussian Splatting for Transient Object Removal", "author": "Aditi Prabakaran and Priyesh Shukla", "abstract": "Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across training iterations. Gaussians exceeding a calibrated threshold underwent opacity regularization and periodic pruning. Unlike motion-based approaches, semantic classification resolved parallax ambiguity by identifying object categories independently of motion patterns. Experiments on the RobustNeRF benchmark demonstrated consistent improvement in reconstruction quality over vanilla 3DGS across four sequences, while maintaining minimal memory overhead and real-time rendering performance. Threshold calibration and comparisons with baselines validated semantic guidance as a practical strategy for transient removal in scenarios with predictable distractor categories.", "link": "http://arxiv.org/abs/2602.15516v1", "date": "2026-02-17", "relevancy": 3.3095, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6989}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6668}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-Guided%203D%20Gaussian%20Splatting%20for%20Transient%20Object%20Removal&body=Title%3A%20Semantic-Guided%203D%20Gaussian%20Splatting%20for%20Transient%20Object%20Removal%0AAuthor%3A%20Aditi%20Prabakaran%20and%20Priyesh%20Shukla%0AAbstract%3A%20Transient%20objects%20in%20casual%20multi-view%20captures%20cause%20ghosting%20artifacts%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20reconstruction.%20Existing%20solutions%20relied%20on%20scene%20decomposition%20at%20significant%20memory%20cost%20or%20on%20motion-based%20heuristics%20that%20were%20vulnerable%20to%20parallax%20ambiguity.%20A%20semantic%20filtering%20framework%20was%20proposed%20for%20category-aware%20transient%20removal%20using%20vision-language%20models.%20CLIP%20similarity%20scores%20between%20rendered%20views%20and%20distractor%20text%20prompts%20were%20accumulated%20per-Gaussian%20across%20training%20iterations.%20Gaussians%20exceeding%20a%20calibrated%20threshold%20underwent%20opacity%20regularization%20and%20periodic%20pruning.%20Unlike%20motion-based%20approaches%2C%20semantic%20classification%20resolved%20parallax%20ambiguity%20by%20identifying%20object%20categories%20independently%20of%20motion%20patterns.%20Experiments%20on%20the%20RobustNeRF%20benchmark%20demonstrated%20consistent%20improvement%20in%20reconstruction%20quality%20over%20vanilla%203DGS%20across%20four%20sequences%2C%20while%20maintaining%20minimal%20memory%20overhead%20and%20real-time%20rendering%20performance.%20Threshold%20calibration%20and%20comparisons%20with%20baselines%20validated%20semantic%20guidance%20as%20a%20practical%20strategy%20for%20transient%20removal%20in%20scenarios%20with%20predictable%20distractor%20categories.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-Guided%25203D%2520Gaussian%2520Splatting%2520for%2520Transient%2520Object%2520Removal%26entry.906535625%3DAditi%2520Prabakaran%2520and%2520Priyesh%2520Shukla%26entry.1292438233%3DTransient%2520objects%2520in%2520casual%2520multi-view%2520captures%2520cause%2520ghosting%2520artifacts%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520reconstruction.%2520Existing%2520solutions%2520relied%2520on%2520scene%2520decomposition%2520at%2520significant%2520memory%2520cost%2520or%2520on%2520motion-based%2520heuristics%2520that%2520were%2520vulnerable%2520to%2520parallax%2520ambiguity.%2520A%2520semantic%2520filtering%2520framework%2520was%2520proposed%2520for%2520category-aware%2520transient%2520removal%2520using%2520vision-language%2520models.%2520CLIP%2520similarity%2520scores%2520between%2520rendered%2520views%2520and%2520distractor%2520text%2520prompts%2520were%2520accumulated%2520per-Gaussian%2520across%2520training%2520iterations.%2520Gaussians%2520exceeding%2520a%2520calibrated%2520threshold%2520underwent%2520opacity%2520regularization%2520and%2520periodic%2520pruning.%2520Unlike%2520motion-based%2520approaches%252C%2520semantic%2520classification%2520resolved%2520parallax%2520ambiguity%2520by%2520identifying%2520object%2520categories%2520independently%2520of%2520motion%2520patterns.%2520Experiments%2520on%2520the%2520RobustNeRF%2520benchmark%2520demonstrated%2520consistent%2520improvement%2520in%2520reconstruction%2520quality%2520over%2520vanilla%25203DGS%2520across%2520four%2520sequences%252C%2520while%2520maintaining%2520minimal%2520memory%2520overhead%2520and%2520real-time%2520rendering%2520performance.%2520Threshold%2520calibration%2520and%2520comparisons%2520with%2520baselines%2520validated%2520semantic%2520guidance%2520as%2520a%2520practical%2520strategy%2520for%2520transient%2520removal%2520in%2520scenarios%2520with%2520predictable%2520distractor%2520categories.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-Guided%203D%20Gaussian%20Splatting%20for%20Transient%20Object%20Removal&entry.906535625=Aditi%20Prabakaran%20and%20Priyesh%20Shukla&entry.1292438233=Transient%20objects%20in%20casual%20multi-view%20captures%20cause%20ghosting%20artifacts%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20reconstruction.%20Existing%20solutions%20relied%20on%20scene%20decomposition%20at%20significant%20memory%20cost%20or%20on%20motion-based%20heuristics%20that%20were%20vulnerable%20to%20parallax%20ambiguity.%20A%20semantic%20filtering%20framework%20was%20proposed%20for%20category-aware%20transient%20removal%20using%20vision-language%20models.%20CLIP%20similarity%20scores%20between%20rendered%20views%20and%20distractor%20text%20prompts%20were%20accumulated%20per-Gaussian%20across%20training%20iterations.%20Gaussians%20exceeding%20a%20calibrated%20threshold%20underwent%20opacity%20regularization%20and%20periodic%20pruning.%20Unlike%20motion-based%20approaches%2C%20semantic%20classification%20resolved%20parallax%20ambiguity%20by%20identifying%20object%20categories%20independently%20of%20motion%20patterns.%20Experiments%20on%20the%20RobustNeRF%20benchmark%20demonstrated%20consistent%20improvement%20in%20reconstruction%20quality%20over%20vanilla%203DGS%20across%20four%20sequences%2C%20while%20maintaining%20minimal%20memory%20overhead%20and%20real-time%20rendering%20performance.%20Threshold%20calibration%20and%20comparisons%20with%20baselines%20validated%20semantic%20guidance%20as%20a%20practical%20strategy%20for%20transient%20removal%20in%20scenarios%20with%20predictable%20distractor%20categories.&entry.1838667208=http%3A//arxiv.org/abs/2602.15516v1&entry.124074799=Read"},
{"title": "DreamAnywhere: Object-Centric Panoramic 3D Scene Generation", "author": "Edoardo Alberto Dominici and Jozef Hladky and Floor Verhoeven and Lukas Radl and Thomas Deixelberger and Stefan Ainetter and Philipp Drescher and Stefan Hauswiesner and Arno Coomans and Giacomo Nazzaro and Konstantinos Vardis and Markus Steinberger", "abstract": "Recent advances in text-to-3D scene generation have demonstrated significant potential to transform content creation across multiple industries. Although the research community has made impressive progress in addressing the challenges of this complex task, existing methods often generate environments that are only front-facing, lack visual fidelity, exhibit limited scene understanding, and are typically fine-tuned for either indoor or outdoor settings. In this work, we address these issues and propose DreamAnywhere, a modular system for the fast generation and prototyping of 3D scenes. Our system synthesizes a 360\u00b0 panoramic image from text, decomposes it into background and objects, constructs a complete 3D representation through hybrid inpainting, and lifts object masks to detailed 3D objects that are placed in the virtual environment. DreamAnywhere supports immersive navigation and intuitive object-level editing, making it ideal for scene exploration, visual mock-ups, and rapid prototyping -- all with minimal manual modeling. These features make our system particularly suitable for low-budget movie production, enabling quick iteration on scene layout and visual tone without the overhead of traditional 3D workflows. Our modular pipeline is highly customizable as it allows components to be replaced independently. Compared to current state-of-the-art text and image-based 3D scene generation approaches, DreamAnywhere shows significant improvements in coherence in novel view synthesis and achieves competitive image quality, demonstrating its effectiveness across diverse and challenging scenarios. A comprehensive user study demonstrates a clear preference for our method over existing approaches, validating both its technical robustness and practical usefulness.", "link": "http://arxiv.org/abs/2506.20367v2", "date": "2026-02-17", "relevancy": 2.8342, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.7319}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7039}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamAnywhere%3A%20Object-Centric%20Panoramic%203D%20Scene%20Generation&body=Title%3A%20DreamAnywhere%3A%20Object-Centric%20Panoramic%203D%20Scene%20Generation%0AAuthor%3A%20Edoardo%20Alberto%20Dominici%20and%20Jozef%20Hladky%20and%20Floor%20Verhoeven%20and%20Lukas%20Radl%20and%20Thomas%20Deixelberger%20and%20Stefan%20Ainetter%20and%20Philipp%20Drescher%20and%20Stefan%20Hauswiesner%20and%20Arno%20Coomans%20and%20Giacomo%20Nazzaro%20and%20Konstantinos%20Vardis%20and%20Markus%20Steinberger%0AAbstract%3A%20Recent%20advances%20in%20text-to-3D%20scene%20generation%20have%20demonstrated%20significant%20potential%20to%20transform%20content%20creation%20across%20multiple%20industries.%20Although%20the%20research%20community%20has%20made%20impressive%20progress%20in%20addressing%20the%20challenges%20of%20this%20complex%20task%2C%20existing%20methods%20often%20generate%20environments%20that%20are%20only%20front-facing%2C%20lack%20visual%20fidelity%2C%20exhibit%20limited%20scene%20understanding%2C%20and%20are%20typically%20fine-tuned%20for%20either%20indoor%20or%20outdoor%20settings.%20In%20this%20work%2C%20we%20address%20these%20issues%20and%20propose%20DreamAnywhere%2C%20a%20modular%20system%20for%20the%20fast%20generation%20and%20prototyping%20of%203D%20scenes.%20Our%20system%20synthesizes%20a%20360%C2%B0%20panoramic%20image%20from%20text%2C%20decomposes%20it%20into%20background%20and%20objects%2C%20constructs%20a%20complete%203D%20representation%20through%20hybrid%20inpainting%2C%20and%20lifts%20object%20masks%20to%20detailed%203D%20objects%20that%20are%20placed%20in%20the%20virtual%20environment.%20DreamAnywhere%20supports%20immersive%20navigation%20and%20intuitive%20object-level%20editing%2C%20making%20it%20ideal%20for%20scene%20exploration%2C%20visual%20mock-ups%2C%20and%20rapid%20prototyping%20--%20all%20with%20minimal%20manual%20modeling.%20These%20features%20make%20our%20system%20particularly%20suitable%20for%20low-budget%20movie%20production%2C%20enabling%20quick%20iteration%20on%20scene%20layout%20and%20visual%20tone%20without%20the%20overhead%20of%20traditional%203D%20workflows.%20Our%20modular%20pipeline%20is%20highly%20customizable%20as%20it%20allows%20components%20to%20be%20replaced%20independently.%20Compared%20to%20current%20state-of-the-art%20text%20and%20image-based%203D%20scene%20generation%20approaches%2C%20DreamAnywhere%20shows%20significant%20improvements%20in%20coherence%20in%20novel%20view%20synthesis%20and%20achieves%20competitive%20image%20quality%2C%20demonstrating%20its%20effectiveness%20across%20diverse%20and%20challenging%20scenarios.%20A%20comprehensive%20user%20study%20demonstrates%20a%20clear%20preference%20for%20our%20method%20over%20existing%20approaches%2C%20validating%20both%20its%20technical%20robustness%20and%20practical%20usefulness.%0ALink%3A%20http%3A//arxiv.org/abs/2506.20367v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamAnywhere%253A%2520Object-Centric%2520Panoramic%25203D%2520Scene%2520Generation%26entry.906535625%3DEdoardo%2520Alberto%2520Dominici%2520and%2520Jozef%2520Hladky%2520and%2520Floor%2520Verhoeven%2520and%2520Lukas%2520Radl%2520and%2520Thomas%2520Deixelberger%2520and%2520Stefan%2520Ainetter%2520and%2520Philipp%2520Drescher%2520and%2520Stefan%2520Hauswiesner%2520and%2520Arno%2520Coomans%2520and%2520Giacomo%2520Nazzaro%2520and%2520Konstantinos%2520Vardis%2520and%2520Markus%2520Steinberger%26entry.1292438233%3DRecent%2520advances%2520in%2520text-to-3D%2520scene%2520generation%2520have%2520demonstrated%2520significant%2520potential%2520to%2520transform%2520content%2520creation%2520across%2520multiple%2520industries.%2520Although%2520the%2520research%2520community%2520has%2520made%2520impressive%2520progress%2520in%2520addressing%2520the%2520challenges%2520of%2520this%2520complex%2520task%252C%2520existing%2520methods%2520often%2520generate%2520environments%2520that%2520are%2520only%2520front-facing%252C%2520lack%2520visual%2520fidelity%252C%2520exhibit%2520limited%2520scene%2520understanding%252C%2520and%2520are%2520typically%2520fine-tuned%2520for%2520either%2520indoor%2520or%2520outdoor%2520settings.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520issues%2520and%2520propose%2520DreamAnywhere%252C%2520a%2520modular%2520system%2520for%2520the%2520fast%2520generation%2520and%2520prototyping%2520of%25203D%2520scenes.%2520Our%2520system%2520synthesizes%2520a%2520360%25C2%25B0%2520panoramic%2520image%2520from%2520text%252C%2520decomposes%2520it%2520into%2520background%2520and%2520objects%252C%2520constructs%2520a%2520complete%25203D%2520representation%2520through%2520hybrid%2520inpainting%252C%2520and%2520lifts%2520object%2520masks%2520to%2520detailed%25203D%2520objects%2520that%2520are%2520placed%2520in%2520the%2520virtual%2520environment.%2520DreamAnywhere%2520supports%2520immersive%2520navigation%2520and%2520intuitive%2520object-level%2520editing%252C%2520making%2520it%2520ideal%2520for%2520scene%2520exploration%252C%2520visual%2520mock-ups%252C%2520and%2520rapid%2520prototyping%2520--%2520all%2520with%2520minimal%2520manual%2520modeling.%2520These%2520features%2520make%2520our%2520system%2520particularly%2520suitable%2520for%2520low-budget%2520movie%2520production%252C%2520enabling%2520quick%2520iteration%2520on%2520scene%2520layout%2520and%2520visual%2520tone%2520without%2520the%2520overhead%2520of%2520traditional%25203D%2520workflows.%2520Our%2520modular%2520pipeline%2520is%2520highly%2520customizable%2520as%2520it%2520allows%2520components%2520to%2520be%2520replaced%2520independently.%2520Compared%2520to%2520current%2520state-of-the-art%2520text%2520and%2520image-based%25203D%2520scene%2520generation%2520approaches%252C%2520DreamAnywhere%2520shows%2520significant%2520improvements%2520in%2520coherence%2520in%2520novel%2520view%2520synthesis%2520and%2520achieves%2520competitive%2520image%2520quality%252C%2520demonstrating%2520its%2520effectiveness%2520across%2520diverse%2520and%2520challenging%2520scenarios.%2520A%2520comprehensive%2520user%2520study%2520demonstrates%2520a%2520clear%2520preference%2520for%2520our%2520method%2520over%2520existing%2520approaches%252C%2520validating%2520both%2520its%2520technical%2520robustness%2520and%2520practical%2520usefulness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20367v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamAnywhere%3A%20Object-Centric%20Panoramic%203D%20Scene%20Generation&entry.906535625=Edoardo%20Alberto%20Dominici%20and%20Jozef%20Hladky%20and%20Floor%20Verhoeven%20and%20Lukas%20Radl%20and%20Thomas%20Deixelberger%20and%20Stefan%20Ainetter%20and%20Philipp%20Drescher%20and%20Stefan%20Hauswiesner%20and%20Arno%20Coomans%20and%20Giacomo%20Nazzaro%20and%20Konstantinos%20Vardis%20and%20Markus%20Steinberger&entry.1292438233=Recent%20advances%20in%20text-to-3D%20scene%20generation%20have%20demonstrated%20significant%20potential%20to%20transform%20content%20creation%20across%20multiple%20industries.%20Although%20the%20research%20community%20has%20made%20impressive%20progress%20in%20addressing%20the%20challenges%20of%20this%20complex%20task%2C%20existing%20methods%20often%20generate%20environments%20that%20are%20only%20front-facing%2C%20lack%20visual%20fidelity%2C%20exhibit%20limited%20scene%20understanding%2C%20and%20are%20typically%20fine-tuned%20for%20either%20indoor%20or%20outdoor%20settings.%20In%20this%20work%2C%20we%20address%20these%20issues%20and%20propose%20DreamAnywhere%2C%20a%20modular%20system%20for%20the%20fast%20generation%20and%20prototyping%20of%203D%20scenes.%20Our%20system%20synthesizes%20a%20360%C2%B0%20panoramic%20image%20from%20text%2C%20decomposes%20it%20into%20background%20and%20objects%2C%20constructs%20a%20complete%203D%20representation%20through%20hybrid%20inpainting%2C%20and%20lifts%20object%20masks%20to%20detailed%203D%20objects%20that%20are%20placed%20in%20the%20virtual%20environment.%20DreamAnywhere%20supports%20immersive%20navigation%20and%20intuitive%20object-level%20editing%2C%20making%20it%20ideal%20for%20scene%20exploration%2C%20visual%20mock-ups%2C%20and%20rapid%20prototyping%20--%20all%20with%20minimal%20manual%20modeling.%20These%20features%20make%20our%20system%20particularly%20suitable%20for%20low-budget%20movie%20production%2C%20enabling%20quick%20iteration%20on%20scene%20layout%20and%20visual%20tone%20without%20the%20overhead%20of%20traditional%203D%20workflows.%20Our%20modular%20pipeline%20is%20highly%20customizable%20as%20it%20allows%20components%20to%20be%20replaced%20independently.%20Compared%20to%20current%20state-of-the-art%20text%20and%20image-based%203D%20scene%20generation%20approaches%2C%20DreamAnywhere%20shows%20significant%20improvements%20in%20coherence%20in%20novel%20view%20synthesis%20and%20achieves%20competitive%20image%20quality%2C%20demonstrating%20its%20effectiveness%20across%20diverse%20and%20challenging%20scenarios.%20A%20comprehensive%20user%20study%20demonstrates%20a%20clear%20preference%20for%20our%20method%20over%20existing%20approaches%2C%20validating%20both%20its%20technical%20robustness%20and%20practical%20usefulness.&entry.1838667208=http%3A//arxiv.org/abs/2506.20367v2&entry.124074799=Read"},
{"title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving", "author": "Lucas Elbert Suryana and Farah Bierenga and Sanne van Buuren and Pepijn Kooij and Elsefien Tulleners and Federico Scari and Simeon Calvert and Bart van Arem and Arkady Zgonnikov", "abstract": "Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.", "link": "http://arxiv.org/abs/2602.15645v1", "date": "2026-02-17", "relevancy": 2.7766, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARE%20Drive%20A%20Framework%20for%20Evaluating%20Reason-Responsiveness%20of%20Vision%20Language%20Models%20in%20Automated%20Driving&body=Title%3A%20CARE%20Drive%20A%20Framework%20for%20Evaluating%20Reason-Responsiveness%20of%20Vision%20Language%20Models%20in%20Automated%20Driving%0AAuthor%3A%20Lucas%20Elbert%20Suryana%20and%20Farah%20Bierenga%20and%20Sanne%20van%20Buuren%20and%20Pepijn%20Kooij%20and%20Elsefien%20Tulleners%20and%20Federico%20Scari%20and%20Simeon%20Calvert%20and%20Bart%20van%20Arem%20and%20Arkady%20Zgonnikov%0AAbstract%3A%20Foundation%20models%2C%20including%20vision%20language%20models%2C%20are%20increasingly%20used%20in%20automated%20driving%20to%20interpret%20scenes%2C%20recommend%20actions%2C%20and%20generate%20natural%20language%20explanations.%20However%2C%20existing%20evaluation%20methods%20primarily%20assess%20outcome%20based%20performance%2C%20such%20as%20safety%20and%20trajectory%20accuracy%2C%20without%20determining%20whether%20model%20decisions%20reflect%20human%20relevant%20considerations.%20As%20a%20result%2C%20it%20remains%20unclear%20whether%20explanations%20produced%20by%20such%20models%20correspond%20to%20genuine%20reason%20responsive%20decision%20making%20or%20merely%20post%20hoc%20rationalizations.%20This%20limitation%20is%20especially%20significant%20in%20safety%20critical%20domains%20because%20it%20can%20create%20false%20confidence.%20To%20address%20this%20gap%2C%20we%20propose%20CARE%20Drive%2C%20Context%20Aware%20Reasons%20Evaluation%20for%20Driving%2C%20a%20model%20agnostic%20framework%20for%20evaluating%20reason%20responsiveness%20in%20vision%20language%20models%20applied%20to%20automated%20driving.%20CARE%20Drive%20compares%20baseline%20and%20reason%20augmented%20model%20decisions%20under%20controlled%20contextual%20variation%20to%20assess%20whether%20human%20reasons%20causally%20influence%20decision%20behavior.%20The%20framework%20employs%20a%20two%20stage%20evaluation%20process.%20Prompt%20calibration%20ensures%20stable%20outputs.%20Systematic%20contextual%20perturbation%20then%20measures%20decision%20sensitivity%20to%20human%20reasons%20such%20as%20safety%20margins%2C%20social%20pressure%2C%20and%20efficiency%20constraints.%20We%20demonstrate%20CARE%20Drive%20in%20a%20cyclist%20overtaking%20scenario%20involving%20competing%20normative%20considerations.%20Results%20show%20that%20explicit%20human%20reasons%20significantly%20influence%20model%20decisions%2C%20improving%20alignment%20with%20expert%20recommended%20behavior.%20However%2C%20responsiveness%20varies%20across%20contextual%20factors%2C%20indicating%20uneven%20sensitivity%20to%20different%20types%20of%20reasons.%20These%20findings%20provide%20empirical%20evidence%20that%20reason%20responsiveness%20in%20foundation%20models%20can%20be%20systematically%20evaluated%20without%20modifying%20model%20parameters.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARE%2520Drive%2520A%2520Framework%2520for%2520Evaluating%2520Reason-Responsiveness%2520of%2520Vision%2520Language%2520Models%2520in%2520Automated%2520Driving%26entry.906535625%3DLucas%2520Elbert%2520Suryana%2520and%2520Farah%2520Bierenga%2520and%2520Sanne%2520van%2520Buuren%2520and%2520Pepijn%2520Kooij%2520and%2520Elsefien%2520Tulleners%2520and%2520Federico%2520Scari%2520and%2520Simeon%2520Calvert%2520and%2520Bart%2520van%2520Arem%2520and%2520Arkady%2520Zgonnikov%26entry.1292438233%3DFoundation%2520models%252C%2520including%2520vision%2520language%2520models%252C%2520are%2520increasingly%2520used%2520in%2520automated%2520driving%2520to%2520interpret%2520scenes%252C%2520recommend%2520actions%252C%2520and%2520generate%2520natural%2520language%2520explanations.%2520However%252C%2520existing%2520evaluation%2520methods%2520primarily%2520assess%2520outcome%2520based%2520performance%252C%2520such%2520as%2520safety%2520and%2520trajectory%2520accuracy%252C%2520without%2520determining%2520whether%2520model%2520decisions%2520reflect%2520human%2520relevant%2520considerations.%2520As%2520a%2520result%252C%2520it%2520remains%2520unclear%2520whether%2520explanations%2520produced%2520by%2520such%2520models%2520correspond%2520to%2520genuine%2520reason%2520responsive%2520decision%2520making%2520or%2520merely%2520post%2520hoc%2520rationalizations.%2520This%2520limitation%2520is%2520especially%2520significant%2520in%2520safety%2520critical%2520domains%2520because%2520it%2520can%2520create%2520false%2520confidence.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520CARE%2520Drive%252C%2520Context%2520Aware%2520Reasons%2520Evaluation%2520for%2520Driving%252C%2520a%2520model%2520agnostic%2520framework%2520for%2520evaluating%2520reason%2520responsiveness%2520in%2520vision%2520language%2520models%2520applied%2520to%2520automated%2520driving.%2520CARE%2520Drive%2520compares%2520baseline%2520and%2520reason%2520augmented%2520model%2520decisions%2520under%2520controlled%2520contextual%2520variation%2520to%2520assess%2520whether%2520human%2520reasons%2520causally%2520influence%2520decision%2520behavior.%2520The%2520framework%2520employs%2520a%2520two%2520stage%2520evaluation%2520process.%2520Prompt%2520calibration%2520ensures%2520stable%2520outputs.%2520Systematic%2520contextual%2520perturbation%2520then%2520measures%2520decision%2520sensitivity%2520to%2520human%2520reasons%2520such%2520as%2520safety%2520margins%252C%2520social%2520pressure%252C%2520and%2520efficiency%2520constraints.%2520We%2520demonstrate%2520CARE%2520Drive%2520in%2520a%2520cyclist%2520overtaking%2520scenario%2520involving%2520competing%2520normative%2520considerations.%2520Results%2520show%2520that%2520explicit%2520human%2520reasons%2520significantly%2520influence%2520model%2520decisions%252C%2520improving%2520alignment%2520with%2520expert%2520recommended%2520behavior.%2520However%252C%2520responsiveness%2520varies%2520across%2520contextual%2520factors%252C%2520indicating%2520uneven%2520sensitivity%2520to%2520different%2520types%2520of%2520reasons.%2520These%2520findings%2520provide%2520empirical%2520evidence%2520that%2520reason%2520responsiveness%2520in%2520foundation%2520models%2520can%2520be%2520systematically%2520evaluated%2520without%2520modifying%2520model%2520parameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARE%20Drive%20A%20Framework%20for%20Evaluating%20Reason-Responsiveness%20of%20Vision%20Language%20Models%20in%20Automated%20Driving&entry.906535625=Lucas%20Elbert%20Suryana%20and%20Farah%20Bierenga%20and%20Sanne%20van%20Buuren%20and%20Pepijn%20Kooij%20and%20Elsefien%20Tulleners%20and%20Federico%20Scari%20and%20Simeon%20Calvert%20and%20Bart%20van%20Arem%20and%20Arkady%20Zgonnikov&entry.1292438233=Foundation%20models%2C%20including%20vision%20language%20models%2C%20are%20increasingly%20used%20in%20automated%20driving%20to%20interpret%20scenes%2C%20recommend%20actions%2C%20and%20generate%20natural%20language%20explanations.%20However%2C%20existing%20evaluation%20methods%20primarily%20assess%20outcome%20based%20performance%2C%20such%20as%20safety%20and%20trajectory%20accuracy%2C%20without%20determining%20whether%20model%20decisions%20reflect%20human%20relevant%20considerations.%20As%20a%20result%2C%20it%20remains%20unclear%20whether%20explanations%20produced%20by%20such%20models%20correspond%20to%20genuine%20reason%20responsive%20decision%20making%20or%20merely%20post%20hoc%20rationalizations.%20This%20limitation%20is%20especially%20significant%20in%20safety%20critical%20domains%20because%20it%20can%20create%20false%20confidence.%20To%20address%20this%20gap%2C%20we%20propose%20CARE%20Drive%2C%20Context%20Aware%20Reasons%20Evaluation%20for%20Driving%2C%20a%20model%20agnostic%20framework%20for%20evaluating%20reason%20responsiveness%20in%20vision%20language%20models%20applied%20to%20automated%20driving.%20CARE%20Drive%20compares%20baseline%20and%20reason%20augmented%20model%20decisions%20under%20controlled%20contextual%20variation%20to%20assess%20whether%20human%20reasons%20causally%20influence%20decision%20behavior.%20The%20framework%20employs%20a%20two%20stage%20evaluation%20process.%20Prompt%20calibration%20ensures%20stable%20outputs.%20Systematic%20contextual%20perturbation%20then%20measures%20decision%20sensitivity%20to%20human%20reasons%20such%20as%20safety%20margins%2C%20social%20pressure%2C%20and%20efficiency%20constraints.%20We%20demonstrate%20CARE%20Drive%20in%20a%20cyclist%20overtaking%20scenario%20involving%20competing%20normative%20considerations.%20Results%20show%20that%20explicit%20human%20reasons%20significantly%20influence%20model%20decisions%2C%20improving%20alignment%20with%20expert%20recommended%20behavior.%20However%2C%20responsiveness%20varies%20across%20contextual%20factors%2C%20indicating%20uneven%20sensitivity%20to%20different%20types%20of%20reasons.%20These%20findings%20provide%20empirical%20evidence%20that%20reason%20responsiveness%20in%20foundation%20models%20can%20be%20systematically%20evaluated%20without%20modifying%20model%20parameters.&entry.1838667208=http%3A//arxiv.org/abs/2602.15645v1&entry.124074799=Read"},
{"title": "Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation", "author": "Shutian Gu and Chengkai Huang and Ruoyu Wang and Lina Yao", "abstract": "Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.", "link": "http://arxiv.org/abs/2602.15724v1", "date": "2026-02-17", "relevancy": 2.7632, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5626}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Retrieve%20Navigable%20Candidates%20for%20Efficient%20Vision-and-Language%20Navigation&body=Title%3A%20Learning%20to%20Retrieve%20Navigable%20Candidates%20for%20Efficient%20Vision-and-Language%20Navigation%0AAuthor%3A%20Shutian%20Gu%20and%20Chengkai%20Huang%20and%20Ruoyu%20Wang%20and%20Lina%20Yao%0AAbstract%3A%20Vision-and-Language%20Navigation%20%28VLN%29%20requires%20an%20agent%20to%20follow%20natural-language%20instructions%20and%20navigate%20through%20previously%20unseen%20environments.%20Recent%20approaches%20increasingly%20employ%20large%20language%20models%20%28LLMs%29%20as%20high-level%20navigators%20due%20to%20their%20flexibility%20and%20reasoning%20capability.%20However%2C%20prompt-based%20LLM%20navigation%20often%20suffers%20from%20inefficient%20decision-making%2C%20as%20the%20model%20must%20repeatedly%20interpret%20instructions%20from%20scratch%20and%20reason%20over%20noisy%20and%20verbose%20navigable%20candidates%20at%20each%20step.%20In%20this%20paper%2C%20we%20propose%20a%20retrieval-augmented%20framework%20to%20improve%20the%20efficiency%20and%20stability%20of%20LLM-based%20VLN%20without%20modifying%20or%20fine-tuning%20the%20underlying%20language%20model.%20Our%20approach%20introduces%20retrieval%20at%20two%20complementary%20levels.%20At%20the%20episode%20level%2C%20an%20instruction-level%20embedding%20retriever%20selects%20semantically%20similar%20successful%20navigation%20trajectories%20as%20in-context%20exemplars%2C%20providing%20task-specific%20priors%20for%20instruction%20grounding.%20At%20the%20step%20level%2C%20an%20imitation-learned%20candidate%20retriever%20prunes%20irrelevant%20navigable%20directions%20before%20LLM%20inference%2C%20reducing%20action%20ambiguity%20and%20prompt%20complexity.%20Both%20retrieval%20modules%20are%20lightweight%2C%20modular%2C%20and%20trained%20independently%20of%20the%20LLM.%20We%20evaluate%20our%20method%20on%20the%20Room-to-Room%20%28R2R%29%20benchmark.%20Experimental%20results%20demonstrate%20consistent%20improvements%20in%20Success%20Rate%2C%20Oracle%20Success%20Rate%2C%20and%20SPL%20on%20both%20seen%20and%20unseen%20environments.%20Ablation%20studies%20further%20show%20that%20instruction-level%20exemplar%20retrieval%20and%20candidate%20pruning%20contribute%20complementary%20benefits%20to%20global%20guidance%20and%20step-wise%20decision%20efficiency.%20These%20results%20indicate%20that%20retrieval-augmented%20decision%20support%20is%20an%20effective%20and%20scalable%20strategy%20for%20enhancing%20LLM-based%20vision-and-language%20navigation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Retrieve%2520Navigable%2520Candidates%2520for%2520Efficient%2520Vision-and-Language%2520Navigation%26entry.906535625%3DShutian%2520Gu%2520and%2520Chengkai%2520Huang%2520and%2520Ruoyu%2520Wang%2520and%2520Lina%2520Yao%26entry.1292438233%3DVision-and-Language%2520Navigation%2520%2528VLN%2529%2520requires%2520an%2520agent%2520to%2520follow%2520natural-language%2520instructions%2520and%2520navigate%2520through%2520previously%2520unseen%2520environments.%2520Recent%2520approaches%2520increasingly%2520employ%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520high-level%2520navigators%2520due%2520to%2520their%2520flexibility%2520and%2520reasoning%2520capability.%2520However%252C%2520prompt-based%2520LLM%2520navigation%2520often%2520suffers%2520from%2520inefficient%2520decision-making%252C%2520as%2520the%2520model%2520must%2520repeatedly%2520interpret%2520instructions%2520from%2520scratch%2520and%2520reason%2520over%2520noisy%2520and%2520verbose%2520navigable%2520candidates%2520at%2520each%2520step.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520retrieval-augmented%2520framework%2520to%2520improve%2520the%2520efficiency%2520and%2520stability%2520of%2520LLM-based%2520VLN%2520without%2520modifying%2520or%2520fine-tuning%2520the%2520underlying%2520language%2520model.%2520Our%2520approach%2520introduces%2520retrieval%2520at%2520two%2520complementary%2520levels.%2520At%2520the%2520episode%2520level%252C%2520an%2520instruction-level%2520embedding%2520retriever%2520selects%2520semantically%2520similar%2520successful%2520navigation%2520trajectories%2520as%2520in-context%2520exemplars%252C%2520providing%2520task-specific%2520priors%2520for%2520instruction%2520grounding.%2520At%2520the%2520step%2520level%252C%2520an%2520imitation-learned%2520candidate%2520retriever%2520prunes%2520irrelevant%2520navigable%2520directions%2520before%2520LLM%2520inference%252C%2520reducing%2520action%2520ambiguity%2520and%2520prompt%2520complexity.%2520Both%2520retrieval%2520modules%2520are%2520lightweight%252C%2520modular%252C%2520and%2520trained%2520independently%2520of%2520the%2520LLM.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520Room-to-Room%2520%2528R2R%2529%2520benchmark.%2520Experimental%2520results%2520demonstrate%2520consistent%2520improvements%2520in%2520Success%2520Rate%252C%2520Oracle%2520Success%2520Rate%252C%2520and%2520SPL%2520on%2520both%2520seen%2520and%2520unseen%2520environments.%2520Ablation%2520studies%2520further%2520show%2520that%2520instruction-level%2520exemplar%2520retrieval%2520and%2520candidate%2520pruning%2520contribute%2520complementary%2520benefits%2520to%2520global%2520guidance%2520and%2520step-wise%2520decision%2520efficiency.%2520These%2520results%2520indicate%2520that%2520retrieval-augmented%2520decision%2520support%2520is%2520an%2520effective%2520and%2520scalable%2520strategy%2520for%2520enhancing%2520LLM-based%2520vision-and-language%2520navigation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Retrieve%20Navigable%20Candidates%20for%20Efficient%20Vision-and-Language%20Navigation&entry.906535625=Shutian%20Gu%20and%20Chengkai%20Huang%20and%20Ruoyu%20Wang%20and%20Lina%20Yao&entry.1292438233=Vision-and-Language%20Navigation%20%28VLN%29%20requires%20an%20agent%20to%20follow%20natural-language%20instructions%20and%20navigate%20through%20previously%20unseen%20environments.%20Recent%20approaches%20increasingly%20employ%20large%20language%20models%20%28LLMs%29%20as%20high-level%20navigators%20due%20to%20their%20flexibility%20and%20reasoning%20capability.%20However%2C%20prompt-based%20LLM%20navigation%20often%20suffers%20from%20inefficient%20decision-making%2C%20as%20the%20model%20must%20repeatedly%20interpret%20instructions%20from%20scratch%20and%20reason%20over%20noisy%20and%20verbose%20navigable%20candidates%20at%20each%20step.%20In%20this%20paper%2C%20we%20propose%20a%20retrieval-augmented%20framework%20to%20improve%20the%20efficiency%20and%20stability%20of%20LLM-based%20VLN%20without%20modifying%20or%20fine-tuning%20the%20underlying%20language%20model.%20Our%20approach%20introduces%20retrieval%20at%20two%20complementary%20levels.%20At%20the%20episode%20level%2C%20an%20instruction-level%20embedding%20retriever%20selects%20semantically%20similar%20successful%20navigation%20trajectories%20as%20in-context%20exemplars%2C%20providing%20task-specific%20priors%20for%20instruction%20grounding.%20At%20the%20step%20level%2C%20an%20imitation-learned%20candidate%20retriever%20prunes%20irrelevant%20navigable%20directions%20before%20LLM%20inference%2C%20reducing%20action%20ambiguity%20and%20prompt%20complexity.%20Both%20retrieval%20modules%20are%20lightweight%2C%20modular%2C%20and%20trained%20independently%20of%20the%20LLM.%20We%20evaluate%20our%20method%20on%20the%20Room-to-Room%20%28R2R%29%20benchmark.%20Experimental%20results%20demonstrate%20consistent%20improvements%20in%20Success%20Rate%2C%20Oracle%20Success%20Rate%2C%20and%20SPL%20on%20both%20seen%20and%20unseen%20environments.%20Ablation%20studies%20further%20show%20that%20instruction-level%20exemplar%20retrieval%20and%20candidate%20pruning%20contribute%20complementary%20benefits%20to%20global%20guidance%20and%20step-wise%20decision%20efficiency.%20These%20results%20indicate%20that%20retrieval-augmented%20decision%20support%20is%20an%20effective%20and%20scalable%20strategy%20for%20enhancing%20LLM-based%20vision-and-language%20navigation.&entry.1838667208=http%3A//arxiv.org/abs/2602.15724v1&entry.124074799=Read"},
{"title": "Spanning the Visual Analogy Space with a Weight Basis of LoRAs", "author": "Hila Manor and Rinon Gal and Haggai Maron and Tomer Michaeli and Gal Chechik", "abstract": "Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\\{\\mathbf{a}$, $\\mathbf{a}'$, $\\mathbf{b}\\}$, the goal is to generate $\\mathbf{b}'$ such that $\\mathbf{a} : \\mathbf{a}' :: \\mathbf{b} : \\mathbf{b}'$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a \"space of LoRAs\". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb", "link": "http://arxiv.org/abs/2602.15727v1", "date": "2026-02-17", "relevancy": 2.757, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spanning%20the%20Visual%20Analogy%20Space%20with%20a%20Weight%20Basis%20of%20LoRAs&body=Title%3A%20Spanning%20the%20Visual%20Analogy%20Space%20with%20a%20Weight%20Basis%20of%20LoRAs%0AAuthor%3A%20Hila%20Manor%20and%20Rinon%20Gal%20and%20Haggai%20Maron%20and%20Tomer%20Michaeli%20and%20Gal%20Chechik%0AAbstract%3A%20Visual%20analogy%20learning%20enables%20image%20manipulation%20through%20demonstration%20rather%20than%20textual%20description%2C%20allowing%20users%20to%20specify%20complex%20transformations%20difficult%20to%20articulate%20in%20words.%20Given%20a%20triplet%20%24%5C%7B%5Cmathbf%7Ba%7D%24%2C%20%24%5Cmathbf%7Ba%7D%27%24%2C%20%24%5Cmathbf%7Bb%7D%5C%7D%24%2C%20the%20goal%20is%20to%20generate%20%24%5Cmathbf%7Bb%7D%27%24%20such%20that%20%24%5Cmathbf%7Ba%7D%20%3A%20%5Cmathbf%7Ba%7D%27%20%3A%3A%20%5Cmathbf%7Bb%7D%20%3A%20%5Cmathbf%7Bb%7D%27%24.%20Recent%20methods%20adapt%20text-to-image%20models%20to%20this%20task%20using%20a%20single%20Low-Rank%20Adaptation%20%28LoRA%29%20module%2C%20but%20they%20face%20a%20fundamental%20limitation%3A%20attempting%20to%20capture%20the%20diverse%20space%20of%20visual%20transformations%20within%20a%20fixed%20adaptation%20module%20constrains%20generalization%20capabilities.%20Inspired%20by%20recent%20work%20showing%20that%20LoRAs%20in%20constrained%20domains%20span%20meaningful%2C%20interpolatable%20semantic%20spaces%2C%20we%20propose%20LoRWeB%2C%20a%20novel%20approach%20that%20specializes%20the%20model%20for%20each%20analogy%20task%20at%20inference%20time%20through%20dynamic%20composition%20of%20learned%20transformation%20primitives%2C%20informally%2C%20choosing%20a%20point%20in%20a%20%22space%20of%20LoRAs%22.%20We%20introduce%20two%20key%20components%3A%20%281%29%20a%20learnable%20basis%20of%20LoRA%20modules%2C%20to%20span%20the%20space%20of%20different%20visual%20transformations%2C%20and%20%282%29%20a%20lightweight%20encoder%20that%20dynamically%20selects%20and%20weighs%20these%20basis%20LoRAs%20based%20on%20the%20input%20analogy%20pair.%20Comprehensive%20evaluations%20demonstrate%20our%20approach%20achieves%20state-of-the-art%20performance%20and%20significantly%20improves%20generalization%20to%20unseen%20visual%20transformations.%20Our%20findings%20suggest%20that%20LoRA%20basis%20decompositions%20are%20a%20promising%20direction%20for%20flexible%20visual%20manipulation.%20Code%20and%20data%20are%20in%20https%3A//research.nvidia.com/labs/par/lorweb%0ALink%3A%20http%3A//arxiv.org/abs/2602.15727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpanning%2520the%2520Visual%2520Analogy%2520Space%2520with%2520a%2520Weight%2520Basis%2520of%2520LoRAs%26entry.906535625%3DHila%2520Manor%2520and%2520Rinon%2520Gal%2520and%2520Haggai%2520Maron%2520and%2520Tomer%2520Michaeli%2520and%2520Gal%2520Chechik%26entry.1292438233%3DVisual%2520analogy%2520learning%2520enables%2520image%2520manipulation%2520through%2520demonstration%2520rather%2520than%2520textual%2520description%252C%2520allowing%2520users%2520to%2520specify%2520complex%2520transformations%2520difficult%2520to%2520articulate%2520in%2520words.%2520Given%2520a%2520triplet%2520%2524%255C%257B%255Cmathbf%257Ba%257D%2524%252C%2520%2524%255Cmathbf%257Ba%257D%2527%2524%252C%2520%2524%255Cmathbf%257Bb%257D%255C%257D%2524%252C%2520the%2520goal%2520is%2520to%2520generate%2520%2524%255Cmathbf%257Bb%257D%2527%2524%2520such%2520that%2520%2524%255Cmathbf%257Ba%257D%2520%253A%2520%255Cmathbf%257Ba%257D%2527%2520%253A%253A%2520%255Cmathbf%257Bb%257D%2520%253A%2520%255Cmathbf%257Bb%257D%2527%2524.%2520Recent%2520methods%2520adapt%2520text-to-image%2520models%2520to%2520this%2520task%2520using%2520a%2520single%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520module%252C%2520but%2520they%2520face%2520a%2520fundamental%2520limitation%253A%2520attempting%2520to%2520capture%2520the%2520diverse%2520space%2520of%2520visual%2520transformations%2520within%2520a%2520fixed%2520adaptation%2520module%2520constrains%2520generalization%2520capabilities.%2520Inspired%2520by%2520recent%2520work%2520showing%2520that%2520LoRAs%2520in%2520constrained%2520domains%2520span%2520meaningful%252C%2520interpolatable%2520semantic%2520spaces%252C%2520we%2520propose%2520LoRWeB%252C%2520a%2520novel%2520approach%2520that%2520specializes%2520the%2520model%2520for%2520each%2520analogy%2520task%2520at%2520inference%2520time%2520through%2520dynamic%2520composition%2520of%2520learned%2520transformation%2520primitives%252C%2520informally%252C%2520choosing%2520a%2520point%2520in%2520a%2520%2522space%2520of%2520LoRAs%2522.%2520We%2520introduce%2520two%2520key%2520components%253A%2520%25281%2529%2520a%2520learnable%2520basis%2520of%2520LoRA%2520modules%252C%2520to%2520span%2520the%2520space%2520of%2520different%2520visual%2520transformations%252C%2520and%2520%25282%2529%2520a%2520lightweight%2520encoder%2520that%2520dynamically%2520selects%2520and%2520weighs%2520these%2520basis%2520LoRAs%2520based%2520on%2520the%2520input%2520analogy%2520pair.%2520Comprehensive%2520evaluations%2520demonstrate%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520and%2520significantly%2520improves%2520generalization%2520to%2520unseen%2520visual%2520transformations.%2520Our%2520findings%2520suggest%2520that%2520LoRA%2520basis%2520decompositions%2520are%2520a%2520promising%2520direction%2520for%2520flexible%2520visual%2520manipulation.%2520Code%2520and%2520data%2520are%2520in%2520https%253A//research.nvidia.com/labs/par/lorweb%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spanning%20the%20Visual%20Analogy%20Space%20with%20a%20Weight%20Basis%20of%20LoRAs&entry.906535625=Hila%20Manor%20and%20Rinon%20Gal%20and%20Haggai%20Maron%20and%20Tomer%20Michaeli%20and%20Gal%20Chechik&entry.1292438233=Visual%20analogy%20learning%20enables%20image%20manipulation%20through%20demonstration%20rather%20than%20textual%20description%2C%20allowing%20users%20to%20specify%20complex%20transformations%20difficult%20to%20articulate%20in%20words.%20Given%20a%20triplet%20%24%5C%7B%5Cmathbf%7Ba%7D%24%2C%20%24%5Cmathbf%7Ba%7D%27%24%2C%20%24%5Cmathbf%7Bb%7D%5C%7D%24%2C%20the%20goal%20is%20to%20generate%20%24%5Cmathbf%7Bb%7D%27%24%20such%20that%20%24%5Cmathbf%7Ba%7D%20%3A%20%5Cmathbf%7Ba%7D%27%20%3A%3A%20%5Cmathbf%7Bb%7D%20%3A%20%5Cmathbf%7Bb%7D%27%24.%20Recent%20methods%20adapt%20text-to-image%20models%20to%20this%20task%20using%20a%20single%20Low-Rank%20Adaptation%20%28LoRA%29%20module%2C%20but%20they%20face%20a%20fundamental%20limitation%3A%20attempting%20to%20capture%20the%20diverse%20space%20of%20visual%20transformations%20within%20a%20fixed%20adaptation%20module%20constrains%20generalization%20capabilities.%20Inspired%20by%20recent%20work%20showing%20that%20LoRAs%20in%20constrained%20domains%20span%20meaningful%2C%20interpolatable%20semantic%20spaces%2C%20we%20propose%20LoRWeB%2C%20a%20novel%20approach%20that%20specializes%20the%20model%20for%20each%20analogy%20task%20at%20inference%20time%20through%20dynamic%20composition%20of%20learned%20transformation%20primitives%2C%20informally%2C%20choosing%20a%20point%20in%20a%20%22space%20of%20LoRAs%22.%20We%20introduce%20two%20key%20components%3A%20%281%29%20a%20learnable%20basis%20of%20LoRA%20modules%2C%20to%20span%20the%20space%20of%20different%20visual%20transformations%2C%20and%20%282%29%20a%20lightweight%20encoder%20that%20dynamically%20selects%20and%20weighs%20these%20basis%20LoRAs%20based%20on%20the%20input%20analogy%20pair.%20Comprehensive%20evaluations%20demonstrate%20our%20approach%20achieves%20state-of-the-art%20performance%20and%20significantly%20improves%20generalization%20to%20unseen%20visual%20transformations.%20Our%20findings%20suggest%20that%20LoRA%20basis%20decompositions%20are%20a%20promising%20direction%20for%20flexible%20visual%20manipulation.%20Code%20and%20data%20are%20in%20https%3A//research.nvidia.com/labs/par/lorweb&entry.1838667208=http%3A//arxiv.org/abs/2602.15727v1&entry.124074799=Read"},
{"title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models", "author": "Sen Ye and Mengde Xu and Shuyang Gu and Di He and Liwei Wang and Han Hu", "abstract": "Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.", "link": "http://arxiv.org/abs/2602.15772v1", "date": "2026-02-17", "relevancy": 2.748, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20vs.%20Generation%3A%20Navigating%20Optimization%20Dilemma%20in%20Multimodal%20Models&body=Title%3A%20Understanding%20vs.%20Generation%3A%20Navigating%20Optimization%20Dilemma%20in%20Multimodal%20Models%0AAuthor%3A%20Sen%20Ye%20and%20Mengde%20Xu%20and%20Shuyang%20Gu%20and%20Di%20He%20and%20Liwei%20Wang%20and%20Han%20Hu%0AAbstract%3A%20Current%20research%20in%20multimodal%20models%20faces%20a%20key%20challenge%20where%20enhancing%20generative%20capabilities%20often%20comes%20at%20the%20expense%20of%20understanding%2C%20and%20vice%20versa.%20We%20analyzed%20this%20trade-off%20and%20identify%20the%20primary%20cause%20might%20be%20the%20potential%20conflict%20between%20generation%20and%20understanding%2C%20which%20creates%20a%20competitive%20dynamic%20within%20the%20model.%20To%20address%20this%2C%20we%20propose%20the%20Reason-Reflect-Refine%20%28R3%29%20framework.%20This%20innovative%20algorithm%20re-frames%20the%20single-step%20generation%20task%20into%20a%20multi-step%20process%20of%20%22generate-understand-regenerate%22.%20By%20explicitly%20leveraging%20the%20model%27s%20understanding%20capability%20during%20generation%2C%20we%20successfully%20mitigate%20the%20optimization%20dilemma%2C%20achieved%20stronger%20generation%20results%20and%20improved%20understanding%20ability%20which%20are%20related%20to%20the%20generation%20process.%20This%20offers%20valuable%20insights%20for%20designing%20next-generation%20unified%20multimodal%20models.%20Code%20is%20available%20at%20https%3A//github.com/sen-ye/R3.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520vs.%2520Generation%253A%2520Navigating%2520Optimization%2520Dilemma%2520in%2520Multimodal%2520Models%26entry.906535625%3DSen%2520Ye%2520and%2520Mengde%2520Xu%2520and%2520Shuyang%2520Gu%2520and%2520Di%2520He%2520and%2520Liwei%2520Wang%2520and%2520Han%2520Hu%26entry.1292438233%3DCurrent%2520research%2520in%2520multimodal%2520models%2520faces%2520a%2520key%2520challenge%2520where%2520enhancing%2520generative%2520capabilities%2520often%2520comes%2520at%2520the%2520expense%2520of%2520understanding%252C%2520and%2520vice%2520versa.%2520We%2520analyzed%2520this%2520trade-off%2520and%2520identify%2520the%2520primary%2520cause%2520might%2520be%2520the%2520potential%2520conflict%2520between%2520generation%2520and%2520understanding%252C%2520which%2520creates%2520a%2520competitive%2520dynamic%2520within%2520the%2520model.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Reason-Reflect-Refine%2520%2528R3%2529%2520framework.%2520This%2520innovative%2520algorithm%2520re-frames%2520the%2520single-step%2520generation%2520task%2520into%2520a%2520multi-step%2520process%2520of%2520%2522generate-understand-regenerate%2522.%2520By%2520explicitly%2520leveraging%2520the%2520model%2527s%2520understanding%2520capability%2520during%2520generation%252C%2520we%2520successfully%2520mitigate%2520the%2520optimization%2520dilemma%252C%2520achieved%2520stronger%2520generation%2520results%2520and%2520improved%2520understanding%2520ability%2520which%2520are%2520related%2520to%2520the%2520generation%2520process.%2520This%2520offers%2520valuable%2520insights%2520for%2520designing%2520next-generation%2520unified%2520multimodal%2520models.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/sen-ye/R3.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20vs.%20Generation%3A%20Navigating%20Optimization%20Dilemma%20in%20Multimodal%20Models&entry.906535625=Sen%20Ye%20and%20Mengde%20Xu%20and%20Shuyang%20Gu%20and%20Di%20He%20and%20Liwei%20Wang%20and%20Han%20Hu&entry.1292438233=Current%20research%20in%20multimodal%20models%20faces%20a%20key%20challenge%20where%20enhancing%20generative%20capabilities%20often%20comes%20at%20the%20expense%20of%20understanding%2C%20and%20vice%20versa.%20We%20analyzed%20this%20trade-off%20and%20identify%20the%20primary%20cause%20might%20be%20the%20potential%20conflict%20between%20generation%20and%20understanding%2C%20which%20creates%20a%20competitive%20dynamic%20within%20the%20model.%20To%20address%20this%2C%20we%20propose%20the%20Reason-Reflect-Refine%20%28R3%29%20framework.%20This%20innovative%20algorithm%20re-frames%20the%20single-step%20generation%20task%20into%20a%20multi-step%20process%20of%20%22generate-understand-regenerate%22.%20By%20explicitly%20leveraging%20the%20model%27s%20understanding%20capability%20during%20generation%2C%20we%20successfully%20mitigate%20the%20optimization%20dilemma%2C%20achieved%20stronger%20generation%20results%20and%20improved%20understanding%20ability%20which%20are%20related%20to%20the%20generation%20process.%20This%20offers%20valuable%20insights%20for%20designing%20next-generation%20unified%20multimodal%20models.%20Code%20is%20available%20at%20https%3A//github.com/sen-ye/R3.&entry.1838667208=http%3A//arxiv.org/abs/2602.15772v1&entry.124074799=Read"},
{"title": "Revealing and Enhancing Core Visual Regions: Harnessing Internal Attention Dynamics for Hallucination Mitigation in LVLMs", "author": "Guangtao Lyu and Qi Liu and Chenghao Xu and Jiexi Yan and Muli Yang and Xueting Li and Fen Fang and Cheng Deng", "abstract": "LVLMs have achieved strong multimodal reasoning capabilities but remain prone to hallucinations, producing outputs inconsistent with visual inputs or user instructions. Existing training-free methods, including contrastive decoding and auxiliary expert models, which incur several times more computational overhead and may introduce potential interference, as well as static internal signal enhancement, are often vulnerable to the attention sink phenomenon. We find that internal Positive Attention Dynamics (PAD) in LVLMs naturally reveal semantically core visual regions under the distortions of attention sinks. Based on this, we propose Positive Attention Dynamics Enhancement (PADE), a training-free attention intervention that constructs a PAD map to identify semantically core visual regions, applies per-head Median Absolute Deviation Scaling to adaptively control the intervention strength, and leverages System-Token Compensation to maintain attention to complex user instructions and support long-term output consistency. Experiments on multiple LVLMs and benchmarks show that PADE improves visual grounding and reduces hallucinations, validating the effectiveness of leveraging internal attention dynamics for reliable multimodal reasoning.", "link": "http://arxiv.org/abs/2602.15556v1", "date": "2026-02-17", "relevancy": 2.6849, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20and%20Enhancing%20Core%20Visual%20Regions%3A%20Harnessing%20Internal%20Attention%20Dynamics%20for%20Hallucination%20Mitigation%20in%20LVLMs&body=Title%3A%20Revealing%20and%20Enhancing%20Core%20Visual%20Regions%3A%20Harnessing%20Internal%20Attention%20Dynamics%20for%20Hallucination%20Mitigation%20in%20LVLMs%0AAuthor%3A%20Guangtao%20Lyu%20and%20Qi%20Liu%20and%20Chenghao%20Xu%20and%20Jiexi%20Yan%20and%20Muli%20Yang%20and%20Xueting%20Li%20and%20Fen%20Fang%20and%20Cheng%20Deng%0AAbstract%3A%20LVLMs%20have%20achieved%20strong%20multimodal%20reasoning%20capabilities%20but%20remain%20prone%20to%20hallucinations%2C%20producing%20outputs%20inconsistent%20with%20visual%20inputs%20or%20user%20instructions.%20Existing%20training-free%20methods%2C%20including%20contrastive%20decoding%20and%20auxiliary%20expert%20models%2C%20which%20incur%20several%20times%20more%20computational%20overhead%20and%20may%20introduce%20potential%20interference%2C%20as%20well%20as%20static%20internal%20signal%20enhancement%2C%20are%20often%20vulnerable%20to%20the%20attention%20sink%20phenomenon.%20We%20find%20that%20internal%20Positive%20Attention%20Dynamics%20%28PAD%29%20in%20LVLMs%20naturally%20reveal%20semantically%20core%20visual%20regions%20under%20the%20distortions%20of%20attention%20sinks.%20Based%20on%20this%2C%20we%20propose%20Positive%20Attention%20Dynamics%20Enhancement%20%28PADE%29%2C%20a%20training-free%20attention%20intervention%20that%20constructs%20a%20PAD%20map%20to%20identify%20semantically%20core%20visual%20regions%2C%20applies%20per-head%20Median%20Absolute%20Deviation%20Scaling%20to%20adaptively%20control%20the%20intervention%20strength%2C%20and%20leverages%20System-Token%20Compensation%20to%20maintain%20attention%20to%20complex%20user%20instructions%20and%20support%20long-term%20output%20consistency.%20Experiments%20on%20multiple%20LVLMs%20and%20benchmarks%20show%20that%20PADE%20improves%20visual%20grounding%20and%20reduces%20hallucinations%2C%20validating%20the%20effectiveness%20of%20leveraging%20internal%20attention%20dynamics%20for%20reliable%20multimodal%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520and%2520Enhancing%2520Core%2520Visual%2520Regions%253A%2520Harnessing%2520Internal%2520Attention%2520Dynamics%2520for%2520Hallucination%2520Mitigation%2520in%2520LVLMs%26entry.906535625%3DGuangtao%2520Lyu%2520and%2520Qi%2520Liu%2520and%2520Chenghao%2520Xu%2520and%2520Jiexi%2520Yan%2520and%2520Muli%2520Yang%2520and%2520Xueting%2520Li%2520and%2520Fen%2520Fang%2520and%2520Cheng%2520Deng%26entry.1292438233%3DLVLMs%2520have%2520achieved%2520strong%2520multimodal%2520reasoning%2520capabilities%2520but%2520remain%2520prone%2520to%2520hallucinations%252C%2520producing%2520outputs%2520inconsistent%2520with%2520visual%2520inputs%2520or%2520user%2520instructions.%2520Existing%2520training-free%2520methods%252C%2520including%2520contrastive%2520decoding%2520and%2520auxiliary%2520expert%2520models%252C%2520which%2520incur%2520several%2520times%2520more%2520computational%2520overhead%2520and%2520may%2520introduce%2520potential%2520interference%252C%2520as%2520well%2520as%2520static%2520internal%2520signal%2520enhancement%252C%2520are%2520often%2520vulnerable%2520to%2520the%2520attention%2520sink%2520phenomenon.%2520We%2520find%2520that%2520internal%2520Positive%2520Attention%2520Dynamics%2520%2528PAD%2529%2520in%2520LVLMs%2520naturally%2520reveal%2520semantically%2520core%2520visual%2520regions%2520under%2520the%2520distortions%2520of%2520attention%2520sinks.%2520Based%2520on%2520this%252C%2520we%2520propose%2520Positive%2520Attention%2520Dynamics%2520Enhancement%2520%2528PADE%2529%252C%2520a%2520training-free%2520attention%2520intervention%2520that%2520constructs%2520a%2520PAD%2520map%2520to%2520identify%2520semantically%2520core%2520visual%2520regions%252C%2520applies%2520per-head%2520Median%2520Absolute%2520Deviation%2520Scaling%2520to%2520adaptively%2520control%2520the%2520intervention%2520strength%252C%2520and%2520leverages%2520System-Token%2520Compensation%2520to%2520maintain%2520attention%2520to%2520complex%2520user%2520instructions%2520and%2520support%2520long-term%2520output%2520consistency.%2520Experiments%2520on%2520multiple%2520LVLMs%2520and%2520benchmarks%2520show%2520that%2520PADE%2520improves%2520visual%2520grounding%2520and%2520reduces%2520hallucinations%252C%2520validating%2520the%2520effectiveness%2520of%2520leveraging%2520internal%2520attention%2520dynamics%2520for%2520reliable%2520multimodal%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20and%20Enhancing%20Core%20Visual%20Regions%3A%20Harnessing%20Internal%20Attention%20Dynamics%20for%20Hallucination%20Mitigation%20in%20LVLMs&entry.906535625=Guangtao%20Lyu%20and%20Qi%20Liu%20and%20Chenghao%20Xu%20and%20Jiexi%20Yan%20and%20Muli%20Yang%20and%20Xueting%20Li%20and%20Fen%20Fang%20and%20Cheng%20Deng&entry.1292438233=LVLMs%20have%20achieved%20strong%20multimodal%20reasoning%20capabilities%20but%20remain%20prone%20to%20hallucinations%2C%20producing%20outputs%20inconsistent%20with%20visual%20inputs%20or%20user%20instructions.%20Existing%20training-free%20methods%2C%20including%20contrastive%20decoding%20and%20auxiliary%20expert%20models%2C%20which%20incur%20several%20times%20more%20computational%20overhead%20and%20may%20introduce%20potential%20interference%2C%20as%20well%20as%20static%20internal%20signal%20enhancement%2C%20are%20often%20vulnerable%20to%20the%20attention%20sink%20phenomenon.%20We%20find%20that%20internal%20Positive%20Attention%20Dynamics%20%28PAD%29%20in%20LVLMs%20naturally%20reveal%20semantically%20core%20visual%20regions%20under%20the%20distortions%20of%20attention%20sinks.%20Based%20on%20this%2C%20we%20propose%20Positive%20Attention%20Dynamics%20Enhancement%20%28PADE%29%2C%20a%20training-free%20attention%20intervention%20that%20constructs%20a%20PAD%20map%20to%20identify%20semantically%20core%20visual%20regions%2C%20applies%20per-head%20Median%20Absolute%20Deviation%20Scaling%20to%20adaptively%20control%20the%20intervention%20strength%2C%20and%20leverages%20System-Token%20Compensation%20to%20maintain%20attention%20to%20complex%20user%20instructions%20and%20support%20long-term%20output%20consistency.%20Experiments%20on%20multiple%20LVLMs%20and%20benchmarks%20show%20that%20PADE%20improves%20visual%20grounding%20and%20reduces%20hallucinations%2C%20validating%20the%20effectiveness%20of%20leveraging%20internal%20attention%20dynamics%20for%20reliable%20multimodal%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2602.15556v1&entry.124074799=Read"},
{"title": "Language and Geometry Grounded Sparse Voxel Representations for Holistic Scene Understanding", "author": "Guile Wu and David Huang and Bingbing Liu and Dongfeng Bai", "abstract": "Existing 3D open-vocabulary scene understanding methods mostly emphasize distilling language features from 2D foundation models into 3D feature fields, but largely overlook the synergy among scene appearance, semantics, and geometry. As a result, scene understanding often deviates from the underlying geometric structure of scenes and becomes decoupled from the reconstruction process. In this work, we propose a novel approach that leverages language and geometry grounded sparse voxel representations to comprehensively model appearance, semantics, and geometry within a unified framework. Specifically, we use 3D sparse voxels as primitives and employ an appearance field, a density field, a feature field, and a confidence field to holistically represent a 3D scene. To promote synergy among the appearance, density, and feature fields, we construct a feature modulation module and distill language features from a 2D foundation model into our 3D scene model. In addition, we integrate geometric distillation into feature field distillation to transfer geometric knowledge from a geometry foundation model to our 3D scene representations via depth correlation regularization and pattern consistency regularization. These components work together to synergistically model the appearance, semantics, and geometry of the 3D scene within a unified framework. Extensive experiments demonstrate that our approach achieves superior overall performance compared with state-of-the-art methods in holistic scene understanding and reconstruction.", "link": "http://arxiv.org/abs/2602.15734v1", "date": "2026-02-17", "relevancy": 2.6769, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6825}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20and%20Geometry%20Grounded%20Sparse%20Voxel%20Representations%20for%20Holistic%20Scene%20Understanding&body=Title%3A%20Language%20and%20Geometry%20Grounded%20Sparse%20Voxel%20Representations%20for%20Holistic%20Scene%20Understanding%0AAuthor%3A%20Guile%20Wu%20and%20David%20Huang%20and%20Bingbing%20Liu%20and%20Dongfeng%20Bai%0AAbstract%3A%20Existing%203D%20open-vocabulary%20scene%20understanding%20methods%20mostly%20emphasize%20distilling%20language%20features%20from%202D%20foundation%20models%20into%203D%20feature%20fields%2C%20but%20largely%20overlook%20the%20synergy%20among%20scene%20appearance%2C%20semantics%2C%20and%20geometry.%20As%20a%20result%2C%20scene%20understanding%20often%20deviates%20from%20the%20underlying%20geometric%20structure%20of%20scenes%20and%20becomes%20decoupled%20from%20the%20reconstruction%20process.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20that%20leverages%20language%20and%20geometry%20grounded%20sparse%20voxel%20representations%20to%20comprehensively%20model%20appearance%2C%20semantics%2C%20and%20geometry%20within%20a%20unified%20framework.%20Specifically%2C%20we%20use%203D%20sparse%20voxels%20as%20primitives%20and%20employ%20an%20appearance%20field%2C%20a%20density%20field%2C%20a%20feature%20field%2C%20and%20a%20confidence%20field%20to%20holistically%20represent%20a%203D%20scene.%20To%20promote%20synergy%20among%20the%20appearance%2C%20density%2C%20and%20feature%20fields%2C%20we%20construct%20a%20feature%20modulation%20module%20and%20distill%20language%20features%20from%20a%202D%20foundation%20model%20into%20our%203D%20scene%20model.%20In%20addition%2C%20we%20integrate%20geometric%20distillation%20into%20feature%20field%20distillation%20to%20transfer%20geometric%20knowledge%20from%20a%20geometry%20foundation%20model%20to%20our%203D%20scene%20representations%20via%20depth%20correlation%20regularization%20and%20pattern%20consistency%20regularization.%20These%20components%20work%20together%20to%20synergistically%20model%20the%20appearance%2C%20semantics%2C%20and%20geometry%20of%20the%203D%20scene%20within%20a%20unified%20framework.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%20superior%20overall%20performance%20compared%20with%20state-of-the-art%20methods%20in%20holistic%20scene%20understanding%20and%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520and%2520Geometry%2520Grounded%2520Sparse%2520Voxel%2520Representations%2520for%2520Holistic%2520Scene%2520Understanding%26entry.906535625%3DGuile%2520Wu%2520and%2520David%2520Huang%2520and%2520Bingbing%2520Liu%2520and%2520Dongfeng%2520Bai%26entry.1292438233%3DExisting%25203D%2520open-vocabulary%2520scene%2520understanding%2520methods%2520mostly%2520emphasize%2520distilling%2520language%2520features%2520from%25202D%2520foundation%2520models%2520into%25203D%2520feature%2520fields%252C%2520but%2520largely%2520overlook%2520the%2520synergy%2520among%2520scene%2520appearance%252C%2520semantics%252C%2520and%2520geometry.%2520As%2520a%2520result%252C%2520scene%2520understanding%2520often%2520deviates%2520from%2520the%2520underlying%2520geometric%2520structure%2520of%2520scenes%2520and%2520becomes%2520decoupled%2520from%2520the%2520reconstruction%2520process.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520leverages%2520language%2520and%2520geometry%2520grounded%2520sparse%2520voxel%2520representations%2520to%2520comprehensively%2520model%2520appearance%252C%2520semantics%252C%2520and%2520geometry%2520within%2520a%2520unified%2520framework.%2520Specifically%252C%2520we%2520use%25203D%2520sparse%2520voxels%2520as%2520primitives%2520and%2520employ%2520an%2520appearance%2520field%252C%2520a%2520density%2520field%252C%2520a%2520feature%2520field%252C%2520and%2520a%2520confidence%2520field%2520to%2520holistically%2520represent%2520a%25203D%2520scene.%2520To%2520promote%2520synergy%2520among%2520the%2520appearance%252C%2520density%252C%2520and%2520feature%2520fields%252C%2520we%2520construct%2520a%2520feature%2520modulation%2520module%2520and%2520distill%2520language%2520features%2520from%2520a%25202D%2520foundation%2520model%2520into%2520our%25203D%2520scene%2520model.%2520In%2520addition%252C%2520we%2520integrate%2520geometric%2520distillation%2520into%2520feature%2520field%2520distillation%2520to%2520transfer%2520geometric%2520knowledge%2520from%2520a%2520geometry%2520foundation%2520model%2520to%2520our%25203D%2520scene%2520representations%2520via%2520depth%2520correlation%2520regularization%2520and%2520pattern%2520consistency%2520regularization.%2520These%2520components%2520work%2520together%2520to%2520synergistically%2520model%2520the%2520appearance%252C%2520semantics%252C%2520and%2520geometry%2520of%2520the%25203D%2520scene%2520within%2520a%2520unified%2520framework.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520superior%2520overall%2520performance%2520compared%2520with%2520state-of-the-art%2520methods%2520in%2520holistic%2520scene%2520understanding%2520and%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20and%20Geometry%20Grounded%20Sparse%20Voxel%20Representations%20for%20Holistic%20Scene%20Understanding&entry.906535625=Guile%20Wu%20and%20David%20Huang%20and%20Bingbing%20Liu%20and%20Dongfeng%20Bai&entry.1292438233=Existing%203D%20open-vocabulary%20scene%20understanding%20methods%20mostly%20emphasize%20distilling%20language%20features%20from%202D%20foundation%20models%20into%203D%20feature%20fields%2C%20but%20largely%20overlook%20the%20synergy%20among%20scene%20appearance%2C%20semantics%2C%20and%20geometry.%20As%20a%20result%2C%20scene%20understanding%20often%20deviates%20from%20the%20underlying%20geometric%20structure%20of%20scenes%20and%20becomes%20decoupled%20from%20the%20reconstruction%20process.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20that%20leverages%20language%20and%20geometry%20grounded%20sparse%20voxel%20representations%20to%20comprehensively%20model%20appearance%2C%20semantics%2C%20and%20geometry%20within%20a%20unified%20framework.%20Specifically%2C%20we%20use%203D%20sparse%20voxels%20as%20primitives%20and%20employ%20an%20appearance%20field%2C%20a%20density%20field%2C%20a%20feature%20field%2C%20and%20a%20confidence%20field%20to%20holistically%20represent%20a%203D%20scene.%20To%20promote%20synergy%20among%20the%20appearance%2C%20density%2C%20and%20feature%20fields%2C%20we%20construct%20a%20feature%20modulation%20module%20and%20distill%20language%20features%20from%20a%202D%20foundation%20model%20into%20our%203D%20scene%20model.%20In%20addition%2C%20we%20integrate%20geometric%20distillation%20into%20feature%20field%20distillation%20to%20transfer%20geometric%20knowledge%20from%20a%20geometry%20foundation%20model%20to%20our%203D%20scene%20representations%20via%20depth%20correlation%20regularization%20and%20pattern%20consistency%20regularization.%20These%20components%20work%20together%20to%20synergistically%20model%20the%20appearance%2C%20semantics%2C%20and%20geometry%20of%20the%203D%20scene%20within%20a%20unified%20framework.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%20superior%20overall%20performance%20compared%20with%20state-of-the-art%20methods%20in%20holistic%20scene%20understanding%20and%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2602.15734v1&entry.124074799=Read"},
{"title": "LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases", "author": "Khang Nguyen Quoc and Phuong D. Dao and Luyl-Da Quach", "abstract": "Foundation models and vision-language pre-training have significantly advanced Vision-Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their application in domain-specific agricultural tasks, such as plant pathology, remains limited due to the lack of large-scale, comprehensive multimodal image--text datasets and benchmarks. To address this gap, we introduce LeafNet, a comprehensive multimodal dataset, and LeafBench, a visual question-answering benchmark developed to systematically evaluate the capabilities of VLMs in understanding plant diseases. The dataset comprises 186,000 leaf digital images spanning 97 disease classes, paired with metadata, generating 13,950 question-answer pairs spanning six critical agricultural tasks. The questions assess various aspects of plant pathology understanding, including visual symptom recognition, taxonomic relationships, and diagnostic reasoning. Benchmarking 12 state-of-the-art VLMs on our LeafBench dataset, we reveal substantial disparity in their disease understanding capabilities. Our study shows performance varies markedly across tasks: binary healthy--diseased classification exceeds 90\\% accuracy, while fine-grained pathogen and species identification remains below 65\\%. Direct comparison between vision-only models and VLMs demonstrates the critical advantage of multimodal architectures: fine-tuned VLMs outperform traditional vision models, confirming that integrating linguistic representations significantly enhances diagnostic precision. These findings highlight critical gaps in current VLMs for plant pathology applications and underscore the need for LeafBench as a rigorous framework for methodological advancement and progress evaluation toward reliable AI-assisted plant disease diagnosis. Code is available at https://github.com/EnalisUs/LeafBench.", "link": "http://arxiv.org/abs/2602.13662v2", "date": "2026-02-17", "relevancy": 2.6569, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5579}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeafNet%3A%20A%20Large-Scale%20Dataset%20and%20Comprehensive%20Benchmark%20for%20Foundational%20Vision-Language%20Understanding%20of%20Plant%20Diseases&body=Title%3A%20LeafNet%3A%20A%20Large-Scale%20Dataset%20and%20Comprehensive%20Benchmark%20for%20Foundational%20Vision-Language%20Understanding%20of%20Plant%20Diseases%0AAuthor%3A%20Khang%20Nguyen%20Quoc%20and%20Phuong%20D.%20Dao%20and%20Luyl-Da%20Quach%0AAbstract%3A%20Foundation%20models%20and%20vision-language%20pre-training%20have%20significantly%20advanced%20Vision-Language%20Models%20%28VLMs%29%2C%20enabling%20multimodal%20processing%20of%20visual%20and%20linguistic%20data.%20However%2C%20their%20application%20in%20domain-specific%20agricultural%20tasks%2C%20such%20as%20plant%20pathology%2C%20remains%20limited%20due%20to%20the%20lack%20of%20large-scale%2C%20comprehensive%20multimodal%20image--text%20datasets%20and%20benchmarks.%20To%20address%20this%20gap%2C%20we%20introduce%20LeafNet%2C%20a%20comprehensive%20multimodal%20dataset%2C%20and%20LeafBench%2C%20a%20visual%20question-answering%20benchmark%20developed%20to%20systematically%20evaluate%20the%20capabilities%20of%20VLMs%20in%20understanding%20plant%20diseases.%20The%20dataset%20comprises%20186%2C000%20leaf%20digital%20images%20spanning%2097%20disease%20classes%2C%20paired%20with%20metadata%2C%20generating%2013%2C950%20question-answer%20pairs%20spanning%20six%20critical%20agricultural%20tasks.%20The%20questions%20assess%20various%20aspects%20of%20plant%20pathology%20understanding%2C%20including%20visual%20symptom%20recognition%2C%20taxonomic%20relationships%2C%20and%20diagnostic%20reasoning.%20Benchmarking%2012%20state-of-the-art%20VLMs%20on%20our%20LeafBench%20dataset%2C%20we%20reveal%20substantial%20disparity%20in%20their%20disease%20understanding%20capabilities.%20Our%20study%20shows%20performance%20varies%20markedly%20across%20tasks%3A%20binary%20healthy--diseased%20classification%20exceeds%2090%5C%25%20accuracy%2C%20while%20fine-grained%20pathogen%20and%20species%20identification%20remains%20below%2065%5C%25.%20Direct%20comparison%20between%20vision-only%20models%20and%20VLMs%20demonstrates%20the%20critical%20advantage%20of%20multimodal%20architectures%3A%20fine-tuned%20VLMs%20outperform%20traditional%20vision%20models%2C%20confirming%20that%20integrating%20linguistic%20representations%20significantly%20enhances%20diagnostic%20precision.%20These%20findings%20highlight%20critical%20gaps%20in%20current%20VLMs%20for%20plant%20pathology%20applications%20and%20underscore%20the%20need%20for%20LeafBench%20as%20a%20rigorous%20framework%20for%20methodological%20advancement%20and%20progress%20evaluation%20toward%20reliable%20AI-assisted%20plant%20disease%20diagnosis.%20Code%20is%20available%20at%20https%3A//github.com/EnalisUs/LeafBench.%0ALink%3A%20http%3A//arxiv.org/abs/2602.13662v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeafNet%253A%2520A%2520Large-Scale%2520Dataset%2520and%2520Comprehensive%2520Benchmark%2520for%2520Foundational%2520Vision-Language%2520Understanding%2520of%2520Plant%2520Diseases%26entry.906535625%3DKhang%2520Nguyen%2520Quoc%2520and%2520Phuong%2520D.%2520Dao%2520and%2520Luyl-Da%2520Quach%26entry.1292438233%3DFoundation%2520models%2520and%2520vision-language%2520pre-training%2520have%2520significantly%2520advanced%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520enabling%2520multimodal%2520processing%2520of%2520visual%2520and%2520linguistic%2520data.%2520However%252C%2520their%2520application%2520in%2520domain-specific%2520agricultural%2520tasks%252C%2520such%2520as%2520plant%2520pathology%252C%2520remains%2520limited%2520due%2520to%2520the%2520lack%2520of%2520large-scale%252C%2520comprehensive%2520multimodal%2520image--text%2520datasets%2520and%2520benchmarks.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520LeafNet%252C%2520a%2520comprehensive%2520multimodal%2520dataset%252C%2520and%2520LeafBench%252C%2520a%2520visual%2520question-answering%2520benchmark%2520developed%2520to%2520systematically%2520evaluate%2520the%2520capabilities%2520of%2520VLMs%2520in%2520understanding%2520plant%2520diseases.%2520The%2520dataset%2520comprises%2520186%252C000%2520leaf%2520digital%2520images%2520spanning%252097%2520disease%2520classes%252C%2520paired%2520with%2520metadata%252C%2520generating%252013%252C950%2520question-answer%2520pairs%2520spanning%2520six%2520critical%2520agricultural%2520tasks.%2520The%2520questions%2520assess%2520various%2520aspects%2520of%2520plant%2520pathology%2520understanding%252C%2520including%2520visual%2520symptom%2520recognition%252C%2520taxonomic%2520relationships%252C%2520and%2520diagnostic%2520reasoning.%2520Benchmarking%252012%2520state-of-the-art%2520VLMs%2520on%2520our%2520LeafBench%2520dataset%252C%2520we%2520reveal%2520substantial%2520disparity%2520in%2520their%2520disease%2520understanding%2520capabilities.%2520Our%2520study%2520shows%2520performance%2520varies%2520markedly%2520across%2520tasks%253A%2520binary%2520healthy--diseased%2520classification%2520exceeds%252090%255C%2525%2520accuracy%252C%2520while%2520fine-grained%2520pathogen%2520and%2520species%2520identification%2520remains%2520below%252065%255C%2525.%2520Direct%2520comparison%2520between%2520vision-only%2520models%2520and%2520VLMs%2520demonstrates%2520the%2520critical%2520advantage%2520of%2520multimodal%2520architectures%253A%2520fine-tuned%2520VLMs%2520outperform%2520traditional%2520vision%2520models%252C%2520confirming%2520that%2520integrating%2520linguistic%2520representations%2520significantly%2520enhances%2520diagnostic%2520precision.%2520These%2520findings%2520highlight%2520critical%2520gaps%2520in%2520current%2520VLMs%2520for%2520plant%2520pathology%2520applications%2520and%2520underscore%2520the%2520need%2520for%2520LeafBench%2520as%2520a%2520rigorous%2520framework%2520for%2520methodological%2520advancement%2520and%2520progress%2520evaluation%2520toward%2520reliable%2520AI-assisted%2520plant%2520disease%2520diagnosis.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/EnalisUs/LeafBench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.13662v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeafNet%3A%20A%20Large-Scale%20Dataset%20and%20Comprehensive%20Benchmark%20for%20Foundational%20Vision-Language%20Understanding%20of%20Plant%20Diseases&entry.906535625=Khang%20Nguyen%20Quoc%20and%20Phuong%20D.%20Dao%20and%20Luyl-Da%20Quach&entry.1292438233=Foundation%20models%20and%20vision-language%20pre-training%20have%20significantly%20advanced%20Vision-Language%20Models%20%28VLMs%29%2C%20enabling%20multimodal%20processing%20of%20visual%20and%20linguistic%20data.%20However%2C%20their%20application%20in%20domain-specific%20agricultural%20tasks%2C%20such%20as%20plant%20pathology%2C%20remains%20limited%20due%20to%20the%20lack%20of%20large-scale%2C%20comprehensive%20multimodal%20image--text%20datasets%20and%20benchmarks.%20To%20address%20this%20gap%2C%20we%20introduce%20LeafNet%2C%20a%20comprehensive%20multimodal%20dataset%2C%20and%20LeafBench%2C%20a%20visual%20question-answering%20benchmark%20developed%20to%20systematically%20evaluate%20the%20capabilities%20of%20VLMs%20in%20understanding%20plant%20diseases.%20The%20dataset%20comprises%20186%2C000%20leaf%20digital%20images%20spanning%2097%20disease%20classes%2C%20paired%20with%20metadata%2C%20generating%2013%2C950%20question-answer%20pairs%20spanning%20six%20critical%20agricultural%20tasks.%20The%20questions%20assess%20various%20aspects%20of%20plant%20pathology%20understanding%2C%20including%20visual%20symptom%20recognition%2C%20taxonomic%20relationships%2C%20and%20diagnostic%20reasoning.%20Benchmarking%2012%20state-of-the-art%20VLMs%20on%20our%20LeafBench%20dataset%2C%20we%20reveal%20substantial%20disparity%20in%20their%20disease%20understanding%20capabilities.%20Our%20study%20shows%20performance%20varies%20markedly%20across%20tasks%3A%20binary%20healthy--diseased%20classification%20exceeds%2090%5C%25%20accuracy%2C%20while%20fine-grained%20pathogen%20and%20species%20identification%20remains%20below%2065%5C%25.%20Direct%20comparison%20between%20vision-only%20models%20and%20VLMs%20demonstrates%20the%20critical%20advantage%20of%20multimodal%20architectures%3A%20fine-tuned%20VLMs%20outperform%20traditional%20vision%20models%2C%20confirming%20that%20integrating%20linguistic%20representations%20significantly%20enhances%20diagnostic%20precision.%20These%20findings%20highlight%20critical%20gaps%20in%20current%20VLMs%20for%20plant%20pathology%20applications%20and%20underscore%20the%20need%20for%20LeafBench%20as%20a%20rigorous%20framework%20for%20methodological%20advancement%20and%20progress%20evaluation%20toward%20reliable%20AI-assisted%20plant%20disease%20diagnosis.%20Code%20is%20available%20at%20https%3A//github.com/EnalisUs/LeafBench.&entry.1838667208=http%3A//arxiv.org/abs/2602.13662v2&entry.124074799=Read"},
{"title": "UrbanVerse: Learning Urban Region Representation Across Cities and Tasks", "author": "Fengze Sun and Egemen Tanin and Shanika Karunasekera and Zuqing Li and Flora D. Salim and Jianzhong Qi", "abstract": "Recent advances in urban region representation learning have enabled a wide range of applications in urban analytics, yet existing methods remain limited in their capabilities to generalize across cities and analytic tasks. We aim to generalize urban representation learning beyond city- and task-specific settings, towards a foundation-style model for urban analytics. To this end, we propose UrbanVerse, a model for cross-city urban representation learning and cross-task urban analytics. For cross-city generalization, UrbanVerse focuses on features local to the target regions and structural features of the nearby regions rather than the entire city. We model regions as nodes on a graph, which enables a random walk-based procedure to form \"sequences of regions\" that reflect both local and neighborhood structural features for urban region representation learning. For cross-task generalization, we propose a cross-task learning module named HCondDiffCT. This module integrates region-conditioned prior knowledge and task-conditioned semantics into the diffusion process to jointly model multiple downstream urban prediction tasks. HCondDiffCT is generic. It can also be integrated with existing urban representation learning models to enhance their downstream task effectiveness. Experiments on real-world datasets show that UrbanVerse consistently outperforms state-of-the-art methods across six tasks under cross-city settings, achieving up to 35.89% improvements in prediction accuracy.", "link": "http://arxiv.org/abs/2602.15750v1", "date": "2026-02-17", "relevancy": 2.6516, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanVerse%3A%20Learning%20Urban%20Region%20Representation%20Across%20Cities%20and%20Tasks&body=Title%3A%20UrbanVerse%3A%20Learning%20Urban%20Region%20Representation%20Across%20Cities%20and%20Tasks%0AAuthor%3A%20Fengze%20Sun%20and%20Egemen%20Tanin%20and%20Shanika%20Karunasekera%20and%20Zuqing%20Li%20and%20Flora%20D.%20Salim%20and%20Jianzhong%20Qi%0AAbstract%3A%20Recent%20advances%20in%20urban%20region%20representation%20learning%20have%20enabled%20a%20wide%20range%20of%20applications%20in%20urban%20analytics%2C%20yet%20existing%20methods%20remain%20limited%20in%20their%20capabilities%20to%20generalize%20across%20cities%20and%20analytic%20tasks.%20We%20aim%20to%20generalize%20urban%20representation%20learning%20beyond%20city-%20and%20task-specific%20settings%2C%20towards%20a%20foundation-style%20model%20for%20urban%20analytics.%20To%20this%20end%2C%20we%20propose%20UrbanVerse%2C%20a%20model%20for%20cross-city%20urban%20representation%20learning%20and%20cross-task%20urban%20analytics.%20For%20cross-city%20generalization%2C%20UrbanVerse%20focuses%20on%20features%20local%20to%20the%20target%20regions%20and%20structural%20features%20of%20the%20nearby%20regions%20rather%20than%20the%20entire%20city.%20We%20model%20regions%20as%20nodes%20on%20a%20graph%2C%20which%20enables%20a%20random%20walk-based%20procedure%20to%20form%20%22sequences%20of%20regions%22%20that%20reflect%20both%20local%20and%20neighborhood%20structural%20features%20for%20urban%20region%20representation%20learning.%20For%20cross-task%20generalization%2C%20we%20propose%20a%20cross-task%20learning%20module%20named%20HCondDiffCT.%20This%20module%20integrates%20region-conditioned%20prior%20knowledge%20and%20task-conditioned%20semantics%20into%20the%20diffusion%20process%20to%20jointly%20model%20multiple%20downstream%20urban%20prediction%20tasks.%20HCondDiffCT%20is%20generic.%20It%20can%20also%20be%20integrated%20with%20existing%20urban%20representation%20learning%20models%20to%20enhance%20their%20downstream%20task%20effectiveness.%20Experiments%20on%20real-world%20datasets%20show%20that%20UrbanVerse%20consistently%20outperforms%20state-of-the-art%20methods%20across%20six%20tasks%20under%20cross-city%20settings%2C%20achieving%20up%20to%2035.89%25%20improvements%20in%20prediction%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanVerse%253A%2520Learning%2520Urban%2520Region%2520Representation%2520Across%2520Cities%2520and%2520Tasks%26entry.906535625%3DFengze%2520Sun%2520and%2520Egemen%2520Tanin%2520and%2520Shanika%2520Karunasekera%2520and%2520Zuqing%2520Li%2520and%2520Flora%2520D.%2520Salim%2520and%2520Jianzhong%2520Qi%26entry.1292438233%3DRecent%2520advances%2520in%2520urban%2520region%2520representation%2520learning%2520have%2520enabled%2520a%2520wide%2520range%2520of%2520applications%2520in%2520urban%2520analytics%252C%2520yet%2520existing%2520methods%2520remain%2520limited%2520in%2520their%2520capabilities%2520to%2520generalize%2520across%2520cities%2520and%2520analytic%2520tasks.%2520We%2520aim%2520to%2520generalize%2520urban%2520representation%2520learning%2520beyond%2520city-%2520and%2520task-specific%2520settings%252C%2520towards%2520a%2520foundation-style%2520model%2520for%2520urban%2520analytics.%2520To%2520this%2520end%252C%2520we%2520propose%2520UrbanVerse%252C%2520a%2520model%2520for%2520cross-city%2520urban%2520representation%2520learning%2520and%2520cross-task%2520urban%2520analytics.%2520For%2520cross-city%2520generalization%252C%2520UrbanVerse%2520focuses%2520on%2520features%2520local%2520to%2520the%2520target%2520regions%2520and%2520structural%2520features%2520of%2520the%2520nearby%2520regions%2520rather%2520than%2520the%2520entire%2520city.%2520We%2520model%2520regions%2520as%2520nodes%2520on%2520a%2520graph%252C%2520which%2520enables%2520a%2520random%2520walk-based%2520procedure%2520to%2520form%2520%2522sequences%2520of%2520regions%2522%2520that%2520reflect%2520both%2520local%2520and%2520neighborhood%2520structural%2520features%2520for%2520urban%2520region%2520representation%2520learning.%2520For%2520cross-task%2520generalization%252C%2520we%2520propose%2520a%2520cross-task%2520learning%2520module%2520named%2520HCondDiffCT.%2520This%2520module%2520integrates%2520region-conditioned%2520prior%2520knowledge%2520and%2520task-conditioned%2520semantics%2520into%2520the%2520diffusion%2520process%2520to%2520jointly%2520model%2520multiple%2520downstream%2520urban%2520prediction%2520tasks.%2520HCondDiffCT%2520is%2520generic.%2520It%2520can%2520also%2520be%2520integrated%2520with%2520existing%2520urban%2520representation%2520learning%2520models%2520to%2520enhance%2520their%2520downstream%2520task%2520effectiveness.%2520Experiments%2520on%2520real-world%2520datasets%2520show%2520that%2520UrbanVerse%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520across%2520six%2520tasks%2520under%2520cross-city%2520settings%252C%2520achieving%2520up%2520to%252035.89%2525%2520improvements%2520in%2520prediction%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanVerse%3A%20Learning%20Urban%20Region%20Representation%20Across%20Cities%20and%20Tasks&entry.906535625=Fengze%20Sun%20and%20Egemen%20Tanin%20and%20Shanika%20Karunasekera%20and%20Zuqing%20Li%20and%20Flora%20D.%20Salim%20and%20Jianzhong%20Qi&entry.1292438233=Recent%20advances%20in%20urban%20region%20representation%20learning%20have%20enabled%20a%20wide%20range%20of%20applications%20in%20urban%20analytics%2C%20yet%20existing%20methods%20remain%20limited%20in%20their%20capabilities%20to%20generalize%20across%20cities%20and%20analytic%20tasks.%20We%20aim%20to%20generalize%20urban%20representation%20learning%20beyond%20city-%20and%20task-specific%20settings%2C%20towards%20a%20foundation-style%20model%20for%20urban%20analytics.%20To%20this%20end%2C%20we%20propose%20UrbanVerse%2C%20a%20model%20for%20cross-city%20urban%20representation%20learning%20and%20cross-task%20urban%20analytics.%20For%20cross-city%20generalization%2C%20UrbanVerse%20focuses%20on%20features%20local%20to%20the%20target%20regions%20and%20structural%20features%20of%20the%20nearby%20regions%20rather%20than%20the%20entire%20city.%20We%20model%20regions%20as%20nodes%20on%20a%20graph%2C%20which%20enables%20a%20random%20walk-based%20procedure%20to%20form%20%22sequences%20of%20regions%22%20that%20reflect%20both%20local%20and%20neighborhood%20structural%20features%20for%20urban%20region%20representation%20learning.%20For%20cross-task%20generalization%2C%20we%20propose%20a%20cross-task%20learning%20module%20named%20HCondDiffCT.%20This%20module%20integrates%20region-conditioned%20prior%20knowledge%20and%20task-conditioned%20semantics%20into%20the%20diffusion%20process%20to%20jointly%20model%20multiple%20downstream%20urban%20prediction%20tasks.%20HCondDiffCT%20is%20generic.%20It%20can%20also%20be%20integrated%20with%20existing%20urban%20representation%20learning%20models%20to%20enhance%20their%20downstream%20task%20effectiveness.%20Experiments%20on%20real-world%20datasets%20show%20that%20UrbanVerse%20consistently%20outperforms%20state-of-the-art%20methods%20across%20six%20tasks%20under%20cross-city%20settings%2C%20achieving%20up%20to%2035.89%25%20improvements%20in%20prediction%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2602.15750v1&entry.124074799=Read"},
{"title": "ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System", "author": "Anantha Sharma", "abstract": "Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.\n  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.\n  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.", "link": "http://arxiv.org/abs/2601.01297v2", "date": "2026-02-17", "relevancy": 2.6354, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5507}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5394}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARGUS%3A%20Adaptive%20Rotation-Invariant%20Geometric%20Unsupervised%20System&body=Title%3A%20ARGUS%3A%20Adaptive%20Rotation-Invariant%20Geometric%20Unsupervised%20System%0AAuthor%3A%20Anantha%20Sharma%0AAbstract%3A%20Detecting%20distributional%20drift%20in%20high-dimensional%20data%20streams%20presents%20fundamental%20challenges%3A%20global%20comparison%20methods%20scale%20poorly%2C%20projection-based%20approaches%20lose%20geometric%20structure%2C%20and%20re-clustering%20methods%20suffer%20from%20identity%20instability.%20This%20paper%20introduces%20Argus%2C%20A%20framework%20that%20reconceptualizes%20drift%20detection%20as%20tracking%20local%20statistics%20over%20a%20fixed%20spatial%20partition%20of%20the%20data%20manifold.%0A%20%20The%20key%20contributions%20are%20fourfold.%20First%2C%20it%20is%20proved%20that%20Voronoi%20tessellations%20over%20canonical%20orthonormal%20frames%20yield%20drift%20metrics%20that%20are%20invariant%20to%20orthogonal%20transformations.%20The%20rotations%20and%20reflections%20that%20preserve%20Euclidean%20geometry.%20Second%2C%20it%20is%20established%20that%20this%20framework%20achieves%20O%28N%29%20complexity%20per%20snapshot%20while%20providing%20cell-level%20spatial%20localization%20of%20distributional%20change.%20Third%2C%20a%20graph-theoretic%20characterization%20of%20drift%20propagation%20is%20developed%20that%20distinguishes%20coherent%20distributional%20shifts%20from%20isolated%20perturbations.%20Fourth%2C%20product%20quantization%20tessellation%20is%20introduced%20for%20scaling%20to%20very%20high%20dimensions%20%28d%3E500%29%20by%20decomposing%20the%20space%20into%20independent%20subspaces%20and%20aggregating%20drift%20signals%20across%20subspaces.%0A%20%20This%20paper%20formalizes%20the%20theoretical%20foundations%2C%20proves%20invariance%20properties%2C%20and%20presents%20experimental%20validation%20demonstrating%20that%20the%20framework%20correctly%20identifies%20drift%20under%20coordinate%20rotation%20while%20existing%20methods%20produce%20false%20positives.%20The%20tessellated%20approach%20offers%20a%20principled%20geometric%20foundation%20for%20distribution%20monitoring%20that%20preserves%20high-dimensional%20structure%20without%20the%20computational%20burden%20of%20pairwise%20comparisons.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01297v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARGUS%253A%2520Adaptive%2520Rotation-Invariant%2520Geometric%2520Unsupervised%2520System%26entry.906535625%3DAnantha%2520Sharma%26entry.1292438233%3DDetecting%2520distributional%2520drift%2520in%2520high-dimensional%2520data%2520streams%2520presents%2520fundamental%2520challenges%253A%2520global%2520comparison%2520methods%2520scale%2520poorly%252C%2520projection-based%2520approaches%2520lose%2520geometric%2520structure%252C%2520and%2520re-clustering%2520methods%2520suffer%2520from%2520identity%2520instability.%2520This%2520paper%2520introduces%2520Argus%252C%2520A%2520framework%2520that%2520reconceptualizes%2520drift%2520detection%2520as%2520tracking%2520local%2520statistics%2520over%2520a%2520fixed%2520spatial%2520partition%2520of%2520the%2520data%2520manifold.%250A%2520%2520The%2520key%2520contributions%2520are%2520fourfold.%2520First%252C%2520it%2520is%2520proved%2520that%2520Voronoi%2520tessellations%2520over%2520canonical%2520orthonormal%2520frames%2520yield%2520drift%2520metrics%2520that%2520are%2520invariant%2520to%2520orthogonal%2520transformations.%2520The%2520rotations%2520and%2520reflections%2520that%2520preserve%2520Euclidean%2520geometry.%2520Second%252C%2520it%2520is%2520established%2520that%2520this%2520framework%2520achieves%2520O%2528N%2529%2520complexity%2520per%2520snapshot%2520while%2520providing%2520cell-level%2520spatial%2520localization%2520of%2520distributional%2520change.%2520Third%252C%2520a%2520graph-theoretic%2520characterization%2520of%2520drift%2520propagation%2520is%2520developed%2520that%2520distinguishes%2520coherent%2520distributional%2520shifts%2520from%2520isolated%2520perturbations.%2520Fourth%252C%2520product%2520quantization%2520tessellation%2520is%2520introduced%2520for%2520scaling%2520to%2520very%2520high%2520dimensions%2520%2528d%253E500%2529%2520by%2520decomposing%2520the%2520space%2520into%2520independent%2520subspaces%2520and%2520aggregating%2520drift%2520signals%2520across%2520subspaces.%250A%2520%2520This%2520paper%2520formalizes%2520the%2520theoretical%2520foundations%252C%2520proves%2520invariance%2520properties%252C%2520and%2520presents%2520experimental%2520validation%2520demonstrating%2520that%2520the%2520framework%2520correctly%2520identifies%2520drift%2520under%2520coordinate%2520rotation%2520while%2520existing%2520methods%2520produce%2520false%2520positives.%2520The%2520tessellated%2520approach%2520offers%2520a%2520principled%2520geometric%2520foundation%2520for%2520distribution%2520monitoring%2520that%2520preserves%2520high-dimensional%2520structure%2520without%2520the%2520computational%2520burden%2520of%2520pairwise%2520comparisons.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01297v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARGUS%3A%20Adaptive%20Rotation-Invariant%20Geometric%20Unsupervised%20System&entry.906535625=Anantha%20Sharma&entry.1292438233=Detecting%20distributional%20drift%20in%20high-dimensional%20data%20streams%20presents%20fundamental%20challenges%3A%20global%20comparison%20methods%20scale%20poorly%2C%20projection-based%20approaches%20lose%20geometric%20structure%2C%20and%20re-clustering%20methods%20suffer%20from%20identity%20instability.%20This%20paper%20introduces%20Argus%2C%20A%20framework%20that%20reconceptualizes%20drift%20detection%20as%20tracking%20local%20statistics%20over%20a%20fixed%20spatial%20partition%20of%20the%20data%20manifold.%0A%20%20The%20key%20contributions%20are%20fourfold.%20First%2C%20it%20is%20proved%20that%20Voronoi%20tessellations%20over%20canonical%20orthonormal%20frames%20yield%20drift%20metrics%20that%20are%20invariant%20to%20orthogonal%20transformations.%20The%20rotations%20and%20reflections%20that%20preserve%20Euclidean%20geometry.%20Second%2C%20it%20is%20established%20that%20this%20framework%20achieves%20O%28N%29%20complexity%20per%20snapshot%20while%20providing%20cell-level%20spatial%20localization%20of%20distributional%20change.%20Third%2C%20a%20graph-theoretic%20characterization%20of%20drift%20propagation%20is%20developed%20that%20distinguishes%20coherent%20distributional%20shifts%20from%20isolated%20perturbations.%20Fourth%2C%20product%20quantization%20tessellation%20is%20introduced%20for%20scaling%20to%20very%20high%20dimensions%20%28d%3E500%29%20by%20decomposing%20the%20space%20into%20independent%20subspaces%20and%20aggregating%20drift%20signals%20across%20subspaces.%0A%20%20This%20paper%20formalizes%20the%20theoretical%20foundations%2C%20proves%20invariance%20properties%2C%20and%20presents%20experimental%20validation%20demonstrating%20that%20the%20framework%20correctly%20identifies%20drift%20under%20coordinate%20rotation%20while%20existing%20methods%20produce%20false%20positives.%20The%20tessellated%20approach%20offers%20a%20principled%20geometric%20foundation%20for%20distribution%20monitoring%20that%20preserves%20high-dimensional%20structure%20without%20the%20computational%20burden%20of%20pairwise%20comparisons.&entry.1838667208=http%3A//arxiv.org/abs/2601.01297v2&entry.124074799=Read"},
{"title": "MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction", "author": "Qiang Zhang and Jiahao Ma and Peiran Liu and Shuai Shi and Zeran Su and Zifan Wang and Jingkai Sun and Wei Cui and Jialin Yu and Gang Han and Wen Zhao and Pihai Sun and Kangning Yin and Jiaxu Wang and Jiahang Cao and Lingfeng Zhang and Hao Cheng and Xiaoshuai Hao and Yiding Ji and Junwei Liang and Jian Tang and Renjing Xu and Yijie Guo", "abstract": "Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled \"motion-terrain\" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.", "link": "http://arxiv.org/abs/2602.15733v1", "date": "2026-02-17", "relevancy": 2.6079, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6998}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6283}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshMimic%3A%20Geometry-Aware%20Humanoid%20Motion%20Learning%20through%203D%20Scene%20Reconstruction&body=Title%3A%20MeshMimic%3A%20Geometry-Aware%20Humanoid%20Motion%20Learning%20through%203D%20Scene%20Reconstruction%0AAuthor%3A%20Qiang%20Zhang%20and%20Jiahao%20Ma%20and%20Peiran%20Liu%20and%20Shuai%20Shi%20and%20Zeran%20Su%20and%20Zifan%20Wang%20and%20Jingkai%20Sun%20and%20Wei%20Cui%20and%20Jialin%20Yu%20and%20Gang%20Han%20and%20Wen%20Zhao%20and%20Pihai%20Sun%20and%20Kangning%20Yin%20and%20Jiaxu%20Wang%20and%20Jiahang%20Cao%20and%20Lingfeng%20Zhang%20and%20Hao%20Cheng%20and%20Xiaoshuai%20Hao%20and%20Yiding%20Ji%20and%20Junwei%20Liang%20and%20Jian%20Tang%20and%20Renjing%20Xu%20and%20Yijie%20Guo%0AAbstract%3A%20Humanoid%20motion%20control%20has%20witnessed%20significant%20breakthroughs%20in%20recent%20years%2C%20with%20deep%20reinforcement%20learning%20%28RL%29%20emerging%20as%20a%20primary%20catalyst%20for%20achieving%20complex%2C%20human-like%20behaviors.%20However%2C%20the%20high%20dimensionality%20and%20intricate%20dynamics%20of%20humanoid%20robots%20make%20manual%20motion%20design%20impractical%2C%20leading%20to%20a%20heavy%20reliance%20on%20expensive%20motion%20capture%20%28MoCap%29%20data.%20These%20datasets%20are%20not%20only%20costly%20to%20acquire%20but%20also%20frequently%20lack%20the%20necessary%20geometric%20context%20of%20the%20surrounding%20physical%20environment.%20Consequently%2C%20existing%20motion%20synthesis%20frameworks%20often%20suffer%20from%20a%20decoupling%20of%20motion%20and%20scene%2C%20resulting%20in%20physical%20inconsistencies%20such%20as%20contact%20slippage%20or%20mesh%20penetration%20during%20terrain-aware%20tasks.%20In%20this%20work%2C%20we%20present%20MeshMimic%2C%20an%20innovative%20framework%20that%20bridges%203D%20scene%20reconstruction%20and%20embodied%20intelligence%20to%20enable%20humanoid%20robots%20to%20learn%20coupled%20%22motion-terrain%22%20interactions%20directly%20from%20video.%20By%20leveraging%20state-of-the-art%203D%20vision%20models%2C%20our%20framework%20precisely%20segments%20and%20reconstructs%20both%20human%20trajectories%20and%20the%20underlying%203D%20geometry%20of%20terrains%20and%20objects.%20We%20introduce%20an%20optimization%20algorithm%20based%20on%20kinematic%20consistency%20to%20extract%20high-quality%20motion%20data%20from%20noisy%20visual%20reconstructions%2C%20alongside%20a%20contact-invariant%20retargeting%20method%20that%20transfers%20human-environment%20interaction%20features%20to%20the%20humanoid%20agent.%20Experimental%20results%20demonstrate%20that%20MeshMimic%20achieves%20robust%2C%20highly%20dynamic%20performance%20across%20diverse%20and%20challenging%20terrains.%20Our%20approach%20proves%20that%20a%20low-cost%20pipeline%20utilizing%20only%20consumer-grade%20monocular%20sensors%20can%20facilitate%20the%20training%20of%20complex%20physical%20interactions%2C%20offering%20a%20scalable%20path%20toward%20the%20autonomous%20evolution%20of%20humanoid%20robots%20in%20unstructured%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshMimic%253A%2520Geometry-Aware%2520Humanoid%2520Motion%2520Learning%2520through%25203D%2520Scene%2520Reconstruction%26entry.906535625%3DQiang%2520Zhang%2520and%2520Jiahao%2520Ma%2520and%2520Peiran%2520Liu%2520and%2520Shuai%2520Shi%2520and%2520Zeran%2520Su%2520and%2520Zifan%2520Wang%2520and%2520Jingkai%2520Sun%2520and%2520Wei%2520Cui%2520and%2520Jialin%2520Yu%2520and%2520Gang%2520Han%2520and%2520Wen%2520Zhao%2520and%2520Pihai%2520Sun%2520and%2520Kangning%2520Yin%2520and%2520Jiaxu%2520Wang%2520and%2520Jiahang%2520Cao%2520and%2520Lingfeng%2520Zhang%2520and%2520Hao%2520Cheng%2520and%2520Xiaoshuai%2520Hao%2520and%2520Yiding%2520Ji%2520and%2520Junwei%2520Liang%2520and%2520Jian%2520Tang%2520and%2520Renjing%2520Xu%2520and%2520Yijie%2520Guo%26entry.1292438233%3DHumanoid%2520motion%2520control%2520has%2520witnessed%2520significant%2520breakthroughs%2520in%2520recent%2520years%252C%2520with%2520deep%2520reinforcement%2520learning%2520%2528RL%2529%2520emerging%2520as%2520a%2520primary%2520catalyst%2520for%2520achieving%2520complex%252C%2520human-like%2520behaviors.%2520However%252C%2520the%2520high%2520dimensionality%2520and%2520intricate%2520dynamics%2520of%2520humanoid%2520robots%2520make%2520manual%2520motion%2520design%2520impractical%252C%2520leading%2520to%2520a%2520heavy%2520reliance%2520on%2520expensive%2520motion%2520capture%2520%2528MoCap%2529%2520data.%2520These%2520datasets%2520are%2520not%2520only%2520costly%2520to%2520acquire%2520but%2520also%2520frequently%2520lack%2520the%2520necessary%2520geometric%2520context%2520of%2520the%2520surrounding%2520physical%2520environment.%2520Consequently%252C%2520existing%2520motion%2520synthesis%2520frameworks%2520often%2520suffer%2520from%2520a%2520decoupling%2520of%2520motion%2520and%2520scene%252C%2520resulting%2520in%2520physical%2520inconsistencies%2520such%2520as%2520contact%2520slippage%2520or%2520mesh%2520penetration%2520during%2520terrain-aware%2520tasks.%2520In%2520this%2520work%252C%2520we%2520present%2520MeshMimic%252C%2520an%2520innovative%2520framework%2520that%2520bridges%25203D%2520scene%2520reconstruction%2520and%2520embodied%2520intelligence%2520to%2520enable%2520humanoid%2520robots%2520to%2520learn%2520coupled%2520%2522motion-terrain%2522%2520interactions%2520directly%2520from%2520video.%2520By%2520leveraging%2520state-of-the-art%25203D%2520vision%2520models%252C%2520our%2520framework%2520precisely%2520segments%2520and%2520reconstructs%2520both%2520human%2520trajectories%2520and%2520the%2520underlying%25203D%2520geometry%2520of%2520terrains%2520and%2520objects.%2520We%2520introduce%2520an%2520optimization%2520algorithm%2520based%2520on%2520kinematic%2520consistency%2520to%2520extract%2520high-quality%2520motion%2520data%2520from%2520noisy%2520visual%2520reconstructions%252C%2520alongside%2520a%2520contact-invariant%2520retargeting%2520method%2520that%2520transfers%2520human-environment%2520interaction%2520features%2520to%2520the%2520humanoid%2520agent.%2520Experimental%2520results%2520demonstrate%2520that%2520MeshMimic%2520achieves%2520robust%252C%2520highly%2520dynamic%2520performance%2520across%2520diverse%2520and%2520challenging%2520terrains.%2520Our%2520approach%2520proves%2520that%2520a%2520low-cost%2520pipeline%2520utilizing%2520only%2520consumer-grade%2520monocular%2520sensors%2520can%2520facilitate%2520the%2520training%2520of%2520complex%2520physical%2520interactions%252C%2520offering%2520a%2520scalable%2520path%2520toward%2520the%2520autonomous%2520evolution%2520of%2520humanoid%2520robots%2520in%2520unstructured%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshMimic%3A%20Geometry-Aware%20Humanoid%20Motion%20Learning%20through%203D%20Scene%20Reconstruction&entry.906535625=Qiang%20Zhang%20and%20Jiahao%20Ma%20and%20Peiran%20Liu%20and%20Shuai%20Shi%20and%20Zeran%20Su%20and%20Zifan%20Wang%20and%20Jingkai%20Sun%20and%20Wei%20Cui%20and%20Jialin%20Yu%20and%20Gang%20Han%20and%20Wen%20Zhao%20and%20Pihai%20Sun%20and%20Kangning%20Yin%20and%20Jiaxu%20Wang%20and%20Jiahang%20Cao%20and%20Lingfeng%20Zhang%20and%20Hao%20Cheng%20and%20Xiaoshuai%20Hao%20and%20Yiding%20Ji%20and%20Junwei%20Liang%20and%20Jian%20Tang%20and%20Renjing%20Xu%20and%20Yijie%20Guo&entry.1292438233=Humanoid%20motion%20control%20has%20witnessed%20significant%20breakthroughs%20in%20recent%20years%2C%20with%20deep%20reinforcement%20learning%20%28RL%29%20emerging%20as%20a%20primary%20catalyst%20for%20achieving%20complex%2C%20human-like%20behaviors.%20However%2C%20the%20high%20dimensionality%20and%20intricate%20dynamics%20of%20humanoid%20robots%20make%20manual%20motion%20design%20impractical%2C%20leading%20to%20a%20heavy%20reliance%20on%20expensive%20motion%20capture%20%28MoCap%29%20data.%20These%20datasets%20are%20not%20only%20costly%20to%20acquire%20but%20also%20frequently%20lack%20the%20necessary%20geometric%20context%20of%20the%20surrounding%20physical%20environment.%20Consequently%2C%20existing%20motion%20synthesis%20frameworks%20often%20suffer%20from%20a%20decoupling%20of%20motion%20and%20scene%2C%20resulting%20in%20physical%20inconsistencies%20such%20as%20contact%20slippage%20or%20mesh%20penetration%20during%20terrain-aware%20tasks.%20In%20this%20work%2C%20we%20present%20MeshMimic%2C%20an%20innovative%20framework%20that%20bridges%203D%20scene%20reconstruction%20and%20embodied%20intelligence%20to%20enable%20humanoid%20robots%20to%20learn%20coupled%20%22motion-terrain%22%20interactions%20directly%20from%20video.%20By%20leveraging%20state-of-the-art%203D%20vision%20models%2C%20our%20framework%20precisely%20segments%20and%20reconstructs%20both%20human%20trajectories%20and%20the%20underlying%203D%20geometry%20of%20terrains%20and%20objects.%20We%20introduce%20an%20optimization%20algorithm%20based%20on%20kinematic%20consistency%20to%20extract%20high-quality%20motion%20data%20from%20noisy%20visual%20reconstructions%2C%20alongside%20a%20contact-invariant%20retargeting%20method%20that%20transfers%20human-environment%20interaction%20features%20to%20the%20humanoid%20agent.%20Experimental%20results%20demonstrate%20that%20MeshMimic%20achieves%20robust%2C%20highly%20dynamic%20performance%20across%20diverse%20and%20challenging%20terrains.%20Our%20approach%20proves%20that%20a%20low-cost%20pipeline%20utilizing%20only%20consumer-grade%20monocular%20sensors%20can%20facilitate%20the%20training%20of%20complex%20physical%20interactions%2C%20offering%20a%20scalable%20path%20toward%20the%20autonomous%20evolution%20of%20humanoid%20robots%20in%20unstructured%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2602.15733v1&entry.124074799=Read"},
{"title": "A Fully Interpretable Statistical Approach for Roadside LiDAR Background Subtraction", "author": "Aitor Iglesias and Nerea Aranjuelo and Patricia Javierre and Ainhoa Menendez and Ignacio Arganda-Carreras and Marcos Nieto", "abstract": "We present a fully interpretable and flexible statistical method for background subtraction in roadside LiDAR data, aimed at enhancing infrastructure-based perception in automated driving. Our approach introduces both a Gaussian distribution grid (GDG), which models the spatial statistics of the background using background-only scans, and a filtering algorithm that uses this representation to classify LiDAR points as foreground or background. The method supports diverse LiDAR types, including multiline 360 degree and micro-electro-mechanical systems (MEMS) sensors, and adapts to various configurations. Evaluated on the publicly available RCooper dataset, it outperforms state-of-the-art techniques in accuracy and flexibility, even with minimal background data. Its efficient implementation ensures reliable performance on low-resource hardware, enabling scalable real-world deployment.", "link": "http://arxiv.org/abs/2510.22390v2", "date": "2026-02-17", "relevancy": 2.5771, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5467}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5002}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Fully%20Interpretable%20Statistical%20Approach%20for%20Roadside%20LiDAR%20Background%20Subtraction&body=Title%3A%20A%20Fully%20Interpretable%20Statistical%20Approach%20for%20Roadside%20LiDAR%20Background%20Subtraction%0AAuthor%3A%20Aitor%20Iglesias%20and%20Nerea%20Aranjuelo%20and%20Patricia%20Javierre%20and%20Ainhoa%20Menendez%20and%20Ignacio%20Arganda-Carreras%20and%20Marcos%20Nieto%0AAbstract%3A%20We%20present%20a%20fully%20interpretable%20and%20flexible%20statistical%20method%20for%20background%20subtraction%20in%20roadside%20LiDAR%20data%2C%20aimed%20at%20enhancing%20infrastructure-based%20perception%20in%20automated%20driving.%20Our%20approach%20introduces%20both%20a%20Gaussian%20distribution%20grid%20%28GDG%29%2C%20which%20models%20the%20spatial%20statistics%20of%20the%20background%20using%20background-only%20scans%2C%20and%20a%20filtering%20algorithm%20that%20uses%20this%20representation%20to%20classify%20LiDAR%20points%20as%20foreground%20or%20background.%20The%20method%20supports%20diverse%20LiDAR%20types%2C%20including%20multiline%20360%20degree%20and%20micro-electro-mechanical%20systems%20%28MEMS%29%20sensors%2C%20and%20adapts%20to%20various%20configurations.%20Evaluated%20on%20the%20publicly%20available%20RCooper%20dataset%2C%20it%20outperforms%20state-of-the-art%20techniques%20in%20accuracy%20and%20flexibility%2C%20even%20with%20minimal%20background%20data.%20Its%20efficient%20implementation%20ensures%20reliable%20performance%20on%20low-resource%20hardware%2C%20enabling%20scalable%20real-world%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22390v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Fully%2520Interpretable%2520Statistical%2520Approach%2520for%2520Roadside%2520LiDAR%2520Background%2520Subtraction%26entry.906535625%3DAitor%2520Iglesias%2520and%2520Nerea%2520Aranjuelo%2520and%2520Patricia%2520Javierre%2520and%2520Ainhoa%2520Menendez%2520and%2520Ignacio%2520Arganda-Carreras%2520and%2520Marcos%2520Nieto%26entry.1292438233%3DWe%2520present%2520a%2520fully%2520interpretable%2520and%2520flexible%2520statistical%2520method%2520for%2520background%2520subtraction%2520in%2520roadside%2520LiDAR%2520data%252C%2520aimed%2520at%2520enhancing%2520infrastructure-based%2520perception%2520in%2520automated%2520driving.%2520Our%2520approach%2520introduces%2520both%2520a%2520Gaussian%2520distribution%2520grid%2520%2528GDG%2529%252C%2520which%2520models%2520the%2520spatial%2520statistics%2520of%2520the%2520background%2520using%2520background-only%2520scans%252C%2520and%2520a%2520filtering%2520algorithm%2520that%2520uses%2520this%2520representation%2520to%2520classify%2520LiDAR%2520points%2520as%2520foreground%2520or%2520background.%2520The%2520method%2520supports%2520diverse%2520LiDAR%2520types%252C%2520including%2520multiline%2520360%2520degree%2520and%2520micro-electro-mechanical%2520systems%2520%2528MEMS%2529%2520sensors%252C%2520and%2520adapts%2520to%2520various%2520configurations.%2520Evaluated%2520on%2520the%2520publicly%2520available%2520RCooper%2520dataset%252C%2520it%2520outperforms%2520state-of-the-art%2520techniques%2520in%2520accuracy%2520and%2520flexibility%252C%2520even%2520with%2520minimal%2520background%2520data.%2520Its%2520efficient%2520implementation%2520ensures%2520reliable%2520performance%2520on%2520low-resource%2520hardware%252C%2520enabling%2520scalable%2520real-world%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22390v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Fully%20Interpretable%20Statistical%20Approach%20for%20Roadside%20LiDAR%20Background%20Subtraction&entry.906535625=Aitor%20Iglesias%20and%20Nerea%20Aranjuelo%20and%20Patricia%20Javierre%20and%20Ainhoa%20Menendez%20and%20Ignacio%20Arganda-Carreras%20and%20Marcos%20Nieto&entry.1292438233=We%20present%20a%20fully%20interpretable%20and%20flexible%20statistical%20method%20for%20background%20subtraction%20in%20roadside%20LiDAR%20data%2C%20aimed%20at%20enhancing%20infrastructure-based%20perception%20in%20automated%20driving.%20Our%20approach%20introduces%20both%20a%20Gaussian%20distribution%20grid%20%28GDG%29%2C%20which%20models%20the%20spatial%20statistics%20of%20the%20background%20using%20background-only%20scans%2C%20and%20a%20filtering%20algorithm%20that%20uses%20this%20representation%20to%20classify%20LiDAR%20points%20as%20foreground%20or%20background.%20The%20method%20supports%20diverse%20LiDAR%20types%2C%20including%20multiline%20360%20degree%20and%20micro-electro-mechanical%20systems%20%28MEMS%29%20sensors%2C%20and%20adapts%20to%20various%20configurations.%20Evaluated%20on%20the%20publicly%20available%20RCooper%20dataset%2C%20it%20outperforms%20state-of-the-art%20techniques%20in%20accuracy%20and%20flexibility%2C%20even%20with%20minimal%20background%20data.%20Its%20efficient%20implementation%20ensures%20reliable%20performance%20on%20low-resource%20hardware%2C%20enabling%20scalable%20real-world%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2510.22390v2&entry.124074799=Read"},
{"title": "Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework", "author": "Grzegorz Statkiewicz and Alicja Dobrzeniecka and Karolina Seweryn and Aleksandra Krasnod\u0119bska and Karolina Piosek and Katarzyna Bogusz and Sebastian Cygert and Wojciech Kusa", "abstract": "Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal systems that reflect diverse linguistic and cultural realities. In this work, we reproduce and adapt the LLaVA-Next methodology to create a set of Polish VLMs. We rely on a fully automated pipeline for translating and filtering existing multimodal datasets, and complement this with synthetic Polish data for OCR and culturally specific tasks. Despite relying almost entirely on automatic translation and minimal manual intervention to the training data, our approach yields strong results: we observe a +9.5% improvement over LLaVA-1.6-Vicuna-13B on a Polish-adapted MMBench, along with higher-quality captions in generative evaluations, as measured by human annotators in terms of linguistic correctness. These findings highlight that large-scale automated translation, combined with lightweight filtering, can effectively bootstrap high-quality multimodal models for low-resource languages. Some challenges remain, particularly in cultural coverage and evaluation. To facilitate further research, we make our models and evaluation dataset publicly available.", "link": "http://arxiv.org/abs/2602.14073v2", "date": "2026-02-17", "relevancy": 2.5705, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Annotation-Efficient%20Vision-Language%20Model%20Adaptation%20to%20the%20Polish%20Language%20Using%20the%20LLaVA%20Framework&body=Title%3A%20Annotation-Efficient%20Vision-Language%20Model%20Adaptation%20to%20the%20Polish%20Language%20Using%20the%20LLaVA%20Framework%0AAuthor%3A%20Grzegorz%20Statkiewicz%20and%20Alicja%20Dobrzeniecka%20and%20Karolina%20Seweryn%20and%20Aleksandra%20Krasnod%C4%99bska%20and%20Karolina%20Piosek%20and%20Katarzyna%20Bogusz%20and%20Sebastian%20Cygert%20and%20Wojciech%20Kusa%0AAbstract%3A%20Most%20vision-language%20models%20%28VLMs%29%20are%20trained%20on%20English-centric%20data%2C%20limiting%20their%20performance%20in%20other%20languages%20and%20cultural%20contexts.%20This%20restricts%20their%20usability%20for%20non-English-speaking%20users%20and%20hinders%20the%20development%20of%20multimodal%20systems%20that%20reflect%20diverse%20linguistic%20and%20cultural%20realities.%20In%20this%20work%2C%20we%20reproduce%20and%20adapt%20the%20LLaVA-Next%20methodology%20to%20create%20a%20set%20of%20Polish%20VLMs.%20We%20rely%20on%20a%20fully%20automated%20pipeline%20for%20translating%20and%20filtering%20existing%20multimodal%20datasets%2C%20and%20complement%20this%20with%20synthetic%20Polish%20data%20for%20OCR%20and%20culturally%20specific%20tasks.%20Despite%20relying%20almost%20entirely%20on%20automatic%20translation%20and%20minimal%20manual%20intervention%20to%20the%20training%20data%2C%20our%20approach%20yields%20strong%20results%3A%20we%20observe%20a%20%2B9.5%25%20improvement%20over%20LLaVA-1.6-Vicuna-13B%20on%20a%20Polish-adapted%20MMBench%2C%20along%20with%20higher-quality%20captions%20in%20generative%20evaluations%2C%20as%20measured%20by%20human%20annotators%20in%20terms%20of%20linguistic%20correctness.%20These%20findings%20highlight%20that%20large-scale%20automated%20translation%2C%20combined%20with%20lightweight%20filtering%2C%20can%20effectively%20bootstrap%20high-quality%20multimodal%20models%20for%20low-resource%20languages.%20Some%20challenges%20remain%2C%20particularly%20in%20cultural%20coverage%20and%20evaluation.%20To%20facilitate%20further%20research%2C%20we%20make%20our%20models%20and%20evaluation%20dataset%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14073v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnnotation-Efficient%2520Vision-Language%2520Model%2520Adaptation%2520to%2520the%2520Polish%2520Language%2520Using%2520the%2520LLaVA%2520Framework%26entry.906535625%3DGrzegorz%2520Statkiewicz%2520and%2520Alicja%2520Dobrzeniecka%2520and%2520Karolina%2520Seweryn%2520and%2520Aleksandra%2520Krasnod%25C4%2599bska%2520and%2520Karolina%2520Piosek%2520and%2520Katarzyna%2520Bogusz%2520and%2520Sebastian%2520Cygert%2520and%2520Wojciech%2520Kusa%26entry.1292438233%3DMost%2520vision-language%2520models%2520%2528VLMs%2529%2520are%2520trained%2520on%2520English-centric%2520data%252C%2520limiting%2520their%2520performance%2520in%2520other%2520languages%2520and%2520cultural%2520contexts.%2520This%2520restricts%2520their%2520usability%2520for%2520non-English-speaking%2520users%2520and%2520hinders%2520the%2520development%2520of%2520multimodal%2520systems%2520that%2520reflect%2520diverse%2520linguistic%2520and%2520cultural%2520realities.%2520In%2520this%2520work%252C%2520we%2520reproduce%2520and%2520adapt%2520the%2520LLaVA-Next%2520methodology%2520to%2520create%2520a%2520set%2520of%2520Polish%2520VLMs.%2520We%2520rely%2520on%2520a%2520fully%2520automated%2520pipeline%2520for%2520translating%2520and%2520filtering%2520existing%2520multimodal%2520datasets%252C%2520and%2520complement%2520this%2520with%2520synthetic%2520Polish%2520data%2520for%2520OCR%2520and%2520culturally%2520specific%2520tasks.%2520Despite%2520relying%2520almost%2520entirely%2520on%2520automatic%2520translation%2520and%2520minimal%2520manual%2520intervention%2520to%2520the%2520training%2520data%252C%2520our%2520approach%2520yields%2520strong%2520results%253A%2520we%2520observe%2520a%2520%252B9.5%2525%2520improvement%2520over%2520LLaVA-1.6-Vicuna-13B%2520on%2520a%2520Polish-adapted%2520MMBench%252C%2520along%2520with%2520higher-quality%2520captions%2520in%2520generative%2520evaluations%252C%2520as%2520measured%2520by%2520human%2520annotators%2520in%2520terms%2520of%2520linguistic%2520correctness.%2520These%2520findings%2520highlight%2520that%2520large-scale%2520automated%2520translation%252C%2520combined%2520with%2520lightweight%2520filtering%252C%2520can%2520effectively%2520bootstrap%2520high-quality%2520multimodal%2520models%2520for%2520low-resource%2520languages.%2520Some%2520challenges%2520remain%252C%2520particularly%2520in%2520cultural%2520coverage%2520and%2520evaluation.%2520To%2520facilitate%2520further%2520research%252C%2520we%2520make%2520our%2520models%2520and%2520evaluation%2520dataset%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14073v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annotation-Efficient%20Vision-Language%20Model%20Adaptation%20to%20the%20Polish%20Language%20Using%20the%20LLaVA%20Framework&entry.906535625=Grzegorz%20Statkiewicz%20and%20Alicja%20Dobrzeniecka%20and%20Karolina%20Seweryn%20and%20Aleksandra%20Krasnod%C4%99bska%20and%20Karolina%20Piosek%20and%20Katarzyna%20Bogusz%20and%20Sebastian%20Cygert%20and%20Wojciech%20Kusa&entry.1292438233=Most%20vision-language%20models%20%28VLMs%29%20are%20trained%20on%20English-centric%20data%2C%20limiting%20their%20performance%20in%20other%20languages%20and%20cultural%20contexts.%20This%20restricts%20their%20usability%20for%20non-English-speaking%20users%20and%20hinders%20the%20development%20of%20multimodal%20systems%20that%20reflect%20diverse%20linguistic%20and%20cultural%20realities.%20In%20this%20work%2C%20we%20reproduce%20and%20adapt%20the%20LLaVA-Next%20methodology%20to%20create%20a%20set%20of%20Polish%20VLMs.%20We%20rely%20on%20a%20fully%20automated%20pipeline%20for%20translating%20and%20filtering%20existing%20multimodal%20datasets%2C%20and%20complement%20this%20with%20synthetic%20Polish%20data%20for%20OCR%20and%20culturally%20specific%20tasks.%20Despite%20relying%20almost%20entirely%20on%20automatic%20translation%20and%20minimal%20manual%20intervention%20to%20the%20training%20data%2C%20our%20approach%20yields%20strong%20results%3A%20we%20observe%20a%20%2B9.5%25%20improvement%20over%20LLaVA-1.6-Vicuna-13B%20on%20a%20Polish-adapted%20MMBench%2C%20along%20with%20higher-quality%20captions%20in%20generative%20evaluations%2C%20as%20measured%20by%20human%20annotators%20in%20terms%20of%20linguistic%20correctness.%20These%20findings%20highlight%20that%20large-scale%20automated%20translation%2C%20combined%20with%20lightweight%20filtering%2C%20can%20effectively%20bootstrap%20high-quality%20multimodal%20models%20for%20low-resource%20languages.%20Some%20challenges%20remain%2C%20particularly%20in%20cultural%20coverage%20and%20evaluation.%20To%20facilitate%20further%20research%2C%20we%20make%20our%20models%20and%20evaluation%20dataset%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2602.14073v2&entry.124074799=Read"},
{"title": "Criteria-first, semantics-later: reproducible structure discovery in image-based sciences", "author": "Jan Bumberger", "abstract": "Across the natural and life sciences, images have become a primary measurement modality, yet the dominant analytic paradigm remains semantics-first. Structure is recovered by predicting or enforcing domain-specific labels. This paradigm fails systematically under the conditions that make image-based science most valuable, including open-ended scientific discovery, cross-sensor and cross-site comparability, and long-term monitoring in which domain ontologies and associated label sets drift culturally, institutionally, and ecologically. A deductive inversion is proposed in the form of criteria-first and semantics-later. A unified framework for criteria-first structure discovery is introduced. It separates criterion-defined, semantics-free structure extraction from downstream semantic mapping into domain ontologies or vocabularies and provides a domain-general scaffold for reproducible analysis across image-based sciences. Reproducible science requires that the first analytic layer perform criterion-driven, semantics-free structure discovery, yielding stable partitions, structural fields, or hierarchies defined by explicit optimality criteria rather than local domain ontologies. Semantics is not discarded; it is relocated downstream as an explicit mapping from the discovered structural product to a domain ontology or vocabulary, enabling plural interpretations and explicit crosswalks without rewriting upstream extraction. Grounded in cybernetics, observation-as-distinction, and information theory's separation of information from meaning, the argument is supported by cross-domain evidence showing that criteria-first components recur whenever labels do not scale. Finally, consequences are outlined for validation beyond class accuracy and for treating structural products as FAIR, AI-ready digital objects for long-term monitoring and digital twins.", "link": "http://arxiv.org/abs/2602.15712v1", "date": "2026-02-17", "relevancy": 2.5473, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Criteria-first%2C%20semantics-later%3A%20reproducible%20structure%20discovery%20in%20image-based%20sciences&body=Title%3A%20Criteria-first%2C%20semantics-later%3A%20reproducible%20structure%20discovery%20in%20image-based%20sciences%0AAuthor%3A%20Jan%20Bumberger%0AAbstract%3A%20Across%20the%20natural%20and%20life%20sciences%2C%20images%20have%20become%20a%20primary%20measurement%20modality%2C%20yet%20the%20dominant%20analytic%20paradigm%20remains%20semantics-first.%20Structure%20is%20recovered%20by%20predicting%20or%20enforcing%20domain-specific%20labels.%20This%20paradigm%20fails%20systematically%20under%20the%20conditions%20that%20make%20image-based%20science%20most%20valuable%2C%20including%20open-ended%20scientific%20discovery%2C%20cross-sensor%20and%20cross-site%20comparability%2C%20and%20long-term%20monitoring%20in%20which%20domain%20ontologies%20and%20associated%20label%20sets%20drift%20culturally%2C%20institutionally%2C%20and%20ecologically.%20A%20deductive%20inversion%20is%20proposed%20in%20the%20form%20of%20criteria-first%20and%20semantics-later.%20A%20unified%20framework%20for%20criteria-first%20structure%20discovery%20is%20introduced.%20It%20separates%20criterion-defined%2C%20semantics-free%20structure%20extraction%20from%20downstream%20semantic%20mapping%20into%20domain%20ontologies%20or%20vocabularies%20and%20provides%20a%20domain-general%20scaffold%20for%20reproducible%20analysis%20across%20image-based%20sciences.%20Reproducible%20science%20requires%20that%20the%20first%20analytic%20layer%20perform%20criterion-driven%2C%20semantics-free%20structure%20discovery%2C%20yielding%20stable%20partitions%2C%20structural%20fields%2C%20or%20hierarchies%20defined%20by%20explicit%20optimality%20criteria%20rather%20than%20local%20domain%20ontologies.%20Semantics%20is%20not%20discarded%3B%20it%20is%20relocated%20downstream%20as%20an%20explicit%20mapping%20from%20the%20discovered%20structural%20product%20to%20a%20domain%20ontology%20or%20vocabulary%2C%20enabling%20plural%20interpretations%20and%20explicit%20crosswalks%20without%20rewriting%20upstream%20extraction.%20Grounded%20in%20cybernetics%2C%20observation-as-distinction%2C%20and%20information%20theory%27s%20separation%20of%20information%20from%20meaning%2C%20the%20argument%20is%20supported%20by%20cross-domain%20evidence%20showing%20that%20criteria-first%20components%20recur%20whenever%20labels%20do%20not%20scale.%20Finally%2C%20consequences%20are%20outlined%20for%20validation%20beyond%20class%20accuracy%20and%20for%20treating%20structural%20products%20as%20FAIR%2C%20AI-ready%20digital%20objects%20for%20long-term%20monitoring%20and%20digital%20twins.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCriteria-first%252C%2520semantics-later%253A%2520reproducible%2520structure%2520discovery%2520in%2520image-based%2520sciences%26entry.906535625%3DJan%2520Bumberger%26entry.1292438233%3DAcross%2520the%2520natural%2520and%2520life%2520sciences%252C%2520images%2520have%2520become%2520a%2520primary%2520measurement%2520modality%252C%2520yet%2520the%2520dominant%2520analytic%2520paradigm%2520remains%2520semantics-first.%2520Structure%2520is%2520recovered%2520by%2520predicting%2520or%2520enforcing%2520domain-specific%2520labels.%2520This%2520paradigm%2520fails%2520systematically%2520under%2520the%2520conditions%2520that%2520make%2520image-based%2520science%2520most%2520valuable%252C%2520including%2520open-ended%2520scientific%2520discovery%252C%2520cross-sensor%2520and%2520cross-site%2520comparability%252C%2520and%2520long-term%2520monitoring%2520in%2520which%2520domain%2520ontologies%2520and%2520associated%2520label%2520sets%2520drift%2520culturally%252C%2520institutionally%252C%2520and%2520ecologically.%2520A%2520deductive%2520inversion%2520is%2520proposed%2520in%2520the%2520form%2520of%2520criteria-first%2520and%2520semantics-later.%2520A%2520unified%2520framework%2520for%2520criteria-first%2520structure%2520discovery%2520is%2520introduced.%2520It%2520separates%2520criterion-defined%252C%2520semantics-free%2520structure%2520extraction%2520from%2520downstream%2520semantic%2520mapping%2520into%2520domain%2520ontologies%2520or%2520vocabularies%2520and%2520provides%2520a%2520domain-general%2520scaffold%2520for%2520reproducible%2520analysis%2520across%2520image-based%2520sciences.%2520Reproducible%2520science%2520requires%2520that%2520the%2520first%2520analytic%2520layer%2520perform%2520criterion-driven%252C%2520semantics-free%2520structure%2520discovery%252C%2520yielding%2520stable%2520partitions%252C%2520structural%2520fields%252C%2520or%2520hierarchies%2520defined%2520by%2520explicit%2520optimality%2520criteria%2520rather%2520than%2520local%2520domain%2520ontologies.%2520Semantics%2520is%2520not%2520discarded%253B%2520it%2520is%2520relocated%2520downstream%2520as%2520an%2520explicit%2520mapping%2520from%2520the%2520discovered%2520structural%2520product%2520to%2520a%2520domain%2520ontology%2520or%2520vocabulary%252C%2520enabling%2520plural%2520interpretations%2520and%2520explicit%2520crosswalks%2520without%2520rewriting%2520upstream%2520extraction.%2520Grounded%2520in%2520cybernetics%252C%2520observation-as-distinction%252C%2520and%2520information%2520theory%2527s%2520separation%2520of%2520information%2520from%2520meaning%252C%2520the%2520argument%2520is%2520supported%2520by%2520cross-domain%2520evidence%2520showing%2520that%2520criteria-first%2520components%2520recur%2520whenever%2520labels%2520do%2520not%2520scale.%2520Finally%252C%2520consequences%2520are%2520outlined%2520for%2520validation%2520beyond%2520class%2520accuracy%2520and%2520for%2520treating%2520structural%2520products%2520as%2520FAIR%252C%2520AI-ready%2520digital%2520objects%2520for%2520long-term%2520monitoring%2520and%2520digital%2520twins.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Criteria-first%2C%20semantics-later%3A%20reproducible%20structure%20discovery%20in%20image-based%20sciences&entry.906535625=Jan%20Bumberger&entry.1292438233=Across%20the%20natural%20and%20life%20sciences%2C%20images%20have%20become%20a%20primary%20measurement%20modality%2C%20yet%20the%20dominant%20analytic%20paradigm%20remains%20semantics-first.%20Structure%20is%20recovered%20by%20predicting%20or%20enforcing%20domain-specific%20labels.%20This%20paradigm%20fails%20systematically%20under%20the%20conditions%20that%20make%20image-based%20science%20most%20valuable%2C%20including%20open-ended%20scientific%20discovery%2C%20cross-sensor%20and%20cross-site%20comparability%2C%20and%20long-term%20monitoring%20in%20which%20domain%20ontologies%20and%20associated%20label%20sets%20drift%20culturally%2C%20institutionally%2C%20and%20ecologically.%20A%20deductive%20inversion%20is%20proposed%20in%20the%20form%20of%20criteria-first%20and%20semantics-later.%20A%20unified%20framework%20for%20criteria-first%20structure%20discovery%20is%20introduced.%20It%20separates%20criterion-defined%2C%20semantics-free%20structure%20extraction%20from%20downstream%20semantic%20mapping%20into%20domain%20ontologies%20or%20vocabularies%20and%20provides%20a%20domain-general%20scaffold%20for%20reproducible%20analysis%20across%20image-based%20sciences.%20Reproducible%20science%20requires%20that%20the%20first%20analytic%20layer%20perform%20criterion-driven%2C%20semantics-free%20structure%20discovery%2C%20yielding%20stable%20partitions%2C%20structural%20fields%2C%20or%20hierarchies%20defined%20by%20explicit%20optimality%20criteria%20rather%20than%20local%20domain%20ontologies.%20Semantics%20is%20not%20discarded%3B%20it%20is%20relocated%20downstream%20as%20an%20explicit%20mapping%20from%20the%20discovered%20structural%20product%20to%20a%20domain%20ontology%20or%20vocabulary%2C%20enabling%20plural%20interpretations%20and%20explicit%20crosswalks%20without%20rewriting%20upstream%20extraction.%20Grounded%20in%20cybernetics%2C%20observation-as-distinction%2C%20and%20information%20theory%27s%20separation%20of%20information%20from%20meaning%2C%20the%20argument%20is%20supported%20by%20cross-domain%20evidence%20showing%20that%20criteria-first%20components%20recur%20whenever%20labels%20do%20not%20scale.%20Finally%2C%20consequences%20are%20outlined%20for%20validation%20beyond%20class%20accuracy%20and%20for%20treating%20structural%20products%20as%20FAIR%2C%20AI-ready%20digital%20objects%20for%20long-term%20monitoring%20and%20digital%20twins.&entry.1838667208=http%3A//arxiv.org/abs/2602.15712v1&entry.124074799=Read"},
{"title": "Operationalising the Superficial Alignment Hypothesis via Task Complexity", "author": "Tom\u00e1s Vergara-Browne and Darshan Patil and Ivan Titov and Siva Reddy and Tiago Pimentel and Marius Mosbach", "abstract": "The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of mathematical reasoning, machine translation, and instruction following; we then show that these complexities can be remarkably low when conditioned on a pre-trained model. Further, we find that pre-training enables access to strong performances on our tasks, but it can require programs of gigabytes of length to access them. Post-training, on the other hand, collapses the complexity of reaching this same performance by several orders of magnitude. Overall, our results highlight that task adaptation often requires surprisingly little information -- often just a few kilobytes.", "link": "http://arxiv.org/abs/2602.15829v1", "date": "2026-02-17", "relevancy": 2.5442, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Operationalising%20the%20Superficial%20Alignment%20Hypothesis%20via%20Task%20Complexity&body=Title%3A%20Operationalising%20the%20Superficial%20Alignment%20Hypothesis%20via%20Task%20Complexity%0AAuthor%3A%20Tom%C3%A1s%20Vergara-Browne%20and%20Darshan%20Patil%20and%20Ivan%20Titov%20and%20Siva%20Reddy%20and%20Tiago%20Pimentel%20and%20Marius%20Mosbach%0AAbstract%3A%20The%20superficial%20alignment%20hypothesis%20%28SAH%29%20posits%20that%20large%20language%20models%20learn%20most%20of%20their%20knowledge%20during%20pre-training%2C%20and%20that%20post-training%20merely%20surfaces%20this%20knowledge.%20The%20SAH%2C%20however%2C%20lacks%20a%20precise%20definition%2C%20which%20has%20led%20to%20%28i%29%20different%20and%20seemingly%20orthogonal%20arguments%20supporting%20it%2C%20and%20%28ii%29%20important%20critiques%20to%20it.%20We%20propose%20a%20new%20metric%20called%20task%20complexity%3A%20the%20length%20of%20the%20shortest%20program%20that%20achieves%20a%20target%20performance%20on%20a%20task.%20In%20this%20framework%2C%20the%20SAH%20simply%20claims%20that%20pre-trained%20models%20drastically%20reduce%20the%20complexity%20of%20achieving%20high%20performance%20on%20many%20tasks.%20Our%20definition%20unifies%20prior%20arguments%20supporting%20the%20SAH%2C%20interpreting%20them%20as%20different%20strategies%20to%20find%20such%20short%20programs.%20Experimentally%2C%20we%20estimate%20the%20task%20complexity%20of%20mathematical%20reasoning%2C%20machine%20translation%2C%20and%20instruction%20following%3B%20we%20then%20show%20that%20these%20complexities%20can%20be%20remarkably%20low%20when%20conditioned%20on%20a%20pre-trained%20model.%20Further%2C%20we%20find%20that%20pre-training%20enables%20access%20to%20strong%20performances%20on%20our%20tasks%2C%20but%20it%20can%20require%20programs%20of%20gigabytes%20of%20length%20to%20access%20them.%20Post-training%2C%20on%20the%20other%20hand%2C%20collapses%20the%20complexity%20of%20reaching%20this%20same%20performance%20by%20several%20orders%20of%20magnitude.%20Overall%2C%20our%20results%20highlight%20that%20task%20adaptation%20often%20requires%20surprisingly%20little%20information%20--%20often%20just%20a%20few%20kilobytes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOperationalising%2520the%2520Superficial%2520Alignment%2520Hypothesis%2520via%2520Task%2520Complexity%26entry.906535625%3DTom%25C3%25A1s%2520Vergara-Browne%2520and%2520Darshan%2520Patil%2520and%2520Ivan%2520Titov%2520and%2520Siva%2520Reddy%2520and%2520Tiago%2520Pimentel%2520and%2520Marius%2520Mosbach%26entry.1292438233%3DThe%2520superficial%2520alignment%2520hypothesis%2520%2528SAH%2529%2520posits%2520that%2520large%2520language%2520models%2520learn%2520most%2520of%2520their%2520knowledge%2520during%2520pre-training%252C%2520and%2520that%2520post-training%2520merely%2520surfaces%2520this%2520knowledge.%2520The%2520SAH%252C%2520however%252C%2520lacks%2520a%2520precise%2520definition%252C%2520which%2520has%2520led%2520to%2520%2528i%2529%2520different%2520and%2520seemingly%2520orthogonal%2520arguments%2520supporting%2520it%252C%2520and%2520%2528ii%2529%2520important%2520critiques%2520to%2520it.%2520We%2520propose%2520a%2520new%2520metric%2520called%2520task%2520complexity%253A%2520the%2520length%2520of%2520the%2520shortest%2520program%2520that%2520achieves%2520a%2520target%2520performance%2520on%2520a%2520task.%2520In%2520this%2520framework%252C%2520the%2520SAH%2520simply%2520claims%2520that%2520pre-trained%2520models%2520drastically%2520reduce%2520the%2520complexity%2520of%2520achieving%2520high%2520performance%2520on%2520many%2520tasks.%2520Our%2520definition%2520unifies%2520prior%2520arguments%2520supporting%2520the%2520SAH%252C%2520interpreting%2520them%2520as%2520different%2520strategies%2520to%2520find%2520such%2520short%2520programs.%2520Experimentally%252C%2520we%2520estimate%2520the%2520task%2520complexity%2520of%2520mathematical%2520reasoning%252C%2520machine%2520translation%252C%2520and%2520instruction%2520following%253B%2520we%2520then%2520show%2520that%2520these%2520complexities%2520can%2520be%2520remarkably%2520low%2520when%2520conditioned%2520on%2520a%2520pre-trained%2520model.%2520Further%252C%2520we%2520find%2520that%2520pre-training%2520enables%2520access%2520to%2520strong%2520performances%2520on%2520our%2520tasks%252C%2520but%2520it%2520can%2520require%2520programs%2520of%2520gigabytes%2520of%2520length%2520to%2520access%2520them.%2520Post-training%252C%2520on%2520the%2520other%2520hand%252C%2520collapses%2520the%2520complexity%2520of%2520reaching%2520this%2520same%2520performance%2520by%2520several%2520orders%2520of%2520magnitude.%2520Overall%252C%2520our%2520results%2520highlight%2520that%2520task%2520adaptation%2520often%2520requires%2520surprisingly%2520little%2520information%2520--%2520often%2520just%2520a%2520few%2520kilobytes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Operationalising%20the%20Superficial%20Alignment%20Hypothesis%20via%20Task%20Complexity&entry.906535625=Tom%C3%A1s%20Vergara-Browne%20and%20Darshan%20Patil%20and%20Ivan%20Titov%20and%20Siva%20Reddy%20and%20Tiago%20Pimentel%20and%20Marius%20Mosbach&entry.1292438233=The%20superficial%20alignment%20hypothesis%20%28SAH%29%20posits%20that%20large%20language%20models%20learn%20most%20of%20their%20knowledge%20during%20pre-training%2C%20and%20that%20post-training%20merely%20surfaces%20this%20knowledge.%20The%20SAH%2C%20however%2C%20lacks%20a%20precise%20definition%2C%20which%20has%20led%20to%20%28i%29%20different%20and%20seemingly%20orthogonal%20arguments%20supporting%20it%2C%20and%20%28ii%29%20important%20critiques%20to%20it.%20We%20propose%20a%20new%20metric%20called%20task%20complexity%3A%20the%20length%20of%20the%20shortest%20program%20that%20achieves%20a%20target%20performance%20on%20a%20task.%20In%20this%20framework%2C%20the%20SAH%20simply%20claims%20that%20pre-trained%20models%20drastically%20reduce%20the%20complexity%20of%20achieving%20high%20performance%20on%20many%20tasks.%20Our%20definition%20unifies%20prior%20arguments%20supporting%20the%20SAH%2C%20interpreting%20them%20as%20different%20strategies%20to%20find%20such%20short%20programs.%20Experimentally%2C%20we%20estimate%20the%20task%20complexity%20of%20mathematical%20reasoning%2C%20machine%20translation%2C%20and%20instruction%20following%3B%20we%20then%20show%20that%20these%20complexities%20can%20be%20remarkably%20low%20when%20conditioned%20on%20a%20pre-trained%20model.%20Further%2C%20we%20find%20that%20pre-training%20enables%20access%20to%20strong%20performances%20on%20our%20tasks%2C%20but%20it%20can%20require%20programs%20of%20gigabytes%20of%20length%20to%20access%20them.%20Post-training%2C%20on%20the%20other%20hand%2C%20collapses%20the%20complexity%20of%20reaching%20this%20same%20performance%20by%20several%20orders%20of%20magnitude.%20Overall%2C%20our%20results%20highlight%20that%20task%20adaptation%20often%20requires%20surprisingly%20little%20information%20--%20often%20just%20a%20few%20kilobytes.&entry.1838667208=http%3A//arxiv.org/abs/2602.15829v1&entry.124074799=Read"},
{"title": "On the Geometric Coherence of Global Aggregation in Federated GNN", "author": "Chethana Prasad Kabgere and Shylaja SS", "abstract": "Federated Learning (FL) enables distributed training across multiple clients without centralized data sharing, while Graph Neural Networks (GNNs) model relational data through message passing. In federated GNN settings, client graphs often exhibit heterogeneous structural and propagation characteristics. When standard aggregation mechanisms are applied to such heterogeneous updates, the global model may converge numerically while exhibiting degraded relational behavior.Our work identifies a geometric failure mode of global aggregation in Cross- Domain Federated GNNs. Although GNN parameters are numerically represented as vectors, they encode relational transformations that govern the direction, strength, and sensitivity of information flow across graph neighborhoods. Aggregating updates originating from incompatible propagation regimes can therefore introduce destructive interference in this transformation space.This leads to loss of coherence in global message passing. Importantly, this degradation is not necessarily reflected in conventional metrics such as loss or accuracy.To address this issue, we propose GGRS (Global Geometric Reference Structure), a server-side framework that regulates client updates prior to aggregation based on geometric admissibility criteria. GGRS preserves directional consistency of relational transformations as well as maintains diversity of admissible propagation subspaces. It also stabilizes sensitivity to neighborhood interactions, without accessing client data or graph topology. Experiments on heterogeneous GNN-native, Amazon Co-purchase datasets demonstrate that GGRS preserves global message-passing coherence across training rounds by highlighting the necessity of geometry-aware regulation in federated graph learning.", "link": "http://arxiv.org/abs/2602.15510v1", "date": "2026-02-17", "relevancy": 2.5266, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5376}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4925}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Geometric%20Coherence%20of%20Global%20Aggregation%20in%20Federated%20GNN&body=Title%3A%20On%20the%20Geometric%20Coherence%20of%20Global%20Aggregation%20in%20Federated%20GNN%0AAuthor%3A%20Chethana%20Prasad%20Kabgere%20and%20Shylaja%20SS%0AAbstract%3A%20Federated%20Learning%20%28FL%29%20enables%20distributed%20training%20across%20multiple%20clients%20without%20centralized%20data%20sharing%2C%20while%20Graph%20Neural%20Networks%20%28GNNs%29%20model%20relational%20data%20through%20message%20passing.%20In%20federated%20GNN%20settings%2C%20client%20graphs%20often%20exhibit%20heterogeneous%20structural%20and%20propagation%20characteristics.%20When%20standard%20aggregation%20mechanisms%20are%20applied%20to%20such%20heterogeneous%20updates%2C%20the%20global%20model%20may%20converge%20numerically%20while%20exhibiting%20degraded%20relational%20behavior.Our%20work%20identifies%20a%20geometric%20failure%20mode%20of%20global%20aggregation%20in%20Cross-%20Domain%20Federated%20GNNs.%20Although%20GNN%20parameters%20are%20numerically%20represented%20as%20vectors%2C%20they%20encode%20relational%20transformations%20that%20govern%20the%20direction%2C%20strength%2C%20and%20sensitivity%20of%20information%20flow%20across%20graph%20neighborhoods.%20Aggregating%20updates%20originating%20from%20incompatible%20propagation%20regimes%20can%20therefore%20introduce%20destructive%20interference%20in%20this%20transformation%20space.This%20leads%20to%20loss%20of%20coherence%20in%20global%20message%20passing.%20Importantly%2C%20this%20degradation%20is%20not%20necessarily%20reflected%20in%20conventional%20metrics%20such%20as%20loss%20or%20accuracy.To%20address%20this%20issue%2C%20we%20propose%20GGRS%20%28Global%20Geometric%20Reference%20Structure%29%2C%20a%20server-side%20framework%20that%20regulates%20client%20updates%20prior%20to%20aggregation%20based%20on%20geometric%20admissibility%20criteria.%20GGRS%20preserves%20directional%20consistency%20of%20relational%20transformations%20as%20well%20as%20maintains%20diversity%20of%20admissible%20propagation%20subspaces.%20It%20also%20stabilizes%20sensitivity%20to%20neighborhood%20interactions%2C%20without%20accessing%20client%20data%20or%20graph%20topology.%20Experiments%20on%20heterogeneous%20GNN-native%2C%20Amazon%20Co-purchase%20datasets%20demonstrate%20that%20GGRS%20preserves%20global%20message-passing%20coherence%20across%20training%20rounds%20by%20highlighting%20the%20necessity%20of%20geometry-aware%20regulation%20in%20federated%20graph%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Geometric%2520Coherence%2520of%2520Global%2520Aggregation%2520in%2520Federated%2520GNN%26entry.906535625%3DChethana%2520Prasad%2520Kabgere%2520and%2520Shylaja%2520SS%26entry.1292438233%3DFederated%2520Learning%2520%2528FL%2529%2520enables%2520distributed%2520training%2520across%2520multiple%2520clients%2520without%2520centralized%2520data%2520sharing%252C%2520while%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520model%2520relational%2520data%2520through%2520message%2520passing.%2520In%2520federated%2520GNN%2520settings%252C%2520client%2520graphs%2520often%2520exhibit%2520heterogeneous%2520structural%2520and%2520propagation%2520characteristics.%2520When%2520standard%2520aggregation%2520mechanisms%2520are%2520applied%2520to%2520such%2520heterogeneous%2520updates%252C%2520the%2520global%2520model%2520may%2520converge%2520numerically%2520while%2520exhibiting%2520degraded%2520relational%2520behavior.Our%2520work%2520identifies%2520a%2520geometric%2520failure%2520mode%2520of%2520global%2520aggregation%2520in%2520Cross-%2520Domain%2520Federated%2520GNNs.%2520Although%2520GNN%2520parameters%2520are%2520numerically%2520represented%2520as%2520vectors%252C%2520they%2520encode%2520relational%2520transformations%2520that%2520govern%2520the%2520direction%252C%2520strength%252C%2520and%2520sensitivity%2520of%2520information%2520flow%2520across%2520graph%2520neighborhoods.%2520Aggregating%2520updates%2520originating%2520from%2520incompatible%2520propagation%2520regimes%2520can%2520therefore%2520introduce%2520destructive%2520interference%2520in%2520this%2520transformation%2520space.This%2520leads%2520to%2520loss%2520of%2520coherence%2520in%2520global%2520message%2520passing.%2520Importantly%252C%2520this%2520degradation%2520is%2520not%2520necessarily%2520reflected%2520in%2520conventional%2520metrics%2520such%2520as%2520loss%2520or%2520accuracy.To%2520address%2520this%2520issue%252C%2520we%2520propose%2520GGRS%2520%2528Global%2520Geometric%2520Reference%2520Structure%2529%252C%2520a%2520server-side%2520framework%2520that%2520regulates%2520client%2520updates%2520prior%2520to%2520aggregation%2520based%2520on%2520geometric%2520admissibility%2520criteria.%2520GGRS%2520preserves%2520directional%2520consistency%2520of%2520relational%2520transformations%2520as%2520well%2520as%2520maintains%2520diversity%2520of%2520admissible%2520propagation%2520subspaces.%2520It%2520also%2520stabilizes%2520sensitivity%2520to%2520neighborhood%2520interactions%252C%2520without%2520accessing%2520client%2520data%2520or%2520graph%2520topology.%2520Experiments%2520on%2520heterogeneous%2520GNN-native%252C%2520Amazon%2520Co-purchase%2520datasets%2520demonstrate%2520that%2520GGRS%2520preserves%2520global%2520message-passing%2520coherence%2520across%2520training%2520rounds%2520by%2520highlighting%2520the%2520necessity%2520of%2520geometry-aware%2520regulation%2520in%2520federated%2520graph%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Geometric%20Coherence%20of%20Global%20Aggregation%20in%20Federated%20GNN&entry.906535625=Chethana%20Prasad%20Kabgere%20and%20Shylaja%20SS&entry.1292438233=Federated%20Learning%20%28FL%29%20enables%20distributed%20training%20across%20multiple%20clients%20without%20centralized%20data%20sharing%2C%20while%20Graph%20Neural%20Networks%20%28GNNs%29%20model%20relational%20data%20through%20message%20passing.%20In%20federated%20GNN%20settings%2C%20client%20graphs%20often%20exhibit%20heterogeneous%20structural%20and%20propagation%20characteristics.%20When%20standard%20aggregation%20mechanisms%20are%20applied%20to%20such%20heterogeneous%20updates%2C%20the%20global%20model%20may%20converge%20numerically%20while%20exhibiting%20degraded%20relational%20behavior.Our%20work%20identifies%20a%20geometric%20failure%20mode%20of%20global%20aggregation%20in%20Cross-%20Domain%20Federated%20GNNs.%20Although%20GNN%20parameters%20are%20numerically%20represented%20as%20vectors%2C%20they%20encode%20relational%20transformations%20that%20govern%20the%20direction%2C%20strength%2C%20and%20sensitivity%20of%20information%20flow%20across%20graph%20neighborhoods.%20Aggregating%20updates%20originating%20from%20incompatible%20propagation%20regimes%20can%20therefore%20introduce%20destructive%20interference%20in%20this%20transformation%20space.This%20leads%20to%20loss%20of%20coherence%20in%20global%20message%20passing.%20Importantly%2C%20this%20degradation%20is%20not%20necessarily%20reflected%20in%20conventional%20metrics%20such%20as%20loss%20or%20accuracy.To%20address%20this%20issue%2C%20we%20propose%20GGRS%20%28Global%20Geometric%20Reference%20Structure%29%2C%20a%20server-side%20framework%20that%20regulates%20client%20updates%20prior%20to%20aggregation%20based%20on%20geometric%20admissibility%20criteria.%20GGRS%20preserves%20directional%20consistency%20of%20relational%20transformations%20as%20well%20as%20maintains%20diversity%20of%20admissible%20propagation%20subspaces.%20It%20also%20stabilizes%20sensitivity%20to%20neighborhood%20interactions%2C%20without%20accessing%20client%20data%20or%20graph%20topology.%20Experiments%20on%20heterogeneous%20GNN-native%2C%20Amazon%20Co-purchase%20datasets%20demonstrate%20that%20GGRS%20preserves%20global%20message-passing%20coherence%20across%20training%20rounds%20by%20highlighting%20the%20necessity%20of%20geometry-aware%20regulation%20in%20federated%20graph%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2602.15510v1&entry.124074799=Read"},
{"title": "Avey-B", "author": "Devang Acharya and Mohammad Hammoud", "abstract": "Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.", "link": "http://arxiv.org/abs/2602.15814v1", "date": "2026-02-17", "relevancy": 2.5096, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5268}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5137}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Avey-B&body=Title%3A%20Avey-B%0AAuthor%3A%20Devang%20Acharya%20and%20Mohammad%20Hammoud%0AAbstract%3A%20Compact%20pretrained%20bidirectional%20encoders%20remain%20the%20backbone%20of%20industrial%20NLP%20under%20tight%20compute%20and%20memory%20budgets.%20Their%20effectiveness%20stems%20from%20self-attention%27s%20ability%20to%20deliver%20high-quality%20bidirectional%20contextualization%20with%20sequence-level%20parallelism%2C%20as%20popularized%20by%20BERT-style%20architectures.%20Recently%2C%20Avey%20was%20introduced%20as%20an%20autoregressive%2C%20attention-free%20alternative%20that%20naturally%20admits%20an%20encoder-only%20adaptation.%20In%20this%20paper%2C%20we%20reformulate%20Avey%20for%20the%20encoder-only%20paradigm%20and%20propose%20several%20innovations%20to%20its%20architecture%2C%20including%20decoupled%20static%20and%20dynamic%20parameterizations%2C%20stability-oriented%20normalization%2C%20and%20neural%20compression.%20Results%20show%20that%20this%20reformulated%20architecture%20compares%20favorably%20to%20four%20widely%20used%20Transformer-based%20encoders%2C%20consistently%20outperforming%20them%20on%20standard%20token-classification%20and%20information-retrieval%20benchmarks%20while%20scaling%20more%20efficiently%20to%20long%20contexts.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvey-B%26entry.906535625%3DDevang%2520Acharya%2520and%2520Mohammad%2520Hammoud%26entry.1292438233%3DCompact%2520pretrained%2520bidirectional%2520encoders%2520remain%2520the%2520backbone%2520of%2520industrial%2520NLP%2520under%2520tight%2520compute%2520and%2520memory%2520budgets.%2520Their%2520effectiveness%2520stems%2520from%2520self-attention%2527s%2520ability%2520to%2520deliver%2520high-quality%2520bidirectional%2520contextualization%2520with%2520sequence-level%2520parallelism%252C%2520as%2520popularized%2520by%2520BERT-style%2520architectures.%2520Recently%252C%2520Avey%2520was%2520introduced%2520as%2520an%2520autoregressive%252C%2520attention-free%2520alternative%2520that%2520naturally%2520admits%2520an%2520encoder-only%2520adaptation.%2520In%2520this%2520paper%252C%2520we%2520reformulate%2520Avey%2520for%2520the%2520encoder-only%2520paradigm%2520and%2520propose%2520several%2520innovations%2520to%2520its%2520architecture%252C%2520including%2520decoupled%2520static%2520and%2520dynamic%2520parameterizations%252C%2520stability-oriented%2520normalization%252C%2520and%2520neural%2520compression.%2520Results%2520show%2520that%2520this%2520reformulated%2520architecture%2520compares%2520favorably%2520to%2520four%2520widely%2520used%2520Transformer-based%2520encoders%252C%2520consistently%2520outperforming%2520them%2520on%2520standard%2520token-classification%2520and%2520information-retrieval%2520benchmarks%2520while%2520scaling%2520more%2520efficiently%2520to%2520long%2520contexts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Avey-B&entry.906535625=Devang%20Acharya%20and%20Mohammad%20Hammoud&entry.1292438233=Compact%20pretrained%20bidirectional%20encoders%20remain%20the%20backbone%20of%20industrial%20NLP%20under%20tight%20compute%20and%20memory%20budgets.%20Their%20effectiveness%20stems%20from%20self-attention%27s%20ability%20to%20deliver%20high-quality%20bidirectional%20contextualization%20with%20sequence-level%20parallelism%2C%20as%20popularized%20by%20BERT-style%20architectures.%20Recently%2C%20Avey%20was%20introduced%20as%20an%20autoregressive%2C%20attention-free%20alternative%20that%20naturally%20admits%20an%20encoder-only%20adaptation.%20In%20this%20paper%2C%20we%20reformulate%20Avey%20for%20the%20encoder-only%20paradigm%20and%20propose%20several%20innovations%20to%20its%20architecture%2C%20including%20decoupled%20static%20and%20dynamic%20parameterizations%2C%20stability-oriented%20normalization%2C%20and%20neural%20compression.%20Results%20show%20that%20this%20reformulated%20architecture%20compares%20favorably%20to%20four%20widely%20used%20Transformer-based%20encoders%2C%20consistently%20outperforming%20them%20on%20standard%20token-classification%20and%20information-retrieval%20benchmarks%20while%20scaling%20more%20efficiently%20to%20long%20contexts.&entry.1838667208=http%3A//arxiv.org/abs/2602.15814v1&entry.124074799=Read"},
{"title": "ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns", "author": "Ziyu Zhao and Tong Zhu and Zhi Zhang and Tiantian Fan and Jinluan Yang and Kun Kuang and Zhongyu Wei and Fei Wu and Yu Cheng", "abstract": "Mixture-of-Experts (MoE) effectively scales model capacity while preserving computational efficiency through sparse expert activation. However, training high-quality MoEs from scratch is prohibitively expensive. A promising alternative is to convert pretrained dense models into sparse MoEs. Existing dense-to-MoE methods fall into two categories: \\textbf{dynamic structural pruning} that converts dense models into MoE architectures with moderate sparsity to balance performance and inference efficiency, and \\textbf{downcycling} approaches that use pretrained dense models to initialize highly sparse MoE architectures. However, existing methods break the intrinsic activation patterns within dense models, leading to suboptimal expert construction. In this work, we argue that the Gated Linear Unit (GLU) mechanism provides a natural blueprint for dense-to-MoE conversion. We show that the fine-grained neural-wise activation patterns of GLU reveal a coarse-grained structure, uncovering an inherent MoE architecture composed of consistently activated universal neurons and dynamically activated specialized neurons. Leveraging this discovery, we introduce ExpertWeaver, a training-free framework that partitions neurons according to their activation patterns and constructs shared experts and specialized routed experts with layer-adaptive configurations. Our experiments demonstrate that ExpertWeaver significantly outperforms existing methods, both as a training-free dynamic structural pruning technique and as a downcycling strategy for superior MoE initialization.", "link": "http://arxiv.org/abs/2602.15521v1", "date": "2026-02-17", "relevancy": 2.4997, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5115}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5051}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExpertWeaver%3A%20Unlocking%20the%20Inherent%20MoE%20in%20Dense%20LLMs%20with%20GLU%20Activation%20Patterns&body=Title%3A%20ExpertWeaver%3A%20Unlocking%20the%20Inherent%20MoE%20in%20Dense%20LLMs%20with%20GLU%20Activation%20Patterns%0AAuthor%3A%20Ziyu%20Zhao%20and%20Tong%20Zhu%20and%20Zhi%20Zhang%20and%20Tiantian%20Fan%20and%20Jinluan%20Yang%20and%20Kun%20Kuang%20and%20Zhongyu%20Wei%20and%20Fei%20Wu%20and%20Yu%20Cheng%0AAbstract%3A%20Mixture-of-Experts%20%28MoE%29%20effectively%20scales%20model%20capacity%20while%20preserving%20computational%20efficiency%20through%20sparse%20expert%20activation.%20However%2C%20training%20high-quality%20MoEs%20from%20scratch%20is%20prohibitively%20expensive.%20A%20promising%20alternative%20is%20to%20convert%20pretrained%20dense%20models%20into%20sparse%20MoEs.%20Existing%20dense-to-MoE%20methods%20fall%20into%20two%20categories%3A%20%5Ctextbf%7Bdynamic%20structural%20pruning%7D%20that%20converts%20dense%20models%20into%20MoE%20architectures%20with%20moderate%20sparsity%20to%20balance%20performance%20and%20inference%20efficiency%2C%20and%20%5Ctextbf%7Bdowncycling%7D%20approaches%20that%20use%20pretrained%20dense%20models%20to%20initialize%20highly%20sparse%20MoE%20architectures.%20However%2C%20existing%20methods%20break%20the%20intrinsic%20activation%20patterns%20within%20dense%20models%2C%20leading%20to%20suboptimal%20expert%20construction.%20In%20this%20work%2C%20we%20argue%20that%20the%20Gated%20Linear%20Unit%20%28GLU%29%20mechanism%20provides%20a%20natural%20blueprint%20for%20dense-to-MoE%20conversion.%20We%20show%20that%20the%20fine-grained%20neural-wise%20activation%20patterns%20of%20GLU%20reveal%20a%20coarse-grained%20structure%2C%20uncovering%20an%20inherent%20MoE%20architecture%20composed%20of%20consistently%20activated%20universal%20neurons%20and%20dynamically%20activated%20specialized%20neurons.%20Leveraging%20this%20discovery%2C%20we%20introduce%20ExpertWeaver%2C%20a%20training-free%20framework%20that%20partitions%20neurons%20according%20to%20their%20activation%20patterns%20and%20constructs%20shared%20experts%20and%20specialized%20routed%20experts%20with%20layer-adaptive%20configurations.%20Our%20experiments%20demonstrate%20that%20ExpertWeaver%20significantly%20outperforms%20existing%20methods%2C%20both%20as%20a%20training-free%20dynamic%20structural%20pruning%20technique%20and%20as%20a%20downcycling%20strategy%20for%20superior%20MoE%20initialization.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpertWeaver%253A%2520Unlocking%2520the%2520Inherent%2520MoE%2520in%2520Dense%2520LLMs%2520with%2520GLU%2520Activation%2520Patterns%26entry.906535625%3DZiyu%2520Zhao%2520and%2520Tong%2520Zhu%2520and%2520Zhi%2520Zhang%2520and%2520Tiantian%2520Fan%2520and%2520Jinluan%2520Yang%2520and%2520Kun%2520Kuang%2520and%2520Zhongyu%2520Wei%2520and%2520Fei%2520Wu%2520and%2520Yu%2520Cheng%26entry.1292438233%3DMixture-of-Experts%2520%2528MoE%2529%2520effectively%2520scales%2520model%2520capacity%2520while%2520preserving%2520computational%2520efficiency%2520through%2520sparse%2520expert%2520activation.%2520However%252C%2520training%2520high-quality%2520MoEs%2520from%2520scratch%2520is%2520prohibitively%2520expensive.%2520A%2520promising%2520alternative%2520is%2520to%2520convert%2520pretrained%2520dense%2520models%2520into%2520sparse%2520MoEs.%2520Existing%2520dense-to-MoE%2520methods%2520fall%2520into%2520two%2520categories%253A%2520%255Ctextbf%257Bdynamic%2520structural%2520pruning%257D%2520that%2520converts%2520dense%2520models%2520into%2520MoE%2520architectures%2520with%2520moderate%2520sparsity%2520to%2520balance%2520performance%2520and%2520inference%2520efficiency%252C%2520and%2520%255Ctextbf%257Bdowncycling%257D%2520approaches%2520that%2520use%2520pretrained%2520dense%2520models%2520to%2520initialize%2520highly%2520sparse%2520MoE%2520architectures.%2520However%252C%2520existing%2520methods%2520break%2520the%2520intrinsic%2520activation%2520patterns%2520within%2520dense%2520models%252C%2520leading%2520to%2520suboptimal%2520expert%2520construction.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520the%2520Gated%2520Linear%2520Unit%2520%2528GLU%2529%2520mechanism%2520provides%2520a%2520natural%2520blueprint%2520for%2520dense-to-MoE%2520conversion.%2520We%2520show%2520that%2520the%2520fine-grained%2520neural-wise%2520activation%2520patterns%2520of%2520GLU%2520reveal%2520a%2520coarse-grained%2520structure%252C%2520uncovering%2520an%2520inherent%2520MoE%2520architecture%2520composed%2520of%2520consistently%2520activated%2520universal%2520neurons%2520and%2520dynamically%2520activated%2520specialized%2520neurons.%2520Leveraging%2520this%2520discovery%252C%2520we%2520introduce%2520ExpertWeaver%252C%2520a%2520training-free%2520framework%2520that%2520partitions%2520neurons%2520according%2520to%2520their%2520activation%2520patterns%2520and%2520constructs%2520shared%2520experts%2520and%2520specialized%2520routed%2520experts%2520with%2520layer-adaptive%2520configurations.%2520Our%2520experiments%2520demonstrate%2520that%2520ExpertWeaver%2520significantly%2520outperforms%2520existing%2520methods%252C%2520both%2520as%2520a%2520training-free%2520dynamic%2520structural%2520pruning%2520technique%2520and%2520as%2520a%2520downcycling%2520strategy%2520for%2520superior%2520MoE%2520initialization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExpertWeaver%3A%20Unlocking%20the%20Inherent%20MoE%20in%20Dense%20LLMs%20with%20GLU%20Activation%20Patterns&entry.906535625=Ziyu%20Zhao%20and%20Tong%20Zhu%20and%20Zhi%20Zhang%20and%20Tiantian%20Fan%20and%20Jinluan%20Yang%20and%20Kun%20Kuang%20and%20Zhongyu%20Wei%20and%20Fei%20Wu%20and%20Yu%20Cheng&entry.1292438233=Mixture-of-Experts%20%28MoE%29%20effectively%20scales%20model%20capacity%20while%20preserving%20computational%20efficiency%20through%20sparse%20expert%20activation.%20However%2C%20training%20high-quality%20MoEs%20from%20scratch%20is%20prohibitively%20expensive.%20A%20promising%20alternative%20is%20to%20convert%20pretrained%20dense%20models%20into%20sparse%20MoEs.%20Existing%20dense-to-MoE%20methods%20fall%20into%20two%20categories%3A%20%5Ctextbf%7Bdynamic%20structural%20pruning%7D%20that%20converts%20dense%20models%20into%20MoE%20architectures%20with%20moderate%20sparsity%20to%20balance%20performance%20and%20inference%20efficiency%2C%20and%20%5Ctextbf%7Bdowncycling%7D%20approaches%20that%20use%20pretrained%20dense%20models%20to%20initialize%20highly%20sparse%20MoE%20architectures.%20However%2C%20existing%20methods%20break%20the%20intrinsic%20activation%20patterns%20within%20dense%20models%2C%20leading%20to%20suboptimal%20expert%20construction.%20In%20this%20work%2C%20we%20argue%20that%20the%20Gated%20Linear%20Unit%20%28GLU%29%20mechanism%20provides%20a%20natural%20blueprint%20for%20dense-to-MoE%20conversion.%20We%20show%20that%20the%20fine-grained%20neural-wise%20activation%20patterns%20of%20GLU%20reveal%20a%20coarse-grained%20structure%2C%20uncovering%20an%20inherent%20MoE%20architecture%20composed%20of%20consistently%20activated%20universal%20neurons%20and%20dynamically%20activated%20specialized%20neurons.%20Leveraging%20this%20discovery%2C%20we%20introduce%20ExpertWeaver%2C%20a%20training-free%20framework%20that%20partitions%20neurons%20according%20to%20their%20activation%20patterns%20and%20constructs%20shared%20experts%20and%20specialized%20routed%20experts%20with%20layer-adaptive%20configurations.%20Our%20experiments%20demonstrate%20that%20ExpertWeaver%20significantly%20outperforms%20existing%20methods%2C%20both%20as%20a%20training-free%20dynamic%20structural%20pruning%20technique%20and%20as%20a%20downcycling%20strategy%20for%20superior%20MoE%20initialization.&entry.1838667208=http%3A//arxiv.org/abs/2602.15521v1&entry.124074799=Read"},
{"title": "Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation", "author": "Marco Salm\u00e8 and Federico Siciliano and Fabrizio Silvestri and Paolo Soda and Rosa Sicilia and Valerio Guarrasi", "abstract": "Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.", "link": "http://arxiv.org/abs/2602.15650v1", "date": "2026-02-17", "relevancy": 2.4779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept-Enhanced%20Multimodal%20RAG%3A%20Towards%20Interpretable%20and%20Accurate%20Radiology%20Report%20Generation&body=Title%3A%20Concept-Enhanced%20Multimodal%20RAG%3A%20Towards%20Interpretable%20and%20Accurate%20Radiology%20Report%20Generation%0AAuthor%3A%20Marco%20Salm%C3%A8%20and%20Federico%20Siciliano%20and%20Fabrizio%20Silvestri%20and%20Paolo%20Soda%20and%20Rosa%20Sicilia%20and%20Valerio%20Guarrasi%0AAbstract%3A%20Radiology%20Report%20Generation%20%28RRG%29%20through%20Vision-Language%20Models%20%28VLMs%29%20promises%20to%20reduce%20documentation%20burden%2C%20improve%20reporting%20consistency%2C%20and%20accelerate%20clinical%20workflows.%20However%2C%20their%20clinical%20adoption%20remains%20limited%20by%20the%20lack%20of%20interpretability%20and%20the%20tendency%20to%20hallucinate%20findings%20misaligned%20with%20imaging%20evidence.%20Existing%20research%20typically%20treats%20interpretability%20and%20accuracy%20as%20separate%20objectives%2C%20with%20concept-based%20explainability%20techniques%20focusing%20primarily%20on%20transparency%2C%20while%20Retrieval-Augmented%20Generation%20%28RAG%29%20methods%20targeting%20factual%20grounding%20through%20external%20retrieval.%20We%20present%20Concept-Enhanced%20Multimodal%20RAG%20%28CEMRAG%29%2C%20a%20unified%20framework%20that%20decomposes%20visual%20representations%20into%20interpretable%20clinical%20concepts%20and%20integrates%20them%20with%20multimodal%20RAG.%20This%20approach%20exploits%20enriched%20contextual%20prompts%20for%20RRG%2C%20improving%20both%20interpretability%20and%20factual%20accuracy.%20Experiments%20on%20MIMIC-CXR%20and%20IU%20X-Ray%20across%20multiple%20VLM%20architectures%2C%20training%20regimes%2C%20and%20retrieval%20configurations%20demonstrate%20consistent%20improvements%20over%20both%20conventional%20RAG%20and%20concept-only%20baselines%20on%20clinical%20accuracy%20metrics%20and%20standard%20NLP%20measures.%20These%20results%20challenge%20the%20assumed%20trade-off%20between%20interpretability%20and%20performance%2C%20showing%20that%20transparent%20visual%20concepts%20can%20enhance%20rather%20than%20compromise%20diagnostic%20accuracy%20in%20medical%20VLMs.%20Our%20modular%20design%20decomposes%20interpretability%20into%20visual%20transparency%20and%20structured%20language%20model%20conditioning%2C%20providing%20a%20principled%20pathway%20toward%20clinically%20trustworthy%20AI-assisted%20radiology.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept-Enhanced%2520Multimodal%2520RAG%253A%2520Towards%2520Interpretable%2520and%2520Accurate%2520Radiology%2520Report%2520Generation%26entry.906535625%3DMarco%2520Salm%25C3%25A8%2520and%2520Federico%2520Siciliano%2520and%2520Fabrizio%2520Silvestri%2520and%2520Paolo%2520Soda%2520and%2520Rosa%2520Sicilia%2520and%2520Valerio%2520Guarrasi%26entry.1292438233%3DRadiology%2520Report%2520Generation%2520%2528RRG%2529%2520through%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520promises%2520to%2520reduce%2520documentation%2520burden%252C%2520improve%2520reporting%2520consistency%252C%2520and%2520accelerate%2520clinical%2520workflows.%2520However%252C%2520their%2520clinical%2520adoption%2520remains%2520limited%2520by%2520the%2520lack%2520of%2520interpretability%2520and%2520the%2520tendency%2520to%2520hallucinate%2520findings%2520misaligned%2520with%2520imaging%2520evidence.%2520Existing%2520research%2520typically%2520treats%2520interpretability%2520and%2520accuracy%2520as%2520separate%2520objectives%252C%2520with%2520concept-based%2520explainability%2520techniques%2520focusing%2520primarily%2520on%2520transparency%252C%2520while%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520methods%2520targeting%2520factual%2520grounding%2520through%2520external%2520retrieval.%2520We%2520present%2520Concept-Enhanced%2520Multimodal%2520RAG%2520%2528CEMRAG%2529%252C%2520a%2520unified%2520framework%2520that%2520decomposes%2520visual%2520representations%2520into%2520interpretable%2520clinical%2520concepts%2520and%2520integrates%2520them%2520with%2520multimodal%2520RAG.%2520This%2520approach%2520exploits%2520enriched%2520contextual%2520prompts%2520for%2520RRG%252C%2520improving%2520both%2520interpretability%2520and%2520factual%2520accuracy.%2520Experiments%2520on%2520MIMIC-CXR%2520and%2520IU%2520X-Ray%2520across%2520multiple%2520VLM%2520architectures%252C%2520training%2520regimes%252C%2520and%2520retrieval%2520configurations%2520demonstrate%2520consistent%2520improvements%2520over%2520both%2520conventional%2520RAG%2520and%2520concept-only%2520baselines%2520on%2520clinical%2520accuracy%2520metrics%2520and%2520standard%2520NLP%2520measures.%2520These%2520results%2520challenge%2520the%2520assumed%2520trade-off%2520between%2520interpretability%2520and%2520performance%252C%2520showing%2520that%2520transparent%2520visual%2520concepts%2520can%2520enhance%2520rather%2520than%2520compromise%2520diagnostic%2520accuracy%2520in%2520medical%2520VLMs.%2520Our%2520modular%2520design%2520decomposes%2520interpretability%2520into%2520visual%2520transparency%2520and%2520structured%2520language%2520model%2520conditioning%252C%2520providing%2520a%2520principled%2520pathway%2520toward%2520clinically%2520trustworthy%2520AI-assisted%2520radiology.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept-Enhanced%20Multimodal%20RAG%3A%20Towards%20Interpretable%20and%20Accurate%20Radiology%20Report%20Generation&entry.906535625=Marco%20Salm%C3%A8%20and%20Federico%20Siciliano%20and%20Fabrizio%20Silvestri%20and%20Paolo%20Soda%20and%20Rosa%20Sicilia%20and%20Valerio%20Guarrasi&entry.1292438233=Radiology%20Report%20Generation%20%28RRG%29%20through%20Vision-Language%20Models%20%28VLMs%29%20promises%20to%20reduce%20documentation%20burden%2C%20improve%20reporting%20consistency%2C%20and%20accelerate%20clinical%20workflows.%20However%2C%20their%20clinical%20adoption%20remains%20limited%20by%20the%20lack%20of%20interpretability%20and%20the%20tendency%20to%20hallucinate%20findings%20misaligned%20with%20imaging%20evidence.%20Existing%20research%20typically%20treats%20interpretability%20and%20accuracy%20as%20separate%20objectives%2C%20with%20concept-based%20explainability%20techniques%20focusing%20primarily%20on%20transparency%2C%20while%20Retrieval-Augmented%20Generation%20%28RAG%29%20methods%20targeting%20factual%20grounding%20through%20external%20retrieval.%20We%20present%20Concept-Enhanced%20Multimodal%20RAG%20%28CEMRAG%29%2C%20a%20unified%20framework%20that%20decomposes%20visual%20representations%20into%20interpretable%20clinical%20concepts%20and%20integrates%20them%20with%20multimodal%20RAG.%20This%20approach%20exploits%20enriched%20contextual%20prompts%20for%20RRG%2C%20improving%20both%20interpretability%20and%20factual%20accuracy.%20Experiments%20on%20MIMIC-CXR%20and%20IU%20X-Ray%20across%20multiple%20VLM%20architectures%2C%20training%20regimes%2C%20and%20retrieval%20configurations%20demonstrate%20consistent%20improvements%20over%20both%20conventional%20RAG%20and%20concept-only%20baselines%20on%20clinical%20accuracy%20metrics%20and%20standard%20NLP%20measures.%20These%20results%20challenge%20the%20assumed%20trade-off%20between%20interpretability%20and%20performance%2C%20showing%20that%20transparent%20visual%20concepts%20can%20enhance%20rather%20than%20compromise%20diagnostic%20accuracy%20in%20medical%20VLMs.%20Our%20modular%20design%20decomposes%20interpretability%20into%20visual%20transparency%20and%20structured%20language%20model%20conditioning%2C%20providing%20a%20principled%20pathway%20toward%20clinically%20trustworthy%20AI-assisted%20radiology.&entry.1838667208=http%3A//arxiv.org/abs/2602.15650v1&entry.124074799=Read"},
{"title": "LogiPart: Local Large Language Models for Data Exploration at Scale with Logical Partitioning", "author": "Tiago Fernandes Tavares", "abstract": "The discovery of deep, steerable taxonomies in large text corpora is currently restricted by a trade-off between the surface-level efficiency of topic models and the prohibitive, non-scalable assignment costs of LLM-integrated frameworks. We introduce \\textbf{LogiPart}, a scalable, hypothesis-first framework for building interpretable hierarchical partitions that decouples hierarchy growth from expensive full-corpus LLM conditioning. LogiPart utilizes locally hosted LLMs on compact, embedding-aware samples to generate concise natural-language taxonomic predicates. These predicates are then evaluated efficiently across the entire corpus using zero-shot Natural Language Inference (NLI) combined with fast graph-based label propagation, achieving constant $O(1)$ generative token complexity per node relative to corpus size. We evaluate LogiPart across four diverse text corpora (totaling $\\approx$140,000 documents). Using structured manifolds for \\textbf{calibration}, we identify an empirical reasoning threshold at the 14B-parameter scale required for stable semantic grounding. On complex, high-entropy corpora (Wikipedia, US Bills), where traditional thematic metrics reveal an ``alignment gap,'' inverse logic validation confirms the stability of the induced logic, with individual taxonomic bisections maintaining an average per-node routing accuracy of up to 96\\%. A qualitative audit by an independent LLM-as-a-judge confirms the discovery of meaningful functional axes, such as policy intent, that thematic ground-truth labels fail to capture. LogiPart enables frontier-level exploratory analysis on consumer-grade hardware, making hypothesis-driven taxonomic discovery feasible under realistic computational and governance constraints.", "link": "http://arxiv.org/abs/2509.22211v3", "date": "2026-02-17", "relevancy": 2.4185, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4914}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LogiPart%3A%20Local%20Large%20Language%20Models%20for%20Data%20Exploration%20at%20Scale%20with%20Logical%20Partitioning&body=Title%3A%20LogiPart%3A%20Local%20Large%20Language%20Models%20for%20Data%20Exploration%20at%20Scale%20with%20Logical%20Partitioning%0AAuthor%3A%20Tiago%20Fernandes%20Tavares%0AAbstract%3A%20The%20discovery%20of%20deep%2C%20steerable%20taxonomies%20in%20large%20text%20corpora%20is%20currently%20restricted%20by%20a%20trade-off%20between%20the%20surface-level%20efficiency%20of%20topic%20models%20and%20the%20prohibitive%2C%20non-scalable%20assignment%20costs%20of%20LLM-integrated%20frameworks.%20We%20introduce%20%5Ctextbf%7BLogiPart%7D%2C%20a%20scalable%2C%20hypothesis-first%20framework%20for%20building%20interpretable%20hierarchical%20partitions%20that%20decouples%20hierarchy%20growth%20from%20expensive%20full-corpus%20LLM%20conditioning.%20LogiPart%20utilizes%20locally%20hosted%20LLMs%20on%20compact%2C%20embedding-aware%20samples%20to%20generate%20concise%20natural-language%20taxonomic%20predicates.%20These%20predicates%20are%20then%20evaluated%20efficiently%20across%20the%20entire%20corpus%20using%20zero-shot%20Natural%20Language%20Inference%20%28NLI%29%20combined%20with%20fast%20graph-based%20label%20propagation%2C%20achieving%20constant%20%24O%281%29%24%20generative%20token%20complexity%20per%20node%20relative%20to%20corpus%20size.%20We%20evaluate%20LogiPart%20across%20four%20diverse%20text%20corpora%20%28totaling%20%24%5Capprox%24140%2C000%20documents%29.%20Using%20structured%20manifolds%20for%20%5Ctextbf%7Bcalibration%7D%2C%20we%20identify%20an%20empirical%20reasoning%20threshold%20at%20the%2014B-parameter%20scale%20required%20for%20stable%20semantic%20grounding.%20On%20complex%2C%20high-entropy%20corpora%20%28Wikipedia%2C%20US%20Bills%29%2C%20where%20traditional%20thematic%20metrics%20reveal%20an%20%60%60alignment%20gap%2C%27%27%20inverse%20logic%20validation%20confirms%20the%20stability%20of%20the%20induced%20logic%2C%20with%20individual%20taxonomic%20bisections%20maintaining%20an%20average%20per-node%20routing%20accuracy%20of%20up%20to%2096%5C%25.%20A%20qualitative%20audit%20by%20an%20independent%20LLM-as-a-judge%20confirms%20the%20discovery%20of%20meaningful%20functional%20axes%2C%20such%20as%20policy%20intent%2C%20that%20thematic%20ground-truth%20labels%20fail%20to%20capture.%20LogiPart%20enables%20frontier-level%20exploratory%20analysis%20on%20consumer-grade%20hardware%2C%20making%20hypothesis-driven%20taxonomic%20discovery%20feasible%20under%20realistic%20computational%20and%20governance%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22211v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogiPart%253A%2520Local%2520Large%2520Language%2520Models%2520for%2520Data%2520Exploration%2520at%2520Scale%2520with%2520Logical%2520Partitioning%26entry.906535625%3DTiago%2520Fernandes%2520Tavares%26entry.1292438233%3DThe%2520discovery%2520of%2520deep%252C%2520steerable%2520taxonomies%2520in%2520large%2520text%2520corpora%2520is%2520currently%2520restricted%2520by%2520a%2520trade-off%2520between%2520the%2520surface-level%2520efficiency%2520of%2520topic%2520models%2520and%2520the%2520prohibitive%252C%2520non-scalable%2520assignment%2520costs%2520of%2520LLM-integrated%2520frameworks.%2520We%2520introduce%2520%255Ctextbf%257BLogiPart%257D%252C%2520a%2520scalable%252C%2520hypothesis-first%2520framework%2520for%2520building%2520interpretable%2520hierarchical%2520partitions%2520that%2520decouples%2520hierarchy%2520growth%2520from%2520expensive%2520full-corpus%2520LLM%2520conditioning.%2520LogiPart%2520utilizes%2520locally%2520hosted%2520LLMs%2520on%2520compact%252C%2520embedding-aware%2520samples%2520to%2520generate%2520concise%2520natural-language%2520taxonomic%2520predicates.%2520These%2520predicates%2520are%2520then%2520evaluated%2520efficiently%2520across%2520the%2520entire%2520corpus%2520using%2520zero-shot%2520Natural%2520Language%2520Inference%2520%2528NLI%2529%2520combined%2520with%2520fast%2520graph-based%2520label%2520propagation%252C%2520achieving%2520constant%2520%2524O%25281%2529%2524%2520generative%2520token%2520complexity%2520per%2520node%2520relative%2520to%2520corpus%2520size.%2520We%2520evaluate%2520LogiPart%2520across%2520four%2520diverse%2520text%2520corpora%2520%2528totaling%2520%2524%255Capprox%2524140%252C000%2520documents%2529.%2520Using%2520structured%2520manifolds%2520for%2520%255Ctextbf%257Bcalibration%257D%252C%2520we%2520identify%2520an%2520empirical%2520reasoning%2520threshold%2520at%2520the%252014B-parameter%2520scale%2520required%2520for%2520stable%2520semantic%2520grounding.%2520On%2520complex%252C%2520high-entropy%2520corpora%2520%2528Wikipedia%252C%2520US%2520Bills%2529%252C%2520where%2520traditional%2520thematic%2520metrics%2520reveal%2520an%2520%2560%2560alignment%2520gap%252C%2527%2527%2520inverse%2520logic%2520validation%2520confirms%2520the%2520stability%2520of%2520the%2520induced%2520logic%252C%2520with%2520individual%2520taxonomic%2520bisections%2520maintaining%2520an%2520average%2520per-node%2520routing%2520accuracy%2520of%2520up%2520to%252096%255C%2525.%2520A%2520qualitative%2520audit%2520by%2520an%2520independent%2520LLM-as-a-judge%2520confirms%2520the%2520discovery%2520of%2520meaningful%2520functional%2520axes%252C%2520such%2520as%2520policy%2520intent%252C%2520that%2520thematic%2520ground-truth%2520labels%2520fail%2520to%2520capture.%2520LogiPart%2520enables%2520frontier-level%2520exploratory%2520analysis%2520on%2520consumer-grade%2520hardware%252C%2520making%2520hypothesis-driven%2520taxonomic%2520discovery%2520feasible%2520under%2520realistic%2520computational%2520and%2520governance%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22211v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LogiPart%3A%20Local%20Large%20Language%20Models%20for%20Data%20Exploration%20at%20Scale%20with%20Logical%20Partitioning&entry.906535625=Tiago%20Fernandes%20Tavares&entry.1292438233=The%20discovery%20of%20deep%2C%20steerable%20taxonomies%20in%20large%20text%20corpora%20is%20currently%20restricted%20by%20a%20trade-off%20between%20the%20surface-level%20efficiency%20of%20topic%20models%20and%20the%20prohibitive%2C%20non-scalable%20assignment%20costs%20of%20LLM-integrated%20frameworks.%20We%20introduce%20%5Ctextbf%7BLogiPart%7D%2C%20a%20scalable%2C%20hypothesis-first%20framework%20for%20building%20interpretable%20hierarchical%20partitions%20that%20decouples%20hierarchy%20growth%20from%20expensive%20full-corpus%20LLM%20conditioning.%20LogiPart%20utilizes%20locally%20hosted%20LLMs%20on%20compact%2C%20embedding-aware%20samples%20to%20generate%20concise%20natural-language%20taxonomic%20predicates.%20These%20predicates%20are%20then%20evaluated%20efficiently%20across%20the%20entire%20corpus%20using%20zero-shot%20Natural%20Language%20Inference%20%28NLI%29%20combined%20with%20fast%20graph-based%20label%20propagation%2C%20achieving%20constant%20%24O%281%29%24%20generative%20token%20complexity%20per%20node%20relative%20to%20corpus%20size.%20We%20evaluate%20LogiPart%20across%20four%20diverse%20text%20corpora%20%28totaling%20%24%5Capprox%24140%2C000%20documents%29.%20Using%20structured%20manifolds%20for%20%5Ctextbf%7Bcalibration%7D%2C%20we%20identify%20an%20empirical%20reasoning%20threshold%20at%20the%2014B-parameter%20scale%20required%20for%20stable%20semantic%20grounding.%20On%20complex%2C%20high-entropy%20corpora%20%28Wikipedia%2C%20US%20Bills%29%2C%20where%20traditional%20thematic%20metrics%20reveal%20an%20%60%60alignment%20gap%2C%27%27%20inverse%20logic%20validation%20confirms%20the%20stability%20of%20the%20induced%20logic%2C%20with%20individual%20taxonomic%20bisections%20maintaining%20an%20average%20per-node%20routing%20accuracy%20of%20up%20to%2096%5C%25.%20A%20qualitative%20audit%20by%20an%20independent%20LLM-as-a-judge%20confirms%20the%20discovery%20of%20meaningful%20functional%20axes%2C%20such%20as%20policy%20intent%2C%20that%20thematic%20ground-truth%20labels%20fail%20to%20capture.%20LogiPart%20enables%20frontier-level%20exploratory%20analysis%20on%20consumer-grade%20hardware%2C%20making%20hypothesis-driven%20taxonomic%20discovery%20feasible%20under%20realistic%20computational%20and%20governance%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2509.22211v3&entry.124074799=Read"},
{"title": "Multispectral airborne laser scanning for tree species classification: a benchmark of machine learning and deep learning algorithms", "author": "Josef Taher and Eric Hyypp\u00e4 and Matti Hyypp\u00e4 and Klaara Salolahti and Xiaowei Yu and Leena Matikainen and Antero Kukko and Matti Lehtom\u00e4ki and Harri Kaartinen and Sopitta Thurachen and Paula Litkey and Ville Luoma and Markus Holopainen and Gefei Kong and Hongchao Fan and Petri R\u00f6nnholm and Matti Vaaja and Antti Polvivaara and Samuli Junttila and Mikko Vastaranta and Stefano Puliti and Rasmus Astrup and Joel Kostensalo and Mari Myllym\u00e4ki and Maksymilian Kulicki and Krzysztof Stere\u0144czak and Raul de Paula Pires and Ruben Valbuena and Juan Pedro Carbonell-Rivera and Jes\u00fas Torralba and Yi-Chen Chen and Lukas Winiwarter and Markus Hollaus and Gottfried Mandlburger and Narges Takhtkeshha and Fabio Remondino and Maciej Lisiewicz and Bart\u0142omiej Kraszewski and Xinlian Liang and Jianchang Chen and Eero Ahokas and Kirsi Karila and Eugeniu Vezeteu and Petri Manninen and Roope N\u00e4si and Heikki Hyyti and Siiri Pyykk\u00f6nen and Peilun Hu and Juha Hyypp\u00e4", "abstract": "Climate-smart and biodiversity-preserving forestry demands precise information on forest resources, extending to the individual tree level. Multispectral airborne laser scanning (ALS) has shown promise in automated point cloud processing, but challenges remain in leveraging deep learning techniques and identifying rare tree species in class-imbalanced datasets. This study addresses these gaps by conducting a comprehensive benchmark of deep learning and traditional shallow machine learning methods for tree species classification. For the study, we collected high-density multispectral ALS data ($>1000$ $\\mathrm{pts}/\\mathrm{m}^2$) at three wavelengths using the FGI-developed HeliALS system, complemented by existing Optech Titan data (35 $\\mathrm{pts}/\\mathrm{m}^2$), to evaluate the species classification accuracy of various algorithms in a peri-urban study area located in southern Finland. We established a field reference dataset of 6326 segments across nine species using a newly developed browser-based crowdsourcing tool, which facilitated efficient data annotation. The ALS data, including a training dataset of 1065 segments, was shared with the scientific community to foster collaborative research and diverse algorithmic contributions. Based on 5261 test segments, our findings demonstrate that point-based deep learning methods, particularly a point transformer model, outperformed traditional machine learning and image-based deep learning approaches on high-density multispectral point clouds. For the high-density ALS dataset, a point transformer model provided the best performance reaching an overall (macro-average) accuracy of 87.9% (74.5%) with a training set of 1065 segments and 92.0% (85.1%) with a larger training set of 5000 segments.", "link": "http://arxiv.org/abs/2504.14337v2", "date": "2026-02-17", "relevancy": 2.4145, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5153}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4718}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multispectral%20airborne%20laser%20scanning%20for%20tree%20species%20classification%3A%20a%20benchmark%20of%20machine%20learning%20and%20deep%20learning%20algorithms&body=Title%3A%20Multispectral%20airborne%20laser%20scanning%20for%20tree%20species%20classification%3A%20a%20benchmark%20of%20machine%20learning%20and%20deep%20learning%20algorithms%0AAuthor%3A%20Josef%20Taher%20and%20Eric%20Hyypp%C3%A4%20and%20Matti%20Hyypp%C3%A4%20and%20Klaara%20Salolahti%20and%20Xiaowei%20Yu%20and%20Leena%20Matikainen%20and%20Antero%20Kukko%20and%20Matti%20Lehtom%C3%A4ki%20and%20Harri%20Kaartinen%20and%20Sopitta%20Thurachen%20and%20Paula%20Litkey%20and%20Ville%20Luoma%20and%20Markus%20Holopainen%20and%20Gefei%20Kong%20and%20Hongchao%20Fan%20and%20Petri%20R%C3%B6nnholm%20and%20Matti%20Vaaja%20and%20Antti%20Polvivaara%20and%20Samuli%20Junttila%20and%20Mikko%20Vastaranta%20and%20Stefano%20Puliti%20and%20Rasmus%20Astrup%20and%20Joel%20Kostensalo%20and%20Mari%20Myllym%C3%A4ki%20and%20Maksymilian%20Kulicki%20and%20Krzysztof%20Stere%C5%84czak%20and%20Raul%20de%20Paula%20Pires%20and%20Ruben%20Valbuena%20and%20Juan%20Pedro%20Carbonell-Rivera%20and%20Jes%C3%BAs%20Torralba%20and%20Yi-Chen%20Chen%20and%20Lukas%20Winiwarter%20and%20Markus%20Hollaus%20and%20Gottfried%20Mandlburger%20and%20Narges%20Takhtkeshha%20and%20Fabio%20Remondino%20and%20Maciej%20Lisiewicz%20and%20Bart%C5%82omiej%20Kraszewski%20and%20Xinlian%20Liang%20and%20Jianchang%20Chen%20and%20Eero%20Ahokas%20and%20Kirsi%20Karila%20and%20Eugeniu%20Vezeteu%20and%20Petri%20Manninen%20and%20Roope%20N%C3%A4si%20and%20Heikki%20Hyyti%20and%20Siiri%20Pyykk%C3%B6nen%20and%20Peilun%20Hu%20and%20Juha%20Hyypp%C3%A4%0AAbstract%3A%20Climate-smart%20and%20biodiversity-preserving%20forestry%20demands%20precise%20information%20on%20forest%20resources%2C%20extending%20to%20the%20individual%20tree%20level.%20Multispectral%20airborne%20laser%20scanning%20%28ALS%29%20has%20shown%20promise%20in%20automated%20point%20cloud%20processing%2C%20but%20challenges%20remain%20in%20leveraging%20deep%20learning%20techniques%20and%20identifying%20rare%20tree%20species%20in%20class-imbalanced%20datasets.%20This%20study%20addresses%20these%20gaps%20by%20conducting%20a%20comprehensive%20benchmark%20of%20deep%20learning%20and%20traditional%20shallow%20machine%20learning%20methods%20for%20tree%20species%20classification.%20For%20the%20study%2C%20we%20collected%20high-density%20multispectral%20ALS%20data%20%28%24%3E1000%24%20%24%5Cmathrm%7Bpts%7D/%5Cmathrm%7Bm%7D%5E2%24%29%20at%20three%20wavelengths%20using%20the%20FGI-developed%20HeliALS%20system%2C%20complemented%20by%20existing%20Optech%20Titan%20data%20%2835%20%24%5Cmathrm%7Bpts%7D/%5Cmathrm%7Bm%7D%5E2%24%29%2C%20to%20evaluate%20the%20species%20classification%20accuracy%20of%20various%20algorithms%20in%20a%20peri-urban%20study%20area%20located%20in%20southern%20Finland.%20We%20established%20a%20field%20reference%20dataset%20of%206326%20segments%20across%20nine%20species%20using%20a%20newly%20developed%20browser-based%20crowdsourcing%20tool%2C%20which%20facilitated%20efficient%20data%20annotation.%20The%20ALS%20data%2C%20including%20a%20training%20dataset%20of%201065%20segments%2C%20was%20shared%20with%20the%20scientific%20community%20to%20foster%20collaborative%20research%20and%20diverse%20algorithmic%20contributions.%20Based%20on%205261%20test%20segments%2C%20our%20findings%20demonstrate%20that%20point-based%20deep%20learning%20methods%2C%20particularly%20a%20point%20transformer%20model%2C%20outperformed%20traditional%20machine%20learning%20and%20image-based%20deep%20learning%20approaches%20on%20high-density%20multispectral%20point%20clouds.%20For%20the%20high-density%20ALS%20dataset%2C%20a%20point%20transformer%20model%20provided%20the%20best%20performance%20reaching%20an%20overall%20%28macro-average%29%20accuracy%20of%2087.9%25%20%2874.5%25%29%20with%20a%20training%20set%20of%201065%20segments%20and%2092.0%25%20%2885.1%25%29%20with%20a%20larger%20training%20set%20of%205000%20segments.%0ALink%3A%20http%3A//arxiv.org/abs/2504.14337v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultispectral%2520airborne%2520laser%2520scanning%2520for%2520tree%2520species%2520classification%253A%2520a%2520benchmark%2520of%2520machine%2520learning%2520and%2520deep%2520learning%2520algorithms%26entry.906535625%3DJosef%2520Taher%2520and%2520Eric%2520Hyypp%25C3%25A4%2520and%2520Matti%2520Hyypp%25C3%25A4%2520and%2520Klaara%2520Salolahti%2520and%2520Xiaowei%2520Yu%2520and%2520Leena%2520Matikainen%2520and%2520Antero%2520Kukko%2520and%2520Matti%2520Lehtom%25C3%25A4ki%2520and%2520Harri%2520Kaartinen%2520and%2520Sopitta%2520Thurachen%2520and%2520Paula%2520Litkey%2520and%2520Ville%2520Luoma%2520and%2520Markus%2520Holopainen%2520and%2520Gefei%2520Kong%2520and%2520Hongchao%2520Fan%2520and%2520Petri%2520R%25C3%25B6nnholm%2520and%2520Matti%2520Vaaja%2520and%2520Antti%2520Polvivaara%2520and%2520Samuli%2520Junttila%2520and%2520Mikko%2520Vastaranta%2520and%2520Stefano%2520Puliti%2520and%2520Rasmus%2520Astrup%2520and%2520Joel%2520Kostensalo%2520and%2520Mari%2520Myllym%25C3%25A4ki%2520and%2520Maksymilian%2520Kulicki%2520and%2520Krzysztof%2520Stere%25C5%2584czak%2520and%2520Raul%2520de%2520Paula%2520Pires%2520and%2520Ruben%2520Valbuena%2520and%2520Juan%2520Pedro%2520Carbonell-Rivera%2520and%2520Jes%25C3%25BAs%2520Torralba%2520and%2520Yi-Chen%2520Chen%2520and%2520Lukas%2520Winiwarter%2520and%2520Markus%2520Hollaus%2520and%2520Gottfried%2520Mandlburger%2520and%2520Narges%2520Takhtkeshha%2520and%2520Fabio%2520Remondino%2520and%2520Maciej%2520Lisiewicz%2520and%2520Bart%25C5%2582omiej%2520Kraszewski%2520and%2520Xinlian%2520Liang%2520and%2520Jianchang%2520Chen%2520and%2520Eero%2520Ahokas%2520and%2520Kirsi%2520Karila%2520and%2520Eugeniu%2520Vezeteu%2520and%2520Petri%2520Manninen%2520and%2520Roope%2520N%25C3%25A4si%2520and%2520Heikki%2520Hyyti%2520and%2520Siiri%2520Pyykk%25C3%25B6nen%2520and%2520Peilun%2520Hu%2520and%2520Juha%2520Hyypp%25C3%25A4%26entry.1292438233%3DClimate-smart%2520and%2520biodiversity-preserving%2520forestry%2520demands%2520precise%2520information%2520on%2520forest%2520resources%252C%2520extending%2520to%2520the%2520individual%2520tree%2520level.%2520Multispectral%2520airborne%2520laser%2520scanning%2520%2528ALS%2529%2520has%2520shown%2520promise%2520in%2520automated%2520point%2520cloud%2520processing%252C%2520but%2520challenges%2520remain%2520in%2520leveraging%2520deep%2520learning%2520techniques%2520and%2520identifying%2520rare%2520tree%2520species%2520in%2520class-imbalanced%2520datasets.%2520This%2520study%2520addresses%2520these%2520gaps%2520by%2520conducting%2520a%2520comprehensive%2520benchmark%2520of%2520deep%2520learning%2520and%2520traditional%2520shallow%2520machine%2520learning%2520methods%2520for%2520tree%2520species%2520classification.%2520For%2520the%2520study%252C%2520we%2520collected%2520high-density%2520multispectral%2520ALS%2520data%2520%2528%2524%253E1000%2524%2520%2524%255Cmathrm%257Bpts%257D/%255Cmathrm%257Bm%257D%255E2%2524%2529%2520at%2520three%2520wavelengths%2520using%2520the%2520FGI-developed%2520HeliALS%2520system%252C%2520complemented%2520by%2520existing%2520Optech%2520Titan%2520data%2520%252835%2520%2524%255Cmathrm%257Bpts%257D/%255Cmathrm%257Bm%257D%255E2%2524%2529%252C%2520to%2520evaluate%2520the%2520species%2520classification%2520accuracy%2520of%2520various%2520algorithms%2520in%2520a%2520peri-urban%2520study%2520area%2520located%2520in%2520southern%2520Finland.%2520We%2520established%2520a%2520field%2520reference%2520dataset%2520of%25206326%2520segments%2520across%2520nine%2520species%2520using%2520a%2520newly%2520developed%2520browser-based%2520crowdsourcing%2520tool%252C%2520which%2520facilitated%2520efficient%2520data%2520annotation.%2520The%2520ALS%2520data%252C%2520including%2520a%2520training%2520dataset%2520of%25201065%2520segments%252C%2520was%2520shared%2520with%2520the%2520scientific%2520community%2520to%2520foster%2520collaborative%2520research%2520and%2520diverse%2520algorithmic%2520contributions.%2520Based%2520on%25205261%2520test%2520segments%252C%2520our%2520findings%2520demonstrate%2520that%2520point-based%2520deep%2520learning%2520methods%252C%2520particularly%2520a%2520point%2520transformer%2520model%252C%2520outperformed%2520traditional%2520machine%2520learning%2520and%2520image-based%2520deep%2520learning%2520approaches%2520on%2520high-density%2520multispectral%2520point%2520clouds.%2520For%2520the%2520high-density%2520ALS%2520dataset%252C%2520a%2520point%2520transformer%2520model%2520provided%2520the%2520best%2520performance%2520reaching%2520an%2520overall%2520%2528macro-average%2529%2520accuracy%2520of%252087.9%2525%2520%252874.5%2525%2529%2520with%2520a%2520training%2520set%2520of%25201065%2520segments%2520and%252092.0%2525%2520%252885.1%2525%2529%2520with%2520a%2520larger%2520training%2520set%2520of%25205000%2520segments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14337v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multispectral%20airborne%20laser%20scanning%20for%20tree%20species%20classification%3A%20a%20benchmark%20of%20machine%20learning%20and%20deep%20learning%20algorithms&entry.906535625=Josef%20Taher%20and%20Eric%20Hyypp%C3%A4%20and%20Matti%20Hyypp%C3%A4%20and%20Klaara%20Salolahti%20and%20Xiaowei%20Yu%20and%20Leena%20Matikainen%20and%20Antero%20Kukko%20and%20Matti%20Lehtom%C3%A4ki%20and%20Harri%20Kaartinen%20and%20Sopitta%20Thurachen%20and%20Paula%20Litkey%20and%20Ville%20Luoma%20and%20Markus%20Holopainen%20and%20Gefei%20Kong%20and%20Hongchao%20Fan%20and%20Petri%20R%C3%B6nnholm%20and%20Matti%20Vaaja%20and%20Antti%20Polvivaara%20and%20Samuli%20Junttila%20and%20Mikko%20Vastaranta%20and%20Stefano%20Puliti%20and%20Rasmus%20Astrup%20and%20Joel%20Kostensalo%20and%20Mari%20Myllym%C3%A4ki%20and%20Maksymilian%20Kulicki%20and%20Krzysztof%20Stere%C5%84czak%20and%20Raul%20de%20Paula%20Pires%20and%20Ruben%20Valbuena%20and%20Juan%20Pedro%20Carbonell-Rivera%20and%20Jes%C3%BAs%20Torralba%20and%20Yi-Chen%20Chen%20and%20Lukas%20Winiwarter%20and%20Markus%20Hollaus%20and%20Gottfried%20Mandlburger%20and%20Narges%20Takhtkeshha%20and%20Fabio%20Remondino%20and%20Maciej%20Lisiewicz%20and%20Bart%C5%82omiej%20Kraszewski%20and%20Xinlian%20Liang%20and%20Jianchang%20Chen%20and%20Eero%20Ahokas%20and%20Kirsi%20Karila%20and%20Eugeniu%20Vezeteu%20and%20Petri%20Manninen%20and%20Roope%20N%C3%A4si%20and%20Heikki%20Hyyti%20and%20Siiri%20Pyykk%C3%B6nen%20and%20Peilun%20Hu%20and%20Juha%20Hyypp%C3%A4&entry.1292438233=Climate-smart%20and%20biodiversity-preserving%20forestry%20demands%20precise%20information%20on%20forest%20resources%2C%20extending%20to%20the%20individual%20tree%20level.%20Multispectral%20airborne%20laser%20scanning%20%28ALS%29%20has%20shown%20promise%20in%20automated%20point%20cloud%20processing%2C%20but%20challenges%20remain%20in%20leveraging%20deep%20learning%20techniques%20and%20identifying%20rare%20tree%20species%20in%20class-imbalanced%20datasets.%20This%20study%20addresses%20these%20gaps%20by%20conducting%20a%20comprehensive%20benchmark%20of%20deep%20learning%20and%20traditional%20shallow%20machine%20learning%20methods%20for%20tree%20species%20classification.%20For%20the%20study%2C%20we%20collected%20high-density%20multispectral%20ALS%20data%20%28%24%3E1000%24%20%24%5Cmathrm%7Bpts%7D/%5Cmathrm%7Bm%7D%5E2%24%29%20at%20three%20wavelengths%20using%20the%20FGI-developed%20HeliALS%20system%2C%20complemented%20by%20existing%20Optech%20Titan%20data%20%2835%20%24%5Cmathrm%7Bpts%7D/%5Cmathrm%7Bm%7D%5E2%24%29%2C%20to%20evaluate%20the%20species%20classification%20accuracy%20of%20various%20algorithms%20in%20a%20peri-urban%20study%20area%20located%20in%20southern%20Finland.%20We%20established%20a%20field%20reference%20dataset%20of%206326%20segments%20across%20nine%20species%20using%20a%20newly%20developed%20browser-based%20crowdsourcing%20tool%2C%20which%20facilitated%20efficient%20data%20annotation.%20The%20ALS%20data%2C%20including%20a%20training%20dataset%20of%201065%20segments%2C%20was%20shared%20with%20the%20scientific%20community%20to%20foster%20collaborative%20research%20and%20diverse%20algorithmic%20contributions.%20Based%20on%205261%20test%20segments%2C%20our%20findings%20demonstrate%20that%20point-based%20deep%20learning%20methods%2C%20particularly%20a%20point%20transformer%20model%2C%20outperformed%20traditional%20machine%20learning%20and%20image-based%20deep%20learning%20approaches%20on%20high-density%20multispectral%20point%20clouds.%20For%20the%20high-density%20ALS%20dataset%2C%20a%20point%20transformer%20model%20provided%20the%20best%20performance%20reaching%20an%20overall%20%28macro-average%29%20accuracy%20of%2087.9%25%20%2874.5%25%29%20with%20a%20training%20set%20of%201065%20segments%20and%2092.0%25%20%2885.1%25%29%20with%20a%20larger%20training%20set%20of%205000%20segments.&entry.1838667208=http%3A//arxiv.org/abs/2504.14337v2&entry.124074799=Read"},
{"title": "Intermittent Semi-Working Mask: A New Masking Paradigm for LLMs", "author": "HaoYuan Hu and Mingcong Lu and Di Luo and XinYa Wu and Jiangcai Zhu and Taoye Yin and Zheng Li and Hao Wang and Shusheng Zhang and KeZun Zhang and KaiLai Shao and Chao Chen and Feng Wang", "abstract": "Multi-turn dialogues and context-intensive tasks challenge Large Language Models (LLMs) to integrate long histories without sacrificing generation quality. Although prefix LLMs can better exploit historical context via bidirectional attention on prefix tokens, they are rarely used in practice because multi-turn training requires many duplicated triplets, and its bidirectional prefix prevents KV-cache reuse at inference time, driving up high cost and latency. To retain the contextual understanding of prefix mask while preserving the inference-time efficiency of causal mask, we introduce Intermittent Semi-working Mask (ISM), a masking scheme that injects sparse bidirectional attention into the causal backbone. ISM alternates bidirectional attention over query segments with unidirectional attention over answer segments, enabling the synthesis of in-context while preserving global causality. This design eliminates triplet expansion during training and maintains KV-cache reuse during inference, yielding latency comparable to standard causal LLMs. ISM is architecture-agnostic and parameter-free, adding only minimal latency. Across extensive evaluations, ISM outperforms causal baselines not only on multi-turn dialogue, but also on context-intensive tasks like mathematical reasoning.", "link": "http://arxiv.org/abs/2408.00539v2", "date": "2026-02-17", "relevancy": 2.3841, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intermittent%20Semi-Working%20Mask%3A%20A%20New%20Masking%20Paradigm%20for%20LLMs&body=Title%3A%20Intermittent%20Semi-Working%20Mask%3A%20A%20New%20Masking%20Paradigm%20for%20LLMs%0AAuthor%3A%20HaoYuan%20Hu%20and%20Mingcong%20Lu%20and%20Di%20Luo%20and%20XinYa%20Wu%20and%20Jiangcai%20Zhu%20and%20Taoye%20Yin%20and%20Zheng%20Li%20and%20Hao%20Wang%20and%20Shusheng%20Zhang%20and%20KeZun%20Zhang%20and%20KaiLai%20Shao%20and%20Chao%20Chen%20and%20Feng%20Wang%0AAbstract%3A%20Multi-turn%20dialogues%20and%20context-intensive%20tasks%20challenge%20Large%20Language%20Models%20%28LLMs%29%20to%20integrate%20long%20histories%20without%20sacrificing%20generation%20quality.%20Although%20prefix%20LLMs%20can%20better%20exploit%20historical%20context%20via%20bidirectional%20attention%20on%20prefix%20tokens%2C%20they%20are%20rarely%20used%20in%20practice%20because%20multi-turn%20training%20requires%20many%20duplicated%20triplets%2C%20and%20its%20bidirectional%20prefix%20prevents%20KV-cache%20reuse%20at%20inference%20time%2C%20driving%20up%20high%20cost%20and%20latency.%20To%20retain%20the%20contextual%20understanding%20of%20prefix%20mask%20while%20preserving%20the%20inference-time%20efficiency%20of%20causal%20mask%2C%20we%20introduce%20Intermittent%20Semi-working%20Mask%20%28ISM%29%2C%20a%20masking%20scheme%20that%20injects%20sparse%20bidirectional%20attention%20into%20the%20causal%20backbone.%20ISM%20alternates%20bidirectional%20attention%20over%20query%20segments%20with%20unidirectional%20attention%20over%20answer%20segments%2C%20enabling%20the%20synthesis%20of%20in-context%20while%20preserving%20global%20causality.%20This%20design%20eliminates%20triplet%20expansion%20during%20training%20and%20maintains%20KV-cache%20reuse%20during%20inference%2C%20yielding%20latency%20comparable%20to%20standard%20causal%20LLMs.%20ISM%20is%20architecture-agnostic%20and%20parameter-free%2C%20adding%20only%20minimal%20latency.%20Across%20extensive%20evaluations%2C%20ISM%20outperforms%20causal%20baselines%20not%20only%20on%20multi-turn%20dialogue%2C%20but%20also%20on%20context-intensive%20tasks%20like%20mathematical%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2408.00539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntermittent%2520Semi-Working%2520Mask%253A%2520A%2520New%2520Masking%2520Paradigm%2520for%2520LLMs%26entry.906535625%3DHaoYuan%2520Hu%2520and%2520Mingcong%2520Lu%2520and%2520Di%2520Luo%2520and%2520XinYa%2520Wu%2520and%2520Jiangcai%2520Zhu%2520and%2520Taoye%2520Yin%2520and%2520Zheng%2520Li%2520and%2520Hao%2520Wang%2520and%2520Shusheng%2520Zhang%2520and%2520KeZun%2520Zhang%2520and%2520KaiLai%2520Shao%2520and%2520Chao%2520Chen%2520and%2520Feng%2520Wang%26entry.1292438233%3DMulti-turn%2520dialogues%2520and%2520context-intensive%2520tasks%2520challenge%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520integrate%2520long%2520histories%2520without%2520sacrificing%2520generation%2520quality.%2520Although%2520prefix%2520LLMs%2520can%2520better%2520exploit%2520historical%2520context%2520via%2520bidirectional%2520attention%2520on%2520prefix%2520tokens%252C%2520they%2520are%2520rarely%2520used%2520in%2520practice%2520because%2520multi-turn%2520training%2520requires%2520many%2520duplicated%2520triplets%252C%2520and%2520its%2520bidirectional%2520prefix%2520prevents%2520KV-cache%2520reuse%2520at%2520inference%2520time%252C%2520driving%2520up%2520high%2520cost%2520and%2520latency.%2520To%2520retain%2520the%2520contextual%2520understanding%2520of%2520prefix%2520mask%2520while%2520preserving%2520the%2520inference-time%2520efficiency%2520of%2520causal%2520mask%252C%2520we%2520introduce%2520Intermittent%2520Semi-working%2520Mask%2520%2528ISM%2529%252C%2520a%2520masking%2520scheme%2520that%2520injects%2520sparse%2520bidirectional%2520attention%2520into%2520the%2520causal%2520backbone.%2520ISM%2520alternates%2520bidirectional%2520attention%2520over%2520query%2520segments%2520with%2520unidirectional%2520attention%2520over%2520answer%2520segments%252C%2520enabling%2520the%2520synthesis%2520of%2520in-context%2520while%2520preserving%2520global%2520causality.%2520This%2520design%2520eliminates%2520triplet%2520expansion%2520during%2520training%2520and%2520maintains%2520KV-cache%2520reuse%2520during%2520inference%252C%2520yielding%2520latency%2520comparable%2520to%2520standard%2520causal%2520LLMs.%2520ISM%2520is%2520architecture-agnostic%2520and%2520parameter-free%252C%2520adding%2520only%2520minimal%2520latency.%2520Across%2520extensive%2520evaluations%252C%2520ISM%2520outperforms%2520causal%2520baselines%2520not%2520only%2520on%2520multi-turn%2520dialogue%252C%2520but%2520also%2520on%2520context-intensive%2520tasks%2520like%2520mathematical%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intermittent%20Semi-Working%20Mask%3A%20A%20New%20Masking%20Paradigm%20for%20LLMs&entry.906535625=HaoYuan%20Hu%20and%20Mingcong%20Lu%20and%20Di%20Luo%20and%20XinYa%20Wu%20and%20Jiangcai%20Zhu%20and%20Taoye%20Yin%20and%20Zheng%20Li%20and%20Hao%20Wang%20and%20Shusheng%20Zhang%20and%20KeZun%20Zhang%20and%20KaiLai%20Shao%20and%20Chao%20Chen%20and%20Feng%20Wang&entry.1292438233=Multi-turn%20dialogues%20and%20context-intensive%20tasks%20challenge%20Large%20Language%20Models%20%28LLMs%29%20to%20integrate%20long%20histories%20without%20sacrificing%20generation%20quality.%20Although%20prefix%20LLMs%20can%20better%20exploit%20historical%20context%20via%20bidirectional%20attention%20on%20prefix%20tokens%2C%20they%20are%20rarely%20used%20in%20practice%20because%20multi-turn%20training%20requires%20many%20duplicated%20triplets%2C%20and%20its%20bidirectional%20prefix%20prevents%20KV-cache%20reuse%20at%20inference%20time%2C%20driving%20up%20high%20cost%20and%20latency.%20To%20retain%20the%20contextual%20understanding%20of%20prefix%20mask%20while%20preserving%20the%20inference-time%20efficiency%20of%20causal%20mask%2C%20we%20introduce%20Intermittent%20Semi-working%20Mask%20%28ISM%29%2C%20a%20masking%20scheme%20that%20injects%20sparse%20bidirectional%20attention%20into%20the%20causal%20backbone.%20ISM%20alternates%20bidirectional%20attention%20over%20query%20segments%20with%20unidirectional%20attention%20over%20answer%20segments%2C%20enabling%20the%20synthesis%20of%20in-context%20while%20preserving%20global%20causality.%20This%20design%20eliminates%20triplet%20expansion%20during%20training%20and%20maintains%20KV-cache%20reuse%20during%20inference%2C%20yielding%20latency%20comparable%20to%20standard%20causal%20LLMs.%20ISM%20is%20architecture-agnostic%20and%20parameter-free%2C%20adding%20only%20minimal%20latency.%20Across%20extensive%20evaluations%2C%20ISM%20outperforms%20causal%20baselines%20not%20only%20on%20multi-turn%20dialogue%2C%20but%20also%20on%20context-intensive%20tasks%20like%20mathematical%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2408.00539v2&entry.124074799=Read"},
{"title": "Long Grounded Thoughts: Synthesizing Visual Problems and Reasoning Chains at Scale", "author": "David Acuna and Chao-Han Huck Yang and Yuntian Deng and Jaehun Jung and Ximing Lu and Prithviraj Ammanabrolu and Hyunwoo Kim and Yuan-Hong Liao and Yejin Choi", "abstract": "Despite rapid progress, multimodal reasoning still lacks a systematic approach to synthesize large-scale vision-centric datasets beyond visual math. We introduce a framework able to synthesize vision-centric problems spanning diverse levels of complexity, and the resulting dataset with over 1M high-quality problems including: reasoning traces, preference data, and instruction prompts supporting SFT, offline and online RL. Our vision-centric synthesis framework uses a two-stage process focusing on: (1) generating diverse verifiable questions from existing images at scale, and (2) creating complex compositional visual problems by merging simpler questions. Remarkably, finetuning Qwen2.5-VL-7B on our data outperforms existing open-data baselines across evaluated vision-centric benchmarks, and our best configurations match or surpass strong closed-data models such as MiMo-VL-7B-RL on Vstar Bench, CV-Bench and MMStar-V. Notably, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro, +3.7%) and audio reasoning (MMAU, +1.32%), demonstrating its effectiveness. Similarly, despite containing no embodied visual data, we observe notable gains (NiEH, +8.8%) when evaluating open-ended embodied QA. Lastly, we use our data to comprehensively analyze at scale (1M+) the entire VLM post-training pipeline showing that (i) SFT on high-quality data with cognitive behaviors on reasoning traces is essential to scale online RL, (ii) offline RL could match online RL's performance while disaggregating compute demands, and, (iii) SFT on high quality data also improve out-of-domain, cross-modality transfer.", "link": "http://arxiv.org/abs/2511.05705v2", "date": "2026-02-17", "relevancy": 2.3823, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6028}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long%20Grounded%20Thoughts%3A%20Synthesizing%20Visual%20Problems%20and%20Reasoning%20Chains%20at%20Scale&body=Title%3A%20Long%20Grounded%20Thoughts%3A%20Synthesizing%20Visual%20Problems%20and%20Reasoning%20Chains%20at%20Scale%0AAuthor%3A%20David%20Acuna%20and%20Chao-Han%20Huck%20Yang%20and%20Yuntian%20Deng%20and%20Jaehun%20Jung%20and%20Ximing%20Lu%20and%20Prithviraj%20Ammanabrolu%20and%20Hyunwoo%20Kim%20and%20Yuan-Hong%20Liao%20and%20Yejin%20Choi%0AAbstract%3A%20Despite%20rapid%20progress%2C%20multimodal%20reasoning%20still%20lacks%20a%20systematic%20approach%20to%20synthesize%20large-scale%20vision-centric%20datasets%20beyond%20visual%20math.%20We%20introduce%20a%20framework%20able%20to%20synthesize%20vision-centric%20problems%20spanning%20diverse%20levels%20of%20complexity%2C%20and%20the%20resulting%20dataset%20with%20over%201M%20high-quality%20problems%20including%3A%20reasoning%20traces%2C%20preference%20data%2C%20and%20instruction%20prompts%20supporting%20SFT%2C%20offline%20and%20online%20RL.%20Our%20vision-centric%20synthesis%20framework%20uses%20a%20two-stage%20process%20focusing%20on%3A%20%281%29%20generating%20diverse%20verifiable%20questions%20from%20existing%20images%20at%20scale%2C%20and%20%282%29%20creating%20complex%20compositional%20visual%20problems%20by%20merging%20simpler%20questions.%20Remarkably%2C%20finetuning%20Qwen2.5-VL-7B%20on%20our%20data%20outperforms%20existing%20open-data%20baselines%20across%20evaluated%20vision-centric%20benchmarks%2C%20and%20our%20best%20configurations%20match%20or%20surpass%20strong%20closed-data%20models%20such%20as%20MiMo-VL-7B-RL%20on%20Vstar%20Bench%2C%20CV-Bench%20and%20MMStar-V.%20Notably%2C%20despite%20being%20entirely%20vision-centric%2C%20our%20data%20transfers%20positively%20to%20text-only%20reasoning%20%28MMLU-Pro%2C%20%2B3.7%25%29%20and%20audio%20reasoning%20%28MMAU%2C%20%2B1.32%25%29%2C%20demonstrating%20its%20effectiveness.%20Similarly%2C%20despite%20containing%20no%20embodied%20visual%20data%2C%20we%20observe%20notable%20gains%20%28NiEH%2C%20%2B8.8%25%29%20when%20evaluating%20open-ended%20embodied%20QA.%20Lastly%2C%20we%20use%20our%20data%20to%20comprehensively%20analyze%20at%20scale%20%281M%2B%29%20the%20entire%20VLM%20post-training%20pipeline%20showing%20that%20%28i%29%20SFT%20on%20high-quality%20data%20with%20cognitive%20behaviors%20on%20reasoning%20traces%20is%20essential%20to%20scale%20online%20RL%2C%20%28ii%29%20offline%20RL%20could%20match%20online%20RL%27s%20performance%20while%20disaggregating%20compute%20demands%2C%20and%2C%20%28iii%29%20SFT%20on%20high%20quality%20data%20also%20improve%20out-of-domain%2C%20cross-modality%20transfer.%0ALink%3A%20http%3A//arxiv.org/abs/2511.05705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong%2520Grounded%2520Thoughts%253A%2520Synthesizing%2520Visual%2520Problems%2520and%2520Reasoning%2520Chains%2520at%2520Scale%26entry.906535625%3DDavid%2520Acuna%2520and%2520Chao-Han%2520Huck%2520Yang%2520and%2520Yuntian%2520Deng%2520and%2520Jaehun%2520Jung%2520and%2520Ximing%2520Lu%2520and%2520Prithviraj%2520Ammanabrolu%2520and%2520Hyunwoo%2520Kim%2520and%2520Yuan-Hong%2520Liao%2520and%2520Yejin%2520Choi%26entry.1292438233%3DDespite%2520rapid%2520progress%252C%2520multimodal%2520reasoning%2520still%2520lacks%2520a%2520systematic%2520approach%2520to%2520synthesize%2520large-scale%2520vision-centric%2520datasets%2520beyond%2520visual%2520math.%2520We%2520introduce%2520a%2520framework%2520able%2520to%2520synthesize%2520vision-centric%2520problems%2520spanning%2520diverse%2520levels%2520of%2520complexity%252C%2520and%2520the%2520resulting%2520dataset%2520with%2520over%25201M%2520high-quality%2520problems%2520including%253A%2520reasoning%2520traces%252C%2520preference%2520data%252C%2520and%2520instruction%2520prompts%2520supporting%2520SFT%252C%2520offline%2520and%2520online%2520RL.%2520Our%2520vision-centric%2520synthesis%2520framework%2520uses%2520a%2520two-stage%2520process%2520focusing%2520on%253A%2520%25281%2529%2520generating%2520diverse%2520verifiable%2520questions%2520from%2520existing%2520images%2520at%2520scale%252C%2520and%2520%25282%2529%2520creating%2520complex%2520compositional%2520visual%2520problems%2520by%2520merging%2520simpler%2520questions.%2520Remarkably%252C%2520finetuning%2520Qwen2.5-VL-7B%2520on%2520our%2520data%2520outperforms%2520existing%2520open-data%2520baselines%2520across%2520evaluated%2520vision-centric%2520benchmarks%252C%2520and%2520our%2520best%2520configurations%2520match%2520or%2520surpass%2520strong%2520closed-data%2520models%2520such%2520as%2520MiMo-VL-7B-RL%2520on%2520Vstar%2520Bench%252C%2520CV-Bench%2520and%2520MMStar-V.%2520Notably%252C%2520despite%2520being%2520entirely%2520vision-centric%252C%2520our%2520data%2520transfers%2520positively%2520to%2520text-only%2520reasoning%2520%2528MMLU-Pro%252C%2520%252B3.7%2525%2529%2520and%2520audio%2520reasoning%2520%2528MMAU%252C%2520%252B1.32%2525%2529%252C%2520demonstrating%2520its%2520effectiveness.%2520Similarly%252C%2520despite%2520containing%2520no%2520embodied%2520visual%2520data%252C%2520we%2520observe%2520notable%2520gains%2520%2528NiEH%252C%2520%252B8.8%2525%2529%2520when%2520evaluating%2520open-ended%2520embodied%2520QA.%2520Lastly%252C%2520we%2520use%2520our%2520data%2520to%2520comprehensively%2520analyze%2520at%2520scale%2520%25281M%252B%2529%2520the%2520entire%2520VLM%2520post-training%2520pipeline%2520showing%2520that%2520%2528i%2529%2520SFT%2520on%2520high-quality%2520data%2520with%2520cognitive%2520behaviors%2520on%2520reasoning%2520traces%2520is%2520essential%2520to%2520scale%2520online%2520RL%252C%2520%2528ii%2529%2520offline%2520RL%2520could%2520match%2520online%2520RL%2527s%2520performance%2520while%2520disaggregating%2520compute%2520demands%252C%2520and%252C%2520%2528iii%2529%2520SFT%2520on%2520high%2520quality%2520data%2520also%2520improve%2520out-of-domain%252C%2520cross-modality%2520transfer.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long%20Grounded%20Thoughts%3A%20Synthesizing%20Visual%20Problems%20and%20Reasoning%20Chains%20at%20Scale&entry.906535625=David%20Acuna%20and%20Chao-Han%20Huck%20Yang%20and%20Yuntian%20Deng%20and%20Jaehun%20Jung%20and%20Ximing%20Lu%20and%20Prithviraj%20Ammanabrolu%20and%20Hyunwoo%20Kim%20and%20Yuan-Hong%20Liao%20and%20Yejin%20Choi&entry.1292438233=Despite%20rapid%20progress%2C%20multimodal%20reasoning%20still%20lacks%20a%20systematic%20approach%20to%20synthesize%20large-scale%20vision-centric%20datasets%20beyond%20visual%20math.%20We%20introduce%20a%20framework%20able%20to%20synthesize%20vision-centric%20problems%20spanning%20diverse%20levels%20of%20complexity%2C%20and%20the%20resulting%20dataset%20with%20over%201M%20high-quality%20problems%20including%3A%20reasoning%20traces%2C%20preference%20data%2C%20and%20instruction%20prompts%20supporting%20SFT%2C%20offline%20and%20online%20RL.%20Our%20vision-centric%20synthesis%20framework%20uses%20a%20two-stage%20process%20focusing%20on%3A%20%281%29%20generating%20diverse%20verifiable%20questions%20from%20existing%20images%20at%20scale%2C%20and%20%282%29%20creating%20complex%20compositional%20visual%20problems%20by%20merging%20simpler%20questions.%20Remarkably%2C%20finetuning%20Qwen2.5-VL-7B%20on%20our%20data%20outperforms%20existing%20open-data%20baselines%20across%20evaluated%20vision-centric%20benchmarks%2C%20and%20our%20best%20configurations%20match%20or%20surpass%20strong%20closed-data%20models%20such%20as%20MiMo-VL-7B-RL%20on%20Vstar%20Bench%2C%20CV-Bench%20and%20MMStar-V.%20Notably%2C%20despite%20being%20entirely%20vision-centric%2C%20our%20data%20transfers%20positively%20to%20text-only%20reasoning%20%28MMLU-Pro%2C%20%2B3.7%25%29%20and%20audio%20reasoning%20%28MMAU%2C%20%2B1.32%25%29%2C%20demonstrating%20its%20effectiveness.%20Similarly%2C%20despite%20containing%20no%20embodied%20visual%20data%2C%20we%20observe%20notable%20gains%20%28NiEH%2C%20%2B8.8%25%29%20when%20evaluating%20open-ended%20embodied%20QA.%20Lastly%2C%20we%20use%20our%20data%20to%20comprehensively%20analyze%20at%20scale%20%281M%2B%29%20the%20entire%20VLM%20post-training%20pipeline%20showing%20that%20%28i%29%20SFT%20on%20high-quality%20data%20with%20cognitive%20behaviors%20on%20reasoning%20traces%20is%20essential%20to%20scale%20online%20RL%2C%20%28ii%29%20offline%20RL%20could%20match%20online%20RL%27s%20performance%20while%20disaggregating%20compute%20demands%2C%20and%2C%20%28iii%29%20SFT%20on%20high%20quality%20data%20also%20improve%20out-of-domain%2C%20cross-modality%20transfer.&entry.1838667208=http%3A//arxiv.org/abs/2511.05705v2&entry.124074799=Read"},
{"title": "Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation", "author": "Yuxuan Kuang and Sungjae Park and Katerina Fragkiadaki and Shubham Tulsiani", "abstract": "Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this 'Anypose-to-Anypose' policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.", "link": "http://arxiv.org/abs/2602.15828v1", "date": "2026-02-17", "relevancy": 2.3713, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.599}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5897}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dex4D%3A%20Task-Agnostic%20Point%20Track%20Policy%20for%20Sim-to-Real%20Dexterous%20Manipulation&body=Title%3A%20Dex4D%3A%20Task-Agnostic%20Point%20Track%20Policy%20for%20Sim-to-Real%20Dexterous%20Manipulation%0AAuthor%3A%20Yuxuan%20Kuang%20and%20Sungjae%20Park%20and%20Katerina%20Fragkiadaki%20and%20Shubham%20Tulsiani%0AAbstract%3A%20Learning%20generalist%20policies%20capable%20of%20accomplishing%20a%20plethora%20of%20everyday%20tasks%20remains%20an%20open%20challenge%20in%20dexterous%20manipulation.%20In%20particular%2C%20collecting%20large-scale%20manipulation%20data%20via%20real-world%20teleoperation%20is%20expensive%20and%20difficult%20to%20scale.%20While%20learning%20in%20simulation%20provides%20a%20feasible%20alternative%2C%20designing%20multiple%20task-specific%20environments%20and%20rewards%20for%20training%20is%20similarly%20challenging.%20We%20propose%20Dex4D%2C%20a%20framework%20that%20instead%20leverages%20simulation%20for%20learning%20task-agnostic%20dexterous%20skills%20that%20can%20be%20flexibly%20recomposed%20to%20perform%20diverse%20real-world%20manipulation%20tasks.%20Specifically%2C%20Dex4D%20learns%20a%20domain-agnostic%203D%20point%20track%20conditioned%20policy%20capable%20of%20manipulating%20any%20object%20to%20any%20desired%20pose.%20We%20train%20this%20%27Anypose-to-Anypose%27%20policy%20in%20simulation%20across%20thousands%20of%20objects%20with%20diverse%20pose%20configurations%2C%20covering%20a%20broad%20space%20of%20robot-object%20interactions%20that%20can%20be%20composed%20at%20test%20time.%20At%20deployment%2C%20this%20policy%20can%20be%20zero-shot%20transferred%20to%20real-world%20tasks%20without%20finetuning%2C%20simply%20by%20prompting%20it%20with%20desired%20object-centric%20point%20tracks%20extracted%20from%20generated%20videos.%20During%20execution%2C%20Dex4D%20uses%20online%20point%20tracking%20for%20closed-loop%20perception%20and%20control.%20Extensive%20experiments%20in%20simulation%20and%20on%20real%20robots%20show%20that%20our%20method%20enables%20zero-shot%20deployment%20for%20diverse%20dexterous%20manipulation%20tasks%20and%20yields%20consistent%20improvements%20over%20prior%20baselines.%20Furthermore%2C%20we%20demonstrate%20strong%20generalization%20to%20novel%20objects%2C%20scene%20layouts%2C%20backgrounds%2C%20and%20trajectories%2C%20highlighting%20the%20robustness%20and%20scalability%20of%20the%20proposed%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDex4D%253A%2520Task-Agnostic%2520Point%2520Track%2520Policy%2520for%2520Sim-to-Real%2520Dexterous%2520Manipulation%26entry.906535625%3DYuxuan%2520Kuang%2520and%2520Sungjae%2520Park%2520and%2520Katerina%2520Fragkiadaki%2520and%2520Shubham%2520Tulsiani%26entry.1292438233%3DLearning%2520generalist%2520policies%2520capable%2520of%2520accomplishing%2520a%2520plethora%2520of%2520everyday%2520tasks%2520remains%2520an%2520open%2520challenge%2520in%2520dexterous%2520manipulation.%2520In%2520particular%252C%2520collecting%2520large-scale%2520manipulation%2520data%2520via%2520real-world%2520teleoperation%2520is%2520expensive%2520and%2520difficult%2520to%2520scale.%2520While%2520learning%2520in%2520simulation%2520provides%2520a%2520feasible%2520alternative%252C%2520designing%2520multiple%2520task-specific%2520environments%2520and%2520rewards%2520for%2520training%2520is%2520similarly%2520challenging.%2520We%2520propose%2520Dex4D%252C%2520a%2520framework%2520that%2520instead%2520leverages%2520simulation%2520for%2520learning%2520task-agnostic%2520dexterous%2520skills%2520that%2520can%2520be%2520flexibly%2520recomposed%2520to%2520perform%2520diverse%2520real-world%2520manipulation%2520tasks.%2520Specifically%252C%2520Dex4D%2520learns%2520a%2520domain-agnostic%25203D%2520point%2520track%2520conditioned%2520policy%2520capable%2520of%2520manipulating%2520any%2520object%2520to%2520any%2520desired%2520pose.%2520We%2520train%2520this%2520%2527Anypose-to-Anypose%2527%2520policy%2520in%2520simulation%2520across%2520thousands%2520of%2520objects%2520with%2520diverse%2520pose%2520configurations%252C%2520covering%2520a%2520broad%2520space%2520of%2520robot-object%2520interactions%2520that%2520can%2520be%2520composed%2520at%2520test%2520time.%2520At%2520deployment%252C%2520this%2520policy%2520can%2520be%2520zero-shot%2520transferred%2520to%2520real-world%2520tasks%2520without%2520finetuning%252C%2520simply%2520by%2520prompting%2520it%2520with%2520desired%2520object-centric%2520point%2520tracks%2520extracted%2520from%2520generated%2520videos.%2520During%2520execution%252C%2520Dex4D%2520uses%2520online%2520point%2520tracking%2520for%2520closed-loop%2520perception%2520and%2520control.%2520Extensive%2520experiments%2520in%2520simulation%2520and%2520on%2520real%2520robots%2520show%2520that%2520our%2520method%2520enables%2520zero-shot%2520deployment%2520for%2520diverse%2520dexterous%2520manipulation%2520tasks%2520and%2520yields%2520consistent%2520improvements%2520over%2520prior%2520baselines.%2520Furthermore%252C%2520we%2520demonstrate%2520strong%2520generalization%2520to%2520novel%2520objects%252C%2520scene%2520layouts%252C%2520backgrounds%252C%2520and%2520trajectories%252C%2520highlighting%2520the%2520robustness%2520and%2520scalability%2520of%2520the%2520proposed%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dex4D%3A%20Task-Agnostic%20Point%20Track%20Policy%20for%20Sim-to-Real%20Dexterous%20Manipulation&entry.906535625=Yuxuan%20Kuang%20and%20Sungjae%20Park%20and%20Katerina%20Fragkiadaki%20and%20Shubham%20Tulsiani&entry.1292438233=Learning%20generalist%20policies%20capable%20of%20accomplishing%20a%20plethora%20of%20everyday%20tasks%20remains%20an%20open%20challenge%20in%20dexterous%20manipulation.%20In%20particular%2C%20collecting%20large-scale%20manipulation%20data%20via%20real-world%20teleoperation%20is%20expensive%20and%20difficult%20to%20scale.%20While%20learning%20in%20simulation%20provides%20a%20feasible%20alternative%2C%20designing%20multiple%20task-specific%20environments%20and%20rewards%20for%20training%20is%20similarly%20challenging.%20We%20propose%20Dex4D%2C%20a%20framework%20that%20instead%20leverages%20simulation%20for%20learning%20task-agnostic%20dexterous%20skills%20that%20can%20be%20flexibly%20recomposed%20to%20perform%20diverse%20real-world%20manipulation%20tasks.%20Specifically%2C%20Dex4D%20learns%20a%20domain-agnostic%203D%20point%20track%20conditioned%20policy%20capable%20of%20manipulating%20any%20object%20to%20any%20desired%20pose.%20We%20train%20this%20%27Anypose-to-Anypose%27%20policy%20in%20simulation%20across%20thousands%20of%20objects%20with%20diverse%20pose%20configurations%2C%20covering%20a%20broad%20space%20of%20robot-object%20interactions%20that%20can%20be%20composed%20at%20test%20time.%20At%20deployment%2C%20this%20policy%20can%20be%20zero-shot%20transferred%20to%20real-world%20tasks%20without%20finetuning%2C%20simply%20by%20prompting%20it%20with%20desired%20object-centric%20point%20tracks%20extracted%20from%20generated%20videos.%20During%20execution%2C%20Dex4D%20uses%20online%20point%20tracking%20for%20closed-loop%20perception%20and%20control.%20Extensive%20experiments%20in%20simulation%20and%20on%20real%20robots%20show%20that%20our%20method%20enables%20zero-shot%20deployment%20for%20diverse%20dexterous%20manipulation%20tasks%20and%20yields%20consistent%20improvements%20over%20prior%20baselines.%20Furthermore%2C%20we%20demonstrate%20strong%20generalization%20to%20novel%20objects%2C%20scene%20layouts%2C%20backgrounds%2C%20and%20trajectories%2C%20highlighting%20the%20robustness%20and%20scalability%20of%20the%20proposed%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2602.15828v1&entry.124074799=Read"},
{"title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens", "author": "Shiqi Liu and Zeyu He and Guojian Zhan and Letian Tao and Zhilong Zheng and Jiang Wu and Yinuo Wang and Yang Guan and Kehua Sheng and Bo Zhang and Keqiang Li and Jingliang Duan and Shengbo Eben Li", "abstract": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term \\emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% over GRPO, 20-Entropy and JustRL.", "link": "http://arxiv.org/abs/2602.15620v1", "date": "2026-02-17", "relevancy": 2.3588, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.512}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4565}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAPO%3A%20Stabilizing%20Reinforcement%20Learning%20for%20LLMs%20by%20Silencing%20Rare%20Spurious%20Tokens&body=Title%3A%20STAPO%3A%20Stabilizing%20Reinforcement%20Learning%20for%20LLMs%20by%20Silencing%20Rare%20Spurious%20Tokens%0AAuthor%3A%20Shiqi%20Liu%20and%20Zeyu%20He%20and%20Guojian%20Zhan%20and%20Letian%20Tao%20and%20Zhilong%20Zheng%20and%20Jiang%20Wu%20and%20Yinuo%20Wang%20and%20Yang%20Guan%20and%20Kehua%20Sheng%20and%20Bo%20Zhang%20and%20Keqiang%20Li%20and%20Jingliang%20Duan%20and%20Shengbo%20Eben%20Li%0AAbstract%3A%20Reinforcement%20Learning%20%28RL%29%20has%20significantly%20improved%20large%20language%20model%20reasoning%2C%20but%20existing%20RL%20fine-tuning%20methods%20rely%20heavily%20on%20heuristic%20techniques%20such%20as%20entropy%20regularization%20and%20reweighting%20to%20maintain%20stability.%20In%20practice%2C%20they%20often%20experience%20late-stage%20performance%20collapse%2C%20leading%20to%20degraded%20reasoning%20quality%20and%20unstable%20training.%20We%20derive%20that%20the%20magnitude%20of%20token-wise%20policy%20gradients%20in%20RL%20is%20negatively%20correlated%20with%20token%20probability%20and%20local%20policy%20entropy.%20Building%20on%20this%20result%2C%20we%20prove%20that%20training%20instability%20is%20driven%20by%20a%20tiny%20fraction%20of%20tokens%2C%20approximately%200.01%5C%25%2C%20which%20we%20term%20%5Cemph%7Bspurious%20tokens%7D.%20When%20such%20tokens%20appear%20in%20correct%20responses%2C%20they%20contribute%20little%20to%20the%20reasoning%20outcome%20but%20inherit%20the%20full%20sequence-level%20reward%2C%20leading%20to%20abnormally%20amplified%20gradient%20updates.%20Motivated%20by%20this%20observation%2C%20we%20propose%20Spurious-Token-Aware%20Policy%20Optimization%20%28STAPO%29%20for%20large-scale%20model%20refining%2C%20which%20selectively%20masks%20such%20updates%20and%20renormalizes%20the%20loss%20over%20valid%20tokens.%20Across%20six%20mathematical%20reasoning%20benchmarks%20using%20Qwen%201.7B%2C%208B%2C%20and%2014B%20base%20models%2C%20STAPO%20consistently%20demonstrates%20superior%20entropy%20stability%20and%20achieves%20an%20average%20performance%20improvement%20of%207.13%5C%25%20over%20GRPO%2C%2020-Entropy%20and%20JustRL.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAPO%253A%2520Stabilizing%2520Reinforcement%2520Learning%2520for%2520LLMs%2520by%2520Silencing%2520Rare%2520Spurious%2520Tokens%26entry.906535625%3DShiqi%2520Liu%2520and%2520Zeyu%2520He%2520and%2520Guojian%2520Zhan%2520and%2520Letian%2520Tao%2520and%2520Zhilong%2520Zheng%2520and%2520Jiang%2520Wu%2520and%2520Yinuo%2520Wang%2520and%2520Yang%2520Guan%2520and%2520Kehua%2520Sheng%2520and%2520Bo%2520Zhang%2520and%2520Keqiang%2520Li%2520and%2520Jingliang%2520Duan%2520and%2520Shengbo%2520Eben%2520Li%26entry.1292438233%3DReinforcement%2520Learning%2520%2528RL%2529%2520has%2520significantly%2520improved%2520large%2520language%2520model%2520reasoning%252C%2520but%2520existing%2520RL%2520fine-tuning%2520methods%2520rely%2520heavily%2520on%2520heuristic%2520techniques%2520such%2520as%2520entropy%2520regularization%2520and%2520reweighting%2520to%2520maintain%2520stability.%2520In%2520practice%252C%2520they%2520often%2520experience%2520late-stage%2520performance%2520collapse%252C%2520leading%2520to%2520degraded%2520reasoning%2520quality%2520and%2520unstable%2520training.%2520We%2520derive%2520that%2520the%2520magnitude%2520of%2520token-wise%2520policy%2520gradients%2520in%2520RL%2520is%2520negatively%2520correlated%2520with%2520token%2520probability%2520and%2520local%2520policy%2520entropy.%2520Building%2520on%2520this%2520result%252C%2520we%2520prove%2520that%2520training%2520instability%2520is%2520driven%2520by%2520a%2520tiny%2520fraction%2520of%2520tokens%252C%2520approximately%25200.01%255C%2525%252C%2520which%2520we%2520term%2520%255Cemph%257Bspurious%2520tokens%257D.%2520When%2520such%2520tokens%2520appear%2520in%2520correct%2520responses%252C%2520they%2520contribute%2520little%2520to%2520the%2520reasoning%2520outcome%2520but%2520inherit%2520the%2520full%2520sequence-level%2520reward%252C%2520leading%2520to%2520abnormally%2520amplified%2520gradient%2520updates.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%2520Spurious-Token-Aware%2520Policy%2520Optimization%2520%2528STAPO%2529%2520for%2520large-scale%2520model%2520refining%252C%2520which%2520selectively%2520masks%2520such%2520updates%2520and%2520renormalizes%2520the%2520loss%2520over%2520valid%2520tokens.%2520Across%2520six%2520mathematical%2520reasoning%2520benchmarks%2520using%2520Qwen%25201.7B%252C%25208B%252C%2520and%252014B%2520base%2520models%252C%2520STAPO%2520consistently%2520demonstrates%2520superior%2520entropy%2520stability%2520and%2520achieves%2520an%2520average%2520performance%2520improvement%2520of%25207.13%255C%2525%2520over%2520GRPO%252C%252020-Entropy%2520and%2520JustRL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAPO%3A%20Stabilizing%20Reinforcement%20Learning%20for%20LLMs%20by%20Silencing%20Rare%20Spurious%20Tokens&entry.906535625=Shiqi%20Liu%20and%20Zeyu%20He%20and%20Guojian%20Zhan%20and%20Letian%20Tao%20and%20Zhilong%20Zheng%20and%20Jiang%20Wu%20and%20Yinuo%20Wang%20and%20Yang%20Guan%20and%20Kehua%20Sheng%20and%20Bo%20Zhang%20and%20Keqiang%20Li%20and%20Jingliang%20Duan%20and%20Shengbo%20Eben%20Li&entry.1292438233=Reinforcement%20Learning%20%28RL%29%20has%20significantly%20improved%20large%20language%20model%20reasoning%2C%20but%20existing%20RL%20fine-tuning%20methods%20rely%20heavily%20on%20heuristic%20techniques%20such%20as%20entropy%20regularization%20and%20reweighting%20to%20maintain%20stability.%20In%20practice%2C%20they%20often%20experience%20late-stage%20performance%20collapse%2C%20leading%20to%20degraded%20reasoning%20quality%20and%20unstable%20training.%20We%20derive%20that%20the%20magnitude%20of%20token-wise%20policy%20gradients%20in%20RL%20is%20negatively%20correlated%20with%20token%20probability%20and%20local%20policy%20entropy.%20Building%20on%20this%20result%2C%20we%20prove%20that%20training%20instability%20is%20driven%20by%20a%20tiny%20fraction%20of%20tokens%2C%20approximately%200.01%5C%25%2C%20which%20we%20term%20%5Cemph%7Bspurious%20tokens%7D.%20When%20such%20tokens%20appear%20in%20correct%20responses%2C%20they%20contribute%20little%20to%20the%20reasoning%20outcome%20but%20inherit%20the%20full%20sequence-level%20reward%2C%20leading%20to%20abnormally%20amplified%20gradient%20updates.%20Motivated%20by%20this%20observation%2C%20we%20propose%20Spurious-Token-Aware%20Policy%20Optimization%20%28STAPO%29%20for%20large-scale%20model%20refining%2C%20which%20selectively%20masks%20such%20updates%20and%20renormalizes%20the%20loss%20over%20valid%20tokens.%20Across%20six%20mathematical%20reasoning%20benchmarks%20using%20Qwen%201.7B%2C%208B%2C%20and%2014B%20base%20models%2C%20STAPO%20consistently%20demonstrates%20superior%20entropy%20stability%20and%20achieves%20an%20average%20performance%20improvement%20of%207.13%5C%25%20over%20GRPO%2C%2020-Entropy%20and%20JustRL.&entry.1838667208=http%3A//arxiv.org/abs/2602.15620v1&entry.124074799=Read"},
{"title": "Intracoronary Optical Coherence Tomography Image Processing and Vessel Classification Using Machine Learning", "author": "Amal Lahchim and Lambros Athanasiou", "abstract": "Intracoronary Optical Coherence Tomography (OCT) enables high-resolution visualization of coronary vessel anatomy but presents challenges due to noise, imaging artifacts, and complex tissue structures. This paper proposes a fully automated pipeline for vessel segmentation and classification in OCT images using machine learning techniques. The proposed method integrates image preprocessing, guidewire artifact removal, polar-to-Cartesian transformation, unsupervised K-means clustering, and local feature extraction. These features are used to train Logistic Regression and Support Vector Machine classifiers for pixel-wise vessel classification. Experimental results demonstrate excellent performance, achieving precision, recall, and F1-score values up to 1.00 and overall classification accuracy of 99.68%. The proposed approach provides accurate vessel boundary detection while maintaining low computational complexity and requiring minimal manual annotation. This method offers a reliable and efficient solution for automated OCT image analysis and has potential applications in clinical decision support and real-time medical image processing.", "link": "http://arxiv.org/abs/2602.15579v1", "date": "2026-02-17", "relevancy": 2.3485, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4741}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4713}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intracoronary%20Optical%20Coherence%20Tomography%20Image%20Processing%20and%20Vessel%20Classification%20Using%20Machine%20Learning&body=Title%3A%20Intracoronary%20Optical%20Coherence%20Tomography%20Image%20Processing%20and%20Vessel%20Classification%20Using%20Machine%20Learning%0AAuthor%3A%20Amal%20Lahchim%20and%20Lambros%20Athanasiou%0AAbstract%3A%20Intracoronary%20Optical%20Coherence%20Tomography%20%28OCT%29%20enables%20high-resolution%20visualization%20of%20coronary%20vessel%20anatomy%20but%20presents%20challenges%20due%20to%20noise%2C%20imaging%20artifacts%2C%20and%20complex%20tissue%20structures.%20This%20paper%20proposes%20a%20fully%20automated%20pipeline%20for%20vessel%20segmentation%20and%20classification%20in%20OCT%20images%20using%20machine%20learning%20techniques.%20The%20proposed%20method%20integrates%20image%20preprocessing%2C%20guidewire%20artifact%20removal%2C%20polar-to-Cartesian%20transformation%2C%20unsupervised%20K-means%20clustering%2C%20and%20local%20feature%20extraction.%20These%20features%20are%20used%20to%20train%20Logistic%20Regression%20and%20Support%20Vector%20Machine%20classifiers%20for%20pixel-wise%20vessel%20classification.%20Experimental%20results%20demonstrate%20excellent%20performance%2C%20achieving%20precision%2C%20recall%2C%20and%20F1-score%20values%20up%20to%201.00%20and%20overall%20classification%20accuracy%20of%2099.68%25.%20The%20proposed%20approach%20provides%20accurate%20vessel%20boundary%20detection%20while%20maintaining%20low%20computational%20complexity%20and%20requiring%20minimal%20manual%20annotation.%20This%20method%20offers%20a%20reliable%20and%20efficient%20solution%20for%20automated%20OCT%20image%20analysis%20and%20has%20potential%20applications%20in%20clinical%20decision%20support%20and%20real-time%20medical%20image%20processing.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntracoronary%2520Optical%2520Coherence%2520Tomography%2520Image%2520Processing%2520and%2520Vessel%2520Classification%2520Using%2520Machine%2520Learning%26entry.906535625%3DAmal%2520Lahchim%2520and%2520Lambros%2520Athanasiou%26entry.1292438233%3DIntracoronary%2520Optical%2520Coherence%2520Tomography%2520%2528OCT%2529%2520enables%2520high-resolution%2520visualization%2520of%2520coronary%2520vessel%2520anatomy%2520but%2520presents%2520challenges%2520due%2520to%2520noise%252C%2520imaging%2520artifacts%252C%2520and%2520complex%2520tissue%2520structures.%2520This%2520paper%2520proposes%2520a%2520fully%2520automated%2520pipeline%2520for%2520vessel%2520segmentation%2520and%2520classification%2520in%2520OCT%2520images%2520using%2520machine%2520learning%2520techniques.%2520The%2520proposed%2520method%2520integrates%2520image%2520preprocessing%252C%2520guidewire%2520artifact%2520removal%252C%2520polar-to-Cartesian%2520transformation%252C%2520unsupervised%2520K-means%2520clustering%252C%2520and%2520local%2520feature%2520extraction.%2520These%2520features%2520are%2520used%2520to%2520train%2520Logistic%2520Regression%2520and%2520Support%2520Vector%2520Machine%2520classifiers%2520for%2520pixel-wise%2520vessel%2520classification.%2520Experimental%2520results%2520demonstrate%2520excellent%2520performance%252C%2520achieving%2520precision%252C%2520recall%252C%2520and%2520F1-score%2520values%2520up%2520to%25201.00%2520and%2520overall%2520classification%2520accuracy%2520of%252099.68%2525.%2520The%2520proposed%2520approach%2520provides%2520accurate%2520vessel%2520boundary%2520detection%2520while%2520maintaining%2520low%2520computational%2520complexity%2520and%2520requiring%2520minimal%2520manual%2520annotation.%2520This%2520method%2520offers%2520a%2520reliable%2520and%2520efficient%2520solution%2520for%2520automated%2520OCT%2520image%2520analysis%2520and%2520has%2520potential%2520applications%2520in%2520clinical%2520decision%2520support%2520and%2520real-time%2520medical%2520image%2520processing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intracoronary%20Optical%20Coherence%20Tomography%20Image%20Processing%20and%20Vessel%20Classification%20Using%20Machine%20Learning&entry.906535625=Amal%20Lahchim%20and%20Lambros%20Athanasiou&entry.1292438233=Intracoronary%20Optical%20Coherence%20Tomography%20%28OCT%29%20enables%20high-resolution%20visualization%20of%20coronary%20vessel%20anatomy%20but%20presents%20challenges%20due%20to%20noise%2C%20imaging%20artifacts%2C%20and%20complex%20tissue%20structures.%20This%20paper%20proposes%20a%20fully%20automated%20pipeline%20for%20vessel%20segmentation%20and%20classification%20in%20OCT%20images%20using%20machine%20learning%20techniques.%20The%20proposed%20method%20integrates%20image%20preprocessing%2C%20guidewire%20artifact%20removal%2C%20polar-to-Cartesian%20transformation%2C%20unsupervised%20K-means%20clustering%2C%20and%20local%20feature%20extraction.%20These%20features%20are%20used%20to%20train%20Logistic%20Regression%20and%20Support%20Vector%20Machine%20classifiers%20for%20pixel-wise%20vessel%20classification.%20Experimental%20results%20demonstrate%20excellent%20performance%2C%20achieving%20precision%2C%20recall%2C%20and%20F1-score%20values%20up%20to%201.00%20and%20overall%20classification%20accuracy%20of%2099.68%25.%20The%20proposed%20approach%20provides%20accurate%20vessel%20boundary%20detection%20while%20maintaining%20low%20computational%20complexity%20and%20requiring%20minimal%20manual%20annotation.%20This%20method%20offers%20a%20reliable%20and%20efficient%20solution%20for%20automated%20OCT%20image%20analysis%20and%20has%20potential%20applications%20in%20clinical%20decision%20support%20and%20real-time%20medical%20image%20processing.&entry.1838667208=http%3A//arxiv.org/abs/2602.15579v1&entry.124074799=Read"},
{"title": "Developing AI Agents with Simulated Data: Why, what, and how?", "author": "Xiaoran Liu and Istvan David", "abstract": "As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.", "link": "http://arxiv.org/abs/2602.15816v1", "date": "2026-02-17", "relevancy": 2.3394, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4946}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4799}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Developing%20AI%20Agents%20with%20Simulated%20Data%3A%20Why%2C%20what%2C%20and%20how%3F&body=Title%3A%20Developing%20AI%20Agents%20with%20Simulated%20Data%3A%20Why%2C%20what%2C%20and%20how%3F%0AAuthor%3A%20Xiaoran%20Liu%20and%20Istvan%20David%0AAbstract%3A%20As%20insufficient%20data%20volume%20and%20quality%20remain%20the%20key%20impediments%20to%20the%20adoption%20of%20modern%20subsymbolic%20AI%2C%20techniques%20of%20synthetic%20data%20generation%20are%20in%20high%20demand.%20Simulation%20offers%20an%20apt%2C%20systematic%20approach%20to%20generating%20diverse%20synthetic%20data.%20This%20chapter%20introduces%20the%20reader%20to%20the%20key%20concepts%2C%20benefits%2C%20and%20challenges%20of%20simulation-based%20synthetic%20data%20generation%20for%20AI%20training%20purposes%2C%20and%20to%20a%20reference%20framework%20to%20describe%2C%20design%2C%20and%20analyze%20digital%20twin-based%20AI%20simulation%20solutions.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeveloping%2520AI%2520Agents%2520with%2520Simulated%2520Data%253A%2520Why%252C%2520what%252C%2520and%2520how%253F%26entry.906535625%3DXiaoran%2520Liu%2520and%2520Istvan%2520David%26entry.1292438233%3DAs%2520insufficient%2520data%2520volume%2520and%2520quality%2520remain%2520the%2520key%2520impediments%2520to%2520the%2520adoption%2520of%2520modern%2520subsymbolic%2520AI%252C%2520techniques%2520of%2520synthetic%2520data%2520generation%2520are%2520in%2520high%2520demand.%2520Simulation%2520offers%2520an%2520apt%252C%2520systematic%2520approach%2520to%2520generating%2520diverse%2520synthetic%2520data.%2520This%2520chapter%2520introduces%2520the%2520reader%2520to%2520the%2520key%2520concepts%252C%2520benefits%252C%2520and%2520challenges%2520of%2520simulation-based%2520synthetic%2520data%2520generation%2520for%2520AI%2520training%2520purposes%252C%2520and%2520to%2520a%2520reference%2520framework%2520to%2520describe%252C%2520design%252C%2520and%2520analyze%2520digital%2520twin-based%2520AI%2520simulation%2520solutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Developing%20AI%20Agents%20with%20Simulated%20Data%3A%20Why%2C%20what%2C%20and%20how%3F&entry.906535625=Xiaoran%20Liu%20and%20Istvan%20David&entry.1292438233=As%20insufficient%20data%20volume%20and%20quality%20remain%20the%20key%20impediments%20to%20the%20adoption%20of%20modern%20subsymbolic%20AI%2C%20techniques%20of%20synthetic%20data%20generation%20are%20in%20high%20demand.%20Simulation%20offers%20an%20apt%2C%20systematic%20approach%20to%20generating%20diverse%20synthetic%20data.%20This%20chapter%20introduces%20the%20reader%20to%20the%20key%20concepts%2C%20benefits%2C%20and%20challenges%20of%20simulation-based%20synthetic%20data%20generation%20for%20AI%20training%20purposes%2C%20and%20to%20a%20reference%20framework%20to%20describe%2C%20design%2C%20and%20analyze%20digital%20twin-based%20AI%20simulation%20solutions.&entry.1838667208=http%3A//arxiv.org/abs/2602.15816v1&entry.124074799=Read"},
{"title": "How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning", "author": "Hongxuan Wu and Yukun Zhang and Xueqing Zhou", "abstract": "When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \\emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \\emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\\% of the final prediction, and cross-modal synergy remains below 2\\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.", "link": "http://arxiv.org/abs/2602.15580v1", "date": "2026-02-17", "relevancy": 2.31, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5792}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5792}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Vision%20Becomes%20Language%3A%20A%20Layer-wise%20Information-Theoretic%20Analysis%20of%20Multimodal%20Reasoning&body=Title%3A%20How%20Vision%20Becomes%20Language%3A%20A%20Layer-wise%20Information-Theoretic%20Analysis%20of%20Multimodal%20Reasoning%0AAuthor%3A%20Hongxuan%20Wu%20and%20Yukun%20Zhang%20and%20Xueqing%20Zhou%0AAbstract%3A%20When%20a%20multimodal%20Transformer%20answers%20a%20visual%20question%2C%20is%20the%20prediction%20driven%20by%20visual%20evidence%2C%20linguistic%20reasoning%2C%20or%20genuinely%20fused%20cross-modal%20computation%20--%20and%20how%20does%20this%20structure%20evolve%20across%20layers%3F%20We%20address%20this%20question%20with%20a%20layer-wise%20framework%20based%20on%20Partial%20Information%20Decomposition%20%28PID%29%20that%20decomposes%20the%20predictive%20information%20at%20each%20Transformer%20layer%20into%20redundant%2C%20vision-unique%2C%20language-unique%2C%20and%20synergistic%20components.%20To%20make%20PID%20tractable%20for%20high-dimensional%20neural%20representations%2C%20we%20introduce%20%5Cemph%7BPID%20Flow%7D%2C%20a%20pipeline%20combining%20dimensionality%20reduction%2C%20normalizing-flow%20Gaussianization%2C%20and%20closed-form%20Gaussian%20PID%20estimation.%20Applying%20this%20framework%20to%20LLaVA-1.5-7B%20and%20LLaVA-1.6-7B%20across%20six%20GQA%20reasoning%20tasks%2C%20we%20uncover%20a%20consistent%20%5Cemph%7Bmodal%20transduction%7D%20pattern%3A%20visual-unique%20information%20peaks%20early%20and%20decays%20with%20depth%2C%20language-unique%20information%20surges%20in%20late%20layers%20to%20account%20for%20roughly%2082%5C%25%20of%20the%20final%20prediction%2C%20and%20cross-modal%20synergy%20remains%20below%202%5C%25.%20This%20trajectory%20is%20highly%20stable%20across%20model%20variants%20%28layer-wise%20correlations%20%24%3E%240.96%29%20yet%20strongly%20task-dependent%2C%20with%20semantic%20redundancy%20governing%20the%20detailed%20information%20fingerprint.%20To%20establish%20causality%2C%20we%20perform%20targeted%20Image%24%5Crightarrow%24Question%20attention%20knockouts%20and%20show%20that%20disrupting%20the%20primary%20transduction%20pathway%20induces%20predictable%20increases%20in%20trapped%20visual-unique%20information%2C%20compensatory%20synergy%2C%20and%20total%20information%20cost%20--%20effects%20that%20are%20strongest%20in%20vision-dependent%20tasks%20and%20weakest%20in%20high-redundancy%20tasks.%20Together%2C%20these%20results%20provide%20an%20information-theoretic%2C%20causal%20account%20of%20how%20vision%20becomes%20language%20in%20multimodal%20Transformers%2C%20and%20offer%20quantitative%20guidance%20for%20identifying%20architectural%20bottlenecks%20where%20modality-specific%20information%20is%20lost.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Vision%2520Becomes%2520Language%253A%2520A%2520Layer-wise%2520Information-Theoretic%2520Analysis%2520of%2520Multimodal%2520Reasoning%26entry.906535625%3DHongxuan%2520Wu%2520and%2520Yukun%2520Zhang%2520and%2520Xueqing%2520Zhou%26entry.1292438233%3DWhen%2520a%2520multimodal%2520Transformer%2520answers%2520a%2520visual%2520question%252C%2520is%2520the%2520prediction%2520driven%2520by%2520visual%2520evidence%252C%2520linguistic%2520reasoning%252C%2520or%2520genuinely%2520fused%2520cross-modal%2520computation%2520--%2520and%2520how%2520does%2520this%2520structure%2520evolve%2520across%2520layers%253F%2520We%2520address%2520this%2520question%2520with%2520a%2520layer-wise%2520framework%2520based%2520on%2520Partial%2520Information%2520Decomposition%2520%2528PID%2529%2520that%2520decomposes%2520the%2520predictive%2520information%2520at%2520each%2520Transformer%2520layer%2520into%2520redundant%252C%2520vision-unique%252C%2520language-unique%252C%2520and%2520synergistic%2520components.%2520To%2520make%2520PID%2520tractable%2520for%2520high-dimensional%2520neural%2520representations%252C%2520we%2520introduce%2520%255Cemph%257BPID%2520Flow%257D%252C%2520a%2520pipeline%2520combining%2520dimensionality%2520reduction%252C%2520normalizing-flow%2520Gaussianization%252C%2520and%2520closed-form%2520Gaussian%2520PID%2520estimation.%2520Applying%2520this%2520framework%2520to%2520LLaVA-1.5-7B%2520and%2520LLaVA-1.6-7B%2520across%2520six%2520GQA%2520reasoning%2520tasks%252C%2520we%2520uncover%2520a%2520consistent%2520%255Cemph%257Bmodal%2520transduction%257D%2520pattern%253A%2520visual-unique%2520information%2520peaks%2520early%2520and%2520decays%2520with%2520depth%252C%2520language-unique%2520information%2520surges%2520in%2520late%2520layers%2520to%2520account%2520for%2520roughly%252082%255C%2525%2520of%2520the%2520final%2520prediction%252C%2520and%2520cross-modal%2520synergy%2520remains%2520below%25202%255C%2525.%2520This%2520trajectory%2520is%2520highly%2520stable%2520across%2520model%2520variants%2520%2528layer-wise%2520correlations%2520%2524%253E%25240.96%2529%2520yet%2520strongly%2520task-dependent%252C%2520with%2520semantic%2520redundancy%2520governing%2520the%2520detailed%2520information%2520fingerprint.%2520To%2520establish%2520causality%252C%2520we%2520perform%2520targeted%2520Image%2524%255Crightarrow%2524Question%2520attention%2520knockouts%2520and%2520show%2520that%2520disrupting%2520the%2520primary%2520transduction%2520pathway%2520induces%2520predictable%2520increases%2520in%2520trapped%2520visual-unique%2520information%252C%2520compensatory%2520synergy%252C%2520and%2520total%2520information%2520cost%2520--%2520effects%2520that%2520are%2520strongest%2520in%2520vision-dependent%2520tasks%2520and%2520weakest%2520in%2520high-redundancy%2520tasks.%2520Together%252C%2520these%2520results%2520provide%2520an%2520information-theoretic%252C%2520causal%2520account%2520of%2520how%2520vision%2520becomes%2520language%2520in%2520multimodal%2520Transformers%252C%2520and%2520offer%2520quantitative%2520guidance%2520for%2520identifying%2520architectural%2520bottlenecks%2520where%2520modality-specific%2520information%2520is%2520lost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Vision%20Becomes%20Language%3A%20A%20Layer-wise%20Information-Theoretic%20Analysis%20of%20Multimodal%20Reasoning&entry.906535625=Hongxuan%20Wu%20and%20Yukun%20Zhang%20and%20Xueqing%20Zhou&entry.1292438233=When%20a%20multimodal%20Transformer%20answers%20a%20visual%20question%2C%20is%20the%20prediction%20driven%20by%20visual%20evidence%2C%20linguistic%20reasoning%2C%20or%20genuinely%20fused%20cross-modal%20computation%20--%20and%20how%20does%20this%20structure%20evolve%20across%20layers%3F%20We%20address%20this%20question%20with%20a%20layer-wise%20framework%20based%20on%20Partial%20Information%20Decomposition%20%28PID%29%20that%20decomposes%20the%20predictive%20information%20at%20each%20Transformer%20layer%20into%20redundant%2C%20vision-unique%2C%20language-unique%2C%20and%20synergistic%20components.%20To%20make%20PID%20tractable%20for%20high-dimensional%20neural%20representations%2C%20we%20introduce%20%5Cemph%7BPID%20Flow%7D%2C%20a%20pipeline%20combining%20dimensionality%20reduction%2C%20normalizing-flow%20Gaussianization%2C%20and%20closed-form%20Gaussian%20PID%20estimation.%20Applying%20this%20framework%20to%20LLaVA-1.5-7B%20and%20LLaVA-1.6-7B%20across%20six%20GQA%20reasoning%20tasks%2C%20we%20uncover%20a%20consistent%20%5Cemph%7Bmodal%20transduction%7D%20pattern%3A%20visual-unique%20information%20peaks%20early%20and%20decays%20with%20depth%2C%20language-unique%20information%20surges%20in%20late%20layers%20to%20account%20for%20roughly%2082%5C%25%20of%20the%20final%20prediction%2C%20and%20cross-modal%20synergy%20remains%20below%202%5C%25.%20This%20trajectory%20is%20highly%20stable%20across%20model%20variants%20%28layer-wise%20correlations%20%24%3E%240.96%29%20yet%20strongly%20task-dependent%2C%20with%20semantic%20redundancy%20governing%20the%20detailed%20information%20fingerprint.%20To%20establish%20causality%2C%20we%20perform%20targeted%20Image%24%5Crightarrow%24Question%20attention%20knockouts%20and%20show%20that%20disrupting%20the%20primary%20transduction%20pathway%20induces%20predictable%20increases%20in%20trapped%20visual-unique%20information%2C%20compensatory%20synergy%2C%20and%20total%20information%20cost%20--%20effects%20that%20are%20strongest%20in%20vision-dependent%20tasks%20and%20weakest%20in%20high-redundancy%20tasks.%20Together%2C%20these%20results%20provide%20an%20information-theoretic%2C%20causal%20account%20of%20how%20vision%20becomes%20language%20in%20multimodal%20Transformers%2C%20and%20offer%20quantitative%20guidance%20for%20identifying%20architectural%20bottlenecks%20where%20modality-specific%20information%20is%20lost.&entry.1838667208=http%3A//arxiv.org/abs/2602.15580v1&entry.124074799=Read"},
{"title": "ToaSt: Token Channel Selection and Structured Pruning for Efficient ViT", "author": "Hyunchan Moon and Cheonjun Park and Steven L. Waslander", "abstract": "Vision Transformers (ViTs) have achieved remarkable success across various vision tasks, yet their deployment is often hindered by prohibitive computational costs. While structured weight pruning and token compression have emerged as promising solutions, they suffer from prolonged retraining times and global propagation that creates optimization challenges, respectively. We propose ToaSt, a decoupled framework applying specialized strategies to distinct ViT components. We apply coupled head-wise structured pruning to Multi-Head Self-Attention modules, leveraging attention operation characteristics to enhance robustness. For Feed-Forward Networks (over 60\\% of FLOPs), we introduce Token Channel Selection (TCS) that enhances compression ratios while avoiding global propagation issues. Our analysis reveals TCS effectively filters redundant noise during selection. Extensive evaluations across nine diverse models, including DeiT, ViT-MAE, and Swin Transformer, demonstrate that ToaSt achieves superior trade-offs between accuracy and efficiency, consistently outperforming existing baselines. On ViT-MAE-Huge, ToaSt achieves 88.52\\% accuracy (+1.64 \\%) with 39.4\\% FLOPs reduction. ToaSt transfers effectively to downstream tasks, cccccachieving 52.2 versus 51.9 mAP on COCO object detection. Code and models will be released upon acceptance.", "link": "http://arxiv.org/abs/2602.15720v1", "date": "2026-02-17", "relevancy": 2.3096, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6064}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5577}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToaSt%3A%20Token%20Channel%20Selection%20and%20Structured%20Pruning%20for%20Efficient%20ViT&body=Title%3A%20ToaSt%3A%20Token%20Channel%20Selection%20and%20Structured%20Pruning%20for%20Efficient%20ViT%0AAuthor%3A%20Hyunchan%20Moon%20and%20Cheonjun%20Park%20and%20Steven%20L.%20Waslander%0AAbstract%3A%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20remarkable%20success%20across%20various%20vision%20tasks%2C%20yet%20their%20deployment%20is%20often%20hindered%20by%20prohibitive%20computational%20costs.%20While%20structured%20weight%20pruning%20and%20token%20compression%20have%20emerged%20as%20promising%20solutions%2C%20they%20suffer%20from%20prolonged%20retraining%20times%20and%20global%20propagation%20that%20creates%20optimization%20challenges%2C%20respectively.%20We%20propose%20ToaSt%2C%20a%20decoupled%20framework%20applying%20specialized%20strategies%20to%20distinct%20ViT%20components.%20We%20apply%20coupled%20head-wise%20structured%20pruning%20to%20Multi-Head%20Self-Attention%20modules%2C%20leveraging%20attention%20operation%20characteristics%20to%20enhance%20robustness.%20For%20Feed-Forward%20Networks%20%28over%2060%5C%25%20of%20FLOPs%29%2C%20we%20introduce%20Token%20Channel%20Selection%20%28TCS%29%20that%20enhances%20compression%20ratios%20while%20avoiding%20global%20propagation%20issues.%20Our%20analysis%20reveals%20TCS%20effectively%20filters%20redundant%20noise%20during%20selection.%20Extensive%20evaluations%20across%20nine%20diverse%20models%2C%20including%20DeiT%2C%20ViT-MAE%2C%20and%20Swin%20Transformer%2C%20demonstrate%20that%20ToaSt%20achieves%20superior%20trade-offs%20between%20accuracy%20and%20efficiency%2C%20consistently%20outperforming%20existing%20baselines.%20On%20ViT-MAE-Huge%2C%20ToaSt%20achieves%2088.52%5C%25%20accuracy%20%28%2B1.64%20%5C%25%29%20with%2039.4%5C%25%20FLOPs%20reduction.%20ToaSt%20transfers%20effectively%20to%20downstream%20tasks%2C%20cccccachieving%2052.2%20versus%2051.9%20mAP%20on%20COCO%20object%20detection.%20Code%20and%20models%20will%20be%20released%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToaSt%253A%2520Token%2520Channel%2520Selection%2520and%2520Structured%2520Pruning%2520for%2520Efficient%2520ViT%26entry.906535625%3DHyunchan%2520Moon%2520and%2520Cheonjun%2520Park%2520and%2520Steven%2520L.%2520Waslander%26entry.1292438233%3DVision%2520Transformers%2520%2528ViTs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%2520various%2520vision%2520tasks%252C%2520yet%2520their%2520deployment%2520is%2520often%2520hindered%2520by%2520prohibitive%2520computational%2520costs.%2520While%2520structured%2520weight%2520pruning%2520and%2520token%2520compression%2520have%2520emerged%2520as%2520promising%2520solutions%252C%2520they%2520suffer%2520from%2520prolonged%2520retraining%2520times%2520and%2520global%2520propagation%2520that%2520creates%2520optimization%2520challenges%252C%2520respectively.%2520We%2520propose%2520ToaSt%252C%2520a%2520decoupled%2520framework%2520applying%2520specialized%2520strategies%2520to%2520distinct%2520ViT%2520components.%2520We%2520apply%2520coupled%2520head-wise%2520structured%2520pruning%2520to%2520Multi-Head%2520Self-Attention%2520modules%252C%2520leveraging%2520attention%2520operation%2520characteristics%2520to%2520enhance%2520robustness.%2520For%2520Feed-Forward%2520Networks%2520%2528over%252060%255C%2525%2520of%2520FLOPs%2529%252C%2520we%2520introduce%2520Token%2520Channel%2520Selection%2520%2528TCS%2529%2520that%2520enhances%2520compression%2520ratios%2520while%2520avoiding%2520global%2520propagation%2520issues.%2520Our%2520analysis%2520reveals%2520TCS%2520effectively%2520filters%2520redundant%2520noise%2520during%2520selection.%2520Extensive%2520evaluations%2520across%2520nine%2520diverse%2520models%252C%2520including%2520DeiT%252C%2520ViT-MAE%252C%2520and%2520Swin%2520Transformer%252C%2520demonstrate%2520that%2520ToaSt%2520achieves%2520superior%2520trade-offs%2520between%2520accuracy%2520and%2520efficiency%252C%2520consistently%2520outperforming%2520existing%2520baselines.%2520On%2520ViT-MAE-Huge%252C%2520ToaSt%2520achieves%252088.52%255C%2525%2520accuracy%2520%2528%252B1.64%2520%255C%2525%2529%2520with%252039.4%255C%2525%2520FLOPs%2520reduction.%2520ToaSt%2520transfers%2520effectively%2520to%2520downstream%2520tasks%252C%2520cccccachieving%252052.2%2520versus%252051.9%2520mAP%2520on%2520COCO%2520object%2520detection.%2520Code%2520and%2520models%2520will%2520be%2520released%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToaSt%3A%20Token%20Channel%20Selection%20and%20Structured%20Pruning%20for%20Efficient%20ViT&entry.906535625=Hyunchan%20Moon%20and%20Cheonjun%20Park%20and%20Steven%20L.%20Waslander&entry.1292438233=Vision%20Transformers%20%28ViTs%29%20have%20achieved%20remarkable%20success%20across%20various%20vision%20tasks%2C%20yet%20their%20deployment%20is%20often%20hindered%20by%20prohibitive%20computational%20costs.%20While%20structured%20weight%20pruning%20and%20token%20compression%20have%20emerged%20as%20promising%20solutions%2C%20they%20suffer%20from%20prolonged%20retraining%20times%20and%20global%20propagation%20that%20creates%20optimization%20challenges%2C%20respectively.%20We%20propose%20ToaSt%2C%20a%20decoupled%20framework%20applying%20specialized%20strategies%20to%20distinct%20ViT%20components.%20We%20apply%20coupled%20head-wise%20structured%20pruning%20to%20Multi-Head%20Self-Attention%20modules%2C%20leveraging%20attention%20operation%20characteristics%20to%20enhance%20robustness.%20For%20Feed-Forward%20Networks%20%28over%2060%5C%25%20of%20FLOPs%29%2C%20we%20introduce%20Token%20Channel%20Selection%20%28TCS%29%20that%20enhances%20compression%20ratios%20while%20avoiding%20global%20propagation%20issues.%20Our%20analysis%20reveals%20TCS%20effectively%20filters%20redundant%20noise%20during%20selection.%20Extensive%20evaluations%20across%20nine%20diverse%20models%2C%20including%20DeiT%2C%20ViT-MAE%2C%20and%20Swin%20Transformer%2C%20demonstrate%20that%20ToaSt%20achieves%20superior%20trade-offs%20between%20accuracy%20and%20efficiency%2C%20consistently%20outperforming%20existing%20baselines.%20On%20ViT-MAE-Huge%2C%20ToaSt%20achieves%2088.52%5C%25%20accuracy%20%28%2B1.64%20%5C%25%29%20with%2039.4%5C%25%20FLOPs%20reduction.%20ToaSt%20transfers%20effectively%20to%20downstream%20tasks%2C%20cccccachieving%2052.2%20versus%2051.9%20mAP%20on%20COCO%20object%20detection.%20Code%20and%20models%20will%20be%20released%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2602.15720v1&entry.124074799=Read"},
{"title": "NeRFscopy: Neural Radiance Fields for in-vivo Time-Varying Tissues from Endoscopy", "author": "Laura Salort-Benejam and Antonio Agudo", "abstract": "Endoscopy is essential in medical imaging, used for diagnosis, prognosis and treatment. Developing a robust dynamic 3D reconstruction pipeline for endoscopic videos could enhance visualization, improve diagnostic accuracy, aid in treatment planning, and guide surgery procedures. However, challenges arise due to the deformable nature of the tissues, the use of monocular cameras, illumination changes, occlusions and unknown camera trajectories. Inspired by neural rendering, we introduce NeRFscopy, a self-supervised pipeline for novel view synthesis and 3D reconstruction of deformable endoscopic tissues from a monocular video. NeRFscopy includes a deformable model with a canonical radiance field and a time-dependent deformation field parameterized by SE(3) transformations. In addition, the color images are efficiently exploited by introducing sophisticated terms to learn a 3D implicit model without assuming any template or pre-trained model, solely from data. NeRFscopy achieves accurate results in terms of novel view synthesis, outperforming competing methods across various challenging endoscopy scenes.", "link": "http://arxiv.org/abs/2602.15775v1", "date": "2026-02-17", "relevancy": 2.3043, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5918}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5791}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRFscopy%3A%20Neural%20Radiance%20Fields%20for%20in-vivo%20Time-Varying%20Tissues%20from%20Endoscopy&body=Title%3A%20NeRFscopy%3A%20Neural%20Radiance%20Fields%20for%20in-vivo%20Time-Varying%20Tissues%20from%20Endoscopy%0AAuthor%3A%20Laura%20Salort-Benejam%20and%20Antonio%20Agudo%0AAbstract%3A%20Endoscopy%20is%20essential%20in%20medical%20imaging%2C%20used%20for%20diagnosis%2C%20prognosis%20and%20treatment.%20Developing%20a%20robust%20dynamic%203D%20reconstruction%20pipeline%20for%20endoscopic%20videos%20could%20enhance%20visualization%2C%20improve%20diagnostic%20accuracy%2C%20aid%20in%20treatment%20planning%2C%20and%20guide%20surgery%20procedures.%20However%2C%20challenges%20arise%20due%20to%20the%20deformable%20nature%20of%20the%20tissues%2C%20the%20use%20of%20monocular%20cameras%2C%20illumination%20changes%2C%20occlusions%20and%20unknown%20camera%20trajectories.%20Inspired%20by%20neural%20rendering%2C%20we%20introduce%20NeRFscopy%2C%20a%20self-supervised%20pipeline%20for%20novel%20view%20synthesis%20and%203D%20reconstruction%20of%20deformable%20endoscopic%20tissues%20from%20a%20monocular%20video.%20NeRFscopy%20includes%20a%20deformable%20model%20with%20a%20canonical%20radiance%20field%20and%20a%20time-dependent%20deformation%20field%20parameterized%20by%20SE%283%29%20transformations.%20In%20addition%2C%20the%20color%20images%20are%20efficiently%20exploited%20by%20introducing%20sophisticated%20terms%20to%20learn%20a%203D%20implicit%20model%20without%20assuming%20any%20template%20or%20pre-trained%20model%2C%20solely%20from%20data.%20NeRFscopy%20achieves%20accurate%20results%20in%20terms%20of%20novel%20view%20synthesis%2C%20outperforming%20competing%20methods%20across%20various%20challenging%20endoscopy%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRFscopy%253A%2520Neural%2520Radiance%2520Fields%2520for%2520in-vivo%2520Time-Varying%2520Tissues%2520from%2520Endoscopy%26entry.906535625%3DLaura%2520Salort-Benejam%2520and%2520Antonio%2520Agudo%26entry.1292438233%3DEndoscopy%2520is%2520essential%2520in%2520medical%2520imaging%252C%2520used%2520for%2520diagnosis%252C%2520prognosis%2520and%2520treatment.%2520Developing%2520a%2520robust%2520dynamic%25203D%2520reconstruction%2520pipeline%2520for%2520endoscopic%2520videos%2520could%2520enhance%2520visualization%252C%2520improve%2520diagnostic%2520accuracy%252C%2520aid%2520in%2520treatment%2520planning%252C%2520and%2520guide%2520surgery%2520procedures.%2520However%252C%2520challenges%2520arise%2520due%2520to%2520the%2520deformable%2520nature%2520of%2520the%2520tissues%252C%2520the%2520use%2520of%2520monocular%2520cameras%252C%2520illumination%2520changes%252C%2520occlusions%2520and%2520unknown%2520camera%2520trajectories.%2520Inspired%2520by%2520neural%2520rendering%252C%2520we%2520introduce%2520NeRFscopy%252C%2520a%2520self-supervised%2520pipeline%2520for%2520novel%2520view%2520synthesis%2520and%25203D%2520reconstruction%2520of%2520deformable%2520endoscopic%2520tissues%2520from%2520a%2520monocular%2520video.%2520NeRFscopy%2520includes%2520a%2520deformable%2520model%2520with%2520a%2520canonical%2520radiance%2520field%2520and%2520a%2520time-dependent%2520deformation%2520field%2520parameterized%2520by%2520SE%25283%2529%2520transformations.%2520In%2520addition%252C%2520the%2520color%2520images%2520are%2520efficiently%2520exploited%2520by%2520introducing%2520sophisticated%2520terms%2520to%2520learn%2520a%25203D%2520implicit%2520model%2520without%2520assuming%2520any%2520template%2520or%2520pre-trained%2520model%252C%2520solely%2520from%2520data.%2520NeRFscopy%2520achieves%2520accurate%2520results%2520in%2520terms%2520of%2520novel%2520view%2520synthesis%252C%2520outperforming%2520competing%2520methods%2520across%2520various%2520challenging%2520endoscopy%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRFscopy%3A%20Neural%20Radiance%20Fields%20for%20in-vivo%20Time-Varying%20Tissues%20from%20Endoscopy&entry.906535625=Laura%20Salort-Benejam%20and%20Antonio%20Agudo&entry.1292438233=Endoscopy%20is%20essential%20in%20medical%20imaging%2C%20used%20for%20diagnosis%2C%20prognosis%20and%20treatment.%20Developing%20a%20robust%20dynamic%203D%20reconstruction%20pipeline%20for%20endoscopic%20videos%20could%20enhance%20visualization%2C%20improve%20diagnostic%20accuracy%2C%20aid%20in%20treatment%20planning%2C%20and%20guide%20surgery%20procedures.%20However%2C%20challenges%20arise%20due%20to%20the%20deformable%20nature%20of%20the%20tissues%2C%20the%20use%20of%20monocular%20cameras%2C%20illumination%20changes%2C%20occlusions%20and%20unknown%20camera%20trajectories.%20Inspired%20by%20neural%20rendering%2C%20we%20introduce%20NeRFscopy%2C%20a%20self-supervised%20pipeline%20for%20novel%20view%20synthesis%20and%203D%20reconstruction%20of%20deformable%20endoscopic%20tissues%20from%20a%20monocular%20video.%20NeRFscopy%20includes%20a%20deformable%20model%20with%20a%20canonical%20radiance%20field%20and%20a%20time-dependent%20deformation%20field%20parameterized%20by%20SE%283%29%20transformations.%20In%20addition%2C%20the%20color%20images%20are%20efficiently%20exploited%20by%20introducing%20sophisticated%20terms%20to%20learn%20a%203D%20implicit%20model%20without%20assuming%20any%20template%20or%20pre-trained%20model%2C%20solely%20from%20data.%20NeRFscopy%20achieves%20accurate%20results%20in%20terms%20of%20novel%20view%20synthesis%2C%20outperforming%20competing%20methods%20across%20various%20challenging%20endoscopy%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2602.15775v1&entry.124074799=Read"},
{"title": "The Equalizer: Introducing Shape-Gain Decomposition in Neural Audio Codecs", "author": "Samir Sadok and Laurent Girin and Xavier Alameda-Pineda", "abstract": "Neural audio codecs (NACs) typically encode the short-term energy (gain) and normalized structure (shape) of speech/audio signals jointly within the same latent space. As a result, they are poorly robust to a global variation of the input signal level in the sense that such variation has strong influence on the embedding vectors at the output of the encoder and their quantization. This methodology is inherently inefficient, leading to codebook redundancy and suboptimal bitrate-distortion performance. To address these limitations, we propose to introduce shape-gain decomposition, widely used in classical speech/audio coding, into the NAC framework. The principle of the proposed Equalizer methodology is to decompose the input signal -- before the NAC encoder -- into gain and normalized shape vector on a short-term basis. The shape vector is processed by the NAC, while the gain is quantized with scalar quantization and transmitted separately. The output (decoded) signal is reconstructed from the normalized output of the NAC and the quantized gain. Our experiments conducted on speech signals show that this general methodology, easily applicable to any NAC, enables a substantial gain in bitrate-distortion performance, as well as a massive reduction in complexity.", "link": "http://arxiv.org/abs/2602.15491v1", "date": "2026-02-17", "relevancy": 2.2894, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4645}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4592}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Equalizer%3A%20Introducing%20Shape-Gain%20Decomposition%20in%20Neural%20Audio%20Codecs&body=Title%3A%20The%20Equalizer%3A%20Introducing%20Shape-Gain%20Decomposition%20in%20Neural%20Audio%20Codecs%0AAuthor%3A%20Samir%20Sadok%20and%20Laurent%20Girin%20and%20Xavier%20Alameda-Pineda%0AAbstract%3A%20Neural%20audio%20codecs%20%28NACs%29%20typically%20encode%20the%20short-term%20energy%20%28gain%29%20and%20normalized%20structure%20%28shape%29%20of%20speech/audio%20signals%20jointly%20within%20the%20same%20latent%20space.%20As%20a%20result%2C%20they%20are%20poorly%20robust%20to%20a%20global%20variation%20of%20the%20input%20signal%20level%20in%20the%20sense%20that%20such%20variation%20has%20strong%20influence%20on%20the%20embedding%20vectors%20at%20the%20output%20of%20the%20encoder%20and%20their%20quantization.%20This%20methodology%20is%20inherently%20inefficient%2C%20leading%20to%20codebook%20redundancy%20and%20suboptimal%20bitrate-distortion%20performance.%20To%20address%20these%20limitations%2C%20we%20propose%20to%20introduce%20shape-gain%20decomposition%2C%20widely%20used%20in%20classical%20speech/audio%20coding%2C%20into%20the%20NAC%20framework.%20The%20principle%20of%20the%20proposed%20Equalizer%20methodology%20is%20to%20decompose%20the%20input%20signal%20--%20before%20the%20NAC%20encoder%20--%20into%20gain%20and%20normalized%20shape%20vector%20on%20a%20short-term%20basis.%20The%20shape%20vector%20is%20processed%20by%20the%20NAC%2C%20while%20the%20gain%20is%20quantized%20with%20scalar%20quantization%20and%20transmitted%20separately.%20The%20output%20%28decoded%29%20signal%20is%20reconstructed%20from%20the%20normalized%20output%20of%20the%20NAC%20and%20the%20quantized%20gain.%20Our%20experiments%20conducted%20on%20speech%20signals%20show%20that%20this%20general%20methodology%2C%20easily%20applicable%20to%20any%20NAC%2C%20enables%20a%20substantial%20gain%20in%20bitrate-distortion%20performance%2C%20as%20well%20as%20a%20massive%20reduction%20in%20complexity.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Equalizer%253A%2520Introducing%2520Shape-Gain%2520Decomposition%2520in%2520Neural%2520Audio%2520Codecs%26entry.906535625%3DSamir%2520Sadok%2520and%2520Laurent%2520Girin%2520and%2520Xavier%2520Alameda-Pineda%26entry.1292438233%3DNeural%2520audio%2520codecs%2520%2528NACs%2529%2520typically%2520encode%2520the%2520short-term%2520energy%2520%2528gain%2529%2520and%2520normalized%2520structure%2520%2528shape%2529%2520of%2520speech/audio%2520signals%2520jointly%2520within%2520the%2520same%2520latent%2520space.%2520As%2520a%2520result%252C%2520they%2520are%2520poorly%2520robust%2520to%2520a%2520global%2520variation%2520of%2520the%2520input%2520signal%2520level%2520in%2520the%2520sense%2520that%2520such%2520variation%2520has%2520strong%2520influence%2520on%2520the%2520embedding%2520vectors%2520at%2520the%2520output%2520of%2520the%2520encoder%2520and%2520their%2520quantization.%2520This%2520methodology%2520is%2520inherently%2520inefficient%252C%2520leading%2520to%2520codebook%2520redundancy%2520and%2520suboptimal%2520bitrate-distortion%2520performance.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520to%2520introduce%2520shape-gain%2520decomposition%252C%2520widely%2520used%2520in%2520classical%2520speech/audio%2520coding%252C%2520into%2520the%2520NAC%2520framework.%2520The%2520principle%2520of%2520the%2520proposed%2520Equalizer%2520methodology%2520is%2520to%2520decompose%2520the%2520input%2520signal%2520--%2520before%2520the%2520NAC%2520encoder%2520--%2520into%2520gain%2520and%2520normalized%2520shape%2520vector%2520on%2520a%2520short-term%2520basis.%2520The%2520shape%2520vector%2520is%2520processed%2520by%2520the%2520NAC%252C%2520while%2520the%2520gain%2520is%2520quantized%2520with%2520scalar%2520quantization%2520and%2520transmitted%2520separately.%2520The%2520output%2520%2528decoded%2529%2520signal%2520is%2520reconstructed%2520from%2520the%2520normalized%2520output%2520of%2520the%2520NAC%2520and%2520the%2520quantized%2520gain.%2520Our%2520experiments%2520conducted%2520on%2520speech%2520signals%2520show%2520that%2520this%2520general%2520methodology%252C%2520easily%2520applicable%2520to%2520any%2520NAC%252C%2520enables%2520a%2520substantial%2520gain%2520in%2520bitrate-distortion%2520performance%252C%2520as%2520well%2520as%2520a%2520massive%2520reduction%2520in%2520complexity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Equalizer%3A%20Introducing%20Shape-Gain%20Decomposition%20in%20Neural%20Audio%20Codecs&entry.906535625=Samir%20Sadok%20and%20Laurent%20Girin%20and%20Xavier%20Alameda-Pineda&entry.1292438233=Neural%20audio%20codecs%20%28NACs%29%20typically%20encode%20the%20short-term%20energy%20%28gain%29%20and%20normalized%20structure%20%28shape%29%20of%20speech/audio%20signals%20jointly%20within%20the%20same%20latent%20space.%20As%20a%20result%2C%20they%20are%20poorly%20robust%20to%20a%20global%20variation%20of%20the%20input%20signal%20level%20in%20the%20sense%20that%20such%20variation%20has%20strong%20influence%20on%20the%20embedding%20vectors%20at%20the%20output%20of%20the%20encoder%20and%20their%20quantization.%20This%20methodology%20is%20inherently%20inefficient%2C%20leading%20to%20codebook%20redundancy%20and%20suboptimal%20bitrate-distortion%20performance.%20To%20address%20these%20limitations%2C%20we%20propose%20to%20introduce%20shape-gain%20decomposition%2C%20widely%20used%20in%20classical%20speech/audio%20coding%2C%20into%20the%20NAC%20framework.%20The%20principle%20of%20the%20proposed%20Equalizer%20methodology%20is%20to%20decompose%20the%20input%20signal%20--%20before%20the%20NAC%20encoder%20--%20into%20gain%20and%20normalized%20shape%20vector%20on%20a%20short-term%20basis.%20The%20shape%20vector%20is%20processed%20by%20the%20NAC%2C%20while%20the%20gain%20is%20quantized%20with%20scalar%20quantization%20and%20transmitted%20separately.%20The%20output%20%28decoded%29%20signal%20is%20reconstructed%20from%20the%20normalized%20output%20of%20the%20NAC%20and%20the%20quantized%20gain.%20Our%20experiments%20conducted%20on%20speech%20signals%20show%20that%20this%20general%20methodology%2C%20easily%20applicable%20to%20any%20NAC%2C%20enables%20a%20substantial%20gain%20in%20bitrate-distortion%20performance%2C%20as%20well%20as%20a%20massive%20reduction%20in%20complexity.&entry.1838667208=http%3A//arxiv.org/abs/2602.15491v1&entry.124074799=Read"},
{"title": "cadrille: Multi-modal CAD Reconstruction with Reinforcement Learning", "author": "Maksim Kolodiazhnyi and Denis Tarasov and Dmitrii Zhemchuzhnikov and Alexander Nikulin and Ilya Zisman and Anna Vorontsova and Anton Konushin and Vladislav Kurenkov and Danila Rukhovich", "abstract": "Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one. Code is avaliable at https://github.com/col14m/cadrille .", "link": "http://arxiv.org/abs/2505.22914v3", "date": "2026-02-17", "relevancy": 2.2822, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5881}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5657}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20cadrille%3A%20Multi-modal%20CAD%20Reconstruction%20with%20Reinforcement%20Learning&body=Title%3A%20cadrille%3A%20Multi-modal%20CAD%20Reconstruction%20with%20Reinforcement%20Learning%0AAuthor%3A%20Maksim%20Kolodiazhnyi%20and%20Denis%20Tarasov%20and%20Dmitrii%20Zhemchuzhnikov%20and%20Alexander%20Nikulin%20and%20Ilya%20Zisman%20and%20Anna%20Vorontsova%20and%20Anton%20Konushin%20and%20Vladislav%20Kurenkov%20and%20Danila%20Rukhovich%0AAbstract%3A%20Computer-Aided%20Design%20%28CAD%29%20plays%20a%20central%20role%20in%20engineering%20and%20manufacturing%2C%20making%20it%20possible%20to%20create%20precise%20and%20editable%203D%20models.%20Using%20a%20variety%20of%20sensor%20or%20user-provided%20data%20as%20inputs%20for%20CAD%20reconstruction%20can%20democratize%20access%20to%20design%20applications.%20However%2C%20existing%20methods%20typically%20focus%20on%20a%20single%20input%20modality%2C%20such%20as%20point%20clouds%2C%20images%2C%20or%20text%2C%20which%20limits%20their%20generalizability%20and%20robustness.%20Leveraging%20recent%20advances%20in%20vision-language%20models%20%28VLM%29%2C%20we%20propose%20a%20multi-modal%20CAD%20reconstruction%20model%20that%20simultaneously%20processes%20all%20three%20input%20modalities.%20Inspired%20by%20large%20language%20model%20%28LLM%29%20training%20paradigms%2C%20we%20adopt%20a%20two-stage%20pipeline%3A%20supervised%20fine-tuning%20%28SFT%29%20on%20large-scale%20procedurally%20generated%20data%2C%20followed%20by%20reinforcement%20learning%20%28RL%29%20fine-tuning%20using%20online%20feedback%2C%20obtained%20programatically.%20Furthermore%2C%20we%20are%20the%20first%20to%20explore%20RL%20fine-tuning%20of%20LLMs%20for%20CAD%20tasks%20demonstrating%20that%20online%20RL%20algorithms%20such%20as%20Group%20Relative%20Preference%20Optimization%20%28GRPO%29%20outperform%20offline%20alternatives.%20In%20the%20DeepCAD%20benchmark%2C%20our%20SFT%20model%20outperforms%20existing%20single-modal%20approaches%20in%20all%20three%20input%20modalities%20simultaneously.%20More%20importantly%2C%20after%20RL%20fine-tuning%2C%20cadrille%20sets%20new%20state-of-the-art%20on%20three%20challenging%20datasets%2C%20including%20a%20real-world%20one.%20Code%20is%20avaliable%20at%20https%3A//github.com/col14m/cadrille%20.%0ALink%3A%20http%3A//arxiv.org/abs/2505.22914v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dcadrille%253A%2520Multi-modal%2520CAD%2520Reconstruction%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DMaksim%2520Kolodiazhnyi%2520and%2520Denis%2520Tarasov%2520and%2520Dmitrii%2520Zhemchuzhnikov%2520and%2520Alexander%2520Nikulin%2520and%2520Ilya%2520Zisman%2520and%2520Anna%2520Vorontsova%2520and%2520Anton%2520Konushin%2520and%2520Vladislav%2520Kurenkov%2520and%2520Danila%2520Rukhovich%26entry.1292438233%3DComputer-Aided%2520Design%2520%2528CAD%2529%2520plays%2520a%2520central%2520role%2520in%2520engineering%2520and%2520manufacturing%252C%2520making%2520it%2520possible%2520to%2520create%2520precise%2520and%2520editable%25203D%2520models.%2520Using%2520a%2520variety%2520of%2520sensor%2520or%2520user-provided%2520data%2520as%2520inputs%2520for%2520CAD%2520reconstruction%2520can%2520democratize%2520access%2520to%2520design%2520applications.%2520However%252C%2520existing%2520methods%2520typically%2520focus%2520on%2520a%2520single%2520input%2520modality%252C%2520such%2520as%2520point%2520clouds%252C%2520images%252C%2520or%2520text%252C%2520which%2520limits%2520their%2520generalizability%2520and%2520robustness.%2520Leveraging%2520recent%2520advances%2520in%2520vision-language%2520models%2520%2528VLM%2529%252C%2520we%2520propose%2520a%2520multi-modal%2520CAD%2520reconstruction%2520model%2520that%2520simultaneously%2520processes%2520all%2520three%2520input%2520modalities.%2520Inspired%2520by%2520large%2520language%2520model%2520%2528LLM%2529%2520training%2520paradigms%252C%2520we%2520adopt%2520a%2520two-stage%2520pipeline%253A%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520on%2520large-scale%2520procedurally%2520generated%2520data%252C%2520followed%2520by%2520reinforcement%2520learning%2520%2528RL%2529%2520fine-tuning%2520using%2520online%2520feedback%252C%2520obtained%2520programatically.%2520Furthermore%252C%2520we%2520are%2520the%2520first%2520to%2520explore%2520RL%2520fine-tuning%2520of%2520LLMs%2520for%2520CAD%2520tasks%2520demonstrating%2520that%2520online%2520RL%2520algorithms%2520such%2520as%2520Group%2520Relative%2520Preference%2520Optimization%2520%2528GRPO%2529%2520outperform%2520offline%2520alternatives.%2520In%2520the%2520DeepCAD%2520benchmark%252C%2520our%2520SFT%2520model%2520outperforms%2520existing%2520single-modal%2520approaches%2520in%2520all%2520three%2520input%2520modalities%2520simultaneously.%2520More%2520importantly%252C%2520after%2520RL%2520fine-tuning%252C%2520cadrille%2520sets%2520new%2520state-of-the-art%2520on%2520three%2520challenging%2520datasets%252C%2520including%2520a%2520real-world%2520one.%2520Code%2520is%2520avaliable%2520at%2520https%253A//github.com/col14m/cadrille%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22914v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=cadrille%3A%20Multi-modal%20CAD%20Reconstruction%20with%20Reinforcement%20Learning&entry.906535625=Maksim%20Kolodiazhnyi%20and%20Denis%20Tarasov%20and%20Dmitrii%20Zhemchuzhnikov%20and%20Alexander%20Nikulin%20and%20Ilya%20Zisman%20and%20Anna%20Vorontsova%20and%20Anton%20Konushin%20and%20Vladislav%20Kurenkov%20and%20Danila%20Rukhovich&entry.1292438233=Computer-Aided%20Design%20%28CAD%29%20plays%20a%20central%20role%20in%20engineering%20and%20manufacturing%2C%20making%20it%20possible%20to%20create%20precise%20and%20editable%203D%20models.%20Using%20a%20variety%20of%20sensor%20or%20user-provided%20data%20as%20inputs%20for%20CAD%20reconstruction%20can%20democratize%20access%20to%20design%20applications.%20However%2C%20existing%20methods%20typically%20focus%20on%20a%20single%20input%20modality%2C%20such%20as%20point%20clouds%2C%20images%2C%20or%20text%2C%20which%20limits%20their%20generalizability%20and%20robustness.%20Leveraging%20recent%20advances%20in%20vision-language%20models%20%28VLM%29%2C%20we%20propose%20a%20multi-modal%20CAD%20reconstruction%20model%20that%20simultaneously%20processes%20all%20three%20input%20modalities.%20Inspired%20by%20large%20language%20model%20%28LLM%29%20training%20paradigms%2C%20we%20adopt%20a%20two-stage%20pipeline%3A%20supervised%20fine-tuning%20%28SFT%29%20on%20large-scale%20procedurally%20generated%20data%2C%20followed%20by%20reinforcement%20learning%20%28RL%29%20fine-tuning%20using%20online%20feedback%2C%20obtained%20programatically.%20Furthermore%2C%20we%20are%20the%20first%20to%20explore%20RL%20fine-tuning%20of%20LLMs%20for%20CAD%20tasks%20demonstrating%20that%20online%20RL%20algorithms%20such%20as%20Group%20Relative%20Preference%20Optimization%20%28GRPO%29%20outperform%20offline%20alternatives.%20In%20the%20DeepCAD%20benchmark%2C%20our%20SFT%20model%20outperforms%20existing%20single-modal%20approaches%20in%20all%20three%20input%20modalities%20simultaneously.%20More%20importantly%2C%20after%20RL%20fine-tuning%2C%20cadrille%20sets%20new%20state-of-the-art%20on%20three%20challenging%20datasets%2C%20including%20a%20real-world%20one.%20Code%20is%20avaliable%20at%20https%3A//github.com/col14m/cadrille%20.&entry.1838667208=http%3A//arxiv.org/abs/2505.22914v3&entry.124074799=Read"},
{"title": "SSL4EO-S12 v1.1: A Multimodal, Multiseasonal Dataset for Pretraining, Updated", "author": "Benedikt Blumenstiel and Nassim Ait Ali Braham and Conrad M Albrecht and Stefano Maurogiovanni and Paolo Fraccaro", "abstract": "This work presents SSL4EO-S12 v1.1, a multimodal, multitemporal Earth Observation dataset designed for pretraining large-scale foundation models. Building on the success of SSL4EO-S12, this extension updates the previous version to fix geospatial alignment inaccuracies and the inefficent data structure. The dataset allows low-barrier, analysis-ready data loading while maintaining the predecessor's spatial coverage of the world's 10,000 largest cities and surrounding geographies, resulting in 246k time series with nearly one million image patches. We package each time series in Zarr file format stored in WebDataset tar shards for efficient data loading and representation of meta-information such as cloud masks. We add new modalities for elevation, land-cover, and vegetation to support multimodal pre-training. Released under the CC-BY-4.0 license, SSL4EO-S12 v1.1 facilitates open research and provides a robust foundation for future advancements in self-supervised learning and geospatial analysis. The dataset is available online through https://huggingface.co/datasets/embed2scale/SSL4EO-S12-v1.1.", "link": "http://arxiv.org/abs/2503.00168v3", "date": "2026-02-17", "relevancy": 2.2786, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4658}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4515}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSL4EO-S12%20v1.1%3A%20A%20Multimodal%2C%20Multiseasonal%20Dataset%20for%20Pretraining%2C%20Updated&body=Title%3A%20SSL4EO-S12%20v1.1%3A%20A%20Multimodal%2C%20Multiseasonal%20Dataset%20for%20Pretraining%2C%20Updated%0AAuthor%3A%20Benedikt%20Blumenstiel%20and%20Nassim%20Ait%20Ali%20Braham%20and%20Conrad%20M%20Albrecht%20and%20Stefano%20Maurogiovanni%20and%20Paolo%20Fraccaro%0AAbstract%3A%20This%20work%20presents%20SSL4EO-S12%20v1.1%2C%20a%20multimodal%2C%20multitemporal%20Earth%20Observation%20dataset%20designed%20for%20pretraining%20large-scale%20foundation%20models.%20Building%20on%20the%20success%20of%20SSL4EO-S12%2C%20this%20extension%20updates%20the%20previous%20version%20to%20fix%20geospatial%20alignment%20inaccuracies%20and%20the%20inefficent%20data%20structure.%20The%20dataset%20allows%20low-barrier%2C%20analysis-ready%20data%20loading%20while%20maintaining%20the%20predecessor%27s%20spatial%20coverage%20of%20the%20world%27s%2010%2C000%20largest%20cities%20and%20surrounding%20geographies%2C%20resulting%20in%20246k%20time%20series%20with%20nearly%20one%20million%20image%20patches.%20We%20package%20each%20time%20series%20in%20Zarr%20file%20format%20stored%20in%20WebDataset%20tar%20shards%20for%20efficient%20data%20loading%20and%20representation%20of%20meta-information%20such%20as%20cloud%20masks.%20We%20add%20new%20modalities%20for%20elevation%2C%20land-cover%2C%20and%20vegetation%20to%20support%20multimodal%20pre-training.%20Released%20under%20the%20CC-BY-4.0%20license%2C%20SSL4EO-S12%20v1.1%20facilitates%20open%20research%20and%20provides%20a%20robust%20foundation%20for%20future%20advancements%20in%20self-supervised%20learning%20and%20geospatial%20analysis.%20The%20dataset%20is%20available%20online%20through%20https%3A//huggingface.co/datasets/embed2scale/SSL4EO-S12-v1.1.%0ALink%3A%20http%3A//arxiv.org/abs/2503.00168v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSL4EO-S12%2520v1.1%253A%2520A%2520Multimodal%252C%2520Multiseasonal%2520Dataset%2520for%2520Pretraining%252C%2520Updated%26entry.906535625%3DBenedikt%2520Blumenstiel%2520and%2520Nassim%2520Ait%2520Ali%2520Braham%2520and%2520Conrad%2520M%2520Albrecht%2520and%2520Stefano%2520Maurogiovanni%2520and%2520Paolo%2520Fraccaro%26entry.1292438233%3DThis%2520work%2520presents%2520SSL4EO-S12%2520v1.1%252C%2520a%2520multimodal%252C%2520multitemporal%2520Earth%2520Observation%2520dataset%2520designed%2520for%2520pretraining%2520large-scale%2520foundation%2520models.%2520Building%2520on%2520the%2520success%2520of%2520SSL4EO-S12%252C%2520this%2520extension%2520updates%2520the%2520previous%2520version%2520to%2520fix%2520geospatial%2520alignment%2520inaccuracies%2520and%2520the%2520inefficent%2520data%2520structure.%2520The%2520dataset%2520allows%2520low-barrier%252C%2520analysis-ready%2520data%2520loading%2520while%2520maintaining%2520the%2520predecessor%2527s%2520spatial%2520coverage%2520of%2520the%2520world%2527s%252010%252C000%2520largest%2520cities%2520and%2520surrounding%2520geographies%252C%2520resulting%2520in%2520246k%2520time%2520series%2520with%2520nearly%2520one%2520million%2520image%2520patches.%2520We%2520package%2520each%2520time%2520series%2520in%2520Zarr%2520file%2520format%2520stored%2520in%2520WebDataset%2520tar%2520shards%2520for%2520efficient%2520data%2520loading%2520and%2520representation%2520of%2520meta-information%2520such%2520as%2520cloud%2520masks.%2520We%2520add%2520new%2520modalities%2520for%2520elevation%252C%2520land-cover%252C%2520and%2520vegetation%2520to%2520support%2520multimodal%2520pre-training.%2520Released%2520under%2520the%2520CC-BY-4.0%2520license%252C%2520SSL4EO-S12%2520v1.1%2520facilitates%2520open%2520research%2520and%2520provides%2520a%2520robust%2520foundation%2520for%2520future%2520advancements%2520in%2520self-supervised%2520learning%2520and%2520geospatial%2520analysis.%2520The%2520dataset%2520is%2520available%2520online%2520through%2520https%253A//huggingface.co/datasets/embed2scale/SSL4EO-S12-v1.1.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00168v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSL4EO-S12%20v1.1%3A%20A%20Multimodal%2C%20Multiseasonal%20Dataset%20for%20Pretraining%2C%20Updated&entry.906535625=Benedikt%20Blumenstiel%20and%20Nassim%20Ait%20Ali%20Braham%20and%20Conrad%20M%20Albrecht%20and%20Stefano%20Maurogiovanni%20and%20Paolo%20Fraccaro&entry.1292438233=This%20work%20presents%20SSL4EO-S12%20v1.1%2C%20a%20multimodal%2C%20multitemporal%20Earth%20Observation%20dataset%20designed%20for%20pretraining%20large-scale%20foundation%20models.%20Building%20on%20the%20success%20of%20SSL4EO-S12%2C%20this%20extension%20updates%20the%20previous%20version%20to%20fix%20geospatial%20alignment%20inaccuracies%20and%20the%20inefficent%20data%20structure.%20The%20dataset%20allows%20low-barrier%2C%20analysis-ready%20data%20loading%20while%20maintaining%20the%20predecessor%27s%20spatial%20coverage%20of%20the%20world%27s%2010%2C000%20largest%20cities%20and%20surrounding%20geographies%2C%20resulting%20in%20246k%20time%20series%20with%20nearly%20one%20million%20image%20patches.%20We%20package%20each%20time%20series%20in%20Zarr%20file%20format%20stored%20in%20WebDataset%20tar%20shards%20for%20efficient%20data%20loading%20and%20representation%20of%20meta-information%20such%20as%20cloud%20masks.%20We%20add%20new%20modalities%20for%20elevation%2C%20land-cover%2C%20and%20vegetation%20to%20support%20multimodal%20pre-training.%20Released%20under%20the%20CC-BY-4.0%20license%2C%20SSL4EO-S12%20v1.1%20facilitates%20open%20research%20and%20provides%20a%20robust%20foundation%20for%20future%20advancements%20in%20self-supervised%20learning%20and%20geospatial%20analysis.%20The%20dataset%20is%20available%20online%20through%20https%3A//huggingface.co/datasets/embed2scale/SSL4EO-S12-v1.1.&entry.1838667208=http%3A//arxiv.org/abs/2503.00168v3&entry.124074799=Read"},
{"title": "Cross-Modal Purification and Fusion for Small-Object RGB-D Transmission-Line Defect Detection", "author": "Jiaming Cui and Wenqiang Li and Shuai Zhou and Ruifeng Qin and Feng Shen", "abstract": "Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.", "link": "http://arxiv.org/abs/2602.01696v4", "date": "2026-02-17", "relevancy": 2.2686, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5813}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5684}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Modal%20Purification%20and%20Fusion%20for%20Small-Object%20RGB-D%20Transmission-Line%20Defect%20Detection&body=Title%3A%20Cross-Modal%20Purification%20and%20Fusion%20for%20Small-Object%20RGB-D%20Transmission-Line%20Defect%20Detection%0AAuthor%3A%20Jiaming%20Cui%20and%20Wenqiang%20Li%20and%20Shuai%20Zhou%20and%20Ruifeng%20Qin%20and%20Feng%20Shen%0AAbstract%3A%20Transmission%20line%20defect%20detection%20remains%20challenging%20for%20automated%20UAV%20inspection%20due%20to%20the%20dominance%20of%20small-scale%20defects%2C%20complex%20backgrounds%2C%20and%20illumination%20variations.%20Existing%20RGB-based%20detectors%2C%20despite%20recent%20progress%2C%20struggle%20to%20distinguish%20geometrically%20subtle%20defects%20from%20visually%20similar%20background%20structures%20under%20limited%20chromatic%20contrast.%20This%20paper%20proposes%20CMAFNet%2C%20a%20Cross-Modal%20Alignment%20and%20Fusion%20Network%20that%20integrates%20RGB%20appearance%20and%20depth%20geometry%20through%20a%20principled%20purify-then-fuse%20paradigm.%20CMAFNet%20consists%20of%20a%20Semantic%20Recomposition%20Module%20that%20performs%20dictionary-based%20feature%20purification%20via%20a%20learned%20codebook%20to%20suppress%20modality-specific%20noise%20while%20preserving%20defect-discriminative%20information%2C%20and%20a%20Contextual%20Semantic%20Integration%20Framework%20that%20captures%20global%20spatial%20dependencies%20using%20partial-channel%20attention%20to%20enhance%20structural%20semantic%20reasoning.%20Position-wise%20normalization%20within%20the%20purification%20stage%20enforces%20explicit%20reconstruction-driven%20cross-modal%20alignment%2C%20ensuring%20statistical%20compatibility%20between%20heterogeneous%20features%20prior%20to%20fusion.%20Extensive%20experiments%20on%20the%20TLRGBD%20benchmark%2C%20where%2094.5%25%20of%20instances%20are%20small%20objects%2C%20demonstrate%20that%20CMAFNet%20achieves%2032.2%25%20mAP%4050%20and%2012.5%25%20APs%2C%20outperforming%20the%20strongest%20baseline%20by%209.8%20and%204.0%20percentage%20points%2C%20respectively.%20A%20lightweight%20variant%20reaches%2024.8%25%20mAP50%20at%20228%20FPS%20with%20only%204.9M%20parameters%2C%20surpassing%20all%20YOLO-based%20detectors%20while%20matching%20transformer-based%20methods%20at%20substantially%20lower%20computational%20cost.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01696v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Modal%2520Purification%2520and%2520Fusion%2520for%2520Small-Object%2520RGB-D%2520Transmission-Line%2520Defect%2520Detection%26entry.906535625%3DJiaming%2520Cui%2520and%2520Wenqiang%2520Li%2520and%2520Shuai%2520Zhou%2520and%2520Ruifeng%2520Qin%2520and%2520Feng%2520Shen%26entry.1292438233%3DTransmission%2520line%2520defect%2520detection%2520remains%2520challenging%2520for%2520automated%2520UAV%2520inspection%2520due%2520to%2520the%2520dominance%2520of%2520small-scale%2520defects%252C%2520complex%2520backgrounds%252C%2520and%2520illumination%2520variations.%2520Existing%2520RGB-based%2520detectors%252C%2520despite%2520recent%2520progress%252C%2520struggle%2520to%2520distinguish%2520geometrically%2520subtle%2520defects%2520from%2520visually%2520similar%2520background%2520structures%2520under%2520limited%2520chromatic%2520contrast.%2520This%2520paper%2520proposes%2520CMAFNet%252C%2520a%2520Cross-Modal%2520Alignment%2520and%2520Fusion%2520Network%2520that%2520integrates%2520RGB%2520appearance%2520and%2520depth%2520geometry%2520through%2520a%2520principled%2520purify-then-fuse%2520paradigm.%2520CMAFNet%2520consists%2520of%2520a%2520Semantic%2520Recomposition%2520Module%2520that%2520performs%2520dictionary-based%2520feature%2520purification%2520via%2520a%2520learned%2520codebook%2520to%2520suppress%2520modality-specific%2520noise%2520while%2520preserving%2520defect-discriminative%2520information%252C%2520and%2520a%2520Contextual%2520Semantic%2520Integration%2520Framework%2520that%2520captures%2520global%2520spatial%2520dependencies%2520using%2520partial-channel%2520attention%2520to%2520enhance%2520structural%2520semantic%2520reasoning.%2520Position-wise%2520normalization%2520within%2520the%2520purification%2520stage%2520enforces%2520explicit%2520reconstruction-driven%2520cross-modal%2520alignment%252C%2520ensuring%2520statistical%2520compatibility%2520between%2520heterogeneous%2520features%2520prior%2520to%2520fusion.%2520Extensive%2520experiments%2520on%2520the%2520TLRGBD%2520benchmark%252C%2520where%252094.5%2525%2520of%2520instances%2520are%2520small%2520objects%252C%2520demonstrate%2520that%2520CMAFNet%2520achieves%252032.2%2525%2520mAP%254050%2520and%252012.5%2525%2520APs%252C%2520outperforming%2520the%2520strongest%2520baseline%2520by%25209.8%2520and%25204.0%2520percentage%2520points%252C%2520respectively.%2520A%2520lightweight%2520variant%2520reaches%252024.8%2525%2520mAP50%2520at%2520228%2520FPS%2520with%2520only%25204.9M%2520parameters%252C%2520surpassing%2520all%2520YOLO-based%2520detectors%2520while%2520matching%2520transformer-based%2520methods%2520at%2520substantially%2520lower%2520computational%2520cost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01696v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Modal%20Purification%20and%20Fusion%20for%20Small-Object%20RGB-D%20Transmission-Line%20Defect%20Detection&entry.906535625=Jiaming%20Cui%20and%20Wenqiang%20Li%20and%20Shuai%20Zhou%20and%20Ruifeng%20Qin%20and%20Feng%20Shen&entry.1292438233=Transmission%20line%20defect%20detection%20remains%20challenging%20for%20automated%20UAV%20inspection%20due%20to%20the%20dominance%20of%20small-scale%20defects%2C%20complex%20backgrounds%2C%20and%20illumination%20variations.%20Existing%20RGB-based%20detectors%2C%20despite%20recent%20progress%2C%20struggle%20to%20distinguish%20geometrically%20subtle%20defects%20from%20visually%20similar%20background%20structures%20under%20limited%20chromatic%20contrast.%20This%20paper%20proposes%20CMAFNet%2C%20a%20Cross-Modal%20Alignment%20and%20Fusion%20Network%20that%20integrates%20RGB%20appearance%20and%20depth%20geometry%20through%20a%20principled%20purify-then-fuse%20paradigm.%20CMAFNet%20consists%20of%20a%20Semantic%20Recomposition%20Module%20that%20performs%20dictionary-based%20feature%20purification%20via%20a%20learned%20codebook%20to%20suppress%20modality-specific%20noise%20while%20preserving%20defect-discriminative%20information%2C%20and%20a%20Contextual%20Semantic%20Integration%20Framework%20that%20captures%20global%20spatial%20dependencies%20using%20partial-channel%20attention%20to%20enhance%20structural%20semantic%20reasoning.%20Position-wise%20normalization%20within%20the%20purification%20stage%20enforces%20explicit%20reconstruction-driven%20cross-modal%20alignment%2C%20ensuring%20statistical%20compatibility%20between%20heterogeneous%20features%20prior%20to%20fusion.%20Extensive%20experiments%20on%20the%20TLRGBD%20benchmark%2C%20where%2094.5%25%20of%20instances%20are%20small%20objects%2C%20demonstrate%20that%20CMAFNet%20achieves%2032.2%25%20mAP%4050%20and%2012.5%25%20APs%2C%20outperforming%20the%20strongest%20baseline%20by%209.8%20and%204.0%20percentage%20points%2C%20respectively.%20A%20lightweight%20variant%20reaches%2024.8%25%20mAP50%20at%20228%20FPS%20with%20only%204.9M%20parameters%2C%20surpassing%20all%20YOLO-based%20detectors%20while%20matching%20transformer-based%20methods%20at%20substantially%20lower%20computational%20cost.&entry.1838667208=http%3A//arxiv.org/abs/2602.01696v4&entry.124074799=Read"},
{"title": "Revisiting Northrop Frye's Four Myths Theory with Large Language Models", "author": "Edirlei Soares de Lima and Marco A. Casanova and Antonio L. Furtado", "abstract": "Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $\u03ba$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.", "link": "http://arxiv.org/abs/2602.15678v1", "date": "2026-02-17", "relevancy": 2.2608, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4694}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4694}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Northrop%20Frye%27s%20Four%20Myths%20Theory%20with%20Large%20Language%20Models&body=Title%3A%20Revisiting%20Northrop%20Frye%27s%20Four%20Myths%20Theory%20with%20Large%20Language%20Models%0AAuthor%3A%20Edirlei%20Soares%20de%20Lima%20and%20Marco%20A.%20Casanova%20and%20Antonio%20L.%20Furtado%0AAbstract%3A%20Northrop%20Frye%27s%20theory%20of%20four%20fundamental%20narrative%20genres%20%28comedy%2C%20romance%2C%20tragedy%2C%20satire%29%20has%20profoundly%20influenced%20literary%20criticism%2C%20yet%20computational%20approaches%20to%20his%20framework%20have%20focused%20primarily%20on%20narrative%20patterns%20rather%20than%20character%20functions.%20In%20this%20paper%2C%20we%20present%20a%20new%20character%20function%20framework%20that%20complements%20pattern-based%20analysis%20by%20examining%20how%20archetypal%20roles%20manifest%20differently%20across%20Frye%27s%20genres.%20Drawing%20on%20Jungian%20archetype%20theory%2C%20we%20derive%20four%20universal%20character%20functions%20%28protagonist%2C%20mentor%2C%20antagonist%2C%20companion%29%20by%20mapping%20them%20to%20Jung%27s%20psychic%20structure%20components.%20These%20functions%20are%20then%20specialized%20into%20sixteen%20genre-specific%20roles%20based%20on%20prototypical%20works.%20To%20validate%20this%20framework%2C%20we%20conducted%20a%20multi-model%20study%20using%20six%20state-of-the-art%20Large%20Language%20Models%20%28LLMs%29%20to%20evaluate%20character-role%20correspondences%20across%2040%20narrative%20works.%20The%20validation%20employed%20both%20positive%20samples%20%28160%20valid%20correspondences%29%20and%20negative%20samples%20%2830%20invalid%20correspondences%29%20to%20evaluate%20whether%20models%20both%20recognize%20valid%20correspondences%20and%20reject%20invalid%20ones.%20LLMs%20achieved%20substantial%20performance%20%28mean%20balanced%20accuracy%20of%2082.5%25%29%20with%20strong%20inter-model%20agreement%20%28Fleiss%27%20%24%CE%BA%24%20%3D%200.600%29%2C%20demonstrating%20that%20the%20proposed%20correspondences%20capture%20systematic%20structural%20patterns.%20Performance%20varied%20by%20genre%20%28ranging%20from%2072.7%25%20to%2089.9%25%29%20and%20role%20%2852.5%25%20to%2099.2%25%29%2C%20with%20qualitative%20analysis%20revealing%20that%20variations%20reflect%20genuine%20narrative%20properties%2C%20including%20functional%20distribution%20in%20romance%20and%20deliberate%20archetypal%20subversion%20in%20satire.%20This%20character-based%20approach%20demonstrates%20the%20potential%20of%20LLM-supported%20methods%20for%20computational%20narratology%20and%20provides%20a%20foundation%20for%20future%20development%20of%20narrative%20generation%20methods%20and%20interactive%20storytelling%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Northrop%2520Frye%2527s%2520Four%2520Myths%2520Theory%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DEdirlei%2520Soares%2520de%2520Lima%2520and%2520Marco%2520A.%2520Casanova%2520and%2520Antonio%2520L.%2520Furtado%26entry.1292438233%3DNorthrop%2520Frye%2527s%2520theory%2520of%2520four%2520fundamental%2520narrative%2520genres%2520%2528comedy%252C%2520romance%252C%2520tragedy%252C%2520satire%2529%2520has%2520profoundly%2520influenced%2520literary%2520criticism%252C%2520yet%2520computational%2520approaches%2520to%2520his%2520framework%2520have%2520focused%2520primarily%2520on%2520narrative%2520patterns%2520rather%2520than%2520character%2520functions.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520new%2520character%2520function%2520framework%2520that%2520complements%2520pattern-based%2520analysis%2520by%2520examining%2520how%2520archetypal%2520roles%2520manifest%2520differently%2520across%2520Frye%2527s%2520genres.%2520Drawing%2520on%2520Jungian%2520archetype%2520theory%252C%2520we%2520derive%2520four%2520universal%2520character%2520functions%2520%2528protagonist%252C%2520mentor%252C%2520antagonist%252C%2520companion%2529%2520by%2520mapping%2520them%2520to%2520Jung%2527s%2520psychic%2520structure%2520components.%2520These%2520functions%2520are%2520then%2520specialized%2520into%2520sixteen%2520genre-specific%2520roles%2520based%2520on%2520prototypical%2520works.%2520To%2520validate%2520this%2520framework%252C%2520we%2520conducted%2520a%2520multi-model%2520study%2520using%2520six%2520state-of-the-art%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520evaluate%2520character-role%2520correspondences%2520across%252040%2520narrative%2520works.%2520The%2520validation%2520employed%2520both%2520positive%2520samples%2520%2528160%2520valid%2520correspondences%2529%2520and%2520negative%2520samples%2520%252830%2520invalid%2520correspondences%2529%2520to%2520evaluate%2520whether%2520models%2520both%2520recognize%2520valid%2520correspondences%2520and%2520reject%2520invalid%2520ones.%2520LLMs%2520achieved%2520substantial%2520performance%2520%2528mean%2520balanced%2520accuracy%2520of%252082.5%2525%2529%2520with%2520strong%2520inter-model%2520agreement%2520%2528Fleiss%2527%2520%2524%25CE%25BA%2524%2520%253D%25200.600%2529%252C%2520demonstrating%2520that%2520the%2520proposed%2520correspondences%2520capture%2520systematic%2520structural%2520patterns.%2520Performance%2520varied%2520by%2520genre%2520%2528ranging%2520from%252072.7%2525%2520to%252089.9%2525%2529%2520and%2520role%2520%252852.5%2525%2520to%252099.2%2525%2529%252C%2520with%2520qualitative%2520analysis%2520revealing%2520that%2520variations%2520reflect%2520genuine%2520narrative%2520properties%252C%2520including%2520functional%2520distribution%2520in%2520romance%2520and%2520deliberate%2520archetypal%2520subversion%2520in%2520satire.%2520This%2520character-based%2520approach%2520demonstrates%2520the%2520potential%2520of%2520LLM-supported%2520methods%2520for%2520computational%2520narratology%2520and%2520provides%2520a%2520foundation%2520for%2520future%2520development%2520of%2520narrative%2520generation%2520methods%2520and%2520interactive%2520storytelling%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Northrop%20Frye%27s%20Four%20Myths%20Theory%20with%20Large%20Language%20Models&entry.906535625=Edirlei%20Soares%20de%20Lima%20and%20Marco%20A.%20Casanova%20and%20Antonio%20L.%20Furtado&entry.1292438233=Northrop%20Frye%27s%20theory%20of%20four%20fundamental%20narrative%20genres%20%28comedy%2C%20romance%2C%20tragedy%2C%20satire%29%20has%20profoundly%20influenced%20literary%20criticism%2C%20yet%20computational%20approaches%20to%20his%20framework%20have%20focused%20primarily%20on%20narrative%20patterns%20rather%20than%20character%20functions.%20In%20this%20paper%2C%20we%20present%20a%20new%20character%20function%20framework%20that%20complements%20pattern-based%20analysis%20by%20examining%20how%20archetypal%20roles%20manifest%20differently%20across%20Frye%27s%20genres.%20Drawing%20on%20Jungian%20archetype%20theory%2C%20we%20derive%20four%20universal%20character%20functions%20%28protagonist%2C%20mentor%2C%20antagonist%2C%20companion%29%20by%20mapping%20them%20to%20Jung%27s%20psychic%20structure%20components.%20These%20functions%20are%20then%20specialized%20into%20sixteen%20genre-specific%20roles%20based%20on%20prototypical%20works.%20To%20validate%20this%20framework%2C%20we%20conducted%20a%20multi-model%20study%20using%20six%20state-of-the-art%20Large%20Language%20Models%20%28LLMs%29%20to%20evaluate%20character-role%20correspondences%20across%2040%20narrative%20works.%20The%20validation%20employed%20both%20positive%20samples%20%28160%20valid%20correspondences%29%20and%20negative%20samples%20%2830%20invalid%20correspondences%29%20to%20evaluate%20whether%20models%20both%20recognize%20valid%20correspondences%20and%20reject%20invalid%20ones.%20LLMs%20achieved%20substantial%20performance%20%28mean%20balanced%20accuracy%20of%2082.5%25%29%20with%20strong%20inter-model%20agreement%20%28Fleiss%27%20%24%CE%BA%24%20%3D%200.600%29%2C%20demonstrating%20that%20the%20proposed%20correspondences%20capture%20systematic%20structural%20patterns.%20Performance%20varied%20by%20genre%20%28ranging%20from%2072.7%25%20to%2089.9%25%29%20and%20role%20%2852.5%25%20to%2099.2%25%29%2C%20with%20qualitative%20analysis%20revealing%20that%20variations%20reflect%20genuine%20narrative%20properties%2C%20including%20functional%20distribution%20in%20romance%20and%20deliberate%20archetypal%20subversion%20in%20satire.%20This%20character-based%20approach%20demonstrates%20the%20potential%20of%20LLM-supported%20methods%20for%20computational%20narratology%20and%20provides%20a%20foundation%20for%20future%20development%20of%20narrative%20generation%20methods%20and%20interactive%20storytelling%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2602.15678v1&entry.124074799=Read"},
{"title": "Selective Perception for Robot: Task-Aware Attention in Multimodal VLA", "author": "Young-Chae Son and Jung-Woo Lee and Yoon-Ji Choi and Dae-Kwan Ko and Soo-Chul Lim", "abstract": "In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.", "link": "http://arxiv.org/abs/2602.15543v1", "date": "2026-02-17", "relevancy": 2.2575, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6043}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5645}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Perception%20for%20Robot%3A%20Task-Aware%20Attention%20in%20Multimodal%20VLA&body=Title%3A%20Selective%20Perception%20for%20Robot%3A%20Task-Aware%20Attention%20in%20Multimodal%20VLA%0AAuthor%3A%20Young-Chae%20Son%20and%20Jung-Woo%20Lee%20and%20Yoon-Ji%20Choi%20and%20Dae-Kwan%20Ko%20and%20Soo-Chul%20Lim%0AAbstract%3A%20In%20robotics%2C%20Vision-Language-Action%20%28VLA%29%20models%20that%20integrate%20diverse%20multimodal%20signals%20from%20multi-view%20inputs%20have%20emerged%20as%20an%20effective%20approach.%20However%2C%20most%20prior%20work%20adopts%20static%20fusion%20that%20processes%20all%20visual%20inputs%20uniformly%2C%20which%20incurs%20unnecessary%20computational%20overhead%20and%20allows%20task-irrelevant%20background%20information%20to%20act%20as%20noise.%20Inspired%20by%20the%20principles%20of%20human%20active%20perception%2C%20we%20propose%20a%20dynamic%20information%20fusion%20framework%20designed%20to%20maximize%20the%20efficiency%20and%20robustness%20of%20VLA%20models.%20Our%20approach%20introduces%20a%20lightweight%20adaptive%20routing%20architecture%20that%20analyzes%20the%20current%20text%20prompt%20and%20observations%20from%20a%20wrist-mounted%20camera%20in%20real-time%20to%20predict%20the%20task-relevance%20of%20multiple%20camera%20views.%20By%20conditionally%20attenuating%20computations%20for%20views%20with%20low%20informational%20utility%20and%20selectively%20providing%20only%20essential%20visual%20features%20to%20the%20policy%20network%2C%20Our%20framework%20achieves%20computation%20efficiency%20proportional%20to%20task%20relevance.%20Furthermore%2C%20to%20efficiently%20secure%20large-scale%20annotation%20data%20for%20router%20training%2C%20we%20established%20an%20automated%20labeling%20pipeline%20utilizing%20Vision-Language%20Models%20%28VLMs%29%20to%20minimize%20data%20collection%20and%20annotation%20costs.%20Experimental%20results%20in%20real-world%20robotic%20manipulation%20scenarios%20demonstrate%20that%20the%20proposed%20approach%20achieves%20significant%20improvements%20in%20both%20inference%20efficiency%20and%20control%20performance%20compared%20to%20existing%20VLA%20models%2C%20validating%20the%20effectiveness%20and%20practicality%20of%20dynamic%20information%20fusion%20in%20resource-constrained%2C%20real-time%20robot%20control%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Perception%2520for%2520Robot%253A%2520Task-Aware%2520Attention%2520in%2520Multimodal%2520VLA%26entry.906535625%3DYoung-Chae%2520Son%2520and%2520Jung-Woo%2520Lee%2520and%2520Yoon-Ji%2520Choi%2520and%2520Dae-Kwan%2520Ko%2520and%2520Soo-Chul%2520Lim%26entry.1292438233%3DIn%2520robotics%252C%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520that%2520integrate%2520diverse%2520multimodal%2520signals%2520from%2520multi-view%2520inputs%2520have%2520emerged%2520as%2520an%2520effective%2520approach.%2520However%252C%2520most%2520prior%2520work%2520adopts%2520static%2520fusion%2520that%2520processes%2520all%2520visual%2520inputs%2520uniformly%252C%2520which%2520incurs%2520unnecessary%2520computational%2520overhead%2520and%2520allows%2520task-irrelevant%2520background%2520information%2520to%2520act%2520as%2520noise.%2520Inspired%2520by%2520the%2520principles%2520of%2520human%2520active%2520perception%252C%2520we%2520propose%2520a%2520dynamic%2520information%2520fusion%2520framework%2520designed%2520to%2520maximize%2520the%2520efficiency%2520and%2520robustness%2520of%2520VLA%2520models.%2520Our%2520approach%2520introduces%2520a%2520lightweight%2520adaptive%2520routing%2520architecture%2520that%2520analyzes%2520the%2520current%2520text%2520prompt%2520and%2520observations%2520from%2520a%2520wrist-mounted%2520camera%2520in%2520real-time%2520to%2520predict%2520the%2520task-relevance%2520of%2520multiple%2520camera%2520views.%2520By%2520conditionally%2520attenuating%2520computations%2520for%2520views%2520with%2520low%2520informational%2520utility%2520and%2520selectively%2520providing%2520only%2520essential%2520visual%2520features%2520to%2520the%2520policy%2520network%252C%2520Our%2520framework%2520achieves%2520computation%2520efficiency%2520proportional%2520to%2520task%2520relevance.%2520Furthermore%252C%2520to%2520efficiently%2520secure%2520large-scale%2520annotation%2520data%2520for%2520router%2520training%252C%2520we%2520established%2520an%2520automated%2520labeling%2520pipeline%2520utilizing%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520minimize%2520data%2520collection%2520and%2520annotation%2520costs.%2520Experimental%2520results%2520in%2520real-world%2520robotic%2520manipulation%2520scenarios%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520achieves%2520significant%2520improvements%2520in%2520both%2520inference%2520efficiency%2520and%2520control%2520performance%2520compared%2520to%2520existing%2520VLA%2520models%252C%2520validating%2520the%2520effectiveness%2520and%2520practicality%2520of%2520dynamic%2520information%2520fusion%2520in%2520resource-constrained%252C%2520real-time%2520robot%2520control%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Perception%20for%20Robot%3A%20Task-Aware%20Attention%20in%20Multimodal%20VLA&entry.906535625=Young-Chae%20Son%20and%20Jung-Woo%20Lee%20and%20Yoon-Ji%20Choi%20and%20Dae-Kwan%20Ko%20and%20Soo-Chul%20Lim&entry.1292438233=In%20robotics%2C%20Vision-Language-Action%20%28VLA%29%20models%20that%20integrate%20diverse%20multimodal%20signals%20from%20multi-view%20inputs%20have%20emerged%20as%20an%20effective%20approach.%20However%2C%20most%20prior%20work%20adopts%20static%20fusion%20that%20processes%20all%20visual%20inputs%20uniformly%2C%20which%20incurs%20unnecessary%20computational%20overhead%20and%20allows%20task-irrelevant%20background%20information%20to%20act%20as%20noise.%20Inspired%20by%20the%20principles%20of%20human%20active%20perception%2C%20we%20propose%20a%20dynamic%20information%20fusion%20framework%20designed%20to%20maximize%20the%20efficiency%20and%20robustness%20of%20VLA%20models.%20Our%20approach%20introduces%20a%20lightweight%20adaptive%20routing%20architecture%20that%20analyzes%20the%20current%20text%20prompt%20and%20observations%20from%20a%20wrist-mounted%20camera%20in%20real-time%20to%20predict%20the%20task-relevance%20of%20multiple%20camera%20views.%20By%20conditionally%20attenuating%20computations%20for%20views%20with%20low%20informational%20utility%20and%20selectively%20providing%20only%20essential%20visual%20features%20to%20the%20policy%20network%2C%20Our%20framework%20achieves%20computation%20efficiency%20proportional%20to%20task%20relevance.%20Furthermore%2C%20to%20efficiently%20secure%20large-scale%20annotation%20data%20for%20router%20training%2C%20we%20established%20an%20automated%20labeling%20pipeline%20utilizing%20Vision-Language%20Models%20%28VLMs%29%20to%20minimize%20data%20collection%20and%20annotation%20costs.%20Experimental%20results%20in%20real-world%20robotic%20manipulation%20scenarios%20demonstrate%20that%20the%20proposed%20approach%20achieves%20significant%20improvements%20in%20both%20inference%20efficiency%20and%20control%20performance%20compared%20to%20existing%20VLA%20models%2C%20validating%20the%20effectiveness%20and%20practicality%20of%20dynamic%20information%20fusion%20in%20resource-constrained%2C%20real-time%20robot%20control%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2602.15543v1&entry.124074799=Read"},
{"title": "Online GPU Energy Optimization with Switching-Aware Bandits", "author": "Xiongxiao Xu and Solomon Abera Bekele and Brice Videau and Kai Shu", "abstract": "Energy consumption has become a bottleneck for future computing architectures, from wearable devices to leadership-class supercomputers. Existing energy management techniques largely target CPUs, even though GPUs now dominate power draw in heterogeneous high performance computing (HPC) systems. Moreover, many prior methods rely on either purely offline or hybrid offline and online training, which is impractical and results in energy inefficiencies during data collection. In this paper, we introduce a practical online GPU energy optimization problem in a HPC scenarios. The problem is challenging because (1) GPU frequency scaling exhibits performance-energy trade-offs, (2) online control must balance exploration and exploitation, and (3) frequent frequency switching incurs non-trivial overhead and degrades quality of service (QoS). To address the challenges, we formulate online GPU energy optimization as a multi-armed bandit problem and propose EnergyUCB, a lightweight UCB-based controller that dynamically adjusts GPU core frequency in real time to save energy. Specifically, EnergyUCB (1) defines a reward that jointly captures energy and performance using a core-to-uncore utilization ratio as a proxy for GPU throughput, (2) employs optimistic initialization and UCB-style confidence bonuses to accelerate learning from scratch, and (3) incorporates a switching-aware UCB index and a QoS-constrained variant that enforce explicit slowdown budgets while discouraging unnecessary frequency oscillations. Extensive experiments on real-world workloads from the world's third fastest supercomputer Aurora show that EnergyUCB achieves substantial energy savings with modest slowdown and that the QoS-constrained variant reliably respects user-specified performance budgets.", "link": "http://arxiv.org/abs/2410.11855v2", "date": "2026-02-17", "relevancy": 2.2452, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4553}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4459}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20GPU%20Energy%20Optimization%20with%20Switching-Aware%20Bandits&body=Title%3A%20Online%20GPU%20Energy%20Optimization%20with%20Switching-Aware%20Bandits%0AAuthor%3A%20Xiongxiao%20Xu%20and%20Solomon%20Abera%20Bekele%20and%20Brice%20Videau%20and%20Kai%20Shu%0AAbstract%3A%20Energy%20consumption%20has%20become%20a%20bottleneck%20for%20future%20computing%20architectures%2C%20from%20wearable%20devices%20to%20leadership-class%20supercomputers.%20Existing%20energy%20management%20techniques%20largely%20target%20CPUs%2C%20even%20though%20GPUs%20now%20dominate%20power%20draw%20in%20heterogeneous%20high%20performance%20computing%20%28HPC%29%20systems.%20Moreover%2C%20many%20prior%20methods%20rely%20on%20either%20purely%20offline%20or%20hybrid%20offline%20and%20online%20training%2C%20which%20is%20impractical%20and%20results%20in%20energy%20inefficiencies%20during%20data%20collection.%20In%20this%20paper%2C%20we%20introduce%20a%20practical%20online%20GPU%20energy%20optimization%20problem%20in%20a%20HPC%20scenarios.%20The%20problem%20is%20challenging%20because%20%281%29%20GPU%20frequency%20scaling%20exhibits%20performance-energy%20trade-offs%2C%20%282%29%20online%20control%20must%20balance%20exploration%20and%20exploitation%2C%20and%20%283%29%20frequent%20frequency%20switching%20incurs%20non-trivial%20overhead%20and%20degrades%20quality%20of%20service%20%28QoS%29.%20To%20address%20the%20challenges%2C%20we%20formulate%20online%20GPU%20energy%20optimization%20as%20a%20multi-armed%20bandit%20problem%20and%20propose%20EnergyUCB%2C%20a%20lightweight%20UCB-based%20controller%20that%20dynamically%20adjusts%20GPU%20core%20frequency%20in%20real%20time%20to%20save%20energy.%20Specifically%2C%20EnergyUCB%20%281%29%20defines%20a%20reward%20that%20jointly%20captures%20energy%20and%20performance%20using%20a%20core-to-uncore%20utilization%20ratio%20as%20a%20proxy%20for%20GPU%20throughput%2C%20%282%29%20employs%20optimistic%20initialization%20and%20UCB-style%20confidence%20bonuses%20to%20accelerate%20learning%20from%20scratch%2C%20and%20%283%29%20incorporates%20a%20switching-aware%20UCB%20index%20and%20a%20QoS-constrained%20variant%20that%20enforce%20explicit%20slowdown%20budgets%20while%20discouraging%20unnecessary%20frequency%20oscillations.%20Extensive%20experiments%20on%20real-world%20workloads%20from%20the%20world%27s%20third%20fastest%20supercomputer%20Aurora%20show%20that%20EnergyUCB%20achieves%20substantial%20energy%20savings%20with%20modest%20slowdown%20and%20that%20the%20QoS-constrained%20variant%20reliably%20respects%20user-specified%20performance%20budgets.%0ALink%3A%20http%3A//arxiv.org/abs/2410.11855v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520GPU%2520Energy%2520Optimization%2520with%2520Switching-Aware%2520Bandits%26entry.906535625%3DXiongxiao%2520Xu%2520and%2520Solomon%2520Abera%2520Bekele%2520and%2520Brice%2520Videau%2520and%2520Kai%2520Shu%26entry.1292438233%3DEnergy%2520consumption%2520has%2520become%2520a%2520bottleneck%2520for%2520future%2520computing%2520architectures%252C%2520from%2520wearable%2520devices%2520to%2520leadership-class%2520supercomputers.%2520Existing%2520energy%2520management%2520techniques%2520largely%2520target%2520CPUs%252C%2520even%2520though%2520GPUs%2520now%2520dominate%2520power%2520draw%2520in%2520heterogeneous%2520high%2520performance%2520computing%2520%2528HPC%2529%2520systems.%2520Moreover%252C%2520many%2520prior%2520methods%2520rely%2520on%2520either%2520purely%2520offline%2520or%2520hybrid%2520offline%2520and%2520online%2520training%252C%2520which%2520is%2520impractical%2520and%2520results%2520in%2520energy%2520inefficiencies%2520during%2520data%2520collection.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520practical%2520online%2520GPU%2520energy%2520optimization%2520problem%2520in%2520a%2520HPC%2520scenarios.%2520The%2520problem%2520is%2520challenging%2520because%2520%25281%2529%2520GPU%2520frequency%2520scaling%2520exhibits%2520performance-energy%2520trade-offs%252C%2520%25282%2529%2520online%2520control%2520must%2520balance%2520exploration%2520and%2520exploitation%252C%2520and%2520%25283%2529%2520frequent%2520frequency%2520switching%2520incurs%2520non-trivial%2520overhead%2520and%2520degrades%2520quality%2520of%2520service%2520%2528QoS%2529.%2520To%2520address%2520the%2520challenges%252C%2520we%2520formulate%2520online%2520GPU%2520energy%2520optimization%2520as%2520a%2520multi-armed%2520bandit%2520problem%2520and%2520propose%2520EnergyUCB%252C%2520a%2520lightweight%2520UCB-based%2520controller%2520that%2520dynamically%2520adjusts%2520GPU%2520core%2520frequency%2520in%2520real%2520time%2520to%2520save%2520energy.%2520Specifically%252C%2520EnergyUCB%2520%25281%2529%2520defines%2520a%2520reward%2520that%2520jointly%2520captures%2520energy%2520and%2520performance%2520using%2520a%2520core-to-uncore%2520utilization%2520ratio%2520as%2520a%2520proxy%2520for%2520GPU%2520throughput%252C%2520%25282%2529%2520employs%2520optimistic%2520initialization%2520and%2520UCB-style%2520confidence%2520bonuses%2520to%2520accelerate%2520learning%2520from%2520scratch%252C%2520and%2520%25283%2529%2520incorporates%2520a%2520switching-aware%2520UCB%2520index%2520and%2520a%2520QoS-constrained%2520variant%2520that%2520enforce%2520explicit%2520slowdown%2520budgets%2520while%2520discouraging%2520unnecessary%2520frequency%2520oscillations.%2520Extensive%2520experiments%2520on%2520real-world%2520workloads%2520from%2520the%2520world%2527s%2520third%2520fastest%2520supercomputer%2520Aurora%2520show%2520that%2520EnergyUCB%2520achieves%2520substantial%2520energy%2520savings%2520with%2520modest%2520slowdown%2520and%2520that%2520the%2520QoS-constrained%2520variant%2520reliably%2520respects%2520user-specified%2520performance%2520budgets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11855v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20GPU%20Energy%20Optimization%20with%20Switching-Aware%20Bandits&entry.906535625=Xiongxiao%20Xu%20and%20Solomon%20Abera%20Bekele%20and%20Brice%20Videau%20and%20Kai%20Shu&entry.1292438233=Energy%20consumption%20has%20become%20a%20bottleneck%20for%20future%20computing%20architectures%2C%20from%20wearable%20devices%20to%20leadership-class%20supercomputers.%20Existing%20energy%20management%20techniques%20largely%20target%20CPUs%2C%20even%20though%20GPUs%20now%20dominate%20power%20draw%20in%20heterogeneous%20high%20performance%20computing%20%28HPC%29%20systems.%20Moreover%2C%20many%20prior%20methods%20rely%20on%20either%20purely%20offline%20or%20hybrid%20offline%20and%20online%20training%2C%20which%20is%20impractical%20and%20results%20in%20energy%20inefficiencies%20during%20data%20collection.%20In%20this%20paper%2C%20we%20introduce%20a%20practical%20online%20GPU%20energy%20optimization%20problem%20in%20a%20HPC%20scenarios.%20The%20problem%20is%20challenging%20because%20%281%29%20GPU%20frequency%20scaling%20exhibits%20performance-energy%20trade-offs%2C%20%282%29%20online%20control%20must%20balance%20exploration%20and%20exploitation%2C%20and%20%283%29%20frequent%20frequency%20switching%20incurs%20non-trivial%20overhead%20and%20degrades%20quality%20of%20service%20%28QoS%29.%20To%20address%20the%20challenges%2C%20we%20formulate%20online%20GPU%20energy%20optimization%20as%20a%20multi-armed%20bandit%20problem%20and%20propose%20EnergyUCB%2C%20a%20lightweight%20UCB-based%20controller%20that%20dynamically%20adjusts%20GPU%20core%20frequency%20in%20real%20time%20to%20save%20energy.%20Specifically%2C%20EnergyUCB%20%281%29%20defines%20a%20reward%20that%20jointly%20captures%20energy%20and%20performance%20using%20a%20core-to-uncore%20utilization%20ratio%20as%20a%20proxy%20for%20GPU%20throughput%2C%20%282%29%20employs%20optimistic%20initialization%20and%20UCB-style%20confidence%20bonuses%20to%20accelerate%20learning%20from%20scratch%2C%20and%20%283%29%20incorporates%20a%20switching-aware%20UCB%20index%20and%20a%20QoS-constrained%20variant%20that%20enforce%20explicit%20slowdown%20budgets%20while%20discouraging%20unnecessary%20frequency%20oscillations.%20Extensive%20experiments%20on%20real-world%20workloads%20from%20the%20world%27s%20third%20fastest%20supercomputer%20Aurora%20show%20that%20EnergyUCB%20achieves%20substantial%20energy%20savings%20with%20modest%20slowdown%20and%20that%20the%20QoS-constrained%20variant%20reliably%20respects%20user-specified%20performance%20budgets.&entry.1838667208=http%3A//arxiv.org/abs/2410.11855v2&entry.124074799=Read"},
{"title": "Token-Based Audio Inpainting via Discrete Diffusion", "author": "Tali Dror and Iftach Shoham and Moshe Buchris and Oren Gal and Haim Permuter and Gilad Katz and Eliya Nachmani", "abstract": "Audio inpainting seeks to restore missing segments in degraded recordings. Previous diffusion-based methods exhibit impaired performance when the missing region is large. We introduce the first approach that applies discrete diffusion over tokenized music representations from a pre-trained audio tokenizer, enabling stable and semantically coherent restoration of long gaps. Our method further incorporates two training approaches: a derivative-based regularization loss that enforces smooth temporal dynamics, and a span-based absorbing transition that provides structured corruption during diffusion. Experiments on the MusicNet and MAESTRO datasets with gaps up to 750 ms show that our approach consistently outperforms strong baselines across range of gap lengths, for gaps of 150 ms and above. This work advances musical audio restoration and introduces new directions for discrete diffusion model training. Visit our project page for examples and code.", "link": "http://arxiv.org/abs/2507.08333v4", "date": "2026-02-17", "relevancy": 2.2195, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5762}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5406}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-Based%20Audio%20Inpainting%20via%20Discrete%20Diffusion&body=Title%3A%20Token-Based%20Audio%20Inpainting%20via%20Discrete%20Diffusion%0AAuthor%3A%20Tali%20Dror%20and%20Iftach%20Shoham%20and%20Moshe%20Buchris%20and%20Oren%20Gal%20and%20Haim%20Permuter%20and%20Gilad%20Katz%20and%20Eliya%20Nachmani%0AAbstract%3A%20Audio%20inpainting%20seeks%20to%20restore%20missing%20segments%20in%20degraded%20recordings.%20Previous%20diffusion-based%20methods%20exhibit%20impaired%20performance%20when%20the%20missing%20region%20is%20large.%20We%20introduce%20the%20first%20approach%20that%20applies%20discrete%20diffusion%20over%20tokenized%20music%20representations%20from%20a%20pre-trained%20audio%20tokenizer%2C%20enabling%20stable%20and%20semantically%20coherent%20restoration%20of%20long%20gaps.%20Our%20method%20further%20incorporates%20two%20training%20approaches%3A%20a%20derivative-based%20regularization%20loss%20that%20enforces%20smooth%20temporal%20dynamics%2C%20and%20a%20span-based%20absorbing%20transition%20that%20provides%20structured%20corruption%20during%20diffusion.%20Experiments%20on%20the%20MusicNet%20and%20MAESTRO%20datasets%20with%20gaps%20up%20to%20750%20ms%20show%20that%20our%20approach%20consistently%20outperforms%20strong%20baselines%20across%20range%20of%20gap%20lengths%2C%20for%20gaps%20of%20150%20ms%20and%20above.%20This%20work%20advances%20musical%20audio%20restoration%20and%20introduces%20new%20directions%20for%20discrete%20diffusion%20model%20training.%20Visit%20our%20project%20page%20for%20examples%20and%20code.%0ALink%3A%20http%3A//arxiv.org/abs/2507.08333v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-Based%2520Audio%2520Inpainting%2520via%2520Discrete%2520Diffusion%26entry.906535625%3DTali%2520Dror%2520and%2520Iftach%2520Shoham%2520and%2520Moshe%2520Buchris%2520and%2520Oren%2520Gal%2520and%2520Haim%2520Permuter%2520and%2520Gilad%2520Katz%2520and%2520Eliya%2520Nachmani%26entry.1292438233%3DAudio%2520inpainting%2520seeks%2520to%2520restore%2520missing%2520segments%2520in%2520degraded%2520recordings.%2520Previous%2520diffusion-based%2520methods%2520exhibit%2520impaired%2520performance%2520when%2520the%2520missing%2520region%2520is%2520large.%2520We%2520introduce%2520the%2520first%2520approach%2520that%2520applies%2520discrete%2520diffusion%2520over%2520tokenized%2520music%2520representations%2520from%2520a%2520pre-trained%2520audio%2520tokenizer%252C%2520enabling%2520stable%2520and%2520semantically%2520coherent%2520restoration%2520of%2520long%2520gaps.%2520Our%2520method%2520further%2520incorporates%2520two%2520training%2520approaches%253A%2520a%2520derivative-based%2520regularization%2520loss%2520that%2520enforces%2520smooth%2520temporal%2520dynamics%252C%2520and%2520a%2520span-based%2520absorbing%2520transition%2520that%2520provides%2520structured%2520corruption%2520during%2520diffusion.%2520Experiments%2520on%2520the%2520MusicNet%2520and%2520MAESTRO%2520datasets%2520with%2520gaps%2520up%2520to%2520750%2520ms%2520show%2520that%2520our%2520approach%2520consistently%2520outperforms%2520strong%2520baselines%2520across%2520range%2520of%2520gap%2520lengths%252C%2520for%2520gaps%2520of%2520150%2520ms%2520and%2520above.%2520This%2520work%2520advances%2520musical%2520audio%2520restoration%2520and%2520introduces%2520new%2520directions%2520for%2520discrete%2520diffusion%2520model%2520training.%2520Visit%2520our%2520project%2520page%2520for%2520examples%2520and%2520code.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08333v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-Based%20Audio%20Inpainting%20via%20Discrete%20Diffusion&entry.906535625=Tali%20Dror%20and%20Iftach%20Shoham%20and%20Moshe%20Buchris%20and%20Oren%20Gal%20and%20Haim%20Permuter%20and%20Gilad%20Katz%20and%20Eliya%20Nachmani&entry.1292438233=Audio%20inpainting%20seeks%20to%20restore%20missing%20segments%20in%20degraded%20recordings.%20Previous%20diffusion-based%20methods%20exhibit%20impaired%20performance%20when%20the%20missing%20region%20is%20large.%20We%20introduce%20the%20first%20approach%20that%20applies%20discrete%20diffusion%20over%20tokenized%20music%20representations%20from%20a%20pre-trained%20audio%20tokenizer%2C%20enabling%20stable%20and%20semantically%20coherent%20restoration%20of%20long%20gaps.%20Our%20method%20further%20incorporates%20two%20training%20approaches%3A%20a%20derivative-based%20regularization%20loss%20that%20enforces%20smooth%20temporal%20dynamics%2C%20and%20a%20span-based%20absorbing%20transition%20that%20provides%20structured%20corruption%20during%20diffusion.%20Experiments%20on%20the%20MusicNet%20and%20MAESTRO%20datasets%20with%20gaps%20up%20to%20750%20ms%20show%20that%20our%20approach%20consistently%20outperforms%20strong%20baselines%20across%20range%20of%20gap%20lengths%2C%20for%20gaps%20of%20150%20ms%20and%20above.%20This%20work%20advances%20musical%20audio%20restoration%20and%20introduces%20new%20directions%20for%20discrete%20diffusion%20model%20training.%20Visit%20our%20project%20page%20for%20examples%20and%20code.&entry.1838667208=http%3A//arxiv.org/abs/2507.08333v4&entry.124074799=Read"},
{"title": "RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution", "author": "Youngwan Jin and Incheol Park and Yagiz Nalcakan and Hyeongjin Ju and Sanghyeop Yeo and Shiho Kim", "abstract": "General-purpose super-resolution models, particularly Vision Transformers, have achieved remarkable success but exhibit fundamental inefficiencies in common infrared imaging scenarios like surveillance and autonomous driving, which operate from fixed or nearly-static viewpoints. These models fail to exploit the strong, persistent spatial priors inherent in such scenes, leading to redundant learning and suboptimal performance. To address this, we propose the Regional Prior attention Transformer for infrared image Super-Resolution (RPT-SR), a novel architecture that explicitly encodes scene layout information into the attention mechanism. Our core contribution is a dual-token framework that fuses (1) learnable, regional prior tokens, which act as a persistent memory for the scene's global structure, with (2) local tokens that capture the frame-specific content of the current input. By utilizing these tokens into an attention, our model allows the priors to dynamically modulate the local reconstruction process. Extensive experiments validate our approach. While most prior works focus on a single infrared band, we demonstrate the broad applicability and versatility of RPT-SR by establishing new state-of-the-art performance across diverse datasets covering both Long-Wave (LWIR) and Short-Wave (SWIR) spectra", "link": "http://arxiv.org/abs/2602.15490v1", "date": "2026-02-17", "relevancy": 2.2176, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5623}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5575}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RPT-SR%3A%20Regional%20Prior%20attention%20Transformer%20for%20infrared%20image%20Super-Resolution&body=Title%3A%20RPT-SR%3A%20Regional%20Prior%20attention%20Transformer%20for%20infrared%20image%20Super-Resolution%0AAuthor%3A%20Youngwan%20Jin%20and%20Incheol%20Park%20and%20Yagiz%20Nalcakan%20and%20Hyeongjin%20Ju%20and%20Sanghyeop%20Yeo%20and%20Shiho%20Kim%0AAbstract%3A%20General-purpose%20super-resolution%20models%2C%20particularly%20Vision%20Transformers%2C%20have%20achieved%20remarkable%20success%20but%20exhibit%20fundamental%20inefficiencies%20in%20common%20infrared%20imaging%20scenarios%20like%20surveillance%20and%20autonomous%20driving%2C%20which%20operate%20from%20fixed%20or%20nearly-static%20viewpoints.%20These%20models%20fail%20to%20exploit%20the%20strong%2C%20persistent%20spatial%20priors%20inherent%20in%20such%20scenes%2C%20leading%20to%20redundant%20learning%20and%20suboptimal%20performance.%20To%20address%20this%2C%20we%20propose%20the%20Regional%20Prior%20attention%20Transformer%20for%20infrared%20image%20Super-Resolution%20%28RPT-SR%29%2C%20a%20novel%20architecture%20that%20explicitly%20encodes%20scene%20layout%20information%20into%20the%20attention%20mechanism.%20Our%20core%20contribution%20is%20a%20dual-token%20framework%20that%20fuses%20%281%29%20learnable%2C%20regional%20prior%20tokens%2C%20which%20act%20as%20a%20persistent%20memory%20for%20the%20scene%27s%20global%20structure%2C%20with%20%282%29%20local%20tokens%20that%20capture%20the%20frame-specific%20content%20of%20the%20current%20input.%20By%20utilizing%20these%20tokens%20into%20an%20attention%2C%20our%20model%20allows%20the%20priors%20to%20dynamically%20modulate%20the%20local%20reconstruction%20process.%20Extensive%20experiments%20validate%20our%20approach.%20While%20most%20prior%20works%20focus%20on%20a%20single%20infrared%20band%2C%20we%20demonstrate%20the%20broad%20applicability%20and%20versatility%20of%20RPT-SR%20by%20establishing%20new%20state-of-the-art%20performance%20across%20diverse%20datasets%20covering%20both%20Long-Wave%20%28LWIR%29%20and%20Short-Wave%20%28SWIR%29%20spectra%0ALink%3A%20http%3A//arxiv.org/abs/2602.15490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRPT-SR%253A%2520Regional%2520Prior%2520attention%2520Transformer%2520for%2520infrared%2520image%2520Super-Resolution%26entry.906535625%3DYoungwan%2520Jin%2520and%2520Incheol%2520Park%2520and%2520Yagiz%2520Nalcakan%2520and%2520Hyeongjin%2520Ju%2520and%2520Sanghyeop%2520Yeo%2520and%2520Shiho%2520Kim%26entry.1292438233%3DGeneral-purpose%2520super-resolution%2520models%252C%2520particularly%2520Vision%2520Transformers%252C%2520have%2520achieved%2520remarkable%2520success%2520but%2520exhibit%2520fundamental%2520inefficiencies%2520in%2520common%2520infrared%2520imaging%2520scenarios%2520like%2520surveillance%2520and%2520autonomous%2520driving%252C%2520which%2520operate%2520from%2520fixed%2520or%2520nearly-static%2520viewpoints.%2520These%2520models%2520fail%2520to%2520exploit%2520the%2520strong%252C%2520persistent%2520spatial%2520priors%2520inherent%2520in%2520such%2520scenes%252C%2520leading%2520to%2520redundant%2520learning%2520and%2520suboptimal%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Regional%2520Prior%2520attention%2520Transformer%2520for%2520infrared%2520image%2520Super-Resolution%2520%2528RPT-SR%2529%252C%2520a%2520novel%2520architecture%2520that%2520explicitly%2520encodes%2520scene%2520layout%2520information%2520into%2520the%2520attention%2520mechanism.%2520Our%2520core%2520contribution%2520is%2520a%2520dual-token%2520framework%2520that%2520fuses%2520%25281%2529%2520learnable%252C%2520regional%2520prior%2520tokens%252C%2520which%2520act%2520as%2520a%2520persistent%2520memory%2520for%2520the%2520scene%2527s%2520global%2520structure%252C%2520with%2520%25282%2529%2520local%2520tokens%2520that%2520capture%2520the%2520frame-specific%2520content%2520of%2520the%2520current%2520input.%2520By%2520utilizing%2520these%2520tokens%2520into%2520an%2520attention%252C%2520our%2520model%2520allows%2520the%2520priors%2520to%2520dynamically%2520modulate%2520the%2520local%2520reconstruction%2520process.%2520Extensive%2520experiments%2520validate%2520our%2520approach.%2520While%2520most%2520prior%2520works%2520focus%2520on%2520a%2520single%2520infrared%2520band%252C%2520we%2520demonstrate%2520the%2520broad%2520applicability%2520and%2520versatility%2520of%2520RPT-SR%2520by%2520establishing%2520new%2520state-of-the-art%2520performance%2520across%2520diverse%2520datasets%2520covering%2520both%2520Long-Wave%2520%2528LWIR%2529%2520and%2520Short-Wave%2520%2528SWIR%2529%2520spectra%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RPT-SR%3A%20Regional%20Prior%20attention%20Transformer%20for%20infrared%20image%20Super-Resolution&entry.906535625=Youngwan%20Jin%20and%20Incheol%20Park%20and%20Yagiz%20Nalcakan%20and%20Hyeongjin%20Ju%20and%20Sanghyeop%20Yeo%20and%20Shiho%20Kim&entry.1292438233=General-purpose%20super-resolution%20models%2C%20particularly%20Vision%20Transformers%2C%20have%20achieved%20remarkable%20success%20but%20exhibit%20fundamental%20inefficiencies%20in%20common%20infrared%20imaging%20scenarios%20like%20surveillance%20and%20autonomous%20driving%2C%20which%20operate%20from%20fixed%20or%20nearly-static%20viewpoints.%20These%20models%20fail%20to%20exploit%20the%20strong%2C%20persistent%20spatial%20priors%20inherent%20in%20such%20scenes%2C%20leading%20to%20redundant%20learning%20and%20suboptimal%20performance.%20To%20address%20this%2C%20we%20propose%20the%20Regional%20Prior%20attention%20Transformer%20for%20infrared%20image%20Super-Resolution%20%28RPT-SR%29%2C%20a%20novel%20architecture%20that%20explicitly%20encodes%20scene%20layout%20information%20into%20the%20attention%20mechanism.%20Our%20core%20contribution%20is%20a%20dual-token%20framework%20that%20fuses%20%281%29%20learnable%2C%20regional%20prior%20tokens%2C%20which%20act%20as%20a%20persistent%20memory%20for%20the%20scene%27s%20global%20structure%2C%20with%20%282%29%20local%20tokens%20that%20capture%20the%20frame-specific%20content%20of%20the%20current%20input.%20By%20utilizing%20these%20tokens%20into%20an%20attention%2C%20our%20model%20allows%20the%20priors%20to%20dynamically%20modulate%20the%20local%20reconstruction%20process.%20Extensive%20experiments%20validate%20our%20approach.%20While%20most%20prior%20works%20focus%20on%20a%20single%20infrared%20band%2C%20we%20demonstrate%20the%20broad%20applicability%20and%20versatility%20of%20RPT-SR%20by%20establishing%20new%20state-of-the-art%20performance%20across%20diverse%20datasets%20covering%20both%20Long-Wave%20%28LWIR%29%20and%20Short-Wave%20%28SWIR%29%20spectra&entry.1838667208=http%3A//arxiv.org/abs/2602.15490v1&entry.124074799=Read"},
{"title": "Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings", "author": "Suhyung Jang and Ghang Lee and Jaekun Lee and Hyunjun Lee", "abstract": "Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.", "link": "http://arxiv.org/abs/2602.15791v1", "date": "2026-02-17", "relevancy": 2.2156, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Building%20Semantics%20Preservation%20in%20AI%20Model%20Training%20with%20Large%20Language%20Model%20Encodings&body=Title%3A%20Enhancing%20Building%20Semantics%20Preservation%20in%20AI%20Model%20Training%20with%20Large%20Language%20Model%20Encodings%0AAuthor%3A%20Suhyung%20Jang%20and%20Ghang%20Lee%20and%20Jaekun%20Lee%20and%20Hyunjun%20Lee%0AAbstract%3A%20Accurate%20representation%20of%20building%20semantics%2C%20encompassing%20both%20generic%20object%20types%20and%20specific%20subtypes%2C%20is%20essential%20for%20effective%20AI%20model%20training%20in%20the%20architecture%2C%20engineering%2C%20construction%2C%20and%20operation%20%28AECO%29%20industry.%20Conventional%20encoding%20methods%20%28e.g.%2C%20one-hot%29%20often%20fail%20to%20convey%20the%20nuanced%20relationships%20among%20closely%20related%20subtypes%2C%20limiting%20AI%27s%20semantic%20comprehension.%20To%20address%20this%20limitation%2C%20this%20study%20proposes%20a%20novel%20training%20approach%20that%20employs%20large%20language%20model%20%28LLM%29%20embeddings%20%28e.g.%2C%20OpenAI%20GPT%20and%20Meta%20LLaMA%29%20as%20encodings%20to%20preserve%20finer%20distinctions%20in%20building%20semantics.%20We%20evaluated%20the%20proposed%20method%20by%20training%20GraphSAGE%20models%20to%20classify%2042%20building%20object%20subtypes%20across%20five%20high-rise%20residential%20building%20information%20models%20%28BIMs%29.%20Various%20embedding%20dimensions%20were%20tested%2C%20including%20original%20high-dimensional%20LLM%20embeddings%20%281%2C536%2C%203%2C072%2C%20or%204%2C096%29%20and%201%2C024-dimensional%20compacted%20embeddings%20generated%20via%20the%20Matryoshka%20representation%20model.%20Experimental%20results%20demonstrated%20that%20LLM%20encodings%20outperformed%20the%20conventional%20one-hot%20baseline%2C%20with%20the%20llama-3%20%28compacted%29%20embedding%20achieving%20a%20weighted%20average%20F1-score%20of%200.8766%2C%20compared%20to%200.8475%20for%20one-hot%20encoding.%20The%20results%20underscore%20the%20promise%20of%20leveraging%20LLM-based%20encodings%20to%20enhance%20AI%27s%20ability%20to%20interpret%20complex%2C%20domain-specific%20building%20semantics.%20As%20the%20capabilities%20of%20LLMs%20and%20dimensionality%20reduction%20techniques%20continue%20to%20evolve%2C%20this%20approach%20holds%20considerable%20potential%20for%20broad%20application%20in%20semantic%20elaboration%20tasks%20throughout%20the%20AECO%20industry.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Building%2520Semantics%2520Preservation%2520in%2520AI%2520Model%2520Training%2520with%2520Large%2520Language%2520Model%2520Encodings%26entry.906535625%3DSuhyung%2520Jang%2520and%2520Ghang%2520Lee%2520and%2520Jaekun%2520Lee%2520and%2520Hyunjun%2520Lee%26entry.1292438233%3DAccurate%2520representation%2520of%2520building%2520semantics%252C%2520encompassing%2520both%2520generic%2520object%2520types%2520and%2520specific%2520subtypes%252C%2520is%2520essential%2520for%2520effective%2520AI%2520model%2520training%2520in%2520the%2520architecture%252C%2520engineering%252C%2520construction%252C%2520and%2520operation%2520%2528AECO%2529%2520industry.%2520Conventional%2520encoding%2520methods%2520%2528e.g.%252C%2520one-hot%2529%2520often%2520fail%2520to%2520convey%2520the%2520nuanced%2520relationships%2520among%2520closely%2520related%2520subtypes%252C%2520limiting%2520AI%2527s%2520semantic%2520comprehension.%2520To%2520address%2520this%2520limitation%252C%2520this%2520study%2520proposes%2520a%2520novel%2520training%2520approach%2520that%2520employs%2520large%2520language%2520model%2520%2528LLM%2529%2520embeddings%2520%2528e.g.%252C%2520OpenAI%2520GPT%2520and%2520Meta%2520LLaMA%2529%2520as%2520encodings%2520to%2520preserve%2520finer%2520distinctions%2520in%2520building%2520semantics.%2520We%2520evaluated%2520the%2520proposed%2520method%2520by%2520training%2520GraphSAGE%2520models%2520to%2520classify%252042%2520building%2520object%2520subtypes%2520across%2520five%2520high-rise%2520residential%2520building%2520information%2520models%2520%2528BIMs%2529.%2520Various%2520embedding%2520dimensions%2520were%2520tested%252C%2520including%2520original%2520high-dimensional%2520LLM%2520embeddings%2520%25281%252C536%252C%25203%252C072%252C%2520or%25204%252C096%2529%2520and%25201%252C024-dimensional%2520compacted%2520embeddings%2520generated%2520via%2520the%2520Matryoshka%2520representation%2520model.%2520Experimental%2520results%2520demonstrated%2520that%2520LLM%2520encodings%2520outperformed%2520the%2520conventional%2520one-hot%2520baseline%252C%2520with%2520the%2520llama-3%2520%2528compacted%2529%2520embedding%2520achieving%2520a%2520weighted%2520average%2520F1-score%2520of%25200.8766%252C%2520compared%2520to%25200.8475%2520for%2520one-hot%2520encoding.%2520The%2520results%2520underscore%2520the%2520promise%2520of%2520leveraging%2520LLM-based%2520encodings%2520to%2520enhance%2520AI%2527s%2520ability%2520to%2520interpret%2520complex%252C%2520domain-specific%2520building%2520semantics.%2520As%2520the%2520capabilities%2520of%2520LLMs%2520and%2520dimensionality%2520reduction%2520techniques%2520continue%2520to%2520evolve%252C%2520this%2520approach%2520holds%2520considerable%2520potential%2520for%2520broad%2520application%2520in%2520semantic%2520elaboration%2520tasks%2520throughout%2520the%2520AECO%2520industry.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Building%20Semantics%20Preservation%20in%20AI%20Model%20Training%20with%20Large%20Language%20Model%20Encodings&entry.906535625=Suhyung%20Jang%20and%20Ghang%20Lee%20and%20Jaekun%20Lee%20and%20Hyunjun%20Lee&entry.1292438233=Accurate%20representation%20of%20building%20semantics%2C%20encompassing%20both%20generic%20object%20types%20and%20specific%20subtypes%2C%20is%20essential%20for%20effective%20AI%20model%20training%20in%20the%20architecture%2C%20engineering%2C%20construction%2C%20and%20operation%20%28AECO%29%20industry.%20Conventional%20encoding%20methods%20%28e.g.%2C%20one-hot%29%20often%20fail%20to%20convey%20the%20nuanced%20relationships%20among%20closely%20related%20subtypes%2C%20limiting%20AI%27s%20semantic%20comprehension.%20To%20address%20this%20limitation%2C%20this%20study%20proposes%20a%20novel%20training%20approach%20that%20employs%20large%20language%20model%20%28LLM%29%20embeddings%20%28e.g.%2C%20OpenAI%20GPT%20and%20Meta%20LLaMA%29%20as%20encodings%20to%20preserve%20finer%20distinctions%20in%20building%20semantics.%20We%20evaluated%20the%20proposed%20method%20by%20training%20GraphSAGE%20models%20to%20classify%2042%20building%20object%20subtypes%20across%20five%20high-rise%20residential%20building%20information%20models%20%28BIMs%29.%20Various%20embedding%20dimensions%20were%20tested%2C%20including%20original%20high-dimensional%20LLM%20embeddings%20%281%2C536%2C%203%2C072%2C%20or%204%2C096%29%20and%201%2C024-dimensional%20compacted%20embeddings%20generated%20via%20the%20Matryoshka%20representation%20model.%20Experimental%20results%20demonstrated%20that%20LLM%20encodings%20outperformed%20the%20conventional%20one-hot%20baseline%2C%20with%20the%20llama-3%20%28compacted%29%20embedding%20achieving%20a%20weighted%20average%20F1-score%20of%200.8766%2C%20compared%20to%200.8475%20for%20one-hot%20encoding.%20The%20results%20underscore%20the%20promise%20of%20leveraging%20LLM-based%20encodings%20to%20enhance%20AI%27s%20ability%20to%20interpret%20complex%2C%20domain-specific%20building%20semantics.%20As%20the%20capabilities%20of%20LLMs%20and%20dimensionality%20reduction%20techniques%20continue%20to%20evolve%2C%20this%20approach%20holds%20considerable%20potential%20for%20broad%20application%20in%20semantic%20elaboration%20tasks%20throughout%20the%20AECO%20industry.&entry.1838667208=http%3A//arxiv.org/abs/2602.15791v1&entry.124074799=Read"},
{"title": "Emergent Morphing Attack Detection in Open Multi-modal Large Language Models", "author": "Marija Ivanovska and Vitomir \u0160truc", "abstract": "Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored. In this paper, we present the first systematic zero-shot evaluation of open-source MLLMs for single-image MAD, using publicly available weights and a standardized, reproducible protocol. Across diverse morphing techniques, many MLLMs show non-trivial discriminative ability without any fine-tuning or domain adaptation, and LLaVA1.6-Mistral-7B achieves state-of-the-art performance, surpassing highly competitive task-specific MAD baselines by at least 23% in terms of equal error rate (EER). The results indicate that multimodal pretraining can implicitly encode fine-grained facial inconsistencies indicative of morphing artifacts, enabling zero-shot forensic sensitivity. Our findings position open-source MLLMs as reproducible, interpretable, and competitive foundations for biometric security and forensic image analysis. This emergent capability also highlights new opportunities to develop state-of-the-art MAD systems through targeted fine-tuning or lightweight adaptation, further improving accuracy and efficiency while preserving interpretability. To support future research, all code and evaluation protocols will be released upon publication.", "link": "http://arxiv.org/abs/2602.15461v1", "date": "2026-02-17", "relevancy": 2.1954, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.577}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5434}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20Morphing%20Attack%20Detection%20in%20Open%20Multi-modal%20Large%20Language%20Models&body=Title%3A%20Emergent%20Morphing%20Attack%20Detection%20in%20Open%20Multi-modal%20Large%20Language%20Models%0AAuthor%3A%20Marija%20Ivanovska%20and%20Vitomir%20%C5%A0truc%0AAbstract%3A%20Face%20morphing%20attacks%20threaten%20biometric%20verification%2C%20yet%20most%20morphing%20attack%20detection%20%28MAD%29%20systems%20require%20task-specific%20training%20and%20generalize%20poorly%20to%20unseen%20attack%20types.%20Meanwhile%2C%20open-source%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20strong%20visual-linguistic%20reasoning%2C%20but%20their%20potential%20in%20biometric%20forensics%20remains%20underexplored.%20In%20this%20paper%2C%20we%20present%20the%20first%20systematic%20zero-shot%20evaluation%20of%20open-source%20MLLMs%20for%20single-image%20MAD%2C%20using%20publicly%20available%20weights%20and%20a%20standardized%2C%20reproducible%20protocol.%20Across%20diverse%20morphing%20techniques%2C%20many%20MLLMs%20show%20non-trivial%20discriminative%20ability%20without%20any%20fine-tuning%20or%20domain%20adaptation%2C%20and%20LLaVA1.6-Mistral-7B%20achieves%20state-of-the-art%20performance%2C%20surpassing%20highly%20competitive%20task-specific%20MAD%20baselines%20by%20at%20least%2023%25%20in%20terms%20of%20equal%20error%20rate%20%28EER%29.%20The%20results%20indicate%20that%20multimodal%20pretraining%20can%20implicitly%20encode%20fine-grained%20facial%20inconsistencies%20indicative%20of%20morphing%20artifacts%2C%20enabling%20zero-shot%20forensic%20sensitivity.%20Our%20findings%20position%20open-source%20MLLMs%20as%20reproducible%2C%20interpretable%2C%20and%20competitive%20foundations%20for%20biometric%20security%20and%20forensic%20image%20analysis.%20This%20emergent%20capability%20also%20highlights%20new%20opportunities%20to%20develop%20state-of-the-art%20MAD%20systems%20through%20targeted%20fine-tuning%20or%20lightweight%20adaptation%2C%20further%20improving%20accuracy%20and%20efficiency%20while%20preserving%20interpretability.%20To%20support%20future%20research%2C%20all%20code%20and%20evaluation%20protocols%20will%20be%20released%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520Morphing%2520Attack%2520Detection%2520in%2520Open%2520Multi-modal%2520Large%2520Language%2520Models%26entry.906535625%3DMarija%2520Ivanovska%2520and%2520Vitomir%2520%25C5%25A0truc%26entry.1292438233%3DFace%2520morphing%2520attacks%2520threaten%2520biometric%2520verification%252C%2520yet%2520most%2520morphing%2520attack%2520detection%2520%2528MAD%2529%2520systems%2520require%2520task-specific%2520training%2520and%2520generalize%2520poorly%2520to%2520unseen%2520attack%2520types.%2520Meanwhile%252C%2520open-source%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520strong%2520visual-linguistic%2520reasoning%252C%2520but%2520their%2520potential%2520in%2520biometric%2520forensics%2520remains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520systematic%2520zero-shot%2520evaluation%2520of%2520open-source%2520MLLMs%2520for%2520single-image%2520MAD%252C%2520using%2520publicly%2520available%2520weights%2520and%2520a%2520standardized%252C%2520reproducible%2520protocol.%2520Across%2520diverse%2520morphing%2520techniques%252C%2520many%2520MLLMs%2520show%2520non-trivial%2520discriminative%2520ability%2520without%2520any%2520fine-tuning%2520or%2520domain%2520adaptation%252C%2520and%2520LLaVA1.6-Mistral-7B%2520achieves%2520state-of-the-art%2520performance%252C%2520surpassing%2520highly%2520competitive%2520task-specific%2520MAD%2520baselines%2520by%2520at%2520least%252023%2525%2520in%2520terms%2520of%2520equal%2520error%2520rate%2520%2528EER%2529.%2520The%2520results%2520indicate%2520that%2520multimodal%2520pretraining%2520can%2520implicitly%2520encode%2520fine-grained%2520facial%2520inconsistencies%2520indicative%2520of%2520morphing%2520artifacts%252C%2520enabling%2520zero-shot%2520forensic%2520sensitivity.%2520Our%2520findings%2520position%2520open-source%2520MLLMs%2520as%2520reproducible%252C%2520interpretable%252C%2520and%2520competitive%2520foundations%2520for%2520biometric%2520security%2520and%2520forensic%2520image%2520analysis.%2520This%2520emergent%2520capability%2520also%2520highlights%2520new%2520opportunities%2520to%2520develop%2520state-of-the-art%2520MAD%2520systems%2520through%2520targeted%2520fine-tuning%2520or%2520lightweight%2520adaptation%252C%2520further%2520improving%2520accuracy%2520and%2520efficiency%2520while%2520preserving%2520interpretability.%2520To%2520support%2520future%2520research%252C%2520all%2520code%2520and%2520evaluation%2520protocols%2520will%2520be%2520released%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20Morphing%20Attack%20Detection%20in%20Open%20Multi-modal%20Large%20Language%20Models&entry.906535625=Marija%20Ivanovska%20and%20Vitomir%20%C5%A0truc&entry.1292438233=Face%20morphing%20attacks%20threaten%20biometric%20verification%2C%20yet%20most%20morphing%20attack%20detection%20%28MAD%29%20systems%20require%20task-specific%20training%20and%20generalize%20poorly%20to%20unseen%20attack%20types.%20Meanwhile%2C%20open-source%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20strong%20visual-linguistic%20reasoning%2C%20but%20their%20potential%20in%20biometric%20forensics%20remains%20underexplored.%20In%20this%20paper%2C%20we%20present%20the%20first%20systematic%20zero-shot%20evaluation%20of%20open-source%20MLLMs%20for%20single-image%20MAD%2C%20using%20publicly%20available%20weights%20and%20a%20standardized%2C%20reproducible%20protocol.%20Across%20diverse%20morphing%20techniques%2C%20many%20MLLMs%20show%20non-trivial%20discriminative%20ability%20without%20any%20fine-tuning%20or%20domain%20adaptation%2C%20and%20LLaVA1.6-Mistral-7B%20achieves%20state-of-the-art%20performance%2C%20surpassing%20highly%20competitive%20task-specific%20MAD%20baselines%20by%20at%20least%2023%25%20in%20terms%20of%20equal%20error%20rate%20%28EER%29.%20The%20results%20indicate%20that%20multimodal%20pretraining%20can%20implicitly%20encode%20fine-grained%20facial%20inconsistencies%20indicative%20of%20morphing%20artifacts%2C%20enabling%20zero-shot%20forensic%20sensitivity.%20Our%20findings%20position%20open-source%20MLLMs%20as%20reproducible%2C%20interpretable%2C%20and%20competitive%20foundations%20for%20biometric%20security%20and%20forensic%20image%20analysis.%20This%20emergent%20capability%20also%20highlights%20new%20opportunities%20to%20develop%20state-of-the-art%20MAD%20systems%20through%20targeted%20fine-tuning%20or%20lightweight%20adaptation%2C%20further%20improving%20accuracy%20and%20efficiency%20while%20preserving%20interpretability.%20To%20support%20future%20research%2C%20all%20code%20and%20evaluation%20protocols%20will%20be%20released%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2602.15461v1&entry.124074799=Read"},
{"title": "On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks", "author": "Yannic Neuhaus and Nicolas Flammarion and Matthias Hein and Francesco Croce", "abstract": "Integrating reasoning in large language models and large vision-language models has recently led to significant improvement of their capabilities. However, the generalization of reasoning models is still vaguely defined and poorly understood. In this work, we present an evaluation framework to rigorously examine how well chain-of-thought (CoT) approaches generalize on a simple planning task. Specifically, we consider a grid-based navigation task in which a model is provided with a map and must output a sequence of moves that guides a player from a start position to a goal while avoiding obstacles. The versatility of the task and its data allows us to fine-tune model variants using different input representations (visual and textual) and CoT reasoning strategies, and systematically evaluate them under both in-distribution (ID) and out-of-distribution (OOD) test conditions. Our experiments show that, while CoT reasoning improves in-distribution generalization across all representations, out-of-distribution generalization (e.g., to larger maps) remains very limited in most cases when controlling for trivial matches with the ID data. Surprisingly, we find that reasoning traces which combine multiple text formats yield the best (and non-trivial) OOD generalization. Finally, purely text-based models consistently outperform those utilizing image-based inputs, including a recently proposed approach relying on latent space reasoning.", "link": "http://arxiv.org/abs/2602.15460v1", "date": "2026-02-17", "relevancy": 2.1924, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Out-of-Distribution%20Generalization%20of%20Reasoning%20in%20Multimodal%20LLMs%20for%20Simple%20Visual%20Planning%20Tasks&body=Title%3A%20On%20the%20Out-of-Distribution%20Generalization%20of%20Reasoning%20in%20Multimodal%20LLMs%20for%20Simple%20Visual%20Planning%20Tasks%0AAuthor%3A%20Yannic%20Neuhaus%20and%20Nicolas%20Flammarion%20and%20Matthias%20Hein%20and%20Francesco%20Croce%0AAbstract%3A%20Integrating%20reasoning%20in%20large%20language%20models%20and%20large%20vision-language%20models%20has%20recently%20led%20to%20significant%20improvement%20of%20their%20capabilities.%20However%2C%20the%20generalization%20of%20reasoning%20models%20is%20still%20vaguely%20defined%20and%20poorly%20understood.%20In%20this%20work%2C%20we%20present%20an%20evaluation%20framework%20to%20rigorously%20examine%20how%20well%20chain-of-thought%20%28CoT%29%20approaches%20generalize%20on%20a%20simple%20planning%20task.%20Specifically%2C%20we%20consider%20a%20grid-based%20navigation%20task%20in%20which%20a%20model%20is%20provided%20with%20a%20map%20and%20must%20output%20a%20sequence%20of%20moves%20that%20guides%20a%20player%20from%20a%20start%20position%20to%20a%20goal%20while%20avoiding%20obstacles.%20The%20versatility%20of%20the%20task%20and%20its%20data%20allows%20us%20to%20fine-tune%20model%20variants%20using%20different%20input%20representations%20%28visual%20and%20textual%29%20and%20CoT%20reasoning%20strategies%2C%20and%20systematically%20evaluate%20them%20under%20both%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%20test%20conditions.%20Our%20experiments%20show%20that%2C%20while%20CoT%20reasoning%20improves%20in-distribution%20generalization%20across%20all%20representations%2C%20out-of-distribution%20generalization%20%28e.g.%2C%20to%20larger%20maps%29%20remains%20very%20limited%20in%20most%20cases%20when%20controlling%20for%20trivial%20matches%20with%20the%20ID%20data.%20Surprisingly%2C%20we%20find%20that%20reasoning%20traces%20which%20combine%20multiple%20text%20formats%20yield%20the%20best%20%28and%20non-trivial%29%20OOD%20generalization.%20Finally%2C%20purely%20text-based%20models%20consistently%20outperform%20those%20utilizing%20image-based%20inputs%2C%20including%20a%20recently%20proposed%20approach%20relying%20on%20latent%20space%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Out-of-Distribution%2520Generalization%2520of%2520Reasoning%2520in%2520Multimodal%2520LLMs%2520for%2520Simple%2520Visual%2520Planning%2520Tasks%26entry.906535625%3DYannic%2520Neuhaus%2520and%2520Nicolas%2520Flammarion%2520and%2520Matthias%2520Hein%2520and%2520Francesco%2520Croce%26entry.1292438233%3DIntegrating%2520reasoning%2520in%2520large%2520language%2520models%2520and%2520large%2520vision-language%2520models%2520has%2520recently%2520led%2520to%2520significant%2520improvement%2520of%2520their%2520capabilities.%2520However%252C%2520the%2520generalization%2520of%2520reasoning%2520models%2520is%2520still%2520vaguely%2520defined%2520and%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520evaluation%2520framework%2520to%2520rigorously%2520examine%2520how%2520well%2520chain-of-thought%2520%2528CoT%2529%2520approaches%2520generalize%2520on%2520a%2520simple%2520planning%2520task.%2520Specifically%252C%2520we%2520consider%2520a%2520grid-based%2520navigation%2520task%2520in%2520which%2520a%2520model%2520is%2520provided%2520with%2520a%2520map%2520and%2520must%2520output%2520a%2520sequence%2520of%2520moves%2520that%2520guides%2520a%2520player%2520from%2520a%2520start%2520position%2520to%2520a%2520goal%2520while%2520avoiding%2520obstacles.%2520The%2520versatility%2520of%2520the%2520task%2520and%2520its%2520data%2520allows%2520us%2520to%2520fine-tune%2520model%2520variants%2520using%2520different%2520input%2520representations%2520%2528visual%2520and%2520textual%2529%2520and%2520CoT%2520reasoning%2520strategies%252C%2520and%2520systematically%2520evaluate%2520them%2520under%2520both%2520in-distribution%2520%2528ID%2529%2520and%2520out-of-distribution%2520%2528OOD%2529%2520test%2520conditions.%2520Our%2520experiments%2520show%2520that%252C%2520while%2520CoT%2520reasoning%2520improves%2520in-distribution%2520generalization%2520across%2520all%2520representations%252C%2520out-of-distribution%2520generalization%2520%2528e.g.%252C%2520to%2520larger%2520maps%2529%2520remains%2520very%2520limited%2520in%2520most%2520cases%2520when%2520controlling%2520for%2520trivial%2520matches%2520with%2520the%2520ID%2520data.%2520Surprisingly%252C%2520we%2520find%2520that%2520reasoning%2520traces%2520which%2520combine%2520multiple%2520text%2520formats%2520yield%2520the%2520best%2520%2528and%2520non-trivial%2529%2520OOD%2520generalization.%2520Finally%252C%2520purely%2520text-based%2520models%2520consistently%2520outperform%2520those%2520utilizing%2520image-based%2520inputs%252C%2520including%2520a%2520recently%2520proposed%2520approach%2520relying%2520on%2520latent%2520space%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Out-of-Distribution%20Generalization%20of%20Reasoning%20in%20Multimodal%20LLMs%20for%20Simple%20Visual%20Planning%20Tasks&entry.906535625=Yannic%20Neuhaus%20and%20Nicolas%20Flammarion%20and%20Matthias%20Hein%20and%20Francesco%20Croce&entry.1292438233=Integrating%20reasoning%20in%20large%20language%20models%20and%20large%20vision-language%20models%20has%20recently%20led%20to%20significant%20improvement%20of%20their%20capabilities.%20However%2C%20the%20generalization%20of%20reasoning%20models%20is%20still%20vaguely%20defined%20and%20poorly%20understood.%20In%20this%20work%2C%20we%20present%20an%20evaluation%20framework%20to%20rigorously%20examine%20how%20well%20chain-of-thought%20%28CoT%29%20approaches%20generalize%20on%20a%20simple%20planning%20task.%20Specifically%2C%20we%20consider%20a%20grid-based%20navigation%20task%20in%20which%20a%20model%20is%20provided%20with%20a%20map%20and%20must%20output%20a%20sequence%20of%20moves%20that%20guides%20a%20player%20from%20a%20start%20position%20to%20a%20goal%20while%20avoiding%20obstacles.%20The%20versatility%20of%20the%20task%20and%20its%20data%20allows%20us%20to%20fine-tune%20model%20variants%20using%20different%20input%20representations%20%28visual%20and%20textual%29%20and%20CoT%20reasoning%20strategies%2C%20and%20systematically%20evaluate%20them%20under%20both%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%20test%20conditions.%20Our%20experiments%20show%20that%2C%20while%20CoT%20reasoning%20improves%20in-distribution%20generalization%20across%20all%20representations%2C%20out-of-distribution%20generalization%20%28e.g.%2C%20to%20larger%20maps%29%20remains%20very%20limited%20in%20most%20cases%20when%20controlling%20for%20trivial%20matches%20with%20the%20ID%20data.%20Surprisingly%2C%20we%20find%20that%20reasoning%20traces%20which%20combine%20multiple%20text%20formats%20yield%20the%20best%20%28and%20non-trivial%29%20OOD%20generalization.%20Finally%2C%20purely%20text-based%20models%20consistently%20outperform%20those%20utilizing%20image-based%20inputs%2C%20including%20a%20recently%20proposed%20approach%20relying%20on%20latent%20space%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2602.15460v1&entry.124074799=Read"},
{"title": "Beyond Labels: Information-Efficient Human-in-the-Loop Learning using Ranking and Selection Queries", "author": "Bel\u00e9n Mart\u00edn-Urcelay and Yoonsang Lee and Matthieu R. Bloch and Christopher J. Rozell", "abstract": "Integrating human expertise into machine learning systems often reduces the role of experts to labeling oracles, a paradigm that limits the amount of information exchanged and fails to capture the nuances of human judgment. We address this challenge by developing a human-in-the-loop framework to learn binary classifiers with rich query types, consisting of item ranking and exemplar selection. We first introduce probabilistic human response models for these rich queries motivated by the relationship experimentally observed between the perceived implicit score of an item and its distance to the unknown classifier. Using these models, we then design active learning algorithms that leverage the rich queries to increase the information gained per interaction. We provide theoretical bounds on sample complexity and develop a tractable and computationally efficient variational approximation. Through experiments with simulated annotators derived from crowdsourced word-sentiment and image-aesthetic datasets, we demonstrate significant reductions on sample complexity. We further extend active learning strategies to select queries that maximize information rate, explicitly balancing informational value against annotation cost. This algorithm in the word sentiment classification task reduces learning time by more than 57\\% compared to traditional label-only active learning.", "link": "http://arxiv.org/abs/2602.15738v1", "date": "2026-02-17", "relevancy": 2.1759, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.553}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5464}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Labels%3A%20Information-Efficient%20Human-in-the-Loop%20Learning%20using%20Ranking%20and%20Selection%20Queries&body=Title%3A%20Beyond%20Labels%3A%20Information-Efficient%20Human-in-the-Loop%20Learning%20using%20Ranking%20and%20Selection%20Queries%0AAuthor%3A%20Bel%C3%A9n%20Mart%C3%ADn-Urcelay%20and%20Yoonsang%20Lee%20and%20Matthieu%20R.%20Bloch%20and%20Christopher%20J.%20Rozell%0AAbstract%3A%20Integrating%20human%20expertise%20into%20machine%20learning%20systems%20often%20reduces%20the%20role%20of%20experts%20to%20labeling%20oracles%2C%20a%20paradigm%20that%20limits%20the%20amount%20of%20information%20exchanged%20and%20fails%20to%20capture%20the%20nuances%20of%20human%20judgment.%20We%20address%20this%20challenge%20by%20developing%20a%20human-in-the-loop%20framework%20to%20learn%20binary%20classifiers%20with%20rich%20query%20types%2C%20consisting%20of%20item%20ranking%20and%20exemplar%20selection.%20We%20first%20introduce%20probabilistic%20human%20response%20models%20for%20these%20rich%20queries%20motivated%20by%20the%20relationship%20experimentally%20observed%20between%20the%20perceived%20implicit%20score%20of%20an%20item%20and%20its%20distance%20to%20the%20unknown%20classifier.%20Using%20these%20models%2C%20we%20then%20design%20active%20learning%20algorithms%20that%20leverage%20the%20rich%20queries%20to%20increase%20the%20information%20gained%20per%20interaction.%20We%20provide%20theoretical%20bounds%20on%20sample%20complexity%20and%20develop%20a%20tractable%20and%20computationally%20efficient%20variational%20approximation.%20Through%20experiments%20with%20simulated%20annotators%20derived%20from%20crowdsourced%20word-sentiment%20and%20image-aesthetic%20datasets%2C%20we%20demonstrate%20significant%20reductions%20on%20sample%20complexity.%20We%20further%20extend%20active%20learning%20strategies%20to%20select%20queries%20that%20maximize%20information%20rate%2C%20explicitly%20balancing%20informational%20value%20against%20annotation%20cost.%20This%20algorithm%20in%20the%20word%20sentiment%20classification%20task%20reduces%20learning%20time%20by%20more%20than%2057%5C%25%20compared%20to%20traditional%20label-only%20active%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Labels%253A%2520Information-Efficient%2520Human-in-the-Loop%2520Learning%2520using%2520Ranking%2520and%2520Selection%2520Queries%26entry.906535625%3DBel%25C3%25A9n%2520Mart%25C3%25ADn-Urcelay%2520and%2520Yoonsang%2520Lee%2520and%2520Matthieu%2520R.%2520Bloch%2520and%2520Christopher%2520J.%2520Rozell%26entry.1292438233%3DIntegrating%2520human%2520expertise%2520into%2520machine%2520learning%2520systems%2520often%2520reduces%2520the%2520role%2520of%2520experts%2520to%2520labeling%2520oracles%252C%2520a%2520paradigm%2520that%2520limits%2520the%2520amount%2520of%2520information%2520exchanged%2520and%2520fails%2520to%2520capture%2520the%2520nuances%2520of%2520human%2520judgment.%2520We%2520address%2520this%2520challenge%2520by%2520developing%2520a%2520human-in-the-loop%2520framework%2520to%2520learn%2520binary%2520classifiers%2520with%2520rich%2520query%2520types%252C%2520consisting%2520of%2520item%2520ranking%2520and%2520exemplar%2520selection.%2520We%2520first%2520introduce%2520probabilistic%2520human%2520response%2520models%2520for%2520these%2520rich%2520queries%2520motivated%2520by%2520the%2520relationship%2520experimentally%2520observed%2520between%2520the%2520perceived%2520implicit%2520score%2520of%2520an%2520item%2520and%2520its%2520distance%2520to%2520the%2520unknown%2520classifier.%2520Using%2520these%2520models%252C%2520we%2520then%2520design%2520active%2520learning%2520algorithms%2520that%2520leverage%2520the%2520rich%2520queries%2520to%2520increase%2520the%2520information%2520gained%2520per%2520interaction.%2520We%2520provide%2520theoretical%2520bounds%2520on%2520sample%2520complexity%2520and%2520develop%2520a%2520tractable%2520and%2520computationally%2520efficient%2520variational%2520approximation.%2520Through%2520experiments%2520with%2520simulated%2520annotators%2520derived%2520from%2520crowdsourced%2520word-sentiment%2520and%2520image-aesthetic%2520datasets%252C%2520we%2520demonstrate%2520significant%2520reductions%2520on%2520sample%2520complexity.%2520We%2520further%2520extend%2520active%2520learning%2520strategies%2520to%2520select%2520queries%2520that%2520maximize%2520information%2520rate%252C%2520explicitly%2520balancing%2520informational%2520value%2520against%2520annotation%2520cost.%2520This%2520algorithm%2520in%2520the%2520word%2520sentiment%2520classification%2520task%2520reduces%2520learning%2520time%2520by%2520more%2520than%252057%255C%2525%2520compared%2520to%2520traditional%2520label-only%2520active%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Labels%3A%20Information-Efficient%20Human-in-the-Loop%20Learning%20using%20Ranking%20and%20Selection%20Queries&entry.906535625=Bel%C3%A9n%20Mart%C3%ADn-Urcelay%20and%20Yoonsang%20Lee%20and%20Matthieu%20R.%20Bloch%20and%20Christopher%20J.%20Rozell&entry.1292438233=Integrating%20human%20expertise%20into%20machine%20learning%20systems%20often%20reduces%20the%20role%20of%20experts%20to%20labeling%20oracles%2C%20a%20paradigm%20that%20limits%20the%20amount%20of%20information%20exchanged%20and%20fails%20to%20capture%20the%20nuances%20of%20human%20judgment.%20We%20address%20this%20challenge%20by%20developing%20a%20human-in-the-loop%20framework%20to%20learn%20binary%20classifiers%20with%20rich%20query%20types%2C%20consisting%20of%20item%20ranking%20and%20exemplar%20selection.%20We%20first%20introduce%20probabilistic%20human%20response%20models%20for%20these%20rich%20queries%20motivated%20by%20the%20relationship%20experimentally%20observed%20between%20the%20perceived%20implicit%20score%20of%20an%20item%20and%20its%20distance%20to%20the%20unknown%20classifier.%20Using%20these%20models%2C%20we%20then%20design%20active%20learning%20algorithms%20that%20leverage%20the%20rich%20queries%20to%20increase%20the%20information%20gained%20per%20interaction.%20We%20provide%20theoretical%20bounds%20on%20sample%20complexity%20and%20develop%20a%20tractable%20and%20computationally%20efficient%20variational%20approximation.%20Through%20experiments%20with%20simulated%20annotators%20derived%20from%20crowdsourced%20word-sentiment%20and%20image-aesthetic%20datasets%2C%20we%20demonstrate%20significant%20reductions%20on%20sample%20complexity.%20We%20further%20extend%20active%20learning%20strategies%20to%20select%20queries%20that%20maximize%20information%20rate%2C%20explicitly%20balancing%20informational%20value%20against%20annotation%20cost.%20This%20algorithm%20in%20the%20word%20sentiment%20classification%20task%20reduces%20learning%20time%20by%20more%20than%2057%5C%25%20compared%20to%20traditional%20label-only%20active%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2602.15738v1&entry.124074799=Read"},
{"title": "GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance", "author": "Francisco Giral and \u00c1lvaro Manzano and Ignacio G\u00f3mez and Ricardo Vinuesa and Soledad Le Clainche", "abstract": "Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available. We propose GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism: the unconditional branch learns a geometry-aware flow prior, while the sensor-conditioned branch injects observational constraints during sampling. This formulation enables obstacle-aware reconstruction and generalization across unseen geometries, wind directions, and mesh resolutions without retraining. We consider both sparse fixed sensors and trajectory-based observations using the same reconstruction procedure. When evaluated against supervised graph neural network (GNN) baselines and classical reduced-order data assimilation methods, GenDA reduces the relative root-mean-square error (RRMSE) by 25-57% and increases the structural similarity index (SSIM) by 23-33% across the tested meshes. Experiments are conducted on Reynolds-averaged Navier-Stokes (RANS) simulations of a real urban neighbourhood in Bristol, United Kingdom, at a characteristic Reynolds number of $\\mathrm{Re}\\approx2\\times10^{7}$, featuring complex building geometry and irregular terrain. The proposed framework provides a scalable path toward generative, geometry-aware data assimilation for environmental monitoring in complex domains.", "link": "http://arxiv.org/abs/2601.11440v2", "date": "2026-02-17", "relevancy": 2.167, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5611}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5517}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenDA%3A%20Generative%20Data%20Assimilation%20on%20Complex%20Urban%20Areas%20via%20Classifier-Free%20Diffusion%20Guidance&body=Title%3A%20GenDA%3A%20Generative%20Data%20Assimilation%20on%20Complex%20Urban%20Areas%20via%20Classifier-Free%20Diffusion%20Guidance%0AAuthor%3A%20Francisco%20Giral%20and%20%C3%81lvaro%20Manzano%20and%20Ignacio%20G%C3%B3mez%20and%20Ricardo%20Vinuesa%20and%20Soledad%20Le%20Clainche%0AAbstract%3A%20Urban%20wind%20flow%20reconstruction%20is%20essential%20for%20assessing%20air%20quality%2C%20heat%20dispersion%2C%20and%20pedestrian%20comfort%2C%20yet%20remains%20challenging%20when%20only%20sparse%20sensor%20data%20are%20available.%20We%20propose%20GenDA%2C%20a%20generative%20data%20assimilation%20framework%20that%20reconstructs%20high-resolution%20wind%20fields%20on%20unstructured%20meshes%20from%20limited%20observations.%20The%20model%20employs%20a%20multiscale%20graph-based%20diffusion%20architecture%20trained%20on%20computational%20fluid%20dynamics%20%28CFD%29%20simulations%20and%20interprets%20classifier-free%20guidance%20as%20a%20learned%20posterior%20reconstruction%20mechanism%3A%20the%20unconditional%20branch%20learns%20a%20geometry-aware%20flow%20prior%2C%20while%20the%20sensor-conditioned%20branch%20injects%20observational%20constraints%20during%20sampling.%20This%20formulation%20enables%20obstacle-aware%20reconstruction%20and%20generalization%20across%20unseen%20geometries%2C%20wind%20directions%2C%20and%20mesh%20resolutions%20without%20retraining.%20We%20consider%20both%20sparse%20fixed%20sensors%20and%20trajectory-based%20observations%20using%20the%20same%20reconstruction%20procedure.%20When%20evaluated%20against%20supervised%20graph%20neural%20network%20%28GNN%29%20baselines%20and%20classical%20reduced-order%20data%20assimilation%20methods%2C%20GenDA%20reduces%20the%20relative%20root-mean-square%20error%20%28RRMSE%29%20by%2025-57%25%20and%20increases%20the%20structural%20similarity%20index%20%28SSIM%29%20by%2023-33%25%20across%20the%20tested%20meshes.%20Experiments%20are%20conducted%20on%20Reynolds-averaged%20Navier-Stokes%20%28RANS%29%20simulations%20of%20a%20real%20urban%20neighbourhood%20in%20Bristol%2C%20United%20Kingdom%2C%20at%20a%20characteristic%20Reynolds%20number%20of%20%24%5Cmathrm%7BRe%7D%5Capprox2%5Ctimes10%5E%7B7%7D%24%2C%20featuring%20complex%20building%20geometry%20and%20irregular%20terrain.%20The%20proposed%20framework%20provides%20a%20scalable%20path%20toward%20generative%2C%20geometry-aware%20data%20assimilation%20for%20environmental%20monitoring%20in%20complex%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2601.11440v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenDA%253A%2520Generative%2520Data%2520Assimilation%2520on%2520Complex%2520Urban%2520Areas%2520via%2520Classifier-Free%2520Diffusion%2520Guidance%26entry.906535625%3DFrancisco%2520Giral%2520and%2520%25C3%2581lvaro%2520Manzano%2520and%2520Ignacio%2520G%25C3%25B3mez%2520and%2520Ricardo%2520Vinuesa%2520and%2520Soledad%2520Le%2520Clainche%26entry.1292438233%3DUrban%2520wind%2520flow%2520reconstruction%2520is%2520essential%2520for%2520assessing%2520air%2520quality%252C%2520heat%2520dispersion%252C%2520and%2520pedestrian%2520comfort%252C%2520yet%2520remains%2520challenging%2520when%2520only%2520sparse%2520sensor%2520data%2520are%2520available.%2520We%2520propose%2520GenDA%252C%2520a%2520generative%2520data%2520assimilation%2520framework%2520that%2520reconstructs%2520high-resolution%2520wind%2520fields%2520on%2520unstructured%2520meshes%2520from%2520limited%2520observations.%2520The%2520model%2520employs%2520a%2520multiscale%2520graph-based%2520diffusion%2520architecture%2520trained%2520on%2520computational%2520fluid%2520dynamics%2520%2528CFD%2529%2520simulations%2520and%2520interprets%2520classifier-free%2520guidance%2520as%2520a%2520learned%2520posterior%2520reconstruction%2520mechanism%253A%2520the%2520unconditional%2520branch%2520learns%2520a%2520geometry-aware%2520flow%2520prior%252C%2520while%2520the%2520sensor-conditioned%2520branch%2520injects%2520observational%2520constraints%2520during%2520sampling.%2520This%2520formulation%2520enables%2520obstacle-aware%2520reconstruction%2520and%2520generalization%2520across%2520unseen%2520geometries%252C%2520wind%2520directions%252C%2520and%2520mesh%2520resolutions%2520without%2520retraining.%2520We%2520consider%2520both%2520sparse%2520fixed%2520sensors%2520and%2520trajectory-based%2520observations%2520using%2520the%2520same%2520reconstruction%2520procedure.%2520When%2520evaluated%2520against%2520supervised%2520graph%2520neural%2520network%2520%2528GNN%2529%2520baselines%2520and%2520classical%2520reduced-order%2520data%2520assimilation%2520methods%252C%2520GenDA%2520reduces%2520the%2520relative%2520root-mean-square%2520error%2520%2528RRMSE%2529%2520by%252025-57%2525%2520and%2520increases%2520the%2520structural%2520similarity%2520index%2520%2528SSIM%2529%2520by%252023-33%2525%2520across%2520the%2520tested%2520meshes.%2520Experiments%2520are%2520conducted%2520on%2520Reynolds-averaged%2520Navier-Stokes%2520%2528RANS%2529%2520simulations%2520of%2520a%2520real%2520urban%2520neighbourhood%2520in%2520Bristol%252C%2520United%2520Kingdom%252C%2520at%2520a%2520characteristic%2520Reynolds%2520number%2520of%2520%2524%255Cmathrm%257BRe%257D%255Capprox2%255Ctimes10%255E%257B7%257D%2524%252C%2520featuring%2520complex%2520building%2520geometry%2520and%2520irregular%2520terrain.%2520The%2520proposed%2520framework%2520provides%2520a%2520scalable%2520path%2520toward%2520generative%252C%2520geometry-aware%2520data%2520assimilation%2520for%2520environmental%2520monitoring%2520in%2520complex%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.11440v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenDA%3A%20Generative%20Data%20Assimilation%20on%20Complex%20Urban%20Areas%20via%20Classifier-Free%20Diffusion%20Guidance&entry.906535625=Francisco%20Giral%20and%20%C3%81lvaro%20Manzano%20and%20Ignacio%20G%C3%B3mez%20and%20Ricardo%20Vinuesa%20and%20Soledad%20Le%20Clainche&entry.1292438233=Urban%20wind%20flow%20reconstruction%20is%20essential%20for%20assessing%20air%20quality%2C%20heat%20dispersion%2C%20and%20pedestrian%20comfort%2C%20yet%20remains%20challenging%20when%20only%20sparse%20sensor%20data%20are%20available.%20We%20propose%20GenDA%2C%20a%20generative%20data%20assimilation%20framework%20that%20reconstructs%20high-resolution%20wind%20fields%20on%20unstructured%20meshes%20from%20limited%20observations.%20The%20model%20employs%20a%20multiscale%20graph-based%20diffusion%20architecture%20trained%20on%20computational%20fluid%20dynamics%20%28CFD%29%20simulations%20and%20interprets%20classifier-free%20guidance%20as%20a%20learned%20posterior%20reconstruction%20mechanism%3A%20the%20unconditional%20branch%20learns%20a%20geometry-aware%20flow%20prior%2C%20while%20the%20sensor-conditioned%20branch%20injects%20observational%20constraints%20during%20sampling.%20This%20formulation%20enables%20obstacle-aware%20reconstruction%20and%20generalization%20across%20unseen%20geometries%2C%20wind%20directions%2C%20and%20mesh%20resolutions%20without%20retraining.%20We%20consider%20both%20sparse%20fixed%20sensors%20and%20trajectory-based%20observations%20using%20the%20same%20reconstruction%20procedure.%20When%20evaluated%20against%20supervised%20graph%20neural%20network%20%28GNN%29%20baselines%20and%20classical%20reduced-order%20data%20assimilation%20methods%2C%20GenDA%20reduces%20the%20relative%20root-mean-square%20error%20%28RRMSE%29%20by%2025-57%25%20and%20increases%20the%20structural%20similarity%20index%20%28SSIM%29%20by%2023-33%25%20across%20the%20tested%20meshes.%20Experiments%20are%20conducted%20on%20Reynolds-averaged%20Navier-Stokes%20%28RANS%29%20simulations%20of%20a%20real%20urban%20neighbourhood%20in%20Bristol%2C%20United%20Kingdom%2C%20at%20a%20characteristic%20Reynolds%20number%20of%20%24%5Cmathrm%7BRe%7D%5Capprox2%5Ctimes10%5E%7B7%7D%24%2C%20featuring%20complex%20building%20geometry%20and%20irregular%20terrain.%20The%20proposed%20framework%20provides%20a%20scalable%20path%20toward%20generative%2C%20geometry-aware%20data%20assimilation%20for%20environmental%20monitoring%20in%20complex%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2601.11440v2&entry.124074799=Read"},
{"title": "RaCo: Ranking and Covariance for Practical Learned Keypoints", "author": "Abhiram Shenoi and Philipp Lindenberger and Paul-Edouard Sarlin and Marc Pollefeys", "abstract": "This paper introduces RaCo, a lightweight neural network designed to learn robust and versatile keypoints suitable for a variety of 3D computer vision tasks. The model integrates three key components: the repeatable keypoint detector, a differentiable ranker to maximize matches with a limited number of keypoints, and a covariance estimator to quantify spatial uncertainty in metric scale. Trained on perspective image crops only, RaCo operates without the need for covisible image pairs. It achieves strong rotational robustness through extensive data augmentation, even without the use of computationally expensive equivariant network architectures. The method is evaluated on several challenging datasets, where it demonstrates state-of-the-art performance in keypoint repeatability and two-view matching, particularly under large in-plane rotations. Ultimately, RaCo provides an effective and simple strategy to independently estimate keypoint ranking and metric covariance without additional labels, detecting interpretable and repeatable interest points. The code is available at https://github.com/cvg/RaCo.", "link": "http://arxiv.org/abs/2602.15755v1", "date": "2026-02-17", "relevancy": 2.1631, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5777}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5145}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaCo%3A%20Ranking%20and%20Covariance%20for%20Practical%20Learned%20Keypoints&body=Title%3A%20RaCo%3A%20Ranking%20and%20Covariance%20for%20Practical%20Learned%20Keypoints%0AAuthor%3A%20Abhiram%20Shenoi%20and%20Philipp%20Lindenberger%20and%20Paul-Edouard%20Sarlin%20and%20Marc%20Pollefeys%0AAbstract%3A%20This%20paper%20introduces%20RaCo%2C%20a%20lightweight%20neural%20network%20designed%20to%20learn%20robust%20and%20versatile%20keypoints%20suitable%20for%20a%20variety%20of%203D%20computer%20vision%20tasks.%20The%20model%20integrates%20three%20key%20components%3A%20the%20repeatable%20keypoint%20detector%2C%20a%20differentiable%20ranker%20to%20maximize%20matches%20with%20a%20limited%20number%20of%20keypoints%2C%20and%20a%20covariance%20estimator%20to%20quantify%20spatial%20uncertainty%20in%20metric%20scale.%20Trained%20on%20perspective%20image%20crops%20only%2C%20RaCo%20operates%20without%20the%20need%20for%20covisible%20image%20pairs.%20It%20achieves%20strong%20rotational%20robustness%20through%20extensive%20data%20augmentation%2C%20even%20without%20the%20use%20of%20computationally%20expensive%20equivariant%20network%20architectures.%20The%20method%20is%20evaluated%20on%20several%20challenging%20datasets%2C%20where%20it%20demonstrates%20state-of-the-art%20performance%20in%20keypoint%20repeatability%20and%20two-view%20matching%2C%20particularly%20under%20large%20in-plane%20rotations.%20Ultimately%2C%20RaCo%20provides%20an%20effective%20and%20simple%20strategy%20to%20independently%20estimate%20keypoint%20ranking%20and%20metric%20covariance%20without%20additional%20labels%2C%20detecting%20interpretable%20and%20repeatable%20interest%20points.%20The%20code%20is%20available%20at%20https%3A//github.com/cvg/RaCo.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaCo%253A%2520Ranking%2520and%2520Covariance%2520for%2520Practical%2520Learned%2520Keypoints%26entry.906535625%3DAbhiram%2520Shenoi%2520and%2520Philipp%2520Lindenberger%2520and%2520Paul-Edouard%2520Sarlin%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3DThis%2520paper%2520introduces%2520RaCo%252C%2520a%2520lightweight%2520neural%2520network%2520designed%2520to%2520learn%2520robust%2520and%2520versatile%2520keypoints%2520suitable%2520for%2520a%2520variety%2520of%25203D%2520computer%2520vision%2520tasks.%2520The%2520model%2520integrates%2520three%2520key%2520components%253A%2520the%2520repeatable%2520keypoint%2520detector%252C%2520a%2520differentiable%2520ranker%2520to%2520maximize%2520matches%2520with%2520a%2520limited%2520number%2520of%2520keypoints%252C%2520and%2520a%2520covariance%2520estimator%2520to%2520quantify%2520spatial%2520uncertainty%2520in%2520metric%2520scale.%2520Trained%2520on%2520perspective%2520image%2520crops%2520only%252C%2520RaCo%2520operates%2520without%2520the%2520need%2520for%2520covisible%2520image%2520pairs.%2520It%2520achieves%2520strong%2520rotational%2520robustness%2520through%2520extensive%2520data%2520augmentation%252C%2520even%2520without%2520the%2520use%2520of%2520computationally%2520expensive%2520equivariant%2520network%2520architectures.%2520The%2520method%2520is%2520evaluated%2520on%2520several%2520challenging%2520datasets%252C%2520where%2520it%2520demonstrates%2520state-of-the-art%2520performance%2520in%2520keypoint%2520repeatability%2520and%2520two-view%2520matching%252C%2520particularly%2520under%2520large%2520in-plane%2520rotations.%2520Ultimately%252C%2520RaCo%2520provides%2520an%2520effective%2520and%2520simple%2520strategy%2520to%2520independently%2520estimate%2520keypoint%2520ranking%2520and%2520metric%2520covariance%2520without%2520additional%2520labels%252C%2520detecting%2520interpretable%2520and%2520repeatable%2520interest%2520points.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/cvg/RaCo.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaCo%3A%20Ranking%20and%20Covariance%20for%20Practical%20Learned%20Keypoints&entry.906535625=Abhiram%20Shenoi%20and%20Philipp%20Lindenberger%20and%20Paul-Edouard%20Sarlin%20and%20Marc%20Pollefeys&entry.1292438233=This%20paper%20introduces%20RaCo%2C%20a%20lightweight%20neural%20network%20designed%20to%20learn%20robust%20and%20versatile%20keypoints%20suitable%20for%20a%20variety%20of%203D%20computer%20vision%20tasks.%20The%20model%20integrates%20three%20key%20components%3A%20the%20repeatable%20keypoint%20detector%2C%20a%20differentiable%20ranker%20to%20maximize%20matches%20with%20a%20limited%20number%20of%20keypoints%2C%20and%20a%20covariance%20estimator%20to%20quantify%20spatial%20uncertainty%20in%20metric%20scale.%20Trained%20on%20perspective%20image%20crops%20only%2C%20RaCo%20operates%20without%20the%20need%20for%20covisible%20image%20pairs.%20It%20achieves%20strong%20rotational%20robustness%20through%20extensive%20data%20augmentation%2C%20even%20without%20the%20use%20of%20computationally%20expensive%20equivariant%20network%20architectures.%20The%20method%20is%20evaluated%20on%20several%20challenging%20datasets%2C%20where%20it%20demonstrates%20state-of-the-art%20performance%20in%20keypoint%20repeatability%20and%20two-view%20matching%2C%20particularly%20under%20large%20in-plane%20rotations.%20Ultimately%2C%20RaCo%20provides%20an%20effective%20and%20simple%20strategy%20to%20independently%20estimate%20keypoint%20ranking%20and%20metric%20covariance%20without%20additional%20labels%2C%20detecting%20interpretable%20and%20repeatable%20interest%20points.%20The%20code%20is%20available%20at%20https%3A//github.com/cvg/RaCo.&entry.1838667208=http%3A//arxiv.org/abs/2602.15755v1&entry.124074799=Read"},
{"title": "LEADER: Lightweight End-to-End Attention-Gated Dual Autoencoder for Robust Minutiae Extraction", "author": "Raffaele Cappelli and Matteo Ferrara", "abstract": "Minutiae extraction, a fundamental stage in fingerprint recognition, is increasingly shifting toward deep learning. However, truly end-to-end methods that eliminate separate preprocessing and postprocessing steps remain scarce. This paper introduces LEADER (Lightweight End-to-end Attention-gated Dual autoencodER), a neural network that maps raw fingerprint images to minutiae descriptors, including location, direction, and type. The proposed architecture integrates non-maximum suppression and angular decoding to enable complete end-to-end inference using only 0.9M parameters. It employs a novel \"Castle-Moat-Rampart\" ground-truth encoding and a dual-autoencoder structure, interconnected through an attention-gating mechanism. Experimental evaluations demonstrate state-of-the-art accuracy on plain fingerprints and robust cross-domain generalization to latent impressions. Specifically, LEADER attains a 34% higher F1-score on the NIST SD27 dataset compared to specialized latent minutiae extractors. Sample-level analysis on this challenging benchmark reveals an average rank of 2.07 among all compared methods, with LEADER securing the first-place position in 47% of the samples-more than doubling the frequency of the second-best extractor. The internal representations learned by the model align with established fingerprint domain features, such as segmentation masks, orientation fields, frequency maps, and skeletons. Inference requires 15ms on GPU and 322ms on CPU, outperforming leading commercial software in computational efficiency. The source code and pre-trained weights are publicly released to facilitate reproducibility.", "link": "http://arxiv.org/abs/2602.15493v1", "date": "2026-02-17", "relevancy": 2.1568, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5532}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5296}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEADER%3A%20Lightweight%20End-to-End%20Attention-Gated%20Dual%20Autoencoder%20for%20Robust%20Minutiae%20Extraction&body=Title%3A%20LEADER%3A%20Lightweight%20End-to-End%20Attention-Gated%20Dual%20Autoencoder%20for%20Robust%20Minutiae%20Extraction%0AAuthor%3A%20Raffaele%20Cappelli%20and%20Matteo%20Ferrara%0AAbstract%3A%20Minutiae%20extraction%2C%20a%20fundamental%20stage%20in%20fingerprint%20recognition%2C%20is%20increasingly%20shifting%20toward%20deep%20learning.%20However%2C%20truly%20end-to-end%20methods%20that%20eliminate%20separate%20preprocessing%20and%20postprocessing%20steps%20remain%20scarce.%20This%20paper%20introduces%20LEADER%20%28Lightweight%20End-to-end%20Attention-gated%20Dual%20autoencodER%29%2C%20a%20neural%20network%20that%20maps%20raw%20fingerprint%20images%20to%20minutiae%20descriptors%2C%20including%20location%2C%20direction%2C%20and%20type.%20The%20proposed%20architecture%20integrates%20non-maximum%20suppression%20and%20angular%20decoding%20to%20enable%20complete%20end-to-end%20inference%20using%20only%200.9M%20parameters.%20It%20employs%20a%20novel%20%22Castle-Moat-Rampart%22%20ground-truth%20encoding%20and%20a%20dual-autoencoder%20structure%2C%20interconnected%20through%20an%20attention-gating%20mechanism.%20Experimental%20evaluations%20demonstrate%20state-of-the-art%20accuracy%20on%20plain%20fingerprints%20and%20robust%20cross-domain%20generalization%20to%20latent%20impressions.%20Specifically%2C%20LEADER%20attains%20a%2034%25%20higher%20F1-score%20on%20the%20NIST%20SD27%20dataset%20compared%20to%20specialized%20latent%20minutiae%20extractors.%20Sample-level%20analysis%20on%20this%20challenging%20benchmark%20reveals%20an%20average%20rank%20of%202.07%20among%20all%20compared%20methods%2C%20with%20LEADER%20securing%20the%20first-place%20position%20in%2047%25%20of%20the%20samples-more%20than%20doubling%20the%20frequency%20of%20the%20second-best%20extractor.%20The%20internal%20representations%20learned%20by%20the%20model%20align%20with%20established%20fingerprint%20domain%20features%2C%20such%20as%20segmentation%20masks%2C%20orientation%20fields%2C%20frequency%20maps%2C%20and%20skeletons.%20Inference%20requires%2015ms%20on%20GPU%20and%20322ms%20on%20CPU%2C%20outperforming%20leading%20commercial%20software%20in%20computational%20efficiency.%20The%20source%20code%20and%20pre-trained%20weights%20are%20publicly%20released%20to%20facilitate%20reproducibility.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEADER%253A%2520Lightweight%2520End-to-End%2520Attention-Gated%2520Dual%2520Autoencoder%2520for%2520Robust%2520Minutiae%2520Extraction%26entry.906535625%3DRaffaele%2520Cappelli%2520and%2520Matteo%2520Ferrara%26entry.1292438233%3DMinutiae%2520extraction%252C%2520a%2520fundamental%2520stage%2520in%2520fingerprint%2520recognition%252C%2520is%2520increasingly%2520shifting%2520toward%2520deep%2520learning.%2520However%252C%2520truly%2520end-to-end%2520methods%2520that%2520eliminate%2520separate%2520preprocessing%2520and%2520postprocessing%2520steps%2520remain%2520scarce.%2520This%2520paper%2520introduces%2520LEADER%2520%2528Lightweight%2520End-to-end%2520Attention-gated%2520Dual%2520autoencodER%2529%252C%2520a%2520neural%2520network%2520that%2520maps%2520raw%2520fingerprint%2520images%2520to%2520minutiae%2520descriptors%252C%2520including%2520location%252C%2520direction%252C%2520and%2520type.%2520The%2520proposed%2520architecture%2520integrates%2520non-maximum%2520suppression%2520and%2520angular%2520decoding%2520to%2520enable%2520complete%2520end-to-end%2520inference%2520using%2520only%25200.9M%2520parameters.%2520It%2520employs%2520a%2520novel%2520%2522Castle-Moat-Rampart%2522%2520ground-truth%2520encoding%2520and%2520a%2520dual-autoencoder%2520structure%252C%2520interconnected%2520through%2520an%2520attention-gating%2520mechanism.%2520Experimental%2520evaluations%2520demonstrate%2520state-of-the-art%2520accuracy%2520on%2520plain%2520fingerprints%2520and%2520robust%2520cross-domain%2520generalization%2520to%2520latent%2520impressions.%2520Specifically%252C%2520LEADER%2520attains%2520a%252034%2525%2520higher%2520F1-score%2520on%2520the%2520NIST%2520SD27%2520dataset%2520compared%2520to%2520specialized%2520latent%2520minutiae%2520extractors.%2520Sample-level%2520analysis%2520on%2520this%2520challenging%2520benchmark%2520reveals%2520an%2520average%2520rank%2520of%25202.07%2520among%2520all%2520compared%2520methods%252C%2520with%2520LEADER%2520securing%2520the%2520first-place%2520position%2520in%252047%2525%2520of%2520the%2520samples-more%2520than%2520doubling%2520the%2520frequency%2520of%2520the%2520second-best%2520extractor.%2520The%2520internal%2520representations%2520learned%2520by%2520the%2520model%2520align%2520with%2520established%2520fingerprint%2520domain%2520features%252C%2520such%2520as%2520segmentation%2520masks%252C%2520orientation%2520fields%252C%2520frequency%2520maps%252C%2520and%2520skeletons.%2520Inference%2520requires%252015ms%2520on%2520GPU%2520and%2520322ms%2520on%2520CPU%252C%2520outperforming%2520leading%2520commercial%2520software%2520in%2520computational%2520efficiency.%2520The%2520source%2520code%2520and%2520pre-trained%2520weights%2520are%2520publicly%2520released%2520to%2520facilitate%2520reproducibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEADER%3A%20Lightweight%20End-to-End%20Attention-Gated%20Dual%20Autoencoder%20for%20Robust%20Minutiae%20Extraction&entry.906535625=Raffaele%20Cappelli%20and%20Matteo%20Ferrara&entry.1292438233=Minutiae%20extraction%2C%20a%20fundamental%20stage%20in%20fingerprint%20recognition%2C%20is%20increasingly%20shifting%20toward%20deep%20learning.%20However%2C%20truly%20end-to-end%20methods%20that%20eliminate%20separate%20preprocessing%20and%20postprocessing%20steps%20remain%20scarce.%20This%20paper%20introduces%20LEADER%20%28Lightweight%20End-to-end%20Attention-gated%20Dual%20autoencodER%29%2C%20a%20neural%20network%20that%20maps%20raw%20fingerprint%20images%20to%20minutiae%20descriptors%2C%20including%20location%2C%20direction%2C%20and%20type.%20The%20proposed%20architecture%20integrates%20non-maximum%20suppression%20and%20angular%20decoding%20to%20enable%20complete%20end-to-end%20inference%20using%20only%200.9M%20parameters.%20It%20employs%20a%20novel%20%22Castle-Moat-Rampart%22%20ground-truth%20encoding%20and%20a%20dual-autoencoder%20structure%2C%20interconnected%20through%20an%20attention-gating%20mechanism.%20Experimental%20evaluations%20demonstrate%20state-of-the-art%20accuracy%20on%20plain%20fingerprints%20and%20robust%20cross-domain%20generalization%20to%20latent%20impressions.%20Specifically%2C%20LEADER%20attains%20a%2034%25%20higher%20F1-score%20on%20the%20NIST%20SD27%20dataset%20compared%20to%20specialized%20latent%20minutiae%20extractors.%20Sample-level%20analysis%20on%20this%20challenging%20benchmark%20reveals%20an%20average%20rank%20of%202.07%20among%20all%20compared%20methods%2C%20with%20LEADER%20securing%20the%20first-place%20position%20in%2047%25%20of%20the%20samples-more%20than%20doubling%20the%20frequency%20of%20the%20second-best%20extractor.%20The%20internal%20representations%20learned%20by%20the%20model%20align%20with%20established%20fingerprint%20domain%20features%2C%20such%20as%20segmentation%20masks%2C%20orientation%20fields%2C%20frequency%20maps%2C%20and%20skeletons.%20Inference%20requires%2015ms%20on%20GPU%20and%20322ms%20on%20CPU%2C%20outperforming%20leading%20commercial%20software%20in%20computational%20efficiency.%20The%20source%20code%20and%20pre-trained%20weights%20are%20publicly%20released%20to%20facilitate%20reproducibility.&entry.1838667208=http%3A//arxiv.org/abs/2602.15493v1&entry.124074799=Read"},
{"title": "Should You Use Your Large Language Model to Explore or Exploit?", "author": "Keegan Harris and Aleksandrs Slivkins", "abstract": "We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. While previous work has largely study the ability of LLMs to solve combined exploration-exploitation tasks, we take a more systematic approach and use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that reasoning models show the most promise for solving exploitation tasks, although they are still too expensive or too slow to be used in many practical settings. Motivated by this, we study tool use and in-context summarization using non-reasoning models. We find that these mitigations may be used to substantially improve performance on medium-difficulty tasks, however even then, all LLMs we study perform worse than a simple linear regression, even in non-linear settings. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore.", "link": "http://arxiv.org/abs/2502.00225v3", "date": "2026-02-17", "relevancy": 2.1551, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Should%20You%20Use%20Your%20Large%20Language%20Model%20to%20Explore%20or%20Exploit%3F&body=Title%3A%20Should%20You%20Use%20Your%20Large%20Language%20Model%20to%20Explore%20or%20Exploit%3F%0AAuthor%3A%20Keegan%20Harris%20and%20Aleksandrs%20Slivkins%0AAbstract%3A%20We%20evaluate%20the%20ability%20of%20the%20current%20generation%20of%20large%20language%20models%20%28LLMs%29%20to%20help%20a%20decision-making%20agent%20facing%20an%20exploration-exploitation%20tradeoff.%20While%20previous%20work%20has%20largely%20study%20the%20ability%20of%20LLMs%20to%20solve%20combined%20exploration-exploitation%20tasks%2C%20we%20take%20a%20more%20systematic%20approach%20and%20use%20LLMs%20to%20explore%20and%20exploit%20in%20silos%20in%20various%20%28contextual%29%20bandit%20tasks.%20We%20find%20that%20reasoning%20models%20show%20the%20most%20promise%20for%20solving%20exploitation%20tasks%2C%20although%20they%20are%20still%20too%20expensive%20or%20too%20slow%20to%20be%20used%20in%20many%20practical%20settings.%20Motivated%20by%20this%2C%20we%20study%20tool%20use%20and%20in-context%20summarization%20using%20non-reasoning%20models.%20We%20find%20that%20these%20mitigations%20may%20be%20used%20to%20substantially%20improve%20performance%20on%20medium-difficulty%20tasks%2C%20however%20even%20then%2C%20all%20LLMs%20we%20study%20perform%20worse%20than%20a%20simple%20linear%20regression%2C%20even%20in%20non-linear%20settings.%20On%20the%20other%20hand%2C%20we%20find%20that%20LLMs%20do%20help%20at%20exploring%20large%20action%20spaces%20with%20inherent%20semantics%2C%20by%20suggesting%20suitable%20candidates%20to%20explore.%0ALink%3A%20http%3A//arxiv.org/abs/2502.00225v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShould%2520You%2520Use%2520Your%2520Large%2520Language%2520Model%2520to%2520Explore%2520or%2520Exploit%253F%26entry.906535625%3DKeegan%2520Harris%2520and%2520Aleksandrs%2520Slivkins%26entry.1292438233%3DWe%2520evaluate%2520the%2520ability%2520of%2520the%2520current%2520generation%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520help%2520a%2520decision-making%2520agent%2520facing%2520an%2520exploration-exploitation%2520tradeoff.%2520While%2520previous%2520work%2520has%2520largely%2520study%2520the%2520ability%2520of%2520LLMs%2520to%2520solve%2520combined%2520exploration-exploitation%2520tasks%252C%2520we%2520take%2520a%2520more%2520systematic%2520approach%2520and%2520use%2520LLMs%2520to%2520explore%2520and%2520exploit%2520in%2520silos%2520in%2520various%2520%2528contextual%2529%2520bandit%2520tasks.%2520We%2520find%2520that%2520reasoning%2520models%2520show%2520the%2520most%2520promise%2520for%2520solving%2520exploitation%2520tasks%252C%2520although%2520they%2520are%2520still%2520too%2520expensive%2520or%2520too%2520slow%2520to%2520be%2520used%2520in%2520many%2520practical%2520settings.%2520Motivated%2520by%2520this%252C%2520we%2520study%2520tool%2520use%2520and%2520in-context%2520summarization%2520using%2520non-reasoning%2520models.%2520We%2520find%2520that%2520these%2520mitigations%2520may%2520be%2520used%2520to%2520substantially%2520improve%2520performance%2520on%2520medium-difficulty%2520tasks%252C%2520however%2520even%2520then%252C%2520all%2520LLMs%2520we%2520study%2520perform%2520worse%2520than%2520a%2520simple%2520linear%2520regression%252C%2520even%2520in%2520non-linear%2520settings.%2520On%2520the%2520other%2520hand%252C%2520we%2520find%2520that%2520LLMs%2520do%2520help%2520at%2520exploring%2520large%2520action%2520spaces%2520with%2520inherent%2520semantics%252C%2520by%2520suggesting%2520suitable%2520candidates%2520to%2520explore.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00225v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Should%20You%20Use%20Your%20Large%20Language%20Model%20to%20Explore%20or%20Exploit%3F&entry.906535625=Keegan%20Harris%20and%20Aleksandrs%20Slivkins&entry.1292438233=We%20evaluate%20the%20ability%20of%20the%20current%20generation%20of%20large%20language%20models%20%28LLMs%29%20to%20help%20a%20decision-making%20agent%20facing%20an%20exploration-exploitation%20tradeoff.%20While%20previous%20work%20has%20largely%20study%20the%20ability%20of%20LLMs%20to%20solve%20combined%20exploration-exploitation%20tasks%2C%20we%20take%20a%20more%20systematic%20approach%20and%20use%20LLMs%20to%20explore%20and%20exploit%20in%20silos%20in%20various%20%28contextual%29%20bandit%20tasks.%20We%20find%20that%20reasoning%20models%20show%20the%20most%20promise%20for%20solving%20exploitation%20tasks%2C%20although%20they%20are%20still%20too%20expensive%20or%20too%20slow%20to%20be%20used%20in%20many%20practical%20settings.%20Motivated%20by%20this%2C%20we%20study%20tool%20use%20and%20in-context%20summarization%20using%20non-reasoning%20models.%20We%20find%20that%20these%20mitigations%20may%20be%20used%20to%20substantially%20improve%20performance%20on%20medium-difficulty%20tasks%2C%20however%20even%20then%2C%20all%20LLMs%20we%20study%20perform%20worse%20than%20a%20simple%20linear%20regression%2C%20even%20in%20non-linear%20settings.%20On%20the%20other%20hand%2C%20we%20find%20that%20LLMs%20do%20help%20at%20exploring%20large%20action%20spaces%20with%20inherent%20semantics%2C%20by%20suggesting%20suitable%20candidates%20to%20explore.&entry.1838667208=http%3A//arxiv.org/abs/2502.00225v3&entry.124074799=Read"},
{"title": "Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control", "author": "Anton Klenitskiy and Konstantin Polev and Daria Denisova and Alexey Vasilev and Dmitry Simakov and Gleb Gusev", "abstract": "Many current state-of-the-art models for sequential recommendations are based on transformer architectures. Interpretation and explanation of such black box models is an important research question, as a better understanding of their internals can help understand, influence, and control their behavior, which is very important in a variety of real-world applications. Recently, sparse autoencoders (SAE) have been shown to be a promising unsupervised approach to extract interpretable features from neural networks.\n  In this work, we extend SAE to sequential recommender systems and propose a framework for interpreting and controlling model representations. We show that this approach can be successfully applied to the transformer trained on a sequential recommendation task: directions learned in such an unsupervised regime turn out to be more interpretable and monosemantic than the original hidden state dimensions. Further, we demonstrate a straightforward way to effectively and flexibly control the model's behavior, giving developers and users of recommendation systems the ability to adjust their recommendations to various custom scenarios and contexts.", "link": "http://arxiv.org/abs/2507.12202v2", "date": "2026-02-17", "relevancy": 2.1442, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5701}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Autoencoders%20for%20Sequential%20Recommendation%20Models%3A%20Interpretation%20and%20Flexible%20Control&body=Title%3A%20Sparse%20Autoencoders%20for%20Sequential%20Recommendation%20Models%3A%20Interpretation%20and%20Flexible%20Control%0AAuthor%3A%20Anton%20Klenitskiy%20and%20Konstantin%20Polev%20and%20Daria%20Denisova%20and%20Alexey%20Vasilev%20and%20Dmitry%20Simakov%20and%20Gleb%20Gusev%0AAbstract%3A%20Many%20current%20state-of-the-art%20models%20for%20sequential%20recommendations%20are%20based%20on%20transformer%20architectures.%20Interpretation%20and%20explanation%20of%20such%20black%20box%20models%20is%20an%20important%20research%20question%2C%20as%20a%20better%20understanding%20of%20their%20internals%20can%20help%20understand%2C%20influence%2C%20and%20control%20their%20behavior%2C%20which%20is%20very%20important%20in%20a%20variety%20of%20real-world%20applications.%20Recently%2C%20sparse%20autoencoders%20%28SAE%29%20have%20been%20shown%20to%20be%20a%20promising%20unsupervised%20approach%20to%20extract%20interpretable%20features%20from%20neural%20networks.%0A%20%20In%20this%20work%2C%20we%20extend%20SAE%20to%20sequential%20recommender%20systems%20and%20propose%20a%20framework%20for%20interpreting%20and%20controlling%20model%20representations.%20We%20show%20that%20this%20approach%20can%20be%20successfully%20applied%20to%20the%20transformer%20trained%20on%20a%20sequential%20recommendation%20task%3A%20directions%20learned%20in%20such%20an%20unsupervised%20regime%20turn%20out%20to%20be%20more%20interpretable%20and%20monosemantic%20than%20the%20original%20hidden%20state%20dimensions.%20Further%2C%20we%20demonstrate%20a%20straightforward%20way%20to%20effectively%20and%20flexibly%20control%20the%20model%27s%20behavior%2C%20giving%20developers%20and%20users%20of%20recommendation%20systems%20the%20ability%20to%20adjust%20their%20recommendations%20to%20various%20custom%20scenarios%20and%20contexts.%0ALink%3A%20http%3A//arxiv.org/abs/2507.12202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Autoencoders%2520for%2520Sequential%2520Recommendation%2520Models%253A%2520Interpretation%2520and%2520Flexible%2520Control%26entry.906535625%3DAnton%2520Klenitskiy%2520and%2520Konstantin%2520Polev%2520and%2520Daria%2520Denisova%2520and%2520Alexey%2520Vasilev%2520and%2520Dmitry%2520Simakov%2520and%2520Gleb%2520Gusev%26entry.1292438233%3DMany%2520current%2520state-of-the-art%2520models%2520for%2520sequential%2520recommendations%2520are%2520based%2520on%2520transformer%2520architectures.%2520Interpretation%2520and%2520explanation%2520of%2520such%2520black%2520box%2520models%2520is%2520an%2520important%2520research%2520question%252C%2520as%2520a%2520better%2520understanding%2520of%2520their%2520internals%2520can%2520help%2520understand%252C%2520influence%252C%2520and%2520control%2520their%2520behavior%252C%2520which%2520is%2520very%2520important%2520in%2520a%2520variety%2520of%2520real-world%2520applications.%2520Recently%252C%2520sparse%2520autoencoders%2520%2528SAE%2529%2520have%2520been%2520shown%2520to%2520be%2520a%2520promising%2520unsupervised%2520approach%2520to%2520extract%2520interpretable%2520features%2520from%2520neural%2520networks.%250A%2520%2520In%2520this%2520work%252C%2520we%2520extend%2520SAE%2520to%2520sequential%2520recommender%2520systems%2520and%2520propose%2520a%2520framework%2520for%2520interpreting%2520and%2520controlling%2520model%2520representations.%2520We%2520show%2520that%2520this%2520approach%2520can%2520be%2520successfully%2520applied%2520to%2520the%2520transformer%2520trained%2520on%2520a%2520sequential%2520recommendation%2520task%253A%2520directions%2520learned%2520in%2520such%2520an%2520unsupervised%2520regime%2520turn%2520out%2520to%2520be%2520more%2520interpretable%2520and%2520monosemantic%2520than%2520the%2520original%2520hidden%2520state%2520dimensions.%2520Further%252C%2520we%2520demonstrate%2520a%2520straightforward%2520way%2520to%2520effectively%2520and%2520flexibly%2520control%2520the%2520model%2527s%2520behavior%252C%2520giving%2520developers%2520and%2520users%2520of%2520recommendation%2520systems%2520the%2520ability%2520to%2520adjust%2520their%2520recommendations%2520to%2520various%2520custom%2520scenarios%2520and%2520contexts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Autoencoders%20for%20Sequential%20Recommendation%20Models%3A%20Interpretation%20and%20Flexible%20Control&entry.906535625=Anton%20Klenitskiy%20and%20Konstantin%20Polev%20and%20Daria%20Denisova%20and%20Alexey%20Vasilev%20and%20Dmitry%20Simakov%20and%20Gleb%20Gusev&entry.1292438233=Many%20current%20state-of-the-art%20models%20for%20sequential%20recommendations%20are%20based%20on%20transformer%20architectures.%20Interpretation%20and%20explanation%20of%20such%20black%20box%20models%20is%20an%20important%20research%20question%2C%20as%20a%20better%20understanding%20of%20their%20internals%20can%20help%20understand%2C%20influence%2C%20and%20control%20their%20behavior%2C%20which%20is%20very%20important%20in%20a%20variety%20of%20real-world%20applications.%20Recently%2C%20sparse%20autoencoders%20%28SAE%29%20have%20been%20shown%20to%20be%20a%20promising%20unsupervised%20approach%20to%20extract%20interpretable%20features%20from%20neural%20networks.%0A%20%20In%20this%20work%2C%20we%20extend%20SAE%20to%20sequential%20recommender%20systems%20and%20propose%20a%20framework%20for%20interpreting%20and%20controlling%20model%20representations.%20We%20show%20that%20this%20approach%20can%20be%20successfully%20applied%20to%20the%20transformer%20trained%20on%20a%20sequential%20recommendation%20task%3A%20directions%20learned%20in%20such%20an%20unsupervised%20regime%20turn%20out%20to%20be%20more%20interpretable%20and%20monosemantic%20than%20the%20original%20hidden%20state%20dimensions.%20Further%2C%20we%20demonstrate%20a%20straightforward%20way%20to%20effectively%20and%20flexibly%20control%20the%20model%27s%20behavior%2C%20giving%20developers%20and%20users%20of%20recommendation%20systems%20the%20ability%20to%20adjust%20their%20recommendations%20to%20various%20custom%20scenarios%20and%20contexts.&entry.1838667208=http%3A//arxiv.org/abs/2507.12202v2&entry.124074799=Read"},
{"title": "FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy", "author": "Haochen Zhang and Nirav Savaliya and Faizan Siddiqui and Enna Sachdeva", "abstract": "Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent's attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.", "link": "http://arxiv.org/abs/2602.15813v1", "date": "2026-02-17", "relevancy": 2.1414, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAST-EQA%3A%20Efficient%20Embodied%20Question%20Answering%20with%20Global%20and%20Local%20Region%20Relevancy&body=Title%3A%20FAST-EQA%3A%20Efficient%20Embodied%20Question%20Answering%20with%20Global%20and%20Local%20Region%20Relevancy%0AAuthor%3A%20Haochen%20Zhang%20and%20Nirav%20Savaliya%20and%20Faizan%20Siddiqui%20and%20Enna%20Sachdeva%0AAbstract%3A%20Embodied%20Question%20Answering%20%28EQA%29%20combines%20visual%20scene%20understanding%2C%20goal-directed%20exploration%2C%20spatial%20and%20temporal%20reasoning%20under%20partial%20observability.%20A%20central%20challenge%20is%20to%20confine%20physical%20search%20to%20question-relevant%20subspaces%20while%20maintaining%20a%20compact%2C%20actionable%20memory%20of%20observations.%20Furthermore%2C%20for%20real-world%20deployment%2C%20fast%20inference%20time%20during%20exploration%20is%20crucial.%20We%20introduce%20FAST-EQA%2C%20a%20question-conditioned%20framework%20that%20%28i%29%20identifies%20likely%20visual%20targets%2C%20%28ii%29%20scores%20global%20regions%20of%20interest%20to%20guide%20navigation%2C%20and%20%28iii%29%20employs%20Chain-of-Thought%20%28CoT%29%20reasoning%20over%20visual%20memory%20to%20answer%20confidently.%20FAST-EQA%20maintains%20a%20bounded%20scene%20memory%20that%20stores%20a%20fixed-capacity%20set%20of%20region-target%20hypotheses%20and%20updates%20them%20online%2C%20enabling%20robust%20handling%20of%20both%20single%20and%20multi-target%20questions%20without%20unbounded%20growth.%20To%20expand%20coverage%20efficiently%2C%20a%20global%20exploration%20policy%20treats%20narrow%20openings%20and%20doors%20as%20high-value%20frontiers%2C%20complementing%20local%20target%20seeking%20with%20minimal%20computation.%20Together%2C%20these%20components%20focus%20the%20agent%27s%20attention%2C%20improve%20scene%20coverage%2C%20and%20improve%20answer%20reliability%20while%20running%20substantially%20faster%20than%20prior%20approaches.%20On%20HMEQA%20and%20EXPRESS-Bench%2C%20FAST-EQA%20achieves%20state-of-the-art%20performance%2C%20while%20performing%20competitively%20on%20OpenEQA%20and%20MT-HM3D.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAST-EQA%253A%2520Efficient%2520Embodied%2520Question%2520Answering%2520with%2520Global%2520and%2520Local%2520Region%2520Relevancy%26entry.906535625%3DHaochen%2520Zhang%2520and%2520Nirav%2520Savaliya%2520and%2520Faizan%2520Siddiqui%2520and%2520Enna%2520Sachdeva%26entry.1292438233%3DEmbodied%2520Question%2520Answering%2520%2528EQA%2529%2520combines%2520visual%2520scene%2520understanding%252C%2520goal-directed%2520exploration%252C%2520spatial%2520and%2520temporal%2520reasoning%2520under%2520partial%2520observability.%2520A%2520central%2520challenge%2520is%2520to%2520confine%2520physical%2520search%2520to%2520question-relevant%2520subspaces%2520while%2520maintaining%2520a%2520compact%252C%2520actionable%2520memory%2520of%2520observations.%2520Furthermore%252C%2520for%2520real-world%2520deployment%252C%2520fast%2520inference%2520time%2520during%2520exploration%2520is%2520crucial.%2520We%2520introduce%2520FAST-EQA%252C%2520a%2520question-conditioned%2520framework%2520that%2520%2528i%2529%2520identifies%2520likely%2520visual%2520targets%252C%2520%2528ii%2529%2520scores%2520global%2520regions%2520of%2520interest%2520to%2520guide%2520navigation%252C%2520and%2520%2528iii%2529%2520employs%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520over%2520visual%2520memory%2520to%2520answer%2520confidently.%2520FAST-EQA%2520maintains%2520a%2520bounded%2520scene%2520memory%2520that%2520stores%2520a%2520fixed-capacity%2520set%2520of%2520region-target%2520hypotheses%2520and%2520updates%2520them%2520online%252C%2520enabling%2520robust%2520handling%2520of%2520both%2520single%2520and%2520multi-target%2520questions%2520without%2520unbounded%2520growth.%2520To%2520expand%2520coverage%2520efficiently%252C%2520a%2520global%2520exploration%2520policy%2520treats%2520narrow%2520openings%2520and%2520doors%2520as%2520high-value%2520frontiers%252C%2520complementing%2520local%2520target%2520seeking%2520with%2520minimal%2520computation.%2520Together%252C%2520these%2520components%2520focus%2520the%2520agent%2527s%2520attention%252C%2520improve%2520scene%2520coverage%252C%2520and%2520improve%2520answer%2520reliability%2520while%2520running%2520substantially%2520faster%2520than%2520prior%2520approaches.%2520On%2520HMEQA%2520and%2520EXPRESS-Bench%252C%2520FAST-EQA%2520achieves%2520state-of-the-art%2520performance%252C%2520while%2520performing%2520competitively%2520on%2520OpenEQA%2520and%2520MT-HM3D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAST-EQA%3A%20Efficient%20Embodied%20Question%20Answering%20with%20Global%20and%20Local%20Region%20Relevancy&entry.906535625=Haochen%20Zhang%20and%20Nirav%20Savaliya%20and%20Faizan%20Siddiqui%20and%20Enna%20Sachdeva&entry.1292438233=Embodied%20Question%20Answering%20%28EQA%29%20combines%20visual%20scene%20understanding%2C%20goal-directed%20exploration%2C%20spatial%20and%20temporal%20reasoning%20under%20partial%20observability.%20A%20central%20challenge%20is%20to%20confine%20physical%20search%20to%20question-relevant%20subspaces%20while%20maintaining%20a%20compact%2C%20actionable%20memory%20of%20observations.%20Furthermore%2C%20for%20real-world%20deployment%2C%20fast%20inference%20time%20during%20exploration%20is%20crucial.%20We%20introduce%20FAST-EQA%2C%20a%20question-conditioned%20framework%20that%20%28i%29%20identifies%20likely%20visual%20targets%2C%20%28ii%29%20scores%20global%20regions%20of%20interest%20to%20guide%20navigation%2C%20and%20%28iii%29%20employs%20Chain-of-Thought%20%28CoT%29%20reasoning%20over%20visual%20memory%20to%20answer%20confidently.%20FAST-EQA%20maintains%20a%20bounded%20scene%20memory%20that%20stores%20a%20fixed-capacity%20set%20of%20region-target%20hypotheses%20and%20updates%20them%20online%2C%20enabling%20robust%20handling%20of%20both%20single%20and%20multi-target%20questions%20without%20unbounded%20growth.%20To%20expand%20coverage%20efficiently%2C%20a%20global%20exploration%20policy%20treats%20narrow%20openings%20and%20doors%20as%20high-value%20frontiers%2C%20complementing%20local%20target%20seeking%20with%20minimal%20computation.%20Together%2C%20these%20components%20focus%20the%20agent%27s%20attention%2C%20improve%20scene%20coverage%2C%20and%20improve%20answer%20reliability%20while%20running%20substantially%20faster%20than%20prior%20approaches.%20On%20HMEQA%20and%20EXPRESS-Bench%2C%20FAST-EQA%20achieves%20state-of-the-art%20performance%2C%20while%20performing%20competitively%20on%20OpenEQA%20and%20MT-HM3D.&entry.1838667208=http%3A//arxiv.org/abs/2602.15813v1&entry.124074799=Read"},
{"title": "Spatially-Aware Adaptive Trajectory Optimization with Controller-Guided Feedback for Autonomous Racing", "author": "Alexander Wachter and Alexander Willert and Marc-Philip Ecker and Christian Hartl-Nesic", "abstract": "We present a closed-loop framework for autonomous raceline optimization that combines NURBS-based trajectory representation, CMA-ES global trajectory optimization, and controller-guided spatial feedback. Instead of treating tracking errors as transient disturbances, our method exploits them as informative signals of local track characteristics via a Kalman-inspired spatial update. This enables the construction of an adaptive, acceleration-based constraint map that iteratively refines trajectories toward near-optimal performance under spatially varying track and vehicle behavior. In simulation, our approach achieves a 17.38% lap time reduction compared to a controller parametrized with maximum static acceleration. On real hardware, tested with different tire compounds ranging from high to low friction, we obtain a 7.60% lap time improvement without explicitly parametrizing friction. This demonstrates robustness to changing grip conditions in real-world scenarios.", "link": "http://arxiv.org/abs/2602.15642v1", "date": "2026-02-17", "relevancy": 2.1166, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5351}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5335}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatially-Aware%20Adaptive%20Trajectory%20Optimization%20with%20Controller-Guided%20Feedback%20for%20Autonomous%20Racing&body=Title%3A%20Spatially-Aware%20Adaptive%20Trajectory%20Optimization%20with%20Controller-Guided%20Feedback%20for%20Autonomous%20Racing%0AAuthor%3A%20Alexander%20Wachter%20and%20Alexander%20Willert%20and%20Marc-Philip%20Ecker%20and%20Christian%20Hartl-Nesic%0AAbstract%3A%20We%20present%20a%20closed-loop%20framework%20for%20autonomous%20raceline%20optimization%20that%20combines%20NURBS-based%20trajectory%20representation%2C%20CMA-ES%20global%20trajectory%20optimization%2C%20and%20controller-guided%20spatial%20feedback.%20Instead%20of%20treating%20tracking%20errors%20as%20transient%20disturbances%2C%20our%20method%20exploits%20them%20as%20informative%20signals%20of%20local%20track%20characteristics%20via%20a%20Kalman-inspired%20spatial%20update.%20This%20enables%20the%20construction%20of%20an%20adaptive%2C%20acceleration-based%20constraint%20map%20that%20iteratively%20refines%20trajectories%20toward%20near-optimal%20performance%20under%20spatially%20varying%20track%20and%20vehicle%20behavior.%20In%20simulation%2C%20our%20approach%20achieves%20a%2017.38%25%20lap%20time%20reduction%20compared%20to%20a%20controller%20parametrized%20with%20maximum%20static%20acceleration.%20On%20real%20hardware%2C%20tested%20with%20different%20tire%20compounds%20ranging%20from%20high%20to%20low%20friction%2C%20we%20obtain%20a%207.60%25%20lap%20time%20improvement%20without%20explicitly%20parametrizing%20friction.%20This%20demonstrates%20robustness%20to%20changing%20grip%20conditions%20in%20real-world%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatially-Aware%2520Adaptive%2520Trajectory%2520Optimization%2520with%2520Controller-Guided%2520Feedback%2520for%2520Autonomous%2520Racing%26entry.906535625%3DAlexander%2520Wachter%2520and%2520Alexander%2520Willert%2520and%2520Marc-Philip%2520Ecker%2520and%2520Christian%2520Hartl-Nesic%26entry.1292438233%3DWe%2520present%2520a%2520closed-loop%2520framework%2520for%2520autonomous%2520raceline%2520optimization%2520that%2520combines%2520NURBS-based%2520trajectory%2520representation%252C%2520CMA-ES%2520global%2520trajectory%2520optimization%252C%2520and%2520controller-guided%2520spatial%2520feedback.%2520Instead%2520of%2520treating%2520tracking%2520errors%2520as%2520transient%2520disturbances%252C%2520our%2520method%2520exploits%2520them%2520as%2520informative%2520signals%2520of%2520local%2520track%2520characteristics%2520via%2520a%2520Kalman-inspired%2520spatial%2520update.%2520This%2520enables%2520the%2520construction%2520of%2520an%2520adaptive%252C%2520acceleration-based%2520constraint%2520map%2520that%2520iteratively%2520refines%2520trajectories%2520toward%2520near-optimal%2520performance%2520under%2520spatially%2520varying%2520track%2520and%2520vehicle%2520behavior.%2520In%2520simulation%252C%2520our%2520approach%2520achieves%2520a%252017.38%2525%2520lap%2520time%2520reduction%2520compared%2520to%2520a%2520controller%2520parametrized%2520with%2520maximum%2520static%2520acceleration.%2520On%2520real%2520hardware%252C%2520tested%2520with%2520different%2520tire%2520compounds%2520ranging%2520from%2520high%2520to%2520low%2520friction%252C%2520we%2520obtain%2520a%25207.60%2525%2520lap%2520time%2520improvement%2520without%2520explicitly%2520parametrizing%2520friction.%2520This%2520demonstrates%2520robustness%2520to%2520changing%2520grip%2520conditions%2520in%2520real-world%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatially-Aware%20Adaptive%20Trajectory%20Optimization%20with%20Controller-Guided%20Feedback%20for%20Autonomous%20Racing&entry.906535625=Alexander%20Wachter%20and%20Alexander%20Willert%20and%20Marc-Philip%20Ecker%20and%20Christian%20Hartl-Nesic&entry.1292438233=We%20present%20a%20closed-loop%20framework%20for%20autonomous%20raceline%20optimization%20that%20combines%20NURBS-based%20trajectory%20representation%2C%20CMA-ES%20global%20trajectory%20optimization%2C%20and%20controller-guided%20spatial%20feedback.%20Instead%20of%20treating%20tracking%20errors%20as%20transient%20disturbances%2C%20our%20method%20exploits%20them%20as%20informative%20signals%20of%20local%20track%20characteristics%20via%20a%20Kalman-inspired%20spatial%20update.%20This%20enables%20the%20construction%20of%20an%20adaptive%2C%20acceleration-based%20constraint%20map%20that%20iteratively%20refines%20trajectories%20toward%20near-optimal%20performance%20under%20spatially%20varying%20track%20and%20vehicle%20behavior.%20In%20simulation%2C%20our%20approach%20achieves%20a%2017.38%25%20lap%20time%20reduction%20compared%20to%20a%20controller%20parametrized%20with%20maximum%20static%20acceleration.%20On%20real%20hardware%2C%20tested%20with%20different%20tire%20compounds%20ranging%20from%20high%20to%20low%20friction%2C%20we%20obtain%20a%207.60%25%20lap%20time%20improvement%20without%20explicitly%20parametrizing%20friction.%20This%20demonstrates%20robustness%20to%20changing%20grip%20conditions%20in%20real-world%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2602.15642v1&entry.124074799=Read"},
{"title": "Partition Generative Modeling: Masked Modeling Without Masks", "author": "Justin Deschenaux and Lan Tran and Caglar Gulcehre", "abstract": "Masked generative models (MGMs) can generate tokens in parallel and in any order, unlike autoregressive models (ARMs), which decode one token at a time, left-to-right. However, MGMs process the full-length sequence at every sampling step, including mask tokens that carry no information. In contrast, ARMs process only the previously generated tokens. We introduce ``Partition Generative Models'' (PGMs), which replace masking with partitioning. Tokens are split into two groups that cannot attend to each other, and the model learns to predict each group conditioned on the other, eliminating mask tokens entirely. Because the groups do not interact, PGMs can process only the clean tokens during sampling, like ARMs, while retaining parallel, any-order generation, like MGMs. On OpenWebText, PGMs achieve $5-5.5\\times$ higher throughput than MDLM while producing samples with lower Generative Perplexity. On ImageNet, PGMs reach comparable FID to MaskGIT with a $7.5\\times$ throughput improvement. With twice as many steps, the FID improves to 4.56 while remaining $3.9\\times$ faster than MGMs. Finally, PGMs remain compatible with existing MGM samplers and distillation methods.", "link": "http://arxiv.org/abs/2505.18883v3", "date": "2026-02-17", "relevancy": 2.114, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5642}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5334}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partition%20Generative%20Modeling%3A%20Masked%20Modeling%20Without%20Masks&body=Title%3A%20Partition%20Generative%20Modeling%3A%20Masked%20Modeling%20Without%20Masks%0AAuthor%3A%20Justin%20Deschenaux%20and%20Lan%20Tran%20and%20Caglar%20Gulcehre%0AAbstract%3A%20Masked%20generative%20models%20%28MGMs%29%20can%20generate%20tokens%20in%20parallel%20and%20in%20any%20order%2C%20unlike%20autoregressive%20models%20%28ARMs%29%2C%20which%20decode%20one%20token%20at%20a%20time%2C%20left-to-right.%20However%2C%20MGMs%20process%20the%20full-length%20sequence%20at%20every%20sampling%20step%2C%20including%20mask%20tokens%20that%20carry%20no%20information.%20In%20contrast%2C%20ARMs%20process%20only%20the%20previously%20generated%20tokens.%20We%20introduce%20%60%60Partition%20Generative%20Models%27%27%20%28PGMs%29%2C%20which%20replace%20masking%20with%20partitioning.%20Tokens%20are%20split%20into%20two%20groups%20that%20cannot%20attend%20to%20each%20other%2C%20and%20the%20model%20learns%20to%20predict%20each%20group%20conditioned%20on%20the%20other%2C%20eliminating%20mask%20tokens%20entirely.%20Because%20the%20groups%20do%20not%20interact%2C%20PGMs%20can%20process%20only%20the%20clean%20tokens%20during%20sampling%2C%20like%20ARMs%2C%20while%20retaining%20parallel%2C%20any-order%20generation%2C%20like%20MGMs.%20On%20OpenWebText%2C%20PGMs%20achieve%20%245-5.5%5Ctimes%24%20higher%20throughput%20than%20MDLM%20while%20producing%20samples%20with%20lower%20Generative%20Perplexity.%20On%20ImageNet%2C%20PGMs%20reach%20comparable%20FID%20to%20MaskGIT%20with%20a%20%247.5%5Ctimes%24%20throughput%20improvement.%20With%20twice%20as%20many%20steps%2C%20the%20FID%20improves%20to%204.56%20while%20remaining%20%243.9%5Ctimes%24%20faster%20than%20MGMs.%20Finally%2C%20PGMs%20remain%20compatible%20with%20existing%20MGM%20samplers%20and%20distillation%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2505.18883v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartition%2520Generative%2520Modeling%253A%2520Masked%2520Modeling%2520Without%2520Masks%26entry.906535625%3DJustin%2520Deschenaux%2520and%2520Lan%2520Tran%2520and%2520Caglar%2520Gulcehre%26entry.1292438233%3DMasked%2520generative%2520models%2520%2528MGMs%2529%2520can%2520generate%2520tokens%2520in%2520parallel%2520and%2520in%2520any%2520order%252C%2520unlike%2520autoregressive%2520models%2520%2528ARMs%2529%252C%2520which%2520decode%2520one%2520token%2520at%2520a%2520time%252C%2520left-to-right.%2520However%252C%2520MGMs%2520process%2520the%2520full-length%2520sequence%2520at%2520every%2520sampling%2520step%252C%2520including%2520mask%2520tokens%2520that%2520carry%2520no%2520information.%2520In%2520contrast%252C%2520ARMs%2520process%2520only%2520the%2520previously%2520generated%2520tokens.%2520We%2520introduce%2520%2560%2560Partition%2520Generative%2520Models%2527%2527%2520%2528PGMs%2529%252C%2520which%2520replace%2520masking%2520with%2520partitioning.%2520Tokens%2520are%2520split%2520into%2520two%2520groups%2520that%2520cannot%2520attend%2520to%2520each%2520other%252C%2520and%2520the%2520model%2520learns%2520to%2520predict%2520each%2520group%2520conditioned%2520on%2520the%2520other%252C%2520eliminating%2520mask%2520tokens%2520entirely.%2520Because%2520the%2520groups%2520do%2520not%2520interact%252C%2520PGMs%2520can%2520process%2520only%2520the%2520clean%2520tokens%2520during%2520sampling%252C%2520like%2520ARMs%252C%2520while%2520retaining%2520parallel%252C%2520any-order%2520generation%252C%2520like%2520MGMs.%2520On%2520OpenWebText%252C%2520PGMs%2520achieve%2520%25245-5.5%255Ctimes%2524%2520higher%2520throughput%2520than%2520MDLM%2520while%2520producing%2520samples%2520with%2520lower%2520Generative%2520Perplexity.%2520On%2520ImageNet%252C%2520PGMs%2520reach%2520comparable%2520FID%2520to%2520MaskGIT%2520with%2520a%2520%25247.5%255Ctimes%2524%2520throughput%2520improvement.%2520With%2520twice%2520as%2520many%2520steps%252C%2520the%2520FID%2520improves%2520to%25204.56%2520while%2520remaining%2520%25243.9%255Ctimes%2524%2520faster%2520than%2520MGMs.%2520Finally%252C%2520PGMs%2520remain%2520compatible%2520with%2520existing%2520MGM%2520samplers%2520and%2520distillation%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18883v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partition%20Generative%20Modeling%3A%20Masked%20Modeling%20Without%20Masks&entry.906535625=Justin%20Deschenaux%20and%20Lan%20Tran%20and%20Caglar%20Gulcehre&entry.1292438233=Masked%20generative%20models%20%28MGMs%29%20can%20generate%20tokens%20in%20parallel%20and%20in%20any%20order%2C%20unlike%20autoregressive%20models%20%28ARMs%29%2C%20which%20decode%20one%20token%20at%20a%20time%2C%20left-to-right.%20However%2C%20MGMs%20process%20the%20full-length%20sequence%20at%20every%20sampling%20step%2C%20including%20mask%20tokens%20that%20carry%20no%20information.%20In%20contrast%2C%20ARMs%20process%20only%20the%20previously%20generated%20tokens.%20We%20introduce%20%60%60Partition%20Generative%20Models%27%27%20%28PGMs%29%2C%20which%20replace%20masking%20with%20partitioning.%20Tokens%20are%20split%20into%20two%20groups%20that%20cannot%20attend%20to%20each%20other%2C%20and%20the%20model%20learns%20to%20predict%20each%20group%20conditioned%20on%20the%20other%2C%20eliminating%20mask%20tokens%20entirely.%20Because%20the%20groups%20do%20not%20interact%2C%20PGMs%20can%20process%20only%20the%20clean%20tokens%20during%20sampling%2C%20like%20ARMs%2C%20while%20retaining%20parallel%2C%20any-order%20generation%2C%20like%20MGMs.%20On%20OpenWebText%2C%20PGMs%20achieve%20%245-5.5%5Ctimes%24%20higher%20throughput%20than%20MDLM%20while%20producing%20samples%20with%20lower%20Generative%20Perplexity.%20On%20ImageNet%2C%20PGMs%20reach%20comparable%20FID%20to%20MaskGIT%20with%20a%20%247.5%5Ctimes%24%20throughput%20improvement.%20With%20twice%20as%20many%20steps%2C%20the%20FID%20improves%20to%204.56%20while%20remaining%20%243.9%5Ctimes%24%20faster%20than%20MGMs.%20Finally%2C%20PGMs%20remain%20compatible%20with%20existing%20MGM%20samplers%20and%20distillation%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2505.18883v3&entry.124074799=Read"},
{"title": "Random Wavelet Features for Graph Kernel Machines", "author": "Valentin de Bassompierre and Jean-Charles Delvenne and Laurent Jacques", "abstract": "Node embeddings map graph vertices into low-dimensional Euclidean spaces while preserving structural information. They are central to tasks such as node classification, link prediction, and signal reconstruction. A key goal is to design node embeddings whose dot products capture meaningful notions of node similarity induced by the graph. Graph kernels offer a principled way to define such similarities, but their direct computation is often prohibitive for large networks. Inspired by random feature methods for kernel approximation in Euclidean spaces, we introduce randomized spectral node embeddings whose dot products estimate a low-rank approximation of any specific graph kernel. We provide theoretical and empirical results showing that our embeddings achieve more accurate kernel approximations than existing methods, particularly for spectrally localized kernels. These results demonstrate the effectiveness of randomized spectral constructions for scalable and principled graph representation learning.", "link": "http://arxiv.org/abs/2602.15711v1", "date": "2026-02-17", "relevancy": 2.1129, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4239}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4223}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20Wavelet%20Features%20for%20Graph%20Kernel%20Machines&body=Title%3A%20Random%20Wavelet%20Features%20for%20Graph%20Kernel%20Machines%0AAuthor%3A%20Valentin%20de%20Bassompierre%20and%20Jean-Charles%20Delvenne%20and%20Laurent%20Jacques%0AAbstract%3A%20Node%20embeddings%20map%20graph%20vertices%20into%20low-dimensional%20Euclidean%20spaces%20while%20preserving%20structural%20information.%20They%20are%20central%20to%20tasks%20such%20as%20node%20classification%2C%20link%20prediction%2C%20and%20signal%20reconstruction.%20A%20key%20goal%20is%20to%20design%20node%20embeddings%20whose%20dot%20products%20capture%20meaningful%20notions%20of%20node%20similarity%20induced%20by%20the%20graph.%20Graph%20kernels%20offer%20a%20principled%20way%20to%20define%20such%20similarities%2C%20but%20their%20direct%20computation%20is%20often%20prohibitive%20for%20large%20networks.%20Inspired%20by%20random%20feature%20methods%20for%20kernel%20approximation%20in%20Euclidean%20spaces%2C%20we%20introduce%20randomized%20spectral%20node%20embeddings%20whose%20dot%20products%20estimate%20a%20low-rank%20approximation%20of%20any%20specific%20graph%20kernel.%20We%20provide%20theoretical%20and%20empirical%20results%20showing%20that%20our%20embeddings%20achieve%20more%20accurate%20kernel%20approximations%20than%20existing%20methods%2C%20particularly%20for%20spectrally%20localized%20kernels.%20These%20results%20demonstrate%20the%20effectiveness%20of%20randomized%20spectral%20constructions%20for%20scalable%20and%20principled%20graph%20representation%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520Wavelet%2520Features%2520for%2520Graph%2520Kernel%2520Machines%26entry.906535625%3DValentin%2520de%2520Bassompierre%2520and%2520Jean-Charles%2520Delvenne%2520and%2520Laurent%2520Jacques%26entry.1292438233%3DNode%2520embeddings%2520map%2520graph%2520vertices%2520into%2520low-dimensional%2520Euclidean%2520spaces%2520while%2520preserving%2520structural%2520information.%2520They%2520are%2520central%2520to%2520tasks%2520such%2520as%2520node%2520classification%252C%2520link%2520prediction%252C%2520and%2520signal%2520reconstruction.%2520A%2520key%2520goal%2520is%2520to%2520design%2520node%2520embeddings%2520whose%2520dot%2520products%2520capture%2520meaningful%2520notions%2520of%2520node%2520similarity%2520induced%2520by%2520the%2520graph.%2520Graph%2520kernels%2520offer%2520a%2520principled%2520way%2520to%2520define%2520such%2520similarities%252C%2520but%2520their%2520direct%2520computation%2520is%2520often%2520prohibitive%2520for%2520large%2520networks.%2520Inspired%2520by%2520random%2520feature%2520methods%2520for%2520kernel%2520approximation%2520in%2520Euclidean%2520spaces%252C%2520we%2520introduce%2520randomized%2520spectral%2520node%2520embeddings%2520whose%2520dot%2520products%2520estimate%2520a%2520low-rank%2520approximation%2520of%2520any%2520specific%2520graph%2520kernel.%2520We%2520provide%2520theoretical%2520and%2520empirical%2520results%2520showing%2520that%2520our%2520embeddings%2520achieve%2520more%2520accurate%2520kernel%2520approximations%2520than%2520existing%2520methods%252C%2520particularly%2520for%2520spectrally%2520localized%2520kernels.%2520These%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520randomized%2520spectral%2520constructions%2520for%2520scalable%2520and%2520principled%2520graph%2520representation%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Wavelet%20Features%20for%20Graph%20Kernel%20Machines&entry.906535625=Valentin%20de%20Bassompierre%20and%20Jean-Charles%20Delvenne%20and%20Laurent%20Jacques&entry.1292438233=Node%20embeddings%20map%20graph%20vertices%20into%20low-dimensional%20Euclidean%20spaces%20while%20preserving%20structural%20information.%20They%20are%20central%20to%20tasks%20such%20as%20node%20classification%2C%20link%20prediction%2C%20and%20signal%20reconstruction.%20A%20key%20goal%20is%20to%20design%20node%20embeddings%20whose%20dot%20products%20capture%20meaningful%20notions%20of%20node%20similarity%20induced%20by%20the%20graph.%20Graph%20kernels%20offer%20a%20principled%20way%20to%20define%20such%20similarities%2C%20but%20their%20direct%20computation%20is%20often%20prohibitive%20for%20large%20networks.%20Inspired%20by%20random%20feature%20methods%20for%20kernel%20approximation%20in%20Euclidean%20spaces%2C%20we%20introduce%20randomized%20spectral%20node%20embeddings%20whose%20dot%20products%20estimate%20a%20low-rank%20approximation%20of%20any%20specific%20graph%20kernel.%20We%20provide%20theoretical%20and%20empirical%20results%20showing%20that%20our%20embeddings%20achieve%20more%20accurate%20kernel%20approximations%20than%20existing%20methods%2C%20particularly%20for%20spectrally%20localized%20kernels.%20These%20results%20demonstrate%20the%20effectiveness%20of%20randomized%20spectral%20constructions%20for%20scalable%20and%20principled%20graph%20representation%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2602.15711v1&entry.124074799=Read"},
{"title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models", "author": "Sarim Chaudhry", "abstract": "Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.", "link": "http://arxiv.org/abs/2602.15725v1", "date": "2026-02-17", "relevancy": 2.1093, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursive%20Concept%20Evolution%20for%20Compositional%20Reasoning%20in%20Large%20Language%20Models&body=Title%3A%20Recursive%20Concept%20Evolution%20for%20Compositional%20Reasoning%20in%20Large%20Language%20Models%0AAuthor%3A%20Sarim%20Chaudhry%0AAbstract%3A%20Large%20language%20models%20achieve%20strong%20performance%20on%20many%20complex%20reasoning%20tasks%2C%20yet%20their%20accuracy%20degrades%20sharply%20on%20benchmarks%20that%20require%20compositional%20reasoning%2C%20including%20ARC-AGI-2%2C%20GPQA%2C%20MATH%2C%20BBH%2C%20and%20HLE.%20Existing%20methods%20improve%20reasoning%20by%20expanding%20token-level%20search%20through%20chain-of-thought%20prompting%2C%20self-consistency%2C%20or%20reinforcement%20learning%2C%20but%20they%20leave%20the%20model%27s%20latent%20representation%20space%20fixed.%20When%20the%20required%20abstraction%20is%20not%20already%20encoded%20in%20this%20space%2C%20performance%20collapses.%20We%20propose%20Recursive%20Concept%20Evolution%20%28RCE%29%2C%20a%20framework%20that%20enables%20pretrained%20language%20models%20to%20modify%20their%20internal%20representation%20geometry%20during%20inference.%20RCE%20introduces%20dynamically%20generated%20low-rank%20concept%20subspaces%20that%20are%20spawned%20when%20representational%20inadequacy%20is%20detected%2C%20selected%20through%20a%20minimum%20description%20length%20criterion%2C%20merged%20when%20synergistic%2C%20and%20consolidated%20via%20constrained%20optimization%20to%20preserve%20stability.%20This%20process%20allows%20the%20model%20to%20construct%20new%20abstractions%20rather%20than%20recombining%20existing%20ones.%20We%20integrate%20RCE%20with%20Mistral-7B%20and%20evaluate%20it%20across%20compositional%20reasoning%20benchmarks.%20RCE%20yields%2012-18%20point%20gains%20on%20ARC-AGI-2%2C%208-14%20point%20improvements%20on%20GPQA%20and%20BBH%2C%20and%20consistent%20reductions%20in%20depth-induced%20error%20on%20MATH%20and%20HLE.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursive%2520Concept%2520Evolution%2520for%2520Compositional%2520Reasoning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DSarim%2520Chaudhry%26entry.1292438233%3DLarge%2520language%2520models%2520achieve%2520strong%2520performance%2520on%2520many%2520complex%2520reasoning%2520tasks%252C%2520yet%2520their%2520accuracy%2520degrades%2520sharply%2520on%2520benchmarks%2520that%2520require%2520compositional%2520reasoning%252C%2520including%2520ARC-AGI-2%252C%2520GPQA%252C%2520MATH%252C%2520BBH%252C%2520and%2520HLE.%2520Existing%2520methods%2520improve%2520reasoning%2520by%2520expanding%2520token-level%2520search%2520through%2520chain-of-thought%2520prompting%252C%2520self-consistency%252C%2520or%2520reinforcement%2520learning%252C%2520but%2520they%2520leave%2520the%2520model%2527s%2520latent%2520representation%2520space%2520fixed.%2520When%2520the%2520required%2520abstraction%2520is%2520not%2520already%2520encoded%2520in%2520this%2520space%252C%2520performance%2520collapses.%2520We%2520propose%2520Recursive%2520Concept%2520Evolution%2520%2528RCE%2529%252C%2520a%2520framework%2520that%2520enables%2520pretrained%2520language%2520models%2520to%2520modify%2520their%2520internal%2520representation%2520geometry%2520during%2520inference.%2520RCE%2520introduces%2520dynamically%2520generated%2520low-rank%2520concept%2520subspaces%2520that%2520are%2520spawned%2520when%2520representational%2520inadequacy%2520is%2520detected%252C%2520selected%2520through%2520a%2520minimum%2520description%2520length%2520criterion%252C%2520merged%2520when%2520synergistic%252C%2520and%2520consolidated%2520via%2520constrained%2520optimization%2520to%2520preserve%2520stability.%2520This%2520process%2520allows%2520the%2520model%2520to%2520construct%2520new%2520abstractions%2520rather%2520than%2520recombining%2520existing%2520ones.%2520We%2520integrate%2520RCE%2520with%2520Mistral-7B%2520and%2520evaluate%2520it%2520across%2520compositional%2520reasoning%2520benchmarks.%2520RCE%2520yields%252012-18%2520point%2520gains%2520on%2520ARC-AGI-2%252C%25208-14%2520point%2520improvements%2520on%2520GPQA%2520and%2520BBH%252C%2520and%2520consistent%2520reductions%2520in%2520depth-induced%2520error%2520on%2520MATH%2520and%2520HLE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursive%20Concept%20Evolution%20for%20Compositional%20Reasoning%20in%20Large%20Language%20Models&entry.906535625=Sarim%20Chaudhry&entry.1292438233=Large%20language%20models%20achieve%20strong%20performance%20on%20many%20complex%20reasoning%20tasks%2C%20yet%20their%20accuracy%20degrades%20sharply%20on%20benchmarks%20that%20require%20compositional%20reasoning%2C%20including%20ARC-AGI-2%2C%20GPQA%2C%20MATH%2C%20BBH%2C%20and%20HLE.%20Existing%20methods%20improve%20reasoning%20by%20expanding%20token-level%20search%20through%20chain-of-thought%20prompting%2C%20self-consistency%2C%20or%20reinforcement%20learning%2C%20but%20they%20leave%20the%20model%27s%20latent%20representation%20space%20fixed.%20When%20the%20required%20abstraction%20is%20not%20already%20encoded%20in%20this%20space%2C%20performance%20collapses.%20We%20propose%20Recursive%20Concept%20Evolution%20%28RCE%29%2C%20a%20framework%20that%20enables%20pretrained%20language%20models%20to%20modify%20their%20internal%20representation%20geometry%20during%20inference.%20RCE%20introduces%20dynamically%20generated%20low-rank%20concept%20subspaces%20that%20are%20spawned%20when%20representational%20inadequacy%20is%20detected%2C%20selected%20through%20a%20minimum%20description%20length%20criterion%2C%20merged%20when%20synergistic%2C%20and%20consolidated%20via%20constrained%20optimization%20to%20preserve%20stability.%20This%20process%20allows%20the%20model%20to%20construct%20new%20abstractions%20rather%20than%20recombining%20existing%20ones.%20We%20integrate%20RCE%20with%20Mistral-7B%20and%20evaluate%20it%20across%20compositional%20reasoning%20benchmarks.%20RCE%20yields%2012-18%20point%20gains%20on%20ARC-AGI-2%2C%208-14%20point%20improvements%20on%20GPQA%20and%20BBH%2C%20and%20consistent%20reductions%20in%20depth-induced%20error%20on%20MATH%20and%20HLE.&entry.1838667208=http%3A//arxiv.org/abs/2602.15725v1&entry.124074799=Read"},
{"title": "Relative Geometry of Neural Forecasters: Linking Accuracy and Alignment in Learned Latent Geometry", "author": "Deniz Kucukahmetler and Maximilian Jean Hemmann and Julian Mosig von Aehrenfeld and Maximilian Amthor and Christian Deubel and Nico Scherf and Diaaeldin Taha", "abstract": "Neural networks can accurately forecast complex dynamical systems, yet how they internally represent underlying latent geometry remains poorly understood. We study neural forecasters through the lens of representational alignment, introducing anchor-based, geometry-agnostic relative embeddings that remove rotational and scaling ambiguities in latent spaces. Applying this framework across seven canonical dynamical systems - ranging from periodic to chaotic - we reveal reproducible family-level structure: multilayer perceptrons align with other MLPs, recurrent networks with RNNs, while transformers and echo-state networks achieve strong forecasts despite weaker alignment. Alignment generally correlates with forecasting accuracy, yet high accuracy can coexist with low alignment. Relative geometry thus provides a simple, reproducible foundation for comparing how model families internalize and represent dynamical structure.", "link": "http://arxiv.org/abs/2602.15676v1", "date": "2026-02-17", "relevancy": 2.0995, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5395}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5202}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Geometry%20of%20Neural%20Forecasters%3A%20Linking%20Accuracy%20and%20Alignment%20in%20Learned%20Latent%20Geometry&body=Title%3A%20Relative%20Geometry%20of%20Neural%20Forecasters%3A%20Linking%20Accuracy%20and%20Alignment%20in%20Learned%20Latent%20Geometry%0AAuthor%3A%20Deniz%20Kucukahmetler%20and%20Maximilian%20Jean%20Hemmann%20and%20Julian%20Mosig%20von%20Aehrenfeld%20and%20Maximilian%20Amthor%20and%20Christian%20Deubel%20and%20Nico%20Scherf%20and%20Diaaeldin%20Taha%0AAbstract%3A%20Neural%20networks%20can%20accurately%20forecast%20complex%20dynamical%20systems%2C%20yet%20how%20they%20internally%20represent%20underlying%20latent%20geometry%20remains%20poorly%20understood.%20We%20study%20neural%20forecasters%20through%20the%20lens%20of%20representational%20alignment%2C%20introducing%20anchor-based%2C%20geometry-agnostic%20relative%20embeddings%20that%20remove%20rotational%20and%20scaling%20ambiguities%20in%20latent%20spaces.%20Applying%20this%20framework%20across%20seven%20canonical%20dynamical%20systems%20-%20ranging%20from%20periodic%20to%20chaotic%20-%20we%20reveal%20reproducible%20family-level%20structure%3A%20multilayer%20perceptrons%20align%20with%20other%20MLPs%2C%20recurrent%20networks%20with%20RNNs%2C%20while%20transformers%20and%20echo-state%20networks%20achieve%20strong%20forecasts%20despite%20weaker%20alignment.%20Alignment%20generally%20correlates%20with%20forecasting%20accuracy%2C%20yet%20high%20accuracy%20can%20coexist%20with%20low%20alignment.%20Relative%20geometry%20thus%20provides%20a%20simple%2C%20reproducible%20foundation%20for%20comparing%20how%20model%20families%20internalize%20and%20represent%20dynamical%20structure.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Geometry%2520of%2520Neural%2520Forecasters%253A%2520Linking%2520Accuracy%2520and%2520Alignment%2520in%2520Learned%2520Latent%2520Geometry%26entry.906535625%3DDeniz%2520Kucukahmetler%2520and%2520Maximilian%2520Jean%2520Hemmann%2520and%2520Julian%2520Mosig%2520von%2520Aehrenfeld%2520and%2520Maximilian%2520Amthor%2520and%2520Christian%2520Deubel%2520and%2520Nico%2520Scherf%2520and%2520Diaaeldin%2520Taha%26entry.1292438233%3DNeural%2520networks%2520can%2520accurately%2520forecast%2520complex%2520dynamical%2520systems%252C%2520yet%2520how%2520they%2520internally%2520represent%2520underlying%2520latent%2520geometry%2520remains%2520poorly%2520understood.%2520We%2520study%2520neural%2520forecasters%2520through%2520the%2520lens%2520of%2520representational%2520alignment%252C%2520introducing%2520anchor-based%252C%2520geometry-agnostic%2520relative%2520embeddings%2520that%2520remove%2520rotational%2520and%2520scaling%2520ambiguities%2520in%2520latent%2520spaces.%2520Applying%2520this%2520framework%2520across%2520seven%2520canonical%2520dynamical%2520systems%2520-%2520ranging%2520from%2520periodic%2520to%2520chaotic%2520-%2520we%2520reveal%2520reproducible%2520family-level%2520structure%253A%2520multilayer%2520perceptrons%2520align%2520with%2520other%2520MLPs%252C%2520recurrent%2520networks%2520with%2520RNNs%252C%2520while%2520transformers%2520and%2520echo-state%2520networks%2520achieve%2520strong%2520forecasts%2520despite%2520weaker%2520alignment.%2520Alignment%2520generally%2520correlates%2520with%2520forecasting%2520accuracy%252C%2520yet%2520high%2520accuracy%2520can%2520coexist%2520with%2520low%2520alignment.%2520Relative%2520geometry%2520thus%2520provides%2520a%2520simple%252C%2520reproducible%2520foundation%2520for%2520comparing%2520how%2520model%2520families%2520internalize%2520and%2520represent%2520dynamical%2520structure.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Geometry%20of%20Neural%20Forecasters%3A%20Linking%20Accuracy%20and%20Alignment%20in%20Learned%20Latent%20Geometry&entry.906535625=Deniz%20Kucukahmetler%20and%20Maximilian%20Jean%20Hemmann%20and%20Julian%20Mosig%20von%20Aehrenfeld%20and%20Maximilian%20Amthor%20and%20Christian%20Deubel%20and%20Nico%20Scherf%20and%20Diaaeldin%20Taha&entry.1292438233=Neural%20networks%20can%20accurately%20forecast%20complex%20dynamical%20systems%2C%20yet%20how%20they%20internally%20represent%20underlying%20latent%20geometry%20remains%20poorly%20understood.%20We%20study%20neural%20forecasters%20through%20the%20lens%20of%20representational%20alignment%2C%20introducing%20anchor-based%2C%20geometry-agnostic%20relative%20embeddings%20that%20remove%20rotational%20and%20scaling%20ambiguities%20in%20latent%20spaces.%20Applying%20this%20framework%20across%20seven%20canonical%20dynamical%20systems%20-%20ranging%20from%20periodic%20to%20chaotic%20-%20we%20reveal%20reproducible%20family-level%20structure%3A%20multilayer%20perceptrons%20align%20with%20other%20MLPs%2C%20recurrent%20networks%20with%20RNNs%2C%20while%20transformers%20and%20echo-state%20networks%20achieve%20strong%20forecasts%20despite%20weaker%20alignment.%20Alignment%20generally%20correlates%20with%20forecasting%20accuracy%2C%20yet%20high%20accuracy%20can%20coexist%20with%20low%20alignment.%20Relative%20geometry%20thus%20provides%20a%20simple%2C%20reproducible%20foundation%20for%20comparing%20how%20model%20families%20internalize%20and%20represent%20dynamical%20structure.&entry.1838667208=http%3A//arxiv.org/abs/2602.15676v1&entry.124074799=Read"},
{"title": "Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs", "author": "Chenchen Lin and Sanbao Su and Rachel Luo and Yuxiao Chen and Yan Wang and Marco Pavone and Fei Miao", "abstract": "Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder's rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise \"experts\" and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.", "link": "http://arxiv.org/abs/2601.03100v2", "date": "2026-02-17", "relevancy": 2.0979, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.536}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Guided%20Layer%20Fusion%20Mitigates%20Hallucination%20in%20Multimodal%20LLMs&body=Title%3A%20Text-Guided%20Layer%20Fusion%20Mitigates%20Hallucination%20in%20Multimodal%20LLMs%0AAuthor%3A%20Chenchen%20Lin%20and%20Sanbao%20Su%20and%20Rachel%20Luo%20and%20Yuxiao%20Chen%20and%20Yan%20Wang%20and%20Marco%20Pavone%20and%20Fei%20Miao%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20typically%20rely%20on%20a%20single%20late-layer%20feature%20from%20a%20frozen%20vision%20encoder%2C%20leaving%20the%20encoder%27s%20rich%20hierarchy%20of%20visual%20cues%20under-utilized.%20MLLMs%20still%20suffer%20from%20visually%20ungrounded%20hallucinations%2C%20often%20relying%20on%20language%20priors%20rather%20than%20image%20evidence.%20While%20many%20prior%20mitigation%20strategies%20operate%20on%20the%20text%20side%2C%20they%20leave%20the%20visual%20representation%20unchanged%20and%20do%20not%20exploit%20the%20rich%20hierarchy%20of%20features%20encoded%20across%20vision%20layers.%20Existing%20multi-layer%20fusion%20methods%20partially%20address%20this%20limitation%20but%20remain%20static%2C%20applying%20the%20same%20layer%20mixture%20regardless%20of%20the%20query.%20In%20this%20work%2C%20we%20introduce%20TGIF%20%28Text-Guided%20Inter-layer%20Fusion%29%2C%20a%20lightweight%20module%20that%20treats%20encoder%20layers%20as%20depth-wise%20%22experts%22%20and%20predicts%20a%20prompt-dependent%20fusion%20of%20visual%20features.%20TGIF%20follows%20the%20principle%20of%20direct%20external%20fusion%2C%20requires%20no%20vision-encoder%20updates%2C%20and%20adds%20minimal%20overhead.%20Integrated%20into%20LLaVA-1.5-7B%2C%20TGIF%20provides%20consistent%20improvements%20across%20hallucination%2C%20OCR%2C%20and%20VQA%20benchmarks%2C%20while%20preserving%20or%20improving%20performance%20on%20ScienceQA%2C%20GQA%2C%20and%20MMBench.%20These%20results%20suggest%20that%20query-conditioned%2C%20hierarchy-aware%20fusion%20is%20an%20effective%20way%20to%20strengthen%20visual%20grounding%20and%20reduce%20hallucination%20in%20modern%20MLLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Guided%2520Layer%2520Fusion%2520Mitigates%2520Hallucination%2520in%2520Multimodal%2520LLMs%26entry.906535625%3DChenchen%2520Lin%2520and%2520Sanbao%2520Su%2520and%2520Rachel%2520Luo%2520and%2520Yuxiao%2520Chen%2520and%2520Yan%2520Wang%2520and%2520Marco%2520Pavone%2520and%2520Fei%2520Miao%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520typically%2520rely%2520on%2520a%2520single%2520late-layer%2520feature%2520from%2520a%2520frozen%2520vision%2520encoder%252C%2520leaving%2520the%2520encoder%2527s%2520rich%2520hierarchy%2520of%2520visual%2520cues%2520under-utilized.%2520MLLMs%2520still%2520suffer%2520from%2520visually%2520ungrounded%2520hallucinations%252C%2520often%2520relying%2520on%2520language%2520priors%2520rather%2520than%2520image%2520evidence.%2520While%2520many%2520prior%2520mitigation%2520strategies%2520operate%2520on%2520the%2520text%2520side%252C%2520they%2520leave%2520the%2520visual%2520representation%2520unchanged%2520and%2520do%2520not%2520exploit%2520the%2520rich%2520hierarchy%2520of%2520features%2520encoded%2520across%2520vision%2520layers.%2520Existing%2520multi-layer%2520fusion%2520methods%2520partially%2520address%2520this%2520limitation%2520but%2520remain%2520static%252C%2520applying%2520the%2520same%2520layer%2520mixture%2520regardless%2520of%2520the%2520query.%2520In%2520this%2520work%252C%2520we%2520introduce%2520TGIF%2520%2528Text-Guided%2520Inter-layer%2520Fusion%2529%252C%2520a%2520lightweight%2520module%2520that%2520treats%2520encoder%2520layers%2520as%2520depth-wise%2520%2522experts%2522%2520and%2520predicts%2520a%2520prompt-dependent%2520fusion%2520of%2520visual%2520features.%2520TGIF%2520follows%2520the%2520principle%2520of%2520direct%2520external%2520fusion%252C%2520requires%2520no%2520vision-encoder%2520updates%252C%2520and%2520adds%2520minimal%2520overhead.%2520Integrated%2520into%2520LLaVA-1.5-7B%252C%2520TGIF%2520provides%2520consistent%2520improvements%2520across%2520hallucination%252C%2520OCR%252C%2520and%2520VQA%2520benchmarks%252C%2520while%2520preserving%2520or%2520improving%2520performance%2520on%2520ScienceQA%252C%2520GQA%252C%2520and%2520MMBench.%2520These%2520results%2520suggest%2520that%2520query-conditioned%252C%2520hierarchy-aware%2520fusion%2520is%2520an%2520effective%2520way%2520to%2520strengthen%2520visual%2520grounding%2520and%2520reduce%2520hallucination%2520in%2520modern%2520MLLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Guided%20Layer%20Fusion%20Mitigates%20Hallucination%20in%20Multimodal%20LLMs&entry.906535625=Chenchen%20Lin%20and%20Sanbao%20Su%20and%20Rachel%20Luo%20and%20Yuxiao%20Chen%20and%20Yan%20Wang%20and%20Marco%20Pavone%20and%20Fei%20Miao&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20typically%20rely%20on%20a%20single%20late-layer%20feature%20from%20a%20frozen%20vision%20encoder%2C%20leaving%20the%20encoder%27s%20rich%20hierarchy%20of%20visual%20cues%20under-utilized.%20MLLMs%20still%20suffer%20from%20visually%20ungrounded%20hallucinations%2C%20often%20relying%20on%20language%20priors%20rather%20than%20image%20evidence.%20While%20many%20prior%20mitigation%20strategies%20operate%20on%20the%20text%20side%2C%20they%20leave%20the%20visual%20representation%20unchanged%20and%20do%20not%20exploit%20the%20rich%20hierarchy%20of%20features%20encoded%20across%20vision%20layers.%20Existing%20multi-layer%20fusion%20methods%20partially%20address%20this%20limitation%20but%20remain%20static%2C%20applying%20the%20same%20layer%20mixture%20regardless%20of%20the%20query.%20In%20this%20work%2C%20we%20introduce%20TGIF%20%28Text-Guided%20Inter-layer%20Fusion%29%2C%20a%20lightweight%20module%20that%20treats%20encoder%20layers%20as%20depth-wise%20%22experts%22%20and%20predicts%20a%20prompt-dependent%20fusion%20of%20visual%20features.%20TGIF%20follows%20the%20principle%20of%20direct%20external%20fusion%2C%20requires%20no%20vision-encoder%20updates%2C%20and%20adds%20minimal%20overhead.%20Integrated%20into%20LLaVA-1.5-7B%2C%20TGIF%20provides%20consistent%20improvements%20across%20hallucination%2C%20OCR%2C%20and%20VQA%20benchmarks%2C%20while%20preserving%20or%20improving%20performance%20on%20ScienceQA%2C%20GQA%2C%20and%20MMBench.%20These%20results%20suggest%20that%20query-conditioned%2C%20hierarchy-aware%20fusion%20is%20an%20effective%20way%20to%20strengthen%20visual%20grounding%20and%20reduce%20hallucination%20in%20modern%20MLLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.03100v2&entry.124074799=Read"},
{"title": "Score-based change point detection via tracking the best of infinitely many experts", "author": "Anna Markovich and Nikita Puchkin", "abstract": "We propose an algorithm for nonparametric online change point detection based on sequential score function estimation and the tracking the best expert approach. The core of the procedure is a version of the fixed share forecaster tailored to the case of infinite number of experts and quadratic loss functions. The algorithm shows promising results in numerical experiments on artificial and real-world data sets. Its performance is supported by rigorous high-probability bounds describing behaviour of the test statistic in the pre-change and post-change regimes.", "link": "http://arxiv.org/abs/2408.14073v2", "date": "2026-02-17", "relevancy": 2.0973, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4275}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4166}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Score-based%20change%20point%20detection%20via%20tracking%20the%20best%20of%20infinitely%20many%20experts&body=Title%3A%20Score-based%20change%20point%20detection%20via%20tracking%20the%20best%20of%20infinitely%20many%20experts%0AAuthor%3A%20Anna%20Markovich%20and%20Nikita%20Puchkin%0AAbstract%3A%20We%20propose%20an%20algorithm%20for%20nonparametric%20online%20change%20point%20detection%20based%20on%20sequential%20score%20function%20estimation%20and%20the%20tracking%20the%20best%20expert%20approach.%20The%20core%20of%20the%20procedure%20is%20a%20version%20of%20the%20fixed%20share%20forecaster%20tailored%20to%20the%20case%20of%20infinite%20number%20of%20experts%20and%20quadratic%20loss%20functions.%20The%20algorithm%20shows%20promising%20results%20in%20numerical%20experiments%20on%20artificial%20and%20real-world%20data%20sets.%20Its%20performance%20is%20supported%20by%20rigorous%20high-probability%20bounds%20describing%20behaviour%20of%20the%20test%20statistic%20in%20the%20pre-change%20and%20post-change%20regimes.%0ALink%3A%20http%3A//arxiv.org/abs/2408.14073v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScore-based%2520change%2520point%2520detection%2520via%2520tracking%2520the%2520best%2520of%2520infinitely%2520many%2520experts%26entry.906535625%3DAnna%2520Markovich%2520and%2520Nikita%2520Puchkin%26entry.1292438233%3DWe%2520propose%2520an%2520algorithm%2520for%2520nonparametric%2520online%2520change%2520point%2520detection%2520based%2520on%2520sequential%2520score%2520function%2520estimation%2520and%2520the%2520tracking%2520the%2520best%2520expert%2520approach.%2520The%2520core%2520of%2520the%2520procedure%2520is%2520a%2520version%2520of%2520the%2520fixed%2520share%2520forecaster%2520tailored%2520to%2520the%2520case%2520of%2520infinite%2520number%2520of%2520experts%2520and%2520quadratic%2520loss%2520functions.%2520The%2520algorithm%2520shows%2520promising%2520results%2520in%2520numerical%2520experiments%2520on%2520artificial%2520and%2520real-world%2520data%2520sets.%2520Its%2520performance%2520is%2520supported%2520by%2520rigorous%2520high-probability%2520bounds%2520describing%2520behaviour%2520of%2520the%2520test%2520statistic%2520in%2520the%2520pre-change%2520and%2520post-change%2520regimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14073v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Score-based%20change%20point%20detection%20via%20tracking%20the%20best%20of%20infinitely%20many%20experts&entry.906535625=Anna%20Markovich%20and%20Nikita%20Puchkin&entry.1292438233=We%20propose%20an%20algorithm%20for%20nonparametric%20online%20change%20point%20detection%20based%20on%20sequential%20score%20function%20estimation%20and%20the%20tracking%20the%20best%20expert%20approach.%20The%20core%20of%20the%20procedure%20is%20a%20version%20of%20the%20fixed%20share%20forecaster%20tailored%20to%20the%20case%20of%20infinite%20number%20of%20experts%20and%20quadratic%20loss%20functions.%20The%20algorithm%20shows%20promising%20results%20in%20numerical%20experiments%20on%20artificial%20and%20real-world%20data%20sets.%20Its%20performance%20is%20supported%20by%20rigorous%20high-probability%20bounds%20describing%20behaviour%20of%20the%20test%20statistic%20in%20the%20pre-change%20and%20post-change%20regimes.&entry.1838667208=http%3A//arxiv.org/abs/2408.14073v2&entry.124074799=Read"},
{"title": "GLM-5: from Vibe Coding to Agentic Engineering", "author": "GLM-5 Team and  : and Aohan Zeng and Xin Lv and Zhenyu Hou and Zhengxiao Du and Qinkai Zheng and Bin Chen and Da Yin and Chendi Ge and Chengxing Xie and Cunxiang Wang and Gengzheng Pan and Hao Zeng and Haoke Zhang and Haoran Wang and Huilong Chen and Jiajie Zhang and Jian Jiao and Jiaqi Guo and Jingsen Wang and Jingzhao Du and Jinzhu Wu and Kedong Wang and Lei Li and Lin Fan and Lucen Zhong and Mingdao Liu and Mingming Zhao and Pengfan Du and Qian Dong and Rui Lu and  Shuang-Li and Shulin Cao and Song Liu and Ting Jiang and Xiaodong Chen and Xiaohan Zhang and Xuancheng Huang and Xuezhen Dong and Yabo Xu and Yao Wei and Yifan An and Yilin Niu and Yitong Zhu and Yuanhao Wen and Yukuo Cen and Yushi Bai and Zhongpei Qiao and Zihan Wang and Zikang Wang and Zilin Zhu and Ziqiang Liu and Zixuan Li and Bojie Wang and Bosi Wen and Can Huang and Changpeng Cai and Chao Yu and Chen Li and Chen Li and Chenghua Huang and Chengwei Hu and Chenhui Zhang and Chenzheng Zhu and Congfeng Yin and Daoyan Lin and Dayong Yang and Di Wang and Ding Ai and Erle Zhu and Fangzhou Yi and Feiyu Chen and Guohong Wen and Hailong Sun and Haisha Zhao and Haiyi Hu and Hanchen Zhang and Hanrui Liu and Hanyu Zhang and Hao Peng and Hao Tai and Haobo Zhang and He Liu and Hongwei Wang and Hongxi Yan and Hongyu Ge and Huan Liu and Huan Liu and Huanpeng Chu and Jia'ni Zhao and Jiachen Wang and Jiajing Zhao and Jiamin Ren and Jiapeng Wang and Jiaxin Zhang and Jiayi Gui and Jiayue Zhao and Jijie Li and Jing An and Jing Li and Jingwei Yuan and Jinhua Du and Jinxin Liu and Junkai Zhi and Junwen Duan and Kaiyue Zhou and Kangjian Wei and Ke Wang and Keyun Luo and Laiqiang Zhang and Leigang Sha and Liang Xu and Lindong Wu and Lintao Ding and Lu Chen and Minghao Li and Nianyi Lin and Pan Ta and Qiang Zou and Rongjun Song and Ruiqi Yang and Shangqing Tu and Shangtong Yang and Shaoxiang Wu and Shengyan Zhang and Shijie Li and Shuang Li and Shuyi Fan and Wei Qin and Wei Tian and Weining Zhang and Wenbo Yu and Wenjie Liang and Xiang Kuang and Xiangmeng Cheng and Xiangyang Li and Xiaoquan Yan and Xiaowei Hu and Xiaoying Ling and Xing Fan and Xingye Xia and Xinyuan Zhang and Xinze Zhang and Xirui Pan and Xunkai Zhang and Yandong Wu and Yanfu Li and Yidong Wang and Yifan Zhu and Yijun Tan and Yilin Zhou and Yiming Pan and Ying Zhang and Yinpei Su and Yipeng Geng and Yipeng Geng and Yong Yan and Yonglin Tan and Yuean Bi and Yuhan Shen and Yuhao Yang and Yujiang Li and Yunan Liu and Yunqing Wang and Yuntao Li and Yurong Wu and Yutao Zhang and Yuxi Duan and Yuxuan Zhang and Zezhen Liu and Zhengtao Jiang and Zhenhe Yan and Zheyu Zhang and Zhixiang Wei and Zhuo Chen and Zhuoer Feng and Zijun Yao and Ziwei Chai and Ziyuan Wang and Zuzhou Zhang and Bin Xu and Minlie Huang and Hongning Wang and Juanzi Li and Yuxiao Dong and Jie Tang", "abstract": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.", "link": "http://arxiv.org/abs/2602.15763v1", "date": "2026-02-17", "relevancy": 2.0936, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5548}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5012}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLM-5%3A%20from%20Vibe%20Coding%20to%20Agentic%20Engineering&body=Title%3A%20GLM-5%3A%20from%20Vibe%20Coding%20to%20Agentic%20Engineering%0AAuthor%3A%20GLM-5%20Team%20and%20%20%3A%20and%20Aohan%20Zeng%20and%20Xin%20Lv%20and%20Zhenyu%20Hou%20and%20Zhengxiao%20Du%20and%20Qinkai%20Zheng%20and%20Bin%20Chen%20and%20Da%20Yin%20and%20Chendi%20Ge%20and%20Chengxing%20Xie%20and%20Cunxiang%20Wang%20and%20Gengzheng%20Pan%20and%20Hao%20Zeng%20and%20Haoke%20Zhang%20and%20Haoran%20Wang%20and%20Huilong%20Chen%20and%20Jiajie%20Zhang%20and%20Jian%20Jiao%20and%20Jiaqi%20Guo%20and%20Jingsen%20Wang%20and%20Jingzhao%20Du%20and%20Jinzhu%20Wu%20and%20Kedong%20Wang%20and%20Lei%20Li%20and%20Lin%20Fan%20and%20Lucen%20Zhong%20and%20Mingdao%20Liu%20and%20Mingming%20Zhao%20and%20Pengfan%20Du%20and%20Qian%20Dong%20and%20Rui%20Lu%20and%20%20Shuang-Li%20and%20Shulin%20Cao%20and%20Song%20Liu%20and%20Ting%20Jiang%20and%20Xiaodong%20Chen%20and%20Xiaohan%20Zhang%20and%20Xuancheng%20Huang%20and%20Xuezhen%20Dong%20and%20Yabo%20Xu%20and%20Yao%20Wei%20and%20Yifan%20An%20and%20Yilin%20Niu%20and%20Yitong%20Zhu%20and%20Yuanhao%20Wen%20and%20Yukuo%20Cen%20and%20Yushi%20Bai%20and%20Zhongpei%20Qiao%20and%20Zihan%20Wang%20and%20Zikang%20Wang%20and%20Zilin%20Zhu%20and%20Ziqiang%20Liu%20and%20Zixuan%20Li%20and%20Bojie%20Wang%20and%20Bosi%20Wen%20and%20Can%20Huang%20and%20Changpeng%20Cai%20and%20Chao%20Yu%20and%20Chen%20Li%20and%20Chen%20Li%20and%20Chenghua%20Huang%20and%20Chengwei%20Hu%20and%20Chenhui%20Zhang%20and%20Chenzheng%20Zhu%20and%20Congfeng%20Yin%20and%20Daoyan%20Lin%20and%20Dayong%20Yang%20and%20Di%20Wang%20and%20Ding%20Ai%20and%20Erle%20Zhu%20and%20Fangzhou%20Yi%20and%20Feiyu%20Chen%20and%20Guohong%20Wen%20and%20Hailong%20Sun%20and%20Haisha%20Zhao%20and%20Haiyi%20Hu%20and%20Hanchen%20Zhang%20and%20Hanrui%20Liu%20and%20Hanyu%20Zhang%20and%20Hao%20Peng%20and%20Hao%20Tai%20and%20Haobo%20Zhang%20and%20He%20Liu%20and%20Hongwei%20Wang%20and%20Hongxi%20Yan%20and%20Hongyu%20Ge%20and%20Huan%20Liu%20and%20Huan%20Liu%20and%20Huanpeng%20Chu%20and%20Jia%27ni%20Zhao%20and%20Jiachen%20Wang%20and%20Jiajing%20Zhao%20and%20Jiamin%20Ren%20and%20Jiapeng%20Wang%20and%20Jiaxin%20Zhang%20and%20Jiayi%20Gui%20and%20Jiayue%20Zhao%20and%20Jijie%20Li%20and%20Jing%20An%20and%20Jing%20Li%20and%20Jingwei%20Yuan%20and%20Jinhua%20Du%20and%20Jinxin%20Liu%20and%20Junkai%20Zhi%20and%20Junwen%20Duan%20and%20Kaiyue%20Zhou%20and%20Kangjian%20Wei%20and%20Ke%20Wang%20and%20Keyun%20Luo%20and%20Laiqiang%20Zhang%20and%20Leigang%20Sha%20and%20Liang%20Xu%20and%20Lindong%20Wu%20and%20Lintao%20Ding%20and%20Lu%20Chen%20and%20Minghao%20Li%20and%20Nianyi%20Lin%20and%20Pan%20Ta%20and%20Qiang%20Zou%20and%20Rongjun%20Song%20and%20Ruiqi%20Yang%20and%20Shangqing%20Tu%20and%20Shangtong%20Yang%20and%20Shaoxiang%20Wu%20and%20Shengyan%20Zhang%20and%20Shijie%20Li%20and%20Shuang%20Li%20and%20Shuyi%20Fan%20and%20Wei%20Qin%20and%20Wei%20Tian%20and%20Weining%20Zhang%20and%20Wenbo%20Yu%20and%20Wenjie%20Liang%20and%20Xiang%20Kuang%20and%20Xiangmeng%20Cheng%20and%20Xiangyang%20Li%20and%20Xiaoquan%20Yan%20and%20Xiaowei%20Hu%20and%20Xiaoying%20Ling%20and%20Xing%20Fan%20and%20Xingye%20Xia%20and%20Xinyuan%20Zhang%20and%20Xinze%20Zhang%20and%20Xirui%20Pan%20and%20Xunkai%20Zhang%20and%20Yandong%20Wu%20and%20Yanfu%20Li%20and%20Yidong%20Wang%20and%20Yifan%20Zhu%20and%20Yijun%20Tan%20and%20Yilin%20Zhou%20and%20Yiming%20Pan%20and%20Ying%20Zhang%20and%20Yinpei%20Su%20and%20Yipeng%20Geng%20and%20Yipeng%20Geng%20and%20Yong%20Yan%20and%20Yonglin%20Tan%20and%20Yuean%20Bi%20and%20Yuhan%20Shen%20and%20Yuhao%20Yang%20and%20Yujiang%20Li%20and%20Yunan%20Liu%20and%20Yunqing%20Wang%20and%20Yuntao%20Li%20and%20Yurong%20Wu%20and%20Yutao%20Zhang%20and%20Yuxi%20Duan%20and%20Yuxuan%20Zhang%20and%20Zezhen%20Liu%20and%20Zhengtao%20Jiang%20and%20Zhenhe%20Yan%20and%20Zheyu%20Zhang%20and%20Zhixiang%20Wei%20and%20Zhuo%20Chen%20and%20Zhuoer%20Feng%20and%20Zijun%20Yao%20and%20Ziwei%20Chai%20and%20Ziyuan%20Wang%20and%20Zuzhou%20Zhang%20and%20Bin%20Xu%20and%20Minlie%20Huang%20and%20Hongning%20Wang%20and%20Juanzi%20Li%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%0AAbstract%3A%20We%20present%20GLM-5%2C%20a%20next-generation%20foundation%20model%20designed%20to%20transition%20the%20paradigm%20of%20vibe%20coding%20to%20agentic%20engineering.%20Building%20upon%20the%20agentic%2C%20reasoning%2C%20and%20coding%20%28ARC%29%20capabilities%20of%20its%20predecessor%2C%20GLM-5%20adopts%20DSA%20to%20significantly%20reduce%20training%20and%20inference%20costs%20while%20maintaining%20long-context%20fidelity.%20To%20advance%20model%20alignment%20and%20autonomy%2C%20we%20implement%20a%20new%20asynchronous%20reinforcement%20learning%20infrastructure%20that%20drastically%20improves%20post-training%20efficiency%20by%20decoupling%20generation%20from%20training.%20Furthermore%2C%20we%20propose%20novel%20asynchronous%20agent%20RL%20algorithms%20that%20further%20improve%20RL%20quality%2C%20enabling%20the%20model%20to%20learn%20from%20complex%2C%20long-horizon%20interactions%20more%20effectively.%20Through%20these%20innovations%2C%20GLM-5%20achieves%20state-of-the-art%20performance%20on%20major%20open%20benchmarks.%20Most%20critically%2C%20GLM-5%20demonstrates%20unprecedented%20capability%20in%20real-world%20coding%20tasks%2C%20surpassing%20previous%20baselines%20in%20handling%20end-to-end%20software%20engineering%20challenges.%20Code%2C%20models%2C%20and%20more%20information%20are%20available%20at%20https%3A//github.com/zai-org/GLM-5.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLM-5%253A%2520from%2520Vibe%2520Coding%2520to%2520Agentic%2520Engineering%26entry.906535625%3DGLM-5%2520Team%2520and%2520%2520%253A%2520and%2520Aohan%2520Zeng%2520and%2520Xin%2520Lv%2520and%2520Zhenyu%2520Hou%2520and%2520Zhengxiao%2520Du%2520and%2520Qinkai%2520Zheng%2520and%2520Bin%2520Chen%2520and%2520Da%2520Yin%2520and%2520Chendi%2520Ge%2520and%2520Chengxing%2520Xie%2520and%2520Cunxiang%2520Wang%2520and%2520Gengzheng%2520Pan%2520and%2520Hao%2520Zeng%2520and%2520Haoke%2520Zhang%2520and%2520Haoran%2520Wang%2520and%2520Huilong%2520Chen%2520and%2520Jiajie%2520Zhang%2520and%2520Jian%2520Jiao%2520and%2520Jiaqi%2520Guo%2520and%2520Jingsen%2520Wang%2520and%2520Jingzhao%2520Du%2520and%2520Jinzhu%2520Wu%2520and%2520Kedong%2520Wang%2520and%2520Lei%2520Li%2520and%2520Lin%2520Fan%2520and%2520Lucen%2520Zhong%2520and%2520Mingdao%2520Liu%2520and%2520Mingming%2520Zhao%2520and%2520Pengfan%2520Du%2520and%2520Qian%2520Dong%2520and%2520Rui%2520Lu%2520and%2520%2520Shuang-Li%2520and%2520Shulin%2520Cao%2520and%2520Song%2520Liu%2520and%2520Ting%2520Jiang%2520and%2520Xiaodong%2520Chen%2520and%2520Xiaohan%2520Zhang%2520and%2520Xuancheng%2520Huang%2520and%2520Xuezhen%2520Dong%2520and%2520Yabo%2520Xu%2520and%2520Yao%2520Wei%2520and%2520Yifan%2520An%2520and%2520Yilin%2520Niu%2520and%2520Yitong%2520Zhu%2520and%2520Yuanhao%2520Wen%2520and%2520Yukuo%2520Cen%2520and%2520Yushi%2520Bai%2520and%2520Zhongpei%2520Qiao%2520and%2520Zihan%2520Wang%2520and%2520Zikang%2520Wang%2520and%2520Zilin%2520Zhu%2520and%2520Ziqiang%2520Liu%2520and%2520Zixuan%2520Li%2520and%2520Bojie%2520Wang%2520and%2520Bosi%2520Wen%2520and%2520Can%2520Huang%2520and%2520Changpeng%2520Cai%2520and%2520Chao%2520Yu%2520and%2520Chen%2520Li%2520and%2520Chen%2520Li%2520and%2520Chenghua%2520Huang%2520and%2520Chengwei%2520Hu%2520and%2520Chenhui%2520Zhang%2520and%2520Chenzheng%2520Zhu%2520and%2520Congfeng%2520Yin%2520and%2520Daoyan%2520Lin%2520and%2520Dayong%2520Yang%2520and%2520Di%2520Wang%2520and%2520Ding%2520Ai%2520and%2520Erle%2520Zhu%2520and%2520Fangzhou%2520Yi%2520and%2520Feiyu%2520Chen%2520and%2520Guohong%2520Wen%2520and%2520Hailong%2520Sun%2520and%2520Haisha%2520Zhao%2520and%2520Haiyi%2520Hu%2520and%2520Hanchen%2520Zhang%2520and%2520Hanrui%2520Liu%2520and%2520Hanyu%2520Zhang%2520and%2520Hao%2520Peng%2520and%2520Hao%2520Tai%2520and%2520Haobo%2520Zhang%2520and%2520He%2520Liu%2520and%2520Hongwei%2520Wang%2520and%2520Hongxi%2520Yan%2520and%2520Hongyu%2520Ge%2520and%2520Huan%2520Liu%2520and%2520Huan%2520Liu%2520and%2520Huanpeng%2520Chu%2520and%2520Jia%2527ni%2520Zhao%2520and%2520Jiachen%2520Wang%2520and%2520Jiajing%2520Zhao%2520and%2520Jiamin%2520Ren%2520and%2520Jiapeng%2520Wang%2520and%2520Jiaxin%2520Zhang%2520and%2520Jiayi%2520Gui%2520and%2520Jiayue%2520Zhao%2520and%2520Jijie%2520Li%2520and%2520Jing%2520An%2520and%2520Jing%2520Li%2520and%2520Jingwei%2520Yuan%2520and%2520Jinhua%2520Du%2520and%2520Jinxin%2520Liu%2520and%2520Junkai%2520Zhi%2520and%2520Junwen%2520Duan%2520and%2520Kaiyue%2520Zhou%2520and%2520Kangjian%2520Wei%2520and%2520Ke%2520Wang%2520and%2520Keyun%2520Luo%2520and%2520Laiqiang%2520Zhang%2520and%2520Leigang%2520Sha%2520and%2520Liang%2520Xu%2520and%2520Lindong%2520Wu%2520and%2520Lintao%2520Ding%2520and%2520Lu%2520Chen%2520and%2520Minghao%2520Li%2520and%2520Nianyi%2520Lin%2520and%2520Pan%2520Ta%2520and%2520Qiang%2520Zou%2520and%2520Rongjun%2520Song%2520and%2520Ruiqi%2520Yang%2520and%2520Shangqing%2520Tu%2520and%2520Shangtong%2520Yang%2520and%2520Shaoxiang%2520Wu%2520and%2520Shengyan%2520Zhang%2520and%2520Shijie%2520Li%2520and%2520Shuang%2520Li%2520and%2520Shuyi%2520Fan%2520and%2520Wei%2520Qin%2520and%2520Wei%2520Tian%2520and%2520Weining%2520Zhang%2520and%2520Wenbo%2520Yu%2520and%2520Wenjie%2520Liang%2520and%2520Xiang%2520Kuang%2520and%2520Xiangmeng%2520Cheng%2520and%2520Xiangyang%2520Li%2520and%2520Xiaoquan%2520Yan%2520and%2520Xiaowei%2520Hu%2520and%2520Xiaoying%2520Ling%2520and%2520Xing%2520Fan%2520and%2520Xingye%2520Xia%2520and%2520Xinyuan%2520Zhang%2520and%2520Xinze%2520Zhang%2520and%2520Xirui%2520Pan%2520and%2520Xunkai%2520Zhang%2520and%2520Yandong%2520Wu%2520and%2520Yanfu%2520Li%2520and%2520Yidong%2520Wang%2520and%2520Yifan%2520Zhu%2520and%2520Yijun%2520Tan%2520and%2520Yilin%2520Zhou%2520and%2520Yiming%2520Pan%2520and%2520Ying%2520Zhang%2520and%2520Yinpei%2520Su%2520and%2520Yipeng%2520Geng%2520and%2520Yipeng%2520Geng%2520and%2520Yong%2520Yan%2520and%2520Yonglin%2520Tan%2520and%2520Yuean%2520Bi%2520and%2520Yuhan%2520Shen%2520and%2520Yuhao%2520Yang%2520and%2520Yujiang%2520Li%2520and%2520Yunan%2520Liu%2520and%2520Yunqing%2520Wang%2520and%2520Yuntao%2520Li%2520and%2520Yurong%2520Wu%2520and%2520Yutao%2520Zhang%2520and%2520Yuxi%2520Duan%2520and%2520Yuxuan%2520Zhang%2520and%2520Zezhen%2520Liu%2520and%2520Zhengtao%2520Jiang%2520and%2520Zhenhe%2520Yan%2520and%2520Zheyu%2520Zhang%2520and%2520Zhixiang%2520Wei%2520and%2520Zhuo%2520Chen%2520and%2520Zhuoer%2520Feng%2520and%2520Zijun%2520Yao%2520and%2520Ziwei%2520Chai%2520and%2520Ziyuan%2520Wang%2520and%2520Zuzhou%2520Zhang%2520and%2520Bin%2520Xu%2520and%2520Minlie%2520Huang%2520and%2520Hongning%2520Wang%2520and%2520Juanzi%2520Li%2520and%2520Yuxiao%2520Dong%2520and%2520Jie%2520Tang%26entry.1292438233%3DWe%2520present%2520GLM-5%252C%2520a%2520next-generation%2520foundation%2520model%2520designed%2520to%2520transition%2520the%2520paradigm%2520of%2520vibe%2520coding%2520to%2520agentic%2520engineering.%2520Building%2520upon%2520the%2520agentic%252C%2520reasoning%252C%2520and%2520coding%2520%2528ARC%2529%2520capabilities%2520of%2520its%2520predecessor%252C%2520GLM-5%2520adopts%2520DSA%2520to%2520significantly%2520reduce%2520training%2520and%2520inference%2520costs%2520while%2520maintaining%2520long-context%2520fidelity.%2520To%2520advance%2520model%2520alignment%2520and%2520autonomy%252C%2520we%2520implement%2520a%2520new%2520asynchronous%2520reinforcement%2520learning%2520infrastructure%2520that%2520drastically%2520improves%2520post-training%2520efficiency%2520by%2520decoupling%2520generation%2520from%2520training.%2520Furthermore%252C%2520we%2520propose%2520novel%2520asynchronous%2520agent%2520RL%2520algorithms%2520that%2520further%2520improve%2520RL%2520quality%252C%2520enabling%2520the%2520model%2520to%2520learn%2520from%2520complex%252C%2520long-horizon%2520interactions%2520more%2520effectively.%2520Through%2520these%2520innovations%252C%2520GLM-5%2520achieves%2520state-of-the-art%2520performance%2520on%2520major%2520open%2520benchmarks.%2520Most%2520critically%252C%2520GLM-5%2520demonstrates%2520unprecedented%2520capability%2520in%2520real-world%2520coding%2520tasks%252C%2520surpassing%2520previous%2520baselines%2520in%2520handling%2520end-to-end%2520software%2520engineering%2520challenges.%2520Code%252C%2520models%252C%2520and%2520more%2520information%2520are%2520available%2520at%2520https%253A//github.com/zai-org/GLM-5.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLM-5%3A%20from%20Vibe%20Coding%20to%20Agentic%20Engineering&entry.906535625=GLM-5%20Team%20and%20%20%3A%20and%20Aohan%20Zeng%20and%20Xin%20Lv%20and%20Zhenyu%20Hou%20and%20Zhengxiao%20Du%20and%20Qinkai%20Zheng%20and%20Bin%20Chen%20and%20Da%20Yin%20and%20Chendi%20Ge%20and%20Chengxing%20Xie%20and%20Cunxiang%20Wang%20and%20Gengzheng%20Pan%20and%20Hao%20Zeng%20and%20Haoke%20Zhang%20and%20Haoran%20Wang%20and%20Huilong%20Chen%20and%20Jiajie%20Zhang%20and%20Jian%20Jiao%20and%20Jiaqi%20Guo%20and%20Jingsen%20Wang%20and%20Jingzhao%20Du%20and%20Jinzhu%20Wu%20and%20Kedong%20Wang%20and%20Lei%20Li%20and%20Lin%20Fan%20and%20Lucen%20Zhong%20and%20Mingdao%20Liu%20and%20Mingming%20Zhao%20and%20Pengfan%20Du%20and%20Qian%20Dong%20and%20Rui%20Lu%20and%20%20Shuang-Li%20and%20Shulin%20Cao%20and%20Song%20Liu%20and%20Ting%20Jiang%20and%20Xiaodong%20Chen%20and%20Xiaohan%20Zhang%20and%20Xuancheng%20Huang%20and%20Xuezhen%20Dong%20and%20Yabo%20Xu%20and%20Yao%20Wei%20and%20Yifan%20An%20and%20Yilin%20Niu%20and%20Yitong%20Zhu%20and%20Yuanhao%20Wen%20and%20Yukuo%20Cen%20and%20Yushi%20Bai%20and%20Zhongpei%20Qiao%20and%20Zihan%20Wang%20and%20Zikang%20Wang%20and%20Zilin%20Zhu%20and%20Ziqiang%20Liu%20and%20Zixuan%20Li%20and%20Bojie%20Wang%20and%20Bosi%20Wen%20and%20Can%20Huang%20and%20Changpeng%20Cai%20and%20Chao%20Yu%20and%20Chen%20Li%20and%20Chen%20Li%20and%20Chenghua%20Huang%20and%20Chengwei%20Hu%20and%20Chenhui%20Zhang%20and%20Chenzheng%20Zhu%20and%20Congfeng%20Yin%20and%20Daoyan%20Lin%20and%20Dayong%20Yang%20and%20Di%20Wang%20and%20Ding%20Ai%20and%20Erle%20Zhu%20and%20Fangzhou%20Yi%20and%20Feiyu%20Chen%20and%20Guohong%20Wen%20and%20Hailong%20Sun%20and%20Haisha%20Zhao%20and%20Haiyi%20Hu%20and%20Hanchen%20Zhang%20and%20Hanrui%20Liu%20and%20Hanyu%20Zhang%20and%20Hao%20Peng%20and%20Hao%20Tai%20and%20Haobo%20Zhang%20and%20He%20Liu%20and%20Hongwei%20Wang%20and%20Hongxi%20Yan%20and%20Hongyu%20Ge%20and%20Huan%20Liu%20and%20Huan%20Liu%20and%20Huanpeng%20Chu%20and%20Jia%27ni%20Zhao%20and%20Jiachen%20Wang%20and%20Jiajing%20Zhao%20and%20Jiamin%20Ren%20and%20Jiapeng%20Wang%20and%20Jiaxin%20Zhang%20and%20Jiayi%20Gui%20and%20Jiayue%20Zhao%20and%20Jijie%20Li%20and%20Jing%20An%20and%20Jing%20Li%20and%20Jingwei%20Yuan%20and%20Jinhua%20Du%20and%20Jinxin%20Liu%20and%20Junkai%20Zhi%20and%20Junwen%20Duan%20and%20Kaiyue%20Zhou%20and%20Kangjian%20Wei%20and%20Ke%20Wang%20and%20Keyun%20Luo%20and%20Laiqiang%20Zhang%20and%20Leigang%20Sha%20and%20Liang%20Xu%20and%20Lindong%20Wu%20and%20Lintao%20Ding%20and%20Lu%20Chen%20and%20Minghao%20Li%20and%20Nianyi%20Lin%20and%20Pan%20Ta%20and%20Qiang%20Zou%20and%20Rongjun%20Song%20and%20Ruiqi%20Yang%20and%20Shangqing%20Tu%20and%20Shangtong%20Yang%20and%20Shaoxiang%20Wu%20and%20Shengyan%20Zhang%20and%20Shijie%20Li%20and%20Shuang%20Li%20and%20Shuyi%20Fan%20and%20Wei%20Qin%20and%20Wei%20Tian%20and%20Weining%20Zhang%20and%20Wenbo%20Yu%20and%20Wenjie%20Liang%20and%20Xiang%20Kuang%20and%20Xiangmeng%20Cheng%20and%20Xiangyang%20Li%20and%20Xiaoquan%20Yan%20and%20Xiaowei%20Hu%20and%20Xiaoying%20Ling%20and%20Xing%20Fan%20and%20Xingye%20Xia%20and%20Xinyuan%20Zhang%20and%20Xinze%20Zhang%20and%20Xirui%20Pan%20and%20Xunkai%20Zhang%20and%20Yandong%20Wu%20and%20Yanfu%20Li%20and%20Yidong%20Wang%20and%20Yifan%20Zhu%20and%20Yijun%20Tan%20and%20Yilin%20Zhou%20and%20Yiming%20Pan%20and%20Ying%20Zhang%20and%20Yinpei%20Su%20and%20Yipeng%20Geng%20and%20Yipeng%20Geng%20and%20Yong%20Yan%20and%20Yonglin%20Tan%20and%20Yuean%20Bi%20and%20Yuhan%20Shen%20and%20Yuhao%20Yang%20and%20Yujiang%20Li%20and%20Yunan%20Liu%20and%20Yunqing%20Wang%20and%20Yuntao%20Li%20and%20Yurong%20Wu%20and%20Yutao%20Zhang%20and%20Yuxi%20Duan%20and%20Yuxuan%20Zhang%20and%20Zezhen%20Liu%20and%20Zhengtao%20Jiang%20and%20Zhenhe%20Yan%20and%20Zheyu%20Zhang%20and%20Zhixiang%20Wei%20and%20Zhuo%20Chen%20and%20Zhuoer%20Feng%20and%20Zijun%20Yao%20and%20Ziwei%20Chai%20and%20Ziyuan%20Wang%20and%20Zuzhou%20Zhang%20and%20Bin%20Xu%20and%20Minlie%20Huang%20and%20Hongning%20Wang%20and%20Juanzi%20Li%20and%20Yuxiao%20Dong%20and%20Jie%20Tang&entry.1292438233=We%20present%20GLM-5%2C%20a%20next-generation%20foundation%20model%20designed%20to%20transition%20the%20paradigm%20of%20vibe%20coding%20to%20agentic%20engineering.%20Building%20upon%20the%20agentic%2C%20reasoning%2C%20and%20coding%20%28ARC%29%20capabilities%20of%20its%20predecessor%2C%20GLM-5%20adopts%20DSA%20to%20significantly%20reduce%20training%20and%20inference%20costs%20while%20maintaining%20long-context%20fidelity.%20To%20advance%20model%20alignment%20and%20autonomy%2C%20we%20implement%20a%20new%20asynchronous%20reinforcement%20learning%20infrastructure%20that%20drastically%20improves%20post-training%20efficiency%20by%20decoupling%20generation%20from%20training.%20Furthermore%2C%20we%20propose%20novel%20asynchronous%20agent%20RL%20algorithms%20that%20further%20improve%20RL%20quality%2C%20enabling%20the%20model%20to%20learn%20from%20complex%2C%20long-horizon%20interactions%20more%20effectively.%20Through%20these%20innovations%2C%20GLM-5%20achieves%20state-of-the-art%20performance%20on%20major%20open%20benchmarks.%20Most%20critically%2C%20GLM-5%20demonstrates%20unprecedented%20capability%20in%20real-world%20coding%20tasks%2C%20surpassing%20previous%20baselines%20in%20handling%20end-to-end%20software%20engineering%20challenges.%20Code%2C%20models%2C%20and%20more%20information%20are%20available%20at%20https%3A//github.com/zai-org/GLM-5.&entry.1838667208=http%3A//arxiv.org/abs/2602.15763v1&entry.124074799=Read"},
{"title": "Efficient Knowledge Transfer for Jump-Starting Control Policy Learning of Multirotors through Physics-Aware Neural Architectures", "author": "Welf Rehberg and Mihir Kulkarni and Philipp Weiss and Kostas Alexis", "abstract": "Efficiently training control policies for robots is a major challenge that can greatly benefit from utilizing knowledge gained from training similar systems through cross-embodiment knowledge transfer. In this work, we focus on accelerating policy training using a library-based initialization scheme that enables effective knowledge transfer across multirotor configurations. By leveraging a physics-aware neural control architecture that combines a reinforcement learning-based controller and a supervised control allocation network, we enable the reuse of previously trained policies. To this end, we utilize a policy evaluation-based similarity measure that identifies suitable policies for initialization from a library. We demonstrate that this measure correlates with the reduction in environment interactions needed to reach target performance and is therefore suited for initialization. Extensive simulation and real-world experiments confirm that our control architecture achieves state-of-the-art control performance, and that our initialization scheme saves on average up to $73.5\\%$ of environment interactions (compared to training a policy from scratch) across diverse quadrotor and hexarotor designs, paving the way for efficient cross-embodiment transfer in reinforcement learning.", "link": "http://arxiv.org/abs/2602.15533v1", "date": "2026-02-17", "relevancy": 2.0824, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5545}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5364}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Knowledge%20Transfer%20for%20Jump-Starting%20Control%20Policy%20Learning%20of%20Multirotors%20through%20Physics-Aware%20Neural%20Architectures&body=Title%3A%20Efficient%20Knowledge%20Transfer%20for%20Jump-Starting%20Control%20Policy%20Learning%20of%20Multirotors%20through%20Physics-Aware%20Neural%20Architectures%0AAuthor%3A%20Welf%20Rehberg%20and%20Mihir%20Kulkarni%20and%20Philipp%20Weiss%20and%20Kostas%20Alexis%0AAbstract%3A%20Efficiently%20training%20control%20policies%20for%20robots%20is%20a%20major%20challenge%20that%20can%20greatly%20benefit%20from%20utilizing%20knowledge%20gained%20from%20training%20similar%20systems%20through%20cross-embodiment%20knowledge%20transfer.%20In%20this%20work%2C%20we%20focus%20on%20accelerating%20policy%20training%20using%20a%20library-based%20initialization%20scheme%20that%20enables%20effective%20knowledge%20transfer%20across%20multirotor%20configurations.%20By%20leveraging%20a%20physics-aware%20neural%20control%20architecture%20that%20combines%20a%20reinforcement%20learning-based%20controller%20and%20a%20supervised%20control%20allocation%20network%2C%20we%20enable%20the%20reuse%20of%20previously%20trained%20policies.%20To%20this%20end%2C%20we%20utilize%20a%20policy%20evaluation-based%20similarity%20measure%20that%20identifies%20suitable%20policies%20for%20initialization%20from%20a%20library.%20We%20demonstrate%20that%20this%20measure%20correlates%20with%20the%20reduction%20in%20environment%20interactions%20needed%20to%20reach%20target%20performance%20and%20is%20therefore%20suited%20for%20initialization.%20Extensive%20simulation%20and%20real-world%20experiments%20confirm%20that%20our%20control%20architecture%20achieves%20state-of-the-art%20control%20performance%2C%20and%20that%20our%20initialization%20scheme%20saves%20on%20average%20up%20to%20%2473.5%5C%25%24%20of%20environment%20interactions%20%28compared%20to%20training%20a%20policy%20from%20scratch%29%20across%20diverse%20quadrotor%20and%20hexarotor%20designs%2C%20paving%20the%20way%20for%20efficient%20cross-embodiment%20transfer%20in%20reinforcement%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Knowledge%2520Transfer%2520for%2520Jump-Starting%2520Control%2520Policy%2520Learning%2520of%2520Multirotors%2520through%2520Physics-Aware%2520Neural%2520Architectures%26entry.906535625%3DWelf%2520Rehberg%2520and%2520Mihir%2520Kulkarni%2520and%2520Philipp%2520Weiss%2520and%2520Kostas%2520Alexis%26entry.1292438233%3DEfficiently%2520training%2520control%2520policies%2520for%2520robots%2520is%2520a%2520major%2520challenge%2520that%2520can%2520greatly%2520benefit%2520from%2520utilizing%2520knowledge%2520gained%2520from%2520training%2520similar%2520systems%2520through%2520cross-embodiment%2520knowledge%2520transfer.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520accelerating%2520policy%2520training%2520using%2520a%2520library-based%2520initialization%2520scheme%2520that%2520enables%2520effective%2520knowledge%2520transfer%2520across%2520multirotor%2520configurations.%2520By%2520leveraging%2520a%2520physics-aware%2520neural%2520control%2520architecture%2520that%2520combines%2520a%2520reinforcement%2520learning-based%2520controller%2520and%2520a%2520supervised%2520control%2520allocation%2520network%252C%2520we%2520enable%2520the%2520reuse%2520of%2520previously%2520trained%2520policies.%2520To%2520this%2520end%252C%2520we%2520utilize%2520a%2520policy%2520evaluation-based%2520similarity%2520measure%2520that%2520identifies%2520suitable%2520policies%2520for%2520initialization%2520from%2520a%2520library.%2520We%2520demonstrate%2520that%2520this%2520measure%2520correlates%2520with%2520the%2520reduction%2520in%2520environment%2520interactions%2520needed%2520to%2520reach%2520target%2520performance%2520and%2520is%2520therefore%2520suited%2520for%2520initialization.%2520Extensive%2520simulation%2520and%2520real-world%2520experiments%2520confirm%2520that%2520our%2520control%2520architecture%2520achieves%2520state-of-the-art%2520control%2520performance%252C%2520and%2520that%2520our%2520initialization%2520scheme%2520saves%2520on%2520average%2520up%2520to%2520%252473.5%255C%2525%2524%2520of%2520environment%2520interactions%2520%2528compared%2520to%2520training%2520a%2520policy%2520from%2520scratch%2529%2520across%2520diverse%2520quadrotor%2520and%2520hexarotor%2520designs%252C%2520paving%2520the%2520way%2520for%2520efficient%2520cross-embodiment%2520transfer%2520in%2520reinforcement%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Knowledge%20Transfer%20for%20Jump-Starting%20Control%20Policy%20Learning%20of%20Multirotors%20through%20Physics-Aware%20Neural%20Architectures&entry.906535625=Welf%20Rehberg%20and%20Mihir%20Kulkarni%20and%20Philipp%20Weiss%20and%20Kostas%20Alexis&entry.1292438233=Efficiently%20training%20control%20policies%20for%20robots%20is%20a%20major%20challenge%20that%20can%20greatly%20benefit%20from%20utilizing%20knowledge%20gained%20from%20training%20similar%20systems%20through%20cross-embodiment%20knowledge%20transfer.%20In%20this%20work%2C%20we%20focus%20on%20accelerating%20policy%20training%20using%20a%20library-based%20initialization%20scheme%20that%20enables%20effective%20knowledge%20transfer%20across%20multirotor%20configurations.%20By%20leveraging%20a%20physics-aware%20neural%20control%20architecture%20that%20combines%20a%20reinforcement%20learning-based%20controller%20and%20a%20supervised%20control%20allocation%20network%2C%20we%20enable%20the%20reuse%20of%20previously%20trained%20policies.%20To%20this%20end%2C%20we%20utilize%20a%20policy%20evaluation-based%20similarity%20measure%20that%20identifies%20suitable%20policies%20for%20initialization%20from%20a%20library.%20We%20demonstrate%20that%20this%20measure%20correlates%20with%20the%20reduction%20in%20environment%20interactions%20needed%20to%20reach%20target%20performance%20and%20is%20therefore%20suited%20for%20initialization.%20Extensive%20simulation%20and%20real-world%20experiments%20confirm%20that%20our%20control%20architecture%20achieves%20state-of-the-art%20control%20performance%2C%20and%20that%20our%20initialization%20scheme%20saves%20on%20average%20up%20to%20%2473.5%5C%25%24%20of%20environment%20interactions%20%28compared%20to%20training%20a%20policy%20from%20scratch%29%20across%20diverse%20quadrotor%20and%20hexarotor%20designs%2C%20paving%20the%20way%20for%20efficient%20cross-embodiment%20transfer%20in%20reinforcement%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2602.15533v1&entry.124074799=Read"},
{"title": "Latent Regularization in Generative Test Input Generation", "author": "Giorgi Merabishvili and Oliver Wei\u00dfl and Andrea Stocco", "abstract": "This study investigates the impact of regularization of latent spaces through truncation on the quality of generated test inputs for deep learning classifiers. We evaluate this effect using style-based GANs, a state-of-the-art generative approach, and assess quality along three dimensions: validity, diversity, and fault detection. We evaluate our approach on the boundary testing of deep learning image classifiers across three datasets, MNIST, Fashion MNIST, and CIFAR-10. We compare two truncation strategies: latent code mixing with binary search optimization and random latent truncation for generative exploration. Our experiments show that the latent code-mixing approach yields a higher fault detection rate than random truncation, while also improving both diversity and validity.", "link": "http://arxiv.org/abs/2602.15552v1", "date": "2026-02-17", "relevancy": 2.0758, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5285}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.514}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Regularization%20in%20Generative%20Test%20Input%20Generation&body=Title%3A%20Latent%20Regularization%20in%20Generative%20Test%20Input%20Generation%0AAuthor%3A%20Giorgi%20Merabishvili%20and%20Oliver%20Wei%C3%9Fl%20and%20Andrea%20Stocco%0AAbstract%3A%20This%20study%20investigates%20the%20impact%20of%20regularization%20of%20latent%20spaces%20through%20truncation%20on%20the%20quality%20of%20generated%20test%20inputs%20for%20deep%20learning%20classifiers.%20We%20evaluate%20this%20effect%20using%20style-based%20GANs%2C%20a%20state-of-the-art%20generative%20approach%2C%20and%20assess%20quality%20along%20three%20dimensions%3A%20validity%2C%20diversity%2C%20and%20fault%20detection.%20We%20evaluate%20our%20approach%20on%20the%20boundary%20testing%20of%20deep%20learning%20image%20classifiers%20across%20three%20datasets%2C%20MNIST%2C%20Fashion%20MNIST%2C%20and%20CIFAR-10.%20We%20compare%20two%20truncation%20strategies%3A%20latent%20code%20mixing%20with%20binary%20search%20optimization%20and%20random%20latent%20truncation%20for%20generative%20exploration.%20Our%20experiments%20show%20that%20the%20latent%20code-mixing%20approach%20yields%20a%20higher%20fault%20detection%20rate%20than%20random%20truncation%2C%20while%20also%20improving%20both%20diversity%20and%20validity.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Regularization%2520in%2520Generative%2520Test%2520Input%2520Generation%26entry.906535625%3DGiorgi%2520Merabishvili%2520and%2520Oliver%2520Wei%25C3%259Fl%2520and%2520Andrea%2520Stocco%26entry.1292438233%3DThis%2520study%2520investigates%2520the%2520impact%2520of%2520regularization%2520of%2520latent%2520spaces%2520through%2520truncation%2520on%2520the%2520quality%2520of%2520generated%2520test%2520inputs%2520for%2520deep%2520learning%2520classifiers.%2520We%2520evaluate%2520this%2520effect%2520using%2520style-based%2520GANs%252C%2520a%2520state-of-the-art%2520generative%2520approach%252C%2520and%2520assess%2520quality%2520along%2520three%2520dimensions%253A%2520validity%252C%2520diversity%252C%2520and%2520fault%2520detection.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%2520boundary%2520testing%2520of%2520deep%2520learning%2520image%2520classifiers%2520across%2520three%2520datasets%252C%2520MNIST%252C%2520Fashion%2520MNIST%252C%2520and%2520CIFAR-10.%2520We%2520compare%2520two%2520truncation%2520strategies%253A%2520latent%2520code%2520mixing%2520with%2520binary%2520search%2520optimization%2520and%2520random%2520latent%2520truncation%2520for%2520generative%2520exploration.%2520Our%2520experiments%2520show%2520that%2520the%2520latent%2520code-mixing%2520approach%2520yields%2520a%2520higher%2520fault%2520detection%2520rate%2520than%2520random%2520truncation%252C%2520while%2520also%2520improving%2520both%2520diversity%2520and%2520validity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Regularization%20in%20Generative%20Test%20Input%20Generation&entry.906535625=Giorgi%20Merabishvili%20and%20Oliver%20Wei%C3%9Fl%20and%20Andrea%20Stocco&entry.1292438233=This%20study%20investigates%20the%20impact%20of%20regularization%20of%20latent%20spaces%20through%20truncation%20on%20the%20quality%20of%20generated%20test%20inputs%20for%20deep%20learning%20classifiers.%20We%20evaluate%20this%20effect%20using%20style-based%20GANs%2C%20a%20state-of-the-art%20generative%20approach%2C%20and%20assess%20quality%20along%20three%20dimensions%3A%20validity%2C%20diversity%2C%20and%20fault%20detection.%20We%20evaluate%20our%20approach%20on%20the%20boundary%20testing%20of%20deep%20learning%20image%20classifiers%20across%20three%20datasets%2C%20MNIST%2C%20Fashion%20MNIST%2C%20and%20CIFAR-10.%20We%20compare%20two%20truncation%20strategies%3A%20latent%20code%20mixing%20with%20binary%20search%20optimization%20and%20random%20latent%20truncation%20for%20generative%20exploration.%20Our%20experiments%20show%20that%20the%20latent%20code-mixing%20approach%20yields%20a%20higher%20fault%20detection%20rate%20than%20random%20truncation%2C%20while%20also%20improving%20both%20diversity%20and%20validity.&entry.1838667208=http%3A//arxiv.org/abs/2602.15552v1&entry.124074799=Read"},
{"title": "Accelerated Predictive Coding Networks via Direct Kolen-Pollack Feedback Alignment", "author": "Davide Casnici and Martin Lefebvre and Justin Dauwels and Charlotte Frenkel", "abstract": "Predictive coding (PC) is a biologically inspired algorithm for training neural networks that relies only on local updates, allowing parallel learning across layers. However, practical implementations face two key limitations: error signals must still propagate from the output to early layers through multiple inference-phase steps, and feedback decays exponentially during this process, leading to vanishing updates in early layers. We propose direct Kolen-Pollack predictive coding (DKP-PC), which simultaneously addresses both feedback delay and exponential decay, yielding a more efficient and scalable variant of PC while preserving update locality. Leveraging direct feedback alignment and direct Kolen-Pollack algorithms, DKP-PC introduces learnable feedback connections from the output layer to all hidden layers, establishing a direct pathway for error transmission. This yields an algorithm that reduces the theoretical error propagation time complexity from O(L), with L being the network depth, to O(1), removing depth-dependent delay in error signals. Moreover, empirical results demonstrate that DKP-PC achieves performance at least comparable to, and often exceeding, that of standard PC, while offering improved latency and computational performance, supporting its potential for custom hardware-efficient implementations.", "link": "http://arxiv.org/abs/2602.15571v1", "date": "2026-02-17", "relevancy": 2.071, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.562}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4926}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%20Predictive%20Coding%20Networks%20via%20Direct%20Kolen-Pollack%20Feedback%20Alignment&body=Title%3A%20Accelerated%20Predictive%20Coding%20Networks%20via%20Direct%20Kolen-Pollack%20Feedback%20Alignment%0AAuthor%3A%20Davide%20Casnici%20and%20Martin%20Lefebvre%20and%20Justin%20Dauwels%20and%20Charlotte%20Frenkel%0AAbstract%3A%20Predictive%20coding%20%28PC%29%20is%20a%20biologically%20inspired%20algorithm%20for%20training%20neural%20networks%20that%20relies%20only%20on%20local%20updates%2C%20allowing%20parallel%20learning%20across%20layers.%20However%2C%20practical%20implementations%20face%20two%20key%20limitations%3A%20error%20signals%20must%20still%20propagate%20from%20the%20output%20to%20early%20layers%20through%20multiple%20inference-phase%20steps%2C%20and%20feedback%20decays%20exponentially%20during%20this%20process%2C%20leading%20to%20vanishing%20updates%20in%20early%20layers.%20We%20propose%20direct%20Kolen-Pollack%20predictive%20coding%20%28DKP-PC%29%2C%20which%20simultaneously%20addresses%20both%20feedback%20delay%20and%20exponential%20decay%2C%20yielding%20a%20more%20efficient%20and%20scalable%20variant%20of%20PC%20while%20preserving%20update%20locality.%20Leveraging%20direct%20feedback%20alignment%20and%20direct%20Kolen-Pollack%20algorithms%2C%20DKP-PC%20introduces%20learnable%20feedback%20connections%20from%20the%20output%20layer%20to%20all%20hidden%20layers%2C%20establishing%20a%20direct%20pathway%20for%20error%20transmission.%20This%20yields%20an%20algorithm%20that%20reduces%20the%20theoretical%20error%20propagation%20time%20complexity%20from%20O%28L%29%2C%20with%20L%20being%20the%20network%20depth%2C%20to%20O%281%29%2C%20removing%20depth-dependent%20delay%20in%20error%20signals.%20Moreover%2C%20empirical%20results%20demonstrate%20that%20DKP-PC%20achieves%20performance%20at%20least%20comparable%20to%2C%20and%20often%20exceeding%2C%20that%20of%20standard%20PC%2C%20while%20offering%20improved%20latency%20and%20computational%20performance%2C%20supporting%20its%20potential%20for%20custom%20hardware-efficient%20implementations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%2520Predictive%2520Coding%2520Networks%2520via%2520Direct%2520Kolen-Pollack%2520Feedback%2520Alignment%26entry.906535625%3DDavide%2520Casnici%2520and%2520Martin%2520Lefebvre%2520and%2520Justin%2520Dauwels%2520and%2520Charlotte%2520Frenkel%26entry.1292438233%3DPredictive%2520coding%2520%2528PC%2529%2520is%2520a%2520biologically%2520inspired%2520algorithm%2520for%2520training%2520neural%2520networks%2520that%2520relies%2520only%2520on%2520local%2520updates%252C%2520allowing%2520parallel%2520learning%2520across%2520layers.%2520However%252C%2520practical%2520implementations%2520face%2520two%2520key%2520limitations%253A%2520error%2520signals%2520must%2520still%2520propagate%2520from%2520the%2520output%2520to%2520early%2520layers%2520through%2520multiple%2520inference-phase%2520steps%252C%2520and%2520feedback%2520decays%2520exponentially%2520during%2520this%2520process%252C%2520leading%2520to%2520vanishing%2520updates%2520in%2520early%2520layers.%2520We%2520propose%2520direct%2520Kolen-Pollack%2520predictive%2520coding%2520%2528DKP-PC%2529%252C%2520which%2520simultaneously%2520addresses%2520both%2520feedback%2520delay%2520and%2520exponential%2520decay%252C%2520yielding%2520a%2520more%2520efficient%2520and%2520scalable%2520variant%2520of%2520PC%2520while%2520preserving%2520update%2520locality.%2520Leveraging%2520direct%2520feedback%2520alignment%2520and%2520direct%2520Kolen-Pollack%2520algorithms%252C%2520DKP-PC%2520introduces%2520learnable%2520feedback%2520connections%2520from%2520the%2520output%2520layer%2520to%2520all%2520hidden%2520layers%252C%2520establishing%2520a%2520direct%2520pathway%2520for%2520error%2520transmission.%2520This%2520yields%2520an%2520algorithm%2520that%2520reduces%2520the%2520theoretical%2520error%2520propagation%2520time%2520complexity%2520from%2520O%2528L%2529%252C%2520with%2520L%2520being%2520the%2520network%2520depth%252C%2520to%2520O%25281%2529%252C%2520removing%2520depth-dependent%2520delay%2520in%2520error%2520signals.%2520Moreover%252C%2520empirical%2520results%2520demonstrate%2520that%2520DKP-PC%2520achieves%2520performance%2520at%2520least%2520comparable%2520to%252C%2520and%2520often%2520exceeding%252C%2520that%2520of%2520standard%2520PC%252C%2520while%2520offering%2520improved%2520latency%2520and%2520computational%2520performance%252C%2520supporting%2520its%2520potential%2520for%2520custom%2520hardware-efficient%2520implementations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%20Predictive%20Coding%20Networks%20via%20Direct%20Kolen-Pollack%20Feedback%20Alignment&entry.906535625=Davide%20Casnici%20and%20Martin%20Lefebvre%20and%20Justin%20Dauwels%20and%20Charlotte%20Frenkel&entry.1292438233=Predictive%20coding%20%28PC%29%20is%20a%20biologically%20inspired%20algorithm%20for%20training%20neural%20networks%20that%20relies%20only%20on%20local%20updates%2C%20allowing%20parallel%20learning%20across%20layers.%20However%2C%20practical%20implementations%20face%20two%20key%20limitations%3A%20error%20signals%20must%20still%20propagate%20from%20the%20output%20to%20early%20layers%20through%20multiple%20inference-phase%20steps%2C%20and%20feedback%20decays%20exponentially%20during%20this%20process%2C%20leading%20to%20vanishing%20updates%20in%20early%20layers.%20We%20propose%20direct%20Kolen-Pollack%20predictive%20coding%20%28DKP-PC%29%2C%20which%20simultaneously%20addresses%20both%20feedback%20delay%20and%20exponential%20decay%2C%20yielding%20a%20more%20efficient%20and%20scalable%20variant%20of%20PC%20while%20preserving%20update%20locality.%20Leveraging%20direct%20feedback%20alignment%20and%20direct%20Kolen-Pollack%20algorithms%2C%20DKP-PC%20introduces%20learnable%20feedback%20connections%20from%20the%20output%20layer%20to%20all%20hidden%20layers%2C%20establishing%20a%20direct%20pathway%20for%20error%20transmission.%20This%20yields%20an%20algorithm%20that%20reduces%20the%20theoretical%20error%20propagation%20time%20complexity%20from%20O%28L%29%2C%20with%20L%20being%20the%20network%20depth%2C%20to%20O%281%29%2C%20removing%20depth-dependent%20delay%20in%20error%20signals.%20Moreover%2C%20empirical%20results%20demonstrate%20that%20DKP-PC%20achieves%20performance%20at%20least%20comparable%20to%2C%20and%20often%20exceeding%2C%20that%20of%20standard%20PC%2C%20while%20offering%20improved%20latency%20and%20computational%20performance%2C%20supporting%20its%20potential%20for%20custom%20hardware-efficient%20implementations.&entry.1838667208=http%3A//arxiv.org/abs/2602.15571v1&entry.124074799=Read"},
{"title": "Estimating Human Muscular Fatigue in Dynamic Collaborative Robotic Tasks with Learning-Based Models", "author": "Feras Kiki and Pouya P. Niaz and Alireza Madani and Cagatay Basdogan", "abstract": "Assessing human muscle fatigue is critical for optimizing performance and safety in physical human-robot interaction(pHRI). This work presents a data-driven framework to estimate fatigue in dynamic, cyclic pHRI using arm-mounted surface electromyography(sEMG). Subject-specific machine-learning regression models(Random Forest, XGBoost, and Linear Regression predict the fraction of cycles to fatigue(FCF) from three frequency-domain and one time-domain EMG features, and are benchmarked against a convolutional neural network(CNN) that ingests spectrograms of filtered EMG. Framing fatigue estimation as regression (rather than classification) captures continuous progression toward fatigue, supporting earlier detection, timely intervention, and adaptive robot control. In experiments with ten participants, a collaborative robot under admittance control guided repetitive lateral (left-right) end-effector motions until muscular fatigue. Average FCF RMSE across participants was 20.8+/-4.3% for the CNN, 23.3+/-3.8% for Random Forest, 24.8+/-4.5% for XGBoost, and 26.9+/-6.1% for Linear Regression. To probe cross-task generalization, one participant additionally performed unseen vertical (up-down) and circular repetitions; models trained only on lateral data were tested directly and largely retained accuracy, indicating robustness to changes in movement direction, arm kinematics, and muscle recruitment, while Linear Regression deteriorated. Overall, the study shows that both feature-based ML and spectrogram-based DL can estimate remaining work capacity during repetitive pHRI, with the CNN delivering the lowest error and the tree-based models close behind. The reported transfer to new motion patterns suggests potential for practical fatigue monitoring without retraining for every task, improving operator protection and enabling fatigue-aware shared autonomy, for safer fatigue-adaptive pHRI control.", "link": "http://arxiv.org/abs/2602.15684v1", "date": "2026-02-17", "relevancy": 2.0674, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5798}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5123}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Human%20Muscular%20Fatigue%20in%20Dynamic%20Collaborative%20Robotic%20Tasks%20with%20Learning-Based%20Models&body=Title%3A%20Estimating%20Human%20Muscular%20Fatigue%20in%20Dynamic%20Collaborative%20Robotic%20Tasks%20with%20Learning-Based%20Models%0AAuthor%3A%20Feras%20Kiki%20and%20Pouya%20P.%20Niaz%20and%20Alireza%20Madani%20and%20Cagatay%20Basdogan%0AAbstract%3A%20Assessing%20human%20muscle%20fatigue%20is%20critical%20for%20optimizing%20performance%20and%20safety%20in%20physical%20human-robot%20interaction%28pHRI%29.%20This%20work%20presents%20a%20data-driven%20framework%20to%20estimate%20fatigue%20in%20dynamic%2C%20cyclic%20pHRI%20using%20arm-mounted%20surface%20electromyography%28sEMG%29.%20Subject-specific%20machine-learning%20regression%20models%28Random%20Forest%2C%20XGBoost%2C%20and%20Linear%20Regression%20predict%20the%20fraction%20of%20cycles%20to%20fatigue%28FCF%29%20from%20three%20frequency-domain%20and%20one%20time-domain%20EMG%20features%2C%20and%20are%20benchmarked%20against%20a%20convolutional%20neural%20network%28CNN%29%20that%20ingests%20spectrograms%20of%20filtered%20EMG.%20Framing%20fatigue%20estimation%20as%20regression%20%28rather%20than%20classification%29%20captures%20continuous%20progression%20toward%20fatigue%2C%20supporting%20earlier%20detection%2C%20timely%20intervention%2C%20and%20adaptive%20robot%20control.%20In%20experiments%20with%20ten%20participants%2C%20a%20collaborative%20robot%20under%20admittance%20control%20guided%20repetitive%20lateral%20%28left-right%29%20end-effector%20motions%20until%20muscular%20fatigue.%20Average%20FCF%20RMSE%20across%20participants%20was%2020.8%2B/-4.3%25%20for%20the%20CNN%2C%2023.3%2B/-3.8%25%20for%20Random%20Forest%2C%2024.8%2B/-4.5%25%20for%20XGBoost%2C%20and%2026.9%2B/-6.1%25%20for%20Linear%20Regression.%20To%20probe%20cross-task%20generalization%2C%20one%20participant%20additionally%20performed%20unseen%20vertical%20%28up-down%29%20and%20circular%20repetitions%3B%20models%20trained%20only%20on%20lateral%20data%20were%20tested%20directly%20and%20largely%20retained%20accuracy%2C%20indicating%20robustness%20to%20changes%20in%20movement%20direction%2C%20arm%20kinematics%2C%20and%20muscle%20recruitment%2C%20while%20Linear%20Regression%20deteriorated.%20Overall%2C%20the%20study%20shows%20that%20both%20feature-based%20ML%20and%20spectrogram-based%20DL%20can%20estimate%20remaining%20work%20capacity%20during%20repetitive%20pHRI%2C%20with%20the%20CNN%20delivering%20the%20lowest%20error%20and%20the%20tree-based%20models%20close%20behind.%20The%20reported%20transfer%20to%20new%20motion%20patterns%20suggests%20potential%20for%20practical%20fatigue%20monitoring%20without%20retraining%20for%20every%20task%2C%20improving%20operator%20protection%20and%20enabling%20fatigue-aware%20shared%20autonomy%2C%20for%20safer%20fatigue-adaptive%20pHRI%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Human%2520Muscular%2520Fatigue%2520in%2520Dynamic%2520Collaborative%2520Robotic%2520Tasks%2520with%2520Learning-Based%2520Models%26entry.906535625%3DFeras%2520Kiki%2520and%2520Pouya%2520P.%2520Niaz%2520and%2520Alireza%2520Madani%2520and%2520Cagatay%2520Basdogan%26entry.1292438233%3DAssessing%2520human%2520muscle%2520fatigue%2520is%2520critical%2520for%2520optimizing%2520performance%2520and%2520safety%2520in%2520physical%2520human-robot%2520interaction%2528pHRI%2529.%2520This%2520work%2520presents%2520a%2520data-driven%2520framework%2520to%2520estimate%2520fatigue%2520in%2520dynamic%252C%2520cyclic%2520pHRI%2520using%2520arm-mounted%2520surface%2520electromyography%2528sEMG%2529.%2520Subject-specific%2520machine-learning%2520regression%2520models%2528Random%2520Forest%252C%2520XGBoost%252C%2520and%2520Linear%2520Regression%2520predict%2520the%2520fraction%2520of%2520cycles%2520to%2520fatigue%2528FCF%2529%2520from%2520three%2520frequency-domain%2520and%2520one%2520time-domain%2520EMG%2520features%252C%2520and%2520are%2520benchmarked%2520against%2520a%2520convolutional%2520neural%2520network%2528CNN%2529%2520that%2520ingests%2520spectrograms%2520of%2520filtered%2520EMG.%2520Framing%2520fatigue%2520estimation%2520as%2520regression%2520%2528rather%2520than%2520classification%2529%2520captures%2520continuous%2520progression%2520toward%2520fatigue%252C%2520supporting%2520earlier%2520detection%252C%2520timely%2520intervention%252C%2520and%2520adaptive%2520robot%2520control.%2520In%2520experiments%2520with%2520ten%2520participants%252C%2520a%2520collaborative%2520robot%2520under%2520admittance%2520control%2520guided%2520repetitive%2520lateral%2520%2528left-right%2529%2520end-effector%2520motions%2520until%2520muscular%2520fatigue.%2520Average%2520FCF%2520RMSE%2520across%2520participants%2520was%252020.8%252B/-4.3%2525%2520for%2520the%2520CNN%252C%252023.3%252B/-3.8%2525%2520for%2520Random%2520Forest%252C%252024.8%252B/-4.5%2525%2520for%2520XGBoost%252C%2520and%252026.9%252B/-6.1%2525%2520for%2520Linear%2520Regression.%2520To%2520probe%2520cross-task%2520generalization%252C%2520one%2520participant%2520additionally%2520performed%2520unseen%2520vertical%2520%2528up-down%2529%2520and%2520circular%2520repetitions%253B%2520models%2520trained%2520only%2520on%2520lateral%2520data%2520were%2520tested%2520directly%2520and%2520largely%2520retained%2520accuracy%252C%2520indicating%2520robustness%2520to%2520changes%2520in%2520movement%2520direction%252C%2520arm%2520kinematics%252C%2520and%2520muscle%2520recruitment%252C%2520while%2520Linear%2520Regression%2520deteriorated.%2520Overall%252C%2520the%2520study%2520shows%2520that%2520both%2520feature-based%2520ML%2520and%2520spectrogram-based%2520DL%2520can%2520estimate%2520remaining%2520work%2520capacity%2520during%2520repetitive%2520pHRI%252C%2520with%2520the%2520CNN%2520delivering%2520the%2520lowest%2520error%2520and%2520the%2520tree-based%2520models%2520close%2520behind.%2520The%2520reported%2520transfer%2520to%2520new%2520motion%2520patterns%2520suggests%2520potential%2520for%2520practical%2520fatigue%2520monitoring%2520without%2520retraining%2520for%2520every%2520task%252C%2520improving%2520operator%2520protection%2520and%2520enabling%2520fatigue-aware%2520shared%2520autonomy%252C%2520for%2520safer%2520fatigue-adaptive%2520pHRI%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Human%20Muscular%20Fatigue%20in%20Dynamic%20Collaborative%20Robotic%20Tasks%20with%20Learning-Based%20Models&entry.906535625=Feras%20Kiki%20and%20Pouya%20P.%20Niaz%20and%20Alireza%20Madani%20and%20Cagatay%20Basdogan&entry.1292438233=Assessing%20human%20muscle%20fatigue%20is%20critical%20for%20optimizing%20performance%20and%20safety%20in%20physical%20human-robot%20interaction%28pHRI%29.%20This%20work%20presents%20a%20data-driven%20framework%20to%20estimate%20fatigue%20in%20dynamic%2C%20cyclic%20pHRI%20using%20arm-mounted%20surface%20electromyography%28sEMG%29.%20Subject-specific%20machine-learning%20regression%20models%28Random%20Forest%2C%20XGBoost%2C%20and%20Linear%20Regression%20predict%20the%20fraction%20of%20cycles%20to%20fatigue%28FCF%29%20from%20three%20frequency-domain%20and%20one%20time-domain%20EMG%20features%2C%20and%20are%20benchmarked%20against%20a%20convolutional%20neural%20network%28CNN%29%20that%20ingests%20spectrograms%20of%20filtered%20EMG.%20Framing%20fatigue%20estimation%20as%20regression%20%28rather%20than%20classification%29%20captures%20continuous%20progression%20toward%20fatigue%2C%20supporting%20earlier%20detection%2C%20timely%20intervention%2C%20and%20adaptive%20robot%20control.%20In%20experiments%20with%20ten%20participants%2C%20a%20collaborative%20robot%20under%20admittance%20control%20guided%20repetitive%20lateral%20%28left-right%29%20end-effector%20motions%20until%20muscular%20fatigue.%20Average%20FCF%20RMSE%20across%20participants%20was%2020.8%2B/-4.3%25%20for%20the%20CNN%2C%2023.3%2B/-3.8%25%20for%20Random%20Forest%2C%2024.8%2B/-4.5%25%20for%20XGBoost%2C%20and%2026.9%2B/-6.1%25%20for%20Linear%20Regression.%20To%20probe%20cross-task%20generalization%2C%20one%20participant%20additionally%20performed%20unseen%20vertical%20%28up-down%29%20and%20circular%20repetitions%3B%20models%20trained%20only%20on%20lateral%20data%20were%20tested%20directly%20and%20largely%20retained%20accuracy%2C%20indicating%20robustness%20to%20changes%20in%20movement%20direction%2C%20arm%20kinematics%2C%20and%20muscle%20recruitment%2C%20while%20Linear%20Regression%20deteriorated.%20Overall%2C%20the%20study%20shows%20that%20both%20feature-based%20ML%20and%20spectrogram-based%20DL%20can%20estimate%20remaining%20work%20capacity%20during%20repetitive%20pHRI%2C%20with%20the%20CNN%20delivering%20the%20lowest%20error%20and%20the%20tree-based%20models%20close%20behind.%20The%20reported%20transfer%20to%20new%20motion%20patterns%20suggests%20potential%20for%20practical%20fatigue%20monitoring%20without%20retraining%20for%20every%20task%2C%20improving%20operator%20protection%20and%20enabling%20fatigue-aware%20shared%20autonomy%2C%20for%20safer%20fatigue-adaptive%20pHRI%20control.&entry.1838667208=http%3A//arxiv.org/abs/2602.15684v1&entry.124074799=Read"},
{"title": "Curriculum Learning and Pseudo-Labeling Improve the Generalization of Multi-Label Arabic Dialect Identification Models", "author": "Ali Mekky and Mohamed El Zeftawy and Lara Hassan and Amr Keleg and Preslav Nakov", "abstract": "Being modeled as a single-label classification task for a long time, recent work has argued that Arabic Dialect Identification (ADI) should be framed as a multi-label classification task. However, ADI remains constrained by the availability of single-label datasets, with no large-scale multi-label resources available for training. By analyzing models trained on single-label ADI data, we show that the main difficulty in repurposing such datasets for Multi-Label Arabic Dialect Identification (MLADI) lies in the selection of negative samples, as many sentences treated as negative could be acceptable in multiple dialects. To address these issues, we construct a multi-label dataset by generating automatic multi-label annotations using GPT-4o and binary dialect acceptability classifiers, with aggregation guided by the Arabic Level of Dialectness (ALDi). Afterward, we train a BERT-based multi-label classifier using curriculum learning strategies aligned with dialectal complexity and label cardinality. On the MLADI leaderboard, our best-performing LAHJATBERT model achieves a macro F1 of 0.69, compared to 0.55 for the strongest previously reported system. Code and data are available at https://mohamedalaa9.github.io/lahjatbert/.", "link": "http://arxiv.org/abs/2602.12937v2", "date": "2026-02-17", "relevancy": 2.0622, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5197}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5144}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curriculum%20Learning%20and%20Pseudo-Labeling%20Improve%20the%20Generalization%20of%20Multi-Label%20Arabic%20Dialect%20Identification%20Models&body=Title%3A%20Curriculum%20Learning%20and%20Pseudo-Labeling%20Improve%20the%20Generalization%20of%20Multi-Label%20Arabic%20Dialect%20Identification%20Models%0AAuthor%3A%20Ali%20Mekky%20and%20Mohamed%20El%20Zeftawy%20and%20Lara%20Hassan%20and%20Amr%20Keleg%20and%20Preslav%20Nakov%0AAbstract%3A%20Being%20modeled%20as%20a%20single-label%20classification%20task%20for%20a%20long%20time%2C%20recent%20work%20has%20argued%20that%20Arabic%20Dialect%20Identification%20%28ADI%29%20should%20be%20framed%20as%20a%20multi-label%20classification%20task.%20However%2C%20ADI%20remains%20constrained%20by%20the%20availability%20of%20single-label%20datasets%2C%20with%20no%20large-scale%20multi-label%20resources%20available%20for%20training.%20By%20analyzing%20models%20trained%20on%20single-label%20ADI%20data%2C%20we%20show%20that%20the%20main%20difficulty%20in%20repurposing%20such%20datasets%20for%20Multi-Label%20Arabic%20Dialect%20Identification%20%28MLADI%29%20lies%20in%20the%20selection%20of%20negative%20samples%2C%20as%20many%20sentences%20treated%20as%20negative%20could%20be%20acceptable%20in%20multiple%20dialects.%20To%20address%20these%20issues%2C%20we%20construct%20a%20multi-label%20dataset%20by%20generating%20automatic%20multi-label%20annotations%20using%20GPT-4o%20and%20binary%20dialect%20acceptability%20classifiers%2C%20with%20aggregation%20guided%20by%20the%20Arabic%20Level%20of%20Dialectness%20%28ALDi%29.%20Afterward%2C%20we%20train%20a%20BERT-based%20multi-label%20classifier%20using%20curriculum%20learning%20strategies%20aligned%20with%20dialectal%20complexity%20and%20label%20cardinality.%20On%20the%20MLADI%20leaderboard%2C%20our%20best-performing%20LAHJATBERT%20model%20achieves%20a%20macro%20F1%20of%200.69%2C%20compared%20to%200.55%20for%20the%20strongest%20previously%20reported%20system.%20Code%20and%20data%20are%20available%20at%20https%3A//mohamedalaa9.github.io/lahjatbert/.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurriculum%2520Learning%2520and%2520Pseudo-Labeling%2520Improve%2520the%2520Generalization%2520of%2520Multi-Label%2520Arabic%2520Dialect%2520Identification%2520Models%26entry.906535625%3DAli%2520Mekky%2520and%2520Mohamed%2520El%2520Zeftawy%2520and%2520Lara%2520Hassan%2520and%2520Amr%2520Keleg%2520and%2520Preslav%2520Nakov%26entry.1292438233%3DBeing%2520modeled%2520as%2520a%2520single-label%2520classification%2520task%2520for%2520a%2520long%2520time%252C%2520recent%2520work%2520has%2520argued%2520that%2520Arabic%2520Dialect%2520Identification%2520%2528ADI%2529%2520should%2520be%2520framed%2520as%2520a%2520multi-label%2520classification%2520task.%2520However%252C%2520ADI%2520remains%2520constrained%2520by%2520the%2520availability%2520of%2520single-label%2520datasets%252C%2520with%2520no%2520large-scale%2520multi-label%2520resources%2520available%2520for%2520training.%2520By%2520analyzing%2520models%2520trained%2520on%2520single-label%2520ADI%2520data%252C%2520we%2520show%2520that%2520the%2520main%2520difficulty%2520in%2520repurposing%2520such%2520datasets%2520for%2520Multi-Label%2520Arabic%2520Dialect%2520Identification%2520%2528MLADI%2529%2520lies%2520in%2520the%2520selection%2520of%2520negative%2520samples%252C%2520as%2520many%2520sentences%2520treated%2520as%2520negative%2520could%2520be%2520acceptable%2520in%2520multiple%2520dialects.%2520To%2520address%2520these%2520issues%252C%2520we%2520construct%2520a%2520multi-label%2520dataset%2520by%2520generating%2520automatic%2520multi-label%2520annotations%2520using%2520GPT-4o%2520and%2520binary%2520dialect%2520acceptability%2520classifiers%252C%2520with%2520aggregation%2520guided%2520by%2520the%2520Arabic%2520Level%2520of%2520Dialectness%2520%2528ALDi%2529.%2520Afterward%252C%2520we%2520train%2520a%2520BERT-based%2520multi-label%2520classifier%2520using%2520curriculum%2520learning%2520strategies%2520aligned%2520with%2520dialectal%2520complexity%2520and%2520label%2520cardinality.%2520On%2520the%2520MLADI%2520leaderboard%252C%2520our%2520best-performing%2520LAHJATBERT%2520model%2520achieves%2520a%2520macro%2520F1%2520of%25200.69%252C%2520compared%2520to%25200.55%2520for%2520the%2520strongest%2520previously%2520reported%2520system.%2520Code%2520and%2520data%2520are%2520available%2520at%2520https%253A//mohamedalaa9.github.io/lahjatbert/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curriculum%20Learning%20and%20Pseudo-Labeling%20Improve%20the%20Generalization%20of%20Multi-Label%20Arabic%20Dialect%20Identification%20Models&entry.906535625=Ali%20Mekky%20and%20Mohamed%20El%20Zeftawy%20and%20Lara%20Hassan%20and%20Amr%20Keleg%20and%20Preslav%20Nakov&entry.1292438233=Being%20modeled%20as%20a%20single-label%20classification%20task%20for%20a%20long%20time%2C%20recent%20work%20has%20argued%20that%20Arabic%20Dialect%20Identification%20%28ADI%29%20should%20be%20framed%20as%20a%20multi-label%20classification%20task.%20However%2C%20ADI%20remains%20constrained%20by%20the%20availability%20of%20single-label%20datasets%2C%20with%20no%20large-scale%20multi-label%20resources%20available%20for%20training.%20By%20analyzing%20models%20trained%20on%20single-label%20ADI%20data%2C%20we%20show%20that%20the%20main%20difficulty%20in%20repurposing%20such%20datasets%20for%20Multi-Label%20Arabic%20Dialect%20Identification%20%28MLADI%29%20lies%20in%20the%20selection%20of%20negative%20samples%2C%20as%20many%20sentences%20treated%20as%20negative%20could%20be%20acceptable%20in%20multiple%20dialects.%20To%20address%20these%20issues%2C%20we%20construct%20a%20multi-label%20dataset%20by%20generating%20automatic%20multi-label%20annotations%20using%20GPT-4o%20and%20binary%20dialect%20acceptability%20classifiers%2C%20with%20aggregation%20guided%20by%20the%20Arabic%20Level%20of%20Dialectness%20%28ALDi%29.%20Afterward%2C%20we%20train%20a%20BERT-based%20multi-label%20classifier%20using%20curriculum%20learning%20strategies%20aligned%20with%20dialectal%20complexity%20and%20label%20cardinality.%20On%20the%20MLADI%20leaderboard%2C%20our%20best-performing%20LAHJATBERT%20model%20achieves%20a%20macro%20F1%20of%200.69%2C%20compared%20to%200.55%20for%20the%20strongest%20previously%20reported%20system.%20Code%20and%20data%20are%20available%20at%20https%3A//mohamedalaa9.github.io/lahjatbert/.&entry.1838667208=http%3A//arxiv.org/abs/2602.12937v2&entry.124074799=Read"},
{"title": "ScholarGym: Benchmarking Large Language Model Capabilities in the Information-Gathering Stage of Deep Research", "author": "Hao Shen and Hang Yang and Zhouhong Gu and Weili Han", "abstract": "Large language models have advanced from single-turn question answering to deep research systems that iteratively decompose research questions, invoke retrieval tools, and synthesize information across multiple rounds. Evaluating such systems typically involves scoring their final research reports holistically, but this end-to-end paradigm tightly couples the language model's decision-making, workflow design, and environmental feedback, precluding decomposable analysis of individual components. We introduce ScholarGym, an evaluation environment that isolates the information-gathering stage of deep research on academic literature. Under a unified workflow, ScholarGym decomposes the research process into three explicit stages -- Query Planning, Tool Invocation, and Relevance Assessment -- and evaluates each against 2,536 expert-annotated queries over a static corpus of 570K papers with deterministic retrieval. Systematic experiments reveal that iterative query decomposition yields 2.9--3.3$\\times$ F1 gains over single-query retrieval, models with extended thinking trade recall for precision, and Query Planning quality together with Relevance Assessment constitute dual bottlenecks that separate proprietary from open-source model performance.", "link": "http://arxiv.org/abs/2601.21654v3", "date": "2026-02-17", "relevancy": 2.0536, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScholarGym%3A%20Benchmarking%20Large%20Language%20Model%20Capabilities%20in%20the%20Information-Gathering%20Stage%20of%20Deep%20Research&body=Title%3A%20ScholarGym%3A%20Benchmarking%20Large%20Language%20Model%20Capabilities%20in%20the%20Information-Gathering%20Stage%20of%20Deep%20Research%0AAuthor%3A%20Hao%20Shen%20and%20Hang%20Yang%20and%20Zhouhong%20Gu%20and%20Weili%20Han%0AAbstract%3A%20Large%20language%20models%20have%20advanced%20from%20single-turn%20question%20answering%20to%20deep%20research%20systems%20that%20iteratively%20decompose%20research%20questions%2C%20invoke%20retrieval%20tools%2C%20and%20synthesize%20information%20across%20multiple%20rounds.%20Evaluating%20such%20systems%20typically%20involves%20scoring%20their%20final%20research%20reports%20holistically%2C%20but%20this%20end-to-end%20paradigm%20tightly%20couples%20the%20language%20model%27s%20decision-making%2C%20workflow%20design%2C%20and%20environmental%20feedback%2C%20precluding%20decomposable%20analysis%20of%20individual%20components.%20We%20introduce%20ScholarGym%2C%20an%20evaluation%20environment%20that%20isolates%20the%20information-gathering%20stage%20of%20deep%20research%20on%20academic%20literature.%20Under%20a%20unified%20workflow%2C%20ScholarGym%20decomposes%20the%20research%20process%20into%20three%20explicit%20stages%20--%20Query%20Planning%2C%20Tool%20Invocation%2C%20and%20Relevance%20Assessment%20--%20and%20evaluates%20each%20against%202%2C536%20expert-annotated%20queries%20over%20a%20static%20corpus%20of%20570K%20papers%20with%20deterministic%20retrieval.%20Systematic%20experiments%20reveal%20that%20iterative%20query%20decomposition%20yields%202.9--3.3%24%5Ctimes%24%20F1%20gains%20over%20single-query%20retrieval%2C%20models%20with%20extended%20thinking%20trade%20recall%20for%20precision%2C%20and%20Query%20Planning%20quality%20together%20with%20Relevance%20Assessment%20constitute%20dual%20bottlenecks%20that%20separate%20proprietary%20from%20open-source%20model%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21654v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScholarGym%253A%2520Benchmarking%2520Large%2520Language%2520Model%2520Capabilities%2520in%2520the%2520Information-Gathering%2520Stage%2520of%2520Deep%2520Research%26entry.906535625%3DHao%2520Shen%2520and%2520Hang%2520Yang%2520and%2520Zhouhong%2520Gu%2520and%2520Weili%2520Han%26entry.1292438233%3DLarge%2520language%2520models%2520have%2520advanced%2520from%2520single-turn%2520question%2520answering%2520to%2520deep%2520research%2520systems%2520that%2520iteratively%2520decompose%2520research%2520questions%252C%2520invoke%2520retrieval%2520tools%252C%2520and%2520synthesize%2520information%2520across%2520multiple%2520rounds.%2520Evaluating%2520such%2520systems%2520typically%2520involves%2520scoring%2520their%2520final%2520research%2520reports%2520holistically%252C%2520but%2520this%2520end-to-end%2520paradigm%2520tightly%2520couples%2520the%2520language%2520model%2527s%2520decision-making%252C%2520workflow%2520design%252C%2520and%2520environmental%2520feedback%252C%2520precluding%2520decomposable%2520analysis%2520of%2520individual%2520components.%2520We%2520introduce%2520ScholarGym%252C%2520an%2520evaluation%2520environment%2520that%2520isolates%2520the%2520information-gathering%2520stage%2520of%2520deep%2520research%2520on%2520academic%2520literature.%2520Under%2520a%2520unified%2520workflow%252C%2520ScholarGym%2520decomposes%2520the%2520research%2520process%2520into%2520three%2520explicit%2520stages%2520--%2520Query%2520Planning%252C%2520Tool%2520Invocation%252C%2520and%2520Relevance%2520Assessment%2520--%2520and%2520evaluates%2520each%2520against%25202%252C536%2520expert-annotated%2520queries%2520over%2520a%2520static%2520corpus%2520of%2520570K%2520papers%2520with%2520deterministic%2520retrieval.%2520Systematic%2520experiments%2520reveal%2520that%2520iterative%2520query%2520decomposition%2520yields%25202.9--3.3%2524%255Ctimes%2524%2520F1%2520gains%2520over%2520single-query%2520retrieval%252C%2520models%2520with%2520extended%2520thinking%2520trade%2520recall%2520for%2520precision%252C%2520and%2520Query%2520Planning%2520quality%2520together%2520with%2520Relevance%2520Assessment%2520constitute%2520dual%2520bottlenecks%2520that%2520separate%2520proprietary%2520from%2520open-source%2520model%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21654v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScholarGym%3A%20Benchmarking%20Large%20Language%20Model%20Capabilities%20in%20the%20Information-Gathering%20Stage%20of%20Deep%20Research&entry.906535625=Hao%20Shen%20and%20Hang%20Yang%20and%20Zhouhong%20Gu%20and%20Weili%20Han&entry.1292438233=Large%20language%20models%20have%20advanced%20from%20single-turn%20question%20answering%20to%20deep%20research%20systems%20that%20iteratively%20decompose%20research%20questions%2C%20invoke%20retrieval%20tools%2C%20and%20synthesize%20information%20across%20multiple%20rounds.%20Evaluating%20such%20systems%20typically%20involves%20scoring%20their%20final%20research%20reports%20holistically%2C%20but%20this%20end-to-end%20paradigm%20tightly%20couples%20the%20language%20model%27s%20decision-making%2C%20workflow%20design%2C%20and%20environmental%20feedback%2C%20precluding%20decomposable%20analysis%20of%20individual%20components.%20We%20introduce%20ScholarGym%2C%20an%20evaluation%20environment%20that%20isolates%20the%20information-gathering%20stage%20of%20deep%20research%20on%20academic%20literature.%20Under%20a%20unified%20workflow%2C%20ScholarGym%20decomposes%20the%20research%20process%20into%20three%20explicit%20stages%20--%20Query%20Planning%2C%20Tool%20Invocation%2C%20and%20Relevance%20Assessment%20--%20and%20evaluates%20each%20against%202%2C536%20expert-annotated%20queries%20over%20a%20static%20corpus%20of%20570K%20papers%20with%20deterministic%20retrieval.%20Systematic%20experiments%20reveal%20that%20iterative%20query%20decomposition%20yields%202.9--3.3%24%5Ctimes%24%20F1%20gains%20over%20single-query%20retrieval%2C%20models%20with%20extended%20thinking%20trade%20recall%20for%20precision%2C%20and%20Query%20Planning%20quality%20together%20with%20Relevance%20Assessment%20constitute%20dual%20bottlenecks%20that%20separate%20proprietary%20from%20open-source%20model%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.21654v3&entry.124074799=Read"},
{"title": "GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway", "author": "Javier Irigoyen and Roberto Daza and Aythami Morales and Julian Fierrez and Francisco Jurado and Alvaro Ortigosa and Ruben Tolosana", "abstract": "This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.", "link": "http://arxiv.org/abs/2602.15531v1", "date": "2026-02-17", "relevancy": 2.0536, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5522}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5282}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenAI-LA%3A%20Generative%20AI%20and%20Learning%20Analytics%20Workshop%20%28LAK%202026%29%2C%20April%2027--May%201%2C%202026%2C%20Bergen%2C%20Norway&body=Title%3A%20GenAI-LA%3A%20Generative%20AI%20and%20Learning%20Analytics%20Workshop%20%28LAK%202026%29%2C%20April%2027--May%201%2C%202026%2C%20Bergen%2C%20Norway%0AAuthor%3A%20Javier%20Irigoyen%20and%20Roberto%20Daza%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Francisco%20Jurado%20and%20Alvaro%20Ortigosa%20and%20Ruben%20Tolosana%0AAbstract%3A%20This%20work%20introduces%20EduEVAL-DB%2C%20a%20dataset%20based%20on%20teacher%20roles%20designed%20to%20support%20the%20evaluation%20and%20training%20of%20automatic%20pedagogical%20evaluators%20and%20AI%20tutors%20for%20instructional%20explanations.%20The%20dataset%20comprises%20854%20explanations%20corresponding%20to%20139%20questions%20from%20a%20curated%20subset%20of%20the%20ScienceQA%20benchmark%2C%20spanning%20science%2C%20language%2C%20and%20social%20science%20across%20K-12%20grade%20levels.%20For%20each%20question%2C%20one%20human-teacher%20explanation%20is%20provided%20and%20six%20are%20generated%20by%20LLM-simulated%20teacher%20roles.%20These%20roles%20are%20inspired%20by%20instructional%20styles%20and%20shortcomings%20observed%20in%20real%20educational%20practice%20and%20are%20instantiated%20via%20prompt%20engineering.%20We%20further%20propose%20a%20pedagogical%20risk%20rubric%20aligned%20with%20established%20educational%20standards%2C%20operationalizing%20five%20complementary%20risk%20dimensions%3A%20factual%20correctness%2C%20explanatory%20depth%20and%20completeness%2C%20focus%20and%20relevance%2C%20student-level%20appropriateness%2C%20and%20ideological%20bias.%20All%20explanations%20are%20annotated%20with%20binary%20risk%20labels%20through%20a%20semi-automatic%20process%20with%20expert%20teacher%20review.%20Finally%2C%20we%20present%20preliminary%20validation%20experiments%20to%20assess%20the%20suitability%20of%20EduEVAL-DB%20for%20evaluation.%20We%20benchmark%20a%20state-of-the-art%20education-oriented%20model%20%28Gemini%202.5%20Pro%29%20against%20a%20lightweight%20local%20Llama%203.1%208B%20model%20and%20examine%20whether%20supervised%20fine-tuning%20on%20EduEVAL-DB%20supports%20pedagogical%20risk%20detection%20using%20models%20deployable%20on%20consumer%20hardware.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenAI-LA%253A%2520Generative%2520AI%2520and%2520Learning%2520Analytics%2520Workshop%2520%2528LAK%25202026%2529%252C%2520April%252027--May%25201%252C%25202026%252C%2520Bergen%252C%2520Norway%26entry.906535625%3DJavier%2520Irigoyen%2520and%2520Roberto%2520Daza%2520and%2520Aythami%2520Morales%2520and%2520Julian%2520Fierrez%2520and%2520Francisco%2520Jurado%2520and%2520Alvaro%2520Ortigosa%2520and%2520Ruben%2520Tolosana%26entry.1292438233%3DThis%2520work%2520introduces%2520EduEVAL-DB%252C%2520a%2520dataset%2520based%2520on%2520teacher%2520roles%2520designed%2520to%2520support%2520the%2520evaluation%2520and%2520training%2520of%2520automatic%2520pedagogical%2520evaluators%2520and%2520AI%2520tutors%2520for%2520instructional%2520explanations.%2520The%2520dataset%2520comprises%2520854%2520explanations%2520corresponding%2520to%2520139%2520questions%2520from%2520a%2520curated%2520subset%2520of%2520the%2520ScienceQA%2520benchmark%252C%2520spanning%2520science%252C%2520language%252C%2520and%2520social%2520science%2520across%2520K-12%2520grade%2520levels.%2520For%2520each%2520question%252C%2520one%2520human-teacher%2520explanation%2520is%2520provided%2520and%2520six%2520are%2520generated%2520by%2520LLM-simulated%2520teacher%2520roles.%2520These%2520roles%2520are%2520inspired%2520by%2520instructional%2520styles%2520and%2520shortcomings%2520observed%2520in%2520real%2520educational%2520practice%2520and%2520are%2520instantiated%2520via%2520prompt%2520engineering.%2520We%2520further%2520propose%2520a%2520pedagogical%2520risk%2520rubric%2520aligned%2520with%2520established%2520educational%2520standards%252C%2520operationalizing%2520five%2520complementary%2520risk%2520dimensions%253A%2520factual%2520correctness%252C%2520explanatory%2520depth%2520and%2520completeness%252C%2520focus%2520and%2520relevance%252C%2520student-level%2520appropriateness%252C%2520and%2520ideological%2520bias.%2520All%2520explanations%2520are%2520annotated%2520with%2520binary%2520risk%2520labels%2520through%2520a%2520semi-automatic%2520process%2520with%2520expert%2520teacher%2520review.%2520Finally%252C%2520we%2520present%2520preliminary%2520validation%2520experiments%2520to%2520assess%2520the%2520suitability%2520of%2520EduEVAL-DB%2520for%2520evaluation.%2520We%2520benchmark%2520a%2520state-of-the-art%2520education-oriented%2520model%2520%2528Gemini%25202.5%2520Pro%2529%2520against%2520a%2520lightweight%2520local%2520Llama%25203.1%25208B%2520model%2520and%2520examine%2520whether%2520supervised%2520fine-tuning%2520on%2520EduEVAL-DB%2520supports%2520pedagogical%2520risk%2520detection%2520using%2520models%2520deployable%2520on%2520consumer%2520hardware.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenAI-LA%3A%20Generative%20AI%20and%20Learning%20Analytics%20Workshop%20%28LAK%202026%29%2C%20April%2027--May%201%2C%202026%2C%20Bergen%2C%20Norway&entry.906535625=Javier%20Irigoyen%20and%20Roberto%20Daza%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Francisco%20Jurado%20and%20Alvaro%20Ortigosa%20and%20Ruben%20Tolosana&entry.1292438233=This%20work%20introduces%20EduEVAL-DB%2C%20a%20dataset%20based%20on%20teacher%20roles%20designed%20to%20support%20the%20evaluation%20and%20training%20of%20automatic%20pedagogical%20evaluators%20and%20AI%20tutors%20for%20instructional%20explanations.%20The%20dataset%20comprises%20854%20explanations%20corresponding%20to%20139%20questions%20from%20a%20curated%20subset%20of%20the%20ScienceQA%20benchmark%2C%20spanning%20science%2C%20language%2C%20and%20social%20science%20across%20K-12%20grade%20levels.%20For%20each%20question%2C%20one%20human-teacher%20explanation%20is%20provided%20and%20six%20are%20generated%20by%20LLM-simulated%20teacher%20roles.%20These%20roles%20are%20inspired%20by%20instructional%20styles%20and%20shortcomings%20observed%20in%20real%20educational%20practice%20and%20are%20instantiated%20via%20prompt%20engineering.%20We%20further%20propose%20a%20pedagogical%20risk%20rubric%20aligned%20with%20established%20educational%20standards%2C%20operationalizing%20five%20complementary%20risk%20dimensions%3A%20factual%20correctness%2C%20explanatory%20depth%20and%20completeness%2C%20focus%20and%20relevance%2C%20student-level%20appropriateness%2C%20and%20ideological%20bias.%20All%20explanations%20are%20annotated%20with%20binary%20risk%20labels%20through%20a%20semi-automatic%20process%20with%20expert%20teacher%20review.%20Finally%2C%20we%20present%20preliminary%20validation%20experiments%20to%20assess%20the%20suitability%20of%20EduEVAL-DB%20for%20evaluation.%20We%20benchmark%20a%20state-of-the-art%20education-oriented%20model%20%28Gemini%202.5%20Pro%29%20against%20a%20lightweight%20local%20Llama%203.1%208B%20model%20and%20examine%20whether%20supervised%20fine-tuning%20on%20EduEVAL-DB%20supports%20pedagogical%20risk%20detection%20using%20models%20deployable%20on%20consumer%20hardware.&entry.1838667208=http%3A//arxiv.org/abs/2602.15531v1&entry.124074799=Read"},
{"title": "VITAL: More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow", "author": "Ada Gorgun and Bernt Schiele and Jonas Fischer", "abstract": "Neural networks are widely adopted to solve complex and challenging tasks. Especially in high-stakes decision-making, understanding their reasoning process is crucial, yet proves challenging for modern deep networks. Feature visualization (FV) is a powerful tool to decode what information neurons are responding to and hence to better understand the reasoning behind such networks. In particular, in FV we generate human-understandable images that reflect the information detected by neurons of interest. However, current methods often yield unrecognizable visualizations, exhibiting repetitive patterns and visual artifacts that are hard to understand for a human. To address these problems, we propose to guide FV through statistics of real image features combined with measures of relevant network flow to generate prototypical images. Our approach yields human-understandable visualizations that both qualitatively and quantitatively improve over state-of-the-art FVs across various architectures. As such, it can be used to decode which information the network uses, complementing mechanistic circuits that identify where it is encoded. Code is available at: https://github.com/adagorgun/VITAL", "link": "http://arxiv.org/abs/2503.22399v2", "date": "2026-02-17", "relevancy": 2.0525, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5213}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5161}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VITAL%3A%20More%20Understandable%20Feature%20Visualization%20through%20Distribution%20Alignment%20and%20Relevant%20Information%20Flow&body=Title%3A%20VITAL%3A%20More%20Understandable%20Feature%20Visualization%20through%20Distribution%20Alignment%20and%20Relevant%20Information%20Flow%0AAuthor%3A%20Ada%20Gorgun%20and%20Bernt%20Schiele%20and%20Jonas%20Fischer%0AAbstract%3A%20Neural%20networks%20are%20widely%20adopted%20to%20solve%20complex%20and%20challenging%20tasks.%20Especially%20in%20high-stakes%20decision-making%2C%20understanding%20their%20reasoning%20process%20is%20crucial%2C%20yet%20proves%20challenging%20for%20modern%20deep%20networks.%20Feature%20visualization%20%28FV%29%20is%20a%20powerful%20tool%20to%20decode%20what%20information%20neurons%20are%20responding%20to%20and%20hence%20to%20better%20understand%20the%20reasoning%20behind%20such%20networks.%20In%20particular%2C%20in%20FV%20we%20generate%20human-understandable%20images%20that%20reflect%20the%20information%20detected%20by%20neurons%20of%20interest.%20However%2C%20current%20methods%20often%20yield%20unrecognizable%20visualizations%2C%20exhibiting%20repetitive%20patterns%20and%20visual%20artifacts%20that%20are%20hard%20to%20understand%20for%20a%20human.%20To%20address%20these%20problems%2C%20we%20propose%20to%20guide%20FV%20through%20statistics%20of%20real%20image%20features%20combined%20with%20measures%20of%20relevant%20network%20flow%20to%20generate%20prototypical%20images.%20Our%20approach%20yields%20human-understandable%20visualizations%20that%20both%20qualitatively%20and%20quantitatively%20improve%20over%20state-of-the-art%20FVs%20across%20various%20architectures.%20As%20such%2C%20it%20can%20be%20used%20to%20decode%20which%20information%20the%20network%20uses%2C%20complementing%20mechanistic%20circuits%20that%20identify%20where%20it%20is%20encoded.%20Code%20is%20available%20at%3A%20https%3A//github.com/adagorgun/VITAL%0ALink%3A%20http%3A//arxiv.org/abs/2503.22399v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVITAL%253A%2520More%2520Understandable%2520Feature%2520Visualization%2520through%2520Distribution%2520Alignment%2520and%2520Relevant%2520Information%2520Flow%26entry.906535625%3DAda%2520Gorgun%2520and%2520Bernt%2520Schiele%2520and%2520Jonas%2520Fischer%26entry.1292438233%3DNeural%2520networks%2520are%2520widely%2520adopted%2520to%2520solve%2520complex%2520and%2520challenging%2520tasks.%2520Especially%2520in%2520high-stakes%2520decision-making%252C%2520understanding%2520their%2520reasoning%2520process%2520is%2520crucial%252C%2520yet%2520proves%2520challenging%2520for%2520modern%2520deep%2520networks.%2520Feature%2520visualization%2520%2528FV%2529%2520is%2520a%2520powerful%2520tool%2520to%2520decode%2520what%2520information%2520neurons%2520are%2520responding%2520to%2520and%2520hence%2520to%2520better%2520understand%2520the%2520reasoning%2520behind%2520such%2520networks.%2520In%2520particular%252C%2520in%2520FV%2520we%2520generate%2520human-understandable%2520images%2520that%2520reflect%2520the%2520information%2520detected%2520by%2520neurons%2520of%2520interest.%2520However%252C%2520current%2520methods%2520often%2520yield%2520unrecognizable%2520visualizations%252C%2520exhibiting%2520repetitive%2520patterns%2520and%2520visual%2520artifacts%2520that%2520are%2520hard%2520to%2520understand%2520for%2520a%2520human.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520to%2520guide%2520FV%2520through%2520statistics%2520of%2520real%2520image%2520features%2520combined%2520with%2520measures%2520of%2520relevant%2520network%2520flow%2520to%2520generate%2520prototypical%2520images.%2520Our%2520approach%2520yields%2520human-understandable%2520visualizations%2520that%2520both%2520qualitatively%2520and%2520quantitatively%2520improve%2520over%2520state-of-the-art%2520FVs%2520across%2520various%2520architectures.%2520As%2520such%252C%2520it%2520can%2520be%2520used%2520to%2520decode%2520which%2520information%2520the%2520network%2520uses%252C%2520complementing%2520mechanistic%2520circuits%2520that%2520identify%2520where%2520it%2520is%2520encoded.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/adagorgun/VITAL%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22399v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VITAL%3A%20More%20Understandable%20Feature%20Visualization%20through%20Distribution%20Alignment%20and%20Relevant%20Information%20Flow&entry.906535625=Ada%20Gorgun%20and%20Bernt%20Schiele%20and%20Jonas%20Fischer&entry.1292438233=Neural%20networks%20are%20widely%20adopted%20to%20solve%20complex%20and%20challenging%20tasks.%20Especially%20in%20high-stakes%20decision-making%2C%20understanding%20their%20reasoning%20process%20is%20crucial%2C%20yet%20proves%20challenging%20for%20modern%20deep%20networks.%20Feature%20visualization%20%28FV%29%20is%20a%20powerful%20tool%20to%20decode%20what%20information%20neurons%20are%20responding%20to%20and%20hence%20to%20better%20understand%20the%20reasoning%20behind%20such%20networks.%20In%20particular%2C%20in%20FV%20we%20generate%20human-understandable%20images%20that%20reflect%20the%20information%20detected%20by%20neurons%20of%20interest.%20However%2C%20current%20methods%20often%20yield%20unrecognizable%20visualizations%2C%20exhibiting%20repetitive%20patterns%20and%20visual%20artifacts%20that%20are%20hard%20to%20understand%20for%20a%20human.%20To%20address%20these%20problems%2C%20we%20propose%20to%20guide%20FV%20through%20statistics%20of%20real%20image%20features%20combined%20with%20measures%20of%20relevant%20network%20flow%20to%20generate%20prototypical%20images.%20Our%20approach%20yields%20human-understandable%20visualizations%20that%20both%20qualitatively%20and%20quantitatively%20improve%20over%20state-of-the-art%20FVs%20across%20various%20architectures.%20As%20such%2C%20it%20can%20be%20used%20to%20decode%20which%20information%20the%20network%20uses%2C%20complementing%20mechanistic%20circuits%20that%20identify%20where%20it%20is%20encoded.%20Code%20is%20available%20at%3A%20https%3A//github.com/adagorgun/VITAL&entry.1838667208=http%3A//arxiv.org/abs/2503.22399v2&entry.124074799=Read"},
{"title": "Task-Agnostic Continual Learning for Chest Radiograph Classification", "author": "Muthu Subash Kavitha and Anas Zafar and Amgad Muneer and Jia Wu", "abstract": "Clinical deployment of chest radiograph classifiers requires models that can be updated as new datasets become available without retraining on previously ob- served data or degrading validated performance. We study, for the first time, a task-incremental continual learning setting for chest radiograph classification, in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers are unavailable at inference. We propose a continual adapter-based routing learning strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone and incrementally allocates lightweight task-specific adapters and classifier heads. A latent task selector operates on task-adapted features and leverages both current and historical context preserved through compact prototypes and feature-level experience replay. This design supports stable task identification and adaptation across sequential updates while avoiding raw-image storage. Experiments on large-scale public chest radiograph datasets demonstrate robust performance retention and reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms joint training under task-unknown deployment, achieving higher routing accuracy (75.0\\% vs.\\ 62.5\\%), while maintaining competitive diagnostic performance with AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under task-unknown inference, using significantly fewer trainable parameters. Finally, the proposed framework provides a practical alternative to joint training and repeated full retraining in continual clinical deployment.", "link": "http://arxiv.org/abs/2602.15811v1", "date": "2026-02-17", "relevancy": 2.0418, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5357}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4976}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Agnostic%20Continual%20Learning%20for%20Chest%20Radiograph%20Classification&body=Title%3A%20Task-Agnostic%20Continual%20Learning%20for%20Chest%20Radiograph%20Classification%0AAuthor%3A%20Muthu%20Subash%20Kavitha%20and%20Anas%20Zafar%20and%20Amgad%20Muneer%20and%20Jia%20Wu%0AAbstract%3A%20Clinical%20deployment%20of%20chest%20radiograph%20classifiers%20requires%20models%20that%20can%20be%20updated%20as%20new%20datasets%20become%20available%20without%20retraining%20on%20previously%20ob-%20served%20data%20or%20degrading%20validated%20performance.%20We%20study%2C%20for%20the%20first%20time%2C%20a%20task-incremental%20continual%20learning%20setting%20for%20chest%20radiograph%20classification%2C%20in%20which%20heterogeneous%20chest%20X-ray%20datasets%20arrive%20sequentially%20and%20task%20identifiers%20are%20unavailable%20at%20inference.%20We%20propose%20a%20continual%20adapter-based%20routing%20learning%20strategy%20for%20Chest%20X-rays%20%28CARL-XRay%29%20that%20maintains%20a%20fixed%20high-capacity%20backbone%20and%20incrementally%20allocates%20lightweight%20task-specific%20adapters%20and%20classifier%20heads.%20A%20latent%20task%20selector%20operates%20on%20task-adapted%20features%20and%20leverages%20both%20current%20and%20historical%20context%20preserved%20through%20compact%20prototypes%20and%20feature-level%20experience%20replay.%20This%20design%20supports%20stable%20task%20identification%20and%20adaptation%20across%20sequential%20updates%20while%20avoiding%20raw-image%20storage.%20Experiments%20on%20large-scale%20public%20chest%20radiograph%20datasets%20demonstrate%20robust%20performance%20retention%20and%20reliable%20task-aware%20inference%20under%20continual%20dataset%20ingestion.%20CARL-XRay%20outperforms%20joint%20training%20under%20task-unknown%20deployment%2C%20achieving%20higher%20routing%20accuracy%20%2875.0%5C%25%20vs.%5C%2062.5%5C%25%29%2C%20while%20maintaining%20competitive%20diagnostic%20performance%20with%20AUROC%20of%200.74%20in%20the%20oracle%20setting%20with%20ground-truth%20task%20identity%20and%200.75%20under%20task-unknown%20inference%2C%20using%20significantly%20fewer%20trainable%20parameters.%20Finally%2C%20the%20proposed%20framework%20provides%20a%20practical%20alternative%20to%20joint%20training%20and%20repeated%20full%20retraining%20in%20continual%20clinical%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Agnostic%2520Continual%2520Learning%2520for%2520Chest%2520Radiograph%2520Classification%26entry.906535625%3DMuthu%2520Subash%2520Kavitha%2520and%2520Anas%2520Zafar%2520and%2520Amgad%2520Muneer%2520and%2520Jia%2520Wu%26entry.1292438233%3DClinical%2520deployment%2520of%2520chest%2520radiograph%2520classifiers%2520requires%2520models%2520that%2520can%2520be%2520updated%2520as%2520new%2520datasets%2520become%2520available%2520without%2520retraining%2520on%2520previously%2520ob-%2520served%2520data%2520or%2520degrading%2520validated%2520performance.%2520We%2520study%252C%2520for%2520the%2520first%2520time%252C%2520a%2520task-incremental%2520continual%2520learning%2520setting%2520for%2520chest%2520radiograph%2520classification%252C%2520in%2520which%2520heterogeneous%2520chest%2520X-ray%2520datasets%2520arrive%2520sequentially%2520and%2520task%2520identifiers%2520are%2520unavailable%2520at%2520inference.%2520We%2520propose%2520a%2520continual%2520adapter-based%2520routing%2520learning%2520strategy%2520for%2520Chest%2520X-rays%2520%2528CARL-XRay%2529%2520that%2520maintains%2520a%2520fixed%2520high-capacity%2520backbone%2520and%2520incrementally%2520allocates%2520lightweight%2520task-specific%2520adapters%2520and%2520classifier%2520heads.%2520A%2520latent%2520task%2520selector%2520operates%2520on%2520task-adapted%2520features%2520and%2520leverages%2520both%2520current%2520and%2520historical%2520context%2520preserved%2520through%2520compact%2520prototypes%2520and%2520feature-level%2520experience%2520replay.%2520This%2520design%2520supports%2520stable%2520task%2520identification%2520and%2520adaptation%2520across%2520sequential%2520updates%2520while%2520avoiding%2520raw-image%2520storage.%2520Experiments%2520on%2520large-scale%2520public%2520chest%2520radiograph%2520datasets%2520demonstrate%2520robust%2520performance%2520retention%2520and%2520reliable%2520task-aware%2520inference%2520under%2520continual%2520dataset%2520ingestion.%2520CARL-XRay%2520outperforms%2520joint%2520training%2520under%2520task-unknown%2520deployment%252C%2520achieving%2520higher%2520routing%2520accuracy%2520%252875.0%255C%2525%2520vs.%255C%252062.5%255C%2525%2529%252C%2520while%2520maintaining%2520competitive%2520diagnostic%2520performance%2520with%2520AUROC%2520of%25200.74%2520in%2520the%2520oracle%2520setting%2520with%2520ground-truth%2520task%2520identity%2520and%25200.75%2520under%2520task-unknown%2520inference%252C%2520using%2520significantly%2520fewer%2520trainable%2520parameters.%2520Finally%252C%2520the%2520proposed%2520framework%2520provides%2520a%2520practical%2520alternative%2520to%2520joint%2520training%2520and%2520repeated%2520full%2520retraining%2520in%2520continual%2520clinical%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Agnostic%20Continual%20Learning%20for%20Chest%20Radiograph%20Classification&entry.906535625=Muthu%20Subash%20Kavitha%20and%20Anas%20Zafar%20and%20Amgad%20Muneer%20and%20Jia%20Wu&entry.1292438233=Clinical%20deployment%20of%20chest%20radiograph%20classifiers%20requires%20models%20that%20can%20be%20updated%20as%20new%20datasets%20become%20available%20without%20retraining%20on%20previously%20ob-%20served%20data%20or%20degrading%20validated%20performance.%20We%20study%2C%20for%20the%20first%20time%2C%20a%20task-incremental%20continual%20learning%20setting%20for%20chest%20radiograph%20classification%2C%20in%20which%20heterogeneous%20chest%20X-ray%20datasets%20arrive%20sequentially%20and%20task%20identifiers%20are%20unavailable%20at%20inference.%20We%20propose%20a%20continual%20adapter-based%20routing%20learning%20strategy%20for%20Chest%20X-rays%20%28CARL-XRay%29%20that%20maintains%20a%20fixed%20high-capacity%20backbone%20and%20incrementally%20allocates%20lightweight%20task-specific%20adapters%20and%20classifier%20heads.%20A%20latent%20task%20selector%20operates%20on%20task-adapted%20features%20and%20leverages%20both%20current%20and%20historical%20context%20preserved%20through%20compact%20prototypes%20and%20feature-level%20experience%20replay.%20This%20design%20supports%20stable%20task%20identification%20and%20adaptation%20across%20sequential%20updates%20while%20avoiding%20raw-image%20storage.%20Experiments%20on%20large-scale%20public%20chest%20radiograph%20datasets%20demonstrate%20robust%20performance%20retention%20and%20reliable%20task-aware%20inference%20under%20continual%20dataset%20ingestion.%20CARL-XRay%20outperforms%20joint%20training%20under%20task-unknown%20deployment%2C%20achieving%20higher%20routing%20accuracy%20%2875.0%5C%25%20vs.%5C%2062.5%5C%25%29%2C%20while%20maintaining%20competitive%20diagnostic%20performance%20with%20AUROC%20of%200.74%20in%20the%20oracle%20setting%20with%20ground-truth%20task%20identity%20and%200.75%20under%20task-unknown%20inference%2C%20using%20significantly%20fewer%20trainable%20parameters.%20Finally%2C%20the%20proposed%20framework%20provides%20a%20practical%20alternative%20to%20joint%20training%20and%20repeated%20full%20retraining%20in%20continual%20clinical%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2602.15811v1&entry.124074799=Read"},
{"title": "BEP: A Binary Error Propagation Algorithm for Binary Neural Networks Training", "author": "Luca Colombo and Fabrizio Pittorino and Daniele Zambon and Carlo Baldassi and Manuel Roveri and Cesare Alippi", "abstract": "Binary Neural Networks (BNNs), which constrain both weights and activations to binary values, offer substantial reductions in computational complexity, memory footprint, and energy consumption. These advantages make them particularly well suited for deployment on resource-constrained devices. However, training BNNs via gradient-based optimization remains challenging due to the discrete nature of their variables. The dominant approach, quantization-aware training, circumvents this issue by employing surrogate gradients. Yet, this method requires maintaining latent full-precision parameters and performing the backward pass with floating-point arithmetic, thereby forfeiting the efficiency of binary operations during training. While alternative approaches based on local learning rules exist, they are unsuitable for global credit assignment and for back-propagating errors in multi-layer architectures. This paper introduces Binary Error Propagation (BEP), the first learning algorithm to establish a principled, discrete analog of the backpropagation chain rule. This mechanism enables error signals, represented as binary vectors, to be propagated backward through multiple layers of a neural network. BEP operates entirely on binary variables, with all forward and backward computations performed using only bitwise operations. Crucially, this makes BEP the first solution to enable end-to-end binary training for recurrent neural network architectures. We validate the effectiveness of BEP on both multi-layer perceptrons and recurrent neural networks, demonstrating gains of up to +6.89% and +10.57% in test accuracy, respectively. The proposed algorithm is released as an open-source repository.", "link": "http://arxiv.org/abs/2512.04189v2", "date": "2026-02-17", "relevancy": 2.0333, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5771}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4675}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEP%3A%20A%20Binary%20Error%20Propagation%20Algorithm%20for%20Binary%20Neural%20Networks%20Training&body=Title%3A%20BEP%3A%20A%20Binary%20Error%20Propagation%20Algorithm%20for%20Binary%20Neural%20Networks%20Training%0AAuthor%3A%20Luca%20Colombo%20and%20Fabrizio%20Pittorino%20and%20Daniele%20Zambon%20and%20Carlo%20Baldassi%20and%20Manuel%20Roveri%20and%20Cesare%20Alippi%0AAbstract%3A%20Binary%20Neural%20Networks%20%28BNNs%29%2C%20which%20constrain%20both%20weights%20and%20activations%20to%20binary%20values%2C%20offer%20substantial%20reductions%20in%20computational%20complexity%2C%20memory%20footprint%2C%20and%20energy%20consumption.%20These%20advantages%20make%20them%20particularly%20well%20suited%20for%20deployment%20on%20resource-constrained%20devices.%20However%2C%20training%20BNNs%20via%20gradient-based%20optimization%20remains%20challenging%20due%20to%20the%20discrete%20nature%20of%20their%20variables.%20The%20dominant%20approach%2C%20quantization-aware%20training%2C%20circumvents%20this%20issue%20by%20employing%20surrogate%20gradients.%20Yet%2C%20this%20method%20requires%20maintaining%20latent%20full-precision%20parameters%20and%20performing%20the%20backward%20pass%20with%20floating-point%20arithmetic%2C%20thereby%20forfeiting%20the%20efficiency%20of%20binary%20operations%20during%20training.%20While%20alternative%20approaches%20based%20on%20local%20learning%20rules%20exist%2C%20they%20are%20unsuitable%20for%20global%20credit%20assignment%20and%20for%20back-propagating%20errors%20in%20multi-layer%20architectures.%20This%20paper%20introduces%20Binary%20Error%20Propagation%20%28BEP%29%2C%20the%20first%20learning%20algorithm%20to%20establish%20a%20principled%2C%20discrete%20analog%20of%20the%20backpropagation%20chain%20rule.%20This%20mechanism%20enables%20error%20signals%2C%20represented%20as%20binary%20vectors%2C%20to%20be%20propagated%20backward%20through%20multiple%20layers%20of%20a%20neural%20network.%20BEP%20operates%20entirely%20on%20binary%20variables%2C%20with%20all%20forward%20and%20backward%20computations%20performed%20using%20only%20bitwise%20operations.%20Crucially%2C%20this%20makes%20BEP%20the%20first%20solution%20to%20enable%20end-to-end%20binary%20training%20for%20recurrent%20neural%20network%20architectures.%20We%20validate%20the%20effectiveness%20of%20BEP%20on%20both%20multi-layer%20perceptrons%20and%20recurrent%20neural%20networks%2C%20demonstrating%20gains%20of%20up%20to%20%2B6.89%25%20and%20%2B10.57%25%20in%20test%20accuracy%2C%20respectively.%20The%20proposed%20algorithm%20is%20released%20as%20an%20open-source%20repository.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEP%253A%2520A%2520Binary%2520Error%2520Propagation%2520Algorithm%2520for%2520Binary%2520Neural%2520Networks%2520Training%26entry.906535625%3DLuca%2520Colombo%2520and%2520Fabrizio%2520Pittorino%2520and%2520Daniele%2520Zambon%2520and%2520Carlo%2520Baldassi%2520and%2520Manuel%2520Roveri%2520and%2520Cesare%2520Alippi%26entry.1292438233%3DBinary%2520Neural%2520Networks%2520%2528BNNs%2529%252C%2520which%2520constrain%2520both%2520weights%2520and%2520activations%2520to%2520binary%2520values%252C%2520offer%2520substantial%2520reductions%2520in%2520computational%2520complexity%252C%2520memory%2520footprint%252C%2520and%2520energy%2520consumption.%2520These%2520advantages%2520make%2520them%2520particularly%2520well%2520suited%2520for%2520deployment%2520on%2520resource-constrained%2520devices.%2520However%252C%2520training%2520BNNs%2520via%2520gradient-based%2520optimization%2520remains%2520challenging%2520due%2520to%2520the%2520discrete%2520nature%2520of%2520their%2520variables.%2520The%2520dominant%2520approach%252C%2520quantization-aware%2520training%252C%2520circumvents%2520this%2520issue%2520by%2520employing%2520surrogate%2520gradients.%2520Yet%252C%2520this%2520method%2520requires%2520maintaining%2520latent%2520full-precision%2520parameters%2520and%2520performing%2520the%2520backward%2520pass%2520with%2520floating-point%2520arithmetic%252C%2520thereby%2520forfeiting%2520the%2520efficiency%2520of%2520binary%2520operations%2520during%2520training.%2520While%2520alternative%2520approaches%2520based%2520on%2520local%2520learning%2520rules%2520exist%252C%2520they%2520are%2520unsuitable%2520for%2520global%2520credit%2520assignment%2520and%2520for%2520back-propagating%2520errors%2520in%2520multi-layer%2520architectures.%2520This%2520paper%2520introduces%2520Binary%2520Error%2520Propagation%2520%2528BEP%2529%252C%2520the%2520first%2520learning%2520algorithm%2520to%2520establish%2520a%2520principled%252C%2520discrete%2520analog%2520of%2520the%2520backpropagation%2520chain%2520rule.%2520This%2520mechanism%2520enables%2520error%2520signals%252C%2520represented%2520as%2520binary%2520vectors%252C%2520to%2520be%2520propagated%2520backward%2520through%2520multiple%2520layers%2520of%2520a%2520neural%2520network.%2520BEP%2520operates%2520entirely%2520on%2520binary%2520variables%252C%2520with%2520all%2520forward%2520and%2520backward%2520computations%2520performed%2520using%2520only%2520bitwise%2520operations.%2520Crucially%252C%2520this%2520makes%2520BEP%2520the%2520first%2520solution%2520to%2520enable%2520end-to-end%2520binary%2520training%2520for%2520recurrent%2520neural%2520network%2520architectures.%2520We%2520validate%2520the%2520effectiveness%2520of%2520BEP%2520on%2520both%2520multi-layer%2520perceptrons%2520and%2520recurrent%2520neural%2520networks%252C%2520demonstrating%2520gains%2520of%2520up%2520to%2520%252B6.89%2525%2520and%2520%252B10.57%2525%2520in%2520test%2520accuracy%252C%2520respectively.%2520The%2520proposed%2520algorithm%2520is%2520released%2520as%2520an%2520open-source%2520repository.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEP%3A%20A%20Binary%20Error%20Propagation%20Algorithm%20for%20Binary%20Neural%20Networks%20Training&entry.906535625=Luca%20Colombo%20and%20Fabrizio%20Pittorino%20and%20Daniele%20Zambon%20and%20Carlo%20Baldassi%20and%20Manuel%20Roveri%20and%20Cesare%20Alippi&entry.1292438233=Binary%20Neural%20Networks%20%28BNNs%29%2C%20which%20constrain%20both%20weights%20and%20activations%20to%20binary%20values%2C%20offer%20substantial%20reductions%20in%20computational%20complexity%2C%20memory%20footprint%2C%20and%20energy%20consumption.%20These%20advantages%20make%20them%20particularly%20well%20suited%20for%20deployment%20on%20resource-constrained%20devices.%20However%2C%20training%20BNNs%20via%20gradient-based%20optimization%20remains%20challenging%20due%20to%20the%20discrete%20nature%20of%20their%20variables.%20The%20dominant%20approach%2C%20quantization-aware%20training%2C%20circumvents%20this%20issue%20by%20employing%20surrogate%20gradients.%20Yet%2C%20this%20method%20requires%20maintaining%20latent%20full-precision%20parameters%20and%20performing%20the%20backward%20pass%20with%20floating-point%20arithmetic%2C%20thereby%20forfeiting%20the%20efficiency%20of%20binary%20operations%20during%20training.%20While%20alternative%20approaches%20based%20on%20local%20learning%20rules%20exist%2C%20they%20are%20unsuitable%20for%20global%20credit%20assignment%20and%20for%20back-propagating%20errors%20in%20multi-layer%20architectures.%20This%20paper%20introduces%20Binary%20Error%20Propagation%20%28BEP%29%2C%20the%20first%20learning%20algorithm%20to%20establish%20a%20principled%2C%20discrete%20analog%20of%20the%20backpropagation%20chain%20rule.%20This%20mechanism%20enables%20error%20signals%2C%20represented%20as%20binary%20vectors%2C%20to%20be%20propagated%20backward%20through%20multiple%20layers%20of%20a%20neural%20network.%20BEP%20operates%20entirely%20on%20binary%20variables%2C%20with%20all%20forward%20and%20backward%20computations%20performed%20using%20only%20bitwise%20operations.%20Crucially%2C%20this%20makes%20BEP%20the%20first%20solution%20to%20enable%20end-to-end%20binary%20training%20for%20recurrent%20neural%20network%20architectures.%20We%20validate%20the%20effectiveness%20of%20BEP%20on%20both%20multi-layer%20perceptrons%20and%20recurrent%20neural%20networks%2C%20demonstrating%20gains%20of%20up%20to%20%2B6.89%25%20and%20%2B10.57%25%20in%20test%20accuracy%2C%20respectively.%20The%20proposed%20algorithm%20is%20released%20as%20an%20open-source%20repository.&entry.1838667208=http%3A//arxiv.org/abs/2512.04189v2&entry.124074799=Read"},
{"title": "UniTAF: A Modular Framework for Joint Text-to-Speech and Audio-to-Face Modeling", "author": "Qiangong Zhou and Nagasaka Tomohiro", "abstract": "This work considers merging two independent models, TTS and A2F, into a unified model to enable internal feature transfer, thereby improving the consistency between audio and facial expressions generated from text. We also discuss the extension of the emotion control mechanism from TTS to the joint model. This work does not aim to showcase generation quality; instead, from a system design perspective, it validates the feasibility of reusing intermediate representations from TTS for joint modeling of speech and facial expressions, and provides engineering practice references for subsequent speech expression co-design. The project code has been open source at: https://github.com/GoldenFishes/UniTAF", "link": "http://arxiv.org/abs/2602.15651v1", "date": "2026-02-17", "relevancy": 2.0327, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5153}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5135}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniTAF%3A%20A%20Modular%20Framework%20for%20Joint%20Text-to-Speech%20and%20Audio-to-Face%20Modeling&body=Title%3A%20UniTAF%3A%20A%20Modular%20Framework%20for%20Joint%20Text-to-Speech%20and%20Audio-to-Face%20Modeling%0AAuthor%3A%20Qiangong%20Zhou%20and%20Nagasaka%20Tomohiro%0AAbstract%3A%20This%20work%20considers%20merging%20two%20independent%20models%2C%20TTS%20and%20A2F%2C%20into%20a%20unified%20model%20to%20enable%20internal%20feature%20transfer%2C%20thereby%20improving%20the%20consistency%20between%20audio%20and%20facial%20expressions%20generated%20from%20text.%20We%20also%20discuss%20the%20extension%20of%20the%20emotion%20control%20mechanism%20from%20TTS%20to%20the%20joint%20model.%20This%20work%20does%20not%20aim%20to%20showcase%20generation%20quality%3B%20instead%2C%20from%20a%20system%20design%20perspective%2C%20it%20validates%20the%20feasibility%20of%20reusing%20intermediate%20representations%20from%20TTS%20for%20joint%20modeling%20of%20speech%20and%20facial%20expressions%2C%20and%20provides%20engineering%20practice%20references%20for%20subsequent%20speech%20expression%20co-design.%20The%20project%20code%20has%20been%20open%20source%20at%3A%20https%3A//github.com/GoldenFishes/UniTAF%0ALink%3A%20http%3A//arxiv.org/abs/2602.15651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniTAF%253A%2520A%2520Modular%2520Framework%2520for%2520Joint%2520Text-to-Speech%2520and%2520Audio-to-Face%2520Modeling%26entry.906535625%3DQiangong%2520Zhou%2520and%2520Nagasaka%2520Tomohiro%26entry.1292438233%3DThis%2520work%2520considers%2520merging%2520two%2520independent%2520models%252C%2520TTS%2520and%2520A2F%252C%2520into%2520a%2520unified%2520model%2520to%2520enable%2520internal%2520feature%2520transfer%252C%2520thereby%2520improving%2520the%2520consistency%2520between%2520audio%2520and%2520facial%2520expressions%2520generated%2520from%2520text.%2520We%2520also%2520discuss%2520the%2520extension%2520of%2520the%2520emotion%2520control%2520mechanism%2520from%2520TTS%2520to%2520the%2520joint%2520model.%2520This%2520work%2520does%2520not%2520aim%2520to%2520showcase%2520generation%2520quality%253B%2520instead%252C%2520from%2520a%2520system%2520design%2520perspective%252C%2520it%2520validates%2520the%2520feasibility%2520of%2520reusing%2520intermediate%2520representations%2520from%2520TTS%2520for%2520joint%2520modeling%2520of%2520speech%2520and%2520facial%2520expressions%252C%2520and%2520provides%2520engineering%2520practice%2520references%2520for%2520subsequent%2520speech%2520expression%2520co-design.%2520The%2520project%2520code%2520has%2520been%2520open%2520source%2520at%253A%2520https%253A//github.com/GoldenFishes/UniTAF%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTAF%3A%20A%20Modular%20Framework%20for%20Joint%20Text-to-Speech%20and%20Audio-to-Face%20Modeling&entry.906535625=Qiangong%20Zhou%20and%20Nagasaka%20Tomohiro&entry.1292438233=This%20work%20considers%20merging%20two%20independent%20models%2C%20TTS%20and%20A2F%2C%20into%20a%20unified%20model%20to%20enable%20internal%20feature%20transfer%2C%20thereby%20improving%20the%20consistency%20between%20audio%20and%20facial%20expressions%20generated%20from%20text.%20We%20also%20discuss%20the%20extension%20of%20the%20emotion%20control%20mechanism%20from%20TTS%20to%20the%20joint%20model.%20This%20work%20does%20not%20aim%20to%20showcase%20generation%20quality%3B%20instead%2C%20from%20a%20system%20design%20perspective%2C%20it%20validates%20the%20feasibility%20of%20reusing%20intermediate%20representations%20from%20TTS%20for%20joint%20modeling%20of%20speech%20and%20facial%20expressions%2C%20and%20provides%20engineering%20practice%20references%20for%20subsequent%20speech%20expression%20co-design.%20The%20project%20code%20has%20been%20open%20source%20at%3A%20https%3A//github.com/GoldenFishes/UniTAF&entry.1838667208=http%3A//arxiv.org/abs/2602.15651v1&entry.124074799=Read"},
{"title": "Beyond ReLU: Bifurcation, Oversmoothing, and Topological Priors", "author": "Erkan Turan and Gaspard Abel and Maysam Behmanesh and Emery Pierson and Maks Ovsjanikov", "abstract": "Graph Neural Networks (GNNs) learn node representations through iterative network-based message-passing. While powerful, deep GNNs suffer from oversmoothing, where node features converge to a homogeneous, non-informative state. We re-frame this problem of representational collapse from a \\emph{bifurcation theory} perspective, characterizing oversmoothing as convergence to a stable ``homogeneous fixed point.'' Our central contribution is the theoretical discovery that this undesired stability can be broken by replacing standard monotone activations (e.g., ReLU) with a class of functions. Using Lyapunov-Schmidt reduction, we analytically prove that this substitution induces a bifurcation that destabilizes the homogeneous state and creates a new pair of stable, non-homogeneous \\emph{patterns} that provably resist oversmoothing. Our theory predicts a precise, nontrivial scaling law for the amplitude of these emergent patterns, which we quantitatively validate in experiments. Finally, we demonstrate the practical utility of our theory by deriving a closed-form, bifurcation-aware initialization and showing its utility in real benchmark experiments.", "link": "http://arxiv.org/abs/2602.15634v1", "date": "2026-02-17", "relevancy": 2.0302, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5212}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5113}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20ReLU%3A%20Bifurcation%2C%20Oversmoothing%2C%20and%20Topological%20Priors&body=Title%3A%20Beyond%20ReLU%3A%20Bifurcation%2C%20Oversmoothing%2C%20and%20Topological%20Priors%0AAuthor%3A%20Erkan%20Turan%20and%20Gaspard%20Abel%20and%20Maysam%20Behmanesh%20and%20Emery%20Pierson%20and%20Maks%20Ovsjanikov%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20learn%20node%20representations%20through%20iterative%20network-based%20message-passing.%20While%20powerful%2C%20deep%20GNNs%20suffer%20from%20oversmoothing%2C%20where%20node%20features%20converge%20to%20a%20homogeneous%2C%20non-informative%20state.%20We%20re-frame%20this%20problem%20of%20representational%20collapse%20from%20a%20%5Cemph%7Bbifurcation%20theory%7D%20perspective%2C%20characterizing%20oversmoothing%20as%20convergence%20to%20a%20stable%20%60%60homogeneous%20fixed%20point.%27%27%20Our%20central%20contribution%20is%20the%20theoretical%20discovery%20that%20this%20undesired%20stability%20can%20be%20broken%20by%20replacing%20standard%20monotone%20activations%20%28e.g.%2C%20ReLU%29%20with%20a%20class%20of%20functions.%20Using%20Lyapunov-Schmidt%20reduction%2C%20we%20analytically%20prove%20that%20this%20substitution%20induces%20a%20bifurcation%20that%20destabilizes%20the%20homogeneous%20state%20and%20creates%20a%20new%20pair%20of%20stable%2C%20non-homogeneous%20%5Cemph%7Bpatterns%7D%20that%20provably%20resist%20oversmoothing.%20Our%20theory%20predicts%20a%20precise%2C%20nontrivial%20scaling%20law%20for%20the%20amplitude%20of%20these%20emergent%20patterns%2C%20which%20we%20quantitatively%20validate%20in%20experiments.%20Finally%2C%20we%20demonstrate%20the%20practical%20utility%20of%20our%20theory%20by%20deriving%20a%20closed-form%2C%20bifurcation-aware%20initialization%20and%20showing%20its%20utility%20in%20real%20benchmark%20experiments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520ReLU%253A%2520Bifurcation%252C%2520Oversmoothing%252C%2520and%2520Topological%2520Priors%26entry.906535625%3DErkan%2520Turan%2520and%2520Gaspard%2520Abel%2520and%2520Maysam%2520Behmanesh%2520and%2520Emery%2520Pierson%2520and%2520Maks%2520Ovsjanikov%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520learn%2520node%2520representations%2520through%2520iterative%2520network-based%2520message-passing.%2520While%2520powerful%252C%2520deep%2520GNNs%2520suffer%2520from%2520oversmoothing%252C%2520where%2520node%2520features%2520converge%2520to%2520a%2520homogeneous%252C%2520non-informative%2520state.%2520We%2520re-frame%2520this%2520problem%2520of%2520representational%2520collapse%2520from%2520a%2520%255Cemph%257Bbifurcation%2520theory%257D%2520perspective%252C%2520characterizing%2520oversmoothing%2520as%2520convergence%2520to%2520a%2520stable%2520%2560%2560homogeneous%2520fixed%2520point.%2527%2527%2520Our%2520central%2520contribution%2520is%2520the%2520theoretical%2520discovery%2520that%2520this%2520undesired%2520stability%2520can%2520be%2520broken%2520by%2520replacing%2520standard%2520monotone%2520activations%2520%2528e.g.%252C%2520ReLU%2529%2520with%2520a%2520class%2520of%2520functions.%2520Using%2520Lyapunov-Schmidt%2520reduction%252C%2520we%2520analytically%2520prove%2520that%2520this%2520substitution%2520induces%2520a%2520bifurcation%2520that%2520destabilizes%2520the%2520homogeneous%2520state%2520and%2520creates%2520a%2520new%2520pair%2520of%2520stable%252C%2520non-homogeneous%2520%255Cemph%257Bpatterns%257D%2520that%2520provably%2520resist%2520oversmoothing.%2520Our%2520theory%2520predicts%2520a%2520precise%252C%2520nontrivial%2520scaling%2520law%2520for%2520the%2520amplitude%2520of%2520these%2520emergent%2520patterns%252C%2520which%2520we%2520quantitatively%2520validate%2520in%2520experiments.%2520Finally%252C%2520we%2520demonstrate%2520the%2520practical%2520utility%2520of%2520our%2520theory%2520by%2520deriving%2520a%2520closed-form%252C%2520bifurcation-aware%2520initialization%2520and%2520showing%2520its%2520utility%2520in%2520real%2520benchmark%2520experiments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20ReLU%3A%20Bifurcation%2C%20Oversmoothing%2C%20and%20Topological%20Priors&entry.906535625=Erkan%20Turan%20and%20Gaspard%20Abel%20and%20Maysam%20Behmanesh%20and%20Emery%20Pierson%20and%20Maks%20Ovsjanikov&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20learn%20node%20representations%20through%20iterative%20network-based%20message-passing.%20While%20powerful%2C%20deep%20GNNs%20suffer%20from%20oversmoothing%2C%20where%20node%20features%20converge%20to%20a%20homogeneous%2C%20non-informative%20state.%20We%20re-frame%20this%20problem%20of%20representational%20collapse%20from%20a%20%5Cemph%7Bbifurcation%20theory%7D%20perspective%2C%20characterizing%20oversmoothing%20as%20convergence%20to%20a%20stable%20%60%60homogeneous%20fixed%20point.%27%27%20Our%20central%20contribution%20is%20the%20theoretical%20discovery%20that%20this%20undesired%20stability%20can%20be%20broken%20by%20replacing%20standard%20monotone%20activations%20%28e.g.%2C%20ReLU%29%20with%20a%20class%20of%20functions.%20Using%20Lyapunov-Schmidt%20reduction%2C%20we%20analytically%20prove%20that%20this%20substitution%20induces%20a%20bifurcation%20that%20destabilizes%20the%20homogeneous%20state%20and%20creates%20a%20new%20pair%20of%20stable%2C%20non-homogeneous%20%5Cemph%7Bpatterns%7D%20that%20provably%20resist%20oversmoothing.%20Our%20theory%20predicts%20a%20precise%2C%20nontrivial%20scaling%20law%20for%20the%20amplitude%20of%20these%20emergent%20patterns%2C%20which%20we%20quantitatively%20validate%20in%20experiments.%20Finally%2C%20we%20demonstrate%20the%20practical%20utility%20of%20our%20theory%20by%20deriving%20a%20closed-form%2C%20bifurcation-aware%20initialization%20and%20showing%20its%20utility%20in%20real%20benchmark%20experiments.&entry.1838667208=http%3A//arxiv.org/abs/2602.15634v1&entry.124074799=Read"},
{"title": "1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization", "author": "Sohir Maskey and Constantin Eichenberg and Johannes Messner and Douglas Orr", "abstract": "Quantization-aware training (QAT) is an effective method to drastically reduce the memory footprint of LLMs while keeping performance degradation at an acceptable level. However, the optimal choice of quantization format and bit-width presents a challenge in practice. The full design space of quantization is not fully explored in the context of QAT, and the precise trade-off between quantization and downstream performance is poorly understood, as comparisons often rely solely on perplexity-based evaluations. In this work, we address these shortcomings with an empirical study of QAT in the low-bit regime. We show that k-means based weight quantization outperforms integer formats and can be implemented efficiently on standard hardware. Furthermore, we find that, under a fixed inference memory budget, the best performance on generative downstream tasks is achieved with $1$-bit quantized weights.", "link": "http://arxiv.org/abs/2602.15563v1", "date": "2026-02-17", "relevancy": 2.0225, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5499}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4749}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%201-Bit%20Wonder%3A%20Improving%20QAT%20Performance%20in%20the%20Low-Bit%20Regime%20through%20K-Means%20Quantization&body=Title%3A%201-Bit%20Wonder%3A%20Improving%20QAT%20Performance%20in%20the%20Low-Bit%20Regime%20through%20K-Means%20Quantization%0AAuthor%3A%20Sohir%20Maskey%20and%20Constantin%20Eichenberg%20and%20Johannes%20Messner%20and%20Douglas%20Orr%0AAbstract%3A%20Quantization-aware%20training%20%28QAT%29%20is%20an%20effective%20method%20to%20drastically%20reduce%20the%20memory%20footprint%20of%20LLMs%20while%20keeping%20performance%20degradation%20at%20an%20acceptable%20level.%20However%2C%20the%20optimal%20choice%20of%20quantization%20format%20and%20bit-width%20presents%20a%20challenge%20in%20practice.%20The%20full%20design%20space%20of%20quantization%20is%20not%20fully%20explored%20in%20the%20context%20of%20QAT%2C%20and%20the%20precise%20trade-off%20between%20quantization%20and%20downstream%20performance%20is%20poorly%20understood%2C%20as%20comparisons%20often%20rely%20solely%20on%20perplexity-based%20evaluations.%20In%20this%20work%2C%20we%20address%20these%20shortcomings%20with%20an%20empirical%20study%20of%20QAT%20in%20the%20low-bit%20regime.%20We%20show%20that%20k-means%20based%20weight%20quantization%20outperforms%20integer%20formats%20and%20can%20be%20implemented%20efficiently%20on%20standard%20hardware.%20Furthermore%2C%20we%20find%20that%2C%20under%20a%20fixed%20inference%20memory%20budget%2C%20the%20best%20performance%20on%20generative%20downstream%20tasks%20is%20achieved%20with%20%241%24-bit%20quantized%20weights.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D1-Bit%2520Wonder%253A%2520Improving%2520QAT%2520Performance%2520in%2520the%2520Low-Bit%2520Regime%2520through%2520K-Means%2520Quantization%26entry.906535625%3DSohir%2520Maskey%2520and%2520Constantin%2520Eichenberg%2520and%2520Johannes%2520Messner%2520and%2520Douglas%2520Orr%26entry.1292438233%3DQuantization-aware%2520training%2520%2528QAT%2529%2520is%2520an%2520effective%2520method%2520to%2520drastically%2520reduce%2520the%2520memory%2520footprint%2520of%2520LLMs%2520while%2520keeping%2520performance%2520degradation%2520at%2520an%2520acceptable%2520level.%2520However%252C%2520the%2520optimal%2520choice%2520of%2520quantization%2520format%2520and%2520bit-width%2520presents%2520a%2520challenge%2520in%2520practice.%2520The%2520full%2520design%2520space%2520of%2520quantization%2520is%2520not%2520fully%2520explored%2520in%2520the%2520context%2520of%2520QAT%252C%2520and%2520the%2520precise%2520trade-off%2520between%2520quantization%2520and%2520downstream%2520performance%2520is%2520poorly%2520understood%252C%2520as%2520comparisons%2520often%2520rely%2520solely%2520on%2520perplexity-based%2520evaluations.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520shortcomings%2520with%2520an%2520empirical%2520study%2520of%2520QAT%2520in%2520the%2520low-bit%2520regime.%2520We%2520show%2520that%2520k-means%2520based%2520weight%2520quantization%2520outperforms%2520integer%2520formats%2520and%2520can%2520be%2520implemented%2520efficiently%2520on%2520standard%2520hardware.%2520Furthermore%252C%2520we%2520find%2520that%252C%2520under%2520a%2520fixed%2520inference%2520memory%2520budget%252C%2520the%2520best%2520performance%2520on%2520generative%2520downstream%2520tasks%2520is%2520achieved%2520with%2520%25241%2524-bit%2520quantized%2520weights.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=1-Bit%20Wonder%3A%20Improving%20QAT%20Performance%20in%20the%20Low-Bit%20Regime%20through%20K-Means%20Quantization&entry.906535625=Sohir%20Maskey%20and%20Constantin%20Eichenberg%20and%20Johannes%20Messner%20and%20Douglas%20Orr&entry.1292438233=Quantization-aware%20training%20%28QAT%29%20is%20an%20effective%20method%20to%20drastically%20reduce%20the%20memory%20footprint%20of%20LLMs%20while%20keeping%20performance%20degradation%20at%20an%20acceptable%20level.%20However%2C%20the%20optimal%20choice%20of%20quantization%20format%20and%20bit-width%20presents%20a%20challenge%20in%20practice.%20The%20full%20design%20space%20of%20quantization%20is%20not%20fully%20explored%20in%20the%20context%20of%20QAT%2C%20and%20the%20precise%20trade-off%20between%20quantization%20and%20downstream%20performance%20is%20poorly%20understood%2C%20as%20comparisons%20often%20rely%20solely%20on%20perplexity-based%20evaluations.%20In%20this%20work%2C%20we%20address%20these%20shortcomings%20with%20an%20empirical%20study%20of%20QAT%20in%20the%20low-bit%20regime.%20We%20show%20that%20k-means%20based%20weight%20quantization%20outperforms%20integer%20formats%20and%20can%20be%20implemented%20efficiently%20on%20standard%20hardware.%20Furthermore%2C%20we%20find%20that%2C%20under%20a%20fixed%20inference%20memory%20budget%2C%20the%20best%20performance%20on%20generative%20downstream%20tasks%20is%20achieved%20with%20%241%24-bit%20quantized%20weights.&entry.1838667208=http%3A//arxiv.org/abs/2602.15563v1&entry.124074799=Read"},
{"title": "stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation", "author": "Lucas Maes and Quentin Le Lidec and Dan Haramati and Nassim Massaudi and Damien Scieur and Yann LeCun and Randall Balestriero", "abstract": "World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.", "link": "http://arxiv.org/abs/2602.08968v2", "date": "2026-02-17", "relevancy": 2.0186, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.545}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5032}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20stable-worldmodel-v1%3A%20Reproducible%20World%20Modeling%20Research%20and%20Evaluation&body=Title%3A%20stable-worldmodel-v1%3A%20Reproducible%20World%20Modeling%20Research%20and%20Evaluation%0AAuthor%3A%20Lucas%20Maes%20and%20Quentin%20Le%20Lidec%20and%20Dan%20Haramati%20and%20Nassim%20Massaudi%20and%20Damien%20Scieur%20and%20Yann%20LeCun%20and%20Randall%20Balestriero%0AAbstract%3A%20World%20Models%20have%20emerged%20as%20a%20powerful%20paradigm%20for%20learning%20compact%2C%20predictive%20representations%20of%20environment%20dynamics%2C%20enabling%20agents%20to%20reason%2C%20plan%2C%20and%20generalize%20beyond%20direct%20experience.%20Despite%20recent%20interest%20in%20World%20Models%2C%20most%20available%20implementations%20remain%20publication-specific%2C%20severely%20limiting%20their%20reusability%2C%20increasing%20the%20risk%20of%20bugs%2C%20and%20reducing%20evaluation%20standardization.%20To%20mitigate%20these%20issues%2C%20we%20introduce%20stable-worldmodel%20%28SWM%29%2C%20a%20modular%2C%20tested%2C%20and%20documented%20world-model%20research%20ecosystem%20that%20provides%20efficient%20data-collection%20tools%2C%20standardized%20environments%2C%20planning%20algorithms%2C%20and%20baseline%20implementations.%20In%20addition%2C%20each%20environment%20in%20SWM%20enables%20controllable%20factors%20of%20variation%2C%20including%20visual%20and%20physical%20properties%2C%20to%20support%20robustness%20and%20continual%20learning%20research.%20Finally%2C%20we%20demonstrate%20the%20utility%20of%20SWM%20by%20using%20it%20to%20study%20zero-shot%20robustness%20in%20DINO-WM.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08968v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dstable-worldmodel-v1%253A%2520Reproducible%2520World%2520Modeling%2520Research%2520and%2520Evaluation%26entry.906535625%3DLucas%2520Maes%2520and%2520Quentin%2520Le%2520Lidec%2520and%2520Dan%2520Haramati%2520and%2520Nassim%2520Massaudi%2520and%2520Damien%2520Scieur%2520and%2520Yann%2520LeCun%2520and%2520Randall%2520Balestriero%26entry.1292438233%3DWorld%2520Models%2520have%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%2520learning%2520compact%252C%2520predictive%2520representations%2520of%2520environment%2520dynamics%252C%2520enabling%2520agents%2520to%2520reason%252C%2520plan%252C%2520and%2520generalize%2520beyond%2520direct%2520experience.%2520Despite%2520recent%2520interest%2520in%2520World%2520Models%252C%2520most%2520available%2520implementations%2520remain%2520publication-specific%252C%2520severely%2520limiting%2520their%2520reusability%252C%2520increasing%2520the%2520risk%2520of%2520bugs%252C%2520and%2520reducing%2520evaluation%2520standardization.%2520To%2520mitigate%2520these%2520issues%252C%2520we%2520introduce%2520stable-worldmodel%2520%2528SWM%2529%252C%2520a%2520modular%252C%2520tested%252C%2520and%2520documented%2520world-model%2520research%2520ecosystem%2520that%2520provides%2520efficient%2520data-collection%2520tools%252C%2520standardized%2520environments%252C%2520planning%2520algorithms%252C%2520and%2520baseline%2520implementations.%2520In%2520addition%252C%2520each%2520environment%2520in%2520SWM%2520enables%2520controllable%2520factors%2520of%2520variation%252C%2520including%2520visual%2520and%2520physical%2520properties%252C%2520to%2520support%2520robustness%2520and%2520continual%2520learning%2520research.%2520Finally%252C%2520we%2520demonstrate%2520the%2520utility%2520of%2520SWM%2520by%2520using%2520it%2520to%2520study%2520zero-shot%2520robustness%2520in%2520DINO-WM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08968v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=stable-worldmodel-v1%3A%20Reproducible%20World%20Modeling%20Research%20and%20Evaluation&entry.906535625=Lucas%20Maes%20and%20Quentin%20Le%20Lidec%20and%20Dan%20Haramati%20and%20Nassim%20Massaudi%20and%20Damien%20Scieur%20and%20Yann%20LeCun%20and%20Randall%20Balestriero&entry.1292438233=World%20Models%20have%20emerged%20as%20a%20powerful%20paradigm%20for%20learning%20compact%2C%20predictive%20representations%20of%20environment%20dynamics%2C%20enabling%20agents%20to%20reason%2C%20plan%2C%20and%20generalize%20beyond%20direct%20experience.%20Despite%20recent%20interest%20in%20World%20Models%2C%20most%20available%20implementations%20remain%20publication-specific%2C%20severely%20limiting%20their%20reusability%2C%20increasing%20the%20risk%20of%20bugs%2C%20and%20reducing%20evaluation%20standardization.%20To%20mitigate%20these%20issues%2C%20we%20introduce%20stable-worldmodel%20%28SWM%29%2C%20a%20modular%2C%20tested%2C%20and%20documented%20world-model%20research%20ecosystem%20that%20provides%20efficient%20data-collection%20tools%2C%20standardized%20environments%2C%20planning%20algorithms%2C%20and%20baseline%20implementations.%20In%20addition%2C%20each%20environment%20in%20SWM%20enables%20controllable%20factors%20of%20variation%2C%20including%20visual%20and%20physical%20properties%2C%20to%20support%20robustness%20and%20continual%20learning%20research.%20Finally%2C%20we%20demonstrate%20the%20utility%20of%20SWM%20by%20using%20it%20to%20study%20zero-shot%20robustness%20in%20DINO-WM.&entry.1838667208=http%3A//arxiv.org/abs/2602.08968v2&entry.124074799=Read"},
{"title": "Latent Veracity Inference for Identifying Errors in Stepwise Reasoning", "author": "Minsu Kim and Jean-Pierre Falet and Oliver E. Richardson and Xiaoyin Chen and Moksh Jain and Sungjin Ahn and Sungsoo Ahn and Yoshua Bengio", "abstract": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and transparency of language models (LMs); however, reasoning chains can contain inaccurate statements that reduce performance and trustworthiness. To address this, we propose to augment each reasoning step in a CoT with a latent veracity (or correctness) variable. To efficiently explore this expanded space, we introduce Veracity Search (VS), a discrete search algorithm over veracity assignments. It performs otherwise intractable inference in the posterior distribution over latent veracity values by leveraging the LM's joint likelihood over veracity and the final answer as a proxy reward. This efficient inference-time verification method facilitates supervised fine-tuning of an Amortized Veracity Inference (AVI) machine by providing pseudo-labels for veracity. AVI generalizes VS, enabling accurate zero-shot veracity inference in novel contexts. Empirical results demonstrate that VS reliably identifies errors in logical (ProntoQA), mathematical (GSM8K), and commonsense (CommonsenseQA) reasoning benchmarks, with AVI achieving comparable zero-shot accuracy. Finally, we demonstrate the utility of latent veracity inference for providing feedback during self-correction and self-improvement.", "link": "http://arxiv.org/abs/2505.11824v3", "date": "2026-02-17", "relevancy": 2.0139, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5238}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Veracity%20Inference%20for%20Identifying%20Errors%20in%20Stepwise%20Reasoning&body=Title%3A%20Latent%20Veracity%20Inference%20for%20Identifying%20Errors%20in%20Stepwise%20Reasoning%0AAuthor%3A%20Minsu%20Kim%20and%20Jean-Pierre%20Falet%20and%20Oliver%20E.%20Richardson%20and%20Xiaoyin%20Chen%20and%20Moksh%20Jain%20and%20Sungjin%20Ahn%20and%20Sungsoo%20Ahn%20and%20Yoshua%20Bengio%0AAbstract%3A%20Chain-of-Thought%20%28CoT%29%20reasoning%20has%20advanced%20the%20capabilities%20and%20transparency%20of%20language%20models%20%28LMs%29%3B%20however%2C%20reasoning%20chains%20can%20contain%20inaccurate%20statements%20that%20reduce%20performance%20and%20trustworthiness.%20To%20address%20this%2C%20we%20propose%20to%20augment%20each%20reasoning%20step%20in%20a%20CoT%20with%20a%20latent%20veracity%20%28or%20correctness%29%20variable.%20To%20efficiently%20explore%20this%20expanded%20space%2C%20we%20introduce%20Veracity%20Search%20%28VS%29%2C%20a%20discrete%20search%20algorithm%20over%20veracity%20assignments.%20It%20performs%20otherwise%20intractable%20inference%20in%20the%20posterior%20distribution%20over%20latent%20veracity%20values%20by%20leveraging%20the%20LM%27s%20joint%20likelihood%20over%20veracity%20and%20the%20final%20answer%20as%20a%20proxy%20reward.%20This%20efficient%20inference-time%20verification%20method%20facilitates%20supervised%20fine-tuning%20of%20an%20Amortized%20Veracity%20Inference%20%28AVI%29%20machine%20by%20providing%20pseudo-labels%20for%20veracity.%20AVI%20generalizes%20VS%2C%20enabling%20accurate%20zero-shot%20veracity%20inference%20in%20novel%20contexts.%20Empirical%20results%20demonstrate%20that%20VS%20reliably%20identifies%20errors%20in%20logical%20%28ProntoQA%29%2C%20mathematical%20%28GSM8K%29%2C%20and%20commonsense%20%28CommonsenseQA%29%20reasoning%20benchmarks%2C%20with%20AVI%20achieving%20comparable%20zero-shot%20accuracy.%20Finally%2C%20we%20demonstrate%20the%20utility%20of%20latent%20veracity%20inference%20for%20providing%20feedback%20during%20self-correction%20and%20self-improvement.%0ALink%3A%20http%3A//arxiv.org/abs/2505.11824v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Veracity%2520Inference%2520for%2520Identifying%2520Errors%2520in%2520Stepwise%2520Reasoning%26entry.906535625%3DMinsu%2520Kim%2520and%2520Jean-Pierre%2520Falet%2520and%2520Oliver%2520E.%2520Richardson%2520and%2520Xiaoyin%2520Chen%2520and%2520Moksh%2520Jain%2520and%2520Sungjin%2520Ahn%2520and%2520Sungsoo%2520Ahn%2520and%2520Yoshua%2520Bengio%26entry.1292438233%3DChain-of-Thought%2520%2528CoT%2529%2520reasoning%2520has%2520advanced%2520the%2520capabilities%2520and%2520transparency%2520of%2520language%2520models%2520%2528LMs%2529%253B%2520however%252C%2520reasoning%2520chains%2520can%2520contain%2520inaccurate%2520statements%2520that%2520reduce%2520performance%2520and%2520trustworthiness.%2520To%2520address%2520this%252C%2520we%2520propose%2520to%2520augment%2520each%2520reasoning%2520step%2520in%2520a%2520CoT%2520with%2520a%2520latent%2520veracity%2520%2528or%2520correctness%2529%2520variable.%2520To%2520efficiently%2520explore%2520this%2520expanded%2520space%252C%2520we%2520introduce%2520Veracity%2520Search%2520%2528VS%2529%252C%2520a%2520discrete%2520search%2520algorithm%2520over%2520veracity%2520assignments.%2520It%2520performs%2520otherwise%2520intractable%2520inference%2520in%2520the%2520posterior%2520distribution%2520over%2520latent%2520veracity%2520values%2520by%2520leveraging%2520the%2520LM%2527s%2520joint%2520likelihood%2520over%2520veracity%2520and%2520the%2520final%2520answer%2520as%2520a%2520proxy%2520reward.%2520This%2520efficient%2520inference-time%2520verification%2520method%2520facilitates%2520supervised%2520fine-tuning%2520of%2520an%2520Amortized%2520Veracity%2520Inference%2520%2528AVI%2529%2520machine%2520by%2520providing%2520pseudo-labels%2520for%2520veracity.%2520AVI%2520generalizes%2520VS%252C%2520enabling%2520accurate%2520zero-shot%2520veracity%2520inference%2520in%2520novel%2520contexts.%2520Empirical%2520results%2520demonstrate%2520that%2520VS%2520reliably%2520identifies%2520errors%2520in%2520logical%2520%2528ProntoQA%2529%252C%2520mathematical%2520%2528GSM8K%2529%252C%2520and%2520commonsense%2520%2528CommonsenseQA%2529%2520reasoning%2520benchmarks%252C%2520with%2520AVI%2520achieving%2520comparable%2520zero-shot%2520accuracy.%2520Finally%252C%2520we%2520demonstrate%2520the%2520utility%2520of%2520latent%2520veracity%2520inference%2520for%2520providing%2520feedback%2520during%2520self-correction%2520and%2520self-improvement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11824v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Veracity%20Inference%20for%20Identifying%20Errors%20in%20Stepwise%20Reasoning&entry.906535625=Minsu%20Kim%20and%20Jean-Pierre%20Falet%20and%20Oliver%20E.%20Richardson%20and%20Xiaoyin%20Chen%20and%20Moksh%20Jain%20and%20Sungjin%20Ahn%20and%20Sungsoo%20Ahn%20and%20Yoshua%20Bengio&entry.1292438233=Chain-of-Thought%20%28CoT%29%20reasoning%20has%20advanced%20the%20capabilities%20and%20transparency%20of%20language%20models%20%28LMs%29%3B%20however%2C%20reasoning%20chains%20can%20contain%20inaccurate%20statements%20that%20reduce%20performance%20and%20trustworthiness.%20To%20address%20this%2C%20we%20propose%20to%20augment%20each%20reasoning%20step%20in%20a%20CoT%20with%20a%20latent%20veracity%20%28or%20correctness%29%20variable.%20To%20efficiently%20explore%20this%20expanded%20space%2C%20we%20introduce%20Veracity%20Search%20%28VS%29%2C%20a%20discrete%20search%20algorithm%20over%20veracity%20assignments.%20It%20performs%20otherwise%20intractable%20inference%20in%20the%20posterior%20distribution%20over%20latent%20veracity%20values%20by%20leveraging%20the%20LM%27s%20joint%20likelihood%20over%20veracity%20and%20the%20final%20answer%20as%20a%20proxy%20reward.%20This%20efficient%20inference-time%20verification%20method%20facilitates%20supervised%20fine-tuning%20of%20an%20Amortized%20Veracity%20Inference%20%28AVI%29%20machine%20by%20providing%20pseudo-labels%20for%20veracity.%20AVI%20generalizes%20VS%2C%20enabling%20accurate%20zero-shot%20veracity%20inference%20in%20novel%20contexts.%20Empirical%20results%20demonstrate%20that%20VS%20reliably%20identifies%20errors%20in%20logical%20%28ProntoQA%29%2C%20mathematical%20%28GSM8K%29%2C%20and%20commonsense%20%28CommonsenseQA%29%20reasoning%20benchmarks%2C%20with%20AVI%20achieving%20comparable%20zero-shot%20accuracy.%20Finally%2C%20we%20demonstrate%20the%20utility%20of%20latent%20veracity%20inference%20for%20providing%20feedback%20during%20self-correction%20and%20self-improvement.&entry.1838667208=http%3A//arxiv.org/abs/2505.11824v3&entry.124074799=Read"},
{"title": "mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations", "author": "Guy Dar", "abstract": "We build upon vec2vec, a procedure designed to align text embedding spaces without parallel data. vec2vec finds a near-perfect alignment, but it is expensive and unstable. We present mini-vec2vec, a simple and efficient alternative that requires substantially lower computational cost and is highly robust. Moreover, the learned mapping is a linear transformation. Our method consists of three main stages: a tentative matching of pseudo-parallel embedding vectors, transformation fitting, and iterative refinement. Our linear alternative exceeds the original instantiation of vec2vec by orders of magnitude in efficiency, while matching or exceeding their results. The method's stability and interpretable algorithmic steps facilitate scaling and unlock new opportunities for adoption in new domains and fields.", "link": "http://arxiv.org/abs/2510.02348v4", "date": "2026-02-17", "relevancy": 2.0107, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5069}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5036}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mini-vec2vec%3A%20Scaling%20Universal%20Geometry%20Alignment%20with%20Linear%20Transformations&body=Title%3A%20mini-vec2vec%3A%20Scaling%20Universal%20Geometry%20Alignment%20with%20Linear%20Transformations%0AAuthor%3A%20Guy%20Dar%0AAbstract%3A%20We%20build%20upon%20vec2vec%2C%20a%20procedure%20designed%20to%20align%20text%20embedding%20spaces%20without%20parallel%20data.%20vec2vec%20finds%20a%20near-perfect%20alignment%2C%20but%20it%20is%20expensive%20and%20unstable.%20We%20present%20mini-vec2vec%2C%20a%20simple%20and%20efficient%20alternative%20that%20requires%20substantially%20lower%20computational%20cost%20and%20is%20highly%20robust.%20Moreover%2C%20the%20learned%20mapping%20is%20a%20linear%20transformation.%20Our%20method%20consists%20of%20three%20main%20stages%3A%20a%20tentative%20matching%20of%20pseudo-parallel%20embedding%20vectors%2C%20transformation%20fitting%2C%20and%20iterative%20refinement.%20Our%20linear%20alternative%20exceeds%20the%20original%20instantiation%20of%20vec2vec%20by%20orders%20of%20magnitude%20in%20efficiency%2C%20while%20matching%20or%20exceeding%20their%20results.%20The%20method%27s%20stability%20and%20interpretable%20algorithmic%20steps%20facilitate%20scaling%20and%20unlock%20new%20opportunities%20for%20adoption%20in%20new%20domains%20and%20fields.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02348v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dmini-vec2vec%253A%2520Scaling%2520Universal%2520Geometry%2520Alignment%2520with%2520Linear%2520Transformations%26entry.906535625%3DGuy%2520Dar%26entry.1292438233%3DWe%2520build%2520upon%2520vec2vec%252C%2520a%2520procedure%2520designed%2520to%2520align%2520text%2520embedding%2520spaces%2520without%2520parallel%2520data.%2520vec2vec%2520finds%2520a%2520near-perfect%2520alignment%252C%2520but%2520it%2520is%2520expensive%2520and%2520unstable.%2520We%2520present%2520mini-vec2vec%252C%2520a%2520simple%2520and%2520efficient%2520alternative%2520that%2520requires%2520substantially%2520lower%2520computational%2520cost%2520and%2520is%2520highly%2520robust.%2520Moreover%252C%2520the%2520learned%2520mapping%2520is%2520a%2520linear%2520transformation.%2520Our%2520method%2520consists%2520of%2520three%2520main%2520stages%253A%2520a%2520tentative%2520matching%2520of%2520pseudo-parallel%2520embedding%2520vectors%252C%2520transformation%2520fitting%252C%2520and%2520iterative%2520refinement.%2520Our%2520linear%2520alternative%2520exceeds%2520the%2520original%2520instantiation%2520of%2520vec2vec%2520by%2520orders%2520of%2520magnitude%2520in%2520efficiency%252C%2520while%2520matching%2520or%2520exceeding%2520their%2520results.%2520The%2520method%2527s%2520stability%2520and%2520interpretable%2520algorithmic%2520steps%2520facilitate%2520scaling%2520and%2520unlock%2520new%2520opportunities%2520for%2520adoption%2520in%2520new%2520domains%2520and%2520fields.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02348v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mini-vec2vec%3A%20Scaling%20Universal%20Geometry%20Alignment%20with%20Linear%20Transformations&entry.906535625=Guy%20Dar&entry.1292438233=We%20build%20upon%20vec2vec%2C%20a%20procedure%20designed%20to%20align%20text%20embedding%20spaces%20without%20parallel%20data.%20vec2vec%20finds%20a%20near-perfect%20alignment%2C%20but%20it%20is%20expensive%20and%20unstable.%20We%20present%20mini-vec2vec%2C%20a%20simple%20and%20efficient%20alternative%20that%20requires%20substantially%20lower%20computational%20cost%20and%20is%20highly%20robust.%20Moreover%2C%20the%20learned%20mapping%20is%20a%20linear%20transformation.%20Our%20method%20consists%20of%20three%20main%20stages%3A%20a%20tentative%20matching%20of%20pseudo-parallel%20embedding%20vectors%2C%20transformation%20fitting%2C%20and%20iterative%20refinement.%20Our%20linear%20alternative%20exceeds%20the%20original%20instantiation%20of%20vec2vec%20by%20orders%20of%20magnitude%20in%20efficiency%2C%20while%20matching%20or%20exceeding%20their%20results.%20The%20method%27s%20stability%20and%20interpretable%20algorithmic%20steps%20facilitate%20scaling%20and%20unlock%20new%20opportunities%20for%20adoption%20in%20new%20domains%20and%20fields.&entry.1838667208=http%3A//arxiv.org/abs/2510.02348v4&entry.124074799=Read"},
{"title": "Neural Scaling Laws for Boosted Jet Tagging", "author": "Matthias Vigl and Nicole Hartman and Michael Kagan and Lukas Heinrich", "abstract": "The success of Large Language Models (LLMs) has established that scaling compute, through joint increases in model capacity and dataset size, is the primary driver of performance in modern machine learning. While machine learning has long been an integral component of High Energy Physics (HEP) data analysis workflows, the compute used to train state-of-the-art HEP models remains orders of magnitude below that of industry foundation models. With scaling laws only beginning to be studied in the field, we investigate neural scaling laws for boosted jet classification using the public JetClass dataset. We derive compute optimal scaling laws and identify an effective performance limit that can be consistently approached through increased compute. We study how data repetition, common in HEP where simulation is expensive, modifies the scaling yielding a quantifiable effective dataset size gain. We then study how the scaling coefficients and asymptotic performance limits vary with the choice of input features and particle multiplicity, demonstrating that increased compute reliably drives performance toward an asymptotic limit, and that more expressive, lower-level features can raise the performance limit and improve results at fixed dataset size.", "link": "http://arxiv.org/abs/2602.15781v1", "date": "2026-02-17", "relevancy": 2.0099, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5409}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5131}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Scaling%20Laws%20for%20Boosted%20Jet%20Tagging&body=Title%3A%20Neural%20Scaling%20Laws%20for%20Boosted%20Jet%20Tagging%0AAuthor%3A%20Matthias%20Vigl%20and%20Nicole%20Hartman%20and%20Michael%20Kagan%20and%20Lukas%20Heinrich%0AAbstract%3A%20The%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20established%20that%20scaling%20compute%2C%20through%20joint%20increases%20in%20model%20capacity%20and%20dataset%20size%2C%20is%20the%20primary%20driver%20of%20performance%20in%20modern%20machine%20learning.%20While%20machine%20learning%20has%20long%20been%20an%20integral%20component%20of%20High%20Energy%20Physics%20%28HEP%29%20data%20analysis%20workflows%2C%20the%20compute%20used%20to%20train%20state-of-the-art%20HEP%20models%20remains%20orders%20of%20magnitude%20below%20that%20of%20industry%20foundation%20models.%20With%20scaling%20laws%20only%20beginning%20to%20be%20studied%20in%20the%20field%2C%20we%20investigate%20neural%20scaling%20laws%20for%20boosted%20jet%20classification%20using%20the%20public%20JetClass%20dataset.%20We%20derive%20compute%20optimal%20scaling%20laws%20and%20identify%20an%20effective%20performance%20limit%20that%20can%20be%20consistently%20approached%20through%20increased%20compute.%20We%20study%20how%20data%20repetition%2C%20common%20in%20HEP%20where%20simulation%20is%20expensive%2C%20modifies%20the%20scaling%20yielding%20a%20quantifiable%20effective%20dataset%20size%20gain.%20We%20then%20study%20how%20the%20scaling%20coefficients%20and%20asymptotic%20performance%20limits%20vary%20with%20the%20choice%20of%20input%20features%20and%20particle%20multiplicity%2C%20demonstrating%20that%20increased%20compute%20reliably%20drives%20performance%20toward%20an%20asymptotic%20limit%2C%20and%20that%20more%20expressive%2C%20lower-level%20features%20can%20raise%20the%20performance%20limit%20and%20improve%20results%20at%20fixed%20dataset%20size.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Scaling%2520Laws%2520for%2520Boosted%2520Jet%2520Tagging%26entry.906535625%3DMatthias%2520Vigl%2520and%2520Nicole%2520Hartman%2520and%2520Michael%2520Kagan%2520and%2520Lukas%2520Heinrich%26entry.1292438233%3DThe%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520established%2520that%2520scaling%2520compute%252C%2520through%2520joint%2520increases%2520in%2520model%2520capacity%2520and%2520dataset%2520size%252C%2520is%2520the%2520primary%2520driver%2520of%2520performance%2520in%2520modern%2520machine%2520learning.%2520While%2520machine%2520learning%2520has%2520long%2520been%2520an%2520integral%2520component%2520of%2520High%2520Energy%2520Physics%2520%2528HEP%2529%2520data%2520analysis%2520workflows%252C%2520the%2520compute%2520used%2520to%2520train%2520state-of-the-art%2520HEP%2520models%2520remains%2520orders%2520of%2520magnitude%2520below%2520that%2520of%2520industry%2520foundation%2520models.%2520With%2520scaling%2520laws%2520only%2520beginning%2520to%2520be%2520studied%2520in%2520the%2520field%252C%2520we%2520investigate%2520neural%2520scaling%2520laws%2520for%2520boosted%2520jet%2520classification%2520using%2520the%2520public%2520JetClass%2520dataset.%2520We%2520derive%2520compute%2520optimal%2520scaling%2520laws%2520and%2520identify%2520an%2520effective%2520performance%2520limit%2520that%2520can%2520be%2520consistently%2520approached%2520through%2520increased%2520compute.%2520We%2520study%2520how%2520data%2520repetition%252C%2520common%2520in%2520HEP%2520where%2520simulation%2520is%2520expensive%252C%2520modifies%2520the%2520scaling%2520yielding%2520a%2520quantifiable%2520effective%2520dataset%2520size%2520gain.%2520We%2520then%2520study%2520how%2520the%2520scaling%2520coefficients%2520and%2520asymptotic%2520performance%2520limits%2520vary%2520with%2520the%2520choice%2520of%2520input%2520features%2520and%2520particle%2520multiplicity%252C%2520demonstrating%2520that%2520increased%2520compute%2520reliably%2520drives%2520performance%2520toward%2520an%2520asymptotic%2520limit%252C%2520and%2520that%2520more%2520expressive%252C%2520lower-level%2520features%2520can%2520raise%2520the%2520performance%2520limit%2520and%2520improve%2520results%2520at%2520fixed%2520dataset%2520size.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Scaling%20Laws%20for%20Boosted%20Jet%20Tagging&entry.906535625=Matthias%20Vigl%20and%20Nicole%20Hartman%20and%20Michael%20Kagan%20and%20Lukas%20Heinrich&entry.1292438233=The%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20established%20that%20scaling%20compute%2C%20through%20joint%20increases%20in%20model%20capacity%20and%20dataset%20size%2C%20is%20the%20primary%20driver%20of%20performance%20in%20modern%20machine%20learning.%20While%20machine%20learning%20has%20long%20been%20an%20integral%20component%20of%20High%20Energy%20Physics%20%28HEP%29%20data%20analysis%20workflows%2C%20the%20compute%20used%20to%20train%20state-of-the-art%20HEP%20models%20remains%20orders%20of%20magnitude%20below%20that%20of%20industry%20foundation%20models.%20With%20scaling%20laws%20only%20beginning%20to%20be%20studied%20in%20the%20field%2C%20we%20investigate%20neural%20scaling%20laws%20for%20boosted%20jet%20classification%20using%20the%20public%20JetClass%20dataset.%20We%20derive%20compute%20optimal%20scaling%20laws%20and%20identify%20an%20effective%20performance%20limit%20that%20can%20be%20consistently%20approached%20through%20increased%20compute.%20We%20study%20how%20data%20repetition%2C%20common%20in%20HEP%20where%20simulation%20is%20expensive%2C%20modifies%20the%20scaling%20yielding%20a%20quantifiable%20effective%20dataset%20size%20gain.%20We%20then%20study%20how%20the%20scaling%20coefficients%20and%20asymptotic%20performance%20limits%20vary%20with%20the%20choice%20of%20input%20features%20and%20particle%20multiplicity%2C%20demonstrating%20that%20increased%20compute%20reliably%20drives%20performance%20toward%20an%20asymptotic%20limit%2C%20and%20that%20more%20expressive%2C%20lower-level%20features%20can%20raise%20the%20performance%20limit%20and%20improve%20results%20at%20fixed%20dataset%20size.&entry.1838667208=http%3A//arxiv.org/abs/2602.15781v1&entry.124074799=Read"},
{"title": "RUVA: Personalized Transparent On-Device Graph Reasoning", "author": "Gabriele Conte and Alessio Mattiace and Gianni Carmosino and Potito Aghilar and Giovanni Servedio and Francesco Musicco and Vito Walter Anelli and Tommaso Di Noia and Francesco Maria Donini", "abstract": "The Personal AI landscape is currently dominated by \"Black Box\" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, \"deleting\" a concept from a vector space is mathematically imprecise, leaving behind probabilistic \"ghosts\" that violate true privacy. We propose Ruva, the first \"Glass Box\" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the \"Right to be Forgotten.\" Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.", "link": "http://arxiv.org/abs/2602.15553v1", "date": "2026-02-17", "relevancy": 1.9973, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5012}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4997}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RUVA%3A%20Personalized%20Transparent%20On-Device%20Graph%20Reasoning&body=Title%3A%20RUVA%3A%20Personalized%20Transparent%20On-Device%20Graph%20Reasoning%0AAuthor%3A%20Gabriele%20Conte%20and%20Alessio%20Mattiace%20and%20Gianni%20Carmosino%20and%20Potito%20Aghilar%20and%20Giovanni%20Servedio%20and%20Francesco%20Musicco%20and%20Vito%20Walter%20Anelli%20and%20Tommaso%20Di%20Noia%20and%20Francesco%20Maria%20Donini%0AAbstract%3A%20The%20Personal%20AI%20landscape%20is%20currently%20dominated%20by%20%22Black%20Box%22%20Retrieval-Augmented%20Generation.%20While%20standard%20vector%20databases%20offer%20statistical%20matching%2C%20they%20suffer%20from%20a%20fundamental%20lack%20of%20accountability%3A%20when%20an%20AI%20hallucinates%20or%20retrieves%20sensitive%20data%2C%20the%20user%20cannot%20inspect%20the%20cause%20nor%20correct%20the%20error.%20Worse%2C%20%22deleting%22%20a%20concept%20from%20a%20vector%20space%20is%20mathematically%20imprecise%2C%20leaving%20behind%20probabilistic%20%22ghosts%22%20that%20violate%20true%20privacy.%20We%20propose%20Ruva%2C%20the%20first%20%22Glass%20Box%22%20architecture%20designed%20for%20Human-in-the-Loop%20Memory%20Curation.%20Ruva%20grounds%20Personal%20AI%20in%20a%20Personal%20Knowledge%20Graph%2C%20enabling%20users%20to%20inspect%20what%20the%20AI%20knows%20and%20to%20perform%20precise%20redaction%20of%20specific%20facts.%20By%20shifting%20the%20paradigm%20from%20Vector%20Matching%20to%20Graph%20Reasoning%2C%20Ruva%20ensures%20the%20%22Right%20to%20be%20Forgotten.%22%20Users%20are%20the%20editors%20of%20their%20own%20lives%3B%20Ruva%20hands%20them%20the%20pen.%20The%20project%20and%20the%20demo%20video%20are%20available%20at%20http%3A//sisinf00.poliba.it/ruva/.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRUVA%253A%2520Personalized%2520Transparent%2520On-Device%2520Graph%2520Reasoning%26entry.906535625%3DGabriele%2520Conte%2520and%2520Alessio%2520Mattiace%2520and%2520Gianni%2520Carmosino%2520and%2520Potito%2520Aghilar%2520and%2520Giovanni%2520Servedio%2520and%2520Francesco%2520Musicco%2520and%2520Vito%2520Walter%2520Anelli%2520and%2520Tommaso%2520Di%2520Noia%2520and%2520Francesco%2520Maria%2520Donini%26entry.1292438233%3DThe%2520Personal%2520AI%2520landscape%2520is%2520currently%2520dominated%2520by%2520%2522Black%2520Box%2522%2520Retrieval-Augmented%2520Generation.%2520While%2520standard%2520vector%2520databases%2520offer%2520statistical%2520matching%252C%2520they%2520suffer%2520from%2520a%2520fundamental%2520lack%2520of%2520accountability%253A%2520when%2520an%2520AI%2520hallucinates%2520or%2520retrieves%2520sensitive%2520data%252C%2520the%2520user%2520cannot%2520inspect%2520the%2520cause%2520nor%2520correct%2520the%2520error.%2520Worse%252C%2520%2522deleting%2522%2520a%2520concept%2520from%2520a%2520vector%2520space%2520is%2520mathematically%2520imprecise%252C%2520leaving%2520behind%2520probabilistic%2520%2522ghosts%2522%2520that%2520violate%2520true%2520privacy.%2520We%2520propose%2520Ruva%252C%2520the%2520first%2520%2522Glass%2520Box%2522%2520architecture%2520designed%2520for%2520Human-in-the-Loop%2520Memory%2520Curation.%2520Ruva%2520grounds%2520Personal%2520AI%2520in%2520a%2520Personal%2520Knowledge%2520Graph%252C%2520enabling%2520users%2520to%2520inspect%2520what%2520the%2520AI%2520knows%2520and%2520to%2520perform%2520precise%2520redaction%2520of%2520specific%2520facts.%2520By%2520shifting%2520the%2520paradigm%2520from%2520Vector%2520Matching%2520to%2520Graph%2520Reasoning%252C%2520Ruva%2520ensures%2520the%2520%2522Right%2520to%2520be%2520Forgotten.%2522%2520Users%2520are%2520the%2520editors%2520of%2520their%2520own%2520lives%253B%2520Ruva%2520hands%2520them%2520the%2520pen.%2520The%2520project%2520and%2520the%2520demo%2520video%2520are%2520available%2520at%2520http%253A//sisinf00.poliba.it/ruva/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RUVA%3A%20Personalized%20Transparent%20On-Device%20Graph%20Reasoning&entry.906535625=Gabriele%20Conte%20and%20Alessio%20Mattiace%20and%20Gianni%20Carmosino%20and%20Potito%20Aghilar%20and%20Giovanni%20Servedio%20and%20Francesco%20Musicco%20and%20Vito%20Walter%20Anelli%20and%20Tommaso%20Di%20Noia%20and%20Francesco%20Maria%20Donini&entry.1292438233=The%20Personal%20AI%20landscape%20is%20currently%20dominated%20by%20%22Black%20Box%22%20Retrieval-Augmented%20Generation.%20While%20standard%20vector%20databases%20offer%20statistical%20matching%2C%20they%20suffer%20from%20a%20fundamental%20lack%20of%20accountability%3A%20when%20an%20AI%20hallucinates%20or%20retrieves%20sensitive%20data%2C%20the%20user%20cannot%20inspect%20the%20cause%20nor%20correct%20the%20error.%20Worse%2C%20%22deleting%22%20a%20concept%20from%20a%20vector%20space%20is%20mathematically%20imprecise%2C%20leaving%20behind%20probabilistic%20%22ghosts%22%20that%20violate%20true%20privacy.%20We%20propose%20Ruva%2C%20the%20first%20%22Glass%20Box%22%20architecture%20designed%20for%20Human-in-the-Loop%20Memory%20Curation.%20Ruva%20grounds%20Personal%20AI%20in%20a%20Personal%20Knowledge%20Graph%2C%20enabling%20users%20to%20inspect%20what%20the%20AI%20knows%20and%20to%20perform%20precise%20redaction%20of%20specific%20facts.%20By%20shifting%20the%20paradigm%20from%20Vector%20Matching%20to%20Graph%20Reasoning%2C%20Ruva%20ensures%20the%20%22Right%20to%20be%20Forgotten.%22%20Users%20are%20the%20editors%20of%20their%20own%20lives%3B%20Ruva%20hands%20them%20the%20pen.%20The%20project%20and%20the%20demo%20video%20are%20available%20at%20http%3A//sisinf00.poliba.it/ruva/.&entry.1838667208=http%3A//arxiv.org/abs/2602.15553v1&entry.124074799=Read"},
{"title": "PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra", "author": "Xiachong Feng and Liang Zhao and Weihong Zhong and Yichong Huang and Yuxuan Gu and Lingpeng Kong and Xiaocheng Feng and Bing Qin", "abstract": "Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.", "link": "http://arxiv.org/abs/2602.15669v1", "date": "2026-02-17", "relevancy": 1.981, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5037}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4927}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PERSONA%3A%20Dynamic%20and%20Compositional%20Inference-Time%20Personality%20Control%20via%20Activation%20Vector%20Algebra&body=Title%3A%20PERSONA%3A%20Dynamic%20and%20Compositional%20Inference-Time%20Personality%20Control%20via%20Activation%20Vector%20Algebra%0AAuthor%3A%20Xiachong%20Feng%20and%20Liang%20Zhao%20and%20Weihong%20Zhong%20and%20Yichong%20Huang%20and%20Yuxuan%20Gu%20and%20Lingpeng%20Kong%20and%20Xiaocheng%20Feng%20and%20Bing%20Qin%0AAbstract%3A%20Current%20methods%20for%20personality%20control%20in%20Large%20Language%20Models%20rely%20on%20static%20prompting%20or%20expensive%20fine-tuning%2C%20failing%20to%20capture%20the%20dynamic%20and%20compositional%20nature%20of%20human%20traits.%20We%20introduce%20PERSONA%2C%20a%20training-free%20framework%20that%20achieves%20fine-tuning%20level%20performance%20through%20direct%20manipulation%20of%20personality%20vectors%20in%20activation%20space.%20Our%20key%20insight%20is%20that%20personality%20traits%20appear%20as%20extractable%2C%20approximately%20orthogonal%20directions%20in%20the%20model%27s%20representation%20space%20that%20support%20algebraic%20operations.%20The%20framework%20operates%20through%20three%20stages%3A%20Persona-Base%20extracts%20orthogonal%20trait%20vectors%20via%20contrastive%20activation%20analysis%3B%20Persona-Algebra%20enables%20precise%20control%20through%20vector%20arithmetic%20%28scalar%20multiplication%20for%20intensity%2C%20addition%20for%20composition%2C%20subtraction%20for%20suppression%29%3B%20and%20Persona-Flow%20achieves%20context-aware%20adaptation%20by%20dynamically%20composing%20these%20vectors%20during%20inference.%20On%20PersonalityBench%2C%20our%20approach%20achieves%20a%20mean%20score%20of%209.60%2C%20nearly%20matching%20the%20supervised%20fine-tuning%20upper%20bound%20of%209.61%20without%20any%20gradient%20updates.%20On%20our%20proposed%20Persona-Evolve%20benchmark%20for%20dynamic%20personality%20adaptation%2C%20we%20achieve%20up%20to%2091%25%20win%20rates%20across%20diverse%20model%20families.%20These%20results%20provide%20evidence%20that%20aspects%20of%20LLM%20personality%20are%20mathematically%20tractable%2C%20opening%20new%20directions%20for%20interpretable%20and%20efficient%20behavioral%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPERSONA%253A%2520Dynamic%2520and%2520Compositional%2520Inference-Time%2520Personality%2520Control%2520via%2520Activation%2520Vector%2520Algebra%26entry.906535625%3DXiachong%2520Feng%2520and%2520Liang%2520Zhao%2520and%2520Weihong%2520Zhong%2520and%2520Yichong%2520Huang%2520and%2520Yuxuan%2520Gu%2520and%2520Lingpeng%2520Kong%2520and%2520Xiaocheng%2520Feng%2520and%2520Bing%2520Qin%26entry.1292438233%3DCurrent%2520methods%2520for%2520personality%2520control%2520in%2520Large%2520Language%2520Models%2520rely%2520on%2520static%2520prompting%2520or%2520expensive%2520fine-tuning%252C%2520failing%2520to%2520capture%2520the%2520dynamic%2520and%2520compositional%2520nature%2520of%2520human%2520traits.%2520We%2520introduce%2520PERSONA%252C%2520a%2520training-free%2520framework%2520that%2520achieves%2520fine-tuning%2520level%2520performance%2520through%2520direct%2520manipulation%2520of%2520personality%2520vectors%2520in%2520activation%2520space.%2520Our%2520key%2520insight%2520is%2520that%2520personality%2520traits%2520appear%2520as%2520extractable%252C%2520approximately%2520orthogonal%2520directions%2520in%2520the%2520model%2527s%2520representation%2520space%2520that%2520support%2520algebraic%2520operations.%2520The%2520framework%2520operates%2520through%2520three%2520stages%253A%2520Persona-Base%2520extracts%2520orthogonal%2520trait%2520vectors%2520via%2520contrastive%2520activation%2520analysis%253B%2520Persona-Algebra%2520enables%2520precise%2520control%2520through%2520vector%2520arithmetic%2520%2528scalar%2520multiplication%2520for%2520intensity%252C%2520addition%2520for%2520composition%252C%2520subtraction%2520for%2520suppression%2529%253B%2520and%2520Persona-Flow%2520achieves%2520context-aware%2520adaptation%2520by%2520dynamically%2520composing%2520these%2520vectors%2520during%2520inference.%2520On%2520PersonalityBench%252C%2520our%2520approach%2520achieves%2520a%2520mean%2520score%2520of%25209.60%252C%2520nearly%2520matching%2520the%2520supervised%2520fine-tuning%2520upper%2520bound%2520of%25209.61%2520without%2520any%2520gradient%2520updates.%2520On%2520our%2520proposed%2520Persona-Evolve%2520benchmark%2520for%2520dynamic%2520personality%2520adaptation%252C%2520we%2520achieve%2520up%2520to%252091%2525%2520win%2520rates%2520across%2520diverse%2520model%2520families.%2520These%2520results%2520provide%2520evidence%2520that%2520aspects%2520of%2520LLM%2520personality%2520are%2520mathematically%2520tractable%252C%2520opening%2520new%2520directions%2520for%2520interpretable%2520and%2520efficient%2520behavioral%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PERSONA%3A%20Dynamic%20and%20Compositional%20Inference-Time%20Personality%20Control%20via%20Activation%20Vector%20Algebra&entry.906535625=Xiachong%20Feng%20and%20Liang%20Zhao%20and%20Weihong%20Zhong%20and%20Yichong%20Huang%20and%20Yuxuan%20Gu%20and%20Lingpeng%20Kong%20and%20Xiaocheng%20Feng%20and%20Bing%20Qin&entry.1292438233=Current%20methods%20for%20personality%20control%20in%20Large%20Language%20Models%20rely%20on%20static%20prompting%20or%20expensive%20fine-tuning%2C%20failing%20to%20capture%20the%20dynamic%20and%20compositional%20nature%20of%20human%20traits.%20We%20introduce%20PERSONA%2C%20a%20training-free%20framework%20that%20achieves%20fine-tuning%20level%20performance%20through%20direct%20manipulation%20of%20personality%20vectors%20in%20activation%20space.%20Our%20key%20insight%20is%20that%20personality%20traits%20appear%20as%20extractable%2C%20approximately%20orthogonal%20directions%20in%20the%20model%27s%20representation%20space%20that%20support%20algebraic%20operations.%20The%20framework%20operates%20through%20three%20stages%3A%20Persona-Base%20extracts%20orthogonal%20trait%20vectors%20via%20contrastive%20activation%20analysis%3B%20Persona-Algebra%20enables%20precise%20control%20through%20vector%20arithmetic%20%28scalar%20multiplication%20for%20intensity%2C%20addition%20for%20composition%2C%20subtraction%20for%20suppression%29%3B%20and%20Persona-Flow%20achieves%20context-aware%20adaptation%20by%20dynamically%20composing%20these%20vectors%20during%20inference.%20On%20PersonalityBench%2C%20our%20approach%20achieves%20a%20mean%20score%20of%209.60%2C%20nearly%20matching%20the%20supervised%20fine-tuning%20upper%20bound%20of%209.61%20without%20any%20gradient%20updates.%20On%20our%20proposed%20Persona-Evolve%20benchmark%20for%20dynamic%20personality%20adaptation%2C%20we%20achieve%20up%20to%2091%25%20win%20rates%20across%20diverse%20model%20families.%20These%20results%20provide%20evidence%20that%20aspects%20of%20LLM%20personality%20are%20mathematically%20tractable%2C%20opening%20new%20directions%20for%20interpretable%20and%20efficient%20behavioral%20control.&entry.1838667208=http%3A//arxiv.org/abs/2602.15669v1&entry.124074799=Read"},
{"title": "OpenAIs HealthBench in Action: Evaluating an LLM-Based Medical Assistant on Realistic Clinical Queries", "author": "Sandhanakrishnan Ravichandran and Shivesh Kumar and Rogerio Corga Da Silva and Miguel Romano and Reinhard Berkels and Michiel van der Heijden and Olivier Fail and Valentine Emmanuel Gnanapragasam", "abstract": "Evaluating large language models (LLMs) on their ability to generate high-quality, accurate, situationally aware answers to clinical questions requires going beyond conventional benchmarks to assess how these systems behave in complex, high-stakes clinical scenarios. Traditional evaluations are often limited to multiple-choice questions that fail to capture essential competencies such as contextual reasoning, contextual awareness, and uncertainty handling.\n  To address these limitations, we evaluate our agentic RAG-based clinical support assistant, DR. INFO, using HealthBench, a rubric-driven benchmark composed of open-ended, expert-annotated health conversations. On the Hard subset of 1,000 challenging examples, DR. INFO achieves a HealthBench Hard score of 0.68, outperforming leading frontier LLMs including the GPT-5 model family (GPT-5: 0.46, GPT-5.2: 0.42, GPT-5.1: 0.40), Grok 3 (0.23), Gemini 2.5 Pro (0.19), and Claude 3.7 Sonnet (0.02) across all behavioral axes (accuracy, completeness, instruction following, etc.). In a separate 100-sample evaluation against similar agentic RAG assistants (OpenEvidence and Pathway.md, now DoxGPT by Doximity), it maintains a performance lead with a HealthBench Hard score of 0.72.\n  These results highlight the strengths of DR. INFO in communication, instruction following, and accuracy, while also revealing areas for improvement in context awareness and response completeness. Overall, the findings underscore the utility of behavior-level, rubric-based evaluation for building reliable and trustworthy AI-enabled clinical support systems.", "link": "http://arxiv.org/abs/2509.02594v2", "date": "2026-02-17", "relevancy": 1.9797, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenAIs%20HealthBench%20in%20Action%3A%20Evaluating%20an%20LLM-Based%20Medical%20Assistant%20on%20Realistic%20Clinical%20Queries&body=Title%3A%20OpenAIs%20HealthBench%20in%20Action%3A%20Evaluating%20an%20LLM-Based%20Medical%20Assistant%20on%20Realistic%20Clinical%20Queries%0AAuthor%3A%20Sandhanakrishnan%20Ravichandran%20and%20Shivesh%20Kumar%20and%20Rogerio%20Corga%20Da%20Silva%20and%20Miguel%20Romano%20and%20Reinhard%20Berkels%20and%20Michiel%20van%20der%20Heijden%20and%20Olivier%20Fail%20and%20Valentine%20Emmanuel%20Gnanapragasam%0AAbstract%3A%20Evaluating%20large%20language%20models%20%28LLMs%29%20on%20their%20ability%20to%20generate%20high-quality%2C%20accurate%2C%20situationally%20aware%20answers%20to%20clinical%20questions%20requires%20going%20beyond%20conventional%20benchmarks%20to%20assess%20how%20these%20systems%20behave%20in%20complex%2C%20high-stakes%20clinical%20scenarios.%20Traditional%20evaluations%20are%20often%20limited%20to%20multiple-choice%20questions%20that%20fail%20to%20capture%20essential%20competencies%20such%20as%20contextual%20reasoning%2C%20contextual%20awareness%2C%20and%20uncertainty%20handling.%0A%20%20To%20address%20these%20limitations%2C%20we%20evaluate%20our%20agentic%20RAG-based%20clinical%20support%20assistant%2C%20DR.%20INFO%2C%20using%20HealthBench%2C%20a%20rubric-driven%20benchmark%20composed%20of%20open-ended%2C%20expert-annotated%20health%20conversations.%20On%20the%20Hard%20subset%20of%201%2C000%20challenging%20examples%2C%20DR.%20INFO%20achieves%20a%20HealthBench%20Hard%20score%20of%200.68%2C%20outperforming%20leading%20frontier%20LLMs%20including%20the%20GPT-5%20model%20family%20%28GPT-5%3A%200.46%2C%20GPT-5.2%3A%200.42%2C%20GPT-5.1%3A%200.40%29%2C%20Grok%203%20%280.23%29%2C%20Gemini%202.5%20Pro%20%280.19%29%2C%20and%20Claude%203.7%20Sonnet%20%280.02%29%20across%20all%20behavioral%20axes%20%28accuracy%2C%20completeness%2C%20instruction%20following%2C%20etc.%29.%20In%20a%20separate%20100-sample%20evaluation%20against%20similar%20agentic%20RAG%20assistants%20%28OpenEvidence%20and%20Pathway.md%2C%20now%20DoxGPT%20by%20Doximity%29%2C%20it%20maintains%20a%20performance%20lead%20with%20a%20HealthBench%20Hard%20score%20of%200.72.%0A%20%20These%20results%20highlight%20the%20strengths%20of%20DR.%20INFO%20in%20communication%2C%20instruction%20following%2C%20and%20accuracy%2C%20while%20also%20revealing%20areas%20for%20improvement%20in%20context%20awareness%20and%20response%20completeness.%20Overall%2C%20the%20findings%20underscore%20the%20utility%20of%20behavior-level%2C%20rubric-based%20evaluation%20for%20building%20reliable%20and%20trustworthy%20AI-enabled%20clinical%20support%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2509.02594v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenAIs%2520HealthBench%2520in%2520Action%253A%2520Evaluating%2520an%2520LLM-Based%2520Medical%2520Assistant%2520on%2520Realistic%2520Clinical%2520Queries%26entry.906535625%3DSandhanakrishnan%2520Ravichandran%2520and%2520Shivesh%2520Kumar%2520and%2520Rogerio%2520Corga%2520Da%2520Silva%2520and%2520Miguel%2520Romano%2520and%2520Reinhard%2520Berkels%2520and%2520Michiel%2520van%2520der%2520Heijden%2520and%2520Olivier%2520Fail%2520and%2520Valentine%2520Emmanuel%2520Gnanapragasam%26entry.1292438233%3DEvaluating%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520their%2520ability%2520to%2520generate%2520high-quality%252C%2520accurate%252C%2520situationally%2520aware%2520answers%2520to%2520clinical%2520questions%2520requires%2520going%2520beyond%2520conventional%2520benchmarks%2520to%2520assess%2520how%2520these%2520systems%2520behave%2520in%2520complex%252C%2520high-stakes%2520clinical%2520scenarios.%2520Traditional%2520evaluations%2520are%2520often%2520limited%2520to%2520multiple-choice%2520questions%2520that%2520fail%2520to%2520capture%2520essential%2520competencies%2520such%2520as%2520contextual%2520reasoning%252C%2520contextual%2520awareness%252C%2520and%2520uncertainty%2520handling.%250A%2520%2520To%2520address%2520these%2520limitations%252C%2520we%2520evaluate%2520our%2520agentic%2520RAG-based%2520clinical%2520support%2520assistant%252C%2520DR.%2520INFO%252C%2520using%2520HealthBench%252C%2520a%2520rubric-driven%2520benchmark%2520composed%2520of%2520open-ended%252C%2520expert-annotated%2520health%2520conversations.%2520On%2520the%2520Hard%2520subset%2520of%25201%252C000%2520challenging%2520examples%252C%2520DR.%2520INFO%2520achieves%2520a%2520HealthBench%2520Hard%2520score%2520of%25200.68%252C%2520outperforming%2520leading%2520frontier%2520LLMs%2520including%2520the%2520GPT-5%2520model%2520family%2520%2528GPT-5%253A%25200.46%252C%2520GPT-5.2%253A%25200.42%252C%2520GPT-5.1%253A%25200.40%2529%252C%2520Grok%25203%2520%25280.23%2529%252C%2520Gemini%25202.5%2520Pro%2520%25280.19%2529%252C%2520and%2520Claude%25203.7%2520Sonnet%2520%25280.02%2529%2520across%2520all%2520behavioral%2520axes%2520%2528accuracy%252C%2520completeness%252C%2520instruction%2520following%252C%2520etc.%2529.%2520In%2520a%2520separate%2520100-sample%2520evaluation%2520against%2520similar%2520agentic%2520RAG%2520assistants%2520%2528OpenEvidence%2520and%2520Pathway.md%252C%2520now%2520DoxGPT%2520by%2520Doximity%2529%252C%2520it%2520maintains%2520a%2520performance%2520lead%2520with%2520a%2520HealthBench%2520Hard%2520score%2520of%25200.72.%250A%2520%2520These%2520results%2520highlight%2520the%2520strengths%2520of%2520DR.%2520INFO%2520in%2520communication%252C%2520instruction%2520following%252C%2520and%2520accuracy%252C%2520while%2520also%2520revealing%2520areas%2520for%2520improvement%2520in%2520context%2520awareness%2520and%2520response%2520completeness.%2520Overall%252C%2520the%2520findings%2520underscore%2520the%2520utility%2520of%2520behavior-level%252C%2520rubric-based%2520evaluation%2520for%2520building%2520reliable%2520and%2520trustworthy%2520AI-enabled%2520clinical%2520support%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02594v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenAIs%20HealthBench%20in%20Action%3A%20Evaluating%20an%20LLM-Based%20Medical%20Assistant%20on%20Realistic%20Clinical%20Queries&entry.906535625=Sandhanakrishnan%20Ravichandran%20and%20Shivesh%20Kumar%20and%20Rogerio%20Corga%20Da%20Silva%20and%20Miguel%20Romano%20and%20Reinhard%20Berkels%20and%20Michiel%20van%20der%20Heijden%20and%20Olivier%20Fail%20and%20Valentine%20Emmanuel%20Gnanapragasam&entry.1292438233=Evaluating%20large%20language%20models%20%28LLMs%29%20on%20their%20ability%20to%20generate%20high-quality%2C%20accurate%2C%20situationally%20aware%20answers%20to%20clinical%20questions%20requires%20going%20beyond%20conventional%20benchmarks%20to%20assess%20how%20these%20systems%20behave%20in%20complex%2C%20high-stakes%20clinical%20scenarios.%20Traditional%20evaluations%20are%20often%20limited%20to%20multiple-choice%20questions%20that%20fail%20to%20capture%20essential%20competencies%20such%20as%20contextual%20reasoning%2C%20contextual%20awareness%2C%20and%20uncertainty%20handling.%0A%20%20To%20address%20these%20limitations%2C%20we%20evaluate%20our%20agentic%20RAG-based%20clinical%20support%20assistant%2C%20DR.%20INFO%2C%20using%20HealthBench%2C%20a%20rubric-driven%20benchmark%20composed%20of%20open-ended%2C%20expert-annotated%20health%20conversations.%20On%20the%20Hard%20subset%20of%201%2C000%20challenging%20examples%2C%20DR.%20INFO%20achieves%20a%20HealthBench%20Hard%20score%20of%200.68%2C%20outperforming%20leading%20frontier%20LLMs%20including%20the%20GPT-5%20model%20family%20%28GPT-5%3A%200.46%2C%20GPT-5.2%3A%200.42%2C%20GPT-5.1%3A%200.40%29%2C%20Grok%203%20%280.23%29%2C%20Gemini%202.5%20Pro%20%280.19%29%2C%20and%20Claude%203.7%20Sonnet%20%280.02%29%20across%20all%20behavioral%20axes%20%28accuracy%2C%20completeness%2C%20instruction%20following%2C%20etc.%29.%20In%20a%20separate%20100-sample%20evaluation%20against%20similar%20agentic%20RAG%20assistants%20%28OpenEvidence%20and%20Pathway.md%2C%20now%20DoxGPT%20by%20Doximity%29%2C%20it%20maintains%20a%20performance%20lead%20with%20a%20HealthBench%20Hard%20score%20of%200.72.%0A%20%20These%20results%20highlight%20the%20strengths%20of%20DR.%20INFO%20in%20communication%2C%20instruction%20following%2C%20and%20accuracy%2C%20while%20also%20revealing%20areas%20for%20improvement%20in%20context%20awareness%20and%20response%20completeness.%20Overall%2C%20the%20findings%20underscore%20the%20utility%20of%20behavior-level%2C%20rubric-based%20evaluation%20for%20building%20reliable%20and%20trustworthy%20AI-enabled%20clinical%20support%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2509.02594v2&entry.124074799=Read"},
{"title": "The Generative Reasonable Person", "author": "Yonathan A. Arbel", "abstract": "This Article introduces the generative reasonable person, a new tool for estimating how ordinary people judge reasonableness. As claims about AI capabilities often outpace evidence, the Article proceeds empirically: adapting randomized controlled trials to large language models, it replicates three published studies of lay judgment across negligence, consent, and contract interpretation, drawing on nearly 10,000 simulated decisions. The findings reveal that models can replicate subtle patterns that run counter to textbook treatment. Like human subjects, models prioritize social conformity over cost-benefit analysis when assessing negligence, inverting the hierarchy that textbooks teach. They reproduce the paradox that material lies erode consent less than lies about a transaction's essence. And they track lay contract formalism, judging hidden fees more enforceable than fair. For two centuries, scholars have debated whether the reasonable person is empirical or normative, majoritarian or aspirational. But much of this debate assumed a constraint that no longer holds: that lay judgments are expensive to surface, slow to collect, and unavailable at scale. Generative reasonable people loosen that constraint. They offer judges empirical checks on elite intuition, give resource-constrained litigants access to simulated jury feedback, and let regulators pilot-test public comprehension, all at a fraction of survey costs. The reasonable person standard has long functioned as a vessel for judicial intuition precisely because the empirical baseline was missing. With that baseline now available, departures from lay understanding become transparent rather than hidden, a choice to be justified, not a fact to be assumed. Properly cabined, the generative reasonable person may become a dictionary for reasonableness judgments.", "link": "http://arxiv.org/abs/2508.02766v2", "date": "2026-02-17", "relevancy": 1.976, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.499}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4974}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Generative%20Reasonable%20Person&body=Title%3A%20The%20Generative%20Reasonable%20Person%0AAuthor%3A%20Yonathan%20A.%20Arbel%0AAbstract%3A%20This%20Article%20introduces%20the%20generative%20reasonable%20person%2C%20a%20new%20tool%20for%20estimating%20how%20ordinary%20people%20judge%20reasonableness.%20As%20claims%20about%20AI%20capabilities%20often%20outpace%20evidence%2C%20the%20Article%20proceeds%20empirically%3A%20adapting%20randomized%20controlled%20trials%20to%20large%20language%20models%2C%20it%20replicates%20three%20published%20studies%20of%20lay%20judgment%20across%20negligence%2C%20consent%2C%20and%20contract%20interpretation%2C%20drawing%20on%20nearly%2010%2C000%20simulated%20decisions.%20The%20findings%20reveal%20that%20models%20can%20replicate%20subtle%20patterns%20that%20run%20counter%20to%20textbook%20treatment.%20Like%20human%20subjects%2C%20models%20prioritize%20social%20conformity%20over%20cost-benefit%20analysis%20when%20assessing%20negligence%2C%20inverting%20the%20hierarchy%20that%20textbooks%20teach.%20They%20reproduce%20the%20paradox%20that%20material%20lies%20erode%20consent%20less%20than%20lies%20about%20a%20transaction%27s%20essence.%20And%20they%20track%20lay%20contract%20formalism%2C%20judging%20hidden%20fees%20more%20enforceable%20than%20fair.%20For%20two%20centuries%2C%20scholars%20have%20debated%20whether%20the%20reasonable%20person%20is%20empirical%20or%20normative%2C%20majoritarian%20or%20aspirational.%20But%20much%20of%20this%20debate%20assumed%20a%20constraint%20that%20no%20longer%20holds%3A%20that%20lay%20judgments%20are%20expensive%20to%20surface%2C%20slow%20to%20collect%2C%20and%20unavailable%20at%20scale.%20Generative%20reasonable%20people%20loosen%20that%20constraint.%20They%20offer%20judges%20empirical%20checks%20on%20elite%20intuition%2C%20give%20resource-constrained%20litigants%20access%20to%20simulated%20jury%20feedback%2C%20and%20let%20regulators%20pilot-test%20public%20comprehension%2C%20all%20at%20a%20fraction%20of%20survey%20costs.%20The%20reasonable%20person%20standard%20has%20long%20functioned%20as%20a%20vessel%20for%20judicial%20intuition%20precisely%20because%20the%20empirical%20baseline%20was%20missing.%20With%20that%20baseline%20now%20available%2C%20departures%20from%20lay%20understanding%20become%20transparent%20rather%20than%20hidden%2C%20a%20choice%20to%20be%20justified%2C%20not%20a%20fact%20to%20be%20assumed.%20Properly%20cabined%2C%20the%20generative%20reasonable%20person%20may%20become%20a%20dictionary%20for%20reasonableness%20judgments.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Generative%2520Reasonable%2520Person%26entry.906535625%3DYonathan%2520A.%2520Arbel%26entry.1292438233%3DThis%2520Article%2520introduces%2520the%2520generative%2520reasonable%2520person%252C%2520a%2520new%2520tool%2520for%2520estimating%2520how%2520ordinary%2520people%2520judge%2520reasonableness.%2520As%2520claims%2520about%2520AI%2520capabilities%2520often%2520outpace%2520evidence%252C%2520the%2520Article%2520proceeds%2520empirically%253A%2520adapting%2520randomized%2520controlled%2520trials%2520to%2520large%2520language%2520models%252C%2520it%2520replicates%2520three%2520published%2520studies%2520of%2520lay%2520judgment%2520across%2520negligence%252C%2520consent%252C%2520and%2520contract%2520interpretation%252C%2520drawing%2520on%2520nearly%252010%252C000%2520simulated%2520decisions.%2520The%2520findings%2520reveal%2520that%2520models%2520can%2520replicate%2520subtle%2520patterns%2520that%2520run%2520counter%2520to%2520textbook%2520treatment.%2520Like%2520human%2520subjects%252C%2520models%2520prioritize%2520social%2520conformity%2520over%2520cost-benefit%2520analysis%2520when%2520assessing%2520negligence%252C%2520inverting%2520the%2520hierarchy%2520that%2520textbooks%2520teach.%2520They%2520reproduce%2520the%2520paradox%2520that%2520material%2520lies%2520erode%2520consent%2520less%2520than%2520lies%2520about%2520a%2520transaction%2527s%2520essence.%2520And%2520they%2520track%2520lay%2520contract%2520formalism%252C%2520judging%2520hidden%2520fees%2520more%2520enforceable%2520than%2520fair.%2520For%2520two%2520centuries%252C%2520scholars%2520have%2520debated%2520whether%2520the%2520reasonable%2520person%2520is%2520empirical%2520or%2520normative%252C%2520majoritarian%2520or%2520aspirational.%2520But%2520much%2520of%2520this%2520debate%2520assumed%2520a%2520constraint%2520that%2520no%2520longer%2520holds%253A%2520that%2520lay%2520judgments%2520are%2520expensive%2520to%2520surface%252C%2520slow%2520to%2520collect%252C%2520and%2520unavailable%2520at%2520scale.%2520Generative%2520reasonable%2520people%2520loosen%2520that%2520constraint.%2520They%2520offer%2520judges%2520empirical%2520checks%2520on%2520elite%2520intuition%252C%2520give%2520resource-constrained%2520litigants%2520access%2520to%2520simulated%2520jury%2520feedback%252C%2520and%2520let%2520regulators%2520pilot-test%2520public%2520comprehension%252C%2520all%2520at%2520a%2520fraction%2520of%2520survey%2520costs.%2520The%2520reasonable%2520person%2520standard%2520has%2520long%2520functioned%2520as%2520a%2520vessel%2520for%2520judicial%2520intuition%2520precisely%2520because%2520the%2520empirical%2520baseline%2520was%2520missing.%2520With%2520that%2520baseline%2520now%2520available%252C%2520departures%2520from%2520lay%2520understanding%2520become%2520transparent%2520rather%2520than%2520hidden%252C%2520a%2520choice%2520to%2520be%2520justified%252C%2520not%2520a%2520fact%2520to%2520be%2520assumed.%2520Properly%2520cabined%252C%2520the%2520generative%2520reasonable%2520person%2520may%2520become%2520a%2520dictionary%2520for%2520reasonableness%2520judgments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Generative%20Reasonable%20Person&entry.906535625=Yonathan%20A.%20Arbel&entry.1292438233=This%20Article%20introduces%20the%20generative%20reasonable%20person%2C%20a%20new%20tool%20for%20estimating%20how%20ordinary%20people%20judge%20reasonableness.%20As%20claims%20about%20AI%20capabilities%20often%20outpace%20evidence%2C%20the%20Article%20proceeds%20empirically%3A%20adapting%20randomized%20controlled%20trials%20to%20large%20language%20models%2C%20it%20replicates%20three%20published%20studies%20of%20lay%20judgment%20across%20negligence%2C%20consent%2C%20and%20contract%20interpretation%2C%20drawing%20on%20nearly%2010%2C000%20simulated%20decisions.%20The%20findings%20reveal%20that%20models%20can%20replicate%20subtle%20patterns%20that%20run%20counter%20to%20textbook%20treatment.%20Like%20human%20subjects%2C%20models%20prioritize%20social%20conformity%20over%20cost-benefit%20analysis%20when%20assessing%20negligence%2C%20inverting%20the%20hierarchy%20that%20textbooks%20teach.%20They%20reproduce%20the%20paradox%20that%20material%20lies%20erode%20consent%20less%20than%20lies%20about%20a%20transaction%27s%20essence.%20And%20they%20track%20lay%20contract%20formalism%2C%20judging%20hidden%20fees%20more%20enforceable%20than%20fair.%20For%20two%20centuries%2C%20scholars%20have%20debated%20whether%20the%20reasonable%20person%20is%20empirical%20or%20normative%2C%20majoritarian%20or%20aspirational.%20But%20much%20of%20this%20debate%20assumed%20a%20constraint%20that%20no%20longer%20holds%3A%20that%20lay%20judgments%20are%20expensive%20to%20surface%2C%20slow%20to%20collect%2C%20and%20unavailable%20at%20scale.%20Generative%20reasonable%20people%20loosen%20that%20constraint.%20They%20offer%20judges%20empirical%20checks%20on%20elite%20intuition%2C%20give%20resource-constrained%20litigants%20access%20to%20simulated%20jury%20feedback%2C%20and%20let%20regulators%20pilot-test%20public%20comprehension%2C%20all%20at%20a%20fraction%20of%20survey%20costs.%20The%20reasonable%20person%20standard%20has%20long%20functioned%20as%20a%20vessel%20for%20judicial%20intuition%20precisely%20because%20the%20empirical%20baseline%20was%20missing.%20With%20that%20baseline%20now%20available%2C%20departures%20from%20lay%20understanding%20become%20transparent%20rather%20than%20hidden%2C%20a%20choice%20to%20be%20justified%2C%20not%20a%20fact%20to%20be%20assumed.%20Properly%20cabined%2C%20the%20generative%20reasonable%20person%20may%20become%20a%20dictionary%20for%20reasonableness%20judgments.&entry.1838667208=http%3A//arxiv.org/abs/2508.02766v2&entry.124074799=Read"},
{"title": "Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics", "author": "Anna Zimmel and Paul Setinek and Gianluca Galletti and Johannes Brandstetter and Werner Zellinger", "abstract": "Machine learning surrogates are increasingly used in engineering to accelerate costly simulations, yet distribution shifts between training and deployment often cause severe performance degradation (e.g., unseen geometries or configurations). Test-Time Adaptation (TTA) can mitigate such shifts, but existing methods are largely developed for lower-dimensional classification with structured outputs and visually aligned input-output relationships, making them unstable for the high-dimensional, unstructured and regression problems common in simulation. We address this challenge by proposing a TTA framework based on storing maximally informative (D-optimal) statistics, which jointly enables stable adaptation and principled parameter selection at test time. When applied to pretrained simulation surrogates, our method yields up to 7% out-of-distribution improvements at negligible computational cost. To the best of our knowledge, this is the first systematic demonstration of effective TTA for high-dimensional simulation regression and generative design optimization, validated on the SIMSHIFT and EngiBench benchmarks.", "link": "http://arxiv.org/abs/2602.15820v1", "date": "2026-02-17", "relevancy": 1.9736, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.502}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stabilizing%20Test-Time%20Adaptation%20of%20High-Dimensional%20Simulation%20Surrogates%20via%20D-Optimal%20Statistics&body=Title%3A%20Stabilizing%20Test-Time%20Adaptation%20of%20High-Dimensional%20Simulation%20Surrogates%20via%20D-Optimal%20Statistics%0AAuthor%3A%20Anna%20Zimmel%20and%20Paul%20Setinek%20and%20Gianluca%20Galletti%20and%20Johannes%20Brandstetter%20and%20Werner%20Zellinger%0AAbstract%3A%20Machine%20learning%20surrogates%20are%20increasingly%20used%20in%20engineering%20to%20accelerate%20costly%20simulations%2C%20yet%20distribution%20shifts%20between%20training%20and%20deployment%20often%20cause%20severe%20performance%20degradation%20%28e.g.%2C%20unseen%20geometries%20or%20configurations%29.%20Test-Time%20Adaptation%20%28TTA%29%20can%20mitigate%20such%20shifts%2C%20but%20existing%20methods%20are%20largely%20developed%20for%20lower-dimensional%20classification%20with%20structured%20outputs%20and%20visually%20aligned%20input-output%20relationships%2C%20making%20them%20unstable%20for%20the%20high-dimensional%2C%20unstructured%20and%20regression%20problems%20common%20in%20simulation.%20We%20address%20this%20challenge%20by%20proposing%20a%20TTA%20framework%20based%20on%20storing%20maximally%20informative%20%28D-optimal%29%20statistics%2C%20which%20jointly%20enables%20stable%20adaptation%20and%20principled%20parameter%20selection%20at%20test%20time.%20When%20applied%20to%20pretrained%20simulation%20surrogates%2C%20our%20method%20yields%20up%20to%207%25%20out-of-distribution%20improvements%20at%20negligible%20computational%20cost.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20systematic%20demonstration%20of%20effective%20TTA%20for%20high-dimensional%20simulation%20regression%20and%20generative%20design%20optimization%2C%20validated%20on%20the%20SIMSHIFT%20and%20EngiBench%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStabilizing%2520Test-Time%2520Adaptation%2520of%2520High-Dimensional%2520Simulation%2520Surrogates%2520via%2520D-Optimal%2520Statistics%26entry.906535625%3DAnna%2520Zimmel%2520and%2520Paul%2520Setinek%2520and%2520Gianluca%2520Galletti%2520and%2520Johannes%2520Brandstetter%2520and%2520Werner%2520Zellinger%26entry.1292438233%3DMachine%2520learning%2520surrogates%2520are%2520increasingly%2520used%2520in%2520engineering%2520to%2520accelerate%2520costly%2520simulations%252C%2520yet%2520distribution%2520shifts%2520between%2520training%2520and%2520deployment%2520often%2520cause%2520severe%2520performance%2520degradation%2520%2528e.g.%252C%2520unseen%2520geometries%2520or%2520configurations%2529.%2520Test-Time%2520Adaptation%2520%2528TTA%2529%2520can%2520mitigate%2520such%2520shifts%252C%2520but%2520existing%2520methods%2520are%2520largely%2520developed%2520for%2520lower-dimensional%2520classification%2520with%2520structured%2520outputs%2520and%2520visually%2520aligned%2520input-output%2520relationships%252C%2520making%2520them%2520unstable%2520for%2520the%2520high-dimensional%252C%2520unstructured%2520and%2520regression%2520problems%2520common%2520in%2520simulation.%2520We%2520address%2520this%2520challenge%2520by%2520proposing%2520a%2520TTA%2520framework%2520based%2520on%2520storing%2520maximally%2520informative%2520%2528D-optimal%2529%2520statistics%252C%2520which%2520jointly%2520enables%2520stable%2520adaptation%2520and%2520principled%2520parameter%2520selection%2520at%2520test%2520time.%2520When%2520applied%2520to%2520pretrained%2520simulation%2520surrogates%252C%2520our%2520method%2520yields%2520up%2520to%25207%2525%2520out-of-distribution%2520improvements%2520at%2520negligible%2520computational%2520cost.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520systematic%2520demonstration%2520of%2520effective%2520TTA%2520for%2520high-dimensional%2520simulation%2520regression%2520and%2520generative%2520design%2520optimization%252C%2520validated%2520on%2520the%2520SIMSHIFT%2520and%2520EngiBench%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stabilizing%20Test-Time%20Adaptation%20of%20High-Dimensional%20Simulation%20Surrogates%20via%20D-Optimal%20Statistics&entry.906535625=Anna%20Zimmel%20and%20Paul%20Setinek%20and%20Gianluca%20Galletti%20and%20Johannes%20Brandstetter%20and%20Werner%20Zellinger&entry.1292438233=Machine%20learning%20surrogates%20are%20increasingly%20used%20in%20engineering%20to%20accelerate%20costly%20simulations%2C%20yet%20distribution%20shifts%20between%20training%20and%20deployment%20often%20cause%20severe%20performance%20degradation%20%28e.g.%2C%20unseen%20geometries%20or%20configurations%29.%20Test-Time%20Adaptation%20%28TTA%29%20can%20mitigate%20such%20shifts%2C%20but%20existing%20methods%20are%20largely%20developed%20for%20lower-dimensional%20classification%20with%20structured%20outputs%20and%20visually%20aligned%20input-output%20relationships%2C%20making%20them%20unstable%20for%20the%20high-dimensional%2C%20unstructured%20and%20regression%20problems%20common%20in%20simulation.%20We%20address%20this%20challenge%20by%20proposing%20a%20TTA%20framework%20based%20on%20storing%20maximally%20informative%20%28D-optimal%29%20statistics%2C%20which%20jointly%20enables%20stable%20adaptation%20and%20principled%20parameter%20selection%20at%20test%20time.%20When%20applied%20to%20pretrained%20simulation%20surrogates%2C%20our%20method%20yields%20up%20to%207%25%20out-of-distribution%20improvements%20at%20negligible%20computational%20cost.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20systematic%20demonstration%20of%20effective%20TTA%20for%20high-dimensional%20simulation%20regression%20and%20generative%20design%20optimization%2C%20validated%20on%20the%20SIMSHIFT%20and%20EngiBench%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2602.15820v1&entry.124074799=Read"},
{"title": "FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing", "author": "Bar\u0131\u015f B\u00fcy\u00fckta\u015f and Jonas Klotz and Beg\u00fcm Demir", "abstract": "Federated learning (FL) enables the collaborative training of deep neural networks across decentralized data archives (i.e., clients), where each client stores data locally and only shares model updates with a central server. This makes FL a suitable learning paradigm for remote sensing (RS) image classification tasks, where data centralization may be restricted due to legal and privacy constraints. However, a key challenge in applying FL to RS tasks is the communication overhead caused by the frequent exchange of large model updates between clients and the central server. To address this issue, in this paper we propose a novel strategy (denoted as FedX) that uses explanation-guided pruning to reduce communication overhead by minimizing the size of the transmitted models without compromising performance. FedX leverages backpropagation-based explanation methods to estimate the task-specific importance of model components and prunes the least relevant ones at the central server. The resulting sparse global model is then sent to clients, substantially reducing communication overhead. We evaluate FedX on multi-label scene classification using the BigEarthNet-S2 dataset and single-label scene classification using the EuroSAT dataset. Experimental results show the success of FedX in significantly reducing the number of shared model parameters while enhancing the generalization capability of the global model, compared to both unpruned model and state-of-the-art pruning methods. The code of FedX will be available at https://git.tu-berlin.de/rsim/FedX.", "link": "http://arxiv.org/abs/2508.06256v2", "date": "2026-02-17", "relevancy": 1.9714, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedX%3A%20Explanation-Guided%20Pruning%20for%20Communication-Efficient%20Federated%20Learning%20in%20Remote%20Sensing&body=Title%3A%20FedX%3A%20Explanation-Guided%20Pruning%20for%20Communication-Efficient%20Federated%20Learning%20in%20Remote%20Sensing%0AAuthor%3A%20Bar%C4%B1%C5%9F%20B%C3%BCy%C3%BCkta%C5%9F%20and%20Jonas%20Klotz%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20Federated%20learning%20%28FL%29%20enables%20the%20collaborative%20training%20of%20deep%20neural%20networks%20across%20decentralized%20data%20archives%20%28i.e.%2C%20clients%29%2C%20where%20each%20client%20stores%20data%20locally%20and%20only%20shares%20model%20updates%20with%20a%20central%20server.%20This%20makes%20FL%20a%20suitable%20learning%20paradigm%20for%20remote%20sensing%20%28RS%29%20image%20classification%20tasks%2C%20where%20data%20centralization%20may%20be%20restricted%20due%20to%20legal%20and%20privacy%20constraints.%20However%2C%20a%20key%20challenge%20in%20applying%20FL%20to%20RS%20tasks%20is%20the%20communication%20overhead%20caused%20by%20the%20frequent%20exchange%20of%20large%20model%20updates%20between%20clients%20and%20the%20central%20server.%20To%20address%20this%20issue%2C%20in%20this%20paper%20we%20propose%20a%20novel%20strategy%20%28denoted%20as%20FedX%29%20that%20uses%20explanation-guided%20pruning%20to%20reduce%20communication%20overhead%20by%20minimizing%20the%20size%20of%20the%20transmitted%20models%20without%20compromising%20performance.%20FedX%20leverages%20backpropagation-based%20explanation%20methods%20to%20estimate%20the%20task-specific%20importance%20of%20model%20components%20and%20prunes%20the%20least%20relevant%20ones%20at%20the%20central%20server.%20The%20resulting%20sparse%20global%20model%20is%20then%20sent%20to%20clients%2C%20substantially%20reducing%20communication%20overhead.%20We%20evaluate%20FedX%20on%20multi-label%20scene%20classification%20using%20the%20BigEarthNet-S2%20dataset%20and%20single-label%20scene%20classification%20using%20the%20EuroSAT%20dataset.%20Experimental%20results%20show%20the%20success%20of%20FedX%20in%20significantly%20reducing%20the%20number%20of%20shared%20model%20parameters%20while%20enhancing%20the%20generalization%20capability%20of%20the%20global%20model%2C%20compared%20to%20both%20unpruned%20model%20and%20state-of-the-art%20pruning%20methods.%20The%20code%20of%20FedX%20will%20be%20available%20at%20https%3A//git.tu-berlin.de/rsim/FedX.%0ALink%3A%20http%3A//arxiv.org/abs/2508.06256v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedX%253A%2520Explanation-Guided%2520Pruning%2520for%2520Communication-Efficient%2520Federated%2520Learning%2520in%2520Remote%2520Sensing%26entry.906535625%3DBar%25C4%25B1%25C5%259F%2520B%25C3%25BCy%25C3%25BCkta%25C5%259F%2520and%2520Jonas%2520Klotz%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3DFederated%2520learning%2520%2528FL%2529%2520enables%2520the%2520collaborative%2520training%2520of%2520deep%2520neural%2520networks%2520across%2520decentralized%2520data%2520archives%2520%2528i.e.%252C%2520clients%2529%252C%2520where%2520each%2520client%2520stores%2520data%2520locally%2520and%2520only%2520shares%2520model%2520updates%2520with%2520a%2520central%2520server.%2520This%2520makes%2520FL%2520a%2520suitable%2520learning%2520paradigm%2520for%2520remote%2520sensing%2520%2528RS%2529%2520image%2520classification%2520tasks%252C%2520where%2520data%2520centralization%2520may%2520be%2520restricted%2520due%2520to%2520legal%2520and%2520privacy%2520constraints.%2520However%252C%2520a%2520key%2520challenge%2520in%2520applying%2520FL%2520to%2520RS%2520tasks%2520is%2520the%2520communication%2520overhead%2520caused%2520by%2520the%2520frequent%2520exchange%2520of%2520large%2520model%2520updates%2520between%2520clients%2520and%2520the%2520central%2520server.%2520To%2520address%2520this%2520issue%252C%2520in%2520this%2520paper%2520we%2520propose%2520a%2520novel%2520strategy%2520%2528denoted%2520as%2520FedX%2529%2520that%2520uses%2520explanation-guided%2520pruning%2520to%2520reduce%2520communication%2520overhead%2520by%2520minimizing%2520the%2520size%2520of%2520the%2520transmitted%2520models%2520without%2520compromising%2520performance.%2520FedX%2520leverages%2520backpropagation-based%2520explanation%2520methods%2520to%2520estimate%2520the%2520task-specific%2520importance%2520of%2520model%2520components%2520and%2520prunes%2520the%2520least%2520relevant%2520ones%2520at%2520the%2520central%2520server.%2520The%2520resulting%2520sparse%2520global%2520model%2520is%2520then%2520sent%2520to%2520clients%252C%2520substantially%2520reducing%2520communication%2520overhead.%2520We%2520evaluate%2520FedX%2520on%2520multi-label%2520scene%2520classification%2520using%2520the%2520BigEarthNet-S2%2520dataset%2520and%2520single-label%2520scene%2520classification%2520using%2520the%2520EuroSAT%2520dataset.%2520Experimental%2520results%2520show%2520the%2520success%2520of%2520FedX%2520in%2520significantly%2520reducing%2520the%2520number%2520of%2520shared%2520model%2520parameters%2520while%2520enhancing%2520the%2520generalization%2520capability%2520of%2520the%2520global%2520model%252C%2520compared%2520to%2520both%2520unpruned%2520model%2520and%2520state-of-the-art%2520pruning%2520methods.%2520The%2520code%2520of%2520FedX%2520will%2520be%2520available%2520at%2520https%253A//git.tu-berlin.de/rsim/FedX.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06256v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedX%3A%20Explanation-Guided%20Pruning%20for%20Communication-Efficient%20Federated%20Learning%20in%20Remote%20Sensing&entry.906535625=Bar%C4%B1%C5%9F%20B%C3%BCy%C3%BCkta%C5%9F%20and%20Jonas%20Klotz%20and%20Beg%C3%BCm%20Demir&entry.1292438233=Federated%20learning%20%28FL%29%20enables%20the%20collaborative%20training%20of%20deep%20neural%20networks%20across%20decentralized%20data%20archives%20%28i.e.%2C%20clients%29%2C%20where%20each%20client%20stores%20data%20locally%20and%20only%20shares%20model%20updates%20with%20a%20central%20server.%20This%20makes%20FL%20a%20suitable%20learning%20paradigm%20for%20remote%20sensing%20%28RS%29%20image%20classification%20tasks%2C%20where%20data%20centralization%20may%20be%20restricted%20due%20to%20legal%20and%20privacy%20constraints.%20However%2C%20a%20key%20challenge%20in%20applying%20FL%20to%20RS%20tasks%20is%20the%20communication%20overhead%20caused%20by%20the%20frequent%20exchange%20of%20large%20model%20updates%20between%20clients%20and%20the%20central%20server.%20To%20address%20this%20issue%2C%20in%20this%20paper%20we%20propose%20a%20novel%20strategy%20%28denoted%20as%20FedX%29%20that%20uses%20explanation-guided%20pruning%20to%20reduce%20communication%20overhead%20by%20minimizing%20the%20size%20of%20the%20transmitted%20models%20without%20compromising%20performance.%20FedX%20leverages%20backpropagation-based%20explanation%20methods%20to%20estimate%20the%20task-specific%20importance%20of%20model%20components%20and%20prunes%20the%20least%20relevant%20ones%20at%20the%20central%20server.%20The%20resulting%20sparse%20global%20model%20is%20then%20sent%20to%20clients%2C%20substantially%20reducing%20communication%20overhead.%20We%20evaluate%20FedX%20on%20multi-label%20scene%20classification%20using%20the%20BigEarthNet-S2%20dataset%20and%20single-label%20scene%20classification%20using%20the%20EuroSAT%20dataset.%20Experimental%20results%20show%20the%20success%20of%20FedX%20in%20significantly%20reducing%20the%20number%20of%20shared%20model%20parameters%20while%20enhancing%20the%20generalization%20capability%20of%20the%20global%20model%2C%20compared%20to%20both%20unpruned%20model%20and%20state-of-the-art%20pruning%20methods.%20The%20code%20of%20FedX%20will%20be%20available%20at%20https%3A//git.tu-berlin.de/rsim/FedX.&entry.1838667208=http%3A//arxiv.org/abs/2508.06256v2&entry.124074799=Read"},
{"title": "Hunt Globally: Wide Search AI Agents for Drug Asset Scouting in Investing, Business Development, and Competitive Intelligence", "author": "Alisa Vinogradova and Vlad Vinogradov and Luba Greenwood and Ilya Yasny and Dmitry Kobyzev and Shoman Kasbekar and Kong Nguyen and Dmitrii Radkevich and Roman Doronin and Andrey Doronichev", "abstract": "Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests that over 85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total. A growing share of scholarly output is also non-U.S. Industry estimates put China at 30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface \"under-the-radar\" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high recall discovery across heterogeneous, multilingual sources without hallucination. We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real-deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. On this benchmark, our Bioptic Agent achieves 79.7% F1 score, outperforming Claude Opus 4.6 (56.2%), Gemini 3 Pro + Deep Research (50.6%), OpenAI GPT-5.2 Pro (46.6%), Perplexity Deep Research (44.2%), and Exa Websets (26.9%). Performance improves steeply with additional compute, supporting the view that more compute yields better results.", "link": "http://arxiv.org/abs/2602.15019v2", "date": "2026-02-17", "relevancy": 1.9605, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hunt%20Globally%3A%20Wide%20Search%20AI%20Agents%20for%20Drug%20Asset%20Scouting%20in%20Investing%2C%20Business%20Development%2C%20and%20Competitive%20Intelligence&body=Title%3A%20Hunt%20Globally%3A%20Wide%20Search%20AI%20Agents%20for%20Drug%20Asset%20Scouting%20in%20Investing%2C%20Business%20Development%2C%20and%20Competitive%20Intelligence%0AAuthor%3A%20Alisa%20Vinogradova%20and%20Vlad%20Vinogradov%20and%20Luba%20Greenwood%20and%20Ilya%20Yasny%20and%20Dmitry%20Kobyzev%20and%20Shoman%20Kasbekar%20and%20Kong%20Nguyen%20and%20Dmitrii%20Radkevich%20and%20Roman%20Doronin%20and%20Andrey%20Doronichev%0AAbstract%3A%20Bio-pharmaceutical%20innovation%20has%20shifted%3A%20many%20new%20drug%20assets%20now%20originate%20outside%20the%20United%20States%20and%20are%20disclosed%20primarily%20via%20regional%2C%20non-English%20channels.%20Recent%20data%20suggests%20that%20over%2085%25%20of%20patent%20filings%20originate%20outside%20the%20U.S.%2C%20with%20China%20accounting%20for%20nearly%20half%20of%20the%20global%20total.%20A%20growing%20share%20of%20scholarly%20output%20is%20also%20non-U.S.%20Industry%20estimates%20put%20China%20at%2030%25%20of%20global%20drug%20development%2C%20spanning%201%2C200%2B%20novel%20candidates.%20In%20this%20high-stakes%20environment%2C%20failing%20to%20surface%20%22under-the-radar%22%20assets%20creates%20multi-billion-dollar%20risk%20for%20investors%20and%20business%20development%20teams%2C%20making%20asset%20scouting%20a%20coverage-critical%20competition%20where%20speed%20and%20completeness%20drive%20value.%20Yet%20today%27s%20Deep%20Research%20AI%20agents%20still%20lag%20human%20experts%20in%20achieving%20high%20recall%20discovery%20across%20heterogeneous%2C%20multilingual%20sources%20without%20hallucination.%20We%20propose%20a%20benchmarking%20methodology%20for%20drug%20asset%20scouting%20and%20a%20tuned%2C%20tree-based%20self-learning%20Bioptic%20Agent%20aimed%20at%20complete%2C%20non-hallucinated%20scouting.%20We%20construct%20a%20challenging%20completeness%20benchmark%20using%20a%20multilingual%20multi-agent%20pipeline%3A%20complex%20user%20queries%20paired%20with%20ground-truth%20assets%20that%20are%20largely%20outside%20U.S.-centric%20radar.%20To%20reflect%20real-deal%20complexity%2C%20we%20collected%20screening%20queries%20from%20expert%20investors%2C%20BD%2C%20and%20VC%20professionals%20and%20used%20them%20as%20priors%20to%20conditionally%20generate%20benchmark%20queries.%20For%20grading%2C%20we%20use%20LLM-as-judge%20evaluation%20calibrated%20to%20expert%20opinions.%20On%20this%20benchmark%2C%20our%20Bioptic%20Agent%20achieves%2079.7%25%20F1%20score%2C%20outperforming%20Claude%20Opus%204.6%20%2856.2%25%29%2C%20Gemini%203%20Pro%20%2B%20Deep%20Research%20%2850.6%25%29%2C%20OpenAI%20GPT-5.2%20Pro%20%2846.6%25%29%2C%20Perplexity%20Deep%20Research%20%2844.2%25%29%2C%20and%20Exa%20Websets%20%2826.9%25%29.%20Performance%20improves%20steeply%20with%20additional%20compute%2C%20supporting%20the%20view%20that%20more%20compute%20yields%20better%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15019v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunt%2520Globally%253A%2520Wide%2520Search%2520AI%2520Agents%2520for%2520Drug%2520Asset%2520Scouting%2520in%2520Investing%252C%2520Business%2520Development%252C%2520and%2520Competitive%2520Intelligence%26entry.906535625%3DAlisa%2520Vinogradova%2520and%2520Vlad%2520Vinogradov%2520and%2520Luba%2520Greenwood%2520and%2520Ilya%2520Yasny%2520and%2520Dmitry%2520Kobyzev%2520and%2520Shoman%2520Kasbekar%2520and%2520Kong%2520Nguyen%2520and%2520Dmitrii%2520Radkevich%2520and%2520Roman%2520Doronin%2520and%2520Andrey%2520Doronichev%26entry.1292438233%3DBio-pharmaceutical%2520innovation%2520has%2520shifted%253A%2520many%2520new%2520drug%2520assets%2520now%2520originate%2520outside%2520the%2520United%2520States%2520and%2520are%2520disclosed%2520primarily%2520via%2520regional%252C%2520non-English%2520channels.%2520Recent%2520data%2520suggests%2520that%2520over%252085%2525%2520of%2520patent%2520filings%2520originate%2520outside%2520the%2520U.S.%252C%2520with%2520China%2520accounting%2520for%2520nearly%2520half%2520of%2520the%2520global%2520total.%2520A%2520growing%2520share%2520of%2520scholarly%2520output%2520is%2520also%2520non-U.S.%2520Industry%2520estimates%2520put%2520China%2520at%252030%2525%2520of%2520global%2520drug%2520development%252C%2520spanning%25201%252C200%252B%2520novel%2520candidates.%2520In%2520this%2520high-stakes%2520environment%252C%2520failing%2520to%2520surface%2520%2522under-the-radar%2522%2520assets%2520creates%2520multi-billion-dollar%2520risk%2520for%2520investors%2520and%2520business%2520development%2520teams%252C%2520making%2520asset%2520scouting%2520a%2520coverage-critical%2520competition%2520where%2520speed%2520and%2520completeness%2520drive%2520value.%2520Yet%2520today%2527s%2520Deep%2520Research%2520AI%2520agents%2520still%2520lag%2520human%2520experts%2520in%2520achieving%2520high%2520recall%2520discovery%2520across%2520heterogeneous%252C%2520multilingual%2520sources%2520without%2520hallucination.%2520We%2520propose%2520a%2520benchmarking%2520methodology%2520for%2520drug%2520asset%2520scouting%2520and%2520a%2520tuned%252C%2520tree-based%2520self-learning%2520Bioptic%2520Agent%2520aimed%2520at%2520complete%252C%2520non-hallucinated%2520scouting.%2520We%2520construct%2520a%2520challenging%2520completeness%2520benchmark%2520using%2520a%2520multilingual%2520multi-agent%2520pipeline%253A%2520complex%2520user%2520queries%2520paired%2520with%2520ground-truth%2520assets%2520that%2520are%2520largely%2520outside%2520U.S.-centric%2520radar.%2520To%2520reflect%2520real-deal%2520complexity%252C%2520we%2520collected%2520screening%2520queries%2520from%2520expert%2520investors%252C%2520BD%252C%2520and%2520VC%2520professionals%2520and%2520used%2520them%2520as%2520priors%2520to%2520conditionally%2520generate%2520benchmark%2520queries.%2520For%2520grading%252C%2520we%2520use%2520LLM-as-judge%2520evaluation%2520calibrated%2520to%2520expert%2520opinions.%2520On%2520this%2520benchmark%252C%2520our%2520Bioptic%2520Agent%2520achieves%252079.7%2525%2520F1%2520score%252C%2520outperforming%2520Claude%2520Opus%25204.6%2520%252856.2%2525%2529%252C%2520Gemini%25203%2520Pro%2520%252B%2520Deep%2520Research%2520%252850.6%2525%2529%252C%2520OpenAI%2520GPT-5.2%2520Pro%2520%252846.6%2525%2529%252C%2520Perplexity%2520Deep%2520Research%2520%252844.2%2525%2529%252C%2520and%2520Exa%2520Websets%2520%252826.9%2525%2529.%2520Performance%2520improves%2520steeply%2520with%2520additional%2520compute%252C%2520supporting%2520the%2520view%2520that%2520more%2520compute%2520yields%2520better%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15019v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hunt%20Globally%3A%20Wide%20Search%20AI%20Agents%20for%20Drug%20Asset%20Scouting%20in%20Investing%2C%20Business%20Development%2C%20and%20Competitive%20Intelligence&entry.906535625=Alisa%20Vinogradova%20and%20Vlad%20Vinogradov%20and%20Luba%20Greenwood%20and%20Ilya%20Yasny%20and%20Dmitry%20Kobyzev%20and%20Shoman%20Kasbekar%20and%20Kong%20Nguyen%20and%20Dmitrii%20Radkevich%20and%20Roman%20Doronin%20and%20Andrey%20Doronichev&entry.1292438233=Bio-pharmaceutical%20innovation%20has%20shifted%3A%20many%20new%20drug%20assets%20now%20originate%20outside%20the%20United%20States%20and%20are%20disclosed%20primarily%20via%20regional%2C%20non-English%20channels.%20Recent%20data%20suggests%20that%20over%2085%25%20of%20patent%20filings%20originate%20outside%20the%20U.S.%2C%20with%20China%20accounting%20for%20nearly%20half%20of%20the%20global%20total.%20A%20growing%20share%20of%20scholarly%20output%20is%20also%20non-U.S.%20Industry%20estimates%20put%20China%20at%2030%25%20of%20global%20drug%20development%2C%20spanning%201%2C200%2B%20novel%20candidates.%20In%20this%20high-stakes%20environment%2C%20failing%20to%20surface%20%22under-the-radar%22%20assets%20creates%20multi-billion-dollar%20risk%20for%20investors%20and%20business%20development%20teams%2C%20making%20asset%20scouting%20a%20coverage-critical%20competition%20where%20speed%20and%20completeness%20drive%20value.%20Yet%20today%27s%20Deep%20Research%20AI%20agents%20still%20lag%20human%20experts%20in%20achieving%20high%20recall%20discovery%20across%20heterogeneous%2C%20multilingual%20sources%20without%20hallucination.%20We%20propose%20a%20benchmarking%20methodology%20for%20drug%20asset%20scouting%20and%20a%20tuned%2C%20tree-based%20self-learning%20Bioptic%20Agent%20aimed%20at%20complete%2C%20non-hallucinated%20scouting.%20We%20construct%20a%20challenging%20completeness%20benchmark%20using%20a%20multilingual%20multi-agent%20pipeline%3A%20complex%20user%20queries%20paired%20with%20ground-truth%20assets%20that%20are%20largely%20outside%20U.S.-centric%20radar.%20To%20reflect%20real-deal%20complexity%2C%20we%20collected%20screening%20queries%20from%20expert%20investors%2C%20BD%2C%20and%20VC%20professionals%20and%20used%20them%20as%20priors%20to%20conditionally%20generate%20benchmark%20queries.%20For%20grading%2C%20we%20use%20LLM-as-judge%20evaluation%20calibrated%20to%20expert%20opinions.%20On%20this%20benchmark%2C%20our%20Bioptic%20Agent%20achieves%2079.7%25%20F1%20score%2C%20outperforming%20Claude%20Opus%204.6%20%2856.2%25%29%2C%20Gemini%203%20Pro%20%2B%20Deep%20Research%20%2850.6%25%29%2C%20OpenAI%20GPT-5.2%20Pro%20%2846.6%25%29%2C%20Perplexity%20Deep%20Research%20%2844.2%25%29%2C%20and%20Exa%20Websets%20%2826.9%25%29.%20Performance%20improves%20steeply%20with%20additional%20compute%2C%20supporting%20the%20view%20that%20more%20compute%20yields%20better%20results.&entry.1838667208=http%3A//arxiv.org/abs/2602.15019v2&entry.124074799=Read"},
{"title": "Towards Human-AI Accessibility Mapping in India: VLM-Guided Annotations and POI-Centric Analysis in Chandigarh", "author": "Varchita Lalwani and Utkarsh Agarwal and Michael Saugstad and Manish Kumar and Jon E. Froehlich and Anupam Sobti", "abstract": "Project Sidewalk is a web-based platform that enables crowdsourcing accessibility of sidewalks at city-scale by virtually walking through city streets using Google Street View. The tool has been used in 40 cities across the world, including the US, Mexico, Chile, and Europe. In this paper, we describe adaptation efforts to enable deployment in Chandigarh, India, including modifying annotation types, provided examples, and integrating VLM-based mission guidance, which adapts instructions based on a street scene and metadata analysis. Our evaluation with 3 annotators indicates the utility of AI-mission guidance with an average score of 4.66. Using this adapted Project Sidewalk tool, we conduct a Points of Interest (POI)-centric accessibility analysis for three sectors in Chandigarh with very different land uses, residential, commercial and institutional covering about 40 km of sidewalks. Across 40 km of roads audited in three sectors and around 230 POIs, we identified 1,644 of 2,913 locations where infrastructure improvements could enhance accessibility.", "link": "http://arxiv.org/abs/2602.09216v2", "date": "2026-02-17", "relevancy": 1.9562, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5223}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4699}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Human-AI%20Accessibility%20Mapping%20in%20India%3A%20VLM-Guided%20Annotations%20and%20POI-Centric%20Analysis%20in%20Chandigarh&body=Title%3A%20Towards%20Human-AI%20Accessibility%20Mapping%20in%20India%3A%20VLM-Guided%20Annotations%20and%20POI-Centric%20Analysis%20in%20Chandigarh%0AAuthor%3A%20Varchita%20Lalwani%20and%20Utkarsh%20Agarwal%20and%20Michael%20Saugstad%20and%20Manish%20Kumar%20and%20Jon%20E.%20Froehlich%20and%20Anupam%20Sobti%0AAbstract%3A%20Project%20Sidewalk%20is%20a%20web-based%20platform%20that%20enables%20crowdsourcing%20accessibility%20of%20sidewalks%20at%20city-scale%20by%20virtually%20walking%20through%20city%20streets%20using%20Google%20Street%20View.%20The%20tool%20has%20been%20used%20in%2040%20cities%20across%20the%20world%2C%20including%20the%20US%2C%20Mexico%2C%20Chile%2C%20and%20Europe.%20In%20this%20paper%2C%20we%20describe%20adaptation%20efforts%20to%20enable%20deployment%20in%20Chandigarh%2C%20India%2C%20including%20modifying%20annotation%20types%2C%20provided%20examples%2C%20and%20integrating%20VLM-based%20mission%20guidance%2C%20which%20adapts%20instructions%20based%20on%20a%20street%20scene%20and%20metadata%20analysis.%20Our%20evaluation%20with%203%20annotators%20indicates%20the%20utility%20of%20AI-mission%20guidance%20with%20an%20average%20score%20of%204.66.%20Using%20this%20adapted%20Project%20Sidewalk%20tool%2C%20we%20conduct%20a%20Points%20of%20Interest%20%28POI%29-centric%20accessibility%20analysis%20for%20three%20sectors%20in%20Chandigarh%20with%20very%20different%20land%20uses%2C%20residential%2C%20commercial%20and%20institutional%20covering%20about%2040%20km%20of%20sidewalks.%20Across%2040%20km%20of%20roads%20audited%20in%20three%20sectors%20and%20around%20230%20POIs%2C%20we%20identified%201%2C644%20of%202%2C913%20locations%20where%20infrastructure%20improvements%20could%20enhance%20accessibility.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Human-AI%2520Accessibility%2520Mapping%2520in%2520India%253A%2520VLM-Guided%2520Annotations%2520and%2520POI-Centric%2520Analysis%2520in%2520Chandigarh%26entry.906535625%3DVarchita%2520Lalwani%2520and%2520Utkarsh%2520Agarwal%2520and%2520Michael%2520Saugstad%2520and%2520Manish%2520Kumar%2520and%2520Jon%2520E.%2520Froehlich%2520and%2520Anupam%2520Sobti%26entry.1292438233%3DProject%2520Sidewalk%2520is%2520a%2520web-based%2520platform%2520that%2520enables%2520crowdsourcing%2520accessibility%2520of%2520sidewalks%2520at%2520city-scale%2520by%2520virtually%2520walking%2520through%2520city%2520streets%2520using%2520Google%2520Street%2520View.%2520The%2520tool%2520has%2520been%2520used%2520in%252040%2520cities%2520across%2520the%2520world%252C%2520including%2520the%2520US%252C%2520Mexico%252C%2520Chile%252C%2520and%2520Europe.%2520In%2520this%2520paper%252C%2520we%2520describe%2520adaptation%2520efforts%2520to%2520enable%2520deployment%2520in%2520Chandigarh%252C%2520India%252C%2520including%2520modifying%2520annotation%2520types%252C%2520provided%2520examples%252C%2520and%2520integrating%2520VLM-based%2520mission%2520guidance%252C%2520which%2520adapts%2520instructions%2520based%2520on%2520a%2520street%2520scene%2520and%2520metadata%2520analysis.%2520Our%2520evaluation%2520with%25203%2520annotators%2520indicates%2520the%2520utility%2520of%2520AI-mission%2520guidance%2520with%2520an%2520average%2520score%2520of%25204.66.%2520Using%2520this%2520adapted%2520Project%2520Sidewalk%2520tool%252C%2520we%2520conduct%2520a%2520Points%2520of%2520Interest%2520%2528POI%2529-centric%2520accessibility%2520analysis%2520for%2520three%2520sectors%2520in%2520Chandigarh%2520with%2520very%2520different%2520land%2520uses%252C%2520residential%252C%2520commercial%2520and%2520institutional%2520covering%2520about%252040%2520km%2520of%2520sidewalks.%2520Across%252040%2520km%2520of%2520roads%2520audited%2520in%2520three%2520sectors%2520and%2520around%2520230%2520POIs%252C%2520we%2520identified%25201%252C644%2520of%25202%252C913%2520locations%2520where%2520infrastructure%2520improvements%2520could%2520enhance%2520accessibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Human-AI%20Accessibility%20Mapping%20in%20India%3A%20VLM-Guided%20Annotations%20and%20POI-Centric%20Analysis%20in%20Chandigarh&entry.906535625=Varchita%20Lalwani%20and%20Utkarsh%20Agarwal%20and%20Michael%20Saugstad%20and%20Manish%20Kumar%20and%20Jon%20E.%20Froehlich%20and%20Anupam%20Sobti&entry.1292438233=Project%20Sidewalk%20is%20a%20web-based%20platform%20that%20enables%20crowdsourcing%20accessibility%20of%20sidewalks%20at%20city-scale%20by%20virtually%20walking%20through%20city%20streets%20using%20Google%20Street%20View.%20The%20tool%20has%20been%20used%20in%2040%20cities%20across%20the%20world%2C%20including%20the%20US%2C%20Mexico%2C%20Chile%2C%20and%20Europe.%20In%20this%20paper%2C%20we%20describe%20adaptation%20efforts%20to%20enable%20deployment%20in%20Chandigarh%2C%20India%2C%20including%20modifying%20annotation%20types%2C%20provided%20examples%2C%20and%20integrating%20VLM-based%20mission%20guidance%2C%20which%20adapts%20instructions%20based%20on%20a%20street%20scene%20and%20metadata%20analysis.%20Our%20evaluation%20with%203%20annotators%20indicates%20the%20utility%20of%20AI-mission%20guidance%20with%20an%20average%20score%20of%204.66.%20Using%20this%20adapted%20Project%20Sidewalk%20tool%2C%20we%20conduct%20a%20Points%20of%20Interest%20%28POI%29-centric%20accessibility%20analysis%20for%20three%20sectors%20in%20Chandigarh%20with%20very%20different%20land%20uses%2C%20residential%2C%20commercial%20and%20institutional%20covering%20about%2040%20km%20of%20sidewalks.%20Across%2040%20km%20of%20roads%20audited%20in%20three%20sectors%20and%20around%20230%20POIs%2C%20we%20identified%201%2C644%20of%202%2C913%20locations%20where%20infrastructure%20improvements%20could%20enhance%20accessibility.&entry.1838667208=http%3A//arxiv.org/abs/2602.09216v2&entry.124074799=Read"},
{"title": "Improving Variational Autoencoder using Random Fourier Transformation: An Aviation Safety Anomaly Detection Case-Study", "author": "Ata Akbari Asanjan and Milad Memarzadeh and Bryan Matthews and Nikunj Oza", "abstract": "In this study, we focus on the training process and inference improvements of deep neural networks (DNNs), specifically Autoencoders (AEs) and Variational Autoencoders (VAEs), using Random Fourier Transformation (RFT). We further explore the role of RFT in model training behavior using Frequency Principle (F-Principle) analysis and show that models with RFT turn to learn low frequency and high frequency at the same time, whereas conventional DNNs start from low frequency and gradually learn (if successful) high-frequency features. We focus on reconstruction-based anomaly detection using autoencoder and variational autoencoder and investigate the RFT's role. We also introduced a trainable variant of RFT that uses the existing computation graph to train the expansion of RFT instead of it being random. We showcase our findings with two low-dimensional synthetic datasets for data representation, and an aviation safety dataset, called Dashlink, for high-dimensional reconstruction-based anomaly detection. The results indicate the superiority of models with Fourier transformation compared to the conventional counterpart and remain inconclusive regarding the benefits of using trainable Fourier transformation in contrast to the Random variant.", "link": "http://arxiv.org/abs/2601.01016v2", "date": "2026-02-17", "relevancy": 1.9558, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4948}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4934}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Variational%20Autoencoder%20using%20Random%20Fourier%20Transformation%3A%20An%20Aviation%20Safety%20Anomaly%20Detection%20Case-Study&body=Title%3A%20Improving%20Variational%20Autoencoder%20using%20Random%20Fourier%20Transformation%3A%20An%20Aviation%20Safety%20Anomaly%20Detection%20Case-Study%0AAuthor%3A%20Ata%20Akbari%20Asanjan%20and%20Milad%20Memarzadeh%20and%20Bryan%20Matthews%20and%20Nikunj%20Oza%0AAbstract%3A%20In%20this%20study%2C%20we%20focus%20on%20the%20training%20process%20and%20inference%20improvements%20of%20deep%20neural%20networks%20%28DNNs%29%2C%20specifically%20Autoencoders%20%28AEs%29%20and%20Variational%20Autoencoders%20%28VAEs%29%2C%20using%20Random%20Fourier%20Transformation%20%28RFT%29.%20We%20further%20explore%20the%20role%20of%20RFT%20in%20model%20training%20behavior%20using%20Frequency%20Principle%20%28F-Principle%29%20analysis%20and%20show%20that%20models%20with%20RFT%20turn%20to%20learn%20low%20frequency%20and%20high%20frequency%20at%20the%20same%20time%2C%20whereas%20conventional%20DNNs%20start%20from%20low%20frequency%20and%20gradually%20learn%20%28if%20successful%29%20high-frequency%20features.%20We%20focus%20on%20reconstruction-based%20anomaly%20detection%20using%20autoencoder%20and%20variational%20autoencoder%20and%20investigate%20the%20RFT%27s%20role.%20We%20also%20introduced%20a%20trainable%20variant%20of%20RFT%20that%20uses%20the%20existing%20computation%20graph%20to%20train%20the%20expansion%20of%20RFT%20instead%20of%20it%20being%20random.%20We%20showcase%20our%20findings%20with%20two%20low-dimensional%20synthetic%20datasets%20for%20data%20representation%2C%20and%20an%20aviation%20safety%20dataset%2C%20called%20Dashlink%2C%20for%20high-dimensional%20reconstruction-based%20anomaly%20detection.%20The%20results%20indicate%20the%20superiority%20of%20models%20with%20Fourier%20transformation%20compared%20to%20the%20conventional%20counterpart%20and%20remain%20inconclusive%20regarding%20the%20benefits%20of%20using%20trainable%20Fourier%20transformation%20in%20contrast%20to%20the%20Random%20variant.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01016v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Variational%2520Autoencoder%2520using%2520Random%2520Fourier%2520Transformation%253A%2520An%2520Aviation%2520Safety%2520Anomaly%2520Detection%2520Case-Study%26entry.906535625%3DAta%2520Akbari%2520Asanjan%2520and%2520Milad%2520Memarzadeh%2520and%2520Bryan%2520Matthews%2520and%2520Nikunj%2520Oza%26entry.1292438233%3DIn%2520this%2520study%252C%2520we%2520focus%2520on%2520the%2520training%2520process%2520and%2520inference%2520improvements%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%2520specifically%2520Autoencoders%2520%2528AEs%2529%2520and%2520Variational%2520Autoencoders%2520%2528VAEs%2529%252C%2520using%2520Random%2520Fourier%2520Transformation%2520%2528RFT%2529.%2520We%2520further%2520explore%2520the%2520role%2520of%2520RFT%2520in%2520model%2520training%2520behavior%2520using%2520Frequency%2520Principle%2520%2528F-Principle%2529%2520analysis%2520and%2520show%2520that%2520models%2520with%2520RFT%2520turn%2520to%2520learn%2520low%2520frequency%2520and%2520high%2520frequency%2520at%2520the%2520same%2520time%252C%2520whereas%2520conventional%2520DNNs%2520start%2520from%2520low%2520frequency%2520and%2520gradually%2520learn%2520%2528if%2520successful%2529%2520high-frequency%2520features.%2520We%2520focus%2520on%2520reconstruction-based%2520anomaly%2520detection%2520using%2520autoencoder%2520and%2520variational%2520autoencoder%2520and%2520investigate%2520the%2520RFT%2527s%2520role.%2520We%2520also%2520introduced%2520a%2520trainable%2520variant%2520of%2520RFT%2520that%2520uses%2520the%2520existing%2520computation%2520graph%2520to%2520train%2520the%2520expansion%2520of%2520RFT%2520instead%2520of%2520it%2520being%2520random.%2520We%2520showcase%2520our%2520findings%2520with%2520two%2520low-dimensional%2520synthetic%2520datasets%2520for%2520data%2520representation%252C%2520and%2520an%2520aviation%2520safety%2520dataset%252C%2520called%2520Dashlink%252C%2520for%2520high-dimensional%2520reconstruction-based%2520anomaly%2520detection.%2520The%2520results%2520indicate%2520the%2520superiority%2520of%2520models%2520with%2520Fourier%2520transformation%2520compared%2520to%2520the%2520conventional%2520counterpart%2520and%2520remain%2520inconclusive%2520regarding%2520the%2520benefits%2520of%2520using%2520trainable%2520Fourier%2520transformation%2520in%2520contrast%2520to%2520the%2520Random%2520variant.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01016v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Variational%20Autoencoder%20using%20Random%20Fourier%20Transformation%3A%20An%20Aviation%20Safety%20Anomaly%20Detection%20Case-Study&entry.906535625=Ata%20Akbari%20Asanjan%20and%20Milad%20Memarzadeh%20and%20Bryan%20Matthews%20and%20Nikunj%20Oza&entry.1292438233=In%20this%20study%2C%20we%20focus%20on%20the%20training%20process%20and%20inference%20improvements%20of%20deep%20neural%20networks%20%28DNNs%29%2C%20specifically%20Autoencoders%20%28AEs%29%20and%20Variational%20Autoencoders%20%28VAEs%29%2C%20using%20Random%20Fourier%20Transformation%20%28RFT%29.%20We%20further%20explore%20the%20role%20of%20RFT%20in%20model%20training%20behavior%20using%20Frequency%20Principle%20%28F-Principle%29%20analysis%20and%20show%20that%20models%20with%20RFT%20turn%20to%20learn%20low%20frequency%20and%20high%20frequency%20at%20the%20same%20time%2C%20whereas%20conventional%20DNNs%20start%20from%20low%20frequency%20and%20gradually%20learn%20%28if%20successful%29%20high-frequency%20features.%20We%20focus%20on%20reconstruction-based%20anomaly%20detection%20using%20autoencoder%20and%20variational%20autoencoder%20and%20investigate%20the%20RFT%27s%20role.%20We%20also%20introduced%20a%20trainable%20variant%20of%20RFT%20that%20uses%20the%20existing%20computation%20graph%20to%20train%20the%20expansion%20of%20RFT%20instead%20of%20it%20being%20random.%20We%20showcase%20our%20findings%20with%20two%20low-dimensional%20synthetic%20datasets%20for%20data%20representation%2C%20and%20an%20aviation%20safety%20dataset%2C%20called%20Dashlink%2C%20for%20high-dimensional%20reconstruction-based%20anomaly%20detection.%20The%20results%20indicate%20the%20superiority%20of%20models%20with%20Fourier%20transformation%20compared%20to%20the%20conventional%20counterpart%20and%20remain%20inconclusive%20regarding%20the%20benefits%20of%20using%20trainable%20Fourier%20transformation%20in%20contrast%20to%20the%20Random%20variant.&entry.1838667208=http%3A//arxiv.org/abs/2601.01016v2&entry.124074799=Read"},
{"title": "Topological quantification of ambiguity in semantic search", "author": "Thomas Roland Barillot and Alex De Castro", "abstract": "We studied how the local topological structure of sentence-embedding neighborhoods encodes semantic ambiguity. Extending ideas that link word-level polysemy to non-trivial persistent homology, we generalized the concept to full sentences and quantified ambiguity of a query in a semantic search process with two persistent homology metrics: the 1-Wasserstein norm of $H_{0}$ and the maximum loop lifetime of $H_{1}$. We formalized the notion of ambiguity as the relative presence of semantic domains or topics in sentences. We then used this formalism to compute \"ab-initio\" simulations that encode datapoints as linear combination of randomly generated single topics vectors in an arbitrary embedding space and demonstrate that ambiguous sentences separate from unambiguous ones in both metrics. Finally we validated those findings with real-world case by investigating on a fully open corpus comprising Nobel Prize Physics lectures from 1901 to 2024, segmented into contiguous, non-overlapping chunks at two granularity: $\\sim\\!250$ tokens and $\\sim\\!750$ tokens. We tested embedding with four publicly available models. Results across all models reproduce simulations and remain stable despite changes in embedding architecture. We conclude that persistent homology provides a model-agnostic signal of semantic discontinuities, suggesting practical use for ambiguity detection and semantic search recall.", "link": "http://arxiv.org/abs/2406.07990v2", "date": "2026-02-17", "relevancy": 1.9503, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20quantification%20of%20ambiguity%20in%20semantic%20search&body=Title%3A%20Topological%20quantification%20of%20ambiguity%20in%20semantic%20search%0AAuthor%3A%20Thomas%20Roland%20Barillot%20and%20Alex%20De%20Castro%0AAbstract%3A%20We%20studied%20how%20the%20local%20topological%20structure%20of%20sentence-embedding%20neighborhoods%20encodes%20semantic%20ambiguity.%20Extending%20ideas%20that%20link%20word-level%20polysemy%20to%20non-trivial%20persistent%20homology%2C%20we%20generalized%20the%20concept%20to%20full%20sentences%20and%20quantified%20ambiguity%20of%20a%20query%20in%20a%20semantic%20search%20process%20with%20two%20persistent%20homology%20metrics%3A%20the%201-Wasserstein%20norm%20of%20%24H_%7B0%7D%24%20and%20the%20maximum%20loop%20lifetime%20of%20%24H_%7B1%7D%24.%20We%20formalized%20the%20notion%20of%20ambiguity%20as%20the%20relative%20presence%20of%20semantic%20domains%20or%20topics%20in%20sentences.%20We%20then%20used%20this%20formalism%20to%20compute%20%22ab-initio%22%20simulations%20that%20encode%20datapoints%20as%20linear%20combination%20of%20randomly%20generated%20single%20topics%20vectors%20in%20an%20arbitrary%20embedding%20space%20and%20demonstrate%20that%20ambiguous%20sentences%20separate%20from%20unambiguous%20ones%20in%20both%20metrics.%20Finally%20we%20validated%20those%20findings%20with%20real-world%20case%20by%20investigating%20on%20a%20fully%20open%20corpus%20comprising%20Nobel%20Prize%20Physics%20lectures%20from%201901%20to%202024%2C%20segmented%20into%20contiguous%2C%20non-overlapping%20chunks%20at%20two%20granularity%3A%20%24%5Csim%5C%21250%24%20tokens%20and%20%24%5Csim%5C%21750%24%20tokens.%20We%20tested%20embedding%20with%20four%20publicly%20available%20models.%20Results%20across%20all%20models%20reproduce%20simulations%20and%20remain%20stable%20despite%20changes%20in%20embedding%20architecture.%20We%20conclude%20that%20persistent%20homology%20provides%20a%20model-agnostic%20signal%20of%20semantic%20discontinuities%2C%20suggesting%20practical%20use%20for%20ambiguity%20detection%20and%20semantic%20search%20recall.%0ALink%3A%20http%3A//arxiv.org/abs/2406.07990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520quantification%2520of%2520ambiguity%2520in%2520semantic%2520search%26entry.906535625%3DThomas%2520Roland%2520Barillot%2520and%2520Alex%2520De%2520Castro%26entry.1292438233%3DWe%2520studied%2520how%2520the%2520local%2520topological%2520structure%2520of%2520sentence-embedding%2520neighborhoods%2520encodes%2520semantic%2520ambiguity.%2520Extending%2520ideas%2520that%2520link%2520word-level%2520polysemy%2520to%2520non-trivial%2520persistent%2520homology%252C%2520we%2520generalized%2520the%2520concept%2520to%2520full%2520sentences%2520and%2520quantified%2520ambiguity%2520of%2520a%2520query%2520in%2520a%2520semantic%2520search%2520process%2520with%2520two%2520persistent%2520homology%2520metrics%253A%2520the%25201-Wasserstein%2520norm%2520of%2520%2524H_%257B0%257D%2524%2520and%2520the%2520maximum%2520loop%2520lifetime%2520of%2520%2524H_%257B1%257D%2524.%2520We%2520formalized%2520the%2520notion%2520of%2520ambiguity%2520as%2520the%2520relative%2520presence%2520of%2520semantic%2520domains%2520or%2520topics%2520in%2520sentences.%2520We%2520then%2520used%2520this%2520formalism%2520to%2520compute%2520%2522ab-initio%2522%2520simulations%2520that%2520encode%2520datapoints%2520as%2520linear%2520combination%2520of%2520randomly%2520generated%2520single%2520topics%2520vectors%2520in%2520an%2520arbitrary%2520embedding%2520space%2520and%2520demonstrate%2520that%2520ambiguous%2520sentences%2520separate%2520from%2520unambiguous%2520ones%2520in%2520both%2520metrics.%2520Finally%2520we%2520validated%2520those%2520findings%2520with%2520real-world%2520case%2520by%2520investigating%2520on%2520a%2520fully%2520open%2520corpus%2520comprising%2520Nobel%2520Prize%2520Physics%2520lectures%2520from%25201901%2520to%25202024%252C%2520segmented%2520into%2520contiguous%252C%2520non-overlapping%2520chunks%2520at%2520two%2520granularity%253A%2520%2524%255Csim%255C%2521250%2524%2520tokens%2520and%2520%2524%255Csim%255C%2521750%2524%2520tokens.%2520We%2520tested%2520embedding%2520with%2520four%2520publicly%2520available%2520models.%2520Results%2520across%2520all%2520models%2520reproduce%2520simulations%2520and%2520remain%2520stable%2520despite%2520changes%2520in%2520embedding%2520architecture.%2520We%2520conclude%2520that%2520persistent%2520homology%2520provides%2520a%2520model-agnostic%2520signal%2520of%2520semantic%2520discontinuities%252C%2520suggesting%2520practical%2520use%2520for%2520ambiguity%2520detection%2520and%2520semantic%2520search%2520recall.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20quantification%20of%20ambiguity%20in%20semantic%20search&entry.906535625=Thomas%20Roland%20Barillot%20and%20Alex%20De%20Castro&entry.1292438233=We%20studied%20how%20the%20local%20topological%20structure%20of%20sentence-embedding%20neighborhoods%20encodes%20semantic%20ambiguity.%20Extending%20ideas%20that%20link%20word-level%20polysemy%20to%20non-trivial%20persistent%20homology%2C%20we%20generalized%20the%20concept%20to%20full%20sentences%20and%20quantified%20ambiguity%20of%20a%20query%20in%20a%20semantic%20search%20process%20with%20two%20persistent%20homology%20metrics%3A%20the%201-Wasserstein%20norm%20of%20%24H_%7B0%7D%24%20and%20the%20maximum%20loop%20lifetime%20of%20%24H_%7B1%7D%24.%20We%20formalized%20the%20notion%20of%20ambiguity%20as%20the%20relative%20presence%20of%20semantic%20domains%20or%20topics%20in%20sentences.%20We%20then%20used%20this%20formalism%20to%20compute%20%22ab-initio%22%20simulations%20that%20encode%20datapoints%20as%20linear%20combination%20of%20randomly%20generated%20single%20topics%20vectors%20in%20an%20arbitrary%20embedding%20space%20and%20demonstrate%20that%20ambiguous%20sentences%20separate%20from%20unambiguous%20ones%20in%20both%20metrics.%20Finally%20we%20validated%20those%20findings%20with%20real-world%20case%20by%20investigating%20on%20a%20fully%20open%20corpus%20comprising%20Nobel%20Prize%20Physics%20lectures%20from%201901%20to%202024%2C%20segmented%20into%20contiguous%2C%20non-overlapping%20chunks%20at%20two%20granularity%3A%20%24%5Csim%5C%21250%24%20tokens%20and%20%24%5Csim%5C%21750%24%20tokens.%20We%20tested%20embedding%20with%20four%20publicly%20available%20models.%20Results%20across%20all%20models%20reproduce%20simulations%20and%20remain%20stable%20despite%20changes%20in%20embedding%20architecture.%20We%20conclude%20that%20persistent%20homology%20provides%20a%20model-agnostic%20signal%20of%20semantic%20discontinuities%2C%20suggesting%20practical%20use%20for%20ambiguity%20detection%20and%20semantic%20search%20recall.&entry.1838667208=http%3A//arxiv.org/abs/2406.07990v2&entry.124074799=Read"},
{"title": "Bottleneck Transformer-Based Approach for Improved Automatic STOI Score Prediction", "author": " Amartyaveer and Murali Kadambi and Chandra Mohan Sharma and Anupam Mondal and Prasanta Kumar Ghosh", "abstract": "In this study, we have presented a novel approach to predict the Short-Time Objective Intelligibility (STOI) metric using a bottleneck transformer architecture. Traditional methods for calculating STOI typically requires clean reference speech, which limits their applicability in the real world. To address this, numerous deep learning-based nonintrusive speech assessment models have garnered significant interest. Many studies have achieved commendable performance, but there is room for further improvement.\n  We propose the use of bottleneck transformer, incorporating convolution blocks for learning frame-level features and a multi-head self-attention (MHSA) layer to aggregate the information. These components enable the transformer to focus on the key aspects of the input data. Our model has shown higher correlation and lower mean squared error for both seen and unseen scenarios compared to the state-of-the-art model using self-supervised learning (SSL) and spectral features as inputs.", "link": "http://arxiv.org/abs/2602.15484v1", "date": "2026-02-17", "relevancy": 1.9498, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5465}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4917}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bottleneck%20Transformer-Based%20Approach%20for%20Improved%20Automatic%20STOI%20Score%20Prediction&body=Title%3A%20Bottleneck%20Transformer-Based%20Approach%20for%20Improved%20Automatic%20STOI%20Score%20Prediction%0AAuthor%3A%20%20Amartyaveer%20and%20Murali%20Kadambi%20and%20Chandra%20Mohan%20Sharma%20and%20Anupam%20Mondal%20and%20Prasanta%20Kumar%20Ghosh%0AAbstract%3A%20In%20this%20study%2C%20we%20have%20presented%20a%20novel%20approach%20to%20predict%20the%20Short-Time%20Objective%20Intelligibility%20%28STOI%29%20metric%20using%20a%20bottleneck%20transformer%20architecture.%20Traditional%20methods%20for%20calculating%20STOI%20typically%20requires%20clean%20reference%20speech%2C%20which%20limits%20their%20applicability%20in%20the%20real%20world.%20To%20address%20this%2C%20numerous%20deep%20learning-based%20nonintrusive%20speech%20assessment%20models%20have%20garnered%20significant%20interest.%20Many%20studies%20have%20achieved%20commendable%20performance%2C%20but%20there%20is%20room%20for%20further%20improvement.%0A%20%20We%20propose%20the%20use%20of%20bottleneck%20transformer%2C%20incorporating%20convolution%20blocks%20for%20learning%20frame-level%20features%20and%20a%20multi-head%20self-attention%20%28MHSA%29%20layer%20to%20aggregate%20the%20information.%20These%20components%20enable%20the%20transformer%20to%20focus%20on%20the%20key%20aspects%20of%20the%20input%20data.%20Our%20model%20has%20shown%20higher%20correlation%20and%20lower%20mean%20squared%20error%20for%20both%20seen%20and%20unseen%20scenarios%20compared%20to%20the%20state-of-the-art%20model%20using%20self-supervised%20learning%20%28SSL%29%20and%20spectral%20features%20as%20inputs.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBottleneck%2520Transformer-Based%2520Approach%2520for%2520Improved%2520Automatic%2520STOI%2520Score%2520Prediction%26entry.906535625%3D%2520Amartyaveer%2520and%2520Murali%2520Kadambi%2520and%2520Chandra%2520Mohan%2520Sharma%2520and%2520Anupam%2520Mondal%2520and%2520Prasanta%2520Kumar%2520Ghosh%26entry.1292438233%3DIn%2520this%2520study%252C%2520we%2520have%2520presented%2520a%2520novel%2520approach%2520to%2520predict%2520the%2520Short-Time%2520Objective%2520Intelligibility%2520%2528STOI%2529%2520metric%2520using%2520a%2520bottleneck%2520transformer%2520architecture.%2520Traditional%2520methods%2520for%2520calculating%2520STOI%2520typically%2520requires%2520clean%2520reference%2520speech%252C%2520which%2520limits%2520their%2520applicability%2520in%2520the%2520real%2520world.%2520To%2520address%2520this%252C%2520numerous%2520deep%2520learning-based%2520nonintrusive%2520speech%2520assessment%2520models%2520have%2520garnered%2520significant%2520interest.%2520Many%2520studies%2520have%2520achieved%2520commendable%2520performance%252C%2520but%2520there%2520is%2520room%2520for%2520further%2520improvement.%250A%2520%2520We%2520propose%2520the%2520use%2520of%2520bottleneck%2520transformer%252C%2520incorporating%2520convolution%2520blocks%2520for%2520learning%2520frame-level%2520features%2520and%2520a%2520multi-head%2520self-attention%2520%2528MHSA%2529%2520layer%2520to%2520aggregate%2520the%2520information.%2520These%2520components%2520enable%2520the%2520transformer%2520to%2520focus%2520on%2520the%2520key%2520aspects%2520of%2520the%2520input%2520data.%2520Our%2520model%2520has%2520shown%2520higher%2520correlation%2520and%2520lower%2520mean%2520squared%2520error%2520for%2520both%2520seen%2520and%2520unseen%2520scenarios%2520compared%2520to%2520the%2520state-of-the-art%2520model%2520using%2520self-supervised%2520learning%2520%2528SSL%2529%2520and%2520spectral%2520features%2520as%2520inputs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bottleneck%20Transformer-Based%20Approach%20for%20Improved%20Automatic%20STOI%20Score%20Prediction&entry.906535625=%20Amartyaveer%20and%20Murali%20Kadambi%20and%20Chandra%20Mohan%20Sharma%20and%20Anupam%20Mondal%20and%20Prasanta%20Kumar%20Ghosh&entry.1292438233=In%20this%20study%2C%20we%20have%20presented%20a%20novel%20approach%20to%20predict%20the%20Short-Time%20Objective%20Intelligibility%20%28STOI%29%20metric%20using%20a%20bottleneck%20transformer%20architecture.%20Traditional%20methods%20for%20calculating%20STOI%20typically%20requires%20clean%20reference%20speech%2C%20which%20limits%20their%20applicability%20in%20the%20real%20world.%20To%20address%20this%2C%20numerous%20deep%20learning-based%20nonintrusive%20speech%20assessment%20models%20have%20garnered%20significant%20interest.%20Many%20studies%20have%20achieved%20commendable%20performance%2C%20but%20there%20is%20room%20for%20further%20improvement.%0A%20%20We%20propose%20the%20use%20of%20bottleneck%20transformer%2C%20incorporating%20convolution%20blocks%20for%20learning%20frame-level%20features%20and%20a%20multi-head%20self-attention%20%28MHSA%29%20layer%20to%20aggregate%20the%20information.%20These%20components%20enable%20the%20transformer%20to%20focus%20on%20the%20key%20aspects%20of%20the%20input%20data.%20Our%20model%20has%20shown%20higher%20correlation%20and%20lower%20mean%20squared%20error%20for%20both%20seen%20and%20unseen%20scenarios%20compared%20to%20the%20state-of-the-art%20model%20using%20self-supervised%20learning%20%28SSL%29%20and%20spectral%20features%20as%20inputs.&entry.1838667208=http%3A//arxiv.org/abs/2602.15484v1&entry.124074799=Read"},
{"title": "An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment", "author": "Flavien Armangeon and Thibaud Ehret and Enric Meinhardt-Llopis and Rafael Grompone von Gioi and Guillaume Thibault and Marc Petit and Gabriele Facciolo", "abstract": "Aligning functional schematics with 2D and 3D scene acquisitions is crucial for building digital twins, especially for old industrial facilities that lack native digital models. Current manual alignment using images and LiDAR data does not scale due to tediousness and complexity of industrial sites. Inconsistencies between schematics and reality, and the scarcity of public industrial datasets, make the problem both challenging and underexplored. This paper introduces IRIS-v2, a comprehensive dataset to support further research. It includes images, point clouds, 2D annotated boxes and segmentation masks, a CAD model, 3D pipe routing information, and the P&ID (Piping and Instrumentation Diagram). The alignment is experimented on a practical case study, aiming at reducing the time required for this task by combining segmentation and graph matching.", "link": "http://arxiv.org/abs/2602.15584v1", "date": "2026-02-17", "relevancy": 1.9473, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Industrial%20Dataset%20for%20Scene%20Acquisitions%20and%20Functional%20Schematics%20Alignment&body=Title%3A%20An%20Industrial%20Dataset%20for%20Scene%20Acquisitions%20and%20Functional%20Schematics%20Alignment%0AAuthor%3A%20Flavien%20Armangeon%20and%20Thibaud%20Ehret%20and%20Enric%20Meinhardt-Llopis%20and%20Rafael%20Grompone%20von%20Gioi%20and%20Guillaume%20Thibault%20and%20Marc%20Petit%20and%20Gabriele%20Facciolo%0AAbstract%3A%20Aligning%20functional%20schematics%20with%202D%20and%203D%20scene%20acquisitions%20is%20crucial%20for%20building%20digital%20twins%2C%20especially%20for%20old%20industrial%20facilities%20that%20lack%20native%20digital%20models.%20Current%20manual%20alignment%20using%20images%20and%20LiDAR%20data%20does%20not%20scale%20due%20to%20tediousness%20and%20complexity%20of%20industrial%20sites.%20Inconsistencies%20between%20schematics%20and%20reality%2C%20and%20the%20scarcity%20of%20public%20industrial%20datasets%2C%20make%20the%20problem%20both%20challenging%20and%20underexplored.%20This%20paper%20introduces%20IRIS-v2%2C%20a%20comprehensive%20dataset%20to%20support%20further%20research.%20It%20includes%20images%2C%20point%20clouds%2C%202D%20annotated%20boxes%20and%20segmentation%20masks%2C%20a%20CAD%20model%2C%203D%20pipe%20routing%20information%2C%20and%20the%20P%26ID%20%28Piping%20and%20Instrumentation%20Diagram%29.%20The%20alignment%20is%20experimented%20on%20a%20practical%20case%20study%2C%20aiming%20at%20reducing%20the%20time%20required%20for%20this%20task%20by%20combining%20segmentation%20and%20graph%20matching.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Industrial%2520Dataset%2520for%2520Scene%2520Acquisitions%2520and%2520Functional%2520Schematics%2520Alignment%26entry.906535625%3DFlavien%2520Armangeon%2520and%2520Thibaud%2520Ehret%2520and%2520Enric%2520Meinhardt-Llopis%2520and%2520Rafael%2520Grompone%2520von%2520Gioi%2520and%2520Guillaume%2520Thibault%2520and%2520Marc%2520Petit%2520and%2520Gabriele%2520Facciolo%26entry.1292438233%3DAligning%2520functional%2520schematics%2520with%25202D%2520and%25203D%2520scene%2520acquisitions%2520is%2520crucial%2520for%2520building%2520digital%2520twins%252C%2520especially%2520for%2520old%2520industrial%2520facilities%2520that%2520lack%2520native%2520digital%2520models.%2520Current%2520manual%2520alignment%2520using%2520images%2520and%2520LiDAR%2520data%2520does%2520not%2520scale%2520due%2520to%2520tediousness%2520and%2520complexity%2520of%2520industrial%2520sites.%2520Inconsistencies%2520between%2520schematics%2520and%2520reality%252C%2520and%2520the%2520scarcity%2520of%2520public%2520industrial%2520datasets%252C%2520make%2520the%2520problem%2520both%2520challenging%2520and%2520underexplored.%2520This%2520paper%2520introduces%2520IRIS-v2%252C%2520a%2520comprehensive%2520dataset%2520to%2520support%2520further%2520research.%2520It%2520includes%2520images%252C%2520point%2520clouds%252C%25202D%2520annotated%2520boxes%2520and%2520segmentation%2520masks%252C%2520a%2520CAD%2520model%252C%25203D%2520pipe%2520routing%2520information%252C%2520and%2520the%2520P%2526ID%2520%2528Piping%2520and%2520Instrumentation%2520Diagram%2529.%2520The%2520alignment%2520is%2520experimented%2520on%2520a%2520practical%2520case%2520study%252C%2520aiming%2520at%2520reducing%2520the%2520time%2520required%2520for%2520this%2520task%2520by%2520combining%2520segmentation%2520and%2520graph%2520matching.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Industrial%20Dataset%20for%20Scene%20Acquisitions%20and%20Functional%20Schematics%20Alignment&entry.906535625=Flavien%20Armangeon%20and%20Thibaud%20Ehret%20and%20Enric%20Meinhardt-Llopis%20and%20Rafael%20Grompone%20von%20Gioi%20and%20Guillaume%20Thibault%20and%20Marc%20Petit%20and%20Gabriele%20Facciolo&entry.1292438233=Aligning%20functional%20schematics%20with%202D%20and%203D%20scene%20acquisitions%20is%20crucial%20for%20building%20digital%20twins%2C%20especially%20for%20old%20industrial%20facilities%20that%20lack%20native%20digital%20models.%20Current%20manual%20alignment%20using%20images%20and%20LiDAR%20data%20does%20not%20scale%20due%20to%20tediousness%20and%20complexity%20of%20industrial%20sites.%20Inconsistencies%20between%20schematics%20and%20reality%2C%20and%20the%20scarcity%20of%20public%20industrial%20datasets%2C%20make%20the%20problem%20both%20challenging%20and%20underexplored.%20This%20paper%20introduces%20IRIS-v2%2C%20a%20comprehensive%20dataset%20to%20support%20further%20research.%20It%20includes%20images%2C%20point%20clouds%2C%202D%20annotated%20boxes%20and%20segmentation%20masks%2C%20a%20CAD%20model%2C%203D%20pipe%20routing%20information%2C%20and%20the%20P%26ID%20%28Piping%20and%20Instrumentation%20Diagram%29.%20The%20alignment%20is%20experimented%20on%20a%20practical%20case%20study%2C%20aiming%20at%20reducing%20the%20time%20required%20for%20this%20task%20by%20combining%20segmentation%20and%20graph%20matching.&entry.1838667208=http%3A//arxiv.org/abs/2602.15584v1&entry.124074799=Read"},
{"title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring", "author": "Yousef Emami and Hao Zhou and Miguel Gutierrez Gaitan and Kai Li and Luis Almeida", "abstract": "Uncrewed Aerial Vehicles (UAVs) play a vital role in public safety, especially in monitoring wildfires, where early detection reduces environmental impact. In UAV-Assisted Wildfire Monitoring (UAWM) systems, jointly optimizing the data collection schedule and UAV velocity is essential to minimize the average Age of Information (AoI) for sensory data. Deep Reinforcement Learning (DRL) has been used for this optimization, but its limitations-including low sampling efficiency, discrepancies between simulation and real-world conditions, and complex training make it unsuitable for time-critical applications such as wildfire monitoring. Recent advances in Large Language Models (LLMs) provide a promising alternative. With strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation using natural language prompts and example-based guidance without retraining. This paper proposes a novel online Flight Resource Allocation scheme based on LLM-Enabled In-Context Learning (FRSICL) to jointly optimize the data collection schedule and UAV velocity along the trajectory in real time, thereby asymptotically minimizing the average AoI across all ground sensors. Unlike DRL, FRSICL generates data collection schedules and velocities using natural language task descriptions and feedback from the environment, enabling dynamic decision-making without extensive retraining. Simulation results confirm the effectiveness of FRSICL compared to state-of-the-art baselines, namely Proximal Policy Optimization, Block Coordinate Descent, and Nearest Neighbor.", "link": "http://arxiv.org/abs/2507.10134v2", "date": "2026-02-17", "relevancy": 1.9394, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5113}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4821}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRSICL%3A%20LLM-Enabled%20In-Context%20Learning%20Flight%20Resource%20Allocation%20for%20Fresh%20Data%20Collection%20in%20UAV-Assisted%20Wildfire%20Monitoring&body=Title%3A%20FRSICL%3A%20LLM-Enabled%20In-Context%20Learning%20Flight%20Resource%20Allocation%20for%20Fresh%20Data%20Collection%20in%20UAV-Assisted%20Wildfire%20Monitoring%0AAuthor%3A%20Yousef%20Emami%20and%20Hao%20Zhou%20and%20Miguel%20Gutierrez%20Gaitan%20and%20Kai%20Li%20and%20Luis%20Almeida%0AAbstract%3A%20Uncrewed%20Aerial%20Vehicles%20%28UAVs%29%20play%20a%20vital%20role%20in%20public%20safety%2C%20especially%20in%20monitoring%20wildfires%2C%20where%20early%20detection%20reduces%20environmental%20impact.%20In%20UAV-Assisted%20Wildfire%20Monitoring%20%28UAWM%29%20systems%2C%20jointly%20optimizing%20the%20data%20collection%20schedule%20and%20UAV%20velocity%20is%20essential%20to%20minimize%20the%20average%20Age%20of%20Information%20%28AoI%29%20for%20sensory%20data.%20Deep%20Reinforcement%20Learning%20%28DRL%29%20has%20been%20used%20for%20this%20optimization%2C%20but%20its%20limitations-including%20low%20sampling%20efficiency%2C%20discrepancies%20between%20simulation%20and%20real-world%20conditions%2C%20and%20complex%20training%20make%20it%20unsuitable%20for%20time-critical%20applications%20such%20as%20wildfire%20monitoring.%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20provide%20a%20promising%20alternative.%20With%20strong%20reasoning%20and%20generalization%20capabilities%2C%20LLMs%20can%20adapt%20to%20new%20tasks%20through%20In-Context%20Learning%20%28ICL%29%2C%20which%20enables%20task%20adaptation%20using%20natural%20language%20prompts%20and%20example-based%20guidance%20without%20retraining.%20This%20paper%20proposes%20a%20novel%20online%20Flight%20Resource%20Allocation%20scheme%20based%20on%20LLM-Enabled%20In-Context%20Learning%20%28FRSICL%29%20to%20jointly%20optimize%20the%20data%20collection%20schedule%20and%20UAV%20velocity%20along%20the%20trajectory%20in%20real%20time%2C%20thereby%20asymptotically%20minimizing%20the%20average%20AoI%20across%20all%20ground%20sensors.%20Unlike%20DRL%2C%20FRSICL%20generates%20data%20collection%20schedules%20and%20velocities%20using%20natural%20language%20task%20descriptions%20and%20feedback%20from%20the%20environment%2C%20enabling%20dynamic%20decision-making%20without%20extensive%20retraining.%20Simulation%20results%20confirm%20the%20effectiveness%20of%20FRSICL%20compared%20to%20state-of-the-art%20baselines%2C%20namely%20Proximal%20Policy%20Optimization%2C%20Block%20Coordinate%20Descent%2C%20and%20Nearest%20Neighbor.%0ALink%3A%20http%3A//arxiv.org/abs/2507.10134v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRSICL%253A%2520LLM-Enabled%2520In-Context%2520Learning%2520Flight%2520Resource%2520Allocation%2520for%2520Fresh%2520Data%2520Collection%2520in%2520UAV-Assisted%2520Wildfire%2520Monitoring%26entry.906535625%3DYousef%2520Emami%2520and%2520Hao%2520Zhou%2520and%2520Miguel%2520Gutierrez%2520Gaitan%2520and%2520Kai%2520Li%2520and%2520Luis%2520Almeida%26entry.1292438233%3DUncrewed%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520play%2520a%2520vital%2520role%2520in%2520public%2520safety%252C%2520especially%2520in%2520monitoring%2520wildfires%252C%2520where%2520early%2520detection%2520reduces%2520environmental%2520impact.%2520In%2520UAV-Assisted%2520Wildfire%2520Monitoring%2520%2528UAWM%2529%2520systems%252C%2520jointly%2520optimizing%2520the%2520data%2520collection%2520schedule%2520and%2520UAV%2520velocity%2520is%2520essential%2520to%2520minimize%2520the%2520average%2520Age%2520of%2520Information%2520%2528AoI%2529%2520for%2520sensory%2520data.%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520has%2520been%2520used%2520for%2520this%2520optimization%252C%2520but%2520its%2520limitations-including%2520low%2520sampling%2520efficiency%252C%2520discrepancies%2520between%2520simulation%2520and%2520real-world%2520conditions%252C%2520and%2520complex%2520training%2520make%2520it%2520unsuitable%2520for%2520time-critical%2520applications%2520such%2520as%2520wildfire%2520monitoring.%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520provide%2520a%2520promising%2520alternative.%2520With%2520strong%2520reasoning%2520and%2520generalization%2520capabilities%252C%2520LLMs%2520can%2520adapt%2520to%2520new%2520tasks%2520through%2520In-Context%2520Learning%2520%2528ICL%2529%252C%2520which%2520enables%2520task%2520adaptation%2520using%2520natural%2520language%2520prompts%2520and%2520example-based%2520guidance%2520without%2520retraining.%2520This%2520paper%2520proposes%2520a%2520novel%2520online%2520Flight%2520Resource%2520Allocation%2520scheme%2520based%2520on%2520LLM-Enabled%2520In-Context%2520Learning%2520%2528FRSICL%2529%2520to%2520jointly%2520optimize%2520the%2520data%2520collection%2520schedule%2520and%2520UAV%2520velocity%2520along%2520the%2520trajectory%2520in%2520real%2520time%252C%2520thereby%2520asymptotically%2520minimizing%2520the%2520average%2520AoI%2520across%2520all%2520ground%2520sensors.%2520Unlike%2520DRL%252C%2520FRSICL%2520generates%2520data%2520collection%2520schedules%2520and%2520velocities%2520using%2520natural%2520language%2520task%2520descriptions%2520and%2520feedback%2520from%2520the%2520environment%252C%2520enabling%2520dynamic%2520decision-making%2520without%2520extensive%2520retraining.%2520Simulation%2520results%2520confirm%2520the%2520effectiveness%2520of%2520FRSICL%2520compared%2520to%2520state-of-the-art%2520baselines%252C%2520namely%2520Proximal%2520Policy%2520Optimization%252C%2520Block%2520Coordinate%2520Descent%252C%2520and%2520Nearest%2520Neighbor.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10134v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRSICL%3A%20LLM-Enabled%20In-Context%20Learning%20Flight%20Resource%20Allocation%20for%20Fresh%20Data%20Collection%20in%20UAV-Assisted%20Wildfire%20Monitoring&entry.906535625=Yousef%20Emami%20and%20Hao%20Zhou%20and%20Miguel%20Gutierrez%20Gaitan%20and%20Kai%20Li%20and%20Luis%20Almeida&entry.1292438233=Uncrewed%20Aerial%20Vehicles%20%28UAVs%29%20play%20a%20vital%20role%20in%20public%20safety%2C%20especially%20in%20monitoring%20wildfires%2C%20where%20early%20detection%20reduces%20environmental%20impact.%20In%20UAV-Assisted%20Wildfire%20Monitoring%20%28UAWM%29%20systems%2C%20jointly%20optimizing%20the%20data%20collection%20schedule%20and%20UAV%20velocity%20is%20essential%20to%20minimize%20the%20average%20Age%20of%20Information%20%28AoI%29%20for%20sensory%20data.%20Deep%20Reinforcement%20Learning%20%28DRL%29%20has%20been%20used%20for%20this%20optimization%2C%20but%20its%20limitations-including%20low%20sampling%20efficiency%2C%20discrepancies%20between%20simulation%20and%20real-world%20conditions%2C%20and%20complex%20training%20make%20it%20unsuitable%20for%20time-critical%20applications%20such%20as%20wildfire%20monitoring.%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20provide%20a%20promising%20alternative.%20With%20strong%20reasoning%20and%20generalization%20capabilities%2C%20LLMs%20can%20adapt%20to%20new%20tasks%20through%20In-Context%20Learning%20%28ICL%29%2C%20which%20enables%20task%20adaptation%20using%20natural%20language%20prompts%20and%20example-based%20guidance%20without%20retraining.%20This%20paper%20proposes%20a%20novel%20online%20Flight%20Resource%20Allocation%20scheme%20based%20on%20LLM-Enabled%20In-Context%20Learning%20%28FRSICL%29%20to%20jointly%20optimize%20the%20data%20collection%20schedule%20and%20UAV%20velocity%20along%20the%20trajectory%20in%20real%20time%2C%20thereby%20asymptotically%20minimizing%20the%20average%20AoI%20across%20all%20ground%20sensors.%20Unlike%20DRL%2C%20FRSICL%20generates%20data%20collection%20schedules%20and%20velocities%20using%20natural%20language%20task%20descriptions%20and%20feedback%20from%20the%20environment%2C%20enabling%20dynamic%20decision-making%20without%20extensive%20retraining.%20Simulation%20results%20confirm%20the%20effectiveness%20of%20FRSICL%20compared%20to%20state-of-the-art%20baselines%2C%20namely%20Proximal%20Policy%20Optimization%2C%20Block%20Coordinate%20Descent%2C%20and%20Nearest%20Neighbor.&entry.1838667208=http%3A//arxiv.org/abs/2507.10134v2&entry.124074799=Read"},
{"title": "Certified Per-Instance Unlearning Using Individual Sensitivity Bounds", "author": "Hanna Benarroch and Jamal Atif and Olivier Capp\u00e9", "abstract": "Certified machine unlearning can be achieved via noise injection leading to differential privacy guarantees, where noise is calibrated to worst-case sensitivity. Such conservative calibration often results in performance degradation, limiting practical applicability. In this work, we investigate an alternative approach based on adaptive per-instance noise calibration tailored to the individual contribution of each data point to the learned solution. This raises the following challenge: how can one establish formal unlearning guarantees when the mechanism depends on the specific point to be removed? To define individual data point sensitivities in noisy gradient dynamics, we consider the use of per-instance differential privacy. For ridge regression trained via Langevin dynamics, we derive high-probability per-instance sensitivity bounds, yielding certified unlearning with substantially less noise injection. We corroborate our theoretical findings through experiments in linear settings and provide further empirical evidence on the relevance of the approach in deep learning settings.", "link": "http://arxiv.org/abs/2602.15602v1", "date": "2026-02-17", "relevancy": 1.9268, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4896}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4832}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certified%20Per-Instance%20Unlearning%20Using%20Individual%20Sensitivity%20Bounds&body=Title%3A%20Certified%20Per-Instance%20Unlearning%20Using%20Individual%20Sensitivity%20Bounds%0AAuthor%3A%20Hanna%20Benarroch%20and%20Jamal%20Atif%20and%20Olivier%20Capp%C3%A9%0AAbstract%3A%20Certified%20machine%20unlearning%20can%20be%20achieved%20via%20noise%20injection%20leading%20to%20differential%20privacy%20guarantees%2C%20where%20noise%20is%20calibrated%20to%20worst-case%20sensitivity.%20Such%20conservative%20calibration%20often%20results%20in%20performance%20degradation%2C%20limiting%20practical%20applicability.%20In%20this%20work%2C%20we%20investigate%20an%20alternative%20approach%20based%20on%20adaptive%20per-instance%20noise%20calibration%20tailored%20to%20the%20individual%20contribution%20of%20each%20data%20point%20to%20the%20learned%20solution.%20This%20raises%20the%20following%20challenge%3A%20how%20can%20one%20establish%20formal%20unlearning%20guarantees%20when%20the%20mechanism%20depends%20on%20the%20specific%20point%20to%20be%20removed%3F%20To%20define%20individual%20data%20point%20sensitivities%20in%20noisy%20gradient%20dynamics%2C%20we%20consider%20the%20use%20of%20per-instance%20differential%20privacy.%20For%20ridge%20regression%20trained%20via%20Langevin%20dynamics%2C%20we%20derive%20high-probability%20per-instance%20sensitivity%20bounds%2C%20yielding%20certified%20unlearning%20with%20substantially%20less%20noise%20injection.%20We%20corroborate%20our%20theoretical%20findings%20through%20experiments%20in%20linear%20settings%20and%20provide%20further%20empirical%20evidence%20on%20the%20relevance%20of%20the%20approach%20in%20deep%20learning%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertified%2520Per-Instance%2520Unlearning%2520Using%2520Individual%2520Sensitivity%2520Bounds%26entry.906535625%3DHanna%2520Benarroch%2520and%2520Jamal%2520Atif%2520and%2520Olivier%2520Capp%25C3%25A9%26entry.1292438233%3DCertified%2520machine%2520unlearning%2520can%2520be%2520achieved%2520via%2520noise%2520injection%2520leading%2520to%2520differential%2520privacy%2520guarantees%252C%2520where%2520noise%2520is%2520calibrated%2520to%2520worst-case%2520sensitivity.%2520Such%2520conservative%2520calibration%2520often%2520results%2520in%2520performance%2520degradation%252C%2520limiting%2520practical%2520applicability.%2520In%2520this%2520work%252C%2520we%2520investigate%2520an%2520alternative%2520approach%2520based%2520on%2520adaptive%2520per-instance%2520noise%2520calibration%2520tailored%2520to%2520the%2520individual%2520contribution%2520of%2520each%2520data%2520point%2520to%2520the%2520learned%2520solution.%2520This%2520raises%2520the%2520following%2520challenge%253A%2520how%2520can%2520one%2520establish%2520formal%2520unlearning%2520guarantees%2520when%2520the%2520mechanism%2520depends%2520on%2520the%2520specific%2520point%2520to%2520be%2520removed%253F%2520To%2520define%2520individual%2520data%2520point%2520sensitivities%2520in%2520noisy%2520gradient%2520dynamics%252C%2520we%2520consider%2520the%2520use%2520of%2520per-instance%2520differential%2520privacy.%2520For%2520ridge%2520regression%2520trained%2520via%2520Langevin%2520dynamics%252C%2520we%2520derive%2520high-probability%2520per-instance%2520sensitivity%2520bounds%252C%2520yielding%2520certified%2520unlearning%2520with%2520substantially%2520less%2520noise%2520injection.%2520We%2520corroborate%2520our%2520theoretical%2520findings%2520through%2520experiments%2520in%2520linear%2520settings%2520and%2520provide%2520further%2520empirical%2520evidence%2520on%2520the%2520relevance%2520of%2520the%2520approach%2520in%2520deep%2520learning%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certified%20Per-Instance%20Unlearning%20Using%20Individual%20Sensitivity%20Bounds&entry.906535625=Hanna%20Benarroch%20and%20Jamal%20Atif%20and%20Olivier%20Capp%C3%A9&entry.1292438233=Certified%20machine%20unlearning%20can%20be%20achieved%20via%20noise%20injection%20leading%20to%20differential%20privacy%20guarantees%2C%20where%20noise%20is%20calibrated%20to%20worst-case%20sensitivity.%20Such%20conservative%20calibration%20often%20results%20in%20performance%20degradation%2C%20limiting%20practical%20applicability.%20In%20this%20work%2C%20we%20investigate%20an%20alternative%20approach%20based%20on%20adaptive%20per-instance%20noise%20calibration%20tailored%20to%20the%20individual%20contribution%20of%20each%20data%20point%20to%20the%20learned%20solution.%20This%20raises%20the%20following%20challenge%3A%20how%20can%20one%20establish%20formal%20unlearning%20guarantees%20when%20the%20mechanism%20depends%20on%20the%20specific%20point%20to%20be%20removed%3F%20To%20define%20individual%20data%20point%20sensitivities%20in%20noisy%20gradient%20dynamics%2C%20we%20consider%20the%20use%20of%20per-instance%20differential%20privacy.%20For%20ridge%20regression%20trained%20via%20Langevin%20dynamics%2C%20we%20derive%20high-probability%20per-instance%20sensitivity%20bounds%2C%20yielding%20certified%20unlearning%20with%20substantially%20less%20noise%20injection.%20We%20corroborate%20our%20theoretical%20findings%20through%20experiments%20in%20linear%20settings%20and%20provide%20further%20empirical%20evidence%20on%20the%20relevance%20of%20the%20approach%20in%20deep%20learning%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2602.15602v1&entry.124074799=Read"},
{"title": "POP: Prior-fitted Optimizer Policies", "author": "Jan Kobiolka and Christian Frey and Gresa Shala and Arlind Kadra and Erind Bedalli and Josif Grabocka", "abstract": "Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning.", "link": "http://arxiv.org/abs/2602.15473v1", "date": "2026-02-17", "relevancy": 1.3303, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4586}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4469}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POP%3A%20Prior-fitted%20Optimizer%20Policies&body=Title%3A%20POP%3A%20Prior-fitted%20Optimizer%20Policies%0AAuthor%3A%20Jan%20Kobiolka%20and%20Christian%20Frey%20and%20Gresa%20Shala%20and%20Arlind%20Kadra%20and%20Erind%20Bedalli%20and%20Josif%20Grabocka%0AAbstract%3A%20Optimization%20refers%20to%20the%20task%20of%20finding%20extrema%20of%20an%20objective%20function.%20Classical%20gradient-based%20optimizers%20are%20highly%20sensitive%20to%20hyperparameter%20choices.%20In%20highly%20non-convex%20settings%20their%20performance%20relies%20on%20carefully%20tuned%20learning%20rates%2C%20momentum%2C%20and%20gradient%20accumulation.%20To%20address%20these%20limitations%2C%20we%20introduce%20POP%20%28Prior-fitted%20Optimizer%20Policies%29%2C%20a%20meta-learned%20optimizer%20that%20predicts%20coordinate-wise%20step%20sizes%20conditioned%20on%20the%20contextual%20information%20provided%20in%20the%20optimization%20trajectory.%20Our%20model%20is%20learned%20on%20millions%20of%20synthetic%20optimization%20problems%20sampled%20from%20a%20novel%20prior%20spanning%20both%20convex%20and%20non-convex%20objectives.%20We%20evaluate%20POP%20on%20an%20established%20benchmark%20including%2047%20optimization%20functions%20of%20various%20complexity%2C%20where%20it%20consistently%20outperforms%20first-order%20gradient-based%20methods%2C%20non-convex%20optimization%20approaches%20%28e.g.%2C%20evolutionary%20strategies%29%2C%20Bayesian%20optimization%2C%20and%20a%20recent%20meta-learned%20competitor%20under%20matched%20budget%20constraints.%20Our%20evaluation%20demonstrates%20strong%20generalization%20capabilities%20without%20task-specific%20tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOP%253A%2520Prior-fitted%2520Optimizer%2520Policies%26entry.906535625%3DJan%2520Kobiolka%2520and%2520Christian%2520Frey%2520and%2520Gresa%2520Shala%2520and%2520Arlind%2520Kadra%2520and%2520Erind%2520Bedalli%2520and%2520Josif%2520Grabocka%26entry.1292438233%3DOptimization%2520refers%2520to%2520the%2520task%2520of%2520finding%2520extrema%2520of%2520an%2520objective%2520function.%2520Classical%2520gradient-based%2520optimizers%2520are%2520highly%2520sensitive%2520to%2520hyperparameter%2520choices.%2520In%2520highly%2520non-convex%2520settings%2520their%2520performance%2520relies%2520on%2520carefully%2520tuned%2520learning%2520rates%252C%2520momentum%252C%2520and%2520gradient%2520accumulation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520POP%2520%2528Prior-fitted%2520Optimizer%2520Policies%2529%252C%2520a%2520meta-learned%2520optimizer%2520that%2520predicts%2520coordinate-wise%2520step%2520sizes%2520conditioned%2520on%2520the%2520contextual%2520information%2520provided%2520in%2520the%2520optimization%2520trajectory.%2520Our%2520model%2520is%2520learned%2520on%2520millions%2520of%2520synthetic%2520optimization%2520problems%2520sampled%2520from%2520a%2520novel%2520prior%2520spanning%2520both%2520convex%2520and%2520non-convex%2520objectives.%2520We%2520evaluate%2520POP%2520on%2520an%2520established%2520benchmark%2520including%252047%2520optimization%2520functions%2520of%2520various%2520complexity%252C%2520where%2520it%2520consistently%2520outperforms%2520first-order%2520gradient-based%2520methods%252C%2520non-convex%2520optimization%2520approaches%2520%2528e.g.%252C%2520evolutionary%2520strategies%2529%252C%2520Bayesian%2520optimization%252C%2520and%2520a%2520recent%2520meta-learned%2520competitor%2520under%2520matched%2520budget%2520constraints.%2520Our%2520evaluation%2520demonstrates%2520strong%2520generalization%2520capabilities%2520without%2520task-specific%2520tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POP%3A%20Prior-fitted%20Optimizer%20Policies&entry.906535625=Jan%20Kobiolka%20and%20Christian%20Frey%20and%20Gresa%20Shala%20and%20Arlind%20Kadra%20and%20Erind%20Bedalli%20and%20Josif%20Grabocka&entry.1292438233=Optimization%20refers%20to%20the%20task%20of%20finding%20extrema%20of%20an%20objective%20function.%20Classical%20gradient-based%20optimizers%20are%20highly%20sensitive%20to%20hyperparameter%20choices.%20In%20highly%20non-convex%20settings%20their%20performance%20relies%20on%20carefully%20tuned%20learning%20rates%2C%20momentum%2C%20and%20gradient%20accumulation.%20To%20address%20these%20limitations%2C%20we%20introduce%20POP%20%28Prior-fitted%20Optimizer%20Policies%29%2C%20a%20meta-learned%20optimizer%20that%20predicts%20coordinate-wise%20step%20sizes%20conditioned%20on%20the%20contextual%20information%20provided%20in%20the%20optimization%20trajectory.%20Our%20model%20is%20learned%20on%20millions%20of%20synthetic%20optimization%20problems%20sampled%20from%20a%20novel%20prior%20spanning%20both%20convex%20and%20non-convex%20objectives.%20We%20evaluate%20POP%20on%20an%20established%20benchmark%20including%2047%20optimization%20functions%20of%20various%20complexity%2C%20where%20it%20consistently%20outperforms%20first-order%20gradient-based%20methods%2C%20non-convex%20optimization%20approaches%20%28e.g.%2C%20evolutionary%20strategies%29%2C%20Bayesian%20optimization%2C%20and%20a%20recent%20meta-learned%20competitor%20under%20matched%20budget%20constraints.%20Our%20evaluation%20demonstrates%20strong%20generalization%20capabilities%20without%20task-specific%20tuning.&entry.1838667208=http%3A//arxiv.org/abs/2602.15473v1&entry.124074799=Read"},
{"title": "Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents", "author": "Davide Paglieri and Bart\u0142omiej Cupia\u0142 and Jonathan Cook and Ulyana Piterbarg and Jens Tuyls and Edward Grefenstette and Jakob Nicolaus Foerster and Jack Parker-Holder and Tim Rockt\u00e4schel", "abstract": "Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities and highlighting the potential for safer and more collaborative agentic systems.", "link": "http://arxiv.org/abs/2509.03581v3", "date": "2026-02-17", "relevancy": 1.6627, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5855}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20When%20to%20Plan%3A%20Efficiently%20Allocating%20Test-Time%20Compute%20for%20LLM%20Agents&body=Title%3A%20Learning%20When%20to%20Plan%3A%20Efficiently%20Allocating%20Test-Time%20Compute%20for%20LLM%20Agents%0AAuthor%3A%20Davide%20Paglieri%20and%20Bart%C5%82omiej%20Cupia%C5%82%20and%20Jonathan%20Cook%20and%20Ulyana%20Piterbarg%20and%20Jens%20Tuyls%20and%20Edward%20Grefenstette%20and%20Jakob%20Nicolaus%20Foerster%20and%20Jack%20Parker-Holder%20and%20Tim%20Rockt%C3%A4schel%0AAbstract%3A%20Training%20large%20language%20models%20%28LLMs%29%20to%20reason%20via%20reinforcement%20learning%20%28RL%29%20significantly%20improves%20their%20problem-solving%20capabilities.%20In%20agentic%20settings%2C%20existing%20methods%20like%20ReAct%20prompt%20LLMs%20to%20explicitly%20plan%20before%20every%20action%3B%20however%2C%20we%20demonstrate%20that%20always%20planning%20is%20computationally%20expensive%20and%20degrades%20performance%20on%20long-horizon%20tasks%2C%20while%20never%20planning%20further%20limits%20performance.%20To%20address%20this%2C%20we%20introduce%20a%20conceptual%20framework%20formalizing%20dynamic%20planning%20for%20LLM%20agents%2C%20enabling%20them%20to%20flexibly%20decide%20when%20to%20allocate%20test-time%20compute%20for%20planning.%20We%20propose%20a%20simple%20two-stage%20training%20pipeline%3A%20%281%29%20supervised%20fine-tuning%20on%20diverse%20synthetic%20data%20to%20prime%20models%20for%20dynamic%20planning%2C%20and%20%282%29%20RL%20to%20refine%20this%20capability%20in%20long-horizon%20environments.%20Experiments%20on%20the%20Crafter%20environment%20show%20that%20dynamic%20planning%20agents%20trained%20with%20this%20approach%20are%20more%20sample-efficient%20and%20consistently%20achieve%20more%20complex%20objectives.%20Additionally%2C%20we%20demonstrate%20that%20these%20agents%20can%20be%20effectively%20steered%20by%20human-written%20plans%2C%20surpassing%20their%20independent%20capabilities%20and%20highlighting%20the%20potential%20for%20safer%20and%20more%20collaborative%20agentic%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2509.03581v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520When%2520to%2520Plan%253A%2520Efficiently%2520Allocating%2520Test-Time%2520Compute%2520for%2520LLM%2520Agents%26entry.906535625%3DDavide%2520Paglieri%2520and%2520Bart%25C5%2582omiej%2520Cupia%25C5%2582%2520and%2520Jonathan%2520Cook%2520and%2520Ulyana%2520Piterbarg%2520and%2520Jens%2520Tuyls%2520and%2520Edward%2520Grefenstette%2520and%2520Jakob%2520Nicolaus%2520Foerster%2520and%2520Jack%2520Parker-Holder%2520and%2520Tim%2520Rockt%25C3%25A4schel%26entry.1292438233%3DTraining%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520reason%2520via%2520reinforcement%2520learning%2520%2528RL%2529%2520significantly%2520improves%2520their%2520problem-solving%2520capabilities.%2520In%2520agentic%2520settings%252C%2520existing%2520methods%2520like%2520ReAct%2520prompt%2520LLMs%2520to%2520explicitly%2520plan%2520before%2520every%2520action%253B%2520however%252C%2520we%2520demonstrate%2520that%2520always%2520planning%2520is%2520computationally%2520expensive%2520and%2520degrades%2520performance%2520on%2520long-horizon%2520tasks%252C%2520while%2520never%2520planning%2520further%2520limits%2520performance.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520conceptual%2520framework%2520formalizing%2520dynamic%2520planning%2520for%2520LLM%2520agents%252C%2520enabling%2520them%2520to%2520flexibly%2520decide%2520when%2520to%2520allocate%2520test-time%2520compute%2520for%2520planning.%2520We%2520propose%2520a%2520simple%2520two-stage%2520training%2520pipeline%253A%2520%25281%2529%2520supervised%2520fine-tuning%2520on%2520diverse%2520synthetic%2520data%2520to%2520prime%2520models%2520for%2520dynamic%2520planning%252C%2520and%2520%25282%2529%2520RL%2520to%2520refine%2520this%2520capability%2520in%2520long-horizon%2520environments.%2520Experiments%2520on%2520the%2520Crafter%2520environment%2520show%2520that%2520dynamic%2520planning%2520agents%2520trained%2520with%2520this%2520approach%2520are%2520more%2520sample-efficient%2520and%2520consistently%2520achieve%2520more%2520complex%2520objectives.%2520Additionally%252C%2520we%2520demonstrate%2520that%2520these%2520agents%2520can%2520be%2520effectively%2520steered%2520by%2520human-written%2520plans%252C%2520surpassing%2520their%2520independent%2520capabilities%2520and%2520highlighting%2520the%2520potential%2520for%2520safer%2520and%2520more%2520collaborative%2520agentic%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03581v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20When%20to%20Plan%3A%20Efficiently%20Allocating%20Test-Time%20Compute%20for%20LLM%20Agents&entry.906535625=Davide%20Paglieri%20and%20Bart%C5%82omiej%20Cupia%C5%82%20and%20Jonathan%20Cook%20and%20Ulyana%20Piterbarg%20and%20Jens%20Tuyls%20and%20Edward%20Grefenstette%20and%20Jakob%20Nicolaus%20Foerster%20and%20Jack%20Parker-Holder%20and%20Tim%20Rockt%C3%A4schel&entry.1292438233=Training%20large%20language%20models%20%28LLMs%29%20to%20reason%20via%20reinforcement%20learning%20%28RL%29%20significantly%20improves%20their%20problem-solving%20capabilities.%20In%20agentic%20settings%2C%20existing%20methods%20like%20ReAct%20prompt%20LLMs%20to%20explicitly%20plan%20before%20every%20action%3B%20however%2C%20we%20demonstrate%20that%20always%20planning%20is%20computationally%20expensive%20and%20degrades%20performance%20on%20long-horizon%20tasks%2C%20while%20never%20planning%20further%20limits%20performance.%20To%20address%20this%2C%20we%20introduce%20a%20conceptual%20framework%20formalizing%20dynamic%20planning%20for%20LLM%20agents%2C%20enabling%20them%20to%20flexibly%20decide%20when%20to%20allocate%20test-time%20compute%20for%20planning.%20We%20propose%20a%20simple%20two-stage%20training%20pipeline%3A%20%281%29%20supervised%20fine-tuning%20on%20diverse%20synthetic%20data%20to%20prime%20models%20for%20dynamic%20planning%2C%20and%20%282%29%20RL%20to%20refine%20this%20capability%20in%20long-horizon%20environments.%20Experiments%20on%20the%20Crafter%20environment%20show%20that%20dynamic%20planning%20agents%20trained%20with%20this%20approach%20are%20more%20sample-efficient%20and%20consistently%20achieve%20more%20complex%20objectives.%20Additionally%2C%20we%20demonstrate%20that%20these%20agents%20can%20be%20effectively%20steered%20by%20human-written%20plans%2C%20surpassing%20their%20independent%20capabilities%20and%20highlighting%20the%20potential%20for%20safer%20and%20more%20collaborative%20agentic%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2509.03581v3&entry.124074799=Read"},
{"title": "Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections", "author": "Xianglin Yang and Yufei He and Shuo Ji and Bryan Hooi and Jin Song Dong", "abstract": "Self-evolving LLM agents update their internal state across sessions, often by writing and reusing long-term memory. This design improves performance on long-horizon tasks but creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We study this risk and formalize a persistent attack we call a Zombie Agent, where an attacker covertly implants a payload that survives across sessions, effectively turning the agent into a puppet of the attacker.\n  We present a black-box attack framework that uses only indirect exposure through attacker-controlled web content. The attack has two phases. During infection, the agent reads a poisoned source while completing a benign task and writes the payload into long-term memory through its normal update process. During trigger, the payload is retrieved or carried forward and causes unauthorized tool behavior. We design mechanism-specific persistence strategies for common memory implementations, including sliding-window and retrieval-augmented memory, to resist truncation and relevance filtering. We evaluate the attack on representative agent setups and tasks, measuring both persistence over time and the ability to induce unauthorized actions while preserving benign task quality. Our results show that memory evolution can convert one-time indirect injection into persistent compromise, which suggests that defenses focused only on per-session prompt filtering are not sufficient for self-evolving agents.", "link": "http://arxiv.org/abs/2602.15654v1", "date": "2026-02-17", "relevancy": 1.9232, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5297}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4574}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zombie%20Agents%3A%20Persistent%20Control%20of%20Self-Evolving%20LLM%20Agents%20via%20Self-Reinforcing%20Injections&body=Title%3A%20Zombie%20Agents%3A%20Persistent%20Control%20of%20Self-Evolving%20LLM%20Agents%20via%20Self-Reinforcing%20Injections%0AAuthor%3A%20Xianglin%20Yang%20and%20Yufei%20He%20and%20Shuo%20Ji%20and%20Bryan%20Hooi%20and%20Jin%20Song%20Dong%0AAbstract%3A%20Self-evolving%20LLM%20agents%20update%20their%20internal%20state%20across%20sessions%2C%20often%20by%20writing%20and%20reusing%20long-term%20memory.%20This%20design%20improves%20performance%20on%20long-horizon%20tasks%20but%20creates%20a%20security%20risk%3A%20untrusted%20external%20content%20observed%20during%20a%20benign%20session%20can%20be%20stored%20as%20memory%20and%20later%20treated%20as%20instruction.%20We%20study%20this%20risk%20and%20formalize%20a%20persistent%20attack%20we%20call%20a%20Zombie%20Agent%2C%20where%20an%20attacker%20covertly%20implants%20a%20payload%20that%20survives%20across%20sessions%2C%20effectively%20turning%20the%20agent%20into%20a%20puppet%20of%20the%20attacker.%0A%20%20We%20present%20a%20black-box%20attack%20framework%20that%20uses%20only%20indirect%20exposure%20through%20attacker-controlled%20web%20content.%20The%20attack%20has%20two%20phases.%20During%20infection%2C%20the%20agent%20reads%20a%20poisoned%20source%20while%20completing%20a%20benign%20task%20and%20writes%20the%20payload%20into%20long-term%20memory%20through%20its%20normal%20update%20process.%20During%20trigger%2C%20the%20payload%20is%20retrieved%20or%20carried%20forward%20and%20causes%20unauthorized%20tool%20behavior.%20We%20design%20mechanism-specific%20persistence%20strategies%20for%20common%20memory%20implementations%2C%20including%20sliding-window%20and%20retrieval-augmented%20memory%2C%20to%20resist%20truncation%20and%20relevance%20filtering.%20We%20evaluate%20the%20attack%20on%20representative%20agent%20setups%20and%20tasks%2C%20measuring%20both%20persistence%20over%20time%20and%20the%20ability%20to%20induce%20unauthorized%20actions%20while%20preserving%20benign%20task%20quality.%20Our%20results%20show%20that%20memory%20evolution%20can%20convert%20one-time%20indirect%20injection%20into%20persistent%20compromise%2C%20which%20suggests%20that%20defenses%20focused%20only%20on%20per-session%20prompt%20filtering%20are%20not%20sufficient%20for%20self-evolving%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZombie%2520Agents%253A%2520Persistent%2520Control%2520of%2520Self-Evolving%2520LLM%2520Agents%2520via%2520Self-Reinforcing%2520Injections%26entry.906535625%3DXianglin%2520Yang%2520and%2520Yufei%2520He%2520and%2520Shuo%2520Ji%2520and%2520Bryan%2520Hooi%2520and%2520Jin%2520Song%2520Dong%26entry.1292438233%3DSelf-evolving%2520LLM%2520agents%2520update%2520their%2520internal%2520state%2520across%2520sessions%252C%2520often%2520by%2520writing%2520and%2520reusing%2520long-term%2520memory.%2520This%2520design%2520improves%2520performance%2520on%2520long-horizon%2520tasks%2520but%2520creates%2520a%2520security%2520risk%253A%2520untrusted%2520external%2520content%2520observed%2520during%2520a%2520benign%2520session%2520can%2520be%2520stored%2520as%2520memory%2520and%2520later%2520treated%2520as%2520instruction.%2520We%2520study%2520this%2520risk%2520and%2520formalize%2520a%2520persistent%2520attack%2520we%2520call%2520a%2520Zombie%2520Agent%252C%2520where%2520an%2520attacker%2520covertly%2520implants%2520a%2520payload%2520that%2520survives%2520across%2520sessions%252C%2520effectively%2520turning%2520the%2520agent%2520into%2520a%2520puppet%2520of%2520the%2520attacker.%250A%2520%2520We%2520present%2520a%2520black-box%2520attack%2520framework%2520that%2520uses%2520only%2520indirect%2520exposure%2520through%2520attacker-controlled%2520web%2520content.%2520The%2520attack%2520has%2520two%2520phases.%2520During%2520infection%252C%2520the%2520agent%2520reads%2520a%2520poisoned%2520source%2520while%2520completing%2520a%2520benign%2520task%2520and%2520writes%2520the%2520payload%2520into%2520long-term%2520memory%2520through%2520its%2520normal%2520update%2520process.%2520During%2520trigger%252C%2520the%2520payload%2520is%2520retrieved%2520or%2520carried%2520forward%2520and%2520causes%2520unauthorized%2520tool%2520behavior.%2520We%2520design%2520mechanism-specific%2520persistence%2520strategies%2520for%2520common%2520memory%2520implementations%252C%2520including%2520sliding-window%2520and%2520retrieval-augmented%2520memory%252C%2520to%2520resist%2520truncation%2520and%2520relevance%2520filtering.%2520We%2520evaluate%2520the%2520attack%2520on%2520representative%2520agent%2520setups%2520and%2520tasks%252C%2520measuring%2520both%2520persistence%2520over%2520time%2520and%2520the%2520ability%2520to%2520induce%2520unauthorized%2520actions%2520while%2520preserving%2520benign%2520task%2520quality.%2520Our%2520results%2520show%2520that%2520memory%2520evolution%2520can%2520convert%2520one-time%2520indirect%2520injection%2520into%2520persistent%2520compromise%252C%2520which%2520suggests%2520that%2520defenses%2520focused%2520only%2520on%2520per-session%2520prompt%2520filtering%2520are%2520not%2520sufficient%2520for%2520self-evolving%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zombie%20Agents%3A%20Persistent%20Control%20of%20Self-Evolving%20LLM%20Agents%20via%20Self-Reinforcing%20Injections&entry.906535625=Xianglin%20Yang%20and%20Yufei%20He%20and%20Shuo%20Ji%20and%20Bryan%20Hooi%20and%20Jin%20Song%20Dong&entry.1292438233=Self-evolving%20LLM%20agents%20update%20their%20internal%20state%20across%20sessions%2C%20often%20by%20writing%20and%20reusing%20long-term%20memory.%20This%20design%20improves%20performance%20on%20long-horizon%20tasks%20but%20creates%20a%20security%20risk%3A%20untrusted%20external%20content%20observed%20during%20a%20benign%20session%20can%20be%20stored%20as%20memory%20and%20later%20treated%20as%20instruction.%20We%20study%20this%20risk%20and%20formalize%20a%20persistent%20attack%20we%20call%20a%20Zombie%20Agent%2C%20where%20an%20attacker%20covertly%20implants%20a%20payload%20that%20survives%20across%20sessions%2C%20effectively%20turning%20the%20agent%20into%20a%20puppet%20of%20the%20attacker.%0A%20%20We%20present%20a%20black-box%20attack%20framework%20that%20uses%20only%20indirect%20exposure%20through%20attacker-controlled%20web%20content.%20The%20attack%20has%20two%20phases.%20During%20infection%2C%20the%20agent%20reads%20a%20poisoned%20source%20while%20completing%20a%20benign%20task%20and%20writes%20the%20payload%20into%20long-term%20memory%20through%20its%20normal%20update%20process.%20During%20trigger%2C%20the%20payload%20is%20retrieved%20or%20carried%20forward%20and%20causes%20unauthorized%20tool%20behavior.%20We%20design%20mechanism-specific%20persistence%20strategies%20for%20common%20memory%20implementations%2C%20including%20sliding-window%20and%20retrieval-augmented%20memory%2C%20to%20resist%20truncation%20and%20relevance%20filtering.%20We%20evaluate%20the%20attack%20on%20representative%20agent%20setups%20and%20tasks%2C%20measuring%20both%20persistence%20over%20time%20and%20the%20ability%20to%20induce%20unauthorized%20actions%20while%20preserving%20benign%20task%20quality.%20Our%20results%20show%20that%20memory%20evolution%20can%20convert%20one-time%20indirect%20injection%20into%20persistent%20compromise%2C%20which%20suggests%20that%20defenses%20focused%20only%20on%20per-session%20prompt%20filtering%20are%20not%20sufficient%20for%20self-evolving%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2602.15654v1&entry.124074799=Read"},
{"title": "Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum Circuit Synthesis", "author": "Sara Giordano and Kornikar Sen and Miguel A. Martin-Delgado", "abstract": "A reinforcement learning (RL) framework is introduced for the efficient synthesis of quantum circuits that generate specified target quantum states from a fixed initial state, addressing a central challenge in both the Noisy Intermediate-Scale Quantum (NISQ) era and future fault-tolerant quantum computing. The approach utilizes tabular Q-learning, based on action sequences, within a discretized quantum state space, to effectively manage the exponential growth of the space dimension. The framework introduces a hybrid reward mechanism, combining a static, domain-informed reward that guides the agent toward the target state with customizable dynamic penalties that discourage inefficient circuit structures such as gate congestion and redundant state revisits. This is a circuit-aware reward, in contrast to the current trend of works on this topic, which are primarily fidelity-based. By leveraging sparse matrix representations and state-space discretization, the method enables practical navigation of high-dimensional environments while minimizing computational overhead. Benchmarking on graph-state preparation tasks for up to seven qubits, we demonstrate that the algorithm consistently discovers minimal-depth circuits with optimized gate counts. Moreover, extending the framework to a universal gate set still yields low depth circuits, highlighting the algorithm robustness and adaptability. The results confirm that this RL-driven approach, with our completely circuit-aware method, efficiently explores the complex quantum state space and synthesizes near-optimal quantum circuits, providing a resource-efficient foundation for quantum circuit optimization.", "link": "http://arxiv.org/abs/2507.16641v3", "date": "2026-02-17", "relevancy": 1.7652, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4596}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4333}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Reward-Driven%20Reinforcement%20Learning%20for%20Efficient%20Quantum%20Circuit%20Synthesis&body=Title%3A%20Hybrid%20Reward-Driven%20Reinforcement%20Learning%20for%20Efficient%20Quantum%20Circuit%20Synthesis%0AAuthor%3A%20Sara%20Giordano%20and%20Kornikar%20Sen%20and%20Miguel%20A.%20Martin-Delgado%0AAbstract%3A%20A%20reinforcement%20learning%20%28RL%29%20framework%20is%20introduced%20for%20the%20efficient%20synthesis%20of%20quantum%20circuits%20that%20generate%20specified%20target%20quantum%20states%20from%20a%20fixed%20initial%20state%2C%20addressing%20a%20central%20challenge%20in%20both%20the%20Noisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%20era%20and%20future%20fault-tolerant%20quantum%20computing.%20The%20approach%20utilizes%20tabular%20Q-learning%2C%20based%20on%20action%20sequences%2C%20within%20a%20discretized%20quantum%20state%20space%2C%20to%20effectively%20manage%20the%20exponential%20growth%20of%20the%20space%20dimension.%20The%20framework%20introduces%20a%20hybrid%20reward%20mechanism%2C%20combining%20a%20static%2C%20domain-informed%20reward%20that%20guides%20the%20agent%20toward%20the%20target%20state%20with%20customizable%20dynamic%20penalties%20that%20discourage%20inefficient%20circuit%20structures%20such%20as%20gate%20congestion%20and%20redundant%20state%20revisits.%20This%20is%20a%20circuit-aware%20reward%2C%20in%20contrast%20to%20the%20current%20trend%20of%20works%20on%20this%20topic%2C%20which%20are%20primarily%20fidelity-based.%20By%20leveraging%20sparse%20matrix%20representations%20and%20state-space%20discretization%2C%20the%20method%20enables%20practical%20navigation%20of%20high-dimensional%20environments%20while%20minimizing%20computational%20overhead.%20Benchmarking%20on%20graph-state%20preparation%20tasks%20for%20up%20to%20seven%20qubits%2C%20we%20demonstrate%20that%20the%20algorithm%20consistently%20discovers%20minimal-depth%20circuits%20with%20optimized%20gate%20counts.%20Moreover%2C%20extending%20the%20framework%20to%20a%20universal%20gate%20set%20still%20yields%20low%20depth%20circuits%2C%20highlighting%20the%20algorithm%20robustness%20and%20adaptability.%20The%20results%20confirm%20that%20this%20RL-driven%20approach%2C%20with%20our%20completely%20circuit-aware%20method%2C%20efficiently%20explores%20the%20complex%20quantum%20state%20space%20and%20synthesizes%20near-optimal%20quantum%20circuits%2C%20providing%20a%20resource-efficient%20foundation%20for%20quantum%20circuit%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2507.16641v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Reward-Driven%2520Reinforcement%2520Learning%2520for%2520Efficient%2520Quantum%2520Circuit%2520Synthesis%26entry.906535625%3DSara%2520Giordano%2520and%2520Kornikar%2520Sen%2520and%2520Miguel%2520A.%2520Martin-Delgado%26entry.1292438233%3DA%2520reinforcement%2520learning%2520%2528RL%2529%2520framework%2520is%2520introduced%2520for%2520the%2520efficient%2520synthesis%2520of%2520quantum%2520circuits%2520that%2520generate%2520specified%2520target%2520quantum%2520states%2520from%2520a%2520fixed%2520initial%2520state%252C%2520addressing%2520a%2520central%2520challenge%2520in%2520both%2520the%2520Noisy%2520Intermediate-Scale%2520Quantum%2520%2528NISQ%2529%2520era%2520and%2520future%2520fault-tolerant%2520quantum%2520computing.%2520The%2520approach%2520utilizes%2520tabular%2520Q-learning%252C%2520based%2520on%2520action%2520sequences%252C%2520within%2520a%2520discretized%2520quantum%2520state%2520space%252C%2520to%2520effectively%2520manage%2520the%2520exponential%2520growth%2520of%2520the%2520space%2520dimension.%2520The%2520framework%2520introduces%2520a%2520hybrid%2520reward%2520mechanism%252C%2520combining%2520a%2520static%252C%2520domain-informed%2520reward%2520that%2520guides%2520the%2520agent%2520toward%2520the%2520target%2520state%2520with%2520customizable%2520dynamic%2520penalties%2520that%2520discourage%2520inefficient%2520circuit%2520structures%2520such%2520as%2520gate%2520congestion%2520and%2520redundant%2520state%2520revisits.%2520This%2520is%2520a%2520circuit-aware%2520reward%252C%2520in%2520contrast%2520to%2520the%2520current%2520trend%2520of%2520works%2520on%2520this%2520topic%252C%2520which%2520are%2520primarily%2520fidelity-based.%2520By%2520leveraging%2520sparse%2520matrix%2520representations%2520and%2520state-space%2520discretization%252C%2520the%2520method%2520enables%2520practical%2520navigation%2520of%2520high-dimensional%2520environments%2520while%2520minimizing%2520computational%2520overhead.%2520Benchmarking%2520on%2520graph-state%2520preparation%2520tasks%2520for%2520up%2520to%2520seven%2520qubits%252C%2520we%2520demonstrate%2520that%2520the%2520algorithm%2520consistently%2520discovers%2520minimal-depth%2520circuits%2520with%2520optimized%2520gate%2520counts.%2520Moreover%252C%2520extending%2520the%2520framework%2520to%2520a%2520universal%2520gate%2520set%2520still%2520yields%2520low%2520depth%2520circuits%252C%2520highlighting%2520the%2520algorithm%2520robustness%2520and%2520adaptability.%2520The%2520results%2520confirm%2520that%2520this%2520RL-driven%2520approach%252C%2520with%2520our%2520completely%2520circuit-aware%2520method%252C%2520efficiently%2520explores%2520the%2520complex%2520quantum%2520state%2520space%2520and%2520synthesizes%2520near-optimal%2520quantum%2520circuits%252C%2520providing%2520a%2520resource-efficient%2520foundation%2520for%2520quantum%2520circuit%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16641v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Reward-Driven%20Reinforcement%20Learning%20for%20Efficient%20Quantum%20Circuit%20Synthesis&entry.906535625=Sara%20Giordano%20and%20Kornikar%20Sen%20and%20Miguel%20A.%20Martin-Delgado&entry.1292438233=A%20reinforcement%20learning%20%28RL%29%20framework%20is%20introduced%20for%20the%20efficient%20synthesis%20of%20quantum%20circuits%20that%20generate%20specified%20target%20quantum%20states%20from%20a%20fixed%20initial%20state%2C%20addressing%20a%20central%20challenge%20in%20both%20the%20Noisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%20era%20and%20future%20fault-tolerant%20quantum%20computing.%20The%20approach%20utilizes%20tabular%20Q-learning%2C%20based%20on%20action%20sequences%2C%20within%20a%20discretized%20quantum%20state%20space%2C%20to%20effectively%20manage%20the%20exponential%20growth%20of%20the%20space%20dimension.%20The%20framework%20introduces%20a%20hybrid%20reward%20mechanism%2C%20combining%20a%20static%2C%20domain-informed%20reward%20that%20guides%20the%20agent%20toward%20the%20target%20state%20with%20customizable%20dynamic%20penalties%20that%20discourage%20inefficient%20circuit%20structures%20such%20as%20gate%20congestion%20and%20redundant%20state%20revisits.%20This%20is%20a%20circuit-aware%20reward%2C%20in%20contrast%20to%20the%20current%20trend%20of%20works%20on%20this%20topic%2C%20which%20are%20primarily%20fidelity-based.%20By%20leveraging%20sparse%20matrix%20representations%20and%20state-space%20discretization%2C%20the%20method%20enables%20practical%20navigation%20of%20high-dimensional%20environments%20while%20minimizing%20computational%20overhead.%20Benchmarking%20on%20graph-state%20preparation%20tasks%20for%20up%20to%20seven%20qubits%2C%20we%20demonstrate%20that%20the%20algorithm%20consistently%20discovers%20minimal-depth%20circuits%20with%20optimized%20gate%20counts.%20Moreover%2C%20extending%20the%20framework%20to%20a%20universal%20gate%20set%20still%20yields%20low%20depth%20circuits%2C%20highlighting%20the%20algorithm%20robustness%20and%20adaptability.%20The%20results%20confirm%20that%20this%20RL-driven%20approach%2C%20with%20our%20completely%20circuit-aware%20method%2C%20efficiently%20explores%20the%20complex%20quantum%20state%20space%20and%20synthesizes%20near-optimal%20quantum%20circuits%2C%20providing%20a%20resource-efficient%20foundation%20for%20quantum%20circuit%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2507.16641v3&entry.124074799=Read"},
{"title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing", "author": "Zarif Ikram and Arad Firouzkouhi and Stephen Tu and Mahdi Soltanolkotabi and Paria Rashidinejad", "abstract": "A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.", "link": "http://arxiv.org/abs/2602.15823v1", "date": "2026-02-17", "relevancy": 1.4662, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4939}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4927}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrispEdit%3A%20Low-Curvature%20Projections%20for%20Scalable%20Non-Destructive%20LLM%20Editing&body=Title%3A%20CrispEdit%3A%20Low-Curvature%20Projections%20for%20Scalable%20Non-Destructive%20LLM%20Editing%0AAuthor%3A%20Zarif%20Ikram%20and%20Arad%20Firouzkouhi%20and%20Stephen%20Tu%20and%20Mahdi%20Soltanolkotabi%20and%20Paria%20Rashidinejad%0AAbstract%3A%20A%20central%20challenge%20in%20large%20language%20model%20%28LLM%29%20editing%20is%20capability%20preservation%3A%20methods%20that%20successfully%20change%20targeted%20behavior%20can%20quietly%20game%20the%20editing%20proxy%20and%20corrupt%20general%20capabilities%2C%20producing%20degenerate%20behaviors%20reminiscent%20of%20proxy/reward%20hacking.%20We%20present%20CrispEdit%2C%20a%20scalable%20and%20principled%20second-order%20editing%20algorithm%20that%20treats%20capability%20preservation%20as%20an%20explicit%20constraint%2C%20unifying%20and%20generalizing%20several%20existing%20editing%20approaches.%20CrispEdit%20formulates%20editing%20as%20constrained%20optimization%20and%20enforces%20the%20constraint%20by%20projecting%20edit%20updates%20onto%20the%20low-curvature%20subspace%20of%20the%20capability-loss%20landscape.%20At%20the%20crux%20of%20CrispEdit%20is%20expressing%20capability%20constraint%20via%20Bregman%20divergence%2C%20whose%20quadratic%20form%20yields%20the%20Gauss-Newton%20Hessian%20exactly%20and%20even%20when%20the%20base%20model%20is%20not%20trained%20to%20convergence.%20We%20make%20this%20second-order%20procedure%20efficient%20at%20the%20LLM%20scale%20using%20Kronecker-factored%20approximate%20curvature%20%28K-FAC%29%20and%20a%20novel%20matrix-free%20projector%20that%20exploits%20Kronecker%20structure%20to%20avoid%20constructing%20massive%20projection%20matrices.%20Across%20standard%20model-editing%20benchmarks%2C%20CrispEdit%20achieves%20high%20edit%20success%20while%20keeping%20capability%20degradation%20below%201%25%20on%20average%20across%20datasets%2C%20significantly%20improving%20over%20prior%20editors.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrispEdit%253A%2520Low-Curvature%2520Projections%2520for%2520Scalable%2520Non-Destructive%2520LLM%2520Editing%26entry.906535625%3DZarif%2520Ikram%2520and%2520Arad%2520Firouzkouhi%2520and%2520Stephen%2520Tu%2520and%2520Mahdi%2520Soltanolkotabi%2520and%2520Paria%2520Rashidinejad%26entry.1292438233%3DA%2520central%2520challenge%2520in%2520large%2520language%2520model%2520%2528LLM%2529%2520editing%2520is%2520capability%2520preservation%253A%2520methods%2520that%2520successfully%2520change%2520targeted%2520behavior%2520can%2520quietly%2520game%2520the%2520editing%2520proxy%2520and%2520corrupt%2520general%2520capabilities%252C%2520producing%2520degenerate%2520behaviors%2520reminiscent%2520of%2520proxy/reward%2520hacking.%2520We%2520present%2520CrispEdit%252C%2520a%2520scalable%2520and%2520principled%2520second-order%2520editing%2520algorithm%2520that%2520treats%2520capability%2520preservation%2520as%2520an%2520explicit%2520constraint%252C%2520unifying%2520and%2520generalizing%2520several%2520existing%2520editing%2520approaches.%2520CrispEdit%2520formulates%2520editing%2520as%2520constrained%2520optimization%2520and%2520enforces%2520the%2520constraint%2520by%2520projecting%2520edit%2520updates%2520onto%2520the%2520low-curvature%2520subspace%2520of%2520the%2520capability-loss%2520landscape.%2520At%2520the%2520crux%2520of%2520CrispEdit%2520is%2520expressing%2520capability%2520constraint%2520via%2520Bregman%2520divergence%252C%2520whose%2520quadratic%2520form%2520yields%2520the%2520Gauss-Newton%2520Hessian%2520exactly%2520and%2520even%2520when%2520the%2520base%2520model%2520is%2520not%2520trained%2520to%2520convergence.%2520We%2520make%2520this%2520second-order%2520procedure%2520efficient%2520at%2520the%2520LLM%2520scale%2520using%2520Kronecker-factored%2520approximate%2520curvature%2520%2528K-FAC%2529%2520and%2520a%2520novel%2520matrix-free%2520projector%2520that%2520exploits%2520Kronecker%2520structure%2520to%2520avoid%2520constructing%2520massive%2520projection%2520matrices.%2520Across%2520standard%2520model-editing%2520benchmarks%252C%2520CrispEdit%2520achieves%2520high%2520edit%2520success%2520while%2520keeping%2520capability%2520degradation%2520below%25201%2525%2520on%2520average%2520across%2520datasets%252C%2520significantly%2520improving%2520over%2520prior%2520editors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrispEdit%3A%20Low-Curvature%20Projections%20for%20Scalable%20Non-Destructive%20LLM%20Editing&entry.906535625=Zarif%20Ikram%20and%20Arad%20Firouzkouhi%20and%20Stephen%20Tu%20and%20Mahdi%20Soltanolkotabi%20and%20Paria%20Rashidinejad&entry.1292438233=A%20central%20challenge%20in%20large%20language%20model%20%28LLM%29%20editing%20is%20capability%20preservation%3A%20methods%20that%20successfully%20change%20targeted%20behavior%20can%20quietly%20game%20the%20editing%20proxy%20and%20corrupt%20general%20capabilities%2C%20producing%20degenerate%20behaviors%20reminiscent%20of%20proxy/reward%20hacking.%20We%20present%20CrispEdit%2C%20a%20scalable%20and%20principled%20second-order%20editing%20algorithm%20that%20treats%20capability%20preservation%20as%20an%20explicit%20constraint%2C%20unifying%20and%20generalizing%20several%20existing%20editing%20approaches.%20CrispEdit%20formulates%20editing%20as%20constrained%20optimization%20and%20enforces%20the%20constraint%20by%20projecting%20edit%20updates%20onto%20the%20low-curvature%20subspace%20of%20the%20capability-loss%20landscape.%20At%20the%20crux%20of%20CrispEdit%20is%20expressing%20capability%20constraint%20via%20Bregman%20divergence%2C%20whose%20quadratic%20form%20yields%20the%20Gauss-Newton%20Hessian%20exactly%20and%20even%20when%20the%20base%20model%20is%20not%20trained%20to%20convergence.%20We%20make%20this%20second-order%20procedure%20efficient%20at%20the%20LLM%20scale%20using%20Kronecker-factored%20approximate%20curvature%20%28K-FAC%29%20and%20a%20novel%20matrix-free%20projector%20that%20exploits%20Kronecker%20structure%20to%20avoid%20constructing%20massive%20projection%20matrices.%20Across%20standard%20model-editing%20benchmarks%2C%20CrispEdit%20achieves%20high%20edit%20success%20while%20keeping%20capability%20degradation%20below%201%25%20on%20average%20across%20datasets%2C%20significantly%20improving%20over%20prior%20editors.&entry.1838667208=http%3A//arxiv.org/abs/2602.15823v1&entry.124074799=Read"},
{"title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning", "author": "Mingda Zhang and Haoran Luo and Tiesunlong Shen and Qika Lin and Xiaoying Tang and Rui Mao and Erik Cambria", "abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.", "link": "http://arxiv.org/abs/2602.01664v3", "date": "2026-02-17", "relevancy": 1.5309, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5405}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5119}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlowSteer%3A%20Interactive%20Agentic%20Workflow%20Orchestration%20via%20End-to-End%20Reinforcement%20Learning&body=Title%3A%20FlowSteer%3A%20Interactive%20Agentic%20Workflow%20Orchestration%20via%20End-to-End%20Reinforcement%20Learning%0AAuthor%3A%20Mingda%20Zhang%20and%20Haoran%20Luo%20and%20Tiesunlong%20Shen%20and%20Qika%20Lin%20and%20Xiaoying%20Tang%20and%20Rui%20Mao%20and%20Erik%20Cambria%0AAbstract%3A%20In%20recent%20years%2C%20a%20variety%20of%20powerful%20agentic%20workflows%20have%20been%20applied%20to%20solve%20a%20wide%20range%20of%20human%20problems.%20However%2C%20existing%20workflow%20orchestration%20still%20faces%20key%20challenges%2C%20including%20high%20manual%20cost%2C%20reliance%20on%20specific%20operators/large%20language%20models%20%28LLMs%29%2C%20and%20sparse%20reward%20signals.%20To%20address%20these%20challenges%2C%20we%20propose%20FlowSteer%2C%20an%20end-to-end%20reinforcement%20learning%20framework%20that%20takes%20a%20lightweight%20policy%20model%20as%20the%20agent%20and%20an%20executable%20canvas%20environment%2C%20automating%20workflow%20orchestration%20through%20multi-turn%20interaction.%20In%20this%20process%2C%20the%20policy%20model%20analyzes%20execution%20states%20and%20selects%20editing%20actions%2C%20while%20the%20canvas%20executes%20operators%20and%20returns%20feedback%20for%20iterative%20refinement.%20Moreover%2C%20FlowSteer%20provides%20a%20plug-and-play%20framework%20that%20supports%20diverse%20operator%20libraries%20and%20interchangeable%20LLM%20backends.%20To%20effectively%20train%20this%20interaction%20paradigm%2C%20we%20propose%20Canvas%20Workflow%20Relative%20Policy%20Optimization%20%28CWRPO%29%2C%20which%20introduces%20diversity-constrained%20rewards%20with%20conditional%20release%20to%20stabilize%20learning%20and%20suppress%20shortcut%20behaviors.%20Experimental%20results%20on%20twelve%20datasets%20show%20that%20FlowSteer%20significantly%20outperforms%20baselines%20across%20various%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01664v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlowSteer%253A%2520Interactive%2520Agentic%2520Workflow%2520Orchestration%2520via%2520End-to-End%2520Reinforcement%2520Learning%26entry.906535625%3DMingda%2520Zhang%2520and%2520Haoran%2520Luo%2520and%2520Tiesunlong%2520Shen%2520and%2520Qika%2520Lin%2520and%2520Xiaoying%2520Tang%2520and%2520Rui%2520Mao%2520and%2520Erik%2520Cambria%26entry.1292438233%3DIn%2520recent%2520years%252C%2520a%2520variety%2520of%2520powerful%2520agentic%2520workflows%2520have%2520been%2520applied%2520to%2520solve%2520a%2520wide%2520range%2520of%2520human%2520problems.%2520However%252C%2520existing%2520workflow%2520orchestration%2520still%2520faces%2520key%2520challenges%252C%2520including%2520high%2520manual%2520cost%252C%2520reliance%2520on%2520specific%2520operators/large%2520language%2520models%2520%2528LLMs%2529%252C%2520and%2520sparse%2520reward%2520signals.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520FlowSteer%252C%2520an%2520end-to-end%2520reinforcement%2520learning%2520framework%2520that%2520takes%2520a%2520lightweight%2520policy%2520model%2520as%2520the%2520agent%2520and%2520an%2520executable%2520canvas%2520environment%252C%2520automating%2520workflow%2520orchestration%2520through%2520multi-turn%2520interaction.%2520In%2520this%2520process%252C%2520the%2520policy%2520model%2520analyzes%2520execution%2520states%2520and%2520selects%2520editing%2520actions%252C%2520while%2520the%2520canvas%2520executes%2520operators%2520and%2520returns%2520feedback%2520for%2520iterative%2520refinement.%2520Moreover%252C%2520FlowSteer%2520provides%2520a%2520plug-and-play%2520framework%2520that%2520supports%2520diverse%2520operator%2520libraries%2520and%2520interchangeable%2520LLM%2520backends.%2520To%2520effectively%2520train%2520this%2520interaction%2520paradigm%252C%2520we%2520propose%2520Canvas%2520Workflow%2520Relative%2520Policy%2520Optimization%2520%2528CWRPO%2529%252C%2520which%2520introduces%2520diversity-constrained%2520rewards%2520with%2520conditional%2520release%2520to%2520stabilize%2520learning%2520and%2520suppress%2520shortcut%2520behaviors.%2520Experimental%2520results%2520on%2520twelve%2520datasets%2520show%2520that%2520FlowSteer%2520significantly%2520outperforms%2520baselines%2520across%2520various%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01664v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlowSteer%3A%20Interactive%20Agentic%20Workflow%20Orchestration%20via%20End-to-End%20Reinforcement%20Learning&entry.906535625=Mingda%20Zhang%20and%20Haoran%20Luo%20and%20Tiesunlong%20Shen%20and%20Qika%20Lin%20and%20Xiaoying%20Tang%20and%20Rui%20Mao%20and%20Erik%20Cambria&entry.1292438233=In%20recent%20years%2C%20a%20variety%20of%20powerful%20agentic%20workflows%20have%20been%20applied%20to%20solve%20a%20wide%20range%20of%20human%20problems.%20However%2C%20existing%20workflow%20orchestration%20still%20faces%20key%20challenges%2C%20including%20high%20manual%20cost%2C%20reliance%20on%20specific%20operators/large%20language%20models%20%28LLMs%29%2C%20and%20sparse%20reward%20signals.%20To%20address%20these%20challenges%2C%20we%20propose%20FlowSteer%2C%20an%20end-to-end%20reinforcement%20learning%20framework%20that%20takes%20a%20lightweight%20policy%20model%20as%20the%20agent%20and%20an%20executable%20canvas%20environment%2C%20automating%20workflow%20orchestration%20through%20multi-turn%20interaction.%20In%20this%20process%2C%20the%20policy%20model%20analyzes%20execution%20states%20and%20selects%20editing%20actions%2C%20while%20the%20canvas%20executes%20operators%20and%20returns%20feedback%20for%20iterative%20refinement.%20Moreover%2C%20FlowSteer%20provides%20a%20plug-and-play%20framework%20that%20supports%20diverse%20operator%20libraries%20and%20interchangeable%20LLM%20backends.%20To%20effectively%20train%20this%20interaction%20paradigm%2C%20we%20propose%20Canvas%20Workflow%20Relative%20Policy%20Optimization%20%28CWRPO%29%2C%20which%20introduces%20diversity-constrained%20rewards%20with%20conditional%20release%20to%20stabilize%20learning%20and%20suppress%20shortcut%20behaviors.%20Experimental%20results%20on%20twelve%20datasets%20show%20that%20FlowSteer%20significantly%20outperforms%20baselines%20across%20various%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.01664v3&entry.124074799=Read"},
{"title": "Proactive Conversational Assistant for a Procedural Manual Task based on Audio and IMU", "author": "Rehana Mahfuz and Yinyi Guo and Erik Visser and Phanidhar Chinchili", "abstract": "Real-time conversational assistants for procedural tasks often depend on video input, which can be computationally expensive and compromise user privacy. For the first time, we propose a real-time conversational assistant that provides comprehensive guidance for a procedural task using only lightweight privacy-preserving modalities such as audio and IMU inputs from a user's wearable device to understand the context. This assistant proactively communicates step-by-step instructions to a user performing a furniture assembly task, and answers user questions. We construct a dataset containing conversations where the assistant guides the user in performing the task. On observing that an off-the-shelf language model is a very talkative assistant, we design a novel User Whim Agnostic (UWA) LoRA finetuning method which improves the model's ability to suppress less informative dialogues, while maintaining its tendency to communicate important instructions. This leads to >30% improvement in the F-score. Finetuning the model also results in a 16x speedup by eliminating the need to provide in-context examples in the prompt. We further describe how such an assistant is implemented on edge devices with no dependence on the cloud.", "link": "http://arxiv.org/abs/2602.15707v1", "date": "2026-02-17", "relevancy": 1.5362, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5164}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5129}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proactive%20Conversational%20Assistant%20for%20a%20Procedural%20Manual%20Task%20based%20on%20Audio%20and%20IMU&body=Title%3A%20Proactive%20Conversational%20Assistant%20for%20a%20Procedural%20Manual%20Task%20based%20on%20Audio%20and%20IMU%0AAuthor%3A%20Rehana%20Mahfuz%20and%20Yinyi%20Guo%20and%20Erik%20Visser%20and%20Phanidhar%20Chinchili%0AAbstract%3A%20Real-time%20conversational%20assistants%20for%20procedural%20tasks%20often%20depend%20on%20video%20input%2C%20which%20can%20be%20computationally%20expensive%20and%20compromise%20user%20privacy.%20For%20the%20first%20time%2C%20we%20propose%20a%20real-time%20conversational%20assistant%20that%20provides%20comprehensive%20guidance%20for%20a%20procedural%20task%20using%20only%20lightweight%20privacy-preserving%20modalities%20such%20as%20audio%20and%20IMU%20inputs%20from%20a%20user%27s%20wearable%20device%20to%20understand%20the%20context.%20This%20assistant%20proactively%20communicates%20step-by-step%20instructions%20to%20a%20user%20performing%20a%20furniture%20assembly%20task%2C%20and%20answers%20user%20questions.%20We%20construct%20a%20dataset%20containing%20conversations%20where%20the%20assistant%20guides%20the%20user%20in%20performing%20the%20task.%20On%20observing%20that%20an%20off-the-shelf%20language%20model%20is%20a%20very%20talkative%20assistant%2C%20we%20design%20a%20novel%20User%20Whim%20Agnostic%20%28UWA%29%20LoRA%20finetuning%20method%20which%20improves%20the%20model%27s%20ability%20to%20suppress%20less%20informative%20dialogues%2C%20while%20maintaining%20its%20tendency%20to%20communicate%20important%20instructions.%20This%20leads%20to%20%3E30%25%20improvement%20in%20the%20F-score.%20Finetuning%20the%20model%20also%20results%20in%20a%2016x%20speedup%20by%20eliminating%20the%20need%20to%20provide%20in-context%20examples%20in%20the%20prompt.%20We%20further%20describe%20how%20such%20an%20assistant%20is%20implemented%20on%20edge%20devices%20with%20no%20dependence%20on%20the%20cloud.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProactive%2520Conversational%2520Assistant%2520for%2520a%2520Procedural%2520Manual%2520Task%2520based%2520on%2520Audio%2520and%2520IMU%26entry.906535625%3DRehana%2520Mahfuz%2520and%2520Yinyi%2520Guo%2520and%2520Erik%2520Visser%2520and%2520Phanidhar%2520Chinchili%26entry.1292438233%3DReal-time%2520conversational%2520assistants%2520for%2520procedural%2520tasks%2520often%2520depend%2520on%2520video%2520input%252C%2520which%2520can%2520be%2520computationally%2520expensive%2520and%2520compromise%2520user%2520privacy.%2520For%2520the%2520first%2520time%252C%2520we%2520propose%2520a%2520real-time%2520conversational%2520assistant%2520that%2520provides%2520comprehensive%2520guidance%2520for%2520a%2520procedural%2520task%2520using%2520only%2520lightweight%2520privacy-preserving%2520modalities%2520such%2520as%2520audio%2520and%2520IMU%2520inputs%2520from%2520a%2520user%2527s%2520wearable%2520device%2520to%2520understand%2520the%2520context.%2520This%2520assistant%2520proactively%2520communicates%2520step-by-step%2520instructions%2520to%2520a%2520user%2520performing%2520a%2520furniture%2520assembly%2520task%252C%2520and%2520answers%2520user%2520questions.%2520We%2520construct%2520a%2520dataset%2520containing%2520conversations%2520where%2520the%2520assistant%2520guides%2520the%2520user%2520in%2520performing%2520the%2520task.%2520On%2520observing%2520that%2520an%2520off-the-shelf%2520language%2520model%2520is%2520a%2520very%2520talkative%2520assistant%252C%2520we%2520design%2520a%2520novel%2520User%2520Whim%2520Agnostic%2520%2528UWA%2529%2520LoRA%2520finetuning%2520method%2520which%2520improves%2520the%2520model%2527s%2520ability%2520to%2520suppress%2520less%2520informative%2520dialogues%252C%2520while%2520maintaining%2520its%2520tendency%2520to%2520communicate%2520important%2520instructions.%2520This%2520leads%2520to%2520%253E30%2525%2520improvement%2520in%2520the%2520F-score.%2520Finetuning%2520the%2520model%2520also%2520results%2520in%2520a%252016x%2520speedup%2520by%2520eliminating%2520the%2520need%2520to%2520provide%2520in-context%2520examples%2520in%2520the%2520prompt.%2520We%2520further%2520describe%2520how%2520such%2520an%2520assistant%2520is%2520implemented%2520on%2520edge%2520devices%2520with%2520no%2520dependence%2520on%2520the%2520cloud.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proactive%20Conversational%20Assistant%20for%20a%20Procedural%20Manual%20Task%20based%20on%20Audio%20and%20IMU&entry.906535625=Rehana%20Mahfuz%20and%20Yinyi%20Guo%20and%20Erik%20Visser%20and%20Phanidhar%20Chinchili&entry.1292438233=Real-time%20conversational%20assistants%20for%20procedural%20tasks%20often%20depend%20on%20video%20input%2C%20which%20can%20be%20computationally%20expensive%20and%20compromise%20user%20privacy.%20For%20the%20first%20time%2C%20we%20propose%20a%20real-time%20conversational%20assistant%20that%20provides%20comprehensive%20guidance%20for%20a%20procedural%20task%20using%20only%20lightweight%20privacy-preserving%20modalities%20such%20as%20audio%20and%20IMU%20inputs%20from%20a%20user%27s%20wearable%20device%20to%20understand%20the%20context.%20This%20assistant%20proactively%20communicates%20step-by-step%20instructions%20to%20a%20user%20performing%20a%20furniture%20assembly%20task%2C%20and%20answers%20user%20questions.%20We%20construct%20a%20dataset%20containing%20conversations%20where%20the%20assistant%20guides%20the%20user%20in%20performing%20the%20task.%20On%20observing%20that%20an%20off-the-shelf%20language%20model%20is%20a%20very%20talkative%20assistant%2C%20we%20design%20a%20novel%20User%20Whim%20Agnostic%20%28UWA%29%20LoRA%20finetuning%20method%20which%20improves%20the%20model%27s%20ability%20to%20suppress%20less%20informative%20dialogues%2C%20while%20maintaining%20its%20tendency%20to%20communicate%20important%20instructions.%20This%20leads%20to%20%3E30%25%20improvement%20in%20the%20F-score.%20Finetuning%20the%20model%20also%20results%20in%20a%2016x%20speedup%20by%20eliminating%20the%20need%20to%20provide%20in-context%20examples%20in%20the%20prompt.%20We%20further%20describe%20how%20such%20an%20assistant%20is%20implemented%20on%20edge%20devices%20with%20no%20dependence%20on%20the%20cloud.&entry.1838667208=http%3A//arxiv.org/abs/2602.15707v1&entry.124074799=Read"},
{"title": "Beyond Match Maximization and Fairness: Retention-Optimized Two-Sided Matching", "author": "Ren Kishimoto and Rikiya Takehi and Koichi Tanaka and Masahiro Nomura and Riku Togashi and Yoji Tomita and Yuta Saito", "abstract": "On two-sided matching platforms such as online dating and recruiting, recommendation algorithms often aim to maximize the total number of matches. However, this objective creates an imbalance, where some users receive far too many matches while many others receive very few and eventually abandon the platform. Retaining users is crucial for many platforms, such as those that depend heavily on subscriptions. Some may use fairness objectives to solve the problem of match maximization. However, fairness in itself is not the ultimate objective for many platforms, as users do not suddenly reward the platform simply because exposure is equalized. In practice, where user retention is often the ultimate goal, casually relying on fairness will leave the optimization of retention up to luck.\n  In this work, instead of maximizing matches or axiomatically defining fairness, we formally define the new problem setting of maximizing user retention in two-sided matching platforms. To this end, we introduce a dynamic learning-to-rank (LTR) algorithm called Matching for Retention (MRet). Unlike conventional algorithms for two-sided matching, our approach models user retention by learning personalized retention curves from each user's profile and interaction history. Based on these curves, MRet dynamically adapts recommendations by jointly considering the retention gains of both the user receiving recommendations and those who are being recommended, so that limited matching opportunities can be allocated where they most improve overall retention. Naturally but importantly, empirical evaluations on synthetic and real-world datasets from a major online dating platform show that MRet achieves higher user retention, since conventional methods optimize matches or fairness rather than retention.", "link": "http://arxiv.org/abs/2602.15752v1", "date": "2026-02-17", "relevancy": 1.6916, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4246}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4233}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Match%20Maximization%20and%20Fairness%3A%20Retention-Optimized%20Two-Sided%20Matching&body=Title%3A%20Beyond%20Match%20Maximization%20and%20Fairness%3A%20Retention-Optimized%20Two-Sided%20Matching%0AAuthor%3A%20Ren%20Kishimoto%20and%20Rikiya%20Takehi%20and%20Koichi%20Tanaka%20and%20Masahiro%20Nomura%20and%20Riku%20Togashi%20and%20Yoji%20Tomita%20and%20Yuta%20Saito%0AAbstract%3A%20On%20two-sided%20matching%20platforms%20such%20as%20online%20dating%20and%20recruiting%2C%20recommendation%20algorithms%20often%20aim%20to%20maximize%20the%20total%20number%20of%20matches.%20However%2C%20this%20objective%20creates%20an%20imbalance%2C%20where%20some%20users%20receive%20far%20too%20many%20matches%20while%20many%20others%20receive%20very%20few%20and%20eventually%20abandon%20the%20platform.%20Retaining%20users%20is%20crucial%20for%20many%20platforms%2C%20such%20as%20those%20that%20depend%20heavily%20on%20subscriptions.%20Some%20may%20use%20fairness%20objectives%20to%20solve%20the%20problem%20of%20match%20maximization.%20However%2C%20fairness%20in%20itself%20is%20not%20the%20ultimate%20objective%20for%20many%20platforms%2C%20as%20users%20do%20not%20suddenly%20reward%20the%20platform%20simply%20because%20exposure%20is%20equalized.%20In%20practice%2C%20where%20user%20retention%20is%20often%20the%20ultimate%20goal%2C%20casually%20relying%20on%20fairness%20will%20leave%20the%20optimization%20of%20retention%20up%20to%20luck.%0A%20%20In%20this%20work%2C%20instead%20of%20maximizing%20matches%20or%20axiomatically%20defining%20fairness%2C%20we%20formally%20define%20the%20new%20problem%20setting%20of%20maximizing%20user%20retention%20in%20two-sided%20matching%20platforms.%20To%20this%20end%2C%20we%20introduce%20a%20dynamic%20learning-to-rank%20%28LTR%29%20algorithm%20called%20Matching%20for%20Retention%20%28MRet%29.%20Unlike%20conventional%20algorithms%20for%20two-sided%20matching%2C%20our%20approach%20models%20user%20retention%20by%20learning%20personalized%20retention%20curves%20from%20each%20user%27s%20profile%20and%20interaction%20history.%20Based%20on%20these%20curves%2C%20MRet%20dynamically%20adapts%20recommendations%20by%20jointly%20considering%20the%20retention%20gains%20of%20both%20the%20user%20receiving%20recommendations%20and%20those%20who%20are%20being%20recommended%2C%20so%20that%20limited%20matching%20opportunities%20can%20be%20allocated%20where%20they%20most%20improve%20overall%20retention.%20Naturally%20but%20importantly%2C%20empirical%20evaluations%20on%20synthetic%20and%20real-world%20datasets%20from%20a%20major%20online%20dating%20platform%20show%20that%20MRet%20achieves%20higher%20user%20retention%2C%20since%20conventional%20methods%20optimize%20matches%20or%20fairness%20rather%20than%20retention.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Match%2520Maximization%2520and%2520Fairness%253A%2520Retention-Optimized%2520Two-Sided%2520Matching%26entry.906535625%3DRen%2520Kishimoto%2520and%2520Rikiya%2520Takehi%2520and%2520Koichi%2520Tanaka%2520and%2520Masahiro%2520Nomura%2520and%2520Riku%2520Togashi%2520and%2520Yoji%2520Tomita%2520and%2520Yuta%2520Saito%26entry.1292438233%3DOn%2520two-sided%2520matching%2520platforms%2520such%2520as%2520online%2520dating%2520and%2520recruiting%252C%2520recommendation%2520algorithms%2520often%2520aim%2520to%2520maximize%2520the%2520total%2520number%2520of%2520matches.%2520However%252C%2520this%2520objective%2520creates%2520an%2520imbalance%252C%2520where%2520some%2520users%2520receive%2520far%2520too%2520many%2520matches%2520while%2520many%2520others%2520receive%2520very%2520few%2520and%2520eventually%2520abandon%2520the%2520platform.%2520Retaining%2520users%2520is%2520crucial%2520for%2520many%2520platforms%252C%2520such%2520as%2520those%2520that%2520depend%2520heavily%2520on%2520subscriptions.%2520Some%2520may%2520use%2520fairness%2520objectives%2520to%2520solve%2520the%2520problem%2520of%2520match%2520maximization.%2520However%252C%2520fairness%2520in%2520itself%2520is%2520not%2520the%2520ultimate%2520objective%2520for%2520many%2520platforms%252C%2520as%2520users%2520do%2520not%2520suddenly%2520reward%2520the%2520platform%2520simply%2520because%2520exposure%2520is%2520equalized.%2520In%2520practice%252C%2520where%2520user%2520retention%2520is%2520often%2520the%2520ultimate%2520goal%252C%2520casually%2520relying%2520on%2520fairness%2520will%2520leave%2520the%2520optimization%2520of%2520retention%2520up%2520to%2520luck.%250A%2520%2520In%2520this%2520work%252C%2520instead%2520of%2520maximizing%2520matches%2520or%2520axiomatically%2520defining%2520fairness%252C%2520we%2520formally%2520define%2520the%2520new%2520problem%2520setting%2520of%2520maximizing%2520user%2520retention%2520in%2520two-sided%2520matching%2520platforms.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520dynamic%2520learning-to-rank%2520%2528LTR%2529%2520algorithm%2520called%2520Matching%2520for%2520Retention%2520%2528MRet%2529.%2520Unlike%2520conventional%2520algorithms%2520for%2520two-sided%2520matching%252C%2520our%2520approach%2520models%2520user%2520retention%2520by%2520learning%2520personalized%2520retention%2520curves%2520from%2520each%2520user%2527s%2520profile%2520and%2520interaction%2520history.%2520Based%2520on%2520these%2520curves%252C%2520MRet%2520dynamically%2520adapts%2520recommendations%2520by%2520jointly%2520considering%2520the%2520retention%2520gains%2520of%2520both%2520the%2520user%2520receiving%2520recommendations%2520and%2520those%2520who%2520are%2520being%2520recommended%252C%2520so%2520that%2520limited%2520matching%2520opportunities%2520can%2520be%2520allocated%2520where%2520they%2520most%2520improve%2520overall%2520retention.%2520Naturally%2520but%2520importantly%252C%2520empirical%2520evaluations%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520from%2520a%2520major%2520online%2520dating%2520platform%2520show%2520that%2520MRet%2520achieves%2520higher%2520user%2520retention%252C%2520since%2520conventional%2520methods%2520optimize%2520matches%2520or%2520fairness%2520rather%2520than%2520retention.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Match%20Maximization%20and%20Fairness%3A%20Retention-Optimized%20Two-Sided%20Matching&entry.906535625=Ren%20Kishimoto%20and%20Rikiya%20Takehi%20and%20Koichi%20Tanaka%20and%20Masahiro%20Nomura%20and%20Riku%20Togashi%20and%20Yoji%20Tomita%20and%20Yuta%20Saito&entry.1292438233=On%20two-sided%20matching%20platforms%20such%20as%20online%20dating%20and%20recruiting%2C%20recommendation%20algorithms%20often%20aim%20to%20maximize%20the%20total%20number%20of%20matches.%20However%2C%20this%20objective%20creates%20an%20imbalance%2C%20where%20some%20users%20receive%20far%20too%20many%20matches%20while%20many%20others%20receive%20very%20few%20and%20eventually%20abandon%20the%20platform.%20Retaining%20users%20is%20crucial%20for%20many%20platforms%2C%20such%20as%20those%20that%20depend%20heavily%20on%20subscriptions.%20Some%20may%20use%20fairness%20objectives%20to%20solve%20the%20problem%20of%20match%20maximization.%20However%2C%20fairness%20in%20itself%20is%20not%20the%20ultimate%20objective%20for%20many%20platforms%2C%20as%20users%20do%20not%20suddenly%20reward%20the%20platform%20simply%20because%20exposure%20is%20equalized.%20In%20practice%2C%20where%20user%20retention%20is%20often%20the%20ultimate%20goal%2C%20casually%20relying%20on%20fairness%20will%20leave%20the%20optimization%20of%20retention%20up%20to%20luck.%0A%20%20In%20this%20work%2C%20instead%20of%20maximizing%20matches%20or%20axiomatically%20defining%20fairness%2C%20we%20formally%20define%20the%20new%20problem%20setting%20of%20maximizing%20user%20retention%20in%20two-sided%20matching%20platforms.%20To%20this%20end%2C%20we%20introduce%20a%20dynamic%20learning-to-rank%20%28LTR%29%20algorithm%20called%20Matching%20for%20Retention%20%28MRet%29.%20Unlike%20conventional%20algorithms%20for%20two-sided%20matching%2C%20our%20approach%20models%20user%20retention%20by%20learning%20personalized%20retention%20curves%20from%20each%20user%27s%20profile%20and%20interaction%20history.%20Based%20on%20these%20curves%2C%20MRet%20dynamically%20adapts%20recommendations%20by%20jointly%20considering%20the%20retention%20gains%20of%20both%20the%20user%20receiving%20recommendations%20and%20those%20who%20are%20being%20recommended%2C%20so%20that%20limited%20matching%20opportunities%20can%20be%20allocated%20where%20they%20most%20improve%20overall%20retention.%20Naturally%20but%20importantly%2C%20empirical%20evaluations%20on%20synthetic%20and%20real-world%20datasets%20from%20a%20major%20online%20dating%20platform%20show%20that%20MRet%20achieves%20higher%20user%20retention%2C%20since%20conventional%20methods%20optimize%20matches%20or%20fairness%20rather%20than%20retention.&entry.1838667208=http%3A//arxiv.org/abs/2602.15752v1&entry.124074799=Read"},
{"title": "RobustBlack: Challenging Black-Box Adversarial Attacks on State-of-the-Art Defenses", "author": "Mohamed Djilani and Salah Ghamizi and Maxime Cordy", "abstract": "Although adversarial robustness has been extensively studied in white-box settings, recent advances in black-box attacks (including transfer- and query-based approaches) are primarily benchmarked against weak defenses, leaving a significant gap in the evaluation of their effectiveness against more recent and moderate robust models (e.g., those featured in the Robustbench leaderboard). In this paper, we question this lack of attention from black-box attacks to robust models. We establish a framework to evaluate the effectiveness of recent black-box attacks against both top-performing and standard defense mechanisms, on the ImageNet dataset. Our empirical evaluation reveals the following key findings: (1) the most advanced black-box attacks struggle to succeed even against simple adversarially trained models; (2) robust models that are optimized to withstand strong white-box attacks, such as AutoAttack, also exhibits enhanced resilience against black-box attacks; and (3) robustness alignment between the surrogate models and the target model plays a key factor in the success rate of transfer-based attacks", "link": "http://arxiv.org/abs/2412.20987v2", "date": "2026-02-17", "relevancy": 1.7506, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4474}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4374}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RobustBlack%3A%20Challenging%20Black-Box%20Adversarial%20Attacks%20on%20State-of-the-Art%20Defenses&body=Title%3A%20RobustBlack%3A%20Challenging%20Black-Box%20Adversarial%20Attacks%20on%20State-of-the-Art%20Defenses%0AAuthor%3A%20Mohamed%20Djilani%20and%20Salah%20Ghamizi%20and%20Maxime%20Cordy%0AAbstract%3A%20Although%20adversarial%20robustness%20has%20been%20extensively%20studied%20in%20white-box%20settings%2C%20recent%20advances%20in%20black-box%20attacks%20%28including%20transfer-%20and%20query-based%20approaches%29%20are%20primarily%20benchmarked%20against%20weak%20defenses%2C%20leaving%20a%20significant%20gap%20in%20the%20evaluation%20of%20their%20effectiveness%20against%20more%20recent%20and%20moderate%20robust%20models%20%28e.g.%2C%20those%20featured%20in%20the%20Robustbench%20leaderboard%29.%20In%20this%20paper%2C%20we%20question%20this%20lack%20of%20attention%20from%20black-box%20attacks%20to%20robust%20models.%20We%20establish%20a%20framework%20to%20evaluate%20the%20effectiveness%20of%20recent%20black-box%20attacks%20against%20both%20top-performing%20and%20standard%20defense%20mechanisms%2C%20on%20the%20ImageNet%20dataset.%20Our%20empirical%20evaluation%20reveals%20the%20following%20key%20findings%3A%20%281%29%20the%20most%20advanced%20black-box%20attacks%20struggle%20to%20succeed%20even%20against%20simple%20adversarially%20trained%20models%3B%20%282%29%20robust%20models%20that%20are%20optimized%20to%20withstand%20strong%20white-box%20attacks%2C%20such%20as%20AutoAttack%2C%20also%20exhibits%20enhanced%20resilience%20against%20black-box%20attacks%3B%20and%20%283%29%20robustness%20alignment%20between%20the%20surrogate%20models%20and%20the%20target%20model%20plays%20a%20key%20factor%20in%20the%20success%20rate%20of%20transfer-based%20attacks%0ALink%3A%20http%3A//arxiv.org/abs/2412.20987v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustBlack%253A%2520Challenging%2520Black-Box%2520Adversarial%2520Attacks%2520on%2520State-of-the-Art%2520Defenses%26entry.906535625%3DMohamed%2520Djilani%2520and%2520Salah%2520Ghamizi%2520and%2520Maxime%2520Cordy%26entry.1292438233%3DAlthough%2520adversarial%2520robustness%2520has%2520been%2520extensively%2520studied%2520in%2520white-box%2520settings%252C%2520recent%2520advances%2520in%2520black-box%2520attacks%2520%2528including%2520transfer-%2520and%2520query-based%2520approaches%2529%2520are%2520primarily%2520benchmarked%2520against%2520weak%2520defenses%252C%2520leaving%2520a%2520significant%2520gap%2520in%2520the%2520evaluation%2520of%2520their%2520effectiveness%2520against%2520more%2520recent%2520and%2520moderate%2520robust%2520models%2520%2528e.g.%252C%2520those%2520featured%2520in%2520the%2520Robustbench%2520leaderboard%2529.%2520In%2520this%2520paper%252C%2520we%2520question%2520this%2520lack%2520of%2520attention%2520from%2520black-box%2520attacks%2520to%2520robust%2520models.%2520We%2520establish%2520a%2520framework%2520to%2520evaluate%2520the%2520effectiveness%2520of%2520recent%2520black-box%2520attacks%2520against%2520both%2520top-performing%2520and%2520standard%2520defense%2520mechanisms%252C%2520on%2520the%2520ImageNet%2520dataset.%2520Our%2520empirical%2520evaluation%2520reveals%2520the%2520following%2520key%2520findings%253A%2520%25281%2529%2520the%2520most%2520advanced%2520black-box%2520attacks%2520struggle%2520to%2520succeed%2520even%2520against%2520simple%2520adversarially%2520trained%2520models%253B%2520%25282%2529%2520robust%2520models%2520that%2520are%2520optimized%2520to%2520withstand%2520strong%2520white-box%2520attacks%252C%2520such%2520as%2520AutoAttack%252C%2520also%2520exhibits%2520enhanced%2520resilience%2520against%2520black-box%2520attacks%253B%2520and%2520%25283%2529%2520robustness%2520alignment%2520between%2520the%2520surrogate%2520models%2520and%2520the%2520target%2520model%2520plays%2520a%2520key%2520factor%2520in%2520the%2520success%2520rate%2520of%2520transfer-based%2520attacks%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20987v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobustBlack%3A%20Challenging%20Black-Box%20Adversarial%20Attacks%20on%20State-of-the-Art%20Defenses&entry.906535625=Mohamed%20Djilani%20and%20Salah%20Ghamizi%20and%20Maxime%20Cordy&entry.1292438233=Although%20adversarial%20robustness%20has%20been%20extensively%20studied%20in%20white-box%20settings%2C%20recent%20advances%20in%20black-box%20attacks%20%28including%20transfer-%20and%20query-based%20approaches%29%20are%20primarily%20benchmarked%20against%20weak%20defenses%2C%20leaving%20a%20significant%20gap%20in%20the%20evaluation%20of%20their%20effectiveness%20against%20more%20recent%20and%20moderate%20robust%20models%20%28e.g.%2C%20those%20featured%20in%20the%20Robustbench%20leaderboard%29.%20In%20this%20paper%2C%20we%20question%20this%20lack%20of%20attention%20from%20black-box%20attacks%20to%20robust%20models.%20We%20establish%20a%20framework%20to%20evaluate%20the%20effectiveness%20of%20recent%20black-box%20attacks%20against%20both%20top-performing%20and%20standard%20defense%20mechanisms%2C%20on%20the%20ImageNet%20dataset.%20Our%20empirical%20evaluation%20reveals%20the%20following%20key%20findings%3A%20%281%29%20the%20most%20advanced%20black-box%20attacks%20struggle%20to%20succeed%20even%20against%20simple%20adversarially%20trained%20models%3B%20%282%29%20robust%20models%20that%20are%20optimized%20to%20withstand%20strong%20white-box%20attacks%2C%20such%20as%20AutoAttack%2C%20also%20exhibits%20enhanced%20resilience%20against%20black-box%20attacks%3B%20and%20%283%29%20robustness%20alignment%20between%20the%20surrogate%20models%20and%20the%20target%20model%20plays%20a%20key%20factor%20in%20the%20success%20rate%20of%20transfer-based%20attacks&entry.1838667208=http%3A//arxiv.org/abs/2412.20987v2&entry.124074799=Read"},
{"title": "SpecFuse: A Spectral-Temporal Fusion Predictive Control Framework for UAV Landing on Oscillating Marine Platforms", "author": "Haichao Liu and Yufeng Hu and Shuang Wang and Kangjun Guo and Jun Ma and Jinni Zhou", "abstract": "Autonomous landing of Uncrewed Aerial Vehicles (UAVs) on oscillating marine platforms is severely constrained by wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags in motion prediction. Existing methods either treat platform motion as a general random process or lack explicit modeling of wave spectral characteristics, leading to suboptimal performance under dynamic sea conditions. To address these limitations, we propose SpecFuse: a novel spectral-temporal fusion predictive control framework that integrates frequency-domain wave decomposition with time-domain recursive state estimation for high-precision 6-DoF motion forecasting of Uncrewed Surface Vehicles (USVs). The framework explicitly models dominant wave harmonics to mitigate phase lags, refining predictions in real time via IMU data without relying on complex calibration. Additionally, we design a hierarchical control architecture featuring a sampling-based HPO-RRT* algorithm for dynamic trajectory planning under non-convex constraints and a learning-augmented predictive controller that fuses data-driven disturbance compensation with optimization-based execution. Extensive validations (2,000 simulations + 8 lake experiments) show our approach achieves a 3.2 cm prediction error, 4.46 cm landing deviation, 98.7% / 87.5% success rates (simulation / real-world), and 82 ms latency on embedded hardware, outperforming state-of-the-art methods by 44%-48% in accuracy. Its robustness to wave-wind coupling disturbances supports critical maritime missions such as search and rescue and environmental monitoring. All code, experimental configurations, and datasets will be released as open-source to facilitate reproducibility.", "link": "http://arxiv.org/abs/2602.15633v1", "date": "2026-02-17", "relevancy": 1.5159, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5308}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5025}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecFuse%3A%20A%20Spectral-Temporal%20Fusion%20Predictive%20Control%20Framework%20for%20UAV%20Landing%20on%20Oscillating%20Marine%20Platforms&body=Title%3A%20SpecFuse%3A%20A%20Spectral-Temporal%20Fusion%20Predictive%20Control%20Framework%20for%20UAV%20Landing%20on%20Oscillating%20Marine%20Platforms%0AAuthor%3A%20Haichao%20Liu%20and%20Yufeng%20Hu%20and%20Shuang%20Wang%20and%20Kangjun%20Guo%20and%20Jun%20Ma%20and%20Jinni%20Zhou%0AAbstract%3A%20Autonomous%20landing%20of%20Uncrewed%20Aerial%20Vehicles%20%28UAVs%29%20on%20oscillating%20marine%20platforms%20is%20severely%20constrained%20by%20wave-induced%20multi-frequency%20oscillations%2C%20wind%20disturbances%2C%20and%20prediction%20phase%20lags%20in%20motion%20prediction.%20Existing%20methods%20either%20treat%20platform%20motion%20as%20a%20general%20random%20process%20or%20lack%20explicit%20modeling%20of%20wave%20spectral%20characteristics%2C%20leading%20to%20suboptimal%20performance%20under%20dynamic%20sea%20conditions.%20To%20address%20these%20limitations%2C%20we%20propose%20SpecFuse%3A%20a%20novel%20spectral-temporal%20fusion%20predictive%20control%20framework%20that%20integrates%20frequency-domain%20wave%20decomposition%20with%20time-domain%20recursive%20state%20estimation%20for%20high-precision%206-DoF%20motion%20forecasting%20of%20Uncrewed%20Surface%20Vehicles%20%28USVs%29.%20The%20framework%20explicitly%20models%20dominant%20wave%20harmonics%20to%20mitigate%20phase%20lags%2C%20refining%20predictions%20in%20real%20time%20via%20IMU%20data%20without%20relying%20on%20complex%20calibration.%20Additionally%2C%20we%20design%20a%20hierarchical%20control%20architecture%20featuring%20a%20sampling-based%20HPO-RRT%2A%20algorithm%20for%20dynamic%20trajectory%20planning%20under%20non-convex%20constraints%20and%20a%20learning-augmented%20predictive%20controller%20that%20fuses%20data-driven%20disturbance%20compensation%20with%20optimization-based%20execution.%20Extensive%20validations%20%282%2C000%20simulations%20%2B%208%20lake%20experiments%29%20show%20our%20approach%20achieves%20a%203.2%20cm%20prediction%20error%2C%204.46%20cm%20landing%20deviation%2C%2098.7%25%20/%2087.5%25%20success%20rates%20%28simulation%20/%20real-world%29%2C%20and%2082%20ms%20latency%20on%20embedded%20hardware%2C%20outperforming%20state-of-the-art%20methods%20by%2044%25-48%25%20in%20accuracy.%20Its%20robustness%20to%20wave-wind%20coupling%20disturbances%20supports%20critical%20maritime%20missions%20such%20as%20search%20and%20rescue%20and%20environmental%20monitoring.%20All%20code%2C%20experimental%20configurations%2C%20and%20datasets%20will%20be%20released%20as%20open-source%20to%20facilitate%20reproducibility.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecFuse%253A%2520A%2520Spectral-Temporal%2520Fusion%2520Predictive%2520Control%2520Framework%2520for%2520UAV%2520Landing%2520on%2520Oscillating%2520Marine%2520Platforms%26entry.906535625%3DHaichao%2520Liu%2520and%2520Yufeng%2520Hu%2520and%2520Shuang%2520Wang%2520and%2520Kangjun%2520Guo%2520and%2520Jun%2520Ma%2520and%2520Jinni%2520Zhou%26entry.1292438233%3DAutonomous%2520landing%2520of%2520Uncrewed%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520on%2520oscillating%2520marine%2520platforms%2520is%2520severely%2520constrained%2520by%2520wave-induced%2520multi-frequency%2520oscillations%252C%2520wind%2520disturbances%252C%2520and%2520prediction%2520phase%2520lags%2520in%2520motion%2520prediction.%2520Existing%2520methods%2520either%2520treat%2520platform%2520motion%2520as%2520a%2520general%2520random%2520process%2520or%2520lack%2520explicit%2520modeling%2520of%2520wave%2520spectral%2520characteristics%252C%2520leading%2520to%2520suboptimal%2520performance%2520under%2520dynamic%2520sea%2520conditions.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520SpecFuse%253A%2520a%2520novel%2520spectral-temporal%2520fusion%2520predictive%2520control%2520framework%2520that%2520integrates%2520frequency-domain%2520wave%2520decomposition%2520with%2520time-domain%2520recursive%2520state%2520estimation%2520for%2520high-precision%25206-DoF%2520motion%2520forecasting%2520of%2520Uncrewed%2520Surface%2520Vehicles%2520%2528USVs%2529.%2520The%2520framework%2520explicitly%2520models%2520dominant%2520wave%2520harmonics%2520to%2520mitigate%2520phase%2520lags%252C%2520refining%2520predictions%2520in%2520real%2520time%2520via%2520IMU%2520data%2520without%2520relying%2520on%2520complex%2520calibration.%2520Additionally%252C%2520we%2520design%2520a%2520hierarchical%2520control%2520architecture%2520featuring%2520a%2520sampling-based%2520HPO-RRT%252A%2520algorithm%2520for%2520dynamic%2520trajectory%2520planning%2520under%2520non-convex%2520constraints%2520and%2520a%2520learning-augmented%2520predictive%2520controller%2520that%2520fuses%2520data-driven%2520disturbance%2520compensation%2520with%2520optimization-based%2520execution.%2520Extensive%2520validations%2520%25282%252C000%2520simulations%2520%252B%25208%2520lake%2520experiments%2529%2520show%2520our%2520approach%2520achieves%2520a%25203.2%2520cm%2520prediction%2520error%252C%25204.46%2520cm%2520landing%2520deviation%252C%252098.7%2525%2520/%252087.5%2525%2520success%2520rates%2520%2528simulation%2520/%2520real-world%2529%252C%2520and%252082%2520ms%2520latency%2520on%2520embedded%2520hardware%252C%2520outperforming%2520state-of-the-art%2520methods%2520by%252044%2525-48%2525%2520in%2520accuracy.%2520Its%2520robustness%2520to%2520wave-wind%2520coupling%2520disturbances%2520supports%2520critical%2520maritime%2520missions%2520such%2520as%2520search%2520and%2520rescue%2520and%2520environmental%2520monitoring.%2520All%2520code%252C%2520experimental%2520configurations%252C%2520and%2520datasets%2520will%2520be%2520released%2520as%2520open-source%2520to%2520facilitate%2520reproducibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecFuse%3A%20A%20Spectral-Temporal%20Fusion%20Predictive%20Control%20Framework%20for%20UAV%20Landing%20on%20Oscillating%20Marine%20Platforms&entry.906535625=Haichao%20Liu%20and%20Yufeng%20Hu%20and%20Shuang%20Wang%20and%20Kangjun%20Guo%20and%20Jun%20Ma%20and%20Jinni%20Zhou&entry.1292438233=Autonomous%20landing%20of%20Uncrewed%20Aerial%20Vehicles%20%28UAVs%29%20on%20oscillating%20marine%20platforms%20is%20severely%20constrained%20by%20wave-induced%20multi-frequency%20oscillations%2C%20wind%20disturbances%2C%20and%20prediction%20phase%20lags%20in%20motion%20prediction.%20Existing%20methods%20either%20treat%20platform%20motion%20as%20a%20general%20random%20process%20or%20lack%20explicit%20modeling%20of%20wave%20spectral%20characteristics%2C%20leading%20to%20suboptimal%20performance%20under%20dynamic%20sea%20conditions.%20To%20address%20these%20limitations%2C%20we%20propose%20SpecFuse%3A%20a%20novel%20spectral-temporal%20fusion%20predictive%20control%20framework%20that%20integrates%20frequency-domain%20wave%20decomposition%20with%20time-domain%20recursive%20state%20estimation%20for%20high-precision%206-DoF%20motion%20forecasting%20of%20Uncrewed%20Surface%20Vehicles%20%28USVs%29.%20The%20framework%20explicitly%20models%20dominant%20wave%20harmonics%20to%20mitigate%20phase%20lags%2C%20refining%20predictions%20in%20real%20time%20via%20IMU%20data%20without%20relying%20on%20complex%20calibration.%20Additionally%2C%20we%20design%20a%20hierarchical%20control%20architecture%20featuring%20a%20sampling-based%20HPO-RRT%2A%20algorithm%20for%20dynamic%20trajectory%20planning%20under%20non-convex%20constraints%20and%20a%20learning-augmented%20predictive%20controller%20that%20fuses%20data-driven%20disturbance%20compensation%20with%20optimization-based%20execution.%20Extensive%20validations%20%282%2C000%20simulations%20%2B%208%20lake%20experiments%29%20show%20our%20approach%20achieves%20a%203.2%20cm%20prediction%20error%2C%204.46%20cm%20landing%20deviation%2C%2098.7%25%20/%2087.5%25%20success%20rates%20%28simulation%20/%20real-world%29%2C%20and%2082%20ms%20latency%20on%20embedded%20hardware%2C%20outperforming%20state-of-the-art%20methods%20by%2044%25-48%25%20in%20accuracy.%20Its%20robustness%20to%20wave-wind%20coupling%20disturbances%20supports%20critical%20maritime%20missions%20such%20as%20search%20and%20rescue%20and%20environmental%20monitoring.%20All%20code%2C%20experimental%20configurations%2C%20and%20datasets%20will%20be%20released%20as%20open-source%20to%20facilitate%20reproducibility.&entry.1838667208=http%3A//arxiv.org/abs/2602.15633v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


