<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250901.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Micro-splatting: Multistage Isotropy-informed Covariance Regularization\n  Optimization for High-Fidelity 3D Gaussian Splatting", "author": "Jee Won Lee and Hansol Lim and Sooyeun Yang and Jongseong Brad Choi", "abstract": "  High-fidelity 3D Gaussian Splatting methods excel at capturing fine textures\nbut often overlook model compactness, resulting in massive splat counts,\nbloated memory, long training, and complex post-processing. We present\nMicro-Splatting: Two-Stage Adaptive Growth and Refinement, a unified,\nin-training pipeline that preserves visual detail while drastically reducing\nmodel complexity without any post-processing or auxiliary neural modules. In\nStage I (Growth), we introduce a trace-based covariance regularization to\nmaintain near-isotropic Gaussians, mitigating low-pass filtering in\nhigh-frequency regions and improving spherical-harmonic color fitting. We then\napply gradient-guided adaptive densification that subdivides splats only in\nvisually complex regions, leaving smooth areas sparse. In Stage II\n(Refinement), we prune low-impact splats using a simple opacity-scale\nimportance score and merge redundant neighbors via lightweight spatial and\nfeature thresholds, producing a lean yet detail-rich model. On four\nobject-centric benchmarks, Micro-Splatting reduces splat count and model size\nby up to 60% and shortens training by 20%, while matching or surpassing\nstate-of-the-art PSNR, SSIM, and LPIPS in real-time rendering. These results\ndemonstrate that Micro-Splatting delivers both compactness and high fidelity in\na single, efficient, end-to-end framework.\n", "link": "http://arxiv.org/abs/2504.05740v2", "date": "2025-09-02", "relevancy": 3.368, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6988}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6802}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Micro-splatting%3A%20Multistage%20Isotropy-informed%20Covariance%20Regularization%0A%20%20Optimization%20for%20High-Fidelity%203D%20Gaussian%20Splatting&body=Title%3A%20Micro-splatting%3A%20Multistage%20Isotropy-informed%20Covariance%20Regularization%0A%20%20Optimization%20for%20High-Fidelity%203D%20Gaussian%20Splatting%0AAuthor%3A%20Jee%20Won%20Lee%20and%20Hansol%20Lim%20and%20Sooyeun%20Yang%20and%20Jongseong%20Brad%20Choi%0AAbstract%3A%20%20%20High-fidelity%203D%20Gaussian%20Splatting%20methods%20excel%20at%20capturing%20fine%20textures%0Abut%20often%20overlook%20model%20compactness%2C%20resulting%20in%20massive%20splat%20counts%2C%0Abloated%20memory%2C%20long%20training%2C%20and%20complex%20post-processing.%20We%20present%0AMicro-Splatting%3A%20Two-Stage%20Adaptive%20Growth%20and%20Refinement%2C%20a%20unified%2C%0Ain-training%20pipeline%20that%20preserves%20visual%20detail%20while%20drastically%20reducing%0Amodel%20complexity%20without%20any%20post-processing%20or%20auxiliary%20neural%20modules.%20In%0AStage%20I%20%28Growth%29%2C%20we%20introduce%20a%20trace-based%20covariance%20regularization%20to%0Amaintain%20near-isotropic%20Gaussians%2C%20mitigating%20low-pass%20filtering%20in%0Ahigh-frequency%20regions%20and%20improving%20spherical-harmonic%20color%20fitting.%20We%20then%0Aapply%20gradient-guided%20adaptive%20densification%20that%20subdivides%20splats%20only%20in%0Avisually%20complex%20regions%2C%20leaving%20smooth%20areas%20sparse.%20In%20Stage%20II%0A%28Refinement%29%2C%20we%20prune%20low-impact%20splats%20using%20a%20simple%20opacity-scale%0Aimportance%20score%20and%20merge%20redundant%20neighbors%20via%20lightweight%20spatial%20and%0Afeature%20thresholds%2C%20producing%20a%20lean%20yet%20detail-rich%20model.%20On%20four%0Aobject-centric%20benchmarks%2C%20Micro-Splatting%20reduces%20splat%20count%20and%20model%20size%0Aby%20up%20to%2060%25%20and%20shortens%20training%20by%2020%25%2C%20while%20matching%20or%20surpassing%0Astate-of-the-art%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20in%20real-time%20rendering.%20These%20results%0Ademonstrate%20that%20Micro-Splatting%20delivers%20both%20compactness%20and%20high%20fidelity%20in%0Aa%20single%2C%20efficient%2C%20end-to-end%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05740v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicro-splatting%253A%2520Multistage%2520Isotropy-informed%2520Covariance%2520Regularization%250A%2520%2520Optimization%2520for%2520High-Fidelity%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DJee%2520Won%2520Lee%2520and%2520Hansol%2520Lim%2520and%2520Sooyeun%2520Yang%2520and%2520Jongseong%2520Brad%2520Choi%26entry.1292438233%3D%2520%2520High-fidelity%25203D%2520Gaussian%2520Splatting%2520methods%2520excel%2520at%2520capturing%2520fine%2520textures%250Abut%2520often%2520overlook%2520model%2520compactness%252C%2520resulting%2520in%2520massive%2520splat%2520counts%252C%250Abloated%2520memory%252C%2520long%2520training%252C%2520and%2520complex%2520post-processing.%2520We%2520present%250AMicro-Splatting%253A%2520Two-Stage%2520Adaptive%2520Growth%2520and%2520Refinement%252C%2520a%2520unified%252C%250Ain-training%2520pipeline%2520that%2520preserves%2520visual%2520detail%2520while%2520drastically%2520reducing%250Amodel%2520complexity%2520without%2520any%2520post-processing%2520or%2520auxiliary%2520neural%2520modules.%2520In%250AStage%2520I%2520%2528Growth%2529%252C%2520we%2520introduce%2520a%2520trace-based%2520covariance%2520regularization%2520to%250Amaintain%2520near-isotropic%2520Gaussians%252C%2520mitigating%2520low-pass%2520filtering%2520in%250Ahigh-frequency%2520regions%2520and%2520improving%2520spherical-harmonic%2520color%2520fitting.%2520We%2520then%250Aapply%2520gradient-guided%2520adaptive%2520densification%2520that%2520subdivides%2520splats%2520only%2520in%250Avisually%2520complex%2520regions%252C%2520leaving%2520smooth%2520areas%2520sparse.%2520In%2520Stage%2520II%250A%2528Refinement%2529%252C%2520we%2520prune%2520low-impact%2520splats%2520using%2520a%2520simple%2520opacity-scale%250Aimportance%2520score%2520and%2520merge%2520redundant%2520neighbors%2520via%2520lightweight%2520spatial%2520and%250Afeature%2520thresholds%252C%2520producing%2520a%2520lean%2520yet%2520detail-rich%2520model.%2520On%2520four%250Aobject-centric%2520benchmarks%252C%2520Micro-Splatting%2520reduces%2520splat%2520count%2520and%2520model%2520size%250Aby%2520up%2520to%252060%2525%2520and%2520shortens%2520training%2520by%252020%2525%252C%2520while%2520matching%2520or%2520surpassing%250Astate-of-the-art%2520PSNR%252C%2520SSIM%252C%2520and%2520LPIPS%2520in%2520real-time%2520rendering.%2520These%2520results%250Ademonstrate%2520that%2520Micro-Splatting%2520delivers%2520both%2520compactness%2520and%2520high%2520fidelity%2520in%250Aa%2520single%252C%2520efficient%252C%2520end-to-end%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05740v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Micro-splatting%3A%20Multistage%20Isotropy-informed%20Covariance%20Regularization%0A%20%20Optimization%20for%20High-Fidelity%203D%20Gaussian%20Splatting&entry.906535625=Jee%20Won%20Lee%20and%20Hansol%20Lim%20and%20Sooyeun%20Yang%20and%20Jongseong%20Brad%20Choi&entry.1292438233=%20%20High-fidelity%203D%20Gaussian%20Splatting%20methods%20excel%20at%20capturing%20fine%20textures%0Abut%20often%20overlook%20model%20compactness%2C%20resulting%20in%20massive%20splat%20counts%2C%0Abloated%20memory%2C%20long%20training%2C%20and%20complex%20post-processing.%20We%20present%0AMicro-Splatting%3A%20Two-Stage%20Adaptive%20Growth%20and%20Refinement%2C%20a%20unified%2C%0Ain-training%20pipeline%20that%20preserves%20visual%20detail%20while%20drastically%20reducing%0Amodel%20complexity%20without%20any%20post-processing%20or%20auxiliary%20neural%20modules.%20In%0AStage%20I%20%28Growth%29%2C%20we%20introduce%20a%20trace-based%20covariance%20regularization%20to%0Amaintain%20near-isotropic%20Gaussians%2C%20mitigating%20low-pass%20filtering%20in%0Ahigh-frequency%20regions%20and%20improving%20spherical-harmonic%20color%20fitting.%20We%20then%0Aapply%20gradient-guided%20adaptive%20densification%20that%20subdivides%20splats%20only%20in%0Avisually%20complex%20regions%2C%20leaving%20smooth%20areas%20sparse.%20In%20Stage%20II%0A%28Refinement%29%2C%20we%20prune%20low-impact%20splats%20using%20a%20simple%20opacity-scale%0Aimportance%20score%20and%20merge%20redundant%20neighbors%20via%20lightweight%20spatial%20and%0Afeature%20thresholds%2C%20producing%20a%20lean%20yet%20detail-rich%20model.%20On%20four%0Aobject-centric%20benchmarks%2C%20Micro-Splatting%20reduces%20splat%20count%20and%20model%20size%0Aby%20up%20to%2060%25%20and%20shortens%20training%20by%2020%25%2C%20while%20matching%20or%20surpassing%0Astate-of-the-art%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20in%20real-time%20rendering.%20These%20results%0Ademonstrate%20that%20Micro-Splatting%20delivers%20both%20compactness%20and%20high%20fidelity%20in%0Aa%20single%2C%20efficient%2C%20end-to-end%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05740v2&entry.124074799=Read"},
{"title": "Learning local and global prototypes with optimal transport for\n  unsupervised anomaly detection and localization", "author": "Robin Trombetta and Carole Lartizien", "abstract": "  Unsupervised anomaly detection aims to detect defective parts of a sample by\nhaving access, during training, to a set of normal, i.e. defect-free, data. It\nhas many applications in fields, such as industrial inspection or medical\nimaging, where acquiring labels is costly or when we want to avoid introducing\nbiases in the type of anomalies that can be spotted. In this work, we propose a\nnovel UAD method based on prototype learning and introduce a metric to compare\na structured set of embeddings that balances a feature-based cost and a\nspatial-based cost. We leverage this metric to learn local and global\nprototypes with optimal transport from latent representations extracted with a\npre-trained image encoder. We demonstrate that our approach can enforce a\nstructural constraint when learning the prototypes, allowing to capture the\nunderlying organization of the normal samples, thus improving the detection of\nincoherencies in images. Our model achieves performance that is on par with\nstrong baselines on two reference benchmarks for anomaly detection on\nindustrial images.\n", "link": "http://arxiv.org/abs/2508.12927v2", "date": "2025-09-02", "relevancy": 2.8965, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5916}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5883}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20local%20and%20global%20prototypes%20with%20optimal%20transport%20for%0A%20%20unsupervised%20anomaly%20detection%20and%20localization&body=Title%3A%20Learning%20local%20and%20global%20prototypes%20with%20optimal%20transport%20for%0A%20%20unsupervised%20anomaly%20detection%20and%20localization%0AAuthor%3A%20Robin%20Trombetta%20and%20Carole%20Lartizien%0AAbstract%3A%20%20%20Unsupervised%20anomaly%20detection%20aims%20to%20detect%20defective%20parts%20of%20a%20sample%20by%0Ahaving%20access%2C%20during%20training%2C%20to%20a%20set%20of%20normal%2C%20i.e.%20defect-free%2C%20data.%20It%0Ahas%20many%20applications%20in%20fields%2C%20such%20as%20industrial%20inspection%20or%20medical%0Aimaging%2C%20where%20acquiring%20labels%20is%20costly%20or%20when%20we%20want%20to%20avoid%20introducing%0Abiases%20in%20the%20type%20of%20anomalies%20that%20can%20be%20spotted.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20UAD%20method%20based%20on%20prototype%20learning%20and%20introduce%20a%20metric%20to%20compare%0Aa%20structured%20set%20of%20embeddings%20that%20balances%20a%20feature-based%20cost%20and%20a%0Aspatial-based%20cost.%20We%20leverage%20this%20metric%20to%20learn%20local%20and%20global%0Aprototypes%20with%20optimal%20transport%20from%20latent%20representations%20extracted%20with%20a%0Apre-trained%20image%20encoder.%20We%20demonstrate%20that%20our%20approach%20can%20enforce%20a%0Astructural%20constraint%20when%20learning%20the%20prototypes%2C%20allowing%20to%20capture%20the%0Aunderlying%20organization%20of%20the%20normal%20samples%2C%20thus%20improving%20the%20detection%20of%0Aincoherencies%20in%20images.%20Our%20model%20achieves%20performance%20that%20is%20on%20par%20with%0Astrong%20baselines%20on%20two%20reference%20benchmarks%20for%20anomaly%20detection%20on%0Aindustrial%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12927v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520local%2520and%2520global%2520prototypes%2520with%2520optimal%2520transport%2520for%250A%2520%2520unsupervised%2520anomaly%2520detection%2520and%2520localization%26entry.906535625%3DRobin%2520Trombetta%2520and%2520Carole%2520Lartizien%26entry.1292438233%3D%2520%2520Unsupervised%2520anomaly%2520detection%2520aims%2520to%2520detect%2520defective%2520parts%2520of%2520a%2520sample%2520by%250Ahaving%2520access%252C%2520during%2520training%252C%2520to%2520a%2520set%2520of%2520normal%252C%2520i.e.%2520defect-free%252C%2520data.%2520It%250Ahas%2520many%2520applications%2520in%2520fields%252C%2520such%2520as%2520industrial%2520inspection%2520or%2520medical%250Aimaging%252C%2520where%2520acquiring%2520labels%2520is%2520costly%2520or%2520when%2520we%2520want%2520to%2520avoid%2520introducing%250Abiases%2520in%2520the%2520type%2520of%2520anomalies%2520that%2520can%2520be%2520spotted.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520UAD%2520method%2520based%2520on%2520prototype%2520learning%2520and%2520introduce%2520a%2520metric%2520to%2520compare%250Aa%2520structured%2520set%2520of%2520embeddings%2520that%2520balances%2520a%2520feature-based%2520cost%2520and%2520a%250Aspatial-based%2520cost.%2520We%2520leverage%2520this%2520metric%2520to%2520learn%2520local%2520and%2520global%250Aprototypes%2520with%2520optimal%2520transport%2520from%2520latent%2520representations%2520extracted%2520with%2520a%250Apre-trained%2520image%2520encoder.%2520We%2520demonstrate%2520that%2520our%2520approach%2520can%2520enforce%2520a%250Astructural%2520constraint%2520when%2520learning%2520the%2520prototypes%252C%2520allowing%2520to%2520capture%2520the%250Aunderlying%2520organization%2520of%2520the%2520normal%2520samples%252C%2520thus%2520improving%2520the%2520detection%2520of%250Aincoherencies%2520in%2520images.%2520Our%2520model%2520achieves%2520performance%2520that%2520is%2520on%2520par%2520with%250Astrong%2520baselines%2520on%2520two%2520reference%2520benchmarks%2520for%2520anomaly%2520detection%2520on%250Aindustrial%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12927v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20local%20and%20global%20prototypes%20with%20optimal%20transport%20for%0A%20%20unsupervised%20anomaly%20detection%20and%20localization&entry.906535625=Robin%20Trombetta%20and%20Carole%20Lartizien&entry.1292438233=%20%20Unsupervised%20anomaly%20detection%20aims%20to%20detect%20defective%20parts%20of%20a%20sample%20by%0Ahaving%20access%2C%20during%20training%2C%20to%20a%20set%20of%20normal%2C%20i.e.%20defect-free%2C%20data.%20It%0Ahas%20many%20applications%20in%20fields%2C%20such%20as%20industrial%20inspection%20or%20medical%0Aimaging%2C%20where%20acquiring%20labels%20is%20costly%20or%20when%20we%20want%20to%20avoid%20introducing%0Abiases%20in%20the%20type%20of%20anomalies%20that%20can%20be%20spotted.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20UAD%20method%20based%20on%20prototype%20learning%20and%20introduce%20a%20metric%20to%20compare%0Aa%20structured%20set%20of%20embeddings%20that%20balances%20a%20feature-based%20cost%20and%20a%0Aspatial-based%20cost.%20We%20leverage%20this%20metric%20to%20learn%20local%20and%20global%0Aprototypes%20with%20optimal%20transport%20from%20latent%20representations%20extracted%20with%20a%0Apre-trained%20image%20encoder.%20We%20demonstrate%20that%20our%20approach%20can%20enforce%20a%0Astructural%20constraint%20when%20learning%20the%20prototypes%2C%20allowing%20to%20capture%20the%0Aunderlying%20organization%20of%20the%20normal%20samples%2C%20thus%20improving%20the%20detection%20of%0Aincoherencies%20in%20images.%20Our%20model%20achieves%20performance%20that%20is%20on%20par%20with%0Astrong%20baselines%20on%20two%20reference%20benchmarks%20for%20anomaly%20detection%20on%0Aindustrial%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12927v2&entry.124074799=Read"},
{"title": "A theoretical framework for self-supervised contrastive learning for\n  continuous dependent data", "author": "Alexander Marusov and Aleksandr Yugay and Alexey Zaytsev", "abstract": "  Self-supervised learning (SSL) has emerged as a powerful approach to learning\nrepresentations, particularly in the field of computer vision. However, its\napplication to dependent data, such as temporal and spatio-temporal domains,\nremains underexplored. Besides, traditional contrastive SSL methods often\nassume \\emph{semantic independence between samples}, which does not hold for\ndependent data exhibiting complex correlations. We propose a novel theoretical\nframework for contrastive SSL tailored to \\emph{continuous dependent data},\nwhich allows the nearest samples to be semantically close to each other. In\nparticular, we propose two possible \\textit{ground truth similarity measures}\nbetween objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive\nan analytical form for the \\textit{estimated similarity matrix} that\naccommodates both types of closeness between samples, thereby introducing\ndependency-aware loss functions. We validate our approach, \\emph{Dependent\nTS2Vec}, on temporal and spatio-temporal downstream problems. Given the\ndependency patterns presented in the data, our approach surpasses modern ones\nfor dependent data, highlighting the effectiveness of our theoretically\ngrounded loss functions for SSL in capturing spatio-temporal dependencies.\nSpecifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with\naccuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on\nthe drought classification task, which involves complex spatio-temporal\npatterns, our method achieves a $7$\\% higher ROC-AUC score.\n", "link": "http://arxiv.org/abs/2506.09785v2", "date": "2025-09-02", "relevancy": 2.5932, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5293}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5225}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20theoretical%20framework%20for%20self-supervised%20contrastive%20learning%20for%0A%20%20continuous%20dependent%20data&body=Title%3A%20A%20theoretical%20framework%20for%20self-supervised%20contrastive%20learning%20for%0A%20%20continuous%20dependent%20data%0AAuthor%3A%20Alexander%20Marusov%20and%20Aleksandr%20Yugay%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20approach%20to%20learning%0Arepresentations%2C%20particularly%20in%20the%20field%20of%20computer%20vision.%20However%2C%20its%0Aapplication%20to%20dependent%20data%2C%20such%20as%20temporal%20and%20spatio-temporal%20domains%2C%0Aremains%20underexplored.%20Besides%2C%20traditional%20contrastive%20SSL%20methods%20often%0Aassume%20%5Cemph%7Bsemantic%20independence%20between%20samples%7D%2C%20which%20does%20not%20hold%20for%0Adependent%20data%20exhibiting%20complex%20correlations.%20We%20propose%20a%20novel%20theoretical%0Aframework%20for%20contrastive%20SSL%20tailored%20to%20%5Cemph%7Bcontinuous%20dependent%20data%7D%2C%0Awhich%20allows%20the%20nearest%20samples%20to%20be%20semantically%20close%20to%20each%20other.%20In%0Aparticular%2C%20we%20propose%20two%20possible%20%5Ctextit%7Bground%20truth%20similarity%20measures%7D%0Abetween%20objects%20--%20%5Cemph%7Bhard%7D%20and%20%5Cemph%7Bsoft%7D%20closeness.%20Under%20it%2C%20we%20derive%0Aan%20analytical%20form%20for%20the%20%5Ctextit%7Bestimated%20similarity%20matrix%7D%20that%0Aaccommodates%20both%20types%20of%20closeness%20between%20samples%2C%20thereby%20introducing%0Adependency-aware%20loss%20functions.%20We%20validate%20our%20approach%2C%20%5Cemph%7BDependent%0ATS2Vec%7D%2C%20on%20temporal%20and%20spatio-temporal%20downstream%20problems.%20Given%20the%0Adependency%20patterns%20presented%20in%20the%20data%2C%20our%20approach%20surpasses%20modern%20ones%0Afor%20dependent%20data%2C%20highlighting%20the%20effectiveness%20of%20our%20theoretically%0Agrounded%20loss%20functions%20for%20SSL%20in%20capturing%20spatio-temporal%20dependencies.%0ASpecifically%2C%20we%20outperform%20TS2Vec%20on%20the%20standard%20UEA%20and%20UCR%20benchmarks%2C%20with%0Aaccuracy%20improvements%20of%20%244.17%24%5C%25%20and%20%242.08%24%5C%25%2C%20respectively.%20Furthermore%2C%20on%0Athe%20drought%20classification%20task%2C%20which%20involves%20complex%20spatio-temporal%0Apatterns%2C%20our%20method%20achieves%20a%20%247%24%5C%25%20higher%20ROC-AUC%20score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09785v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520theoretical%2520framework%2520for%2520self-supervised%2520contrastive%2520learning%2520for%250A%2520%2520continuous%2520dependent%2520data%26entry.906535625%3DAlexander%2520Marusov%2520and%2520Aleksandr%2520Yugay%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520approach%2520to%2520learning%250Arepresentations%252C%2520particularly%2520in%2520the%2520field%2520of%2520computer%2520vision.%2520However%252C%2520its%250Aapplication%2520to%2520dependent%2520data%252C%2520such%2520as%2520temporal%2520and%2520spatio-temporal%2520domains%252C%250Aremains%2520underexplored.%2520Besides%252C%2520traditional%2520contrastive%2520SSL%2520methods%2520often%250Aassume%2520%255Cemph%257Bsemantic%2520independence%2520between%2520samples%257D%252C%2520which%2520does%2520not%2520hold%2520for%250Adependent%2520data%2520exhibiting%2520complex%2520correlations.%2520We%2520propose%2520a%2520novel%2520theoretical%250Aframework%2520for%2520contrastive%2520SSL%2520tailored%2520to%2520%255Cemph%257Bcontinuous%2520dependent%2520data%257D%252C%250Awhich%2520allows%2520the%2520nearest%2520samples%2520to%2520be%2520semantically%2520close%2520to%2520each%2520other.%2520In%250Aparticular%252C%2520we%2520propose%2520two%2520possible%2520%255Ctextit%257Bground%2520truth%2520similarity%2520measures%257D%250Abetween%2520objects%2520--%2520%255Cemph%257Bhard%257D%2520and%2520%255Cemph%257Bsoft%257D%2520closeness.%2520Under%2520it%252C%2520we%2520derive%250Aan%2520analytical%2520form%2520for%2520the%2520%255Ctextit%257Bestimated%2520similarity%2520matrix%257D%2520that%250Aaccommodates%2520both%2520types%2520of%2520closeness%2520between%2520samples%252C%2520thereby%2520introducing%250Adependency-aware%2520loss%2520functions.%2520We%2520validate%2520our%2520approach%252C%2520%255Cemph%257BDependent%250ATS2Vec%257D%252C%2520on%2520temporal%2520and%2520spatio-temporal%2520downstream%2520problems.%2520Given%2520the%250Adependency%2520patterns%2520presented%2520in%2520the%2520data%252C%2520our%2520approach%2520surpasses%2520modern%2520ones%250Afor%2520dependent%2520data%252C%2520highlighting%2520the%2520effectiveness%2520of%2520our%2520theoretically%250Agrounded%2520loss%2520functions%2520for%2520SSL%2520in%2520capturing%2520spatio-temporal%2520dependencies.%250ASpecifically%252C%2520we%2520outperform%2520TS2Vec%2520on%2520the%2520standard%2520UEA%2520and%2520UCR%2520benchmarks%252C%2520with%250Aaccuracy%2520improvements%2520of%2520%25244.17%2524%255C%2525%2520and%2520%25242.08%2524%255C%2525%252C%2520respectively.%2520Furthermore%252C%2520on%250Athe%2520drought%2520classification%2520task%252C%2520which%2520involves%2520complex%2520spatio-temporal%250Apatterns%252C%2520our%2520method%2520achieves%2520a%2520%25247%2524%255C%2525%2520higher%2520ROC-AUC%2520score.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09785v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20theoretical%20framework%20for%20self-supervised%20contrastive%20learning%20for%0A%20%20continuous%20dependent%20data&entry.906535625=Alexander%20Marusov%20and%20Aleksandr%20Yugay%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20approach%20to%20learning%0Arepresentations%2C%20particularly%20in%20the%20field%20of%20computer%20vision.%20However%2C%20its%0Aapplication%20to%20dependent%20data%2C%20such%20as%20temporal%20and%20spatio-temporal%20domains%2C%0Aremains%20underexplored.%20Besides%2C%20traditional%20contrastive%20SSL%20methods%20often%0Aassume%20%5Cemph%7Bsemantic%20independence%20between%20samples%7D%2C%20which%20does%20not%20hold%20for%0Adependent%20data%20exhibiting%20complex%20correlations.%20We%20propose%20a%20novel%20theoretical%0Aframework%20for%20contrastive%20SSL%20tailored%20to%20%5Cemph%7Bcontinuous%20dependent%20data%7D%2C%0Awhich%20allows%20the%20nearest%20samples%20to%20be%20semantically%20close%20to%20each%20other.%20In%0Aparticular%2C%20we%20propose%20two%20possible%20%5Ctextit%7Bground%20truth%20similarity%20measures%7D%0Abetween%20objects%20--%20%5Cemph%7Bhard%7D%20and%20%5Cemph%7Bsoft%7D%20closeness.%20Under%20it%2C%20we%20derive%0Aan%20analytical%20form%20for%20the%20%5Ctextit%7Bestimated%20similarity%20matrix%7D%20that%0Aaccommodates%20both%20types%20of%20closeness%20between%20samples%2C%20thereby%20introducing%0Adependency-aware%20loss%20functions.%20We%20validate%20our%20approach%2C%20%5Cemph%7BDependent%0ATS2Vec%7D%2C%20on%20temporal%20and%20spatio-temporal%20downstream%20problems.%20Given%20the%0Adependency%20patterns%20presented%20in%20the%20data%2C%20our%20approach%20surpasses%20modern%20ones%0Afor%20dependent%20data%2C%20highlighting%20the%20effectiveness%20of%20our%20theoretically%0Agrounded%20loss%20functions%20for%20SSL%20in%20capturing%20spatio-temporal%20dependencies.%0ASpecifically%2C%20we%20outperform%20TS2Vec%20on%20the%20standard%20UEA%20and%20UCR%20benchmarks%2C%20with%0Aaccuracy%20improvements%20of%20%244.17%24%5C%25%20and%20%242.08%24%5C%25%2C%20respectively.%20Furthermore%2C%20on%0Athe%20drought%20classification%20task%2C%20which%20involves%20complex%20spatio-temporal%0Apatterns%2C%20our%20method%20achieves%20a%20%247%24%5C%25%20higher%20ROC-AUC%20score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09785v2&entry.124074799=Read"},
{"title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning", "author": "Huanxuan Liao and Yixing Xu and Shizhu He and Guanchen Li and Xuanwu Yin and Dong Li and Emad Barsoum and Jun Zhao and Kang Liu", "abstract": "  Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.\n", "link": "http://arxiv.org/abs/2508.15212v2", "date": "2025-09-02", "relevancy": 2.4281, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4816}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparK%3A%20Query-Aware%20Unstructured%20Sparsity%20with%20Recoverable%20KV%20Cache%0A%20%20Channel%20Pruning&body=Title%3A%20SparK%3A%20Query-Aware%20Unstructured%20Sparsity%20with%20Recoverable%20KV%20Cache%0A%20%20Channel%20Pruning%0AAuthor%3A%20Huanxuan%20Liao%20and%20Yixing%20Xu%20and%20Shizhu%20He%20and%20Guanchen%20Li%20and%20Xuanwu%20Yin%20and%20Dong%20Li%20and%20Emad%20Barsoum%20and%20Jun%20Zhao%20and%20Kang%20Liu%0AAbstract%3A%20%20%20Long-context%20inference%20in%20large%20language%20models%20%28LLMs%29%20is%20increasingly%0Aconstrained%20by%20the%20KV%20cache%20bottleneck%3A%20memory%20usage%20grows%20linearly%20with%0Asequence%20length%2C%20while%20attention%20computation%20scales%20quadratically.%20Existing%0Aapproaches%20address%20this%20issue%20by%20compressing%20the%20KV%20cache%20along%20the%20temporal%0Aaxis%20through%20strategies%20such%20as%20token%20eviction%20or%20merging%20to%20reduce%20memory%20and%0Acomputational%20overhead.%20However%2C%20these%20methods%20often%20neglect%20fine-grained%0Aimportance%20variations%20across%20feature%20dimensions%20%28i.e.%2C%20the%20channel%20axis%29%2C%0Athereby%20limiting%20their%20ability%20to%20effectively%20balance%20efficiency%20and%20model%0Aaccuracy.%20In%20reality%2C%20we%20observe%20that%20channel%20saliency%20varies%20dramatically%0Aacross%20both%20queries%20and%20positions%3A%20certain%20feature%20channels%20carry%20near-zero%0Ainformation%20for%20a%20given%20query%2C%20while%20others%20spike%20in%20relevance.%20To%20address%20this%0Aoversight%2C%20we%20propose%20SPARK%2C%20a%20training-free%20plug-and-play%20method%20that%20applies%0Aunstructured%20sparsity%20by%20pruning%20KV%20at%20the%20channel%20level%2C%20while%20dynamically%0Arestoring%20the%20pruned%20entries%20during%20attention%20score%20computation.%20Notably%2C%20our%0Aapproach%20is%20orthogonal%20to%20existing%20KV%20compression%20and%20quantization%20techniques%2C%0Amaking%20it%20compatible%20for%20integration%20with%20them%20to%20achieve%20further%20acceleration.%0ABy%20reducing%20channel-level%20redundancy%2C%20SPARK%20enables%20processing%20of%20longer%0Asequences%20within%20the%20same%20memory%20budget.%20For%20sequences%20of%20equal%20length%2C%20SPARK%0Anot%20only%20preserves%20or%20improves%20model%20accuracy%20but%20also%20reduces%20KV%20cache%20storage%0Aby%20over%2030%25%20compared%20to%20eviction-based%20methods.%20Furthermore%2C%20even%20with%20an%0Aaggressive%20pruning%20ratio%20of%2080%25%2C%20SPARK%20maintains%20performance%20with%20less%0Adegradation%20than%205%25%20compared%20to%20the%20baseline%20eviction%20method%2C%20demonstrating%20its%0Arobustness%20and%20effectiveness.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Xnhyacinth/SparK.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15212v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparK%253A%2520Query-Aware%2520Unstructured%2520Sparsity%2520with%2520Recoverable%2520KV%2520Cache%250A%2520%2520Channel%2520Pruning%26entry.906535625%3DHuanxuan%2520Liao%2520and%2520Yixing%2520Xu%2520and%2520Shizhu%2520He%2520and%2520Guanchen%2520Li%2520and%2520Xuanwu%2520Yin%2520and%2520Dong%2520Li%2520and%2520Emad%2520Barsoum%2520and%2520Jun%2520Zhao%2520and%2520Kang%2520Liu%26entry.1292438233%3D%2520%2520Long-context%2520inference%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520increasingly%250Aconstrained%2520by%2520the%2520KV%2520cache%2520bottleneck%253A%2520memory%2520usage%2520grows%2520linearly%2520with%250Asequence%2520length%252C%2520while%2520attention%2520computation%2520scales%2520quadratically.%2520Existing%250Aapproaches%2520address%2520this%2520issue%2520by%2520compressing%2520the%2520KV%2520cache%2520along%2520the%2520temporal%250Aaxis%2520through%2520strategies%2520such%2520as%2520token%2520eviction%2520or%2520merging%2520to%2520reduce%2520memory%2520and%250Acomputational%2520overhead.%2520However%252C%2520these%2520methods%2520often%2520neglect%2520fine-grained%250Aimportance%2520variations%2520across%2520feature%2520dimensions%2520%2528i.e.%252C%2520the%2520channel%2520axis%2529%252C%250Athereby%2520limiting%2520their%2520ability%2520to%2520effectively%2520balance%2520efficiency%2520and%2520model%250Aaccuracy.%2520In%2520reality%252C%2520we%2520observe%2520that%2520channel%2520saliency%2520varies%2520dramatically%250Aacross%2520both%2520queries%2520and%2520positions%253A%2520certain%2520feature%2520channels%2520carry%2520near-zero%250Ainformation%2520for%2520a%2520given%2520query%252C%2520while%2520others%2520spike%2520in%2520relevance.%2520To%2520address%2520this%250Aoversight%252C%2520we%2520propose%2520SPARK%252C%2520a%2520training-free%2520plug-and-play%2520method%2520that%2520applies%250Aunstructured%2520sparsity%2520by%2520pruning%2520KV%2520at%2520the%2520channel%2520level%252C%2520while%2520dynamically%250Arestoring%2520the%2520pruned%2520entries%2520during%2520attention%2520score%2520computation.%2520Notably%252C%2520our%250Aapproach%2520is%2520orthogonal%2520to%2520existing%2520KV%2520compression%2520and%2520quantization%2520techniques%252C%250Amaking%2520it%2520compatible%2520for%2520integration%2520with%2520them%2520to%2520achieve%2520further%2520acceleration.%250ABy%2520reducing%2520channel-level%2520redundancy%252C%2520SPARK%2520enables%2520processing%2520of%2520longer%250Asequences%2520within%2520the%2520same%2520memory%2520budget.%2520For%2520sequences%2520of%2520equal%2520length%252C%2520SPARK%250Anot%2520only%2520preserves%2520or%2520improves%2520model%2520accuracy%2520but%2520also%2520reduces%2520KV%2520cache%2520storage%250Aby%2520over%252030%2525%2520compared%2520to%2520eviction-based%2520methods.%2520Furthermore%252C%2520even%2520with%2520an%250Aaggressive%2520pruning%2520ratio%2520of%252080%2525%252C%2520SPARK%2520maintains%2520performance%2520with%2520less%250Adegradation%2520than%25205%2525%2520compared%2520to%2520the%2520baseline%2520eviction%2520method%252C%2520demonstrating%2520its%250Arobustness%2520and%2520effectiveness.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/Xnhyacinth/SparK.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15212v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparK%3A%20Query-Aware%20Unstructured%20Sparsity%20with%20Recoverable%20KV%20Cache%0A%20%20Channel%20Pruning&entry.906535625=Huanxuan%20Liao%20and%20Yixing%20Xu%20and%20Shizhu%20He%20and%20Guanchen%20Li%20and%20Xuanwu%20Yin%20and%20Dong%20Li%20and%20Emad%20Barsoum%20and%20Jun%20Zhao%20and%20Kang%20Liu&entry.1292438233=%20%20Long-context%20inference%20in%20large%20language%20models%20%28LLMs%29%20is%20increasingly%0Aconstrained%20by%20the%20KV%20cache%20bottleneck%3A%20memory%20usage%20grows%20linearly%20with%0Asequence%20length%2C%20while%20attention%20computation%20scales%20quadratically.%20Existing%0Aapproaches%20address%20this%20issue%20by%20compressing%20the%20KV%20cache%20along%20the%20temporal%0Aaxis%20through%20strategies%20such%20as%20token%20eviction%20or%20merging%20to%20reduce%20memory%20and%0Acomputational%20overhead.%20However%2C%20these%20methods%20often%20neglect%20fine-grained%0Aimportance%20variations%20across%20feature%20dimensions%20%28i.e.%2C%20the%20channel%20axis%29%2C%0Athereby%20limiting%20their%20ability%20to%20effectively%20balance%20efficiency%20and%20model%0Aaccuracy.%20In%20reality%2C%20we%20observe%20that%20channel%20saliency%20varies%20dramatically%0Aacross%20both%20queries%20and%20positions%3A%20certain%20feature%20channels%20carry%20near-zero%0Ainformation%20for%20a%20given%20query%2C%20while%20others%20spike%20in%20relevance.%20To%20address%20this%0Aoversight%2C%20we%20propose%20SPARK%2C%20a%20training-free%20plug-and-play%20method%20that%20applies%0Aunstructured%20sparsity%20by%20pruning%20KV%20at%20the%20channel%20level%2C%20while%20dynamically%0Arestoring%20the%20pruned%20entries%20during%20attention%20score%20computation.%20Notably%2C%20our%0Aapproach%20is%20orthogonal%20to%20existing%20KV%20compression%20and%20quantization%20techniques%2C%0Amaking%20it%20compatible%20for%20integration%20with%20them%20to%20achieve%20further%20acceleration.%0ABy%20reducing%20channel-level%20redundancy%2C%20SPARK%20enables%20processing%20of%20longer%0Asequences%20within%20the%20same%20memory%20budget.%20For%20sequences%20of%20equal%20length%2C%20SPARK%0Anot%20only%20preserves%20or%20improves%20model%20accuracy%20but%20also%20reduces%20KV%20cache%20storage%0Aby%20over%2030%25%20compared%20to%20eviction-based%20methods.%20Furthermore%2C%20even%20with%20an%0Aaggressive%20pruning%20ratio%20of%2080%25%2C%20SPARK%20maintains%20performance%20with%20less%0Adegradation%20than%205%25%20compared%20to%20the%20baseline%20eviction%20method%2C%20demonstrating%20its%0Arobustness%20and%20effectiveness.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Xnhyacinth/SparK.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15212v2&entry.124074799=Read"},
{"title": "NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation", "author": "Max Gandyra and Alessandro Santonicola and Michael Beetz", "abstract": "  Instance segmentation of novel objects instances in RGB images, given some\nexample images for each object, is a well known problem in computer vision.\nDesigning a model general enough to be employed for all kinds of novel objects\nwithout (re-) training has proven to be a difficult task. To handle this, we\npresent a new training-free framework, called: Novel Object Cyclic Threshold\nbased Instance Segmentation (NOCTIS). NOCTIS integrates two pre-trained models:\nGrounded-SAM 2 for object proposals with precise bounding boxes and\ncorresponding segmentation masks; and DINOv2 for robust class and patch\nembeddings, due to its zero-shot capabilities. Internally, the proposal-object\nmatching is realized by determining an object matching score based on the\nsimilarity of the class embeddings and the average maximum similarity of the\npatch embeddings with a new cyclic thresholding (CT) mechanism that mitigates\nunstable matches caused by repetitive textures or visually similar patterns.\nBeyond CT, NOCTIS introduces: (i) an appearance score that is unaffected by\nobject selection bias; (ii) the usage of the average confidence of the\nproposals bounding box and mask as a scoring component; and (iii) an RGB-only\npipeline that performs even better than RGB-D ones. We empirically show that\nNOCTIS, without further training/fine tuning, attains state-of-the-art results\nregarding the mean AP score, w.r.t. the best RGB and RGB-D methods on the seven\ncore datasets of the BOP 2023 challenge for the \"Model-based 2D segmentation of\nunseen objects\" task.\n", "link": "http://arxiv.org/abs/2507.01463v2", "date": "2025-09-02", "relevancy": 2.2107, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NOCTIS%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation&body=Title%3A%20NOCTIS%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation%0AAuthor%3A%20Max%20Gandyra%20and%20Alessandro%20Santonicola%20and%20Michael%20Beetz%0AAbstract%3A%20%20%20Instance%20segmentation%20of%20novel%20objects%20instances%20in%20RGB%20images%2C%20given%20some%0Aexample%20images%20for%20each%20object%2C%20is%20a%20well%20known%20problem%20in%20computer%20vision.%0ADesigning%20a%20model%20general%20enough%20to%20be%20employed%20for%20all%20kinds%20of%20novel%20objects%0Awithout%20%28re-%29%20training%20has%20proven%20to%20be%20a%20difficult%20task.%20To%20handle%20this%2C%20we%0Apresent%20a%20new%20training-free%20framework%2C%20called%3A%20Novel%20Object%20Cyclic%20Threshold%0Abased%20Instance%20Segmentation%20%28NOCTIS%29.%20NOCTIS%20integrates%20two%20pre-trained%20models%3A%0AGrounded-SAM%202%20for%20object%20proposals%20with%20precise%20bounding%20boxes%20and%0Acorresponding%20segmentation%20masks%3B%20and%20DINOv2%20for%20robust%20class%20and%20patch%0Aembeddings%2C%20due%20to%20its%20zero-shot%20capabilities.%20Internally%2C%20the%20proposal-object%0Amatching%20is%20realized%20by%20determining%20an%20object%20matching%20score%20based%20on%20the%0Asimilarity%20of%20the%20class%20embeddings%20and%20the%20average%20maximum%20similarity%20of%20the%0Apatch%20embeddings%20with%20a%20new%20cyclic%20thresholding%20%28CT%29%20mechanism%20that%20mitigates%0Aunstable%20matches%20caused%20by%20repetitive%20textures%20or%20visually%20similar%20patterns.%0ABeyond%20CT%2C%20NOCTIS%20introduces%3A%20%28i%29%20an%20appearance%20score%20that%20is%20unaffected%20by%0Aobject%20selection%20bias%3B%20%28ii%29%20the%20usage%20of%20the%20average%20confidence%20of%20the%0Aproposals%20bounding%20box%20and%20mask%20as%20a%20scoring%20component%3B%20and%20%28iii%29%20an%20RGB-only%0Apipeline%20that%20performs%20even%20better%20than%20RGB-D%20ones.%20We%20empirically%20show%20that%0ANOCTIS%2C%20without%20further%20training/fine%20tuning%2C%20attains%20state-of-the-art%20results%0Aregarding%20the%20mean%20AP%20score%2C%20w.r.t.%20the%20best%20RGB%20and%20RGB-D%20methods%20on%20the%20seven%0Acore%20datasets%20of%20the%20BOP%202023%20challenge%20for%20the%20%22Model-based%202D%20segmentation%20of%0Aunseen%20objects%22%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNOCTIS%253A%2520Novel%2520Object%2520Cyclic%2520Threshold%2520based%2520Instance%2520Segmentation%26entry.906535625%3DMax%2520Gandyra%2520and%2520Alessandro%2520Santonicola%2520and%2520Michael%2520Beetz%26entry.1292438233%3D%2520%2520Instance%2520segmentation%2520of%2520novel%2520objects%2520instances%2520in%2520RGB%2520images%252C%2520given%2520some%250Aexample%2520images%2520for%2520each%2520object%252C%2520is%2520a%2520well%2520known%2520problem%2520in%2520computer%2520vision.%250ADesigning%2520a%2520model%2520general%2520enough%2520to%2520be%2520employed%2520for%2520all%2520kinds%2520of%2520novel%2520objects%250Awithout%2520%2528re-%2529%2520training%2520has%2520proven%2520to%2520be%2520a%2520difficult%2520task.%2520To%2520handle%2520this%252C%2520we%250Apresent%2520a%2520new%2520training-free%2520framework%252C%2520called%253A%2520Novel%2520Object%2520Cyclic%2520Threshold%250Abased%2520Instance%2520Segmentation%2520%2528NOCTIS%2529.%2520NOCTIS%2520integrates%2520two%2520pre-trained%2520models%253A%250AGrounded-SAM%25202%2520for%2520object%2520proposals%2520with%2520precise%2520bounding%2520boxes%2520and%250Acorresponding%2520segmentation%2520masks%253B%2520and%2520DINOv2%2520for%2520robust%2520class%2520and%2520patch%250Aembeddings%252C%2520due%2520to%2520its%2520zero-shot%2520capabilities.%2520Internally%252C%2520the%2520proposal-object%250Amatching%2520is%2520realized%2520by%2520determining%2520an%2520object%2520matching%2520score%2520based%2520on%2520the%250Asimilarity%2520of%2520the%2520class%2520embeddings%2520and%2520the%2520average%2520maximum%2520similarity%2520of%2520the%250Apatch%2520embeddings%2520with%2520a%2520new%2520cyclic%2520thresholding%2520%2528CT%2529%2520mechanism%2520that%2520mitigates%250Aunstable%2520matches%2520caused%2520by%2520repetitive%2520textures%2520or%2520visually%2520similar%2520patterns.%250ABeyond%2520CT%252C%2520NOCTIS%2520introduces%253A%2520%2528i%2529%2520an%2520appearance%2520score%2520that%2520is%2520unaffected%2520by%250Aobject%2520selection%2520bias%253B%2520%2528ii%2529%2520the%2520usage%2520of%2520the%2520average%2520confidence%2520of%2520the%250Aproposals%2520bounding%2520box%2520and%2520mask%2520as%2520a%2520scoring%2520component%253B%2520and%2520%2528iii%2529%2520an%2520RGB-only%250Apipeline%2520that%2520performs%2520even%2520better%2520than%2520RGB-D%2520ones.%2520We%2520empirically%2520show%2520that%250ANOCTIS%252C%2520without%2520further%2520training/fine%2520tuning%252C%2520attains%2520state-of-the-art%2520results%250Aregarding%2520the%2520mean%2520AP%2520score%252C%2520w.r.t.%2520the%2520best%2520RGB%2520and%2520RGB-D%2520methods%2520on%2520the%2520seven%250Acore%2520datasets%2520of%2520the%2520BOP%25202023%2520challenge%2520for%2520the%2520%2522Model-based%25202D%2520segmentation%2520of%250Aunseen%2520objects%2522%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NOCTIS%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation&entry.906535625=Max%20Gandyra%20and%20Alessandro%20Santonicola%20and%20Michael%20Beetz&entry.1292438233=%20%20Instance%20segmentation%20of%20novel%20objects%20instances%20in%20RGB%20images%2C%20given%20some%0Aexample%20images%20for%20each%20object%2C%20is%20a%20well%20known%20problem%20in%20computer%20vision.%0ADesigning%20a%20model%20general%20enough%20to%20be%20employed%20for%20all%20kinds%20of%20novel%20objects%0Awithout%20%28re-%29%20training%20has%20proven%20to%20be%20a%20difficult%20task.%20To%20handle%20this%2C%20we%0Apresent%20a%20new%20training-free%20framework%2C%20called%3A%20Novel%20Object%20Cyclic%20Threshold%0Abased%20Instance%20Segmentation%20%28NOCTIS%29.%20NOCTIS%20integrates%20two%20pre-trained%20models%3A%0AGrounded-SAM%202%20for%20object%20proposals%20with%20precise%20bounding%20boxes%20and%0Acorresponding%20segmentation%20masks%3B%20and%20DINOv2%20for%20robust%20class%20and%20patch%0Aembeddings%2C%20due%20to%20its%20zero-shot%20capabilities.%20Internally%2C%20the%20proposal-object%0Amatching%20is%20realized%20by%20determining%20an%20object%20matching%20score%20based%20on%20the%0Asimilarity%20of%20the%20class%20embeddings%20and%20the%20average%20maximum%20similarity%20of%20the%0Apatch%20embeddings%20with%20a%20new%20cyclic%20thresholding%20%28CT%29%20mechanism%20that%20mitigates%0Aunstable%20matches%20caused%20by%20repetitive%20textures%20or%20visually%20similar%20patterns.%0ABeyond%20CT%2C%20NOCTIS%20introduces%3A%20%28i%29%20an%20appearance%20score%20that%20is%20unaffected%20by%0Aobject%20selection%20bias%3B%20%28ii%29%20the%20usage%20of%20the%20average%20confidence%20of%20the%0Aproposals%20bounding%20box%20and%20mask%20as%20a%20scoring%20component%3B%20and%20%28iii%29%20an%20RGB-only%0Apipeline%20that%20performs%20even%20better%20than%20RGB-D%20ones.%20We%20empirically%20show%20that%0ANOCTIS%2C%20without%20further%20training/fine%20tuning%2C%20attains%20state-of-the-art%20results%0Aregarding%20the%20mean%20AP%20score%2C%20w.r.t.%20the%20best%20RGB%20and%20RGB-D%20methods%20on%20the%20seven%0Acore%20datasets%20of%20the%20BOP%202023%20challenge%20for%20the%20%22Model-based%202D%20segmentation%20of%0Aunseen%20objects%22%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01463v2&entry.124074799=Read"},
{"title": "Text Meets Topology: Rethinking Out-of-distribution Detection in\n  Text-Rich Networks", "author": "Danny Wang and Ruihong Qiu and Guangdong Bai and Zi Huang", "abstract": "  Out-of-distribution (OOD) detection remains challenging in text-rich\nnetworks, where textual features intertwine with topological structures.\nExisting methods primarily address label shifts or rudimentary domain-based\nsplits, overlooking the intricate textual-structural diversity. For example, in\nsocial networks, where users represent nodes with textual features (name, bio)\nwhile edges indicate friendship status, OOD may stem from the distinct language\npatterns between bot and normal users. To address this gap, we introduce the\nTextTopoOOD framework for evaluating detection across diverse OOD scenarios:\n(1) attribute-level shifts via text augmentations and embedding perturbations;\n(2) structural shifts through edge rewiring and semantic connections; (3)\nthematically-guided label shifts; and (4) domain-based divisions. Furthermore,\nwe propose TNT-OOD to model the complex interplay between Text aNd Topology\nusing: 1) a novel cross-attention module to fuse local structure into\nnode-level text representations, and 2) a HyperNetwork to generate\nnode-specific transformation parameters. This aligns topological and semantic\nfeatures of ID nodes, enhancing ID/OOD distinction across structural and\ntextual shifts. Experiments on 11 datasets across four OOD scenarios\ndemonstrate the nuanced challenge of TextTopoOOD for evaluating OOD detection\nin text-rich networks.\n", "link": "http://arxiv.org/abs/2508.17690v2", "date": "2025-09-02", "relevancy": 1.4615, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4844}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text%20Meets%20Topology%3A%20Rethinking%20Out-of-distribution%20Detection%20in%0A%20%20Text-Rich%20Networks&body=Title%3A%20Text%20Meets%20Topology%3A%20Rethinking%20Out-of-distribution%20Detection%20in%0A%20%20Text-Rich%20Networks%0AAuthor%3A%20Danny%20Wang%20and%20Ruihong%20Qiu%20and%20Guangdong%20Bai%20and%20Zi%20Huang%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20remains%20challenging%20in%20text-rich%0Anetworks%2C%20where%20textual%20features%20intertwine%20with%20topological%20structures.%0AExisting%20methods%20primarily%20address%20label%20shifts%20or%20rudimentary%20domain-based%0Asplits%2C%20overlooking%20the%20intricate%20textual-structural%20diversity.%20For%20example%2C%20in%0Asocial%20networks%2C%20where%20users%20represent%20nodes%20with%20textual%20features%20%28name%2C%20bio%29%0Awhile%20edges%20indicate%20friendship%20status%2C%20OOD%20may%20stem%20from%20the%20distinct%20language%0Apatterns%20between%20bot%20and%20normal%20users.%20To%20address%20this%20gap%2C%20we%20introduce%20the%0ATextTopoOOD%20framework%20for%20evaluating%20detection%20across%20diverse%20OOD%20scenarios%3A%0A%281%29%20attribute-level%20shifts%20via%20text%20augmentations%20and%20embedding%20perturbations%3B%0A%282%29%20structural%20shifts%20through%20edge%20rewiring%20and%20semantic%20connections%3B%20%283%29%0Athematically-guided%20label%20shifts%3B%20and%20%284%29%20domain-based%20divisions.%20Furthermore%2C%0Awe%20propose%20TNT-OOD%20to%20model%20the%20complex%20interplay%20between%20Text%20aNd%20Topology%0Ausing%3A%201%29%20a%20novel%20cross-attention%20module%20to%20fuse%20local%20structure%20into%0Anode-level%20text%20representations%2C%20and%202%29%20a%20HyperNetwork%20to%20generate%0Anode-specific%20transformation%20parameters.%20This%20aligns%20topological%20and%20semantic%0Afeatures%20of%20ID%20nodes%2C%20enhancing%20ID/OOD%20distinction%20across%20structural%20and%0Atextual%20shifts.%20Experiments%20on%2011%20datasets%20across%20four%20OOD%20scenarios%0Ademonstrate%20the%20nuanced%20challenge%20of%20TextTopoOOD%20for%20evaluating%20OOD%20detection%0Ain%20text-rich%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText%2520Meets%2520Topology%253A%2520Rethinking%2520Out-of-distribution%2520Detection%2520in%250A%2520%2520Text-Rich%2520Networks%26entry.906535625%3DDanny%2520Wang%2520and%2520Ruihong%2520Qiu%2520and%2520Guangdong%2520Bai%2520and%2520Zi%2520Huang%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520remains%2520challenging%2520in%2520text-rich%250Anetworks%252C%2520where%2520textual%2520features%2520intertwine%2520with%2520topological%2520structures.%250AExisting%2520methods%2520primarily%2520address%2520label%2520shifts%2520or%2520rudimentary%2520domain-based%250Asplits%252C%2520overlooking%2520the%2520intricate%2520textual-structural%2520diversity.%2520For%2520example%252C%2520in%250Asocial%2520networks%252C%2520where%2520users%2520represent%2520nodes%2520with%2520textual%2520features%2520%2528name%252C%2520bio%2529%250Awhile%2520edges%2520indicate%2520friendship%2520status%252C%2520OOD%2520may%2520stem%2520from%2520the%2520distinct%2520language%250Apatterns%2520between%2520bot%2520and%2520normal%2520users.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%250ATextTopoOOD%2520framework%2520for%2520evaluating%2520detection%2520across%2520diverse%2520OOD%2520scenarios%253A%250A%25281%2529%2520attribute-level%2520shifts%2520via%2520text%2520augmentations%2520and%2520embedding%2520perturbations%253B%250A%25282%2529%2520structural%2520shifts%2520through%2520edge%2520rewiring%2520and%2520semantic%2520connections%253B%2520%25283%2529%250Athematically-guided%2520label%2520shifts%253B%2520and%2520%25284%2529%2520domain-based%2520divisions.%2520Furthermore%252C%250Awe%2520propose%2520TNT-OOD%2520to%2520model%2520the%2520complex%2520interplay%2520between%2520Text%2520aNd%2520Topology%250Ausing%253A%25201%2529%2520a%2520novel%2520cross-attention%2520module%2520to%2520fuse%2520local%2520structure%2520into%250Anode-level%2520text%2520representations%252C%2520and%25202%2529%2520a%2520HyperNetwork%2520to%2520generate%250Anode-specific%2520transformation%2520parameters.%2520This%2520aligns%2520topological%2520and%2520semantic%250Afeatures%2520of%2520ID%2520nodes%252C%2520enhancing%2520ID/OOD%2520distinction%2520across%2520structural%2520and%250Atextual%2520shifts.%2520Experiments%2520on%252011%2520datasets%2520across%2520four%2520OOD%2520scenarios%250Ademonstrate%2520the%2520nuanced%2520challenge%2520of%2520TextTopoOOD%2520for%2520evaluating%2520OOD%2520detection%250Ain%2520text-rich%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text%20Meets%20Topology%3A%20Rethinking%20Out-of-distribution%20Detection%20in%0A%20%20Text-Rich%20Networks&entry.906535625=Danny%20Wang%20and%20Ruihong%20Qiu%20and%20Guangdong%20Bai%20and%20Zi%20Huang&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20remains%20challenging%20in%20text-rich%0Anetworks%2C%20where%20textual%20features%20intertwine%20with%20topological%20structures.%0AExisting%20methods%20primarily%20address%20label%20shifts%20or%20rudimentary%20domain-based%0Asplits%2C%20overlooking%20the%20intricate%20textual-structural%20diversity.%20For%20example%2C%20in%0Asocial%20networks%2C%20where%20users%20represent%20nodes%20with%20textual%20features%20%28name%2C%20bio%29%0Awhile%20edges%20indicate%20friendship%20status%2C%20OOD%20may%20stem%20from%20the%20distinct%20language%0Apatterns%20between%20bot%20and%20normal%20users.%20To%20address%20this%20gap%2C%20we%20introduce%20the%0ATextTopoOOD%20framework%20for%20evaluating%20detection%20across%20diverse%20OOD%20scenarios%3A%0A%281%29%20attribute-level%20shifts%20via%20text%20augmentations%20and%20embedding%20perturbations%3B%0A%282%29%20structural%20shifts%20through%20edge%20rewiring%20and%20semantic%20connections%3B%20%283%29%0Athematically-guided%20label%20shifts%3B%20and%20%284%29%20domain-based%20divisions.%20Furthermore%2C%0Awe%20propose%20TNT-OOD%20to%20model%20the%20complex%20interplay%20between%20Text%20aNd%20Topology%0Ausing%3A%201%29%20a%20novel%20cross-attention%20module%20to%20fuse%20local%20structure%20into%0Anode-level%20text%20representations%2C%20and%202%29%20a%20HyperNetwork%20to%20generate%0Anode-specific%20transformation%20parameters.%20This%20aligns%20topological%20and%20semantic%0Afeatures%20of%20ID%20nodes%2C%20enhancing%20ID/OOD%20distinction%20across%20structural%20and%0Atextual%20shifts.%20Experiments%20on%2011%20datasets%20across%20four%20OOD%20scenarios%0Ademonstrate%20the%20nuanced%20challenge%20of%20TextTopoOOD%20for%20evaluating%20OOD%20detection%0Ain%20text-rich%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17690v2&entry.124074799=Read"},
{"title": "Programmable k-local Ising Machines and all-optical Kolmogorov-Arnold\n  Networks on Photonic Platforms", "author": "Nikita Stroev and Natalia G. Berloff", "abstract": "  Photonic computing promises energy-efficient acceleration for optimization\nand learning, yet discrete combinatorial search and continuous function\napproximation have largely required distinct devices and control stacks. Here\nwe unify k-local Ising optimization and optical Kolmogorov-Arnold network (KAN)\nlearning on a single photonic platform, establishing a critical convergence\npoint in optical computing. We introduce an SLM-centric primitive that\nrealizes, in one stroke, all-optical k-local Ising interactions and fully\noptical KAN layers. The key idea is to convert the structural nonlinearity of a\nnominally linear scatterer into a per-window computational resource by adding a\nsingle relay pass through the same spatial light modulator: a folded 4f relay\nre-images the first Fourier plane onto the SLM so that each selected clique or\nchannel occupies a disjoint window with its own second pass phase patch.\nPropagation remains linear in the optical field, yet the measured intensity in\neach window becomes a freely programmable polynomial of the clique sum or\nprojection amplitude. This yields native, per clique k-local couplings without\nnonlinear media and, in parallel, the many independent univariate\nnonlinearities required by KAN layers, all trainable with in-situ physical\ngradients using two frames (forward and adjoint). We outline implementations on\nspatial photonic Ising machines, injection-locked vertical cavity surface\nemitting laser (VCSEL) arrays, and Microsoft analog optical computers; in all\ncases the hardware change is one extra lens and a fold (or an on-chip 4f loop),\nenabling a minimal overhead, massively parallel route to high-order Ising\noptimization and trainable, all-optical KAN processing on one platform.\n", "link": "http://arxiv.org/abs/2508.17440v2", "date": "2025-09-02", "relevancy": 1.4117, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4825}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4679}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Programmable%20k-local%20Ising%20Machines%20and%20all-optical%20Kolmogorov-Arnold%0A%20%20Networks%20on%20Photonic%20Platforms&body=Title%3A%20Programmable%20k-local%20Ising%20Machines%20and%20all-optical%20Kolmogorov-Arnold%0A%20%20Networks%20on%20Photonic%20Platforms%0AAuthor%3A%20Nikita%20Stroev%20and%20Natalia%20G.%20Berloff%0AAbstract%3A%20%20%20Photonic%20computing%20promises%20energy-efficient%20acceleration%20for%20optimization%0Aand%20learning%2C%20yet%20discrete%20combinatorial%20search%20and%20continuous%20function%0Aapproximation%20have%20largely%20required%20distinct%20devices%20and%20control%20stacks.%20Here%0Awe%20unify%20k-local%20Ising%20optimization%20and%20optical%20Kolmogorov-Arnold%20network%20%28KAN%29%0Alearning%20on%20a%20single%20photonic%20platform%2C%20establishing%20a%20critical%20convergence%0Apoint%20in%20optical%20computing.%20We%20introduce%20an%20SLM-centric%20primitive%20that%0Arealizes%2C%20in%20one%20stroke%2C%20all-optical%20k-local%20Ising%20interactions%20and%20fully%0Aoptical%20KAN%20layers.%20The%20key%20idea%20is%20to%20convert%20the%20structural%20nonlinearity%20of%20a%0Anominally%20linear%20scatterer%20into%20a%20per-window%20computational%20resource%20by%20adding%20a%0Asingle%20relay%20pass%20through%20the%20same%20spatial%20light%20modulator%3A%20a%20folded%204f%20relay%0Are-images%20the%20first%20Fourier%20plane%20onto%20the%20SLM%20so%20that%20each%20selected%20clique%20or%0Achannel%20occupies%20a%20disjoint%20window%20with%20its%20own%20second%20pass%20phase%20patch.%0APropagation%20remains%20linear%20in%20the%20optical%20field%2C%20yet%20the%20measured%20intensity%20in%0Aeach%20window%20becomes%20a%20freely%20programmable%20polynomial%20of%20the%20clique%20sum%20or%0Aprojection%20amplitude.%20This%20yields%20native%2C%20per%20clique%20k-local%20couplings%20without%0Anonlinear%20media%20and%2C%20in%20parallel%2C%20the%20many%20independent%20univariate%0Anonlinearities%20required%20by%20KAN%20layers%2C%20all%20trainable%20with%20in-situ%20physical%0Agradients%20using%20two%20frames%20%28forward%20and%20adjoint%29.%20We%20outline%20implementations%20on%0Aspatial%20photonic%20Ising%20machines%2C%20injection-locked%20vertical%20cavity%20surface%0Aemitting%20laser%20%28VCSEL%29%20arrays%2C%20and%20Microsoft%20analog%20optical%20computers%3B%20in%20all%0Acases%20the%20hardware%20change%20is%20one%20extra%20lens%20and%20a%20fold%20%28or%20an%20on-chip%204f%20loop%29%2C%0Aenabling%20a%20minimal%20overhead%2C%20massively%20parallel%20route%20to%20high-order%20Ising%0Aoptimization%20and%20trainable%2C%20all-optical%20KAN%20processing%20on%20one%20platform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17440v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgrammable%2520k-local%2520Ising%2520Machines%2520and%2520all-optical%2520Kolmogorov-Arnold%250A%2520%2520Networks%2520on%2520Photonic%2520Platforms%26entry.906535625%3DNikita%2520Stroev%2520and%2520Natalia%2520G.%2520Berloff%26entry.1292438233%3D%2520%2520Photonic%2520computing%2520promises%2520energy-efficient%2520acceleration%2520for%2520optimization%250Aand%2520learning%252C%2520yet%2520discrete%2520combinatorial%2520search%2520and%2520continuous%2520function%250Aapproximation%2520have%2520largely%2520required%2520distinct%2520devices%2520and%2520control%2520stacks.%2520Here%250Awe%2520unify%2520k-local%2520Ising%2520optimization%2520and%2520optical%2520Kolmogorov-Arnold%2520network%2520%2528KAN%2529%250Alearning%2520on%2520a%2520single%2520photonic%2520platform%252C%2520establishing%2520a%2520critical%2520convergence%250Apoint%2520in%2520optical%2520computing.%2520We%2520introduce%2520an%2520SLM-centric%2520primitive%2520that%250Arealizes%252C%2520in%2520one%2520stroke%252C%2520all-optical%2520k-local%2520Ising%2520interactions%2520and%2520fully%250Aoptical%2520KAN%2520layers.%2520The%2520key%2520idea%2520is%2520to%2520convert%2520the%2520structural%2520nonlinearity%2520of%2520a%250Anominally%2520linear%2520scatterer%2520into%2520a%2520per-window%2520computational%2520resource%2520by%2520adding%2520a%250Asingle%2520relay%2520pass%2520through%2520the%2520same%2520spatial%2520light%2520modulator%253A%2520a%2520folded%25204f%2520relay%250Are-images%2520the%2520first%2520Fourier%2520plane%2520onto%2520the%2520SLM%2520so%2520that%2520each%2520selected%2520clique%2520or%250Achannel%2520occupies%2520a%2520disjoint%2520window%2520with%2520its%2520own%2520second%2520pass%2520phase%2520patch.%250APropagation%2520remains%2520linear%2520in%2520the%2520optical%2520field%252C%2520yet%2520the%2520measured%2520intensity%2520in%250Aeach%2520window%2520becomes%2520a%2520freely%2520programmable%2520polynomial%2520of%2520the%2520clique%2520sum%2520or%250Aprojection%2520amplitude.%2520This%2520yields%2520native%252C%2520per%2520clique%2520k-local%2520couplings%2520without%250Anonlinear%2520media%2520and%252C%2520in%2520parallel%252C%2520the%2520many%2520independent%2520univariate%250Anonlinearities%2520required%2520by%2520KAN%2520layers%252C%2520all%2520trainable%2520with%2520in-situ%2520physical%250Agradients%2520using%2520two%2520frames%2520%2528forward%2520and%2520adjoint%2529.%2520We%2520outline%2520implementations%2520on%250Aspatial%2520photonic%2520Ising%2520machines%252C%2520injection-locked%2520vertical%2520cavity%2520surface%250Aemitting%2520laser%2520%2528VCSEL%2529%2520arrays%252C%2520and%2520Microsoft%2520analog%2520optical%2520computers%253B%2520in%2520all%250Acases%2520the%2520hardware%2520change%2520is%2520one%2520extra%2520lens%2520and%2520a%2520fold%2520%2528or%2520an%2520on-chip%25204f%2520loop%2529%252C%250Aenabling%2520a%2520minimal%2520overhead%252C%2520massively%2520parallel%2520route%2520to%2520high-order%2520Ising%250Aoptimization%2520and%2520trainable%252C%2520all-optical%2520KAN%2520processing%2520on%2520one%2520platform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17440v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Programmable%20k-local%20Ising%20Machines%20and%20all-optical%20Kolmogorov-Arnold%0A%20%20Networks%20on%20Photonic%20Platforms&entry.906535625=Nikita%20Stroev%20and%20Natalia%20G.%20Berloff&entry.1292438233=%20%20Photonic%20computing%20promises%20energy-efficient%20acceleration%20for%20optimization%0Aand%20learning%2C%20yet%20discrete%20combinatorial%20search%20and%20continuous%20function%0Aapproximation%20have%20largely%20required%20distinct%20devices%20and%20control%20stacks.%20Here%0Awe%20unify%20k-local%20Ising%20optimization%20and%20optical%20Kolmogorov-Arnold%20network%20%28KAN%29%0Alearning%20on%20a%20single%20photonic%20platform%2C%20establishing%20a%20critical%20convergence%0Apoint%20in%20optical%20computing.%20We%20introduce%20an%20SLM-centric%20primitive%20that%0Arealizes%2C%20in%20one%20stroke%2C%20all-optical%20k-local%20Ising%20interactions%20and%20fully%0Aoptical%20KAN%20layers.%20The%20key%20idea%20is%20to%20convert%20the%20structural%20nonlinearity%20of%20a%0Anominally%20linear%20scatterer%20into%20a%20per-window%20computational%20resource%20by%20adding%20a%0Asingle%20relay%20pass%20through%20the%20same%20spatial%20light%20modulator%3A%20a%20folded%204f%20relay%0Are-images%20the%20first%20Fourier%20plane%20onto%20the%20SLM%20so%20that%20each%20selected%20clique%20or%0Achannel%20occupies%20a%20disjoint%20window%20with%20its%20own%20second%20pass%20phase%20patch.%0APropagation%20remains%20linear%20in%20the%20optical%20field%2C%20yet%20the%20measured%20intensity%20in%0Aeach%20window%20becomes%20a%20freely%20programmable%20polynomial%20of%20the%20clique%20sum%20or%0Aprojection%20amplitude.%20This%20yields%20native%2C%20per%20clique%20k-local%20couplings%20without%0Anonlinear%20media%20and%2C%20in%20parallel%2C%20the%20many%20independent%20univariate%0Anonlinearities%20required%20by%20KAN%20layers%2C%20all%20trainable%20with%20in-situ%20physical%0Agradients%20using%20two%20frames%20%28forward%20and%20adjoint%29.%20We%20outline%20implementations%20on%0Aspatial%20photonic%20Ising%20machines%2C%20injection-locked%20vertical%20cavity%20surface%0Aemitting%20laser%20%28VCSEL%29%20arrays%2C%20and%20Microsoft%20analog%20optical%20computers%3B%20in%20all%0Acases%20the%20hardware%20change%20is%20one%20extra%20lens%20and%20a%20fold%20%28or%20an%20on-chip%204f%20loop%29%2C%0Aenabling%20a%20minimal%20overhead%2C%20massively%20parallel%20route%20to%20high-order%20Ising%0Aoptimization%20and%20trainable%2C%20all-optical%20KAN%20processing%20on%20one%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17440v2&entry.124074799=Read"},
{"title": "End to End Autoencoder MLP Framework for Sepsis Prediction", "author": "Hejiang Cai and Di Wu and Ji Xu and Xiang Liu and Yiziting Zhu and Xin Shu and Yujie Li and Bin Yi", "abstract": "  Sepsis is a life threatening condition that requires timely detection in\nintensive care settings. Traditional machine learning approaches, including\nNaive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often\nrely on manual feature engineering and struggle with irregular, incomplete\ntime-series data commonly present in electronic health records. We introduce an\nend-to-end deep learning framework integrating an unsupervised autoencoder for\nautomatic feature extraction with a multilayer perceptron classifier for binary\nsepsis risk prediction. To enhance clinical applicability, we implement a\ncustomized down sampling strategy that extracts high information density\nsegments during training and a non-overlapping dynamic sliding window mechanism\nfor real-time inference. Preprocessed time series data are represented as fixed\ndimension vectors with explicit missingness indicators, mitigating bias and\nnoise. We validate our approach on three ICU cohorts. Our end-to-end model\nachieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,\nrespectively, consistently outperforming traditional machine learning\nbaselines. These results demonstrate the framework's superior robustness,\ngeneralizability, and clinical utility for early sepsis detection across\nheterogeneous ICU environments.\n", "link": "http://arxiv.org/abs/2508.18688v2", "date": "2025-09-02", "relevancy": 1.4102, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4788}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4719}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End%20to%20End%20Autoencoder%20MLP%20Framework%20for%20Sepsis%20Prediction&body=Title%3A%20End%20to%20End%20Autoencoder%20MLP%20Framework%20for%20Sepsis%20Prediction%0AAuthor%3A%20Hejiang%20Cai%20and%20Di%20Wu%20and%20Ji%20Xu%20and%20Xiang%20Liu%20and%20Yiziting%20Zhu%20and%20Xin%20Shu%20and%20Yujie%20Li%20and%20Bin%20Yi%0AAbstract%3A%20%20%20Sepsis%20is%20a%20life%20threatening%20condition%20that%20requires%20timely%20detection%20in%0Aintensive%20care%20settings.%20Traditional%20machine%20learning%20approaches%2C%20including%0ANaive%20Bayes%2C%20Support%20Vector%20Machine%20%28SVM%29%2C%20Random%20Forest%2C%20and%20XGBoost%2C%20often%0Arely%20on%20manual%20feature%20engineering%20and%20struggle%20with%20irregular%2C%20incomplete%0Atime-series%20data%20commonly%20present%20in%20electronic%20health%20records.%20We%20introduce%20an%0Aend-to-end%20deep%20learning%20framework%20integrating%20an%20unsupervised%20autoencoder%20for%0Aautomatic%20feature%20extraction%20with%20a%20multilayer%20perceptron%20classifier%20for%20binary%0Asepsis%20risk%20prediction.%20To%20enhance%20clinical%20applicability%2C%20we%20implement%20a%0Acustomized%20down%20sampling%20strategy%20that%20extracts%20high%20information%20density%0Asegments%20during%20training%20and%20a%20non-overlapping%20dynamic%20sliding%20window%20mechanism%0Afor%20real-time%20inference.%20Preprocessed%20time%20series%20data%20are%20represented%20as%20fixed%0Adimension%20vectors%20with%20explicit%20missingness%20indicators%2C%20mitigating%20bias%20and%0Anoise.%20We%20validate%20our%20approach%20on%20three%20ICU%20cohorts.%20Our%20end-to-end%20model%0Aachieves%20accuracies%20of%2074.6%20percent%2C%2080.6%20percent%2C%20and%2093.5%20percent%2C%0Arespectively%2C%20consistently%20outperforming%20traditional%20machine%20learning%0Abaselines.%20These%20results%20demonstrate%20the%20framework%27s%20superior%20robustness%2C%0Ageneralizability%2C%20and%20clinical%20utility%20for%20early%20sepsis%20detection%20across%0Aheterogeneous%20ICU%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18688v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd%2520to%2520End%2520Autoencoder%2520MLP%2520Framework%2520for%2520Sepsis%2520Prediction%26entry.906535625%3DHejiang%2520Cai%2520and%2520Di%2520Wu%2520and%2520Ji%2520Xu%2520and%2520Xiang%2520Liu%2520and%2520Yiziting%2520Zhu%2520and%2520Xin%2520Shu%2520and%2520Yujie%2520Li%2520and%2520Bin%2520Yi%26entry.1292438233%3D%2520%2520Sepsis%2520is%2520a%2520life%2520threatening%2520condition%2520that%2520requires%2520timely%2520detection%2520in%250Aintensive%2520care%2520settings.%2520Traditional%2520machine%2520learning%2520approaches%252C%2520including%250ANaive%2520Bayes%252C%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%252C%2520Random%2520Forest%252C%2520and%2520XGBoost%252C%2520often%250Arely%2520on%2520manual%2520feature%2520engineering%2520and%2520struggle%2520with%2520irregular%252C%2520incomplete%250Atime-series%2520data%2520commonly%2520present%2520in%2520electronic%2520health%2520records.%2520We%2520introduce%2520an%250Aend-to-end%2520deep%2520learning%2520framework%2520integrating%2520an%2520unsupervised%2520autoencoder%2520for%250Aautomatic%2520feature%2520extraction%2520with%2520a%2520multilayer%2520perceptron%2520classifier%2520for%2520binary%250Asepsis%2520risk%2520prediction.%2520To%2520enhance%2520clinical%2520applicability%252C%2520we%2520implement%2520a%250Acustomized%2520down%2520sampling%2520strategy%2520that%2520extracts%2520high%2520information%2520density%250Asegments%2520during%2520training%2520and%2520a%2520non-overlapping%2520dynamic%2520sliding%2520window%2520mechanism%250Afor%2520real-time%2520inference.%2520Preprocessed%2520time%2520series%2520data%2520are%2520represented%2520as%2520fixed%250Adimension%2520vectors%2520with%2520explicit%2520missingness%2520indicators%252C%2520mitigating%2520bias%2520and%250Anoise.%2520We%2520validate%2520our%2520approach%2520on%2520three%2520ICU%2520cohorts.%2520Our%2520end-to-end%2520model%250Aachieves%2520accuracies%2520of%252074.6%2520percent%252C%252080.6%2520percent%252C%2520and%252093.5%2520percent%252C%250Arespectively%252C%2520consistently%2520outperforming%2520traditional%2520machine%2520learning%250Abaselines.%2520These%2520results%2520demonstrate%2520the%2520framework%2527s%2520superior%2520robustness%252C%250Ageneralizability%252C%2520and%2520clinical%2520utility%2520for%2520early%2520sepsis%2520detection%2520across%250Aheterogeneous%2520ICU%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18688v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End%20to%20End%20Autoencoder%20MLP%20Framework%20for%20Sepsis%20Prediction&entry.906535625=Hejiang%20Cai%20and%20Di%20Wu%20and%20Ji%20Xu%20and%20Xiang%20Liu%20and%20Yiziting%20Zhu%20and%20Xin%20Shu%20and%20Yujie%20Li%20and%20Bin%20Yi&entry.1292438233=%20%20Sepsis%20is%20a%20life%20threatening%20condition%20that%20requires%20timely%20detection%20in%0Aintensive%20care%20settings.%20Traditional%20machine%20learning%20approaches%2C%20including%0ANaive%20Bayes%2C%20Support%20Vector%20Machine%20%28SVM%29%2C%20Random%20Forest%2C%20and%20XGBoost%2C%20often%0Arely%20on%20manual%20feature%20engineering%20and%20struggle%20with%20irregular%2C%20incomplete%0Atime-series%20data%20commonly%20present%20in%20electronic%20health%20records.%20We%20introduce%20an%0Aend-to-end%20deep%20learning%20framework%20integrating%20an%20unsupervised%20autoencoder%20for%0Aautomatic%20feature%20extraction%20with%20a%20multilayer%20perceptron%20classifier%20for%20binary%0Asepsis%20risk%20prediction.%20To%20enhance%20clinical%20applicability%2C%20we%20implement%20a%0Acustomized%20down%20sampling%20strategy%20that%20extracts%20high%20information%20density%0Asegments%20during%20training%20and%20a%20non-overlapping%20dynamic%20sliding%20window%20mechanism%0Afor%20real-time%20inference.%20Preprocessed%20time%20series%20data%20are%20represented%20as%20fixed%0Adimension%20vectors%20with%20explicit%20missingness%20indicators%2C%20mitigating%20bias%20and%0Anoise.%20We%20validate%20our%20approach%20on%20three%20ICU%20cohorts.%20Our%20end-to-end%20model%0Aachieves%20accuracies%20of%2074.6%20percent%2C%2080.6%20percent%2C%20and%2093.5%20percent%2C%0Arespectively%2C%20consistently%20outperforming%20traditional%20machine%20learning%0Abaselines.%20These%20results%20demonstrate%20the%20framework%27s%20superior%20robustness%2C%0Ageneralizability%2C%20and%20clinical%20utility%20for%20early%20sepsis%20detection%20across%0Aheterogeneous%20ICU%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18688v2&entry.124074799=Read"},
{"title": "SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts\n  outperforming numerical weather prediction across the USA", "author": "Mingliang Bai and Zuliang Fang and Shengyu Tao and Siqi Xiang and Jiang Bian and Yanfei Xiang and Pengcheng Zhao and Weixin Jin and Jonathan A. Weyn and Haiyu Dong and Bin Zhang and Hongyu Sun and Kit Thambiratnam and Qi Zhang and Hongbin Sun and Xuan Zhang and Qiuwei Wu", "abstract": "  Accurate 24-hour solar irradiance forecasting is essential for the safe and\neconomic operation of solar photovoltaic systems. Traditional numerical weather\nprediction (NWP) models represent the state-of-the-art in forecasting\nperformance but rely on computationally costly data assimilation and solving\ncomplicated partial differential equations (PDEs) that simulate atmospheric\nphysics. Here, we introduce SolarSeer, an end-to-end large artificial\nintelligence (AI) model for solar irradiance forecasting across the Contiguous\nUnited States (CONUS). SolarSeer is designed to directly map the historical\nsatellite observations to future forecasts, eliminating the computational\noverhead of data assimilation and PDEs solving. This efficiency allows\nSolarSeer to operate over 1,500 times faster than traditional NWP, generating\n24-hour cloud cover and solar irradiance forecasts for the CONUS at 5-kilometer\nresolution in under 3 seconds. Compared with the state-of-the-art NWP in the\nCONUS, i.e., High-Resolution Rapid Refresh (HRRR), SolarSeer significantly\nreduces the root mean squared error of solar irradiance forecasting by 27.28%\nin reanalysis data and 15.35% across 1,800 stations. SolarSeer also effectively\ncaptures solar irradiance fluctuations and significantly enhances the\nfirst-order irradiance difference forecasting accuracy. SolarSeer's ultrafast,\naccurate 24-hour solar irradiance forecasts provide strong support for the\ntransition to sustainable, net-zero energy systems.\n", "link": "http://arxiv.org/abs/2508.03590v2", "date": "2025-09-02", "relevancy": 1.1736, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4099}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SolarSeer%3A%20Ultrafast%20and%20accurate%2024-hour%20solar%20irradiance%20forecasts%0A%20%20outperforming%20numerical%20weather%20prediction%20across%20the%20USA&body=Title%3A%20SolarSeer%3A%20Ultrafast%20and%20accurate%2024-hour%20solar%20irradiance%20forecasts%0A%20%20outperforming%20numerical%20weather%20prediction%20across%20the%20USA%0AAuthor%3A%20Mingliang%20Bai%20and%20Zuliang%20Fang%20and%20Shengyu%20Tao%20and%20Siqi%20Xiang%20and%20Jiang%20Bian%20and%20Yanfei%20Xiang%20and%20Pengcheng%20Zhao%20and%20Weixin%20Jin%20and%20Jonathan%20A.%20Weyn%20and%20Haiyu%20Dong%20and%20Bin%20Zhang%20and%20Hongyu%20Sun%20and%20Kit%20Thambiratnam%20and%20Qi%20Zhang%20and%20Hongbin%20Sun%20and%20Xuan%20Zhang%20and%20Qiuwei%20Wu%0AAbstract%3A%20%20%20Accurate%2024-hour%20solar%20irradiance%20forecasting%20is%20essential%20for%20the%20safe%20and%0Aeconomic%20operation%20of%20solar%20photovoltaic%20systems.%20Traditional%20numerical%20weather%0Aprediction%20%28NWP%29%20models%20represent%20the%20state-of-the-art%20in%20forecasting%0Aperformance%20but%20rely%20on%20computationally%20costly%20data%20assimilation%20and%20solving%0Acomplicated%20partial%20differential%20equations%20%28PDEs%29%20that%20simulate%20atmospheric%0Aphysics.%20Here%2C%20we%20introduce%20SolarSeer%2C%20an%20end-to-end%20large%20artificial%0Aintelligence%20%28AI%29%20model%20for%20solar%20irradiance%20forecasting%20across%20the%20Contiguous%0AUnited%20States%20%28CONUS%29.%20SolarSeer%20is%20designed%20to%20directly%20map%20the%20historical%0Asatellite%20observations%20to%20future%20forecasts%2C%20eliminating%20the%20computational%0Aoverhead%20of%20data%20assimilation%20and%20PDEs%20solving.%20This%20efficiency%20allows%0ASolarSeer%20to%20operate%20over%201%2C500%20times%20faster%20than%20traditional%20NWP%2C%20generating%0A24-hour%20cloud%20cover%20and%20solar%20irradiance%20forecasts%20for%20the%20CONUS%20at%205-kilometer%0Aresolution%20in%20under%203%20seconds.%20Compared%20with%20the%20state-of-the-art%20NWP%20in%20the%0ACONUS%2C%20i.e.%2C%20High-Resolution%20Rapid%20Refresh%20%28HRRR%29%2C%20SolarSeer%20significantly%0Areduces%20the%20root%20mean%20squared%20error%20of%20solar%20irradiance%20forecasting%20by%2027.28%25%0Ain%20reanalysis%20data%20and%2015.35%25%20across%201%2C800%20stations.%20SolarSeer%20also%20effectively%0Acaptures%20solar%20irradiance%20fluctuations%20and%20significantly%20enhances%20the%0Afirst-order%20irradiance%20difference%20forecasting%20accuracy.%20SolarSeer%27s%20ultrafast%2C%0Aaccurate%2024-hour%20solar%20irradiance%20forecasts%20provide%20strong%20support%20for%20the%0Atransition%20to%20sustainable%2C%20net-zero%20energy%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03590v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolarSeer%253A%2520Ultrafast%2520and%2520accurate%252024-hour%2520solar%2520irradiance%2520forecasts%250A%2520%2520outperforming%2520numerical%2520weather%2520prediction%2520across%2520the%2520USA%26entry.906535625%3DMingliang%2520Bai%2520and%2520Zuliang%2520Fang%2520and%2520Shengyu%2520Tao%2520and%2520Siqi%2520Xiang%2520and%2520Jiang%2520Bian%2520and%2520Yanfei%2520Xiang%2520and%2520Pengcheng%2520Zhao%2520and%2520Weixin%2520Jin%2520and%2520Jonathan%2520A.%2520Weyn%2520and%2520Haiyu%2520Dong%2520and%2520Bin%2520Zhang%2520and%2520Hongyu%2520Sun%2520and%2520Kit%2520Thambiratnam%2520and%2520Qi%2520Zhang%2520and%2520Hongbin%2520Sun%2520and%2520Xuan%2520Zhang%2520and%2520Qiuwei%2520Wu%26entry.1292438233%3D%2520%2520Accurate%252024-hour%2520solar%2520irradiance%2520forecasting%2520is%2520essential%2520for%2520the%2520safe%2520and%250Aeconomic%2520operation%2520of%2520solar%2520photovoltaic%2520systems.%2520Traditional%2520numerical%2520weather%250Aprediction%2520%2528NWP%2529%2520models%2520represent%2520the%2520state-of-the-art%2520in%2520forecasting%250Aperformance%2520but%2520rely%2520on%2520computationally%2520costly%2520data%2520assimilation%2520and%2520solving%250Acomplicated%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520that%2520simulate%2520atmospheric%250Aphysics.%2520Here%252C%2520we%2520introduce%2520SolarSeer%252C%2520an%2520end-to-end%2520large%2520artificial%250Aintelligence%2520%2528AI%2529%2520model%2520for%2520solar%2520irradiance%2520forecasting%2520across%2520the%2520Contiguous%250AUnited%2520States%2520%2528CONUS%2529.%2520SolarSeer%2520is%2520designed%2520to%2520directly%2520map%2520the%2520historical%250Asatellite%2520observations%2520to%2520future%2520forecasts%252C%2520eliminating%2520the%2520computational%250Aoverhead%2520of%2520data%2520assimilation%2520and%2520PDEs%2520solving.%2520This%2520efficiency%2520allows%250ASolarSeer%2520to%2520operate%2520over%25201%252C500%2520times%2520faster%2520than%2520traditional%2520NWP%252C%2520generating%250A24-hour%2520cloud%2520cover%2520and%2520solar%2520irradiance%2520forecasts%2520for%2520the%2520CONUS%2520at%25205-kilometer%250Aresolution%2520in%2520under%25203%2520seconds.%2520Compared%2520with%2520the%2520state-of-the-art%2520NWP%2520in%2520the%250ACONUS%252C%2520i.e.%252C%2520High-Resolution%2520Rapid%2520Refresh%2520%2528HRRR%2529%252C%2520SolarSeer%2520significantly%250Areduces%2520the%2520root%2520mean%2520squared%2520error%2520of%2520solar%2520irradiance%2520forecasting%2520by%252027.28%2525%250Ain%2520reanalysis%2520data%2520and%252015.35%2525%2520across%25201%252C800%2520stations.%2520SolarSeer%2520also%2520effectively%250Acaptures%2520solar%2520irradiance%2520fluctuations%2520and%2520significantly%2520enhances%2520the%250Afirst-order%2520irradiance%2520difference%2520forecasting%2520accuracy.%2520SolarSeer%2527s%2520ultrafast%252C%250Aaccurate%252024-hour%2520solar%2520irradiance%2520forecasts%2520provide%2520strong%2520support%2520for%2520the%250Atransition%2520to%2520sustainable%252C%2520net-zero%2520energy%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03590v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SolarSeer%3A%20Ultrafast%20and%20accurate%2024-hour%20solar%20irradiance%20forecasts%0A%20%20outperforming%20numerical%20weather%20prediction%20across%20the%20USA&entry.906535625=Mingliang%20Bai%20and%20Zuliang%20Fang%20and%20Shengyu%20Tao%20and%20Siqi%20Xiang%20and%20Jiang%20Bian%20and%20Yanfei%20Xiang%20and%20Pengcheng%20Zhao%20and%20Weixin%20Jin%20and%20Jonathan%20A.%20Weyn%20and%20Haiyu%20Dong%20and%20Bin%20Zhang%20and%20Hongyu%20Sun%20and%20Kit%20Thambiratnam%20and%20Qi%20Zhang%20and%20Hongbin%20Sun%20and%20Xuan%20Zhang%20and%20Qiuwei%20Wu&entry.1292438233=%20%20Accurate%2024-hour%20solar%20irradiance%20forecasting%20is%20essential%20for%20the%20safe%20and%0Aeconomic%20operation%20of%20solar%20photovoltaic%20systems.%20Traditional%20numerical%20weather%0Aprediction%20%28NWP%29%20models%20represent%20the%20state-of-the-art%20in%20forecasting%0Aperformance%20but%20rely%20on%20computationally%20costly%20data%20assimilation%20and%20solving%0Acomplicated%20partial%20differential%20equations%20%28PDEs%29%20that%20simulate%20atmospheric%0Aphysics.%20Here%2C%20we%20introduce%20SolarSeer%2C%20an%20end-to-end%20large%20artificial%0Aintelligence%20%28AI%29%20model%20for%20solar%20irradiance%20forecasting%20across%20the%20Contiguous%0AUnited%20States%20%28CONUS%29.%20SolarSeer%20is%20designed%20to%20directly%20map%20the%20historical%0Asatellite%20observations%20to%20future%20forecasts%2C%20eliminating%20the%20computational%0Aoverhead%20of%20data%20assimilation%20and%20PDEs%20solving.%20This%20efficiency%20allows%0ASolarSeer%20to%20operate%20over%201%2C500%20times%20faster%20than%20traditional%20NWP%2C%20generating%0A24-hour%20cloud%20cover%20and%20solar%20irradiance%20forecasts%20for%20the%20CONUS%20at%205-kilometer%0Aresolution%20in%20under%203%20seconds.%20Compared%20with%20the%20state-of-the-art%20NWP%20in%20the%0ACONUS%2C%20i.e.%2C%20High-Resolution%20Rapid%20Refresh%20%28HRRR%29%2C%20SolarSeer%20significantly%0Areduces%20the%20root%20mean%20squared%20error%20of%20solar%20irradiance%20forecasting%20by%2027.28%25%0Ain%20reanalysis%20data%20and%2015.35%25%20across%201%2C800%20stations.%20SolarSeer%20also%20effectively%0Acaptures%20solar%20irradiance%20fluctuations%20and%20significantly%20enhances%20the%0Afirst-order%20irradiance%20difference%20forecasting%20accuracy.%20SolarSeer%27s%20ultrafast%2C%0Aaccurate%2024-hour%20solar%20irradiance%20forecasts%20provide%20strong%20support%20for%20the%0Atransition%20to%20sustainable%2C%20net-zero%20energy%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03590v2&entry.124074799=Read"},
{"title": "Identifying Macro Causal Effects in a C-DMG over ADMGs", "author": "Simon Ferreira and Charles K. Assaad", "abstract": "  Causal effect identification using causal graphs is a fundamental challenge\nin causal inference. While extensive research has been conducted in this area,\nmost existing methods assume the availability of fully specified directed\nacyclic graphs or acyclic directed mixed graphs. However, in complex domains\nsuch as medicine and epidemiology, complete causal knowledge is often\nunavailable, and only partial information about the system is accessible. This\npaper focuses on causal effect identification within partially specified causal\ngraphs, with particular emphasis on cluster-directed mixed graphs (C-DMGs)\nwhich can represent many different acyclic directed mixed graphs (ADMGs). These\ngraphs provide a higher-level representation of causal relationships by\ngrouping variables into clusters, offering a more practical approach for\nhandling complex systems. Unlike fully specified ADMGs, C-DMGs can contain\ncycles, which complicate their analysis and interpretation. Furthermore, their\ncluster-based nature introduces new challenges, as it gives rise to two\ndistinct types of causal effects: macro causal effects and micro causal\neffects, each with different properties. In this work, we focus on macro causal\neffects, which describe the effects of entire clusters on other clusters. We\nestablish that the do-calculus is both sound and complete for identifying these\neffects in C-DMGs over ADMGs when the cluster sizes are either unknown or of\nsize greater than one. Additionally, we provide a graphical characterization of\nnon-identifiability for macro causal effects in these graphs.\n", "link": "http://arxiv.org/abs/2504.01551v2", "date": "2025-09-02", "relevancy": 1.1685, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.396}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3824}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Macro%20Causal%20Effects%20in%20a%20C-DMG%20over%20ADMGs&body=Title%3A%20Identifying%20Macro%20Causal%20Effects%20in%20a%20C-DMG%20over%20ADMGs%0AAuthor%3A%20Simon%20Ferreira%20and%20Charles%20K.%20Assaad%0AAbstract%3A%20%20%20Causal%20effect%20identification%20using%20causal%20graphs%20is%20a%20fundamental%20challenge%0Ain%20causal%20inference.%20While%20extensive%20research%20has%20been%20conducted%20in%20this%20area%2C%0Amost%20existing%20methods%20assume%20the%20availability%20of%20fully%20specified%20directed%0Aacyclic%20graphs%20or%20acyclic%20directed%20mixed%20graphs.%20However%2C%20in%20complex%20domains%0Asuch%20as%20medicine%20and%20epidemiology%2C%20complete%20causal%20knowledge%20is%20often%0Aunavailable%2C%20and%20only%20partial%20information%20about%20the%20system%20is%20accessible.%20This%0Apaper%20focuses%20on%20causal%20effect%20identification%20within%20partially%20specified%20causal%0Agraphs%2C%20with%20particular%20emphasis%20on%20cluster-directed%20mixed%20graphs%20%28C-DMGs%29%0Awhich%20can%20represent%20many%20different%20acyclic%20directed%20mixed%20graphs%20%28ADMGs%29.%20These%0Agraphs%20provide%20a%20higher-level%20representation%20of%20causal%20relationships%20by%0Agrouping%20variables%20into%20clusters%2C%20offering%20a%20more%20practical%20approach%20for%0Ahandling%20complex%20systems.%20Unlike%20fully%20specified%20ADMGs%2C%20C-DMGs%20can%20contain%0Acycles%2C%20which%20complicate%20their%20analysis%20and%20interpretation.%20Furthermore%2C%20their%0Acluster-based%20nature%20introduces%20new%20challenges%2C%20as%20it%20gives%20rise%20to%20two%0Adistinct%20types%20of%20causal%20effects%3A%20macro%20causal%20effects%20and%20micro%20causal%0Aeffects%2C%20each%20with%20different%20properties.%20In%20this%20work%2C%20we%20focus%20on%20macro%20causal%0Aeffects%2C%20which%20describe%20the%20effects%20of%20entire%20clusters%20on%20other%20clusters.%20We%0Aestablish%20that%20the%20do-calculus%20is%20both%20sound%20and%20complete%20for%20identifying%20these%0Aeffects%20in%20C-DMGs%20over%20ADMGs%20when%20the%20cluster%20sizes%20are%20either%20unknown%20or%20of%0Asize%20greater%20than%20one.%20Additionally%2C%20we%20provide%20a%20graphical%20characterization%20of%0Anon-identifiability%20for%20macro%20causal%20effects%20in%20these%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Macro%2520Causal%2520Effects%2520in%2520a%2520C-DMG%2520over%2520ADMGs%26entry.906535625%3DSimon%2520Ferreira%2520and%2520Charles%2520K.%2520Assaad%26entry.1292438233%3D%2520%2520Causal%2520effect%2520identification%2520using%2520causal%2520graphs%2520is%2520a%2520fundamental%2520challenge%250Ain%2520causal%2520inference.%2520While%2520extensive%2520research%2520has%2520been%2520conducted%2520in%2520this%2520area%252C%250Amost%2520existing%2520methods%2520assume%2520the%2520availability%2520of%2520fully%2520specified%2520directed%250Aacyclic%2520graphs%2520or%2520acyclic%2520directed%2520mixed%2520graphs.%2520However%252C%2520in%2520complex%2520domains%250Asuch%2520as%2520medicine%2520and%2520epidemiology%252C%2520complete%2520causal%2520knowledge%2520is%2520often%250Aunavailable%252C%2520and%2520only%2520partial%2520information%2520about%2520the%2520system%2520is%2520accessible.%2520This%250Apaper%2520focuses%2520on%2520causal%2520effect%2520identification%2520within%2520partially%2520specified%2520causal%250Agraphs%252C%2520with%2520particular%2520emphasis%2520on%2520cluster-directed%2520mixed%2520graphs%2520%2528C-DMGs%2529%250Awhich%2520can%2520represent%2520many%2520different%2520acyclic%2520directed%2520mixed%2520graphs%2520%2528ADMGs%2529.%2520These%250Agraphs%2520provide%2520a%2520higher-level%2520representation%2520of%2520causal%2520relationships%2520by%250Agrouping%2520variables%2520into%2520clusters%252C%2520offering%2520a%2520more%2520practical%2520approach%2520for%250Ahandling%2520complex%2520systems.%2520Unlike%2520fully%2520specified%2520ADMGs%252C%2520C-DMGs%2520can%2520contain%250Acycles%252C%2520which%2520complicate%2520their%2520analysis%2520and%2520interpretation.%2520Furthermore%252C%2520their%250Acluster-based%2520nature%2520introduces%2520new%2520challenges%252C%2520as%2520it%2520gives%2520rise%2520to%2520two%250Adistinct%2520types%2520of%2520causal%2520effects%253A%2520macro%2520causal%2520effects%2520and%2520micro%2520causal%250Aeffects%252C%2520each%2520with%2520different%2520properties.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520macro%2520causal%250Aeffects%252C%2520which%2520describe%2520the%2520effects%2520of%2520entire%2520clusters%2520on%2520other%2520clusters.%2520We%250Aestablish%2520that%2520the%2520do-calculus%2520is%2520both%2520sound%2520and%2520complete%2520for%2520identifying%2520these%250Aeffects%2520in%2520C-DMGs%2520over%2520ADMGs%2520when%2520the%2520cluster%2520sizes%2520are%2520either%2520unknown%2520or%2520of%250Asize%2520greater%2520than%2520one.%2520Additionally%252C%2520we%2520provide%2520a%2520graphical%2520characterization%2520of%250Anon-identifiability%2520for%2520macro%2520causal%2520effects%2520in%2520these%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Macro%20Causal%20Effects%20in%20a%20C-DMG%20over%20ADMGs&entry.906535625=Simon%20Ferreira%20and%20Charles%20K.%20Assaad&entry.1292438233=%20%20Causal%20effect%20identification%20using%20causal%20graphs%20is%20a%20fundamental%20challenge%0Ain%20causal%20inference.%20While%20extensive%20research%20has%20been%20conducted%20in%20this%20area%2C%0Amost%20existing%20methods%20assume%20the%20availability%20of%20fully%20specified%20directed%0Aacyclic%20graphs%20or%20acyclic%20directed%20mixed%20graphs.%20However%2C%20in%20complex%20domains%0Asuch%20as%20medicine%20and%20epidemiology%2C%20complete%20causal%20knowledge%20is%20often%0Aunavailable%2C%20and%20only%20partial%20information%20about%20the%20system%20is%20accessible.%20This%0Apaper%20focuses%20on%20causal%20effect%20identification%20within%20partially%20specified%20causal%0Agraphs%2C%20with%20particular%20emphasis%20on%20cluster-directed%20mixed%20graphs%20%28C-DMGs%29%0Awhich%20can%20represent%20many%20different%20acyclic%20directed%20mixed%20graphs%20%28ADMGs%29.%20These%0Agraphs%20provide%20a%20higher-level%20representation%20of%20causal%20relationships%20by%0Agrouping%20variables%20into%20clusters%2C%20offering%20a%20more%20practical%20approach%20for%0Ahandling%20complex%20systems.%20Unlike%20fully%20specified%20ADMGs%2C%20C-DMGs%20can%20contain%0Acycles%2C%20which%20complicate%20their%20analysis%20and%20interpretation.%20Furthermore%2C%20their%0Acluster-based%20nature%20introduces%20new%20challenges%2C%20as%20it%20gives%20rise%20to%20two%0Adistinct%20types%20of%20causal%20effects%3A%20macro%20causal%20effects%20and%20micro%20causal%0Aeffects%2C%20each%20with%20different%20properties.%20In%20this%20work%2C%20we%20focus%20on%20macro%20causal%0Aeffects%2C%20which%20describe%20the%20effects%20of%20entire%20clusters%20on%20other%20clusters.%20We%0Aestablish%20that%20the%20do-calculus%20is%20both%20sound%20and%20complete%20for%20identifying%20these%0Aeffects%20in%20C-DMGs%20over%20ADMGs%20when%20the%20cluster%20sizes%20are%20either%20unknown%20or%20of%0Asize%20greater%20than%20one.%20Additionally%2C%20we%20provide%20a%20graphical%20characterization%20of%0Anon-identifiability%20for%20macro%20causal%20effects%20in%20these%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01551v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


