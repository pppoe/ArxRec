<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250513.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting", "author": "Zhihao Guo and Jingxuan Su and Shenglin Wang and Jinlong Fan and Jing Zhang and Wei Zhou and Hadi Amirpour and Yunlong Zhao and Liangxiu Han and Peng Wang", "abstract": "  3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds often limits scene reconstruction quality. To address the\nlimitation, this paper proposes a novel 3D reconstruction framework, Gaussian\nProcesses enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian\nProcess model is developed to enable adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. These densified point clouds\nprovide high-quality initial 3D Gaussians, enhancing reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework.\n", "link": "http://arxiv.org/abs/2502.02283v5", "date": "2025-05-13", "relevancy": 3.5893, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7507}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7229}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.68}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting&body=Title%3A%20GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting%0AAuthor%3A%20Zhihao%20Guo%20and%20Jingxuan%20Su%20and%20Shenglin%20Wang%20and%20Jinlong%20Fan%20and%20Jing%20Zhang%20and%20Wei%20Zhou%20and%20Hadi%20Amirpour%20and%20Yunlong%20Zhao%20and%20Liangxiu%20Han%20and%20Peng%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20emerged%20as%20an%20efficient%20photorealistic%20novel%20view%0Asynthesis%20method.%20However%2C%20its%20reliance%20on%20sparse%20Structure-from-Motion%20%28SfM%29%0Apoint%20clouds%20often%20limits%20scene%20reconstruction%20quality.%20To%20address%20the%0Alimitation%2C%20this%20paper%20proposes%20a%20novel%203D%20reconstruction%20framework%2C%20Gaussian%0AProcesses%20enhanced%20Gaussian%20Splatting%20%28GP-GS%29%2C%20in%20which%20a%20multi-output%20Gaussian%0AProcess%20model%20is%20developed%20to%20enable%20adaptive%20and%20uncertainty-guided%0Adensification%20of%20sparse%20SfM%20point%20clouds.%20Specifically%2C%20we%20propose%20a%20dynamic%0Asampling%20and%20filtering%20pipeline%20that%20adaptively%20expands%20the%20SfM%20point%20clouds%20by%0Aleveraging%20GP-based%20predictions%20to%20infer%20new%20candidate%20points%20from%20the%20input%202D%0Apixels%20and%20depth%20maps.%20The%20pipeline%20utilizes%20uncertainty%20estimates%20to%20guide%20the%0Apruning%20of%20high-variance%20predictions%2C%20ensuring%20geometric%20consistency%20and%0Aenabling%20the%20generation%20of%20dense%20point%20clouds.%20These%20densified%20point%20clouds%0Aprovide%20high-quality%20initial%203D%20Gaussians%2C%20enhancing%20reconstruction%0Aperformance.%20Extensive%20experiments%20conducted%20on%20synthetic%20and%20real-world%0Adatasets%20across%20various%20scales%20validate%20the%20effectiveness%20and%20practicality%20of%0Athe%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02283v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGP-GS%253A%2520Gaussian%2520Processes%2520for%2520Enhanced%2520Gaussian%2520Splatting%26entry.906535625%3DZhihao%2520Guo%2520and%2520Jingxuan%2520Su%2520and%2520Shenglin%2520Wang%2520and%2520Jinlong%2520Fan%2520and%2520Jing%2520Zhang%2520and%2520Wei%2520Zhou%2520and%2520Hadi%2520Amirpour%2520and%2520Yunlong%2520Zhao%2520and%2520Liangxiu%2520Han%2520and%2520Peng%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520emerged%2520as%2520an%2520efficient%2520photorealistic%2520novel%2520view%250Asynthesis%2520method.%2520However%252C%2520its%2520reliance%2520on%2520sparse%2520Structure-from-Motion%2520%2528SfM%2529%250Apoint%2520clouds%2520often%2520limits%2520scene%2520reconstruction%2520quality.%2520To%2520address%2520the%250Alimitation%252C%2520this%2520paper%2520proposes%2520a%2520novel%25203D%2520reconstruction%2520framework%252C%2520Gaussian%250AProcesses%2520enhanced%2520Gaussian%2520Splatting%2520%2528GP-GS%2529%252C%2520in%2520which%2520a%2520multi-output%2520Gaussian%250AProcess%2520model%2520is%2520developed%2520to%2520enable%2520adaptive%2520and%2520uncertainty-guided%250Adensification%2520of%2520sparse%2520SfM%2520point%2520clouds.%2520Specifically%252C%2520we%2520propose%2520a%2520dynamic%250Asampling%2520and%2520filtering%2520pipeline%2520that%2520adaptively%2520expands%2520the%2520SfM%2520point%2520clouds%2520by%250Aleveraging%2520GP-based%2520predictions%2520to%2520infer%2520new%2520candidate%2520points%2520from%2520the%2520input%25202D%250Apixels%2520and%2520depth%2520maps.%2520The%2520pipeline%2520utilizes%2520uncertainty%2520estimates%2520to%2520guide%2520the%250Apruning%2520of%2520high-variance%2520predictions%252C%2520ensuring%2520geometric%2520consistency%2520and%250Aenabling%2520the%2520generation%2520of%2520dense%2520point%2520clouds.%2520These%2520densified%2520point%2520clouds%250Aprovide%2520high-quality%2520initial%25203D%2520Gaussians%252C%2520enhancing%2520reconstruction%250Aperformance.%2520Extensive%2520experiments%2520conducted%2520on%2520synthetic%2520and%2520real-world%250Adatasets%2520across%2520various%2520scales%2520validate%2520the%2520effectiveness%2520and%2520practicality%2520of%250Athe%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02283v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting&entry.906535625=Zhihao%20Guo%20and%20Jingxuan%20Su%20and%20Shenglin%20Wang%20and%20Jinlong%20Fan%20and%20Jing%20Zhang%20and%20Wei%20Zhou%20and%20Hadi%20Amirpour%20and%20Yunlong%20Zhao%20and%20Liangxiu%20Han%20and%20Peng%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20emerged%20as%20an%20efficient%20photorealistic%20novel%20view%0Asynthesis%20method.%20However%2C%20its%20reliance%20on%20sparse%20Structure-from-Motion%20%28SfM%29%0Apoint%20clouds%20often%20limits%20scene%20reconstruction%20quality.%20To%20address%20the%0Alimitation%2C%20this%20paper%20proposes%20a%20novel%203D%20reconstruction%20framework%2C%20Gaussian%0AProcesses%20enhanced%20Gaussian%20Splatting%20%28GP-GS%29%2C%20in%20which%20a%20multi-output%20Gaussian%0AProcess%20model%20is%20developed%20to%20enable%20adaptive%20and%20uncertainty-guided%0Adensification%20of%20sparse%20SfM%20point%20clouds.%20Specifically%2C%20we%20propose%20a%20dynamic%0Asampling%20and%20filtering%20pipeline%20that%20adaptively%20expands%20the%20SfM%20point%20clouds%20by%0Aleveraging%20GP-based%20predictions%20to%20infer%20new%20candidate%20points%20from%20the%20input%202D%0Apixels%20and%20depth%20maps.%20The%20pipeline%20utilizes%20uncertainty%20estimates%20to%20guide%20the%0Apruning%20of%20high-variance%20predictions%2C%20ensuring%20geometric%20consistency%20and%0Aenabling%20the%20generation%20of%20dense%20point%20clouds.%20These%20densified%20point%20clouds%0Aprovide%20high-quality%20initial%203D%20Gaussians%2C%20enhancing%20reconstruction%0Aperformance.%20Extensive%20experiments%20conducted%20on%20synthetic%20and%20real-world%0Adatasets%20across%20various%20scales%20validate%20the%20effectiveness%20and%20practicality%20of%0Athe%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02283v5&entry.124074799=Read"},
{"title": "High-Quality Spatial Reconstruction and Orthoimage Generation Using\n  Efficient 2D Gaussian Splatting", "author": "Qian Wang and Zhihao Zhan and Jialei He and Zhituo Tu and Xiang Zhu and Jie Yuan", "abstract": "  Highly accurate geometric precision and dense image features characterize\nTrue Digital Orthophoto Maps (TDOMs), which are in great demand for\napplications such as urban planning, infrastructure management, and\nenvironmental monitoring.Traditional TDOM generation methods need sophisticated\nprocesses, such as Digital Surface Models (DSM) and occlusion detection, which\nare computationally expensive and prone to errors.This work presents an\nalternative technique rooted in 2D Gaussian Splatting (2DGS), free of explicit\nDSM and occlusion detection. With depth map generation, spatial information for\nevery pixel within the TDOM is retrieved and can reconstruct the scene with\nhigh precision. Divide-and-conquer strategy achieves excellent GS training and\nrendering with high-resolution TDOMs at a lower resource cost, which preserves\nhigher quality of rendering on complex terrain and thin structure without a\ndecrease in efficiency. Experimental results demonstrate the efficiency of\nlarge-scale scene reconstruction and high-precision terrain modeling. This\napproach provides accurate spatial data, which assists users in better planning\nand decision-making based on maps.\n", "link": "http://arxiv.org/abs/2503.19703v2", "date": "2025-05-13", "relevancy": 3.2546, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6937}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6398}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Quality%20Spatial%20Reconstruction%20and%20Orthoimage%20Generation%20Using%0A%20%20Efficient%202D%20Gaussian%20Splatting&body=Title%3A%20High-Quality%20Spatial%20Reconstruction%20and%20Orthoimage%20Generation%20Using%0A%20%20Efficient%202D%20Gaussian%20Splatting%0AAuthor%3A%20Qian%20Wang%20and%20Zhihao%20Zhan%20and%20Jialei%20He%20and%20Zhituo%20Tu%20and%20Xiang%20Zhu%20and%20Jie%20Yuan%0AAbstract%3A%20%20%20Highly%20accurate%20geometric%20precision%20and%20dense%20image%20features%20characterize%0ATrue%20Digital%20Orthophoto%20Maps%20%28TDOMs%29%2C%20which%20are%20in%20great%20demand%20for%0Aapplications%20such%20as%20urban%20planning%2C%20infrastructure%20management%2C%20and%0Aenvironmental%20monitoring.Traditional%20TDOM%20generation%20methods%20need%20sophisticated%0Aprocesses%2C%20such%20as%20Digital%20Surface%20Models%20%28DSM%29%20and%20occlusion%20detection%2C%20which%0Aare%20computationally%20expensive%20and%20prone%20to%20errors.This%20work%20presents%20an%0Aalternative%20technique%20rooted%20in%202D%20Gaussian%20Splatting%20%282DGS%29%2C%20free%20of%20explicit%0ADSM%20and%20occlusion%20detection.%20With%20depth%20map%20generation%2C%20spatial%20information%20for%0Aevery%20pixel%20within%20the%20TDOM%20is%20retrieved%20and%20can%20reconstruct%20the%20scene%20with%0Ahigh%20precision.%20Divide-and-conquer%20strategy%20achieves%20excellent%20GS%20training%20and%0Arendering%20with%20high-resolution%20TDOMs%20at%20a%20lower%20resource%20cost%2C%20which%20preserves%0Ahigher%20quality%20of%20rendering%20on%20complex%20terrain%20and%20thin%20structure%20without%20a%0Adecrease%20in%20efficiency.%20Experimental%20results%20demonstrate%20the%20efficiency%20of%0Alarge-scale%20scene%20reconstruction%20and%20high-precision%20terrain%20modeling.%20This%0Aapproach%20provides%20accurate%20spatial%20data%2C%20which%20assists%20users%20in%20better%20planning%0Aand%20decision-making%20based%20on%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Quality%2520Spatial%2520Reconstruction%2520and%2520Orthoimage%2520Generation%2520Using%250A%2520%2520Efficient%25202D%2520Gaussian%2520Splatting%26entry.906535625%3DQian%2520Wang%2520and%2520Zhihao%2520Zhan%2520and%2520Jialei%2520He%2520and%2520Zhituo%2520Tu%2520and%2520Xiang%2520Zhu%2520and%2520Jie%2520Yuan%26entry.1292438233%3D%2520%2520Highly%2520accurate%2520geometric%2520precision%2520and%2520dense%2520image%2520features%2520characterize%250ATrue%2520Digital%2520Orthophoto%2520Maps%2520%2528TDOMs%2529%252C%2520which%2520are%2520in%2520great%2520demand%2520for%250Aapplications%2520such%2520as%2520urban%2520planning%252C%2520infrastructure%2520management%252C%2520and%250Aenvironmental%2520monitoring.Traditional%2520TDOM%2520generation%2520methods%2520need%2520sophisticated%250Aprocesses%252C%2520such%2520as%2520Digital%2520Surface%2520Models%2520%2528DSM%2529%2520and%2520occlusion%2520detection%252C%2520which%250Aare%2520computationally%2520expensive%2520and%2520prone%2520to%2520errors.This%2520work%2520presents%2520an%250Aalternative%2520technique%2520rooted%2520in%25202D%2520Gaussian%2520Splatting%2520%25282DGS%2529%252C%2520free%2520of%2520explicit%250ADSM%2520and%2520occlusion%2520detection.%2520With%2520depth%2520map%2520generation%252C%2520spatial%2520information%2520for%250Aevery%2520pixel%2520within%2520the%2520TDOM%2520is%2520retrieved%2520and%2520can%2520reconstruct%2520the%2520scene%2520with%250Ahigh%2520precision.%2520Divide-and-conquer%2520strategy%2520achieves%2520excellent%2520GS%2520training%2520and%250Arendering%2520with%2520high-resolution%2520TDOMs%2520at%2520a%2520lower%2520resource%2520cost%252C%2520which%2520preserves%250Ahigher%2520quality%2520of%2520rendering%2520on%2520complex%2520terrain%2520and%2520thin%2520structure%2520without%2520a%250Adecrease%2520in%2520efficiency.%2520Experimental%2520results%2520demonstrate%2520the%2520efficiency%2520of%250Alarge-scale%2520scene%2520reconstruction%2520and%2520high-precision%2520terrain%2520modeling.%2520This%250Aapproach%2520provides%2520accurate%2520spatial%2520data%252C%2520which%2520assists%2520users%2520in%2520better%2520planning%250Aand%2520decision-making%2520based%2520on%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Quality%20Spatial%20Reconstruction%20and%20Orthoimage%20Generation%20Using%0A%20%20Efficient%202D%20Gaussian%20Splatting&entry.906535625=Qian%20Wang%20and%20Zhihao%20Zhan%20and%20Jialei%20He%20and%20Zhituo%20Tu%20and%20Xiang%20Zhu%20and%20Jie%20Yuan&entry.1292438233=%20%20Highly%20accurate%20geometric%20precision%20and%20dense%20image%20features%20characterize%0ATrue%20Digital%20Orthophoto%20Maps%20%28TDOMs%29%2C%20which%20are%20in%20great%20demand%20for%0Aapplications%20such%20as%20urban%20planning%2C%20infrastructure%20management%2C%20and%0Aenvironmental%20monitoring.Traditional%20TDOM%20generation%20methods%20need%20sophisticated%0Aprocesses%2C%20such%20as%20Digital%20Surface%20Models%20%28DSM%29%20and%20occlusion%20detection%2C%20which%0Aare%20computationally%20expensive%20and%20prone%20to%20errors.This%20work%20presents%20an%0Aalternative%20technique%20rooted%20in%202D%20Gaussian%20Splatting%20%282DGS%29%2C%20free%20of%20explicit%0ADSM%20and%20occlusion%20detection.%20With%20depth%20map%20generation%2C%20spatial%20information%20for%0Aevery%20pixel%20within%20the%20TDOM%20is%20retrieved%20and%20can%20reconstruct%20the%20scene%20with%0Ahigh%20precision.%20Divide-and-conquer%20strategy%20achieves%20excellent%20GS%20training%20and%0Arendering%20with%20high-resolution%20TDOMs%20at%20a%20lower%20resource%20cost%2C%20which%20preserves%0Ahigher%20quality%20of%20rendering%20on%20complex%20terrain%20and%20thin%20structure%20without%20a%0Adecrease%20in%20efficiency.%20Experimental%20results%20demonstrate%20the%20efficiency%20of%0Alarge-scale%20scene%20reconstruction%20and%20high-precision%20terrain%20modeling.%20This%0Aapproach%20provides%20accurate%20spatial%20data%2C%20which%20assists%20users%20in%20better%20planning%0Aand%20decision-making%20based%20on%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19703v2&entry.124074799=Read"},
{"title": "DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian\n  Splatting", "author": "Holly Dinkel and Marcel B\u00fcsching and Alberta Longhini and Brian Coltin and Trey Smith and Danica Kragic and M\u00e5rten Bj\u00f6rkman and Timothy Bretl", "abstract": "  This work presents DLO-Splatting, an algorithm for estimating the 3D shape of\nDeformable Linear Objects (DLOs) from multi-view RGB images and gripper state\ninformation through prediction-update filtering. The DLO-Splatting algorithm\nuses a position-based dynamics model with shape smoothness and rigidity\ndampening corrections to predict the object shape. Optimization with a 3D\nGaussian Splatting-based rendering loss iteratively renders and refines the\nprediction to align it with the visual observations in the update step. Initial\nexperiments demonstrate promising results in a knot tying scenario, which is\nchallenging for existing vision-only methods.\n", "link": "http://arxiv.org/abs/2505.08644v1", "date": "2025-05-13", "relevancy": 3.1694, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.666}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6574}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DLO-Splatting%3A%20Tracking%20Deformable%20Linear%20Objects%20Using%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20DLO-Splatting%3A%20Tracking%20Deformable%20Linear%20Objects%20Using%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Holly%20Dinkel%20and%20Marcel%20B%C3%BCsching%20and%20Alberta%20Longhini%20and%20Brian%20Coltin%20and%20Trey%20Smith%20and%20Danica%20Kragic%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Timothy%20Bretl%0AAbstract%3A%20%20%20This%20work%20presents%20DLO-Splatting%2C%20an%20algorithm%20for%20estimating%20the%203D%20shape%20of%0ADeformable%20Linear%20Objects%20%28DLOs%29%20from%20multi-view%20RGB%20images%20and%20gripper%20state%0Ainformation%20through%20prediction-update%20filtering.%20The%20DLO-Splatting%20algorithm%0Auses%20a%20position-based%20dynamics%20model%20with%20shape%20smoothness%20and%20rigidity%0Adampening%20corrections%20to%20predict%20the%20object%20shape.%20Optimization%20with%20a%203D%0AGaussian%20Splatting-based%20rendering%20loss%20iteratively%20renders%20and%20refines%20the%0Aprediction%20to%20align%20it%20with%20the%20visual%20observations%20in%20the%20update%20step.%20Initial%0Aexperiments%20demonstrate%20promising%20results%20in%20a%20knot%20tying%20scenario%2C%20which%20is%0Achallenging%20for%20existing%20vision-only%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDLO-Splatting%253A%2520Tracking%2520Deformable%2520Linear%2520Objects%2520Using%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DHolly%2520Dinkel%2520and%2520Marcel%2520B%25C3%25BCsching%2520and%2520Alberta%2520Longhini%2520and%2520Brian%2520Coltin%2520and%2520Trey%2520Smith%2520and%2520Danica%2520Kragic%2520and%2520M%25C3%25A5rten%2520Bj%25C3%25B6rkman%2520and%2520Timothy%2520Bretl%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520DLO-Splatting%252C%2520an%2520algorithm%2520for%2520estimating%2520the%25203D%2520shape%2520of%250ADeformable%2520Linear%2520Objects%2520%2528DLOs%2529%2520from%2520multi-view%2520RGB%2520images%2520and%2520gripper%2520state%250Ainformation%2520through%2520prediction-update%2520filtering.%2520The%2520DLO-Splatting%2520algorithm%250Auses%2520a%2520position-based%2520dynamics%2520model%2520with%2520shape%2520smoothness%2520and%2520rigidity%250Adampening%2520corrections%2520to%2520predict%2520the%2520object%2520shape.%2520Optimization%2520with%2520a%25203D%250AGaussian%2520Splatting-based%2520rendering%2520loss%2520iteratively%2520renders%2520and%2520refines%2520the%250Aprediction%2520to%2520align%2520it%2520with%2520the%2520visual%2520observations%2520in%2520the%2520update%2520step.%2520Initial%250Aexperiments%2520demonstrate%2520promising%2520results%2520in%2520a%2520knot%2520tying%2520scenario%252C%2520which%2520is%250Achallenging%2520for%2520existing%2520vision-only%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DLO-Splatting%3A%20Tracking%20Deformable%20Linear%20Objects%20Using%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Holly%20Dinkel%20and%20Marcel%20B%C3%BCsching%20and%20Alberta%20Longhini%20and%20Brian%20Coltin%20and%20Trey%20Smith%20and%20Danica%20Kragic%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Timothy%20Bretl&entry.1292438233=%20%20This%20work%20presents%20DLO-Splatting%2C%20an%20algorithm%20for%20estimating%20the%203D%20shape%20of%0ADeformable%20Linear%20Objects%20%28DLOs%29%20from%20multi-view%20RGB%20images%20and%20gripper%20state%0Ainformation%20through%20prediction-update%20filtering.%20The%20DLO-Splatting%20algorithm%0Auses%20a%20position-based%20dynamics%20model%20with%20shape%20smoothness%20and%20rigidity%0Adampening%20corrections%20to%20predict%20the%20object%20shape.%20Optimization%20with%20a%203D%0AGaussian%20Splatting-based%20rendering%20loss%20iteratively%20renders%20and%20refines%20the%0Aprediction%20to%20align%20it%20with%20the%20visual%20observations%20in%20the%20update%20step.%20Initial%0Aexperiments%20demonstrate%20promising%20results%20in%20a%20knot%20tying%20scenario%2C%20which%20is%0Achallenging%20for%20existing%20vision-only%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08644v1&entry.124074799=Read"},
{"title": "Extending Large Vision-Language Model for Diverse Interactive Tasks in\n  Autonomous Driving", "author": "Zongchuang Zhao and Haoyu Fu and Dingkang Liang and Xin Zhou and Dingyuan Zhang and Hongwei Xie and Bing Wang and Xiang Bai", "abstract": "  The Large Visual-Language Models (LVLMs) have significantly advanced image\nunderstanding. Their comprehension and reasoning capabilities enable promising\napplications in autonomous driving scenarios. However, existing research\ntypically focuses on front-view perspectives and partial objects within scenes,\nstruggling to achieve comprehensive scene understanding. Meanwhile, existing\nLVLMs suffer from the lack of mapping relationship between 2D and 3D and\ninsufficient integration of 3D object localization and instruction\nunderstanding. To tackle these limitations, we first introduce NuInteract, a\nlarge-scale dataset with over 1.5M multi-view image language pairs spanning\ndense scene captions and diverse interactive tasks. Furthermore, we propose\nDriveMonkey, a simple yet effective framework that seamlessly integrates LVLMs\nwith a spatial processor using a series of learnable queries. The spatial\nprocessor, designed as a plug-and-play component, can be initialized with\npre-trained 3D detectors to improve 3D perception. Our experiments show that\nDriveMonkey outperforms general LVLMs, especially achieving a 9.86% notable\nimprovement on the 3D visual grounding task. The dataset and code will be\nreleased at https://github.com/zc-zhao/DriveMonkey.\n", "link": "http://arxiv.org/abs/2505.08725v1", "date": "2025-05-13", "relevancy": 3.1582, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6495}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extending%20Large%20Vision-Language%20Model%20for%20Diverse%20Interactive%20Tasks%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20Extending%20Large%20Vision-Language%20Model%20for%20Diverse%20Interactive%20Tasks%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Zongchuang%20Zhao%20and%20Haoyu%20Fu%20and%20Dingkang%20Liang%20and%20Xin%20Zhou%20and%20Dingyuan%20Zhang%20and%20Hongwei%20Xie%20and%20Bing%20Wang%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20The%20Large%20Visual-Language%20Models%20%28LVLMs%29%20have%20significantly%20advanced%20image%0Aunderstanding.%20Their%20comprehension%20and%20reasoning%20capabilities%20enable%20promising%0Aapplications%20in%20autonomous%20driving%20scenarios.%20However%2C%20existing%20research%0Atypically%20focuses%20on%20front-view%20perspectives%20and%20partial%20objects%20within%20scenes%2C%0Astruggling%20to%20achieve%20comprehensive%20scene%20understanding.%20Meanwhile%2C%20existing%0ALVLMs%20suffer%20from%20the%20lack%20of%20mapping%20relationship%20between%202D%20and%203D%20and%0Ainsufficient%20integration%20of%203D%20object%20localization%20and%20instruction%0Aunderstanding.%20To%20tackle%20these%20limitations%2C%20we%20first%20introduce%20NuInteract%2C%20a%0Alarge-scale%20dataset%20with%20over%201.5M%20multi-view%20image%20language%20pairs%20spanning%0Adense%20scene%20captions%20and%20diverse%20interactive%20tasks.%20Furthermore%2C%20we%20propose%0ADriveMonkey%2C%20a%20simple%20yet%20effective%20framework%20that%20seamlessly%20integrates%20LVLMs%0Awith%20a%20spatial%20processor%20using%20a%20series%20of%20learnable%20queries.%20The%20spatial%0Aprocessor%2C%20designed%20as%20a%20plug-and-play%20component%2C%20can%20be%20initialized%20with%0Apre-trained%203D%20detectors%20to%20improve%203D%20perception.%20Our%20experiments%20show%20that%0ADriveMonkey%20outperforms%20general%20LVLMs%2C%20especially%20achieving%20a%209.86%25%20notable%0Aimprovement%20on%20the%203D%20visual%20grounding%20task.%20The%20dataset%20and%20code%20will%20be%0Areleased%20at%20https%3A//github.com/zc-zhao/DriveMonkey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtending%2520Large%2520Vision-Language%2520Model%2520for%2520Diverse%2520Interactive%2520Tasks%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DZongchuang%2520Zhao%2520and%2520Haoyu%2520Fu%2520and%2520Dingkang%2520Liang%2520and%2520Xin%2520Zhou%2520and%2520Dingyuan%2520Zhang%2520and%2520Hongwei%2520Xie%2520and%2520Bing%2520Wang%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520The%2520Large%2520Visual-Language%2520Models%2520%2528LVLMs%2529%2520have%2520significantly%2520advanced%2520image%250Aunderstanding.%2520Their%2520comprehension%2520and%2520reasoning%2520capabilities%2520enable%2520promising%250Aapplications%2520in%2520autonomous%2520driving%2520scenarios.%2520However%252C%2520existing%2520research%250Atypically%2520focuses%2520on%2520front-view%2520perspectives%2520and%2520partial%2520objects%2520within%2520scenes%252C%250Astruggling%2520to%2520achieve%2520comprehensive%2520scene%2520understanding.%2520Meanwhile%252C%2520existing%250ALVLMs%2520suffer%2520from%2520the%2520lack%2520of%2520mapping%2520relationship%2520between%25202D%2520and%25203D%2520and%250Ainsufficient%2520integration%2520of%25203D%2520object%2520localization%2520and%2520instruction%250Aunderstanding.%2520To%2520tackle%2520these%2520limitations%252C%2520we%2520first%2520introduce%2520NuInteract%252C%2520a%250Alarge-scale%2520dataset%2520with%2520over%25201.5M%2520multi-view%2520image%2520language%2520pairs%2520spanning%250Adense%2520scene%2520captions%2520and%2520diverse%2520interactive%2520tasks.%2520Furthermore%252C%2520we%2520propose%250ADriveMonkey%252C%2520a%2520simple%2520yet%2520effective%2520framework%2520that%2520seamlessly%2520integrates%2520LVLMs%250Awith%2520a%2520spatial%2520processor%2520using%2520a%2520series%2520of%2520learnable%2520queries.%2520The%2520spatial%250Aprocessor%252C%2520designed%2520as%2520a%2520plug-and-play%2520component%252C%2520can%2520be%2520initialized%2520with%250Apre-trained%25203D%2520detectors%2520to%2520improve%25203D%2520perception.%2520Our%2520experiments%2520show%2520that%250ADriveMonkey%2520outperforms%2520general%2520LVLMs%252C%2520especially%2520achieving%2520a%25209.86%2525%2520notable%250Aimprovement%2520on%2520the%25203D%2520visual%2520grounding%2520task.%2520The%2520dataset%2520and%2520code%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/zc-zhao/DriveMonkey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extending%20Large%20Vision-Language%20Model%20for%20Diverse%20Interactive%20Tasks%20in%0A%20%20Autonomous%20Driving&entry.906535625=Zongchuang%20Zhao%20and%20Haoyu%20Fu%20and%20Dingkang%20Liang%20and%20Xin%20Zhou%20and%20Dingyuan%20Zhang%20and%20Hongwei%20Xie%20and%20Bing%20Wang%20and%20Xiang%20Bai&entry.1292438233=%20%20The%20Large%20Visual-Language%20Models%20%28LVLMs%29%20have%20significantly%20advanced%20image%0Aunderstanding.%20Their%20comprehension%20and%20reasoning%20capabilities%20enable%20promising%0Aapplications%20in%20autonomous%20driving%20scenarios.%20However%2C%20existing%20research%0Atypically%20focuses%20on%20front-view%20perspectives%20and%20partial%20objects%20within%20scenes%2C%0Astruggling%20to%20achieve%20comprehensive%20scene%20understanding.%20Meanwhile%2C%20existing%0ALVLMs%20suffer%20from%20the%20lack%20of%20mapping%20relationship%20between%202D%20and%203D%20and%0Ainsufficient%20integration%20of%203D%20object%20localization%20and%20instruction%0Aunderstanding.%20To%20tackle%20these%20limitations%2C%20we%20first%20introduce%20NuInteract%2C%20a%0Alarge-scale%20dataset%20with%20over%201.5M%20multi-view%20image%20language%20pairs%20spanning%0Adense%20scene%20captions%20and%20diverse%20interactive%20tasks.%20Furthermore%2C%20we%20propose%0ADriveMonkey%2C%20a%20simple%20yet%20effective%20framework%20that%20seamlessly%20integrates%20LVLMs%0Awith%20a%20spatial%20processor%20using%20a%20series%20of%20learnable%20queries.%20The%20spatial%0Aprocessor%2C%20designed%20as%20a%20plug-and-play%20component%2C%20can%20be%20initialized%20with%0Apre-trained%203D%20detectors%20to%20improve%203D%20perception.%20Our%20experiments%20show%20that%0ADriveMonkey%20outperforms%20general%20LVLMs%2C%20especially%20achieving%20a%209.86%25%20notable%0Aimprovement%20on%20the%203D%20visual%20grounding%20task.%20The%20dataset%20and%20code%20will%20be%0Areleased%20at%20https%3A//github.com/zc-zhao/DriveMonkey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08725v1&entry.124074799=Read"},
{"title": "A Survey of 3D Reconstruction with Event Cameras: From Event-based\n  Geometry to Neural 3D Rendering", "author": "Chuanzhi Xu and Haoxian Zhou and Langyi Chen and Haodong Chen and Ying Zhou and Vera Chung and Qiang Qu", "abstract": "  Event cameras have emerged as promising sensors for 3D reconstruction due to\ntheir ability to capture per-pixel brightness changes asynchronously. Unlike\nconventional frame-based cameras, they produce sparse and temporally rich data\nstreams, which enable more accurate 3D reconstruction and open up the\npossibility of performing reconstruction in extreme environments such as\nhigh-speed motion, low light, or high dynamic range scenes. In this survey, we\nprovide the first comprehensive review focused exclusively on 3D reconstruction\nusing event cameras. The survey categorises existing works into three major\ntypes based on input modality - stereo, monocular, and multimodal systems, and\nfurther classifies them by reconstruction approach, including geometry-based,\ndeep learning-based, and recent neural rendering techniques such as Neural\nRadiance Fields and 3D Gaussian Splatting. Methods with a similar research\nfocus were organised chronologically into the most subdivided groups. We also\nsummarise public datasets relevant to event-based 3D reconstruction. Finally,\nwe highlight current research limitations in data availability, evaluation,\nrepresentation, and dynamic scene handling, and outline promising future\nresearch directions. This survey aims to serve as a comprehensive reference and\na roadmap for future developments in event-driven 3D reconstruction.\n", "link": "http://arxiv.org/abs/2505.08438v1", "date": "2025-05-13", "relevancy": 3.0867, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6323}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6098}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%203D%20Reconstruction%20with%20Event%20Cameras%3A%20From%20Event-based%0A%20%20Geometry%20to%20Neural%203D%20Rendering&body=Title%3A%20A%20Survey%20of%203D%20Reconstruction%20with%20Event%20Cameras%3A%20From%20Event-based%0A%20%20Geometry%20to%20Neural%203D%20Rendering%0AAuthor%3A%20Chuanzhi%20Xu%20and%20Haoxian%20Zhou%20and%20Langyi%20Chen%20and%20Haodong%20Chen%20and%20Ying%20Zhou%20and%20Vera%20Chung%20and%20Qiang%20Qu%0AAbstract%3A%20%20%20Event%20cameras%20have%20emerged%20as%20promising%20sensors%20for%203D%20reconstruction%20due%20to%0Atheir%20ability%20to%20capture%20per-pixel%20brightness%20changes%20asynchronously.%20Unlike%0Aconventional%20frame-based%20cameras%2C%20they%20produce%20sparse%20and%20temporally%20rich%20data%0Astreams%2C%20which%20enable%20more%20accurate%203D%20reconstruction%20and%20open%20up%20the%0Apossibility%20of%20performing%20reconstruction%20in%20extreme%20environments%20such%20as%0Ahigh-speed%20motion%2C%20low%20light%2C%20or%20high%20dynamic%20range%20scenes.%20In%20this%20survey%2C%20we%0Aprovide%20the%20first%20comprehensive%20review%20focused%20exclusively%20on%203D%20reconstruction%0Ausing%20event%20cameras.%20The%20survey%20categorises%20existing%20works%20into%20three%20major%0Atypes%20based%20on%20input%20modality%20-%20stereo%2C%20monocular%2C%20and%20multimodal%20systems%2C%20and%0Afurther%20classifies%20them%20by%20reconstruction%20approach%2C%20including%20geometry-based%2C%0Adeep%20learning-based%2C%20and%20recent%20neural%20rendering%20techniques%20such%20as%20Neural%0ARadiance%20Fields%20and%203D%20Gaussian%20Splatting.%20Methods%20with%20a%20similar%20research%0Afocus%20were%20organised%20chronologically%20into%20the%20most%20subdivided%20groups.%20We%20also%0Asummarise%20public%20datasets%20relevant%20to%20event-based%203D%20reconstruction.%20Finally%2C%0Awe%20highlight%20current%20research%20limitations%20in%20data%20availability%2C%20evaluation%2C%0Arepresentation%2C%20and%20dynamic%20scene%20handling%2C%20and%20outline%20promising%20future%0Aresearch%20directions.%20This%20survey%20aims%20to%20serve%20as%20a%20comprehensive%20reference%20and%0Aa%20roadmap%20for%20future%20developments%20in%20event-driven%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%25203D%2520Reconstruction%2520with%2520Event%2520Cameras%253A%2520From%2520Event-based%250A%2520%2520Geometry%2520to%2520Neural%25203D%2520Rendering%26entry.906535625%3DChuanzhi%2520Xu%2520and%2520Haoxian%2520Zhou%2520and%2520Langyi%2520Chen%2520and%2520Haodong%2520Chen%2520and%2520Ying%2520Zhou%2520and%2520Vera%2520Chung%2520and%2520Qiang%2520Qu%26entry.1292438233%3D%2520%2520Event%2520cameras%2520have%2520emerged%2520as%2520promising%2520sensors%2520for%25203D%2520reconstruction%2520due%2520to%250Atheir%2520ability%2520to%2520capture%2520per-pixel%2520brightness%2520changes%2520asynchronously.%2520Unlike%250Aconventional%2520frame-based%2520cameras%252C%2520they%2520produce%2520sparse%2520and%2520temporally%2520rich%2520data%250Astreams%252C%2520which%2520enable%2520more%2520accurate%25203D%2520reconstruction%2520and%2520open%2520up%2520the%250Apossibility%2520of%2520performing%2520reconstruction%2520in%2520extreme%2520environments%2520such%2520as%250Ahigh-speed%2520motion%252C%2520low%2520light%252C%2520or%2520high%2520dynamic%2520range%2520scenes.%2520In%2520this%2520survey%252C%2520we%250Aprovide%2520the%2520first%2520comprehensive%2520review%2520focused%2520exclusively%2520on%25203D%2520reconstruction%250Ausing%2520event%2520cameras.%2520The%2520survey%2520categorises%2520existing%2520works%2520into%2520three%2520major%250Atypes%2520based%2520on%2520input%2520modality%2520-%2520stereo%252C%2520monocular%252C%2520and%2520multimodal%2520systems%252C%2520and%250Afurther%2520classifies%2520them%2520by%2520reconstruction%2520approach%252C%2520including%2520geometry-based%252C%250Adeep%2520learning-based%252C%2520and%2520recent%2520neural%2520rendering%2520techniques%2520such%2520as%2520Neural%250ARadiance%2520Fields%2520and%25203D%2520Gaussian%2520Splatting.%2520Methods%2520with%2520a%2520similar%2520research%250Afocus%2520were%2520organised%2520chronologically%2520into%2520the%2520most%2520subdivided%2520groups.%2520We%2520also%250Asummarise%2520public%2520datasets%2520relevant%2520to%2520event-based%25203D%2520reconstruction.%2520Finally%252C%250Awe%2520highlight%2520current%2520research%2520limitations%2520in%2520data%2520availability%252C%2520evaluation%252C%250Arepresentation%252C%2520and%2520dynamic%2520scene%2520handling%252C%2520and%2520outline%2520promising%2520future%250Aresearch%2520directions.%2520This%2520survey%2520aims%2520to%2520serve%2520as%2520a%2520comprehensive%2520reference%2520and%250Aa%2520roadmap%2520for%2520future%2520developments%2520in%2520event-driven%25203D%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%203D%20Reconstruction%20with%20Event%20Cameras%3A%20From%20Event-based%0A%20%20Geometry%20to%20Neural%203D%20Rendering&entry.906535625=Chuanzhi%20Xu%20and%20Haoxian%20Zhou%20and%20Langyi%20Chen%20and%20Haodong%20Chen%20and%20Ying%20Zhou%20and%20Vera%20Chung%20and%20Qiang%20Qu&entry.1292438233=%20%20Event%20cameras%20have%20emerged%20as%20promising%20sensors%20for%203D%20reconstruction%20due%20to%0Atheir%20ability%20to%20capture%20per-pixel%20brightness%20changes%20asynchronously.%20Unlike%0Aconventional%20frame-based%20cameras%2C%20they%20produce%20sparse%20and%20temporally%20rich%20data%0Astreams%2C%20which%20enable%20more%20accurate%203D%20reconstruction%20and%20open%20up%20the%0Apossibility%20of%20performing%20reconstruction%20in%20extreme%20environments%20such%20as%0Ahigh-speed%20motion%2C%20low%20light%2C%20or%20high%20dynamic%20range%20scenes.%20In%20this%20survey%2C%20we%0Aprovide%20the%20first%20comprehensive%20review%20focused%20exclusively%20on%203D%20reconstruction%0Ausing%20event%20cameras.%20The%20survey%20categorises%20existing%20works%20into%20three%20major%0Atypes%20based%20on%20input%20modality%20-%20stereo%2C%20monocular%2C%20and%20multimodal%20systems%2C%20and%0Afurther%20classifies%20them%20by%20reconstruction%20approach%2C%20including%20geometry-based%2C%0Adeep%20learning-based%2C%20and%20recent%20neural%20rendering%20techniques%20such%20as%20Neural%0ARadiance%20Fields%20and%203D%20Gaussian%20Splatting.%20Methods%20with%20a%20similar%20research%0Afocus%20were%20organised%20chronologically%20into%20the%20most%20subdivided%20groups.%20We%20also%0Asummarise%20public%20datasets%20relevant%20to%20event-based%203D%20reconstruction.%20Finally%2C%0Awe%20highlight%20current%20research%20limitations%20in%20data%20availability%2C%20evaluation%2C%0Arepresentation%2C%20and%20dynamic%20scene%20handling%2C%20and%20outline%20promising%20future%0Aresearch%20directions.%20This%20survey%20aims%20to%20serve%20as%20a%20comprehensive%20reference%20and%0Aa%20roadmap%20for%20future%20developments%20in%20event-driven%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08438v1&entry.124074799=Read"},
{"title": "FOCI: Trajectory Optimization on Gaussian Splats", "author": "Mario Gomez Andreu and Maximum Wilder-Smith and Victor Klemm and Vaishakh Patil and Jesus Tordesillas and Marco Hutter", "abstract": "  3D Gaussian Splatting (3DGS) has recently gained popularity as a faster\nalternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view\nsynthesis methods. Leveraging the spatial information encoded in 3DGS, this\nwork proposes FOCI (Field Overlap Collision Integral), an algorithm that is\nable to optimize trajectories directly on the Gaussians themselves. FOCI\nleverages a novel and interpretable collision formulation for 3DGS using the\nnotion of the overlap integral between Gaussians. Contrary to other approaches,\nwhich represent the robot with conservative bounding boxes that underestimate\nthe traversability of the environment, we propose to represent the environment\nand the robot as Gaussian Splats. This not only has desirable computational\nproperties, but also allows for orientation-aware planning, allowing the robot\nto pass through very tight and narrow spaces. We extensively test our algorithm\nin both synthetic and real Gaussian Splats, showcasing that collision-free\ntrajectories for the ANYmal legged robot that can be computed in a few seconds,\neven with hundreds of thousands of Gaussians making up the environment. The\nproject page and code are available at\nhttps://rffr.leggedrobotics.com/works/foci/\n", "link": "http://arxiv.org/abs/2505.08510v1", "date": "2025-05-13", "relevancy": 3.0667, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6508}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5984}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOCI%3A%20Trajectory%20Optimization%20on%20Gaussian%20Splats&body=Title%3A%20FOCI%3A%20Trajectory%20Optimization%20on%20Gaussian%20Splats%0AAuthor%3A%20Mario%20Gomez%20Andreu%20and%20Maximum%20Wilder-Smith%20and%20Victor%20Klemm%20and%20Vaishakh%20Patil%20and%20Jesus%20Tordesillas%20and%20Marco%20Hutter%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20gained%20popularity%20as%20a%20faster%0Aalternative%20to%20Neural%20Radiance%20Fields%20%28NeRFs%29%20in%203D%20reconstruction%20and%20view%0Asynthesis%20methods.%20Leveraging%20the%20spatial%20information%20encoded%20in%203DGS%2C%20this%0Awork%20proposes%20FOCI%20%28Field%20Overlap%20Collision%20Integral%29%2C%20an%20algorithm%20that%20is%0Aable%20to%20optimize%20trajectories%20directly%20on%20the%20Gaussians%20themselves.%20FOCI%0Aleverages%20a%20novel%20and%20interpretable%20collision%20formulation%20for%203DGS%20using%20the%0Anotion%20of%20the%20overlap%20integral%20between%20Gaussians.%20Contrary%20to%20other%20approaches%2C%0Awhich%20represent%20the%20robot%20with%20conservative%20bounding%20boxes%20that%20underestimate%0Athe%20traversability%20of%20the%20environment%2C%20we%20propose%20to%20represent%20the%20environment%0Aand%20the%20robot%20as%20Gaussian%20Splats.%20This%20not%20only%20has%20desirable%20computational%0Aproperties%2C%20but%20also%20allows%20for%20orientation-aware%20planning%2C%20allowing%20the%20robot%0Ato%20pass%20through%20very%20tight%20and%20narrow%20spaces.%20We%20extensively%20test%20our%20algorithm%0Ain%20both%20synthetic%20and%20real%20Gaussian%20Splats%2C%20showcasing%20that%20collision-free%0Atrajectories%20for%20the%20ANYmal%20legged%20robot%20that%20can%20be%20computed%20in%20a%20few%20seconds%2C%0Aeven%20with%20hundreds%20of%20thousands%20of%20Gaussians%20making%20up%20the%20environment.%20The%0Aproject%20page%20and%20code%20are%20available%20at%0Ahttps%3A//rffr.leggedrobotics.com/works/foci/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOCI%253A%2520Trajectory%2520Optimization%2520on%2520Gaussian%2520Splats%26entry.906535625%3DMario%2520Gomez%2520Andreu%2520and%2520Maximum%2520Wilder-Smith%2520and%2520Victor%2520Klemm%2520and%2520Vaishakh%2520Patil%2520and%2520Jesus%2520Tordesillas%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520gained%2520popularity%2520as%2520a%2520faster%250Aalternative%2520to%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520in%25203D%2520reconstruction%2520and%2520view%250Asynthesis%2520methods.%2520Leveraging%2520the%2520spatial%2520information%2520encoded%2520in%25203DGS%252C%2520this%250Awork%2520proposes%2520FOCI%2520%2528Field%2520Overlap%2520Collision%2520Integral%2529%252C%2520an%2520algorithm%2520that%2520is%250Aable%2520to%2520optimize%2520trajectories%2520directly%2520on%2520the%2520Gaussians%2520themselves.%2520FOCI%250Aleverages%2520a%2520novel%2520and%2520interpretable%2520collision%2520formulation%2520for%25203DGS%2520using%2520the%250Anotion%2520of%2520the%2520overlap%2520integral%2520between%2520Gaussians.%2520Contrary%2520to%2520other%2520approaches%252C%250Awhich%2520represent%2520the%2520robot%2520with%2520conservative%2520bounding%2520boxes%2520that%2520underestimate%250Athe%2520traversability%2520of%2520the%2520environment%252C%2520we%2520propose%2520to%2520represent%2520the%2520environment%250Aand%2520the%2520robot%2520as%2520Gaussian%2520Splats.%2520This%2520not%2520only%2520has%2520desirable%2520computational%250Aproperties%252C%2520but%2520also%2520allows%2520for%2520orientation-aware%2520planning%252C%2520allowing%2520the%2520robot%250Ato%2520pass%2520through%2520very%2520tight%2520and%2520narrow%2520spaces.%2520We%2520extensively%2520test%2520our%2520algorithm%250Ain%2520both%2520synthetic%2520and%2520real%2520Gaussian%2520Splats%252C%2520showcasing%2520that%2520collision-free%250Atrajectories%2520for%2520the%2520ANYmal%2520legged%2520robot%2520that%2520can%2520be%2520computed%2520in%2520a%2520few%2520seconds%252C%250Aeven%2520with%2520hundreds%2520of%2520thousands%2520of%2520Gaussians%2520making%2520up%2520the%2520environment.%2520The%250Aproject%2520page%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//rffr.leggedrobotics.com/works/foci/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOCI%3A%20Trajectory%20Optimization%20on%20Gaussian%20Splats&entry.906535625=Mario%20Gomez%20Andreu%20and%20Maximum%20Wilder-Smith%20and%20Victor%20Klemm%20and%20Vaishakh%20Patil%20and%20Jesus%20Tordesillas%20and%20Marco%20Hutter&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20gained%20popularity%20as%20a%20faster%0Aalternative%20to%20Neural%20Radiance%20Fields%20%28NeRFs%29%20in%203D%20reconstruction%20and%20view%0Asynthesis%20methods.%20Leveraging%20the%20spatial%20information%20encoded%20in%203DGS%2C%20this%0Awork%20proposes%20FOCI%20%28Field%20Overlap%20Collision%20Integral%29%2C%20an%20algorithm%20that%20is%0Aable%20to%20optimize%20trajectories%20directly%20on%20the%20Gaussians%20themselves.%20FOCI%0Aleverages%20a%20novel%20and%20interpretable%20collision%20formulation%20for%203DGS%20using%20the%0Anotion%20of%20the%20overlap%20integral%20between%20Gaussians.%20Contrary%20to%20other%20approaches%2C%0Awhich%20represent%20the%20robot%20with%20conservative%20bounding%20boxes%20that%20underestimate%0Athe%20traversability%20of%20the%20environment%2C%20we%20propose%20to%20represent%20the%20environment%0Aand%20the%20robot%20as%20Gaussian%20Splats.%20This%20not%20only%20has%20desirable%20computational%0Aproperties%2C%20but%20also%20allows%20for%20orientation-aware%20planning%2C%20allowing%20the%20robot%0Ato%20pass%20through%20very%20tight%20and%20narrow%20spaces.%20We%20extensively%20test%20our%20algorithm%0Ain%20both%20synthetic%20and%20real%20Gaussian%20Splats%2C%20showcasing%20that%20collision-free%0Atrajectories%20for%20the%20ANYmal%20legged%20robot%20that%20can%20be%20computed%20in%20a%20few%20seconds%2C%0Aeven%20with%20hundreds%20of%20thousands%20of%20Gaussians%20making%20up%20the%20environment.%20The%0Aproject%20page%20and%20code%20are%20available%20at%0Ahttps%3A//rffr.leggedrobotics.com/works/foci/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08510v1&entry.124074799=Read"},
{"title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining", "author": "Wenqi Zhang and Hang Zhang and Xin Li and Jiashuo Sun and Yongliang Shen and Weiming Lu and Deli Zhao and Yueting Zhuang and Lidong Bing", "abstract": "  Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving. Our code are available\nat https://github.com/DAMO-NLP-SG/multimodal_textbook.\n", "link": "http://arxiv.org/abs/2501.00958v4", "date": "2025-05-13", "relevancy": 2.9439, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202.5%20Years%20in%20Class%3A%20A%20Multimodal%20Textbook%20for%20Vision-Language%0A%20%20Pretraining&body=Title%3A%202.5%20Years%20in%20Class%3A%20A%20Multimodal%20Textbook%20for%20Vision-Language%0A%20%20Pretraining%0AAuthor%3A%20Wenqi%20Zhang%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Jiashuo%20Sun%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Deli%20Zhao%20and%20Yueting%20Zhuang%20and%20Lidong%20Bing%0AAbstract%3A%20%20%20Compared%20to%20image-text%20pair%20data%2C%20interleaved%20corpora%20enable%20Vision-Language%0AModels%20%28VLMs%29%20to%20understand%20the%20world%20more%20naturally%20like%20humans.%20However%2C%20such%0Aexisting%20datasets%20are%20crawled%20from%20webpage%2C%20facing%20challenges%20like%20low%0Aknowledge%20density%2C%20loose%20image-text%20relations%2C%20and%20poor%20logical%20coherence%0Abetween%20images.%20On%20the%20other%20hand%2C%20the%20internet%20hosts%20vast%20instructional%20videos%0A%28e.g.%2C%20online%20geometry%20courses%29%20that%20are%20widely%20used%20by%20humans%20to%20learn%0Afoundational%20subjects%2C%20yet%20these%20valuable%20resources%20remain%20underexplored%20in%20VLM%0Atraining.%20In%20this%20paper%2C%20we%20introduce%20a%20high-quality%20%5Ctextbf%7Bmultimodal%0Atextbook%7D%20corpus%20with%20richer%20foundational%20knowledge%20for%20VLM%20pretraining.%20It%0Acollects%20over%202.5%20years%20of%20instructional%20videos%2C%20totaling%2022%2C000%20class%20hours.%0AWe%20first%20use%20an%20LLM-proposed%20taxonomy%20to%20systematically%20gather%20instructional%0Avideos.%20Then%20we%20progressively%20extract%20and%20refine%20visual%20%28keyframes%29%2C%20audio%0A%28ASR%29%2C%20and%20textual%20knowledge%20%28OCR%29%20from%20the%20videos%2C%20and%20organize%20as%20an%0Aimage-text%20interleaved%20corpus%20based%20on%20temporal%20order.%20Compared%20to%20its%0Acounterparts%2C%20our%20video-centric%20textbook%20offers%20more%20coherent%20context%2C%20richer%0Aknowledge%2C%20and%20better%20image-text%20alignment.%20Experiments%20demonstrate%20its%20superb%0Apretraining%20performance%2C%20particularly%20in%20knowledge-%20and%20reasoning-intensive%0Atasks%20like%20ScienceQA%20and%20MathVista.%20Moreover%2C%20VLMs%20pre-trained%20on%20our%20textbook%0Aexhibit%20outstanding%20interleaved%20context%20awareness%2C%20leveraging%20visual%20and%0Atextual%20cues%20in%20their%20few-shot%20context%20for%20task%20solving.%20Our%20code%20are%20available%0Aat%20https%3A//github.com/DAMO-NLP-SG/multimodal_textbook.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00958v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2.5%2520Years%2520in%2520Class%253A%2520A%2520Multimodal%2520Textbook%2520for%2520Vision-Language%250A%2520%2520Pretraining%26entry.906535625%3DWenqi%2520Zhang%2520and%2520Hang%2520Zhang%2520and%2520Xin%2520Li%2520and%2520Jiashuo%2520Sun%2520and%2520Yongliang%2520Shen%2520and%2520Weiming%2520Lu%2520and%2520Deli%2520Zhao%2520and%2520Yueting%2520Zhuang%2520and%2520Lidong%2520Bing%26entry.1292438233%3D%2520%2520Compared%2520to%2520image-text%2520pair%2520data%252C%2520interleaved%2520corpora%2520enable%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520to%2520understand%2520the%2520world%2520more%2520naturally%2520like%2520humans.%2520However%252C%2520such%250Aexisting%2520datasets%2520are%2520crawled%2520from%2520webpage%252C%2520facing%2520challenges%2520like%2520low%250Aknowledge%2520density%252C%2520loose%2520image-text%2520relations%252C%2520and%2520poor%2520logical%2520coherence%250Abetween%2520images.%2520On%2520the%2520other%2520hand%252C%2520the%2520internet%2520hosts%2520vast%2520instructional%2520videos%250A%2528e.g.%252C%2520online%2520geometry%2520courses%2529%2520that%2520are%2520widely%2520used%2520by%2520humans%2520to%2520learn%250Afoundational%2520subjects%252C%2520yet%2520these%2520valuable%2520resources%2520remain%2520underexplored%2520in%2520VLM%250Atraining.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520high-quality%2520%255Ctextbf%257Bmultimodal%250Atextbook%257D%2520corpus%2520with%2520richer%2520foundational%2520knowledge%2520for%2520VLM%2520pretraining.%2520It%250Acollects%2520over%25202.5%2520years%2520of%2520instructional%2520videos%252C%2520totaling%252022%252C000%2520class%2520hours.%250AWe%2520first%2520use%2520an%2520LLM-proposed%2520taxonomy%2520to%2520systematically%2520gather%2520instructional%250Avideos.%2520Then%2520we%2520progressively%2520extract%2520and%2520refine%2520visual%2520%2528keyframes%2529%252C%2520audio%250A%2528ASR%2529%252C%2520and%2520textual%2520knowledge%2520%2528OCR%2529%2520from%2520the%2520videos%252C%2520and%2520organize%2520as%2520an%250Aimage-text%2520interleaved%2520corpus%2520based%2520on%2520temporal%2520order.%2520Compared%2520to%2520its%250Acounterparts%252C%2520our%2520video-centric%2520textbook%2520offers%2520more%2520coherent%2520context%252C%2520richer%250Aknowledge%252C%2520and%2520better%2520image-text%2520alignment.%2520Experiments%2520demonstrate%2520its%2520superb%250Apretraining%2520performance%252C%2520particularly%2520in%2520knowledge-%2520and%2520reasoning-intensive%250Atasks%2520like%2520ScienceQA%2520and%2520MathVista.%2520Moreover%252C%2520VLMs%2520pre-trained%2520on%2520our%2520textbook%250Aexhibit%2520outstanding%2520interleaved%2520context%2520awareness%252C%2520leveraging%2520visual%2520and%250Atextual%2520cues%2520in%2520their%2520few-shot%2520context%2520for%2520task%2520solving.%2520Our%2520code%2520are%2520available%250Aat%2520https%253A//github.com/DAMO-NLP-SG/multimodal_textbook.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00958v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2.5%20Years%20in%20Class%3A%20A%20Multimodal%20Textbook%20for%20Vision-Language%0A%20%20Pretraining&entry.906535625=Wenqi%20Zhang%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Jiashuo%20Sun%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Deli%20Zhao%20and%20Yueting%20Zhuang%20and%20Lidong%20Bing&entry.1292438233=%20%20Compared%20to%20image-text%20pair%20data%2C%20interleaved%20corpora%20enable%20Vision-Language%0AModels%20%28VLMs%29%20to%20understand%20the%20world%20more%20naturally%20like%20humans.%20However%2C%20such%0Aexisting%20datasets%20are%20crawled%20from%20webpage%2C%20facing%20challenges%20like%20low%0Aknowledge%20density%2C%20loose%20image-text%20relations%2C%20and%20poor%20logical%20coherence%0Abetween%20images.%20On%20the%20other%20hand%2C%20the%20internet%20hosts%20vast%20instructional%20videos%0A%28e.g.%2C%20online%20geometry%20courses%29%20that%20are%20widely%20used%20by%20humans%20to%20learn%0Afoundational%20subjects%2C%20yet%20these%20valuable%20resources%20remain%20underexplored%20in%20VLM%0Atraining.%20In%20this%20paper%2C%20we%20introduce%20a%20high-quality%20%5Ctextbf%7Bmultimodal%0Atextbook%7D%20corpus%20with%20richer%20foundational%20knowledge%20for%20VLM%20pretraining.%20It%0Acollects%20over%202.5%20years%20of%20instructional%20videos%2C%20totaling%2022%2C000%20class%20hours.%0AWe%20first%20use%20an%20LLM-proposed%20taxonomy%20to%20systematically%20gather%20instructional%0Avideos.%20Then%20we%20progressively%20extract%20and%20refine%20visual%20%28keyframes%29%2C%20audio%0A%28ASR%29%2C%20and%20textual%20knowledge%20%28OCR%29%20from%20the%20videos%2C%20and%20organize%20as%20an%0Aimage-text%20interleaved%20corpus%20based%20on%20temporal%20order.%20Compared%20to%20its%0Acounterparts%2C%20our%20video-centric%20textbook%20offers%20more%20coherent%20context%2C%20richer%0Aknowledge%2C%20and%20better%20image-text%20alignment.%20Experiments%20demonstrate%20its%20superb%0Apretraining%20performance%2C%20particularly%20in%20knowledge-%20and%20reasoning-intensive%0Atasks%20like%20ScienceQA%20and%20MathVista.%20Moreover%2C%20VLMs%20pre-trained%20on%20our%20textbook%0Aexhibit%20outstanding%20interleaved%20context%20awareness%2C%20leveraging%20visual%20and%0Atextual%20cues%20in%20their%20few-shot%20context%20for%20task%20solving.%20Our%20code%20are%20available%0Aat%20https%3A//github.com/DAMO-NLP-SG/multimodal_textbook.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00958v4&entry.124074799=Read"},
{"title": "Towards Foundation Models for Experimental Readout Systems Combining\n  Discrete and Continuous Data", "author": "James Giroux and Cristiano Fanelli", "abstract": "  We present a (proto) Foundation Model for Nuclear Physics, capable of\noperating on low-level detector inputs from Imaging Cherenkov Detectors at the\nfuture Electron Ion Collider. To address limitations in existing next-token\nprediction approaches-namely resolution loss from VQ-VAE tokenization and lack\nof conditional generation-we propose three key innovations: (i) separate\nvocabularies for discrete spatial features and continuous variates, combined\nvia Causal Multi-Head Cross-Attention (CMHCA), (ii) continuous kinematic\nconditioning through prepended context embeddings, and (iii) scalable and\nsimple, high-resolution continuous variate tokenization without joint\nvocabulary inflation. Our model enables fast, high-fidelity generation of pixel\nand time sequences for Cherenkov photons, validated through closure tests in\nthe High Performance DIRC. We also show our model generalizes to reconstruction\ntasks such as pion and kaon identification, in which we show its ability to\nleverage fine-tuning.\n", "link": "http://arxiv.org/abs/2505.08736v1", "date": "2025-05-13", "relevancy": 2.9193, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Foundation%20Models%20for%20Experimental%20Readout%20Systems%20Combining%0A%20%20Discrete%20and%20Continuous%20Data&body=Title%3A%20Towards%20Foundation%20Models%20for%20Experimental%20Readout%20Systems%20Combining%0A%20%20Discrete%20and%20Continuous%20Data%0AAuthor%3A%20James%20Giroux%20and%20Cristiano%20Fanelli%0AAbstract%3A%20%20%20We%20present%20a%20%28proto%29%20Foundation%20Model%20for%20Nuclear%20Physics%2C%20capable%20of%0Aoperating%20on%20low-level%20detector%20inputs%20from%20Imaging%20Cherenkov%20Detectors%20at%20the%0Afuture%20Electron%20Ion%20Collider.%20To%20address%20limitations%20in%20existing%20next-token%0Aprediction%20approaches-namely%20resolution%20loss%20from%20VQ-VAE%20tokenization%20and%20lack%0Aof%20conditional%20generation-we%20propose%20three%20key%20innovations%3A%20%28i%29%20separate%0Avocabularies%20for%20discrete%20spatial%20features%20and%20continuous%20variates%2C%20combined%0Avia%20Causal%20Multi-Head%20Cross-Attention%20%28CMHCA%29%2C%20%28ii%29%20continuous%20kinematic%0Aconditioning%20through%20prepended%20context%20embeddings%2C%20and%20%28iii%29%20scalable%20and%0Asimple%2C%20high-resolution%20continuous%20variate%20tokenization%20without%20joint%0Avocabulary%20inflation.%20Our%20model%20enables%20fast%2C%20high-fidelity%20generation%20of%20pixel%0Aand%20time%20sequences%20for%20Cherenkov%20photons%2C%20validated%20through%20closure%20tests%20in%0Athe%20High%20Performance%20DIRC.%20We%20also%20show%20our%20model%20generalizes%20to%20reconstruction%0Atasks%20such%20as%20pion%20and%20kaon%20identification%2C%20in%20which%20we%20show%20its%20ability%20to%0Aleverage%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Foundation%2520Models%2520for%2520Experimental%2520Readout%2520Systems%2520Combining%250A%2520%2520Discrete%2520and%2520Continuous%2520Data%26entry.906535625%3DJames%2520Giroux%2520and%2520Cristiano%2520Fanelli%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520%2528proto%2529%2520Foundation%2520Model%2520for%2520Nuclear%2520Physics%252C%2520capable%2520of%250Aoperating%2520on%2520low-level%2520detector%2520inputs%2520from%2520Imaging%2520Cherenkov%2520Detectors%2520at%2520the%250Afuture%2520Electron%2520Ion%2520Collider.%2520To%2520address%2520limitations%2520in%2520existing%2520next-token%250Aprediction%2520approaches-namely%2520resolution%2520loss%2520from%2520VQ-VAE%2520tokenization%2520and%2520lack%250Aof%2520conditional%2520generation-we%2520propose%2520three%2520key%2520innovations%253A%2520%2528i%2529%2520separate%250Avocabularies%2520for%2520discrete%2520spatial%2520features%2520and%2520continuous%2520variates%252C%2520combined%250Avia%2520Causal%2520Multi-Head%2520Cross-Attention%2520%2528CMHCA%2529%252C%2520%2528ii%2529%2520continuous%2520kinematic%250Aconditioning%2520through%2520prepended%2520context%2520embeddings%252C%2520and%2520%2528iii%2529%2520scalable%2520and%250Asimple%252C%2520high-resolution%2520continuous%2520variate%2520tokenization%2520without%2520joint%250Avocabulary%2520inflation.%2520Our%2520model%2520enables%2520fast%252C%2520high-fidelity%2520generation%2520of%2520pixel%250Aand%2520time%2520sequences%2520for%2520Cherenkov%2520photons%252C%2520validated%2520through%2520closure%2520tests%2520in%250Athe%2520High%2520Performance%2520DIRC.%2520We%2520also%2520show%2520our%2520model%2520generalizes%2520to%2520reconstruction%250Atasks%2520such%2520as%2520pion%2520and%2520kaon%2520identification%252C%2520in%2520which%2520we%2520show%2520its%2520ability%2520to%250Aleverage%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Foundation%20Models%20for%20Experimental%20Readout%20Systems%20Combining%0A%20%20Discrete%20and%20Continuous%20Data&entry.906535625=James%20Giroux%20and%20Cristiano%20Fanelli&entry.1292438233=%20%20We%20present%20a%20%28proto%29%20Foundation%20Model%20for%20Nuclear%20Physics%2C%20capable%20of%0Aoperating%20on%20low-level%20detector%20inputs%20from%20Imaging%20Cherenkov%20Detectors%20at%20the%0Afuture%20Electron%20Ion%20Collider.%20To%20address%20limitations%20in%20existing%20next-token%0Aprediction%20approaches-namely%20resolution%20loss%20from%20VQ-VAE%20tokenization%20and%20lack%0Aof%20conditional%20generation-we%20propose%20three%20key%20innovations%3A%20%28i%29%20separate%0Avocabularies%20for%20discrete%20spatial%20features%20and%20continuous%20variates%2C%20combined%0Avia%20Causal%20Multi-Head%20Cross-Attention%20%28CMHCA%29%2C%20%28ii%29%20continuous%20kinematic%0Aconditioning%20through%20prepended%20context%20embeddings%2C%20and%20%28iii%29%20scalable%20and%0Asimple%2C%20high-resolution%20continuous%20variate%20tokenization%20without%20joint%0Avocabulary%20inflation.%20Our%20model%20enables%20fast%2C%20high-fidelity%20generation%20of%20pixel%0Aand%20time%20sequences%20for%20Cherenkov%20photons%2C%20validated%20through%20closure%20tests%20in%0Athe%20High%20Performance%20DIRC.%20We%20also%20show%20our%20model%20generalizes%20to%20reconstruction%0Atasks%20such%20as%20pion%20and%20kaon%20identification%2C%20in%20which%20we%20show%20its%20ability%20to%0Aleverage%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08736v1&entry.124074799=Read"},
{"title": "Visual Image Reconstruction from Brain Activity via Latent\n  Representation", "author": "Yukiyasu Kamitani and Misato Tanaka and Ken Shirakawa", "abstract": "  Visual image reconstruction, the decoding of perceptual content from brain\nactivity into images, has advanced significantly with the integration of deep\nneural networks (DNNs) and generative models. This review traces the field's\nevolution from early classification approaches to sophisticated reconstructions\nthat capture detailed, subjective visual experiences, emphasizing the roles of\nhierarchical latent representations, compositional strategies, and modular\narchitectures. Despite notable progress, challenges remain, such as achieving\ntrue zero-shot generalization for unseen images and accurately modeling the\ncomplex, subjective aspects of perception. We discuss the need for diverse\ndatasets, refined evaluation metrics aligned with human perceptual judgments,\nand compositional representations that strengthen model robustness and\ngeneralizability. Ethical issues, including privacy, consent, and potential\nmisuse, are underscored as critical considerations for responsible development.\nVisual image reconstruction offers promising insights into neural coding and\nenables new psychological measurements of visual experiences, with applications\nspanning clinical diagnostics and brain-machine interfaces.\n", "link": "http://arxiv.org/abs/2505.08429v1", "date": "2025-05-13", "relevancy": 2.8885, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5871}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Image%20Reconstruction%20from%20Brain%20Activity%20via%20Latent%0A%20%20Representation&body=Title%3A%20Visual%20Image%20Reconstruction%20from%20Brain%20Activity%20via%20Latent%0A%20%20Representation%0AAuthor%3A%20Yukiyasu%20Kamitani%20and%20Misato%20Tanaka%20and%20Ken%20Shirakawa%0AAbstract%3A%20%20%20Visual%20image%20reconstruction%2C%20the%20decoding%20of%20perceptual%20content%20from%20brain%0Aactivity%20into%20images%2C%20has%20advanced%20significantly%20with%20the%20integration%20of%20deep%0Aneural%20networks%20%28DNNs%29%20and%20generative%20models.%20This%20review%20traces%20the%20field%27s%0Aevolution%20from%20early%20classification%20approaches%20to%20sophisticated%20reconstructions%0Athat%20capture%20detailed%2C%20subjective%20visual%20experiences%2C%20emphasizing%20the%20roles%20of%0Ahierarchical%20latent%20representations%2C%20compositional%20strategies%2C%20and%20modular%0Aarchitectures.%20Despite%20notable%20progress%2C%20challenges%20remain%2C%20such%20as%20achieving%0Atrue%20zero-shot%20generalization%20for%20unseen%20images%20and%20accurately%20modeling%20the%0Acomplex%2C%20subjective%20aspects%20of%20perception.%20We%20discuss%20the%20need%20for%20diverse%0Adatasets%2C%20refined%20evaluation%20metrics%20aligned%20with%20human%20perceptual%20judgments%2C%0Aand%20compositional%20representations%20that%20strengthen%20model%20robustness%20and%0Ageneralizability.%20Ethical%20issues%2C%20including%20privacy%2C%20consent%2C%20and%20potential%0Amisuse%2C%20are%20underscored%20as%20critical%20considerations%20for%20responsible%20development.%0AVisual%20image%20reconstruction%20offers%20promising%20insights%20into%20neural%20coding%20and%0Aenables%20new%20psychological%20measurements%20of%20visual%20experiences%2C%20with%20applications%0Aspanning%20clinical%20diagnostics%20and%20brain-machine%20interfaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Image%2520Reconstruction%2520from%2520Brain%2520Activity%2520via%2520Latent%250A%2520%2520Representation%26entry.906535625%3DYukiyasu%2520Kamitani%2520and%2520Misato%2520Tanaka%2520and%2520Ken%2520Shirakawa%26entry.1292438233%3D%2520%2520Visual%2520image%2520reconstruction%252C%2520the%2520decoding%2520of%2520perceptual%2520content%2520from%2520brain%250Aactivity%2520into%2520images%252C%2520has%2520advanced%2520significantly%2520with%2520the%2520integration%2520of%2520deep%250Aneural%2520networks%2520%2528DNNs%2529%2520and%2520generative%2520models.%2520This%2520review%2520traces%2520the%2520field%2527s%250Aevolution%2520from%2520early%2520classification%2520approaches%2520to%2520sophisticated%2520reconstructions%250Athat%2520capture%2520detailed%252C%2520subjective%2520visual%2520experiences%252C%2520emphasizing%2520the%2520roles%2520of%250Ahierarchical%2520latent%2520representations%252C%2520compositional%2520strategies%252C%2520and%2520modular%250Aarchitectures.%2520Despite%2520notable%2520progress%252C%2520challenges%2520remain%252C%2520such%2520as%2520achieving%250Atrue%2520zero-shot%2520generalization%2520for%2520unseen%2520images%2520and%2520accurately%2520modeling%2520the%250Acomplex%252C%2520subjective%2520aspects%2520of%2520perception.%2520We%2520discuss%2520the%2520need%2520for%2520diverse%250Adatasets%252C%2520refined%2520evaluation%2520metrics%2520aligned%2520with%2520human%2520perceptual%2520judgments%252C%250Aand%2520compositional%2520representations%2520that%2520strengthen%2520model%2520robustness%2520and%250Ageneralizability.%2520Ethical%2520issues%252C%2520including%2520privacy%252C%2520consent%252C%2520and%2520potential%250Amisuse%252C%2520are%2520underscored%2520as%2520critical%2520considerations%2520for%2520responsible%2520development.%250AVisual%2520image%2520reconstruction%2520offers%2520promising%2520insights%2520into%2520neural%2520coding%2520and%250Aenables%2520new%2520psychological%2520measurements%2520of%2520visual%2520experiences%252C%2520with%2520applications%250Aspanning%2520clinical%2520diagnostics%2520and%2520brain-machine%2520interfaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Image%20Reconstruction%20from%20Brain%20Activity%20via%20Latent%0A%20%20Representation&entry.906535625=Yukiyasu%20Kamitani%20and%20Misato%20Tanaka%20and%20Ken%20Shirakawa&entry.1292438233=%20%20Visual%20image%20reconstruction%2C%20the%20decoding%20of%20perceptual%20content%20from%20brain%0Aactivity%20into%20images%2C%20has%20advanced%20significantly%20with%20the%20integration%20of%20deep%0Aneural%20networks%20%28DNNs%29%20and%20generative%20models.%20This%20review%20traces%20the%20field%27s%0Aevolution%20from%20early%20classification%20approaches%20to%20sophisticated%20reconstructions%0Athat%20capture%20detailed%2C%20subjective%20visual%20experiences%2C%20emphasizing%20the%20roles%20of%0Ahierarchical%20latent%20representations%2C%20compositional%20strategies%2C%20and%20modular%0Aarchitectures.%20Despite%20notable%20progress%2C%20challenges%20remain%2C%20such%20as%20achieving%0Atrue%20zero-shot%20generalization%20for%20unseen%20images%20and%20accurately%20modeling%20the%0Acomplex%2C%20subjective%20aspects%20of%20perception.%20We%20discuss%20the%20need%20for%20diverse%0Adatasets%2C%20refined%20evaluation%20metrics%20aligned%20with%20human%20perceptual%20judgments%2C%0Aand%20compositional%20representations%20that%20strengthen%20model%20robustness%20and%0Ageneralizability.%20Ethical%20issues%2C%20including%20privacy%2C%20consent%2C%20and%20potential%0Amisuse%2C%20are%20underscored%20as%20critical%20considerations%20for%20responsible%20development.%0AVisual%20image%20reconstruction%20offers%20promising%20insights%20into%20neural%20coding%20and%0Aenables%20new%20psychological%20measurements%20of%20visual%20experiences%2C%20with%20applications%0Aspanning%20clinical%20diagnostics%20and%20brain-machine%20interfaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08429v1&entry.124074799=Read"},
{"title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality", "author": "Saurabh Dash and Yiyang Nan and John Dang and Arash Ahmadian and Shivalika Singh and Madeline Smith and Bharat Venkitesh and Vlad Shmyhlo and Viraat Aryabumi and Walter Beller-Morales and Jeremy Pekmez and Jason Ozuzu and Pierre Richemond and Acyr Locatelli and Nick Frosst and Phil Blunsom and Aidan Gomez and Ivan Zhang and Marzieh Fadaee and Manoj Govindassamy and Sudip Roy and Matthias Gall\u00e9 and Beyza Ermis and Ahmet \u00dcst\u00fcn and Sara Hooker", "abstract": "  Building multimodal language models is fundamentally challenging: it requires\naligning vision and language modalities, curating high-quality instruction\ndata, and avoiding the degradation of existing text-only capabilities once\nvision is introduced. These difficulties are further magnified in the\nmultilingual setting, where the need for multimodal data in different languages\nexacerbates existing data scarcity, machine translation often distorts meaning,\nand catastrophic forgetting is more pronounced. To address the aforementioned\nchallenges, we introduce novel techniques spanning both data and modeling.\nFirst, we develop a synthetic annotation framework that curates high-quality,\ndiverse multilingual multimodal instruction data, enabling Aya Vision models to\nproduce natural, human-preferred responses to multimodal inputs across many\nlanguages. Complementing this, we propose a cross-modal model merging technique\nthat mitigates catastrophic forgetting, effectively preserving text-only\ncapabilities while simultaneously enhancing multimodal generative performance.\nAya-Vision-8B achieves best-in-class performance compared to strong multimodal\nmodels such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger\nLlama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which\noutperforms models more than twice its size, such as Molmo-72B and\nLLaMA-3.2-90B-Vision. Our work advances multilingual progress on the\nmulti-modal frontier, and provides insights into techniques that effectively\nbend the need for compute while delivering extremely high performance.\n", "link": "http://arxiv.org/abs/2505.08751v1", "date": "2025-05-13", "relevancy": 2.8501, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5747}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aya%20Vision%3A%20Advancing%20the%20Frontier%20of%20Multilingual%20Multimodality&body=Title%3A%20Aya%20Vision%3A%20Advancing%20the%20Frontier%20of%20Multilingual%20Multimodality%0AAuthor%3A%20Saurabh%20Dash%20and%20Yiyang%20Nan%20and%20John%20Dang%20and%20Arash%20Ahmadian%20and%20Shivalika%20Singh%20and%20Madeline%20Smith%20and%20Bharat%20Venkitesh%20and%20Vlad%20Shmyhlo%20and%20Viraat%20Aryabumi%20and%20Walter%20Beller-Morales%20and%20Jeremy%20Pekmez%20and%20Jason%20Ozuzu%20and%20Pierre%20Richemond%20and%20Acyr%20Locatelli%20and%20Nick%20Frosst%20and%20Phil%20Blunsom%20and%20Aidan%20Gomez%20and%20Ivan%20Zhang%20and%20Marzieh%20Fadaee%20and%20Manoj%20Govindassamy%20and%20Sudip%20Roy%20and%20Matthias%20Gall%C3%A9%20and%20Beyza%20Ermis%20and%20Ahmet%20%C3%9Cst%C3%BCn%20and%20Sara%20Hooker%0AAbstract%3A%20%20%20Building%20multimodal%20language%20models%20is%20fundamentally%20challenging%3A%20it%20requires%0Aaligning%20vision%20and%20language%20modalities%2C%20curating%20high-quality%20instruction%0Adata%2C%20and%20avoiding%20the%20degradation%20of%20existing%20text-only%20capabilities%20once%0Avision%20is%20introduced.%20These%20difficulties%20are%20further%20magnified%20in%20the%0Amultilingual%20setting%2C%20where%20the%20need%20for%20multimodal%20data%20in%20different%20languages%0Aexacerbates%20existing%20data%20scarcity%2C%20machine%20translation%20often%20distorts%20meaning%2C%0Aand%20catastrophic%20forgetting%20is%20more%20pronounced.%20To%20address%20the%20aforementioned%0Achallenges%2C%20we%20introduce%20novel%20techniques%20spanning%20both%20data%20and%20modeling.%0AFirst%2C%20we%20develop%20a%20synthetic%20annotation%20framework%20that%20curates%20high-quality%2C%0Adiverse%20multilingual%20multimodal%20instruction%20data%2C%20enabling%20Aya%20Vision%20models%20to%0Aproduce%20natural%2C%20human-preferred%20responses%20to%20multimodal%20inputs%20across%20many%0Alanguages.%20Complementing%20this%2C%20we%20propose%20a%20cross-modal%20model%20merging%20technique%0Athat%20mitigates%20catastrophic%20forgetting%2C%20effectively%20preserving%20text-only%0Acapabilities%20while%20simultaneously%20enhancing%20multimodal%20generative%20performance.%0AAya-Vision-8B%20achieves%20best-in-class%20performance%20compared%20to%20strong%20multimodal%0Amodels%20such%20as%20Qwen-2.5-VL-7B%2C%20Pixtral-12B%2C%20and%20even%20much%20larger%0ALlama-3.2-90B-Vision.%20We%20further%20scale%20this%20approach%20with%20Aya-Vision-32B%2C%20which%0Aoutperforms%20models%20more%20than%20twice%20its%20size%2C%20such%20as%20Molmo-72B%20and%0ALLaMA-3.2-90B-Vision.%20Our%20work%20advances%20multilingual%20progress%20on%20the%0Amulti-modal%20frontier%2C%20and%20provides%20insights%20into%20techniques%20that%20effectively%0Abend%20the%20need%20for%20compute%20while%20delivering%20extremely%20high%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAya%2520Vision%253A%2520Advancing%2520the%2520Frontier%2520of%2520Multilingual%2520Multimodality%26entry.906535625%3DSaurabh%2520Dash%2520and%2520Yiyang%2520Nan%2520and%2520John%2520Dang%2520and%2520Arash%2520Ahmadian%2520and%2520Shivalika%2520Singh%2520and%2520Madeline%2520Smith%2520and%2520Bharat%2520Venkitesh%2520and%2520Vlad%2520Shmyhlo%2520and%2520Viraat%2520Aryabumi%2520and%2520Walter%2520Beller-Morales%2520and%2520Jeremy%2520Pekmez%2520and%2520Jason%2520Ozuzu%2520and%2520Pierre%2520Richemond%2520and%2520Acyr%2520Locatelli%2520and%2520Nick%2520Frosst%2520and%2520Phil%2520Blunsom%2520and%2520Aidan%2520Gomez%2520and%2520Ivan%2520Zhang%2520and%2520Marzieh%2520Fadaee%2520and%2520Manoj%2520Govindassamy%2520and%2520Sudip%2520Roy%2520and%2520Matthias%2520Gall%25C3%25A9%2520and%2520Beyza%2520Ermis%2520and%2520Ahmet%2520%25C3%259Cst%25C3%25BCn%2520and%2520Sara%2520Hooker%26entry.1292438233%3D%2520%2520Building%2520multimodal%2520language%2520models%2520is%2520fundamentally%2520challenging%253A%2520it%2520requires%250Aaligning%2520vision%2520and%2520language%2520modalities%252C%2520curating%2520high-quality%2520instruction%250Adata%252C%2520and%2520avoiding%2520the%2520degradation%2520of%2520existing%2520text-only%2520capabilities%2520once%250Avision%2520is%2520introduced.%2520These%2520difficulties%2520are%2520further%2520magnified%2520in%2520the%250Amultilingual%2520setting%252C%2520where%2520the%2520need%2520for%2520multimodal%2520data%2520in%2520different%2520languages%250Aexacerbates%2520existing%2520data%2520scarcity%252C%2520machine%2520translation%2520often%2520distorts%2520meaning%252C%250Aand%2520catastrophic%2520forgetting%2520is%2520more%2520pronounced.%2520To%2520address%2520the%2520aforementioned%250Achallenges%252C%2520we%2520introduce%2520novel%2520techniques%2520spanning%2520both%2520data%2520and%2520modeling.%250AFirst%252C%2520we%2520develop%2520a%2520synthetic%2520annotation%2520framework%2520that%2520curates%2520high-quality%252C%250Adiverse%2520multilingual%2520multimodal%2520instruction%2520data%252C%2520enabling%2520Aya%2520Vision%2520models%2520to%250Aproduce%2520natural%252C%2520human-preferred%2520responses%2520to%2520multimodal%2520inputs%2520across%2520many%250Alanguages.%2520Complementing%2520this%252C%2520we%2520propose%2520a%2520cross-modal%2520model%2520merging%2520technique%250Athat%2520mitigates%2520catastrophic%2520forgetting%252C%2520effectively%2520preserving%2520text-only%250Acapabilities%2520while%2520simultaneously%2520enhancing%2520multimodal%2520generative%2520performance.%250AAya-Vision-8B%2520achieves%2520best-in-class%2520performance%2520compared%2520to%2520strong%2520multimodal%250Amodels%2520such%2520as%2520Qwen-2.5-VL-7B%252C%2520Pixtral-12B%252C%2520and%2520even%2520much%2520larger%250ALlama-3.2-90B-Vision.%2520We%2520further%2520scale%2520this%2520approach%2520with%2520Aya-Vision-32B%252C%2520which%250Aoutperforms%2520models%2520more%2520than%2520twice%2520its%2520size%252C%2520such%2520as%2520Molmo-72B%2520and%250ALLaMA-3.2-90B-Vision.%2520Our%2520work%2520advances%2520multilingual%2520progress%2520on%2520the%250Amulti-modal%2520frontier%252C%2520and%2520provides%2520insights%2520into%2520techniques%2520that%2520effectively%250Abend%2520the%2520need%2520for%2520compute%2520while%2520delivering%2520extremely%2520high%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aya%20Vision%3A%20Advancing%20the%20Frontier%20of%20Multilingual%20Multimodality&entry.906535625=Saurabh%20Dash%20and%20Yiyang%20Nan%20and%20John%20Dang%20and%20Arash%20Ahmadian%20and%20Shivalika%20Singh%20and%20Madeline%20Smith%20and%20Bharat%20Venkitesh%20and%20Vlad%20Shmyhlo%20and%20Viraat%20Aryabumi%20and%20Walter%20Beller-Morales%20and%20Jeremy%20Pekmez%20and%20Jason%20Ozuzu%20and%20Pierre%20Richemond%20and%20Acyr%20Locatelli%20and%20Nick%20Frosst%20and%20Phil%20Blunsom%20and%20Aidan%20Gomez%20and%20Ivan%20Zhang%20and%20Marzieh%20Fadaee%20and%20Manoj%20Govindassamy%20and%20Sudip%20Roy%20and%20Matthias%20Gall%C3%A9%20and%20Beyza%20Ermis%20and%20Ahmet%20%C3%9Cst%C3%BCn%20and%20Sara%20Hooker&entry.1292438233=%20%20Building%20multimodal%20language%20models%20is%20fundamentally%20challenging%3A%20it%20requires%0Aaligning%20vision%20and%20language%20modalities%2C%20curating%20high-quality%20instruction%0Adata%2C%20and%20avoiding%20the%20degradation%20of%20existing%20text-only%20capabilities%20once%0Avision%20is%20introduced.%20These%20difficulties%20are%20further%20magnified%20in%20the%0Amultilingual%20setting%2C%20where%20the%20need%20for%20multimodal%20data%20in%20different%20languages%0Aexacerbates%20existing%20data%20scarcity%2C%20machine%20translation%20often%20distorts%20meaning%2C%0Aand%20catastrophic%20forgetting%20is%20more%20pronounced.%20To%20address%20the%20aforementioned%0Achallenges%2C%20we%20introduce%20novel%20techniques%20spanning%20both%20data%20and%20modeling.%0AFirst%2C%20we%20develop%20a%20synthetic%20annotation%20framework%20that%20curates%20high-quality%2C%0Adiverse%20multilingual%20multimodal%20instruction%20data%2C%20enabling%20Aya%20Vision%20models%20to%0Aproduce%20natural%2C%20human-preferred%20responses%20to%20multimodal%20inputs%20across%20many%0Alanguages.%20Complementing%20this%2C%20we%20propose%20a%20cross-modal%20model%20merging%20technique%0Athat%20mitigates%20catastrophic%20forgetting%2C%20effectively%20preserving%20text-only%0Acapabilities%20while%20simultaneously%20enhancing%20multimodal%20generative%20performance.%0AAya-Vision-8B%20achieves%20best-in-class%20performance%20compared%20to%20strong%20multimodal%0Amodels%20such%20as%20Qwen-2.5-VL-7B%2C%20Pixtral-12B%2C%20and%20even%20much%20larger%0ALlama-3.2-90B-Vision.%20We%20further%20scale%20this%20approach%20with%20Aya-Vision-32B%2C%20which%0Aoutperforms%20models%20more%20than%20twice%20its%20size%2C%20such%20as%20Molmo-72B%20and%0ALLaMA-3.2-90B-Vision.%20Our%20work%20advances%20multilingual%20progress%20on%20the%0Amulti-modal%20frontier%2C%20and%20provides%20insights%20into%20techniques%20that%20effectively%0Abend%20the%20need%20for%20compute%20while%20delivering%20extremely%20high%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08751v1&entry.124074799=Read"},
{"title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin\n  Benchmark Dataset", "author": "Olaf Wysocki and Benedikt Schwab and Manoj Kumar Biswanath and Michael Greza and Qilin Zhang and Jingwei Zhu and Thomas Froech and Medhini Heeramaglore and Ihab Hijazi and Khaoula Kanna and Mathias Pechinger and Zhaiyu Chen and Yao Sun and Alejandro Rueda Segura and Ziyang Xu and Omar AbdelGafar and Mansour Mehranfar and Chandan Yeshwanth and Yueh-Cheng Liu and Hadi Yazdi and Jiapan Wang and Stefan Auer and Katharina Anders and Klaus Bogenberger and Andre Borrmann and Angela Dai and Ludwig Hoegner and Christoph Holst and Thomas H. Kolbe and Ferdinand Ludwig and Matthias Nie\u00dfner and Frank Petzold and Xiao Xiang Zhu and Boris Jutzi", "abstract": "  Urban Digital Twins (UDTs) have become essential for managing cities and\nintegrating complex, heterogeneous data from diverse sources. Creating UDTs\ninvolves challenges at multiple process stages, including acquiring accurate 3D\nsource data, reconstructing high-fidelity 3D models, maintaining models'\nupdates, and ensuring seamless interoperability to downstream tasks. Current\ndatasets are usually limited to one part of the processing chain, hampering\ncomprehensive UDTs validation. To address these challenges, we introduce the\nfirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.\nThis dataset includes georeferenced, semantically aligned 3D models and\nnetworks along with various terrestrial, mobile, aerial, and satellite\nobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently\n767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high\naccuracy, and multimodal data integration, the benchmark supports robust\nanalysis of sensors and the development of advanced reconstruction methods.\nAdditionally, we explore downstream tasks demonstrating the potential of\nTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar\npotential analysis, point cloud semantic segmentation, and LoD3 building\nreconstruction. We are convinced this contribution lays a foundation for\novercoming current limitations in UDT creation, fostering new research\ndirections and practical solutions for smarter, data-driven urban environments.\nThe project is available under: https://tum2t.win\n", "link": "http://arxiv.org/abs/2505.07396v2", "date": "2025-05-13", "relevancy": 2.8242, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5812}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5812}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TUM2TWIN%3A%20Introducing%20the%20Large-Scale%20Multimodal%20Urban%20Digital%20Twin%0A%20%20Benchmark%20Dataset&body=Title%3A%20TUM2TWIN%3A%20Introducing%20the%20Large-Scale%20Multimodal%20Urban%20Digital%20Twin%0A%20%20Benchmark%20Dataset%0AAuthor%3A%20Olaf%20Wysocki%20and%20Benedikt%20Schwab%20and%20Manoj%20Kumar%20Biswanath%20and%20Michael%20Greza%20and%20Qilin%20Zhang%20and%20Jingwei%20Zhu%20and%20Thomas%20Froech%20and%20Medhini%20Heeramaglore%20and%20Ihab%20Hijazi%20and%20Khaoula%20Kanna%20and%20Mathias%20Pechinger%20and%20Zhaiyu%20Chen%20and%20Yao%20Sun%20and%20Alejandro%20Rueda%20Segura%20and%20Ziyang%20Xu%20and%20Omar%20AbdelGafar%20and%20Mansour%20Mehranfar%20and%20Chandan%20Yeshwanth%20and%20Yueh-Cheng%20Liu%20and%20Hadi%20Yazdi%20and%20Jiapan%20Wang%20and%20Stefan%20Auer%20and%20Katharina%20Anders%20and%20Klaus%20Bogenberger%20and%20Andre%20Borrmann%20and%20Angela%20Dai%20and%20Ludwig%20Hoegner%20and%20Christoph%20Holst%20and%20Thomas%20H.%20Kolbe%20and%20Ferdinand%20Ludwig%20and%20Matthias%20Nie%C3%9Fner%20and%20Frank%20Petzold%20and%20Xiao%20Xiang%20Zhu%20and%20Boris%20Jutzi%0AAbstract%3A%20%20%20Urban%20Digital%20Twins%20%28UDTs%29%20have%20become%20essential%20for%20managing%20cities%20and%0Aintegrating%20complex%2C%20heterogeneous%20data%20from%20diverse%20sources.%20Creating%20UDTs%0Ainvolves%20challenges%20at%20multiple%20process%20stages%2C%20including%20acquiring%20accurate%203D%0Asource%20data%2C%20reconstructing%20high-fidelity%203D%20models%2C%20maintaining%20models%27%0Aupdates%2C%20and%20ensuring%20seamless%20interoperability%20to%20downstream%20tasks.%20Current%0Adatasets%20are%20usually%20limited%20to%20one%20part%20of%20the%20processing%20chain%2C%20hampering%0Acomprehensive%20UDTs%20validation.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%0Afirst%20comprehensive%20multimodal%20Urban%20Digital%20Twin%20benchmark%20dataset%3A%20TUM2TWIN.%0AThis%20dataset%20includes%20georeferenced%2C%20semantically%20aligned%203D%20models%20and%0Anetworks%20along%20with%20various%20terrestrial%2C%20mobile%2C%20aerial%2C%20and%20satellite%0Aobservations%20boasting%2032%20data%20subsets%20over%20roughly%20100%2C000%20%24m%5E2%24%20and%20currently%0A767%20GB%20of%20data.%20By%20ensuring%20georeferenced%20indoor-outdoor%20acquisition%2C%20high%0Aaccuracy%2C%20and%20multimodal%20data%20integration%2C%20the%20benchmark%20supports%20robust%0Aanalysis%20of%20sensors%20and%20the%20development%20of%20advanced%20reconstruction%20methods.%0AAdditionally%2C%20we%20explore%20downstream%20tasks%20demonstrating%20the%20potential%20of%0ATUM2TWIN%2C%20including%20novel%20view%20synthesis%20of%20NeRF%20and%20Gaussian%20Splatting%2C%20solar%0Apotential%20analysis%2C%20point%20cloud%20semantic%20segmentation%2C%20and%20LoD3%20building%0Areconstruction.%20We%20are%20convinced%20this%20contribution%20lays%20a%20foundation%20for%0Aovercoming%20current%20limitations%20in%20UDT%20creation%2C%20fostering%20new%20research%0Adirections%20and%20practical%20solutions%20for%20smarter%2C%20data-driven%20urban%20environments.%0AThe%20project%20is%20available%20under%3A%20https%3A//tum2t.win%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07396v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTUM2TWIN%253A%2520Introducing%2520the%2520Large-Scale%2520Multimodal%2520Urban%2520Digital%2520Twin%250A%2520%2520Benchmark%2520Dataset%26entry.906535625%3DOlaf%2520Wysocki%2520and%2520Benedikt%2520Schwab%2520and%2520Manoj%2520Kumar%2520Biswanath%2520and%2520Michael%2520Greza%2520and%2520Qilin%2520Zhang%2520and%2520Jingwei%2520Zhu%2520and%2520Thomas%2520Froech%2520and%2520Medhini%2520Heeramaglore%2520and%2520Ihab%2520Hijazi%2520and%2520Khaoula%2520Kanna%2520and%2520Mathias%2520Pechinger%2520and%2520Zhaiyu%2520Chen%2520and%2520Yao%2520Sun%2520and%2520Alejandro%2520Rueda%2520Segura%2520and%2520Ziyang%2520Xu%2520and%2520Omar%2520AbdelGafar%2520and%2520Mansour%2520Mehranfar%2520and%2520Chandan%2520Yeshwanth%2520and%2520Yueh-Cheng%2520Liu%2520and%2520Hadi%2520Yazdi%2520and%2520Jiapan%2520Wang%2520and%2520Stefan%2520Auer%2520and%2520Katharina%2520Anders%2520and%2520Klaus%2520Bogenberger%2520and%2520Andre%2520Borrmann%2520and%2520Angela%2520Dai%2520and%2520Ludwig%2520Hoegner%2520and%2520Christoph%2520Holst%2520and%2520Thomas%2520H.%2520Kolbe%2520and%2520Ferdinand%2520Ludwig%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Frank%2520Petzold%2520and%2520Xiao%2520Xiang%2520Zhu%2520and%2520Boris%2520Jutzi%26entry.1292438233%3D%2520%2520Urban%2520Digital%2520Twins%2520%2528UDTs%2529%2520have%2520become%2520essential%2520for%2520managing%2520cities%2520and%250Aintegrating%2520complex%252C%2520heterogeneous%2520data%2520from%2520diverse%2520sources.%2520Creating%2520UDTs%250Ainvolves%2520challenges%2520at%2520multiple%2520process%2520stages%252C%2520including%2520acquiring%2520accurate%25203D%250Asource%2520data%252C%2520reconstructing%2520high-fidelity%25203D%2520models%252C%2520maintaining%2520models%2527%250Aupdates%252C%2520and%2520ensuring%2520seamless%2520interoperability%2520to%2520downstream%2520tasks.%2520Current%250Adatasets%2520are%2520usually%2520limited%2520to%2520one%2520part%2520of%2520the%2520processing%2520chain%252C%2520hampering%250Acomprehensive%2520UDTs%2520validation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520the%250Afirst%2520comprehensive%2520multimodal%2520Urban%2520Digital%2520Twin%2520benchmark%2520dataset%253A%2520TUM2TWIN.%250AThis%2520dataset%2520includes%2520georeferenced%252C%2520semantically%2520aligned%25203D%2520models%2520and%250Anetworks%2520along%2520with%2520various%2520terrestrial%252C%2520mobile%252C%2520aerial%252C%2520and%2520satellite%250Aobservations%2520boasting%252032%2520data%2520subsets%2520over%2520roughly%2520100%252C000%2520%2524m%255E2%2524%2520and%2520currently%250A767%2520GB%2520of%2520data.%2520By%2520ensuring%2520georeferenced%2520indoor-outdoor%2520acquisition%252C%2520high%250Aaccuracy%252C%2520and%2520multimodal%2520data%2520integration%252C%2520the%2520benchmark%2520supports%2520robust%250Aanalysis%2520of%2520sensors%2520and%2520the%2520development%2520of%2520advanced%2520reconstruction%2520methods.%250AAdditionally%252C%2520we%2520explore%2520downstream%2520tasks%2520demonstrating%2520the%2520potential%2520of%250ATUM2TWIN%252C%2520including%2520novel%2520view%2520synthesis%2520of%2520NeRF%2520and%2520Gaussian%2520Splatting%252C%2520solar%250Apotential%2520analysis%252C%2520point%2520cloud%2520semantic%2520segmentation%252C%2520and%2520LoD3%2520building%250Areconstruction.%2520We%2520are%2520convinced%2520this%2520contribution%2520lays%2520a%2520foundation%2520for%250Aovercoming%2520current%2520limitations%2520in%2520UDT%2520creation%252C%2520fostering%2520new%2520research%250Adirections%2520and%2520practical%2520solutions%2520for%2520smarter%252C%2520data-driven%2520urban%2520environments.%250AThe%2520project%2520is%2520available%2520under%253A%2520https%253A//tum2t.win%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07396v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TUM2TWIN%3A%20Introducing%20the%20Large-Scale%20Multimodal%20Urban%20Digital%20Twin%0A%20%20Benchmark%20Dataset&entry.906535625=Olaf%20Wysocki%20and%20Benedikt%20Schwab%20and%20Manoj%20Kumar%20Biswanath%20and%20Michael%20Greza%20and%20Qilin%20Zhang%20and%20Jingwei%20Zhu%20and%20Thomas%20Froech%20and%20Medhini%20Heeramaglore%20and%20Ihab%20Hijazi%20and%20Khaoula%20Kanna%20and%20Mathias%20Pechinger%20and%20Zhaiyu%20Chen%20and%20Yao%20Sun%20and%20Alejandro%20Rueda%20Segura%20and%20Ziyang%20Xu%20and%20Omar%20AbdelGafar%20and%20Mansour%20Mehranfar%20and%20Chandan%20Yeshwanth%20and%20Yueh-Cheng%20Liu%20and%20Hadi%20Yazdi%20and%20Jiapan%20Wang%20and%20Stefan%20Auer%20and%20Katharina%20Anders%20and%20Klaus%20Bogenberger%20and%20Andre%20Borrmann%20and%20Angela%20Dai%20and%20Ludwig%20Hoegner%20and%20Christoph%20Holst%20and%20Thomas%20H.%20Kolbe%20and%20Ferdinand%20Ludwig%20and%20Matthias%20Nie%C3%9Fner%20and%20Frank%20Petzold%20and%20Xiao%20Xiang%20Zhu%20and%20Boris%20Jutzi&entry.1292438233=%20%20Urban%20Digital%20Twins%20%28UDTs%29%20have%20become%20essential%20for%20managing%20cities%20and%0Aintegrating%20complex%2C%20heterogeneous%20data%20from%20diverse%20sources.%20Creating%20UDTs%0Ainvolves%20challenges%20at%20multiple%20process%20stages%2C%20including%20acquiring%20accurate%203D%0Asource%20data%2C%20reconstructing%20high-fidelity%203D%20models%2C%20maintaining%20models%27%0Aupdates%2C%20and%20ensuring%20seamless%20interoperability%20to%20downstream%20tasks.%20Current%0Adatasets%20are%20usually%20limited%20to%20one%20part%20of%20the%20processing%20chain%2C%20hampering%0Acomprehensive%20UDTs%20validation.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%0Afirst%20comprehensive%20multimodal%20Urban%20Digital%20Twin%20benchmark%20dataset%3A%20TUM2TWIN.%0AThis%20dataset%20includes%20georeferenced%2C%20semantically%20aligned%203D%20models%20and%0Anetworks%20along%20with%20various%20terrestrial%2C%20mobile%2C%20aerial%2C%20and%20satellite%0Aobservations%20boasting%2032%20data%20subsets%20over%20roughly%20100%2C000%20%24m%5E2%24%20and%20currently%0A767%20GB%20of%20data.%20By%20ensuring%20georeferenced%20indoor-outdoor%20acquisition%2C%20high%0Aaccuracy%2C%20and%20multimodal%20data%20integration%2C%20the%20benchmark%20supports%20robust%0Aanalysis%20of%20sensors%20and%20the%20development%20of%20advanced%20reconstruction%20methods.%0AAdditionally%2C%20we%20explore%20downstream%20tasks%20demonstrating%20the%20potential%20of%0ATUM2TWIN%2C%20including%20novel%20view%20synthesis%20of%20NeRF%20and%20Gaussian%20Splatting%2C%20solar%0Apotential%20analysis%2C%20point%20cloud%20semantic%20segmentation%2C%20and%20LoD3%20building%0Areconstruction.%20We%20are%20convinced%20this%20contribution%20lays%20a%20foundation%20for%0Aovercoming%20current%20limitations%20in%20UDT%20creation%2C%20fostering%20new%20research%0Adirections%20and%20practical%20solutions%20for%20smarter%2C%20data-driven%20urban%20environments.%0AThe%20project%20is%20available%20under%3A%20https%3A//tum2t.win%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07396v2&entry.124074799=Read"},
{"title": "AniSora: Exploring the Frontiers of Animation Video Generation in the\n  Sora Era", "author": "Yudong Jiang and Baohan Xu and Siqian Yang and Mingyu Yin and Jing Liu and Chao Xu and Siqi Wang and Yidi Wu and Bingwen Zhu and Xinwen Zhang and Xingyu Zheng and Jixuan Xu and Yue Zhang and Jinlong Hou and Huyang Sun", "abstract": "  Animation has gained significant interest in the recent film and TV industry.\nDespite the success of advanced video generation models like Sora, Kling, and\nCogVideoX in generating natural videos, they lack the same effectiveness in\nhandling animation videos. Evaluating animation video generation is also a\ngreat challenge due to its unique artist styles, violating the laws of physics\nand exaggerated motions. In this paper, we present a comprehensive system,\nAniSora, designed for animation video generation, which includes a data\nprocessing pipeline, a controllable generation model, and an evaluation\nbenchmark. Supported by the data processing pipeline with over 10M high-quality\ndata, the generation model incorporates a spatiotemporal mask module to\nfacilitate key animation production functions such as image-to-video\ngeneration, frame interpolation, and localized image-guided animation. We also\ncollect an evaluation benchmark of 948 various animation videos, with\nspecifically developed metrics for animation video generation. Our entire\nproject is publicly available on\nhttps://github.com/bilibili/Index-anisora/tree/main.\n", "link": "http://arxiv.org/abs/2412.10255v4", "date": "2025-05-13", "relevancy": 2.8162, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5737}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5601}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AniSora%3A%20Exploring%20the%20Frontiers%20of%20Animation%20Video%20Generation%20in%20the%0A%20%20Sora%20Era&body=Title%3A%20AniSora%3A%20Exploring%20the%20Frontiers%20of%20Animation%20Video%20Generation%20in%20the%0A%20%20Sora%20Era%0AAuthor%3A%20Yudong%20Jiang%20and%20Baohan%20Xu%20and%20Siqian%20Yang%20and%20Mingyu%20Yin%20and%20Jing%20Liu%20and%20Chao%20Xu%20and%20Siqi%20Wang%20and%20Yidi%20Wu%20and%20Bingwen%20Zhu%20and%20Xinwen%20Zhang%20and%20Xingyu%20Zheng%20and%20Jixuan%20Xu%20and%20Yue%20Zhang%20and%20Jinlong%20Hou%20and%20Huyang%20Sun%0AAbstract%3A%20%20%20Animation%20has%20gained%20significant%20interest%20in%20the%20recent%20film%20and%20TV%20industry.%0ADespite%20the%20success%20of%20advanced%20video%20generation%20models%20like%20Sora%2C%20Kling%2C%20and%0ACogVideoX%20in%20generating%20natural%20videos%2C%20they%20lack%20the%20same%20effectiveness%20in%0Ahandling%20animation%20videos.%20Evaluating%20animation%20video%20generation%20is%20also%20a%0Agreat%20challenge%20due%20to%20its%20unique%20artist%20styles%2C%20violating%20the%20laws%20of%20physics%0Aand%20exaggerated%20motions.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20system%2C%0AAniSora%2C%20designed%20for%20animation%20video%20generation%2C%20which%20includes%20a%20data%0Aprocessing%20pipeline%2C%20a%20controllable%20generation%20model%2C%20and%20an%20evaluation%0Abenchmark.%20Supported%20by%20the%20data%20processing%20pipeline%20with%20over%2010M%20high-quality%0Adata%2C%20the%20generation%20model%20incorporates%20a%20spatiotemporal%20mask%20module%20to%0Afacilitate%20key%20animation%20production%20functions%20such%20as%20image-to-video%0Ageneration%2C%20frame%20interpolation%2C%20and%20localized%20image-guided%20animation.%20We%20also%0Acollect%20an%20evaluation%20benchmark%20of%20948%20various%20animation%20videos%2C%20with%0Aspecifically%20developed%20metrics%20for%20animation%20video%20generation.%20Our%20entire%0Aproject%20is%20publicly%20available%20on%0Ahttps%3A//github.com/bilibili/Index-anisora/tree/main.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10255v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAniSora%253A%2520Exploring%2520the%2520Frontiers%2520of%2520Animation%2520Video%2520Generation%2520in%2520the%250A%2520%2520Sora%2520Era%26entry.906535625%3DYudong%2520Jiang%2520and%2520Baohan%2520Xu%2520and%2520Siqian%2520Yang%2520and%2520Mingyu%2520Yin%2520and%2520Jing%2520Liu%2520and%2520Chao%2520Xu%2520and%2520Siqi%2520Wang%2520and%2520Yidi%2520Wu%2520and%2520Bingwen%2520Zhu%2520and%2520Xinwen%2520Zhang%2520and%2520Xingyu%2520Zheng%2520and%2520Jixuan%2520Xu%2520and%2520Yue%2520Zhang%2520and%2520Jinlong%2520Hou%2520and%2520Huyang%2520Sun%26entry.1292438233%3D%2520%2520Animation%2520has%2520gained%2520significant%2520interest%2520in%2520the%2520recent%2520film%2520and%2520TV%2520industry.%250ADespite%2520the%2520success%2520of%2520advanced%2520video%2520generation%2520models%2520like%2520Sora%252C%2520Kling%252C%2520and%250ACogVideoX%2520in%2520generating%2520natural%2520videos%252C%2520they%2520lack%2520the%2520same%2520effectiveness%2520in%250Ahandling%2520animation%2520videos.%2520Evaluating%2520animation%2520video%2520generation%2520is%2520also%2520a%250Agreat%2520challenge%2520due%2520to%2520its%2520unique%2520artist%2520styles%252C%2520violating%2520the%2520laws%2520of%2520physics%250Aand%2520exaggerated%2520motions.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520system%252C%250AAniSora%252C%2520designed%2520for%2520animation%2520video%2520generation%252C%2520which%2520includes%2520a%2520data%250Aprocessing%2520pipeline%252C%2520a%2520controllable%2520generation%2520model%252C%2520and%2520an%2520evaluation%250Abenchmark.%2520Supported%2520by%2520the%2520data%2520processing%2520pipeline%2520with%2520over%252010M%2520high-quality%250Adata%252C%2520the%2520generation%2520model%2520incorporates%2520a%2520spatiotemporal%2520mask%2520module%2520to%250Afacilitate%2520key%2520animation%2520production%2520functions%2520such%2520as%2520image-to-video%250Ageneration%252C%2520frame%2520interpolation%252C%2520and%2520localized%2520image-guided%2520animation.%2520We%2520also%250Acollect%2520an%2520evaluation%2520benchmark%2520of%2520948%2520various%2520animation%2520videos%252C%2520with%250Aspecifically%2520developed%2520metrics%2520for%2520animation%2520video%2520generation.%2520Our%2520entire%250Aproject%2520is%2520publicly%2520available%2520on%250Ahttps%253A//github.com/bilibili/Index-anisora/tree/main.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10255v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AniSora%3A%20Exploring%20the%20Frontiers%20of%20Animation%20Video%20Generation%20in%20the%0A%20%20Sora%20Era&entry.906535625=Yudong%20Jiang%20and%20Baohan%20Xu%20and%20Siqian%20Yang%20and%20Mingyu%20Yin%20and%20Jing%20Liu%20and%20Chao%20Xu%20and%20Siqi%20Wang%20and%20Yidi%20Wu%20and%20Bingwen%20Zhu%20and%20Xinwen%20Zhang%20and%20Xingyu%20Zheng%20and%20Jixuan%20Xu%20and%20Yue%20Zhang%20and%20Jinlong%20Hou%20and%20Huyang%20Sun&entry.1292438233=%20%20Animation%20has%20gained%20significant%20interest%20in%20the%20recent%20film%20and%20TV%20industry.%0ADespite%20the%20success%20of%20advanced%20video%20generation%20models%20like%20Sora%2C%20Kling%2C%20and%0ACogVideoX%20in%20generating%20natural%20videos%2C%20they%20lack%20the%20same%20effectiveness%20in%0Ahandling%20animation%20videos.%20Evaluating%20animation%20video%20generation%20is%20also%20a%0Agreat%20challenge%20due%20to%20its%20unique%20artist%20styles%2C%20violating%20the%20laws%20of%20physics%0Aand%20exaggerated%20motions.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20system%2C%0AAniSora%2C%20designed%20for%20animation%20video%20generation%2C%20which%20includes%20a%20data%0Aprocessing%20pipeline%2C%20a%20controllable%20generation%20model%2C%20and%20an%20evaluation%0Abenchmark.%20Supported%20by%20the%20data%20processing%20pipeline%20with%20over%2010M%20high-quality%0Adata%2C%20the%20generation%20model%20incorporates%20a%20spatiotemporal%20mask%20module%20to%0Afacilitate%20key%20animation%20production%20functions%20such%20as%20image-to-video%0Ageneration%2C%20frame%20interpolation%2C%20and%20localized%20image-guided%20animation.%20We%20also%0Acollect%20an%20evaluation%20benchmark%20of%20948%20various%20animation%20videos%2C%20with%0Aspecifically%20developed%20metrics%20for%20animation%20video%20generation.%20Our%20entire%0Aproject%20is%20publicly%20available%20on%0Ahttps%3A//github.com/bilibili/Index-anisora/tree/main.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10255v4&entry.124074799=Read"},
{"title": "GBT-SAM: Adapting a Foundational Deep Learning Model for Generalizable\n  Brain Tumor Segmentation via Efficient Integration of Multi-Parametric MRI\n  Data", "author": "Cecilia Diana-Albelda and Roberto Alcover-Couso and \u00c1lvaro Garc\u00eda-Mart\u00edn and Jesus Bescos and Marcos Escudero-Vi\u00f1olo", "abstract": "  Gliomas are aggressive brain tumors that require accurate imaging-based\ndiagnosis, with segmentation playing a critical role in evaluating morphology\nand treatment decisions. Manual delineation of gliomas is time-consuming and\nprone to variability, motivating the use of deep learning to improve\nconsistency and alleviate clinical workload. However, existing methods often\nfail to fully exploit the information available in multi-parametric MRI\n(mp-MRI), particularly inter-slice contextual features, and typically require\nconsiderable computational resources while lacking robustness across tumor type\nvariations. We present GBT-SAM, a parameter-efficient deep learning framework\nthat adapts the Segment Anything Model (SAM), a large-scale vision model, to\nvolumetric mp-MRI data. GBT-SAM reduces input complexity by selecting fewer\nthan 2.6\\% of slices per scan while incorporating all four MRI modalities,\npreserving essential tumor-related information with minimal cost. Furthermore,\nour model is trained by a two-step fine-tuning strategy that incorporates a\ndepth-aware module to capture inter-slice correlations and lightweight\nadaptation layers, resulting in just 6.5M trainable parameters, which is the\nlowest among SAM-based approaches. GBT-SAM achieves a Dice Score of 93.54 on\nthe BraTS Adult Glioma dataset and demonstrates robust performance on\nMeningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. These results\nhighlight GBT-SAM's potential as a computationally efficient and domain-robust\nframework for brain tumor segmentation using mp-MRI. Our code and models are\navailable at https://github.com/vpulab/med-sam-brain .\n", "link": "http://arxiv.org/abs/2503.04325v3", "date": "2025-05-13", "relevancy": 2.7887, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5819}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5623}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GBT-SAM%3A%20Adapting%20a%20Foundational%20Deep%20Learning%20Model%20for%20Generalizable%0A%20%20Brain%20Tumor%20Segmentation%20via%20Efficient%20Integration%20of%20Multi-Parametric%20MRI%0A%20%20Data&body=Title%3A%20GBT-SAM%3A%20Adapting%20a%20Foundational%20Deep%20Learning%20Model%20for%20Generalizable%0A%20%20Brain%20Tumor%20Segmentation%20via%20Efficient%20Integration%20of%20Multi-Parametric%20MRI%0A%20%20Data%0AAuthor%3A%20Cecilia%20Diana-Albelda%20and%20Roberto%20Alcover-Couso%20and%20%C3%81lvaro%20Garc%C3%ADa-Mart%C3%ADn%20and%20Jesus%20Bescos%20and%20Marcos%20Escudero-Vi%C3%B1olo%0AAbstract%3A%20%20%20Gliomas%20are%20aggressive%20brain%20tumors%20that%20require%20accurate%20imaging-based%0Adiagnosis%2C%20with%20segmentation%20playing%20a%20critical%20role%20in%20evaluating%20morphology%0Aand%20treatment%20decisions.%20Manual%20delineation%20of%20gliomas%20is%20time-consuming%20and%0Aprone%20to%20variability%2C%20motivating%20the%20use%20of%20deep%20learning%20to%20improve%0Aconsistency%20and%20alleviate%20clinical%20workload.%20However%2C%20existing%20methods%20often%0Afail%20to%20fully%20exploit%20the%20information%20available%20in%20multi-parametric%20MRI%0A%28mp-MRI%29%2C%20particularly%20inter-slice%20contextual%20features%2C%20and%20typically%20require%0Aconsiderable%20computational%20resources%20while%20lacking%20robustness%20across%20tumor%20type%0Avariations.%20We%20present%20GBT-SAM%2C%20a%20parameter-efficient%20deep%20learning%20framework%0Athat%20adapts%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%20a%20large-scale%20vision%20model%2C%20to%0Avolumetric%20mp-MRI%20data.%20GBT-SAM%20reduces%20input%20complexity%20by%20selecting%20fewer%0Athan%202.6%5C%25%20of%20slices%20per%20scan%20while%20incorporating%20all%20four%20MRI%20modalities%2C%0Apreserving%20essential%20tumor-related%20information%20with%20minimal%20cost.%20Furthermore%2C%0Aour%20model%20is%20trained%20by%20a%20two-step%20fine-tuning%20strategy%20that%20incorporates%20a%0Adepth-aware%20module%20to%20capture%20inter-slice%20correlations%20and%20lightweight%0Aadaptation%20layers%2C%20resulting%20in%20just%206.5M%20trainable%20parameters%2C%20which%20is%20the%0Alowest%20among%20SAM-based%20approaches.%20GBT-SAM%20achieves%20a%20Dice%20Score%20of%2093.54%20on%0Athe%20BraTS%20Adult%20Glioma%20dataset%20and%20demonstrates%20robust%20performance%20on%0AMeningioma%2C%20Pediatric%20Glioma%2C%20and%20Sub-Saharan%20Glioma%20datasets.%20These%20results%0Ahighlight%20GBT-SAM%27s%20potential%20as%20a%20computationally%20efficient%20and%20domain-robust%0Aframework%20for%20brain%20tumor%20segmentation%20using%20mp-MRI.%20Our%20code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/vpulab/med-sam-brain%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04325v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGBT-SAM%253A%2520Adapting%2520a%2520Foundational%2520Deep%2520Learning%2520Model%2520for%2520Generalizable%250A%2520%2520Brain%2520Tumor%2520Segmentation%2520via%2520Efficient%2520Integration%2520of%2520Multi-Parametric%2520MRI%250A%2520%2520Data%26entry.906535625%3DCecilia%2520Diana-Albelda%2520and%2520Roberto%2520Alcover-Couso%2520and%2520%25C3%2581lvaro%2520Garc%25C3%25ADa-Mart%25C3%25ADn%2520and%2520Jesus%2520Bescos%2520and%2520Marcos%2520Escudero-Vi%25C3%25B1olo%26entry.1292438233%3D%2520%2520Gliomas%2520are%2520aggressive%2520brain%2520tumors%2520that%2520require%2520accurate%2520imaging-based%250Adiagnosis%252C%2520with%2520segmentation%2520playing%2520a%2520critical%2520role%2520in%2520evaluating%2520morphology%250Aand%2520treatment%2520decisions.%2520Manual%2520delineation%2520of%2520gliomas%2520is%2520time-consuming%2520and%250Aprone%2520to%2520variability%252C%2520motivating%2520the%2520use%2520of%2520deep%2520learning%2520to%2520improve%250Aconsistency%2520and%2520alleviate%2520clinical%2520workload.%2520However%252C%2520existing%2520methods%2520often%250Afail%2520to%2520fully%2520exploit%2520the%2520information%2520available%2520in%2520multi-parametric%2520MRI%250A%2528mp-MRI%2529%252C%2520particularly%2520inter-slice%2520contextual%2520features%252C%2520and%2520typically%2520require%250Aconsiderable%2520computational%2520resources%2520while%2520lacking%2520robustness%2520across%2520tumor%2520type%250Avariations.%2520We%2520present%2520GBT-SAM%252C%2520a%2520parameter-efficient%2520deep%2520learning%2520framework%250Athat%2520adapts%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520a%2520large-scale%2520vision%2520model%252C%2520to%250Avolumetric%2520mp-MRI%2520data.%2520GBT-SAM%2520reduces%2520input%2520complexity%2520by%2520selecting%2520fewer%250Athan%25202.6%255C%2525%2520of%2520slices%2520per%2520scan%2520while%2520incorporating%2520all%2520four%2520MRI%2520modalities%252C%250Apreserving%2520essential%2520tumor-related%2520information%2520with%2520minimal%2520cost.%2520Furthermore%252C%250Aour%2520model%2520is%2520trained%2520by%2520a%2520two-step%2520fine-tuning%2520strategy%2520that%2520incorporates%2520a%250Adepth-aware%2520module%2520to%2520capture%2520inter-slice%2520correlations%2520and%2520lightweight%250Aadaptation%2520layers%252C%2520resulting%2520in%2520just%25206.5M%2520trainable%2520parameters%252C%2520which%2520is%2520the%250Alowest%2520among%2520SAM-based%2520approaches.%2520GBT-SAM%2520achieves%2520a%2520Dice%2520Score%2520of%252093.54%2520on%250Athe%2520BraTS%2520Adult%2520Glioma%2520dataset%2520and%2520demonstrates%2520robust%2520performance%2520on%250AMeningioma%252C%2520Pediatric%2520Glioma%252C%2520and%2520Sub-Saharan%2520Glioma%2520datasets.%2520These%2520results%250Ahighlight%2520GBT-SAM%2527s%2520potential%2520as%2520a%2520computationally%2520efficient%2520and%2520domain-robust%250Aframework%2520for%2520brain%2520tumor%2520segmentation%2520using%2520mp-MRI.%2520Our%2520code%2520and%2520models%2520are%250Aavailable%2520at%2520https%253A//github.com/vpulab/med-sam-brain%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04325v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GBT-SAM%3A%20Adapting%20a%20Foundational%20Deep%20Learning%20Model%20for%20Generalizable%0A%20%20Brain%20Tumor%20Segmentation%20via%20Efficient%20Integration%20of%20Multi-Parametric%20MRI%0A%20%20Data&entry.906535625=Cecilia%20Diana-Albelda%20and%20Roberto%20Alcover-Couso%20and%20%C3%81lvaro%20Garc%C3%ADa-Mart%C3%ADn%20and%20Jesus%20Bescos%20and%20Marcos%20Escudero-Vi%C3%B1olo&entry.1292438233=%20%20Gliomas%20are%20aggressive%20brain%20tumors%20that%20require%20accurate%20imaging-based%0Adiagnosis%2C%20with%20segmentation%20playing%20a%20critical%20role%20in%20evaluating%20morphology%0Aand%20treatment%20decisions.%20Manual%20delineation%20of%20gliomas%20is%20time-consuming%20and%0Aprone%20to%20variability%2C%20motivating%20the%20use%20of%20deep%20learning%20to%20improve%0Aconsistency%20and%20alleviate%20clinical%20workload.%20However%2C%20existing%20methods%20often%0Afail%20to%20fully%20exploit%20the%20information%20available%20in%20multi-parametric%20MRI%0A%28mp-MRI%29%2C%20particularly%20inter-slice%20contextual%20features%2C%20and%20typically%20require%0Aconsiderable%20computational%20resources%20while%20lacking%20robustness%20across%20tumor%20type%0Avariations.%20We%20present%20GBT-SAM%2C%20a%20parameter-efficient%20deep%20learning%20framework%0Athat%20adapts%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%20a%20large-scale%20vision%20model%2C%20to%0Avolumetric%20mp-MRI%20data.%20GBT-SAM%20reduces%20input%20complexity%20by%20selecting%20fewer%0Athan%202.6%5C%25%20of%20slices%20per%20scan%20while%20incorporating%20all%20four%20MRI%20modalities%2C%0Apreserving%20essential%20tumor-related%20information%20with%20minimal%20cost.%20Furthermore%2C%0Aour%20model%20is%20trained%20by%20a%20two-step%20fine-tuning%20strategy%20that%20incorporates%20a%0Adepth-aware%20module%20to%20capture%20inter-slice%20correlations%20and%20lightweight%0Aadaptation%20layers%2C%20resulting%20in%20just%206.5M%20trainable%20parameters%2C%20which%20is%20the%0Alowest%20among%20SAM-based%20approaches.%20GBT-SAM%20achieves%20a%20Dice%20Score%20of%2093.54%20on%0Athe%20BraTS%20Adult%20Glioma%20dataset%20and%20demonstrates%20robust%20performance%20on%0AMeningioma%2C%20Pediatric%20Glioma%2C%20and%20Sub-Saharan%20Glioma%20datasets.%20These%20results%0Ahighlight%20GBT-SAM%27s%20potential%20as%20a%20computationally%20efficient%20and%20domain-robust%0Aframework%20for%20brain%20tumor%20segmentation%20using%20mp-MRI.%20Our%20code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/vpulab/med-sam-brain%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04325v3&entry.124074799=Read"},
{"title": "OpenThinkIMG: Learning to Think with Images via Visual Tool\n  Reinforcement Learning", "author": "Zhaochen Su and Linjie Li and Mingyang Song and Yunzhuo Hao and Zhengyuan Yang and Jun Zhang and Guanjie Chen and Jiawei Gu and Juntao Li and Xiaoye Qu and Yu Cheng", "abstract": "  While humans can flexibly leverage interactive visual cognition for complex\nproblem-solving, enabling Large Vision-Language Models (LVLMs) to learn\nsimilarly adaptive behaviors with visual tools remains challenging. A\nsignificant hurdle is the current lack of standardized infrastructure, which\nhinders integrating diverse tools, generating rich interaction data, and\ntraining robust agents effectively. To address these gaps, we introduce\nOpenThinkIMG, the first open-source, comprehensive end-to-end framework for\ntool-augmented LVLMs. It features standardized vision tool interfaces, scalable\ntrajectory generation for policy initialization, and a flexible training\nenvironment. Furthermore, considering supervised fine-tuning (SFT) on static\ndemonstrations offers limited policy generalization for dynamic tool\ninvocation, we propose a novel reinforcement learning (RL) framework V-ToolRL\nto train LVLMs to learn adaptive policies for invoking external vision tools.\nV-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies\nby directly optimizing for task success using feedback from tool interactions.\nWe empirically validate V-ToolRL on challenging chart reasoning tasks. Our\nRL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its\nSFT-initialized counterpart (+28.83 points) and surpasses established\nsupervised tool-learning baselines like Taco and CogCom by an average of +12.7\npoints. Notably, it also surpasses prominent closed-source models like GPT-4.1\nby +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational\nframework for advancing dynamic, tool-augmented visual reasoning, helping the\ncommunity develop AI agents that can genuinely \"think with images\".\n", "link": "http://arxiv.org/abs/2505.08617v1", "date": "2025-05-13", "relevancy": 2.7761, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5637}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenThinkIMG%3A%20Learning%20to%20Think%20with%20Images%20via%20Visual%20Tool%0A%20%20Reinforcement%20Learning&body=Title%3A%20OpenThinkIMG%3A%20Learning%20to%20Think%20with%20Images%20via%20Visual%20Tool%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Zhaochen%20Su%20and%20Linjie%20Li%20and%20Mingyang%20Song%20and%20Yunzhuo%20Hao%20and%20Zhengyuan%20Yang%20and%20Jun%20Zhang%20and%20Guanjie%20Chen%20and%20Jiawei%20Gu%20and%20Juntao%20Li%20and%20Xiaoye%20Qu%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20While%20humans%20can%20flexibly%20leverage%20interactive%20visual%20cognition%20for%20complex%0Aproblem-solving%2C%20enabling%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20learn%0Asimilarly%20adaptive%20behaviors%20with%20visual%20tools%20remains%20challenging.%20A%0Asignificant%20hurdle%20is%20the%20current%20lack%20of%20standardized%20infrastructure%2C%20which%0Ahinders%20integrating%20diverse%20tools%2C%20generating%20rich%20interaction%20data%2C%20and%0Atraining%20robust%20agents%20effectively.%20To%20address%20these%20gaps%2C%20we%20introduce%0AOpenThinkIMG%2C%20the%20first%20open-source%2C%20comprehensive%20end-to-end%20framework%20for%0Atool-augmented%20LVLMs.%20It%20features%20standardized%20vision%20tool%20interfaces%2C%20scalable%0Atrajectory%20generation%20for%20policy%20initialization%2C%20and%20a%20flexible%20training%0Aenvironment.%20Furthermore%2C%20considering%20supervised%20fine-tuning%20%28SFT%29%20on%20static%0Ademonstrations%20offers%20limited%20policy%20generalization%20for%20dynamic%20tool%0Ainvocation%2C%20we%20propose%20a%20novel%20reinforcement%20learning%20%28RL%29%20framework%20V-ToolRL%0Ato%20train%20LVLMs%20to%20learn%20adaptive%20policies%20for%20invoking%20external%20vision%20tools.%0AV-ToolRL%20enables%20LVLMs%20to%20autonomously%20discover%20optimal%20tool-usage%20strategies%0Aby%20directly%20optimizing%20for%20task%20success%20using%20feedback%20from%20tool%20interactions.%0AWe%20empirically%20validate%20V-ToolRL%20on%20challenging%20chart%20reasoning%20tasks.%20Our%0ARL-trained%20agent%2C%20built%20upon%20a%20Qwen2-VL-2B%2C%20significantly%20outperforms%20its%0ASFT-initialized%20counterpart%20%28%2B28.83%20points%29%20and%20surpasses%20established%0Asupervised%20tool-learning%20baselines%20like%20Taco%20and%20CogCom%20by%20an%20average%20of%20%2B12.7%0Apoints.%20Notably%2C%20it%20also%20surpasses%20prominent%20closed-source%20models%20like%20GPT-4.1%0Aby%20%2B8.68%20accuracy%20points.%20We%20hope%20OpenThinkIMG%20can%20serve%20as%20a%20foundational%0Aframework%20for%20advancing%20dynamic%2C%20tool-augmented%20visual%20reasoning%2C%20helping%20the%0Acommunity%20develop%20AI%20agents%20that%20can%20genuinely%20%22think%20with%20images%22.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenThinkIMG%253A%2520Learning%2520to%2520Think%2520with%2520Images%2520via%2520Visual%2520Tool%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DZhaochen%2520Su%2520and%2520Linjie%2520Li%2520and%2520Mingyang%2520Song%2520and%2520Yunzhuo%2520Hao%2520and%2520Zhengyuan%2520Yang%2520and%2520Jun%2520Zhang%2520and%2520Guanjie%2520Chen%2520and%2520Jiawei%2520Gu%2520and%2520Juntao%2520Li%2520and%2520Xiaoye%2520Qu%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520While%2520humans%2520can%2520flexibly%2520leverage%2520interactive%2520visual%2520cognition%2520for%2520complex%250Aproblem-solving%252C%2520enabling%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520to%2520learn%250Asimilarly%2520adaptive%2520behaviors%2520with%2520visual%2520tools%2520remains%2520challenging.%2520A%250Asignificant%2520hurdle%2520is%2520the%2520current%2520lack%2520of%2520standardized%2520infrastructure%252C%2520which%250Ahinders%2520integrating%2520diverse%2520tools%252C%2520generating%2520rich%2520interaction%2520data%252C%2520and%250Atraining%2520robust%2520agents%2520effectively.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%250AOpenThinkIMG%252C%2520the%2520first%2520open-source%252C%2520comprehensive%2520end-to-end%2520framework%2520for%250Atool-augmented%2520LVLMs.%2520It%2520features%2520standardized%2520vision%2520tool%2520interfaces%252C%2520scalable%250Atrajectory%2520generation%2520for%2520policy%2520initialization%252C%2520and%2520a%2520flexible%2520training%250Aenvironment.%2520Furthermore%252C%2520considering%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520on%2520static%250Ademonstrations%2520offers%2520limited%2520policy%2520generalization%2520for%2520dynamic%2520tool%250Ainvocation%252C%2520we%2520propose%2520a%2520novel%2520reinforcement%2520learning%2520%2528RL%2529%2520framework%2520V-ToolRL%250Ato%2520train%2520LVLMs%2520to%2520learn%2520adaptive%2520policies%2520for%2520invoking%2520external%2520vision%2520tools.%250AV-ToolRL%2520enables%2520LVLMs%2520to%2520autonomously%2520discover%2520optimal%2520tool-usage%2520strategies%250Aby%2520directly%2520optimizing%2520for%2520task%2520success%2520using%2520feedback%2520from%2520tool%2520interactions.%250AWe%2520empirically%2520validate%2520V-ToolRL%2520on%2520challenging%2520chart%2520reasoning%2520tasks.%2520Our%250ARL-trained%2520agent%252C%2520built%2520upon%2520a%2520Qwen2-VL-2B%252C%2520significantly%2520outperforms%2520its%250ASFT-initialized%2520counterpart%2520%2528%252B28.83%2520points%2529%2520and%2520surpasses%2520established%250Asupervised%2520tool-learning%2520baselines%2520like%2520Taco%2520and%2520CogCom%2520by%2520an%2520average%2520of%2520%252B12.7%250Apoints.%2520Notably%252C%2520it%2520also%2520surpasses%2520prominent%2520closed-source%2520models%2520like%2520GPT-4.1%250Aby%2520%252B8.68%2520accuracy%2520points.%2520We%2520hope%2520OpenThinkIMG%2520can%2520serve%2520as%2520a%2520foundational%250Aframework%2520for%2520advancing%2520dynamic%252C%2520tool-augmented%2520visual%2520reasoning%252C%2520helping%2520the%250Acommunity%2520develop%2520AI%2520agents%2520that%2520can%2520genuinely%2520%2522think%2520with%2520images%2522.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenThinkIMG%3A%20Learning%20to%20Think%20with%20Images%20via%20Visual%20Tool%0A%20%20Reinforcement%20Learning&entry.906535625=Zhaochen%20Su%20and%20Linjie%20Li%20and%20Mingyang%20Song%20and%20Yunzhuo%20Hao%20and%20Zhengyuan%20Yang%20and%20Jun%20Zhang%20and%20Guanjie%20Chen%20and%20Jiawei%20Gu%20and%20Juntao%20Li%20and%20Xiaoye%20Qu%20and%20Yu%20Cheng&entry.1292438233=%20%20While%20humans%20can%20flexibly%20leverage%20interactive%20visual%20cognition%20for%20complex%0Aproblem-solving%2C%20enabling%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20learn%0Asimilarly%20adaptive%20behaviors%20with%20visual%20tools%20remains%20challenging.%20A%0Asignificant%20hurdle%20is%20the%20current%20lack%20of%20standardized%20infrastructure%2C%20which%0Ahinders%20integrating%20diverse%20tools%2C%20generating%20rich%20interaction%20data%2C%20and%0Atraining%20robust%20agents%20effectively.%20To%20address%20these%20gaps%2C%20we%20introduce%0AOpenThinkIMG%2C%20the%20first%20open-source%2C%20comprehensive%20end-to-end%20framework%20for%0Atool-augmented%20LVLMs.%20It%20features%20standardized%20vision%20tool%20interfaces%2C%20scalable%0Atrajectory%20generation%20for%20policy%20initialization%2C%20and%20a%20flexible%20training%0Aenvironment.%20Furthermore%2C%20considering%20supervised%20fine-tuning%20%28SFT%29%20on%20static%0Ademonstrations%20offers%20limited%20policy%20generalization%20for%20dynamic%20tool%0Ainvocation%2C%20we%20propose%20a%20novel%20reinforcement%20learning%20%28RL%29%20framework%20V-ToolRL%0Ato%20train%20LVLMs%20to%20learn%20adaptive%20policies%20for%20invoking%20external%20vision%20tools.%0AV-ToolRL%20enables%20LVLMs%20to%20autonomously%20discover%20optimal%20tool-usage%20strategies%0Aby%20directly%20optimizing%20for%20task%20success%20using%20feedback%20from%20tool%20interactions.%0AWe%20empirically%20validate%20V-ToolRL%20on%20challenging%20chart%20reasoning%20tasks.%20Our%0ARL-trained%20agent%2C%20built%20upon%20a%20Qwen2-VL-2B%2C%20significantly%20outperforms%20its%0ASFT-initialized%20counterpart%20%28%2B28.83%20points%29%20and%20surpasses%20established%0Asupervised%20tool-learning%20baselines%20like%20Taco%20and%20CogCom%20by%20an%20average%20of%20%2B12.7%0Apoints.%20Notably%2C%20it%20also%20surpasses%20prominent%20closed-source%20models%20like%20GPT-4.1%0Aby%20%2B8.68%20accuracy%20points.%20We%20hope%20OpenThinkIMG%20can%20serve%20as%20a%20foundational%0Aframework%20for%20advancing%20dynamic%2C%20tool-augmented%20visual%20reasoning%2C%20helping%20the%0Acommunity%20develop%20AI%20agents%20that%20can%20genuinely%20%22think%20with%20images%22.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08617v1&entry.124074799=Read"},
{"title": "Unsupervised Urban Land Use Mapping with Street View Contrastive\n  Clustering and a Geographical Prior", "author": "Lin Che and Yizi Chen and Tanhua Jin and Martin Raubal and Konrad Schindler and Peter Kiefer", "abstract": "  Urban land use classification and mapping are critical for urban planning,\nresource management, and environmental monitoring. Existing remote sensing\ntechniques often lack precision in complex urban environments due to the\nabsence of ground-level details. Unlike aerial perspectives, street view images\nprovide a ground-level view that captures more human and social activities\nrelevant to land use in complex urban scenes. Existing street view-based\nmethods primarily rely on supervised classification, which is challenged by the\nscarcity of high-quality labeled data and the difficulty of generalizing across\ndiverse urban landscapes. This study introduces an unsupervised contrastive\nclustering model for street view images with a built-in geographical prior, to\nenhance clustering performance. When combined with a simple visual assignment\nof the clusters, our approach offers a flexible and customizable solution to\nland use mapping, tailored to the specific needs of urban planners. We\nexperimentally show that our method can generate land use maps from geotagged\nstreet view image datasets of two cities. As our methodology relies on the\nuniversal spatial coherence of geospatial data (\"Tobler's law\"), it can be\nadapted to various settings where street view images are available, to enable\nscalable, unsupervised land use mapping and updating. The code will be\navailable at https://github.com/lin102/CCGP.\n", "link": "http://arxiv.org/abs/2504.17551v2", "date": "2025-05-13", "relevancy": 2.7608, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5967}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5343}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Urban%20Land%20Use%20Mapping%20with%20Street%20View%20Contrastive%0A%20%20Clustering%20and%20a%20Geographical%20Prior&body=Title%3A%20Unsupervised%20Urban%20Land%20Use%20Mapping%20with%20Street%20View%20Contrastive%0A%20%20Clustering%20and%20a%20Geographical%20Prior%0AAuthor%3A%20Lin%20Che%20and%20Yizi%20Chen%20and%20Tanhua%20Jin%20and%20Martin%20Raubal%20and%20Konrad%20Schindler%20and%20Peter%20Kiefer%0AAbstract%3A%20%20%20Urban%20land%20use%20classification%20and%20mapping%20are%20critical%20for%20urban%20planning%2C%0Aresource%20management%2C%20and%20environmental%20monitoring.%20Existing%20remote%20sensing%0Atechniques%20often%20lack%20precision%20in%20complex%20urban%20environments%20due%20to%20the%0Aabsence%20of%20ground-level%20details.%20Unlike%20aerial%20perspectives%2C%20street%20view%20images%0Aprovide%20a%20ground-level%20view%20that%20captures%20more%20human%20and%20social%20activities%0Arelevant%20to%20land%20use%20in%20complex%20urban%20scenes.%20Existing%20street%20view-based%0Amethods%20primarily%20rely%20on%20supervised%20classification%2C%20which%20is%20challenged%20by%20the%0Ascarcity%20of%20high-quality%20labeled%20data%20and%20the%20difficulty%20of%20generalizing%20across%0Adiverse%20urban%20landscapes.%20This%20study%20introduces%20an%20unsupervised%20contrastive%0Aclustering%20model%20for%20street%20view%20images%20with%20a%20built-in%20geographical%20prior%2C%20to%0Aenhance%20clustering%20performance.%20When%20combined%20with%20a%20simple%20visual%20assignment%0Aof%20the%20clusters%2C%20our%20approach%20offers%20a%20flexible%20and%20customizable%20solution%20to%0Aland%20use%20mapping%2C%20tailored%20to%20the%20specific%20needs%20of%20urban%20planners.%20We%0Aexperimentally%20show%20that%20our%20method%20can%20generate%20land%20use%20maps%20from%20geotagged%0Astreet%20view%20image%20datasets%20of%20two%20cities.%20As%20our%20methodology%20relies%20on%20the%0Auniversal%20spatial%20coherence%20of%20geospatial%20data%20%28%22Tobler%27s%20law%22%29%2C%20it%20can%20be%0Aadapted%20to%20various%20settings%20where%20street%20view%20images%20are%20available%2C%20to%20enable%0Ascalable%2C%20unsupervised%20land%20use%20mapping%20and%20updating.%20The%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/lin102/CCGP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Urban%2520Land%2520Use%2520Mapping%2520with%2520Street%2520View%2520Contrastive%250A%2520%2520Clustering%2520and%2520a%2520Geographical%2520Prior%26entry.906535625%3DLin%2520Che%2520and%2520Yizi%2520Chen%2520and%2520Tanhua%2520Jin%2520and%2520Martin%2520Raubal%2520and%2520Konrad%2520Schindler%2520and%2520Peter%2520Kiefer%26entry.1292438233%3D%2520%2520Urban%2520land%2520use%2520classification%2520and%2520mapping%2520are%2520critical%2520for%2520urban%2520planning%252C%250Aresource%2520management%252C%2520and%2520environmental%2520monitoring.%2520Existing%2520remote%2520sensing%250Atechniques%2520often%2520lack%2520precision%2520in%2520complex%2520urban%2520environments%2520due%2520to%2520the%250Aabsence%2520of%2520ground-level%2520details.%2520Unlike%2520aerial%2520perspectives%252C%2520street%2520view%2520images%250Aprovide%2520a%2520ground-level%2520view%2520that%2520captures%2520more%2520human%2520and%2520social%2520activities%250Arelevant%2520to%2520land%2520use%2520in%2520complex%2520urban%2520scenes.%2520Existing%2520street%2520view-based%250Amethods%2520primarily%2520rely%2520on%2520supervised%2520classification%252C%2520which%2520is%2520challenged%2520by%2520the%250Ascarcity%2520of%2520high-quality%2520labeled%2520data%2520and%2520the%2520difficulty%2520of%2520generalizing%2520across%250Adiverse%2520urban%2520landscapes.%2520This%2520study%2520introduces%2520an%2520unsupervised%2520contrastive%250Aclustering%2520model%2520for%2520street%2520view%2520images%2520with%2520a%2520built-in%2520geographical%2520prior%252C%2520to%250Aenhance%2520clustering%2520performance.%2520When%2520combined%2520with%2520a%2520simple%2520visual%2520assignment%250Aof%2520the%2520clusters%252C%2520our%2520approach%2520offers%2520a%2520flexible%2520and%2520customizable%2520solution%2520to%250Aland%2520use%2520mapping%252C%2520tailored%2520to%2520the%2520specific%2520needs%2520of%2520urban%2520planners.%2520We%250Aexperimentally%2520show%2520that%2520our%2520method%2520can%2520generate%2520land%2520use%2520maps%2520from%2520geotagged%250Astreet%2520view%2520image%2520datasets%2520of%2520two%2520cities.%2520As%2520our%2520methodology%2520relies%2520on%2520the%250Auniversal%2520spatial%2520coherence%2520of%2520geospatial%2520data%2520%2528%2522Tobler%2527s%2520law%2522%2529%252C%2520it%2520can%2520be%250Aadapted%2520to%2520various%2520settings%2520where%2520street%2520view%2520images%2520are%2520available%252C%2520to%2520enable%250Ascalable%252C%2520unsupervised%2520land%2520use%2520mapping%2520and%2520updating.%2520The%2520code%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/lin102/CCGP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Urban%20Land%20Use%20Mapping%20with%20Street%20View%20Contrastive%0A%20%20Clustering%20and%20a%20Geographical%20Prior&entry.906535625=Lin%20Che%20and%20Yizi%20Chen%20and%20Tanhua%20Jin%20and%20Martin%20Raubal%20and%20Konrad%20Schindler%20and%20Peter%20Kiefer&entry.1292438233=%20%20Urban%20land%20use%20classification%20and%20mapping%20are%20critical%20for%20urban%20planning%2C%0Aresource%20management%2C%20and%20environmental%20monitoring.%20Existing%20remote%20sensing%0Atechniques%20often%20lack%20precision%20in%20complex%20urban%20environments%20due%20to%20the%0Aabsence%20of%20ground-level%20details.%20Unlike%20aerial%20perspectives%2C%20street%20view%20images%0Aprovide%20a%20ground-level%20view%20that%20captures%20more%20human%20and%20social%20activities%0Arelevant%20to%20land%20use%20in%20complex%20urban%20scenes.%20Existing%20street%20view-based%0Amethods%20primarily%20rely%20on%20supervised%20classification%2C%20which%20is%20challenged%20by%20the%0Ascarcity%20of%20high-quality%20labeled%20data%20and%20the%20difficulty%20of%20generalizing%20across%0Adiverse%20urban%20landscapes.%20This%20study%20introduces%20an%20unsupervised%20contrastive%0Aclustering%20model%20for%20street%20view%20images%20with%20a%20built-in%20geographical%20prior%2C%20to%0Aenhance%20clustering%20performance.%20When%20combined%20with%20a%20simple%20visual%20assignment%0Aof%20the%20clusters%2C%20our%20approach%20offers%20a%20flexible%20and%20customizable%20solution%20to%0Aland%20use%20mapping%2C%20tailored%20to%20the%20specific%20needs%20of%20urban%20planners.%20We%0Aexperimentally%20show%20that%20our%20method%20can%20generate%20land%20use%20maps%20from%20geotagged%0Astreet%20view%20image%20datasets%20of%20two%20cities.%20As%20our%20methodology%20relies%20on%20the%0Auniversal%20spatial%20coherence%20of%20geospatial%20data%20%28%22Tobler%27s%20law%22%29%2C%20it%20can%20be%0Aadapted%20to%20various%20settings%20where%20street%20view%20images%20are%20available%2C%20to%20enable%0Ascalable%2C%20unsupervised%20land%20use%20mapping%20and%20updating.%20The%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/lin102/CCGP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17551v2&entry.124074799=Read"},
{"title": "An integrated language-vision foundation model for conversational\n  diagnostics and triaging in primary eye care", "author": "Zhi Da Soh and Yang Bai and Kai Yu and Yang Zhou and Xiaofeng Lei and Sahil Thakur and Zann Lee and Lee Ching Linette Phang and Qingsheng Peng and Can Can Xue and Rachel Shujuan Chong and Quan V. Hoang and Lavanya Raghavan and Yih Chung Tham and Charumathi Sabanayagam and Wei-Chi Wu and Ming-Chih Ho and Jiangnan He and Preeti Gupta and Ecosse Lamoureux and Seang Mei Saw and Vinay Nangia and Songhomitra Panda-Jonas and Jie Xu and Ya Xing Wang and Xinxing Xu and Jost B. Jonas and Tien Yin Wong and Rick Siow Mong Goh and Yong Liu and Ching-Yu Cheng", "abstract": "  Current deep learning models are mostly task specific and lack a\nuser-friendly interface to operate. We present Meta-EyeFM, a multi-function\nfoundation model that integrates a large language model (LLM) with vision\nfoundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a\nrouting mechanism to enable accurate task-specific analysis based on text\nqueries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and\nsystemic diseases, differentiate ocular disease severity, and identify common\nocular signs. The model achieved 100% accuracy in routing fundus images to\nappropriate VFMs, which achieved $\\ge$ 82.2% accuracy in disease detection,\n$\\ge$ 89% in severity differentiation, $\\ge$ 76% in sign identification.\nMeta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o\nLMMs in detecting various eye diseases and comparable to an ophthalmologist.\nThis system offers enhanced usability and diagnostic performance, making it a\nvaluable decision support tool for primary eye care or an online LLM for fundus\nevaluation.\n", "link": "http://arxiv.org/abs/2505.08414v1", "date": "2025-05-13", "relevancy": 2.7452, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20integrated%20language-vision%20foundation%20model%20for%20conversational%0A%20%20diagnostics%20and%20triaging%20in%20primary%20eye%20care&body=Title%3A%20An%20integrated%20language-vision%20foundation%20model%20for%20conversational%0A%20%20diagnostics%20and%20triaging%20in%20primary%20eye%20care%0AAuthor%3A%20Zhi%20Da%20Soh%20and%20Yang%20Bai%20and%20Kai%20Yu%20and%20Yang%20Zhou%20and%20Xiaofeng%20Lei%20and%20Sahil%20Thakur%20and%20Zann%20Lee%20and%20Lee%20Ching%20Linette%20Phang%20and%20Qingsheng%20Peng%20and%20Can%20Can%20Xue%20and%20Rachel%20Shujuan%20Chong%20and%20Quan%20V.%20Hoang%20and%20Lavanya%20Raghavan%20and%20Yih%20Chung%20Tham%20and%20Charumathi%20Sabanayagam%20and%20Wei-Chi%20Wu%20and%20Ming-Chih%20Ho%20and%20Jiangnan%20He%20and%20Preeti%20Gupta%20and%20Ecosse%20Lamoureux%20and%20Seang%20Mei%20Saw%20and%20Vinay%20Nangia%20and%20Songhomitra%20Panda-Jonas%20and%20Jie%20Xu%20and%20Ya%20Xing%20Wang%20and%20Xinxing%20Xu%20and%20Jost%20B.%20Jonas%20and%20Tien%20Yin%20Wong%20and%20Rick%20Siow%20Mong%20Goh%20and%20Yong%20Liu%20and%20Ching-Yu%20Cheng%0AAbstract%3A%20%20%20Current%20deep%20learning%20models%20are%20mostly%20task%20specific%20and%20lack%20a%0Auser-friendly%20interface%20to%20operate.%20We%20present%20Meta-EyeFM%2C%20a%20multi-function%0Afoundation%20model%20that%20integrates%20a%20large%20language%20model%20%28LLM%29%20with%20vision%0Afoundation%20models%20%28VFMs%29%20for%20ocular%20disease%20assessment.%20Meta-EyeFM%20leverages%20a%0Arouting%20mechanism%20to%20enable%20accurate%20task-specific%20analysis%20based%20on%20text%0Aqueries.%20Using%20Low%20Rank%20Adaptation%2C%20we%20fine-tuned%20our%20VFMs%20to%20detect%20ocular%20and%0Asystemic%20diseases%2C%20differentiate%20ocular%20disease%20severity%2C%20and%20identify%20common%0Aocular%20signs.%20The%20model%20achieved%20100%25%20accuracy%20in%20routing%20fundus%20images%20to%0Aappropriate%20VFMs%2C%20which%20achieved%20%24%5Cge%24%2082.2%25%20accuracy%20in%20disease%20detection%2C%0A%24%5Cge%24%2089%25%20in%20severity%20differentiation%2C%20%24%5Cge%24%2076%25%20in%20sign%20identification.%0AMeta-EyeFM%20was%2011%25%20to%2043%25%20more%20accurate%20than%20Gemini-1.5-flash%20and%20ChatGPT-4o%0ALMMs%20in%20detecting%20various%20eye%20diseases%20and%20comparable%20to%20an%20ophthalmologist.%0AThis%20system%20offers%20enhanced%20usability%20and%20diagnostic%20performance%2C%20making%20it%20a%0Avaluable%20decision%20support%20tool%20for%20primary%20eye%20care%20or%20an%20online%20LLM%20for%20fundus%0Aevaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520integrated%2520language-vision%2520foundation%2520model%2520for%2520conversational%250A%2520%2520diagnostics%2520and%2520triaging%2520in%2520primary%2520eye%2520care%26entry.906535625%3DZhi%2520Da%2520Soh%2520and%2520Yang%2520Bai%2520and%2520Kai%2520Yu%2520and%2520Yang%2520Zhou%2520and%2520Xiaofeng%2520Lei%2520and%2520Sahil%2520Thakur%2520and%2520Zann%2520Lee%2520and%2520Lee%2520Ching%2520Linette%2520Phang%2520and%2520Qingsheng%2520Peng%2520and%2520Can%2520Can%2520Xue%2520and%2520Rachel%2520Shujuan%2520Chong%2520and%2520Quan%2520V.%2520Hoang%2520and%2520Lavanya%2520Raghavan%2520and%2520Yih%2520Chung%2520Tham%2520and%2520Charumathi%2520Sabanayagam%2520and%2520Wei-Chi%2520Wu%2520and%2520Ming-Chih%2520Ho%2520and%2520Jiangnan%2520He%2520and%2520Preeti%2520Gupta%2520and%2520Ecosse%2520Lamoureux%2520and%2520Seang%2520Mei%2520Saw%2520and%2520Vinay%2520Nangia%2520and%2520Songhomitra%2520Panda-Jonas%2520and%2520Jie%2520Xu%2520and%2520Ya%2520Xing%2520Wang%2520and%2520Xinxing%2520Xu%2520and%2520Jost%2520B.%2520Jonas%2520and%2520Tien%2520Yin%2520Wong%2520and%2520Rick%2520Siow%2520Mong%2520Goh%2520and%2520Yong%2520Liu%2520and%2520Ching-Yu%2520Cheng%26entry.1292438233%3D%2520%2520Current%2520deep%2520learning%2520models%2520are%2520mostly%2520task%2520specific%2520and%2520lack%2520a%250Auser-friendly%2520interface%2520to%2520operate.%2520We%2520present%2520Meta-EyeFM%252C%2520a%2520multi-function%250Afoundation%2520model%2520that%2520integrates%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520with%2520vision%250Afoundation%2520models%2520%2528VFMs%2529%2520for%2520ocular%2520disease%2520assessment.%2520Meta-EyeFM%2520leverages%2520a%250Arouting%2520mechanism%2520to%2520enable%2520accurate%2520task-specific%2520analysis%2520based%2520on%2520text%250Aqueries.%2520Using%2520Low%2520Rank%2520Adaptation%252C%2520we%2520fine-tuned%2520our%2520VFMs%2520to%2520detect%2520ocular%2520and%250Asystemic%2520diseases%252C%2520differentiate%2520ocular%2520disease%2520severity%252C%2520and%2520identify%2520common%250Aocular%2520signs.%2520The%2520model%2520achieved%2520100%2525%2520accuracy%2520in%2520routing%2520fundus%2520images%2520to%250Aappropriate%2520VFMs%252C%2520which%2520achieved%2520%2524%255Cge%2524%252082.2%2525%2520accuracy%2520in%2520disease%2520detection%252C%250A%2524%255Cge%2524%252089%2525%2520in%2520severity%2520differentiation%252C%2520%2524%255Cge%2524%252076%2525%2520in%2520sign%2520identification.%250AMeta-EyeFM%2520was%252011%2525%2520to%252043%2525%2520more%2520accurate%2520than%2520Gemini-1.5-flash%2520and%2520ChatGPT-4o%250ALMMs%2520in%2520detecting%2520various%2520eye%2520diseases%2520and%2520comparable%2520to%2520an%2520ophthalmologist.%250AThis%2520system%2520offers%2520enhanced%2520usability%2520and%2520diagnostic%2520performance%252C%2520making%2520it%2520a%250Avaluable%2520decision%2520support%2520tool%2520for%2520primary%2520eye%2520care%2520or%2520an%2520online%2520LLM%2520for%2520fundus%250Aevaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20integrated%20language-vision%20foundation%20model%20for%20conversational%0A%20%20diagnostics%20and%20triaging%20in%20primary%20eye%20care&entry.906535625=Zhi%20Da%20Soh%20and%20Yang%20Bai%20and%20Kai%20Yu%20and%20Yang%20Zhou%20and%20Xiaofeng%20Lei%20and%20Sahil%20Thakur%20and%20Zann%20Lee%20and%20Lee%20Ching%20Linette%20Phang%20and%20Qingsheng%20Peng%20and%20Can%20Can%20Xue%20and%20Rachel%20Shujuan%20Chong%20and%20Quan%20V.%20Hoang%20and%20Lavanya%20Raghavan%20and%20Yih%20Chung%20Tham%20and%20Charumathi%20Sabanayagam%20and%20Wei-Chi%20Wu%20and%20Ming-Chih%20Ho%20and%20Jiangnan%20He%20and%20Preeti%20Gupta%20and%20Ecosse%20Lamoureux%20and%20Seang%20Mei%20Saw%20and%20Vinay%20Nangia%20and%20Songhomitra%20Panda-Jonas%20and%20Jie%20Xu%20and%20Ya%20Xing%20Wang%20and%20Xinxing%20Xu%20and%20Jost%20B.%20Jonas%20and%20Tien%20Yin%20Wong%20and%20Rick%20Siow%20Mong%20Goh%20and%20Yong%20Liu%20and%20Ching-Yu%20Cheng&entry.1292438233=%20%20Current%20deep%20learning%20models%20are%20mostly%20task%20specific%20and%20lack%20a%0Auser-friendly%20interface%20to%20operate.%20We%20present%20Meta-EyeFM%2C%20a%20multi-function%0Afoundation%20model%20that%20integrates%20a%20large%20language%20model%20%28LLM%29%20with%20vision%0Afoundation%20models%20%28VFMs%29%20for%20ocular%20disease%20assessment.%20Meta-EyeFM%20leverages%20a%0Arouting%20mechanism%20to%20enable%20accurate%20task-specific%20analysis%20based%20on%20text%0Aqueries.%20Using%20Low%20Rank%20Adaptation%2C%20we%20fine-tuned%20our%20VFMs%20to%20detect%20ocular%20and%0Asystemic%20diseases%2C%20differentiate%20ocular%20disease%20severity%2C%20and%20identify%20common%0Aocular%20signs.%20The%20model%20achieved%20100%25%20accuracy%20in%20routing%20fundus%20images%20to%0Aappropriate%20VFMs%2C%20which%20achieved%20%24%5Cge%24%2082.2%25%20accuracy%20in%20disease%20detection%2C%0A%24%5Cge%24%2089%25%20in%20severity%20differentiation%2C%20%24%5Cge%24%2076%25%20in%20sign%20identification.%0AMeta-EyeFM%20was%2011%25%20to%2043%25%20more%20accurate%20than%20Gemini-1.5-flash%20and%20ChatGPT-4o%0ALMMs%20in%20detecting%20various%20eye%20diseases%20and%20comparable%20to%20an%20ophthalmologist.%0AThis%20system%20offers%20enhanced%20usability%20and%20diagnostic%20performance%2C%20making%20it%20a%0Avaluable%20decision%20support%20tool%20for%20primary%20eye%20care%20or%20an%20online%20LLM%20for%20fundus%0Aevaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08414v1&entry.124074799=Read"},
{"title": "CAD-Coder:Text-Guided CAD Files Code Generation", "author": "Changqi He and Shuhan Zhang and Liguo Zhang and Jiajun Miao", "abstract": "  Computer-aided design (CAD) is a way to digitally create 2D drawings and 3D\nmodels of real-world products. Traditional CAD typically relies on hand-drawing\nby experts or modifications of existing library files, which doesn't allow for\nrapid personalization. With the emergence of generative artificial\nintelligence, convenient and efficient personalized CAD generation has become\npossible. However, existing generative methods typically produce outputs that\nlack interactive editability and geometric annotations, limiting their\npractical applications in manufacturing. To enable interactive generative CAD,\nwe propose CAD-Coder, a framework that transforms natural language instructions\ninto CAD script codes, which can be executed in Python environments to generate\nhuman-editable CAD files (.Dxf). To facilitate the generation of editable CAD\nsketches with annotation information, we construct a comprehensive dataset\ncomprising 29,130 Dxf files with their corresponding script codes, where each\nsketch preserves both editability and geometric annotations. We evaluate\nCAD-Coder on various 2D/3D CAD generation tasks against existing methods,\ndemonstrating superior interactive capabilities while uniquely providing\neditable sketches with geometric annotations.\n", "link": "http://arxiv.org/abs/2505.08686v1", "date": "2025-05-13", "relevancy": 2.7303, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6257}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5063}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD-Coder%3AText-Guided%20CAD%20Files%20Code%20Generation&body=Title%3A%20CAD-Coder%3AText-Guided%20CAD%20Files%20Code%20Generation%0AAuthor%3A%20Changqi%20He%20and%20Shuhan%20Zhang%20and%20Liguo%20Zhang%20and%20Jiajun%20Miao%0AAbstract%3A%20%20%20Computer-aided%20design%20%28CAD%29%20is%20a%20way%20to%20digitally%20create%202D%20drawings%20and%203D%0Amodels%20of%20real-world%20products.%20Traditional%20CAD%20typically%20relies%20on%20hand-drawing%0Aby%20experts%20or%20modifications%20of%20existing%20library%20files%2C%20which%20doesn%27t%20allow%20for%0Arapid%20personalization.%20With%20the%20emergence%20of%20generative%20artificial%0Aintelligence%2C%20convenient%20and%20efficient%20personalized%20CAD%20generation%20has%20become%0Apossible.%20However%2C%20existing%20generative%20methods%20typically%20produce%20outputs%20that%0Alack%20interactive%20editability%20and%20geometric%20annotations%2C%20limiting%20their%0Apractical%20applications%20in%20manufacturing.%20To%20enable%20interactive%20generative%20CAD%2C%0Awe%20propose%20CAD-Coder%2C%20a%20framework%20that%20transforms%20natural%20language%20instructions%0Ainto%20CAD%20script%20codes%2C%20which%20can%20be%20executed%20in%20Python%20environments%20to%20generate%0Ahuman-editable%20CAD%20files%20%28.Dxf%29.%20To%20facilitate%20the%20generation%20of%20editable%20CAD%0Asketches%20with%20annotation%20information%2C%20we%20construct%20a%20comprehensive%20dataset%0Acomprising%2029%2C130%20Dxf%20files%20with%20their%20corresponding%20script%20codes%2C%20where%20each%0Asketch%20preserves%20both%20editability%20and%20geometric%20annotations.%20We%20evaluate%0ACAD-Coder%20on%20various%202D/3D%20CAD%20generation%20tasks%20against%20existing%20methods%2C%0Ademonstrating%20superior%20interactive%20capabilities%20while%20uniquely%20providing%0Aeditable%20sketches%20with%20geometric%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD-Coder%253AText-Guided%2520CAD%2520Files%2520Code%2520Generation%26entry.906535625%3DChangqi%2520He%2520and%2520Shuhan%2520Zhang%2520and%2520Liguo%2520Zhang%2520and%2520Jiajun%2520Miao%26entry.1292438233%3D%2520%2520Computer-aided%2520design%2520%2528CAD%2529%2520is%2520a%2520way%2520to%2520digitally%2520create%25202D%2520drawings%2520and%25203D%250Amodels%2520of%2520real-world%2520products.%2520Traditional%2520CAD%2520typically%2520relies%2520on%2520hand-drawing%250Aby%2520experts%2520or%2520modifications%2520of%2520existing%2520library%2520files%252C%2520which%2520doesn%2527t%2520allow%2520for%250Arapid%2520personalization.%2520With%2520the%2520emergence%2520of%2520generative%2520artificial%250Aintelligence%252C%2520convenient%2520and%2520efficient%2520personalized%2520CAD%2520generation%2520has%2520become%250Apossible.%2520However%252C%2520existing%2520generative%2520methods%2520typically%2520produce%2520outputs%2520that%250Alack%2520interactive%2520editability%2520and%2520geometric%2520annotations%252C%2520limiting%2520their%250Apractical%2520applications%2520in%2520manufacturing.%2520To%2520enable%2520interactive%2520generative%2520CAD%252C%250Awe%2520propose%2520CAD-Coder%252C%2520a%2520framework%2520that%2520transforms%2520natural%2520language%2520instructions%250Ainto%2520CAD%2520script%2520codes%252C%2520which%2520can%2520be%2520executed%2520in%2520Python%2520environments%2520to%2520generate%250Ahuman-editable%2520CAD%2520files%2520%2528.Dxf%2529.%2520To%2520facilitate%2520the%2520generation%2520of%2520editable%2520CAD%250Asketches%2520with%2520annotation%2520information%252C%2520we%2520construct%2520a%2520comprehensive%2520dataset%250Acomprising%252029%252C130%2520Dxf%2520files%2520with%2520their%2520corresponding%2520script%2520codes%252C%2520where%2520each%250Asketch%2520preserves%2520both%2520editability%2520and%2520geometric%2520annotations.%2520We%2520evaluate%250ACAD-Coder%2520on%2520various%25202D/3D%2520CAD%2520generation%2520tasks%2520against%2520existing%2520methods%252C%250Ademonstrating%2520superior%2520interactive%2520capabilities%2520while%2520uniquely%2520providing%250Aeditable%2520sketches%2520with%2520geometric%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-Coder%3AText-Guided%20CAD%20Files%20Code%20Generation&entry.906535625=Changqi%20He%20and%20Shuhan%20Zhang%20and%20Liguo%20Zhang%20and%20Jiajun%20Miao&entry.1292438233=%20%20Computer-aided%20design%20%28CAD%29%20is%20a%20way%20to%20digitally%20create%202D%20drawings%20and%203D%0Amodels%20of%20real-world%20products.%20Traditional%20CAD%20typically%20relies%20on%20hand-drawing%0Aby%20experts%20or%20modifications%20of%20existing%20library%20files%2C%20which%20doesn%27t%20allow%20for%0Arapid%20personalization.%20With%20the%20emergence%20of%20generative%20artificial%0Aintelligence%2C%20convenient%20and%20efficient%20personalized%20CAD%20generation%20has%20become%0Apossible.%20However%2C%20existing%20generative%20methods%20typically%20produce%20outputs%20that%0Alack%20interactive%20editability%20and%20geometric%20annotations%2C%20limiting%20their%0Apractical%20applications%20in%20manufacturing.%20To%20enable%20interactive%20generative%20CAD%2C%0Awe%20propose%20CAD-Coder%2C%20a%20framework%20that%20transforms%20natural%20language%20instructions%0Ainto%20CAD%20script%20codes%2C%20which%20can%20be%20executed%20in%20Python%20environments%20to%20generate%0Ahuman-editable%20CAD%20files%20%28.Dxf%29.%20To%20facilitate%20the%20generation%20of%20editable%20CAD%0Asketches%20with%20annotation%20information%2C%20we%20construct%20a%20comprehensive%20dataset%0Acomprising%2029%2C130%20Dxf%20files%20with%20their%20corresponding%20script%20codes%2C%20where%20each%0Asketch%20preserves%20both%20editability%20and%20geometric%20annotations.%20We%20evaluate%0ACAD-Coder%20on%20various%202D/3D%20CAD%20generation%20tasks%20against%20existing%20methods%2C%0Ademonstrating%20superior%20interactive%20capabilities%20while%20uniquely%20providing%0Aeditable%20sketches%20with%20geometric%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08686v1&entry.124074799=Read"},
{"title": "MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for\n  Few-Shot WSI Classification", "author": "Anh-Tien Nguyen and Duy Minh Ho Nguyen and Nghiem Tuong Diep and Trung Quoc Nguyen and Nhat Ho and Jacqueline Michelle Metsch and Miriam Cindy Maurer and Daniel Sonntag and Hanibal Bohnenberger and Anne-Christin Hauschild", "abstract": "  Whole slide pathology image classification presents challenges due to\ngigapixel image sizes and limited annotation labels, hindering model\ngeneralization. This paper introduces a prompt learning method to adapt large\nvision-language models for few-shot pathology classification. We first extend\nthe Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology\nimage tiles, into a vision-language model by adding adaptors and aligning it\nwith medical text encoders via contrastive learning on 923K image-text pairs.\nThe model is then used to extract visual features and text embeddings from\nfew-shot annotations and fine-tunes with learnable prompt embeddings. Unlike\nprior methods that combine prompts with frozen features using prefix embeddings\nor self-attention, we propose multi-granular attention that compares\ninteractions between learnable prompts with individual image patches and groups\nof them. This approach improves the model's ability to capture both\nfine-grained details and broader context, enhancing its recognition of complex\npatterns across sub-regions. To further improve accuracy, we leverage\n(unbalanced) optimal transport-based visual-text distance to secure model\nrobustness by mitigating perturbations that might occur during the data\naugmentation process. Empirical experiments on lung, kidney, and breast\npathology modalities validate the effectiveness of our approach; thereby, we\nsurpass several of the latest competitors and consistently improve performance\nacross diverse architectures, including CLIP, PLIP, and Prov-GigaPath\nintegrated PLIP. We release our implementations and pre-trained models at this\nMGPATH.\n", "link": "http://arxiv.org/abs/2502.07409v2", "date": "2025-05-13", "relevancy": 2.6947, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGPATH%3A%20Vision-Language%20Model%20with%20Multi-Granular%20Prompt%20Learning%20for%0A%20%20Few-Shot%20WSI%20Classification&body=Title%3A%20MGPATH%3A%20Vision-Language%20Model%20with%20Multi-Granular%20Prompt%20Learning%20for%0A%20%20Few-Shot%20WSI%20Classification%0AAuthor%3A%20Anh-Tien%20Nguyen%20and%20Duy%20Minh%20Ho%20Nguyen%20and%20Nghiem%20Tuong%20Diep%20and%20Trung%20Quoc%20Nguyen%20and%20Nhat%20Ho%20and%20Jacqueline%20Michelle%20Metsch%20and%20Miriam%20Cindy%20Maurer%20and%20Daniel%20Sonntag%20and%20Hanibal%20Bohnenberger%20and%20Anne-Christin%20Hauschild%0AAbstract%3A%20%20%20Whole%20slide%20pathology%20image%20classification%20presents%20challenges%20due%20to%0Agigapixel%20image%20sizes%20and%20limited%20annotation%20labels%2C%20hindering%20model%0Ageneralization.%20This%20paper%20introduces%20a%20prompt%20learning%20method%20to%20adapt%20large%0Avision-language%20models%20for%20few-shot%20pathology%20classification.%20We%20first%20extend%0Athe%20Prov-GigaPath%20vision%20foundation%20model%2C%20pre-trained%20on%201.3%20billion%20pathology%0Aimage%20tiles%2C%20into%20a%20vision-language%20model%20by%20adding%20adaptors%20and%20aligning%20it%0Awith%20medical%20text%20encoders%20via%20contrastive%20learning%20on%20923K%20image-text%20pairs.%0AThe%20model%20is%20then%20used%20to%20extract%20visual%20features%20and%20text%20embeddings%20from%0Afew-shot%20annotations%20and%20fine-tunes%20with%20learnable%20prompt%20embeddings.%20Unlike%0Aprior%20methods%20that%20combine%20prompts%20with%20frozen%20features%20using%20prefix%20embeddings%0Aor%20self-attention%2C%20we%20propose%20multi-granular%20attention%20that%20compares%0Ainteractions%20between%20learnable%20prompts%20with%20individual%20image%20patches%20and%20groups%0Aof%20them.%20This%20approach%20improves%20the%20model%27s%20ability%20to%20capture%20both%0Afine-grained%20details%20and%20broader%20context%2C%20enhancing%20its%20recognition%20of%20complex%0Apatterns%20across%20sub-regions.%20To%20further%20improve%20accuracy%2C%20we%20leverage%0A%28unbalanced%29%20optimal%20transport-based%20visual-text%20distance%20to%20secure%20model%0Arobustness%20by%20mitigating%20perturbations%20that%20might%20occur%20during%20the%20data%0Aaugmentation%20process.%20Empirical%20experiments%20on%20lung%2C%20kidney%2C%20and%20breast%0Apathology%20modalities%20validate%20the%20effectiveness%20of%20our%20approach%3B%20thereby%2C%20we%0Asurpass%20several%20of%20the%20latest%20competitors%20and%20consistently%20improve%20performance%0Aacross%20diverse%20architectures%2C%20including%20CLIP%2C%20PLIP%2C%20and%20Prov-GigaPath%0Aintegrated%20PLIP.%20We%20release%20our%20implementations%20and%20pre-trained%20models%20at%20this%0AMGPATH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07409v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGPATH%253A%2520Vision-Language%2520Model%2520with%2520Multi-Granular%2520Prompt%2520Learning%2520for%250A%2520%2520Few-Shot%2520WSI%2520Classification%26entry.906535625%3DAnh-Tien%2520Nguyen%2520and%2520Duy%2520Minh%2520Ho%2520Nguyen%2520and%2520Nghiem%2520Tuong%2520Diep%2520and%2520Trung%2520Quoc%2520Nguyen%2520and%2520Nhat%2520Ho%2520and%2520Jacqueline%2520Michelle%2520Metsch%2520and%2520Miriam%2520Cindy%2520Maurer%2520and%2520Daniel%2520Sonntag%2520and%2520Hanibal%2520Bohnenberger%2520and%2520Anne-Christin%2520Hauschild%26entry.1292438233%3D%2520%2520Whole%2520slide%2520pathology%2520image%2520classification%2520presents%2520challenges%2520due%2520to%250Agigapixel%2520image%2520sizes%2520and%2520limited%2520annotation%2520labels%252C%2520hindering%2520model%250Ageneralization.%2520This%2520paper%2520introduces%2520a%2520prompt%2520learning%2520method%2520to%2520adapt%2520large%250Avision-language%2520models%2520for%2520few-shot%2520pathology%2520classification.%2520We%2520first%2520extend%250Athe%2520Prov-GigaPath%2520vision%2520foundation%2520model%252C%2520pre-trained%2520on%25201.3%2520billion%2520pathology%250Aimage%2520tiles%252C%2520into%2520a%2520vision-language%2520model%2520by%2520adding%2520adaptors%2520and%2520aligning%2520it%250Awith%2520medical%2520text%2520encoders%2520via%2520contrastive%2520learning%2520on%2520923K%2520image-text%2520pairs.%250AThe%2520model%2520is%2520then%2520used%2520to%2520extract%2520visual%2520features%2520and%2520text%2520embeddings%2520from%250Afew-shot%2520annotations%2520and%2520fine-tunes%2520with%2520learnable%2520prompt%2520embeddings.%2520Unlike%250Aprior%2520methods%2520that%2520combine%2520prompts%2520with%2520frozen%2520features%2520using%2520prefix%2520embeddings%250Aor%2520self-attention%252C%2520we%2520propose%2520multi-granular%2520attention%2520that%2520compares%250Ainteractions%2520between%2520learnable%2520prompts%2520with%2520individual%2520image%2520patches%2520and%2520groups%250Aof%2520them.%2520This%2520approach%2520improves%2520the%2520model%2527s%2520ability%2520to%2520capture%2520both%250Afine-grained%2520details%2520and%2520broader%2520context%252C%2520enhancing%2520its%2520recognition%2520of%2520complex%250Apatterns%2520across%2520sub-regions.%2520To%2520further%2520improve%2520accuracy%252C%2520we%2520leverage%250A%2528unbalanced%2529%2520optimal%2520transport-based%2520visual-text%2520distance%2520to%2520secure%2520model%250Arobustness%2520by%2520mitigating%2520perturbations%2520that%2520might%2520occur%2520during%2520the%2520data%250Aaugmentation%2520process.%2520Empirical%2520experiments%2520on%2520lung%252C%2520kidney%252C%2520and%2520breast%250Apathology%2520modalities%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%253B%2520thereby%252C%2520we%250Asurpass%2520several%2520of%2520the%2520latest%2520competitors%2520and%2520consistently%2520improve%2520performance%250Aacross%2520diverse%2520architectures%252C%2520including%2520CLIP%252C%2520PLIP%252C%2520and%2520Prov-GigaPath%250Aintegrated%2520PLIP.%2520We%2520release%2520our%2520implementations%2520and%2520pre-trained%2520models%2520at%2520this%250AMGPATH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07409v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGPATH%3A%20Vision-Language%20Model%20with%20Multi-Granular%20Prompt%20Learning%20for%0A%20%20Few-Shot%20WSI%20Classification&entry.906535625=Anh-Tien%20Nguyen%20and%20Duy%20Minh%20Ho%20Nguyen%20and%20Nghiem%20Tuong%20Diep%20and%20Trung%20Quoc%20Nguyen%20and%20Nhat%20Ho%20and%20Jacqueline%20Michelle%20Metsch%20and%20Miriam%20Cindy%20Maurer%20and%20Daniel%20Sonntag%20and%20Hanibal%20Bohnenberger%20and%20Anne-Christin%20Hauschild&entry.1292438233=%20%20Whole%20slide%20pathology%20image%20classification%20presents%20challenges%20due%20to%0Agigapixel%20image%20sizes%20and%20limited%20annotation%20labels%2C%20hindering%20model%0Ageneralization.%20This%20paper%20introduces%20a%20prompt%20learning%20method%20to%20adapt%20large%0Avision-language%20models%20for%20few-shot%20pathology%20classification.%20We%20first%20extend%0Athe%20Prov-GigaPath%20vision%20foundation%20model%2C%20pre-trained%20on%201.3%20billion%20pathology%0Aimage%20tiles%2C%20into%20a%20vision-language%20model%20by%20adding%20adaptors%20and%20aligning%20it%0Awith%20medical%20text%20encoders%20via%20contrastive%20learning%20on%20923K%20image-text%20pairs.%0AThe%20model%20is%20then%20used%20to%20extract%20visual%20features%20and%20text%20embeddings%20from%0Afew-shot%20annotations%20and%20fine-tunes%20with%20learnable%20prompt%20embeddings.%20Unlike%0Aprior%20methods%20that%20combine%20prompts%20with%20frozen%20features%20using%20prefix%20embeddings%0Aor%20self-attention%2C%20we%20propose%20multi-granular%20attention%20that%20compares%0Ainteractions%20between%20learnable%20prompts%20with%20individual%20image%20patches%20and%20groups%0Aof%20them.%20This%20approach%20improves%20the%20model%27s%20ability%20to%20capture%20both%0Afine-grained%20details%20and%20broader%20context%2C%20enhancing%20its%20recognition%20of%20complex%0Apatterns%20across%20sub-regions.%20To%20further%20improve%20accuracy%2C%20we%20leverage%0A%28unbalanced%29%20optimal%20transport-based%20visual-text%20distance%20to%20secure%20model%0Arobustness%20by%20mitigating%20perturbations%20that%20might%20occur%20during%20the%20data%0Aaugmentation%20process.%20Empirical%20experiments%20on%20lung%2C%20kidney%2C%20and%20breast%0Apathology%20modalities%20validate%20the%20effectiveness%20of%20our%20approach%3B%20thereby%2C%20we%0Asurpass%20several%20of%20the%20latest%20competitors%20and%20consistently%20improve%20performance%0Aacross%20diverse%20architectures%2C%20including%20CLIP%2C%20PLIP%2C%20and%20Prov-GigaPath%0Aintegrated%20PLIP.%20We%20release%20our%20implementations%20and%20pre-trained%20models%20at%20this%0AMGPATH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07409v2&entry.124074799=Read"},
{"title": "Leveraging Segment Anything Model for Source-Free Domain Adaptation via\n  Dual Feature Guided Auto-Prompting", "author": "Zheang Huai and Hui Tang and Yi Li and Zhuangzhuang Chen and Xiaomeng Li", "abstract": "  Source-free domain adaptation (SFDA) for segmentation aims at adapting a\nmodel trained in the source domain to perform well in the target domain with\nonly the source model and unlabeled target data.Inspired by the recent success\nof Segment Anything Model (SAM) which exhibits the generality of segmenting\nimages of various modalities and in different domains given human-annotated\nprompts like bounding boxes or points, we for the first time explore the\npotentials of Segment Anything Model for SFDA via automatedly finding an\naccurate bounding box prompt. We find that the bounding boxes directly\ngenerated with existing SFDA approaches are defective due to the domain gap.To\ntackle this issue, we propose a novel Dual Feature Guided (DFG) auto-prompting\napproach to search for the box prompt. Specifically, the source model is first\ntrained in a feature aggregation phase, which not only preliminarily adapts the\nsource model to the target domain but also builds a feature distribution\nwell-prepared for box prompt search. In the second phase, based on two feature\ndistribution observations, we gradually expand the box prompt with the guidance\nof the target model feature and the SAM feature to handle the class-wise\nclustered target features and the class-wise dispersed target features,\nrespectively. To remove the potentially enlarged false positive regions caused\nby the over-confident prediction of the target model, the refined pseudo-labels\nproduced by SAM are further postprocessed based on connectivity analysis.\nExperiments on 3D and 2D datasets indicate that our approach yields superior\nperformance compared to conventional methods. Code is available at\nhttps://github.com/zheangh/DFG.\n", "link": "http://arxiv.org/abs/2505.08527v1", "date": "2025-05-13", "relevancy": 2.6773, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5248}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Segment%20Anything%20Model%20for%20Source-Free%20Domain%20Adaptation%20via%0A%20%20Dual%20Feature%20Guided%20Auto-Prompting&body=Title%3A%20Leveraging%20Segment%20Anything%20Model%20for%20Source-Free%20Domain%20Adaptation%20via%0A%20%20Dual%20Feature%20Guided%20Auto-Prompting%0AAuthor%3A%20Zheang%20Huai%20and%20Hui%20Tang%20and%20Yi%20Li%20and%20Zhuangzhuang%20Chen%20and%20Xiaomeng%20Li%0AAbstract%3A%20%20%20Source-free%20domain%20adaptation%20%28SFDA%29%20for%20segmentation%20aims%20at%20adapting%20a%0Amodel%20trained%20in%20the%20source%20domain%20to%20perform%20well%20in%20the%20target%20domain%20with%0Aonly%20the%20source%20model%20and%20unlabeled%20target%20data.Inspired%20by%20the%20recent%20success%0Aof%20Segment%20Anything%20Model%20%28SAM%29%20which%20exhibits%20the%20generality%20of%20segmenting%0Aimages%20of%20various%20modalities%20and%20in%20different%20domains%20given%20human-annotated%0Aprompts%20like%20bounding%20boxes%20or%20points%2C%20we%20for%20the%20first%20time%20explore%20the%0Apotentials%20of%20Segment%20Anything%20Model%20for%20SFDA%20via%20automatedly%20finding%20an%0Aaccurate%20bounding%20box%20prompt.%20We%20find%20that%20the%20bounding%20boxes%20directly%0Agenerated%20with%20existing%20SFDA%20approaches%20are%20defective%20due%20to%20the%20domain%20gap.To%0Atackle%20this%20issue%2C%20we%20propose%20a%20novel%20Dual%20Feature%20Guided%20%28DFG%29%20auto-prompting%0Aapproach%20to%20search%20for%20the%20box%20prompt.%20Specifically%2C%20the%20source%20model%20is%20first%0Atrained%20in%20a%20feature%20aggregation%20phase%2C%20which%20not%20only%20preliminarily%20adapts%20the%0Asource%20model%20to%20the%20target%20domain%20but%20also%20builds%20a%20feature%20distribution%0Awell-prepared%20for%20box%20prompt%20search.%20In%20the%20second%20phase%2C%20based%20on%20two%20feature%0Adistribution%20observations%2C%20we%20gradually%20expand%20the%20box%20prompt%20with%20the%20guidance%0Aof%20the%20target%20model%20feature%20and%20the%20SAM%20feature%20to%20handle%20the%20class-wise%0Aclustered%20target%20features%20and%20the%20class-wise%20dispersed%20target%20features%2C%0Arespectively.%20To%20remove%20the%20potentially%20enlarged%20false%20positive%20regions%20caused%0Aby%20the%20over-confident%20prediction%20of%20the%20target%20model%2C%20the%20refined%20pseudo-labels%0Aproduced%20by%20SAM%20are%20further%20postprocessed%20based%20on%20connectivity%20analysis.%0AExperiments%20on%203D%20and%202D%20datasets%20indicate%20that%20our%20approach%20yields%20superior%0Aperformance%20compared%20to%20conventional%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zheangh/DFG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Segment%2520Anything%2520Model%2520for%2520Source-Free%2520Domain%2520Adaptation%2520via%250A%2520%2520Dual%2520Feature%2520Guided%2520Auto-Prompting%26entry.906535625%3DZheang%2520Huai%2520and%2520Hui%2520Tang%2520and%2520Yi%2520Li%2520and%2520Zhuangzhuang%2520Chen%2520and%2520Xiaomeng%2520Li%26entry.1292438233%3D%2520%2520Source-free%2520domain%2520adaptation%2520%2528SFDA%2529%2520for%2520segmentation%2520aims%2520at%2520adapting%2520a%250Amodel%2520trained%2520in%2520the%2520source%2520domain%2520to%2520perform%2520well%2520in%2520the%2520target%2520domain%2520with%250Aonly%2520the%2520source%2520model%2520and%2520unlabeled%2520target%2520data.Inspired%2520by%2520the%2520recent%2520success%250Aof%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520which%2520exhibits%2520the%2520generality%2520of%2520segmenting%250Aimages%2520of%2520various%2520modalities%2520and%2520in%2520different%2520domains%2520given%2520human-annotated%250Aprompts%2520like%2520bounding%2520boxes%2520or%2520points%252C%2520we%2520for%2520the%2520first%2520time%2520explore%2520the%250Apotentials%2520of%2520Segment%2520Anything%2520Model%2520for%2520SFDA%2520via%2520automatedly%2520finding%2520an%250Aaccurate%2520bounding%2520box%2520prompt.%2520We%2520find%2520that%2520the%2520bounding%2520boxes%2520directly%250Agenerated%2520with%2520existing%2520SFDA%2520approaches%2520are%2520defective%2520due%2520to%2520the%2520domain%2520gap.To%250Atackle%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520Dual%2520Feature%2520Guided%2520%2528DFG%2529%2520auto-prompting%250Aapproach%2520to%2520search%2520for%2520the%2520box%2520prompt.%2520Specifically%252C%2520the%2520source%2520model%2520is%2520first%250Atrained%2520in%2520a%2520feature%2520aggregation%2520phase%252C%2520which%2520not%2520only%2520preliminarily%2520adapts%2520the%250Asource%2520model%2520to%2520the%2520target%2520domain%2520but%2520also%2520builds%2520a%2520feature%2520distribution%250Awell-prepared%2520for%2520box%2520prompt%2520search.%2520In%2520the%2520second%2520phase%252C%2520based%2520on%2520two%2520feature%250Adistribution%2520observations%252C%2520we%2520gradually%2520expand%2520the%2520box%2520prompt%2520with%2520the%2520guidance%250Aof%2520the%2520target%2520model%2520feature%2520and%2520the%2520SAM%2520feature%2520to%2520handle%2520the%2520class-wise%250Aclustered%2520target%2520features%2520and%2520the%2520class-wise%2520dispersed%2520target%2520features%252C%250Arespectively.%2520To%2520remove%2520the%2520potentially%2520enlarged%2520false%2520positive%2520regions%2520caused%250Aby%2520the%2520over-confident%2520prediction%2520of%2520the%2520target%2520model%252C%2520the%2520refined%2520pseudo-labels%250Aproduced%2520by%2520SAM%2520are%2520further%2520postprocessed%2520based%2520on%2520connectivity%2520analysis.%250AExperiments%2520on%25203D%2520and%25202D%2520datasets%2520indicate%2520that%2520our%2520approach%2520yields%2520superior%250Aperformance%2520compared%2520to%2520conventional%2520methods.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/zheangh/DFG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Segment%20Anything%20Model%20for%20Source-Free%20Domain%20Adaptation%20via%0A%20%20Dual%20Feature%20Guided%20Auto-Prompting&entry.906535625=Zheang%20Huai%20and%20Hui%20Tang%20and%20Yi%20Li%20and%20Zhuangzhuang%20Chen%20and%20Xiaomeng%20Li&entry.1292438233=%20%20Source-free%20domain%20adaptation%20%28SFDA%29%20for%20segmentation%20aims%20at%20adapting%20a%0Amodel%20trained%20in%20the%20source%20domain%20to%20perform%20well%20in%20the%20target%20domain%20with%0Aonly%20the%20source%20model%20and%20unlabeled%20target%20data.Inspired%20by%20the%20recent%20success%0Aof%20Segment%20Anything%20Model%20%28SAM%29%20which%20exhibits%20the%20generality%20of%20segmenting%0Aimages%20of%20various%20modalities%20and%20in%20different%20domains%20given%20human-annotated%0Aprompts%20like%20bounding%20boxes%20or%20points%2C%20we%20for%20the%20first%20time%20explore%20the%0Apotentials%20of%20Segment%20Anything%20Model%20for%20SFDA%20via%20automatedly%20finding%20an%0Aaccurate%20bounding%20box%20prompt.%20We%20find%20that%20the%20bounding%20boxes%20directly%0Agenerated%20with%20existing%20SFDA%20approaches%20are%20defective%20due%20to%20the%20domain%20gap.To%0Atackle%20this%20issue%2C%20we%20propose%20a%20novel%20Dual%20Feature%20Guided%20%28DFG%29%20auto-prompting%0Aapproach%20to%20search%20for%20the%20box%20prompt.%20Specifically%2C%20the%20source%20model%20is%20first%0Atrained%20in%20a%20feature%20aggregation%20phase%2C%20which%20not%20only%20preliminarily%20adapts%20the%0Asource%20model%20to%20the%20target%20domain%20but%20also%20builds%20a%20feature%20distribution%0Awell-prepared%20for%20box%20prompt%20search.%20In%20the%20second%20phase%2C%20based%20on%20two%20feature%0Adistribution%20observations%2C%20we%20gradually%20expand%20the%20box%20prompt%20with%20the%20guidance%0Aof%20the%20target%20model%20feature%20and%20the%20SAM%20feature%20to%20handle%20the%20class-wise%0Aclustered%20target%20features%20and%20the%20class-wise%20dispersed%20target%20features%2C%0Arespectively.%20To%20remove%20the%20potentially%20enlarged%20false%20positive%20regions%20caused%0Aby%20the%20over-confident%20prediction%20of%20the%20target%20model%2C%20the%20refined%20pseudo-labels%0Aproduced%20by%20SAM%20are%20further%20postprocessed%20based%20on%20connectivity%20analysis.%0AExperiments%20on%203D%20and%202D%20datasets%20indicate%20that%20our%20approach%20yields%20superior%0Aperformance%20compared%20to%20conventional%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zheangh/DFG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08527v1&entry.124074799=Read"},
{"title": "CursorCore: Assist Programming through Aligning Anything", "author": "Hao Jiang and Qi Liu and Rui Li and Shengyu Ye and Shijin Wang", "abstract": "  Large language models have been successfully applied to programming\nassistance tasks, such as code completion, code insertion, and instructional\ncode editing. However, these applications remain insufficiently automated and\nstruggle to effectively integrate various types of information during the\nprogramming process, including coding history, current code, and user\ninstructions. In this work, we propose a new conversational framework that\ncomprehensively integrates these information sources, collect data to train our\nmodels and evaluate their performance. Firstly, to thoroughly evaluate how well\nmodels align with different types of information and the quality of their\noutputs, we introduce a new benchmark, APEval (Assist Programming Eval), to\ncomprehensively assess the performance of models in programming assistance\ntasks. Then, for data collection, we develop a data generation pipeline,\nProgramming-Instruct, which synthesizes training data from diverse sources,\nsuch as GitHub and online judge platforms. This pipeline can automatically\ngenerate various types of messages throughout the programming process. Finally,\nusing this pipeline, we generate 219K samples, fine-tune multiple models, and\ndevelop the CursorCore series. We show that CursorCore outperforms other models\nof comparable size. This framework unifies applications such as inline chat and\nautomated editing, contributes to the advancement of coding assistants. Code,\nmodels and data are freely available at\nhttps://github.com/TechxGenus/CursorCore.\n", "link": "http://arxiv.org/abs/2410.07002v3", "date": "2025-05-13", "relevancy": 2.671, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CursorCore%3A%20Assist%20Programming%20through%20Aligning%20Anything&body=Title%3A%20CursorCore%3A%20Assist%20Programming%20through%20Aligning%20Anything%0AAuthor%3A%20Hao%20Jiang%20and%20Qi%20Liu%20and%20Rui%20Li%20and%20Shengyu%20Ye%20and%20Shijin%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20have%20been%20successfully%20applied%20to%20programming%0Aassistance%20tasks%2C%20such%20as%20code%20completion%2C%20code%20insertion%2C%20and%20instructional%0Acode%20editing.%20However%2C%20these%20applications%20remain%20insufficiently%20automated%20and%0Astruggle%20to%20effectively%20integrate%20various%20types%20of%20information%20during%20the%0Aprogramming%20process%2C%20including%20coding%20history%2C%20current%20code%2C%20and%20user%0Ainstructions.%20In%20this%20work%2C%20we%20propose%20a%20new%20conversational%20framework%20that%0Acomprehensively%20integrates%20these%20information%20sources%2C%20collect%20data%20to%20train%20our%0Amodels%20and%20evaluate%20their%20performance.%20Firstly%2C%20to%20thoroughly%20evaluate%20how%20well%0Amodels%20align%20with%20different%20types%20of%20information%20and%20the%20quality%20of%20their%0Aoutputs%2C%20we%20introduce%20a%20new%20benchmark%2C%20APEval%20%28Assist%20Programming%20Eval%29%2C%20to%0Acomprehensively%20assess%20the%20performance%20of%20models%20in%20programming%20assistance%0Atasks.%20Then%2C%20for%20data%20collection%2C%20we%20develop%20a%20data%20generation%20pipeline%2C%0AProgramming-Instruct%2C%20which%20synthesizes%20training%20data%20from%20diverse%20sources%2C%0Asuch%20as%20GitHub%20and%20online%20judge%20platforms.%20This%20pipeline%20can%20automatically%0Agenerate%20various%20types%20of%20messages%20throughout%20the%20programming%20process.%20Finally%2C%0Ausing%20this%20pipeline%2C%20we%20generate%20219K%20samples%2C%20fine-tune%20multiple%20models%2C%20and%0Adevelop%20the%20CursorCore%20series.%20We%20show%20that%20CursorCore%20outperforms%20other%20models%0Aof%20comparable%20size.%20This%20framework%20unifies%20applications%20such%20as%20inline%20chat%20and%0Aautomated%20editing%2C%20contributes%20to%20the%20advancement%20of%20coding%20assistants.%20Code%2C%0Amodels%20and%20data%20are%20freely%20available%20at%0Ahttps%3A//github.com/TechxGenus/CursorCore.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07002v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCursorCore%253A%2520Assist%2520Programming%2520through%2520Aligning%2520Anything%26entry.906535625%3DHao%2520Jiang%2520and%2520Qi%2520Liu%2520and%2520Rui%2520Li%2520and%2520Shengyu%2520Ye%2520and%2520Shijin%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520been%2520successfully%2520applied%2520to%2520programming%250Aassistance%2520tasks%252C%2520such%2520as%2520code%2520completion%252C%2520code%2520insertion%252C%2520and%2520instructional%250Acode%2520editing.%2520However%252C%2520these%2520applications%2520remain%2520insufficiently%2520automated%2520and%250Astruggle%2520to%2520effectively%2520integrate%2520various%2520types%2520of%2520information%2520during%2520the%250Aprogramming%2520process%252C%2520including%2520coding%2520history%252C%2520current%2520code%252C%2520and%2520user%250Ainstructions.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520conversational%2520framework%2520that%250Acomprehensively%2520integrates%2520these%2520information%2520sources%252C%2520collect%2520data%2520to%2520train%2520our%250Amodels%2520and%2520evaluate%2520their%2520performance.%2520Firstly%252C%2520to%2520thoroughly%2520evaluate%2520how%2520well%250Amodels%2520align%2520with%2520different%2520types%2520of%2520information%2520and%2520the%2520quality%2520of%2520their%250Aoutputs%252C%2520we%2520introduce%2520a%2520new%2520benchmark%252C%2520APEval%2520%2528Assist%2520Programming%2520Eval%2529%252C%2520to%250Acomprehensively%2520assess%2520the%2520performance%2520of%2520models%2520in%2520programming%2520assistance%250Atasks.%2520Then%252C%2520for%2520data%2520collection%252C%2520we%2520develop%2520a%2520data%2520generation%2520pipeline%252C%250AProgramming-Instruct%252C%2520which%2520synthesizes%2520training%2520data%2520from%2520diverse%2520sources%252C%250Asuch%2520as%2520GitHub%2520and%2520online%2520judge%2520platforms.%2520This%2520pipeline%2520can%2520automatically%250Agenerate%2520various%2520types%2520of%2520messages%2520throughout%2520the%2520programming%2520process.%2520Finally%252C%250Ausing%2520this%2520pipeline%252C%2520we%2520generate%2520219K%2520samples%252C%2520fine-tune%2520multiple%2520models%252C%2520and%250Adevelop%2520the%2520CursorCore%2520series.%2520We%2520show%2520that%2520CursorCore%2520outperforms%2520other%2520models%250Aof%2520comparable%2520size.%2520This%2520framework%2520unifies%2520applications%2520such%2520as%2520inline%2520chat%2520and%250Aautomated%2520editing%252C%2520contributes%2520to%2520the%2520advancement%2520of%2520coding%2520assistants.%2520Code%252C%250Amodels%2520and%2520data%2520are%2520freely%2520available%2520at%250Ahttps%253A//github.com/TechxGenus/CursorCore.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07002v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CursorCore%3A%20Assist%20Programming%20through%20Aligning%20Anything&entry.906535625=Hao%20Jiang%20and%20Qi%20Liu%20and%20Rui%20Li%20and%20Shengyu%20Ye%20and%20Shijin%20Wang&entry.1292438233=%20%20Large%20language%20models%20have%20been%20successfully%20applied%20to%20programming%0Aassistance%20tasks%2C%20such%20as%20code%20completion%2C%20code%20insertion%2C%20and%20instructional%0Acode%20editing.%20However%2C%20these%20applications%20remain%20insufficiently%20automated%20and%0Astruggle%20to%20effectively%20integrate%20various%20types%20of%20information%20during%20the%0Aprogramming%20process%2C%20including%20coding%20history%2C%20current%20code%2C%20and%20user%0Ainstructions.%20In%20this%20work%2C%20we%20propose%20a%20new%20conversational%20framework%20that%0Acomprehensively%20integrates%20these%20information%20sources%2C%20collect%20data%20to%20train%20our%0Amodels%20and%20evaluate%20their%20performance.%20Firstly%2C%20to%20thoroughly%20evaluate%20how%20well%0Amodels%20align%20with%20different%20types%20of%20information%20and%20the%20quality%20of%20their%0Aoutputs%2C%20we%20introduce%20a%20new%20benchmark%2C%20APEval%20%28Assist%20Programming%20Eval%29%2C%20to%0Acomprehensively%20assess%20the%20performance%20of%20models%20in%20programming%20assistance%0Atasks.%20Then%2C%20for%20data%20collection%2C%20we%20develop%20a%20data%20generation%20pipeline%2C%0AProgramming-Instruct%2C%20which%20synthesizes%20training%20data%20from%20diverse%20sources%2C%0Asuch%20as%20GitHub%20and%20online%20judge%20platforms.%20This%20pipeline%20can%20automatically%0Agenerate%20various%20types%20of%20messages%20throughout%20the%20programming%20process.%20Finally%2C%0Ausing%20this%20pipeline%2C%20we%20generate%20219K%20samples%2C%20fine-tune%20multiple%20models%2C%20and%0Adevelop%20the%20CursorCore%20series.%20We%20show%20that%20CursorCore%20outperforms%20other%20models%0Aof%20comparable%20size.%20This%20framework%20unifies%20applications%20such%20as%20inline%20chat%20and%0Aautomated%20editing%2C%20contributes%20to%20the%20advancement%20of%20coding%20assistants.%20Code%2C%0Amodels%20and%20data%20are%20freely%20available%20at%0Ahttps%3A//github.com/TechxGenus/CursorCore.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07002v3&entry.124074799=Read"},
{"title": "An adaptive sampling algorithm for data-generation to build a\n  data-manifold for physical problem surrogate modeling", "author": "Chetra Mang and Axel TahmasebiMoradi and David Danan and Mouadh Yagoubi", "abstract": "  Physical models classically involved Partial Differential equations (PDE) and\ndepending of their underlying complexity and the level of accuracy required,\nand known to be computationally expensive to numerically solve them. Thus, an\nidea would be to create a surrogate model relying on data generated by such\nsolver. However, training such a model on an imbalanced data have been shown to\nbe a very difficult task. Indeed, if the distribution of input leads to a poor\nresponse manifold representation, the model may not learn well and\nconsequently, it may not predict the outcome with acceptable accuracy. In this\nwork, we present an Adaptive Sampling Algorithm for Data Generation (ASADG)\ninvolving a physical model. As the initial input data may not accurately\nrepresent the response manifold in higher dimension, this algorithm iteratively\nadds input data into it. At each step the barycenter of each simplicial\ncomplex, that the manifold is discretized into, is added as new input data, if\na certain threshold is satisfied. We demonstrate the efficiency of the data\nsampling algorithm in comparison with LHS method for generating more\nrepresentative input data. To do so, we focus on the construction of a harmonic\ntransport problem metamodel by generating data through a classical solver. By\nusing such algorithm, it is possible to generate the same number of input data\nas LHS while providing a better representation of the response manifold.\n", "link": "http://arxiv.org/abs/2505.08487v1", "date": "2025-05-13", "relevancy": 2.6339, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5297}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.528}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20adaptive%20sampling%20algorithm%20for%20data-generation%20to%20build%20a%0A%20%20data-manifold%20for%20physical%20problem%20surrogate%20modeling&body=Title%3A%20An%20adaptive%20sampling%20algorithm%20for%20data-generation%20to%20build%20a%0A%20%20data-manifold%20for%20physical%20problem%20surrogate%20modeling%0AAuthor%3A%20Chetra%20Mang%20and%20Axel%20TahmasebiMoradi%20and%20David%20Danan%20and%20Mouadh%20Yagoubi%0AAbstract%3A%20%20%20Physical%20models%20classically%20involved%20Partial%20Differential%20equations%20%28PDE%29%20and%0Adepending%20of%20their%20underlying%20complexity%20and%20the%20level%20of%20accuracy%20required%2C%0Aand%20known%20to%20be%20computationally%20expensive%20to%20numerically%20solve%20them.%20Thus%2C%20an%0Aidea%20would%20be%20to%20create%20a%20surrogate%20model%20relying%20on%20data%20generated%20by%20such%0Asolver.%20However%2C%20training%20such%20a%20model%20on%20an%20imbalanced%20data%20have%20been%20shown%20to%0Abe%20a%20very%20difficult%20task.%20Indeed%2C%20if%20the%20distribution%20of%20input%20leads%20to%20a%20poor%0Aresponse%20manifold%20representation%2C%20the%20model%20may%20not%20learn%20well%20and%0Aconsequently%2C%20it%20may%20not%20predict%20the%20outcome%20with%20acceptable%20accuracy.%20In%20this%0Awork%2C%20we%20present%20an%20Adaptive%20Sampling%20Algorithm%20for%20Data%20Generation%20%28ASADG%29%0Ainvolving%20a%20physical%20model.%20As%20the%20initial%20input%20data%20may%20not%20accurately%0Arepresent%20the%20response%20manifold%20in%20higher%20dimension%2C%20this%20algorithm%20iteratively%0Aadds%20input%20data%20into%20it.%20At%20each%20step%20the%20barycenter%20of%20each%20simplicial%0Acomplex%2C%20that%20the%20manifold%20is%20discretized%20into%2C%20is%20added%20as%20new%20input%20data%2C%20if%0Aa%20certain%20threshold%20is%20satisfied.%20We%20demonstrate%20the%20efficiency%20of%20the%20data%0Asampling%20algorithm%20in%20comparison%20with%20LHS%20method%20for%20generating%20more%0Arepresentative%20input%20data.%20To%20do%20so%2C%20we%20focus%20on%20the%20construction%20of%20a%20harmonic%0Atransport%20problem%20metamodel%20by%20generating%20data%20through%20a%20classical%20solver.%20By%0Ausing%20such%20algorithm%2C%20it%20is%20possible%20to%20generate%20the%20same%20number%20of%20input%20data%0Aas%20LHS%20while%20providing%20a%20better%20representation%20of%20the%20response%20manifold.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520adaptive%2520sampling%2520algorithm%2520for%2520data-generation%2520to%2520build%2520a%250A%2520%2520data-manifold%2520for%2520physical%2520problem%2520surrogate%2520modeling%26entry.906535625%3DChetra%2520Mang%2520and%2520Axel%2520TahmasebiMoradi%2520and%2520David%2520Danan%2520and%2520Mouadh%2520Yagoubi%26entry.1292438233%3D%2520%2520Physical%2520models%2520classically%2520involved%2520Partial%2520Differential%2520equations%2520%2528PDE%2529%2520and%250Adepending%2520of%2520their%2520underlying%2520complexity%2520and%2520the%2520level%2520of%2520accuracy%2520required%252C%250Aand%2520known%2520to%2520be%2520computationally%2520expensive%2520to%2520numerically%2520solve%2520them.%2520Thus%252C%2520an%250Aidea%2520would%2520be%2520to%2520create%2520a%2520surrogate%2520model%2520relying%2520on%2520data%2520generated%2520by%2520such%250Asolver.%2520However%252C%2520training%2520such%2520a%2520model%2520on%2520an%2520imbalanced%2520data%2520have%2520been%2520shown%2520to%250Abe%2520a%2520very%2520difficult%2520task.%2520Indeed%252C%2520if%2520the%2520distribution%2520of%2520input%2520leads%2520to%2520a%2520poor%250Aresponse%2520manifold%2520representation%252C%2520the%2520model%2520may%2520not%2520learn%2520well%2520and%250Aconsequently%252C%2520it%2520may%2520not%2520predict%2520the%2520outcome%2520with%2520acceptable%2520accuracy.%2520In%2520this%250Awork%252C%2520we%2520present%2520an%2520Adaptive%2520Sampling%2520Algorithm%2520for%2520Data%2520Generation%2520%2528ASADG%2529%250Ainvolving%2520a%2520physical%2520model.%2520As%2520the%2520initial%2520input%2520data%2520may%2520not%2520accurately%250Arepresent%2520the%2520response%2520manifold%2520in%2520higher%2520dimension%252C%2520this%2520algorithm%2520iteratively%250Aadds%2520input%2520data%2520into%2520it.%2520At%2520each%2520step%2520the%2520barycenter%2520of%2520each%2520simplicial%250Acomplex%252C%2520that%2520the%2520manifold%2520is%2520discretized%2520into%252C%2520is%2520added%2520as%2520new%2520input%2520data%252C%2520if%250Aa%2520certain%2520threshold%2520is%2520satisfied.%2520We%2520demonstrate%2520the%2520efficiency%2520of%2520the%2520data%250Asampling%2520algorithm%2520in%2520comparison%2520with%2520LHS%2520method%2520for%2520generating%2520more%250Arepresentative%2520input%2520data.%2520To%2520do%2520so%252C%2520we%2520focus%2520on%2520the%2520construction%2520of%2520a%2520harmonic%250Atransport%2520problem%2520metamodel%2520by%2520generating%2520data%2520through%2520a%2520classical%2520solver.%2520By%250Ausing%2520such%2520algorithm%252C%2520it%2520is%2520possible%2520to%2520generate%2520the%2520same%2520number%2520of%2520input%2520data%250Aas%2520LHS%2520while%2520providing%2520a%2520better%2520representation%2520of%2520the%2520response%2520manifold.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20adaptive%20sampling%20algorithm%20for%20data-generation%20to%20build%20a%0A%20%20data-manifold%20for%20physical%20problem%20surrogate%20modeling&entry.906535625=Chetra%20Mang%20and%20Axel%20TahmasebiMoradi%20and%20David%20Danan%20and%20Mouadh%20Yagoubi&entry.1292438233=%20%20Physical%20models%20classically%20involved%20Partial%20Differential%20equations%20%28PDE%29%20and%0Adepending%20of%20their%20underlying%20complexity%20and%20the%20level%20of%20accuracy%20required%2C%0Aand%20known%20to%20be%20computationally%20expensive%20to%20numerically%20solve%20them.%20Thus%2C%20an%0Aidea%20would%20be%20to%20create%20a%20surrogate%20model%20relying%20on%20data%20generated%20by%20such%0Asolver.%20However%2C%20training%20such%20a%20model%20on%20an%20imbalanced%20data%20have%20been%20shown%20to%0Abe%20a%20very%20difficult%20task.%20Indeed%2C%20if%20the%20distribution%20of%20input%20leads%20to%20a%20poor%0Aresponse%20manifold%20representation%2C%20the%20model%20may%20not%20learn%20well%20and%0Aconsequently%2C%20it%20may%20not%20predict%20the%20outcome%20with%20acceptable%20accuracy.%20In%20this%0Awork%2C%20we%20present%20an%20Adaptive%20Sampling%20Algorithm%20for%20Data%20Generation%20%28ASADG%29%0Ainvolving%20a%20physical%20model.%20As%20the%20initial%20input%20data%20may%20not%20accurately%0Arepresent%20the%20response%20manifold%20in%20higher%20dimension%2C%20this%20algorithm%20iteratively%0Aadds%20input%20data%20into%20it.%20At%20each%20step%20the%20barycenter%20of%20each%20simplicial%0Acomplex%2C%20that%20the%20manifold%20is%20discretized%20into%2C%20is%20added%20as%20new%20input%20data%2C%20if%0Aa%20certain%20threshold%20is%20satisfied.%20We%20demonstrate%20the%20efficiency%20of%20the%20data%0Asampling%20algorithm%20in%20comparison%20with%20LHS%20method%20for%20generating%20more%0Arepresentative%20input%20data.%20To%20do%20so%2C%20we%20focus%20on%20the%20construction%20of%20a%20harmonic%0Atransport%20problem%20metamodel%20by%20generating%20data%20through%20a%20classical%20solver.%20By%0Ausing%20such%20algorithm%2C%20it%20is%20possible%20to%20generate%20the%20same%20number%20of%20input%20data%0Aas%20LHS%20while%20providing%20a%20better%20representation%20of%20the%20response%20manifold.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08487v1&entry.124074799=Read"},
{"title": "SPAT: Sensitivity-based Multihead-attention Pruning on Time Series\n  Forecasting Models", "author": "Suhan Guo and Jiahong Deng and Mengjun Yi and Furao Shen and Jian Zhao", "abstract": "  Attention-based architectures have achieved superior performance in\nmultivariate time series forecasting but are computationally expensive.\nTechniques such as patching and adaptive masking have been developed to reduce\ntheir sizes and latencies. In this work, we propose a structured pruning\nmethod, SPAT ($\\textbf{S}$ensitivity $\\textbf{P}$runer for\n$\\textbf{At}$tention), which selectively removes redundant attention mechanisms\nand yields highly effective models. Different from previous approaches, SPAT\naims to remove the entire attention module, which reduces the risk of\noverfitting and enables speed-up without demanding specialized hardware. We\npropose a dynamic sensitivity metric, $\\textbf{S}$ensitivity\n$\\textbf{E}$nhanced $\\textbf{N}$ormalized $\\textbf{D}$ispersion (SEND) that\nmeasures the importance of each attention module during the pre-training phase.\nExperiments on multivariate datasets demonstrate that SPAT-pruned models\nachieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs.\nFurthermore, SPAT-pruned models outperform existing lightweight, Mamba-based\nand LLM-based SOTA methods in both standard and zero-shot inference,\nhighlighting the importance of retaining only the most effective attention\nmechanisms. We have made our code publicly available\nhttps://anonymous.4open.science/r/SPAT-6042.\n", "link": "http://arxiv.org/abs/2505.08768v1", "date": "2025-05-13", "relevancy": 2.6093, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5451}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5278}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPAT%3A%20Sensitivity-based%20Multihead-attention%20Pruning%20on%20Time%20Series%0A%20%20Forecasting%20Models&body=Title%3A%20SPAT%3A%20Sensitivity-based%20Multihead-attention%20Pruning%20on%20Time%20Series%0A%20%20Forecasting%20Models%0AAuthor%3A%20Suhan%20Guo%20and%20Jiahong%20Deng%20and%20Mengjun%20Yi%20and%20Furao%20Shen%20and%20Jian%20Zhao%0AAbstract%3A%20%20%20Attention-based%20architectures%20have%20achieved%20superior%20performance%20in%0Amultivariate%20time%20series%20forecasting%20but%20are%20computationally%20expensive.%0ATechniques%20such%20as%20patching%20and%20adaptive%20masking%20have%20been%20developed%20to%20reduce%0Atheir%20sizes%20and%20latencies.%20In%20this%20work%2C%20we%20propose%20a%20structured%20pruning%0Amethod%2C%20SPAT%20%28%24%5Ctextbf%7BS%7D%24ensitivity%20%24%5Ctextbf%7BP%7D%24runer%20for%0A%24%5Ctextbf%7BAt%7D%24tention%29%2C%20which%20selectively%20removes%20redundant%20attention%20mechanisms%0Aand%20yields%20highly%20effective%20models.%20Different%20from%20previous%20approaches%2C%20SPAT%0Aaims%20to%20remove%20the%20entire%20attention%20module%2C%20which%20reduces%20the%20risk%20of%0Aoverfitting%20and%20enables%20speed-up%20without%20demanding%20specialized%20hardware.%20We%0Apropose%20a%20dynamic%20sensitivity%20metric%2C%20%24%5Ctextbf%7BS%7D%24ensitivity%0A%24%5Ctextbf%7BE%7D%24nhanced%20%24%5Ctextbf%7BN%7D%24ormalized%20%24%5Ctextbf%7BD%7D%24ispersion%20%28SEND%29%20that%0Ameasures%20the%20importance%20of%20each%20attention%20module%20during%20the%20pre-training%20phase.%0AExperiments%20on%20multivariate%20datasets%20demonstrate%20that%20SPAT-pruned%20models%0Aachieve%20reductions%20of%202.842%25%20in%20MSE%2C%201.996%25%20in%20MAE%2C%20and%2035.274%25%20in%20FLOPs.%0AFurthermore%2C%20SPAT-pruned%20models%20outperform%20existing%20lightweight%2C%20Mamba-based%0Aand%20LLM-based%20SOTA%20methods%20in%20both%20standard%20and%20zero-shot%20inference%2C%0Ahighlighting%20the%20importance%20of%20retaining%20only%20the%20most%20effective%20attention%0Amechanisms.%20We%20have%20made%20our%20code%20publicly%20available%0Ahttps%3A//anonymous.4open.science/r/SPAT-6042.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPAT%253A%2520Sensitivity-based%2520Multihead-attention%2520Pruning%2520on%2520Time%2520Series%250A%2520%2520Forecasting%2520Models%26entry.906535625%3DSuhan%2520Guo%2520and%2520Jiahong%2520Deng%2520and%2520Mengjun%2520Yi%2520and%2520Furao%2520Shen%2520and%2520Jian%2520Zhao%26entry.1292438233%3D%2520%2520Attention-based%2520architectures%2520have%2520achieved%2520superior%2520performance%2520in%250Amultivariate%2520time%2520series%2520forecasting%2520but%2520are%2520computationally%2520expensive.%250ATechniques%2520such%2520as%2520patching%2520and%2520adaptive%2520masking%2520have%2520been%2520developed%2520to%2520reduce%250Atheir%2520sizes%2520and%2520latencies.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520structured%2520pruning%250Amethod%252C%2520SPAT%2520%2528%2524%255Ctextbf%257BS%257D%2524ensitivity%2520%2524%255Ctextbf%257BP%257D%2524runer%2520for%250A%2524%255Ctextbf%257BAt%257D%2524tention%2529%252C%2520which%2520selectively%2520removes%2520redundant%2520attention%2520mechanisms%250Aand%2520yields%2520highly%2520effective%2520models.%2520Different%2520from%2520previous%2520approaches%252C%2520SPAT%250Aaims%2520to%2520remove%2520the%2520entire%2520attention%2520module%252C%2520which%2520reduces%2520the%2520risk%2520of%250Aoverfitting%2520and%2520enables%2520speed-up%2520without%2520demanding%2520specialized%2520hardware.%2520We%250Apropose%2520a%2520dynamic%2520sensitivity%2520metric%252C%2520%2524%255Ctextbf%257BS%257D%2524ensitivity%250A%2524%255Ctextbf%257BE%257D%2524nhanced%2520%2524%255Ctextbf%257BN%257D%2524ormalized%2520%2524%255Ctextbf%257BD%257D%2524ispersion%2520%2528SEND%2529%2520that%250Ameasures%2520the%2520importance%2520of%2520each%2520attention%2520module%2520during%2520the%2520pre-training%2520phase.%250AExperiments%2520on%2520multivariate%2520datasets%2520demonstrate%2520that%2520SPAT-pruned%2520models%250Aachieve%2520reductions%2520of%25202.842%2525%2520in%2520MSE%252C%25201.996%2525%2520in%2520MAE%252C%2520and%252035.274%2525%2520in%2520FLOPs.%250AFurthermore%252C%2520SPAT-pruned%2520models%2520outperform%2520existing%2520lightweight%252C%2520Mamba-based%250Aand%2520LLM-based%2520SOTA%2520methods%2520in%2520both%2520standard%2520and%2520zero-shot%2520inference%252C%250Ahighlighting%2520the%2520importance%2520of%2520retaining%2520only%2520the%2520most%2520effective%2520attention%250Amechanisms.%2520We%2520have%2520made%2520our%2520code%2520publicly%2520available%250Ahttps%253A//anonymous.4open.science/r/SPAT-6042.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPAT%3A%20Sensitivity-based%20Multihead-attention%20Pruning%20on%20Time%20Series%0A%20%20Forecasting%20Models&entry.906535625=Suhan%20Guo%20and%20Jiahong%20Deng%20and%20Mengjun%20Yi%20and%20Furao%20Shen%20and%20Jian%20Zhao&entry.1292438233=%20%20Attention-based%20architectures%20have%20achieved%20superior%20performance%20in%0Amultivariate%20time%20series%20forecasting%20but%20are%20computationally%20expensive.%0ATechniques%20such%20as%20patching%20and%20adaptive%20masking%20have%20been%20developed%20to%20reduce%0Atheir%20sizes%20and%20latencies.%20In%20this%20work%2C%20we%20propose%20a%20structured%20pruning%0Amethod%2C%20SPAT%20%28%24%5Ctextbf%7BS%7D%24ensitivity%20%24%5Ctextbf%7BP%7D%24runer%20for%0A%24%5Ctextbf%7BAt%7D%24tention%29%2C%20which%20selectively%20removes%20redundant%20attention%20mechanisms%0Aand%20yields%20highly%20effective%20models.%20Different%20from%20previous%20approaches%2C%20SPAT%0Aaims%20to%20remove%20the%20entire%20attention%20module%2C%20which%20reduces%20the%20risk%20of%0Aoverfitting%20and%20enables%20speed-up%20without%20demanding%20specialized%20hardware.%20We%0Apropose%20a%20dynamic%20sensitivity%20metric%2C%20%24%5Ctextbf%7BS%7D%24ensitivity%0A%24%5Ctextbf%7BE%7D%24nhanced%20%24%5Ctextbf%7BN%7D%24ormalized%20%24%5Ctextbf%7BD%7D%24ispersion%20%28SEND%29%20that%0Ameasures%20the%20importance%20of%20each%20attention%20module%20during%20the%20pre-training%20phase.%0AExperiments%20on%20multivariate%20datasets%20demonstrate%20that%20SPAT-pruned%20models%0Aachieve%20reductions%20of%202.842%25%20in%20MSE%2C%201.996%25%20in%20MAE%2C%20and%2035.274%25%20in%20FLOPs.%0AFurthermore%2C%20SPAT-pruned%20models%20outperform%20existing%20lightweight%2C%20Mamba-based%0Aand%20LLM-based%20SOTA%20methods%20in%20both%20standard%20and%20zero-shot%20inference%2C%0Ahighlighting%20the%20importance%20of%20retaining%20only%20the%20most%20effective%20attention%0Amechanisms.%20We%20have%20made%20our%20code%20publicly%20available%0Ahttps%3A//anonymous.4open.science/r/SPAT-6042.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08768v1&entry.124074799=Read"},
{"title": "DArFace: Deformation Aware Robustness for Low Quality Face Recognition", "author": "Sadaf Gulshad and Abdullah Aldahlawi Thakaa", "abstract": "  Facial recognition systems have achieved remarkable success by leveraging\ndeep neural networks, advanced loss functions, and large-scale datasets.\nHowever, their performance often deteriorates in real-world scenarios involving\nlow-quality facial images. Such degradations, common in surveillance footage or\nstandoff imaging include low resolution, motion blur, and various distortions,\nresulting in a substantial domain gap from the high-quality data typically used\nduring training. While existing approaches attempt to address robustness by\nmodifying network architectures or modeling global spatial transformations,\nthey frequently overlook local, non-rigid deformations that are inherently\npresent in real-world settings. In this work, we introduce DArFace, a\nDeformation-Aware robust Face recognition framework that enhances robustness to\nsuch degradations without requiring paired high- and low-quality training\nsamples. Our method adversarially integrates both global transformations (e.g.,\nrotation, translation) and local elastic deformations during training to\nsimulate realistic low-quality conditions. Moreover, we introduce a contrastive\nobjective to enforce identity consistency across different deformed views.\nExtensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and\nIJB-C demonstrate that DArFace surpasses state-of-the-art methods, with\nsignificant gains attributed to the inclusion of local deformation modeling.\n", "link": "http://arxiv.org/abs/2505.08423v1", "date": "2025-05-13", "relevancy": 2.5904, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5219}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5218}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DArFace%3A%20Deformation%20Aware%20Robustness%20for%20Low%20Quality%20Face%20Recognition&body=Title%3A%20DArFace%3A%20Deformation%20Aware%20Robustness%20for%20Low%20Quality%20Face%20Recognition%0AAuthor%3A%20Sadaf%20Gulshad%20and%20Abdullah%20Aldahlawi%20Thakaa%0AAbstract%3A%20%20%20Facial%20recognition%20systems%20have%20achieved%20remarkable%20success%20by%20leveraging%0Adeep%20neural%20networks%2C%20advanced%20loss%20functions%2C%20and%20large-scale%20datasets.%0AHowever%2C%20their%20performance%20often%20deteriorates%20in%20real-world%20scenarios%20involving%0Alow-quality%20facial%20images.%20Such%20degradations%2C%20common%20in%20surveillance%20footage%20or%0Astandoff%20imaging%20include%20low%20resolution%2C%20motion%20blur%2C%20and%20various%20distortions%2C%0Aresulting%20in%20a%20substantial%20domain%20gap%20from%20the%20high-quality%20data%20typically%20used%0Aduring%20training.%20While%20existing%20approaches%20attempt%20to%20address%20robustness%20by%0Amodifying%20network%20architectures%20or%20modeling%20global%20spatial%20transformations%2C%0Athey%20frequently%20overlook%20local%2C%20non-rigid%20deformations%20that%20are%20inherently%0Apresent%20in%20real-world%20settings.%20In%20this%20work%2C%20we%20introduce%20DArFace%2C%20a%0ADeformation-Aware%20robust%20Face%20recognition%20framework%20that%20enhances%20robustness%20to%0Asuch%20degradations%20without%20requiring%20paired%20high-%20and%20low-quality%20training%0Asamples.%20Our%20method%20adversarially%20integrates%20both%20global%20transformations%20%28e.g.%2C%0Arotation%2C%20translation%29%20and%20local%20elastic%20deformations%20during%20training%20to%0Asimulate%20realistic%20low-quality%20conditions.%20Moreover%2C%20we%20introduce%20a%20contrastive%0Aobjective%20to%20enforce%20identity%20consistency%20across%20different%20deformed%20views.%0AExtensive%20evaluations%20on%20low-quality%20benchmarks%20including%20TinyFace%2C%20IJB-B%2C%20and%0AIJB-C%20demonstrate%20that%20DArFace%20surpasses%20state-of-the-art%20methods%2C%20with%0Asignificant%20gains%20attributed%20to%20the%20inclusion%20of%20local%20deformation%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDArFace%253A%2520Deformation%2520Aware%2520Robustness%2520for%2520Low%2520Quality%2520Face%2520Recognition%26entry.906535625%3DSadaf%2520Gulshad%2520and%2520Abdullah%2520Aldahlawi%2520Thakaa%26entry.1292438233%3D%2520%2520Facial%2520recognition%2520systems%2520have%2520achieved%2520remarkable%2520success%2520by%2520leveraging%250Adeep%2520neural%2520networks%252C%2520advanced%2520loss%2520functions%252C%2520and%2520large-scale%2520datasets.%250AHowever%252C%2520their%2520performance%2520often%2520deteriorates%2520in%2520real-world%2520scenarios%2520involving%250Alow-quality%2520facial%2520images.%2520Such%2520degradations%252C%2520common%2520in%2520surveillance%2520footage%2520or%250Astandoff%2520imaging%2520include%2520low%2520resolution%252C%2520motion%2520blur%252C%2520and%2520various%2520distortions%252C%250Aresulting%2520in%2520a%2520substantial%2520domain%2520gap%2520from%2520the%2520high-quality%2520data%2520typically%2520used%250Aduring%2520training.%2520While%2520existing%2520approaches%2520attempt%2520to%2520address%2520robustness%2520by%250Amodifying%2520network%2520architectures%2520or%2520modeling%2520global%2520spatial%2520transformations%252C%250Athey%2520frequently%2520overlook%2520local%252C%2520non-rigid%2520deformations%2520that%2520are%2520inherently%250Apresent%2520in%2520real-world%2520settings.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DArFace%252C%2520a%250ADeformation-Aware%2520robust%2520Face%2520recognition%2520framework%2520that%2520enhances%2520robustness%2520to%250Asuch%2520degradations%2520without%2520requiring%2520paired%2520high-%2520and%2520low-quality%2520training%250Asamples.%2520Our%2520method%2520adversarially%2520integrates%2520both%2520global%2520transformations%2520%2528e.g.%252C%250Arotation%252C%2520translation%2529%2520and%2520local%2520elastic%2520deformations%2520during%2520training%2520to%250Asimulate%2520realistic%2520low-quality%2520conditions.%2520Moreover%252C%2520we%2520introduce%2520a%2520contrastive%250Aobjective%2520to%2520enforce%2520identity%2520consistency%2520across%2520different%2520deformed%2520views.%250AExtensive%2520evaluations%2520on%2520low-quality%2520benchmarks%2520including%2520TinyFace%252C%2520IJB-B%252C%2520and%250AIJB-C%2520demonstrate%2520that%2520DArFace%2520surpasses%2520state-of-the-art%2520methods%252C%2520with%250Asignificant%2520gains%2520attributed%2520to%2520the%2520inclusion%2520of%2520local%2520deformation%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DArFace%3A%20Deformation%20Aware%20Robustness%20for%20Low%20Quality%20Face%20Recognition&entry.906535625=Sadaf%20Gulshad%20and%20Abdullah%20Aldahlawi%20Thakaa&entry.1292438233=%20%20Facial%20recognition%20systems%20have%20achieved%20remarkable%20success%20by%20leveraging%0Adeep%20neural%20networks%2C%20advanced%20loss%20functions%2C%20and%20large-scale%20datasets.%0AHowever%2C%20their%20performance%20often%20deteriorates%20in%20real-world%20scenarios%20involving%0Alow-quality%20facial%20images.%20Such%20degradations%2C%20common%20in%20surveillance%20footage%20or%0Astandoff%20imaging%20include%20low%20resolution%2C%20motion%20blur%2C%20and%20various%20distortions%2C%0Aresulting%20in%20a%20substantial%20domain%20gap%20from%20the%20high-quality%20data%20typically%20used%0Aduring%20training.%20While%20existing%20approaches%20attempt%20to%20address%20robustness%20by%0Amodifying%20network%20architectures%20or%20modeling%20global%20spatial%20transformations%2C%0Athey%20frequently%20overlook%20local%2C%20non-rigid%20deformations%20that%20are%20inherently%0Apresent%20in%20real-world%20settings.%20In%20this%20work%2C%20we%20introduce%20DArFace%2C%20a%0ADeformation-Aware%20robust%20Face%20recognition%20framework%20that%20enhances%20robustness%20to%0Asuch%20degradations%20without%20requiring%20paired%20high-%20and%20low-quality%20training%0Asamples.%20Our%20method%20adversarially%20integrates%20both%20global%20transformations%20%28e.g.%2C%0Arotation%2C%20translation%29%20and%20local%20elastic%20deformations%20during%20training%20to%0Asimulate%20realistic%20low-quality%20conditions.%20Moreover%2C%20we%20introduce%20a%20contrastive%0Aobjective%20to%20enforce%20identity%20consistency%20across%20different%20deformed%20views.%0AExtensive%20evaluations%20on%20low-quality%20benchmarks%20including%20TinyFace%2C%20IJB-B%2C%20and%0AIJB-C%20demonstrate%20that%20DArFace%20surpasses%20state-of-the-art%20methods%2C%20with%0Asignificant%20gains%20attributed%20to%20the%20inclusion%20of%20local%20deformation%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08423v1&entry.124074799=Read"},
{"title": "GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in\n  Class-Incremental Learning", "author": "Minsu Kim and Seong-Hyeon Hwang and Steven Euijong Whang", "abstract": "  In the context of continual learning, acquiring new knowledge while\nmaintaining previous knowledge presents a significant challenge. Existing\nmethods often use experience replay techniques that store a small portion of\nprevious task data for training. In experience replay approaches, data\naugmentation has emerged as a promising strategy to further improve the model\nperformance by mixing limited previous task data with sufficient current task\ndata. However, we theoretically and empirically analyze that training with\nmixed samples from random sample pairs may harm the knowledge of previous tasks\nand cause greater catastrophic forgetting. We then propose GradMix, a robust\ndata augmentation method specifically designed for mitigating catastrophic\nforgetting in class-incremental learning. GradMix performs gradient-based\nselective mixup using a class-based criterion that mixes only samples from\nhelpful class pairs and not from detrimental class pairs for reducing\ncatastrophic forgetting. Our experiments on various real datasets show that\nGradMix outperforms data augmentation baselines in accuracy by minimizing the\nforgetting of previous knowledge.\n", "link": "http://arxiv.org/abs/2505.08528v1", "date": "2025-05-13", "relevancy": 2.5844, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5389}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5117}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GradMix%3A%20Gradient-based%20Selective%20Mixup%20for%20Robust%20Data%20Augmentation%20in%0A%20%20Class-Incremental%20Learning&body=Title%3A%20GradMix%3A%20Gradient-based%20Selective%20Mixup%20for%20Robust%20Data%20Augmentation%20in%0A%20%20Class-Incremental%20Learning%0AAuthor%3A%20Minsu%20Kim%20and%20Seong-Hyeon%20Hwang%20and%20Steven%20Euijong%20Whang%0AAbstract%3A%20%20%20In%20the%20context%20of%20continual%20learning%2C%20acquiring%20new%20knowledge%20while%0Amaintaining%20previous%20knowledge%20presents%20a%20significant%20challenge.%20Existing%0Amethods%20often%20use%20experience%20replay%20techniques%20that%20store%20a%20small%20portion%20of%0Aprevious%20task%20data%20for%20training.%20In%20experience%20replay%20approaches%2C%20data%0Aaugmentation%20has%20emerged%20as%20a%20promising%20strategy%20to%20further%20improve%20the%20model%0Aperformance%20by%20mixing%20limited%20previous%20task%20data%20with%20sufficient%20current%20task%0Adata.%20However%2C%20we%20theoretically%20and%20empirically%20analyze%20that%20training%20with%0Amixed%20samples%20from%20random%20sample%20pairs%20may%20harm%20the%20knowledge%20of%20previous%20tasks%0Aand%20cause%20greater%20catastrophic%20forgetting.%20We%20then%20propose%20GradMix%2C%20a%20robust%0Adata%20augmentation%20method%20specifically%20designed%20for%20mitigating%20catastrophic%0Aforgetting%20in%20class-incremental%20learning.%20GradMix%20performs%20gradient-based%0Aselective%20mixup%20using%20a%20class-based%20criterion%20that%20mixes%20only%20samples%20from%0Ahelpful%20class%20pairs%20and%20not%20from%20detrimental%20class%20pairs%20for%20reducing%0Acatastrophic%20forgetting.%20Our%20experiments%20on%20various%20real%20datasets%20show%20that%0AGradMix%20outperforms%20data%20augmentation%20baselines%20in%20accuracy%20by%20minimizing%20the%0Aforgetting%20of%20previous%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradMix%253A%2520Gradient-based%2520Selective%2520Mixup%2520for%2520Robust%2520Data%2520Augmentation%2520in%250A%2520%2520Class-Incremental%2520Learning%26entry.906535625%3DMinsu%2520Kim%2520and%2520Seong-Hyeon%2520Hwang%2520and%2520Steven%2520Euijong%2520Whang%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520continual%2520learning%252C%2520acquiring%2520new%2520knowledge%2520while%250Amaintaining%2520previous%2520knowledge%2520presents%2520a%2520significant%2520challenge.%2520Existing%250Amethods%2520often%2520use%2520experience%2520replay%2520techniques%2520that%2520store%2520a%2520small%2520portion%2520of%250Aprevious%2520task%2520data%2520for%2520training.%2520In%2520experience%2520replay%2520approaches%252C%2520data%250Aaugmentation%2520has%2520emerged%2520as%2520a%2520promising%2520strategy%2520to%2520further%2520improve%2520the%2520model%250Aperformance%2520by%2520mixing%2520limited%2520previous%2520task%2520data%2520with%2520sufficient%2520current%2520task%250Adata.%2520However%252C%2520we%2520theoretically%2520and%2520empirically%2520analyze%2520that%2520training%2520with%250Amixed%2520samples%2520from%2520random%2520sample%2520pairs%2520may%2520harm%2520the%2520knowledge%2520of%2520previous%2520tasks%250Aand%2520cause%2520greater%2520catastrophic%2520forgetting.%2520We%2520then%2520propose%2520GradMix%252C%2520a%2520robust%250Adata%2520augmentation%2520method%2520specifically%2520designed%2520for%2520mitigating%2520catastrophic%250Aforgetting%2520in%2520class-incremental%2520learning.%2520GradMix%2520performs%2520gradient-based%250Aselective%2520mixup%2520using%2520a%2520class-based%2520criterion%2520that%2520mixes%2520only%2520samples%2520from%250Ahelpful%2520class%2520pairs%2520and%2520not%2520from%2520detrimental%2520class%2520pairs%2520for%2520reducing%250Acatastrophic%2520forgetting.%2520Our%2520experiments%2520on%2520various%2520real%2520datasets%2520show%2520that%250AGradMix%2520outperforms%2520data%2520augmentation%2520baselines%2520in%2520accuracy%2520by%2520minimizing%2520the%250Aforgetting%2520of%2520previous%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GradMix%3A%20Gradient-based%20Selective%20Mixup%20for%20Robust%20Data%20Augmentation%20in%0A%20%20Class-Incremental%20Learning&entry.906535625=Minsu%20Kim%20and%20Seong-Hyeon%20Hwang%20and%20Steven%20Euijong%20Whang&entry.1292438233=%20%20In%20the%20context%20of%20continual%20learning%2C%20acquiring%20new%20knowledge%20while%0Amaintaining%20previous%20knowledge%20presents%20a%20significant%20challenge.%20Existing%0Amethods%20often%20use%20experience%20replay%20techniques%20that%20store%20a%20small%20portion%20of%0Aprevious%20task%20data%20for%20training.%20In%20experience%20replay%20approaches%2C%20data%0Aaugmentation%20has%20emerged%20as%20a%20promising%20strategy%20to%20further%20improve%20the%20model%0Aperformance%20by%20mixing%20limited%20previous%20task%20data%20with%20sufficient%20current%20task%0Adata.%20However%2C%20we%20theoretically%20and%20empirically%20analyze%20that%20training%20with%0Amixed%20samples%20from%20random%20sample%20pairs%20may%20harm%20the%20knowledge%20of%20previous%20tasks%0Aand%20cause%20greater%20catastrophic%20forgetting.%20We%20then%20propose%20GradMix%2C%20a%20robust%0Adata%20augmentation%20method%20specifically%20designed%20for%20mitigating%20catastrophic%0Aforgetting%20in%20class-incremental%20learning.%20GradMix%20performs%20gradient-based%0Aselective%20mixup%20using%20a%20class-based%20criterion%20that%20mixes%20only%20samples%20from%0Ahelpful%20class%20pairs%20and%20not%20from%20detrimental%20class%20pairs%20for%20reducing%0Acatastrophic%20forgetting.%20Our%20experiments%20on%20various%20real%20datasets%20show%20that%0AGradMix%20outperforms%20data%20augmentation%20baselines%20in%20accuracy%20by%20minimizing%20the%0Aforgetting%20of%20previous%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08528v1&entry.124074799=Read"},
{"title": "Breast Cancer Histopathology Classification using CBAM-EfficientNetV2\n  with Transfer Learning", "author": "Naren Sengodan", "abstract": "  Breast cancer histopathology image classification is critical for early\ndetection and improved patient outcomes. 1 This study introduces a novel\napproach leveraging EfficientNetV2 models, to improve feature extraction and\nfocus on relevant tissue regions. The proposed models were evaluated on the\nBreakHis dataset across multiple magnification scales (40X, 100X, 200X, and\n400X). 2 Among them, the EfficientNetV2-XL with CBAM achieved outstanding\nperformance, reaching a peak accuracy of 99.01 percent and an F1-score of 98.31\npercent at 400X magnification, outperforming state-of-the-art methods. 3 By\nintegrating Contrast Limited Adaptive Histogram Equalization (CLAHE) for\npreprocessing and optimizing computational efficiency, this method demonstrates\nits suitability for real-time clinical deployment. 3 The results underscore the\npotential of attention-enhanced scalable architectures in advancing diagnostic\nprecision for breast cancer detection.\n", "link": "http://arxiv.org/abs/2410.22392v6", "date": "2025-05-13", "relevancy": 2.5843, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5222}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5154}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breast%20Cancer%20Histopathology%20Classification%20using%20CBAM-EfficientNetV2%0A%20%20with%20Transfer%20Learning&body=Title%3A%20Breast%20Cancer%20Histopathology%20Classification%20using%20CBAM-EfficientNetV2%0A%20%20with%20Transfer%20Learning%0AAuthor%3A%20Naren%20Sengodan%0AAbstract%3A%20%20%20Breast%20cancer%20histopathology%20image%20classification%20is%20critical%20for%20early%0Adetection%20and%20improved%20patient%20outcomes.%201%20This%20study%20introduces%20a%20novel%0Aapproach%20leveraging%20EfficientNetV2%20models%2C%20to%20improve%20feature%20extraction%20and%0Afocus%20on%20relevant%20tissue%20regions.%20The%20proposed%20models%20were%20evaluated%20on%20the%0ABreakHis%20dataset%20across%20multiple%20magnification%20scales%20%2840X%2C%20100X%2C%20200X%2C%20and%0A400X%29.%202%20Among%20them%2C%20the%20EfficientNetV2-XL%20with%20CBAM%20achieved%20outstanding%0Aperformance%2C%20reaching%20a%20peak%20accuracy%20of%2099.01%20percent%20and%20an%20F1-score%20of%2098.31%0Apercent%20at%20400X%20magnification%2C%20outperforming%20state-of-the-art%20methods.%203%20By%0Aintegrating%20Contrast%20Limited%20Adaptive%20Histogram%20Equalization%20%28CLAHE%29%20for%0Apreprocessing%20and%20optimizing%20computational%20efficiency%2C%20this%20method%20demonstrates%0Aits%20suitability%20for%20real-time%20clinical%20deployment.%203%20The%20results%20underscore%20the%0Apotential%20of%20attention-enhanced%20scalable%20architectures%20in%20advancing%20diagnostic%0Aprecision%20for%20breast%20cancer%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22392v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreast%2520Cancer%2520Histopathology%2520Classification%2520using%2520CBAM-EfficientNetV2%250A%2520%2520with%2520Transfer%2520Learning%26entry.906535625%3DNaren%2520Sengodan%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520histopathology%2520image%2520classification%2520is%2520critical%2520for%2520early%250Adetection%2520and%2520improved%2520patient%2520outcomes.%25201%2520This%2520study%2520introduces%2520a%2520novel%250Aapproach%2520leveraging%2520EfficientNetV2%2520models%252C%2520to%2520improve%2520feature%2520extraction%2520and%250Afocus%2520on%2520relevant%2520tissue%2520regions.%2520The%2520proposed%2520models%2520were%2520evaluated%2520on%2520the%250ABreakHis%2520dataset%2520across%2520multiple%2520magnification%2520scales%2520%252840X%252C%2520100X%252C%2520200X%252C%2520and%250A400X%2529.%25202%2520Among%2520them%252C%2520the%2520EfficientNetV2-XL%2520with%2520CBAM%2520achieved%2520outstanding%250Aperformance%252C%2520reaching%2520a%2520peak%2520accuracy%2520of%252099.01%2520percent%2520and%2520an%2520F1-score%2520of%252098.31%250Apercent%2520at%2520400X%2520magnification%252C%2520outperforming%2520state-of-the-art%2520methods.%25203%2520By%250Aintegrating%2520Contrast%2520Limited%2520Adaptive%2520Histogram%2520Equalization%2520%2528CLAHE%2529%2520for%250Apreprocessing%2520and%2520optimizing%2520computational%2520efficiency%252C%2520this%2520method%2520demonstrates%250Aits%2520suitability%2520for%2520real-time%2520clinical%2520deployment.%25203%2520The%2520results%2520underscore%2520the%250Apotential%2520of%2520attention-enhanced%2520scalable%2520architectures%2520in%2520advancing%2520diagnostic%250Aprecision%2520for%2520breast%2520cancer%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22392v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breast%20Cancer%20Histopathology%20Classification%20using%20CBAM-EfficientNetV2%0A%20%20with%20Transfer%20Learning&entry.906535625=Naren%20Sengodan&entry.1292438233=%20%20Breast%20cancer%20histopathology%20image%20classification%20is%20critical%20for%20early%0Adetection%20and%20improved%20patient%20outcomes.%201%20This%20study%20introduces%20a%20novel%0Aapproach%20leveraging%20EfficientNetV2%20models%2C%20to%20improve%20feature%20extraction%20and%0Afocus%20on%20relevant%20tissue%20regions.%20The%20proposed%20models%20were%20evaluated%20on%20the%0ABreakHis%20dataset%20across%20multiple%20magnification%20scales%20%2840X%2C%20100X%2C%20200X%2C%20and%0A400X%29.%202%20Among%20them%2C%20the%20EfficientNetV2-XL%20with%20CBAM%20achieved%20outstanding%0Aperformance%2C%20reaching%20a%20peak%20accuracy%20of%2099.01%20percent%20and%20an%20F1-score%20of%2098.31%0Apercent%20at%20400X%20magnification%2C%20outperforming%20state-of-the-art%20methods.%203%20By%0Aintegrating%20Contrast%20Limited%20Adaptive%20Histogram%20Equalization%20%28CLAHE%29%20for%0Apreprocessing%20and%20optimizing%20computational%20efficiency%2C%20this%20method%20demonstrates%0Aits%20suitability%20for%20real-time%20clinical%20deployment.%203%20The%20results%20underscore%20the%0Apotential%20of%20attention-enhanced%20scalable%20architectures%20in%20advancing%20diagnostic%0Aprecision%20for%20breast%20cancer%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22392v6&entry.124074799=Read"},
{"title": "Deep Representation Learning for Unsupervised Clustering of Myocardial\n  Fiber Trajectories in Cardiac Diffusion Tensor Imaging", "author": "Mohini Anand and Xavier Tricoche", "abstract": "  Understanding the complex myocardial architecture is critical for diagnosing\nand treating heart disease. However, existing methods often struggle to\naccurately capture this intricate structure from Diffusion Tensor Imaging (DTI)\ndata, particularly due to the lack of ground truth labels and the ambiguous,\nintertwined nature of fiber trajectories. We present a novel deep learning\nframework for unsupervised clustering of myocardial fibers, providing a\ndata-driven approach to identifying distinct fiber bundles. We uniquely combine\na Bidirectional Long Short-Term Memory network to capture local sequential\ninformation along fibers, with a Transformer autoencoder to learn global shape\nfeatures, with pointwise incorporation of essential anatomical context.\nClustering these representations using a density-based algorithm identifies 33\nto 62 robust clusters, successfully capturing the subtle distinctions in fiber\ntrajectories with varying levels of granularity. Our framework offers a new,\nflexible, and quantitative way to analyze myocardial structure, achieving a\nlevel of delineation that, to our knowledge, has not been previously achieved,\nwith potential applications in improving surgical planning, characterizing\ndisease-related remodeling, and ultimately, advancing personalized cardiac\ncare.\n", "link": "http://arxiv.org/abs/2504.01953v2", "date": "2025-05-13", "relevancy": 2.5736, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5162}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.514}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Representation%20Learning%20for%20Unsupervised%20Clustering%20of%20Myocardial%0A%20%20Fiber%20Trajectories%20in%20Cardiac%20Diffusion%20Tensor%20Imaging&body=Title%3A%20Deep%20Representation%20Learning%20for%20Unsupervised%20Clustering%20of%20Myocardial%0A%20%20Fiber%20Trajectories%20in%20Cardiac%20Diffusion%20Tensor%20Imaging%0AAuthor%3A%20Mohini%20Anand%20and%20Xavier%20Tricoche%0AAbstract%3A%20%20%20Understanding%20the%20complex%20myocardial%20architecture%20is%20critical%20for%20diagnosing%0Aand%20treating%20heart%20disease.%20However%2C%20existing%20methods%20often%20struggle%20to%0Aaccurately%20capture%20this%20intricate%20structure%20from%20Diffusion%20Tensor%20Imaging%20%28DTI%29%0Adata%2C%20particularly%20due%20to%20the%20lack%20of%20ground%20truth%20labels%20and%20the%20ambiguous%2C%0Aintertwined%20nature%20of%20fiber%20trajectories.%20We%20present%20a%20novel%20deep%20learning%0Aframework%20for%20unsupervised%20clustering%20of%20myocardial%20fibers%2C%20providing%20a%0Adata-driven%20approach%20to%20identifying%20distinct%20fiber%20bundles.%20We%20uniquely%20combine%0Aa%20Bidirectional%20Long%20Short-Term%20Memory%20network%20to%20capture%20local%20sequential%0Ainformation%20along%20fibers%2C%20with%20a%20Transformer%20autoencoder%20to%20learn%20global%20shape%0Afeatures%2C%20with%20pointwise%20incorporation%20of%20essential%20anatomical%20context.%0AClustering%20these%20representations%20using%20a%20density-based%20algorithm%20identifies%2033%0Ato%2062%20robust%20clusters%2C%20successfully%20capturing%20the%20subtle%20distinctions%20in%20fiber%0Atrajectories%20with%20varying%20levels%20of%20granularity.%20Our%20framework%20offers%20a%20new%2C%0Aflexible%2C%20and%20quantitative%20way%20to%20analyze%20myocardial%20structure%2C%20achieving%20a%0Alevel%20of%20delineation%20that%2C%20to%20our%20knowledge%2C%20has%20not%20been%20previously%20achieved%2C%0Awith%20potential%20applications%20in%20improving%20surgical%20planning%2C%20characterizing%0Adisease-related%20remodeling%2C%20and%20ultimately%2C%20advancing%20personalized%20cardiac%0Acare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Representation%2520Learning%2520for%2520Unsupervised%2520Clustering%2520of%2520Myocardial%250A%2520%2520Fiber%2520Trajectories%2520in%2520Cardiac%2520Diffusion%2520Tensor%2520Imaging%26entry.906535625%3DMohini%2520Anand%2520and%2520Xavier%2520Tricoche%26entry.1292438233%3D%2520%2520Understanding%2520the%2520complex%2520myocardial%2520architecture%2520is%2520critical%2520for%2520diagnosing%250Aand%2520treating%2520heart%2520disease.%2520However%252C%2520existing%2520methods%2520often%2520struggle%2520to%250Aaccurately%2520capture%2520this%2520intricate%2520structure%2520from%2520Diffusion%2520Tensor%2520Imaging%2520%2528DTI%2529%250Adata%252C%2520particularly%2520due%2520to%2520the%2520lack%2520of%2520ground%2520truth%2520labels%2520and%2520the%2520ambiguous%252C%250Aintertwined%2520nature%2520of%2520fiber%2520trajectories.%2520We%2520present%2520a%2520novel%2520deep%2520learning%250Aframework%2520for%2520unsupervised%2520clustering%2520of%2520myocardial%2520fibers%252C%2520providing%2520a%250Adata-driven%2520approach%2520to%2520identifying%2520distinct%2520fiber%2520bundles.%2520We%2520uniquely%2520combine%250Aa%2520Bidirectional%2520Long%2520Short-Term%2520Memory%2520network%2520to%2520capture%2520local%2520sequential%250Ainformation%2520along%2520fibers%252C%2520with%2520a%2520Transformer%2520autoencoder%2520to%2520learn%2520global%2520shape%250Afeatures%252C%2520with%2520pointwise%2520incorporation%2520of%2520essential%2520anatomical%2520context.%250AClustering%2520these%2520representations%2520using%2520a%2520density-based%2520algorithm%2520identifies%252033%250Ato%252062%2520robust%2520clusters%252C%2520successfully%2520capturing%2520the%2520subtle%2520distinctions%2520in%2520fiber%250Atrajectories%2520with%2520varying%2520levels%2520of%2520granularity.%2520Our%2520framework%2520offers%2520a%2520new%252C%250Aflexible%252C%2520and%2520quantitative%2520way%2520to%2520analyze%2520myocardial%2520structure%252C%2520achieving%2520a%250Alevel%2520of%2520delineation%2520that%252C%2520to%2520our%2520knowledge%252C%2520has%2520not%2520been%2520previously%2520achieved%252C%250Awith%2520potential%2520applications%2520in%2520improving%2520surgical%2520planning%252C%2520characterizing%250Adisease-related%2520remodeling%252C%2520and%2520ultimately%252C%2520advancing%2520personalized%2520cardiac%250Acare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Representation%20Learning%20for%20Unsupervised%20Clustering%20of%20Myocardial%0A%20%20Fiber%20Trajectories%20in%20Cardiac%20Diffusion%20Tensor%20Imaging&entry.906535625=Mohini%20Anand%20and%20Xavier%20Tricoche&entry.1292438233=%20%20Understanding%20the%20complex%20myocardial%20architecture%20is%20critical%20for%20diagnosing%0Aand%20treating%20heart%20disease.%20However%2C%20existing%20methods%20often%20struggle%20to%0Aaccurately%20capture%20this%20intricate%20structure%20from%20Diffusion%20Tensor%20Imaging%20%28DTI%29%0Adata%2C%20particularly%20due%20to%20the%20lack%20of%20ground%20truth%20labels%20and%20the%20ambiguous%2C%0Aintertwined%20nature%20of%20fiber%20trajectories.%20We%20present%20a%20novel%20deep%20learning%0Aframework%20for%20unsupervised%20clustering%20of%20myocardial%20fibers%2C%20providing%20a%0Adata-driven%20approach%20to%20identifying%20distinct%20fiber%20bundles.%20We%20uniquely%20combine%0Aa%20Bidirectional%20Long%20Short-Term%20Memory%20network%20to%20capture%20local%20sequential%0Ainformation%20along%20fibers%2C%20with%20a%20Transformer%20autoencoder%20to%20learn%20global%20shape%0Afeatures%2C%20with%20pointwise%20incorporation%20of%20essential%20anatomical%20context.%0AClustering%20these%20representations%20using%20a%20density-based%20algorithm%20identifies%2033%0Ato%2062%20robust%20clusters%2C%20successfully%20capturing%20the%20subtle%20distinctions%20in%20fiber%0Atrajectories%20with%20varying%20levels%20of%20granularity.%20Our%20framework%20offers%20a%20new%2C%0Aflexible%2C%20and%20quantitative%20way%20to%20analyze%20myocardial%20structure%2C%20achieving%20a%0Alevel%20of%20delineation%20that%2C%20to%20our%20knowledge%2C%20has%20not%20been%20previously%20achieved%2C%0Awith%20potential%20applications%20in%20improving%20surgical%20planning%2C%20characterizing%0Adisease-related%20remodeling%2C%20and%20ultimately%2C%20advancing%20personalized%20cardiac%0Acare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01953v2&entry.124074799=Read"},
{"title": "Optimized View and Geometry Distillation from Multi-view Diffuser", "author": "Youjia Zhang and Zikai Song and Junqing Yu and Yawei Luo and Wei Yang", "abstract": "  Generating multi-view images from a single input view using image-conditioned\ndiffusion models is a recent advancement and has shown considerable potential.\nHowever, issues such as the lack of consistency in synthesized views and\nover-smoothing in extracted geometry persist. Previous methods integrate\nmulti-view consistency modules or impose additional supervisory to enhance view\nconsistency while compromising on the flexibility of camera positioning and\nlimiting the versatility of view synthesis. In this study, we consider the\nradiance field optimized during geometry extraction as a more rigid consistency\nprior, compared to volume and ray aggregation used in previous works. We\nfurther identify and rectify a critical bias in the traditional radiance field\noptimization process through score distillation from a multi-view diffuser. We\nintroduce an Unbiased Score Distillation (USD) that utilizes unconditioned\nnoises from a 2D diffusion model, greatly refining the radiance field fidelity.\nWe leverage the rendered views from the optimized radiance field as the basis\nand develop a two-step specialization process of a 2D diffusion model, which is\nadept at conducting object-specific denoising and generating high-quality\nmulti-view images. Finally, we recover faithful geometry and texture directly\nfrom the refined multi-view images. Empirical evaluations demonstrate that our\noptimized geometry and view distillation technique generates comparable results\nto the state-of-the-art models trained on extensive datasets, all while\nmaintaining freedom in camera positioning. Please see our project page at\nhttps://youjiazhang.github.io/USD/.\n", "link": "http://arxiv.org/abs/2312.06198v4", "date": "2025-05-13", "relevancy": 2.5681, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6489}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6489}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimized%20View%20and%20Geometry%20Distillation%20from%20Multi-view%20Diffuser&body=Title%3A%20Optimized%20View%20and%20Geometry%20Distillation%20from%20Multi-view%20Diffuser%0AAuthor%3A%20Youjia%20Zhang%20and%20Zikai%20Song%20and%20Junqing%20Yu%20and%20Yawei%20Luo%20and%20Wei%20Yang%0AAbstract%3A%20%20%20Generating%20multi-view%20images%20from%20a%20single%20input%20view%20using%20image-conditioned%0Adiffusion%20models%20is%20a%20recent%20advancement%20and%20has%20shown%20considerable%20potential.%0AHowever%2C%20issues%20such%20as%20the%20lack%20of%20consistency%20in%20synthesized%20views%20and%0Aover-smoothing%20in%20extracted%20geometry%20persist.%20Previous%20methods%20integrate%0Amulti-view%20consistency%20modules%20or%20impose%20additional%20supervisory%20to%20enhance%20view%0Aconsistency%20while%20compromising%20on%20the%20flexibility%20of%20camera%20positioning%20and%0Alimiting%20the%20versatility%20of%20view%20synthesis.%20In%20this%20study%2C%20we%20consider%20the%0Aradiance%20field%20optimized%20during%20geometry%20extraction%20as%20a%20more%20rigid%20consistency%0Aprior%2C%20compared%20to%20volume%20and%20ray%20aggregation%20used%20in%20previous%20works.%20We%0Afurther%20identify%20and%20rectify%20a%20critical%20bias%20in%20the%20traditional%20radiance%20field%0Aoptimization%20process%20through%20score%20distillation%20from%20a%20multi-view%20diffuser.%20We%0Aintroduce%20an%20Unbiased%20Score%20Distillation%20%28USD%29%20that%20utilizes%20unconditioned%0Anoises%20from%20a%202D%20diffusion%20model%2C%20greatly%20refining%20the%20radiance%20field%20fidelity.%0AWe%20leverage%20the%20rendered%20views%20from%20the%20optimized%20radiance%20field%20as%20the%20basis%0Aand%20develop%20a%20two-step%20specialization%20process%20of%20a%202D%20diffusion%20model%2C%20which%20is%0Aadept%20at%20conducting%20object-specific%20denoising%20and%20generating%20high-quality%0Amulti-view%20images.%20Finally%2C%20we%20recover%20faithful%20geometry%20and%20texture%20directly%0Afrom%20the%20refined%20multi-view%20images.%20Empirical%20evaluations%20demonstrate%20that%20our%0Aoptimized%20geometry%20and%20view%20distillation%20technique%20generates%20comparable%20results%0Ato%20the%20state-of-the-art%20models%20trained%20on%20extensive%20datasets%2C%20all%20while%0Amaintaining%20freedom%20in%20camera%20positioning.%20Please%20see%20our%20project%20page%20at%0Ahttps%3A//youjiazhang.github.io/USD/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06198v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimized%2520View%2520and%2520Geometry%2520Distillation%2520from%2520Multi-view%2520Diffuser%26entry.906535625%3DYoujia%2520Zhang%2520and%2520Zikai%2520Song%2520and%2520Junqing%2520Yu%2520and%2520Yawei%2520Luo%2520and%2520Wei%2520Yang%26entry.1292438233%3D%2520%2520Generating%2520multi-view%2520images%2520from%2520a%2520single%2520input%2520view%2520using%2520image-conditioned%250Adiffusion%2520models%2520is%2520a%2520recent%2520advancement%2520and%2520has%2520shown%2520considerable%2520potential.%250AHowever%252C%2520issues%2520such%2520as%2520the%2520lack%2520of%2520consistency%2520in%2520synthesized%2520views%2520and%250Aover-smoothing%2520in%2520extracted%2520geometry%2520persist.%2520Previous%2520methods%2520integrate%250Amulti-view%2520consistency%2520modules%2520or%2520impose%2520additional%2520supervisory%2520to%2520enhance%2520view%250Aconsistency%2520while%2520compromising%2520on%2520the%2520flexibility%2520of%2520camera%2520positioning%2520and%250Alimiting%2520the%2520versatility%2520of%2520view%2520synthesis.%2520In%2520this%2520study%252C%2520we%2520consider%2520the%250Aradiance%2520field%2520optimized%2520during%2520geometry%2520extraction%2520as%2520a%2520more%2520rigid%2520consistency%250Aprior%252C%2520compared%2520to%2520volume%2520and%2520ray%2520aggregation%2520used%2520in%2520previous%2520works.%2520We%250Afurther%2520identify%2520and%2520rectify%2520a%2520critical%2520bias%2520in%2520the%2520traditional%2520radiance%2520field%250Aoptimization%2520process%2520through%2520score%2520distillation%2520from%2520a%2520multi-view%2520diffuser.%2520We%250Aintroduce%2520an%2520Unbiased%2520Score%2520Distillation%2520%2528USD%2529%2520that%2520utilizes%2520unconditioned%250Anoises%2520from%2520a%25202D%2520diffusion%2520model%252C%2520greatly%2520refining%2520the%2520radiance%2520field%2520fidelity.%250AWe%2520leverage%2520the%2520rendered%2520views%2520from%2520the%2520optimized%2520radiance%2520field%2520as%2520the%2520basis%250Aand%2520develop%2520a%2520two-step%2520specialization%2520process%2520of%2520a%25202D%2520diffusion%2520model%252C%2520which%2520is%250Aadept%2520at%2520conducting%2520object-specific%2520denoising%2520and%2520generating%2520high-quality%250Amulti-view%2520images.%2520Finally%252C%2520we%2520recover%2520faithful%2520geometry%2520and%2520texture%2520directly%250Afrom%2520the%2520refined%2520multi-view%2520images.%2520Empirical%2520evaluations%2520demonstrate%2520that%2520our%250Aoptimized%2520geometry%2520and%2520view%2520distillation%2520technique%2520generates%2520comparable%2520results%250Ato%2520the%2520state-of-the-art%2520models%2520trained%2520on%2520extensive%2520datasets%252C%2520all%2520while%250Amaintaining%2520freedom%2520in%2520camera%2520positioning.%2520Please%2520see%2520our%2520project%2520page%2520at%250Ahttps%253A//youjiazhang.github.io/USD/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06198v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimized%20View%20and%20Geometry%20Distillation%20from%20Multi-view%20Diffuser&entry.906535625=Youjia%20Zhang%20and%20Zikai%20Song%20and%20Junqing%20Yu%20and%20Yawei%20Luo%20and%20Wei%20Yang&entry.1292438233=%20%20Generating%20multi-view%20images%20from%20a%20single%20input%20view%20using%20image-conditioned%0Adiffusion%20models%20is%20a%20recent%20advancement%20and%20has%20shown%20considerable%20potential.%0AHowever%2C%20issues%20such%20as%20the%20lack%20of%20consistency%20in%20synthesized%20views%20and%0Aover-smoothing%20in%20extracted%20geometry%20persist.%20Previous%20methods%20integrate%0Amulti-view%20consistency%20modules%20or%20impose%20additional%20supervisory%20to%20enhance%20view%0Aconsistency%20while%20compromising%20on%20the%20flexibility%20of%20camera%20positioning%20and%0Alimiting%20the%20versatility%20of%20view%20synthesis.%20In%20this%20study%2C%20we%20consider%20the%0Aradiance%20field%20optimized%20during%20geometry%20extraction%20as%20a%20more%20rigid%20consistency%0Aprior%2C%20compared%20to%20volume%20and%20ray%20aggregation%20used%20in%20previous%20works.%20We%0Afurther%20identify%20and%20rectify%20a%20critical%20bias%20in%20the%20traditional%20radiance%20field%0Aoptimization%20process%20through%20score%20distillation%20from%20a%20multi-view%20diffuser.%20We%0Aintroduce%20an%20Unbiased%20Score%20Distillation%20%28USD%29%20that%20utilizes%20unconditioned%0Anoises%20from%20a%202D%20diffusion%20model%2C%20greatly%20refining%20the%20radiance%20field%20fidelity.%0AWe%20leverage%20the%20rendered%20views%20from%20the%20optimized%20radiance%20field%20as%20the%20basis%0Aand%20develop%20a%20two-step%20specialization%20process%20of%20a%202D%20diffusion%20model%2C%20which%20is%0Aadept%20at%20conducting%20object-specific%20denoising%20and%20generating%20high-quality%0Amulti-view%20images.%20Finally%2C%20we%20recover%20faithful%20geometry%20and%20texture%20directly%0Afrom%20the%20refined%20multi-view%20images.%20Empirical%20evaluations%20demonstrate%20that%20our%0Aoptimized%20geometry%20and%20view%20distillation%20technique%20generates%20comparable%20results%0Ato%20the%20state-of-the-art%20models%20trained%20on%20extensive%20datasets%2C%20all%20while%0Amaintaining%20freedom%20in%20camera%20positioning.%20Please%20see%20our%20project%20page%20at%0Ahttps%3A//youjiazhang.github.io/USD/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06198v4&entry.124074799=Read"},
{"title": "RepCali: High Efficient Fine-tuning Via Representation Calibration in\n  Latent Space for Pre-trained Language Models", "author": "Fujun Zhang and XiangDong Su", "abstract": "  Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm\nin applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs\nstill struggle with the discrepancies between the representation obtained from\nthe PLMs' encoder and the optimal input to the PLMs' decoder. This paper\ntackles this challenge by learning to calibrate the representation of PLMs in\nthe latent space. In the proposed representation calibration method (RepCali),\nwe integrate a specific calibration block to the latent space after the encoder\nand use the calibrated output as the decoder input. The merits of the proposed\nRepCali include its universality to all PLMs with encoder-decoder\narchitectures, its plug-and-play nature, and ease of implementation. Extensive\nexperiments on 25 PLM-based models across 8 tasks (including both English and\nChinese datasets) demonstrate that the proposed RepCali offers desirable\nenhancements to PLMs (including LLMs) and significantly improves the\nperformance of downstream tasks. Comparison experiments across 4 benchmark\ntasks indicate that RepCali is superior to the representative fine-tuning\nbaselines.\n", "link": "http://arxiv.org/abs/2505.08463v1", "date": "2025-05-13", "relevancy": 2.5609, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RepCali%3A%20High%20Efficient%20Fine-tuning%20Via%20Representation%20Calibration%20in%0A%20%20Latent%20Space%20for%20Pre-trained%20Language%20Models&body=Title%3A%20RepCali%3A%20High%20Efficient%20Fine-tuning%20Via%20Representation%20Calibration%20in%0A%20%20Latent%20Space%20for%20Pre-trained%20Language%20Models%0AAuthor%3A%20Fujun%20Zhang%20and%20XiangDong%20Su%0AAbstract%3A%20%20%20Fine-tuning%20pre-trained%20language%20models%20%28PLMs%29%20has%20become%20a%20dominant%20paradigm%0Ain%20applying%20PLMs%20to%20downstream%20tasks.%20However%2C%20with%20limited%20fine-tuning%2C%20PLMs%0Astill%20struggle%20with%20the%20discrepancies%20between%20the%20representation%20obtained%20from%0Athe%20PLMs%27%20encoder%20and%20the%20optimal%20input%20to%20the%20PLMs%27%20decoder.%20This%20paper%0Atackles%20this%20challenge%20by%20learning%20to%20calibrate%20the%20representation%20of%20PLMs%20in%0Athe%20latent%20space.%20In%20the%20proposed%20representation%20calibration%20method%20%28RepCali%29%2C%0Awe%20integrate%20a%20specific%20calibration%20block%20to%20the%20latent%20space%20after%20the%20encoder%0Aand%20use%20the%20calibrated%20output%20as%20the%20decoder%20input.%20The%20merits%20of%20the%20proposed%0ARepCali%20include%20its%20universality%20to%20all%20PLMs%20with%20encoder-decoder%0Aarchitectures%2C%20its%20plug-and-play%20nature%2C%20and%20ease%20of%20implementation.%20Extensive%0Aexperiments%20on%2025%20PLM-based%20models%20across%208%20tasks%20%28including%20both%20English%20and%0AChinese%20datasets%29%20demonstrate%20that%20the%20proposed%20RepCali%20offers%20desirable%0Aenhancements%20to%20PLMs%20%28including%20LLMs%29%20and%20significantly%20improves%20the%0Aperformance%20of%20downstream%20tasks.%20Comparison%20experiments%20across%204%20benchmark%0Atasks%20indicate%20that%20RepCali%20is%20superior%20to%20the%20representative%20fine-tuning%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepCali%253A%2520High%2520Efficient%2520Fine-tuning%2520Via%2520Representation%2520Calibration%2520in%250A%2520%2520Latent%2520Space%2520for%2520Pre-trained%2520Language%2520Models%26entry.906535625%3DFujun%2520Zhang%2520and%2520XiangDong%2520Su%26entry.1292438233%3D%2520%2520Fine-tuning%2520pre-trained%2520language%2520models%2520%2528PLMs%2529%2520has%2520become%2520a%2520dominant%2520paradigm%250Ain%2520applying%2520PLMs%2520to%2520downstream%2520tasks.%2520However%252C%2520with%2520limited%2520fine-tuning%252C%2520PLMs%250Astill%2520struggle%2520with%2520the%2520discrepancies%2520between%2520the%2520representation%2520obtained%2520from%250Athe%2520PLMs%2527%2520encoder%2520and%2520the%2520optimal%2520input%2520to%2520the%2520PLMs%2527%2520decoder.%2520This%2520paper%250Atackles%2520this%2520challenge%2520by%2520learning%2520to%2520calibrate%2520the%2520representation%2520of%2520PLMs%2520in%250Athe%2520latent%2520space.%2520In%2520the%2520proposed%2520representation%2520calibration%2520method%2520%2528RepCali%2529%252C%250Awe%2520integrate%2520a%2520specific%2520calibration%2520block%2520to%2520the%2520latent%2520space%2520after%2520the%2520encoder%250Aand%2520use%2520the%2520calibrated%2520output%2520as%2520the%2520decoder%2520input.%2520The%2520merits%2520of%2520the%2520proposed%250ARepCali%2520include%2520its%2520universality%2520to%2520all%2520PLMs%2520with%2520encoder-decoder%250Aarchitectures%252C%2520its%2520plug-and-play%2520nature%252C%2520and%2520ease%2520of%2520implementation.%2520Extensive%250Aexperiments%2520on%252025%2520PLM-based%2520models%2520across%25208%2520tasks%2520%2528including%2520both%2520English%2520and%250AChinese%2520datasets%2529%2520demonstrate%2520that%2520the%2520proposed%2520RepCali%2520offers%2520desirable%250Aenhancements%2520to%2520PLMs%2520%2528including%2520LLMs%2529%2520and%2520significantly%2520improves%2520the%250Aperformance%2520of%2520downstream%2520tasks.%2520Comparison%2520experiments%2520across%25204%2520benchmark%250Atasks%2520indicate%2520that%2520RepCali%2520is%2520superior%2520to%2520the%2520representative%2520fine-tuning%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RepCali%3A%20High%20Efficient%20Fine-tuning%20Via%20Representation%20Calibration%20in%0A%20%20Latent%20Space%20for%20Pre-trained%20Language%20Models&entry.906535625=Fujun%20Zhang%20and%20XiangDong%20Su&entry.1292438233=%20%20Fine-tuning%20pre-trained%20language%20models%20%28PLMs%29%20has%20become%20a%20dominant%20paradigm%0Ain%20applying%20PLMs%20to%20downstream%20tasks.%20However%2C%20with%20limited%20fine-tuning%2C%20PLMs%0Astill%20struggle%20with%20the%20discrepancies%20between%20the%20representation%20obtained%20from%0Athe%20PLMs%27%20encoder%20and%20the%20optimal%20input%20to%20the%20PLMs%27%20decoder.%20This%20paper%0Atackles%20this%20challenge%20by%20learning%20to%20calibrate%20the%20representation%20of%20PLMs%20in%0Athe%20latent%20space.%20In%20the%20proposed%20representation%20calibration%20method%20%28RepCali%29%2C%0Awe%20integrate%20a%20specific%20calibration%20block%20to%20the%20latent%20space%20after%20the%20encoder%0Aand%20use%20the%20calibrated%20output%20as%20the%20decoder%20input.%20The%20merits%20of%20the%20proposed%0ARepCali%20include%20its%20universality%20to%20all%20PLMs%20with%20encoder-decoder%0Aarchitectures%2C%20its%20plug-and-play%20nature%2C%20and%20ease%20of%20implementation.%20Extensive%0Aexperiments%20on%2025%20PLM-based%20models%20across%208%20tasks%20%28including%20both%20English%20and%0AChinese%20datasets%29%20demonstrate%20that%20the%20proposed%20RepCali%20offers%20desirable%0Aenhancements%20to%20PLMs%20%28including%20LLMs%29%20and%20significantly%20improves%20the%0Aperformance%20of%20downstream%20tasks.%20Comparison%20experiments%20across%204%20benchmark%0Atasks%20indicate%20that%20RepCali%20is%20superior%20to%20the%20representative%20fine-tuning%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08463v1&entry.124074799=Read"},
{"title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large\n  Language Models", "author": "Yubo Li and Xiaobin Shen and Xinyu Yao and Xueying Ding and Yidi Miao and Ramayya Krishnan and Rema Padman", "abstract": "  Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.\n", "link": "http://arxiv.org/abs/2504.04717v3", "date": "2025-05-13", "relevancy": 2.511, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Single-Turn%3A%20A%20Survey%20on%20Multi-Turn%20Interactions%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20Beyond%20Single-Turn%3A%20A%20Survey%20on%20Multi-Turn%20Interactions%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yubo%20Li%20and%20Xiaobin%20Shen%20and%20Xinyu%20Yao%20and%20Xueying%20Ding%20and%20Yidi%20Miao%20and%20Ramayya%20Krishnan%20and%20Rema%20Padman%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20revolutionized%20their%0Aability%20to%20handle%20single-turn%20tasks%2C%20yet%20real-world%20applications%20demand%0Asophisticated%20multi-turn%20interactions.%20This%20survey%20provides%20a%20comprehensive%0Areview%20of%20recent%20advancements%20in%20evaluating%20and%20enhancing%20multi-turn%0Ainteractions%20in%20LLMs.%20Focusing%20on%20task-specific%20scenarios%2C%20from%20instruction%0Afollowing%20in%20diverse%20domains%20such%20as%20math%20and%20coding%20to%20complex%20conversational%0Aengagements%20in%20roleplay%2C%20healthcare%2C%20education%2C%20and%20even%20adversarial%20jailbreak%0Asettings%2C%20we%20systematically%20examine%20the%20challenges%20of%20maintaining%20context%2C%0Acoherence%2C%20fairness%2C%20and%20responsiveness%20over%20prolonged%20dialogues.%20The%20paper%0Aorganizes%20current%20benchmarks%20and%20datasets%20into%20coherent%20categories%20that%20reflect%0Athe%20evolving%20landscape%20of%20multi-turn%20dialogue%20evaluation.%20In%20addition%2C%20we%0Areview%20a%20range%20of%20enhancement%20methodologies%20under%20multi-turn%20settings%2C%0Aincluding%20model-centric%20strategies%20%28contextual%20learning%2C%20supervised%0Afine-tuning%2C%20reinforcement%20learning%2C%20and%20new%20architectures%29%2C%20external%0Aintegration%20approaches%20%28memory-augmented%2C%20retrieval-based%20methods%2C%20and%0Aknowledge%20graph%29%2C%20and%20agent-based%20techniques%20for%20collaborative%20interactions.%0AFinally%2C%20we%20discuss%20open%20challenges%20and%20propose%20future%20directions%20for%20research%0Ato%20further%20advance%20the%20robustness%20and%20effectiveness%20of%20multi-turn%20interactions%0Ain%20LLMs.%20Related%20resources%20and%20papers%20are%20available%20at%0Ahttps%3A//github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04717v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Single-Turn%253A%2520A%2520Survey%2520on%2520Multi-Turn%2520Interactions%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYubo%2520Li%2520and%2520Xiaobin%2520Shen%2520and%2520Xinyu%2520Yao%2520and%2520Xueying%2520Ding%2520and%2520Yidi%2520Miao%2520and%2520Ramayya%2520Krishnan%2520and%2520Rema%2520Padman%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%2520their%250Aability%2520to%2520handle%2520single-turn%2520tasks%252C%2520yet%2520real-world%2520applications%2520demand%250Asophisticated%2520multi-turn%2520interactions.%2520This%2520survey%2520provides%2520a%2520comprehensive%250Areview%2520of%2520recent%2520advancements%2520in%2520evaluating%2520and%2520enhancing%2520multi-turn%250Ainteractions%2520in%2520LLMs.%2520Focusing%2520on%2520task-specific%2520scenarios%252C%2520from%2520instruction%250Afollowing%2520in%2520diverse%2520domains%2520such%2520as%2520math%2520and%2520coding%2520to%2520complex%2520conversational%250Aengagements%2520in%2520roleplay%252C%2520healthcare%252C%2520education%252C%2520and%2520even%2520adversarial%2520jailbreak%250Asettings%252C%2520we%2520systematically%2520examine%2520the%2520challenges%2520of%2520maintaining%2520context%252C%250Acoherence%252C%2520fairness%252C%2520and%2520responsiveness%2520over%2520prolonged%2520dialogues.%2520The%2520paper%250Aorganizes%2520current%2520benchmarks%2520and%2520datasets%2520into%2520coherent%2520categories%2520that%2520reflect%250Athe%2520evolving%2520landscape%2520of%2520multi-turn%2520dialogue%2520evaluation.%2520In%2520addition%252C%2520we%250Areview%2520a%2520range%2520of%2520enhancement%2520methodologies%2520under%2520multi-turn%2520settings%252C%250Aincluding%2520model-centric%2520strategies%2520%2528contextual%2520learning%252C%2520supervised%250Afine-tuning%252C%2520reinforcement%2520learning%252C%2520and%2520new%2520architectures%2529%252C%2520external%250Aintegration%2520approaches%2520%2528memory-augmented%252C%2520retrieval-based%2520methods%252C%2520and%250Aknowledge%2520graph%2529%252C%2520and%2520agent-based%2520techniques%2520for%2520collaborative%2520interactions.%250AFinally%252C%2520we%2520discuss%2520open%2520challenges%2520and%2520propose%2520future%2520directions%2520for%2520research%250Ato%2520further%2520advance%2520the%2520robustness%2520and%2520effectiveness%2520of%2520multi-turn%2520interactions%250Ain%2520LLMs.%2520Related%2520resources%2520and%2520papers%2520are%2520available%2520at%250Ahttps%253A//github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04717v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Single-Turn%3A%20A%20Survey%20on%20Multi-Turn%20Interactions%20with%20Large%0A%20%20Language%20Models&entry.906535625=Yubo%20Li%20and%20Xiaobin%20Shen%20and%20Xinyu%20Yao%20and%20Xueying%20Ding%20and%20Yidi%20Miao%20and%20Ramayya%20Krishnan%20and%20Rema%20Padman&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20revolutionized%20their%0Aability%20to%20handle%20single-turn%20tasks%2C%20yet%20real-world%20applications%20demand%0Asophisticated%20multi-turn%20interactions.%20This%20survey%20provides%20a%20comprehensive%0Areview%20of%20recent%20advancements%20in%20evaluating%20and%20enhancing%20multi-turn%0Ainteractions%20in%20LLMs.%20Focusing%20on%20task-specific%20scenarios%2C%20from%20instruction%0Afollowing%20in%20diverse%20domains%20such%20as%20math%20and%20coding%20to%20complex%20conversational%0Aengagements%20in%20roleplay%2C%20healthcare%2C%20education%2C%20and%20even%20adversarial%20jailbreak%0Asettings%2C%20we%20systematically%20examine%20the%20challenges%20of%20maintaining%20context%2C%0Acoherence%2C%20fairness%2C%20and%20responsiveness%20over%20prolonged%20dialogues.%20The%20paper%0Aorganizes%20current%20benchmarks%20and%20datasets%20into%20coherent%20categories%20that%20reflect%0Athe%20evolving%20landscape%20of%20multi-turn%20dialogue%20evaluation.%20In%20addition%2C%20we%0Areview%20a%20range%20of%20enhancement%20methodologies%20under%20multi-turn%20settings%2C%0Aincluding%20model-centric%20strategies%20%28contextual%20learning%2C%20supervised%0Afine-tuning%2C%20reinforcement%20learning%2C%20and%20new%20architectures%29%2C%20external%0Aintegration%20approaches%20%28memory-augmented%2C%20retrieval-based%20methods%2C%20and%0Aknowledge%20graph%29%2C%20and%20agent-based%20techniques%20for%20collaborative%20interactions.%0AFinally%2C%20we%20discuss%20open%20challenges%20and%20propose%20future%20directions%20for%20research%0Ato%20further%20advance%20the%20robustness%20and%20effectiveness%20of%20multi-turn%20interactions%0Ain%20LLMs.%20Related%20resources%20and%20papers%20are%20available%20at%0Ahttps%3A//github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04717v3&entry.124074799=Read"},
{"title": "GNCAF: A GNN-based Neighboring Context Aggregation Framework for\n  Tertiary Lymphoid Structures Semantic Segmentation in WSI", "author": "Lei Su", "abstract": "  Tertiary lymphoid structures (TLS) are organized clusters of immune cells,\nwhose maturity and area can be quantified in whole slide image (WSI) for\nvarious prognostic tasks. Existing methods for assessing these characteristics\ntypically rely on cell proxy tasks and require additional post-processing\nsteps. In this work, We focus on a novel task-TLS Semantic Segmentation\n(TLS-SS)-which segments both the regions and maturation stages of TLS in WSI in\nan end-to-end manner. Due to the extensive scale of WSI and patch-based\nsegmentation strategies, TLS-SS necessitates integrating from neighboring\npatches to guide target patch (target) segmentation. Previous techniques often\nemploy on multi-resolution approaches, constraining the capacity to leverage\nthe broader neighboring context while tend to preserve coarse-grained\ninformation. To address this, we propose a GNN-based Neighboring Context\nAggregation Framework (GNCAF), which progressively aggregates multi-hop\nneighboring context from the target and employs a self-attention mechanism to\nguide the segmentation of the target. GNCAF can be integrated with various\nsegmentation models to enhance their ability to perceive contextual information\noutside of the patch. We build two TLS-SS datasets, called TCGA-COAD and\nINHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publicly\navailable. Experiments on these datasets demonstrate the superiority of GNCAF,\nachieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU,\nrespectively. Additionally, we also validate the task scalability of GNCAF on\nsegmentation of lymph node metastases.\n", "link": "http://arxiv.org/abs/2505.08430v1", "date": "2025-05-13", "relevancy": 2.5033, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5215}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GNCAF%3A%20A%20GNN-based%20Neighboring%20Context%20Aggregation%20Framework%20for%0A%20%20Tertiary%20Lymphoid%20Structures%20Semantic%20Segmentation%20in%20WSI&body=Title%3A%20GNCAF%3A%20A%20GNN-based%20Neighboring%20Context%20Aggregation%20Framework%20for%0A%20%20Tertiary%20Lymphoid%20Structures%20Semantic%20Segmentation%20in%20WSI%0AAuthor%3A%20Lei%20Su%0AAbstract%3A%20%20%20Tertiary%20lymphoid%20structures%20%28TLS%29%20are%20organized%20clusters%20of%20immune%20cells%2C%0Awhose%20maturity%20and%20area%20can%20be%20quantified%20in%20whole%20slide%20image%20%28WSI%29%20for%0Avarious%20prognostic%20tasks.%20Existing%20methods%20for%20assessing%20these%20characteristics%0Atypically%20rely%20on%20cell%20proxy%20tasks%20and%20require%20additional%20post-processing%0Asteps.%20In%20this%20work%2C%20We%20focus%20on%20a%20novel%20task-TLS%20Semantic%20Segmentation%0A%28TLS-SS%29-which%20segments%20both%20the%20regions%20and%20maturation%20stages%20of%20TLS%20in%20WSI%20in%0Aan%20end-to-end%20manner.%20Due%20to%20the%20extensive%20scale%20of%20WSI%20and%20patch-based%0Asegmentation%20strategies%2C%20TLS-SS%20necessitates%20integrating%20from%20neighboring%0Apatches%20to%20guide%20target%20patch%20%28target%29%20segmentation.%20Previous%20techniques%20often%0Aemploy%20on%20multi-resolution%20approaches%2C%20constraining%20the%20capacity%20to%20leverage%0Athe%20broader%20neighboring%20context%20while%20tend%20to%20preserve%20coarse-grained%0Ainformation.%20To%20address%20this%2C%20we%20propose%20a%20GNN-based%20Neighboring%20Context%0AAggregation%20Framework%20%28GNCAF%29%2C%20which%20progressively%20aggregates%20multi-hop%0Aneighboring%20context%20from%20the%20target%20and%20employs%20a%20self-attention%20mechanism%20to%0Aguide%20the%20segmentation%20of%20the%20target.%20GNCAF%20can%20be%20integrated%20with%20various%0Asegmentation%20models%20to%20enhance%20their%20ability%20to%20perceive%20contextual%20information%0Aoutside%20of%20the%20patch.%20We%20build%20two%20TLS-SS%20datasets%2C%20called%20TCGA-COAD%20and%0AINHOUSE-PAAD%2C%20and%20make%20the%20former%20%28comprising%20225%20WSIs%20and%205041%20TLSs%29%20publicly%0Aavailable.%20Experiments%20on%20these%20datasets%20demonstrate%20the%20superiority%20of%20GNCAF%2C%0Aachieving%20a%20maximum%20of%2022.08%25%20and%2026.57%25%20improvement%20in%20mF1%20and%20mIoU%2C%0Arespectively.%20Additionally%2C%20we%20also%20validate%20the%20task%20scalability%20of%20GNCAF%20on%0Asegmentation%20of%20lymph%20node%20metastases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGNCAF%253A%2520A%2520GNN-based%2520Neighboring%2520Context%2520Aggregation%2520Framework%2520for%250A%2520%2520Tertiary%2520Lymphoid%2520Structures%2520Semantic%2520Segmentation%2520in%2520WSI%26entry.906535625%3DLei%2520Su%26entry.1292438233%3D%2520%2520Tertiary%2520lymphoid%2520structures%2520%2528TLS%2529%2520are%2520organized%2520clusters%2520of%2520immune%2520cells%252C%250Awhose%2520maturity%2520and%2520area%2520can%2520be%2520quantified%2520in%2520whole%2520slide%2520image%2520%2528WSI%2529%2520for%250Avarious%2520prognostic%2520tasks.%2520Existing%2520methods%2520for%2520assessing%2520these%2520characteristics%250Atypically%2520rely%2520on%2520cell%2520proxy%2520tasks%2520and%2520require%2520additional%2520post-processing%250Asteps.%2520In%2520this%2520work%252C%2520We%2520focus%2520on%2520a%2520novel%2520task-TLS%2520Semantic%2520Segmentation%250A%2528TLS-SS%2529-which%2520segments%2520both%2520the%2520regions%2520and%2520maturation%2520stages%2520of%2520TLS%2520in%2520WSI%2520in%250Aan%2520end-to-end%2520manner.%2520Due%2520to%2520the%2520extensive%2520scale%2520of%2520WSI%2520and%2520patch-based%250Asegmentation%2520strategies%252C%2520TLS-SS%2520necessitates%2520integrating%2520from%2520neighboring%250Apatches%2520to%2520guide%2520target%2520patch%2520%2528target%2529%2520segmentation.%2520Previous%2520techniques%2520often%250Aemploy%2520on%2520multi-resolution%2520approaches%252C%2520constraining%2520the%2520capacity%2520to%2520leverage%250Athe%2520broader%2520neighboring%2520context%2520while%2520tend%2520to%2520preserve%2520coarse-grained%250Ainformation.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520GNN-based%2520Neighboring%2520Context%250AAggregation%2520Framework%2520%2528GNCAF%2529%252C%2520which%2520progressively%2520aggregates%2520multi-hop%250Aneighboring%2520context%2520from%2520the%2520target%2520and%2520employs%2520a%2520self-attention%2520mechanism%2520to%250Aguide%2520the%2520segmentation%2520of%2520the%2520target.%2520GNCAF%2520can%2520be%2520integrated%2520with%2520various%250Asegmentation%2520models%2520to%2520enhance%2520their%2520ability%2520to%2520perceive%2520contextual%2520information%250Aoutside%2520of%2520the%2520patch.%2520We%2520build%2520two%2520TLS-SS%2520datasets%252C%2520called%2520TCGA-COAD%2520and%250AINHOUSE-PAAD%252C%2520and%2520make%2520the%2520former%2520%2528comprising%2520225%2520WSIs%2520and%25205041%2520TLSs%2529%2520publicly%250Aavailable.%2520Experiments%2520on%2520these%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520GNCAF%252C%250Aachieving%2520a%2520maximum%2520of%252022.08%2525%2520and%252026.57%2525%2520improvement%2520in%2520mF1%2520and%2520mIoU%252C%250Arespectively.%2520Additionally%252C%2520we%2520also%2520validate%2520the%2520task%2520scalability%2520of%2520GNCAF%2520on%250Asegmentation%2520of%2520lymph%2520node%2520metastases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNCAF%3A%20A%20GNN-based%20Neighboring%20Context%20Aggregation%20Framework%20for%0A%20%20Tertiary%20Lymphoid%20Structures%20Semantic%20Segmentation%20in%20WSI&entry.906535625=Lei%20Su&entry.1292438233=%20%20Tertiary%20lymphoid%20structures%20%28TLS%29%20are%20organized%20clusters%20of%20immune%20cells%2C%0Awhose%20maturity%20and%20area%20can%20be%20quantified%20in%20whole%20slide%20image%20%28WSI%29%20for%0Avarious%20prognostic%20tasks.%20Existing%20methods%20for%20assessing%20these%20characteristics%0Atypically%20rely%20on%20cell%20proxy%20tasks%20and%20require%20additional%20post-processing%0Asteps.%20In%20this%20work%2C%20We%20focus%20on%20a%20novel%20task-TLS%20Semantic%20Segmentation%0A%28TLS-SS%29-which%20segments%20both%20the%20regions%20and%20maturation%20stages%20of%20TLS%20in%20WSI%20in%0Aan%20end-to-end%20manner.%20Due%20to%20the%20extensive%20scale%20of%20WSI%20and%20patch-based%0Asegmentation%20strategies%2C%20TLS-SS%20necessitates%20integrating%20from%20neighboring%0Apatches%20to%20guide%20target%20patch%20%28target%29%20segmentation.%20Previous%20techniques%20often%0Aemploy%20on%20multi-resolution%20approaches%2C%20constraining%20the%20capacity%20to%20leverage%0Athe%20broader%20neighboring%20context%20while%20tend%20to%20preserve%20coarse-grained%0Ainformation.%20To%20address%20this%2C%20we%20propose%20a%20GNN-based%20Neighboring%20Context%0AAggregation%20Framework%20%28GNCAF%29%2C%20which%20progressively%20aggregates%20multi-hop%0Aneighboring%20context%20from%20the%20target%20and%20employs%20a%20self-attention%20mechanism%20to%0Aguide%20the%20segmentation%20of%20the%20target.%20GNCAF%20can%20be%20integrated%20with%20various%0Asegmentation%20models%20to%20enhance%20their%20ability%20to%20perceive%20contextual%20information%0Aoutside%20of%20the%20patch.%20We%20build%20two%20TLS-SS%20datasets%2C%20called%20TCGA-COAD%20and%0AINHOUSE-PAAD%2C%20and%20make%20the%20former%20%28comprising%20225%20WSIs%20and%205041%20TLSs%29%20publicly%0Aavailable.%20Experiments%20on%20these%20datasets%20demonstrate%20the%20superiority%20of%20GNCAF%2C%0Aachieving%20a%20maximum%20of%2022.08%25%20and%2026.57%25%20improvement%20in%20mF1%20and%20mIoU%2C%0Arespectively.%20Additionally%2C%20we%20also%20validate%20the%20task%20scalability%20of%20GNCAF%20on%0Asegmentation%20of%20lymph%20node%20metastases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08430v1&entry.124074799=Read"},
{"title": "Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss\n  for Tubular Structure Segmentation", "author": "Yiqi Chen and Ganghai Huang and Sheng Zhang and Jianglin Dai", "abstract": "  Accurate segmentation of tubular topological structures (e.g., fissures and\nvasculature) is critical in various fields to guarantee dependable downstream\nquantitative analysis and modeling. However, in dense prediction tasks such as\nsemantic segmentation and super-resolution, conventional upsampling operators\ncannot accommodate the slenderness of tubular structures and the curvature of\nmorphology. This paper introduces a dynamic snake upsampling operators and a\nboundary-skeleton weighted loss tailored for topological tubular structures.\nSpecifically, we design a snake upsampling operators based on an adaptive\nsampling domain, which dynamically adjusts the sampling stride according to the\nfeature map and selects a set of subpixel sampling points along the serpentine\npath, enabling more accurate subpixel-level feature recovery for tubular\nstructures. Meanwhile, we propose a skeleton-to-boundary increasing weighted\nloss that trades off main body and boundary weight allocation based on mask\nclass ratio and distance field, preserving main body overlap while enhancing\nfocus on target topological continuity and boundary alignment precision.\nExperiments across various domain datasets and backbone networks show that this\nplug-and-play dynamic snake upsampling operator and boundary-skeleton weighted\nloss boost both pixel-wise segmentation accuracy and topological consistency of\nresults.\n", "link": "http://arxiv.org/abs/2505.08525v1", "date": "2025-05-13", "relevancy": 2.5023, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5208}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4944}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Snake%20Upsampling%20Operater%20and%20Boundary-Skeleton%20Weighted%20Loss%0A%20%20for%20Tubular%20Structure%20Segmentation&body=Title%3A%20Dynamic%20Snake%20Upsampling%20Operater%20and%20Boundary-Skeleton%20Weighted%20Loss%0A%20%20for%20Tubular%20Structure%20Segmentation%0AAuthor%3A%20Yiqi%20Chen%20and%20Ganghai%20Huang%20and%20Sheng%20Zhang%20and%20Jianglin%20Dai%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20tubular%20topological%20structures%20%28e.g.%2C%20fissures%20and%0Avasculature%29%20is%20critical%20in%20various%20fields%20to%20guarantee%20dependable%20downstream%0Aquantitative%20analysis%20and%20modeling.%20However%2C%20in%20dense%20prediction%20tasks%20such%20as%0Asemantic%20segmentation%20and%20super-resolution%2C%20conventional%20upsampling%20operators%0Acannot%20accommodate%20the%20slenderness%20of%20tubular%20structures%20and%20the%20curvature%20of%0Amorphology.%20This%20paper%20introduces%20a%20dynamic%20snake%20upsampling%20operators%20and%20a%0Aboundary-skeleton%20weighted%20loss%20tailored%20for%20topological%20tubular%20structures.%0ASpecifically%2C%20we%20design%20a%20snake%20upsampling%20operators%20based%20on%20an%20adaptive%0Asampling%20domain%2C%20which%20dynamically%20adjusts%20the%20sampling%20stride%20according%20to%20the%0Afeature%20map%20and%20selects%20a%20set%20of%20subpixel%20sampling%20points%20along%20the%20serpentine%0Apath%2C%20enabling%20more%20accurate%20subpixel-level%20feature%20recovery%20for%20tubular%0Astructures.%20Meanwhile%2C%20we%20propose%20a%20skeleton-to-boundary%20increasing%20weighted%0Aloss%20that%20trades%20off%20main%20body%20and%20boundary%20weight%20allocation%20based%20on%20mask%0Aclass%20ratio%20and%20distance%20field%2C%20preserving%20main%20body%20overlap%20while%20enhancing%0Afocus%20on%20target%20topological%20continuity%20and%20boundary%20alignment%20precision.%0AExperiments%20across%20various%20domain%20datasets%20and%20backbone%20networks%20show%20that%20this%0Aplug-and-play%20dynamic%20snake%20upsampling%20operator%20and%20boundary-skeleton%20weighted%0Aloss%20boost%20both%20pixel-wise%20segmentation%20accuracy%20and%20topological%20consistency%20of%0Aresults.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Snake%2520Upsampling%2520Operater%2520and%2520Boundary-Skeleton%2520Weighted%2520Loss%250A%2520%2520for%2520Tubular%2520Structure%2520Segmentation%26entry.906535625%3DYiqi%2520Chen%2520and%2520Ganghai%2520Huang%2520and%2520Sheng%2520Zhang%2520and%2520Jianglin%2520Dai%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520tubular%2520topological%2520structures%2520%2528e.g.%252C%2520fissures%2520and%250Avasculature%2529%2520is%2520critical%2520in%2520various%2520fields%2520to%2520guarantee%2520dependable%2520downstream%250Aquantitative%2520analysis%2520and%2520modeling.%2520However%252C%2520in%2520dense%2520prediction%2520tasks%2520such%2520as%250Asemantic%2520segmentation%2520and%2520super-resolution%252C%2520conventional%2520upsampling%2520operators%250Acannot%2520accommodate%2520the%2520slenderness%2520of%2520tubular%2520structures%2520and%2520the%2520curvature%2520of%250Amorphology.%2520This%2520paper%2520introduces%2520a%2520dynamic%2520snake%2520upsampling%2520operators%2520and%2520a%250Aboundary-skeleton%2520weighted%2520loss%2520tailored%2520for%2520topological%2520tubular%2520structures.%250ASpecifically%252C%2520we%2520design%2520a%2520snake%2520upsampling%2520operators%2520based%2520on%2520an%2520adaptive%250Asampling%2520domain%252C%2520which%2520dynamically%2520adjusts%2520the%2520sampling%2520stride%2520according%2520to%2520the%250Afeature%2520map%2520and%2520selects%2520a%2520set%2520of%2520subpixel%2520sampling%2520points%2520along%2520the%2520serpentine%250Apath%252C%2520enabling%2520more%2520accurate%2520subpixel-level%2520feature%2520recovery%2520for%2520tubular%250Astructures.%2520Meanwhile%252C%2520we%2520propose%2520a%2520skeleton-to-boundary%2520increasing%2520weighted%250Aloss%2520that%2520trades%2520off%2520main%2520body%2520and%2520boundary%2520weight%2520allocation%2520based%2520on%2520mask%250Aclass%2520ratio%2520and%2520distance%2520field%252C%2520preserving%2520main%2520body%2520overlap%2520while%2520enhancing%250Afocus%2520on%2520target%2520topological%2520continuity%2520and%2520boundary%2520alignment%2520precision.%250AExperiments%2520across%2520various%2520domain%2520datasets%2520and%2520backbone%2520networks%2520show%2520that%2520this%250Aplug-and-play%2520dynamic%2520snake%2520upsampling%2520operator%2520and%2520boundary-skeleton%2520weighted%250Aloss%2520boost%2520both%2520pixel-wise%2520segmentation%2520accuracy%2520and%2520topological%2520consistency%2520of%250Aresults.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Snake%20Upsampling%20Operater%20and%20Boundary-Skeleton%20Weighted%20Loss%0A%20%20for%20Tubular%20Structure%20Segmentation&entry.906535625=Yiqi%20Chen%20and%20Ganghai%20Huang%20and%20Sheng%20Zhang%20and%20Jianglin%20Dai&entry.1292438233=%20%20Accurate%20segmentation%20of%20tubular%20topological%20structures%20%28e.g.%2C%20fissures%20and%0Avasculature%29%20is%20critical%20in%20various%20fields%20to%20guarantee%20dependable%20downstream%0Aquantitative%20analysis%20and%20modeling.%20However%2C%20in%20dense%20prediction%20tasks%20such%20as%0Asemantic%20segmentation%20and%20super-resolution%2C%20conventional%20upsampling%20operators%0Acannot%20accommodate%20the%20slenderness%20of%20tubular%20structures%20and%20the%20curvature%20of%0Amorphology.%20This%20paper%20introduces%20a%20dynamic%20snake%20upsampling%20operators%20and%20a%0Aboundary-skeleton%20weighted%20loss%20tailored%20for%20topological%20tubular%20structures.%0ASpecifically%2C%20we%20design%20a%20snake%20upsampling%20operators%20based%20on%20an%20adaptive%0Asampling%20domain%2C%20which%20dynamically%20adjusts%20the%20sampling%20stride%20according%20to%20the%0Afeature%20map%20and%20selects%20a%20set%20of%20subpixel%20sampling%20points%20along%20the%20serpentine%0Apath%2C%20enabling%20more%20accurate%20subpixel-level%20feature%20recovery%20for%20tubular%0Astructures.%20Meanwhile%2C%20we%20propose%20a%20skeleton-to-boundary%20increasing%20weighted%0Aloss%20that%20trades%20off%20main%20body%20and%20boundary%20weight%20allocation%20based%20on%20mask%0Aclass%20ratio%20and%20distance%20field%2C%20preserving%20main%20body%20overlap%20while%20enhancing%0Afocus%20on%20target%20topological%20continuity%20and%20boundary%20alignment%20precision.%0AExperiments%20across%20various%20domain%20datasets%20and%20backbone%20networks%20show%20that%20this%0Aplug-and-play%20dynamic%20snake%20upsampling%20operator%20and%20boundary-skeleton%20weighted%0Aloss%20boost%20both%20pixel-wise%20segmentation%20accuracy%20and%20topological%20consistency%20of%0Aresults.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08525v1&entry.124074799=Read"},
{"title": "InfoPO: On Mutual Information Maximization for Large Language Model\n  Alignment", "author": "Teng Xiao and Zhen Ge and Sujay Sanghavi and Tian Wang and Julian Katz-Samuels and Marc Versage and Qingjun Cui and Trishul Chilimbi", "abstract": "  We study the post-training of large language models (LLMs) with human\npreference data. Recently, direct preference optimization and its variants have\nshown considerable promise in aligning language models, eliminating the need\nfor reward models and online sampling. Despite these benefits, these methods\nrely on explicit assumptions about the Bradley-Terry (BT) model, which makes\nthem prone to overfitting and results in suboptimal performance, particularly\non reasoning-heavy tasks. To address these challenges, we propose a principled\npreference fine-tuning algorithm called InfoPO, which effectively and\nefficiently aligns large language models using preference data. InfoPO\neliminates the reliance on the BT model and prevents the likelihood of the\nchosen response from decreasing. Extensive experiments confirm that InfoPO\nconsistently outperforms established baselines on widely used open benchmarks,\nparticularly in reasoning tasks.\n", "link": "http://arxiv.org/abs/2505.08507v1", "date": "2025-05-13", "relevancy": 2.4544, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4923}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfoPO%3A%20On%20Mutual%20Information%20Maximization%20for%20Large%20Language%20Model%0A%20%20Alignment&body=Title%3A%20InfoPO%3A%20On%20Mutual%20Information%20Maximization%20for%20Large%20Language%20Model%0A%20%20Alignment%0AAuthor%3A%20Teng%20Xiao%20and%20Zhen%20Ge%20and%20Sujay%20Sanghavi%20and%20Tian%20Wang%20and%20Julian%20Katz-Samuels%20and%20Marc%20Versage%20and%20Qingjun%20Cui%20and%20Trishul%20Chilimbi%0AAbstract%3A%20%20%20We%20study%20the%20post-training%20of%20large%20language%20models%20%28LLMs%29%20with%20human%0Apreference%20data.%20Recently%2C%20direct%20preference%20optimization%20and%20its%20variants%20have%0Ashown%20considerable%20promise%20in%20aligning%20language%20models%2C%20eliminating%20the%20need%0Afor%20reward%20models%20and%20online%20sampling.%20Despite%20these%20benefits%2C%20these%20methods%0Arely%20on%20explicit%20assumptions%20about%20the%20Bradley-Terry%20%28BT%29%20model%2C%20which%20makes%0Athem%20prone%20to%20overfitting%20and%20results%20in%20suboptimal%20performance%2C%20particularly%0Aon%20reasoning-heavy%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20principled%0Apreference%20fine-tuning%20algorithm%20called%20InfoPO%2C%20which%20effectively%20and%0Aefficiently%20aligns%20large%20language%20models%20using%20preference%20data.%20InfoPO%0Aeliminates%20the%20reliance%20on%20the%20BT%20model%20and%20prevents%20the%20likelihood%20of%20the%0Achosen%20response%20from%20decreasing.%20Extensive%20experiments%20confirm%20that%20InfoPO%0Aconsistently%20outperforms%20established%20baselines%20on%20widely%20used%20open%20benchmarks%2C%0Aparticularly%20in%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfoPO%253A%2520On%2520Mutual%2520Information%2520Maximization%2520for%2520Large%2520Language%2520Model%250A%2520%2520Alignment%26entry.906535625%3DTeng%2520Xiao%2520and%2520Zhen%2520Ge%2520and%2520Sujay%2520Sanghavi%2520and%2520Tian%2520Wang%2520and%2520Julian%2520Katz-Samuels%2520and%2520Marc%2520Versage%2520and%2520Qingjun%2520Cui%2520and%2520Trishul%2520Chilimbi%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520post-training%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%250Apreference%2520data.%2520Recently%252C%2520direct%2520preference%2520optimization%2520and%2520its%2520variants%2520have%250Ashown%2520considerable%2520promise%2520in%2520aligning%2520language%2520models%252C%2520eliminating%2520the%2520need%250Afor%2520reward%2520models%2520and%2520online%2520sampling.%2520Despite%2520these%2520benefits%252C%2520these%2520methods%250Arely%2520on%2520explicit%2520assumptions%2520about%2520the%2520Bradley-Terry%2520%2528BT%2529%2520model%252C%2520which%2520makes%250Athem%2520prone%2520to%2520overfitting%2520and%2520results%2520in%2520suboptimal%2520performance%252C%2520particularly%250Aon%2520reasoning-heavy%2520tasks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520principled%250Apreference%2520fine-tuning%2520algorithm%2520called%2520InfoPO%252C%2520which%2520effectively%2520and%250Aefficiently%2520aligns%2520large%2520language%2520models%2520using%2520preference%2520data.%2520InfoPO%250Aeliminates%2520the%2520reliance%2520on%2520the%2520BT%2520model%2520and%2520prevents%2520the%2520likelihood%2520of%2520the%250Achosen%2520response%2520from%2520decreasing.%2520Extensive%2520experiments%2520confirm%2520that%2520InfoPO%250Aconsistently%2520outperforms%2520established%2520baselines%2520on%2520widely%2520used%2520open%2520benchmarks%252C%250Aparticularly%2520in%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfoPO%3A%20On%20Mutual%20Information%20Maximization%20for%20Large%20Language%20Model%0A%20%20Alignment&entry.906535625=Teng%20Xiao%20and%20Zhen%20Ge%20and%20Sujay%20Sanghavi%20and%20Tian%20Wang%20and%20Julian%20Katz-Samuels%20and%20Marc%20Versage%20and%20Qingjun%20Cui%20and%20Trishul%20Chilimbi&entry.1292438233=%20%20We%20study%20the%20post-training%20of%20large%20language%20models%20%28LLMs%29%20with%20human%0Apreference%20data.%20Recently%2C%20direct%20preference%20optimization%20and%20its%20variants%20have%0Ashown%20considerable%20promise%20in%20aligning%20language%20models%2C%20eliminating%20the%20need%0Afor%20reward%20models%20and%20online%20sampling.%20Despite%20these%20benefits%2C%20these%20methods%0Arely%20on%20explicit%20assumptions%20about%20the%20Bradley-Terry%20%28BT%29%20model%2C%20which%20makes%0Athem%20prone%20to%20overfitting%20and%20results%20in%20suboptimal%20performance%2C%20particularly%0Aon%20reasoning-heavy%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20principled%0Apreference%20fine-tuning%20algorithm%20called%20InfoPO%2C%20which%20effectively%20and%0Aefficiently%20aligns%20large%20language%20models%20using%20preference%20data.%20InfoPO%0Aeliminates%20the%20reliance%20on%20the%20BT%20model%20and%20prevents%20the%20likelihood%20of%20the%0Achosen%20response%20from%20decreasing.%20Extensive%20experiments%20confirm%20that%20InfoPO%0Aconsistently%20outperforms%20established%20baselines%20on%20widely%20used%20open%20benchmarks%2C%0Aparticularly%20in%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08507v1&entry.124074799=Read"},
{"title": "neuralGAM: An R Package for Fitting Generalized Additive Neural Networks", "author": "Ines Ortega-Fernandez and Marta Sestelo", "abstract": "  Nowadays, Neural Networks are considered one of the most effective methods\nfor various tasks such as anomaly detection, computer-aided disease detection,\nor natural language processing. However, these networks suffer from the\n``black-box'' problem which makes it difficult to understand how they make\ndecisions. In order to solve this issue, an R package called neuralGAM is\nintroduced. This package implements a Neural Network topology based on\nGeneralized Additive Models, allowing to fit an independent Neural Network to\nestimate the contribution of each feature to the output variable, yielding a\nhighly accurate and interpretable Deep Learning model. The neuralGAM package\nprovides a flexible framework for training Generalized Additive Neural\nNetworks, which does not impose any restrictions on the Neural Network\narchitecture. We illustrate the use of the neuralGAM package in both synthetic\nand real data examples.\n", "link": "http://arxiv.org/abs/2505.08610v1", "date": "2025-05-13", "relevancy": 2.4511, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5092}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4829}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20neuralGAM%3A%20An%20R%20Package%20for%20Fitting%20Generalized%20Additive%20Neural%20Networks&body=Title%3A%20neuralGAM%3A%20An%20R%20Package%20for%20Fitting%20Generalized%20Additive%20Neural%20Networks%0AAuthor%3A%20Ines%20Ortega-Fernandez%20and%20Marta%20Sestelo%0AAbstract%3A%20%20%20Nowadays%2C%20Neural%20Networks%20are%20considered%20one%20of%20the%20most%20effective%20methods%0Afor%20various%20tasks%20such%20as%20anomaly%20detection%2C%20computer-aided%20disease%20detection%2C%0Aor%20natural%20language%20processing.%20However%2C%20these%20networks%20suffer%20from%20the%0A%60%60black-box%27%27%20problem%20which%20makes%20it%20difficult%20to%20understand%20how%20they%20make%0Adecisions.%20In%20order%20to%20solve%20this%20issue%2C%20an%20R%20package%20called%20neuralGAM%20is%0Aintroduced.%20This%20package%20implements%20a%20Neural%20Network%20topology%20based%20on%0AGeneralized%20Additive%20Models%2C%20allowing%20to%20fit%20an%20independent%20Neural%20Network%20to%0Aestimate%20the%20contribution%20of%20each%20feature%20to%20the%20output%20variable%2C%20yielding%20a%0Ahighly%20accurate%20and%20interpretable%20Deep%20Learning%20model.%20The%20neuralGAM%20package%0Aprovides%20a%20flexible%20framework%20for%20training%20Generalized%20Additive%20Neural%0ANetworks%2C%20which%20does%20not%20impose%20any%20restrictions%20on%20the%20Neural%20Network%0Aarchitecture.%20We%20illustrate%20the%20use%20of%20the%20neuralGAM%20package%20in%20both%20synthetic%0Aand%20real%20data%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DneuralGAM%253A%2520An%2520R%2520Package%2520for%2520Fitting%2520Generalized%2520Additive%2520Neural%2520Networks%26entry.906535625%3DInes%2520Ortega-Fernandez%2520and%2520Marta%2520Sestelo%26entry.1292438233%3D%2520%2520Nowadays%252C%2520Neural%2520Networks%2520are%2520considered%2520one%2520of%2520the%2520most%2520effective%2520methods%250Afor%2520various%2520tasks%2520such%2520as%2520anomaly%2520detection%252C%2520computer-aided%2520disease%2520detection%252C%250Aor%2520natural%2520language%2520processing.%2520However%252C%2520these%2520networks%2520suffer%2520from%2520the%250A%2560%2560black-box%2527%2527%2520problem%2520which%2520makes%2520it%2520difficult%2520to%2520understand%2520how%2520they%2520make%250Adecisions.%2520In%2520order%2520to%2520solve%2520this%2520issue%252C%2520an%2520R%2520package%2520called%2520neuralGAM%2520is%250Aintroduced.%2520This%2520package%2520implements%2520a%2520Neural%2520Network%2520topology%2520based%2520on%250AGeneralized%2520Additive%2520Models%252C%2520allowing%2520to%2520fit%2520an%2520independent%2520Neural%2520Network%2520to%250Aestimate%2520the%2520contribution%2520of%2520each%2520feature%2520to%2520the%2520output%2520variable%252C%2520yielding%2520a%250Ahighly%2520accurate%2520and%2520interpretable%2520Deep%2520Learning%2520model.%2520The%2520neuralGAM%2520package%250Aprovides%2520a%2520flexible%2520framework%2520for%2520training%2520Generalized%2520Additive%2520Neural%250ANetworks%252C%2520which%2520does%2520not%2520impose%2520any%2520restrictions%2520on%2520the%2520Neural%2520Network%250Aarchitecture.%2520We%2520illustrate%2520the%2520use%2520of%2520the%2520neuralGAM%2520package%2520in%2520both%2520synthetic%250Aand%2520real%2520data%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=neuralGAM%3A%20An%20R%20Package%20for%20Fitting%20Generalized%20Additive%20Neural%20Networks&entry.906535625=Ines%20Ortega-Fernandez%20and%20Marta%20Sestelo&entry.1292438233=%20%20Nowadays%2C%20Neural%20Networks%20are%20considered%20one%20of%20the%20most%20effective%20methods%0Afor%20various%20tasks%20such%20as%20anomaly%20detection%2C%20computer-aided%20disease%20detection%2C%0Aor%20natural%20language%20processing.%20However%2C%20these%20networks%20suffer%20from%20the%0A%60%60black-box%27%27%20problem%20which%20makes%20it%20difficult%20to%20understand%20how%20they%20make%0Adecisions.%20In%20order%20to%20solve%20this%20issue%2C%20an%20R%20package%20called%20neuralGAM%20is%0Aintroduced.%20This%20package%20implements%20a%20Neural%20Network%20topology%20based%20on%0AGeneralized%20Additive%20Models%2C%20allowing%20to%20fit%20an%20independent%20Neural%20Network%20to%0Aestimate%20the%20contribution%20of%20each%20feature%20to%20the%20output%20variable%2C%20yielding%20a%0Ahighly%20accurate%20and%20interpretable%20Deep%20Learning%20model.%20The%20neuralGAM%20package%0Aprovides%20a%20flexible%20framework%20for%20training%20Generalized%20Additive%20Neural%0ANetworks%2C%20which%20does%20not%20impose%20any%20restrictions%20on%20the%20Neural%20Network%0Aarchitecture.%20We%20illustrate%20the%20use%20of%20the%20neuralGAM%20package%20in%20both%20synthetic%0Aand%20real%20data%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08610v1&entry.124074799=Read"},
{"title": "VizCV: AI-assisted visualization of researchers' publications tracks", "author": "Vladim\u00edr Laz\u00e1rik and Marco Agus and Barbora Kozl\u00edkov\u00e1 and Pere-Pau V\u00e1zquez", "abstract": "  Analyzing how the publication records of scientists and research groups have\nevolved over the years is crucial for assessing their expertise since it can\nsupport the management of academic environments by assisting with career\nplanning and evaluation. We introduce VizCV, a novel web-based end-to-end\nvisual analytics framework that enables the interactive exploration of\nresearchers' scientific trajectories. It incorporates AI-assisted analysis and\nsupports automated reporting of career evolution. Our system aims to model\ncareer progression through three key dimensions: a) research topic evolution to\ndetect and visualize shifts in scholarly focus over time, b) publication record\nand the corresponding impact, c) collaboration dynamics depicting the growth\nand transformation of a researcher's co-authorship network. AI-driven insights\nprovide automated explanations of career transitions, detecting significant\nshifts in research direction, impact surges, or collaboration expansions. The\nsystem also supports comparative analysis between researchers, allowing users\nto compare topic trajectories and impact growth. Our interactive, multi-tab and\nmultiview system allows for the exploratory analysis of career milestones under\ndifferent perspectives, such as the most impactful articles, emerging research\nthemes, or obtaining a detailed analysis of the contribution of the researcher\nin a subfield. The key contributions include AI/ML techniques for: a) topic\nanalysis, b) dimensionality reduction for visualizing patterns and trends, c)\nthe interactive creation of textual descriptions of facets of data through\nconfigurable prompt generation and large language models, that include key\nindicators, to help understanding the career development of individuals or\ngroups.\n", "link": "http://arxiv.org/abs/2505.08691v1", "date": "2025-05-13", "relevancy": 2.4361, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4965}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4965}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VizCV%3A%20AI-assisted%20visualization%20of%20researchers%27%20publications%20tracks&body=Title%3A%20VizCV%3A%20AI-assisted%20visualization%20of%20researchers%27%20publications%20tracks%0AAuthor%3A%20Vladim%C3%ADr%20Laz%C3%A1rik%20and%20Marco%20Agus%20and%20Barbora%20Kozl%C3%ADkov%C3%A1%20and%20Pere-Pau%20V%C3%A1zquez%0AAbstract%3A%20%20%20Analyzing%20how%20the%20publication%20records%20of%20scientists%20and%20research%20groups%20have%0Aevolved%20over%20the%20years%20is%20crucial%20for%20assessing%20their%20expertise%20since%20it%20can%0Asupport%20the%20management%20of%20academic%20environments%20by%20assisting%20with%20career%0Aplanning%20and%20evaluation.%20We%20introduce%20VizCV%2C%20a%20novel%20web-based%20end-to-end%0Avisual%20analytics%20framework%20that%20enables%20the%20interactive%20exploration%20of%0Aresearchers%27%20scientific%20trajectories.%20It%20incorporates%20AI-assisted%20analysis%20and%0Asupports%20automated%20reporting%20of%20career%20evolution.%20Our%20system%20aims%20to%20model%0Acareer%20progression%20through%20three%20key%20dimensions%3A%20a%29%20research%20topic%20evolution%20to%0Adetect%20and%20visualize%20shifts%20in%20scholarly%20focus%20over%20time%2C%20b%29%20publication%20record%0Aand%20the%20corresponding%20impact%2C%20c%29%20collaboration%20dynamics%20depicting%20the%20growth%0Aand%20transformation%20of%20a%20researcher%27s%20co-authorship%20network.%20AI-driven%20insights%0Aprovide%20automated%20explanations%20of%20career%20transitions%2C%20detecting%20significant%0Ashifts%20in%20research%20direction%2C%20impact%20surges%2C%20or%20collaboration%20expansions.%20The%0Asystem%20also%20supports%20comparative%20analysis%20between%20researchers%2C%20allowing%20users%0Ato%20compare%20topic%20trajectories%20and%20impact%20growth.%20Our%20interactive%2C%20multi-tab%20and%0Amultiview%20system%20allows%20for%20the%20exploratory%20analysis%20of%20career%20milestones%20under%0Adifferent%20perspectives%2C%20such%20as%20the%20most%20impactful%20articles%2C%20emerging%20research%0Athemes%2C%20or%20obtaining%20a%20detailed%20analysis%20of%20the%20contribution%20of%20the%20researcher%0Ain%20a%20subfield.%20The%20key%20contributions%20include%20AI/ML%20techniques%20for%3A%20a%29%20topic%0Aanalysis%2C%20b%29%20dimensionality%20reduction%20for%20visualizing%20patterns%20and%20trends%2C%20c%29%0Athe%20interactive%20creation%20of%20textual%20descriptions%20of%20facets%20of%20data%20through%0Aconfigurable%20prompt%20generation%20and%20large%20language%20models%2C%20that%20include%20key%0Aindicators%2C%20to%20help%20understanding%20the%20career%20development%20of%20individuals%20or%0Agroups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVizCV%253A%2520AI-assisted%2520visualization%2520of%2520researchers%2527%2520publications%2520tracks%26entry.906535625%3DVladim%25C3%25ADr%2520Laz%25C3%25A1rik%2520and%2520Marco%2520Agus%2520and%2520Barbora%2520Kozl%25C3%25ADkov%25C3%25A1%2520and%2520Pere-Pau%2520V%25C3%25A1zquez%26entry.1292438233%3D%2520%2520Analyzing%2520how%2520the%2520publication%2520records%2520of%2520scientists%2520and%2520research%2520groups%2520have%250Aevolved%2520over%2520the%2520years%2520is%2520crucial%2520for%2520assessing%2520their%2520expertise%2520since%2520it%2520can%250Asupport%2520the%2520management%2520of%2520academic%2520environments%2520by%2520assisting%2520with%2520career%250Aplanning%2520and%2520evaluation.%2520We%2520introduce%2520VizCV%252C%2520a%2520novel%2520web-based%2520end-to-end%250Avisual%2520analytics%2520framework%2520that%2520enables%2520the%2520interactive%2520exploration%2520of%250Aresearchers%2527%2520scientific%2520trajectories.%2520It%2520incorporates%2520AI-assisted%2520analysis%2520and%250Asupports%2520automated%2520reporting%2520of%2520career%2520evolution.%2520Our%2520system%2520aims%2520to%2520model%250Acareer%2520progression%2520through%2520three%2520key%2520dimensions%253A%2520a%2529%2520research%2520topic%2520evolution%2520to%250Adetect%2520and%2520visualize%2520shifts%2520in%2520scholarly%2520focus%2520over%2520time%252C%2520b%2529%2520publication%2520record%250Aand%2520the%2520corresponding%2520impact%252C%2520c%2529%2520collaboration%2520dynamics%2520depicting%2520the%2520growth%250Aand%2520transformation%2520of%2520a%2520researcher%2527s%2520co-authorship%2520network.%2520AI-driven%2520insights%250Aprovide%2520automated%2520explanations%2520of%2520career%2520transitions%252C%2520detecting%2520significant%250Ashifts%2520in%2520research%2520direction%252C%2520impact%2520surges%252C%2520or%2520collaboration%2520expansions.%2520The%250Asystem%2520also%2520supports%2520comparative%2520analysis%2520between%2520researchers%252C%2520allowing%2520users%250Ato%2520compare%2520topic%2520trajectories%2520and%2520impact%2520growth.%2520Our%2520interactive%252C%2520multi-tab%2520and%250Amultiview%2520system%2520allows%2520for%2520the%2520exploratory%2520analysis%2520of%2520career%2520milestones%2520under%250Adifferent%2520perspectives%252C%2520such%2520as%2520the%2520most%2520impactful%2520articles%252C%2520emerging%2520research%250Athemes%252C%2520or%2520obtaining%2520a%2520detailed%2520analysis%2520of%2520the%2520contribution%2520of%2520the%2520researcher%250Ain%2520a%2520subfield.%2520The%2520key%2520contributions%2520include%2520AI/ML%2520techniques%2520for%253A%2520a%2529%2520topic%250Aanalysis%252C%2520b%2529%2520dimensionality%2520reduction%2520for%2520visualizing%2520patterns%2520and%2520trends%252C%2520c%2529%250Athe%2520interactive%2520creation%2520of%2520textual%2520descriptions%2520of%2520facets%2520of%2520data%2520through%250Aconfigurable%2520prompt%2520generation%2520and%2520large%2520language%2520models%252C%2520that%2520include%2520key%250Aindicators%252C%2520to%2520help%2520understanding%2520the%2520career%2520development%2520of%2520individuals%2520or%250Agroups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VizCV%3A%20AI-assisted%20visualization%20of%20researchers%27%20publications%20tracks&entry.906535625=Vladim%C3%ADr%20Laz%C3%A1rik%20and%20Marco%20Agus%20and%20Barbora%20Kozl%C3%ADkov%C3%A1%20and%20Pere-Pau%20V%C3%A1zquez&entry.1292438233=%20%20Analyzing%20how%20the%20publication%20records%20of%20scientists%20and%20research%20groups%20have%0Aevolved%20over%20the%20years%20is%20crucial%20for%20assessing%20their%20expertise%20since%20it%20can%0Asupport%20the%20management%20of%20academic%20environments%20by%20assisting%20with%20career%0Aplanning%20and%20evaluation.%20We%20introduce%20VizCV%2C%20a%20novel%20web-based%20end-to-end%0Avisual%20analytics%20framework%20that%20enables%20the%20interactive%20exploration%20of%0Aresearchers%27%20scientific%20trajectories.%20It%20incorporates%20AI-assisted%20analysis%20and%0Asupports%20automated%20reporting%20of%20career%20evolution.%20Our%20system%20aims%20to%20model%0Acareer%20progression%20through%20three%20key%20dimensions%3A%20a%29%20research%20topic%20evolution%20to%0Adetect%20and%20visualize%20shifts%20in%20scholarly%20focus%20over%20time%2C%20b%29%20publication%20record%0Aand%20the%20corresponding%20impact%2C%20c%29%20collaboration%20dynamics%20depicting%20the%20growth%0Aand%20transformation%20of%20a%20researcher%27s%20co-authorship%20network.%20AI-driven%20insights%0Aprovide%20automated%20explanations%20of%20career%20transitions%2C%20detecting%20significant%0Ashifts%20in%20research%20direction%2C%20impact%20surges%2C%20or%20collaboration%20expansions.%20The%0Asystem%20also%20supports%20comparative%20analysis%20between%20researchers%2C%20allowing%20users%0Ato%20compare%20topic%20trajectories%20and%20impact%20growth.%20Our%20interactive%2C%20multi-tab%20and%0Amultiview%20system%20allows%20for%20the%20exploratory%20analysis%20of%20career%20milestones%20under%0Adifferent%20perspectives%2C%20such%20as%20the%20most%20impactful%20articles%2C%20emerging%20research%0Athemes%2C%20or%20obtaining%20a%20detailed%20analysis%20of%20the%20contribution%20of%20the%20researcher%0Ain%20a%20subfield.%20The%20key%20contributions%20include%20AI/ML%20techniques%20for%3A%20a%29%20topic%0Aanalysis%2C%20b%29%20dimensionality%20reduction%20for%20visualizing%20patterns%20and%20trends%2C%20c%29%0Athe%20interactive%20creation%20of%20textual%20descriptions%20of%20facets%20of%20data%20through%0Aconfigurable%20prompt%20generation%20and%20large%20language%20models%2C%20that%20include%20key%0Aindicators%2C%20to%20help%20understanding%20the%20career%20development%20of%20individuals%20or%0Agroups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08691v1&entry.124074799=Read"},
{"title": "Visual Imitation Enables Contextual Humanoid Control", "author": "Arthur Allshire and Hongsuk Choi and Junyi Zhang and David McAllister and Anthony Zhang and Chung Min Kim and Trevor Darrell and Pieter Abbeel and Jitendra Malik and Angjoo Kanazawa", "abstract": "  How can we teach humanoids to climb staircases and sit on chairs using the\nsurrounding environment context? Arguably, the simplest way is to just show\nthem-casually capture a human motion video and feed it to humanoids. We\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\nvideos, jointly reconstructs the humans and the environment, and produces\nwhole-body control policies for humanoid robots that perform the corresponding\nskills. We demonstrate the results of our pipeline on real humanoid robots,\nshowing robust, repeatable contextual control such as staircase ascents and\ndescents, sitting and standing from chairs and benches, as well as other\ndynamic whole-body skills-all from a single policy, conditioned on the\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\nteaching humanoids to operate in diverse real-world environments.\n", "link": "http://arxiv.org/abs/2505.03729v3", "date": "2025-05-13", "relevancy": 2.4311, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6172}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6154}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Imitation%20Enables%20Contextual%20Humanoid%20Control&body=Title%3A%20Visual%20Imitation%20Enables%20Contextual%20Humanoid%20Control%0AAuthor%3A%20Arthur%20Allshire%20and%20Hongsuk%20Choi%20and%20Junyi%20Zhang%20and%20David%20McAllister%20and%20Anthony%20Zhang%20and%20Chung%20Min%20Kim%20and%20Trevor%20Darrell%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20How%20can%20we%20teach%20humanoids%20to%20climb%20staircases%20and%20sit%20on%20chairs%20using%20the%0Asurrounding%20environment%20context%3F%20Arguably%2C%20the%20simplest%20way%20is%20to%20just%20show%0Athem-casually%20capture%20a%20human%20motion%20video%20and%20feed%20it%20to%20humanoids.%20We%0Aintroduce%20VIDEOMIMIC%2C%20a%20real-to-sim-to-real%20pipeline%20that%20mines%20everyday%0Avideos%2C%20jointly%20reconstructs%20the%20humans%20and%20the%20environment%2C%20and%20produces%0Awhole-body%20control%20policies%20for%20humanoid%20robots%20that%20perform%20the%20corresponding%0Askills.%20We%20demonstrate%20the%20results%20of%20our%20pipeline%20on%20real%20humanoid%20robots%2C%0Ashowing%20robust%2C%20repeatable%20contextual%20control%20such%20as%20staircase%20ascents%20and%0Adescents%2C%20sitting%20and%20standing%20from%20chairs%20and%20benches%2C%20as%20well%20as%20other%0Adynamic%20whole-body%20skills-all%20from%20a%20single%20policy%2C%20conditioned%20on%20the%0Aenvironment%20and%20global%20root%20commands.%20VIDEOMIMIC%20offers%20a%20scalable%20path%20towards%0Ateaching%20humanoids%20to%20operate%20in%20diverse%20real-world%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03729v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Imitation%2520Enables%2520Contextual%2520Humanoid%2520Control%26entry.906535625%3DArthur%2520Allshire%2520and%2520Hongsuk%2520Choi%2520and%2520Junyi%2520Zhang%2520and%2520David%2520McAllister%2520and%2520Anthony%2520Zhang%2520and%2520Chung%2520Min%2520Kim%2520and%2520Trevor%2520Darrell%2520and%2520Pieter%2520Abbeel%2520and%2520Jitendra%2520Malik%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520How%2520can%2520we%2520teach%2520humanoids%2520to%2520climb%2520staircases%2520and%2520sit%2520on%2520chairs%2520using%2520the%250Asurrounding%2520environment%2520context%253F%2520Arguably%252C%2520the%2520simplest%2520way%2520is%2520to%2520just%2520show%250Athem-casually%2520capture%2520a%2520human%2520motion%2520video%2520and%2520feed%2520it%2520to%2520humanoids.%2520We%250Aintroduce%2520VIDEOMIMIC%252C%2520a%2520real-to-sim-to-real%2520pipeline%2520that%2520mines%2520everyday%250Avideos%252C%2520jointly%2520reconstructs%2520the%2520humans%2520and%2520the%2520environment%252C%2520and%2520produces%250Awhole-body%2520control%2520policies%2520for%2520humanoid%2520robots%2520that%2520perform%2520the%2520corresponding%250Askills.%2520We%2520demonstrate%2520the%2520results%2520of%2520our%2520pipeline%2520on%2520real%2520humanoid%2520robots%252C%250Ashowing%2520robust%252C%2520repeatable%2520contextual%2520control%2520such%2520as%2520staircase%2520ascents%2520and%250Adescents%252C%2520sitting%2520and%2520standing%2520from%2520chairs%2520and%2520benches%252C%2520as%2520well%2520as%2520other%250Adynamic%2520whole-body%2520skills-all%2520from%2520a%2520single%2520policy%252C%2520conditioned%2520on%2520the%250Aenvironment%2520and%2520global%2520root%2520commands.%2520VIDEOMIMIC%2520offers%2520a%2520scalable%2520path%2520towards%250Ateaching%2520humanoids%2520to%2520operate%2520in%2520diverse%2520real-world%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03729v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Imitation%20Enables%20Contextual%20Humanoid%20Control&entry.906535625=Arthur%20Allshire%20and%20Hongsuk%20Choi%20and%20Junyi%20Zhang%20and%20David%20McAllister%20and%20Anthony%20Zhang%20and%20Chung%20Min%20Kim%20and%20Trevor%20Darrell%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20How%20can%20we%20teach%20humanoids%20to%20climb%20staircases%20and%20sit%20on%20chairs%20using%20the%0Asurrounding%20environment%20context%3F%20Arguably%2C%20the%20simplest%20way%20is%20to%20just%20show%0Athem-casually%20capture%20a%20human%20motion%20video%20and%20feed%20it%20to%20humanoids.%20We%0Aintroduce%20VIDEOMIMIC%2C%20a%20real-to-sim-to-real%20pipeline%20that%20mines%20everyday%0Avideos%2C%20jointly%20reconstructs%20the%20humans%20and%20the%20environment%2C%20and%20produces%0Awhole-body%20control%20policies%20for%20humanoid%20robots%20that%20perform%20the%20corresponding%0Askills.%20We%20demonstrate%20the%20results%20of%20our%20pipeline%20on%20real%20humanoid%20robots%2C%0Ashowing%20robust%2C%20repeatable%20contextual%20control%20such%20as%20staircase%20ascents%20and%0Adescents%2C%20sitting%20and%20standing%20from%20chairs%20and%20benches%2C%20as%20well%20as%20other%0Adynamic%20whole-body%20skills-all%20from%20a%20single%20policy%2C%20conditioned%20on%20the%0Aenvironment%20and%20global%20root%20commands.%20VIDEOMIMIC%20offers%20a%20scalable%20path%20towards%0Ateaching%20humanoids%20to%20operate%20in%20diverse%20real-world%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03729v3&entry.124074799=Read"},
{"title": "DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General\n  Robot Control", "author": "Junjie Wen and Yichen Zhu and Jinming Li and Zhibin Tang and Chaomin Shen and Feifei Feng", "abstract": "  Enabling robots to perform diverse tasks across varied environments is a\ncentral challenge in robot learning. While vision-language-action (VLA) models\nhave shown promise for generalizable robot skills, realizing their full\npotential requires addressing limitations in action representation and\nefficient training. Current VLA models often focus on scaling the\nvision-language model (VLM) component, while the action space representation\nremains a critical bottleneck. This paper introduces DexVLA, a novel framework\ndesigned to enhance the efficiency and generalization capabilities of VLAs for\ncomplex, long-horizon tasks across diverse robot embodiments. DexVLA features a\nnovel diffusion-based action expert, scaled to one billion parameters, designed\nfor cross-embodiment learning. A novel embodiment curriculum learning strategy\nfacilitates efficient training: (1) pre-training the diffusion expert that is\nseparable from the VLA on cross-embodiment data, (2) aligning the VLA model to\nspecific embodiments, and (3) post-training for rapid adaptation to new tasks.\nWe conduct comprehensive experiments across multiple embodiments, including\nsingle-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability\nto challenging tasks without task-specific adaptation, its ability to learn\ndexterous skills on novel embodiments with limited data, and its capacity to\ncomplete complex, long-horizon tasks using only direct language prompting, such\nas laundry folding. In all settings, our method demonstrates superior\nperformance compared to state-of-the-art models like Octo, OpenVLA, and\nDiffusion Policy.\n", "link": "http://arxiv.org/abs/2502.05855v2", "date": "2025-05-13", "relevancy": 2.4221, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6088}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexVLA%3A%20Vision-Language%20Model%20with%20Plug-In%20Diffusion%20Expert%20for%20General%0A%20%20Robot%20Control&body=Title%3A%20DexVLA%3A%20Vision-Language%20Model%20with%20Plug-In%20Diffusion%20Expert%20for%20General%0A%20%20Robot%20Control%0AAuthor%3A%20Junjie%20Wen%20and%20Yichen%20Zhu%20and%20Jinming%20Li%20and%20Zhibin%20Tang%20and%20Chaomin%20Shen%20and%20Feifei%20Feng%0AAbstract%3A%20%20%20Enabling%20robots%20to%20perform%20diverse%20tasks%20across%20varied%20environments%20is%20a%0Acentral%20challenge%20in%20robot%20learning.%20While%20vision-language-action%20%28VLA%29%20models%0Ahave%20shown%20promise%20for%20generalizable%20robot%20skills%2C%20realizing%20their%20full%0Apotential%20requires%20addressing%20limitations%20in%20action%20representation%20and%0Aefficient%20training.%20Current%20VLA%20models%20often%20focus%20on%20scaling%20the%0Avision-language%20model%20%28VLM%29%20component%2C%20while%20the%20action%20space%20representation%0Aremains%20a%20critical%20bottleneck.%20This%20paper%20introduces%20DexVLA%2C%20a%20novel%20framework%0Adesigned%20to%20enhance%20the%20efficiency%20and%20generalization%20capabilities%20of%20VLAs%20for%0Acomplex%2C%20long-horizon%20tasks%20across%20diverse%20robot%20embodiments.%20DexVLA%20features%20a%0Anovel%20diffusion-based%20action%20expert%2C%20scaled%20to%20one%20billion%20parameters%2C%20designed%0Afor%20cross-embodiment%20learning.%20A%20novel%20embodiment%20curriculum%20learning%20strategy%0Afacilitates%20efficient%20training%3A%20%281%29%20pre-training%20the%20diffusion%20expert%20that%20is%0Aseparable%20from%20the%20VLA%20on%20cross-embodiment%20data%2C%20%282%29%20aligning%20the%20VLA%20model%20to%0Aspecific%20embodiments%2C%20and%20%283%29%20post-training%20for%20rapid%20adaptation%20to%20new%20tasks.%0AWe%20conduct%20comprehensive%20experiments%20across%20multiple%20embodiments%2C%20including%0Asingle-arm%2C%20bimanual%2C%20and%20dexterous%20hand%2C%20demonstrating%20DexVLA%27s%20adaptability%0Ato%20challenging%20tasks%20without%20task-specific%20adaptation%2C%20its%20ability%20to%20learn%0Adexterous%20skills%20on%20novel%20embodiments%20with%20limited%20data%2C%20and%20its%20capacity%20to%0Acomplete%20complex%2C%20long-horizon%20tasks%20using%20only%20direct%20language%20prompting%2C%20such%0Aas%20laundry%20folding.%20In%20all%20settings%2C%20our%20method%20demonstrates%20superior%0Aperformance%20compared%20to%20state-of-the-art%20models%20like%20Octo%2C%20OpenVLA%2C%20and%0ADiffusion%20Policy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05855v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexVLA%253A%2520Vision-Language%2520Model%2520with%2520Plug-In%2520Diffusion%2520Expert%2520for%2520General%250A%2520%2520Robot%2520Control%26entry.906535625%3DJunjie%2520Wen%2520and%2520Yichen%2520Zhu%2520and%2520Jinming%2520Li%2520and%2520Zhibin%2520Tang%2520and%2520Chaomin%2520Shen%2520and%2520Feifei%2520Feng%26entry.1292438233%3D%2520%2520Enabling%2520robots%2520to%2520perform%2520diverse%2520tasks%2520across%2520varied%2520environments%2520is%2520a%250Acentral%2520challenge%2520in%2520robot%2520learning.%2520While%2520vision-language-action%2520%2528VLA%2529%2520models%250Ahave%2520shown%2520promise%2520for%2520generalizable%2520robot%2520skills%252C%2520realizing%2520their%2520full%250Apotential%2520requires%2520addressing%2520limitations%2520in%2520action%2520representation%2520and%250Aefficient%2520training.%2520Current%2520VLA%2520models%2520often%2520focus%2520on%2520scaling%2520the%250Avision-language%2520model%2520%2528VLM%2529%2520component%252C%2520while%2520the%2520action%2520space%2520representation%250Aremains%2520a%2520critical%2520bottleneck.%2520This%2520paper%2520introduces%2520DexVLA%252C%2520a%2520novel%2520framework%250Adesigned%2520to%2520enhance%2520the%2520efficiency%2520and%2520generalization%2520capabilities%2520of%2520VLAs%2520for%250Acomplex%252C%2520long-horizon%2520tasks%2520across%2520diverse%2520robot%2520embodiments.%2520DexVLA%2520features%2520a%250Anovel%2520diffusion-based%2520action%2520expert%252C%2520scaled%2520to%2520one%2520billion%2520parameters%252C%2520designed%250Afor%2520cross-embodiment%2520learning.%2520A%2520novel%2520embodiment%2520curriculum%2520learning%2520strategy%250Afacilitates%2520efficient%2520training%253A%2520%25281%2529%2520pre-training%2520the%2520diffusion%2520expert%2520that%2520is%250Aseparable%2520from%2520the%2520VLA%2520on%2520cross-embodiment%2520data%252C%2520%25282%2529%2520aligning%2520the%2520VLA%2520model%2520to%250Aspecific%2520embodiments%252C%2520and%2520%25283%2529%2520post-training%2520for%2520rapid%2520adaptation%2520to%2520new%2520tasks.%250AWe%2520conduct%2520comprehensive%2520experiments%2520across%2520multiple%2520embodiments%252C%2520including%250Asingle-arm%252C%2520bimanual%252C%2520and%2520dexterous%2520hand%252C%2520demonstrating%2520DexVLA%2527s%2520adaptability%250Ato%2520challenging%2520tasks%2520without%2520task-specific%2520adaptation%252C%2520its%2520ability%2520to%2520learn%250Adexterous%2520skills%2520on%2520novel%2520embodiments%2520with%2520limited%2520data%252C%2520and%2520its%2520capacity%2520to%250Acomplete%2520complex%252C%2520long-horizon%2520tasks%2520using%2520only%2520direct%2520language%2520prompting%252C%2520such%250Aas%2520laundry%2520folding.%2520In%2520all%2520settings%252C%2520our%2520method%2520demonstrates%2520superior%250Aperformance%2520compared%2520to%2520state-of-the-art%2520models%2520like%2520Octo%252C%2520OpenVLA%252C%2520and%250ADiffusion%2520Policy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05855v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexVLA%3A%20Vision-Language%20Model%20with%20Plug-In%20Diffusion%20Expert%20for%20General%0A%20%20Robot%20Control&entry.906535625=Junjie%20Wen%20and%20Yichen%20Zhu%20and%20Jinming%20Li%20and%20Zhibin%20Tang%20and%20Chaomin%20Shen%20and%20Feifei%20Feng&entry.1292438233=%20%20Enabling%20robots%20to%20perform%20diverse%20tasks%20across%20varied%20environments%20is%20a%0Acentral%20challenge%20in%20robot%20learning.%20While%20vision-language-action%20%28VLA%29%20models%0Ahave%20shown%20promise%20for%20generalizable%20robot%20skills%2C%20realizing%20their%20full%0Apotential%20requires%20addressing%20limitations%20in%20action%20representation%20and%0Aefficient%20training.%20Current%20VLA%20models%20often%20focus%20on%20scaling%20the%0Avision-language%20model%20%28VLM%29%20component%2C%20while%20the%20action%20space%20representation%0Aremains%20a%20critical%20bottleneck.%20This%20paper%20introduces%20DexVLA%2C%20a%20novel%20framework%0Adesigned%20to%20enhance%20the%20efficiency%20and%20generalization%20capabilities%20of%20VLAs%20for%0Acomplex%2C%20long-horizon%20tasks%20across%20diverse%20robot%20embodiments.%20DexVLA%20features%20a%0Anovel%20diffusion-based%20action%20expert%2C%20scaled%20to%20one%20billion%20parameters%2C%20designed%0Afor%20cross-embodiment%20learning.%20A%20novel%20embodiment%20curriculum%20learning%20strategy%0Afacilitates%20efficient%20training%3A%20%281%29%20pre-training%20the%20diffusion%20expert%20that%20is%0Aseparable%20from%20the%20VLA%20on%20cross-embodiment%20data%2C%20%282%29%20aligning%20the%20VLA%20model%20to%0Aspecific%20embodiments%2C%20and%20%283%29%20post-training%20for%20rapid%20adaptation%20to%20new%20tasks.%0AWe%20conduct%20comprehensive%20experiments%20across%20multiple%20embodiments%2C%20including%0Asingle-arm%2C%20bimanual%2C%20and%20dexterous%20hand%2C%20demonstrating%20DexVLA%27s%20adaptability%0Ato%20challenging%20tasks%20without%20task-specific%20adaptation%2C%20its%20ability%20to%20learn%0Adexterous%20skills%20on%20novel%20embodiments%20with%20limited%20data%2C%20and%20its%20capacity%20to%0Acomplete%20complex%2C%20long-horizon%20tasks%20using%20only%20direct%20language%20prompting%2C%20such%0Aas%20laundry%20folding.%20In%20all%20settings%2C%20our%20method%20demonstrates%20superior%0Aperformance%20compared%20to%20state-of-the-art%20models%20like%20Octo%2C%20OpenVLA%2C%20and%0ADiffusion%20Policy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05855v2&entry.124074799=Read"},
{"title": "PrePrompt: Predictive prompting for class incremental learning", "author": "Libo Huang and Zhulin An and Chuanguang Yang and Boyu Diao and Fei Wang and Yan Zeng and Zhifeng Hao and Yongjun Xu", "abstract": "  Class Incremental Learning (CIL) based on pre-trained models offers a\npromising direction for open-world continual learning. Existing methods\ntypically rely on correlation-based strategies, where an image's classification\nfeature is used as a query to retrieve the most related key prompts and select\nthe corresponding value prompts for training. However, these approaches face an\ninherent limitation: fitting the entire feature space of all tasks with only a\nfew trainable prompts is fundamentally challenging. We propose Predictive\nPrompting (PrePrompt), a novel CIL framework that circumvents correlation-based\nlimitations by leveraging pre-trained models' natural classification ability to\npredict task-specific prompts. Specifically, PrePrompt decomposes CIL into a\ntwo-stage prediction framework: task-specific prompt prediction followed by\nlabel prediction. While theoretically appealing, this framework risks bias\ntoward recent classes due to missing historical data for older classifier\ncalibration. PrePrompt then mitigates this by incorporating feature\ntranslation, dynamically balancing stability and plasticity. Experiments across\nmultiple benchmarks demonstrate PrePrompt's superiority over state-of-the-art\nprompt-based CIL methods. The code will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2505.08586v1", "date": "2025-05-13", "relevancy": 2.4149, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4861}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrePrompt%3A%20Predictive%20prompting%20for%20class%20incremental%20learning&body=Title%3A%20PrePrompt%3A%20Predictive%20prompting%20for%20class%20incremental%20learning%0AAuthor%3A%20Libo%20Huang%20and%20Zhulin%20An%20and%20Chuanguang%20Yang%20and%20Boyu%20Diao%20and%20Fei%20Wang%20and%20Yan%20Zeng%20and%20Zhifeng%20Hao%20and%20Yongjun%20Xu%0AAbstract%3A%20%20%20Class%20Incremental%20Learning%20%28CIL%29%20based%20on%20pre-trained%20models%20offers%20a%0Apromising%20direction%20for%20open-world%20continual%20learning.%20Existing%20methods%0Atypically%20rely%20on%20correlation-based%20strategies%2C%20where%20an%20image%27s%20classification%0Afeature%20is%20used%20as%20a%20query%20to%20retrieve%20the%20most%20related%20key%20prompts%20and%20select%0Athe%20corresponding%20value%20prompts%20for%20training.%20However%2C%20these%20approaches%20face%20an%0Ainherent%20limitation%3A%20fitting%20the%20entire%20feature%20space%20of%20all%20tasks%20with%20only%20a%0Afew%20trainable%20prompts%20is%20fundamentally%20challenging.%20We%20propose%20Predictive%0APrompting%20%28PrePrompt%29%2C%20a%20novel%20CIL%20framework%20that%20circumvents%20correlation-based%0Alimitations%20by%20leveraging%20pre-trained%20models%27%20natural%20classification%20ability%20to%0Apredict%20task-specific%20prompts.%20Specifically%2C%20PrePrompt%20decomposes%20CIL%20into%20a%0Atwo-stage%20prediction%20framework%3A%20task-specific%20prompt%20prediction%20followed%20by%0Alabel%20prediction.%20While%20theoretically%20appealing%2C%20this%20framework%20risks%20bias%0Atoward%20recent%20classes%20due%20to%20missing%20historical%20data%20for%20older%20classifier%0Acalibration.%20PrePrompt%20then%20mitigates%20this%20by%20incorporating%20feature%0Atranslation%2C%20dynamically%20balancing%20stability%20and%20plasticity.%20Experiments%20across%0Amultiple%20benchmarks%20demonstrate%20PrePrompt%27s%20superiority%20over%20state-of-the-art%0Aprompt-based%20CIL%20methods.%20The%20code%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrePrompt%253A%2520Predictive%2520prompting%2520for%2520class%2520incremental%2520learning%26entry.906535625%3DLibo%2520Huang%2520and%2520Zhulin%2520An%2520and%2520Chuanguang%2520Yang%2520and%2520Boyu%2520Diao%2520and%2520Fei%2520Wang%2520and%2520Yan%2520Zeng%2520and%2520Zhifeng%2520Hao%2520and%2520Yongjun%2520Xu%26entry.1292438233%3D%2520%2520Class%2520Incremental%2520Learning%2520%2528CIL%2529%2520based%2520on%2520pre-trained%2520models%2520offers%2520a%250Apromising%2520direction%2520for%2520open-world%2520continual%2520learning.%2520Existing%2520methods%250Atypically%2520rely%2520on%2520correlation-based%2520strategies%252C%2520where%2520an%2520image%2527s%2520classification%250Afeature%2520is%2520used%2520as%2520a%2520query%2520to%2520retrieve%2520the%2520most%2520related%2520key%2520prompts%2520and%2520select%250Athe%2520corresponding%2520value%2520prompts%2520for%2520training.%2520However%252C%2520these%2520approaches%2520face%2520an%250Ainherent%2520limitation%253A%2520fitting%2520the%2520entire%2520feature%2520space%2520of%2520all%2520tasks%2520with%2520only%2520a%250Afew%2520trainable%2520prompts%2520is%2520fundamentally%2520challenging.%2520We%2520propose%2520Predictive%250APrompting%2520%2528PrePrompt%2529%252C%2520a%2520novel%2520CIL%2520framework%2520that%2520circumvents%2520correlation-based%250Alimitations%2520by%2520leveraging%2520pre-trained%2520models%2527%2520natural%2520classification%2520ability%2520to%250Apredict%2520task-specific%2520prompts.%2520Specifically%252C%2520PrePrompt%2520decomposes%2520CIL%2520into%2520a%250Atwo-stage%2520prediction%2520framework%253A%2520task-specific%2520prompt%2520prediction%2520followed%2520by%250Alabel%2520prediction.%2520While%2520theoretically%2520appealing%252C%2520this%2520framework%2520risks%2520bias%250Atoward%2520recent%2520classes%2520due%2520to%2520missing%2520historical%2520data%2520for%2520older%2520classifier%250Acalibration.%2520PrePrompt%2520then%2520mitigates%2520this%2520by%2520incorporating%2520feature%250Atranslation%252C%2520dynamically%2520balancing%2520stability%2520and%2520plasticity.%2520Experiments%2520across%250Amultiple%2520benchmarks%2520demonstrate%2520PrePrompt%2527s%2520superiority%2520over%2520state-of-the-art%250Aprompt-based%2520CIL%2520methods.%2520The%2520code%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrePrompt%3A%20Predictive%20prompting%20for%20class%20incremental%20learning&entry.906535625=Libo%20Huang%20and%20Zhulin%20An%20and%20Chuanguang%20Yang%20and%20Boyu%20Diao%20and%20Fei%20Wang%20and%20Yan%20Zeng%20and%20Zhifeng%20Hao%20and%20Yongjun%20Xu&entry.1292438233=%20%20Class%20Incremental%20Learning%20%28CIL%29%20based%20on%20pre-trained%20models%20offers%20a%0Apromising%20direction%20for%20open-world%20continual%20learning.%20Existing%20methods%0Atypically%20rely%20on%20correlation-based%20strategies%2C%20where%20an%20image%27s%20classification%0Afeature%20is%20used%20as%20a%20query%20to%20retrieve%20the%20most%20related%20key%20prompts%20and%20select%0Athe%20corresponding%20value%20prompts%20for%20training.%20However%2C%20these%20approaches%20face%20an%0Ainherent%20limitation%3A%20fitting%20the%20entire%20feature%20space%20of%20all%20tasks%20with%20only%20a%0Afew%20trainable%20prompts%20is%20fundamentally%20challenging.%20We%20propose%20Predictive%0APrompting%20%28PrePrompt%29%2C%20a%20novel%20CIL%20framework%20that%20circumvents%20correlation-based%0Alimitations%20by%20leveraging%20pre-trained%20models%27%20natural%20classification%20ability%20to%0Apredict%20task-specific%20prompts.%20Specifically%2C%20PrePrompt%20decomposes%20CIL%20into%20a%0Atwo-stage%20prediction%20framework%3A%20task-specific%20prompt%20prediction%20followed%20by%0Alabel%20prediction.%20While%20theoretically%20appealing%2C%20this%20framework%20risks%20bias%0Atoward%20recent%20classes%20due%20to%20missing%20historical%20data%20for%20older%20classifier%0Acalibration.%20PrePrompt%20then%20mitigates%20this%20by%20incorporating%20feature%0Atranslation%2C%20dynamically%20balancing%20stability%20and%20plasticity.%20Experiments%20across%0Amultiple%20benchmarks%20demonstrate%20PrePrompt%27s%20superiority%20over%20state-of-the-art%0Aprompt-based%20CIL%20methods.%20The%20code%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08586v1&entry.124074799=Read"},
{"title": "DiTPainter: Efficient Video Inpainting with Diffusion Transformers", "author": "Xian Wu and Chang Liu", "abstract": "  Many existing video inpainting algorithms utilize optical flows to construct\nthe corresponding maps and then propagate pixels from adjacent frames to\nmissing areas by mapping. Despite the effectiveness of the propagation\nmechanism, they might encounter blurry and inconsistencies when dealing with\ninaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)\nhas emerged as a revolutionary technique for video generation tasks. However,\npretrained DiT models for video generation all contain a large amount of\nparameters, which makes it very time consuming to apply to video inpainting\ntasks. In this paper, we present DiTPainter, an end-to-end video inpainting\nmodel based on Diffusion Transformer (DiT). DiTPainter uses an efficient\ntransformer network designed for video inpainting, which is trained from\nscratch instead of initializing from any large pretrained models. DiTPainter\ncan address videos with arbitrary lengths and can be applied to video\ndecaptioning and video completion tasks with an acceptable time cost.\nExperiments show that DiTPainter outperforms existing video inpainting\nalgorithms with higher quality and better spatial-temporal consistency.\n", "link": "http://arxiv.org/abs/2504.15661v2", "date": "2025-05-13", "relevancy": 2.414, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6162}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6026}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiTPainter%3A%20Efficient%20Video%20Inpainting%20with%20Diffusion%20Transformers&body=Title%3A%20DiTPainter%3A%20Efficient%20Video%20Inpainting%20with%20Diffusion%20Transformers%0AAuthor%3A%20Xian%20Wu%20and%20Chang%20Liu%0AAbstract%3A%20%20%20Many%20existing%20video%20inpainting%20algorithms%20utilize%20optical%20flows%20to%20construct%0Athe%20corresponding%20maps%20and%20then%20propagate%20pixels%20from%20adjacent%20frames%20to%0Amissing%20areas%20by%20mapping.%20Despite%20the%20effectiveness%20of%20the%20propagation%0Amechanism%2C%20they%20might%20encounter%20blurry%20and%20inconsistencies%20when%20dealing%20with%0Ainaccurate%20optical%20flows%20or%20large%20masks.%20Recently%2C%20Diffusion%20Transformer%20%28DiT%29%0Ahas%20emerged%20as%20a%20revolutionary%20technique%20for%20video%20generation%20tasks.%20However%2C%0Apretrained%20DiT%20models%20for%20video%20generation%20all%20contain%20a%20large%20amount%20of%0Aparameters%2C%20which%20makes%20it%20very%20time%20consuming%20to%20apply%20to%20video%20inpainting%0Atasks.%20In%20this%20paper%2C%20we%20present%20DiTPainter%2C%20an%20end-to-end%20video%20inpainting%0Amodel%20based%20on%20Diffusion%20Transformer%20%28DiT%29.%20DiTPainter%20uses%20an%20efficient%0Atransformer%20network%20designed%20for%20video%20inpainting%2C%20which%20is%20trained%20from%0Ascratch%20instead%20of%20initializing%20from%20any%20large%20pretrained%20models.%20DiTPainter%0Acan%20address%20videos%20with%20arbitrary%20lengths%20and%20can%20be%20applied%20to%20video%0Adecaptioning%20and%20video%20completion%20tasks%20with%20an%20acceptable%20time%20cost.%0AExperiments%20show%20that%20DiTPainter%20outperforms%20existing%20video%20inpainting%0Aalgorithms%20with%20higher%20quality%20and%20better%20spatial-temporal%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15661v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiTPainter%253A%2520Efficient%2520Video%2520Inpainting%2520with%2520Diffusion%2520Transformers%26entry.906535625%3DXian%2520Wu%2520and%2520Chang%2520Liu%26entry.1292438233%3D%2520%2520Many%2520existing%2520video%2520inpainting%2520algorithms%2520utilize%2520optical%2520flows%2520to%2520construct%250Athe%2520corresponding%2520maps%2520and%2520then%2520propagate%2520pixels%2520from%2520adjacent%2520frames%2520to%250Amissing%2520areas%2520by%2520mapping.%2520Despite%2520the%2520effectiveness%2520of%2520the%2520propagation%250Amechanism%252C%2520they%2520might%2520encounter%2520blurry%2520and%2520inconsistencies%2520when%2520dealing%2520with%250Ainaccurate%2520optical%2520flows%2520or%2520large%2520masks.%2520Recently%252C%2520Diffusion%2520Transformer%2520%2528DiT%2529%250Ahas%2520emerged%2520as%2520a%2520revolutionary%2520technique%2520for%2520video%2520generation%2520tasks.%2520However%252C%250Apretrained%2520DiT%2520models%2520for%2520video%2520generation%2520all%2520contain%2520a%2520large%2520amount%2520of%250Aparameters%252C%2520which%2520makes%2520it%2520very%2520time%2520consuming%2520to%2520apply%2520to%2520video%2520inpainting%250Atasks.%2520In%2520this%2520paper%252C%2520we%2520present%2520DiTPainter%252C%2520an%2520end-to-end%2520video%2520inpainting%250Amodel%2520based%2520on%2520Diffusion%2520Transformer%2520%2528DiT%2529.%2520DiTPainter%2520uses%2520an%2520efficient%250Atransformer%2520network%2520designed%2520for%2520video%2520inpainting%252C%2520which%2520is%2520trained%2520from%250Ascratch%2520instead%2520of%2520initializing%2520from%2520any%2520large%2520pretrained%2520models.%2520DiTPainter%250Acan%2520address%2520videos%2520with%2520arbitrary%2520lengths%2520and%2520can%2520be%2520applied%2520to%2520video%250Adecaptioning%2520and%2520video%2520completion%2520tasks%2520with%2520an%2520acceptable%2520time%2520cost.%250AExperiments%2520show%2520that%2520DiTPainter%2520outperforms%2520existing%2520video%2520inpainting%250Aalgorithms%2520with%2520higher%2520quality%2520and%2520better%2520spatial-temporal%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15661v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiTPainter%3A%20Efficient%20Video%20Inpainting%20with%20Diffusion%20Transformers&entry.906535625=Xian%20Wu%20and%20Chang%20Liu&entry.1292438233=%20%20Many%20existing%20video%20inpainting%20algorithms%20utilize%20optical%20flows%20to%20construct%0Athe%20corresponding%20maps%20and%20then%20propagate%20pixels%20from%20adjacent%20frames%20to%0Amissing%20areas%20by%20mapping.%20Despite%20the%20effectiveness%20of%20the%20propagation%0Amechanism%2C%20they%20might%20encounter%20blurry%20and%20inconsistencies%20when%20dealing%20with%0Ainaccurate%20optical%20flows%20or%20large%20masks.%20Recently%2C%20Diffusion%20Transformer%20%28DiT%29%0Ahas%20emerged%20as%20a%20revolutionary%20technique%20for%20video%20generation%20tasks.%20However%2C%0Apretrained%20DiT%20models%20for%20video%20generation%20all%20contain%20a%20large%20amount%20of%0Aparameters%2C%20which%20makes%20it%20very%20time%20consuming%20to%20apply%20to%20video%20inpainting%0Atasks.%20In%20this%20paper%2C%20we%20present%20DiTPainter%2C%20an%20end-to-end%20video%20inpainting%0Amodel%20based%20on%20Diffusion%20Transformer%20%28DiT%29.%20DiTPainter%20uses%20an%20efficient%0Atransformer%20network%20designed%20for%20video%20inpainting%2C%20which%20is%20trained%20from%0Ascratch%20instead%20of%20initializing%20from%20any%20large%20pretrained%20models.%20DiTPainter%0Acan%20address%20videos%20with%20arbitrary%20lengths%20and%20can%20be%20applied%20to%20video%0Adecaptioning%20and%20video%20completion%20tasks%20with%20an%20acceptable%20time%20cost.%0AExperiments%20show%20that%20DiTPainter%20outperforms%20existing%20video%20inpainting%0Aalgorithms%20with%20higher%20quality%20and%20better%20spatial-temporal%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15661v2&entry.124074799=Read"},
{"title": "LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using\n  Large Language Models", "author": "Takumi Shibata and Yuichi Miyamura", "abstract": "  Recent advances in large language models (LLMs) have enabled zero-shot\nautomated essay scoring (AES), providing a promising way to reduce the cost and\neffort of essay scoring in comparison with manual grading. However, most\nexisting zero-shot approaches rely on LLMs to directly generate absolute\nscores, which often diverge from human evaluations owing to model biases and\ninconsistent scoring. To address these limitations, we propose LLM-based\nComparative Essay Scoring (LCES), a method that formulates AES as a pairwise\ncomparison task. Specifically, we instruct LLMs to judge which of two essays is\nbetter, collect many such comparisons, and convert them into continuous scores.\nConsidering that the number of possible comparisons grows quadratically with\nthe number of essays, we improve scalability by employing RankNet to\nefficiently transform LLM preferences into scalar scores. Experiments using AES\nbenchmark datasets show that LCES outperforms conventional zero-shot methods in\naccuracy while maintaining computational efficiency. Moreover, LCES is robust\nacross different LLM backbones, highlighting its applicability to real-world\nzero-shot AES.\n", "link": "http://arxiv.org/abs/2505.08498v1", "date": "2025-05-13", "relevancy": 2.4054, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4908}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LCES%3A%20Zero-shot%20Automated%20Essay%20Scoring%20via%20Pairwise%20Comparisons%20Using%0A%20%20Large%20Language%20Models&body=Title%3A%20LCES%3A%20Zero-shot%20Automated%20Essay%20Scoring%20via%20Pairwise%20Comparisons%20Using%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Takumi%20Shibata%20and%20Yuichi%20Miyamura%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20enabled%20zero-shot%0Aautomated%20essay%20scoring%20%28AES%29%2C%20providing%20a%20promising%20way%20to%20reduce%20the%20cost%20and%0Aeffort%20of%20essay%20scoring%20in%20comparison%20with%20manual%20grading.%20However%2C%20most%0Aexisting%20zero-shot%20approaches%20rely%20on%20LLMs%20to%20directly%20generate%20absolute%0Ascores%2C%20which%20often%20diverge%20from%20human%20evaluations%20owing%20to%20model%20biases%20and%0Ainconsistent%20scoring.%20To%20address%20these%20limitations%2C%20we%20propose%20LLM-based%0AComparative%20Essay%20Scoring%20%28LCES%29%2C%20a%20method%20that%20formulates%20AES%20as%20a%20pairwise%0Acomparison%20task.%20Specifically%2C%20we%20instruct%20LLMs%20to%20judge%20which%20of%20two%20essays%20is%0Abetter%2C%20collect%20many%20such%20comparisons%2C%20and%20convert%20them%20into%20continuous%20scores.%0AConsidering%20that%20the%20number%20of%20possible%20comparisons%20grows%20quadratically%20with%0Athe%20number%20of%20essays%2C%20we%20improve%20scalability%20by%20employing%20RankNet%20to%0Aefficiently%20transform%20LLM%20preferences%20into%20scalar%20scores.%20Experiments%20using%20AES%0Abenchmark%20datasets%20show%20that%20LCES%20outperforms%20conventional%20zero-shot%20methods%20in%0Aaccuracy%20while%20maintaining%20computational%20efficiency.%20Moreover%2C%20LCES%20is%20robust%0Aacross%20different%20LLM%20backbones%2C%20highlighting%20its%20applicability%20to%20real-world%0Azero-shot%20AES.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLCES%253A%2520Zero-shot%2520Automated%2520Essay%2520Scoring%2520via%2520Pairwise%2520Comparisons%2520Using%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DTakumi%2520Shibata%2520and%2520Yuichi%2520Miyamura%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520enabled%2520zero-shot%250Aautomated%2520essay%2520scoring%2520%2528AES%2529%252C%2520providing%2520a%2520promising%2520way%2520to%2520reduce%2520the%2520cost%2520and%250Aeffort%2520of%2520essay%2520scoring%2520in%2520comparison%2520with%2520manual%2520grading.%2520However%252C%2520most%250Aexisting%2520zero-shot%2520approaches%2520rely%2520on%2520LLMs%2520to%2520directly%2520generate%2520absolute%250Ascores%252C%2520which%2520often%2520diverge%2520from%2520human%2520evaluations%2520owing%2520to%2520model%2520biases%2520and%250Ainconsistent%2520scoring.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520LLM-based%250AComparative%2520Essay%2520Scoring%2520%2528LCES%2529%252C%2520a%2520method%2520that%2520formulates%2520AES%2520as%2520a%2520pairwise%250Acomparison%2520task.%2520Specifically%252C%2520we%2520instruct%2520LLMs%2520to%2520judge%2520which%2520of%2520two%2520essays%2520is%250Abetter%252C%2520collect%2520many%2520such%2520comparisons%252C%2520and%2520convert%2520them%2520into%2520continuous%2520scores.%250AConsidering%2520that%2520the%2520number%2520of%2520possible%2520comparisons%2520grows%2520quadratically%2520with%250Athe%2520number%2520of%2520essays%252C%2520we%2520improve%2520scalability%2520by%2520employing%2520RankNet%2520to%250Aefficiently%2520transform%2520LLM%2520preferences%2520into%2520scalar%2520scores.%2520Experiments%2520using%2520AES%250Abenchmark%2520datasets%2520show%2520that%2520LCES%2520outperforms%2520conventional%2520zero-shot%2520methods%2520in%250Aaccuracy%2520while%2520maintaining%2520computational%2520efficiency.%2520Moreover%252C%2520LCES%2520is%2520robust%250Aacross%2520different%2520LLM%2520backbones%252C%2520highlighting%2520its%2520applicability%2520to%2520real-world%250Azero-shot%2520AES.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LCES%3A%20Zero-shot%20Automated%20Essay%20Scoring%20via%20Pairwise%20Comparisons%20Using%0A%20%20Large%20Language%20Models&entry.906535625=Takumi%20Shibata%20and%20Yuichi%20Miyamura&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20enabled%20zero-shot%0Aautomated%20essay%20scoring%20%28AES%29%2C%20providing%20a%20promising%20way%20to%20reduce%20the%20cost%20and%0Aeffort%20of%20essay%20scoring%20in%20comparison%20with%20manual%20grading.%20However%2C%20most%0Aexisting%20zero-shot%20approaches%20rely%20on%20LLMs%20to%20directly%20generate%20absolute%0Ascores%2C%20which%20often%20diverge%20from%20human%20evaluations%20owing%20to%20model%20biases%20and%0Ainconsistent%20scoring.%20To%20address%20these%20limitations%2C%20we%20propose%20LLM-based%0AComparative%20Essay%20Scoring%20%28LCES%29%2C%20a%20method%20that%20formulates%20AES%20as%20a%20pairwise%0Acomparison%20task.%20Specifically%2C%20we%20instruct%20LLMs%20to%20judge%20which%20of%20two%20essays%20is%0Abetter%2C%20collect%20many%20such%20comparisons%2C%20and%20convert%20them%20into%20continuous%20scores.%0AConsidering%20that%20the%20number%20of%20possible%20comparisons%20grows%20quadratically%20with%0Athe%20number%20of%20essays%2C%20we%20improve%20scalability%20by%20employing%20RankNet%20to%0Aefficiently%20transform%20LLM%20preferences%20into%20scalar%20scores.%20Experiments%20using%20AES%0Abenchmark%20datasets%20show%20that%20LCES%20outperforms%20conventional%20zero-shot%20methods%20in%0Aaccuracy%20while%20maintaining%20computational%20efficiency.%20Moreover%2C%20LCES%20is%20robust%0Aacross%20different%20LLM%20backbones%2C%20highlighting%20its%20applicability%20to%20real-world%0Azero-shot%20AES.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08498v1&entry.124074799=Read"},
{"title": "Claycode: Stylable and Deformable 2D Scannable Codes", "author": "Marco Maida and Alberto Crescini and Marco Perronet and Elena Camuffo", "abstract": "  This paper introduces Claycode, a novel 2D scannable code designed for\nextensive stylization and deformation. Unlike traditional matrix-based codes\n(e.g., QR codes), Claycodes encode their message in a tree structure. During\nthe encoding process, bits are mapped into a topology tree, which is then\ndepicted as a nesting of color regions drawn within the boundaries of a target\npolygon shape. When decoding, Claycodes are extracted and interpreted in\nreal-time from a camera stream. We detail the end-to-end pipeline and show that\nClaycodes allow for extensive stylization without compromising their\nfunctionality. We then empirically demonstrate Claycode's high tolerance to\nheavy deformations, outperforming traditional 2D scannable codes in scenarios\nwhere they typically fail.\n", "link": "http://arxiv.org/abs/2505.08666v1", "date": "2025-05-13", "relevancy": 2.3921, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4844}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4768}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Claycode%3A%20Stylable%20and%20Deformable%202D%20Scannable%20Codes&body=Title%3A%20Claycode%3A%20Stylable%20and%20Deformable%202D%20Scannable%20Codes%0AAuthor%3A%20Marco%20Maida%20and%20Alberto%20Crescini%20and%20Marco%20Perronet%20and%20Elena%20Camuffo%0AAbstract%3A%20%20%20This%20paper%20introduces%20Claycode%2C%20a%20novel%202D%20scannable%20code%20designed%20for%0Aextensive%20stylization%20and%20deformation.%20Unlike%20traditional%20matrix-based%20codes%0A%28e.g.%2C%20QR%20codes%29%2C%20Claycodes%20encode%20their%20message%20in%20a%20tree%20structure.%20During%0Athe%20encoding%20process%2C%20bits%20are%20mapped%20into%20a%20topology%20tree%2C%20which%20is%20then%0Adepicted%20as%20a%20nesting%20of%20color%20regions%20drawn%20within%20the%20boundaries%20of%20a%20target%0Apolygon%20shape.%20When%20decoding%2C%20Claycodes%20are%20extracted%20and%20interpreted%20in%0Areal-time%20from%20a%20camera%20stream.%20We%20detail%20the%20end-to-end%20pipeline%20and%20show%20that%0AClaycodes%20allow%20for%20extensive%20stylization%20without%20compromising%20their%0Afunctionality.%20We%20then%20empirically%20demonstrate%20Claycode%27s%20high%20tolerance%20to%0Aheavy%20deformations%2C%20outperforming%20traditional%202D%20scannable%20codes%20in%20scenarios%0Awhere%20they%20typically%20fail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClaycode%253A%2520Stylable%2520and%2520Deformable%25202D%2520Scannable%2520Codes%26entry.906535625%3DMarco%2520Maida%2520and%2520Alberto%2520Crescini%2520and%2520Marco%2520Perronet%2520and%2520Elena%2520Camuffo%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Claycode%252C%2520a%2520novel%25202D%2520scannable%2520code%2520designed%2520for%250Aextensive%2520stylization%2520and%2520deformation.%2520Unlike%2520traditional%2520matrix-based%2520codes%250A%2528e.g.%252C%2520QR%2520codes%2529%252C%2520Claycodes%2520encode%2520their%2520message%2520in%2520a%2520tree%2520structure.%2520During%250Athe%2520encoding%2520process%252C%2520bits%2520are%2520mapped%2520into%2520a%2520topology%2520tree%252C%2520which%2520is%2520then%250Adepicted%2520as%2520a%2520nesting%2520of%2520color%2520regions%2520drawn%2520within%2520the%2520boundaries%2520of%2520a%2520target%250Apolygon%2520shape.%2520When%2520decoding%252C%2520Claycodes%2520are%2520extracted%2520and%2520interpreted%2520in%250Areal-time%2520from%2520a%2520camera%2520stream.%2520We%2520detail%2520the%2520end-to-end%2520pipeline%2520and%2520show%2520that%250AClaycodes%2520allow%2520for%2520extensive%2520stylization%2520without%2520compromising%2520their%250Afunctionality.%2520We%2520then%2520empirically%2520demonstrate%2520Claycode%2527s%2520high%2520tolerance%2520to%250Aheavy%2520deformations%252C%2520outperforming%2520traditional%25202D%2520scannable%2520codes%2520in%2520scenarios%250Awhere%2520they%2520typically%2520fail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Claycode%3A%20Stylable%20and%20Deformable%202D%20Scannable%20Codes&entry.906535625=Marco%20Maida%20and%20Alberto%20Crescini%20and%20Marco%20Perronet%20and%20Elena%20Camuffo&entry.1292438233=%20%20This%20paper%20introduces%20Claycode%2C%20a%20novel%202D%20scannable%20code%20designed%20for%0Aextensive%20stylization%20and%20deformation.%20Unlike%20traditional%20matrix-based%20codes%0A%28e.g.%2C%20QR%20codes%29%2C%20Claycodes%20encode%20their%20message%20in%20a%20tree%20structure.%20During%0Athe%20encoding%20process%2C%20bits%20are%20mapped%20into%20a%20topology%20tree%2C%20which%20is%20then%0Adepicted%20as%20a%20nesting%20of%20color%20regions%20drawn%20within%20the%20boundaries%20of%20a%20target%0Apolygon%20shape.%20When%20decoding%2C%20Claycodes%20are%20extracted%20and%20interpreted%20in%0Areal-time%20from%20a%20camera%20stream.%20We%20detail%20the%20end-to-end%20pipeline%20and%20show%20that%0AClaycodes%20allow%20for%20extensive%20stylization%20without%20compromising%20their%0Afunctionality.%20We%20then%20empirically%20demonstrate%20Claycode%27s%20high%20tolerance%20to%0Aheavy%20deformations%2C%20outperforming%20traditional%202D%20scannable%20codes%20in%20scenarios%0Awhere%20they%20typically%20fail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08666v1&entry.124074799=Read"},
{"title": "Modular Federated Learning: A Meta-Framework Perspective", "author": "Frederico Vicente and Cl\u00e1udia Soares and Du\u0161an Jakoveti\u0107", "abstract": "  Federated Learning (FL) enables distributed machine learning training while\npreserving privacy, representing a paradigm shift for data-sensitive and\ndecentralized environments. Despite its rapid advancements, FL remains a\ncomplex and multifaceted field, requiring a structured understanding of its\nmethodologies, challenges, and applications. In this survey, we introduce a\nmeta-framework perspective, conceptualising FL as a composition of modular\ncomponents that systematically address core aspects such as communication,\noptimisation, security, and privacy. We provide a historical contextualisation\nof FL, tracing its evolution from distributed optimisation to modern\ndistributed learning paradigms. Additionally, we propose a novel taxonomy\ndistinguishing Aggregation from Alignment, introducing the concept of alignment\nas a fundamental operator alongside aggregation. To bridge theory with\npractice, we explore available FL frameworks in Python, facilitating real-world\nimplementation. Finally, we systematise key challenges across FL sub-fields,\nproviding insights into open research questions throughout the meta-framework\nmodules. By structuring FL within a meta-framework of modular components and\nemphasising the dual role of Aggregation and Alignment, this survey provides a\nholistic and adaptable foundation for understanding and advancing FL research\nand deployment.\n", "link": "http://arxiv.org/abs/2505.08646v1", "date": "2025-05-13", "relevancy": 2.3889, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Federated%20Learning%3A%20A%20Meta-Framework%20Perspective&body=Title%3A%20Modular%20Federated%20Learning%3A%20A%20Meta-Framework%20Perspective%0AAuthor%3A%20Frederico%20Vicente%20and%20Cl%C3%A1udia%20Soares%20and%20Du%C5%A1an%20Jakoveti%C4%87%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20distributed%20machine%20learning%20training%20while%0Apreserving%20privacy%2C%20representing%20a%20paradigm%20shift%20for%20data-sensitive%20and%0Adecentralized%20environments.%20Despite%20its%20rapid%20advancements%2C%20FL%20remains%20a%0Acomplex%20and%20multifaceted%20field%2C%20requiring%20a%20structured%20understanding%20of%20its%0Amethodologies%2C%20challenges%2C%20and%20applications.%20In%20this%20survey%2C%20we%20introduce%20a%0Ameta-framework%20perspective%2C%20conceptualising%20FL%20as%20a%20composition%20of%20modular%0Acomponents%20that%20systematically%20address%20core%20aspects%20such%20as%20communication%2C%0Aoptimisation%2C%20security%2C%20and%20privacy.%20We%20provide%20a%20historical%20contextualisation%0Aof%20FL%2C%20tracing%20its%20evolution%20from%20distributed%20optimisation%20to%20modern%0Adistributed%20learning%20paradigms.%20Additionally%2C%20we%20propose%20a%20novel%20taxonomy%0Adistinguishing%20Aggregation%20from%20Alignment%2C%20introducing%20the%20concept%20of%20alignment%0Aas%20a%20fundamental%20operator%20alongside%20aggregation.%20To%20bridge%20theory%20with%0Apractice%2C%20we%20explore%20available%20FL%20frameworks%20in%20Python%2C%20facilitating%20real-world%0Aimplementation.%20Finally%2C%20we%20systematise%20key%20challenges%20across%20FL%20sub-fields%2C%0Aproviding%20insights%20into%20open%20research%20questions%20throughout%20the%20meta-framework%0Amodules.%20By%20structuring%20FL%20within%20a%20meta-framework%20of%20modular%20components%20and%0Aemphasising%20the%20dual%20role%20of%20Aggregation%20and%20Alignment%2C%20this%20survey%20provides%20a%0Aholistic%20and%20adaptable%20foundation%20for%20understanding%20and%20advancing%20FL%20research%0Aand%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Federated%2520Learning%253A%2520A%2520Meta-Framework%2520Perspective%26entry.906535625%3DFrederico%2520Vicente%2520and%2520Cl%25C3%25A1udia%2520Soares%2520and%2520Du%25C5%25A1an%2520Jakoveti%25C4%2587%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520distributed%2520machine%2520learning%2520training%2520while%250Apreserving%2520privacy%252C%2520representing%2520a%2520paradigm%2520shift%2520for%2520data-sensitive%2520and%250Adecentralized%2520environments.%2520Despite%2520its%2520rapid%2520advancements%252C%2520FL%2520remains%2520a%250Acomplex%2520and%2520multifaceted%2520field%252C%2520requiring%2520a%2520structured%2520understanding%2520of%2520its%250Amethodologies%252C%2520challenges%252C%2520and%2520applications.%2520In%2520this%2520survey%252C%2520we%2520introduce%2520a%250Ameta-framework%2520perspective%252C%2520conceptualising%2520FL%2520as%2520a%2520composition%2520of%2520modular%250Acomponents%2520that%2520systematically%2520address%2520core%2520aspects%2520such%2520as%2520communication%252C%250Aoptimisation%252C%2520security%252C%2520and%2520privacy.%2520We%2520provide%2520a%2520historical%2520contextualisation%250Aof%2520FL%252C%2520tracing%2520its%2520evolution%2520from%2520distributed%2520optimisation%2520to%2520modern%250Adistributed%2520learning%2520paradigms.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520taxonomy%250Adistinguishing%2520Aggregation%2520from%2520Alignment%252C%2520introducing%2520the%2520concept%2520of%2520alignment%250Aas%2520a%2520fundamental%2520operator%2520alongside%2520aggregation.%2520To%2520bridge%2520theory%2520with%250Apractice%252C%2520we%2520explore%2520available%2520FL%2520frameworks%2520in%2520Python%252C%2520facilitating%2520real-world%250Aimplementation.%2520Finally%252C%2520we%2520systematise%2520key%2520challenges%2520across%2520FL%2520sub-fields%252C%250Aproviding%2520insights%2520into%2520open%2520research%2520questions%2520throughout%2520the%2520meta-framework%250Amodules.%2520By%2520structuring%2520FL%2520within%2520a%2520meta-framework%2520of%2520modular%2520components%2520and%250Aemphasising%2520the%2520dual%2520role%2520of%2520Aggregation%2520and%2520Alignment%252C%2520this%2520survey%2520provides%2520a%250Aholistic%2520and%2520adaptable%2520foundation%2520for%2520understanding%2520and%2520advancing%2520FL%2520research%250Aand%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Federated%20Learning%3A%20A%20Meta-Framework%20Perspective&entry.906535625=Frederico%20Vicente%20and%20Cl%C3%A1udia%20Soares%20and%20Du%C5%A1an%20Jakoveti%C4%87&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20distributed%20machine%20learning%20training%20while%0Apreserving%20privacy%2C%20representing%20a%20paradigm%20shift%20for%20data-sensitive%20and%0Adecentralized%20environments.%20Despite%20its%20rapid%20advancements%2C%20FL%20remains%20a%0Acomplex%20and%20multifaceted%20field%2C%20requiring%20a%20structured%20understanding%20of%20its%0Amethodologies%2C%20challenges%2C%20and%20applications.%20In%20this%20survey%2C%20we%20introduce%20a%0Ameta-framework%20perspective%2C%20conceptualising%20FL%20as%20a%20composition%20of%20modular%0Acomponents%20that%20systematically%20address%20core%20aspects%20such%20as%20communication%2C%0Aoptimisation%2C%20security%2C%20and%20privacy.%20We%20provide%20a%20historical%20contextualisation%0Aof%20FL%2C%20tracing%20its%20evolution%20from%20distributed%20optimisation%20to%20modern%0Adistributed%20learning%20paradigms.%20Additionally%2C%20we%20propose%20a%20novel%20taxonomy%0Adistinguishing%20Aggregation%20from%20Alignment%2C%20introducing%20the%20concept%20of%20alignment%0Aas%20a%20fundamental%20operator%20alongside%20aggregation.%20To%20bridge%20theory%20with%0Apractice%2C%20we%20explore%20available%20FL%20frameworks%20in%20Python%2C%20facilitating%20real-world%0Aimplementation.%20Finally%2C%20we%20systematise%20key%20challenges%20across%20FL%20sub-fields%2C%0Aproviding%20insights%20into%20open%20research%20questions%20throughout%20the%20meta-framework%0Amodules.%20By%20structuring%20FL%20within%20a%20meta-framework%20of%20modular%20components%20and%0Aemphasising%20the%20dual%20role%20of%20Aggregation%20and%20Alignment%2C%20this%20survey%20provides%20a%0Aholistic%20and%20adaptable%20foundation%20for%20understanding%20and%20advancing%20FL%20research%0Aand%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08646v1&entry.124074799=Read"},
{"title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach", "author": "Sara Abdali and Can Goksen and Saeed Amizadeh and Julie E. Maybee and Kazuhito Koishida", "abstract": "  Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance.\n", "link": "http://arxiv.org/abs/2501.14917v5", "date": "2025-05-13", "relevancy": 2.3727, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-reflecting%20Large%20Language%20Models%3A%20A%20Hegelian%20Dialectical%20Approach&body=Title%3A%20Self-reflecting%20Large%20Language%20Models%3A%20A%20Hegelian%20Dialectical%20Approach%0AAuthor%3A%20Sara%20Abdali%20and%20Can%20Goksen%20and%20Saeed%20Amizadeh%20and%20Julie%20E.%20Maybee%20and%20Kazuhito%20Koishida%0AAbstract%3A%20%20%20Investigating%20NLP%20through%20a%20philosophical%20lens%20has%20recently%20caught%0Aresearcher%27s%20eyes%20as%20it%20connects%20computational%20methods%20with%20classical%20schools%0Aof%20philosophy.%20This%20paper%20introduces%20a%20philosophical%20approach%20inspired%20by%20the%0A%5Ctextit%7BHegelian%20Dialectic%7D%20for%20LLMs%27%20%5Ctextit%7Bself-reflection%7D%2C%20utilizing%20a%0Aself-dialectical%20approach%20to%20emulate%20internal%20critiques%20and%20then%20synthesize%20new%0Aideas%20by%20resolving%20the%20opposing%20points%20of%20view.%20Moreover%2C%20this%20paper%0Ainvestigates%20the%20effect%20of%20LLMs%27%20temperature%20for%20generation%20by%20establishing%20a%0Adynamic%20annealing%20approach%2C%20which%20promotes%20the%20creativity%20in%20the%20early%20stages%0Aand%20gradually%20refines%20it%20by%20focusing%20on%20the%20nuances%2C%20as%20well%20as%20a%0Afixed-temperature%20strategy%20for%20generation.%20We%20assess%20the%20effectiveness%20of%20our%0Aproposed%20method%20in%20generating%20novel%20ideas%20and%20in%20improving%20the%20reasoning%0Aabilities%20of%20LLMs%20during%20problem-solving.%20Moreover%2C%20we%20implement%20a%20Multi-Agent%0AMajority%20Voting%20%28MAMV%29%20strategy%20to%20assess%20the%20validity%20and%20novelty%20of%20the%0Agenerated%20ideas%2C%20which%20proves%20useful%20in%20the%20absence%20of%20domain%20experts.%20Our%0Aexperiments%20demonstrate%20promising%20results%20in%20generating%20ideas%20and%20enhancing%0Aproblem-solving%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14917v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-reflecting%2520Large%2520Language%2520Models%253A%2520A%2520Hegelian%2520Dialectical%2520Approach%26entry.906535625%3DSara%2520Abdali%2520and%2520Can%2520Goksen%2520and%2520Saeed%2520Amizadeh%2520and%2520Julie%2520E.%2520Maybee%2520and%2520Kazuhito%2520Koishida%26entry.1292438233%3D%2520%2520Investigating%2520NLP%2520through%2520a%2520philosophical%2520lens%2520has%2520recently%2520caught%250Aresearcher%2527s%2520eyes%2520as%2520it%2520connects%2520computational%2520methods%2520with%2520classical%2520schools%250Aof%2520philosophy.%2520This%2520paper%2520introduces%2520a%2520philosophical%2520approach%2520inspired%2520by%2520the%250A%255Ctextit%257BHegelian%2520Dialectic%257D%2520for%2520LLMs%2527%2520%255Ctextit%257Bself-reflection%257D%252C%2520utilizing%2520a%250Aself-dialectical%2520approach%2520to%2520emulate%2520internal%2520critiques%2520and%2520then%2520synthesize%2520new%250Aideas%2520by%2520resolving%2520the%2520opposing%2520points%2520of%2520view.%2520Moreover%252C%2520this%2520paper%250Ainvestigates%2520the%2520effect%2520of%2520LLMs%2527%2520temperature%2520for%2520generation%2520by%2520establishing%2520a%250Adynamic%2520annealing%2520approach%252C%2520which%2520promotes%2520the%2520creativity%2520in%2520the%2520early%2520stages%250Aand%2520gradually%2520refines%2520it%2520by%2520focusing%2520on%2520the%2520nuances%252C%2520as%2520well%2520as%2520a%250Afixed-temperature%2520strategy%2520for%2520generation.%2520We%2520assess%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520method%2520in%2520generating%2520novel%2520ideas%2520and%2520in%2520improving%2520the%2520reasoning%250Aabilities%2520of%2520LLMs%2520during%2520problem-solving.%2520Moreover%252C%2520we%2520implement%2520a%2520Multi-Agent%250AMajority%2520Voting%2520%2528MAMV%2529%2520strategy%2520to%2520assess%2520the%2520validity%2520and%2520novelty%2520of%2520the%250Agenerated%2520ideas%252C%2520which%2520proves%2520useful%2520in%2520the%2520absence%2520of%2520domain%2520experts.%2520Our%250Aexperiments%2520demonstrate%2520promising%2520results%2520in%2520generating%2520ideas%2520and%2520enhancing%250Aproblem-solving%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14917v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-reflecting%20Large%20Language%20Models%3A%20A%20Hegelian%20Dialectical%20Approach&entry.906535625=Sara%20Abdali%20and%20Can%20Goksen%20and%20Saeed%20Amizadeh%20and%20Julie%20E.%20Maybee%20and%20Kazuhito%20Koishida&entry.1292438233=%20%20Investigating%20NLP%20through%20a%20philosophical%20lens%20has%20recently%20caught%0Aresearcher%27s%20eyes%20as%20it%20connects%20computational%20methods%20with%20classical%20schools%0Aof%20philosophy.%20This%20paper%20introduces%20a%20philosophical%20approach%20inspired%20by%20the%0A%5Ctextit%7BHegelian%20Dialectic%7D%20for%20LLMs%27%20%5Ctextit%7Bself-reflection%7D%2C%20utilizing%20a%0Aself-dialectical%20approach%20to%20emulate%20internal%20critiques%20and%20then%20synthesize%20new%0Aideas%20by%20resolving%20the%20opposing%20points%20of%20view.%20Moreover%2C%20this%20paper%0Ainvestigates%20the%20effect%20of%20LLMs%27%20temperature%20for%20generation%20by%20establishing%20a%0Adynamic%20annealing%20approach%2C%20which%20promotes%20the%20creativity%20in%20the%20early%20stages%0Aand%20gradually%20refines%20it%20by%20focusing%20on%20the%20nuances%2C%20as%20well%20as%20a%0Afixed-temperature%20strategy%20for%20generation.%20We%20assess%20the%20effectiveness%20of%20our%0Aproposed%20method%20in%20generating%20novel%20ideas%20and%20in%20improving%20the%20reasoning%0Aabilities%20of%20LLMs%20during%20problem-solving.%20Moreover%2C%20we%20implement%20a%20Multi-Agent%0AMajority%20Voting%20%28MAMV%29%20strategy%20to%20assess%20the%20validity%20and%20novelty%20of%20the%0Agenerated%20ideas%2C%20which%20proves%20useful%20in%20the%20absence%20of%20domain%20experts.%20Our%0Aexperiments%20demonstrate%20promising%20results%20in%20generating%20ideas%20and%20enhancing%0Aproblem-solving%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14917v5&entry.124074799=Read"},
{"title": "Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided\n  Adaptive Token Selection", "author": "Ayush K. Rai and Kyle Min and Tarun Krishna and Feiyan Hu and Alan F. Smeaton and Noel E. O'Connor", "abstract": "  Masked video modeling~(MVM) has emerged as a highly effective pre-training\nstrategy for visual foundation models, whereby the model reconstructs masked\nspatiotemporal tokens using information from visible tokens. However, a key\nchallenge in such approaches lies in selecting an appropriate masking strategy.\nPrevious studies have explored predefined masking techniques, including random\nand tube-based masking, as well as approaches that leverage key motion priors,\noptical flow and semantic cues from externally pre-trained models. In this\nwork, we introduce a novel and generalizable Trajectory-Aware Adaptive Token\nSampler (TATS), which models the motion dynamics of tokens and can be\nseamlessly integrated into the masked autoencoder (MAE) framework to select\nmotion-centric tokens in videos. Additionally, we propose a unified training\nstrategy that enables joint optimization of both MAE and TATS from scratch\nusing Proximal Policy Optimization (PPO). We show that our model allows for\naggressive masking without compromising performance on the downstream task of\naction recognition while also ensuring that the pre-training remains memory\nefficient. Extensive experiments of the proposed approach across four\nbenchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51,\ndemonstrate the effectiveness, transferability, generalization, and efficiency\nof our work compared to other state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.08561v1", "date": "2025-05-13", "relevancy": 2.3648, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6183}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.599}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20meets%20Masked%20Video%20Modeling%20%3A%20Trajectory-Guided%0A%20%20Adaptive%20Token%20Selection&body=Title%3A%20Reinforcement%20Learning%20meets%20Masked%20Video%20Modeling%20%3A%20Trajectory-Guided%0A%20%20Adaptive%20Token%20Selection%0AAuthor%3A%20Ayush%20K.%20Rai%20and%20Kyle%20Min%20and%20Tarun%20Krishna%20and%20Feiyan%20Hu%20and%20Alan%20F.%20Smeaton%20and%20Noel%20E.%20O%27Connor%0AAbstract%3A%20%20%20Masked%20video%20modeling~%28MVM%29%20has%20emerged%20as%20a%20highly%20effective%20pre-training%0Astrategy%20for%20visual%20foundation%20models%2C%20whereby%20the%20model%20reconstructs%20masked%0Aspatiotemporal%20tokens%20using%20information%20from%20visible%20tokens.%20However%2C%20a%20key%0Achallenge%20in%20such%20approaches%20lies%20in%20selecting%20an%20appropriate%20masking%20strategy.%0APrevious%20studies%20have%20explored%20predefined%20masking%20techniques%2C%20including%20random%0Aand%20tube-based%20masking%2C%20as%20well%20as%20approaches%20that%20leverage%20key%20motion%20priors%2C%0Aoptical%20flow%20and%20semantic%20cues%20from%20externally%20pre-trained%20models.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20and%20generalizable%20Trajectory-Aware%20Adaptive%20Token%0ASampler%20%28TATS%29%2C%20which%20models%20the%20motion%20dynamics%20of%20tokens%20and%20can%20be%0Aseamlessly%20integrated%20into%20the%20masked%20autoencoder%20%28MAE%29%20framework%20to%20select%0Amotion-centric%20tokens%20in%20videos.%20Additionally%2C%20we%20propose%20a%20unified%20training%0Astrategy%20that%20enables%20joint%20optimization%20of%20both%20MAE%20and%20TATS%20from%20scratch%0Ausing%20Proximal%20Policy%20Optimization%20%28PPO%29.%20We%20show%20that%20our%20model%20allows%20for%0Aaggressive%20masking%20without%20compromising%20performance%20on%20the%20downstream%20task%20of%0Aaction%20recognition%20while%20also%20ensuring%20that%20the%20pre-training%20remains%20memory%0Aefficient.%20Extensive%20experiments%20of%20the%20proposed%20approach%20across%20four%0Abenchmarks%2C%20including%20Something-Something%20v2%2C%20Kinetics-400%2C%20UCF101%2C%20and%20HMDB51%2C%0Ademonstrate%20the%20effectiveness%2C%20transferability%2C%20generalization%2C%20and%20efficiency%0Aof%20our%20work%20compared%20to%20other%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520meets%2520Masked%2520Video%2520Modeling%2520%253A%2520Trajectory-Guided%250A%2520%2520Adaptive%2520Token%2520Selection%26entry.906535625%3DAyush%2520K.%2520Rai%2520and%2520Kyle%2520Min%2520and%2520Tarun%2520Krishna%2520and%2520Feiyan%2520Hu%2520and%2520Alan%2520F.%2520Smeaton%2520and%2520Noel%2520E.%2520O%2527Connor%26entry.1292438233%3D%2520%2520Masked%2520video%2520modeling~%2528MVM%2529%2520has%2520emerged%2520as%2520a%2520highly%2520effective%2520pre-training%250Astrategy%2520for%2520visual%2520foundation%2520models%252C%2520whereby%2520the%2520model%2520reconstructs%2520masked%250Aspatiotemporal%2520tokens%2520using%2520information%2520from%2520visible%2520tokens.%2520However%252C%2520a%2520key%250Achallenge%2520in%2520such%2520approaches%2520lies%2520in%2520selecting%2520an%2520appropriate%2520masking%2520strategy.%250APrevious%2520studies%2520have%2520explored%2520predefined%2520masking%2520techniques%252C%2520including%2520random%250Aand%2520tube-based%2520masking%252C%2520as%2520well%2520as%2520approaches%2520that%2520leverage%2520key%2520motion%2520priors%252C%250Aoptical%2520flow%2520and%2520semantic%2520cues%2520from%2520externally%2520pre-trained%2520models.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520novel%2520and%2520generalizable%2520Trajectory-Aware%2520Adaptive%2520Token%250ASampler%2520%2528TATS%2529%252C%2520which%2520models%2520the%2520motion%2520dynamics%2520of%2520tokens%2520and%2520can%2520be%250Aseamlessly%2520integrated%2520into%2520the%2520masked%2520autoencoder%2520%2528MAE%2529%2520framework%2520to%2520select%250Amotion-centric%2520tokens%2520in%2520videos.%2520Additionally%252C%2520we%2520propose%2520a%2520unified%2520training%250Astrategy%2520that%2520enables%2520joint%2520optimization%2520of%2520both%2520MAE%2520and%2520TATS%2520from%2520scratch%250Ausing%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529.%2520We%2520show%2520that%2520our%2520model%2520allows%2520for%250Aaggressive%2520masking%2520without%2520compromising%2520performance%2520on%2520the%2520downstream%2520task%2520of%250Aaction%2520recognition%2520while%2520also%2520ensuring%2520that%2520the%2520pre-training%2520remains%2520memory%250Aefficient.%2520Extensive%2520experiments%2520of%2520the%2520proposed%2520approach%2520across%2520four%250Abenchmarks%252C%2520including%2520Something-Something%2520v2%252C%2520Kinetics-400%252C%2520UCF101%252C%2520and%2520HMDB51%252C%250Ademonstrate%2520the%2520effectiveness%252C%2520transferability%252C%2520generalization%252C%2520and%2520efficiency%250Aof%2520our%2520work%2520compared%2520to%2520other%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20meets%20Masked%20Video%20Modeling%20%3A%20Trajectory-Guided%0A%20%20Adaptive%20Token%20Selection&entry.906535625=Ayush%20K.%20Rai%20and%20Kyle%20Min%20and%20Tarun%20Krishna%20and%20Feiyan%20Hu%20and%20Alan%20F.%20Smeaton%20and%20Noel%20E.%20O%27Connor&entry.1292438233=%20%20Masked%20video%20modeling~%28MVM%29%20has%20emerged%20as%20a%20highly%20effective%20pre-training%0Astrategy%20for%20visual%20foundation%20models%2C%20whereby%20the%20model%20reconstructs%20masked%0Aspatiotemporal%20tokens%20using%20information%20from%20visible%20tokens.%20However%2C%20a%20key%0Achallenge%20in%20such%20approaches%20lies%20in%20selecting%20an%20appropriate%20masking%20strategy.%0APrevious%20studies%20have%20explored%20predefined%20masking%20techniques%2C%20including%20random%0Aand%20tube-based%20masking%2C%20as%20well%20as%20approaches%20that%20leverage%20key%20motion%20priors%2C%0Aoptical%20flow%20and%20semantic%20cues%20from%20externally%20pre-trained%20models.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20and%20generalizable%20Trajectory-Aware%20Adaptive%20Token%0ASampler%20%28TATS%29%2C%20which%20models%20the%20motion%20dynamics%20of%20tokens%20and%20can%20be%0Aseamlessly%20integrated%20into%20the%20masked%20autoencoder%20%28MAE%29%20framework%20to%20select%0Amotion-centric%20tokens%20in%20videos.%20Additionally%2C%20we%20propose%20a%20unified%20training%0Astrategy%20that%20enables%20joint%20optimization%20of%20both%20MAE%20and%20TATS%20from%20scratch%0Ausing%20Proximal%20Policy%20Optimization%20%28PPO%29.%20We%20show%20that%20our%20model%20allows%20for%0Aaggressive%20masking%20without%20compromising%20performance%20on%20the%20downstream%20task%20of%0Aaction%20recognition%20while%20also%20ensuring%20that%20the%20pre-training%20remains%20memory%0Aefficient.%20Extensive%20experiments%20of%20the%20proposed%20approach%20across%20four%0Abenchmarks%2C%20including%20Something-Something%20v2%2C%20Kinetics-400%2C%20UCF101%2C%20and%20HMDB51%2C%0Ademonstrate%20the%20effectiveness%2C%20transferability%2C%20generalization%2C%20and%20efficiency%0Aof%20our%20work%20compared%20to%20other%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08561v1&entry.124074799=Read"},
{"title": "Small but Significant: On the Promise of Small Language Models for\n  Accessible AIED", "author": "Yumou Wei and Paulo Carvalho and John Stamper", "abstract": "  GPT has become nearly synonymous with large language models (LLMs), an\nincreasingly popular term in AIED proceedings. A simple keyword-based search\nreveals that 61% of the 76 long and short papers presented at AIED 2024\ndescribe novel solutions using LLMs to address some of the long-standing\nchallenges in education, and 43% specifically mention GPT. Although LLMs\npioneered by GPT create exciting opportunities to strengthen the impact of AI\non education, we argue that the field's predominant focus on GPT and other\nresource-intensive LLMs (with more than 10B parameters) risks neglecting the\npotential impact that small language models (SLMs) can make in providing\nresource-constrained institutions with equitable and affordable access to\nhigh-quality AI tools. Supported by positive results on knowledge component\n(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as\nPhi-2 can produce an effective solution without elaborate prompting strategies.\nHence, we call for more attention to developing SLM-based AIED approaches.\n", "link": "http://arxiv.org/abs/2505.08588v1", "date": "2025-05-13", "relevancy": 2.3345, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Small%20but%20Significant%3A%20On%20the%20Promise%20of%20Small%20Language%20Models%20for%0A%20%20Accessible%20AIED&body=Title%3A%20Small%20but%20Significant%3A%20On%20the%20Promise%20of%20Small%20Language%20Models%20for%0A%20%20Accessible%20AIED%0AAuthor%3A%20Yumou%20Wei%20and%20Paulo%20Carvalho%20and%20John%20Stamper%0AAbstract%3A%20%20%20GPT%20has%20become%20nearly%20synonymous%20with%20large%20language%20models%20%28LLMs%29%2C%20an%0Aincreasingly%20popular%20term%20in%20AIED%20proceedings.%20A%20simple%20keyword-based%20search%0Areveals%20that%2061%25%20of%20the%2076%20long%20and%20short%20papers%20presented%20at%20AIED%202024%0Adescribe%20novel%20solutions%20using%20LLMs%20to%20address%20some%20of%20the%20long-standing%0Achallenges%20in%20education%2C%20and%2043%25%20specifically%20mention%20GPT.%20Although%20LLMs%0Apioneered%20by%20GPT%20create%20exciting%20opportunities%20to%20strengthen%20the%20impact%20of%20AI%0Aon%20education%2C%20we%20argue%20that%20the%20field%27s%20predominant%20focus%20on%20GPT%20and%20other%0Aresource-intensive%20LLMs%20%28with%20more%20than%2010B%20parameters%29%20risks%20neglecting%20the%0Apotential%20impact%20that%20small%20language%20models%20%28SLMs%29%20can%20make%20in%20providing%0Aresource-constrained%20institutions%20with%20equitable%20and%20affordable%20access%20to%0Ahigh-quality%20AI%20tools.%20Supported%20by%20positive%20results%20on%20knowledge%20component%0A%28KC%29%20discovery%2C%20a%20critical%20challenge%20in%20AIED%2C%20we%20demonstrate%20that%20SLMs%20such%20as%0APhi-2%20can%20produce%20an%20effective%20solution%20without%20elaborate%20prompting%20strategies.%0AHence%2C%20we%20call%20for%20more%20attention%20to%20developing%20SLM-based%20AIED%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmall%2520but%2520Significant%253A%2520On%2520the%2520Promise%2520of%2520Small%2520Language%2520Models%2520for%250A%2520%2520Accessible%2520AIED%26entry.906535625%3DYumou%2520Wei%2520and%2520Paulo%2520Carvalho%2520and%2520John%2520Stamper%26entry.1292438233%3D%2520%2520GPT%2520has%2520become%2520nearly%2520synonymous%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520an%250Aincreasingly%2520popular%2520term%2520in%2520AIED%2520proceedings.%2520A%2520simple%2520keyword-based%2520search%250Areveals%2520that%252061%2525%2520of%2520the%252076%2520long%2520and%2520short%2520papers%2520presented%2520at%2520AIED%25202024%250Adescribe%2520novel%2520solutions%2520using%2520LLMs%2520to%2520address%2520some%2520of%2520the%2520long-standing%250Achallenges%2520in%2520education%252C%2520and%252043%2525%2520specifically%2520mention%2520GPT.%2520Although%2520LLMs%250Apioneered%2520by%2520GPT%2520create%2520exciting%2520opportunities%2520to%2520strengthen%2520the%2520impact%2520of%2520AI%250Aon%2520education%252C%2520we%2520argue%2520that%2520the%2520field%2527s%2520predominant%2520focus%2520on%2520GPT%2520and%2520other%250Aresource-intensive%2520LLMs%2520%2528with%2520more%2520than%252010B%2520parameters%2529%2520risks%2520neglecting%2520the%250Apotential%2520impact%2520that%2520small%2520language%2520models%2520%2528SLMs%2529%2520can%2520make%2520in%2520providing%250Aresource-constrained%2520institutions%2520with%2520equitable%2520and%2520affordable%2520access%2520to%250Ahigh-quality%2520AI%2520tools.%2520Supported%2520by%2520positive%2520results%2520on%2520knowledge%2520component%250A%2528KC%2529%2520discovery%252C%2520a%2520critical%2520challenge%2520in%2520AIED%252C%2520we%2520demonstrate%2520that%2520SLMs%2520such%2520as%250APhi-2%2520can%2520produce%2520an%2520effective%2520solution%2520without%2520elaborate%2520prompting%2520strategies.%250AHence%252C%2520we%2520call%2520for%2520more%2520attention%2520to%2520developing%2520SLM-based%2520AIED%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Small%20but%20Significant%3A%20On%20the%20Promise%20of%20Small%20Language%20Models%20for%0A%20%20Accessible%20AIED&entry.906535625=Yumou%20Wei%20and%20Paulo%20Carvalho%20and%20John%20Stamper&entry.1292438233=%20%20GPT%20has%20become%20nearly%20synonymous%20with%20large%20language%20models%20%28LLMs%29%2C%20an%0Aincreasingly%20popular%20term%20in%20AIED%20proceedings.%20A%20simple%20keyword-based%20search%0Areveals%20that%2061%25%20of%20the%2076%20long%20and%20short%20papers%20presented%20at%20AIED%202024%0Adescribe%20novel%20solutions%20using%20LLMs%20to%20address%20some%20of%20the%20long-standing%0Achallenges%20in%20education%2C%20and%2043%25%20specifically%20mention%20GPT.%20Although%20LLMs%0Apioneered%20by%20GPT%20create%20exciting%20opportunities%20to%20strengthen%20the%20impact%20of%20AI%0Aon%20education%2C%20we%20argue%20that%20the%20field%27s%20predominant%20focus%20on%20GPT%20and%20other%0Aresource-intensive%20LLMs%20%28with%20more%20than%2010B%20parameters%29%20risks%20neglecting%20the%0Apotential%20impact%20that%20small%20language%20models%20%28SLMs%29%20can%20make%20in%20providing%0Aresource-constrained%20institutions%20with%20equitable%20and%20affordable%20access%20to%0Ahigh-quality%20AI%20tools.%20Supported%20by%20positive%20results%20on%20knowledge%20component%0A%28KC%29%20discovery%2C%20a%20critical%20challenge%20in%20AIED%2C%20we%20demonstrate%20that%20SLMs%20such%20as%0APhi-2%20can%20produce%20an%20effective%20solution%20without%20elaborate%20prompting%20strategies.%0AHence%2C%20we%20call%20for%20more%20attention%20to%20developing%20SLM-based%20AIED%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08588v1&entry.124074799=Read"},
{"title": "TextCenGen: Attention-Guided Text-Centric Background Adaptation for\n  Text-to-Image Generation", "author": "Tianyi Liang and Jiangqi Liu and Yifei Huang and Shiqi Jiang and Jianshen Shi and Changbo Wang and Chenhui Li", "abstract": "  Text-to-image (T2I) generation has made remarkable progress in producing\nhigh-quality images, but a fundamental challenge remains: creating backgrounds\nthat naturally accommodate text placement without compromising image quality.\nThis capability is non-trivial for real-world applications like graphic design,\nwhere clear visual hierarchy between content and text is essential. Prior work\nhas primarily focused on arranging layouts within existing static images,\nleaving unexplored the potential of T2I models for generating text-friendly\nbackgrounds. We present TextCenGen, a training-free dynamic background\nadaptation in the blank region for text-friendly image generation. Instead of\ndirectly reducing attention in text areas, which degrades image quality, we\nrelocate conflicting objects before background optimization. Our method\nanalyzes cross-attention maps to identify conflicting objects overlapping with\ntext regions and uses a force-directed graph approach to guide their\nrelocation, followed by attention excluding constraints to ensure smooth\nbackgrounds. Our method is plug-and-play, requiring no additional training\nwhile well balancing both semantic fidelity and visual quality. Evaluated on\nour proposed text-friendly T2I benchmark of 27,000 images across four seed\ndatasets, TextCenGen outperforms existing methods by achieving 23% lower\nsaliency overlap in text regions while maintaining 98% of the semantic fidelity\nmeasured by CLIP score and our proposed Visual-Textual Concordance Metric\n(VTCM).\n", "link": "http://arxiv.org/abs/2404.11824v5", "date": "2025-05-13", "relevancy": 2.3343, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5909}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5843}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextCenGen%3A%20Attention-Guided%20Text-Centric%20Background%20Adaptation%20for%0A%20%20Text-to-Image%20Generation&body=Title%3A%20TextCenGen%3A%20Attention-Guided%20Text-Centric%20Background%20Adaptation%20for%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Tianyi%20Liang%20and%20Jiangqi%20Liu%20and%20Yifei%20Huang%20and%20Shiqi%20Jiang%20and%20Jianshen%20Shi%20and%20Changbo%20Wang%20and%20Chenhui%20Li%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20generation%20has%20made%20remarkable%20progress%20in%20producing%0Ahigh-quality%20images%2C%20but%20a%20fundamental%20challenge%20remains%3A%20creating%20backgrounds%0Athat%20naturally%20accommodate%20text%20placement%20without%20compromising%20image%20quality.%0AThis%20capability%20is%20non-trivial%20for%20real-world%20applications%20like%20graphic%20design%2C%0Awhere%20clear%20visual%20hierarchy%20between%20content%20and%20text%20is%20essential.%20Prior%20work%0Ahas%20primarily%20focused%20on%20arranging%20layouts%20within%20existing%20static%20images%2C%0Aleaving%20unexplored%20the%20potential%20of%20T2I%20models%20for%20generating%20text-friendly%0Abackgrounds.%20We%20present%20TextCenGen%2C%20a%20training-free%20dynamic%20background%0Aadaptation%20in%20the%20blank%20region%20for%20text-friendly%20image%20generation.%20Instead%20of%0Adirectly%20reducing%20attention%20in%20text%20areas%2C%20which%20degrades%20image%20quality%2C%20we%0Arelocate%20conflicting%20objects%20before%20background%20optimization.%20Our%20method%0Aanalyzes%20cross-attention%20maps%20to%20identify%20conflicting%20objects%20overlapping%20with%0Atext%20regions%20and%20uses%20a%20force-directed%20graph%20approach%20to%20guide%20their%0Arelocation%2C%20followed%20by%20attention%20excluding%20constraints%20to%20ensure%20smooth%0Abackgrounds.%20Our%20method%20is%20plug-and-play%2C%20requiring%20no%20additional%20training%0Awhile%20well%20balancing%20both%20semantic%20fidelity%20and%20visual%20quality.%20Evaluated%20on%0Aour%20proposed%20text-friendly%20T2I%20benchmark%20of%2027%2C000%20images%20across%20four%20seed%0Adatasets%2C%20TextCenGen%20outperforms%20existing%20methods%20by%20achieving%2023%25%20lower%0Asaliency%20overlap%20in%20text%20regions%20while%20maintaining%2098%25%20of%20the%20semantic%20fidelity%0Ameasured%20by%20CLIP%20score%20and%20our%20proposed%20Visual-Textual%20Concordance%20Metric%0A%28VTCM%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11824v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextCenGen%253A%2520Attention-Guided%2520Text-Centric%2520Background%2520Adaptation%2520for%250A%2520%2520Text-to-Image%2520Generation%26entry.906535625%3DTianyi%2520Liang%2520and%2520Jiangqi%2520Liu%2520and%2520Yifei%2520Huang%2520and%2520Shiqi%2520Jiang%2520and%2520Jianshen%2520Shi%2520and%2520Changbo%2520Wang%2520and%2520Chenhui%2520Li%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520generation%2520has%2520made%2520remarkable%2520progress%2520in%2520producing%250Ahigh-quality%2520images%252C%2520but%2520a%2520fundamental%2520challenge%2520remains%253A%2520creating%2520backgrounds%250Athat%2520naturally%2520accommodate%2520text%2520placement%2520without%2520compromising%2520image%2520quality.%250AThis%2520capability%2520is%2520non-trivial%2520for%2520real-world%2520applications%2520like%2520graphic%2520design%252C%250Awhere%2520clear%2520visual%2520hierarchy%2520between%2520content%2520and%2520text%2520is%2520essential.%2520Prior%2520work%250Ahas%2520primarily%2520focused%2520on%2520arranging%2520layouts%2520within%2520existing%2520static%2520images%252C%250Aleaving%2520unexplored%2520the%2520potential%2520of%2520T2I%2520models%2520for%2520generating%2520text-friendly%250Abackgrounds.%2520We%2520present%2520TextCenGen%252C%2520a%2520training-free%2520dynamic%2520background%250Aadaptation%2520in%2520the%2520blank%2520region%2520for%2520text-friendly%2520image%2520generation.%2520Instead%2520of%250Adirectly%2520reducing%2520attention%2520in%2520text%2520areas%252C%2520which%2520degrades%2520image%2520quality%252C%2520we%250Arelocate%2520conflicting%2520objects%2520before%2520background%2520optimization.%2520Our%2520method%250Aanalyzes%2520cross-attention%2520maps%2520to%2520identify%2520conflicting%2520objects%2520overlapping%2520with%250Atext%2520regions%2520and%2520uses%2520a%2520force-directed%2520graph%2520approach%2520to%2520guide%2520their%250Arelocation%252C%2520followed%2520by%2520attention%2520excluding%2520constraints%2520to%2520ensure%2520smooth%250Abackgrounds.%2520Our%2520method%2520is%2520plug-and-play%252C%2520requiring%2520no%2520additional%2520training%250Awhile%2520well%2520balancing%2520both%2520semantic%2520fidelity%2520and%2520visual%2520quality.%2520Evaluated%2520on%250Aour%2520proposed%2520text-friendly%2520T2I%2520benchmark%2520of%252027%252C000%2520images%2520across%2520four%2520seed%250Adatasets%252C%2520TextCenGen%2520outperforms%2520existing%2520methods%2520by%2520achieving%252023%2525%2520lower%250Asaliency%2520overlap%2520in%2520text%2520regions%2520while%2520maintaining%252098%2525%2520of%2520the%2520semantic%2520fidelity%250Ameasured%2520by%2520CLIP%2520score%2520and%2520our%2520proposed%2520Visual-Textual%2520Concordance%2520Metric%250A%2528VTCM%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11824v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextCenGen%3A%20Attention-Guided%20Text-Centric%20Background%20Adaptation%20for%0A%20%20Text-to-Image%20Generation&entry.906535625=Tianyi%20Liang%20and%20Jiangqi%20Liu%20and%20Yifei%20Huang%20and%20Shiqi%20Jiang%20and%20Jianshen%20Shi%20and%20Changbo%20Wang%20and%20Chenhui%20Li&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20generation%20has%20made%20remarkable%20progress%20in%20producing%0Ahigh-quality%20images%2C%20but%20a%20fundamental%20challenge%20remains%3A%20creating%20backgrounds%0Athat%20naturally%20accommodate%20text%20placement%20without%20compromising%20image%20quality.%0AThis%20capability%20is%20non-trivial%20for%20real-world%20applications%20like%20graphic%20design%2C%0Awhere%20clear%20visual%20hierarchy%20between%20content%20and%20text%20is%20essential.%20Prior%20work%0Ahas%20primarily%20focused%20on%20arranging%20layouts%20within%20existing%20static%20images%2C%0Aleaving%20unexplored%20the%20potential%20of%20T2I%20models%20for%20generating%20text-friendly%0Abackgrounds.%20We%20present%20TextCenGen%2C%20a%20training-free%20dynamic%20background%0Aadaptation%20in%20the%20blank%20region%20for%20text-friendly%20image%20generation.%20Instead%20of%0Adirectly%20reducing%20attention%20in%20text%20areas%2C%20which%20degrades%20image%20quality%2C%20we%0Arelocate%20conflicting%20objects%20before%20background%20optimization.%20Our%20method%0Aanalyzes%20cross-attention%20maps%20to%20identify%20conflicting%20objects%20overlapping%20with%0Atext%20regions%20and%20uses%20a%20force-directed%20graph%20approach%20to%20guide%20their%0Arelocation%2C%20followed%20by%20attention%20excluding%20constraints%20to%20ensure%20smooth%0Abackgrounds.%20Our%20method%20is%20plug-and-play%2C%20requiring%20no%20additional%20training%0Awhile%20well%20balancing%20both%20semantic%20fidelity%20and%20visual%20quality.%20Evaluated%20on%0Aour%20proposed%20text-friendly%20T2I%20benchmark%20of%2027%2C000%20images%20across%20four%20seed%0Adatasets%2C%20TextCenGen%20outperforms%20existing%20methods%20by%20achieving%2023%25%20lower%0Asaliency%20overlap%20in%20text%20regions%20while%20maintaining%2098%25%20of%20the%20semantic%20fidelity%0Ameasured%20by%20CLIP%20score%20and%20our%20proposed%20Visual-Textual%20Concordance%20Metric%0A%28VTCM%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11824v5&entry.124074799=Read"},
{"title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models", "author": "Pritam Sarkar and Ali Etemad", "abstract": "  Despite recent advances in video understanding, the capabilities of Large\nVideo Language Models (LVLMs) to perform video-based causal reasoning remains\nunderexplored, largely due to the absence of relevant and dedicated benchmarks\nfor evaluating causal reasoning in visually grounded and goal-driven settings.\nTo fill this gap, we introduce a novel benchmark named Video-based long-form\nCausal Reasoning (VCRBench). We create VCRBench using procedural videos of\nsimple everyday activities, where the steps are deliberately shuffled with each\nclip capturing a key causal event, to test whether LVLMs can identify, reason\nabout, and correctly sequence the events needed to accomplish a specific goal.\nMoreover, the benchmark is carefully designed to prevent LVLMs from exploiting\nlinguistic shortcuts, as seen in multiple-choice or binary QA formats, while\nalso avoiding the challenges associated with evaluating open-ended QA. Our\nevaluation of state-of-the-art LVLMs on VCRBench suggests that these models\nstruggle with video-based long-form causal reasoning, primarily due to their\ndifficulty in modeling long-range causal dependencies directly from visual\nobservations. As a simple step toward enabling such capabilities, we propose\nRecognition-Reasoning Decomposition (RRD), a modular approach that breaks\nvideo-based causal reasoning into two sub-tasks of video recognition and causal\nreasoning. Our experiments on VCRBench show that RRD significantly boosts\naccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis\nreveals interesting insights, for instance, that LVLMs primarily rely on\nlanguage knowledge for complex video-based long-form causal reasoning tasks.\n", "link": "http://arxiv.org/abs/2505.08455v1", "date": "2025-05-13", "relevancy": 2.3264, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5945}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5945}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCRBench%3A%20Exploring%20Long-form%20Causal%20Reasoning%20Capabilities%20of%20Large%0A%20%20Video%20Language%20Models&body=Title%3A%20VCRBench%3A%20Exploring%20Long-form%20Causal%20Reasoning%20Capabilities%20of%20Large%0A%20%20Video%20Language%20Models%0AAuthor%3A%20Pritam%20Sarkar%20and%20Ali%20Etemad%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20video%20understanding%2C%20the%20capabilities%20of%20Large%0AVideo%20Language%20Models%20%28LVLMs%29%20to%20perform%20video-based%20causal%20reasoning%20remains%0Aunderexplored%2C%20largely%20due%20to%20the%20absence%20of%20relevant%20and%20dedicated%20benchmarks%0Afor%20evaluating%20causal%20reasoning%20in%20visually%20grounded%20and%20goal-driven%20settings.%0ATo%20fill%20this%20gap%2C%20we%20introduce%20a%20novel%20benchmark%20named%20Video-based%20long-form%0ACausal%20Reasoning%20%28VCRBench%29.%20We%20create%20VCRBench%20using%20procedural%20videos%20of%0Asimple%20everyday%20activities%2C%20where%20the%20steps%20are%20deliberately%20shuffled%20with%20each%0Aclip%20capturing%20a%20key%20causal%20event%2C%20to%20test%20whether%20LVLMs%20can%20identify%2C%20reason%0Aabout%2C%20and%20correctly%20sequence%20the%20events%20needed%20to%20accomplish%20a%20specific%20goal.%0AMoreover%2C%20the%20benchmark%20is%20carefully%20designed%20to%20prevent%20LVLMs%20from%20exploiting%0Alinguistic%20shortcuts%2C%20as%20seen%20in%20multiple-choice%20or%20binary%20QA%20formats%2C%20while%0Aalso%20avoiding%20the%20challenges%20associated%20with%20evaluating%20open-ended%20QA.%20Our%0Aevaluation%20of%20state-of-the-art%20LVLMs%20on%20VCRBench%20suggests%20that%20these%20models%0Astruggle%20with%20video-based%20long-form%20causal%20reasoning%2C%20primarily%20due%20to%20their%0Adifficulty%20in%20modeling%20long-range%20causal%20dependencies%20directly%20from%20visual%0Aobservations.%20As%20a%20simple%20step%20toward%20enabling%20such%20capabilities%2C%20we%20propose%0ARecognition-Reasoning%20Decomposition%20%28RRD%29%2C%20a%20modular%20approach%20that%20breaks%0Avideo-based%20causal%20reasoning%20into%20two%20sub-tasks%20of%20video%20recognition%20and%20causal%0Areasoning.%20Our%20experiments%20on%20VCRBench%20show%20that%20RRD%20significantly%20boosts%0Aaccuracy%20on%20VCRBench%2C%20with%20gains%20of%20up%20to%2025.2%25.%20Finally%2C%20our%20thorough%20analysis%0Areveals%20interesting%20insights%2C%20for%20instance%2C%20that%20LVLMs%20primarily%20rely%20on%0Alanguage%20knowledge%20for%20complex%20video-based%20long-form%20causal%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCRBench%253A%2520Exploring%2520Long-form%2520Causal%2520Reasoning%2520Capabilities%2520of%2520Large%250A%2520%2520Video%2520Language%2520Models%26entry.906535625%3DPritam%2520Sarkar%2520and%2520Ali%2520Etemad%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520video%2520understanding%252C%2520the%2520capabilities%2520of%2520Large%250AVideo%2520Language%2520Models%2520%2528LVLMs%2529%2520to%2520perform%2520video-based%2520causal%2520reasoning%2520remains%250Aunderexplored%252C%2520largely%2520due%2520to%2520the%2520absence%2520of%2520relevant%2520and%2520dedicated%2520benchmarks%250Afor%2520evaluating%2520causal%2520reasoning%2520in%2520visually%2520grounded%2520and%2520goal-driven%2520settings.%250ATo%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520a%2520novel%2520benchmark%2520named%2520Video-based%2520long-form%250ACausal%2520Reasoning%2520%2528VCRBench%2529.%2520We%2520create%2520VCRBench%2520using%2520procedural%2520videos%2520of%250Asimple%2520everyday%2520activities%252C%2520where%2520the%2520steps%2520are%2520deliberately%2520shuffled%2520with%2520each%250Aclip%2520capturing%2520a%2520key%2520causal%2520event%252C%2520to%2520test%2520whether%2520LVLMs%2520can%2520identify%252C%2520reason%250Aabout%252C%2520and%2520correctly%2520sequence%2520the%2520events%2520needed%2520to%2520accomplish%2520a%2520specific%2520goal.%250AMoreover%252C%2520the%2520benchmark%2520is%2520carefully%2520designed%2520to%2520prevent%2520LVLMs%2520from%2520exploiting%250Alinguistic%2520shortcuts%252C%2520as%2520seen%2520in%2520multiple-choice%2520or%2520binary%2520QA%2520formats%252C%2520while%250Aalso%2520avoiding%2520the%2520challenges%2520associated%2520with%2520evaluating%2520open-ended%2520QA.%2520Our%250Aevaluation%2520of%2520state-of-the-art%2520LVLMs%2520on%2520VCRBench%2520suggests%2520that%2520these%2520models%250Astruggle%2520with%2520video-based%2520long-form%2520causal%2520reasoning%252C%2520primarily%2520due%2520to%2520their%250Adifficulty%2520in%2520modeling%2520long-range%2520causal%2520dependencies%2520directly%2520from%2520visual%250Aobservations.%2520As%2520a%2520simple%2520step%2520toward%2520enabling%2520such%2520capabilities%252C%2520we%2520propose%250ARecognition-Reasoning%2520Decomposition%2520%2528RRD%2529%252C%2520a%2520modular%2520approach%2520that%2520breaks%250Avideo-based%2520causal%2520reasoning%2520into%2520two%2520sub-tasks%2520of%2520video%2520recognition%2520and%2520causal%250Areasoning.%2520Our%2520experiments%2520on%2520VCRBench%2520show%2520that%2520RRD%2520significantly%2520boosts%250Aaccuracy%2520on%2520VCRBench%252C%2520with%2520gains%2520of%2520up%2520to%252025.2%2525.%2520Finally%252C%2520our%2520thorough%2520analysis%250Areveals%2520interesting%2520insights%252C%2520for%2520instance%252C%2520that%2520LVLMs%2520primarily%2520rely%2520on%250Alanguage%2520knowledge%2520for%2520complex%2520video-based%2520long-form%2520causal%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCRBench%3A%20Exploring%20Long-form%20Causal%20Reasoning%20Capabilities%20of%20Large%0A%20%20Video%20Language%20Models&entry.906535625=Pritam%20Sarkar%20and%20Ali%20Etemad&entry.1292438233=%20%20Despite%20recent%20advances%20in%20video%20understanding%2C%20the%20capabilities%20of%20Large%0AVideo%20Language%20Models%20%28LVLMs%29%20to%20perform%20video-based%20causal%20reasoning%20remains%0Aunderexplored%2C%20largely%20due%20to%20the%20absence%20of%20relevant%20and%20dedicated%20benchmarks%0Afor%20evaluating%20causal%20reasoning%20in%20visually%20grounded%20and%20goal-driven%20settings.%0ATo%20fill%20this%20gap%2C%20we%20introduce%20a%20novel%20benchmark%20named%20Video-based%20long-form%0ACausal%20Reasoning%20%28VCRBench%29.%20We%20create%20VCRBench%20using%20procedural%20videos%20of%0Asimple%20everyday%20activities%2C%20where%20the%20steps%20are%20deliberately%20shuffled%20with%20each%0Aclip%20capturing%20a%20key%20causal%20event%2C%20to%20test%20whether%20LVLMs%20can%20identify%2C%20reason%0Aabout%2C%20and%20correctly%20sequence%20the%20events%20needed%20to%20accomplish%20a%20specific%20goal.%0AMoreover%2C%20the%20benchmark%20is%20carefully%20designed%20to%20prevent%20LVLMs%20from%20exploiting%0Alinguistic%20shortcuts%2C%20as%20seen%20in%20multiple-choice%20or%20binary%20QA%20formats%2C%20while%0Aalso%20avoiding%20the%20challenges%20associated%20with%20evaluating%20open-ended%20QA.%20Our%0Aevaluation%20of%20state-of-the-art%20LVLMs%20on%20VCRBench%20suggests%20that%20these%20models%0Astruggle%20with%20video-based%20long-form%20causal%20reasoning%2C%20primarily%20due%20to%20their%0Adifficulty%20in%20modeling%20long-range%20causal%20dependencies%20directly%20from%20visual%0Aobservations.%20As%20a%20simple%20step%20toward%20enabling%20such%20capabilities%2C%20we%20propose%0ARecognition-Reasoning%20Decomposition%20%28RRD%29%2C%20a%20modular%20approach%20that%20breaks%0Avideo-based%20causal%20reasoning%20into%20two%20sub-tasks%20of%20video%20recognition%20and%20causal%0Areasoning.%20Our%20experiments%20on%20VCRBench%20show%20that%20RRD%20significantly%20boosts%0Aaccuracy%20on%20VCRBench%2C%20with%20gains%20of%20up%20to%2025.2%25.%20Finally%2C%20our%20thorough%20analysis%0Areveals%20interesting%20insights%2C%20for%20instance%2C%20that%20LVLMs%20primarily%20rely%20on%0Alanguage%20knowledge%20for%20complex%20video-based%20long-form%20causal%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08455v1&entry.124074799=Read"},
{"title": "TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human\n  Body Forgery Detection", "author": "Wenkui Yang and Zhida Zhang and Xiaoqiang Zhou and Junxian Duan and Jie Cao", "abstract": "  The emergence and popularity of facial deepfake methods spur the vigorous\ndevelopment of deepfake datasets and facial forgery detection, which to some\nextent alleviates the security concerns about facial-related artificial\nintelligence technologies. However, when it comes to human body forgery, there\nhas been a persistent lack of datasets and detection methods, due to the later\ninception and complexity of human body generation methods. To mitigate this\nissue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale\ndiffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic\nframes, specifically tailored for body forgery detection. TT-DF offers a wide\nvariety of forgery methods, involving multiple advanced human image animation\nmodels utilized for manipulation, two generative configurations based on the\ndisentanglement of identity and pose information, as well as different\ncompressed versions. The aim is to simulate any potential unseen forged data in\nthe wild as comprehensively as possible, and we also furnish a benchmark on\nTT-DF. Additionally, we propose an adapted body forgery detection model,\nTemporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal\ninconsistencies and optical flow distribution differences between natural data\nand forged data. Our experiments demonstrate that TOF-Net achieves favorable\nperformance on TT-DF, outperforming current state-of-the-art extendable facial\nforgery detection models. For our TT-DF dataset, please refer to\nhttps://github.com/HashTAG00002/TT-DF.\n", "link": "http://arxiv.org/abs/2505.08437v1", "date": "2025-05-13", "relevancy": 2.3045, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5861}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5725}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TT-DF%3A%20A%20Large-Scale%20Diffusion-Based%20Dataset%20and%20Benchmark%20for%20Human%0A%20%20Body%20Forgery%20Detection&body=Title%3A%20TT-DF%3A%20A%20Large-Scale%20Diffusion-Based%20Dataset%20and%20Benchmark%20for%20Human%0A%20%20Body%20Forgery%20Detection%0AAuthor%3A%20Wenkui%20Yang%20and%20Zhida%20Zhang%20and%20Xiaoqiang%20Zhou%20and%20Junxian%20Duan%20and%20Jie%20Cao%0AAbstract%3A%20%20%20The%20emergence%20and%20popularity%20of%20facial%20deepfake%20methods%20spur%20the%20vigorous%0Adevelopment%20of%20deepfake%20datasets%20and%20facial%20forgery%20detection%2C%20which%20to%20some%0Aextent%20alleviates%20the%20security%20concerns%20about%20facial-related%20artificial%0Aintelligence%20technologies.%20However%2C%20when%20it%20comes%20to%20human%20body%20forgery%2C%20there%0Ahas%20been%20a%20persistent%20lack%20of%20datasets%20and%20detection%20methods%2C%20due%20to%20the%20later%0Ainception%20and%20complexity%20of%20human%20body%20generation%20methods.%20To%20mitigate%20this%0Aissue%2C%20we%20introduce%20TikTok-DeepFake%20%28TT-DF%29%2C%20a%20novel%20large-scale%0Adiffusion-based%20dataset%20containing%206%2C120%20forged%20videos%20with%201%2C378%2C857%20synthetic%0Aframes%2C%20specifically%20tailored%20for%20body%20forgery%20detection.%20TT-DF%20offers%20a%20wide%0Avariety%20of%20forgery%20methods%2C%20involving%20multiple%20advanced%20human%20image%20animation%0Amodels%20utilized%20for%20manipulation%2C%20two%20generative%20configurations%20based%20on%20the%0Adisentanglement%20of%20identity%20and%20pose%20information%2C%20as%20well%20as%20different%0Acompressed%20versions.%20The%20aim%20is%20to%20simulate%20any%20potential%20unseen%20forged%20data%20in%0Athe%20wild%20as%20comprehensively%20as%20possible%2C%20and%20we%20also%20furnish%20a%20benchmark%20on%0ATT-DF.%20Additionally%2C%20we%20propose%20an%20adapted%20body%20forgery%20detection%20model%2C%0ATemporal%20Optical%20Flow%20Network%20%28TOF-Net%29%2C%20which%20exploits%20the%20spatiotemporal%0Ainconsistencies%20and%20optical%20flow%20distribution%20differences%20between%20natural%20data%0Aand%20forged%20data.%20Our%20experiments%20demonstrate%20that%20TOF-Net%20achieves%20favorable%0Aperformance%20on%20TT-DF%2C%20outperforming%20current%20state-of-the-art%20extendable%20facial%0Aforgery%20detection%20models.%20For%20our%20TT-DF%20dataset%2C%20please%20refer%20to%0Ahttps%3A//github.com/HashTAG00002/TT-DF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTT-DF%253A%2520A%2520Large-Scale%2520Diffusion-Based%2520Dataset%2520and%2520Benchmark%2520for%2520Human%250A%2520%2520Body%2520Forgery%2520Detection%26entry.906535625%3DWenkui%2520Yang%2520and%2520Zhida%2520Zhang%2520and%2520Xiaoqiang%2520Zhou%2520and%2520Junxian%2520Duan%2520and%2520Jie%2520Cao%26entry.1292438233%3D%2520%2520The%2520emergence%2520and%2520popularity%2520of%2520facial%2520deepfake%2520methods%2520spur%2520the%2520vigorous%250Adevelopment%2520of%2520deepfake%2520datasets%2520and%2520facial%2520forgery%2520detection%252C%2520which%2520to%2520some%250Aextent%2520alleviates%2520the%2520security%2520concerns%2520about%2520facial-related%2520artificial%250Aintelligence%2520technologies.%2520However%252C%2520when%2520it%2520comes%2520to%2520human%2520body%2520forgery%252C%2520there%250Ahas%2520been%2520a%2520persistent%2520lack%2520of%2520datasets%2520and%2520detection%2520methods%252C%2520due%2520to%2520the%2520later%250Ainception%2520and%2520complexity%2520of%2520human%2520body%2520generation%2520methods.%2520To%2520mitigate%2520this%250Aissue%252C%2520we%2520introduce%2520TikTok-DeepFake%2520%2528TT-DF%2529%252C%2520a%2520novel%2520large-scale%250Adiffusion-based%2520dataset%2520containing%25206%252C120%2520forged%2520videos%2520with%25201%252C378%252C857%2520synthetic%250Aframes%252C%2520specifically%2520tailored%2520for%2520body%2520forgery%2520detection.%2520TT-DF%2520offers%2520a%2520wide%250Avariety%2520of%2520forgery%2520methods%252C%2520involving%2520multiple%2520advanced%2520human%2520image%2520animation%250Amodels%2520utilized%2520for%2520manipulation%252C%2520two%2520generative%2520configurations%2520based%2520on%2520the%250Adisentanglement%2520of%2520identity%2520and%2520pose%2520information%252C%2520as%2520well%2520as%2520different%250Acompressed%2520versions.%2520The%2520aim%2520is%2520to%2520simulate%2520any%2520potential%2520unseen%2520forged%2520data%2520in%250Athe%2520wild%2520as%2520comprehensively%2520as%2520possible%252C%2520and%2520we%2520also%2520furnish%2520a%2520benchmark%2520on%250ATT-DF.%2520Additionally%252C%2520we%2520propose%2520an%2520adapted%2520body%2520forgery%2520detection%2520model%252C%250ATemporal%2520Optical%2520Flow%2520Network%2520%2528TOF-Net%2529%252C%2520which%2520exploits%2520the%2520spatiotemporal%250Ainconsistencies%2520and%2520optical%2520flow%2520distribution%2520differences%2520between%2520natural%2520data%250Aand%2520forged%2520data.%2520Our%2520experiments%2520demonstrate%2520that%2520TOF-Net%2520achieves%2520favorable%250Aperformance%2520on%2520TT-DF%252C%2520outperforming%2520current%2520state-of-the-art%2520extendable%2520facial%250Aforgery%2520detection%2520models.%2520For%2520our%2520TT-DF%2520dataset%252C%2520please%2520refer%2520to%250Ahttps%253A//github.com/HashTAG00002/TT-DF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TT-DF%3A%20A%20Large-Scale%20Diffusion-Based%20Dataset%20and%20Benchmark%20for%20Human%0A%20%20Body%20Forgery%20Detection&entry.906535625=Wenkui%20Yang%20and%20Zhida%20Zhang%20and%20Xiaoqiang%20Zhou%20and%20Junxian%20Duan%20and%20Jie%20Cao&entry.1292438233=%20%20The%20emergence%20and%20popularity%20of%20facial%20deepfake%20methods%20spur%20the%20vigorous%0Adevelopment%20of%20deepfake%20datasets%20and%20facial%20forgery%20detection%2C%20which%20to%20some%0Aextent%20alleviates%20the%20security%20concerns%20about%20facial-related%20artificial%0Aintelligence%20technologies.%20However%2C%20when%20it%20comes%20to%20human%20body%20forgery%2C%20there%0Ahas%20been%20a%20persistent%20lack%20of%20datasets%20and%20detection%20methods%2C%20due%20to%20the%20later%0Ainception%20and%20complexity%20of%20human%20body%20generation%20methods.%20To%20mitigate%20this%0Aissue%2C%20we%20introduce%20TikTok-DeepFake%20%28TT-DF%29%2C%20a%20novel%20large-scale%0Adiffusion-based%20dataset%20containing%206%2C120%20forged%20videos%20with%201%2C378%2C857%20synthetic%0Aframes%2C%20specifically%20tailored%20for%20body%20forgery%20detection.%20TT-DF%20offers%20a%20wide%0Avariety%20of%20forgery%20methods%2C%20involving%20multiple%20advanced%20human%20image%20animation%0Amodels%20utilized%20for%20manipulation%2C%20two%20generative%20configurations%20based%20on%20the%0Adisentanglement%20of%20identity%20and%20pose%20information%2C%20as%20well%20as%20different%0Acompressed%20versions.%20The%20aim%20is%20to%20simulate%20any%20potential%20unseen%20forged%20data%20in%0Athe%20wild%20as%20comprehensively%20as%20possible%2C%20and%20we%20also%20furnish%20a%20benchmark%20on%0ATT-DF.%20Additionally%2C%20we%20propose%20an%20adapted%20body%20forgery%20detection%20model%2C%0ATemporal%20Optical%20Flow%20Network%20%28TOF-Net%29%2C%20which%20exploits%20the%20spatiotemporal%0Ainconsistencies%20and%20optical%20flow%20distribution%20differences%20between%20natural%20data%0Aand%20forged%20data.%20Our%20experiments%20demonstrate%20that%20TOF-Net%20achieves%20favorable%0Aperformance%20on%20TT-DF%2C%20outperforming%20current%20state-of-the-art%20extendable%20facial%0Aforgery%20detection%20models.%20For%20our%20TT-DF%20dataset%2C%20please%20refer%20to%0Ahttps%3A//github.com/HashTAG00002/TT-DF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08437v1&entry.124074799=Read"},
{"title": "VIViT: Variable-Input Vision Transformer Framework for 3D MR Image\n  Segmentation", "author": "Badhan Kumar Das and Ajay Singh and Gengyan Zhao and Han Liu and Thomas J. Re and Dorin Comaniciu and Eli Gibson and Andreas Maier", "abstract": "  Self-supervised pretrain techniques have been widely used to improve the\ndownstream tasks' performance. However, real-world magnetic resonance (MR)\nstudies usually consist of different sets of contrasts due to different\nacquisition protocols, which poses challenges for the current deep learning\nmethods on large-scale pretrain and different downstream tasks with different\ninput requirements, since these methods typically require a fixed set of input\nmodalities or, contrasts. To address this challenge, we propose variable-input\nViT (VIViT), a transformer-based framework designed for self-supervised\npretraining and segmentation finetuning for variable contrasts in each study.\nWith this ability, our approach can maximize the data availability in pretrain,\nand can transfer the learned knowledge from pretrain to downstream tasks\ndespite variations in input requirements. We validate our method on brain\ninfarct and brain tumor segmentation, where our method outperforms current CNN\nand ViT-based models with a mean Dice score of 0.624 and 0.883 respectively.\nThese results highlight the efficacy of our design for better adaptability and\nperformance on tasks with real-world heterogeneous MR data.\n", "link": "http://arxiv.org/abs/2505.08693v1", "date": "2025-05-13", "relevancy": 2.2949, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5883}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5637}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIViT%3A%20Variable-Input%20Vision%20Transformer%20Framework%20for%203D%20MR%20Image%0A%20%20Segmentation&body=Title%3A%20VIViT%3A%20Variable-Input%20Vision%20Transformer%20Framework%20for%203D%20MR%20Image%0A%20%20Segmentation%0AAuthor%3A%20Badhan%20Kumar%20Das%20and%20Ajay%20Singh%20and%20Gengyan%20Zhao%20and%20Han%20Liu%20and%20Thomas%20J.%20Re%20and%20Dorin%20Comaniciu%20and%20Eli%20Gibson%20and%20Andreas%20Maier%0AAbstract%3A%20%20%20Self-supervised%20pretrain%20techniques%20have%20been%20widely%20used%20to%20improve%20the%0Adownstream%20tasks%27%20performance.%20However%2C%20real-world%20magnetic%20resonance%20%28MR%29%0Astudies%20usually%20consist%20of%20different%20sets%20of%20contrasts%20due%20to%20different%0Aacquisition%20protocols%2C%20which%20poses%20challenges%20for%20the%20current%20deep%20learning%0Amethods%20on%20large-scale%20pretrain%20and%20different%20downstream%20tasks%20with%20different%0Ainput%20requirements%2C%20since%20these%20methods%20typically%20require%20a%20fixed%20set%20of%20input%0Amodalities%20or%2C%20contrasts.%20To%20address%20this%20challenge%2C%20we%20propose%20variable-input%0AViT%20%28VIViT%29%2C%20a%20transformer-based%20framework%20designed%20for%20self-supervised%0Apretraining%20and%20segmentation%20finetuning%20for%20variable%20contrasts%20in%20each%20study.%0AWith%20this%20ability%2C%20our%20approach%20can%20maximize%20the%20data%20availability%20in%20pretrain%2C%0Aand%20can%20transfer%20the%20learned%20knowledge%20from%20pretrain%20to%20downstream%20tasks%0Adespite%20variations%20in%20input%20requirements.%20We%20validate%20our%20method%20on%20brain%0Ainfarct%20and%20brain%20tumor%20segmentation%2C%20where%20our%20method%20outperforms%20current%20CNN%0Aand%20ViT-based%20models%20with%20a%20mean%20Dice%20score%20of%200.624%20and%200.883%20respectively.%0AThese%20results%20highlight%20the%20efficacy%20of%20our%20design%20for%20better%20adaptability%20and%0Aperformance%20on%20tasks%20with%20real-world%20heterogeneous%20MR%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIViT%253A%2520Variable-Input%2520Vision%2520Transformer%2520Framework%2520for%25203D%2520MR%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DBadhan%2520Kumar%2520Das%2520and%2520Ajay%2520Singh%2520and%2520Gengyan%2520Zhao%2520and%2520Han%2520Liu%2520and%2520Thomas%2520J.%2520Re%2520and%2520Dorin%2520Comaniciu%2520and%2520Eli%2520Gibson%2520and%2520Andreas%2520Maier%26entry.1292438233%3D%2520%2520Self-supervised%2520pretrain%2520techniques%2520have%2520been%2520widely%2520used%2520to%2520improve%2520the%250Adownstream%2520tasks%2527%2520performance.%2520However%252C%2520real-world%2520magnetic%2520resonance%2520%2528MR%2529%250Astudies%2520usually%2520consist%2520of%2520different%2520sets%2520of%2520contrasts%2520due%2520to%2520different%250Aacquisition%2520protocols%252C%2520which%2520poses%2520challenges%2520for%2520the%2520current%2520deep%2520learning%250Amethods%2520on%2520large-scale%2520pretrain%2520and%2520different%2520downstream%2520tasks%2520with%2520different%250Ainput%2520requirements%252C%2520since%2520these%2520methods%2520typically%2520require%2520a%2520fixed%2520set%2520of%2520input%250Amodalities%2520or%252C%2520contrasts.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520variable-input%250AViT%2520%2528VIViT%2529%252C%2520a%2520transformer-based%2520framework%2520designed%2520for%2520self-supervised%250Apretraining%2520and%2520segmentation%2520finetuning%2520for%2520variable%2520contrasts%2520in%2520each%2520study.%250AWith%2520this%2520ability%252C%2520our%2520approach%2520can%2520maximize%2520the%2520data%2520availability%2520in%2520pretrain%252C%250Aand%2520can%2520transfer%2520the%2520learned%2520knowledge%2520from%2520pretrain%2520to%2520downstream%2520tasks%250Adespite%2520variations%2520in%2520input%2520requirements.%2520We%2520validate%2520our%2520method%2520on%2520brain%250Ainfarct%2520and%2520brain%2520tumor%2520segmentation%252C%2520where%2520our%2520method%2520outperforms%2520current%2520CNN%250Aand%2520ViT-based%2520models%2520with%2520a%2520mean%2520Dice%2520score%2520of%25200.624%2520and%25200.883%2520respectively.%250AThese%2520results%2520highlight%2520the%2520efficacy%2520of%2520our%2520design%2520for%2520better%2520adaptability%2520and%250Aperformance%2520on%2520tasks%2520with%2520real-world%2520heterogeneous%2520MR%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIViT%3A%20Variable-Input%20Vision%20Transformer%20Framework%20for%203D%20MR%20Image%0A%20%20Segmentation&entry.906535625=Badhan%20Kumar%20Das%20and%20Ajay%20Singh%20and%20Gengyan%20Zhao%20and%20Han%20Liu%20and%20Thomas%20J.%20Re%20and%20Dorin%20Comaniciu%20and%20Eli%20Gibson%20and%20Andreas%20Maier&entry.1292438233=%20%20Self-supervised%20pretrain%20techniques%20have%20been%20widely%20used%20to%20improve%20the%0Adownstream%20tasks%27%20performance.%20However%2C%20real-world%20magnetic%20resonance%20%28MR%29%0Astudies%20usually%20consist%20of%20different%20sets%20of%20contrasts%20due%20to%20different%0Aacquisition%20protocols%2C%20which%20poses%20challenges%20for%20the%20current%20deep%20learning%0Amethods%20on%20large-scale%20pretrain%20and%20different%20downstream%20tasks%20with%20different%0Ainput%20requirements%2C%20since%20these%20methods%20typically%20require%20a%20fixed%20set%20of%20input%0Amodalities%20or%2C%20contrasts.%20To%20address%20this%20challenge%2C%20we%20propose%20variable-input%0AViT%20%28VIViT%29%2C%20a%20transformer-based%20framework%20designed%20for%20self-supervised%0Apretraining%20and%20segmentation%20finetuning%20for%20variable%20contrasts%20in%20each%20study.%0AWith%20this%20ability%2C%20our%20approach%20can%20maximize%20the%20data%20availability%20in%20pretrain%2C%0Aand%20can%20transfer%20the%20learned%20knowledge%20from%20pretrain%20to%20downstream%20tasks%0Adespite%20variations%20in%20input%20requirements.%20We%20validate%20our%20method%20on%20brain%0Ainfarct%20and%20brain%20tumor%20segmentation%2C%20where%20our%20method%20outperforms%20current%20CNN%0Aand%20ViT-based%20models%20with%20a%20mean%20Dice%20score%20of%200.624%20and%200.883%20respectively.%0AThese%20results%20highlight%20the%20efficacy%20of%20our%20design%20for%20better%20adaptability%20and%0Aperformance%20on%20tasks%20with%20real-world%20heterogeneous%20MR%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08693v1&entry.124074799=Read"},
{"title": "PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory\n  Planner", "author": "Kota Kondo and Claudius T. Tewari and Andrea Tagliabue and Jesus Tordesillas and Parker C. Lusk and Mason B. Peterson and Jonathan P. How", "abstract": "  In decentralized multiagent trajectory planners, agents need to communicate\nand exchange their positions to generate collision-free trajectories. However,\ndue to localization errors/uncertainties, trajectory deconfliction can fail\neven if trajectories are perfectly shared between agents. To address this\nissue, we first present PARM and PARM*, perception-aware, decentralized,\nasynchronous multiagent trajectory planners that enable a team of agents to\nnavigate uncertain environments while deconflicting trajectories and avoiding\nobstacles using perception information. PARM* differs from PARM as it is less\nconservative, using more computation to find closer-to-optimal solutions. While\nthese methods achieve state-of-the-art performance, they suffer from high\ncomputational costs as they need to solve large optimization problems onboard,\nmaking it difficult for agents to replan at high rates. To overcome this\nchallenge, we present our second key contribution, PRIMER, a learning-based\nplanner trained with imitation learning (IL) using PARM* as the expert\ndemonstrator. PRIMER leverages the low computational requirements at deployment\nof neural networks and achieves a computation speed up to 5500 times faster\nthan optimization-based approaches.\n", "link": "http://arxiv.org/abs/2406.10060v4", "date": "2025-05-13", "relevancy": 2.2751, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5829}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5775}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner&body=Title%3A%20PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner%0AAuthor%3A%20Kota%20Kondo%20and%20Claudius%20T.%20Tewari%20and%20Andrea%20Tagliabue%20and%20Jesus%20Tordesillas%20and%20Parker%20C.%20Lusk%20and%20Mason%20B.%20Peterson%20and%20Jonathan%20P.%20How%0AAbstract%3A%20%20%20In%20decentralized%20multiagent%20trajectory%20planners%2C%20agents%20need%20to%20communicate%0Aand%20exchange%20their%20positions%20to%20generate%20collision-free%20trajectories.%20However%2C%0Adue%20to%20localization%20errors/uncertainties%2C%20trajectory%20deconfliction%20can%20fail%0Aeven%20if%20trajectories%20are%20perfectly%20shared%20between%20agents.%20To%20address%20this%0Aissue%2C%20we%20first%20present%20PARM%20and%20PARM%2A%2C%20perception-aware%2C%20decentralized%2C%0Aasynchronous%20multiagent%20trajectory%20planners%20that%20enable%20a%20team%20of%20agents%20to%0Anavigate%20uncertain%20environments%20while%20deconflicting%20trajectories%20and%20avoiding%0Aobstacles%20using%20perception%20information.%20PARM%2A%20differs%20from%20PARM%20as%20it%20is%20less%0Aconservative%2C%20using%20more%20computation%20to%20find%20closer-to-optimal%20solutions.%20While%0Athese%20methods%20achieve%20state-of-the-art%20performance%2C%20they%20suffer%20from%20high%0Acomputational%20costs%20as%20they%20need%20to%20solve%20large%20optimization%20problems%20onboard%2C%0Amaking%20it%20difficult%20for%20agents%20to%20replan%20at%20high%20rates.%20To%20overcome%20this%0Achallenge%2C%20we%20present%20our%20second%20key%20contribution%2C%20PRIMER%2C%20a%20learning-based%0Aplanner%20trained%20with%20imitation%20learning%20%28IL%29%20using%20PARM%2A%20as%20the%20expert%0Ademonstrator.%20PRIMER%20leverages%20the%20low%20computational%20requirements%20at%20deployment%0Aof%20neural%20networks%20and%20achieves%20a%20computation%20speed%20up%20to%205500%20times%20faster%0Athan%20optimization-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10060v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRIMER%253A%2520Perception-Aware%2520Robust%2520Learning-based%2520Multiagent%2520Trajectory%250A%2520%2520Planner%26entry.906535625%3DKota%2520Kondo%2520and%2520Claudius%2520T.%2520Tewari%2520and%2520Andrea%2520Tagliabue%2520and%2520Jesus%2520Tordesillas%2520and%2520Parker%2520C.%2520Lusk%2520and%2520Mason%2520B.%2520Peterson%2520and%2520Jonathan%2520P.%2520How%26entry.1292438233%3D%2520%2520In%2520decentralized%2520multiagent%2520trajectory%2520planners%252C%2520agents%2520need%2520to%2520communicate%250Aand%2520exchange%2520their%2520positions%2520to%2520generate%2520collision-free%2520trajectories.%2520However%252C%250Adue%2520to%2520localization%2520errors/uncertainties%252C%2520trajectory%2520deconfliction%2520can%2520fail%250Aeven%2520if%2520trajectories%2520are%2520perfectly%2520shared%2520between%2520agents.%2520To%2520address%2520this%250Aissue%252C%2520we%2520first%2520present%2520PARM%2520and%2520PARM%252A%252C%2520perception-aware%252C%2520decentralized%252C%250Aasynchronous%2520multiagent%2520trajectory%2520planners%2520that%2520enable%2520a%2520team%2520of%2520agents%2520to%250Anavigate%2520uncertain%2520environments%2520while%2520deconflicting%2520trajectories%2520and%2520avoiding%250Aobstacles%2520using%2520perception%2520information.%2520PARM%252A%2520differs%2520from%2520PARM%2520as%2520it%2520is%2520less%250Aconservative%252C%2520using%2520more%2520computation%2520to%2520find%2520closer-to-optimal%2520solutions.%2520While%250Athese%2520methods%2520achieve%2520state-of-the-art%2520performance%252C%2520they%2520suffer%2520from%2520high%250Acomputational%2520costs%2520as%2520they%2520need%2520to%2520solve%2520large%2520optimization%2520problems%2520onboard%252C%250Amaking%2520it%2520difficult%2520for%2520agents%2520to%2520replan%2520at%2520high%2520rates.%2520To%2520overcome%2520this%250Achallenge%252C%2520we%2520present%2520our%2520second%2520key%2520contribution%252C%2520PRIMER%252C%2520a%2520learning-based%250Aplanner%2520trained%2520with%2520imitation%2520learning%2520%2528IL%2529%2520using%2520PARM%252A%2520as%2520the%2520expert%250Ademonstrator.%2520PRIMER%2520leverages%2520the%2520low%2520computational%2520requirements%2520at%2520deployment%250Aof%2520neural%2520networks%2520and%2520achieves%2520a%2520computation%2520speed%2520up%2520to%25205500%2520times%2520faster%250Athan%2520optimization-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10060v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner&entry.906535625=Kota%20Kondo%20and%20Claudius%20T.%20Tewari%20and%20Andrea%20Tagliabue%20and%20Jesus%20Tordesillas%20and%20Parker%20C.%20Lusk%20and%20Mason%20B.%20Peterson%20and%20Jonathan%20P.%20How&entry.1292438233=%20%20In%20decentralized%20multiagent%20trajectory%20planners%2C%20agents%20need%20to%20communicate%0Aand%20exchange%20their%20positions%20to%20generate%20collision-free%20trajectories.%20However%2C%0Adue%20to%20localization%20errors/uncertainties%2C%20trajectory%20deconfliction%20can%20fail%0Aeven%20if%20trajectories%20are%20perfectly%20shared%20between%20agents.%20To%20address%20this%0Aissue%2C%20we%20first%20present%20PARM%20and%20PARM%2A%2C%20perception-aware%2C%20decentralized%2C%0Aasynchronous%20multiagent%20trajectory%20planners%20that%20enable%20a%20team%20of%20agents%20to%0Anavigate%20uncertain%20environments%20while%20deconflicting%20trajectories%20and%20avoiding%0Aobstacles%20using%20perception%20information.%20PARM%2A%20differs%20from%20PARM%20as%20it%20is%20less%0Aconservative%2C%20using%20more%20computation%20to%20find%20closer-to-optimal%20solutions.%20While%0Athese%20methods%20achieve%20state-of-the-art%20performance%2C%20they%20suffer%20from%20high%0Acomputational%20costs%20as%20they%20need%20to%20solve%20large%20optimization%20problems%20onboard%2C%0Amaking%20it%20difficult%20for%20agents%20to%20replan%20at%20high%20rates.%20To%20overcome%20this%0Achallenge%2C%20we%20present%20our%20second%20key%20contribution%2C%20PRIMER%2C%20a%20learning-based%0Aplanner%20trained%20with%20imitation%20learning%20%28IL%29%20using%20PARM%2A%20as%20the%20expert%0Ademonstrator.%20PRIMER%20leverages%20the%20low%20computational%20requirements%20at%20deployment%0Aof%20neural%20networks%20and%20achieves%20a%20computation%20speed%20up%20to%205500%20times%20faster%0Athan%20optimization-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10060v4&entry.124074799=Read"},
{"title": "Physics-informed neural networks viewpoint for solving the\n  Dyson-Schwinger equations of quantum electrodynamics", "author": "Rodrigo Carmo Terin", "abstract": "  Physics-informed neural networks (PINNs) are employed to solve the\nDyson--Schwinger equations of quantum electrodynamics (QED) in Euclidean space,\nwith a focus on the non-perturbative generation of the fermion's dynamical mass\nfunction in the Landau gauge. By inserting the integral equation directly into\nthe loss function, our PINN framework enables a single neural network to learn\na continuous and differentiable representation of the mass function over a\nspectrum of momenta. Also, we benchmark our approach against a traditional\nnumerical algorithm showing the main differences among them. Our novel\nstrategy, which is expected to be extended to other quantum field theories, is\nthe first step towards forefront applications of machine learning in high-level\ntheoretical physics.\n", "link": "http://arxiv.org/abs/2411.02177v3", "date": "2025-05-13", "relevancy": 2.2718, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4609}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4567}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-informed%20neural%20networks%20viewpoint%20for%20solving%20the%0A%20%20Dyson-Schwinger%20equations%20of%20quantum%20electrodynamics&body=Title%3A%20Physics-informed%20neural%20networks%20viewpoint%20for%20solving%20the%0A%20%20Dyson-Schwinger%20equations%20of%20quantum%20electrodynamics%0AAuthor%3A%20Rodrigo%20Carmo%20Terin%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20are%20employed%20to%20solve%20the%0ADyson--Schwinger%20equations%20of%20quantum%20electrodynamics%20%28QED%29%20in%20Euclidean%20space%2C%0Awith%20a%20focus%20on%20the%20non-perturbative%20generation%20of%20the%20fermion%27s%20dynamical%20mass%0Afunction%20in%20the%20Landau%20gauge.%20By%20inserting%20the%20integral%20equation%20directly%20into%0Athe%20loss%20function%2C%20our%20PINN%20framework%20enables%20a%20single%20neural%20network%20to%20learn%0Aa%20continuous%20and%20differentiable%20representation%20of%20the%20mass%20function%20over%20a%0Aspectrum%20of%20momenta.%20Also%2C%20we%20benchmark%20our%20approach%20against%20a%20traditional%0Anumerical%20algorithm%20showing%20the%20main%20differences%20among%20them.%20Our%20novel%0Astrategy%2C%20which%20is%20expected%20to%20be%20extended%20to%20other%20quantum%20field%20theories%2C%20is%0Athe%20first%20step%20towards%20forefront%20applications%20of%20machine%20learning%20in%20high-level%0Atheoretical%20physics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02177v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-informed%2520neural%2520networks%2520viewpoint%2520for%2520solving%2520the%250A%2520%2520Dyson-Schwinger%2520equations%2520of%2520quantum%2520electrodynamics%26entry.906535625%3DRodrigo%2520Carmo%2520Terin%26entry.1292438233%3D%2520%2520Physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520are%2520employed%2520to%2520solve%2520the%250ADyson--Schwinger%2520equations%2520of%2520quantum%2520electrodynamics%2520%2528QED%2529%2520in%2520Euclidean%2520space%252C%250Awith%2520a%2520focus%2520on%2520the%2520non-perturbative%2520generation%2520of%2520the%2520fermion%2527s%2520dynamical%2520mass%250Afunction%2520in%2520the%2520Landau%2520gauge.%2520By%2520inserting%2520the%2520integral%2520equation%2520directly%2520into%250Athe%2520loss%2520function%252C%2520our%2520PINN%2520framework%2520enables%2520a%2520single%2520neural%2520network%2520to%2520learn%250Aa%2520continuous%2520and%2520differentiable%2520representation%2520of%2520the%2520mass%2520function%2520over%2520a%250Aspectrum%2520of%2520momenta.%2520Also%252C%2520we%2520benchmark%2520our%2520approach%2520against%2520a%2520traditional%250Anumerical%2520algorithm%2520showing%2520the%2520main%2520differences%2520among%2520them.%2520Our%2520novel%250Astrategy%252C%2520which%2520is%2520expected%2520to%2520be%2520extended%2520to%2520other%2520quantum%2520field%2520theories%252C%2520is%250Athe%2520first%2520step%2520towards%2520forefront%2520applications%2520of%2520machine%2520learning%2520in%2520high-level%250Atheoretical%2520physics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02177v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-informed%20neural%20networks%20viewpoint%20for%20solving%20the%0A%20%20Dyson-Schwinger%20equations%20of%20quantum%20electrodynamics&entry.906535625=Rodrigo%20Carmo%20Terin&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20are%20employed%20to%20solve%20the%0ADyson--Schwinger%20equations%20of%20quantum%20electrodynamics%20%28QED%29%20in%20Euclidean%20space%2C%0Awith%20a%20focus%20on%20the%20non-perturbative%20generation%20of%20the%20fermion%27s%20dynamical%20mass%0Afunction%20in%20the%20Landau%20gauge.%20By%20inserting%20the%20integral%20equation%20directly%20into%0Athe%20loss%20function%2C%20our%20PINN%20framework%20enables%20a%20single%20neural%20network%20to%20learn%0Aa%20continuous%20and%20differentiable%20representation%20of%20the%20mass%20function%20over%20a%0Aspectrum%20of%20momenta.%20Also%2C%20we%20benchmark%20our%20approach%20against%20a%20traditional%0Anumerical%20algorithm%20showing%20the%20main%20differences%20among%20them.%20Our%20novel%0Astrategy%2C%20which%20is%20expected%20to%20be%20extended%20to%20other%20quantum%20field%20theories%2C%20is%0Athe%20first%20step%20towards%20forefront%20applications%20of%20machine%20learning%20in%20high-level%0Atheoretical%20physics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02177v3&entry.124074799=Read"},
{"title": "VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video\n  Generation", "author": "Wenhao Wang and Yi Yang", "abstract": "  Text-to-video generative models convert textual prompts into dynamic visual\ncontent, offering wide-ranging applications in film production, gaming, and\neducation. However, their real-world performance often falls short of user\nexpectations. One key reason is that these models have not been trained on\nvideos related to some topics users want to create. In this paper, we propose\nVideoUFO, the first Video dataset specifically curated to align with Users'\nFOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1)\nminimal (0.29%) overlap with existing video datasets, and (2) videos searched\nexclusively via YouTube's official API under the Creative Commons license.\nThese two attributes provide future researchers with greater freedom to broaden\ntheir training sources. The VideoUFO comprises over 1.09 million video clips,\neach paired with both a brief and a detailed caption (description).\nSpecifically, through clustering, we first identify 1,291 user-focused topics\nfrom the million-scale real text-to-video prompt dataset, VidProM. Then, we use\nthese topics to retrieve videos from YouTube, split the retrieved videos into\nclips, and generate both brief and detailed captions for each clip. After\nverifying the clips with specified topics, we are left with about 1.09 million\nvideo clips. Our experiments reveal that (1) current 16 text-to-video models do\nnot achieve consistent performance across all user-focused topics; and (2) a\nsimple model trained on VideoUFO outperforms others on worst-performing topics.\nThe dataset and code are publicly available at\nhttps://huggingface.co/datasets/WenhaoWang/VideoUFO and\nhttps://github.com/WangWenhao0716/BenchUFO under the CC BY 4.0 License.\n", "link": "http://arxiv.org/abs/2503.01739v2", "date": "2025-05-13", "relevancy": 2.2634, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.589}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoUFO%3A%20A%20Million-Scale%20User-Focused%20Dataset%20for%20Text-to-Video%0A%20%20Generation&body=Title%3A%20VideoUFO%3A%20A%20Million-Scale%20User-Focused%20Dataset%20for%20Text-to-Video%0A%20%20Generation%0AAuthor%3A%20Wenhao%20Wang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Text-to-video%20generative%20models%20convert%20textual%20prompts%20into%20dynamic%20visual%0Acontent%2C%20offering%20wide-ranging%20applications%20in%20film%20production%2C%20gaming%2C%20and%0Aeducation.%20However%2C%20their%20real-world%20performance%20often%20falls%20short%20of%20user%0Aexpectations.%20One%20key%20reason%20is%20that%20these%20models%20have%20not%20been%20trained%20on%0Avideos%20related%20to%20some%20topics%20users%20want%20to%20create.%20In%20this%20paper%2C%20we%20propose%0AVideoUFO%2C%20the%20first%20Video%20dataset%20specifically%20curated%20to%20align%20with%20Users%27%0AFOcus%20in%20real-world%20scenarios.%20Beyond%20this%2C%20our%20VideoUFO%20also%20features%3A%20%281%29%0Aminimal%20%280.29%25%29%20overlap%20with%20existing%20video%20datasets%2C%20and%20%282%29%20videos%20searched%0Aexclusively%20via%20YouTube%27s%20official%20API%20under%20the%20Creative%20Commons%20license.%0AThese%20two%20attributes%20provide%20future%20researchers%20with%20greater%20freedom%20to%20broaden%0Atheir%20training%20sources.%20The%20VideoUFO%20comprises%20over%201.09%20million%20video%20clips%2C%0Aeach%20paired%20with%20both%20a%20brief%20and%20a%20detailed%20caption%20%28description%29.%0ASpecifically%2C%20through%20clustering%2C%20we%20first%20identify%201%2C291%20user-focused%20topics%0Afrom%20the%20million-scale%20real%20text-to-video%20prompt%20dataset%2C%20VidProM.%20Then%2C%20we%20use%0Athese%20topics%20to%20retrieve%20videos%20from%20YouTube%2C%20split%20the%20retrieved%20videos%20into%0Aclips%2C%20and%20generate%20both%20brief%20and%20detailed%20captions%20for%20each%20clip.%20After%0Averifying%20the%20clips%20with%20specified%20topics%2C%20we%20are%20left%20with%20about%201.09%20million%0Avideo%20clips.%20Our%20experiments%20reveal%20that%20%281%29%20current%2016%20text-to-video%20models%20do%0Anot%20achieve%20consistent%20performance%20across%20all%20user-focused%20topics%3B%20and%20%282%29%20a%0Asimple%20model%20trained%20on%20VideoUFO%20outperforms%20others%20on%20worst-performing%20topics.%0AThe%20dataset%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//huggingface.co/datasets/WenhaoWang/VideoUFO%20and%0Ahttps%3A//github.com/WangWenhao0716/BenchUFO%20under%20the%20CC%20BY%204.0%20License.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01739v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoUFO%253A%2520A%2520Million-Scale%2520User-Focused%2520Dataset%2520for%2520Text-to-Video%250A%2520%2520Generation%26entry.906535625%3DWenhao%2520Wang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Text-to-video%2520generative%2520models%2520convert%2520textual%2520prompts%2520into%2520dynamic%2520visual%250Acontent%252C%2520offering%2520wide-ranging%2520applications%2520in%2520film%2520production%252C%2520gaming%252C%2520and%250Aeducation.%2520However%252C%2520their%2520real-world%2520performance%2520often%2520falls%2520short%2520of%2520user%250Aexpectations.%2520One%2520key%2520reason%2520is%2520that%2520these%2520models%2520have%2520not%2520been%2520trained%2520on%250Avideos%2520related%2520to%2520some%2520topics%2520users%2520want%2520to%2520create.%2520In%2520this%2520paper%252C%2520we%2520propose%250AVideoUFO%252C%2520the%2520first%2520Video%2520dataset%2520specifically%2520curated%2520to%2520align%2520with%2520Users%2527%250AFOcus%2520in%2520real-world%2520scenarios.%2520Beyond%2520this%252C%2520our%2520VideoUFO%2520also%2520features%253A%2520%25281%2529%250Aminimal%2520%25280.29%2525%2529%2520overlap%2520with%2520existing%2520video%2520datasets%252C%2520and%2520%25282%2529%2520videos%2520searched%250Aexclusively%2520via%2520YouTube%2527s%2520official%2520API%2520under%2520the%2520Creative%2520Commons%2520license.%250AThese%2520two%2520attributes%2520provide%2520future%2520researchers%2520with%2520greater%2520freedom%2520to%2520broaden%250Atheir%2520training%2520sources.%2520The%2520VideoUFO%2520comprises%2520over%25201.09%2520million%2520video%2520clips%252C%250Aeach%2520paired%2520with%2520both%2520a%2520brief%2520and%2520a%2520detailed%2520caption%2520%2528description%2529.%250ASpecifically%252C%2520through%2520clustering%252C%2520we%2520first%2520identify%25201%252C291%2520user-focused%2520topics%250Afrom%2520the%2520million-scale%2520real%2520text-to-video%2520prompt%2520dataset%252C%2520VidProM.%2520Then%252C%2520we%2520use%250Athese%2520topics%2520to%2520retrieve%2520videos%2520from%2520YouTube%252C%2520split%2520the%2520retrieved%2520videos%2520into%250Aclips%252C%2520and%2520generate%2520both%2520brief%2520and%2520detailed%2520captions%2520for%2520each%2520clip.%2520After%250Averifying%2520the%2520clips%2520with%2520specified%2520topics%252C%2520we%2520are%2520left%2520with%2520about%25201.09%2520million%250Avideo%2520clips.%2520Our%2520experiments%2520reveal%2520that%2520%25281%2529%2520current%252016%2520text-to-video%2520models%2520do%250Anot%2520achieve%2520consistent%2520performance%2520across%2520all%2520user-focused%2520topics%253B%2520and%2520%25282%2529%2520a%250Asimple%2520model%2520trained%2520on%2520VideoUFO%2520outperforms%2520others%2520on%2520worst-performing%2520topics.%250AThe%2520dataset%2520and%2520code%2520are%2520publicly%2520available%2520at%250Ahttps%253A//huggingface.co/datasets/WenhaoWang/VideoUFO%2520and%250Ahttps%253A//github.com/WangWenhao0716/BenchUFO%2520under%2520the%2520CC%2520BY%25204.0%2520License.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01739v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoUFO%3A%20A%20Million-Scale%20User-Focused%20Dataset%20for%20Text-to-Video%0A%20%20Generation&entry.906535625=Wenhao%20Wang%20and%20Yi%20Yang&entry.1292438233=%20%20Text-to-video%20generative%20models%20convert%20textual%20prompts%20into%20dynamic%20visual%0Acontent%2C%20offering%20wide-ranging%20applications%20in%20film%20production%2C%20gaming%2C%20and%0Aeducation.%20However%2C%20their%20real-world%20performance%20often%20falls%20short%20of%20user%0Aexpectations.%20One%20key%20reason%20is%20that%20these%20models%20have%20not%20been%20trained%20on%0Avideos%20related%20to%20some%20topics%20users%20want%20to%20create.%20In%20this%20paper%2C%20we%20propose%0AVideoUFO%2C%20the%20first%20Video%20dataset%20specifically%20curated%20to%20align%20with%20Users%27%0AFOcus%20in%20real-world%20scenarios.%20Beyond%20this%2C%20our%20VideoUFO%20also%20features%3A%20%281%29%0Aminimal%20%280.29%25%29%20overlap%20with%20existing%20video%20datasets%2C%20and%20%282%29%20videos%20searched%0Aexclusively%20via%20YouTube%27s%20official%20API%20under%20the%20Creative%20Commons%20license.%0AThese%20two%20attributes%20provide%20future%20researchers%20with%20greater%20freedom%20to%20broaden%0Atheir%20training%20sources.%20The%20VideoUFO%20comprises%20over%201.09%20million%20video%20clips%2C%0Aeach%20paired%20with%20both%20a%20brief%20and%20a%20detailed%20caption%20%28description%29.%0ASpecifically%2C%20through%20clustering%2C%20we%20first%20identify%201%2C291%20user-focused%20topics%0Afrom%20the%20million-scale%20real%20text-to-video%20prompt%20dataset%2C%20VidProM.%20Then%2C%20we%20use%0Athese%20topics%20to%20retrieve%20videos%20from%20YouTube%2C%20split%20the%20retrieved%20videos%20into%0Aclips%2C%20and%20generate%20both%20brief%20and%20detailed%20captions%20for%20each%20clip.%20After%0Averifying%20the%20clips%20with%20specified%20topics%2C%20we%20are%20left%20with%20about%201.09%20million%0Avideo%20clips.%20Our%20experiments%20reveal%20that%20%281%29%20current%2016%20text-to-video%20models%20do%0Anot%20achieve%20consistent%20performance%20across%20all%20user-focused%20topics%3B%20and%20%282%29%20a%0Asimple%20model%20trained%20on%20VideoUFO%20outperforms%20others%20on%20worst-performing%20topics.%0AThe%20dataset%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//huggingface.co/datasets/WenhaoWang/VideoUFO%20and%0Ahttps%3A//github.com/WangWenhao0716/BenchUFO%20under%20the%20CC%20BY%204.0%20License.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01739v2&entry.124074799=Read"},
{"title": "Symbolically-Guided Visual Plan Inference from Uncurated Video Data", "author": "Wenyan Yang and Ahmet Tikna and Yi Zhao and Yuying Zhang and Luigi Palopoli and Marco Roveri and Joni Pajarinen", "abstract": "  Visual planning, by offering a sequence of intermediate visual subgoals to a\ngoal-conditioned low-level policy, achieves promising performance on\nlong-horizon manipulation tasks. To obtain the subgoals, existing methods\ntypically resort to video generation models but suffer from model hallucination\nand computational cost. We present Vis2Plan, an efficient, explainable and\nwhite-box visual planning framework powered by symbolic guidance. From raw,\nunlabeled play data, Vis2Plan harnesses vision foundation models to\nautomatically extract a compact set of task symbols, which allows building a\nhigh-level symbolic transition graph for multi-goal, multi-stage planning. At\ntest time, given a desired task goal, our planner conducts planning at the\nsymbolic level and assembles a sequence of physically consistent intermediate\nsub-goal images grounded by the underlying symbolic representation. Our\nVis2Plan outperforms strong diffusion video generation-based visual planners by\ndelivering 53\\% higher aggregate success rate in real robot settings while\ngenerating visual plans 35$\\times$ faster. The results indicate that Vis2Plan\nis able to generate physically consistent image goals while offering fully\ninspectable reasoning steps.\n", "link": "http://arxiv.org/abs/2505.08444v1", "date": "2025-05-13", "relevancy": 2.2464, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symbolically-Guided%20Visual%20Plan%20Inference%20from%20Uncurated%20Video%20Data&body=Title%3A%20Symbolically-Guided%20Visual%20Plan%20Inference%20from%20Uncurated%20Video%20Data%0AAuthor%3A%20Wenyan%20Yang%20and%20Ahmet%20Tikna%20and%20Yi%20Zhao%20and%20Yuying%20Zhang%20and%20Luigi%20Palopoli%20and%20Marco%20Roveri%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Visual%20planning%2C%20by%20offering%20a%20sequence%20of%20intermediate%20visual%20subgoals%20to%20a%0Agoal-conditioned%20low-level%20policy%2C%20achieves%20promising%20performance%20on%0Along-horizon%20manipulation%20tasks.%20To%20obtain%20the%20subgoals%2C%20existing%20methods%0Atypically%20resort%20to%20video%20generation%20models%20but%20suffer%20from%20model%20hallucination%0Aand%20computational%20cost.%20We%20present%20Vis2Plan%2C%20an%20efficient%2C%20explainable%20and%0Awhite-box%20visual%20planning%20framework%20powered%20by%20symbolic%20guidance.%20From%20raw%2C%0Aunlabeled%20play%20data%2C%20Vis2Plan%20harnesses%20vision%20foundation%20models%20to%0Aautomatically%20extract%20a%20compact%20set%20of%20task%20symbols%2C%20which%20allows%20building%20a%0Ahigh-level%20symbolic%20transition%20graph%20for%20multi-goal%2C%20multi-stage%20planning.%20At%0Atest%20time%2C%20given%20a%20desired%20task%20goal%2C%20our%20planner%20conducts%20planning%20at%20the%0Asymbolic%20level%20and%20assembles%20a%20sequence%20of%20physically%20consistent%20intermediate%0Asub-goal%20images%20grounded%20by%20the%20underlying%20symbolic%20representation.%20Our%0AVis2Plan%20outperforms%20strong%20diffusion%20video%20generation-based%20visual%20planners%20by%0Adelivering%2053%5C%25%20higher%20aggregate%20success%20rate%20in%20real%20robot%20settings%20while%0Agenerating%20visual%20plans%2035%24%5Ctimes%24%20faster.%20The%20results%20indicate%20that%20Vis2Plan%0Ais%20able%20to%20generate%20physically%20consistent%20image%20goals%20while%20offering%20fully%0Ainspectable%20reasoning%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymbolically-Guided%2520Visual%2520Plan%2520Inference%2520from%2520Uncurated%2520Video%2520Data%26entry.906535625%3DWenyan%2520Yang%2520and%2520Ahmet%2520Tikna%2520and%2520Yi%2520Zhao%2520and%2520Yuying%2520Zhang%2520and%2520Luigi%2520Palopoli%2520and%2520Marco%2520Roveri%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Visual%2520planning%252C%2520by%2520offering%2520a%2520sequence%2520of%2520intermediate%2520visual%2520subgoals%2520to%2520a%250Agoal-conditioned%2520low-level%2520policy%252C%2520achieves%2520promising%2520performance%2520on%250Along-horizon%2520manipulation%2520tasks.%2520To%2520obtain%2520the%2520subgoals%252C%2520existing%2520methods%250Atypically%2520resort%2520to%2520video%2520generation%2520models%2520but%2520suffer%2520from%2520model%2520hallucination%250Aand%2520computational%2520cost.%2520We%2520present%2520Vis2Plan%252C%2520an%2520efficient%252C%2520explainable%2520and%250Awhite-box%2520visual%2520planning%2520framework%2520powered%2520by%2520symbolic%2520guidance.%2520From%2520raw%252C%250Aunlabeled%2520play%2520data%252C%2520Vis2Plan%2520harnesses%2520vision%2520foundation%2520models%2520to%250Aautomatically%2520extract%2520a%2520compact%2520set%2520of%2520task%2520symbols%252C%2520which%2520allows%2520building%2520a%250Ahigh-level%2520symbolic%2520transition%2520graph%2520for%2520multi-goal%252C%2520multi-stage%2520planning.%2520At%250Atest%2520time%252C%2520given%2520a%2520desired%2520task%2520goal%252C%2520our%2520planner%2520conducts%2520planning%2520at%2520the%250Asymbolic%2520level%2520and%2520assembles%2520a%2520sequence%2520of%2520physically%2520consistent%2520intermediate%250Asub-goal%2520images%2520grounded%2520by%2520the%2520underlying%2520symbolic%2520representation.%2520Our%250AVis2Plan%2520outperforms%2520strong%2520diffusion%2520video%2520generation-based%2520visual%2520planners%2520by%250Adelivering%252053%255C%2525%2520higher%2520aggregate%2520success%2520rate%2520in%2520real%2520robot%2520settings%2520while%250Agenerating%2520visual%2520plans%252035%2524%255Ctimes%2524%2520faster.%2520The%2520results%2520indicate%2520that%2520Vis2Plan%250Ais%2520able%2520to%2520generate%2520physically%2520consistent%2520image%2520goals%2520while%2520offering%2520fully%250Ainspectable%2520reasoning%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symbolically-Guided%20Visual%20Plan%20Inference%20from%20Uncurated%20Video%20Data&entry.906535625=Wenyan%20Yang%20and%20Ahmet%20Tikna%20and%20Yi%20Zhao%20and%20Yuying%20Zhang%20and%20Luigi%20Palopoli%20and%20Marco%20Roveri%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Visual%20planning%2C%20by%20offering%20a%20sequence%20of%20intermediate%20visual%20subgoals%20to%20a%0Agoal-conditioned%20low-level%20policy%2C%20achieves%20promising%20performance%20on%0Along-horizon%20manipulation%20tasks.%20To%20obtain%20the%20subgoals%2C%20existing%20methods%0Atypically%20resort%20to%20video%20generation%20models%20but%20suffer%20from%20model%20hallucination%0Aand%20computational%20cost.%20We%20present%20Vis2Plan%2C%20an%20efficient%2C%20explainable%20and%0Awhite-box%20visual%20planning%20framework%20powered%20by%20symbolic%20guidance.%20From%20raw%2C%0Aunlabeled%20play%20data%2C%20Vis2Plan%20harnesses%20vision%20foundation%20models%20to%0Aautomatically%20extract%20a%20compact%20set%20of%20task%20symbols%2C%20which%20allows%20building%20a%0Ahigh-level%20symbolic%20transition%20graph%20for%20multi-goal%2C%20multi-stage%20planning.%20At%0Atest%20time%2C%20given%20a%20desired%20task%20goal%2C%20our%20planner%20conducts%20planning%20at%20the%0Asymbolic%20level%20and%20assembles%20a%20sequence%20of%20physically%20consistent%20intermediate%0Asub-goal%20images%20grounded%20by%20the%20underlying%20symbolic%20representation.%20Our%0AVis2Plan%20outperforms%20strong%20diffusion%20video%20generation-based%20visual%20planners%20by%0Adelivering%2053%5C%25%20higher%20aggregate%20success%20rate%20in%20real%20robot%20settings%20while%0Agenerating%20visual%20plans%2035%24%5Ctimes%24%20faster.%20The%20results%20indicate%20that%20Vis2Plan%0Ais%20able%20to%20generate%20physically%20consistent%20image%20goals%20while%20offering%20fully%0Ainspectable%20reasoning%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08444v1&entry.124074799=Read"},
{"title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation", "author": "Edoardo Bianchi and Antonio Liotta", "abstract": "  Assessing human skill levels in complex activities is a challenging problem\nwith applications in sports, rehabilitation, and training. In this work, we\npresent SkillFormer, a parameter-efficient architecture for unified multi-view\nproficiency estimation from egocentric and exocentric videos. Building on the\nTimeSformer backbone, SkillFormer introduces a CrossViewFusion module that\nfuses view-specific features using multi-head cross-attention, learnable\ngating, and adaptive self-calibration. We leverage Low-Rank Adaptation to\nfine-tune only a small subset of parameters, significantly reducing training\ncosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves\nstate-of-the-art accuracy in multi-view settings while demonstrating remarkable\ncomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer\ntraining epochs than prior baselines. It excels in multiple structured tasks,\nconfirming the value of multi-view integration for fine-grained skill\nassessment.\n", "link": "http://arxiv.org/abs/2505.08665v1", "date": "2025-05-13", "relevancy": 2.2282, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkillFormer%3A%20Unified%20Multi-View%20Video%20Understanding%20for%20Proficiency%0A%20%20Estimation&body=Title%3A%20SkillFormer%3A%20Unified%20Multi-View%20Video%20Understanding%20for%20Proficiency%0A%20%20Estimation%0AAuthor%3A%20Edoardo%20Bianchi%20and%20Antonio%20Liotta%0AAbstract%3A%20%20%20Assessing%20human%20skill%20levels%20in%20complex%20activities%20is%20a%20challenging%20problem%0Awith%20applications%20in%20sports%2C%20rehabilitation%2C%20and%20training.%20In%20this%20work%2C%20we%0Apresent%20SkillFormer%2C%20a%20parameter-efficient%20architecture%20for%20unified%20multi-view%0Aproficiency%20estimation%20from%20egocentric%20and%20exocentric%20videos.%20Building%20on%20the%0ATimeSformer%20backbone%2C%20SkillFormer%20introduces%20a%20CrossViewFusion%20module%20that%0Afuses%20view-specific%20features%20using%20multi-head%20cross-attention%2C%20learnable%0Agating%2C%20and%20adaptive%20self-calibration.%20We%20leverage%20Low-Rank%20Adaptation%20to%0Afine-tune%20only%20a%20small%20subset%20of%20parameters%2C%20significantly%20reducing%20training%0Acosts.%20In%20fact%2C%20when%20evaluated%20on%20the%20EgoExo4D%20dataset%2C%20SkillFormer%20achieves%0Astate-of-the-art%20accuracy%20in%20multi-view%20settings%20while%20demonstrating%20remarkable%0Acomputational%20efficiency%2C%20using%204.5x%20fewer%20parameters%20and%20requiring%203.75x%20fewer%0Atraining%20epochs%20than%20prior%20baselines.%20It%20excels%20in%20multiple%20structured%20tasks%2C%0Aconfirming%20the%20value%20of%20multi-view%20integration%20for%20fine-grained%20skill%0Aassessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkillFormer%253A%2520Unified%2520Multi-View%2520Video%2520Understanding%2520for%2520Proficiency%250A%2520%2520Estimation%26entry.906535625%3DEdoardo%2520Bianchi%2520and%2520Antonio%2520Liotta%26entry.1292438233%3D%2520%2520Assessing%2520human%2520skill%2520levels%2520in%2520complex%2520activities%2520is%2520a%2520challenging%2520problem%250Awith%2520applications%2520in%2520sports%252C%2520rehabilitation%252C%2520and%2520training.%2520In%2520this%2520work%252C%2520we%250Apresent%2520SkillFormer%252C%2520a%2520parameter-efficient%2520architecture%2520for%2520unified%2520multi-view%250Aproficiency%2520estimation%2520from%2520egocentric%2520and%2520exocentric%2520videos.%2520Building%2520on%2520the%250ATimeSformer%2520backbone%252C%2520SkillFormer%2520introduces%2520a%2520CrossViewFusion%2520module%2520that%250Afuses%2520view-specific%2520features%2520using%2520multi-head%2520cross-attention%252C%2520learnable%250Agating%252C%2520and%2520adaptive%2520self-calibration.%2520We%2520leverage%2520Low-Rank%2520Adaptation%2520to%250Afine-tune%2520only%2520a%2520small%2520subset%2520of%2520parameters%252C%2520significantly%2520reducing%2520training%250Acosts.%2520In%2520fact%252C%2520when%2520evaluated%2520on%2520the%2520EgoExo4D%2520dataset%252C%2520SkillFormer%2520achieves%250Astate-of-the-art%2520accuracy%2520in%2520multi-view%2520settings%2520while%2520demonstrating%2520remarkable%250Acomputational%2520efficiency%252C%2520using%25204.5x%2520fewer%2520parameters%2520and%2520requiring%25203.75x%2520fewer%250Atraining%2520epochs%2520than%2520prior%2520baselines.%2520It%2520excels%2520in%2520multiple%2520structured%2520tasks%252C%250Aconfirming%2520the%2520value%2520of%2520multi-view%2520integration%2520for%2520fine-grained%2520skill%250Aassessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkillFormer%3A%20Unified%20Multi-View%20Video%20Understanding%20for%20Proficiency%0A%20%20Estimation&entry.906535625=Edoardo%20Bianchi%20and%20Antonio%20Liotta&entry.1292438233=%20%20Assessing%20human%20skill%20levels%20in%20complex%20activities%20is%20a%20challenging%20problem%0Awith%20applications%20in%20sports%2C%20rehabilitation%2C%20and%20training.%20In%20this%20work%2C%20we%0Apresent%20SkillFormer%2C%20a%20parameter-efficient%20architecture%20for%20unified%20multi-view%0Aproficiency%20estimation%20from%20egocentric%20and%20exocentric%20videos.%20Building%20on%20the%0ATimeSformer%20backbone%2C%20SkillFormer%20introduces%20a%20CrossViewFusion%20module%20that%0Afuses%20view-specific%20features%20using%20multi-head%20cross-attention%2C%20learnable%0Agating%2C%20and%20adaptive%20self-calibration.%20We%20leverage%20Low-Rank%20Adaptation%20to%0Afine-tune%20only%20a%20small%20subset%20of%20parameters%2C%20significantly%20reducing%20training%0Acosts.%20In%20fact%2C%20when%20evaluated%20on%20the%20EgoExo4D%20dataset%2C%20SkillFormer%20achieves%0Astate-of-the-art%20accuracy%20in%20multi-view%20settings%20while%20demonstrating%20remarkable%0Acomputational%20efficiency%2C%20using%204.5x%20fewer%20parameters%20and%20requiring%203.75x%20fewer%0Atraining%20epochs%20than%20prior%20baselines.%20It%20excels%20in%20multiple%20structured%20tasks%2C%0Aconfirming%20the%20value%20of%20multi-view%20integration%20for%20fine-grained%20skill%0Aassessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08665v1&entry.124074799=Read"},
{"title": "Calibration and Uncertainty for multiRater Volume Assessment in\n  multiorgan Segmentation (CURVAS) challenge results", "author": "Meritxell Riera-Marin and Sikha O K and Julia Rodriguez-Comas and Matthias Stefan May and Zhaohong Pan and Xiang Zhou and Xiaokun Liang and Franciskus Xaverius Erick and Andrea Prenner and Cedric Hemon and Valentin Boussot and Jean-Louis Dillenseger and Jean-Claude Nunes and Abdul Qayyum and Moona Mazher and Steven A Niederer and Kaisar Kushibar and Carlos Martin-Isla and Petia Radeva and Karim Lekadir and Theodore Barfoot and Luis C. Garcia Peraza Herrera and Ben Glocker and Tom Vercauteren and Lucas Gago and Justin Englemann and Joy-Marie Kleiss and Anton Aubanell and Andreu Antolin and Javier Garcia-Lopez and Miguel A. Gonzalez Ballester and Adrian Galdran", "abstract": "  Deep learning (DL) has become the dominant approach for medical image\nsegmentation, yet ensuring the reliability and clinical applicability of these\nmodels requires addressing key challenges such as annotation variability,\ncalibration, and uncertainty estimation. This is why we created the Calibration\nand Uncertainty for multiRater Volume Assessment in multiorgan Segmentation\n(CURVAS), which highlights the critical role of multiple annotators in\nestablishing a more comprehensive ground truth, emphasizing that segmentation\nis inherently subjective and that leveraging inter-annotator variability is\nessential for robust model evaluation. Seven teams participated in the\nchallenge, submitting a variety of DL models evaluated using metrics such as\nDice Similarity Coefficient (DSC), Expected Calibration Error (ECE), and\nContinuous Ranked Probability Score (CRPS). By incorporating consensus and\ndissensus ground truth, we assess how DL models handle uncertainty and whether\ntheir confidence estimates align with true segmentation performance. Our\nfindings reinforce the importance of well-calibrated models, as better\ncalibration is strongly correlated with the quality of the results.\nFurthermore, we demonstrate that segmentation models trained on diverse\ndatasets and enriched with pre-trained knowledge exhibit greater robustness,\nparticularly in cases deviating from standard anatomical structures. Notably,\nthe best-performing models achieved high DSC and well-calibrated uncertainty\nestimates. This work underscores the need for multi-annotator ground truth,\nthorough calibration assessments, and uncertainty-aware evaluations to develop\ntrustworthy and clinically reliable DL-based medical image segmentation models.\n", "link": "http://arxiv.org/abs/2505.08685v1", "date": "2025-05-13", "relevancy": 2.2281, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6442}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibration%20and%20Uncertainty%20for%20multiRater%20Volume%20Assessment%20in%0A%20%20multiorgan%20Segmentation%20%28CURVAS%29%20challenge%20results&body=Title%3A%20Calibration%20and%20Uncertainty%20for%20multiRater%20Volume%20Assessment%20in%0A%20%20multiorgan%20Segmentation%20%28CURVAS%29%20challenge%20results%0AAuthor%3A%20Meritxell%20Riera-Marin%20and%20Sikha%20O%20K%20and%20Julia%20Rodriguez-Comas%20and%20Matthias%20Stefan%20May%20and%20Zhaohong%20Pan%20and%20Xiang%20Zhou%20and%20Xiaokun%20Liang%20and%20Franciskus%20Xaverius%20Erick%20and%20Andrea%20Prenner%20and%20Cedric%20Hemon%20and%20Valentin%20Boussot%20and%20Jean-Louis%20Dillenseger%20and%20Jean-Claude%20Nunes%20and%20Abdul%20Qayyum%20and%20Moona%20Mazher%20and%20Steven%20A%20Niederer%20and%20Kaisar%20Kushibar%20and%20Carlos%20Martin-Isla%20and%20Petia%20Radeva%20and%20Karim%20Lekadir%20and%20Theodore%20Barfoot%20and%20Luis%20C.%20Garcia%20Peraza%20Herrera%20and%20Ben%20Glocker%20and%20Tom%20Vercauteren%20and%20Lucas%20Gago%20and%20Justin%20Englemann%20and%20Joy-Marie%20Kleiss%20and%20Anton%20Aubanell%20and%20Andreu%20Antolin%20and%20Javier%20Garcia-Lopez%20and%20Miguel%20A.%20Gonzalez%20Ballester%20and%20Adrian%20Galdran%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20has%20become%20the%20dominant%20approach%20for%20medical%20image%0Asegmentation%2C%20yet%20ensuring%20the%20reliability%20and%20clinical%20applicability%20of%20these%0Amodels%20requires%20addressing%20key%20challenges%20such%20as%20annotation%20variability%2C%0Acalibration%2C%20and%20uncertainty%20estimation.%20This%20is%20why%20we%20created%20the%20Calibration%0Aand%20Uncertainty%20for%20multiRater%20Volume%20Assessment%20in%20multiorgan%20Segmentation%0A%28CURVAS%29%2C%20which%20highlights%20the%20critical%20role%20of%20multiple%20annotators%20in%0Aestablishing%20a%20more%20comprehensive%20ground%20truth%2C%20emphasizing%20that%20segmentation%0Ais%20inherently%20subjective%20and%20that%20leveraging%20inter-annotator%20variability%20is%0Aessential%20for%20robust%20model%20evaluation.%20Seven%20teams%20participated%20in%20the%0Achallenge%2C%20submitting%20a%20variety%20of%20DL%20models%20evaluated%20using%20metrics%20such%20as%0ADice%20Similarity%20Coefficient%20%28DSC%29%2C%20Expected%20Calibration%20Error%20%28ECE%29%2C%20and%0AContinuous%20Ranked%20Probability%20Score%20%28CRPS%29.%20By%20incorporating%20consensus%20and%0Adissensus%20ground%20truth%2C%20we%20assess%20how%20DL%20models%20handle%20uncertainty%20and%20whether%0Atheir%20confidence%20estimates%20align%20with%20true%20segmentation%20performance.%20Our%0Afindings%20reinforce%20the%20importance%20of%20well-calibrated%20models%2C%20as%20better%0Acalibration%20is%20strongly%20correlated%20with%20the%20quality%20of%20the%20results.%0AFurthermore%2C%20we%20demonstrate%20that%20segmentation%20models%20trained%20on%20diverse%0Adatasets%20and%20enriched%20with%20pre-trained%20knowledge%20exhibit%20greater%20robustness%2C%0Aparticularly%20in%20cases%20deviating%20from%20standard%20anatomical%20structures.%20Notably%2C%0Athe%20best-performing%20models%20achieved%20high%20DSC%20and%20well-calibrated%20uncertainty%0Aestimates.%20This%20work%20underscores%20the%20need%20for%20multi-annotator%20ground%20truth%2C%0Athorough%20calibration%20assessments%2C%20and%20uncertainty-aware%20evaluations%20to%20develop%0Atrustworthy%20and%20clinically%20reliable%20DL-based%20medical%20image%20segmentation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibration%2520and%2520Uncertainty%2520for%2520multiRater%2520Volume%2520Assessment%2520in%250A%2520%2520multiorgan%2520Segmentation%2520%2528CURVAS%2529%2520challenge%2520results%26entry.906535625%3DMeritxell%2520Riera-Marin%2520and%2520Sikha%2520O%2520K%2520and%2520Julia%2520Rodriguez-Comas%2520and%2520Matthias%2520Stefan%2520May%2520and%2520Zhaohong%2520Pan%2520and%2520Xiang%2520Zhou%2520and%2520Xiaokun%2520Liang%2520and%2520Franciskus%2520Xaverius%2520Erick%2520and%2520Andrea%2520Prenner%2520and%2520Cedric%2520Hemon%2520and%2520Valentin%2520Boussot%2520and%2520Jean-Louis%2520Dillenseger%2520and%2520Jean-Claude%2520Nunes%2520and%2520Abdul%2520Qayyum%2520and%2520Moona%2520Mazher%2520and%2520Steven%2520A%2520Niederer%2520and%2520Kaisar%2520Kushibar%2520and%2520Carlos%2520Martin-Isla%2520and%2520Petia%2520Radeva%2520and%2520Karim%2520Lekadir%2520and%2520Theodore%2520Barfoot%2520and%2520Luis%2520C.%2520Garcia%2520Peraza%2520Herrera%2520and%2520Ben%2520Glocker%2520and%2520Tom%2520Vercauteren%2520and%2520Lucas%2520Gago%2520and%2520Justin%2520Englemann%2520and%2520Joy-Marie%2520Kleiss%2520and%2520Anton%2520Aubanell%2520and%2520Andreu%2520Antolin%2520and%2520Javier%2520Garcia-Lopez%2520and%2520Miguel%2520A.%2520Gonzalez%2520Ballester%2520and%2520Adrian%2520Galdran%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520has%2520become%2520the%2520dominant%2520approach%2520for%2520medical%2520image%250Asegmentation%252C%2520yet%2520ensuring%2520the%2520reliability%2520and%2520clinical%2520applicability%2520of%2520these%250Amodels%2520requires%2520addressing%2520key%2520challenges%2520such%2520as%2520annotation%2520variability%252C%250Acalibration%252C%2520and%2520uncertainty%2520estimation.%2520This%2520is%2520why%2520we%2520created%2520the%2520Calibration%250Aand%2520Uncertainty%2520for%2520multiRater%2520Volume%2520Assessment%2520in%2520multiorgan%2520Segmentation%250A%2528CURVAS%2529%252C%2520which%2520highlights%2520the%2520critical%2520role%2520of%2520multiple%2520annotators%2520in%250Aestablishing%2520a%2520more%2520comprehensive%2520ground%2520truth%252C%2520emphasizing%2520that%2520segmentation%250Ais%2520inherently%2520subjective%2520and%2520that%2520leveraging%2520inter-annotator%2520variability%2520is%250Aessential%2520for%2520robust%2520model%2520evaluation.%2520Seven%2520teams%2520participated%2520in%2520the%250Achallenge%252C%2520submitting%2520a%2520variety%2520of%2520DL%2520models%2520evaluated%2520using%2520metrics%2520such%2520as%250ADice%2520Similarity%2520Coefficient%2520%2528DSC%2529%252C%2520Expected%2520Calibration%2520Error%2520%2528ECE%2529%252C%2520and%250AContinuous%2520Ranked%2520Probability%2520Score%2520%2528CRPS%2529.%2520By%2520incorporating%2520consensus%2520and%250Adissensus%2520ground%2520truth%252C%2520we%2520assess%2520how%2520DL%2520models%2520handle%2520uncertainty%2520and%2520whether%250Atheir%2520confidence%2520estimates%2520align%2520with%2520true%2520segmentation%2520performance.%2520Our%250Afindings%2520reinforce%2520the%2520importance%2520of%2520well-calibrated%2520models%252C%2520as%2520better%250Acalibration%2520is%2520strongly%2520correlated%2520with%2520the%2520quality%2520of%2520the%2520results.%250AFurthermore%252C%2520we%2520demonstrate%2520that%2520segmentation%2520models%2520trained%2520on%2520diverse%250Adatasets%2520and%2520enriched%2520with%2520pre-trained%2520knowledge%2520exhibit%2520greater%2520robustness%252C%250Aparticularly%2520in%2520cases%2520deviating%2520from%2520standard%2520anatomical%2520structures.%2520Notably%252C%250Athe%2520best-performing%2520models%2520achieved%2520high%2520DSC%2520and%2520well-calibrated%2520uncertainty%250Aestimates.%2520This%2520work%2520underscores%2520the%2520need%2520for%2520multi-annotator%2520ground%2520truth%252C%250Athorough%2520calibration%2520assessments%252C%2520and%2520uncertainty-aware%2520evaluations%2520to%2520develop%250Atrustworthy%2520and%2520clinically%2520reliable%2520DL-based%2520medical%2520image%2520segmentation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibration%20and%20Uncertainty%20for%20multiRater%20Volume%20Assessment%20in%0A%20%20multiorgan%20Segmentation%20%28CURVAS%29%20challenge%20results&entry.906535625=Meritxell%20Riera-Marin%20and%20Sikha%20O%20K%20and%20Julia%20Rodriguez-Comas%20and%20Matthias%20Stefan%20May%20and%20Zhaohong%20Pan%20and%20Xiang%20Zhou%20and%20Xiaokun%20Liang%20and%20Franciskus%20Xaverius%20Erick%20and%20Andrea%20Prenner%20and%20Cedric%20Hemon%20and%20Valentin%20Boussot%20and%20Jean-Louis%20Dillenseger%20and%20Jean-Claude%20Nunes%20and%20Abdul%20Qayyum%20and%20Moona%20Mazher%20and%20Steven%20A%20Niederer%20and%20Kaisar%20Kushibar%20and%20Carlos%20Martin-Isla%20and%20Petia%20Radeva%20and%20Karim%20Lekadir%20and%20Theodore%20Barfoot%20and%20Luis%20C.%20Garcia%20Peraza%20Herrera%20and%20Ben%20Glocker%20and%20Tom%20Vercauteren%20and%20Lucas%20Gago%20and%20Justin%20Englemann%20and%20Joy-Marie%20Kleiss%20and%20Anton%20Aubanell%20and%20Andreu%20Antolin%20and%20Javier%20Garcia-Lopez%20and%20Miguel%20A.%20Gonzalez%20Ballester%20and%20Adrian%20Galdran&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20has%20become%20the%20dominant%20approach%20for%20medical%20image%0Asegmentation%2C%20yet%20ensuring%20the%20reliability%20and%20clinical%20applicability%20of%20these%0Amodels%20requires%20addressing%20key%20challenges%20such%20as%20annotation%20variability%2C%0Acalibration%2C%20and%20uncertainty%20estimation.%20This%20is%20why%20we%20created%20the%20Calibration%0Aand%20Uncertainty%20for%20multiRater%20Volume%20Assessment%20in%20multiorgan%20Segmentation%0A%28CURVAS%29%2C%20which%20highlights%20the%20critical%20role%20of%20multiple%20annotators%20in%0Aestablishing%20a%20more%20comprehensive%20ground%20truth%2C%20emphasizing%20that%20segmentation%0Ais%20inherently%20subjective%20and%20that%20leveraging%20inter-annotator%20variability%20is%0Aessential%20for%20robust%20model%20evaluation.%20Seven%20teams%20participated%20in%20the%0Achallenge%2C%20submitting%20a%20variety%20of%20DL%20models%20evaluated%20using%20metrics%20such%20as%0ADice%20Similarity%20Coefficient%20%28DSC%29%2C%20Expected%20Calibration%20Error%20%28ECE%29%2C%20and%0AContinuous%20Ranked%20Probability%20Score%20%28CRPS%29.%20By%20incorporating%20consensus%20and%0Adissensus%20ground%20truth%2C%20we%20assess%20how%20DL%20models%20handle%20uncertainty%20and%20whether%0Atheir%20confidence%20estimates%20align%20with%20true%20segmentation%20performance.%20Our%0Afindings%20reinforce%20the%20importance%20of%20well-calibrated%20models%2C%20as%20better%0Acalibration%20is%20strongly%20correlated%20with%20the%20quality%20of%20the%20results.%0AFurthermore%2C%20we%20demonstrate%20that%20segmentation%20models%20trained%20on%20diverse%0Adatasets%20and%20enriched%20with%20pre-trained%20knowledge%20exhibit%20greater%20robustness%2C%0Aparticularly%20in%20cases%20deviating%20from%20standard%20anatomical%20structures.%20Notably%2C%0Athe%20best-performing%20models%20achieved%20high%20DSC%20and%20well-calibrated%20uncertainty%0Aestimates.%20This%20work%20underscores%20the%20need%20for%20multi-annotator%20ground%20truth%2C%0Athorough%20calibration%20assessments%2C%20and%20uncertainty-aware%20evaluations%20to%20develop%0Atrustworthy%20and%20clinically%20reliable%20DL-based%20medical%20image%20segmentation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08685v1&entry.124074799=Read"},
{"title": "Efficient Adaptation For Remote Sensing Visual Grounding", "author": "Hasan Moughnieh and Mohamad Chalhoub and Hasan Nasrallah and Cristiano Nattero and Paolo Campanella and Giovanni Nico and Ali J. Ghandour", "abstract": "  Adapting pre-trained models has become an effective strategy in artificial\nintelligence, offering a scalable and efficient alternative to training models\nfrom scratch. In the context of remote sensing (RS), where visual grounding(VG)\nremains underexplored, this approach enables the deployment of powerful\nvision-language models to achieve robust cross-modal understanding while\nsignificantly reducing computational overhead. To address this, we applied\nParameter Efficient Fine Tuning (PEFT) techniques to adapt these models for\nRS-specific VG tasks. Specifically, we evaluated LoRA placement across\ndifferent modules in Grounding DINO and used BitFit and adapters to fine-tune\nthe OFA foundation model pre-trained on general-purpose VG datasets. This\napproach achieved performance comparable to or surpassing current State Of The\nArt (SOTA) models while significantly reducing computational costs. This study\nhighlights the potential of PEFT techniques to advance efficient and precise\nmulti-modal analysis in RS, offering a practical and cost-effective alternative\nto full model training.\n", "link": "http://arxiv.org/abs/2503.23083v2", "date": "2025-05-13", "relevancy": 2.2219, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5608}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Adaptation%20For%20Remote%20Sensing%20Visual%20Grounding&body=Title%3A%20Efficient%20Adaptation%20For%20Remote%20Sensing%20Visual%20Grounding%0AAuthor%3A%20Hasan%20Moughnieh%20and%20Mohamad%20Chalhoub%20and%20Hasan%20Nasrallah%20and%20Cristiano%20Nattero%20and%20Paolo%20Campanella%20and%20Giovanni%20Nico%20and%20Ali%20J.%20Ghandour%0AAbstract%3A%20%20%20Adapting%20pre-trained%20models%20has%20become%20an%20effective%20strategy%20in%20artificial%0Aintelligence%2C%20offering%20a%20scalable%20and%20efficient%20alternative%20to%20training%20models%0Afrom%20scratch.%20In%20the%20context%20of%20remote%20sensing%20%28RS%29%2C%20where%20visual%20grounding%28VG%29%0Aremains%20underexplored%2C%20this%20approach%20enables%20the%20deployment%20of%20powerful%0Avision-language%20models%20to%20achieve%20robust%20cross-modal%20understanding%20while%0Asignificantly%20reducing%20computational%20overhead.%20To%20address%20this%2C%20we%20applied%0AParameter%20Efficient%20Fine%20Tuning%20%28PEFT%29%20techniques%20to%20adapt%20these%20models%20for%0ARS-specific%20VG%20tasks.%20Specifically%2C%20we%20evaluated%20LoRA%20placement%20across%0Adifferent%20modules%20in%20Grounding%20DINO%20and%20used%20BitFit%20and%20adapters%20to%20fine-tune%0Athe%20OFA%20foundation%20model%20pre-trained%20on%20general-purpose%20VG%20datasets.%20This%0Aapproach%20achieved%20performance%20comparable%20to%20or%20surpassing%20current%20State%20Of%20The%0AArt%20%28SOTA%29%20models%20while%20significantly%20reducing%20computational%20costs.%20This%20study%0Ahighlights%20the%20potential%20of%20PEFT%20techniques%20to%20advance%20efficient%20and%20precise%0Amulti-modal%20analysis%20in%20RS%2C%20offering%20a%20practical%20and%20cost-effective%20alternative%0Ato%20full%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Adaptation%2520For%2520Remote%2520Sensing%2520Visual%2520Grounding%26entry.906535625%3DHasan%2520Moughnieh%2520and%2520Mohamad%2520Chalhoub%2520and%2520Hasan%2520Nasrallah%2520and%2520Cristiano%2520Nattero%2520and%2520Paolo%2520Campanella%2520and%2520Giovanni%2520Nico%2520and%2520Ali%2520J.%2520Ghandour%26entry.1292438233%3D%2520%2520Adapting%2520pre-trained%2520models%2520has%2520become%2520an%2520effective%2520strategy%2520in%2520artificial%250Aintelligence%252C%2520offering%2520a%2520scalable%2520and%2520efficient%2520alternative%2520to%2520training%2520models%250Afrom%2520scratch.%2520In%2520the%2520context%2520of%2520remote%2520sensing%2520%2528RS%2529%252C%2520where%2520visual%2520grounding%2528VG%2529%250Aremains%2520underexplored%252C%2520this%2520approach%2520enables%2520the%2520deployment%2520of%2520powerful%250Avision-language%2520models%2520to%2520achieve%2520robust%2520cross-modal%2520understanding%2520while%250Asignificantly%2520reducing%2520computational%2520overhead.%2520To%2520address%2520this%252C%2520we%2520applied%250AParameter%2520Efficient%2520Fine%2520Tuning%2520%2528PEFT%2529%2520techniques%2520to%2520adapt%2520these%2520models%2520for%250ARS-specific%2520VG%2520tasks.%2520Specifically%252C%2520we%2520evaluated%2520LoRA%2520placement%2520across%250Adifferent%2520modules%2520in%2520Grounding%2520DINO%2520and%2520used%2520BitFit%2520and%2520adapters%2520to%2520fine-tune%250Athe%2520OFA%2520foundation%2520model%2520pre-trained%2520on%2520general-purpose%2520VG%2520datasets.%2520This%250Aapproach%2520achieved%2520performance%2520comparable%2520to%2520or%2520surpassing%2520current%2520State%2520Of%2520The%250AArt%2520%2528SOTA%2529%2520models%2520while%2520significantly%2520reducing%2520computational%2520costs.%2520This%2520study%250Ahighlights%2520the%2520potential%2520of%2520PEFT%2520techniques%2520to%2520advance%2520efficient%2520and%2520precise%250Amulti-modal%2520analysis%2520in%2520RS%252C%2520offering%2520a%2520practical%2520and%2520cost-effective%2520alternative%250Ato%2520full%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Adaptation%20For%20Remote%20Sensing%20Visual%20Grounding&entry.906535625=Hasan%20Moughnieh%20and%20Mohamad%20Chalhoub%20and%20Hasan%20Nasrallah%20and%20Cristiano%20Nattero%20and%20Paolo%20Campanella%20and%20Giovanni%20Nico%20and%20Ali%20J.%20Ghandour&entry.1292438233=%20%20Adapting%20pre-trained%20models%20has%20become%20an%20effective%20strategy%20in%20artificial%0Aintelligence%2C%20offering%20a%20scalable%20and%20efficient%20alternative%20to%20training%20models%0Afrom%20scratch.%20In%20the%20context%20of%20remote%20sensing%20%28RS%29%2C%20where%20visual%20grounding%28VG%29%0Aremains%20underexplored%2C%20this%20approach%20enables%20the%20deployment%20of%20powerful%0Avision-language%20models%20to%20achieve%20robust%20cross-modal%20understanding%20while%0Asignificantly%20reducing%20computational%20overhead.%20To%20address%20this%2C%20we%20applied%0AParameter%20Efficient%20Fine%20Tuning%20%28PEFT%29%20techniques%20to%20adapt%20these%20models%20for%0ARS-specific%20VG%20tasks.%20Specifically%2C%20we%20evaluated%20LoRA%20placement%20across%0Adifferent%20modules%20in%20Grounding%20DINO%20and%20used%20BitFit%20and%20adapters%20to%20fine-tune%0Athe%20OFA%20foundation%20model%20pre-trained%20on%20general-purpose%20VG%20datasets.%20This%0Aapproach%20achieved%20performance%20comparable%20to%20or%20surpassing%20current%20State%20Of%20The%0AArt%20%28SOTA%29%20models%20while%20significantly%20reducing%20computational%20costs.%20This%20study%0Ahighlights%20the%20potential%20of%20PEFT%20techniques%20to%20advance%20efficient%20and%20precise%0Amulti-modal%20analysis%20in%20RS%2C%20offering%20a%20practical%20and%20cost-effective%20alternative%0Ato%20full%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23083v2&entry.124074799=Read"},
{"title": "Isolation Forest in Novelty Detection Scenario", "author": "Adam Ulrich and Jan Kr\u0148\u00e1vek and Roman \u0160enke\u0159\u00edk and Zuzana Kom\u00ednkov\u00e1 Oplatkov\u00e1 and Radek Vala", "abstract": "  Data mining offers a diverse toolbox for extracting meaningful structures\nfrom complex datasets, with anomaly detection emerging as a critical subfield\nparticularly in the context of streaming or real-time data. Within anomaly\ndetection, novelty detection focuses on identifying previously unseen patterns\nafter training solely on regular data. While classic algorithms such as\nOne-Class SVM or Local Outlier Factor (LOF) have been widely applied, they\noften lack interpretability and scalability. In this work, we explore the\nHalf-Space Tree (HST) algorithm, originally proposed for streaming anomaly\ndetection, and propose a novel theoretical modification to adapt it\nspecifically for novelty detection tasks. Our approach is grounded in the idea\nthat anomalies i.e., novelties tend to appear in the higher leaves of the tree,\nwhich are less frequently visited by regular instances. We analytically\ndemonstrate the effectiveness of this approach using probabilistic analysis,\nexpected depth (EXD) calculations, and combinatorial reasoning. A comparative\nanalysis of expected depths between our modified HST and the original Isolation\nForest highlights that novelty points are significantly more isolated in our\napproach. This supports the hypothesis that HSTs, with appropriate structural\nadaptation, can serve as interpretable and efficient novelty detectors. The\npaper contributes a theoretical foundation and supporting analysis for this\nadaptation, setting the stage for further application and experimentation.\n", "link": "http://arxiv.org/abs/2505.08489v1", "date": "2025-05-13", "relevancy": 2.2176, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4584}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4443}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Isolation%20Forest%20in%20Novelty%20Detection%20Scenario&body=Title%3A%20Isolation%20Forest%20in%20Novelty%20Detection%20Scenario%0AAuthor%3A%20Adam%20Ulrich%20and%20Jan%20Kr%C5%88%C3%A1vek%20and%20Roman%20%C5%A0enke%C5%99%C3%ADk%20and%20Zuzana%20Kom%C3%ADnkov%C3%A1%20Oplatkov%C3%A1%20and%20Radek%20Vala%0AAbstract%3A%20%20%20Data%20mining%20offers%20a%20diverse%20toolbox%20for%20extracting%20meaningful%20structures%0Afrom%20complex%20datasets%2C%20with%20anomaly%20detection%20emerging%20as%20a%20critical%20subfield%0Aparticularly%20in%20the%20context%20of%20streaming%20or%20real-time%20data.%20Within%20anomaly%0Adetection%2C%20novelty%20detection%20focuses%20on%20identifying%20previously%20unseen%20patterns%0Aafter%20training%20solely%20on%20regular%20data.%20While%20classic%20algorithms%20such%20as%0AOne-Class%20SVM%20or%20Local%20Outlier%20Factor%20%28LOF%29%20have%20been%20widely%20applied%2C%20they%0Aoften%20lack%20interpretability%20and%20scalability.%20In%20this%20work%2C%20we%20explore%20the%0AHalf-Space%20Tree%20%28HST%29%20algorithm%2C%20originally%20proposed%20for%20streaming%20anomaly%0Adetection%2C%20and%20propose%20a%20novel%20theoretical%20modification%20to%20adapt%20it%0Aspecifically%20for%20novelty%20detection%20tasks.%20Our%20approach%20is%20grounded%20in%20the%20idea%0Athat%20anomalies%20i.e.%2C%20novelties%20tend%20to%20appear%20in%20the%20higher%20leaves%20of%20the%20tree%2C%0Awhich%20are%20less%20frequently%20visited%20by%20regular%20instances.%20We%20analytically%0Ademonstrate%20the%20effectiveness%20of%20this%20approach%20using%20probabilistic%20analysis%2C%0Aexpected%20depth%20%28EXD%29%20calculations%2C%20and%20combinatorial%20reasoning.%20A%20comparative%0Aanalysis%20of%20expected%20depths%20between%20our%20modified%20HST%20and%20the%20original%20Isolation%0AForest%20highlights%20that%20novelty%20points%20are%20significantly%20more%20isolated%20in%20our%0Aapproach.%20This%20supports%20the%20hypothesis%20that%20HSTs%2C%20with%20appropriate%20structural%0Aadaptation%2C%20can%20serve%20as%20interpretable%20and%20efficient%20novelty%20detectors.%20The%0Apaper%20contributes%20a%20theoretical%20foundation%20and%20supporting%20analysis%20for%20this%0Aadaptation%2C%20setting%20the%20stage%20for%20further%20application%20and%20experimentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIsolation%2520Forest%2520in%2520Novelty%2520Detection%2520Scenario%26entry.906535625%3DAdam%2520Ulrich%2520and%2520Jan%2520Kr%25C5%2588%25C3%25A1vek%2520and%2520Roman%2520%25C5%25A0enke%25C5%2599%25C3%25ADk%2520and%2520Zuzana%2520Kom%25C3%25ADnkov%25C3%25A1%2520Oplatkov%25C3%25A1%2520and%2520Radek%2520Vala%26entry.1292438233%3D%2520%2520Data%2520mining%2520offers%2520a%2520diverse%2520toolbox%2520for%2520extracting%2520meaningful%2520structures%250Afrom%2520complex%2520datasets%252C%2520with%2520anomaly%2520detection%2520emerging%2520as%2520a%2520critical%2520subfield%250Aparticularly%2520in%2520the%2520context%2520of%2520streaming%2520or%2520real-time%2520data.%2520Within%2520anomaly%250Adetection%252C%2520novelty%2520detection%2520focuses%2520on%2520identifying%2520previously%2520unseen%2520patterns%250Aafter%2520training%2520solely%2520on%2520regular%2520data.%2520While%2520classic%2520algorithms%2520such%2520as%250AOne-Class%2520SVM%2520or%2520Local%2520Outlier%2520Factor%2520%2528LOF%2529%2520have%2520been%2520widely%2520applied%252C%2520they%250Aoften%2520lack%2520interpretability%2520and%2520scalability.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%250AHalf-Space%2520Tree%2520%2528HST%2529%2520algorithm%252C%2520originally%2520proposed%2520for%2520streaming%2520anomaly%250Adetection%252C%2520and%2520propose%2520a%2520novel%2520theoretical%2520modification%2520to%2520adapt%2520it%250Aspecifically%2520for%2520novelty%2520detection%2520tasks.%2520Our%2520approach%2520is%2520grounded%2520in%2520the%2520idea%250Athat%2520anomalies%2520i.e.%252C%2520novelties%2520tend%2520to%2520appear%2520in%2520the%2520higher%2520leaves%2520of%2520the%2520tree%252C%250Awhich%2520are%2520less%2520frequently%2520visited%2520by%2520regular%2520instances.%2520We%2520analytically%250Ademonstrate%2520the%2520effectiveness%2520of%2520this%2520approach%2520using%2520probabilistic%2520analysis%252C%250Aexpected%2520depth%2520%2528EXD%2529%2520calculations%252C%2520and%2520combinatorial%2520reasoning.%2520A%2520comparative%250Aanalysis%2520of%2520expected%2520depths%2520between%2520our%2520modified%2520HST%2520and%2520the%2520original%2520Isolation%250AForest%2520highlights%2520that%2520novelty%2520points%2520are%2520significantly%2520more%2520isolated%2520in%2520our%250Aapproach.%2520This%2520supports%2520the%2520hypothesis%2520that%2520HSTs%252C%2520with%2520appropriate%2520structural%250Aadaptation%252C%2520can%2520serve%2520as%2520interpretable%2520and%2520efficient%2520novelty%2520detectors.%2520The%250Apaper%2520contributes%2520a%2520theoretical%2520foundation%2520and%2520supporting%2520analysis%2520for%2520this%250Aadaptation%252C%2520setting%2520the%2520stage%2520for%2520further%2520application%2520and%2520experimentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Isolation%20Forest%20in%20Novelty%20Detection%20Scenario&entry.906535625=Adam%20Ulrich%20and%20Jan%20Kr%C5%88%C3%A1vek%20and%20Roman%20%C5%A0enke%C5%99%C3%ADk%20and%20Zuzana%20Kom%C3%ADnkov%C3%A1%20Oplatkov%C3%A1%20and%20Radek%20Vala&entry.1292438233=%20%20Data%20mining%20offers%20a%20diverse%20toolbox%20for%20extracting%20meaningful%20structures%0Afrom%20complex%20datasets%2C%20with%20anomaly%20detection%20emerging%20as%20a%20critical%20subfield%0Aparticularly%20in%20the%20context%20of%20streaming%20or%20real-time%20data.%20Within%20anomaly%0Adetection%2C%20novelty%20detection%20focuses%20on%20identifying%20previously%20unseen%20patterns%0Aafter%20training%20solely%20on%20regular%20data.%20While%20classic%20algorithms%20such%20as%0AOne-Class%20SVM%20or%20Local%20Outlier%20Factor%20%28LOF%29%20have%20been%20widely%20applied%2C%20they%0Aoften%20lack%20interpretability%20and%20scalability.%20In%20this%20work%2C%20we%20explore%20the%0AHalf-Space%20Tree%20%28HST%29%20algorithm%2C%20originally%20proposed%20for%20streaming%20anomaly%0Adetection%2C%20and%20propose%20a%20novel%20theoretical%20modification%20to%20adapt%20it%0Aspecifically%20for%20novelty%20detection%20tasks.%20Our%20approach%20is%20grounded%20in%20the%20idea%0Athat%20anomalies%20i.e.%2C%20novelties%20tend%20to%20appear%20in%20the%20higher%20leaves%20of%20the%20tree%2C%0Awhich%20are%20less%20frequently%20visited%20by%20regular%20instances.%20We%20analytically%0Ademonstrate%20the%20effectiveness%20of%20this%20approach%20using%20probabilistic%20analysis%2C%0Aexpected%20depth%20%28EXD%29%20calculations%2C%20and%20combinatorial%20reasoning.%20A%20comparative%0Aanalysis%20of%20expected%20depths%20between%20our%20modified%20HST%20and%20the%20original%20Isolation%0AForest%20highlights%20that%20novelty%20points%20are%20significantly%20more%20isolated%20in%20our%0Aapproach.%20This%20supports%20the%20hypothesis%20that%20HSTs%2C%20with%20appropriate%20structural%0Aadaptation%2C%20can%20serve%20as%20interpretable%20and%20efficient%20novelty%20detectors.%20The%0Apaper%20contributes%20a%20theoretical%20foundation%20and%20supporting%20analysis%20for%20this%0Aadaptation%2C%20setting%20the%20stage%20for%20further%20application%20and%20experimentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08489v1&entry.124074799=Read"},
{"title": "DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for\n  Unconstrained Gaze Estimation", "author": "Franko \u0160iki\u0107 and Donik Vr\u0161nak and Sven Lon\u010dari\u0107", "abstract": "  Unconstrained gaze estimation is the process of determining where a subject\nis directing their visual attention in uncontrolled environments. Gaze\nestimation systems are important for a myriad of tasks such as driver\ndistraction monitoring, exam proctoring, accessibility features in modern\nsoftware, etc. However, these systems face challenges in real-world scenarios,\npartially due to the low resolution of in-the-wild images and partially due to\ninsufficient modeling of head-eye interactions in current state-of-the-art\n(SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based\nmethod that advances gaze prediction through super-resolution (SR) and a dual\nhead-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone\nprocesses eye and multiscale SR head images, while the proposed DHECA module\nenables bidirectional feature refinement between the extracted visual features\nthrough cross-attention mechanisms. Furthermore, we identified critical\nannotation errors in one of the most diverse and widely used gaze estimation\ndatasets, Gaze360, and rectified the mislabeled data. Performance evaluation on\nGaze360 and GFIE datasets demonstrates superior within-dataset performance of\nthe proposed method, reducing angular error (AE) by 0.48{\\deg} (Gaze360) and\n2.95{\\deg} (GFIE) in static configurations, and 0.59{\\deg} (Gaze360) and\n3.00{\\deg} (GFIE) in temporal settings compared to prior SOTA methods.\nCross-dataset testing shows improvements in AE of more than 1.53{\\deg}\n(Gaze360) and 3.99{\\deg} (GFIE) in both static and temporal settings,\nvalidating the robust generalization properties of our approach.\n", "link": "http://arxiv.org/abs/2505.08426v1", "date": "2025-05-13", "relevancy": 2.214, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5652}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.552}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DHECA-SuperGaze%3A%20Dual%20Head-Eye%20Cross-Attention%20and%20Super-Resolution%20for%0A%20%20Unconstrained%20Gaze%20Estimation&body=Title%3A%20DHECA-SuperGaze%3A%20Dual%20Head-Eye%20Cross-Attention%20and%20Super-Resolution%20for%0A%20%20Unconstrained%20Gaze%20Estimation%0AAuthor%3A%20Franko%20%C5%A0iki%C4%87%20and%20Donik%20Vr%C5%A1nak%20and%20Sven%20Lon%C4%8Dari%C4%87%0AAbstract%3A%20%20%20Unconstrained%20gaze%20estimation%20is%20the%20process%20of%20determining%20where%20a%20subject%0Ais%20directing%20their%20visual%20attention%20in%20uncontrolled%20environments.%20Gaze%0Aestimation%20systems%20are%20important%20for%20a%20myriad%20of%20tasks%20such%20as%20driver%0Adistraction%20monitoring%2C%20exam%20proctoring%2C%20accessibility%20features%20in%20modern%0Asoftware%2C%20etc.%20However%2C%20these%20systems%20face%20challenges%20in%20real-world%20scenarios%2C%0Apartially%20due%20to%20the%20low%20resolution%20of%20in-the-wild%20images%20and%20partially%20due%20to%0Ainsufficient%20modeling%20of%20head-eye%20interactions%20in%20current%20state-of-the-art%0A%28SOTA%29%20methods.%20This%20paper%20introduces%20DHECA-SuperGaze%2C%20a%20deep%20learning-based%0Amethod%20that%20advances%20gaze%20prediction%20through%20super-resolution%20%28SR%29%20and%20a%20dual%0Ahead-eye%20cross-attention%20%28DHECA%29%20module.%20Our%20dual-branch%20convolutional%20backbone%0Aprocesses%20eye%20and%20multiscale%20SR%20head%20images%2C%20while%20the%20proposed%20DHECA%20module%0Aenables%20bidirectional%20feature%20refinement%20between%20the%20extracted%20visual%20features%0Athrough%20cross-attention%20mechanisms.%20Furthermore%2C%20we%20identified%20critical%0Aannotation%20errors%20in%20one%20of%20the%20most%20diverse%20and%20widely%20used%20gaze%20estimation%0Adatasets%2C%20Gaze360%2C%20and%20rectified%20the%20mislabeled%20data.%20Performance%20evaluation%20on%0AGaze360%20and%20GFIE%20datasets%20demonstrates%20superior%20within-dataset%20performance%20of%0Athe%20proposed%20method%2C%20reducing%20angular%20error%20%28AE%29%20by%200.48%7B%5Cdeg%7D%20%28Gaze360%29%20and%0A2.95%7B%5Cdeg%7D%20%28GFIE%29%20in%20static%20configurations%2C%20and%200.59%7B%5Cdeg%7D%20%28Gaze360%29%20and%0A3.00%7B%5Cdeg%7D%20%28GFIE%29%20in%20temporal%20settings%20compared%20to%20prior%20SOTA%20methods.%0ACross-dataset%20testing%20shows%20improvements%20in%20AE%20of%20more%20than%201.53%7B%5Cdeg%7D%0A%28Gaze360%29%20and%203.99%7B%5Cdeg%7D%20%28GFIE%29%20in%20both%20static%20and%20temporal%20settings%2C%0Avalidating%20the%20robust%20generalization%20properties%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDHECA-SuperGaze%253A%2520Dual%2520Head-Eye%2520Cross-Attention%2520and%2520Super-Resolution%2520for%250A%2520%2520Unconstrained%2520Gaze%2520Estimation%26entry.906535625%3DFranko%2520%25C5%25A0iki%25C4%2587%2520and%2520Donik%2520Vr%25C5%25A1nak%2520and%2520Sven%2520Lon%25C4%258Dari%25C4%2587%26entry.1292438233%3D%2520%2520Unconstrained%2520gaze%2520estimation%2520is%2520the%2520process%2520of%2520determining%2520where%2520a%2520subject%250Ais%2520directing%2520their%2520visual%2520attention%2520in%2520uncontrolled%2520environments.%2520Gaze%250Aestimation%2520systems%2520are%2520important%2520for%2520a%2520myriad%2520of%2520tasks%2520such%2520as%2520driver%250Adistraction%2520monitoring%252C%2520exam%2520proctoring%252C%2520accessibility%2520features%2520in%2520modern%250Asoftware%252C%2520etc.%2520However%252C%2520these%2520systems%2520face%2520challenges%2520in%2520real-world%2520scenarios%252C%250Apartially%2520due%2520to%2520the%2520low%2520resolution%2520of%2520in-the-wild%2520images%2520and%2520partially%2520due%2520to%250Ainsufficient%2520modeling%2520of%2520head-eye%2520interactions%2520in%2520current%2520state-of-the-art%250A%2528SOTA%2529%2520methods.%2520This%2520paper%2520introduces%2520DHECA-SuperGaze%252C%2520a%2520deep%2520learning-based%250Amethod%2520that%2520advances%2520gaze%2520prediction%2520through%2520super-resolution%2520%2528SR%2529%2520and%2520a%2520dual%250Ahead-eye%2520cross-attention%2520%2528DHECA%2529%2520module.%2520Our%2520dual-branch%2520convolutional%2520backbone%250Aprocesses%2520eye%2520and%2520multiscale%2520SR%2520head%2520images%252C%2520while%2520the%2520proposed%2520DHECA%2520module%250Aenables%2520bidirectional%2520feature%2520refinement%2520between%2520the%2520extracted%2520visual%2520features%250Athrough%2520cross-attention%2520mechanisms.%2520Furthermore%252C%2520we%2520identified%2520critical%250Aannotation%2520errors%2520in%2520one%2520of%2520the%2520most%2520diverse%2520and%2520widely%2520used%2520gaze%2520estimation%250Adatasets%252C%2520Gaze360%252C%2520and%2520rectified%2520the%2520mislabeled%2520data.%2520Performance%2520evaluation%2520on%250AGaze360%2520and%2520GFIE%2520datasets%2520demonstrates%2520superior%2520within-dataset%2520performance%2520of%250Athe%2520proposed%2520method%252C%2520reducing%2520angular%2520error%2520%2528AE%2529%2520by%25200.48%257B%255Cdeg%257D%2520%2528Gaze360%2529%2520and%250A2.95%257B%255Cdeg%257D%2520%2528GFIE%2529%2520in%2520static%2520configurations%252C%2520and%25200.59%257B%255Cdeg%257D%2520%2528Gaze360%2529%2520and%250A3.00%257B%255Cdeg%257D%2520%2528GFIE%2529%2520in%2520temporal%2520settings%2520compared%2520to%2520prior%2520SOTA%2520methods.%250ACross-dataset%2520testing%2520shows%2520improvements%2520in%2520AE%2520of%2520more%2520than%25201.53%257B%255Cdeg%257D%250A%2528Gaze360%2529%2520and%25203.99%257B%255Cdeg%257D%2520%2528GFIE%2529%2520in%2520both%2520static%2520and%2520temporal%2520settings%252C%250Avalidating%2520the%2520robust%2520generalization%2520properties%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DHECA-SuperGaze%3A%20Dual%20Head-Eye%20Cross-Attention%20and%20Super-Resolution%20for%0A%20%20Unconstrained%20Gaze%20Estimation&entry.906535625=Franko%20%C5%A0iki%C4%87%20and%20Donik%20Vr%C5%A1nak%20and%20Sven%20Lon%C4%8Dari%C4%87&entry.1292438233=%20%20Unconstrained%20gaze%20estimation%20is%20the%20process%20of%20determining%20where%20a%20subject%0Ais%20directing%20their%20visual%20attention%20in%20uncontrolled%20environments.%20Gaze%0Aestimation%20systems%20are%20important%20for%20a%20myriad%20of%20tasks%20such%20as%20driver%0Adistraction%20monitoring%2C%20exam%20proctoring%2C%20accessibility%20features%20in%20modern%0Asoftware%2C%20etc.%20However%2C%20these%20systems%20face%20challenges%20in%20real-world%20scenarios%2C%0Apartially%20due%20to%20the%20low%20resolution%20of%20in-the-wild%20images%20and%20partially%20due%20to%0Ainsufficient%20modeling%20of%20head-eye%20interactions%20in%20current%20state-of-the-art%0A%28SOTA%29%20methods.%20This%20paper%20introduces%20DHECA-SuperGaze%2C%20a%20deep%20learning-based%0Amethod%20that%20advances%20gaze%20prediction%20through%20super-resolution%20%28SR%29%20and%20a%20dual%0Ahead-eye%20cross-attention%20%28DHECA%29%20module.%20Our%20dual-branch%20convolutional%20backbone%0Aprocesses%20eye%20and%20multiscale%20SR%20head%20images%2C%20while%20the%20proposed%20DHECA%20module%0Aenables%20bidirectional%20feature%20refinement%20between%20the%20extracted%20visual%20features%0Athrough%20cross-attention%20mechanisms.%20Furthermore%2C%20we%20identified%20critical%0Aannotation%20errors%20in%20one%20of%20the%20most%20diverse%20and%20widely%20used%20gaze%20estimation%0Adatasets%2C%20Gaze360%2C%20and%20rectified%20the%20mislabeled%20data.%20Performance%20evaluation%20on%0AGaze360%20and%20GFIE%20datasets%20demonstrates%20superior%20within-dataset%20performance%20of%0Athe%20proposed%20method%2C%20reducing%20angular%20error%20%28AE%29%20by%200.48%7B%5Cdeg%7D%20%28Gaze360%29%20and%0A2.95%7B%5Cdeg%7D%20%28GFIE%29%20in%20static%20configurations%2C%20and%200.59%7B%5Cdeg%7D%20%28Gaze360%29%20and%0A3.00%7B%5Cdeg%7D%20%28GFIE%29%20in%20temporal%20settings%20compared%20to%20prior%20SOTA%20methods.%0ACross-dataset%20testing%20shows%20improvements%20in%20AE%20of%20more%20than%201.53%7B%5Cdeg%7D%0A%28Gaze360%29%20and%203.99%7B%5Cdeg%7D%20%28GFIE%29%20in%20both%20static%20and%20temporal%20settings%2C%0Avalidating%20the%20robust%20generalization%20properties%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08426v1&entry.124074799=Read"},
{"title": "RT-GAN: Recurrent Temporal GAN for Adding Lightweight Temporal\n  Consistency to Frame-Based Domain Translation Approaches", "author": "Shawn Mathew and Saad Nadeem and Alvin C. Goh and Arie Kaufman", "abstract": "  Fourteen million colonoscopies are performed annually just in the U.S.\nHowever, the videos from these colonoscopies are not saved due to storage\nconstraints (each video from a high-definition colonoscope camera can be in\ntens of gigabytes). Instead, a few relevant individual frames are saved for\ndocumentation/reporting purposes and these are the frames on which most current\ncolonoscopy AI models are trained on. While developing new unsupervised domain\ntranslation methods for colonoscopy (e.g. to translate between real optical and\nvirtual/CT colonoscopy), it is thus typical to start with approaches that\ninitially work for individual frames without temporal consistency. Once an\nindividual-frame model has been finalized, additional contiguous frames are\nadded with a modified deep learning architecture to train a new model from\nscratch for temporal consistency. This transition to temporally-consistent deep\nlearning models, however, requires significantly more computational and memory\nresources for training. In this paper, we present a lightweight solution with a\ntunable temporal parameter, RT-GAN (Recurrent Temporal GAN), for adding\ntemporal consistency to individual frame-based approaches that reduces training\nrequirements by a factor of 5. We demonstrate the effectiveness of our approach\non two challenging use cases in colonoscopy: haustral fold segmentation\n(indicative of missed surface) and realistic colonoscopy simulator video\ngeneration. We also release a first-of-its kind temporal dataset for\ncolonoscopy for the above use cases. The datasets, accompanying code, and\npretrained models will be made available on our Computational Endoscopy\nPlatform GitHub (https://github.com/nadeemlab/CEP). The supplementary video is\navailable at https://youtu.be/UMVP-uIXwWk.\n", "link": "http://arxiv.org/abs/2310.00868v2", "date": "2025-05-13", "relevancy": 2.1968, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5642}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5393}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RT-GAN%3A%20Recurrent%20Temporal%20GAN%20for%20Adding%20Lightweight%20Temporal%0A%20%20Consistency%20to%20Frame-Based%20Domain%20Translation%20Approaches&body=Title%3A%20RT-GAN%3A%20Recurrent%20Temporal%20GAN%20for%20Adding%20Lightweight%20Temporal%0A%20%20Consistency%20to%20Frame-Based%20Domain%20Translation%20Approaches%0AAuthor%3A%20Shawn%20Mathew%20and%20Saad%20Nadeem%20and%20Alvin%20C.%20Goh%20and%20Arie%20Kaufman%0AAbstract%3A%20%20%20Fourteen%20million%20colonoscopies%20are%20performed%20annually%20just%20in%20the%20U.S.%0AHowever%2C%20the%20videos%20from%20these%20colonoscopies%20are%20not%20saved%20due%20to%20storage%0Aconstraints%20%28each%20video%20from%20a%20high-definition%20colonoscope%20camera%20can%20be%20in%0Atens%20of%20gigabytes%29.%20Instead%2C%20a%20few%20relevant%20individual%20frames%20are%20saved%20for%0Adocumentation/reporting%20purposes%20and%20these%20are%20the%20frames%20on%20which%20most%20current%0Acolonoscopy%20AI%20models%20are%20trained%20on.%20While%20developing%20new%20unsupervised%20domain%0Atranslation%20methods%20for%20colonoscopy%20%28e.g.%20to%20translate%20between%20real%20optical%20and%0Avirtual/CT%20colonoscopy%29%2C%20it%20is%20thus%20typical%20to%20start%20with%20approaches%20that%0Ainitially%20work%20for%20individual%20frames%20without%20temporal%20consistency.%20Once%20an%0Aindividual-frame%20model%20has%20been%20finalized%2C%20additional%20contiguous%20frames%20are%0Aadded%20with%20a%20modified%20deep%20learning%20architecture%20to%20train%20a%20new%20model%20from%0Ascratch%20for%20temporal%20consistency.%20This%20transition%20to%20temporally-consistent%20deep%0Alearning%20models%2C%20however%2C%20requires%20significantly%20more%20computational%20and%20memory%0Aresources%20for%20training.%20In%20this%20paper%2C%20we%20present%20a%20lightweight%20solution%20with%20a%0Atunable%20temporal%20parameter%2C%20RT-GAN%20%28Recurrent%20Temporal%20GAN%29%2C%20for%20adding%0Atemporal%20consistency%20to%20individual%20frame-based%20approaches%20that%20reduces%20training%0Arequirements%20by%20a%20factor%20of%205.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Aon%20two%20challenging%20use%20cases%20in%20colonoscopy%3A%20haustral%20fold%20segmentation%0A%28indicative%20of%20missed%20surface%29%20and%20realistic%20colonoscopy%20simulator%20video%0Ageneration.%20We%20also%20release%20a%20first-of-its%20kind%20temporal%20dataset%20for%0Acolonoscopy%20for%20the%20above%20use%20cases.%20The%20datasets%2C%20accompanying%20code%2C%20and%0Apretrained%20models%20will%20be%20made%20available%20on%20our%20Computational%20Endoscopy%0APlatform%20GitHub%20%28https%3A//github.com/nadeemlab/CEP%29.%20The%20supplementary%20video%20is%0Aavailable%20at%20https%3A//youtu.be/UMVP-uIXwWk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRT-GAN%253A%2520Recurrent%2520Temporal%2520GAN%2520for%2520Adding%2520Lightweight%2520Temporal%250A%2520%2520Consistency%2520to%2520Frame-Based%2520Domain%2520Translation%2520Approaches%26entry.906535625%3DShawn%2520Mathew%2520and%2520Saad%2520Nadeem%2520and%2520Alvin%2520C.%2520Goh%2520and%2520Arie%2520Kaufman%26entry.1292438233%3D%2520%2520Fourteen%2520million%2520colonoscopies%2520are%2520performed%2520annually%2520just%2520in%2520the%2520U.S.%250AHowever%252C%2520the%2520videos%2520from%2520these%2520colonoscopies%2520are%2520not%2520saved%2520due%2520to%2520storage%250Aconstraints%2520%2528each%2520video%2520from%2520a%2520high-definition%2520colonoscope%2520camera%2520can%2520be%2520in%250Atens%2520of%2520gigabytes%2529.%2520Instead%252C%2520a%2520few%2520relevant%2520individual%2520frames%2520are%2520saved%2520for%250Adocumentation/reporting%2520purposes%2520and%2520these%2520are%2520the%2520frames%2520on%2520which%2520most%2520current%250Acolonoscopy%2520AI%2520models%2520are%2520trained%2520on.%2520While%2520developing%2520new%2520unsupervised%2520domain%250Atranslation%2520methods%2520for%2520colonoscopy%2520%2528e.g.%2520to%2520translate%2520between%2520real%2520optical%2520and%250Avirtual/CT%2520colonoscopy%2529%252C%2520it%2520is%2520thus%2520typical%2520to%2520start%2520with%2520approaches%2520that%250Ainitially%2520work%2520for%2520individual%2520frames%2520without%2520temporal%2520consistency.%2520Once%2520an%250Aindividual-frame%2520model%2520has%2520been%2520finalized%252C%2520additional%2520contiguous%2520frames%2520are%250Aadded%2520with%2520a%2520modified%2520deep%2520learning%2520architecture%2520to%2520train%2520a%2520new%2520model%2520from%250Ascratch%2520for%2520temporal%2520consistency.%2520This%2520transition%2520to%2520temporally-consistent%2520deep%250Alearning%2520models%252C%2520however%252C%2520requires%2520significantly%2520more%2520computational%2520and%2520memory%250Aresources%2520for%2520training.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520lightweight%2520solution%2520with%2520a%250Atunable%2520temporal%2520parameter%252C%2520RT-GAN%2520%2528Recurrent%2520Temporal%2520GAN%2529%252C%2520for%2520adding%250Atemporal%2520consistency%2520to%2520individual%2520frame-based%2520approaches%2520that%2520reduces%2520training%250Arequirements%2520by%2520a%2520factor%2520of%25205.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%250Aon%2520two%2520challenging%2520use%2520cases%2520in%2520colonoscopy%253A%2520haustral%2520fold%2520segmentation%250A%2528indicative%2520of%2520missed%2520surface%2529%2520and%2520realistic%2520colonoscopy%2520simulator%2520video%250Ageneration.%2520We%2520also%2520release%2520a%2520first-of-its%2520kind%2520temporal%2520dataset%2520for%250Acolonoscopy%2520for%2520the%2520above%2520use%2520cases.%2520The%2520datasets%252C%2520accompanying%2520code%252C%2520and%250Apretrained%2520models%2520will%2520be%2520made%2520available%2520on%2520our%2520Computational%2520Endoscopy%250APlatform%2520GitHub%2520%2528https%253A//github.com/nadeemlab/CEP%2529.%2520The%2520supplementary%2520video%2520is%250Aavailable%2520at%2520https%253A//youtu.be/UMVP-uIXwWk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RT-GAN%3A%20Recurrent%20Temporal%20GAN%20for%20Adding%20Lightweight%20Temporal%0A%20%20Consistency%20to%20Frame-Based%20Domain%20Translation%20Approaches&entry.906535625=Shawn%20Mathew%20and%20Saad%20Nadeem%20and%20Alvin%20C.%20Goh%20and%20Arie%20Kaufman&entry.1292438233=%20%20Fourteen%20million%20colonoscopies%20are%20performed%20annually%20just%20in%20the%20U.S.%0AHowever%2C%20the%20videos%20from%20these%20colonoscopies%20are%20not%20saved%20due%20to%20storage%0Aconstraints%20%28each%20video%20from%20a%20high-definition%20colonoscope%20camera%20can%20be%20in%0Atens%20of%20gigabytes%29.%20Instead%2C%20a%20few%20relevant%20individual%20frames%20are%20saved%20for%0Adocumentation/reporting%20purposes%20and%20these%20are%20the%20frames%20on%20which%20most%20current%0Acolonoscopy%20AI%20models%20are%20trained%20on.%20While%20developing%20new%20unsupervised%20domain%0Atranslation%20methods%20for%20colonoscopy%20%28e.g.%20to%20translate%20between%20real%20optical%20and%0Avirtual/CT%20colonoscopy%29%2C%20it%20is%20thus%20typical%20to%20start%20with%20approaches%20that%0Ainitially%20work%20for%20individual%20frames%20without%20temporal%20consistency.%20Once%20an%0Aindividual-frame%20model%20has%20been%20finalized%2C%20additional%20contiguous%20frames%20are%0Aadded%20with%20a%20modified%20deep%20learning%20architecture%20to%20train%20a%20new%20model%20from%0Ascratch%20for%20temporal%20consistency.%20This%20transition%20to%20temporally-consistent%20deep%0Alearning%20models%2C%20however%2C%20requires%20significantly%20more%20computational%20and%20memory%0Aresources%20for%20training.%20In%20this%20paper%2C%20we%20present%20a%20lightweight%20solution%20with%20a%0Atunable%20temporal%20parameter%2C%20RT-GAN%20%28Recurrent%20Temporal%20GAN%29%2C%20for%20adding%0Atemporal%20consistency%20to%20individual%20frame-based%20approaches%20that%20reduces%20training%0Arequirements%20by%20a%20factor%20of%205.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Aon%20two%20challenging%20use%20cases%20in%20colonoscopy%3A%20haustral%20fold%20segmentation%0A%28indicative%20of%20missed%20surface%29%20and%20realistic%20colonoscopy%20simulator%20video%0Ageneration.%20We%20also%20release%20a%20first-of-its%20kind%20temporal%20dataset%20for%0Acolonoscopy%20for%20the%20above%20use%20cases.%20The%20datasets%2C%20accompanying%20code%2C%20and%0Apretrained%20models%20will%20be%20made%20available%20on%20our%20Computational%20Endoscopy%0APlatform%20GitHub%20%28https%3A//github.com/nadeemlab/CEP%29.%20The%20supplementary%20video%20is%0Aavailable%20at%20https%3A//youtu.be/UMVP-uIXwWk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00868v2&entry.124074799=Read"},
{"title": "Generative Molecular Design with Steerable and Granular Synthesizability\n  Control", "author": "Jeff Guo and V\u00edctor Sabanza-Gil and Zlatko Jon\u010dev and Jeremy S. Luterbacher and Philippe Schwaller", "abstract": "  Synthesizability in small molecule generative design remains a bottleneck.\nExisting works that do consider synthesizability can output predicted synthesis\nroutes for generated molecules. However, there has been minimal attention in\naddressing the ease of synthesis and enabling flexibility to incorporate\ndesired reaction constraints. In this work, we propose a small molecule\ngenerative design framework that enables steerable and granular\nsynthesizability control. Generated molecules satisfy arbitrary multi-parameter\noptimization objectives with predicted synthesis routes containing pre-defined\nallowed reactions, while optionally avoiding others. One can also enforce that\nall reactions belong to a pre-defined set. We show the capability to\nmix-and-match these reaction constraints across the most common medicinal\nchemistry transformations. Next, we show how our framework can be used to\nvalorize industrial byproducts towards de novo optimized molecules. Going\nfurther, we demonstrate how granular control over synthesizability constraints\ncan loosely mimic virtual screening of ultra-large make-on-demand libraries.\nUsing only a single GPU, we generate and dock 15k molecules to identify\npromising candidates in Freedom 4.0 constituting 142B make-on-demand molecules\n(assessing only 0.00001% of the library). Generated molecules satisfying the\nreaction constraints have > 90% exact match rate. Lastly, we benchmark our\nframework against recent synthesizability-constrained generative models and\ndemonstrate the highest sample efficiency even when imposing the additional\nconstraint that all molecules must be synthesizable from a single reaction\ntype. The main theme is demonstrating that a pre-trained generalist molecular\ngenerative model can be incentivized to generate property-optimized small\nmolecules under challenging synthesizability constraints through reinforcement\nlearning.\n", "link": "http://arxiv.org/abs/2505.08774v1", "date": "2025-05-13", "relevancy": 2.1923, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5765}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5429}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Molecular%20Design%20with%20Steerable%20and%20Granular%20Synthesizability%0A%20%20Control&body=Title%3A%20Generative%20Molecular%20Design%20with%20Steerable%20and%20Granular%20Synthesizability%0A%20%20Control%0AAuthor%3A%20Jeff%20Guo%20and%20V%C3%ADctor%20Sabanza-Gil%20and%20Zlatko%20Jon%C4%8Dev%20and%20Jeremy%20S.%20Luterbacher%20and%20Philippe%20Schwaller%0AAbstract%3A%20%20%20Synthesizability%20in%20small%20molecule%20generative%20design%20remains%20a%20bottleneck.%0AExisting%20works%20that%20do%20consider%20synthesizability%20can%20output%20predicted%20synthesis%0Aroutes%20for%20generated%20molecules.%20However%2C%20there%20has%20been%20minimal%20attention%20in%0Aaddressing%20the%20ease%20of%20synthesis%20and%20enabling%20flexibility%20to%20incorporate%0Adesired%20reaction%20constraints.%20In%20this%20work%2C%20we%20propose%20a%20small%20molecule%0Agenerative%20design%20framework%20that%20enables%20steerable%20and%20granular%0Asynthesizability%20control.%20Generated%20molecules%20satisfy%20arbitrary%20multi-parameter%0Aoptimization%20objectives%20with%20predicted%20synthesis%20routes%20containing%20pre-defined%0Aallowed%20reactions%2C%20while%20optionally%20avoiding%20others.%20One%20can%20also%20enforce%20that%0Aall%20reactions%20belong%20to%20a%20pre-defined%20set.%20We%20show%20the%20capability%20to%0Amix-and-match%20these%20reaction%20constraints%20across%20the%20most%20common%20medicinal%0Achemistry%20transformations.%20Next%2C%20we%20show%20how%20our%20framework%20can%20be%20used%20to%0Avalorize%20industrial%20byproducts%20towards%20de%20novo%20optimized%20molecules.%20Going%0Afurther%2C%20we%20demonstrate%20how%20granular%20control%20over%20synthesizability%20constraints%0Acan%20loosely%20mimic%20virtual%20screening%20of%20ultra-large%20make-on-demand%20libraries.%0AUsing%20only%20a%20single%20GPU%2C%20we%20generate%20and%20dock%2015k%20molecules%20to%20identify%0Apromising%20candidates%20in%20Freedom%204.0%20constituting%20142B%20make-on-demand%20molecules%0A%28assessing%20only%200.00001%25%20of%20the%20library%29.%20Generated%20molecules%20satisfying%20the%0Areaction%20constraints%20have%20%3E%2090%25%20exact%20match%20rate.%20Lastly%2C%20we%20benchmark%20our%0Aframework%20against%20recent%20synthesizability-constrained%20generative%20models%20and%0Ademonstrate%20the%20highest%20sample%20efficiency%20even%20when%20imposing%20the%20additional%0Aconstraint%20that%20all%20molecules%20must%20be%20synthesizable%20from%20a%20single%20reaction%0Atype.%20The%20main%20theme%20is%20demonstrating%20that%20a%20pre-trained%20generalist%20molecular%0Agenerative%20model%20can%20be%20incentivized%20to%20generate%20property-optimized%20small%0Amolecules%20under%20challenging%20synthesizability%20constraints%20through%20reinforcement%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Molecular%2520Design%2520with%2520Steerable%2520and%2520Granular%2520Synthesizability%250A%2520%2520Control%26entry.906535625%3DJeff%2520Guo%2520and%2520V%25C3%25ADctor%2520Sabanza-Gil%2520and%2520Zlatko%2520Jon%25C4%258Dev%2520and%2520Jeremy%2520S.%2520Luterbacher%2520and%2520Philippe%2520Schwaller%26entry.1292438233%3D%2520%2520Synthesizability%2520in%2520small%2520molecule%2520generative%2520design%2520remains%2520a%2520bottleneck.%250AExisting%2520works%2520that%2520do%2520consider%2520synthesizability%2520can%2520output%2520predicted%2520synthesis%250Aroutes%2520for%2520generated%2520molecules.%2520However%252C%2520there%2520has%2520been%2520minimal%2520attention%2520in%250Aaddressing%2520the%2520ease%2520of%2520synthesis%2520and%2520enabling%2520flexibility%2520to%2520incorporate%250Adesired%2520reaction%2520constraints.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520small%2520molecule%250Agenerative%2520design%2520framework%2520that%2520enables%2520steerable%2520and%2520granular%250Asynthesizability%2520control.%2520Generated%2520molecules%2520satisfy%2520arbitrary%2520multi-parameter%250Aoptimization%2520objectives%2520with%2520predicted%2520synthesis%2520routes%2520containing%2520pre-defined%250Aallowed%2520reactions%252C%2520while%2520optionally%2520avoiding%2520others.%2520One%2520can%2520also%2520enforce%2520that%250Aall%2520reactions%2520belong%2520to%2520a%2520pre-defined%2520set.%2520We%2520show%2520the%2520capability%2520to%250Amix-and-match%2520these%2520reaction%2520constraints%2520across%2520the%2520most%2520common%2520medicinal%250Achemistry%2520transformations.%2520Next%252C%2520we%2520show%2520how%2520our%2520framework%2520can%2520be%2520used%2520to%250Avalorize%2520industrial%2520byproducts%2520towards%2520de%2520novo%2520optimized%2520molecules.%2520Going%250Afurther%252C%2520we%2520demonstrate%2520how%2520granular%2520control%2520over%2520synthesizability%2520constraints%250Acan%2520loosely%2520mimic%2520virtual%2520screening%2520of%2520ultra-large%2520make-on-demand%2520libraries.%250AUsing%2520only%2520a%2520single%2520GPU%252C%2520we%2520generate%2520and%2520dock%252015k%2520molecules%2520to%2520identify%250Apromising%2520candidates%2520in%2520Freedom%25204.0%2520constituting%2520142B%2520make-on-demand%2520molecules%250A%2528assessing%2520only%25200.00001%2525%2520of%2520the%2520library%2529.%2520Generated%2520molecules%2520satisfying%2520the%250Areaction%2520constraints%2520have%2520%253E%252090%2525%2520exact%2520match%2520rate.%2520Lastly%252C%2520we%2520benchmark%2520our%250Aframework%2520against%2520recent%2520synthesizability-constrained%2520generative%2520models%2520and%250Ademonstrate%2520the%2520highest%2520sample%2520efficiency%2520even%2520when%2520imposing%2520the%2520additional%250Aconstraint%2520that%2520all%2520molecules%2520must%2520be%2520synthesizable%2520from%2520a%2520single%2520reaction%250Atype.%2520The%2520main%2520theme%2520is%2520demonstrating%2520that%2520a%2520pre-trained%2520generalist%2520molecular%250Agenerative%2520model%2520can%2520be%2520incentivized%2520to%2520generate%2520property-optimized%2520small%250Amolecules%2520under%2520challenging%2520synthesizability%2520constraints%2520through%2520reinforcement%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Molecular%20Design%20with%20Steerable%20and%20Granular%20Synthesizability%0A%20%20Control&entry.906535625=Jeff%20Guo%20and%20V%C3%ADctor%20Sabanza-Gil%20and%20Zlatko%20Jon%C4%8Dev%20and%20Jeremy%20S.%20Luterbacher%20and%20Philippe%20Schwaller&entry.1292438233=%20%20Synthesizability%20in%20small%20molecule%20generative%20design%20remains%20a%20bottleneck.%0AExisting%20works%20that%20do%20consider%20synthesizability%20can%20output%20predicted%20synthesis%0Aroutes%20for%20generated%20molecules.%20However%2C%20there%20has%20been%20minimal%20attention%20in%0Aaddressing%20the%20ease%20of%20synthesis%20and%20enabling%20flexibility%20to%20incorporate%0Adesired%20reaction%20constraints.%20In%20this%20work%2C%20we%20propose%20a%20small%20molecule%0Agenerative%20design%20framework%20that%20enables%20steerable%20and%20granular%0Asynthesizability%20control.%20Generated%20molecules%20satisfy%20arbitrary%20multi-parameter%0Aoptimization%20objectives%20with%20predicted%20synthesis%20routes%20containing%20pre-defined%0Aallowed%20reactions%2C%20while%20optionally%20avoiding%20others.%20One%20can%20also%20enforce%20that%0Aall%20reactions%20belong%20to%20a%20pre-defined%20set.%20We%20show%20the%20capability%20to%0Amix-and-match%20these%20reaction%20constraints%20across%20the%20most%20common%20medicinal%0Achemistry%20transformations.%20Next%2C%20we%20show%20how%20our%20framework%20can%20be%20used%20to%0Avalorize%20industrial%20byproducts%20towards%20de%20novo%20optimized%20molecules.%20Going%0Afurther%2C%20we%20demonstrate%20how%20granular%20control%20over%20synthesizability%20constraints%0Acan%20loosely%20mimic%20virtual%20screening%20of%20ultra-large%20make-on-demand%20libraries.%0AUsing%20only%20a%20single%20GPU%2C%20we%20generate%20and%20dock%2015k%20molecules%20to%20identify%0Apromising%20candidates%20in%20Freedom%204.0%20constituting%20142B%20make-on-demand%20molecules%0A%28assessing%20only%200.00001%25%20of%20the%20library%29.%20Generated%20molecules%20satisfying%20the%0Areaction%20constraints%20have%20%3E%2090%25%20exact%20match%20rate.%20Lastly%2C%20we%20benchmark%20our%0Aframework%20against%20recent%20synthesizability-constrained%20generative%20models%20and%0Ademonstrate%20the%20highest%20sample%20efficiency%20even%20when%20imposing%20the%20additional%0Aconstraint%20that%20all%20molecules%20must%20be%20synthesizable%20from%20a%20single%20reaction%0Atype.%20The%20main%20theme%20is%20demonstrating%20that%20a%20pre-trained%20generalist%20molecular%0Agenerative%20model%20can%20be%20incentivized%20to%20generate%20property-optimized%20small%0Amolecules%20under%20challenging%20synthesizability%20constraints%20through%20reinforcement%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08774v1&entry.124074799=Read"},
{"title": "A new methodology to decompose a parametric domain using reduced order\n  data manifold in machine learning", "author": "Chetra Mang and Axel TahmasebiMoradi and Mouadh Yagoubi", "abstract": "  We propose a new methodology for parametric domain decomposition using\niterative principal component analysis. Starting with iterative principle\ncomponent analysis, the high dimension manifold is reduced to the lower\ndimension manifold. Moreover, two approaches are developed to reconstruct the\ninverse projector to project from the lower data component to the original one.\nAfterward, we provide a detailed strategy to decompose the parametric domain\nbased on the low dimension manifold. Finally, numerical examples of harmonic\ntransport problem are given to illustrate the efficiency and effectiveness of\nthe proposed method comparing to the classical meta-models such as neural\nnetworks.\n", "link": "http://arxiv.org/abs/2505.08497v1", "date": "2025-05-13", "relevancy": 2.1901, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4472}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4346}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20new%20methodology%20to%20decompose%20a%20parametric%20domain%20using%20reduced%20order%0A%20%20data%20manifold%20in%20machine%20learning&body=Title%3A%20A%20new%20methodology%20to%20decompose%20a%20parametric%20domain%20using%20reduced%20order%0A%20%20data%20manifold%20in%20machine%20learning%0AAuthor%3A%20Chetra%20Mang%20and%20Axel%20TahmasebiMoradi%20and%20Mouadh%20Yagoubi%0AAbstract%3A%20%20%20We%20propose%20a%20new%20methodology%20for%20parametric%20domain%20decomposition%20using%0Aiterative%20principal%20component%20analysis.%20Starting%20with%20iterative%20principle%0Acomponent%20analysis%2C%20the%20high%20dimension%20manifold%20is%20reduced%20to%20the%20lower%0Adimension%20manifold.%20Moreover%2C%20two%20approaches%20are%20developed%20to%20reconstruct%20the%0Ainverse%20projector%20to%20project%20from%20the%20lower%20data%20component%20to%20the%20original%20one.%0AAfterward%2C%20we%20provide%20a%20detailed%20strategy%20to%20decompose%20the%20parametric%20domain%0Abased%20on%20the%20low%20dimension%20manifold.%20Finally%2C%20numerical%20examples%20of%20harmonic%0Atransport%20problem%20are%20given%20to%20illustrate%20the%20efficiency%20and%20effectiveness%20of%0Athe%20proposed%20method%20comparing%20to%20the%20classical%20meta-models%20such%20as%20neural%0Anetworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520new%2520methodology%2520to%2520decompose%2520a%2520parametric%2520domain%2520using%2520reduced%2520order%250A%2520%2520data%2520manifold%2520in%2520machine%2520learning%26entry.906535625%3DChetra%2520Mang%2520and%2520Axel%2520TahmasebiMoradi%2520and%2520Mouadh%2520Yagoubi%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520methodology%2520for%2520parametric%2520domain%2520decomposition%2520using%250Aiterative%2520principal%2520component%2520analysis.%2520Starting%2520with%2520iterative%2520principle%250Acomponent%2520analysis%252C%2520the%2520high%2520dimension%2520manifold%2520is%2520reduced%2520to%2520the%2520lower%250Adimension%2520manifold.%2520Moreover%252C%2520two%2520approaches%2520are%2520developed%2520to%2520reconstruct%2520the%250Ainverse%2520projector%2520to%2520project%2520from%2520the%2520lower%2520data%2520component%2520to%2520the%2520original%2520one.%250AAfterward%252C%2520we%2520provide%2520a%2520detailed%2520strategy%2520to%2520decompose%2520the%2520parametric%2520domain%250Abased%2520on%2520the%2520low%2520dimension%2520manifold.%2520Finally%252C%2520numerical%2520examples%2520of%2520harmonic%250Atransport%2520problem%2520are%2520given%2520to%2520illustrate%2520the%2520efficiency%2520and%2520effectiveness%2520of%250Athe%2520proposed%2520method%2520comparing%2520to%2520the%2520classical%2520meta-models%2520such%2520as%2520neural%250Anetworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20new%20methodology%20to%20decompose%20a%20parametric%20domain%20using%20reduced%20order%0A%20%20data%20manifold%20in%20machine%20learning&entry.906535625=Chetra%20Mang%20and%20Axel%20TahmasebiMoradi%20and%20Mouadh%20Yagoubi&entry.1292438233=%20%20We%20propose%20a%20new%20methodology%20for%20parametric%20domain%20decomposition%20using%0Aiterative%20principal%20component%20analysis.%20Starting%20with%20iterative%20principle%0Acomponent%20analysis%2C%20the%20high%20dimension%20manifold%20is%20reduced%20to%20the%20lower%0Adimension%20manifold.%20Moreover%2C%20two%20approaches%20are%20developed%20to%20reconstruct%20the%0Ainverse%20projector%20to%20project%20from%20the%20lower%20data%20component%20to%20the%20original%20one.%0AAfterward%2C%20we%20provide%20a%20detailed%20strategy%20to%20decompose%20the%20parametric%20domain%0Abased%20on%20the%20low%20dimension%20manifold.%20Finally%2C%20numerical%20examples%20of%20harmonic%0Atransport%20problem%20are%20given%20to%20illustrate%20the%20efficiency%20and%20effectiveness%20of%0Athe%20proposed%20method%20comparing%20to%20the%20classical%20meta-models%20such%20as%20neural%0Anetworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08497v1&entry.124074799=Read"},
{"title": "Hakim: Farsi Text Embedding Model", "author": "Mehran Sarmadi and Morteza Alikhani and Erfan Zinvandi and Zahra Pourbahman", "abstract": "  Recent advancements in text embedding have significantly improved natural\nlanguage understanding across many languages, yet Persian remains notably\nunderrepresented in large-scale embedding research. In this paper, we present\nHakim, a novel state-of-the-art Persian text embedding model that achieves a\n8.5% performance improvement over existing approaches on the FaMTEB benchmark,\noutperforming all previously developed Persian language models. As part of this\nwork, we introduce three new datasets - Corpesia, Pairsia-sup, and\nPairsia-unsup - to support supervised and unsupervised training scenarios.\nAdditionally, Hakim is designed for applications in chatbots and\nretrieval-augmented generation (RAG) systems, particularly addressing retrieval\ntasks that require incorporating message history within these systems. We also\npropose a new baseline model built on the BERT architecture. Our language model\nconsistently achieves higher accuracy across various Persian NLP tasks, while\nthe RetroMAE-based model proves particularly effective for textual information\nretrieval applications. Together, these contributions establish a new\nfoundation for advancing Persian language understanding.\n", "link": "http://arxiv.org/abs/2505.08435v1", "date": "2025-05-13", "relevancy": 2.1689, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4271}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hakim%3A%20Farsi%20Text%20Embedding%20Model&body=Title%3A%20Hakim%3A%20Farsi%20Text%20Embedding%20Model%0AAuthor%3A%20Mehran%20Sarmadi%20and%20Morteza%20Alikhani%20and%20Erfan%20Zinvandi%20and%20Zahra%20Pourbahman%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text%20embedding%20have%20significantly%20improved%20natural%0Alanguage%20understanding%20across%20many%20languages%2C%20yet%20Persian%20remains%20notably%0Aunderrepresented%20in%20large-scale%20embedding%20research.%20In%20this%20paper%2C%20we%20present%0AHakim%2C%20a%20novel%20state-of-the-art%20Persian%20text%20embedding%20model%20that%20achieves%20a%0A8.5%25%20performance%20improvement%20over%20existing%20approaches%20on%20the%20FaMTEB%20benchmark%2C%0Aoutperforming%20all%20previously%20developed%20Persian%20language%20models.%20As%20part%20of%20this%0Awork%2C%20we%20introduce%20three%20new%20datasets%20-%20Corpesia%2C%20Pairsia-sup%2C%20and%0APairsia-unsup%20-%20to%20support%20supervised%20and%20unsupervised%20training%20scenarios.%0AAdditionally%2C%20Hakim%20is%20designed%20for%20applications%20in%20chatbots%20and%0Aretrieval-augmented%20generation%20%28RAG%29%20systems%2C%20particularly%20addressing%20retrieval%0Atasks%20that%20require%20incorporating%20message%20history%20within%20these%20systems.%20We%20also%0Apropose%20a%20new%20baseline%20model%20built%20on%20the%20BERT%20architecture.%20Our%20language%20model%0Aconsistently%20achieves%20higher%20accuracy%20across%20various%20Persian%20NLP%20tasks%2C%20while%0Athe%20RetroMAE-based%20model%20proves%20particularly%20effective%20for%20textual%20information%0Aretrieval%20applications.%20Together%2C%20these%20contributions%20establish%20a%20new%0Afoundation%20for%20advancing%20Persian%20language%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHakim%253A%2520Farsi%2520Text%2520Embedding%2520Model%26entry.906535625%3DMehran%2520Sarmadi%2520and%2520Morteza%2520Alikhani%2520and%2520Erfan%2520Zinvandi%2520and%2520Zahra%2520Pourbahman%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text%2520embedding%2520have%2520significantly%2520improved%2520natural%250Alanguage%2520understanding%2520across%2520many%2520languages%252C%2520yet%2520Persian%2520remains%2520notably%250Aunderrepresented%2520in%2520large-scale%2520embedding%2520research.%2520In%2520this%2520paper%252C%2520we%2520present%250AHakim%252C%2520a%2520novel%2520state-of-the-art%2520Persian%2520text%2520embedding%2520model%2520that%2520achieves%2520a%250A8.5%2525%2520performance%2520improvement%2520over%2520existing%2520approaches%2520on%2520the%2520FaMTEB%2520benchmark%252C%250Aoutperforming%2520all%2520previously%2520developed%2520Persian%2520language%2520models.%2520As%2520part%2520of%2520this%250Awork%252C%2520we%2520introduce%2520three%2520new%2520datasets%2520-%2520Corpesia%252C%2520Pairsia-sup%252C%2520and%250APairsia-unsup%2520-%2520to%2520support%2520supervised%2520and%2520unsupervised%2520training%2520scenarios.%250AAdditionally%252C%2520Hakim%2520is%2520designed%2520for%2520applications%2520in%2520chatbots%2520and%250Aretrieval-augmented%2520generation%2520%2528RAG%2529%2520systems%252C%2520particularly%2520addressing%2520retrieval%250Atasks%2520that%2520require%2520incorporating%2520message%2520history%2520within%2520these%2520systems.%2520We%2520also%250Apropose%2520a%2520new%2520baseline%2520model%2520built%2520on%2520the%2520BERT%2520architecture.%2520Our%2520language%2520model%250Aconsistently%2520achieves%2520higher%2520accuracy%2520across%2520various%2520Persian%2520NLP%2520tasks%252C%2520while%250Athe%2520RetroMAE-based%2520model%2520proves%2520particularly%2520effective%2520for%2520textual%2520information%250Aretrieval%2520applications.%2520Together%252C%2520these%2520contributions%2520establish%2520a%2520new%250Afoundation%2520for%2520advancing%2520Persian%2520language%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hakim%3A%20Farsi%20Text%20Embedding%20Model&entry.906535625=Mehran%20Sarmadi%20and%20Morteza%20Alikhani%20and%20Erfan%20Zinvandi%20and%20Zahra%20Pourbahman&entry.1292438233=%20%20Recent%20advancements%20in%20text%20embedding%20have%20significantly%20improved%20natural%0Alanguage%20understanding%20across%20many%20languages%2C%20yet%20Persian%20remains%20notably%0Aunderrepresented%20in%20large-scale%20embedding%20research.%20In%20this%20paper%2C%20we%20present%0AHakim%2C%20a%20novel%20state-of-the-art%20Persian%20text%20embedding%20model%20that%20achieves%20a%0A8.5%25%20performance%20improvement%20over%20existing%20approaches%20on%20the%20FaMTEB%20benchmark%2C%0Aoutperforming%20all%20previously%20developed%20Persian%20language%20models.%20As%20part%20of%20this%0Awork%2C%20we%20introduce%20three%20new%20datasets%20-%20Corpesia%2C%20Pairsia-sup%2C%20and%0APairsia-unsup%20-%20to%20support%20supervised%20and%20unsupervised%20training%20scenarios.%0AAdditionally%2C%20Hakim%20is%20designed%20for%20applications%20in%20chatbots%20and%0Aretrieval-augmented%20generation%20%28RAG%29%20systems%2C%20particularly%20addressing%20retrieval%0Atasks%20that%20require%20incorporating%20message%20history%20within%20these%20systems.%20We%20also%0Apropose%20a%20new%20baseline%20model%20built%20on%20the%20BERT%20architecture.%20Our%20language%20model%0Aconsistently%20achieves%20higher%20accuracy%20across%20various%20Persian%20NLP%20tasks%2C%20while%0Athe%20RetroMAE-based%20model%20proves%20particularly%20effective%20for%20textual%20information%0Aretrieval%20applications.%20Together%2C%20these%20contributions%20establish%20a%20new%0Afoundation%20for%20advancing%20Persian%20language%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08435v1&entry.124074799=Read"},
{"title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking", "author": "Haofeng Liu and Mingqi Gao and Xuxiao Luo and Ziyue Wang and Guanyi Qin and Junde Wu and Yueming Jin", "abstract": "  Surgical scene segmentation is critical in computer-assisted surgery and is\nvital for enhancing surgical quality and patient outcomes. Recently, referring\nsurgical segmentation is emerging, given its advantage of providing surgeons\nwith an interactive experience to segment the target object. However, existing\nmethods are limited by low efficiency and short-term tracking, hindering their\napplicability in complex real-world surgical scenarios. In this paper, we\nintroduce ReSurgSAM2, a two-stage surgical referring segmentation framework\nthat leverages Segment Anything Model 2 to perform text-referred target\ndetection, followed by tracking with reliable initial frame identification and\ndiversity-driven long-term memory. For the detection stage, we propose a\ncross-modal spatial-temporal Mamba to generate precise detection and\nsegmentation results. Based on these results, our credible initial frame\nselection strategy identifies the reliable frame for the subsequent tracking.\nUpon selecting the initial frame, our method transitions to the tracking stage,\nwhere it incorporates a diversity-driven memory mechanism that maintains a\ncredible and diverse memory bank, ensuring consistent long-term tracking.\nExtensive experiments demonstrate that ReSurgSAM2 achieves substantial\nimprovements in accuracy and efficiency compared to existing methods, operating\nin real-time at 61.2 FPS. Our code and datasets will be available at\nhttps://github.com/jinlab-imvr/ReSurgSAM2.\n", "link": "http://arxiv.org/abs/2505.08581v1", "date": "2025-05-13", "relevancy": 2.1686, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5748}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5199}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReSurgSAM2%3A%20Referring%20Segment%20Anything%20in%20Surgical%20Video%20via%20Credible%0A%20%20Long-term%20Tracking&body=Title%3A%20ReSurgSAM2%3A%20Referring%20Segment%20Anything%20in%20Surgical%20Video%20via%20Credible%0A%20%20Long-term%20Tracking%0AAuthor%3A%20Haofeng%20Liu%20and%20Mingqi%20Gao%20and%20Xuxiao%20Luo%20and%20Ziyue%20Wang%20and%20Guanyi%20Qin%20and%20Junde%20Wu%20and%20Yueming%20Jin%0AAbstract%3A%20%20%20Surgical%20scene%20segmentation%20is%20critical%20in%20computer-assisted%20surgery%20and%20is%0Avital%20for%20enhancing%20surgical%20quality%20and%20patient%20outcomes.%20Recently%2C%20referring%0Asurgical%20segmentation%20is%20emerging%2C%20given%20its%20advantage%20of%20providing%20surgeons%0Awith%20an%20interactive%20experience%20to%20segment%20the%20target%20object.%20However%2C%20existing%0Amethods%20are%20limited%20by%20low%20efficiency%20and%20short-term%20tracking%2C%20hindering%20their%0Aapplicability%20in%20complex%20real-world%20surgical%20scenarios.%20In%20this%20paper%2C%20we%0Aintroduce%20ReSurgSAM2%2C%20a%20two-stage%20surgical%20referring%20segmentation%20framework%0Athat%20leverages%20Segment%20Anything%20Model%202%20to%20perform%20text-referred%20target%0Adetection%2C%20followed%20by%20tracking%20with%20reliable%20initial%20frame%20identification%20and%0Adiversity-driven%20long-term%20memory.%20For%20the%20detection%20stage%2C%20we%20propose%20a%0Across-modal%20spatial-temporal%20Mamba%20to%20generate%20precise%20detection%20and%0Asegmentation%20results.%20Based%20on%20these%20results%2C%20our%20credible%20initial%20frame%0Aselection%20strategy%20identifies%20the%20reliable%20frame%20for%20the%20subsequent%20tracking.%0AUpon%20selecting%20the%20initial%20frame%2C%20our%20method%20transitions%20to%20the%20tracking%20stage%2C%0Awhere%20it%20incorporates%20a%20diversity-driven%20memory%20mechanism%20that%20maintains%20a%0Acredible%20and%20diverse%20memory%20bank%2C%20ensuring%20consistent%20long-term%20tracking.%0AExtensive%20experiments%20demonstrate%20that%20ReSurgSAM2%20achieves%20substantial%0Aimprovements%20in%20accuracy%20and%20efficiency%20compared%20to%20existing%20methods%2C%20operating%0Ain%20real-time%20at%2061.2%20FPS.%20Our%20code%20and%20datasets%20will%20be%20available%20at%0Ahttps%3A//github.com/jinlab-imvr/ReSurgSAM2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReSurgSAM2%253A%2520Referring%2520Segment%2520Anything%2520in%2520Surgical%2520Video%2520via%2520Credible%250A%2520%2520Long-term%2520Tracking%26entry.906535625%3DHaofeng%2520Liu%2520and%2520Mingqi%2520Gao%2520and%2520Xuxiao%2520Luo%2520and%2520Ziyue%2520Wang%2520and%2520Guanyi%2520Qin%2520and%2520Junde%2520Wu%2520and%2520Yueming%2520Jin%26entry.1292438233%3D%2520%2520Surgical%2520scene%2520segmentation%2520is%2520critical%2520in%2520computer-assisted%2520surgery%2520and%2520is%250Avital%2520for%2520enhancing%2520surgical%2520quality%2520and%2520patient%2520outcomes.%2520Recently%252C%2520referring%250Asurgical%2520segmentation%2520is%2520emerging%252C%2520given%2520its%2520advantage%2520of%2520providing%2520surgeons%250Awith%2520an%2520interactive%2520experience%2520to%2520segment%2520the%2520target%2520object.%2520However%252C%2520existing%250Amethods%2520are%2520limited%2520by%2520low%2520efficiency%2520and%2520short-term%2520tracking%252C%2520hindering%2520their%250Aapplicability%2520in%2520complex%2520real-world%2520surgical%2520scenarios.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520ReSurgSAM2%252C%2520a%2520two-stage%2520surgical%2520referring%2520segmentation%2520framework%250Athat%2520leverages%2520Segment%2520Anything%2520Model%25202%2520to%2520perform%2520text-referred%2520target%250Adetection%252C%2520followed%2520by%2520tracking%2520with%2520reliable%2520initial%2520frame%2520identification%2520and%250Adiversity-driven%2520long-term%2520memory.%2520For%2520the%2520detection%2520stage%252C%2520we%2520propose%2520a%250Across-modal%2520spatial-temporal%2520Mamba%2520to%2520generate%2520precise%2520detection%2520and%250Asegmentation%2520results.%2520Based%2520on%2520these%2520results%252C%2520our%2520credible%2520initial%2520frame%250Aselection%2520strategy%2520identifies%2520the%2520reliable%2520frame%2520for%2520the%2520subsequent%2520tracking.%250AUpon%2520selecting%2520the%2520initial%2520frame%252C%2520our%2520method%2520transitions%2520to%2520the%2520tracking%2520stage%252C%250Awhere%2520it%2520incorporates%2520a%2520diversity-driven%2520memory%2520mechanism%2520that%2520maintains%2520a%250Acredible%2520and%2520diverse%2520memory%2520bank%252C%2520ensuring%2520consistent%2520long-term%2520tracking.%250AExtensive%2520experiments%2520demonstrate%2520that%2520ReSurgSAM2%2520achieves%2520substantial%250Aimprovements%2520in%2520accuracy%2520and%2520efficiency%2520compared%2520to%2520existing%2520methods%252C%2520operating%250Ain%2520real-time%2520at%252061.2%2520FPS.%2520Our%2520code%2520and%2520datasets%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/jinlab-imvr/ReSurgSAM2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReSurgSAM2%3A%20Referring%20Segment%20Anything%20in%20Surgical%20Video%20via%20Credible%0A%20%20Long-term%20Tracking&entry.906535625=Haofeng%20Liu%20and%20Mingqi%20Gao%20and%20Xuxiao%20Luo%20and%20Ziyue%20Wang%20and%20Guanyi%20Qin%20and%20Junde%20Wu%20and%20Yueming%20Jin&entry.1292438233=%20%20Surgical%20scene%20segmentation%20is%20critical%20in%20computer-assisted%20surgery%20and%20is%0Avital%20for%20enhancing%20surgical%20quality%20and%20patient%20outcomes.%20Recently%2C%20referring%0Asurgical%20segmentation%20is%20emerging%2C%20given%20its%20advantage%20of%20providing%20surgeons%0Awith%20an%20interactive%20experience%20to%20segment%20the%20target%20object.%20However%2C%20existing%0Amethods%20are%20limited%20by%20low%20efficiency%20and%20short-term%20tracking%2C%20hindering%20their%0Aapplicability%20in%20complex%20real-world%20surgical%20scenarios.%20In%20this%20paper%2C%20we%0Aintroduce%20ReSurgSAM2%2C%20a%20two-stage%20surgical%20referring%20segmentation%20framework%0Athat%20leverages%20Segment%20Anything%20Model%202%20to%20perform%20text-referred%20target%0Adetection%2C%20followed%20by%20tracking%20with%20reliable%20initial%20frame%20identification%20and%0Adiversity-driven%20long-term%20memory.%20For%20the%20detection%20stage%2C%20we%20propose%20a%0Across-modal%20spatial-temporal%20Mamba%20to%20generate%20precise%20detection%20and%0Asegmentation%20results.%20Based%20on%20these%20results%2C%20our%20credible%20initial%20frame%0Aselection%20strategy%20identifies%20the%20reliable%20frame%20for%20the%20subsequent%20tracking.%0AUpon%20selecting%20the%20initial%20frame%2C%20our%20method%20transitions%20to%20the%20tracking%20stage%2C%0Awhere%20it%20incorporates%20a%20diversity-driven%20memory%20mechanism%20that%20maintains%20a%0Acredible%20and%20diverse%20memory%20bank%2C%20ensuring%20consistent%20long-term%20tracking.%0AExtensive%20experiments%20demonstrate%20that%20ReSurgSAM2%20achieves%20substantial%0Aimprovements%20in%20accuracy%20and%20efficiency%20compared%20to%20existing%20methods%2C%20operating%0Ain%20real-time%20at%2061.2%20FPS.%20Our%20code%20and%20datasets%20will%20be%20available%20at%0Ahttps%3A//github.com/jinlab-imvr/ReSurgSAM2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08581v1&entry.124074799=Read"},
{"title": "Model Steering: Learning with a Reference Model Improves Generalization\n  Bounds and Scaling Laws", "author": "Xiyuan Wei and Ming Lin and Fanjiang Ye and Fengguang Song and Liangliang Cao and My T. Thai and Tianbao Yang", "abstract": "  This paper formalizes an emerging learning paradigm that uses a trained model\nas a reference to guide and enhance the training of a target model through\nstrategic data selection or weighting, named $\\textbf{model steering}$. While\nad-hoc methods have been used in various contexts, including the training of\nlarge foundation models, its underlying principles remain insufficiently\nunderstood, leading to sub-optimal performance. In this work, we propose a\ntheory-driven framework for model steering called $\\textbf{DRRho risk\nminimization}$, which is rooted in Distributionally Robust Optimization (DRO).\nThrough a generalization analysis, we provide theoretical insights into why\nthis approach improves generalization and data efficiency compared to training\nwithout a reference model. To the best of our knowledge, this is the first time\nsuch theoretical insights are provided for the new learning paradigm, which\nsignificantly enhance our understanding and practice of model steering.\nBuilding on these insights and the connection between contrastive learning and\nDRO, we introduce a novel method for Contrastive Language-Image Pretraining\n(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments\nvalidate the theoretical insights, reveal a superior scaling law compared to\nCLIP without a reference model, and demonstrate its strength over existing\nheuristic approaches.\n", "link": "http://arxiv.org/abs/2505.06699v2", "date": "2025-05-13", "relevancy": 2.1593, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Steering%3A%20Learning%20with%20a%20Reference%20Model%20Improves%20Generalization%0A%20%20Bounds%20and%20Scaling%20Laws&body=Title%3A%20Model%20Steering%3A%20Learning%20with%20a%20Reference%20Model%20Improves%20Generalization%0A%20%20Bounds%20and%20Scaling%20Laws%0AAuthor%3A%20Xiyuan%20Wei%20and%20Ming%20Lin%20and%20Fanjiang%20Ye%20and%20Fengguang%20Song%20and%20Liangliang%20Cao%20and%20My%20T.%20Thai%20and%20Tianbao%20Yang%0AAbstract%3A%20%20%20This%20paper%20formalizes%20an%20emerging%20learning%20paradigm%20that%20uses%20a%20trained%20model%0Aas%20a%20reference%20to%20guide%20and%20enhance%20the%20training%20of%20a%20target%20model%20through%0Astrategic%20data%20selection%20or%20weighting%2C%20named%20%24%5Ctextbf%7Bmodel%20steering%7D%24.%20While%0Aad-hoc%20methods%20have%20been%20used%20in%20various%20contexts%2C%20including%20the%20training%20of%0Alarge%20foundation%20models%2C%20its%20underlying%20principles%20remain%20insufficiently%0Aunderstood%2C%20leading%20to%20sub-optimal%20performance.%20In%20this%20work%2C%20we%20propose%20a%0Atheory-driven%20framework%20for%20model%20steering%20called%20%24%5Ctextbf%7BDRRho%20risk%0Aminimization%7D%24%2C%20which%20is%20rooted%20in%20Distributionally%20Robust%20Optimization%20%28DRO%29.%0AThrough%20a%20generalization%20analysis%2C%20we%20provide%20theoretical%20insights%20into%20why%0Athis%20approach%20improves%20generalization%20and%20data%20efficiency%20compared%20to%20training%0Awithout%20a%20reference%20model.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20time%0Asuch%20theoretical%20insights%20are%20provided%20for%20the%20new%20learning%20paradigm%2C%20which%0Asignificantly%20enhance%20our%20understanding%20and%20practice%20of%20model%20steering.%0ABuilding%20on%20these%20insights%20and%20the%20connection%20between%20contrastive%20learning%20and%0ADRO%2C%20we%20introduce%20a%20novel%20method%20for%20Contrastive%20Language-Image%20Pretraining%0A%28CLIP%29%20with%20a%20reference%20model%2C%20termed%20DRRho-CLIP.%20Extensive%20experiments%0Avalidate%20the%20theoretical%20insights%2C%20reveal%20a%20superior%20scaling%20law%20compared%20to%0ACLIP%20without%20a%20reference%20model%2C%20and%20demonstrate%20its%20strength%20over%20existing%0Aheuristic%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06699v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Steering%253A%2520Learning%2520with%2520a%2520Reference%2520Model%2520Improves%2520Generalization%250A%2520%2520Bounds%2520and%2520Scaling%2520Laws%26entry.906535625%3DXiyuan%2520Wei%2520and%2520Ming%2520Lin%2520and%2520Fanjiang%2520Ye%2520and%2520Fengguang%2520Song%2520and%2520Liangliang%2520Cao%2520and%2520My%2520T.%2520Thai%2520and%2520Tianbao%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520formalizes%2520an%2520emerging%2520learning%2520paradigm%2520that%2520uses%2520a%2520trained%2520model%250Aas%2520a%2520reference%2520to%2520guide%2520and%2520enhance%2520the%2520training%2520of%2520a%2520target%2520model%2520through%250Astrategic%2520data%2520selection%2520or%2520weighting%252C%2520named%2520%2524%255Ctextbf%257Bmodel%2520steering%257D%2524.%2520While%250Aad-hoc%2520methods%2520have%2520been%2520used%2520in%2520various%2520contexts%252C%2520including%2520the%2520training%2520of%250Alarge%2520foundation%2520models%252C%2520its%2520underlying%2520principles%2520remain%2520insufficiently%250Aunderstood%252C%2520leading%2520to%2520sub-optimal%2520performance.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Atheory-driven%2520framework%2520for%2520model%2520steering%2520called%2520%2524%255Ctextbf%257BDRRho%2520risk%250Aminimization%257D%2524%252C%2520which%2520is%2520rooted%2520in%2520Distributionally%2520Robust%2520Optimization%2520%2528DRO%2529.%250AThrough%2520a%2520generalization%2520analysis%252C%2520we%2520provide%2520theoretical%2520insights%2520into%2520why%250Athis%2520approach%2520improves%2520generalization%2520and%2520data%2520efficiency%2520compared%2520to%2520training%250Awithout%2520a%2520reference%2520model.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520time%250Asuch%2520theoretical%2520insights%2520are%2520provided%2520for%2520the%2520new%2520learning%2520paradigm%252C%2520which%250Asignificantly%2520enhance%2520our%2520understanding%2520and%2520practice%2520of%2520model%2520steering.%250ABuilding%2520on%2520these%2520insights%2520and%2520the%2520connection%2520between%2520contrastive%2520learning%2520and%250ADRO%252C%2520we%2520introduce%2520a%2520novel%2520method%2520for%2520Contrastive%2520Language-Image%2520Pretraining%250A%2528CLIP%2529%2520with%2520a%2520reference%2520model%252C%2520termed%2520DRRho-CLIP.%2520Extensive%2520experiments%250Avalidate%2520the%2520theoretical%2520insights%252C%2520reveal%2520a%2520superior%2520scaling%2520law%2520compared%2520to%250ACLIP%2520without%2520a%2520reference%2520model%252C%2520and%2520demonstrate%2520its%2520strength%2520over%2520existing%250Aheuristic%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06699v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Steering%3A%20Learning%20with%20a%20Reference%20Model%20Improves%20Generalization%0A%20%20Bounds%20and%20Scaling%20Laws&entry.906535625=Xiyuan%20Wei%20and%20Ming%20Lin%20and%20Fanjiang%20Ye%20and%20Fengguang%20Song%20and%20Liangliang%20Cao%20and%20My%20T.%20Thai%20and%20Tianbao%20Yang&entry.1292438233=%20%20This%20paper%20formalizes%20an%20emerging%20learning%20paradigm%20that%20uses%20a%20trained%20model%0Aas%20a%20reference%20to%20guide%20and%20enhance%20the%20training%20of%20a%20target%20model%20through%0Astrategic%20data%20selection%20or%20weighting%2C%20named%20%24%5Ctextbf%7Bmodel%20steering%7D%24.%20While%0Aad-hoc%20methods%20have%20been%20used%20in%20various%20contexts%2C%20including%20the%20training%20of%0Alarge%20foundation%20models%2C%20its%20underlying%20principles%20remain%20insufficiently%0Aunderstood%2C%20leading%20to%20sub-optimal%20performance.%20In%20this%20work%2C%20we%20propose%20a%0Atheory-driven%20framework%20for%20model%20steering%20called%20%24%5Ctextbf%7BDRRho%20risk%0Aminimization%7D%24%2C%20which%20is%20rooted%20in%20Distributionally%20Robust%20Optimization%20%28DRO%29.%0AThrough%20a%20generalization%20analysis%2C%20we%20provide%20theoretical%20insights%20into%20why%0Athis%20approach%20improves%20generalization%20and%20data%20efficiency%20compared%20to%20training%0Awithout%20a%20reference%20model.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20time%0Asuch%20theoretical%20insights%20are%20provided%20for%20the%20new%20learning%20paradigm%2C%20which%0Asignificantly%20enhance%20our%20understanding%20and%20practice%20of%20model%20steering.%0ABuilding%20on%20these%20insights%20and%20the%20connection%20between%20contrastive%20learning%20and%0ADRO%2C%20we%20introduce%20a%20novel%20method%20for%20Contrastive%20Language-Image%20Pretraining%0A%28CLIP%29%20with%20a%20reference%20model%2C%20termed%20DRRho-CLIP.%20Extensive%20experiments%0Avalidate%20the%20theoretical%20insights%2C%20reveal%20a%20superior%20scaling%20law%20compared%20to%0ACLIP%20without%20a%20reference%20model%2C%20and%20demonstrate%20its%20strength%20over%20existing%0Aheuristic%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06699v2&entry.124074799=Read"},
{"title": "PCS-UQ: Uncertainty Quantification via the\n  Predictability-Computability-Stability Framework", "author": "Abhineet Agarwal and Michael Xiao and Rebecca Barter and Omer Ronen and Boyu Fan and Bin Yu", "abstract": "  As machine learning (ML) models are increasingly deployed in high-stakes\ndomains, trustworthy uncertainty quantification (UQ) is critical for ensuring\nthe safety and reliability of these models. Traditional UQ methods rely on\nspecifying a true generative model and are not robust to misspecification. On\nthe other hand, conformal inference allows for arbitrary ML models but does not\nconsider model selection, which leads to large interval sizes. We tackle these\ndrawbacks by proposing a UQ method based on the predictability, computability,\nand stability (PCS) framework for veridical data science proposed by Yu and\nKumbier. Specifically, PCS-UQ addresses model selection by using a prediction\ncheck to screen out unsuitable models. PCS-UQ then fits these screened\nalgorithms across multiple bootstraps to assess inter-sample variability and\nalgorithmic instability, enabling more reliable uncertainty estimates. Further,\nwe propose a novel calibration scheme that improves local adaptivity of our\nprediction sets. Experiments across $17$ regression and $6$ classification\ndatasets show that PCS-UQ achieves the desired coverage and reduces width over\nconformal approaches by $\\approx 20\\%$. Further, our local analysis shows\nPCS-UQ often achieves target coverage across subgroups while conformal methods\nfail to do so. For large deep-learning models, we propose computationally\nefficient approximation schemes that avoid the expensive multiple bootstrap\ntrainings of PCS-UQ. Across three computer vision benchmarks, PCS-UQ reduces\nprediction set size over conformal methods by $20\\%$. Theoretically, we show a\nmodified PCS-UQ algorithm is a form of split conformal inference and achieves\nthe desired coverage with exchangeable data.\n", "link": "http://arxiv.org/abs/2505.08784v1", "date": "2025-05-13", "relevancy": 2.1562, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5617}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5597}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCS-UQ%3A%20Uncertainty%20Quantification%20via%20the%0A%20%20Predictability-Computability-Stability%20Framework&body=Title%3A%20PCS-UQ%3A%20Uncertainty%20Quantification%20via%20the%0A%20%20Predictability-Computability-Stability%20Framework%0AAuthor%3A%20Abhineet%20Agarwal%20and%20Michael%20Xiao%20and%20Rebecca%20Barter%20and%20Omer%20Ronen%20and%20Boyu%20Fan%20and%20Bin%20Yu%0AAbstract%3A%20%20%20As%20machine%20learning%20%28ML%29%20models%20are%20increasingly%20deployed%20in%20high-stakes%0Adomains%2C%20trustworthy%20uncertainty%20quantification%20%28UQ%29%20is%20critical%20for%20ensuring%0Athe%20safety%20and%20reliability%20of%20these%20models.%20Traditional%20UQ%20methods%20rely%20on%0Aspecifying%20a%20true%20generative%20model%20and%20are%20not%20robust%20to%20misspecification.%20On%0Athe%20other%20hand%2C%20conformal%20inference%20allows%20for%20arbitrary%20ML%20models%20but%20does%20not%0Aconsider%20model%20selection%2C%20which%20leads%20to%20large%20interval%20sizes.%20We%20tackle%20these%0Adrawbacks%20by%20proposing%20a%20UQ%20method%20based%20on%20the%20predictability%2C%20computability%2C%0Aand%20stability%20%28PCS%29%20framework%20for%20veridical%20data%20science%20proposed%20by%20Yu%20and%0AKumbier.%20Specifically%2C%20PCS-UQ%20addresses%20model%20selection%20by%20using%20a%20prediction%0Acheck%20to%20screen%20out%20unsuitable%20models.%20PCS-UQ%20then%20fits%20these%20screened%0Aalgorithms%20across%20multiple%20bootstraps%20to%20assess%20inter-sample%20variability%20and%0Aalgorithmic%20instability%2C%20enabling%20more%20reliable%20uncertainty%20estimates.%20Further%2C%0Awe%20propose%20a%20novel%20calibration%20scheme%20that%20improves%20local%20adaptivity%20of%20our%0Aprediction%20sets.%20Experiments%20across%20%2417%24%20regression%20and%20%246%24%20classification%0Adatasets%20show%20that%20PCS-UQ%20achieves%20the%20desired%20coverage%20and%20reduces%20width%20over%0Aconformal%20approaches%20by%20%24%5Capprox%2020%5C%25%24.%20Further%2C%20our%20local%20analysis%20shows%0APCS-UQ%20often%20achieves%20target%20coverage%20across%20subgroups%20while%20conformal%20methods%0Afail%20to%20do%20so.%20For%20large%20deep-learning%20models%2C%20we%20propose%20computationally%0Aefficient%20approximation%20schemes%20that%20avoid%20the%20expensive%20multiple%20bootstrap%0Atrainings%20of%20PCS-UQ.%20Across%20three%20computer%20vision%20benchmarks%2C%20PCS-UQ%20reduces%0Aprediction%20set%20size%20over%20conformal%20methods%20by%20%2420%5C%25%24.%20Theoretically%2C%20we%20show%20a%0Amodified%20PCS-UQ%20algorithm%20is%20a%20form%20of%20split%20conformal%20inference%20and%20achieves%0Athe%20desired%20coverage%20with%20exchangeable%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCS-UQ%253A%2520Uncertainty%2520Quantification%2520via%2520the%250A%2520%2520Predictability-Computability-Stability%2520Framework%26entry.906535625%3DAbhineet%2520Agarwal%2520and%2520Michael%2520Xiao%2520and%2520Rebecca%2520Barter%2520and%2520Omer%2520Ronen%2520and%2520Boyu%2520Fan%2520and%2520Bin%2520Yu%26entry.1292438233%3D%2520%2520As%2520machine%2520learning%2520%2528ML%2529%2520models%2520are%2520increasingly%2520deployed%2520in%2520high-stakes%250Adomains%252C%2520trustworthy%2520uncertainty%2520quantification%2520%2528UQ%2529%2520is%2520critical%2520for%2520ensuring%250Athe%2520safety%2520and%2520reliability%2520of%2520these%2520models.%2520Traditional%2520UQ%2520methods%2520rely%2520on%250Aspecifying%2520a%2520true%2520generative%2520model%2520and%2520are%2520not%2520robust%2520to%2520misspecification.%2520On%250Athe%2520other%2520hand%252C%2520conformal%2520inference%2520allows%2520for%2520arbitrary%2520ML%2520models%2520but%2520does%2520not%250Aconsider%2520model%2520selection%252C%2520which%2520leads%2520to%2520large%2520interval%2520sizes.%2520We%2520tackle%2520these%250Adrawbacks%2520by%2520proposing%2520a%2520UQ%2520method%2520based%2520on%2520the%2520predictability%252C%2520computability%252C%250Aand%2520stability%2520%2528PCS%2529%2520framework%2520for%2520veridical%2520data%2520science%2520proposed%2520by%2520Yu%2520and%250AKumbier.%2520Specifically%252C%2520PCS-UQ%2520addresses%2520model%2520selection%2520by%2520using%2520a%2520prediction%250Acheck%2520to%2520screen%2520out%2520unsuitable%2520models.%2520PCS-UQ%2520then%2520fits%2520these%2520screened%250Aalgorithms%2520across%2520multiple%2520bootstraps%2520to%2520assess%2520inter-sample%2520variability%2520and%250Aalgorithmic%2520instability%252C%2520enabling%2520more%2520reliable%2520uncertainty%2520estimates.%2520Further%252C%250Awe%2520propose%2520a%2520novel%2520calibration%2520scheme%2520that%2520improves%2520local%2520adaptivity%2520of%2520our%250Aprediction%2520sets.%2520Experiments%2520across%2520%252417%2524%2520regression%2520and%2520%25246%2524%2520classification%250Adatasets%2520show%2520that%2520PCS-UQ%2520achieves%2520the%2520desired%2520coverage%2520and%2520reduces%2520width%2520over%250Aconformal%2520approaches%2520by%2520%2524%255Capprox%252020%255C%2525%2524.%2520Further%252C%2520our%2520local%2520analysis%2520shows%250APCS-UQ%2520often%2520achieves%2520target%2520coverage%2520across%2520subgroups%2520while%2520conformal%2520methods%250Afail%2520to%2520do%2520so.%2520For%2520large%2520deep-learning%2520models%252C%2520we%2520propose%2520computationally%250Aefficient%2520approximation%2520schemes%2520that%2520avoid%2520the%2520expensive%2520multiple%2520bootstrap%250Atrainings%2520of%2520PCS-UQ.%2520Across%2520three%2520computer%2520vision%2520benchmarks%252C%2520PCS-UQ%2520reduces%250Aprediction%2520set%2520size%2520over%2520conformal%2520methods%2520by%2520%252420%255C%2525%2524.%2520Theoretically%252C%2520we%2520show%2520a%250Amodified%2520PCS-UQ%2520algorithm%2520is%2520a%2520form%2520of%2520split%2520conformal%2520inference%2520and%2520achieves%250Athe%2520desired%2520coverage%2520with%2520exchangeable%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCS-UQ%3A%20Uncertainty%20Quantification%20via%20the%0A%20%20Predictability-Computability-Stability%20Framework&entry.906535625=Abhineet%20Agarwal%20and%20Michael%20Xiao%20and%20Rebecca%20Barter%20and%20Omer%20Ronen%20and%20Boyu%20Fan%20and%20Bin%20Yu&entry.1292438233=%20%20As%20machine%20learning%20%28ML%29%20models%20are%20increasingly%20deployed%20in%20high-stakes%0Adomains%2C%20trustworthy%20uncertainty%20quantification%20%28UQ%29%20is%20critical%20for%20ensuring%0Athe%20safety%20and%20reliability%20of%20these%20models.%20Traditional%20UQ%20methods%20rely%20on%0Aspecifying%20a%20true%20generative%20model%20and%20are%20not%20robust%20to%20misspecification.%20On%0Athe%20other%20hand%2C%20conformal%20inference%20allows%20for%20arbitrary%20ML%20models%20but%20does%20not%0Aconsider%20model%20selection%2C%20which%20leads%20to%20large%20interval%20sizes.%20We%20tackle%20these%0Adrawbacks%20by%20proposing%20a%20UQ%20method%20based%20on%20the%20predictability%2C%20computability%2C%0Aand%20stability%20%28PCS%29%20framework%20for%20veridical%20data%20science%20proposed%20by%20Yu%20and%0AKumbier.%20Specifically%2C%20PCS-UQ%20addresses%20model%20selection%20by%20using%20a%20prediction%0Acheck%20to%20screen%20out%20unsuitable%20models.%20PCS-UQ%20then%20fits%20these%20screened%0Aalgorithms%20across%20multiple%20bootstraps%20to%20assess%20inter-sample%20variability%20and%0Aalgorithmic%20instability%2C%20enabling%20more%20reliable%20uncertainty%20estimates.%20Further%2C%0Awe%20propose%20a%20novel%20calibration%20scheme%20that%20improves%20local%20adaptivity%20of%20our%0Aprediction%20sets.%20Experiments%20across%20%2417%24%20regression%20and%20%246%24%20classification%0Adatasets%20show%20that%20PCS-UQ%20achieves%20the%20desired%20coverage%20and%20reduces%20width%20over%0Aconformal%20approaches%20by%20%24%5Capprox%2020%5C%25%24.%20Further%2C%20our%20local%20analysis%20shows%0APCS-UQ%20often%20achieves%20target%20coverage%20across%20subgroups%20while%20conformal%20methods%0Afail%20to%20do%20so.%20For%20large%20deep-learning%20models%2C%20we%20propose%20computationally%0Aefficient%20approximation%20schemes%20that%20avoid%20the%20expensive%20multiple%20bootstrap%0Atrainings%20of%20PCS-UQ.%20Across%20three%20computer%20vision%20benchmarks%2C%20PCS-UQ%20reduces%0Aprediction%20set%20size%20over%20conformal%20methods%20by%20%2420%5C%25%24.%20Theoretically%2C%20we%20show%20a%0Amodified%20PCS-UQ%20algorithm%20is%20a%20form%20of%20split%20conformal%20inference%20and%20achieves%0Athe%20desired%20coverage%20with%20exchangeable%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08784v1&entry.124074799=Read"},
{"title": "TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series", "author": "Xiaolei Qin and Di Wang and Jing Zhang and Fengxiang Wang and Xin Su and Bo Du and Liangpei Zhang", "abstract": "  Satellite image time series (SITS) provide continuous observations of the\nEarth's surface, making them essential for applications such as environmental\nmanagement and disaster assessment. However, existing spatiotemporal foundation\nmodels rely on plain vision transformers, which encode entire temporal\nsequences without explicitly capturing multiscale spatiotemporal relationships\nbetween land objects. This limitation hinders their effectiveness in downstream\ntasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision\ntransformer foundation model tailored for SITS analysis. At its core, we\nintroduce a spatiotemporal gyroscope attention mechanism that dynamically\ncaptures evolving multiscale patterns across both time and space. For\npre-training, we curate MillionST, a large-scale dataset of one million images\nfrom 100,000 geographic locations, each captured across 10 temporal phases over\nfive years, encompassing diverse geospatial changes and seasonal variations.\nLeveraging this dataset, we adapt masked image modeling to pre-train TiMo,\nenabling it to effectively learn and encode generalizable spatiotemporal\nrepresentations.Extensive experiments across multiple spatiotemporal\ntasks-including deforestation monitoring, land cover segmentation, crop type\nclassification, and flood detection-demonstrate TiMo's superiority over\nstate-of-the-art methods. Code, model, and dataset will be released at\nhttps://github.com/MiliLab/TiMo.\n", "link": "http://arxiv.org/abs/2505.08723v1", "date": "2025-05-13", "relevancy": 2.1518, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5653}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5281}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiMo%3A%20Spatiotemporal%20Foundation%20Model%20for%20Satellite%20Image%20Time%20Series&body=Title%3A%20TiMo%3A%20Spatiotemporal%20Foundation%20Model%20for%20Satellite%20Image%20Time%20Series%0AAuthor%3A%20Xiaolei%20Qin%20and%20Di%20Wang%20and%20Jing%20Zhang%20and%20Fengxiang%20Wang%20and%20Xin%20Su%20and%20Bo%20Du%20and%20Liangpei%20Zhang%0AAbstract%3A%20%20%20Satellite%20image%20time%20series%20%28SITS%29%20provide%20continuous%20observations%20of%20the%0AEarth%27s%20surface%2C%20making%20them%20essential%20for%20applications%20such%20as%20environmental%0Amanagement%20and%20disaster%20assessment.%20However%2C%20existing%20spatiotemporal%20foundation%0Amodels%20rely%20on%20plain%20vision%20transformers%2C%20which%20encode%20entire%20temporal%0Asequences%20without%20explicitly%20capturing%20multiscale%20spatiotemporal%20relationships%0Abetween%20land%20objects.%20This%20limitation%20hinders%20their%20effectiveness%20in%20downstream%0Atasks.%20To%20overcome%20this%20challenge%2C%20we%20propose%20TiMo%2C%20a%20novel%20hierarchical%20vision%0Atransformer%20foundation%20model%20tailored%20for%20SITS%20analysis.%20At%20its%20core%2C%20we%0Aintroduce%20a%20spatiotemporal%20gyroscope%20attention%20mechanism%20that%20dynamically%0Acaptures%20evolving%20multiscale%20patterns%20across%20both%20time%20and%20space.%20For%0Apre-training%2C%20we%20curate%20MillionST%2C%20a%20large-scale%20dataset%20of%20one%20million%20images%0Afrom%20100%2C000%20geographic%20locations%2C%20each%20captured%20across%2010%20temporal%20phases%20over%0Afive%20years%2C%20encompassing%20diverse%20geospatial%20changes%20and%20seasonal%20variations.%0ALeveraging%20this%20dataset%2C%20we%20adapt%20masked%20image%20modeling%20to%20pre-train%20TiMo%2C%0Aenabling%20it%20to%20effectively%20learn%20and%20encode%20generalizable%20spatiotemporal%0Arepresentations.Extensive%20experiments%20across%20multiple%20spatiotemporal%0Atasks-including%20deforestation%20monitoring%2C%20land%20cover%20segmentation%2C%20crop%20type%0Aclassification%2C%20and%20flood%20detection-demonstrate%20TiMo%27s%20superiority%20over%0Astate-of-the-art%20methods.%20Code%2C%20model%2C%20and%20dataset%20will%20be%20released%20at%0Ahttps%3A//github.com/MiliLab/TiMo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiMo%253A%2520Spatiotemporal%2520Foundation%2520Model%2520for%2520Satellite%2520Image%2520Time%2520Series%26entry.906535625%3DXiaolei%2520Qin%2520and%2520Di%2520Wang%2520and%2520Jing%2520Zhang%2520and%2520Fengxiang%2520Wang%2520and%2520Xin%2520Su%2520and%2520Bo%2520Du%2520and%2520Liangpei%2520Zhang%26entry.1292438233%3D%2520%2520Satellite%2520image%2520time%2520series%2520%2528SITS%2529%2520provide%2520continuous%2520observations%2520of%2520the%250AEarth%2527s%2520surface%252C%2520making%2520them%2520essential%2520for%2520applications%2520such%2520as%2520environmental%250Amanagement%2520and%2520disaster%2520assessment.%2520However%252C%2520existing%2520spatiotemporal%2520foundation%250Amodels%2520rely%2520on%2520plain%2520vision%2520transformers%252C%2520which%2520encode%2520entire%2520temporal%250Asequences%2520without%2520explicitly%2520capturing%2520multiscale%2520spatiotemporal%2520relationships%250Abetween%2520land%2520objects.%2520This%2520limitation%2520hinders%2520their%2520effectiveness%2520in%2520downstream%250Atasks.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%2520TiMo%252C%2520a%2520novel%2520hierarchical%2520vision%250Atransformer%2520foundation%2520model%2520tailored%2520for%2520SITS%2520analysis.%2520At%2520its%2520core%252C%2520we%250Aintroduce%2520a%2520spatiotemporal%2520gyroscope%2520attention%2520mechanism%2520that%2520dynamically%250Acaptures%2520evolving%2520multiscale%2520patterns%2520across%2520both%2520time%2520and%2520space.%2520For%250Apre-training%252C%2520we%2520curate%2520MillionST%252C%2520a%2520large-scale%2520dataset%2520of%2520one%2520million%2520images%250Afrom%2520100%252C000%2520geographic%2520locations%252C%2520each%2520captured%2520across%252010%2520temporal%2520phases%2520over%250Afive%2520years%252C%2520encompassing%2520diverse%2520geospatial%2520changes%2520and%2520seasonal%2520variations.%250ALeveraging%2520this%2520dataset%252C%2520we%2520adapt%2520masked%2520image%2520modeling%2520to%2520pre-train%2520TiMo%252C%250Aenabling%2520it%2520to%2520effectively%2520learn%2520and%2520encode%2520generalizable%2520spatiotemporal%250Arepresentations.Extensive%2520experiments%2520across%2520multiple%2520spatiotemporal%250Atasks-including%2520deforestation%2520monitoring%252C%2520land%2520cover%2520segmentation%252C%2520crop%2520type%250Aclassification%252C%2520and%2520flood%2520detection-demonstrate%2520TiMo%2527s%2520superiority%2520over%250Astate-of-the-art%2520methods.%2520Code%252C%2520model%252C%2520and%2520dataset%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/MiliLab/TiMo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiMo%3A%20Spatiotemporal%20Foundation%20Model%20for%20Satellite%20Image%20Time%20Series&entry.906535625=Xiaolei%20Qin%20and%20Di%20Wang%20and%20Jing%20Zhang%20and%20Fengxiang%20Wang%20and%20Xin%20Su%20and%20Bo%20Du%20and%20Liangpei%20Zhang&entry.1292438233=%20%20Satellite%20image%20time%20series%20%28SITS%29%20provide%20continuous%20observations%20of%20the%0AEarth%27s%20surface%2C%20making%20them%20essential%20for%20applications%20such%20as%20environmental%0Amanagement%20and%20disaster%20assessment.%20However%2C%20existing%20spatiotemporal%20foundation%0Amodels%20rely%20on%20plain%20vision%20transformers%2C%20which%20encode%20entire%20temporal%0Asequences%20without%20explicitly%20capturing%20multiscale%20spatiotemporal%20relationships%0Abetween%20land%20objects.%20This%20limitation%20hinders%20their%20effectiveness%20in%20downstream%0Atasks.%20To%20overcome%20this%20challenge%2C%20we%20propose%20TiMo%2C%20a%20novel%20hierarchical%20vision%0Atransformer%20foundation%20model%20tailored%20for%20SITS%20analysis.%20At%20its%20core%2C%20we%0Aintroduce%20a%20spatiotemporal%20gyroscope%20attention%20mechanism%20that%20dynamically%0Acaptures%20evolving%20multiscale%20patterns%20across%20both%20time%20and%20space.%20For%0Apre-training%2C%20we%20curate%20MillionST%2C%20a%20large-scale%20dataset%20of%20one%20million%20images%0Afrom%20100%2C000%20geographic%20locations%2C%20each%20captured%20across%2010%20temporal%20phases%20over%0Afive%20years%2C%20encompassing%20diverse%20geospatial%20changes%20and%20seasonal%20variations.%0ALeveraging%20this%20dataset%2C%20we%20adapt%20masked%20image%20modeling%20to%20pre-train%20TiMo%2C%0Aenabling%20it%20to%20effectively%20learn%20and%20encode%20generalizable%20spatiotemporal%0Arepresentations.Extensive%20experiments%20across%20multiple%20spatiotemporal%0Atasks-including%20deforestation%20monitoring%2C%20land%20cover%20segmentation%2C%20crop%20type%0Aclassification%2C%20and%20flood%20detection-demonstrate%20TiMo%27s%20superiority%20over%0Astate-of-the-art%20methods.%20Code%2C%20model%2C%20and%20dataset%20will%20be%20released%20at%0Ahttps%3A//github.com/MiliLab/TiMo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08723v1&entry.124074799=Read"},
{"title": "Unsupervised Out-of-Distribution Detection in Medical Imaging Using\n  Multi-Exit Class Activation Maps and Feature Masking", "author": "Yu-Jen Chen and Xueyang Li and Yiyu Shi and Tsung-Yi Ho", "abstract": "  Out-of-distribution (OOD) detection is essential for ensuring the reliability\nof deep learning models in medical imaging applications. This work is motivated\nby the observation that class activation maps (CAMs) for in-distribution (ID)\ndata typically emphasize regions that are highly relevant to the model's\npredictions, whereas OOD data often lacks such focused activations. By masking\ninput images with inverted CAMs, the feature representations of ID data undergo\nmore substantial changes compared to those of OOD data, offering a robust\ncriterion for differentiation. In this paper, we introduce a novel unsupervised\nOOD detection framework, Multi-Exit Class Activation Map (MECAM), which\nleverages multi-exit CAMs and feature masking. By utilizing mult-exit networks\nthat combine CAMs from varying resolutions and depths, our method captures both\nglobal and local feature representations, thereby enhancing the robustness of\nOOD detection. We evaluate MECAM on multiple ID datasets, including ISIC19 and\nPathMNIST, and test its performance against three medical OOD datasets, RSNA\nPneumonia, COVID-19, and HeadCT, and one natural image OOD dataset, iSUN.\nComprehensive comparisons with state-of-the-art OOD detection methods validate\nthe effectiveness of our approach. Our findings emphasize the potential of\nmulti-exit networks and feature masking for advancing unsupervised OOD\ndetection in medical imaging, paving the way for more reliable and\ninterpretable models in clinical practice.\n", "link": "http://arxiv.org/abs/2505.08604v1", "date": "2025-05-13", "relevancy": 2.1344, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5658}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5274}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Out-of-Distribution%20Detection%20in%20Medical%20Imaging%20Using%0A%20%20Multi-Exit%20Class%20Activation%20Maps%20and%20Feature%20Masking&body=Title%3A%20Unsupervised%20Out-of-Distribution%20Detection%20in%20Medical%20Imaging%20Using%0A%20%20Multi-Exit%20Class%20Activation%20Maps%20and%20Feature%20Masking%0AAuthor%3A%20Yu-Jen%20Chen%20and%20Xueyang%20Li%20and%20Yiyu%20Shi%20and%20Tsung-Yi%20Ho%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20essential%20for%20ensuring%20the%20reliability%0Aof%20deep%20learning%20models%20in%20medical%20imaging%20applications.%20This%20work%20is%20motivated%0Aby%20the%20observation%20that%20class%20activation%20maps%20%28CAMs%29%20for%20in-distribution%20%28ID%29%0Adata%20typically%20emphasize%20regions%20that%20are%20highly%20relevant%20to%20the%20model%27s%0Apredictions%2C%20whereas%20OOD%20data%20often%20lacks%20such%20focused%20activations.%20By%20masking%0Ainput%20images%20with%20inverted%20CAMs%2C%20the%20feature%20representations%20of%20ID%20data%20undergo%0Amore%20substantial%20changes%20compared%20to%20those%20of%20OOD%20data%2C%20offering%20a%20robust%0Acriterion%20for%20differentiation.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20unsupervised%0AOOD%20detection%20framework%2C%20Multi-Exit%20Class%20Activation%20Map%20%28MECAM%29%2C%20which%0Aleverages%20multi-exit%20CAMs%20and%20feature%20masking.%20By%20utilizing%20mult-exit%20networks%0Athat%20combine%20CAMs%20from%20varying%20resolutions%20and%20depths%2C%20our%20method%20captures%20both%0Aglobal%20and%20local%20feature%20representations%2C%20thereby%20enhancing%20the%20robustness%20of%0AOOD%20detection.%20We%20evaluate%20MECAM%20on%20multiple%20ID%20datasets%2C%20including%20ISIC19%20and%0APathMNIST%2C%20and%20test%20its%20performance%20against%20three%20medical%20OOD%20datasets%2C%20RSNA%0APneumonia%2C%20COVID-19%2C%20and%20HeadCT%2C%20and%20one%20natural%20image%20OOD%20dataset%2C%20iSUN.%0AComprehensive%20comparisons%20with%20state-of-the-art%20OOD%20detection%20methods%20validate%0Athe%20effectiveness%20of%20our%20approach.%20Our%20findings%20emphasize%20the%20potential%20of%0Amulti-exit%20networks%20and%20feature%20masking%20for%20advancing%20unsupervised%20OOD%0Adetection%20in%20medical%20imaging%2C%20paving%20the%20way%20for%20more%20reliable%20and%0Ainterpretable%20models%20in%20clinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Out-of-Distribution%2520Detection%2520in%2520Medical%2520Imaging%2520Using%250A%2520%2520Multi-Exit%2520Class%2520Activation%2520Maps%2520and%2520Feature%2520Masking%26entry.906535625%3DYu-Jen%2520Chen%2520and%2520Xueyang%2520Li%2520and%2520Yiyu%2520Shi%2520and%2520Tsung-Yi%2520Ho%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520is%2520essential%2520for%2520ensuring%2520the%2520reliability%250Aof%2520deep%2520learning%2520models%2520in%2520medical%2520imaging%2520applications.%2520This%2520work%2520is%2520motivated%250Aby%2520the%2520observation%2520that%2520class%2520activation%2520maps%2520%2528CAMs%2529%2520for%2520in-distribution%2520%2528ID%2529%250Adata%2520typically%2520emphasize%2520regions%2520that%2520are%2520highly%2520relevant%2520to%2520the%2520model%2527s%250Apredictions%252C%2520whereas%2520OOD%2520data%2520often%2520lacks%2520such%2520focused%2520activations.%2520By%2520masking%250Ainput%2520images%2520with%2520inverted%2520CAMs%252C%2520the%2520feature%2520representations%2520of%2520ID%2520data%2520undergo%250Amore%2520substantial%2520changes%2520compared%2520to%2520those%2520of%2520OOD%2520data%252C%2520offering%2520a%2520robust%250Acriterion%2520for%2520differentiation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520unsupervised%250AOOD%2520detection%2520framework%252C%2520Multi-Exit%2520Class%2520Activation%2520Map%2520%2528MECAM%2529%252C%2520which%250Aleverages%2520multi-exit%2520CAMs%2520and%2520feature%2520masking.%2520By%2520utilizing%2520mult-exit%2520networks%250Athat%2520combine%2520CAMs%2520from%2520varying%2520resolutions%2520and%2520depths%252C%2520our%2520method%2520captures%2520both%250Aglobal%2520and%2520local%2520feature%2520representations%252C%2520thereby%2520enhancing%2520the%2520robustness%2520of%250AOOD%2520detection.%2520We%2520evaluate%2520MECAM%2520on%2520multiple%2520ID%2520datasets%252C%2520including%2520ISIC19%2520and%250APathMNIST%252C%2520and%2520test%2520its%2520performance%2520against%2520three%2520medical%2520OOD%2520datasets%252C%2520RSNA%250APneumonia%252C%2520COVID-19%252C%2520and%2520HeadCT%252C%2520and%2520one%2520natural%2520image%2520OOD%2520dataset%252C%2520iSUN.%250AComprehensive%2520comparisons%2520with%2520state-of-the-art%2520OOD%2520detection%2520methods%2520validate%250Athe%2520effectiveness%2520of%2520our%2520approach.%2520Our%2520findings%2520emphasize%2520the%2520potential%2520of%250Amulti-exit%2520networks%2520and%2520feature%2520masking%2520for%2520advancing%2520unsupervised%2520OOD%250Adetection%2520in%2520medical%2520imaging%252C%2520paving%2520the%2520way%2520for%2520more%2520reliable%2520and%250Ainterpretable%2520models%2520in%2520clinical%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Out-of-Distribution%20Detection%20in%20Medical%20Imaging%20Using%0A%20%20Multi-Exit%20Class%20Activation%20Maps%20and%20Feature%20Masking&entry.906535625=Yu-Jen%20Chen%20and%20Xueyang%20Li%20and%20Yiyu%20Shi%20and%20Tsung-Yi%20Ho&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20essential%20for%20ensuring%20the%20reliability%0Aof%20deep%20learning%20models%20in%20medical%20imaging%20applications.%20This%20work%20is%20motivated%0Aby%20the%20observation%20that%20class%20activation%20maps%20%28CAMs%29%20for%20in-distribution%20%28ID%29%0Adata%20typically%20emphasize%20regions%20that%20are%20highly%20relevant%20to%20the%20model%27s%0Apredictions%2C%20whereas%20OOD%20data%20often%20lacks%20such%20focused%20activations.%20By%20masking%0Ainput%20images%20with%20inverted%20CAMs%2C%20the%20feature%20representations%20of%20ID%20data%20undergo%0Amore%20substantial%20changes%20compared%20to%20those%20of%20OOD%20data%2C%20offering%20a%20robust%0Acriterion%20for%20differentiation.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20unsupervised%0AOOD%20detection%20framework%2C%20Multi-Exit%20Class%20Activation%20Map%20%28MECAM%29%2C%20which%0Aleverages%20multi-exit%20CAMs%20and%20feature%20masking.%20By%20utilizing%20mult-exit%20networks%0Athat%20combine%20CAMs%20from%20varying%20resolutions%20and%20depths%2C%20our%20method%20captures%20both%0Aglobal%20and%20local%20feature%20representations%2C%20thereby%20enhancing%20the%20robustness%20of%0AOOD%20detection.%20We%20evaluate%20MECAM%20on%20multiple%20ID%20datasets%2C%20including%20ISIC19%20and%0APathMNIST%2C%20and%20test%20its%20performance%20against%20three%20medical%20OOD%20datasets%2C%20RSNA%0APneumonia%2C%20COVID-19%2C%20and%20HeadCT%2C%20and%20one%20natural%20image%20OOD%20dataset%2C%20iSUN.%0AComprehensive%20comparisons%20with%20state-of-the-art%20OOD%20detection%20methods%20validate%0Athe%20effectiveness%20of%20our%20approach.%20Our%20findings%20emphasize%20the%20potential%20of%0Amulti-exit%20networks%20and%20feature%20masking%20for%20advancing%20unsupervised%20OOD%0Adetection%20in%20medical%20imaging%2C%20paving%20the%20way%20for%20more%20reliable%20and%0Ainterpretable%20models%20in%20clinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08604v1&entry.124074799=Read"},
{"title": "Large Language Models Meet Stance Detection: A Survey of Tasks, Methods,\n  Applications, Challenges and Future Directions", "author": "Lata Pangtey and Anukriti Bhatnagar and Shubhi Bansal and Shahid Shafi Dar and Nagendra Kumar", "abstract": "  Stance detection is essential for understanding subjective content across\nvarious platforms such as social media, news articles, and online reviews.\nRecent advances in Large Language Models (LLMs) have revolutionized stance\ndetection by introducing novel capabilities in contextual understanding,\ncross-domain generalization, and multimodal analysis. Despite these\nprogressions, existing surveys often lack comprehensive coverage of approaches\nthat specifically leverage LLMs for stance detection. To bridge this critical\ngap, our review article conducts a systematic analysis of stance detection,\ncomprehensively examining recent advancements of LLMs transforming the field,\nincluding foundational concepts, methodologies, datasets, applications, and\nemerging challenges. We present a novel taxonomy for LLM-based stance detection\napproaches, structured along three key dimensions: 1) learning methods,\nincluding supervised, unsupervised, few-shot, and zero-shot; 2) data\nmodalities, such as unimodal, multimodal, and hybrid; and 3) target\nrelationships, encompassing in-target, cross-target, and multi-target\nscenarios. Furthermore, we discuss the evaluation techniques and analyze\nbenchmark datasets and performance trends, highlighting the strengths and\nlimitations of different architectures. Key applications in misinformation\ndetection, political analysis, public health monitoring, and social media\nmoderation are discussed. Finally, we identify critical challenges such as\nimplicit stance expression, cultural biases, and computational constraints,\nwhile outlining promising future directions, including explainable stance\nreasoning, low-resource adaptation, and real-time deployment frameworks. Our\nsurvey highlights emerging trends, open challenges, and future directions to\nguide researchers and practitioners in developing next-generation stance\ndetection systems powered by large language models.\n", "link": "http://arxiv.org/abs/2505.08464v1", "date": "2025-05-13", "relevancy": 2.1304, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5396}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5396}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Meet%20Stance%20Detection%3A%20A%20Survey%20of%20Tasks%2C%20Methods%2C%0A%20%20Applications%2C%20Challenges%20and%20Future%20Directions&body=Title%3A%20Large%20Language%20Models%20Meet%20Stance%20Detection%3A%20A%20Survey%20of%20Tasks%2C%20Methods%2C%0A%20%20Applications%2C%20Challenges%20and%20Future%20Directions%0AAuthor%3A%20Lata%20Pangtey%20and%20Anukriti%20Bhatnagar%20and%20Shubhi%20Bansal%20and%20Shahid%20Shafi%20Dar%20and%20Nagendra%20Kumar%0AAbstract%3A%20%20%20Stance%20detection%20is%20essential%20for%20understanding%20subjective%20content%20across%0Avarious%20platforms%20such%20as%20social%20media%2C%20news%20articles%2C%20and%20online%20reviews.%0ARecent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20stance%0Adetection%20by%20introducing%20novel%20capabilities%20in%20contextual%20understanding%2C%0Across-domain%20generalization%2C%20and%20multimodal%20analysis.%20Despite%20these%0Aprogressions%2C%20existing%20surveys%20often%20lack%20comprehensive%20coverage%20of%20approaches%0Athat%20specifically%20leverage%20LLMs%20for%20stance%20detection.%20To%20bridge%20this%20critical%0Agap%2C%20our%20review%20article%20conducts%20a%20systematic%20analysis%20of%20stance%20detection%2C%0Acomprehensively%20examining%20recent%20advancements%20of%20LLMs%20transforming%20the%20field%2C%0Aincluding%20foundational%20concepts%2C%20methodologies%2C%20datasets%2C%20applications%2C%20and%0Aemerging%20challenges.%20We%20present%20a%20novel%20taxonomy%20for%20LLM-based%20stance%20detection%0Aapproaches%2C%20structured%20along%20three%20key%20dimensions%3A%201%29%20learning%20methods%2C%0Aincluding%20supervised%2C%20unsupervised%2C%20few-shot%2C%20and%20zero-shot%3B%202%29%20data%0Amodalities%2C%20such%20as%20unimodal%2C%20multimodal%2C%20and%20hybrid%3B%20and%203%29%20target%0Arelationships%2C%20encompassing%20in-target%2C%20cross-target%2C%20and%20multi-target%0Ascenarios.%20Furthermore%2C%20we%20discuss%20the%20evaluation%20techniques%20and%20analyze%0Abenchmark%20datasets%20and%20performance%20trends%2C%20highlighting%20the%20strengths%20and%0Alimitations%20of%20different%20architectures.%20Key%20applications%20in%20misinformation%0Adetection%2C%20political%20analysis%2C%20public%20health%20monitoring%2C%20and%20social%20media%0Amoderation%20are%20discussed.%20Finally%2C%20we%20identify%20critical%20challenges%20such%20as%0Aimplicit%20stance%20expression%2C%20cultural%20biases%2C%20and%20computational%20constraints%2C%0Awhile%20outlining%20promising%20future%20directions%2C%20including%20explainable%20stance%0Areasoning%2C%20low-resource%20adaptation%2C%20and%20real-time%20deployment%20frameworks.%20Our%0Asurvey%20highlights%20emerging%20trends%2C%20open%20challenges%2C%20and%20future%20directions%20to%0Aguide%20researchers%20and%20practitioners%20in%20developing%20next-generation%20stance%0Adetection%20systems%20powered%20by%20large%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Meet%2520Stance%2520Detection%253A%2520A%2520Survey%2520of%2520Tasks%252C%2520Methods%252C%250A%2520%2520Applications%252C%2520Challenges%2520and%2520Future%2520Directions%26entry.906535625%3DLata%2520Pangtey%2520and%2520Anukriti%2520Bhatnagar%2520and%2520Shubhi%2520Bansal%2520and%2520Shahid%2520Shafi%2520Dar%2520and%2520Nagendra%2520Kumar%26entry.1292438233%3D%2520%2520Stance%2520detection%2520is%2520essential%2520for%2520understanding%2520subjective%2520content%2520across%250Avarious%2520platforms%2520such%2520as%2520social%2520media%252C%2520news%2520articles%252C%2520and%2520online%2520reviews.%250ARecent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520stance%250Adetection%2520by%2520introducing%2520novel%2520capabilities%2520in%2520contextual%2520understanding%252C%250Across-domain%2520generalization%252C%2520and%2520multimodal%2520analysis.%2520Despite%2520these%250Aprogressions%252C%2520existing%2520surveys%2520often%2520lack%2520comprehensive%2520coverage%2520of%2520approaches%250Athat%2520specifically%2520leverage%2520LLMs%2520for%2520stance%2520detection.%2520To%2520bridge%2520this%2520critical%250Agap%252C%2520our%2520review%2520article%2520conducts%2520a%2520systematic%2520analysis%2520of%2520stance%2520detection%252C%250Acomprehensively%2520examining%2520recent%2520advancements%2520of%2520LLMs%2520transforming%2520the%2520field%252C%250Aincluding%2520foundational%2520concepts%252C%2520methodologies%252C%2520datasets%252C%2520applications%252C%2520and%250Aemerging%2520challenges.%2520We%2520present%2520a%2520novel%2520taxonomy%2520for%2520LLM-based%2520stance%2520detection%250Aapproaches%252C%2520structured%2520along%2520three%2520key%2520dimensions%253A%25201%2529%2520learning%2520methods%252C%250Aincluding%2520supervised%252C%2520unsupervised%252C%2520few-shot%252C%2520and%2520zero-shot%253B%25202%2529%2520data%250Amodalities%252C%2520such%2520as%2520unimodal%252C%2520multimodal%252C%2520and%2520hybrid%253B%2520and%25203%2529%2520target%250Arelationships%252C%2520encompassing%2520in-target%252C%2520cross-target%252C%2520and%2520multi-target%250Ascenarios.%2520Furthermore%252C%2520we%2520discuss%2520the%2520evaluation%2520techniques%2520and%2520analyze%250Abenchmark%2520datasets%2520and%2520performance%2520trends%252C%2520highlighting%2520the%2520strengths%2520and%250Alimitations%2520of%2520different%2520architectures.%2520Key%2520applications%2520in%2520misinformation%250Adetection%252C%2520political%2520analysis%252C%2520public%2520health%2520monitoring%252C%2520and%2520social%2520media%250Amoderation%2520are%2520discussed.%2520Finally%252C%2520we%2520identify%2520critical%2520challenges%2520such%2520as%250Aimplicit%2520stance%2520expression%252C%2520cultural%2520biases%252C%2520and%2520computational%2520constraints%252C%250Awhile%2520outlining%2520promising%2520future%2520directions%252C%2520including%2520explainable%2520stance%250Areasoning%252C%2520low-resource%2520adaptation%252C%2520and%2520real-time%2520deployment%2520frameworks.%2520Our%250Asurvey%2520highlights%2520emerging%2520trends%252C%2520open%2520challenges%252C%2520and%2520future%2520directions%2520to%250Aguide%2520researchers%2520and%2520practitioners%2520in%2520developing%2520next-generation%2520stance%250Adetection%2520systems%2520powered%2520by%2520large%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Meet%20Stance%20Detection%3A%20A%20Survey%20of%20Tasks%2C%20Methods%2C%0A%20%20Applications%2C%20Challenges%20and%20Future%20Directions&entry.906535625=Lata%20Pangtey%20and%20Anukriti%20Bhatnagar%20and%20Shubhi%20Bansal%20and%20Shahid%20Shafi%20Dar%20and%20Nagendra%20Kumar&entry.1292438233=%20%20Stance%20detection%20is%20essential%20for%20understanding%20subjective%20content%20across%0Avarious%20platforms%20such%20as%20social%20media%2C%20news%20articles%2C%20and%20online%20reviews.%0ARecent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20stance%0Adetection%20by%20introducing%20novel%20capabilities%20in%20contextual%20understanding%2C%0Across-domain%20generalization%2C%20and%20multimodal%20analysis.%20Despite%20these%0Aprogressions%2C%20existing%20surveys%20often%20lack%20comprehensive%20coverage%20of%20approaches%0Athat%20specifically%20leverage%20LLMs%20for%20stance%20detection.%20To%20bridge%20this%20critical%0Agap%2C%20our%20review%20article%20conducts%20a%20systematic%20analysis%20of%20stance%20detection%2C%0Acomprehensively%20examining%20recent%20advancements%20of%20LLMs%20transforming%20the%20field%2C%0Aincluding%20foundational%20concepts%2C%20methodologies%2C%20datasets%2C%20applications%2C%20and%0Aemerging%20challenges.%20We%20present%20a%20novel%20taxonomy%20for%20LLM-based%20stance%20detection%0Aapproaches%2C%20structured%20along%20three%20key%20dimensions%3A%201%29%20learning%20methods%2C%0Aincluding%20supervised%2C%20unsupervised%2C%20few-shot%2C%20and%20zero-shot%3B%202%29%20data%0Amodalities%2C%20such%20as%20unimodal%2C%20multimodal%2C%20and%20hybrid%3B%20and%203%29%20target%0Arelationships%2C%20encompassing%20in-target%2C%20cross-target%2C%20and%20multi-target%0Ascenarios.%20Furthermore%2C%20we%20discuss%20the%20evaluation%20techniques%20and%20analyze%0Abenchmark%20datasets%20and%20performance%20trends%2C%20highlighting%20the%20strengths%20and%0Alimitations%20of%20different%20architectures.%20Key%20applications%20in%20misinformation%0Adetection%2C%20political%20analysis%2C%20public%20health%20monitoring%2C%20and%20social%20media%0Amoderation%20are%20discussed.%20Finally%2C%20we%20identify%20critical%20challenges%20such%20as%0Aimplicit%20stance%20expression%2C%20cultural%20biases%2C%20and%20computational%20constraints%2C%0Awhile%20outlining%20promising%20future%20directions%2C%20including%20explainable%20stance%0Areasoning%2C%20low-resource%20adaptation%2C%20and%20real-time%20deployment%20frameworks.%20Our%0Asurvey%20highlights%20emerging%20trends%2C%20open%20challenges%2C%20and%20future%20directions%20to%0Aguide%20researchers%20and%20practitioners%20in%20developing%20next-generation%20stance%0Adetection%20systems%20powered%20by%20large%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08464v1&entry.124074799=Read"},
{"title": "A Large-scale Benchmark on Geological Fault Delineation Models: Domain\n  Shift, Training Dynamics, Generalizability, Evaluation and Inferential\n  Behavior", "author": "Jorge Quesada and Chen Zhou and Prithwijit Chowdhury and Mohammad Alotaibi and Ahmad Mustafa and Yusufjon Kumamnov and Mohit Prabhushankar and Ghassan AlRegib", "abstract": "  Machine learning has taken a critical role in seismic interpretation\nworkflows, especially in fault delineation tasks. However, despite the recent\nproliferation of pretrained models and synthetic datasets, the field still\nlacks a systematic understanding of the generalizability limits of these models\nacross seismic data representing a variety of geologic, acquisition and\nprocessing settings. Distributional shifts between different data sources,\nlimitations in fine-tuning strategies and labeled data accessibility, and\ninconsistent evaluation protocols all represent major roadblocks in the\ndeployment of reliable and robust models in real-world exploration settings. In\nthis paper, we present the first large-scale benchmarking study explicitly\ndesigned to provide answers and guidelines for domain shift strategies in\nseismic interpretation. Our benchmark encompasses over $200$ models trained and\nevaluated on three heterogeneous datasets (synthetic and real data) including\nFaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining,\nfine-tuning, and joint training strategies under varying degrees of domain\nshift. Our analysis highlights the fragility of current fine-tuning practices,\nthe emergence of catastrophic forgetting, and the challenges of interpreting\nperformance in a systematic manner. We establish a robust experimental baseline\nto provide insights into the tradeoffs inherent to current fault delineation\nworkflows, and shed light on directions for developing more generalizable,\ninterpretable and effective machine learning models for seismic interpretation.\nThe insights and analyses reported provide a set of guidelines on the\ndeployment of fault delineation models within seismic interpretation workflows.\n", "link": "http://arxiv.org/abs/2505.08585v1", "date": "2025-05-13", "relevancy": 2.1263, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large-scale%20Benchmark%20on%20Geological%20Fault%20Delineation%20Models%3A%20Domain%0A%20%20Shift%2C%20Training%20Dynamics%2C%20Generalizability%2C%20Evaluation%20and%20Inferential%0A%20%20Behavior&body=Title%3A%20A%20Large-scale%20Benchmark%20on%20Geological%20Fault%20Delineation%20Models%3A%20Domain%0A%20%20Shift%2C%20Training%20Dynamics%2C%20Generalizability%2C%20Evaluation%20and%20Inferential%0A%20%20Behavior%0AAuthor%3A%20Jorge%20Quesada%20and%20Chen%20Zhou%20and%20Prithwijit%20Chowdhury%20and%20Mohammad%20Alotaibi%20and%20Ahmad%20Mustafa%20and%20Yusufjon%20Kumamnov%20and%20Mohit%20Prabhushankar%20and%20Ghassan%20AlRegib%0AAbstract%3A%20%20%20Machine%20learning%20has%20taken%20a%20critical%20role%20in%20seismic%20interpretation%0Aworkflows%2C%20especially%20in%20fault%20delineation%20tasks.%20However%2C%20despite%20the%20recent%0Aproliferation%20of%20pretrained%20models%20and%20synthetic%20datasets%2C%20the%20field%20still%0Alacks%20a%20systematic%20understanding%20of%20the%20generalizability%20limits%20of%20these%20models%0Aacross%20seismic%20data%20representing%20a%20variety%20of%20geologic%2C%20acquisition%20and%0Aprocessing%20settings.%20Distributional%20shifts%20between%20different%20data%20sources%2C%0Alimitations%20in%20fine-tuning%20strategies%20and%20labeled%20data%20accessibility%2C%20and%0Ainconsistent%20evaluation%20protocols%20all%20represent%20major%20roadblocks%20in%20the%0Adeployment%20of%20reliable%20and%20robust%20models%20in%20real-world%20exploration%20settings.%20In%0Athis%20paper%2C%20we%20present%20the%20first%20large-scale%20benchmarking%20study%20explicitly%0Adesigned%20to%20provide%20answers%20and%20guidelines%20for%20domain%20shift%20strategies%20in%0Aseismic%20interpretation.%20Our%20benchmark%20encompasses%20over%20%24200%24%20models%20trained%20and%0Aevaluated%20on%20three%20heterogeneous%20datasets%20%28synthetic%20and%20real%20data%29%20including%0AFaultSeg3D%2C%20CRACKS%2C%20and%20Thebe.%20We%20systematically%20assess%20pretraining%2C%0Afine-tuning%2C%20and%20joint%20training%20strategies%20under%20varying%20degrees%20of%20domain%0Ashift.%20Our%20analysis%20highlights%20the%20fragility%20of%20current%20fine-tuning%20practices%2C%0Athe%20emergence%20of%20catastrophic%20forgetting%2C%20and%20the%20challenges%20of%20interpreting%0Aperformance%20in%20a%20systematic%20manner.%20We%20establish%20a%20robust%20experimental%20baseline%0Ato%20provide%20insights%20into%20the%20tradeoffs%20inherent%20to%20current%20fault%20delineation%0Aworkflows%2C%20and%20shed%20light%20on%20directions%20for%20developing%20more%20generalizable%2C%0Ainterpretable%20and%20effective%20machine%20learning%20models%20for%20seismic%20interpretation.%0AThe%20insights%20and%20analyses%20reported%20provide%20a%20set%20of%20guidelines%20on%20the%0Adeployment%20of%20fault%20delineation%20models%20within%20seismic%20interpretation%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large-scale%2520Benchmark%2520on%2520Geological%2520Fault%2520Delineation%2520Models%253A%2520Domain%250A%2520%2520Shift%252C%2520Training%2520Dynamics%252C%2520Generalizability%252C%2520Evaluation%2520and%2520Inferential%250A%2520%2520Behavior%26entry.906535625%3DJorge%2520Quesada%2520and%2520Chen%2520Zhou%2520and%2520Prithwijit%2520Chowdhury%2520and%2520Mohammad%2520Alotaibi%2520and%2520Ahmad%2520Mustafa%2520and%2520Yusufjon%2520Kumamnov%2520and%2520Mohit%2520Prabhushankar%2520and%2520Ghassan%2520AlRegib%26entry.1292438233%3D%2520%2520Machine%2520learning%2520has%2520taken%2520a%2520critical%2520role%2520in%2520seismic%2520interpretation%250Aworkflows%252C%2520especially%2520in%2520fault%2520delineation%2520tasks.%2520However%252C%2520despite%2520the%2520recent%250Aproliferation%2520of%2520pretrained%2520models%2520and%2520synthetic%2520datasets%252C%2520the%2520field%2520still%250Alacks%2520a%2520systematic%2520understanding%2520of%2520the%2520generalizability%2520limits%2520of%2520these%2520models%250Aacross%2520seismic%2520data%2520representing%2520a%2520variety%2520of%2520geologic%252C%2520acquisition%2520and%250Aprocessing%2520settings.%2520Distributional%2520shifts%2520between%2520different%2520data%2520sources%252C%250Alimitations%2520in%2520fine-tuning%2520strategies%2520and%2520labeled%2520data%2520accessibility%252C%2520and%250Ainconsistent%2520evaluation%2520protocols%2520all%2520represent%2520major%2520roadblocks%2520in%2520the%250Adeployment%2520of%2520reliable%2520and%2520robust%2520models%2520in%2520real-world%2520exploration%2520settings.%2520In%250Athis%2520paper%252C%2520we%2520present%2520the%2520first%2520large-scale%2520benchmarking%2520study%2520explicitly%250Adesigned%2520to%2520provide%2520answers%2520and%2520guidelines%2520for%2520domain%2520shift%2520strategies%2520in%250Aseismic%2520interpretation.%2520Our%2520benchmark%2520encompasses%2520over%2520%2524200%2524%2520models%2520trained%2520and%250Aevaluated%2520on%2520three%2520heterogeneous%2520datasets%2520%2528synthetic%2520and%2520real%2520data%2529%2520including%250AFaultSeg3D%252C%2520CRACKS%252C%2520and%2520Thebe.%2520We%2520systematically%2520assess%2520pretraining%252C%250Afine-tuning%252C%2520and%2520joint%2520training%2520strategies%2520under%2520varying%2520degrees%2520of%2520domain%250Ashift.%2520Our%2520analysis%2520highlights%2520the%2520fragility%2520of%2520current%2520fine-tuning%2520practices%252C%250Athe%2520emergence%2520of%2520catastrophic%2520forgetting%252C%2520and%2520the%2520challenges%2520of%2520interpreting%250Aperformance%2520in%2520a%2520systematic%2520manner.%2520We%2520establish%2520a%2520robust%2520experimental%2520baseline%250Ato%2520provide%2520insights%2520into%2520the%2520tradeoffs%2520inherent%2520to%2520current%2520fault%2520delineation%250Aworkflows%252C%2520and%2520shed%2520light%2520on%2520directions%2520for%2520developing%2520more%2520generalizable%252C%250Ainterpretable%2520and%2520effective%2520machine%2520learning%2520models%2520for%2520seismic%2520interpretation.%250AThe%2520insights%2520and%2520analyses%2520reported%2520provide%2520a%2520set%2520of%2520guidelines%2520on%2520the%250Adeployment%2520of%2520fault%2520delineation%2520models%2520within%2520seismic%2520interpretation%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large-scale%20Benchmark%20on%20Geological%20Fault%20Delineation%20Models%3A%20Domain%0A%20%20Shift%2C%20Training%20Dynamics%2C%20Generalizability%2C%20Evaluation%20and%20Inferential%0A%20%20Behavior&entry.906535625=Jorge%20Quesada%20and%20Chen%20Zhou%20and%20Prithwijit%20Chowdhury%20and%20Mohammad%20Alotaibi%20and%20Ahmad%20Mustafa%20and%20Yusufjon%20Kumamnov%20and%20Mohit%20Prabhushankar%20and%20Ghassan%20AlRegib&entry.1292438233=%20%20Machine%20learning%20has%20taken%20a%20critical%20role%20in%20seismic%20interpretation%0Aworkflows%2C%20especially%20in%20fault%20delineation%20tasks.%20However%2C%20despite%20the%20recent%0Aproliferation%20of%20pretrained%20models%20and%20synthetic%20datasets%2C%20the%20field%20still%0Alacks%20a%20systematic%20understanding%20of%20the%20generalizability%20limits%20of%20these%20models%0Aacross%20seismic%20data%20representing%20a%20variety%20of%20geologic%2C%20acquisition%20and%0Aprocessing%20settings.%20Distributional%20shifts%20between%20different%20data%20sources%2C%0Alimitations%20in%20fine-tuning%20strategies%20and%20labeled%20data%20accessibility%2C%20and%0Ainconsistent%20evaluation%20protocols%20all%20represent%20major%20roadblocks%20in%20the%0Adeployment%20of%20reliable%20and%20robust%20models%20in%20real-world%20exploration%20settings.%20In%0Athis%20paper%2C%20we%20present%20the%20first%20large-scale%20benchmarking%20study%20explicitly%0Adesigned%20to%20provide%20answers%20and%20guidelines%20for%20domain%20shift%20strategies%20in%0Aseismic%20interpretation.%20Our%20benchmark%20encompasses%20over%20%24200%24%20models%20trained%20and%0Aevaluated%20on%20three%20heterogeneous%20datasets%20%28synthetic%20and%20real%20data%29%20including%0AFaultSeg3D%2C%20CRACKS%2C%20and%20Thebe.%20We%20systematically%20assess%20pretraining%2C%0Afine-tuning%2C%20and%20joint%20training%20strategies%20under%20varying%20degrees%20of%20domain%0Ashift.%20Our%20analysis%20highlights%20the%20fragility%20of%20current%20fine-tuning%20practices%2C%0Athe%20emergence%20of%20catastrophic%20forgetting%2C%20and%20the%20challenges%20of%20interpreting%0Aperformance%20in%20a%20systematic%20manner.%20We%20establish%20a%20robust%20experimental%20baseline%0Ato%20provide%20insights%20into%20the%20tradeoffs%20inherent%20to%20current%20fault%20delineation%0Aworkflows%2C%20and%20shed%20light%20on%20directions%20for%20developing%20more%20generalizable%2C%0Ainterpretable%20and%20effective%20machine%20learning%20models%20for%20seismic%20interpretation.%0AThe%20insights%20and%20analyses%20reported%20provide%20a%20set%20of%20guidelines%20on%20the%0Adeployment%20of%20fault%20delineation%20models%20within%20seismic%20interpretation%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08585v1&entry.124074799=Read"},
{"title": "Building-Block Aware Generative Modeling for 3D Crystals of Metal\n  Organic Frameworks", "author": "Chenru Duan and Aditya Nandy and Sizhan Liu and Yuanqi Du and Liu He and Yi Qu and Haojun Jia and Jin-Hu Dou", "abstract": "  Metal-organic frameworks (MOFs) marry inorganic nodes, organic edges, and\ntopological nets into programmable porous crystals, yet their astronomical\ndesign space defies brute-force synthesis. Generative modeling holds ultimate\npromise, but existing models either recycle known building blocks or are\nrestricted to small unit cells. We introduce Building-Block-Aware MOF Diffusion\n(BBA MOF Diffusion), an SE(3)-equivariant diffusion model that learns 3D\nall-atom representations of individual building blocks, encoding\ncrystallographic topological nets explicitly. Trained on the CoRE-MOF database,\nBBA MOF Diffusion readily samples MOFs with unit cells containing 1000 atoms\nwith great geometric validity, novelty, and diversity mirroring experimental\ndatabases. Its native building-block representation produces unprecedented\nmetal nodes and organic edges, expanding accessible chemical space by orders of\nmagnitude. One high-scoring [Zn(1,4-TDC)(EtOH)2] MOF predicted by the model was\nsynthesized, where powder X-ray diffraction, thermogravimetric analysis, and N2\nsorption confirm its structural fidelity. BBA-Diff thus furnishes a practical\npathway to synthesizable and high-performing MOFs.\n", "link": "http://arxiv.org/abs/2505.08531v1", "date": "2025-05-13", "relevancy": 2.1234, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.546}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5278}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building-Block%20Aware%20Generative%20Modeling%20for%203D%20Crystals%20of%20Metal%0A%20%20Organic%20Frameworks&body=Title%3A%20Building-Block%20Aware%20Generative%20Modeling%20for%203D%20Crystals%20of%20Metal%0A%20%20Organic%20Frameworks%0AAuthor%3A%20Chenru%20Duan%20and%20Aditya%20Nandy%20and%20Sizhan%20Liu%20and%20Yuanqi%20Du%20and%20Liu%20He%20and%20Yi%20Qu%20and%20Haojun%20Jia%20and%20Jin-Hu%20Dou%0AAbstract%3A%20%20%20Metal-organic%20frameworks%20%28MOFs%29%20marry%20inorganic%20nodes%2C%20organic%20edges%2C%20and%0Atopological%20nets%20into%20programmable%20porous%20crystals%2C%20yet%20their%20astronomical%0Adesign%20space%20defies%20brute-force%20synthesis.%20Generative%20modeling%20holds%20ultimate%0Apromise%2C%20but%20existing%20models%20either%20recycle%20known%20building%20blocks%20or%20are%0Arestricted%20to%20small%20unit%20cells.%20We%20introduce%20Building-Block-Aware%20MOF%20Diffusion%0A%28BBA%20MOF%20Diffusion%29%2C%20an%20SE%283%29-equivariant%20diffusion%20model%20that%20learns%203D%0Aall-atom%20representations%20of%20individual%20building%20blocks%2C%20encoding%0Acrystallographic%20topological%20nets%20explicitly.%20Trained%20on%20the%20CoRE-MOF%20database%2C%0ABBA%20MOF%20Diffusion%20readily%20samples%20MOFs%20with%20unit%20cells%20containing%201000%20atoms%0Awith%20great%20geometric%20validity%2C%20novelty%2C%20and%20diversity%20mirroring%20experimental%0Adatabases.%20Its%20native%20building-block%20representation%20produces%20unprecedented%0Ametal%20nodes%20and%20organic%20edges%2C%20expanding%20accessible%20chemical%20space%20by%20orders%20of%0Amagnitude.%20One%20high-scoring%20%5BZn%281%2C4-TDC%29%28EtOH%292%5D%20MOF%20predicted%20by%20the%20model%20was%0Asynthesized%2C%20where%20powder%20X-ray%20diffraction%2C%20thermogravimetric%20analysis%2C%20and%20N2%0Asorption%20confirm%20its%20structural%20fidelity.%20BBA-Diff%20thus%20furnishes%20a%20practical%0Apathway%20to%20synthesizable%20and%20high-performing%20MOFs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding-Block%2520Aware%2520Generative%2520Modeling%2520for%25203D%2520Crystals%2520of%2520Metal%250A%2520%2520Organic%2520Frameworks%26entry.906535625%3DChenru%2520Duan%2520and%2520Aditya%2520Nandy%2520and%2520Sizhan%2520Liu%2520and%2520Yuanqi%2520Du%2520and%2520Liu%2520He%2520and%2520Yi%2520Qu%2520and%2520Haojun%2520Jia%2520and%2520Jin-Hu%2520Dou%26entry.1292438233%3D%2520%2520Metal-organic%2520frameworks%2520%2528MOFs%2529%2520marry%2520inorganic%2520nodes%252C%2520organic%2520edges%252C%2520and%250Atopological%2520nets%2520into%2520programmable%2520porous%2520crystals%252C%2520yet%2520their%2520astronomical%250Adesign%2520space%2520defies%2520brute-force%2520synthesis.%2520Generative%2520modeling%2520holds%2520ultimate%250Apromise%252C%2520but%2520existing%2520models%2520either%2520recycle%2520known%2520building%2520blocks%2520or%2520are%250Arestricted%2520to%2520small%2520unit%2520cells.%2520We%2520introduce%2520Building-Block-Aware%2520MOF%2520Diffusion%250A%2528BBA%2520MOF%2520Diffusion%2529%252C%2520an%2520SE%25283%2529-equivariant%2520diffusion%2520model%2520that%2520learns%25203D%250Aall-atom%2520representations%2520of%2520individual%2520building%2520blocks%252C%2520encoding%250Acrystallographic%2520topological%2520nets%2520explicitly.%2520Trained%2520on%2520the%2520CoRE-MOF%2520database%252C%250ABBA%2520MOF%2520Diffusion%2520readily%2520samples%2520MOFs%2520with%2520unit%2520cells%2520containing%25201000%2520atoms%250Awith%2520great%2520geometric%2520validity%252C%2520novelty%252C%2520and%2520diversity%2520mirroring%2520experimental%250Adatabases.%2520Its%2520native%2520building-block%2520representation%2520produces%2520unprecedented%250Ametal%2520nodes%2520and%2520organic%2520edges%252C%2520expanding%2520accessible%2520chemical%2520space%2520by%2520orders%2520of%250Amagnitude.%2520One%2520high-scoring%2520%255BZn%25281%252C4-TDC%2529%2528EtOH%25292%255D%2520MOF%2520predicted%2520by%2520the%2520model%2520was%250Asynthesized%252C%2520where%2520powder%2520X-ray%2520diffraction%252C%2520thermogravimetric%2520analysis%252C%2520and%2520N2%250Asorption%2520confirm%2520its%2520structural%2520fidelity.%2520BBA-Diff%2520thus%2520furnishes%2520a%2520practical%250Apathway%2520to%2520synthesizable%2520and%2520high-performing%2520MOFs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building-Block%20Aware%20Generative%20Modeling%20for%203D%20Crystals%20of%20Metal%0A%20%20Organic%20Frameworks&entry.906535625=Chenru%20Duan%20and%20Aditya%20Nandy%20and%20Sizhan%20Liu%20and%20Yuanqi%20Du%20and%20Liu%20He%20and%20Yi%20Qu%20and%20Haojun%20Jia%20and%20Jin-Hu%20Dou&entry.1292438233=%20%20Metal-organic%20frameworks%20%28MOFs%29%20marry%20inorganic%20nodes%2C%20organic%20edges%2C%20and%0Atopological%20nets%20into%20programmable%20porous%20crystals%2C%20yet%20their%20astronomical%0Adesign%20space%20defies%20brute-force%20synthesis.%20Generative%20modeling%20holds%20ultimate%0Apromise%2C%20but%20existing%20models%20either%20recycle%20known%20building%20blocks%20or%20are%0Arestricted%20to%20small%20unit%20cells.%20We%20introduce%20Building-Block-Aware%20MOF%20Diffusion%0A%28BBA%20MOF%20Diffusion%29%2C%20an%20SE%283%29-equivariant%20diffusion%20model%20that%20learns%203D%0Aall-atom%20representations%20of%20individual%20building%20blocks%2C%20encoding%0Acrystallographic%20topological%20nets%20explicitly.%20Trained%20on%20the%20CoRE-MOF%20database%2C%0ABBA%20MOF%20Diffusion%20readily%20samples%20MOFs%20with%20unit%20cells%20containing%201000%20atoms%0Awith%20great%20geometric%20validity%2C%20novelty%2C%20and%20diversity%20mirroring%20experimental%0Adatabases.%20Its%20native%20building-block%20representation%20produces%20unprecedented%0Ametal%20nodes%20and%20organic%20edges%2C%20expanding%20accessible%20chemical%20space%20by%20orders%20of%0Amagnitude.%20One%20high-scoring%20%5BZn%281%2C4-TDC%29%28EtOH%292%5D%20MOF%20predicted%20by%20the%20model%20was%0Asynthesized%2C%20where%20powder%20X-ray%20diffraction%2C%20thermogravimetric%20analysis%2C%20and%20N2%0Asorption%20confirm%20its%20structural%20fidelity.%20BBA-Diff%20thus%20furnishes%20a%20practical%0Apathway%20to%20synthesizable%20and%20high-performing%20MOFs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08531v1&entry.124074799=Read"},
{"title": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate\n  Chart Comprehension and Reasoning?", "author": "Md Tahmid Rahman Laskar and Mohammed Saidul Islam and Ridwan Mahbub and Ahmed Masry and Mizanur Rahman and Amran Bhuiyan and Mir Tafseer Nayeem and Shafiq Joty and Enamul Hoque and Jimmy Huang", "abstract": "  Charts are ubiquitous as they help people understand and reason with data.\nRecently, various downstream tasks, such as chart question answering,\nchart2text, and fact-checking, have emerged. Large Vision-Language Models\n(LVLMs) show promise in tackling these tasks, but their evaluation is costly\nand time-consuming, limiting real-world deployment. While using LVLMs as judges\nto assess the chart comprehension capabilities of other LVLMs could streamline\nevaluation processes, challenges like proprietary datasets, restricted access\nto powerful models, and evaluation costs hinder their adoption in industrial\nsettings. To this end, we present a comprehensive evaluation of 13 open-source\nLVLMs as judges for diverse chart comprehension and reasoning tasks. We design\nboth pairwise and pointwise evaluation tasks covering criteria like factual\ncorrectness, informativeness, and relevancy. Additionally, we analyze LVLM\njudges based on format adherence, positional consistency, length bias, and\ninstruction-following. We focus on cost-effective LVLMs (<10B parameters)\nsuitable for both research and commercial use, following a standardized\nevaluation protocol and rubric to measure the LVLM judge's accuracy.\nExperimental results reveal notable variability: while some open LVLM judges\nachieve GPT-4-level evaluation performance (about 80% agreement with GPT-4\njudgments), others struggle (below ~10% agreement). Our findings highlight that\nstate-of-the-art open-source LVLMs can serve as cost-effective automatic\nevaluators for chart-related tasks, though biases such as positional preference\nand length bias persist.\n", "link": "http://arxiv.org/abs/2505.08468v1", "date": "2025-05-13", "relevancy": 2.1199, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Judging%20the%20Judges%3A%20Can%20Large%20Vision-Language%20Models%20Fairly%20Evaluate%0A%20%20Chart%20Comprehension%20and%20Reasoning%3F&body=Title%3A%20Judging%20the%20Judges%3A%20Can%20Large%20Vision-Language%20Models%20Fairly%20Evaluate%0A%20%20Chart%20Comprehension%20and%20Reasoning%3F%0AAuthor%3A%20Md%20Tahmid%20Rahman%20Laskar%20and%20Mohammed%20Saidul%20Islam%20and%20Ridwan%20Mahbub%20and%20Ahmed%20Masry%20and%20Mizanur%20Rahman%20and%20Amran%20Bhuiyan%20and%20Mir%20Tafseer%20Nayeem%20and%20Shafiq%20Joty%20and%20Enamul%20Hoque%20and%20Jimmy%20Huang%0AAbstract%3A%20%20%20Charts%20are%20ubiquitous%20as%20they%20help%20people%20understand%20and%20reason%20with%20data.%0ARecently%2C%20various%20downstream%20tasks%2C%20such%20as%20chart%20question%20answering%2C%0Achart2text%2C%20and%20fact-checking%2C%20have%20emerged.%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20show%20promise%20in%20tackling%20these%20tasks%2C%20but%20their%20evaluation%20is%20costly%0Aand%20time-consuming%2C%20limiting%20real-world%20deployment.%20While%20using%20LVLMs%20as%20judges%0Ato%20assess%20the%20chart%20comprehension%20capabilities%20of%20other%20LVLMs%20could%20streamline%0Aevaluation%20processes%2C%20challenges%20like%20proprietary%20datasets%2C%20restricted%20access%0Ato%20powerful%20models%2C%20and%20evaluation%20costs%20hinder%20their%20adoption%20in%20industrial%0Asettings.%20To%20this%20end%2C%20we%20present%20a%20comprehensive%20evaluation%20of%2013%20open-source%0ALVLMs%20as%20judges%20for%20diverse%20chart%20comprehension%20and%20reasoning%20tasks.%20We%20design%0Aboth%20pairwise%20and%20pointwise%20evaluation%20tasks%20covering%20criteria%20like%20factual%0Acorrectness%2C%20informativeness%2C%20and%20relevancy.%20Additionally%2C%20we%20analyze%20LVLM%0Ajudges%20based%20on%20format%20adherence%2C%20positional%20consistency%2C%20length%20bias%2C%20and%0Ainstruction-following.%20We%20focus%20on%20cost-effective%20LVLMs%20%28%3C10B%20parameters%29%0Asuitable%20for%20both%20research%20and%20commercial%20use%2C%20following%20a%20standardized%0Aevaluation%20protocol%20and%20rubric%20to%20measure%20the%20LVLM%20judge%27s%20accuracy.%0AExperimental%20results%20reveal%20notable%20variability%3A%20while%20some%20open%20LVLM%20judges%0Aachieve%20GPT-4-level%20evaluation%20performance%20%28about%2080%25%20agreement%20with%20GPT-4%0Ajudgments%29%2C%20others%20struggle%20%28below%20~10%25%20agreement%29.%20Our%20findings%20highlight%20that%0Astate-of-the-art%20open-source%20LVLMs%20can%20serve%20as%20cost-effective%20automatic%0Aevaluators%20for%20chart-related%20tasks%2C%20though%20biases%20such%20as%20positional%20preference%0Aand%20length%20bias%20persist.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJudging%2520the%2520Judges%253A%2520Can%2520Large%2520Vision-Language%2520Models%2520Fairly%2520Evaluate%250A%2520%2520Chart%2520Comprehension%2520and%2520Reasoning%253F%26entry.906535625%3DMd%2520Tahmid%2520Rahman%2520Laskar%2520and%2520Mohammed%2520Saidul%2520Islam%2520and%2520Ridwan%2520Mahbub%2520and%2520Ahmed%2520Masry%2520and%2520Mizanur%2520Rahman%2520and%2520Amran%2520Bhuiyan%2520and%2520Mir%2520Tafseer%2520Nayeem%2520and%2520Shafiq%2520Joty%2520and%2520Enamul%2520Hoque%2520and%2520Jimmy%2520Huang%26entry.1292438233%3D%2520%2520Charts%2520are%2520ubiquitous%2520as%2520they%2520help%2520people%2520understand%2520and%2520reason%2520with%2520data.%250ARecently%252C%2520various%2520downstream%2520tasks%252C%2520such%2520as%2520chart%2520question%2520answering%252C%250Achart2text%252C%2520and%2520fact-checking%252C%2520have%2520emerged.%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529%2520show%2520promise%2520in%2520tackling%2520these%2520tasks%252C%2520but%2520their%2520evaluation%2520is%2520costly%250Aand%2520time-consuming%252C%2520limiting%2520real-world%2520deployment.%2520While%2520using%2520LVLMs%2520as%2520judges%250Ato%2520assess%2520the%2520chart%2520comprehension%2520capabilities%2520of%2520other%2520LVLMs%2520could%2520streamline%250Aevaluation%2520processes%252C%2520challenges%2520like%2520proprietary%2520datasets%252C%2520restricted%2520access%250Ato%2520powerful%2520models%252C%2520and%2520evaluation%2520costs%2520hinder%2520their%2520adoption%2520in%2520industrial%250Asettings.%2520To%2520this%2520end%252C%2520we%2520present%2520a%2520comprehensive%2520evaluation%2520of%252013%2520open-source%250ALVLMs%2520as%2520judges%2520for%2520diverse%2520chart%2520comprehension%2520and%2520reasoning%2520tasks.%2520We%2520design%250Aboth%2520pairwise%2520and%2520pointwise%2520evaluation%2520tasks%2520covering%2520criteria%2520like%2520factual%250Acorrectness%252C%2520informativeness%252C%2520and%2520relevancy.%2520Additionally%252C%2520we%2520analyze%2520LVLM%250Ajudges%2520based%2520on%2520format%2520adherence%252C%2520positional%2520consistency%252C%2520length%2520bias%252C%2520and%250Ainstruction-following.%2520We%2520focus%2520on%2520cost-effective%2520LVLMs%2520%2528%253C10B%2520parameters%2529%250Asuitable%2520for%2520both%2520research%2520and%2520commercial%2520use%252C%2520following%2520a%2520standardized%250Aevaluation%2520protocol%2520and%2520rubric%2520to%2520measure%2520the%2520LVLM%2520judge%2527s%2520accuracy.%250AExperimental%2520results%2520reveal%2520notable%2520variability%253A%2520while%2520some%2520open%2520LVLM%2520judges%250Aachieve%2520GPT-4-level%2520evaluation%2520performance%2520%2528about%252080%2525%2520agreement%2520with%2520GPT-4%250Ajudgments%2529%252C%2520others%2520struggle%2520%2528below%2520~10%2525%2520agreement%2529.%2520Our%2520findings%2520highlight%2520that%250Astate-of-the-art%2520open-source%2520LVLMs%2520can%2520serve%2520as%2520cost-effective%2520automatic%250Aevaluators%2520for%2520chart-related%2520tasks%252C%2520though%2520biases%2520such%2520as%2520positional%2520preference%250Aand%2520length%2520bias%2520persist.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Judging%20the%20Judges%3A%20Can%20Large%20Vision-Language%20Models%20Fairly%20Evaluate%0A%20%20Chart%20Comprehension%20and%20Reasoning%3F&entry.906535625=Md%20Tahmid%20Rahman%20Laskar%20and%20Mohammed%20Saidul%20Islam%20and%20Ridwan%20Mahbub%20and%20Ahmed%20Masry%20and%20Mizanur%20Rahman%20and%20Amran%20Bhuiyan%20and%20Mir%20Tafseer%20Nayeem%20and%20Shafiq%20Joty%20and%20Enamul%20Hoque%20and%20Jimmy%20Huang&entry.1292438233=%20%20Charts%20are%20ubiquitous%20as%20they%20help%20people%20understand%20and%20reason%20with%20data.%0ARecently%2C%20various%20downstream%20tasks%2C%20such%20as%20chart%20question%20answering%2C%0Achart2text%2C%20and%20fact-checking%2C%20have%20emerged.%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20show%20promise%20in%20tackling%20these%20tasks%2C%20but%20their%20evaluation%20is%20costly%0Aand%20time-consuming%2C%20limiting%20real-world%20deployment.%20While%20using%20LVLMs%20as%20judges%0Ato%20assess%20the%20chart%20comprehension%20capabilities%20of%20other%20LVLMs%20could%20streamline%0Aevaluation%20processes%2C%20challenges%20like%20proprietary%20datasets%2C%20restricted%20access%0Ato%20powerful%20models%2C%20and%20evaluation%20costs%20hinder%20their%20adoption%20in%20industrial%0Asettings.%20To%20this%20end%2C%20we%20present%20a%20comprehensive%20evaluation%20of%2013%20open-source%0ALVLMs%20as%20judges%20for%20diverse%20chart%20comprehension%20and%20reasoning%20tasks.%20We%20design%0Aboth%20pairwise%20and%20pointwise%20evaluation%20tasks%20covering%20criteria%20like%20factual%0Acorrectness%2C%20informativeness%2C%20and%20relevancy.%20Additionally%2C%20we%20analyze%20LVLM%0Ajudges%20based%20on%20format%20adherence%2C%20positional%20consistency%2C%20length%20bias%2C%20and%0Ainstruction-following.%20We%20focus%20on%20cost-effective%20LVLMs%20%28%3C10B%20parameters%29%0Asuitable%20for%20both%20research%20and%20commercial%20use%2C%20following%20a%20standardized%0Aevaluation%20protocol%20and%20rubric%20to%20measure%20the%20LVLM%20judge%27s%20accuracy.%0AExperimental%20results%20reveal%20notable%20variability%3A%20while%20some%20open%20LVLM%20judges%0Aachieve%20GPT-4-level%20evaluation%20performance%20%28about%2080%25%20agreement%20with%20GPT-4%0Ajudgments%29%2C%20others%20struggle%20%28below%20~10%25%20agreement%29.%20Our%20findings%20highlight%20that%0Astate-of-the-art%20open-source%20LVLMs%20can%20serve%20as%20cost-effective%20automatic%0Aevaluators%20for%20chart-related%20tasks%2C%20though%20biases%20such%20as%20positional%20preference%0Aand%20length%20bias%20persist.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08468v1&entry.124074799=Read"},
{"title": "Is Centralized Training with Decentralized Execution Framework\n  Centralized Enough for MARL?", "author": "Yihe Zhou and Shunyu Liu and Yunpeng Qing and Kaixuan Chen and Tongya Zheng and Jie Song and Mingli Song", "abstract": "  Centralized Training with Decentralized Execution (CTDE) has recently emerged\nas a popular framework for cooperative Multi-Agent Reinforcement Learning\n(MARL), where agents can use additional global state information to guide\ntraining in a centralized way and make their own decisions only based on\ndecentralized local policies. Despite the encouraging results achieved, CTDE\nmakes an independence assumption on agent policies, which limits agents to\nadopt global cooperative information from each other during centralized\ntraining. Therefore, we argue that existing CTDE methods cannot fully utilize\nglobal information for training, leading to an inefficient joint-policy\nexploration and even suboptimal results. In this paper, we introduce a novel\nCentralized Advising and Decentralized Pruning (CADP) framework for multi-agent\nreinforcement learning, that not only enables an efficacious message exchange\namong agents during training but also guarantees the independent policies for\nexecution. Firstly, CADP endows agents the explicit communication channel to\nseek and take advices from different agents for more centralized training. To\nfurther ensure the decentralized execution, we propose a smooth model pruning\nmechanism to progressively constraint the agent communication into a closed one\nwithout degradation in agent cooperation capability. Empirical evaluations on\nStarCraft II micromanagement and Google Research Football benchmarks\ndemonstrate that the proposed framework achieves superior performance compared\nwith the state-of-the-art counterparts. Our code will be made publicly\navailable.\n", "link": "http://arxiv.org/abs/2305.17352v2", "date": "2025-05-13", "relevancy": 2.1198, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5837}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4932}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Centralized%20Training%20with%20Decentralized%20Execution%20Framework%0A%20%20Centralized%20Enough%20for%20MARL%3F&body=Title%3A%20Is%20Centralized%20Training%20with%20Decentralized%20Execution%20Framework%0A%20%20Centralized%20Enough%20for%20MARL%3F%0AAuthor%3A%20Yihe%20Zhou%20and%20Shunyu%20Liu%20and%20Yunpeng%20Qing%20and%20Kaixuan%20Chen%20and%20Tongya%20Zheng%20and%20Jie%20Song%20and%20Mingli%20Song%0AAbstract%3A%20%20%20Centralized%20Training%20with%20Decentralized%20Execution%20%28CTDE%29%20has%20recently%20emerged%0Aas%20a%20popular%20framework%20for%20cooperative%20Multi-Agent%20Reinforcement%20Learning%0A%28MARL%29%2C%20where%20agents%20can%20use%20additional%20global%20state%20information%20to%20guide%0Atraining%20in%20a%20centralized%20way%20and%20make%20their%20own%20decisions%20only%20based%20on%0Adecentralized%20local%20policies.%20Despite%20the%20encouraging%20results%20achieved%2C%20CTDE%0Amakes%20an%20independence%20assumption%20on%20agent%20policies%2C%20which%20limits%20agents%20to%0Aadopt%20global%20cooperative%20information%20from%20each%20other%20during%20centralized%0Atraining.%20Therefore%2C%20we%20argue%20that%20existing%20CTDE%20methods%20cannot%20fully%20utilize%0Aglobal%20information%20for%20training%2C%20leading%20to%20an%20inefficient%20joint-policy%0Aexploration%20and%20even%20suboptimal%20results.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0ACentralized%20Advising%20and%20Decentralized%20Pruning%20%28CADP%29%20framework%20for%20multi-agent%0Areinforcement%20learning%2C%20that%20not%20only%20enables%20an%20efficacious%20message%20exchange%0Aamong%20agents%20during%20training%20but%20also%20guarantees%20the%20independent%20policies%20for%0Aexecution.%20Firstly%2C%20CADP%20endows%20agents%20the%20explicit%20communication%20channel%20to%0Aseek%20and%20take%20advices%20from%20different%20agents%20for%20more%20centralized%20training.%20To%0Afurther%20ensure%20the%20decentralized%20execution%2C%20we%20propose%20a%20smooth%20model%20pruning%0Amechanism%20to%20progressively%20constraint%20the%20agent%20communication%20into%20a%20closed%20one%0Awithout%20degradation%20in%20agent%20cooperation%20capability.%20Empirical%20evaluations%20on%0AStarCraft%20II%20micromanagement%20and%20Google%20Research%20Football%20benchmarks%0Ademonstrate%20that%20the%20proposed%20framework%20achieves%20superior%20performance%20compared%0Awith%20the%20state-of-the-art%20counterparts.%20Our%20code%20will%20be%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.17352v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Centralized%2520Training%2520with%2520Decentralized%2520Execution%2520Framework%250A%2520%2520Centralized%2520Enough%2520for%2520MARL%253F%26entry.906535625%3DYihe%2520Zhou%2520and%2520Shunyu%2520Liu%2520and%2520Yunpeng%2520Qing%2520and%2520Kaixuan%2520Chen%2520and%2520Tongya%2520Zheng%2520and%2520Jie%2520Song%2520and%2520Mingli%2520Song%26entry.1292438233%3D%2520%2520Centralized%2520Training%2520with%2520Decentralized%2520Execution%2520%2528CTDE%2529%2520has%2520recently%2520emerged%250Aas%2520a%2520popular%2520framework%2520for%2520cooperative%2520Multi-Agent%2520Reinforcement%2520Learning%250A%2528MARL%2529%252C%2520where%2520agents%2520can%2520use%2520additional%2520global%2520state%2520information%2520to%2520guide%250Atraining%2520in%2520a%2520centralized%2520way%2520and%2520make%2520their%2520own%2520decisions%2520only%2520based%2520on%250Adecentralized%2520local%2520policies.%2520Despite%2520the%2520encouraging%2520results%2520achieved%252C%2520CTDE%250Amakes%2520an%2520independence%2520assumption%2520on%2520agent%2520policies%252C%2520which%2520limits%2520agents%2520to%250Aadopt%2520global%2520cooperative%2520information%2520from%2520each%2520other%2520during%2520centralized%250Atraining.%2520Therefore%252C%2520we%2520argue%2520that%2520existing%2520CTDE%2520methods%2520cannot%2520fully%2520utilize%250Aglobal%2520information%2520for%2520training%252C%2520leading%2520to%2520an%2520inefficient%2520joint-policy%250Aexploration%2520and%2520even%2520suboptimal%2520results.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250ACentralized%2520Advising%2520and%2520Decentralized%2520Pruning%2520%2528CADP%2529%2520framework%2520for%2520multi-agent%250Areinforcement%2520learning%252C%2520that%2520not%2520only%2520enables%2520an%2520efficacious%2520message%2520exchange%250Aamong%2520agents%2520during%2520training%2520but%2520also%2520guarantees%2520the%2520independent%2520policies%2520for%250Aexecution.%2520Firstly%252C%2520CADP%2520endows%2520agents%2520the%2520explicit%2520communication%2520channel%2520to%250Aseek%2520and%2520take%2520advices%2520from%2520different%2520agents%2520for%2520more%2520centralized%2520training.%2520To%250Afurther%2520ensure%2520the%2520decentralized%2520execution%252C%2520we%2520propose%2520a%2520smooth%2520model%2520pruning%250Amechanism%2520to%2520progressively%2520constraint%2520the%2520agent%2520communication%2520into%2520a%2520closed%2520one%250Awithout%2520degradation%2520in%2520agent%2520cooperation%2520capability.%2520Empirical%2520evaluations%2520on%250AStarCraft%2520II%2520micromanagement%2520and%2520Google%2520Research%2520Football%2520benchmarks%250Ademonstrate%2520that%2520the%2520proposed%2520framework%2520achieves%2520superior%2520performance%2520compared%250Awith%2520the%2520state-of-the-art%2520counterparts.%2520Our%2520code%2520will%2520be%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.17352v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Centralized%20Training%20with%20Decentralized%20Execution%20Framework%0A%20%20Centralized%20Enough%20for%20MARL%3F&entry.906535625=Yihe%20Zhou%20and%20Shunyu%20Liu%20and%20Yunpeng%20Qing%20and%20Kaixuan%20Chen%20and%20Tongya%20Zheng%20and%20Jie%20Song%20and%20Mingli%20Song&entry.1292438233=%20%20Centralized%20Training%20with%20Decentralized%20Execution%20%28CTDE%29%20has%20recently%20emerged%0Aas%20a%20popular%20framework%20for%20cooperative%20Multi-Agent%20Reinforcement%20Learning%0A%28MARL%29%2C%20where%20agents%20can%20use%20additional%20global%20state%20information%20to%20guide%0Atraining%20in%20a%20centralized%20way%20and%20make%20their%20own%20decisions%20only%20based%20on%0Adecentralized%20local%20policies.%20Despite%20the%20encouraging%20results%20achieved%2C%20CTDE%0Amakes%20an%20independence%20assumption%20on%20agent%20policies%2C%20which%20limits%20agents%20to%0Aadopt%20global%20cooperative%20information%20from%20each%20other%20during%20centralized%0Atraining.%20Therefore%2C%20we%20argue%20that%20existing%20CTDE%20methods%20cannot%20fully%20utilize%0Aglobal%20information%20for%20training%2C%20leading%20to%20an%20inefficient%20joint-policy%0Aexploration%20and%20even%20suboptimal%20results.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0ACentralized%20Advising%20and%20Decentralized%20Pruning%20%28CADP%29%20framework%20for%20multi-agent%0Areinforcement%20learning%2C%20that%20not%20only%20enables%20an%20efficacious%20message%20exchange%0Aamong%20agents%20during%20training%20but%20also%20guarantees%20the%20independent%20policies%20for%0Aexecution.%20Firstly%2C%20CADP%20endows%20agents%20the%20explicit%20communication%20channel%20to%0Aseek%20and%20take%20advices%20from%20different%20agents%20for%20more%20centralized%20training.%20To%0Afurther%20ensure%20the%20decentralized%20execution%2C%20we%20propose%20a%20smooth%20model%20pruning%0Amechanism%20to%20progressively%20constraint%20the%20agent%20communication%20into%20a%20closed%20one%0Awithout%20degradation%20in%20agent%20cooperation%20capability.%20Empirical%20evaluations%20on%0AStarCraft%20II%20micromanagement%20and%20Google%20Research%20Football%20benchmarks%0Ademonstrate%20that%20the%20proposed%20framework%20achieves%20superior%20performance%20compared%0Awith%20the%20state-of-the-art%20counterparts.%20Our%20code%20will%20be%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.17352v2&entry.124074799=Read"},
{"title": "Hierarchical and Multimodal Data for Daily Activity Understanding", "author": "Ghazal Kaviani and Yavuz Yarici and Seulgi Kim and Mohit Prabhushankar and Ghassan AlRegib and Mashhour Solh and Ameya Patil", "abstract": "  Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/\n", "link": "http://arxiv.org/abs/2504.17696v3", "date": "2025-05-13", "relevancy": 2.0957, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5326}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20and%20Multimodal%20Data%20for%20Daily%20Activity%20Understanding&body=Title%3A%20Hierarchical%20and%20Multimodal%20Data%20for%20Daily%20Activity%20Understanding%0AAuthor%3A%20Ghazal%20Kaviani%20and%20Yavuz%20Yarici%20and%20Seulgi%20Kim%20and%20Mohit%20Prabhushankar%20and%20Ghassan%20AlRegib%20and%20Mashhour%20Solh%20and%20Ameya%20Patil%0AAbstract%3A%20%20%20Daily%20Activity%20Recordings%20for%20Artificial%20Intelligence%20%28DARai%2C%20pronounced%0A%22Dahr-ree%22%29%20is%20a%20multimodal%2C%20hierarchically%20annotated%20dataset%20constructed%20to%0Aunderstand%20human%20activities%20in%20real-world%20settings.%20DARai%20consists%20of%0Acontinuous%20scripted%20and%20unscripted%20recordings%20of%2050%20participants%20in%2010%0Adifferent%20environments%2C%20totaling%20over%20200%20hours%20of%20data%20from%2020%20sensors%0Aincluding%20multiple%20camera%20views%2C%20depth%20and%20radar%20sensors%2C%20wearable%20inertial%0Ameasurement%20units%20%28IMUs%29%2C%20electromyography%20%28EMG%29%2C%20insole%20pressure%20sensors%2C%0Abiomonitor%20sensors%2C%20and%20gaze%20tracker.%0A%20%20To%20capture%20the%20complexity%20in%20human%20activities%2C%20DARai%20is%20annotated%20at%20three%0Alevels%20of%20hierarchy%3A%20%28i%29%20high-level%20activities%20%28L1%29%20that%20are%20independent%20tasks%2C%0A%28ii%29%20lower-level%20actions%20%28L2%29%20that%20are%20patterns%20shared%20between%20activities%2C%20and%0A%28iii%29%20fine-grained%20procedures%20%28L3%29%20that%20detail%20the%20exact%20execution%20steps%20for%0Aactions.%20The%20dataset%20annotations%20and%20recordings%20are%20designed%20so%20that%2022.7%25%20of%0AL2%20actions%20are%20shared%20between%20L1%20activities%20and%2014.2%25%20of%20L3%20procedures%20are%0Ashared%20between%20L2%20actions.%20The%20overlap%20and%20unscripted%20nature%20of%20DARai%20allows%0Acounterfactual%20activities%20in%20the%20dataset.%0A%20%20Experiments%20with%20various%20machine%20learning%20models%20showcase%20the%20value%20of%20DARai%0Ain%20uncovering%20important%20challenges%20in%20human-centered%20applications.%0ASpecifically%2C%20we%20conduct%20unimodal%20and%20multimodal%20sensor%20fusion%20experiments%20for%0Arecognition%2C%20temporal%20localization%2C%20and%20future%20action%20anticipation%20across%20all%0Ahierarchical%20annotation%20levels.%20To%20highlight%20the%20limitations%20of%20individual%0Asensors%2C%20we%20also%20conduct%20domain-variant%20experiments%20that%20are%20enabled%20by%20DARai%27s%0Amulti-sensor%20and%20counterfactual%20activity%20design%20setup.%0A%20%20The%20code%2C%20documentation%2C%20and%20dataset%20are%20available%20at%20the%20dedicated%20DARai%0Awebsite%3A%0Ahttps%3A//alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17696v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520and%2520Multimodal%2520Data%2520for%2520Daily%2520Activity%2520Understanding%26entry.906535625%3DGhazal%2520Kaviani%2520and%2520Yavuz%2520Yarici%2520and%2520Seulgi%2520Kim%2520and%2520Mohit%2520Prabhushankar%2520and%2520Ghassan%2520AlRegib%2520and%2520Mashhour%2520Solh%2520and%2520Ameya%2520Patil%26entry.1292438233%3D%2520%2520Daily%2520Activity%2520Recordings%2520for%2520Artificial%2520Intelligence%2520%2528DARai%252C%2520pronounced%250A%2522Dahr-ree%2522%2529%2520is%2520a%2520multimodal%252C%2520hierarchically%2520annotated%2520dataset%2520constructed%2520to%250Aunderstand%2520human%2520activities%2520in%2520real-world%2520settings.%2520DARai%2520consists%2520of%250Acontinuous%2520scripted%2520and%2520unscripted%2520recordings%2520of%252050%2520participants%2520in%252010%250Adifferent%2520environments%252C%2520totaling%2520over%2520200%2520hours%2520of%2520data%2520from%252020%2520sensors%250Aincluding%2520multiple%2520camera%2520views%252C%2520depth%2520and%2520radar%2520sensors%252C%2520wearable%2520inertial%250Ameasurement%2520units%2520%2528IMUs%2529%252C%2520electromyography%2520%2528EMG%2529%252C%2520insole%2520pressure%2520sensors%252C%250Abiomonitor%2520sensors%252C%2520and%2520gaze%2520tracker.%250A%2520%2520To%2520capture%2520the%2520complexity%2520in%2520human%2520activities%252C%2520DARai%2520is%2520annotated%2520at%2520three%250Alevels%2520of%2520hierarchy%253A%2520%2528i%2529%2520high-level%2520activities%2520%2528L1%2529%2520that%2520are%2520independent%2520tasks%252C%250A%2528ii%2529%2520lower-level%2520actions%2520%2528L2%2529%2520that%2520are%2520patterns%2520shared%2520between%2520activities%252C%2520and%250A%2528iii%2529%2520fine-grained%2520procedures%2520%2528L3%2529%2520that%2520detail%2520the%2520exact%2520execution%2520steps%2520for%250Aactions.%2520The%2520dataset%2520annotations%2520and%2520recordings%2520are%2520designed%2520so%2520that%252022.7%2525%2520of%250AL2%2520actions%2520are%2520shared%2520between%2520L1%2520activities%2520and%252014.2%2525%2520of%2520L3%2520procedures%2520are%250Ashared%2520between%2520L2%2520actions.%2520The%2520overlap%2520and%2520unscripted%2520nature%2520of%2520DARai%2520allows%250Acounterfactual%2520activities%2520in%2520the%2520dataset.%250A%2520%2520Experiments%2520with%2520various%2520machine%2520learning%2520models%2520showcase%2520the%2520value%2520of%2520DARai%250Ain%2520uncovering%2520important%2520challenges%2520in%2520human-centered%2520applications.%250ASpecifically%252C%2520we%2520conduct%2520unimodal%2520and%2520multimodal%2520sensor%2520fusion%2520experiments%2520for%250Arecognition%252C%2520temporal%2520localization%252C%2520and%2520future%2520action%2520anticipation%2520across%2520all%250Ahierarchical%2520annotation%2520levels.%2520To%2520highlight%2520the%2520limitations%2520of%2520individual%250Asensors%252C%2520we%2520also%2520conduct%2520domain-variant%2520experiments%2520that%2520are%2520enabled%2520by%2520DARai%2527s%250Amulti-sensor%2520and%2520counterfactual%2520activity%2520design%2520setup.%250A%2520%2520The%2520code%252C%2520documentation%252C%2520and%2520dataset%2520are%2520available%2520at%2520the%2520dedicated%2520DARai%250Awebsite%253A%250Ahttps%253A//alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17696v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20and%20Multimodal%20Data%20for%20Daily%20Activity%20Understanding&entry.906535625=Ghazal%20Kaviani%20and%20Yavuz%20Yarici%20and%20Seulgi%20Kim%20and%20Mohit%20Prabhushankar%20and%20Ghassan%20AlRegib%20and%20Mashhour%20Solh%20and%20Ameya%20Patil&entry.1292438233=%20%20Daily%20Activity%20Recordings%20for%20Artificial%20Intelligence%20%28DARai%2C%20pronounced%0A%22Dahr-ree%22%29%20is%20a%20multimodal%2C%20hierarchically%20annotated%20dataset%20constructed%20to%0Aunderstand%20human%20activities%20in%20real-world%20settings.%20DARai%20consists%20of%0Acontinuous%20scripted%20and%20unscripted%20recordings%20of%2050%20participants%20in%2010%0Adifferent%20environments%2C%20totaling%20over%20200%20hours%20of%20data%20from%2020%20sensors%0Aincluding%20multiple%20camera%20views%2C%20depth%20and%20radar%20sensors%2C%20wearable%20inertial%0Ameasurement%20units%20%28IMUs%29%2C%20electromyography%20%28EMG%29%2C%20insole%20pressure%20sensors%2C%0Abiomonitor%20sensors%2C%20and%20gaze%20tracker.%0A%20%20To%20capture%20the%20complexity%20in%20human%20activities%2C%20DARai%20is%20annotated%20at%20three%0Alevels%20of%20hierarchy%3A%20%28i%29%20high-level%20activities%20%28L1%29%20that%20are%20independent%20tasks%2C%0A%28ii%29%20lower-level%20actions%20%28L2%29%20that%20are%20patterns%20shared%20between%20activities%2C%20and%0A%28iii%29%20fine-grained%20procedures%20%28L3%29%20that%20detail%20the%20exact%20execution%20steps%20for%0Aactions.%20The%20dataset%20annotations%20and%20recordings%20are%20designed%20so%20that%2022.7%25%20of%0AL2%20actions%20are%20shared%20between%20L1%20activities%20and%2014.2%25%20of%20L3%20procedures%20are%0Ashared%20between%20L2%20actions.%20The%20overlap%20and%20unscripted%20nature%20of%20DARai%20allows%0Acounterfactual%20activities%20in%20the%20dataset.%0A%20%20Experiments%20with%20various%20machine%20learning%20models%20showcase%20the%20value%20of%20DARai%0Ain%20uncovering%20important%20challenges%20in%20human-centered%20applications.%0ASpecifically%2C%20we%20conduct%20unimodal%20and%20multimodal%20sensor%20fusion%20experiments%20for%0Arecognition%2C%20temporal%20localization%2C%20and%20future%20action%20anticipation%20across%20all%0Ahierarchical%20annotation%20levels.%20To%20highlight%20the%20limitations%20of%20individual%0Asensors%2C%20we%20also%20conduct%20domain-variant%20experiments%20that%20are%20enabled%20by%20DARai%27s%0Amulti-sensor%20and%20counterfactual%20activity%20design%20setup.%0A%20%20The%20code%2C%20documentation%2C%20and%20dataset%20are%20available%20at%20the%20dedicated%20DARai%0Awebsite%3A%0Ahttps%3A//alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17696v3&entry.124074799=Read"},
{"title": "WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree\n  Complex Wavelet and Graph Neural Networks", "author": "Ziyuan He and Zhiqing Guo and Liejun Wang and Gaobo Yang and Yunfeng Diao and Dan Ma", "abstract": "  Deepfake technology poses increasing risks such as privacy invasion and\nidentity theft. To address these threats, we propose WaveGuard, a proactive\nwatermarking framework that enhances robustness and imperceptibility via\nfrequency-domain embedding and graph-based structural consistency.\nSpecifically, we embed watermarks into high-frequency sub-bands using Dual-Tree\nComplex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph\nNeural Network (SC-GNN) to preserve visual quality. We also design an attention\nmodule to refine embedding precision. Experimental results on face swap and\nreenactment tasks demonstrate that WaveGuard outperforms state-of-the-art\nmethods in both robustness and visual quality. Code is available at\nhttps://github.com/vpsg-research/WaveGuard.\n", "link": "http://arxiv.org/abs/2505.08614v1", "date": "2025-05-13", "relevancy": 2.0787, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5275}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5202}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WaveGuard%3A%20Robust%20Deepfake%20Detection%20and%20Source%20Tracing%20via%20Dual-Tree%0A%20%20Complex%20Wavelet%20and%20Graph%20Neural%20Networks&body=Title%3A%20WaveGuard%3A%20Robust%20Deepfake%20Detection%20and%20Source%20Tracing%20via%20Dual-Tree%0A%20%20Complex%20Wavelet%20and%20Graph%20Neural%20Networks%0AAuthor%3A%20Ziyuan%20He%20and%20Zhiqing%20Guo%20and%20Liejun%20Wang%20and%20Gaobo%20Yang%20and%20Yunfeng%20Diao%20and%20Dan%20Ma%0AAbstract%3A%20%20%20Deepfake%20technology%20poses%20increasing%20risks%20such%20as%20privacy%20invasion%20and%0Aidentity%20theft.%20To%20address%20these%20threats%2C%20we%20propose%20WaveGuard%2C%20a%20proactive%0Awatermarking%20framework%20that%20enhances%20robustness%20and%20imperceptibility%20via%0Afrequency-domain%20embedding%20and%20graph-based%20structural%20consistency.%0ASpecifically%2C%20we%20embed%20watermarks%20into%20high-frequency%20sub-bands%20using%20Dual-Tree%0AComplex%20Wavelet%20Transform%20%28DT-CWT%29%20and%20employ%20a%20Structural%20Consistency%20Graph%0ANeural%20Network%20%28SC-GNN%29%20to%20preserve%20visual%20quality.%20We%20also%20design%20an%20attention%0Amodule%20to%20refine%20embedding%20precision.%20Experimental%20results%20on%20face%20swap%20and%0Areenactment%20tasks%20demonstrate%20that%20WaveGuard%20outperforms%20state-of-the-art%0Amethods%20in%20both%20robustness%20and%20visual%20quality.%20Code%20is%20available%20at%0Ahttps%3A//github.com/vpsg-research/WaveGuard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWaveGuard%253A%2520Robust%2520Deepfake%2520Detection%2520and%2520Source%2520Tracing%2520via%2520Dual-Tree%250A%2520%2520Complex%2520Wavelet%2520and%2520Graph%2520Neural%2520Networks%26entry.906535625%3DZiyuan%2520He%2520and%2520Zhiqing%2520Guo%2520and%2520Liejun%2520Wang%2520and%2520Gaobo%2520Yang%2520and%2520Yunfeng%2520Diao%2520and%2520Dan%2520Ma%26entry.1292438233%3D%2520%2520Deepfake%2520technology%2520poses%2520increasing%2520risks%2520such%2520as%2520privacy%2520invasion%2520and%250Aidentity%2520theft.%2520To%2520address%2520these%2520threats%252C%2520we%2520propose%2520WaveGuard%252C%2520a%2520proactive%250Awatermarking%2520framework%2520that%2520enhances%2520robustness%2520and%2520imperceptibility%2520via%250Afrequency-domain%2520embedding%2520and%2520graph-based%2520structural%2520consistency.%250ASpecifically%252C%2520we%2520embed%2520watermarks%2520into%2520high-frequency%2520sub-bands%2520using%2520Dual-Tree%250AComplex%2520Wavelet%2520Transform%2520%2528DT-CWT%2529%2520and%2520employ%2520a%2520Structural%2520Consistency%2520Graph%250ANeural%2520Network%2520%2528SC-GNN%2529%2520to%2520preserve%2520visual%2520quality.%2520We%2520also%2520design%2520an%2520attention%250Amodule%2520to%2520refine%2520embedding%2520precision.%2520Experimental%2520results%2520on%2520face%2520swap%2520and%250Areenactment%2520tasks%2520demonstrate%2520that%2520WaveGuard%2520outperforms%2520state-of-the-art%250Amethods%2520in%2520both%2520robustness%2520and%2520visual%2520quality.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/vpsg-research/WaveGuard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WaveGuard%3A%20Robust%20Deepfake%20Detection%20and%20Source%20Tracing%20via%20Dual-Tree%0A%20%20Complex%20Wavelet%20and%20Graph%20Neural%20Networks&entry.906535625=Ziyuan%20He%20and%20Zhiqing%20Guo%20and%20Liejun%20Wang%20and%20Gaobo%20Yang%20and%20Yunfeng%20Diao%20and%20Dan%20Ma&entry.1292438233=%20%20Deepfake%20technology%20poses%20increasing%20risks%20such%20as%20privacy%20invasion%20and%0Aidentity%20theft.%20To%20address%20these%20threats%2C%20we%20propose%20WaveGuard%2C%20a%20proactive%0Awatermarking%20framework%20that%20enhances%20robustness%20and%20imperceptibility%20via%0Afrequency-domain%20embedding%20and%20graph-based%20structural%20consistency.%0ASpecifically%2C%20we%20embed%20watermarks%20into%20high-frequency%20sub-bands%20using%20Dual-Tree%0AComplex%20Wavelet%20Transform%20%28DT-CWT%29%20and%20employ%20a%20Structural%20Consistency%20Graph%0ANeural%20Network%20%28SC-GNN%29%20to%20preserve%20visual%20quality.%20We%20also%20design%20an%20attention%0Amodule%20to%20refine%20embedding%20precision.%20Experimental%20results%20on%20face%20swap%20and%0Areenactment%20tasks%20demonstrate%20that%20WaveGuard%20outperforms%20state-of-the-art%0Amethods%20in%20both%20robustness%20and%20visual%20quality.%20Code%20is%20available%20at%0Ahttps%3A//github.com/vpsg-research/WaveGuard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08614v1&entry.124074799=Read"},
{"title": "Boosting Zero-shot Stereo Matching using Large-scale Mixed Images\n  Sources in the Real World", "author": "Yuran Wang and Yingping Liang and Ying Fu", "abstract": "  Stereo matching methods rely on dense pixel-wise ground truth labels, which\nare laborious to obtain, especially for real-world datasets. The scarcity of\nlabeled data and domain gaps between synthetic and real-world images also pose\nnotable challenges. In this paper, we propose a novel framework,\n\\textbf{BooSTer}, that leverages both vision foundation models and large-scale\nmixed image sources, including synthetic, real, and single-view images. First,\nto fully unleash the potential of large-scale single-view images, we design a\ndata generation strategy combining monocular depth estimation and diffusion\nmodels to generate dense stereo matching data from single-view images. Second,\nto tackle sparse labels in real-world datasets, we transfer knowledge from\nmonocular depth estimation models, using pseudo-mono depth labels and a dynamic\nscale- and shift-invariant loss for additional supervision. Furthermore, we\nincorporate vision foundation model as an encoder to extract robust and\ntransferable features, boosting accuracy and generalization. Extensive\nexperiments on benchmark datasets demonstrate the effectiveness of our\napproach, achieving significant improvements in accuracy over existing methods,\nparticularly in scenarios with limited labeled data and domain shifts.\n", "link": "http://arxiv.org/abs/2505.08607v1", "date": "2025-05-13", "relevancy": 2.0765, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5291}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5143}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Zero-shot%20Stereo%20Matching%20using%20Large-scale%20Mixed%20Images%0A%20%20Sources%20in%20the%20Real%20World&body=Title%3A%20Boosting%20Zero-shot%20Stereo%20Matching%20using%20Large-scale%20Mixed%20Images%0A%20%20Sources%20in%20the%20Real%20World%0AAuthor%3A%20Yuran%20Wang%20and%20Yingping%20Liang%20and%20Ying%20Fu%0AAbstract%3A%20%20%20Stereo%20matching%20methods%20rely%20on%20dense%20pixel-wise%20ground%20truth%20labels%2C%20which%0Aare%20laborious%20to%20obtain%2C%20especially%20for%20real-world%20datasets.%20The%20scarcity%20of%0Alabeled%20data%20and%20domain%20gaps%20between%20synthetic%20and%20real-world%20images%20also%20pose%0Anotable%20challenges.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%2C%0A%5Ctextbf%7BBooSTer%7D%2C%20that%20leverages%20both%20vision%20foundation%20models%20and%20large-scale%0Amixed%20image%20sources%2C%20including%20synthetic%2C%20real%2C%20and%20single-view%20images.%20First%2C%0Ato%20fully%20unleash%20the%20potential%20of%20large-scale%20single-view%20images%2C%20we%20design%20a%0Adata%20generation%20strategy%20combining%20monocular%20depth%20estimation%20and%20diffusion%0Amodels%20to%20generate%20dense%20stereo%20matching%20data%20from%20single-view%20images.%20Second%2C%0Ato%20tackle%20sparse%20labels%20in%20real-world%20datasets%2C%20we%20transfer%20knowledge%20from%0Amonocular%20depth%20estimation%20models%2C%20using%20pseudo-mono%20depth%20labels%20and%20a%20dynamic%0Ascale-%20and%20shift-invariant%20loss%20for%20additional%20supervision.%20Furthermore%2C%20we%0Aincorporate%20vision%20foundation%20model%20as%20an%20encoder%20to%20extract%20robust%20and%0Atransferable%20features%2C%20boosting%20accuracy%20and%20generalization.%20Extensive%0Aexperiments%20on%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%2C%20achieving%20significant%20improvements%20in%20accuracy%20over%20existing%20methods%2C%0Aparticularly%20in%20scenarios%20with%20limited%20labeled%20data%20and%20domain%20shifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Zero-shot%2520Stereo%2520Matching%2520using%2520Large-scale%2520Mixed%2520Images%250A%2520%2520Sources%2520in%2520the%2520Real%2520World%26entry.906535625%3DYuran%2520Wang%2520and%2520Yingping%2520Liang%2520and%2520Ying%2520Fu%26entry.1292438233%3D%2520%2520Stereo%2520matching%2520methods%2520rely%2520on%2520dense%2520pixel-wise%2520ground%2520truth%2520labels%252C%2520which%250Aare%2520laborious%2520to%2520obtain%252C%2520especially%2520for%2520real-world%2520datasets.%2520The%2520scarcity%2520of%250Alabeled%2520data%2520and%2520domain%2520gaps%2520between%2520synthetic%2520and%2520real-world%2520images%2520also%2520pose%250Anotable%2520challenges.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%250A%255Ctextbf%257BBooSTer%257D%252C%2520that%2520leverages%2520both%2520vision%2520foundation%2520models%2520and%2520large-scale%250Amixed%2520image%2520sources%252C%2520including%2520synthetic%252C%2520real%252C%2520and%2520single-view%2520images.%2520First%252C%250Ato%2520fully%2520unleash%2520the%2520potential%2520of%2520large-scale%2520single-view%2520images%252C%2520we%2520design%2520a%250Adata%2520generation%2520strategy%2520combining%2520monocular%2520depth%2520estimation%2520and%2520diffusion%250Amodels%2520to%2520generate%2520dense%2520stereo%2520matching%2520data%2520from%2520single-view%2520images.%2520Second%252C%250Ato%2520tackle%2520sparse%2520labels%2520in%2520real-world%2520datasets%252C%2520we%2520transfer%2520knowledge%2520from%250Amonocular%2520depth%2520estimation%2520models%252C%2520using%2520pseudo-mono%2520depth%2520labels%2520and%2520a%2520dynamic%250Ascale-%2520and%2520shift-invariant%2520loss%2520for%2520additional%2520supervision.%2520Furthermore%252C%2520we%250Aincorporate%2520vision%2520foundation%2520model%2520as%2520an%2520encoder%2520to%2520extract%2520robust%2520and%250Atransferable%2520features%252C%2520boosting%2520accuracy%2520and%2520generalization.%2520Extensive%250Aexperiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aapproach%252C%2520achieving%2520significant%2520improvements%2520in%2520accuracy%2520over%2520existing%2520methods%252C%250Aparticularly%2520in%2520scenarios%2520with%2520limited%2520labeled%2520data%2520and%2520domain%2520shifts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Zero-shot%20Stereo%20Matching%20using%20Large-scale%20Mixed%20Images%0A%20%20Sources%20in%20the%20Real%20World&entry.906535625=Yuran%20Wang%20and%20Yingping%20Liang%20and%20Ying%20Fu&entry.1292438233=%20%20Stereo%20matching%20methods%20rely%20on%20dense%20pixel-wise%20ground%20truth%20labels%2C%20which%0Aare%20laborious%20to%20obtain%2C%20especially%20for%20real-world%20datasets.%20The%20scarcity%20of%0Alabeled%20data%20and%20domain%20gaps%20between%20synthetic%20and%20real-world%20images%20also%20pose%0Anotable%20challenges.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%2C%0A%5Ctextbf%7BBooSTer%7D%2C%20that%20leverages%20both%20vision%20foundation%20models%20and%20large-scale%0Amixed%20image%20sources%2C%20including%20synthetic%2C%20real%2C%20and%20single-view%20images.%20First%2C%0Ato%20fully%20unleash%20the%20potential%20of%20large-scale%20single-view%20images%2C%20we%20design%20a%0Adata%20generation%20strategy%20combining%20monocular%20depth%20estimation%20and%20diffusion%0Amodels%20to%20generate%20dense%20stereo%20matching%20data%20from%20single-view%20images.%20Second%2C%0Ato%20tackle%20sparse%20labels%20in%20real-world%20datasets%2C%20we%20transfer%20knowledge%20from%0Amonocular%20depth%20estimation%20models%2C%20using%20pseudo-mono%20depth%20labels%20and%20a%20dynamic%0Ascale-%20and%20shift-invariant%20loss%20for%20additional%20supervision.%20Furthermore%2C%20we%0Aincorporate%20vision%20foundation%20model%20as%20an%20encoder%20to%20extract%20robust%20and%0Atransferable%20features%2C%20boosting%20accuracy%20and%20generalization.%20Extensive%0Aexperiments%20on%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%2C%20achieving%20significant%20improvements%20in%20accuracy%20over%20existing%20methods%2C%0Aparticularly%20in%20scenarios%20with%20limited%20labeled%20data%20and%20domain%20shifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08607v1&entry.124074799=Read"},
{"title": "Scaling Context, Not Parameters: Training a Compact 7B Language Model\n  for Efficient Long-Context Processing", "author": "Chen Wu and Yin Song", "abstract": "  We present MegaBeam-Mistral-7B, a language model that supports 512K-token\ncontext length. Our work addresses practical limitations in long-context\ntraining, supporting real-world tasks such as compliance monitoring and\nverification. Evaluated on three long-context benchmarks, our 7B-parameter\nmodel demonstrates superior in-context learning performance on HELMET and\nrobust retrieval and tracing capability on RULER. It is currently the only open\nmodel to achieve competitive long-range reasoning on BABILong at 512K context\nlength without RAG or targeted fine-tuning. Released as fully open source under\nthe Apache 2.0 license, the model has been downloaded over 100,000 times on\nHugging Face. Model available at:\nhttps://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k\n", "link": "http://arxiv.org/abs/2505.08651v1", "date": "2025-05-13", "relevancy": 2.0568, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Context%2C%20Not%20Parameters%3A%20Training%20a%20Compact%207B%20Language%20Model%0A%20%20for%20Efficient%20Long-Context%20Processing&body=Title%3A%20Scaling%20Context%2C%20Not%20Parameters%3A%20Training%20a%20Compact%207B%20Language%20Model%0A%20%20for%20Efficient%20Long-Context%20Processing%0AAuthor%3A%20Chen%20Wu%20and%20Yin%20Song%0AAbstract%3A%20%20%20We%20present%20MegaBeam-Mistral-7B%2C%20a%20language%20model%20that%20supports%20512K-token%0Acontext%20length.%20Our%20work%20addresses%20practical%20limitations%20in%20long-context%0Atraining%2C%20supporting%20real-world%20tasks%20such%20as%20compliance%20monitoring%20and%0Averification.%20Evaluated%20on%20three%20long-context%20benchmarks%2C%20our%207B-parameter%0Amodel%20demonstrates%20superior%20in-context%20learning%20performance%20on%20HELMET%20and%0Arobust%20retrieval%20and%20tracing%20capability%20on%20RULER.%20It%20is%20currently%20the%20only%20open%0Amodel%20to%20achieve%20competitive%20long-range%20reasoning%20on%20BABILong%20at%20512K%20context%0Alength%20without%20RAG%20or%20targeted%20fine-tuning.%20Released%20as%20fully%20open%20source%20under%0Athe%20Apache%202.0%20license%2C%20the%20model%20has%20been%20downloaded%20over%20100%2C000%20times%20on%0AHugging%20Face.%20Model%20available%20at%3A%0Ahttps%3A//huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Context%252C%2520Not%2520Parameters%253A%2520Training%2520a%2520Compact%25207B%2520Language%2520Model%250A%2520%2520for%2520Efficient%2520Long-Context%2520Processing%26entry.906535625%3DChen%2520Wu%2520and%2520Yin%2520Song%26entry.1292438233%3D%2520%2520We%2520present%2520MegaBeam-Mistral-7B%252C%2520a%2520language%2520model%2520that%2520supports%2520512K-token%250Acontext%2520length.%2520Our%2520work%2520addresses%2520practical%2520limitations%2520in%2520long-context%250Atraining%252C%2520supporting%2520real-world%2520tasks%2520such%2520as%2520compliance%2520monitoring%2520and%250Averification.%2520Evaluated%2520on%2520three%2520long-context%2520benchmarks%252C%2520our%25207B-parameter%250Amodel%2520demonstrates%2520superior%2520in-context%2520learning%2520performance%2520on%2520HELMET%2520and%250Arobust%2520retrieval%2520and%2520tracing%2520capability%2520on%2520RULER.%2520It%2520is%2520currently%2520the%2520only%2520open%250Amodel%2520to%2520achieve%2520competitive%2520long-range%2520reasoning%2520on%2520BABILong%2520at%2520512K%2520context%250Alength%2520without%2520RAG%2520or%2520targeted%2520fine-tuning.%2520Released%2520as%2520fully%2520open%2520source%2520under%250Athe%2520Apache%25202.0%2520license%252C%2520the%2520model%2520has%2520been%2520downloaded%2520over%2520100%252C000%2520times%2520on%250AHugging%2520Face.%2520Model%2520available%2520at%253A%250Ahttps%253A//huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Context%2C%20Not%20Parameters%3A%20Training%20a%20Compact%207B%20Language%20Model%0A%20%20for%20Efficient%20Long-Context%20Processing&entry.906535625=Chen%20Wu%20and%20Yin%20Song&entry.1292438233=%20%20We%20present%20MegaBeam-Mistral-7B%2C%20a%20language%20model%20that%20supports%20512K-token%0Acontext%20length.%20Our%20work%20addresses%20practical%20limitations%20in%20long-context%0Atraining%2C%20supporting%20real-world%20tasks%20such%20as%20compliance%20monitoring%20and%0Averification.%20Evaluated%20on%20three%20long-context%20benchmarks%2C%20our%207B-parameter%0Amodel%20demonstrates%20superior%20in-context%20learning%20performance%20on%20HELMET%20and%0Arobust%20retrieval%20and%20tracing%20capability%20on%20RULER.%20It%20is%20currently%20the%20only%20open%0Amodel%20to%20achieve%20competitive%20long-range%20reasoning%20on%20BABILong%20at%20512K%20context%0Alength%20without%20RAG%20or%20targeted%20fine-tuning.%20Released%20as%20fully%20open%20source%20under%0Athe%20Apache%202.0%20license%2C%20the%20model%20has%20been%20downloaded%20over%20100%2C000%20times%20on%0AHugging%20Face.%20Model%20available%20at%3A%0Ahttps%3A//huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08651v1&entry.124074799=Read"},
{"title": "The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large\n  Language Models Unmask Fake News", "author": "Yuhan Liu and Yuxuan Liu and Xiaoqing Zhang and Xiuying Chen and Rui Yan", "abstract": "  In today's digital environment, the rapid propagation of fake news via social\nnetworks poses significant social challenges. Most existing detection methods\neither employ traditional classification models, which suffer from low\ninterpretability and limited generalization capabilities, or craft specific\nprompts for large language models (LLMs) to produce explanations and results\ndirectly, failing to leverage LLMs' reasoning abilities fully. Inspired by the\nsaying that \"truth becomes clearer through debate,\" our study introduces a\nnovel multi-agent system with LLMs named TruEDebate (TED) to enhance the\ninterpretability and effectiveness of fake news detection. TED employs a\nrigorous debate process inspired by formal debate settings. Central to our\napproach are two innovative components: the DebateFlow Agents and the\nInsightFlow Agents. The DebateFlow Agents organize agents into two teams, where\none supports and the other challenges the truth of the news. These agents\nengage in opening statements, cross-examination, rebuttal, and closing\nstatements, simulating a rigorous debate process akin to human discourse\nanalysis, allowing for a thorough evaluation of news content. Concurrently, the\nInsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent\nand the Analysis Agent. The Synthesis Agent summarizes the debates and provides\nan overarching viewpoint, ensuring a coherent and comprehensive evaluation. The\nAnalysis Agent, which includes a role-aware encoder and a debate graph,\nintegrates role embeddings and models the interactions between debate roles and\narguments using an attention mechanism, providing the final judgment.\n", "link": "http://arxiv.org/abs/2505.08532v1", "date": "2025-05-13", "relevancy": 2.0554, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5113}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Truth%20Becomes%20Clearer%20Through%20Debate%21%20Multi-Agent%20Systems%20with%20Large%0A%20%20Language%20Models%20Unmask%20Fake%20News&body=Title%3A%20The%20Truth%20Becomes%20Clearer%20Through%20Debate%21%20Multi-Agent%20Systems%20with%20Large%0A%20%20Language%20Models%20Unmask%20Fake%20News%0AAuthor%3A%20Yuhan%20Liu%20and%20Yuxuan%20Liu%20and%20Xiaoqing%20Zhang%20and%20Xiuying%20Chen%20and%20Rui%20Yan%0AAbstract%3A%20%20%20In%20today%27s%20digital%20environment%2C%20the%20rapid%20propagation%20of%20fake%20news%20via%20social%0Anetworks%20poses%20significant%20social%20challenges.%20Most%20existing%20detection%20methods%0Aeither%20employ%20traditional%20classification%20models%2C%20which%20suffer%20from%20low%0Ainterpretability%20and%20limited%20generalization%20capabilities%2C%20or%20craft%20specific%0Aprompts%20for%20large%20language%20models%20%28LLMs%29%20to%20produce%20explanations%20and%20results%0Adirectly%2C%20failing%20to%20leverage%20LLMs%27%20reasoning%20abilities%20fully.%20Inspired%20by%20the%0Asaying%20that%20%22truth%20becomes%20clearer%20through%20debate%2C%22%20our%20study%20introduces%20a%0Anovel%20multi-agent%20system%20with%20LLMs%20named%20TruEDebate%20%28TED%29%20to%20enhance%20the%0Ainterpretability%20and%20effectiveness%20of%20fake%20news%20detection.%20TED%20employs%20a%0Arigorous%20debate%20process%20inspired%20by%20formal%20debate%20settings.%20Central%20to%20our%0Aapproach%20are%20two%20innovative%20components%3A%20the%20DebateFlow%20Agents%20and%20the%0AInsightFlow%20Agents.%20The%20DebateFlow%20Agents%20organize%20agents%20into%20two%20teams%2C%20where%0Aone%20supports%20and%20the%20other%20challenges%20the%20truth%20of%20the%20news.%20These%20agents%0Aengage%20in%20opening%20statements%2C%20cross-examination%2C%20rebuttal%2C%20and%20closing%0Astatements%2C%20simulating%20a%20rigorous%20debate%20process%20akin%20to%20human%20discourse%0Aanalysis%2C%20allowing%20for%20a%20thorough%20evaluation%20of%20news%20content.%20Concurrently%2C%20the%0AInsightFlow%20Agents%20consist%20of%20two%20specialized%20sub-agents%3A%20the%20Synthesis%20Agent%0Aand%20the%20Analysis%20Agent.%20The%20Synthesis%20Agent%20summarizes%20the%20debates%20and%20provides%0Aan%20overarching%20viewpoint%2C%20ensuring%20a%20coherent%20and%20comprehensive%20evaluation.%20The%0AAnalysis%20Agent%2C%20which%20includes%20a%20role-aware%20encoder%20and%20a%20debate%20graph%2C%0Aintegrates%20role%20embeddings%20and%20models%20the%20interactions%20between%20debate%20roles%20and%0Aarguments%20using%20an%20attention%20mechanism%2C%20providing%20the%20final%20judgment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Truth%2520Becomes%2520Clearer%2520Through%2520Debate%2521%2520Multi-Agent%2520Systems%2520with%2520Large%250A%2520%2520Language%2520Models%2520Unmask%2520Fake%2520News%26entry.906535625%3DYuhan%2520Liu%2520and%2520Yuxuan%2520Liu%2520and%2520Xiaoqing%2520Zhang%2520and%2520Xiuying%2520Chen%2520and%2520Rui%2520Yan%26entry.1292438233%3D%2520%2520In%2520today%2527s%2520digital%2520environment%252C%2520the%2520rapid%2520propagation%2520of%2520fake%2520news%2520via%2520social%250Anetworks%2520poses%2520significant%2520social%2520challenges.%2520Most%2520existing%2520detection%2520methods%250Aeither%2520employ%2520traditional%2520classification%2520models%252C%2520which%2520suffer%2520from%2520low%250Ainterpretability%2520and%2520limited%2520generalization%2520capabilities%252C%2520or%2520craft%2520specific%250Aprompts%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520produce%2520explanations%2520and%2520results%250Adirectly%252C%2520failing%2520to%2520leverage%2520LLMs%2527%2520reasoning%2520abilities%2520fully.%2520Inspired%2520by%2520the%250Asaying%2520that%2520%2522truth%2520becomes%2520clearer%2520through%2520debate%252C%2522%2520our%2520study%2520introduces%2520a%250Anovel%2520multi-agent%2520system%2520with%2520LLMs%2520named%2520TruEDebate%2520%2528TED%2529%2520to%2520enhance%2520the%250Ainterpretability%2520and%2520effectiveness%2520of%2520fake%2520news%2520detection.%2520TED%2520employs%2520a%250Arigorous%2520debate%2520process%2520inspired%2520by%2520formal%2520debate%2520settings.%2520Central%2520to%2520our%250Aapproach%2520are%2520two%2520innovative%2520components%253A%2520the%2520DebateFlow%2520Agents%2520and%2520the%250AInsightFlow%2520Agents.%2520The%2520DebateFlow%2520Agents%2520organize%2520agents%2520into%2520two%2520teams%252C%2520where%250Aone%2520supports%2520and%2520the%2520other%2520challenges%2520the%2520truth%2520of%2520the%2520news.%2520These%2520agents%250Aengage%2520in%2520opening%2520statements%252C%2520cross-examination%252C%2520rebuttal%252C%2520and%2520closing%250Astatements%252C%2520simulating%2520a%2520rigorous%2520debate%2520process%2520akin%2520to%2520human%2520discourse%250Aanalysis%252C%2520allowing%2520for%2520a%2520thorough%2520evaluation%2520of%2520news%2520content.%2520Concurrently%252C%2520the%250AInsightFlow%2520Agents%2520consist%2520of%2520two%2520specialized%2520sub-agents%253A%2520the%2520Synthesis%2520Agent%250Aand%2520the%2520Analysis%2520Agent.%2520The%2520Synthesis%2520Agent%2520summarizes%2520the%2520debates%2520and%2520provides%250Aan%2520overarching%2520viewpoint%252C%2520ensuring%2520a%2520coherent%2520and%2520comprehensive%2520evaluation.%2520The%250AAnalysis%2520Agent%252C%2520which%2520includes%2520a%2520role-aware%2520encoder%2520and%2520a%2520debate%2520graph%252C%250Aintegrates%2520role%2520embeddings%2520and%2520models%2520the%2520interactions%2520between%2520debate%2520roles%2520and%250Aarguments%2520using%2520an%2520attention%2520mechanism%252C%2520providing%2520the%2520final%2520judgment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Truth%20Becomes%20Clearer%20Through%20Debate%21%20Multi-Agent%20Systems%20with%20Large%0A%20%20Language%20Models%20Unmask%20Fake%20News&entry.906535625=Yuhan%20Liu%20and%20Yuxuan%20Liu%20and%20Xiaoqing%20Zhang%20and%20Xiuying%20Chen%20and%20Rui%20Yan&entry.1292438233=%20%20In%20today%27s%20digital%20environment%2C%20the%20rapid%20propagation%20of%20fake%20news%20via%20social%0Anetworks%20poses%20significant%20social%20challenges.%20Most%20existing%20detection%20methods%0Aeither%20employ%20traditional%20classification%20models%2C%20which%20suffer%20from%20low%0Ainterpretability%20and%20limited%20generalization%20capabilities%2C%20or%20craft%20specific%0Aprompts%20for%20large%20language%20models%20%28LLMs%29%20to%20produce%20explanations%20and%20results%0Adirectly%2C%20failing%20to%20leverage%20LLMs%27%20reasoning%20abilities%20fully.%20Inspired%20by%20the%0Asaying%20that%20%22truth%20becomes%20clearer%20through%20debate%2C%22%20our%20study%20introduces%20a%0Anovel%20multi-agent%20system%20with%20LLMs%20named%20TruEDebate%20%28TED%29%20to%20enhance%20the%0Ainterpretability%20and%20effectiveness%20of%20fake%20news%20detection.%20TED%20employs%20a%0Arigorous%20debate%20process%20inspired%20by%20formal%20debate%20settings.%20Central%20to%20our%0Aapproach%20are%20two%20innovative%20components%3A%20the%20DebateFlow%20Agents%20and%20the%0AInsightFlow%20Agents.%20The%20DebateFlow%20Agents%20organize%20agents%20into%20two%20teams%2C%20where%0Aone%20supports%20and%20the%20other%20challenges%20the%20truth%20of%20the%20news.%20These%20agents%0Aengage%20in%20opening%20statements%2C%20cross-examination%2C%20rebuttal%2C%20and%20closing%0Astatements%2C%20simulating%20a%20rigorous%20debate%20process%20akin%20to%20human%20discourse%0Aanalysis%2C%20allowing%20for%20a%20thorough%20evaluation%20of%20news%20content.%20Concurrently%2C%20the%0AInsightFlow%20Agents%20consist%20of%20two%20specialized%20sub-agents%3A%20the%20Synthesis%20Agent%0Aand%20the%20Analysis%20Agent.%20The%20Synthesis%20Agent%20summarizes%20the%20debates%20and%20provides%0Aan%20overarching%20viewpoint%2C%20ensuring%20a%20coherent%20and%20comprehensive%20evaluation.%20The%0AAnalysis%20Agent%2C%20which%20includes%20a%20role-aware%20encoder%20and%20a%20debate%20graph%2C%0Aintegrates%20role%20embeddings%20and%20models%20the%20interactions%20between%20debate%20roles%20and%0Aarguments%20using%20an%20attention%20mechanism%2C%20providing%20the%20final%20judgment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08532v1&entry.124074799=Read"},
{"title": "Learning Advanced Self-Attention for Linear Transformers in the Singular\n  Value Domain", "author": "Hyowon Wi and Jeongwhan Choi and Noseong Park", "abstract": "  Transformers have demonstrated remarkable performance across diverse domains.\nThe key component of Transformers is self-attention, which learns the\nrelationship between any two tokens in the input sequence. Recent studies have\nrevealed that the self-attention can be understood as a normalized adjacency\nmatrix of a graph. Notably, from the perspective of graph signal processing\n(GSP), the self-attention can be equivalently defined as a simple graph filter,\napplying GSP using the value vector as the signal. However, the self-attention\nis a graph filter defined with only the first order of the polynomial matrix,\nand acts as a low-pass filter preventing the effective leverage of various\nfrequency information. Consequently, existing self-attention mechanisms are\ndesigned in a rather simplified manner. Therefore, we propose a novel method,\ncalled \\underline{\\textbf{A}}ttentive \\underline{\\textbf{G}}raph\n\\underline{\\textbf{F}}ilter (AGF), interpreting the self-attention as learning\nthe graph filter in the singular value domain from the perspective of graph\nsignal processing for directed graphs with the linear complexity w.r.t. the\ninput length $n$, i.e., $\\mathcal{O}(nd^2)$. In our experiments, we demonstrate\nthat AGF achieves state-of-the-art performance on various tasks, including Long\nRange Arena benchmark and time series classification.\n", "link": "http://arxiv.org/abs/2505.08516v1", "date": "2025-05-13", "relevancy": 2.0306, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5412}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.501}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Advanced%20Self-Attention%20for%20Linear%20Transformers%20in%20the%20Singular%0A%20%20Value%20Domain&body=Title%3A%20Learning%20Advanced%20Self-Attention%20for%20Linear%20Transformers%20in%20the%20Singular%0A%20%20Value%20Domain%0AAuthor%3A%20Hyowon%20Wi%20and%20Jeongwhan%20Choi%20and%20Noseong%20Park%0AAbstract%3A%20%20%20Transformers%20have%20demonstrated%20remarkable%20performance%20across%20diverse%20domains.%0AThe%20key%20component%20of%20Transformers%20is%20self-attention%2C%20which%20learns%20the%0Arelationship%20between%20any%20two%20tokens%20in%20the%20input%20sequence.%20Recent%20studies%20have%0Arevealed%20that%20the%20self-attention%20can%20be%20understood%20as%20a%20normalized%20adjacency%0Amatrix%20of%20a%20graph.%20Notably%2C%20from%20the%20perspective%20of%20graph%20signal%20processing%0A%28GSP%29%2C%20the%20self-attention%20can%20be%20equivalently%20defined%20as%20a%20simple%20graph%20filter%2C%0Aapplying%20GSP%20using%20the%20value%20vector%20as%20the%20signal.%20However%2C%20the%20self-attention%0Ais%20a%20graph%20filter%20defined%20with%20only%20the%20first%20order%20of%20the%20polynomial%20matrix%2C%0Aand%20acts%20as%20a%20low-pass%20filter%20preventing%20the%20effective%20leverage%20of%20various%0Afrequency%20information.%20Consequently%2C%20existing%20self-attention%20mechanisms%20are%0Adesigned%20in%20a%20rather%20simplified%20manner.%20Therefore%2C%20we%20propose%20a%20novel%20method%2C%0Acalled%20%5Cunderline%7B%5Ctextbf%7BA%7D%7Dttentive%20%5Cunderline%7B%5Ctextbf%7BG%7D%7Draph%0A%5Cunderline%7B%5Ctextbf%7BF%7D%7Dilter%20%28AGF%29%2C%20interpreting%20the%20self-attention%20as%20learning%0Athe%20graph%20filter%20in%20the%20singular%20value%20domain%20from%20the%20perspective%20of%20graph%0Asignal%20processing%20for%20directed%20graphs%20with%20the%20linear%20complexity%20w.r.t.%20the%0Ainput%20length%20%24n%24%2C%20i.e.%2C%20%24%5Cmathcal%7BO%7D%28nd%5E2%29%24.%20In%20our%20experiments%2C%20we%20demonstrate%0Athat%20AGF%20achieves%20state-of-the-art%20performance%20on%20various%20tasks%2C%20including%20Long%0ARange%20Arena%20benchmark%20and%20time%20series%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Advanced%2520Self-Attention%2520for%2520Linear%2520Transformers%2520in%2520the%2520Singular%250A%2520%2520Value%2520Domain%26entry.906535625%3DHyowon%2520Wi%2520and%2520Jeongwhan%2520Choi%2520and%2520Noseong%2520Park%26entry.1292438233%3D%2520%2520Transformers%2520have%2520demonstrated%2520remarkable%2520performance%2520across%2520diverse%2520domains.%250AThe%2520key%2520component%2520of%2520Transformers%2520is%2520self-attention%252C%2520which%2520learns%2520the%250Arelationship%2520between%2520any%2520two%2520tokens%2520in%2520the%2520input%2520sequence.%2520Recent%2520studies%2520have%250Arevealed%2520that%2520the%2520self-attention%2520can%2520be%2520understood%2520as%2520a%2520normalized%2520adjacency%250Amatrix%2520of%2520a%2520graph.%2520Notably%252C%2520from%2520the%2520perspective%2520of%2520graph%2520signal%2520processing%250A%2528GSP%2529%252C%2520the%2520self-attention%2520can%2520be%2520equivalently%2520defined%2520as%2520a%2520simple%2520graph%2520filter%252C%250Aapplying%2520GSP%2520using%2520the%2520value%2520vector%2520as%2520the%2520signal.%2520However%252C%2520the%2520self-attention%250Ais%2520a%2520graph%2520filter%2520defined%2520with%2520only%2520the%2520first%2520order%2520of%2520the%2520polynomial%2520matrix%252C%250Aand%2520acts%2520as%2520a%2520low-pass%2520filter%2520preventing%2520the%2520effective%2520leverage%2520of%2520various%250Afrequency%2520information.%2520Consequently%252C%2520existing%2520self-attention%2520mechanisms%2520are%250Adesigned%2520in%2520a%2520rather%2520simplified%2520manner.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520method%252C%250Acalled%2520%255Cunderline%257B%255Ctextbf%257BA%257D%257Dttentive%2520%255Cunderline%257B%255Ctextbf%257BG%257D%257Draph%250A%255Cunderline%257B%255Ctextbf%257BF%257D%257Dilter%2520%2528AGF%2529%252C%2520interpreting%2520the%2520self-attention%2520as%2520learning%250Athe%2520graph%2520filter%2520in%2520the%2520singular%2520value%2520domain%2520from%2520the%2520perspective%2520of%2520graph%250Asignal%2520processing%2520for%2520directed%2520graphs%2520with%2520the%2520linear%2520complexity%2520w.r.t.%2520the%250Ainput%2520length%2520%2524n%2524%252C%2520i.e.%252C%2520%2524%255Cmathcal%257BO%257D%2528nd%255E2%2529%2524.%2520In%2520our%2520experiments%252C%2520we%2520demonstrate%250Athat%2520AGF%2520achieves%2520state-of-the-art%2520performance%2520on%2520various%2520tasks%252C%2520including%2520Long%250ARange%2520Arena%2520benchmark%2520and%2520time%2520series%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Advanced%20Self-Attention%20for%20Linear%20Transformers%20in%20the%20Singular%0A%20%20Value%20Domain&entry.906535625=Hyowon%20Wi%20and%20Jeongwhan%20Choi%20and%20Noseong%20Park&entry.1292438233=%20%20Transformers%20have%20demonstrated%20remarkable%20performance%20across%20diverse%20domains.%0AThe%20key%20component%20of%20Transformers%20is%20self-attention%2C%20which%20learns%20the%0Arelationship%20between%20any%20two%20tokens%20in%20the%20input%20sequence.%20Recent%20studies%20have%0Arevealed%20that%20the%20self-attention%20can%20be%20understood%20as%20a%20normalized%20adjacency%0Amatrix%20of%20a%20graph.%20Notably%2C%20from%20the%20perspective%20of%20graph%20signal%20processing%0A%28GSP%29%2C%20the%20self-attention%20can%20be%20equivalently%20defined%20as%20a%20simple%20graph%20filter%2C%0Aapplying%20GSP%20using%20the%20value%20vector%20as%20the%20signal.%20However%2C%20the%20self-attention%0Ais%20a%20graph%20filter%20defined%20with%20only%20the%20first%20order%20of%20the%20polynomial%20matrix%2C%0Aand%20acts%20as%20a%20low-pass%20filter%20preventing%20the%20effective%20leverage%20of%20various%0Afrequency%20information.%20Consequently%2C%20existing%20self-attention%20mechanisms%20are%0Adesigned%20in%20a%20rather%20simplified%20manner.%20Therefore%2C%20we%20propose%20a%20novel%20method%2C%0Acalled%20%5Cunderline%7B%5Ctextbf%7BA%7D%7Dttentive%20%5Cunderline%7B%5Ctextbf%7BG%7D%7Draph%0A%5Cunderline%7B%5Ctextbf%7BF%7D%7Dilter%20%28AGF%29%2C%20interpreting%20the%20self-attention%20as%20learning%0Athe%20graph%20filter%20in%20the%20singular%20value%20domain%20from%20the%20perspective%20of%20graph%0Asignal%20processing%20for%20directed%20graphs%20with%20the%20linear%20complexity%20w.r.t.%20the%0Ainput%20length%20%24n%24%2C%20i.e.%2C%20%24%5Cmathcal%7BO%7D%28nd%5E2%29%24.%20In%20our%20experiments%2C%20we%20demonstrate%0Athat%20AGF%20achieves%20state-of-the-art%20performance%20on%20various%20tasks%2C%20including%20Long%0ARange%20Arena%20benchmark%20and%20time%20series%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08516v1&entry.124074799=Read"},
{"title": "AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based\n  Physics-Informed Kolmogorov-Arnold Networks", "author": "Hangwei Zhang and Zhimu Huang and Yan Wang", "abstract": "  Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving\npartial differential equations (PDEs). Yet their original formulation is\ncomputationally and memory intensive, motivating the introduction of Chebyshev\nType-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed\nthe vanilla KANs architecture, our rigorous theoretical analysis reveals that\nthey still suffer from rank collapse, ultimately limiting their expressive\ncapacity. To overcome these limitations, we enhance Chebyshev1KANs by\nintegrating wavelet-activated MLPs with learnable parameters and an internal\nattention mechanism. We prove that this design preserves a full-rank Jacobian\nand is capable of approximating solutions to PDEs of arbitrary order.\nFurthermore, to alleviate the loss instability and imbalance introduced by the\nChebyshev polynomial basis, we externally incorporate a Residual Gradient\nAttention (RGA) mechanism that dynamically re-weights individual loss terms\naccording to their gradient norms and residual magnitudes. By jointly\nleveraging internal and external attention, we present AC-PKAN, a novel\narchitecture that constitutes an enhancement to weakly supervised\nPhysics-Informed Neural Networks (PINNs) and extends the expressive power of\nKANs. Experimental results from nine benchmark tasks across three domains show\nthat AC-PKAN consistently outperforms or matches state-of-the-art models such\nas PINNsFormer, establishing it as a highly effective tool for solving complex\nreal-world engineering problems in zero-data or data-sparse regimes. The code\nwill be made publicly available upon acceptance.\n", "link": "http://arxiv.org/abs/2505.08687v1", "date": "2025-05-13", "relevancy": 2.0244, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5139}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5087}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AC-PKAN%3A%20Attention-Enhanced%20and%20Chebyshev%20Polynomial-Based%0A%20%20Physics-Informed%20Kolmogorov-Arnold%20Networks&body=Title%3A%20AC-PKAN%3A%20Attention-Enhanced%20and%20Chebyshev%20Polynomial-Based%0A%20%20Physics-Informed%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Hangwei%20Zhang%20and%20Zhimu%20Huang%20and%20Yan%20Wang%0AAbstract%3A%20%20%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20have%20recently%20shown%20promise%20for%20solving%0Apartial%20differential%20equations%20%28PDEs%29.%20Yet%20their%20original%20formulation%20is%0Acomputationally%20and%20memory%20intensive%2C%20motivating%20the%20introduction%20of%20Chebyshev%0AType-I-based%20KANs%20%28Chebyshev1KANs%29.%20Although%20Chebyshev1KANs%20have%20outperformed%0Athe%20vanilla%20KANs%20architecture%2C%20our%20rigorous%20theoretical%20analysis%20reveals%20that%0Athey%20still%20suffer%20from%20rank%20collapse%2C%20ultimately%20limiting%20their%20expressive%0Acapacity.%20To%20overcome%20these%20limitations%2C%20we%20enhance%20Chebyshev1KANs%20by%0Aintegrating%20wavelet-activated%20MLPs%20with%20learnable%20parameters%20and%20an%20internal%0Aattention%20mechanism.%20We%20prove%20that%20this%20design%20preserves%20a%20full-rank%20Jacobian%0Aand%20is%20capable%20of%20approximating%20solutions%20to%20PDEs%20of%20arbitrary%20order.%0AFurthermore%2C%20to%20alleviate%20the%20loss%20instability%20and%20imbalance%20introduced%20by%20the%0AChebyshev%20polynomial%20basis%2C%20we%20externally%20incorporate%20a%20Residual%20Gradient%0AAttention%20%28RGA%29%20mechanism%20that%20dynamically%20re-weights%20individual%20loss%20terms%0Aaccording%20to%20their%20gradient%20norms%20and%20residual%20magnitudes.%20By%20jointly%0Aleveraging%20internal%20and%20external%20attention%2C%20we%20present%20AC-PKAN%2C%20a%20novel%0Aarchitecture%20that%20constitutes%20an%20enhancement%20to%20weakly%20supervised%0APhysics-Informed%20Neural%20Networks%20%28PINNs%29%20and%20extends%20the%20expressive%20power%20of%0AKANs.%20Experimental%20results%20from%20nine%20benchmark%20tasks%20across%20three%20domains%20show%0Athat%20AC-PKAN%20consistently%20outperforms%20or%20matches%20state-of-the-art%20models%20such%0Aas%20PINNsFormer%2C%20establishing%20it%20as%20a%20highly%20effective%20tool%20for%20solving%20complex%0Areal-world%20engineering%20problems%20in%20zero-data%20or%20data-sparse%20regimes.%20The%20code%0Awill%20be%20made%20publicly%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAC-PKAN%253A%2520Attention-Enhanced%2520and%2520Chebyshev%2520Polynomial-Based%250A%2520%2520Physics-Informed%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DHangwei%2520Zhang%2520and%2520Zhimu%2520Huang%2520and%2520Yan%2520Wang%26entry.1292438233%3D%2520%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520have%2520recently%2520shown%2520promise%2520for%2520solving%250Apartial%2520differential%2520equations%2520%2528PDEs%2529.%2520Yet%2520their%2520original%2520formulation%2520is%250Acomputationally%2520and%2520memory%2520intensive%252C%2520motivating%2520the%2520introduction%2520of%2520Chebyshev%250AType-I-based%2520KANs%2520%2528Chebyshev1KANs%2529.%2520Although%2520Chebyshev1KANs%2520have%2520outperformed%250Athe%2520vanilla%2520KANs%2520architecture%252C%2520our%2520rigorous%2520theoretical%2520analysis%2520reveals%2520that%250Athey%2520still%2520suffer%2520from%2520rank%2520collapse%252C%2520ultimately%2520limiting%2520their%2520expressive%250Acapacity.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520enhance%2520Chebyshev1KANs%2520by%250Aintegrating%2520wavelet-activated%2520MLPs%2520with%2520learnable%2520parameters%2520and%2520an%2520internal%250Aattention%2520mechanism.%2520We%2520prove%2520that%2520this%2520design%2520preserves%2520a%2520full-rank%2520Jacobian%250Aand%2520is%2520capable%2520of%2520approximating%2520solutions%2520to%2520PDEs%2520of%2520arbitrary%2520order.%250AFurthermore%252C%2520to%2520alleviate%2520the%2520loss%2520instability%2520and%2520imbalance%2520introduced%2520by%2520the%250AChebyshev%2520polynomial%2520basis%252C%2520we%2520externally%2520incorporate%2520a%2520Residual%2520Gradient%250AAttention%2520%2528RGA%2529%2520mechanism%2520that%2520dynamically%2520re-weights%2520individual%2520loss%2520terms%250Aaccording%2520to%2520their%2520gradient%2520norms%2520and%2520residual%2520magnitudes.%2520By%2520jointly%250Aleveraging%2520internal%2520and%2520external%2520attention%252C%2520we%2520present%2520AC-PKAN%252C%2520a%2520novel%250Aarchitecture%2520that%2520constitutes%2520an%2520enhancement%2520to%2520weakly%2520supervised%250APhysics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520and%2520extends%2520the%2520expressive%2520power%2520of%250AKANs.%2520Experimental%2520results%2520from%2520nine%2520benchmark%2520tasks%2520across%2520three%2520domains%2520show%250Athat%2520AC-PKAN%2520consistently%2520outperforms%2520or%2520matches%2520state-of-the-art%2520models%2520such%250Aas%2520PINNsFormer%252C%2520establishing%2520it%2520as%2520a%2520highly%2520effective%2520tool%2520for%2520solving%2520complex%250Areal-world%2520engineering%2520problems%2520in%2520zero-data%2520or%2520data-sparse%2520regimes.%2520The%2520code%250Awill%2520be%2520made%2520publicly%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AC-PKAN%3A%20Attention-Enhanced%20and%20Chebyshev%20Polynomial-Based%0A%20%20Physics-Informed%20Kolmogorov-Arnold%20Networks&entry.906535625=Hangwei%20Zhang%20and%20Zhimu%20Huang%20and%20Yan%20Wang&entry.1292438233=%20%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20have%20recently%20shown%20promise%20for%20solving%0Apartial%20differential%20equations%20%28PDEs%29.%20Yet%20their%20original%20formulation%20is%0Acomputationally%20and%20memory%20intensive%2C%20motivating%20the%20introduction%20of%20Chebyshev%0AType-I-based%20KANs%20%28Chebyshev1KANs%29.%20Although%20Chebyshev1KANs%20have%20outperformed%0Athe%20vanilla%20KANs%20architecture%2C%20our%20rigorous%20theoretical%20analysis%20reveals%20that%0Athey%20still%20suffer%20from%20rank%20collapse%2C%20ultimately%20limiting%20their%20expressive%0Acapacity.%20To%20overcome%20these%20limitations%2C%20we%20enhance%20Chebyshev1KANs%20by%0Aintegrating%20wavelet-activated%20MLPs%20with%20learnable%20parameters%20and%20an%20internal%0Aattention%20mechanism.%20We%20prove%20that%20this%20design%20preserves%20a%20full-rank%20Jacobian%0Aand%20is%20capable%20of%20approximating%20solutions%20to%20PDEs%20of%20arbitrary%20order.%0AFurthermore%2C%20to%20alleviate%20the%20loss%20instability%20and%20imbalance%20introduced%20by%20the%0AChebyshev%20polynomial%20basis%2C%20we%20externally%20incorporate%20a%20Residual%20Gradient%0AAttention%20%28RGA%29%20mechanism%20that%20dynamically%20re-weights%20individual%20loss%20terms%0Aaccording%20to%20their%20gradient%20norms%20and%20residual%20magnitudes.%20By%20jointly%0Aleveraging%20internal%20and%20external%20attention%2C%20we%20present%20AC-PKAN%2C%20a%20novel%0Aarchitecture%20that%20constitutes%20an%20enhancement%20to%20weakly%20supervised%0APhysics-Informed%20Neural%20Networks%20%28PINNs%29%20and%20extends%20the%20expressive%20power%20of%0AKANs.%20Experimental%20results%20from%20nine%20benchmark%20tasks%20across%20three%20domains%20show%0Athat%20AC-PKAN%20consistently%20outperforms%20or%20matches%20state-of-the-art%20models%20such%0Aas%20PINNsFormer%2C%20establishing%20it%20as%20a%20highly%20effective%20tool%20for%20solving%20complex%0Areal-world%20engineering%20problems%20in%20zero-data%20or%20data-sparse%20regimes.%20The%20code%0Awill%20be%20made%20publicly%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08687v1&entry.124074799=Read"},
{"title": "Joint Metric Space Embedding by Unbalanced OT with Gromov-Wasserstein\n  Marginal Penalization", "author": "Florian Beier and Moritz Piening and Robert Beinert and Gabriele Steidl", "abstract": "  We propose a new approach for unsupervised alignment of heterogeneous\ndatasets, which maps data from two different domains without any known\ncorrespondences to a common metric space. Our method is based on an unbalanced\noptimal transport problem with Gromov-Wasserstein marginal penalization. It can\nbe seen as a counterpart to the recently introduced joint multidimensional\nscaling method. We prove that there exists a minimizer of our functional and\nthat for penalization parameters going to infinity, the corresponding sequence\nof minimizers converges to a minimizer of the so-called embedded Wasserstein\ndistance. Our model can be reformulated as a quadratic, multi-marginal,\nunbalanced optimal transport problem, for which a bi-convex relaxation admits a\nnumerical solver via block-coordinate descent. We provide numerical examples\nfor joint embeddings in Euclidean as well as non-Euclidean spaces.\n", "link": "http://arxiv.org/abs/2502.07510v2", "date": "2025-05-13", "relevancy": 2.0212, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5129}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5018}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Metric%20Space%20Embedding%20by%20Unbalanced%20OT%20with%20Gromov-Wasserstein%0A%20%20Marginal%20Penalization&body=Title%3A%20Joint%20Metric%20Space%20Embedding%20by%20Unbalanced%20OT%20with%20Gromov-Wasserstein%0A%20%20Marginal%20Penalization%0AAuthor%3A%20Florian%20Beier%20and%20Moritz%20Piening%20and%20Robert%20Beinert%20and%20Gabriele%20Steidl%0AAbstract%3A%20%20%20We%20propose%20a%20new%20approach%20for%20unsupervised%20alignment%20of%20heterogeneous%0Adatasets%2C%20which%20maps%20data%20from%20two%20different%20domains%20without%20any%20known%0Acorrespondences%20to%20a%20common%20metric%20space.%20Our%20method%20is%20based%20on%20an%20unbalanced%0Aoptimal%20transport%20problem%20with%20Gromov-Wasserstein%20marginal%20penalization.%20It%20can%0Abe%20seen%20as%20a%20counterpart%20to%20the%20recently%20introduced%20joint%20multidimensional%0Ascaling%20method.%20We%20prove%20that%20there%20exists%20a%20minimizer%20of%20our%20functional%20and%0Athat%20for%20penalization%20parameters%20going%20to%20infinity%2C%20the%20corresponding%20sequence%0Aof%20minimizers%20converges%20to%20a%20minimizer%20of%20the%20so-called%20embedded%20Wasserstein%0Adistance.%20Our%20model%20can%20be%20reformulated%20as%20a%20quadratic%2C%20multi-marginal%2C%0Aunbalanced%20optimal%20transport%20problem%2C%20for%20which%20a%20bi-convex%20relaxation%20admits%20a%0Anumerical%20solver%20via%20block-coordinate%20descent.%20We%20provide%20numerical%20examples%0Afor%20joint%20embeddings%20in%20Euclidean%20as%20well%20as%20non-Euclidean%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07510v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Metric%2520Space%2520Embedding%2520by%2520Unbalanced%2520OT%2520with%2520Gromov-Wasserstein%250A%2520%2520Marginal%2520Penalization%26entry.906535625%3DFlorian%2520Beier%2520and%2520Moritz%2520Piening%2520and%2520Robert%2520Beinert%2520and%2520Gabriele%2520Steidl%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520approach%2520for%2520unsupervised%2520alignment%2520of%2520heterogeneous%250Adatasets%252C%2520which%2520maps%2520data%2520from%2520two%2520different%2520domains%2520without%2520any%2520known%250Acorrespondences%2520to%2520a%2520common%2520metric%2520space.%2520Our%2520method%2520is%2520based%2520on%2520an%2520unbalanced%250Aoptimal%2520transport%2520problem%2520with%2520Gromov-Wasserstein%2520marginal%2520penalization.%2520It%2520can%250Abe%2520seen%2520as%2520a%2520counterpart%2520to%2520the%2520recently%2520introduced%2520joint%2520multidimensional%250Ascaling%2520method.%2520We%2520prove%2520that%2520there%2520exists%2520a%2520minimizer%2520of%2520our%2520functional%2520and%250Athat%2520for%2520penalization%2520parameters%2520going%2520to%2520infinity%252C%2520the%2520corresponding%2520sequence%250Aof%2520minimizers%2520converges%2520to%2520a%2520minimizer%2520of%2520the%2520so-called%2520embedded%2520Wasserstein%250Adistance.%2520Our%2520model%2520can%2520be%2520reformulated%2520as%2520a%2520quadratic%252C%2520multi-marginal%252C%250Aunbalanced%2520optimal%2520transport%2520problem%252C%2520for%2520which%2520a%2520bi-convex%2520relaxation%2520admits%2520a%250Anumerical%2520solver%2520via%2520block-coordinate%2520descent.%2520We%2520provide%2520numerical%2520examples%250Afor%2520joint%2520embeddings%2520in%2520Euclidean%2520as%2520well%2520as%2520non-Euclidean%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07510v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Metric%20Space%20Embedding%20by%20Unbalanced%20OT%20with%20Gromov-Wasserstein%0A%20%20Marginal%20Penalization&entry.906535625=Florian%20Beier%20and%20Moritz%20Piening%20and%20Robert%20Beinert%20and%20Gabriele%20Steidl&entry.1292438233=%20%20We%20propose%20a%20new%20approach%20for%20unsupervised%20alignment%20of%20heterogeneous%0Adatasets%2C%20which%20maps%20data%20from%20two%20different%20domains%20without%20any%20known%0Acorrespondences%20to%20a%20common%20metric%20space.%20Our%20method%20is%20based%20on%20an%20unbalanced%0Aoptimal%20transport%20problem%20with%20Gromov-Wasserstein%20marginal%20penalization.%20It%20can%0Abe%20seen%20as%20a%20counterpart%20to%20the%20recently%20introduced%20joint%20multidimensional%0Ascaling%20method.%20We%20prove%20that%20there%20exists%20a%20minimizer%20of%20our%20functional%20and%0Athat%20for%20penalization%20parameters%20going%20to%20infinity%2C%20the%20corresponding%20sequence%0Aof%20minimizers%20converges%20to%20a%20minimizer%20of%20the%20so-called%20embedded%20Wasserstein%0Adistance.%20Our%20model%20can%20be%20reformulated%20as%20a%20quadratic%2C%20multi-marginal%2C%0Aunbalanced%20optimal%20transport%20problem%2C%20for%20which%20a%20bi-convex%20relaxation%20admits%20a%0Anumerical%20solver%20via%20block-coordinate%20descent.%20We%20provide%20numerical%20examples%0Afor%20joint%20embeddings%20in%20Euclidean%20as%20well%20as%20non-Euclidean%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07510v2&entry.124074799=Read"},
{"title": "Optimal Trajectory Planning with Collision Avoidance for Autonomous\n  Vehicle Maneuvering", "author": "Jason Zalev", "abstract": "  To perform autonomous driving maneuvers, such as parallel or perpendicular\nparking, a vehicle requires continual speed and steering adjustments to follow\na generated path. In consequence, the path's quality is a limiting factor of\nthe vehicle maneuver's performance. While most path planning approaches include\nfinding a collision-free route, optimal trajectory planning involves solving\nthe best transition from initial to final states, minimizing the action over\nall paths permitted by a kinematic model. Here we propose a novel method based\non sequential convex optimization, which permits flexible and efficient optimal\ntrajectory generation. The objective is to achieve the fastest time, shortest\ndistance, and fewest number of path segments to satisfy motion requirements,\nwhile avoiding sensor blind-spots. In our approach, vehicle kinematics are\nrepresented by a discretized Dubins model. To avoid collisions, each waypoint\nis constrained by linear inequalities representing closest distance of\nobstacles to a polygon specifying the vehicle's extent. To promote smooth and\nvalid trajectories, the solved kinematic state and control variables are\nconstrained and regularized by penalty terms in the model's cost function,\nwhich enforces physical restrictions including limits for steering angle,\nacceleration and speed. In this paper, we analyze trajectories obtained for\nseveral parking scenarios. Results demonstrate efficient and collision-free\nmotion generated by the proposed technique.\n", "link": "http://arxiv.org/abs/2505.08724v1", "date": "2025-05-13", "relevancy": 2.0171, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5214}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5059}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Trajectory%20Planning%20with%20Collision%20Avoidance%20for%20Autonomous%0A%20%20Vehicle%20Maneuvering&body=Title%3A%20Optimal%20Trajectory%20Planning%20with%20Collision%20Avoidance%20for%20Autonomous%0A%20%20Vehicle%20Maneuvering%0AAuthor%3A%20Jason%20Zalev%0AAbstract%3A%20%20%20To%20perform%20autonomous%20driving%20maneuvers%2C%20such%20as%20parallel%20or%20perpendicular%0Aparking%2C%20a%20vehicle%20requires%20continual%20speed%20and%20steering%20adjustments%20to%20follow%0Aa%20generated%20path.%20In%20consequence%2C%20the%20path%27s%20quality%20is%20a%20limiting%20factor%20of%0Athe%20vehicle%20maneuver%27s%20performance.%20While%20most%20path%20planning%20approaches%20include%0Afinding%20a%20collision-free%20route%2C%20optimal%20trajectory%20planning%20involves%20solving%0Athe%20best%20transition%20from%20initial%20to%20final%20states%2C%20minimizing%20the%20action%20over%0Aall%20paths%20permitted%20by%20a%20kinematic%20model.%20Here%20we%20propose%20a%20novel%20method%20based%0Aon%20sequential%20convex%20optimization%2C%20which%20permits%20flexible%20and%20efficient%20optimal%0Atrajectory%20generation.%20The%20objective%20is%20to%20achieve%20the%20fastest%20time%2C%20shortest%0Adistance%2C%20and%20fewest%20number%20of%20path%20segments%20to%20satisfy%20motion%20requirements%2C%0Awhile%20avoiding%20sensor%20blind-spots.%20In%20our%20approach%2C%20vehicle%20kinematics%20are%0Arepresented%20by%20a%20discretized%20Dubins%20model.%20To%20avoid%20collisions%2C%20each%20waypoint%0Ais%20constrained%20by%20linear%20inequalities%20representing%20closest%20distance%20of%0Aobstacles%20to%20a%20polygon%20specifying%20the%20vehicle%27s%20extent.%20To%20promote%20smooth%20and%0Avalid%20trajectories%2C%20the%20solved%20kinematic%20state%20and%20control%20variables%20are%0Aconstrained%20and%20regularized%20by%20penalty%20terms%20in%20the%20model%27s%20cost%20function%2C%0Awhich%20enforces%20physical%20restrictions%20including%20limits%20for%20steering%20angle%2C%0Aacceleration%20and%20speed.%20In%20this%20paper%2C%20we%20analyze%20trajectories%20obtained%20for%0Aseveral%20parking%20scenarios.%20Results%20demonstrate%20efficient%20and%20collision-free%0Amotion%20generated%20by%20the%20proposed%20technique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Trajectory%2520Planning%2520with%2520Collision%2520Avoidance%2520for%2520Autonomous%250A%2520%2520Vehicle%2520Maneuvering%26entry.906535625%3DJason%2520Zalev%26entry.1292438233%3D%2520%2520To%2520perform%2520autonomous%2520driving%2520maneuvers%252C%2520such%2520as%2520parallel%2520or%2520perpendicular%250Aparking%252C%2520a%2520vehicle%2520requires%2520continual%2520speed%2520and%2520steering%2520adjustments%2520to%2520follow%250Aa%2520generated%2520path.%2520In%2520consequence%252C%2520the%2520path%2527s%2520quality%2520is%2520a%2520limiting%2520factor%2520of%250Athe%2520vehicle%2520maneuver%2527s%2520performance.%2520While%2520most%2520path%2520planning%2520approaches%2520include%250Afinding%2520a%2520collision-free%2520route%252C%2520optimal%2520trajectory%2520planning%2520involves%2520solving%250Athe%2520best%2520transition%2520from%2520initial%2520to%2520final%2520states%252C%2520minimizing%2520the%2520action%2520over%250Aall%2520paths%2520permitted%2520by%2520a%2520kinematic%2520model.%2520Here%2520we%2520propose%2520a%2520novel%2520method%2520based%250Aon%2520sequential%2520convex%2520optimization%252C%2520which%2520permits%2520flexible%2520and%2520efficient%2520optimal%250Atrajectory%2520generation.%2520The%2520objective%2520is%2520to%2520achieve%2520the%2520fastest%2520time%252C%2520shortest%250Adistance%252C%2520and%2520fewest%2520number%2520of%2520path%2520segments%2520to%2520satisfy%2520motion%2520requirements%252C%250Awhile%2520avoiding%2520sensor%2520blind-spots.%2520In%2520our%2520approach%252C%2520vehicle%2520kinematics%2520are%250Arepresented%2520by%2520a%2520discretized%2520Dubins%2520model.%2520To%2520avoid%2520collisions%252C%2520each%2520waypoint%250Ais%2520constrained%2520by%2520linear%2520inequalities%2520representing%2520closest%2520distance%2520of%250Aobstacles%2520to%2520a%2520polygon%2520specifying%2520the%2520vehicle%2527s%2520extent.%2520To%2520promote%2520smooth%2520and%250Avalid%2520trajectories%252C%2520the%2520solved%2520kinematic%2520state%2520and%2520control%2520variables%2520are%250Aconstrained%2520and%2520regularized%2520by%2520penalty%2520terms%2520in%2520the%2520model%2527s%2520cost%2520function%252C%250Awhich%2520enforces%2520physical%2520restrictions%2520including%2520limits%2520for%2520steering%2520angle%252C%250Aacceleration%2520and%2520speed.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520trajectories%2520obtained%2520for%250Aseveral%2520parking%2520scenarios.%2520Results%2520demonstrate%2520efficient%2520and%2520collision-free%250Amotion%2520generated%2520by%2520the%2520proposed%2520technique.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Trajectory%20Planning%20with%20Collision%20Avoidance%20for%20Autonomous%0A%20%20Vehicle%20Maneuvering&entry.906535625=Jason%20Zalev&entry.1292438233=%20%20To%20perform%20autonomous%20driving%20maneuvers%2C%20such%20as%20parallel%20or%20perpendicular%0Aparking%2C%20a%20vehicle%20requires%20continual%20speed%20and%20steering%20adjustments%20to%20follow%0Aa%20generated%20path.%20In%20consequence%2C%20the%20path%27s%20quality%20is%20a%20limiting%20factor%20of%0Athe%20vehicle%20maneuver%27s%20performance.%20While%20most%20path%20planning%20approaches%20include%0Afinding%20a%20collision-free%20route%2C%20optimal%20trajectory%20planning%20involves%20solving%0Athe%20best%20transition%20from%20initial%20to%20final%20states%2C%20minimizing%20the%20action%20over%0Aall%20paths%20permitted%20by%20a%20kinematic%20model.%20Here%20we%20propose%20a%20novel%20method%20based%0Aon%20sequential%20convex%20optimization%2C%20which%20permits%20flexible%20and%20efficient%20optimal%0Atrajectory%20generation.%20The%20objective%20is%20to%20achieve%20the%20fastest%20time%2C%20shortest%0Adistance%2C%20and%20fewest%20number%20of%20path%20segments%20to%20satisfy%20motion%20requirements%2C%0Awhile%20avoiding%20sensor%20blind-spots.%20In%20our%20approach%2C%20vehicle%20kinematics%20are%0Arepresented%20by%20a%20discretized%20Dubins%20model.%20To%20avoid%20collisions%2C%20each%20waypoint%0Ais%20constrained%20by%20linear%20inequalities%20representing%20closest%20distance%20of%0Aobstacles%20to%20a%20polygon%20specifying%20the%20vehicle%27s%20extent.%20To%20promote%20smooth%20and%0Avalid%20trajectories%2C%20the%20solved%20kinematic%20state%20and%20control%20variables%20are%0Aconstrained%20and%20regularized%20by%20penalty%20terms%20in%20the%20model%27s%20cost%20function%2C%0Awhich%20enforces%20physical%20restrictions%20including%20limits%20for%20steering%20angle%2C%0Aacceleration%20and%20speed.%20In%20this%20paper%2C%20we%20analyze%20trajectories%20obtained%20for%0Aseveral%20parking%20scenarios.%20Results%20demonstrate%20efficient%20and%20collision-free%0Amotion%20generated%20by%20the%20proposed%20technique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08724v1&entry.124074799=Read"},
{"title": "Round and Round We Go! What makes Rotary Positional Encodings useful?", "author": "Federico Barbero and Alex Vitvitskyi and Christos Perivolaropoulos and Razvan Pascanu and Petar Veli\u010dkovi\u0107", "abstract": "  Positional Encodings (PEs) are a critical component of Transformer-based\nLarge Language Models (LLMs), providing the attention mechanism with important\nsequence-position information. One of the most popular types of encoding used\ntoday in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries\nand keys based on their relative distance. A common belief is that RoPE is\nuseful because it helps to decay token dependency as relative distance\nincreases. In this work, we argue that this is unlikely to be the core reason.\nWe study the internals of a trained Gemma 7B model to understand how RoPE is\nbeing used at a mechanical level. We find that Gemma learns to use RoPE to\nconstruct robust \"positional\" attention patterns by exploiting the highest\nfrequencies. We also find that, in general, Gemma greatly prefers to use the\nlowest frequencies of RoPE, which we suspect are used to carry semantic\ninformation. We mathematically prove interesting behaviours of RoPE and conduct\nexperiments to verify our findings, proposing a modification of RoPE that fixes\nsome highlighted issues and improves performance. We believe that this work\nrepresents an interesting step in better understanding PEs in LLMs, which we\nbelieve holds crucial value for scaling LLMs to large sizes and context\nlengths.\n", "link": "http://arxiv.org/abs/2410.06205v3", "date": "2025-05-13", "relevancy": 2.0144, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5077}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5077}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Round%20and%20Round%20We%20Go%21%20What%20makes%20Rotary%20Positional%20Encodings%20useful%3F&body=Title%3A%20Round%20and%20Round%20We%20Go%21%20What%20makes%20Rotary%20Positional%20Encodings%20useful%3F%0AAuthor%3A%20Federico%20Barbero%20and%20Alex%20Vitvitskyi%20and%20Christos%20Perivolaropoulos%20and%20Razvan%20Pascanu%20and%20Petar%20Veli%C4%8Dkovi%C4%87%0AAbstract%3A%20%20%20Positional%20Encodings%20%28PEs%29%20are%20a%20critical%20component%20of%20Transformer-based%0ALarge%20Language%20Models%20%28LLMs%29%2C%20providing%20the%20attention%20mechanism%20with%20important%0Asequence-position%20information.%20One%20of%20the%20most%20popular%20types%20of%20encoding%20used%0Atoday%20in%20LLMs%20are%20Rotary%20Positional%20Encodings%20%28RoPE%29%2C%20that%20rotate%20the%20queries%0Aand%20keys%20based%20on%20their%20relative%20distance.%20A%20common%20belief%20is%20that%20RoPE%20is%0Auseful%20because%20it%20helps%20to%20decay%20token%20dependency%20as%20relative%20distance%0Aincreases.%20In%20this%20work%2C%20we%20argue%20that%20this%20is%20unlikely%20to%20be%20the%20core%20reason.%0AWe%20study%20the%20internals%20of%20a%20trained%20Gemma%207B%20model%20to%20understand%20how%20RoPE%20is%0Abeing%20used%20at%20a%20mechanical%20level.%20We%20find%20that%20Gemma%20learns%20to%20use%20RoPE%20to%0Aconstruct%20robust%20%22positional%22%20attention%20patterns%20by%20exploiting%20the%20highest%0Afrequencies.%20We%20also%20find%20that%2C%20in%20general%2C%20Gemma%20greatly%20prefers%20to%20use%20the%0Alowest%20frequencies%20of%20RoPE%2C%20which%20we%20suspect%20are%20used%20to%20carry%20semantic%0Ainformation.%20We%20mathematically%20prove%20interesting%20behaviours%20of%20RoPE%20and%20conduct%0Aexperiments%20to%20verify%20our%20findings%2C%20proposing%20a%20modification%20of%20RoPE%20that%20fixes%0Asome%20highlighted%20issues%20and%20improves%20performance.%20We%20believe%20that%20this%20work%0Arepresents%20an%20interesting%20step%20in%20better%20understanding%20PEs%20in%20LLMs%2C%20which%20we%0Abelieve%20holds%20crucial%20value%20for%20scaling%20LLMs%20to%20large%20sizes%20and%20context%0Alengths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06205v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRound%2520and%2520Round%2520We%2520Go%2521%2520What%2520makes%2520Rotary%2520Positional%2520Encodings%2520useful%253F%26entry.906535625%3DFederico%2520Barbero%2520and%2520Alex%2520Vitvitskyi%2520and%2520Christos%2520Perivolaropoulos%2520and%2520Razvan%2520Pascanu%2520and%2520Petar%2520Veli%25C4%258Dkovi%25C4%2587%26entry.1292438233%3D%2520%2520Positional%2520Encodings%2520%2528PEs%2529%2520are%2520a%2520critical%2520component%2520of%2520Transformer-based%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520providing%2520the%2520attention%2520mechanism%2520with%2520important%250Asequence-position%2520information.%2520One%2520of%2520the%2520most%2520popular%2520types%2520of%2520encoding%2520used%250Atoday%2520in%2520LLMs%2520are%2520Rotary%2520Positional%2520Encodings%2520%2528RoPE%2529%252C%2520that%2520rotate%2520the%2520queries%250Aand%2520keys%2520based%2520on%2520their%2520relative%2520distance.%2520A%2520common%2520belief%2520is%2520that%2520RoPE%2520is%250Auseful%2520because%2520it%2520helps%2520to%2520decay%2520token%2520dependency%2520as%2520relative%2520distance%250Aincreases.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520this%2520is%2520unlikely%2520to%2520be%2520the%2520core%2520reason.%250AWe%2520study%2520the%2520internals%2520of%2520a%2520trained%2520Gemma%25207B%2520model%2520to%2520understand%2520how%2520RoPE%2520is%250Abeing%2520used%2520at%2520a%2520mechanical%2520level.%2520We%2520find%2520that%2520Gemma%2520learns%2520to%2520use%2520RoPE%2520to%250Aconstruct%2520robust%2520%2522positional%2522%2520attention%2520patterns%2520by%2520exploiting%2520the%2520highest%250Afrequencies.%2520We%2520also%2520find%2520that%252C%2520in%2520general%252C%2520Gemma%2520greatly%2520prefers%2520to%2520use%2520the%250Alowest%2520frequencies%2520of%2520RoPE%252C%2520which%2520we%2520suspect%2520are%2520used%2520to%2520carry%2520semantic%250Ainformation.%2520We%2520mathematically%2520prove%2520interesting%2520behaviours%2520of%2520RoPE%2520and%2520conduct%250Aexperiments%2520to%2520verify%2520our%2520findings%252C%2520proposing%2520a%2520modification%2520of%2520RoPE%2520that%2520fixes%250Asome%2520highlighted%2520issues%2520and%2520improves%2520performance.%2520We%2520believe%2520that%2520this%2520work%250Arepresents%2520an%2520interesting%2520step%2520in%2520better%2520understanding%2520PEs%2520in%2520LLMs%252C%2520which%2520we%250Abelieve%2520holds%2520crucial%2520value%2520for%2520scaling%2520LLMs%2520to%2520large%2520sizes%2520and%2520context%250Alengths.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06205v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Round%20and%20Round%20We%20Go%21%20What%20makes%20Rotary%20Positional%20Encodings%20useful%3F&entry.906535625=Federico%20Barbero%20and%20Alex%20Vitvitskyi%20and%20Christos%20Perivolaropoulos%20and%20Razvan%20Pascanu%20and%20Petar%20Veli%C4%8Dkovi%C4%87&entry.1292438233=%20%20Positional%20Encodings%20%28PEs%29%20are%20a%20critical%20component%20of%20Transformer-based%0ALarge%20Language%20Models%20%28LLMs%29%2C%20providing%20the%20attention%20mechanism%20with%20important%0Asequence-position%20information.%20One%20of%20the%20most%20popular%20types%20of%20encoding%20used%0Atoday%20in%20LLMs%20are%20Rotary%20Positional%20Encodings%20%28RoPE%29%2C%20that%20rotate%20the%20queries%0Aand%20keys%20based%20on%20their%20relative%20distance.%20A%20common%20belief%20is%20that%20RoPE%20is%0Auseful%20because%20it%20helps%20to%20decay%20token%20dependency%20as%20relative%20distance%0Aincreases.%20In%20this%20work%2C%20we%20argue%20that%20this%20is%20unlikely%20to%20be%20the%20core%20reason.%0AWe%20study%20the%20internals%20of%20a%20trained%20Gemma%207B%20model%20to%20understand%20how%20RoPE%20is%0Abeing%20used%20at%20a%20mechanical%20level.%20We%20find%20that%20Gemma%20learns%20to%20use%20RoPE%20to%0Aconstruct%20robust%20%22positional%22%20attention%20patterns%20by%20exploiting%20the%20highest%0Afrequencies.%20We%20also%20find%20that%2C%20in%20general%2C%20Gemma%20greatly%20prefers%20to%20use%20the%0Alowest%20frequencies%20of%20RoPE%2C%20which%20we%20suspect%20are%20used%20to%20carry%20semantic%0Ainformation.%20We%20mathematically%20prove%20interesting%20behaviours%20of%20RoPE%20and%20conduct%0Aexperiments%20to%20verify%20our%20findings%2C%20proposing%20a%20modification%20of%20RoPE%20that%20fixes%0Asome%20highlighted%20issues%20and%20improves%20performance.%20We%20believe%20that%20this%20work%0Arepresents%20an%20interesting%20step%20in%20better%20understanding%20PEs%20in%20LLMs%2C%20which%20we%0Abelieve%20holds%20crucial%20value%20for%20scaling%20LLMs%20to%20large%20sizes%20and%20context%0Alengths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06205v3&entry.124074799=Read"},
{"title": "Strategy-Augmented Planning for Large Language Models via Opponent\n  Exploitation", "author": "Shuai Xu and Sijia Cui and Yanna Wang and Bo Xu and Qi Wang", "abstract": "  Efficiently modeling and exploiting opponents is a long-standing challenge in\nadversarial domains. Large Language Models (LLMs) trained on extensive textual\ndata have recently demonstrated outstanding performance in general tasks,\nintroducing new research directions for opponent modeling. Some studies\nprimarily focus on directly using LLMs to generate decisions based on the\nelaborate prompt context that incorporates opponent descriptions, while these\napproaches are limited to scenarios where LLMs possess adequate domain\nexpertise. To address that, we introduce a two-stage Strategy-Augmented\nPlanning (SAP) framework that significantly enhances the opponent exploitation\ncapabilities of LLM-based agents by utilizing a critical component, the\nStrategy Evaluation Network (SEN). Specifically, in the offline stage, we\nconstruct an explicit strategy space and subsequently collect strategy-outcome\npair data for training the SEN network. During the online phase, SAP\ndynamically recognizes the opponent's strategies and greedily exploits them by\nsearching best response strategy on the well-trained SEN, finally translating\nstrategy to a course of actions by carefully designed prompts. Experimental\nresults show that SAP exhibits robust generalization capabilities, allowing it\nto perform effectively not only against previously encountered opponent\nstrategies but also against novel, unseen strategies. In the MicroRTS\nenvironment, SAP achieves a 85.35\\% performance improvement over baseline\nmethods and matches the competitiveness of reinforcement learning approaches\nagainst state-of-the-art (SOTA) rule-based AI.\n", "link": "http://arxiv.org/abs/2505.08459v1", "date": "2025-05-13", "relevancy": 2.0137, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5408}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strategy-Augmented%20Planning%20for%20Large%20Language%20Models%20via%20Opponent%0A%20%20Exploitation&body=Title%3A%20Strategy-Augmented%20Planning%20for%20Large%20Language%20Models%20via%20Opponent%0A%20%20Exploitation%0AAuthor%3A%20Shuai%20Xu%20and%20Sijia%20Cui%20and%20Yanna%20Wang%20and%20Bo%20Xu%20and%20Qi%20Wang%0AAbstract%3A%20%20%20Efficiently%20modeling%20and%20exploiting%20opponents%20is%20a%20long-standing%20challenge%20in%0Aadversarial%20domains.%20Large%20Language%20Models%20%28LLMs%29%20trained%20on%20extensive%20textual%0Adata%20have%20recently%20demonstrated%20outstanding%20performance%20in%20general%20tasks%2C%0Aintroducing%20new%20research%20directions%20for%20opponent%20modeling.%20Some%20studies%0Aprimarily%20focus%20on%20directly%20using%20LLMs%20to%20generate%20decisions%20based%20on%20the%0Aelaborate%20prompt%20context%20that%20incorporates%20opponent%20descriptions%2C%20while%20these%0Aapproaches%20are%20limited%20to%20scenarios%20where%20LLMs%20possess%20adequate%20domain%0Aexpertise.%20To%20address%20that%2C%20we%20introduce%20a%20two-stage%20Strategy-Augmented%0APlanning%20%28SAP%29%20framework%20that%20significantly%20enhances%20the%20opponent%20exploitation%0Acapabilities%20of%20LLM-based%20agents%20by%20utilizing%20a%20critical%20component%2C%20the%0AStrategy%20Evaluation%20Network%20%28SEN%29.%20Specifically%2C%20in%20the%20offline%20stage%2C%20we%0Aconstruct%20an%20explicit%20strategy%20space%20and%20subsequently%20collect%20strategy-outcome%0Apair%20data%20for%20training%20the%20SEN%20network.%20During%20the%20online%20phase%2C%20SAP%0Adynamically%20recognizes%20the%20opponent%27s%20strategies%20and%20greedily%20exploits%20them%20by%0Asearching%20best%20response%20strategy%20on%20the%20well-trained%20SEN%2C%20finally%20translating%0Astrategy%20to%20a%20course%20of%20actions%20by%20carefully%20designed%20prompts.%20Experimental%0Aresults%20show%20that%20SAP%20exhibits%20robust%20generalization%20capabilities%2C%20allowing%20it%0Ato%20perform%20effectively%20not%20only%20against%20previously%20encountered%20opponent%0Astrategies%20but%20also%20against%20novel%2C%20unseen%20strategies.%20In%20the%20MicroRTS%0Aenvironment%2C%20SAP%20achieves%20a%2085.35%5C%25%20performance%20improvement%20over%20baseline%0Amethods%20and%20matches%20the%20competitiveness%20of%20reinforcement%20learning%20approaches%0Aagainst%20state-of-the-art%20%28SOTA%29%20rule-based%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrategy-Augmented%2520Planning%2520for%2520Large%2520Language%2520Models%2520via%2520Opponent%250A%2520%2520Exploitation%26entry.906535625%3DShuai%2520Xu%2520and%2520Sijia%2520Cui%2520and%2520Yanna%2520Wang%2520and%2520Bo%2520Xu%2520and%2520Qi%2520Wang%26entry.1292438233%3D%2520%2520Efficiently%2520modeling%2520and%2520exploiting%2520opponents%2520is%2520a%2520long-standing%2520challenge%2520in%250Aadversarial%2520domains.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520trained%2520on%2520extensive%2520textual%250Adata%2520have%2520recently%2520demonstrated%2520outstanding%2520performance%2520in%2520general%2520tasks%252C%250Aintroducing%2520new%2520research%2520directions%2520for%2520opponent%2520modeling.%2520Some%2520studies%250Aprimarily%2520focus%2520on%2520directly%2520using%2520LLMs%2520to%2520generate%2520decisions%2520based%2520on%2520the%250Aelaborate%2520prompt%2520context%2520that%2520incorporates%2520opponent%2520descriptions%252C%2520while%2520these%250Aapproaches%2520are%2520limited%2520to%2520scenarios%2520where%2520LLMs%2520possess%2520adequate%2520domain%250Aexpertise.%2520To%2520address%2520that%252C%2520we%2520introduce%2520a%2520two-stage%2520Strategy-Augmented%250APlanning%2520%2528SAP%2529%2520framework%2520that%2520significantly%2520enhances%2520the%2520opponent%2520exploitation%250Acapabilities%2520of%2520LLM-based%2520agents%2520by%2520utilizing%2520a%2520critical%2520component%252C%2520the%250AStrategy%2520Evaluation%2520Network%2520%2528SEN%2529.%2520Specifically%252C%2520in%2520the%2520offline%2520stage%252C%2520we%250Aconstruct%2520an%2520explicit%2520strategy%2520space%2520and%2520subsequently%2520collect%2520strategy-outcome%250Apair%2520data%2520for%2520training%2520the%2520SEN%2520network.%2520During%2520the%2520online%2520phase%252C%2520SAP%250Adynamically%2520recognizes%2520the%2520opponent%2527s%2520strategies%2520and%2520greedily%2520exploits%2520them%2520by%250Asearching%2520best%2520response%2520strategy%2520on%2520the%2520well-trained%2520SEN%252C%2520finally%2520translating%250Astrategy%2520to%2520a%2520course%2520of%2520actions%2520by%2520carefully%2520designed%2520prompts.%2520Experimental%250Aresults%2520show%2520that%2520SAP%2520exhibits%2520robust%2520generalization%2520capabilities%252C%2520allowing%2520it%250Ato%2520perform%2520effectively%2520not%2520only%2520against%2520previously%2520encountered%2520opponent%250Astrategies%2520but%2520also%2520against%2520novel%252C%2520unseen%2520strategies.%2520In%2520the%2520MicroRTS%250Aenvironment%252C%2520SAP%2520achieves%2520a%252085.35%255C%2525%2520performance%2520improvement%2520over%2520baseline%250Amethods%2520and%2520matches%2520the%2520competitiveness%2520of%2520reinforcement%2520learning%2520approaches%250Aagainst%2520state-of-the-art%2520%2528SOTA%2529%2520rule-based%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strategy-Augmented%20Planning%20for%20Large%20Language%20Models%20via%20Opponent%0A%20%20Exploitation&entry.906535625=Shuai%20Xu%20and%20Sijia%20Cui%20and%20Yanna%20Wang%20and%20Bo%20Xu%20and%20Qi%20Wang&entry.1292438233=%20%20Efficiently%20modeling%20and%20exploiting%20opponents%20is%20a%20long-standing%20challenge%20in%0Aadversarial%20domains.%20Large%20Language%20Models%20%28LLMs%29%20trained%20on%20extensive%20textual%0Adata%20have%20recently%20demonstrated%20outstanding%20performance%20in%20general%20tasks%2C%0Aintroducing%20new%20research%20directions%20for%20opponent%20modeling.%20Some%20studies%0Aprimarily%20focus%20on%20directly%20using%20LLMs%20to%20generate%20decisions%20based%20on%20the%0Aelaborate%20prompt%20context%20that%20incorporates%20opponent%20descriptions%2C%20while%20these%0Aapproaches%20are%20limited%20to%20scenarios%20where%20LLMs%20possess%20adequate%20domain%0Aexpertise.%20To%20address%20that%2C%20we%20introduce%20a%20two-stage%20Strategy-Augmented%0APlanning%20%28SAP%29%20framework%20that%20significantly%20enhances%20the%20opponent%20exploitation%0Acapabilities%20of%20LLM-based%20agents%20by%20utilizing%20a%20critical%20component%2C%20the%0AStrategy%20Evaluation%20Network%20%28SEN%29.%20Specifically%2C%20in%20the%20offline%20stage%2C%20we%0Aconstruct%20an%20explicit%20strategy%20space%20and%20subsequently%20collect%20strategy-outcome%0Apair%20data%20for%20training%20the%20SEN%20network.%20During%20the%20online%20phase%2C%20SAP%0Adynamically%20recognizes%20the%20opponent%27s%20strategies%20and%20greedily%20exploits%20them%20by%0Asearching%20best%20response%20strategy%20on%20the%20well-trained%20SEN%2C%20finally%20translating%0Astrategy%20to%20a%20course%20of%20actions%20by%20carefully%20designed%20prompts.%20Experimental%0Aresults%20show%20that%20SAP%20exhibits%20robust%20generalization%20capabilities%2C%20allowing%20it%0Ato%20perform%20effectively%20not%20only%20against%20previously%20encountered%20opponent%0Astrategies%20but%20also%20against%20novel%2C%20unseen%20strategies.%20In%20the%20MicroRTS%0Aenvironment%2C%20SAP%20achieves%20a%2085.35%5C%25%20performance%20improvement%20over%20baseline%0Amethods%20and%20matches%20the%20competitiveness%20of%20reinforcement%20learning%20approaches%0Aagainst%20state-of-the-art%20%28SOTA%29%20rule-based%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08459v1&entry.124074799=Read"},
{"title": "Memorization-Compression Cycles Improve Generalization", "author": "Fangyuan Yu", "abstract": "  We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation.\n", "link": "http://arxiv.org/abs/2505.08727v1", "date": "2025-05-13", "relevancy": 2.002, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5258}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5007}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memorization-Compression%20Cycles%20Improve%20Generalization&body=Title%3A%20Memorization-Compression%20Cycles%20Improve%20Generalization%0AAuthor%3A%20Fangyuan%20Yu%0AAbstract%3A%20%20%20We%20prove%20theoretically%20that%20generalization%20improves%20not%20only%20through%20data%0Ascaling%20but%20also%20by%20compressing%20internal%20representations.%20To%20operationalize%0Athis%20insight%2C%20we%20introduce%20the%20Information%20Bottleneck%20Language%20Modeling%20%28IBLM%29%0Aobjective%2C%20which%20reframes%20language%20modeling%20as%20a%20constrained%20optimization%0Aproblem%3A%20minimizing%20representation%20entropy%20subject%20to%20optimal%20prediction%0Aperformance.%20Empirically%2C%20we%20observe%20an%20emergent%20memorization-compression%20cycle%0Aduring%20LLM%20pretraining%2C%20evidenced%20by%20oscillation%20positive/negative%20gradient%0Aalignment%20between%20cross-entropy%20and%20Matrix-Based%20Entropy%20%28MBE%29%2C%20a%20measure%20of%0Arepresentation%20entropy.%20This%20pattern%20closely%20mirrors%20the%20predictive-compressive%0Atrade-off%20prescribed%20by%20IBLM%20and%20also%20parallels%20the%20biological%20alternation%0Abetween%20awake%20learning%20and%20sleep%20consolidation.%20Motivated%20by%20this%20observation%2C%0Awe%20propose%20Gated%20Phase%20Transition%20%28GAPT%29%2C%20a%20training%20algorithm%20that%20adaptively%0Aswitches%20between%20memorization%20and%20compression%20phases.%20When%20applied%20to%20GPT-2%0Apretraining%20on%20FineWeb%20dataset%2C%20GAPT%20reduces%20MBE%20by%2050%25%20and%20improves%0Across-entropy%20by%204.8%25.%20GAPT%20improves%20OOD%20generalizatino%20by%2035%25%20in%20a%20pretraining%0Atask%20on%20arithmetic%20multiplication.%20In%20a%20setting%20designed%20to%20simulate%0Acatastrophic%20forgetting%2C%20GAPT%20reduces%20interference%20by%20compressing%20and%0Aseparating%20representations%2C%20achieving%20a%2097%25%20improvement%20in%20separation%20-%0Aparalleling%20the%20functional%20role%20of%20sleep%20consolidation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemorization-Compression%2520Cycles%2520Improve%2520Generalization%26entry.906535625%3DFangyuan%2520Yu%26entry.1292438233%3D%2520%2520We%2520prove%2520theoretically%2520that%2520generalization%2520improves%2520not%2520only%2520through%2520data%250Ascaling%2520but%2520also%2520by%2520compressing%2520internal%2520representations.%2520To%2520operationalize%250Athis%2520insight%252C%2520we%2520introduce%2520the%2520Information%2520Bottleneck%2520Language%2520Modeling%2520%2528IBLM%2529%250Aobjective%252C%2520which%2520reframes%2520language%2520modeling%2520as%2520a%2520constrained%2520optimization%250Aproblem%253A%2520minimizing%2520representation%2520entropy%2520subject%2520to%2520optimal%2520prediction%250Aperformance.%2520Empirically%252C%2520we%2520observe%2520an%2520emergent%2520memorization-compression%2520cycle%250Aduring%2520LLM%2520pretraining%252C%2520evidenced%2520by%2520oscillation%2520positive/negative%2520gradient%250Aalignment%2520between%2520cross-entropy%2520and%2520Matrix-Based%2520Entropy%2520%2528MBE%2529%252C%2520a%2520measure%2520of%250Arepresentation%2520entropy.%2520This%2520pattern%2520closely%2520mirrors%2520the%2520predictive-compressive%250Atrade-off%2520prescribed%2520by%2520IBLM%2520and%2520also%2520parallels%2520the%2520biological%2520alternation%250Abetween%2520awake%2520learning%2520and%2520sleep%2520consolidation.%2520Motivated%2520by%2520this%2520observation%252C%250Awe%2520propose%2520Gated%2520Phase%2520Transition%2520%2528GAPT%2529%252C%2520a%2520training%2520algorithm%2520that%2520adaptively%250Aswitches%2520between%2520memorization%2520and%2520compression%2520phases.%2520When%2520applied%2520to%2520GPT-2%250Apretraining%2520on%2520FineWeb%2520dataset%252C%2520GAPT%2520reduces%2520MBE%2520by%252050%2525%2520and%2520improves%250Across-entropy%2520by%25204.8%2525.%2520GAPT%2520improves%2520OOD%2520generalizatino%2520by%252035%2525%2520in%2520a%2520pretraining%250Atask%2520on%2520arithmetic%2520multiplication.%2520In%2520a%2520setting%2520designed%2520to%2520simulate%250Acatastrophic%2520forgetting%252C%2520GAPT%2520reduces%2520interference%2520by%2520compressing%2520and%250Aseparating%2520representations%252C%2520achieving%2520a%252097%2525%2520improvement%2520in%2520separation%2520-%250Aparalleling%2520the%2520functional%2520role%2520of%2520sleep%2520consolidation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorization-Compression%20Cycles%20Improve%20Generalization&entry.906535625=Fangyuan%20Yu&entry.1292438233=%20%20We%20prove%20theoretically%20that%20generalization%20improves%20not%20only%20through%20data%0Ascaling%20but%20also%20by%20compressing%20internal%20representations.%20To%20operationalize%0Athis%20insight%2C%20we%20introduce%20the%20Information%20Bottleneck%20Language%20Modeling%20%28IBLM%29%0Aobjective%2C%20which%20reframes%20language%20modeling%20as%20a%20constrained%20optimization%0Aproblem%3A%20minimizing%20representation%20entropy%20subject%20to%20optimal%20prediction%0Aperformance.%20Empirically%2C%20we%20observe%20an%20emergent%20memorization-compression%20cycle%0Aduring%20LLM%20pretraining%2C%20evidenced%20by%20oscillation%20positive/negative%20gradient%0Aalignment%20between%20cross-entropy%20and%20Matrix-Based%20Entropy%20%28MBE%29%2C%20a%20measure%20of%0Arepresentation%20entropy.%20This%20pattern%20closely%20mirrors%20the%20predictive-compressive%0Atrade-off%20prescribed%20by%20IBLM%20and%20also%20parallels%20the%20biological%20alternation%0Abetween%20awake%20learning%20and%20sleep%20consolidation.%20Motivated%20by%20this%20observation%2C%0Awe%20propose%20Gated%20Phase%20Transition%20%28GAPT%29%2C%20a%20training%20algorithm%20that%20adaptively%0Aswitches%20between%20memorization%20and%20compression%20phases.%20When%20applied%20to%20GPT-2%0Apretraining%20on%20FineWeb%20dataset%2C%20GAPT%20reduces%20MBE%20by%2050%25%20and%20improves%0Across-entropy%20by%204.8%25.%20GAPT%20improves%20OOD%20generalizatino%20by%2035%25%20in%20a%20pretraining%0Atask%20on%20arithmetic%20multiplication.%20In%20a%20setting%20designed%20to%20simulate%0Acatastrophic%20forgetting%2C%20GAPT%20reduces%20interference%20by%20compressing%20and%0Aseparating%20representations%2C%20achieving%20a%2097%25%20improvement%20in%20separation%20-%0Aparalleling%20the%20functional%20role%20of%20sleep%20consolidation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08727v1&entry.124074799=Read"},
{"title": "Wilsonian Renormalization of Neural Network Gaussian Processes", "author": "Jessica N. Howard and Ro Jefferson and Anindita Maiti and Zohar Ringel", "abstract": "  Separating relevant and irrelevant information is key to any modeling process\nor scientific inquiry. Theoretical physics offers a powerful tool for achieving\nthis in the form of the renormalization group (RG). Here we demonstrate a\npractical approach to performing Wilsonian RG in the context of Gaussian\nProcess (GP) Regression. We systematically integrate out the unlearnable modes\nof the GP kernel, thereby obtaining an RG flow of the GP in which the data sets\nthe IR scale. In simple cases, this results in a universal flow of the ridge\nparameter, which becomes input-dependent in the richer scenario in which\nnon-Gaussianities are included. In addition to being analytically tractable,\nthis approach goes beyond structural analogies between RG and neural networks\nby providing a natural connection between RG flow and learnable vs. unlearnable\nmodes. Studying such flows may improve our understanding of feature learning in\ndeep neural networks, and enable us to identify potential universality classes\nin these models.\n", "link": "http://arxiv.org/abs/2405.06008v3", "date": "2025-05-13", "relevancy": 1.9884, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5223}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4824}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wilsonian%20Renormalization%20of%20Neural%20Network%20Gaussian%20Processes&body=Title%3A%20Wilsonian%20Renormalization%20of%20Neural%20Network%20Gaussian%20Processes%0AAuthor%3A%20Jessica%20N.%20Howard%20and%20Ro%20Jefferson%20and%20Anindita%20Maiti%20and%20Zohar%20Ringel%0AAbstract%3A%20%20%20Separating%20relevant%20and%20irrelevant%20information%20is%20key%20to%20any%20modeling%20process%0Aor%20scientific%20inquiry.%20Theoretical%20physics%20offers%20a%20powerful%20tool%20for%20achieving%0Athis%20in%20the%20form%20of%20the%20renormalization%20group%20%28RG%29.%20Here%20we%20demonstrate%20a%0Apractical%20approach%20to%20performing%20Wilsonian%20RG%20in%20the%20context%20of%20Gaussian%0AProcess%20%28GP%29%20Regression.%20We%20systematically%20integrate%20out%20the%20unlearnable%20modes%0Aof%20the%20GP%20kernel%2C%20thereby%20obtaining%20an%20RG%20flow%20of%20the%20GP%20in%20which%20the%20data%20sets%0Athe%20IR%20scale.%20In%20simple%20cases%2C%20this%20results%20in%20a%20universal%20flow%20of%20the%20ridge%0Aparameter%2C%20which%20becomes%20input-dependent%20in%20the%20richer%20scenario%20in%20which%0Anon-Gaussianities%20are%20included.%20In%20addition%20to%20being%20analytically%20tractable%2C%0Athis%20approach%20goes%20beyond%20structural%20analogies%20between%20RG%20and%20neural%20networks%0Aby%20providing%20a%20natural%20connection%20between%20RG%20flow%20and%20learnable%20vs.%20unlearnable%0Amodes.%20Studying%20such%20flows%20may%20improve%20our%20understanding%20of%20feature%20learning%20in%0Adeep%20neural%20networks%2C%20and%20enable%20us%20to%20identify%20potential%20universality%20classes%0Ain%20these%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06008v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWilsonian%2520Renormalization%2520of%2520Neural%2520Network%2520Gaussian%2520Processes%26entry.906535625%3DJessica%2520N.%2520Howard%2520and%2520Ro%2520Jefferson%2520and%2520Anindita%2520Maiti%2520and%2520Zohar%2520Ringel%26entry.1292438233%3D%2520%2520Separating%2520relevant%2520and%2520irrelevant%2520information%2520is%2520key%2520to%2520any%2520modeling%2520process%250Aor%2520scientific%2520inquiry.%2520Theoretical%2520physics%2520offers%2520a%2520powerful%2520tool%2520for%2520achieving%250Athis%2520in%2520the%2520form%2520of%2520the%2520renormalization%2520group%2520%2528RG%2529.%2520Here%2520we%2520demonstrate%2520a%250Apractical%2520approach%2520to%2520performing%2520Wilsonian%2520RG%2520in%2520the%2520context%2520of%2520Gaussian%250AProcess%2520%2528GP%2529%2520Regression.%2520We%2520systematically%2520integrate%2520out%2520the%2520unlearnable%2520modes%250Aof%2520the%2520GP%2520kernel%252C%2520thereby%2520obtaining%2520an%2520RG%2520flow%2520of%2520the%2520GP%2520in%2520which%2520the%2520data%2520sets%250Athe%2520IR%2520scale.%2520In%2520simple%2520cases%252C%2520this%2520results%2520in%2520a%2520universal%2520flow%2520of%2520the%2520ridge%250Aparameter%252C%2520which%2520becomes%2520input-dependent%2520in%2520the%2520richer%2520scenario%2520in%2520which%250Anon-Gaussianities%2520are%2520included.%2520In%2520addition%2520to%2520being%2520analytically%2520tractable%252C%250Athis%2520approach%2520goes%2520beyond%2520structural%2520analogies%2520between%2520RG%2520and%2520neural%2520networks%250Aby%2520providing%2520a%2520natural%2520connection%2520between%2520RG%2520flow%2520and%2520learnable%2520vs.%2520unlearnable%250Amodes.%2520Studying%2520such%2520flows%2520may%2520improve%2520our%2520understanding%2520of%2520feature%2520learning%2520in%250Adeep%2520neural%2520networks%252C%2520and%2520enable%2520us%2520to%2520identify%2520potential%2520universality%2520classes%250Ain%2520these%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06008v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wilsonian%20Renormalization%20of%20Neural%20Network%20Gaussian%20Processes&entry.906535625=Jessica%20N.%20Howard%20and%20Ro%20Jefferson%20and%20Anindita%20Maiti%20and%20Zohar%20Ringel&entry.1292438233=%20%20Separating%20relevant%20and%20irrelevant%20information%20is%20key%20to%20any%20modeling%20process%0Aor%20scientific%20inquiry.%20Theoretical%20physics%20offers%20a%20powerful%20tool%20for%20achieving%0Athis%20in%20the%20form%20of%20the%20renormalization%20group%20%28RG%29.%20Here%20we%20demonstrate%20a%0Apractical%20approach%20to%20performing%20Wilsonian%20RG%20in%20the%20context%20of%20Gaussian%0AProcess%20%28GP%29%20Regression.%20We%20systematically%20integrate%20out%20the%20unlearnable%20modes%0Aof%20the%20GP%20kernel%2C%20thereby%20obtaining%20an%20RG%20flow%20of%20the%20GP%20in%20which%20the%20data%20sets%0Athe%20IR%20scale.%20In%20simple%20cases%2C%20this%20results%20in%20a%20universal%20flow%20of%20the%20ridge%0Aparameter%2C%20which%20becomes%20input-dependent%20in%20the%20richer%20scenario%20in%20which%0Anon-Gaussianities%20are%20included.%20In%20addition%20to%20being%20analytically%20tractable%2C%0Athis%20approach%20goes%20beyond%20structural%20analogies%20between%20RG%20and%20neural%20networks%0Aby%20providing%20a%20natural%20connection%20between%20RG%20flow%20and%20learnable%20vs.%20unlearnable%0Amodes.%20Studying%20such%20flows%20may%20improve%20our%20understanding%20of%20feature%20learning%20in%0Adeep%20neural%20networks%2C%20and%20enable%20us%20to%20identify%20potential%20universality%20classes%0Ain%20these%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06008v3&entry.124074799=Read"},
{"title": "USEFUSE: Uniform Stride for Enhanced Performance in Fused Layer\n  Architecture of Deep Neural Networks", "author": "Muhammad Sohail Ibrahim and Muhammad Usman and Jeong-A Lee", "abstract": "  Convolutional Neural Networks (CNNs) are crucial in various applications, but\ntheir deployment on resource-constrained edge devices poses challenges. This\nstudy presents the Sum-of-Products (SOP) units for convolution, which utilize\nlow-latency left-to-right bit-serial arithmetic to minimize response time and\nenhance overall performance. The study proposes a methodology for fusing\nmultiple convolution layers to reduce off-chip memory communication and\nincrease overall performance. An effective mechanism detects and skips\ninefficient convolutions after ReLU layers, minimizing power consumption\nwithout compromising accuracy. Furthermore, efficient tile movement guarantees\nuniform access to the fusion pyramid. An analysis demonstrates the utile stride\nstrategy improves operational intensity. Two designs cater to varied demands:\none focuses on minimal response time for mission-critical applications, and\nanother focuses on resource-constrained devices with comparable latency. This\napproach notably reduced redundant computations, improving the efficiency of\nCNN deployment on edge devices.\n", "link": "http://arxiv.org/abs/2412.13724v2", "date": "2025-05-13", "relevancy": 1.9863, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5432}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4941}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20USEFUSE%3A%20Uniform%20Stride%20for%20Enhanced%20Performance%20in%20Fused%20Layer%0A%20%20Architecture%20of%20Deep%20Neural%20Networks&body=Title%3A%20USEFUSE%3A%20Uniform%20Stride%20for%20Enhanced%20Performance%20in%20Fused%20Layer%0A%20%20Architecture%20of%20Deep%20Neural%20Networks%0AAuthor%3A%20Muhammad%20Sohail%20Ibrahim%20and%20Muhammad%20Usman%20and%20Jeong-A%20Lee%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20are%20crucial%20in%20various%20applications%2C%20but%0Atheir%20deployment%20on%20resource-constrained%20edge%20devices%20poses%20challenges.%20This%0Astudy%20presents%20the%20Sum-of-Products%20%28SOP%29%20units%20for%20convolution%2C%20which%20utilize%0Alow-latency%20left-to-right%20bit-serial%20arithmetic%20to%20minimize%20response%20time%20and%0Aenhance%20overall%20performance.%20The%20study%20proposes%20a%20methodology%20for%20fusing%0Amultiple%20convolution%20layers%20to%20reduce%20off-chip%20memory%20communication%20and%0Aincrease%20overall%20performance.%20An%20effective%20mechanism%20detects%20and%20skips%0Ainefficient%20convolutions%20after%20ReLU%20layers%2C%20minimizing%20power%20consumption%0Awithout%20compromising%20accuracy.%20Furthermore%2C%20efficient%20tile%20movement%20guarantees%0Auniform%20access%20to%20the%20fusion%20pyramid.%20An%20analysis%20demonstrates%20the%20utile%20stride%0Astrategy%20improves%20operational%20intensity.%20Two%20designs%20cater%20to%20varied%20demands%3A%0Aone%20focuses%20on%20minimal%20response%20time%20for%20mission-critical%20applications%2C%20and%0Aanother%20focuses%20on%20resource-constrained%20devices%20with%20comparable%20latency.%20This%0Aapproach%20notably%20reduced%20redundant%20computations%2C%20improving%20the%20efficiency%20of%0ACNN%20deployment%20on%20edge%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUSEFUSE%253A%2520Uniform%2520Stride%2520for%2520Enhanced%2520Performance%2520in%2520Fused%2520Layer%250A%2520%2520Architecture%2520of%2520Deep%2520Neural%2520Networks%26entry.906535625%3DMuhammad%2520Sohail%2520Ibrahim%2520and%2520Muhammad%2520Usman%2520and%2520Jeong-A%2520Lee%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520are%2520crucial%2520in%2520various%2520applications%252C%2520but%250Atheir%2520deployment%2520on%2520resource-constrained%2520edge%2520devices%2520poses%2520challenges.%2520This%250Astudy%2520presents%2520the%2520Sum-of-Products%2520%2528SOP%2529%2520units%2520for%2520convolution%252C%2520which%2520utilize%250Alow-latency%2520left-to-right%2520bit-serial%2520arithmetic%2520to%2520minimize%2520response%2520time%2520and%250Aenhance%2520overall%2520performance.%2520The%2520study%2520proposes%2520a%2520methodology%2520for%2520fusing%250Amultiple%2520convolution%2520layers%2520to%2520reduce%2520off-chip%2520memory%2520communication%2520and%250Aincrease%2520overall%2520performance.%2520An%2520effective%2520mechanism%2520detects%2520and%2520skips%250Ainefficient%2520convolutions%2520after%2520ReLU%2520layers%252C%2520minimizing%2520power%2520consumption%250Awithout%2520compromising%2520accuracy.%2520Furthermore%252C%2520efficient%2520tile%2520movement%2520guarantees%250Auniform%2520access%2520to%2520the%2520fusion%2520pyramid.%2520An%2520analysis%2520demonstrates%2520the%2520utile%2520stride%250Astrategy%2520improves%2520operational%2520intensity.%2520Two%2520designs%2520cater%2520to%2520varied%2520demands%253A%250Aone%2520focuses%2520on%2520minimal%2520response%2520time%2520for%2520mission-critical%2520applications%252C%2520and%250Aanother%2520focuses%2520on%2520resource-constrained%2520devices%2520with%2520comparable%2520latency.%2520This%250Aapproach%2520notably%2520reduced%2520redundant%2520computations%252C%2520improving%2520the%2520efficiency%2520of%250ACNN%2520deployment%2520on%2520edge%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USEFUSE%3A%20Uniform%20Stride%20for%20Enhanced%20Performance%20in%20Fused%20Layer%0A%20%20Architecture%20of%20Deep%20Neural%20Networks&entry.906535625=Muhammad%20Sohail%20Ibrahim%20and%20Muhammad%20Usman%20and%20Jeong-A%20Lee&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20are%20crucial%20in%20various%20applications%2C%20but%0Atheir%20deployment%20on%20resource-constrained%20edge%20devices%20poses%20challenges.%20This%0Astudy%20presents%20the%20Sum-of-Products%20%28SOP%29%20units%20for%20convolution%2C%20which%20utilize%0Alow-latency%20left-to-right%20bit-serial%20arithmetic%20to%20minimize%20response%20time%20and%0Aenhance%20overall%20performance.%20The%20study%20proposes%20a%20methodology%20for%20fusing%0Amultiple%20convolution%20layers%20to%20reduce%20off-chip%20memory%20communication%20and%0Aincrease%20overall%20performance.%20An%20effective%20mechanism%20detects%20and%20skips%0Ainefficient%20convolutions%20after%20ReLU%20layers%2C%20minimizing%20power%20consumption%0Awithout%20compromising%20accuracy.%20Furthermore%2C%20efficient%20tile%20movement%20guarantees%0Auniform%20access%20to%20the%20fusion%20pyramid.%20An%20analysis%20demonstrates%20the%20utile%20stride%0Astrategy%20improves%20operational%20intensity.%20Two%20designs%20cater%20to%20varied%20demands%3A%0Aone%20focuses%20on%20minimal%20response%20time%20for%20mission-critical%20applications%2C%20and%0Aanother%20focuses%20on%20resource-constrained%20devices%20with%20comparable%20latency.%20This%0Aapproach%20notably%20reduced%20redundant%20computations%2C%20improving%20the%20efficiency%20of%0ACNN%20deployment%20on%20edge%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13724v2&entry.124074799=Read"},
{"title": "Trade-off between Gradient Measurement Efficiency and Expressivity in\n  Deep Quantum Neural Networks", "author": "Koki Chinzei and Shinichiro Yamano and Quoc Hoan Tran and Yasuhiro Endo and Hirotaka Oshima", "abstract": "  Quantum neural networks (QNNs) require an efficient training algorithm to\nachieve practical quantum advantages. A promising approach is gradient-based\noptimization, where gradients are estimated by quantum measurements. However,\nQNNs currently lack general quantum algorithms for efficiently measuring\ngradients, which limits their scalability. To elucidate the fundamental limits\nand potentials of efficient gradient estimation, we rigorously prove a\ntrade-off between gradient measurement efficiency (the mean number of\nsimultaneously measurable gradient components) and expressivity in deep QNNs.\nThis trade-off indicates that more expressive QNNs require higher measurement\ncosts per parameter for gradient estimation, while reducing QNN expressivity to\nsuit a given task can increase gradient measurement efficiency. We further\npropose a general QNN ansatz called the stabilizer-logical product ansatz\n(SLPA), which achieves the trade-off upper bound by exploiting the symmetric\nstructure of the quantum circuit. Numerical experiments show that the SLPA\ndrastically reduces the sample complexity needed for training while maintaining\naccuracy and trainability compared to well-designed circuits based on the\nparameter-shift method.\n", "link": "http://arxiv.org/abs/2406.18316v3", "date": "2025-05-13", "relevancy": 1.9723, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4989}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4988}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trade-off%20between%20Gradient%20Measurement%20Efficiency%20and%20Expressivity%20in%0A%20%20Deep%20Quantum%20Neural%20Networks&body=Title%3A%20Trade-off%20between%20Gradient%20Measurement%20Efficiency%20and%20Expressivity%20in%0A%20%20Deep%20Quantum%20Neural%20Networks%0AAuthor%3A%20Koki%20Chinzei%20and%20Shinichiro%20Yamano%20and%20Quoc%20Hoan%20Tran%20and%20Yasuhiro%20Endo%20and%20Hirotaka%20Oshima%0AAbstract%3A%20%20%20Quantum%20neural%20networks%20%28QNNs%29%20require%20an%20efficient%20training%20algorithm%20to%0Aachieve%20practical%20quantum%20advantages.%20A%20promising%20approach%20is%20gradient-based%0Aoptimization%2C%20where%20gradients%20are%20estimated%20by%20quantum%20measurements.%20However%2C%0AQNNs%20currently%20lack%20general%20quantum%20algorithms%20for%20efficiently%20measuring%0Agradients%2C%20which%20limits%20their%20scalability.%20To%20elucidate%20the%20fundamental%20limits%0Aand%20potentials%20of%20efficient%20gradient%20estimation%2C%20we%20rigorously%20prove%20a%0Atrade-off%20between%20gradient%20measurement%20efficiency%20%28the%20mean%20number%20of%0Asimultaneously%20measurable%20gradient%20components%29%20and%20expressivity%20in%20deep%20QNNs.%0AThis%20trade-off%20indicates%20that%20more%20expressive%20QNNs%20require%20higher%20measurement%0Acosts%20per%20parameter%20for%20gradient%20estimation%2C%20while%20reducing%20QNN%20expressivity%20to%0Asuit%20a%20given%20task%20can%20increase%20gradient%20measurement%20efficiency.%20We%20further%0Apropose%20a%20general%20QNN%20ansatz%20called%20the%20stabilizer-logical%20product%20ansatz%0A%28SLPA%29%2C%20which%20achieves%20the%20trade-off%20upper%20bound%20by%20exploiting%20the%20symmetric%0Astructure%20of%20the%20quantum%20circuit.%20Numerical%20experiments%20show%20that%20the%20SLPA%0Adrastically%20reduces%20the%20sample%20complexity%20needed%20for%20training%20while%20maintaining%0Aaccuracy%20and%20trainability%20compared%20to%20well-designed%20circuits%20based%20on%20the%0Aparameter-shift%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18316v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrade-off%2520between%2520Gradient%2520Measurement%2520Efficiency%2520and%2520Expressivity%2520in%250A%2520%2520Deep%2520Quantum%2520Neural%2520Networks%26entry.906535625%3DKoki%2520Chinzei%2520and%2520Shinichiro%2520Yamano%2520and%2520Quoc%2520Hoan%2520Tran%2520and%2520Yasuhiro%2520Endo%2520and%2520Hirotaka%2520Oshima%26entry.1292438233%3D%2520%2520Quantum%2520neural%2520networks%2520%2528QNNs%2529%2520require%2520an%2520efficient%2520training%2520algorithm%2520to%250Aachieve%2520practical%2520quantum%2520advantages.%2520A%2520promising%2520approach%2520is%2520gradient-based%250Aoptimization%252C%2520where%2520gradients%2520are%2520estimated%2520by%2520quantum%2520measurements.%2520However%252C%250AQNNs%2520currently%2520lack%2520general%2520quantum%2520algorithms%2520for%2520efficiently%2520measuring%250Agradients%252C%2520which%2520limits%2520their%2520scalability.%2520To%2520elucidate%2520the%2520fundamental%2520limits%250Aand%2520potentials%2520of%2520efficient%2520gradient%2520estimation%252C%2520we%2520rigorously%2520prove%2520a%250Atrade-off%2520between%2520gradient%2520measurement%2520efficiency%2520%2528the%2520mean%2520number%2520of%250Asimultaneously%2520measurable%2520gradient%2520components%2529%2520and%2520expressivity%2520in%2520deep%2520QNNs.%250AThis%2520trade-off%2520indicates%2520that%2520more%2520expressive%2520QNNs%2520require%2520higher%2520measurement%250Acosts%2520per%2520parameter%2520for%2520gradient%2520estimation%252C%2520while%2520reducing%2520QNN%2520expressivity%2520to%250Asuit%2520a%2520given%2520task%2520can%2520increase%2520gradient%2520measurement%2520efficiency.%2520We%2520further%250Apropose%2520a%2520general%2520QNN%2520ansatz%2520called%2520the%2520stabilizer-logical%2520product%2520ansatz%250A%2528SLPA%2529%252C%2520which%2520achieves%2520the%2520trade-off%2520upper%2520bound%2520by%2520exploiting%2520the%2520symmetric%250Astructure%2520of%2520the%2520quantum%2520circuit.%2520Numerical%2520experiments%2520show%2520that%2520the%2520SLPA%250Adrastically%2520reduces%2520the%2520sample%2520complexity%2520needed%2520for%2520training%2520while%2520maintaining%250Aaccuracy%2520and%2520trainability%2520compared%2520to%2520well-designed%2520circuits%2520based%2520on%2520the%250Aparameter-shift%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18316v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trade-off%20between%20Gradient%20Measurement%20Efficiency%20and%20Expressivity%20in%0A%20%20Deep%20Quantum%20Neural%20Networks&entry.906535625=Koki%20Chinzei%20and%20Shinichiro%20Yamano%20and%20Quoc%20Hoan%20Tran%20and%20Yasuhiro%20Endo%20and%20Hirotaka%20Oshima&entry.1292438233=%20%20Quantum%20neural%20networks%20%28QNNs%29%20require%20an%20efficient%20training%20algorithm%20to%0Aachieve%20practical%20quantum%20advantages.%20A%20promising%20approach%20is%20gradient-based%0Aoptimization%2C%20where%20gradients%20are%20estimated%20by%20quantum%20measurements.%20However%2C%0AQNNs%20currently%20lack%20general%20quantum%20algorithms%20for%20efficiently%20measuring%0Agradients%2C%20which%20limits%20their%20scalability.%20To%20elucidate%20the%20fundamental%20limits%0Aand%20potentials%20of%20efficient%20gradient%20estimation%2C%20we%20rigorously%20prove%20a%0Atrade-off%20between%20gradient%20measurement%20efficiency%20%28the%20mean%20number%20of%0Asimultaneously%20measurable%20gradient%20components%29%20and%20expressivity%20in%20deep%20QNNs.%0AThis%20trade-off%20indicates%20that%20more%20expressive%20QNNs%20require%20higher%20measurement%0Acosts%20per%20parameter%20for%20gradient%20estimation%2C%20while%20reducing%20QNN%20expressivity%20to%0Asuit%20a%20given%20task%20can%20increase%20gradient%20measurement%20efficiency.%20We%20further%0Apropose%20a%20general%20QNN%20ansatz%20called%20the%20stabilizer-logical%20product%20ansatz%0A%28SLPA%29%2C%20which%20achieves%20the%20trade-off%20upper%20bound%20by%20exploiting%20the%20symmetric%0Astructure%20of%20the%20quantum%20circuit.%20Numerical%20experiments%20show%20that%20the%20SLPA%0Adrastically%20reduces%20the%20sample%20complexity%20needed%20for%20training%20while%20maintaining%0Aaccuracy%20and%20trainability%20compared%20to%20well-designed%20circuits%20based%20on%20the%0Aparameter-shift%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18316v3&entry.124074799=Read"},
{"title": "A Survey of Deep Learning for Complex Speech Spectrograms", "author": "Yuying Xie and Zheng-Hua Tan", "abstract": "  Recent advancements in deep learning have significantly impacted the field of\nspeech signal processing, particularly in the analysis and manipulation of\ncomplex spectrograms. This survey provides a comprehensive overview of the\nstate-of-the-art techniques leveraging deep neural networks for processing\ncomplex spectrograms, which encapsulate both magnitude and phase information.\nWe begin by introducing complex spectrograms and their associated features for\nvarious speech processing tasks. Next, we explore the key components and\narchitectures of complex-valued neural networks, which are specifically\ndesigned to handle complex-valued data and have been applied for complex\nspectrogram processing. We then discuss various training strategies and loss\nfunctions tailored for training neural networks to process and model complex\nspectrograms. The survey further examines key applications, including phase\nretrieval, speech enhancement, and speech separation, where deep learning has\nachieved significant progress by leveraging complex spectrograms or their\nderived feature representations. Additionally, we examine the intersection of\ncomplex spectrograms with generative models. This survey aims to serve as a\nvaluable resource for researchers and practitioners in the field of speech\nsignal processing and complex-valued neural networks.\n", "link": "http://arxiv.org/abs/2505.08694v1", "date": "2025-05-13", "relevancy": 1.9698, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5032}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5032}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Deep%20Learning%20for%20Complex%20Speech%20Spectrograms&body=Title%3A%20A%20Survey%20of%20Deep%20Learning%20for%20Complex%20Speech%20Spectrograms%0AAuthor%3A%20Yuying%20Xie%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep%20learning%20have%20significantly%20impacted%20the%20field%20of%0Aspeech%20signal%20processing%2C%20particularly%20in%20the%20analysis%20and%20manipulation%20of%0Acomplex%20spectrograms.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%20the%0Astate-of-the-art%20techniques%20leveraging%20deep%20neural%20networks%20for%20processing%0Acomplex%20spectrograms%2C%20which%20encapsulate%20both%20magnitude%20and%20phase%20information.%0AWe%20begin%20by%20introducing%20complex%20spectrograms%20and%20their%20associated%20features%20for%0Avarious%20speech%20processing%20tasks.%20Next%2C%20we%20explore%20the%20key%20components%20and%0Aarchitectures%20of%20complex-valued%20neural%20networks%2C%20which%20are%20specifically%0Adesigned%20to%20handle%20complex-valued%20data%20and%20have%20been%20applied%20for%20complex%0Aspectrogram%20processing.%20We%20then%20discuss%20various%20training%20strategies%20and%20loss%0Afunctions%20tailored%20for%20training%20neural%20networks%20to%20process%20and%20model%20complex%0Aspectrograms.%20The%20survey%20further%20examines%20key%20applications%2C%20including%20phase%0Aretrieval%2C%20speech%20enhancement%2C%20and%20speech%20separation%2C%20where%20deep%20learning%20has%0Aachieved%20significant%20progress%20by%20leveraging%20complex%20spectrograms%20or%20their%0Aderived%20feature%20representations.%20Additionally%2C%20we%20examine%20the%20intersection%20of%0Acomplex%20spectrograms%20with%20generative%20models.%20This%20survey%20aims%20to%20serve%20as%20a%0Avaluable%20resource%20for%20researchers%20and%20practitioners%20in%20the%20field%20of%20speech%0Asignal%20processing%20and%20complex-valued%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Deep%2520Learning%2520for%2520Complex%2520Speech%2520Spectrograms%26entry.906535625%3DYuying%2520Xie%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520deep%2520learning%2520have%2520significantly%2520impacted%2520the%2520field%2520of%250Aspeech%2520signal%2520processing%252C%2520particularly%2520in%2520the%2520analysis%2520and%2520manipulation%2520of%250Acomplex%2520spectrograms.%2520This%2520survey%2520provides%2520a%2520comprehensive%2520overview%2520of%2520the%250Astate-of-the-art%2520techniques%2520leveraging%2520deep%2520neural%2520networks%2520for%2520processing%250Acomplex%2520spectrograms%252C%2520which%2520encapsulate%2520both%2520magnitude%2520and%2520phase%2520information.%250AWe%2520begin%2520by%2520introducing%2520complex%2520spectrograms%2520and%2520their%2520associated%2520features%2520for%250Avarious%2520speech%2520processing%2520tasks.%2520Next%252C%2520we%2520explore%2520the%2520key%2520components%2520and%250Aarchitectures%2520of%2520complex-valued%2520neural%2520networks%252C%2520which%2520are%2520specifically%250Adesigned%2520to%2520handle%2520complex-valued%2520data%2520and%2520have%2520been%2520applied%2520for%2520complex%250Aspectrogram%2520processing.%2520We%2520then%2520discuss%2520various%2520training%2520strategies%2520and%2520loss%250Afunctions%2520tailored%2520for%2520training%2520neural%2520networks%2520to%2520process%2520and%2520model%2520complex%250Aspectrograms.%2520The%2520survey%2520further%2520examines%2520key%2520applications%252C%2520including%2520phase%250Aretrieval%252C%2520speech%2520enhancement%252C%2520and%2520speech%2520separation%252C%2520where%2520deep%2520learning%2520has%250Aachieved%2520significant%2520progress%2520by%2520leveraging%2520complex%2520spectrograms%2520or%2520their%250Aderived%2520feature%2520representations.%2520Additionally%252C%2520we%2520examine%2520the%2520intersection%2520of%250Acomplex%2520spectrograms%2520with%2520generative%2520models.%2520This%2520survey%2520aims%2520to%2520serve%2520as%2520a%250Avaluable%2520resource%2520for%2520researchers%2520and%2520practitioners%2520in%2520the%2520field%2520of%2520speech%250Asignal%2520processing%2520and%2520complex-valued%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Deep%20Learning%20for%20Complex%20Speech%20Spectrograms&entry.906535625=Yuying%20Xie%20and%20Zheng-Hua%20Tan&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20learning%20have%20significantly%20impacted%20the%20field%20of%0Aspeech%20signal%20processing%2C%20particularly%20in%20the%20analysis%20and%20manipulation%20of%0Acomplex%20spectrograms.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%20the%0Astate-of-the-art%20techniques%20leveraging%20deep%20neural%20networks%20for%20processing%0Acomplex%20spectrograms%2C%20which%20encapsulate%20both%20magnitude%20and%20phase%20information.%0AWe%20begin%20by%20introducing%20complex%20spectrograms%20and%20their%20associated%20features%20for%0Avarious%20speech%20processing%20tasks.%20Next%2C%20we%20explore%20the%20key%20components%20and%0Aarchitectures%20of%20complex-valued%20neural%20networks%2C%20which%20are%20specifically%0Adesigned%20to%20handle%20complex-valued%20data%20and%20have%20been%20applied%20for%20complex%0Aspectrogram%20processing.%20We%20then%20discuss%20various%20training%20strategies%20and%20loss%0Afunctions%20tailored%20for%20training%20neural%20networks%20to%20process%20and%20model%20complex%0Aspectrograms.%20The%20survey%20further%20examines%20key%20applications%2C%20including%20phase%0Aretrieval%2C%20speech%20enhancement%2C%20and%20speech%20separation%2C%20where%20deep%20learning%20has%0Aachieved%20significant%20progress%20by%20leveraging%20complex%20spectrograms%20or%20their%0Aderived%20feature%20representations.%20Additionally%2C%20we%20examine%20the%20intersection%20of%0Acomplex%20spectrograms%20with%20generative%20models.%20This%20survey%20aims%20to%20serve%20as%20a%0Avaluable%20resource%20for%20researchers%20and%20practitioners%20in%20the%20field%20of%20speech%0Asignal%20processing%20and%20complex-valued%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08694v1&entry.124074799=Read"},
{"title": "Resource-Efficient Language Models: Quantization for Fast and Accessible\n  Inference", "author": "Tollef Emil J\u00f8rgensen", "abstract": "  Large language models have significantly advanced natural language\nprocessing, yet their heavy resource demands pose severe challenges regarding\nhardware accessibility and energy consumption. This paper presents a focused\nand high-level review of post-training quantization (PTQ) techniques designed\nto optimize the inference efficiency of LLMs by the end-user, including details\non various quantization schemes, granularities, and trade-offs. The aim is to\nprovide a balanced overview between the theory and applications of\npost-training quantization.\n", "link": "http://arxiv.org/abs/2505.08620v1", "date": "2025-05-13", "relevancy": 1.9686, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5026}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resource-Efficient%20Language%20Models%3A%20Quantization%20for%20Fast%20and%20Accessible%0A%20%20Inference&body=Title%3A%20Resource-Efficient%20Language%20Models%3A%20Quantization%20for%20Fast%20and%20Accessible%0A%20%20Inference%0AAuthor%3A%20Tollef%20Emil%20J%C3%B8rgensen%0AAbstract%3A%20%20%20Large%20language%20models%20have%20significantly%20advanced%20natural%20language%0Aprocessing%2C%20yet%20their%20heavy%20resource%20demands%20pose%20severe%20challenges%20regarding%0Ahardware%20accessibility%20and%20energy%20consumption.%20This%20paper%20presents%20a%20focused%0Aand%20high-level%20review%20of%20post-training%20quantization%20%28PTQ%29%20techniques%20designed%0Ato%20optimize%20the%20inference%20efficiency%20of%20LLMs%20by%20the%20end-user%2C%20including%20details%0Aon%20various%20quantization%20schemes%2C%20granularities%2C%20and%20trade-offs.%20The%20aim%20is%20to%0Aprovide%20a%20balanced%20overview%20between%20the%20theory%20and%20applications%20of%0Apost-training%20quantization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResource-Efficient%2520Language%2520Models%253A%2520Quantization%2520for%2520Fast%2520and%2520Accessible%250A%2520%2520Inference%26entry.906535625%3DTollef%2520Emil%2520J%25C3%25B8rgensen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520significantly%2520advanced%2520natural%2520language%250Aprocessing%252C%2520yet%2520their%2520heavy%2520resource%2520demands%2520pose%2520severe%2520challenges%2520regarding%250Ahardware%2520accessibility%2520and%2520energy%2520consumption.%2520This%2520paper%2520presents%2520a%2520focused%250Aand%2520high-level%2520review%2520of%2520post-training%2520quantization%2520%2528PTQ%2529%2520techniques%2520designed%250Ato%2520optimize%2520the%2520inference%2520efficiency%2520of%2520LLMs%2520by%2520the%2520end-user%252C%2520including%2520details%250Aon%2520various%2520quantization%2520schemes%252C%2520granularities%252C%2520and%2520trade-offs.%2520The%2520aim%2520is%2520to%250Aprovide%2520a%2520balanced%2520overview%2520between%2520the%2520theory%2520and%2520applications%2520of%250Apost-training%2520quantization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resource-Efficient%20Language%20Models%3A%20Quantization%20for%20Fast%20and%20Accessible%0A%20%20Inference&entry.906535625=Tollef%20Emil%20J%C3%B8rgensen&entry.1292438233=%20%20Large%20language%20models%20have%20significantly%20advanced%20natural%20language%0Aprocessing%2C%20yet%20their%20heavy%20resource%20demands%20pose%20severe%20challenges%20regarding%0Ahardware%20accessibility%20and%20energy%20consumption.%20This%20paper%20presents%20a%20focused%0Aand%20high-level%20review%20of%20post-training%20quantization%20%28PTQ%29%20techniques%20designed%0Ato%20optimize%20the%20inference%20efficiency%20of%20LLMs%20by%20the%20end-user%2C%20including%20details%0Aon%20various%20quantization%20schemes%2C%20granularities%2C%20and%20trade-offs.%20The%20aim%20is%20to%0Aprovide%20a%20balanced%20overview%20between%20the%20theory%20and%20applications%20of%0Apost-training%20quantization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08620v1&entry.124074799=Read"},
{"title": "A primal-dual perspective for distributed TD-learning", "author": "Han-Dong Lim and Donghwan Lee", "abstract": "  The goal of this paper is to investigate distributed temporal difference (TD)\nlearning for a networked multi-agent Markov decision process. The proposed\napproach is based on distributed optimization algorithms, which can be\ninterpreted as primal-dual Ordinary differential equation (ODE) dynamics\nsubject to null-space constraints. Based on the exponential convergence\nbehavior of the primal-dual ODE dynamics subject to null-space constraints, we\nexamine the behavior of the final iterate in various distributed TD-learning\nscenarios, considering both constant and diminishing step-sizes and\nincorporating both i.i.d. and Markovian observation models. Unlike existing\nmethods, the proposed algorithm does not require the assumption that the\nunderlying communication network structure is characterized by a doubly\nstochastic matrix.\n", "link": "http://arxiv.org/abs/2310.00638v3", "date": "2025-05-13", "relevancy": 1.9603, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5139}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4937}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20primal-dual%20perspective%20for%20distributed%20TD-learning&body=Title%3A%20A%20primal-dual%20perspective%20for%20distributed%20TD-learning%0AAuthor%3A%20Han-Dong%20Lim%20and%20Donghwan%20Lee%0AAbstract%3A%20%20%20The%20goal%20of%20this%20paper%20is%20to%20investigate%20distributed%20temporal%20difference%20%28TD%29%0Alearning%20for%20a%20networked%20multi-agent%20Markov%20decision%20process.%20The%20proposed%0Aapproach%20is%20based%20on%20distributed%20optimization%20algorithms%2C%20which%20can%20be%0Ainterpreted%20as%20primal-dual%20Ordinary%20differential%20equation%20%28ODE%29%20dynamics%0Asubject%20to%20null-space%20constraints.%20Based%20on%20the%20exponential%20convergence%0Abehavior%20of%20the%20primal-dual%20ODE%20dynamics%20subject%20to%20null-space%20constraints%2C%20we%0Aexamine%20the%20behavior%20of%20the%20final%20iterate%20in%20various%20distributed%20TD-learning%0Ascenarios%2C%20considering%20both%20constant%20and%20diminishing%20step-sizes%20and%0Aincorporating%20both%20i.i.d.%20and%20Markovian%20observation%20models.%20Unlike%20existing%0Amethods%2C%20the%20proposed%20algorithm%20does%20not%20require%20the%20assumption%20that%20the%0Aunderlying%20communication%20network%20structure%20is%20characterized%20by%20a%20doubly%0Astochastic%20matrix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00638v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520primal-dual%2520perspective%2520for%2520distributed%2520TD-learning%26entry.906535625%3DHan-Dong%2520Lim%2520and%2520Donghwan%2520Lee%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520this%2520paper%2520is%2520to%2520investigate%2520distributed%2520temporal%2520difference%2520%2528TD%2529%250Alearning%2520for%2520a%2520networked%2520multi-agent%2520Markov%2520decision%2520process.%2520The%2520proposed%250Aapproach%2520is%2520based%2520on%2520distributed%2520optimization%2520algorithms%252C%2520which%2520can%2520be%250Ainterpreted%2520as%2520primal-dual%2520Ordinary%2520differential%2520equation%2520%2528ODE%2529%2520dynamics%250Asubject%2520to%2520null-space%2520constraints.%2520Based%2520on%2520the%2520exponential%2520convergence%250Abehavior%2520of%2520the%2520primal-dual%2520ODE%2520dynamics%2520subject%2520to%2520null-space%2520constraints%252C%2520we%250Aexamine%2520the%2520behavior%2520of%2520the%2520final%2520iterate%2520in%2520various%2520distributed%2520TD-learning%250Ascenarios%252C%2520considering%2520both%2520constant%2520and%2520diminishing%2520step-sizes%2520and%250Aincorporating%2520both%2520i.i.d.%2520and%2520Markovian%2520observation%2520models.%2520Unlike%2520existing%250Amethods%252C%2520the%2520proposed%2520algorithm%2520does%2520not%2520require%2520the%2520assumption%2520that%2520the%250Aunderlying%2520communication%2520network%2520structure%2520is%2520characterized%2520by%2520a%2520doubly%250Astochastic%2520matrix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00638v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20primal-dual%20perspective%20for%20distributed%20TD-learning&entry.906535625=Han-Dong%20Lim%20and%20Donghwan%20Lee&entry.1292438233=%20%20The%20goal%20of%20this%20paper%20is%20to%20investigate%20distributed%20temporal%20difference%20%28TD%29%0Alearning%20for%20a%20networked%20multi-agent%20Markov%20decision%20process.%20The%20proposed%0Aapproach%20is%20based%20on%20distributed%20optimization%20algorithms%2C%20which%20can%20be%0Ainterpreted%20as%20primal-dual%20Ordinary%20differential%20equation%20%28ODE%29%20dynamics%0Asubject%20to%20null-space%20constraints.%20Based%20on%20the%20exponential%20convergence%0Abehavior%20of%20the%20primal-dual%20ODE%20dynamics%20subject%20to%20null-space%20constraints%2C%20we%0Aexamine%20the%20behavior%20of%20the%20final%20iterate%20in%20various%20distributed%20TD-learning%0Ascenarios%2C%20considering%20both%20constant%20and%20diminishing%20step-sizes%20and%0Aincorporating%20both%20i.i.d.%20and%20Markovian%20observation%20models.%20Unlike%20existing%0Amethods%2C%20the%20proposed%20algorithm%20does%20not%20require%20the%20assumption%20that%20the%0Aunderlying%20communication%20network%20structure%20is%20characterized%20by%20a%20doubly%0Astochastic%20matrix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00638v3&entry.124074799=Read"},
{"title": "MC-Swarm: Minimal-Communication Multi-Agent Trajectory Planning and\n  Deadlock Resolution for Quadrotor Swarm", "author": "Yunwoo Lee and Jungwon Park", "abstract": "  For effective multi-agent trajectory planning, it is important to consider\nlightweight communication and its potential asynchrony. This paper presents a\ndistributed trajectory planning algorithm for a quadrotor swarm that operates\nasynchronously and requires no communication except during the initial planning\nphase. Moreover, our algorithm guarantees no deadlock under asynchronous\nupdates and absence of communication during flight. To effectively ensure these\npoints, we build two main modules: coordination state updater and trajectory\noptimizer. The coordination state updater computes waypoints for each agent\ntoward its goal and performs subgoal optimization while considering deadlocks,\nas well as safety constraints with respect to neighbor agents and obstacles.\nThen, the trajectory optimizer generates a trajectory that ensures collision\navoidance even with the asynchronous planning updates of neighboring agents. We\nprovide a theoretical guarantee of collision avoidance with deadlock resolution\nand evaluate the effectiveness of our method in complex simulation\nenvironments, including random forests and narrow-gap mazes. Additionally, to\nreduce the total mission time, we design a faster coordination state update\nusing lightweight communication. Lastly, our approach is validated through\nextensive simulations and real-world experiments with cluttered environment\nscenarios.\n", "link": "http://arxiv.org/abs/2505.08593v1", "date": "2025-05-13", "relevancy": 1.9568, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5043}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5002}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MC-Swarm%3A%20Minimal-Communication%20Multi-Agent%20Trajectory%20Planning%20and%0A%20%20Deadlock%20Resolution%20for%20Quadrotor%20Swarm&body=Title%3A%20MC-Swarm%3A%20Minimal-Communication%20Multi-Agent%20Trajectory%20Planning%20and%0A%20%20Deadlock%20Resolution%20for%20Quadrotor%20Swarm%0AAuthor%3A%20Yunwoo%20Lee%20and%20Jungwon%20Park%0AAbstract%3A%20%20%20For%20effective%20multi-agent%20trajectory%20planning%2C%20it%20is%20important%20to%20consider%0Alightweight%20communication%20and%20its%20potential%20asynchrony.%20This%20paper%20presents%20a%0Adistributed%20trajectory%20planning%20algorithm%20for%20a%20quadrotor%20swarm%20that%20operates%0Aasynchronously%20and%20requires%20no%20communication%20except%20during%20the%20initial%20planning%0Aphase.%20Moreover%2C%20our%20algorithm%20guarantees%20no%20deadlock%20under%20asynchronous%0Aupdates%20and%20absence%20of%20communication%20during%20flight.%20To%20effectively%20ensure%20these%0Apoints%2C%20we%20build%20two%20main%20modules%3A%20coordination%20state%20updater%20and%20trajectory%0Aoptimizer.%20The%20coordination%20state%20updater%20computes%20waypoints%20for%20each%20agent%0Atoward%20its%20goal%20and%20performs%20subgoal%20optimization%20while%20considering%20deadlocks%2C%0Aas%20well%20as%20safety%20constraints%20with%20respect%20to%20neighbor%20agents%20and%20obstacles.%0AThen%2C%20the%20trajectory%20optimizer%20generates%20a%20trajectory%20that%20ensures%20collision%0Aavoidance%20even%20with%20the%20asynchronous%20planning%20updates%20of%20neighboring%20agents.%20We%0Aprovide%20a%20theoretical%20guarantee%20of%20collision%20avoidance%20with%20deadlock%20resolution%0Aand%20evaluate%20the%20effectiveness%20of%20our%20method%20in%20complex%20simulation%0Aenvironments%2C%20including%20random%20forests%20and%20narrow-gap%20mazes.%20Additionally%2C%20to%0Areduce%20the%20total%20mission%20time%2C%20we%20design%20a%20faster%20coordination%20state%20update%0Ausing%20lightweight%20communication.%20Lastly%2C%20our%20approach%20is%20validated%20through%0Aextensive%20simulations%20and%20real-world%20experiments%20with%20cluttered%20environment%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMC-Swarm%253A%2520Minimal-Communication%2520Multi-Agent%2520Trajectory%2520Planning%2520and%250A%2520%2520Deadlock%2520Resolution%2520for%2520Quadrotor%2520Swarm%26entry.906535625%3DYunwoo%2520Lee%2520and%2520Jungwon%2520Park%26entry.1292438233%3D%2520%2520For%2520effective%2520multi-agent%2520trajectory%2520planning%252C%2520it%2520is%2520important%2520to%2520consider%250Alightweight%2520communication%2520and%2520its%2520potential%2520asynchrony.%2520This%2520paper%2520presents%2520a%250Adistributed%2520trajectory%2520planning%2520algorithm%2520for%2520a%2520quadrotor%2520swarm%2520that%2520operates%250Aasynchronously%2520and%2520requires%2520no%2520communication%2520except%2520during%2520the%2520initial%2520planning%250Aphase.%2520Moreover%252C%2520our%2520algorithm%2520guarantees%2520no%2520deadlock%2520under%2520asynchronous%250Aupdates%2520and%2520absence%2520of%2520communication%2520during%2520flight.%2520To%2520effectively%2520ensure%2520these%250Apoints%252C%2520we%2520build%2520two%2520main%2520modules%253A%2520coordination%2520state%2520updater%2520and%2520trajectory%250Aoptimizer.%2520The%2520coordination%2520state%2520updater%2520computes%2520waypoints%2520for%2520each%2520agent%250Atoward%2520its%2520goal%2520and%2520performs%2520subgoal%2520optimization%2520while%2520considering%2520deadlocks%252C%250Aas%2520well%2520as%2520safety%2520constraints%2520with%2520respect%2520to%2520neighbor%2520agents%2520and%2520obstacles.%250AThen%252C%2520the%2520trajectory%2520optimizer%2520generates%2520a%2520trajectory%2520that%2520ensures%2520collision%250Aavoidance%2520even%2520with%2520the%2520asynchronous%2520planning%2520updates%2520of%2520neighboring%2520agents.%2520We%250Aprovide%2520a%2520theoretical%2520guarantee%2520of%2520collision%2520avoidance%2520with%2520deadlock%2520resolution%250Aand%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520complex%2520simulation%250Aenvironments%252C%2520including%2520random%2520forests%2520and%2520narrow-gap%2520mazes.%2520Additionally%252C%2520to%250Areduce%2520the%2520total%2520mission%2520time%252C%2520we%2520design%2520a%2520faster%2520coordination%2520state%2520update%250Ausing%2520lightweight%2520communication.%2520Lastly%252C%2520our%2520approach%2520is%2520validated%2520through%250Aextensive%2520simulations%2520and%2520real-world%2520experiments%2520with%2520cluttered%2520environment%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MC-Swarm%3A%20Minimal-Communication%20Multi-Agent%20Trajectory%20Planning%20and%0A%20%20Deadlock%20Resolution%20for%20Quadrotor%20Swarm&entry.906535625=Yunwoo%20Lee%20and%20Jungwon%20Park&entry.1292438233=%20%20For%20effective%20multi-agent%20trajectory%20planning%2C%20it%20is%20important%20to%20consider%0Alightweight%20communication%20and%20its%20potential%20asynchrony.%20This%20paper%20presents%20a%0Adistributed%20trajectory%20planning%20algorithm%20for%20a%20quadrotor%20swarm%20that%20operates%0Aasynchronously%20and%20requires%20no%20communication%20except%20during%20the%20initial%20planning%0Aphase.%20Moreover%2C%20our%20algorithm%20guarantees%20no%20deadlock%20under%20asynchronous%0Aupdates%20and%20absence%20of%20communication%20during%20flight.%20To%20effectively%20ensure%20these%0Apoints%2C%20we%20build%20two%20main%20modules%3A%20coordination%20state%20updater%20and%20trajectory%0Aoptimizer.%20The%20coordination%20state%20updater%20computes%20waypoints%20for%20each%20agent%0Atoward%20its%20goal%20and%20performs%20subgoal%20optimization%20while%20considering%20deadlocks%2C%0Aas%20well%20as%20safety%20constraints%20with%20respect%20to%20neighbor%20agents%20and%20obstacles.%0AThen%2C%20the%20trajectory%20optimizer%20generates%20a%20trajectory%20that%20ensures%20collision%0Aavoidance%20even%20with%20the%20asynchronous%20planning%20updates%20of%20neighboring%20agents.%20We%0Aprovide%20a%20theoretical%20guarantee%20of%20collision%20avoidance%20with%20deadlock%20resolution%0Aand%20evaluate%20the%20effectiveness%20of%20our%20method%20in%20complex%20simulation%0Aenvironments%2C%20including%20random%20forests%20and%20narrow-gap%20mazes.%20Additionally%2C%20to%0Areduce%20the%20total%20mission%20time%2C%20we%20design%20a%20faster%20coordination%20state%20update%0Ausing%20lightweight%20communication.%20Lastly%2C%20our%20approach%20is%20validated%20through%0Aextensive%20simulations%20and%20real-world%20experiments%20with%20cluttered%20environment%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08593v1&entry.124074799=Read"},
{"title": "PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts", "author": "Yang Su and Na Yan and Yansha Deng and Robert Schober", "abstract": "  Large language models (LLMs) hosted on cloud servers alleviate the\ncomputational and storage burdens on local devices but raise privacy concerns\ndue to sensitive data transmission and require substantial communication\nbandwidth, which is challenging in constrained environments. In contrast, small\nlanguage models (SLMs) running locally enhance privacy but suffer from limited\nperformance on complex tasks. To balance computational cost, performance, and\nprivacy protection under bandwidth constraints, we propose a privacy-aware\nwireless collaborative mixture of experts (PWC-MoE) framework. Specifically,\nPWC-MoE employs a sparse privacy-aware gating network to dynamically route\nsensitive tokens to privacy experts located on local clients, while\nnon-sensitive tokens are routed to non-privacy experts located at the remote\nbase station. To achieve computational efficiency, the gating network ensures\nthat each token is dynamically routed to and processed by only one expert. To\nenhance scalability and prevent overloading of specific experts, we introduce a\ngroup-wise load-balancing mechanism for the gating network that evenly\ndistributes sensitive tokens among privacy experts and non-sensitive tokens\namong non-privacy experts. To adapt to bandwidth constraints while preserving\nmodel performance, we propose a bandwidth-adaptive and importance-aware token\noffloading scheme. This scheme incorporates an importance predictor to evaluate\nthe importance scores of non-sensitive tokens, prioritizing the most important\ntokens for transmission to the base station based on their predicted importance\nand the available bandwidth. Experiments demonstrate that the PWC-MoE framework\neffectively preserves privacy and maintains high performance even in\nbandwidth-constrained environments, offering a practical solution for deploying\nLLMs in privacy-sensitive and bandwidth-limited scenarios.\n", "link": "http://arxiv.org/abs/2505.08719v1", "date": "2025-05-13", "relevancy": 1.4285, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4847}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4827}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PWC-MoE%3A%20Privacy-Aware%20Wireless%20Collaborative%20Mixture%20of%20Experts&body=Title%3A%20PWC-MoE%3A%20Privacy-Aware%20Wireless%20Collaborative%20Mixture%20of%20Experts%0AAuthor%3A%20Yang%20Su%20and%20Na%20Yan%20and%20Yansha%20Deng%20and%20Robert%20Schober%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20hosted%20on%20cloud%20servers%20alleviate%20the%0Acomputational%20and%20storage%20burdens%20on%20local%20devices%20but%20raise%20privacy%20concerns%0Adue%20to%20sensitive%20data%20transmission%20and%20require%20substantial%20communication%0Abandwidth%2C%20which%20is%20challenging%20in%20constrained%20environments.%20In%20contrast%2C%20small%0Alanguage%20models%20%28SLMs%29%20running%20locally%20enhance%20privacy%20but%20suffer%20from%20limited%0Aperformance%20on%20complex%20tasks.%20To%20balance%20computational%20cost%2C%20performance%2C%20and%0Aprivacy%20protection%20under%20bandwidth%20constraints%2C%20we%20propose%20a%20privacy-aware%0Awireless%20collaborative%20mixture%20of%20experts%20%28PWC-MoE%29%20framework.%20Specifically%2C%0APWC-MoE%20employs%20a%20sparse%20privacy-aware%20gating%20network%20to%20dynamically%20route%0Asensitive%20tokens%20to%20privacy%20experts%20located%20on%20local%20clients%2C%20while%0Anon-sensitive%20tokens%20are%20routed%20to%20non-privacy%20experts%20located%20at%20the%20remote%0Abase%20station.%20To%20achieve%20computational%20efficiency%2C%20the%20gating%20network%20ensures%0Athat%20each%20token%20is%20dynamically%20routed%20to%20and%20processed%20by%20only%20one%20expert.%20To%0Aenhance%20scalability%20and%20prevent%20overloading%20of%20specific%20experts%2C%20we%20introduce%20a%0Agroup-wise%20load-balancing%20mechanism%20for%20the%20gating%20network%20that%20evenly%0Adistributes%20sensitive%20tokens%20among%20privacy%20experts%20and%20non-sensitive%20tokens%0Aamong%20non-privacy%20experts.%20To%20adapt%20to%20bandwidth%20constraints%20while%20preserving%0Amodel%20performance%2C%20we%20propose%20a%20bandwidth-adaptive%20and%20importance-aware%20token%0Aoffloading%20scheme.%20This%20scheme%20incorporates%20an%20importance%20predictor%20to%20evaluate%0Athe%20importance%20scores%20of%20non-sensitive%20tokens%2C%20prioritizing%20the%20most%20important%0Atokens%20for%20transmission%20to%20the%20base%20station%20based%20on%20their%20predicted%20importance%0Aand%20the%20available%20bandwidth.%20Experiments%20demonstrate%20that%20the%20PWC-MoE%20framework%0Aeffectively%20preserves%20privacy%20and%20maintains%20high%20performance%20even%20in%0Abandwidth-constrained%20environments%2C%20offering%20a%20practical%20solution%20for%20deploying%0ALLMs%20in%20privacy-sensitive%20and%20bandwidth-limited%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPWC-MoE%253A%2520Privacy-Aware%2520Wireless%2520Collaborative%2520Mixture%2520of%2520Experts%26entry.906535625%3DYang%2520Su%2520and%2520Na%2520Yan%2520and%2520Yansha%2520Deng%2520and%2520Robert%2520Schober%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520hosted%2520on%2520cloud%2520servers%2520alleviate%2520the%250Acomputational%2520and%2520storage%2520burdens%2520on%2520local%2520devices%2520but%2520raise%2520privacy%2520concerns%250Adue%2520to%2520sensitive%2520data%2520transmission%2520and%2520require%2520substantial%2520communication%250Abandwidth%252C%2520which%2520is%2520challenging%2520in%2520constrained%2520environments.%2520In%2520contrast%252C%2520small%250Alanguage%2520models%2520%2528SLMs%2529%2520running%2520locally%2520enhance%2520privacy%2520but%2520suffer%2520from%2520limited%250Aperformance%2520on%2520complex%2520tasks.%2520To%2520balance%2520computational%2520cost%252C%2520performance%252C%2520and%250Aprivacy%2520protection%2520under%2520bandwidth%2520constraints%252C%2520we%2520propose%2520a%2520privacy-aware%250Awireless%2520collaborative%2520mixture%2520of%2520experts%2520%2528PWC-MoE%2529%2520framework.%2520Specifically%252C%250APWC-MoE%2520employs%2520a%2520sparse%2520privacy-aware%2520gating%2520network%2520to%2520dynamically%2520route%250Asensitive%2520tokens%2520to%2520privacy%2520experts%2520located%2520on%2520local%2520clients%252C%2520while%250Anon-sensitive%2520tokens%2520are%2520routed%2520to%2520non-privacy%2520experts%2520located%2520at%2520the%2520remote%250Abase%2520station.%2520To%2520achieve%2520computational%2520efficiency%252C%2520the%2520gating%2520network%2520ensures%250Athat%2520each%2520token%2520is%2520dynamically%2520routed%2520to%2520and%2520processed%2520by%2520only%2520one%2520expert.%2520To%250Aenhance%2520scalability%2520and%2520prevent%2520overloading%2520of%2520specific%2520experts%252C%2520we%2520introduce%2520a%250Agroup-wise%2520load-balancing%2520mechanism%2520for%2520the%2520gating%2520network%2520that%2520evenly%250Adistributes%2520sensitive%2520tokens%2520among%2520privacy%2520experts%2520and%2520non-sensitive%2520tokens%250Aamong%2520non-privacy%2520experts.%2520To%2520adapt%2520to%2520bandwidth%2520constraints%2520while%2520preserving%250Amodel%2520performance%252C%2520we%2520propose%2520a%2520bandwidth-adaptive%2520and%2520importance-aware%2520token%250Aoffloading%2520scheme.%2520This%2520scheme%2520incorporates%2520an%2520importance%2520predictor%2520to%2520evaluate%250Athe%2520importance%2520scores%2520of%2520non-sensitive%2520tokens%252C%2520prioritizing%2520the%2520most%2520important%250Atokens%2520for%2520transmission%2520to%2520the%2520base%2520station%2520based%2520on%2520their%2520predicted%2520importance%250Aand%2520the%2520available%2520bandwidth.%2520Experiments%2520demonstrate%2520that%2520the%2520PWC-MoE%2520framework%250Aeffectively%2520preserves%2520privacy%2520and%2520maintains%2520high%2520performance%2520even%2520in%250Abandwidth-constrained%2520environments%252C%2520offering%2520a%2520practical%2520solution%2520for%2520deploying%250ALLMs%2520in%2520privacy-sensitive%2520and%2520bandwidth-limited%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PWC-MoE%3A%20Privacy-Aware%20Wireless%20Collaborative%20Mixture%20of%20Experts&entry.906535625=Yang%20Su%20and%20Na%20Yan%20and%20Yansha%20Deng%20and%20Robert%20Schober&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20hosted%20on%20cloud%20servers%20alleviate%20the%0Acomputational%20and%20storage%20burdens%20on%20local%20devices%20but%20raise%20privacy%20concerns%0Adue%20to%20sensitive%20data%20transmission%20and%20require%20substantial%20communication%0Abandwidth%2C%20which%20is%20challenging%20in%20constrained%20environments.%20In%20contrast%2C%20small%0Alanguage%20models%20%28SLMs%29%20running%20locally%20enhance%20privacy%20but%20suffer%20from%20limited%0Aperformance%20on%20complex%20tasks.%20To%20balance%20computational%20cost%2C%20performance%2C%20and%0Aprivacy%20protection%20under%20bandwidth%20constraints%2C%20we%20propose%20a%20privacy-aware%0Awireless%20collaborative%20mixture%20of%20experts%20%28PWC-MoE%29%20framework.%20Specifically%2C%0APWC-MoE%20employs%20a%20sparse%20privacy-aware%20gating%20network%20to%20dynamically%20route%0Asensitive%20tokens%20to%20privacy%20experts%20located%20on%20local%20clients%2C%20while%0Anon-sensitive%20tokens%20are%20routed%20to%20non-privacy%20experts%20located%20at%20the%20remote%0Abase%20station.%20To%20achieve%20computational%20efficiency%2C%20the%20gating%20network%20ensures%0Athat%20each%20token%20is%20dynamically%20routed%20to%20and%20processed%20by%20only%20one%20expert.%20To%0Aenhance%20scalability%20and%20prevent%20overloading%20of%20specific%20experts%2C%20we%20introduce%20a%0Agroup-wise%20load-balancing%20mechanism%20for%20the%20gating%20network%20that%20evenly%0Adistributes%20sensitive%20tokens%20among%20privacy%20experts%20and%20non-sensitive%20tokens%0Aamong%20non-privacy%20experts.%20To%20adapt%20to%20bandwidth%20constraints%20while%20preserving%0Amodel%20performance%2C%20we%20propose%20a%20bandwidth-adaptive%20and%20importance-aware%20token%0Aoffloading%20scheme.%20This%20scheme%20incorporates%20an%20importance%20predictor%20to%20evaluate%0Athe%20importance%20scores%20of%20non-sensitive%20tokens%2C%20prioritizing%20the%20most%20important%0Atokens%20for%20transmission%20to%20the%20base%20station%20based%20on%20their%20predicted%20importance%0Aand%20the%20available%20bandwidth.%20Experiments%20demonstrate%20that%20the%20PWC-MoE%20framework%0Aeffectively%20preserves%20privacy%20and%20maintains%20high%20performance%20even%20in%0Abandwidth-constrained%20environments%2C%20offering%20a%20practical%20solution%20for%20deploying%0ALLMs%20in%20privacy-sensitive%20and%20bandwidth-limited%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08719v1&entry.124074799=Read"},
{"title": "Do You Trust Your Model? Emerging Malware Threats in the Deep Learning\n  Ecosystem", "author": "Dorjan Hitaj and Giulio Pagnotta and Fabio De Gaspari and Sediola Ruko and Briland Hitaj and Luigi V. Mancini and Fernando Perez-Cruz", "abstract": "  Training high-quality deep learning models is a challenging task due to\ncomputational and technical requirements. A growing number of individuals,\ninstitutions, and companies increasingly rely on pre-trained, third-party\nmodels made available in public repositories. These models are often used\ndirectly or integrated in product pipelines with no particular precautions,\nsince they are effectively just data in tensor form and considered safe. In\nthis paper, we raise awareness of a new machine learning supply chain threat\ntargeting neural networks. We introduce MaleficNet 2.0, a novel technique to\nembed self-extracting, self-executing malware in neural networks. MaleficNet\n2.0 uses spread-spectrum channel coding combined with error correction\ntechniques to inject malicious payloads in the parameters of deep neural\nnetworks. MaleficNet 2.0 injection technique is stealthy, does not degrade the\nperformance of the model, and is robust against removal techniques. We design\nour approach to work both in traditional and distributed learning settings such\nas Federated Learning, and demonstrate that it is effective even when a reduced\nnumber of bits is used for the model parameters. Finally, we implement a\nproof-of-concept self-extracting neural network malware using MaleficNet 2.0,\ndemonstrating the practicality of the attack against a widely adopted machine\nlearning framework. Our aim with this work is to raise awareness against these\nnew, dangerous attacks both in the research community and industry, and we hope\nto encourage further research in mitigation techniques against such threats.\n", "link": "http://arxiv.org/abs/2403.03593v2", "date": "2025-05-13", "relevancy": 1.4544, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4951}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4879}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20You%20Trust%20Your%20Model%3F%20Emerging%20Malware%20Threats%20in%20the%20Deep%20Learning%0A%20%20Ecosystem&body=Title%3A%20Do%20You%20Trust%20Your%20Model%3F%20Emerging%20Malware%20Threats%20in%20the%20Deep%20Learning%0A%20%20Ecosystem%0AAuthor%3A%20Dorjan%20Hitaj%20and%20Giulio%20Pagnotta%20and%20Fabio%20De%20Gaspari%20and%20Sediola%20Ruko%20and%20Briland%20Hitaj%20and%20Luigi%20V.%20Mancini%20and%20Fernando%20Perez-Cruz%0AAbstract%3A%20%20%20Training%20high-quality%20deep%20learning%20models%20is%20a%20challenging%20task%20due%20to%0Acomputational%20and%20technical%20requirements.%20A%20growing%20number%20of%20individuals%2C%0Ainstitutions%2C%20and%20companies%20increasingly%20rely%20on%20pre-trained%2C%20third-party%0Amodels%20made%20available%20in%20public%20repositories.%20These%20models%20are%20often%20used%0Adirectly%20or%20integrated%20in%20product%20pipelines%20with%20no%20particular%20precautions%2C%0Asince%20they%20are%20effectively%20just%20data%20in%20tensor%20form%20and%20considered%20safe.%20In%0Athis%20paper%2C%20we%20raise%20awareness%20of%20a%20new%20machine%20learning%20supply%20chain%20threat%0Atargeting%20neural%20networks.%20We%20introduce%20MaleficNet%202.0%2C%20a%20novel%20technique%20to%0Aembed%20self-extracting%2C%20self-executing%20malware%20in%20neural%20networks.%20MaleficNet%0A2.0%20uses%20spread-spectrum%20channel%20coding%20combined%20with%20error%20correction%0Atechniques%20to%20inject%20malicious%20payloads%20in%20the%20parameters%20of%20deep%20neural%0Anetworks.%20MaleficNet%202.0%20injection%20technique%20is%20stealthy%2C%20does%20not%20degrade%20the%0Aperformance%20of%20the%20model%2C%20and%20is%20robust%20against%20removal%20techniques.%20We%20design%0Aour%20approach%20to%20work%20both%20in%20traditional%20and%20distributed%20learning%20settings%20such%0Aas%20Federated%20Learning%2C%20and%20demonstrate%20that%20it%20is%20effective%20even%20when%20a%20reduced%0Anumber%20of%20bits%20is%20used%20for%20the%20model%20parameters.%20Finally%2C%20we%20implement%20a%0Aproof-of-concept%20self-extracting%20neural%20network%20malware%20using%20MaleficNet%202.0%2C%0Ademonstrating%20the%20practicality%20of%20the%20attack%20against%20a%20widely%20adopted%20machine%0Alearning%20framework.%20Our%20aim%20with%20this%20work%20is%20to%20raise%20awareness%20against%20these%0Anew%2C%20dangerous%20attacks%20both%20in%20the%20research%20community%20and%20industry%2C%20and%20we%20hope%0Ato%20encourage%20further%20research%20in%20mitigation%20techniques%20against%20such%20threats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520You%2520Trust%2520Your%2520Model%253F%2520Emerging%2520Malware%2520Threats%2520in%2520the%2520Deep%2520Learning%250A%2520%2520Ecosystem%26entry.906535625%3DDorjan%2520Hitaj%2520and%2520Giulio%2520Pagnotta%2520and%2520Fabio%2520De%2520Gaspari%2520and%2520Sediola%2520Ruko%2520and%2520Briland%2520Hitaj%2520and%2520Luigi%2520V.%2520Mancini%2520and%2520Fernando%2520Perez-Cruz%26entry.1292438233%3D%2520%2520Training%2520high-quality%2520deep%2520learning%2520models%2520is%2520a%2520challenging%2520task%2520due%2520to%250Acomputational%2520and%2520technical%2520requirements.%2520A%2520growing%2520number%2520of%2520individuals%252C%250Ainstitutions%252C%2520and%2520companies%2520increasingly%2520rely%2520on%2520pre-trained%252C%2520third-party%250Amodels%2520made%2520available%2520in%2520public%2520repositories.%2520These%2520models%2520are%2520often%2520used%250Adirectly%2520or%2520integrated%2520in%2520product%2520pipelines%2520with%2520no%2520particular%2520precautions%252C%250Asince%2520they%2520are%2520effectively%2520just%2520data%2520in%2520tensor%2520form%2520and%2520considered%2520safe.%2520In%250Athis%2520paper%252C%2520we%2520raise%2520awareness%2520of%2520a%2520new%2520machine%2520learning%2520supply%2520chain%2520threat%250Atargeting%2520neural%2520networks.%2520We%2520introduce%2520MaleficNet%25202.0%252C%2520a%2520novel%2520technique%2520to%250Aembed%2520self-extracting%252C%2520self-executing%2520malware%2520in%2520neural%2520networks.%2520MaleficNet%250A2.0%2520uses%2520spread-spectrum%2520channel%2520coding%2520combined%2520with%2520error%2520correction%250Atechniques%2520to%2520inject%2520malicious%2520payloads%2520in%2520the%2520parameters%2520of%2520deep%2520neural%250Anetworks.%2520MaleficNet%25202.0%2520injection%2520technique%2520is%2520stealthy%252C%2520does%2520not%2520degrade%2520the%250Aperformance%2520of%2520the%2520model%252C%2520and%2520is%2520robust%2520against%2520removal%2520techniques.%2520We%2520design%250Aour%2520approach%2520to%2520work%2520both%2520in%2520traditional%2520and%2520distributed%2520learning%2520settings%2520such%250Aas%2520Federated%2520Learning%252C%2520and%2520demonstrate%2520that%2520it%2520is%2520effective%2520even%2520when%2520a%2520reduced%250Anumber%2520of%2520bits%2520is%2520used%2520for%2520the%2520model%2520parameters.%2520Finally%252C%2520we%2520implement%2520a%250Aproof-of-concept%2520self-extracting%2520neural%2520network%2520malware%2520using%2520MaleficNet%25202.0%252C%250Ademonstrating%2520the%2520practicality%2520of%2520the%2520attack%2520against%2520a%2520widely%2520adopted%2520machine%250Alearning%2520framework.%2520Our%2520aim%2520with%2520this%2520work%2520is%2520to%2520raise%2520awareness%2520against%2520these%250Anew%252C%2520dangerous%2520attacks%2520both%2520in%2520the%2520research%2520community%2520and%2520industry%252C%2520and%2520we%2520hope%250Ato%2520encourage%2520further%2520research%2520in%2520mitigation%2520techniques%2520against%2520such%2520threats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20You%20Trust%20Your%20Model%3F%20Emerging%20Malware%20Threats%20in%20the%20Deep%20Learning%0A%20%20Ecosystem&entry.906535625=Dorjan%20Hitaj%20and%20Giulio%20Pagnotta%20and%20Fabio%20De%20Gaspari%20and%20Sediola%20Ruko%20and%20Briland%20Hitaj%20and%20Luigi%20V.%20Mancini%20and%20Fernando%20Perez-Cruz&entry.1292438233=%20%20Training%20high-quality%20deep%20learning%20models%20is%20a%20challenging%20task%20due%20to%0Acomputational%20and%20technical%20requirements.%20A%20growing%20number%20of%20individuals%2C%0Ainstitutions%2C%20and%20companies%20increasingly%20rely%20on%20pre-trained%2C%20third-party%0Amodels%20made%20available%20in%20public%20repositories.%20These%20models%20are%20often%20used%0Adirectly%20or%20integrated%20in%20product%20pipelines%20with%20no%20particular%20precautions%2C%0Asince%20they%20are%20effectively%20just%20data%20in%20tensor%20form%20and%20considered%20safe.%20In%0Athis%20paper%2C%20we%20raise%20awareness%20of%20a%20new%20machine%20learning%20supply%20chain%20threat%0Atargeting%20neural%20networks.%20We%20introduce%20MaleficNet%202.0%2C%20a%20novel%20technique%20to%0Aembed%20self-extracting%2C%20self-executing%20malware%20in%20neural%20networks.%20MaleficNet%0A2.0%20uses%20spread-spectrum%20channel%20coding%20combined%20with%20error%20correction%0Atechniques%20to%20inject%20malicious%20payloads%20in%20the%20parameters%20of%20deep%20neural%0Anetworks.%20MaleficNet%202.0%20injection%20technique%20is%20stealthy%2C%20does%20not%20degrade%20the%0Aperformance%20of%20the%20model%2C%20and%20is%20robust%20against%20removal%20techniques.%20We%20design%0Aour%20approach%20to%20work%20both%20in%20traditional%20and%20distributed%20learning%20settings%20such%0Aas%20Federated%20Learning%2C%20and%20demonstrate%20that%20it%20is%20effective%20even%20when%20a%20reduced%0Anumber%20of%20bits%20is%20used%20for%20the%20model%20parameters.%20Finally%2C%20we%20implement%20a%0Aproof-of-concept%20self-extracting%20neural%20network%20malware%20using%20MaleficNet%202.0%2C%0Ademonstrating%20the%20practicality%20of%20the%20attack%20against%20a%20widely%20adopted%20machine%0Alearning%20framework.%20Our%20aim%20with%20this%20work%20is%20to%20raise%20awareness%20against%20these%0Anew%2C%20dangerous%20attacks%20both%20in%20the%20research%20community%20and%20industry%2C%20and%20we%20hope%0Ato%20encourage%20further%20research%20in%20mitigation%20techniques%20against%20such%20threats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03593v2&entry.124074799=Read"},
{"title": "UniSkill: Imitating Human Videos via Cross-Embodiment Skill\n  Representations", "author": "Hanjung Kim and Jaehyun Kang and Hyolim Kang and Meedeum Cho and Seon Joo Kim and Youngwoon Lee", "abstract": "  Mimicry is a fundamental learning mechanism in humans, enabling individuals\nto learn new tasks by observing and imitating experts. However, applying this\nability to robots presents significant challenges due to the inherent\ndifferences between human and robot embodiments in both their visual appearance\nand physical capabilities. While previous methods bridge this gap using\ncross-embodiment datasets with shared scenes and tasks, collecting such aligned\ndata between humans and robots at scale is not trivial. In this paper, we\npropose UniSkill, a novel framework that learns embodiment-agnostic skill\nrepresentations from large-scale cross-embodiment video data without any\nlabels, enabling skills extracted from human video prompts to effectively\ntransfer to robot policies trained only on robot data. Our experiments in both\nsimulation and real-world environments show that our cross-embodiment skills\nsuccessfully guide robots in selecting appropriate actions, even with unseen\nvideo prompts. The project website can be found at:\nhttps://kimhanjung.github.io/UniSkill.\n", "link": "http://arxiv.org/abs/2505.08787v1", "date": "2025-05-13", "relevancy": 1.5903, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5426}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5378}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniSkill%3A%20Imitating%20Human%20Videos%20via%20Cross-Embodiment%20Skill%0A%20%20Representations&body=Title%3A%20UniSkill%3A%20Imitating%20Human%20Videos%20via%20Cross-Embodiment%20Skill%0A%20%20Representations%0AAuthor%3A%20Hanjung%20Kim%20and%20Jaehyun%20Kang%20and%20Hyolim%20Kang%20and%20Meedeum%20Cho%20and%20Seon%20Joo%20Kim%20and%20Youngwoon%20Lee%0AAbstract%3A%20%20%20Mimicry%20is%20a%20fundamental%20learning%20mechanism%20in%20humans%2C%20enabling%20individuals%0Ato%20learn%20new%20tasks%20by%20observing%20and%20imitating%20experts.%20However%2C%20applying%20this%0Aability%20to%20robots%20presents%20significant%20challenges%20due%20to%20the%20inherent%0Adifferences%20between%20human%20and%20robot%20embodiments%20in%20both%20their%20visual%20appearance%0Aand%20physical%20capabilities.%20While%20previous%20methods%20bridge%20this%20gap%20using%0Across-embodiment%20datasets%20with%20shared%20scenes%20and%20tasks%2C%20collecting%20such%20aligned%0Adata%20between%20humans%20and%20robots%20at%20scale%20is%20not%20trivial.%20In%20this%20paper%2C%20we%0Apropose%20UniSkill%2C%20a%20novel%20framework%20that%20learns%20embodiment-agnostic%20skill%0Arepresentations%20from%20large-scale%20cross-embodiment%20video%20data%20without%20any%0Alabels%2C%20enabling%20skills%20extracted%20from%20human%20video%20prompts%20to%20effectively%0Atransfer%20to%20robot%20policies%20trained%20only%20on%20robot%20data.%20Our%20experiments%20in%20both%0Asimulation%20and%20real-world%20environments%20show%20that%20our%20cross-embodiment%20skills%0Asuccessfully%20guide%20robots%20in%20selecting%20appropriate%20actions%2C%20even%20with%20unseen%0Avideo%20prompts.%20The%20project%20website%20can%20be%20found%20at%3A%0Ahttps%3A//kimhanjung.github.io/UniSkill.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniSkill%253A%2520Imitating%2520Human%2520Videos%2520via%2520Cross-Embodiment%2520Skill%250A%2520%2520Representations%26entry.906535625%3DHanjung%2520Kim%2520and%2520Jaehyun%2520Kang%2520and%2520Hyolim%2520Kang%2520and%2520Meedeum%2520Cho%2520and%2520Seon%2520Joo%2520Kim%2520and%2520Youngwoon%2520Lee%26entry.1292438233%3D%2520%2520Mimicry%2520is%2520a%2520fundamental%2520learning%2520mechanism%2520in%2520humans%252C%2520enabling%2520individuals%250Ato%2520learn%2520new%2520tasks%2520by%2520observing%2520and%2520imitating%2520experts.%2520However%252C%2520applying%2520this%250Aability%2520to%2520robots%2520presents%2520significant%2520challenges%2520due%2520to%2520the%2520inherent%250Adifferences%2520between%2520human%2520and%2520robot%2520embodiments%2520in%2520both%2520their%2520visual%2520appearance%250Aand%2520physical%2520capabilities.%2520While%2520previous%2520methods%2520bridge%2520this%2520gap%2520using%250Across-embodiment%2520datasets%2520with%2520shared%2520scenes%2520and%2520tasks%252C%2520collecting%2520such%2520aligned%250Adata%2520between%2520humans%2520and%2520robots%2520at%2520scale%2520is%2520not%2520trivial.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520UniSkill%252C%2520a%2520novel%2520framework%2520that%2520learns%2520embodiment-agnostic%2520skill%250Arepresentations%2520from%2520large-scale%2520cross-embodiment%2520video%2520data%2520without%2520any%250Alabels%252C%2520enabling%2520skills%2520extracted%2520from%2520human%2520video%2520prompts%2520to%2520effectively%250Atransfer%2520to%2520robot%2520policies%2520trained%2520only%2520on%2520robot%2520data.%2520Our%2520experiments%2520in%2520both%250Asimulation%2520and%2520real-world%2520environments%2520show%2520that%2520our%2520cross-embodiment%2520skills%250Asuccessfully%2520guide%2520robots%2520in%2520selecting%2520appropriate%2520actions%252C%2520even%2520with%2520unseen%250Avideo%2520prompts.%2520The%2520project%2520website%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//kimhanjung.github.io/UniSkill.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniSkill%3A%20Imitating%20Human%20Videos%20via%20Cross-Embodiment%20Skill%0A%20%20Representations&entry.906535625=Hanjung%20Kim%20and%20Jaehyun%20Kang%20and%20Hyolim%20Kang%20and%20Meedeum%20Cho%20and%20Seon%20Joo%20Kim%20and%20Youngwoon%20Lee&entry.1292438233=%20%20Mimicry%20is%20a%20fundamental%20learning%20mechanism%20in%20humans%2C%20enabling%20individuals%0Ato%20learn%20new%20tasks%20by%20observing%20and%20imitating%20experts.%20However%2C%20applying%20this%0Aability%20to%20robots%20presents%20significant%20challenges%20due%20to%20the%20inherent%0Adifferences%20between%20human%20and%20robot%20embodiments%20in%20both%20their%20visual%20appearance%0Aand%20physical%20capabilities.%20While%20previous%20methods%20bridge%20this%20gap%20using%0Across-embodiment%20datasets%20with%20shared%20scenes%20and%20tasks%2C%20collecting%20such%20aligned%0Adata%20between%20humans%20and%20robots%20at%20scale%20is%20not%20trivial.%20In%20this%20paper%2C%20we%0Apropose%20UniSkill%2C%20a%20novel%20framework%20that%20learns%20embodiment-agnostic%20skill%0Arepresentations%20from%20large-scale%20cross-embodiment%20video%20data%20without%20any%0Alabels%2C%20enabling%20skills%20extracted%20from%20human%20video%20prompts%20to%20effectively%0Atransfer%20to%20robot%20policies%20trained%20only%20on%20robot%20data.%20Our%20experiments%20in%20both%0Asimulation%20and%20real-world%20environments%20show%20that%20our%20cross-embodiment%20skills%0Asuccessfully%20guide%20robots%20in%20selecting%20appropriate%20actions%2C%20even%20with%20unseen%0Avideo%20prompts.%20The%20project%20website%20can%20be%20found%20at%3A%0Ahttps%3A//kimhanjung.github.io/UniSkill.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08787v1&entry.124074799=Read"},
{"title": "BAT: Benchmark for Auto-bidding Task", "author": "Alexandra Khirianova and Ekaterina Solodneva and Andrey Pudovikov and Sergey Osokin and Egor Samosvat and Yuriy Dorn and Alexander Ledovsky and Yana Zenkova", "abstract": "  The optimization of bidding strategies for online advertising slot auctions\npresents a critical challenge across numerous digital marketplaces. A\nsignificant obstacle to the development, evaluation, and refinement of\nreal-time autobidding algorithms is the scarcity of comprehensive datasets and\nstandardized benchmarks.\n  To address this deficiency, we present an auction benchmark encompassing the\ntwo most prevalent auction formats. We implement a series of robust baselines\non a novel dataset, addressing the most salient Real-Time Bidding (RTB) problem\ndomains: budget pacing uniformity and Cost Per Click (CPC) constraint\noptimization. This benchmark provides a user-friendly and intuitive framework\nfor researchers and practitioners to develop and refine innovative autobidding\nalgorithms, thereby facilitating advancements in the field of programmatic\nadvertising. The implementation and additional resources can be accessed at the\nfollowing repository (https://github.com/avito-tech/bat-autobidding-benchmark,\nhttps://doi.org/10.5281/zenodo.14794182).\n", "link": "http://arxiv.org/abs/2505.08485v1", "date": "2025-05-13", "relevancy": 1.7279, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4618}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.421}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BAT%3A%20Benchmark%20for%20Auto-bidding%20Task&body=Title%3A%20BAT%3A%20Benchmark%20for%20Auto-bidding%20Task%0AAuthor%3A%20Alexandra%20Khirianova%20and%20Ekaterina%20Solodneva%20and%20Andrey%20Pudovikov%20and%20Sergey%20Osokin%20and%20Egor%20Samosvat%20and%20Yuriy%20Dorn%20and%20Alexander%20Ledovsky%20and%20Yana%20Zenkova%0AAbstract%3A%20%20%20The%20optimization%20of%20bidding%20strategies%20for%20online%20advertising%20slot%20auctions%0Apresents%20a%20critical%20challenge%20across%20numerous%20digital%20marketplaces.%20A%0Asignificant%20obstacle%20to%20the%20development%2C%20evaluation%2C%20and%20refinement%20of%0Areal-time%20autobidding%20algorithms%20is%20the%20scarcity%20of%20comprehensive%20datasets%20and%0Astandardized%20benchmarks.%0A%20%20To%20address%20this%20deficiency%2C%20we%20present%20an%20auction%20benchmark%20encompassing%20the%0Atwo%20most%20prevalent%20auction%20formats.%20We%20implement%20a%20series%20of%20robust%20baselines%0Aon%20a%20novel%20dataset%2C%20addressing%20the%20most%20salient%20Real-Time%20Bidding%20%28RTB%29%20problem%0Adomains%3A%20budget%20pacing%20uniformity%20and%20Cost%20Per%20Click%20%28CPC%29%20constraint%0Aoptimization.%20This%20benchmark%20provides%20a%20user-friendly%20and%20intuitive%20framework%0Afor%20researchers%20and%20practitioners%20to%20develop%20and%20refine%20innovative%20autobidding%0Aalgorithms%2C%20thereby%20facilitating%20advancements%20in%20the%20field%20of%20programmatic%0Aadvertising.%20The%20implementation%20and%20additional%20resources%20can%20be%20accessed%20at%20the%0Afollowing%20repository%20%28https%3A//github.com/avito-tech/bat-autobidding-benchmark%2C%0Ahttps%3A//doi.org/10.5281/zenodo.14794182%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBAT%253A%2520Benchmark%2520for%2520Auto-bidding%2520Task%26entry.906535625%3DAlexandra%2520Khirianova%2520and%2520Ekaterina%2520Solodneva%2520and%2520Andrey%2520Pudovikov%2520and%2520Sergey%2520Osokin%2520and%2520Egor%2520Samosvat%2520and%2520Yuriy%2520Dorn%2520and%2520Alexander%2520Ledovsky%2520and%2520Yana%2520Zenkova%26entry.1292438233%3D%2520%2520The%2520optimization%2520of%2520bidding%2520strategies%2520for%2520online%2520advertising%2520slot%2520auctions%250Apresents%2520a%2520critical%2520challenge%2520across%2520numerous%2520digital%2520marketplaces.%2520A%250Asignificant%2520obstacle%2520to%2520the%2520development%252C%2520evaluation%252C%2520and%2520refinement%2520of%250Areal-time%2520autobidding%2520algorithms%2520is%2520the%2520scarcity%2520of%2520comprehensive%2520datasets%2520and%250Astandardized%2520benchmarks.%250A%2520%2520To%2520address%2520this%2520deficiency%252C%2520we%2520present%2520an%2520auction%2520benchmark%2520encompassing%2520the%250Atwo%2520most%2520prevalent%2520auction%2520formats.%2520We%2520implement%2520a%2520series%2520of%2520robust%2520baselines%250Aon%2520a%2520novel%2520dataset%252C%2520addressing%2520the%2520most%2520salient%2520Real-Time%2520Bidding%2520%2528RTB%2529%2520problem%250Adomains%253A%2520budget%2520pacing%2520uniformity%2520and%2520Cost%2520Per%2520Click%2520%2528CPC%2529%2520constraint%250Aoptimization.%2520This%2520benchmark%2520provides%2520a%2520user-friendly%2520and%2520intuitive%2520framework%250Afor%2520researchers%2520and%2520practitioners%2520to%2520develop%2520and%2520refine%2520innovative%2520autobidding%250Aalgorithms%252C%2520thereby%2520facilitating%2520advancements%2520in%2520the%2520field%2520of%2520programmatic%250Aadvertising.%2520The%2520implementation%2520and%2520additional%2520resources%2520can%2520be%2520accessed%2520at%2520the%250Afollowing%2520repository%2520%2528https%253A//github.com/avito-tech/bat-autobidding-benchmark%252C%250Ahttps%253A//doi.org/10.5281/zenodo.14794182%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAT%3A%20Benchmark%20for%20Auto-bidding%20Task&entry.906535625=Alexandra%20Khirianova%20and%20Ekaterina%20Solodneva%20and%20Andrey%20Pudovikov%20and%20Sergey%20Osokin%20and%20Egor%20Samosvat%20and%20Yuriy%20Dorn%20and%20Alexander%20Ledovsky%20and%20Yana%20Zenkova&entry.1292438233=%20%20The%20optimization%20of%20bidding%20strategies%20for%20online%20advertising%20slot%20auctions%0Apresents%20a%20critical%20challenge%20across%20numerous%20digital%20marketplaces.%20A%0Asignificant%20obstacle%20to%20the%20development%2C%20evaluation%2C%20and%20refinement%20of%0Areal-time%20autobidding%20algorithms%20is%20the%20scarcity%20of%20comprehensive%20datasets%20and%0Astandardized%20benchmarks.%0A%20%20To%20address%20this%20deficiency%2C%20we%20present%20an%20auction%20benchmark%20encompassing%20the%0Atwo%20most%20prevalent%20auction%20formats.%20We%20implement%20a%20series%20of%20robust%20baselines%0Aon%20a%20novel%20dataset%2C%20addressing%20the%20most%20salient%20Real-Time%20Bidding%20%28RTB%29%20problem%0Adomains%3A%20budget%20pacing%20uniformity%20and%20Cost%20Per%20Click%20%28CPC%29%20constraint%0Aoptimization.%20This%20benchmark%20provides%20a%20user-friendly%20and%20intuitive%20framework%0Afor%20researchers%20and%20practitioners%20to%20develop%20and%20refine%20innovative%20autobidding%0Aalgorithms%2C%20thereby%20facilitating%20advancements%20in%20the%20field%20of%20programmatic%0Aadvertising.%20The%20implementation%20and%20additional%20resources%20can%20be%20accessed%20at%20the%0Afollowing%20repository%20%28https%3A//github.com/avito-tech/bat-autobidding-benchmark%2C%0Ahttps%3A//doi.org/10.5281/zenodo.14794182%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08485v1&entry.124074799=Read"},
{"title": "Understanding molecular ratios in the carbon and oxygen poor outer Milky\n  Way with interpretable machine learning", "author": "Gijs Vermari\u00ebn and Serena Viti and Johannes Heyl and Francesco Fontani", "abstract": "  Context. The outer Milky Way has a lower metallicity than our solar\nneighbourhood, but still many molecules are detected in the region. Molecular\nline ratios can serve as probes to better understand the chemistry and physics\nin these regions. Aims. We use interpretable machine learning to study 9\ndifferent molecular ratios, helping us understand the forward connection\nbetween the physics of these environments and the carbon and oxygen\nchemistries. Methods. Using a large grid of astrochemical models generated\nusing UCLCHEM, we study the properties of molecular clouds of low oxygen and\ncarbon initial abundance. We first try to understand the line ratios using a\nclassical analysis. We then move on to using interpretable machine learning,\nnamely Shapley Additive Explanations (SHAP), to understand the higher order\ndependencies of the ratios over the entire parameter grid. Lastly we use the\nUniform Manifold Approximation and Projection technique (UMAP) as a reduction\nmethod to create intuitive groupings of models. Results. We find that the\nparameter space is well covered by the line ratios, allowing us to investigate\nall input parameters. SHAP analysis shows that the temperature and density are\nthe most important features, but the carbon and oxygen abundances are important\nin parts of the parameter space. Lastly, we find that we can group different\ntypes of ratios using UMAP. Conclusions. We show the chosen ratios are mostly\nsensitive to changes in the carbon initial abundance, together with the\ntemperature and density. Especially the CN/HCN and HNC/HCN ratio are shown to\nbe sensitive to the initial carbon abundance, making them excellent probes for\nthis parameter. Out of the ratios, only CS/SO shows a sensitivity to the oxygen\nabundance.\n", "link": "http://arxiv.org/abs/2505.08410v1", "date": "2025-05-13", "relevancy": 1.779, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20molecular%20ratios%20in%20the%20carbon%20and%20oxygen%20poor%20outer%20Milky%0A%20%20Way%20with%20interpretable%20machine%20learning&body=Title%3A%20Understanding%20molecular%20ratios%20in%20the%20carbon%20and%20oxygen%20poor%20outer%20Milky%0A%20%20Way%20with%20interpretable%20machine%20learning%0AAuthor%3A%20Gijs%20Vermari%C3%ABn%20and%20Serena%20Viti%20and%20Johannes%20Heyl%20and%20Francesco%20Fontani%0AAbstract%3A%20%20%20Context.%20The%20outer%20Milky%20Way%20has%20a%20lower%20metallicity%20than%20our%20solar%0Aneighbourhood%2C%20but%20still%20many%20molecules%20are%20detected%20in%20the%20region.%20Molecular%0Aline%20ratios%20can%20serve%20as%20probes%20to%20better%20understand%20the%20chemistry%20and%20physics%0Ain%20these%20regions.%20Aims.%20We%20use%20interpretable%20machine%20learning%20to%20study%209%0Adifferent%20molecular%20ratios%2C%20helping%20us%20understand%20the%20forward%20connection%0Abetween%20the%20physics%20of%20these%20environments%20and%20the%20carbon%20and%20oxygen%0Achemistries.%20Methods.%20Using%20a%20large%20grid%20of%20astrochemical%20models%20generated%0Ausing%20UCLCHEM%2C%20we%20study%20the%20properties%20of%20molecular%20clouds%20of%20low%20oxygen%20and%0Acarbon%20initial%20abundance.%20We%20first%20try%20to%20understand%20the%20line%20ratios%20using%20a%0Aclassical%20analysis.%20We%20then%20move%20on%20to%20using%20interpretable%20machine%20learning%2C%0Anamely%20Shapley%20Additive%20Explanations%20%28SHAP%29%2C%20to%20understand%20the%20higher%20order%0Adependencies%20of%20the%20ratios%20over%20the%20entire%20parameter%20grid.%20Lastly%20we%20use%20the%0AUniform%20Manifold%20Approximation%20and%20Projection%20technique%20%28UMAP%29%20as%20a%20reduction%0Amethod%20to%20create%20intuitive%20groupings%20of%20models.%20Results.%20We%20find%20that%20the%0Aparameter%20space%20is%20well%20covered%20by%20the%20line%20ratios%2C%20allowing%20us%20to%20investigate%0Aall%20input%20parameters.%20SHAP%20analysis%20shows%20that%20the%20temperature%20and%20density%20are%0Athe%20most%20important%20features%2C%20but%20the%20carbon%20and%20oxygen%20abundances%20are%20important%0Ain%20parts%20of%20the%20parameter%20space.%20Lastly%2C%20we%20find%20that%20we%20can%20group%20different%0Atypes%20of%20ratios%20using%20UMAP.%20Conclusions.%20We%20show%20the%20chosen%20ratios%20are%20mostly%0Asensitive%20to%20changes%20in%20the%20carbon%20initial%20abundance%2C%20together%20with%20the%0Atemperature%20and%20density.%20Especially%20the%20CN/HCN%20and%20HNC/HCN%20ratio%20are%20shown%20to%0Abe%20sensitive%20to%20the%20initial%20carbon%20abundance%2C%20making%20them%20excellent%20probes%20for%0Athis%20parameter.%20Out%20of%20the%20ratios%2C%20only%20CS/SO%20shows%20a%20sensitivity%20to%20the%20oxygen%0Aabundance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520molecular%2520ratios%2520in%2520the%2520carbon%2520and%2520oxygen%2520poor%2520outer%2520Milky%250A%2520%2520Way%2520with%2520interpretable%2520machine%2520learning%26entry.906535625%3DGijs%2520Vermari%25C3%25ABn%2520and%2520Serena%2520Viti%2520and%2520Johannes%2520Heyl%2520and%2520Francesco%2520Fontani%26entry.1292438233%3D%2520%2520Context.%2520The%2520outer%2520Milky%2520Way%2520has%2520a%2520lower%2520metallicity%2520than%2520our%2520solar%250Aneighbourhood%252C%2520but%2520still%2520many%2520molecules%2520are%2520detected%2520in%2520the%2520region.%2520Molecular%250Aline%2520ratios%2520can%2520serve%2520as%2520probes%2520to%2520better%2520understand%2520the%2520chemistry%2520and%2520physics%250Ain%2520these%2520regions.%2520Aims.%2520We%2520use%2520interpretable%2520machine%2520learning%2520to%2520study%25209%250Adifferent%2520molecular%2520ratios%252C%2520helping%2520us%2520understand%2520the%2520forward%2520connection%250Abetween%2520the%2520physics%2520of%2520these%2520environments%2520and%2520the%2520carbon%2520and%2520oxygen%250Achemistries.%2520Methods.%2520Using%2520a%2520large%2520grid%2520of%2520astrochemical%2520models%2520generated%250Ausing%2520UCLCHEM%252C%2520we%2520study%2520the%2520properties%2520of%2520molecular%2520clouds%2520of%2520low%2520oxygen%2520and%250Acarbon%2520initial%2520abundance.%2520We%2520first%2520try%2520to%2520understand%2520the%2520line%2520ratios%2520using%2520a%250Aclassical%2520analysis.%2520We%2520then%2520move%2520on%2520to%2520using%2520interpretable%2520machine%2520learning%252C%250Anamely%2520Shapley%2520Additive%2520Explanations%2520%2528SHAP%2529%252C%2520to%2520understand%2520the%2520higher%2520order%250Adependencies%2520of%2520the%2520ratios%2520over%2520the%2520entire%2520parameter%2520grid.%2520Lastly%2520we%2520use%2520the%250AUniform%2520Manifold%2520Approximation%2520and%2520Projection%2520technique%2520%2528UMAP%2529%2520as%2520a%2520reduction%250Amethod%2520to%2520create%2520intuitive%2520groupings%2520of%2520models.%2520Results.%2520We%2520find%2520that%2520the%250Aparameter%2520space%2520is%2520well%2520covered%2520by%2520the%2520line%2520ratios%252C%2520allowing%2520us%2520to%2520investigate%250Aall%2520input%2520parameters.%2520SHAP%2520analysis%2520shows%2520that%2520the%2520temperature%2520and%2520density%2520are%250Athe%2520most%2520important%2520features%252C%2520but%2520the%2520carbon%2520and%2520oxygen%2520abundances%2520are%2520important%250Ain%2520parts%2520of%2520the%2520parameter%2520space.%2520Lastly%252C%2520we%2520find%2520that%2520we%2520can%2520group%2520different%250Atypes%2520of%2520ratios%2520using%2520UMAP.%2520Conclusions.%2520We%2520show%2520the%2520chosen%2520ratios%2520are%2520mostly%250Asensitive%2520to%2520changes%2520in%2520the%2520carbon%2520initial%2520abundance%252C%2520together%2520with%2520the%250Atemperature%2520and%2520density.%2520Especially%2520the%2520CN/HCN%2520and%2520HNC/HCN%2520ratio%2520are%2520shown%2520to%250Abe%2520sensitive%2520to%2520the%2520initial%2520carbon%2520abundance%252C%2520making%2520them%2520excellent%2520probes%2520for%250Athis%2520parameter.%2520Out%2520of%2520the%2520ratios%252C%2520only%2520CS/SO%2520shows%2520a%2520sensitivity%2520to%2520the%2520oxygen%250Aabundance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20molecular%20ratios%20in%20the%20carbon%20and%20oxygen%20poor%20outer%20Milky%0A%20%20Way%20with%20interpretable%20machine%20learning&entry.906535625=Gijs%20Vermari%C3%ABn%20and%20Serena%20Viti%20and%20Johannes%20Heyl%20and%20Francesco%20Fontani&entry.1292438233=%20%20Context.%20The%20outer%20Milky%20Way%20has%20a%20lower%20metallicity%20than%20our%20solar%0Aneighbourhood%2C%20but%20still%20many%20molecules%20are%20detected%20in%20the%20region.%20Molecular%0Aline%20ratios%20can%20serve%20as%20probes%20to%20better%20understand%20the%20chemistry%20and%20physics%0Ain%20these%20regions.%20Aims.%20We%20use%20interpretable%20machine%20learning%20to%20study%209%0Adifferent%20molecular%20ratios%2C%20helping%20us%20understand%20the%20forward%20connection%0Abetween%20the%20physics%20of%20these%20environments%20and%20the%20carbon%20and%20oxygen%0Achemistries.%20Methods.%20Using%20a%20large%20grid%20of%20astrochemical%20models%20generated%0Ausing%20UCLCHEM%2C%20we%20study%20the%20properties%20of%20molecular%20clouds%20of%20low%20oxygen%20and%0Acarbon%20initial%20abundance.%20We%20first%20try%20to%20understand%20the%20line%20ratios%20using%20a%0Aclassical%20analysis.%20We%20then%20move%20on%20to%20using%20interpretable%20machine%20learning%2C%0Anamely%20Shapley%20Additive%20Explanations%20%28SHAP%29%2C%20to%20understand%20the%20higher%20order%0Adependencies%20of%20the%20ratios%20over%20the%20entire%20parameter%20grid.%20Lastly%20we%20use%20the%0AUniform%20Manifold%20Approximation%20and%20Projection%20technique%20%28UMAP%29%20as%20a%20reduction%0Amethod%20to%20create%20intuitive%20groupings%20of%20models.%20Results.%20We%20find%20that%20the%0Aparameter%20space%20is%20well%20covered%20by%20the%20line%20ratios%2C%20allowing%20us%20to%20investigate%0Aall%20input%20parameters.%20SHAP%20analysis%20shows%20that%20the%20temperature%20and%20density%20are%0Athe%20most%20important%20features%2C%20but%20the%20carbon%20and%20oxygen%20abundances%20are%20important%0Ain%20parts%20of%20the%20parameter%20space.%20Lastly%2C%20we%20find%20that%20we%20can%20group%20different%0Atypes%20of%20ratios%20using%20UMAP.%20Conclusions.%20We%20show%20the%20chosen%20ratios%20are%20mostly%0Asensitive%20to%20changes%20in%20the%20carbon%20initial%20abundance%2C%20together%20with%20the%0Atemperature%20and%20density.%20Especially%20the%20CN/HCN%20and%20HNC/HCN%20ratio%20are%20shown%20to%0Abe%20sensitive%20to%20the%20initial%20carbon%20abundance%2C%20making%20them%20excellent%20probes%20for%0Athis%20parameter.%20Out%20of%20the%20ratios%2C%20only%20CS/SO%20shows%20a%20sensitivity%20to%20the%20oxygen%0Aabundance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08410v1&entry.124074799=Read"},
{"title": "Online Learning and Unlearning", "author": "Yaxi Hu and Bernhard Sch\u00f6lkopf and Amartya Sanyal", "abstract": "  We formalize the problem of online learning-unlearning, where a model is\nupdated sequentially in an online setting while accommodating unlearning\nrequests between updates. After a data point is unlearned, all subsequent\noutputs must be statistically indistinguishable from those of a model trained\nwithout that point. We present two online learner-unlearner (OLU) algorithms,\nboth built upon online gradient descent (OGD). The first, passive OLU,\nleverages OGD's contractive property and injects noise when unlearning occurs,\nincurring no additional computation. The second, active OLU, uses an offline\nunlearning algorithm that shifts the model toward a solution excluding the\ndeleted data. Under standard convexity and smoothness assumptions, both methods\nachieve regret bounds comparable to those of standard OGD, demonstrating that\none can maintain competitive regret bounds while providing unlearning\nguarantees.\n", "link": "http://arxiv.org/abs/2505.08557v1", "date": "2025-05-13", "relevancy": 1.935, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4983}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4938}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Learning%20and%20Unlearning&body=Title%3A%20Online%20Learning%20and%20Unlearning%0AAuthor%3A%20Yaxi%20Hu%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Amartya%20Sanyal%0AAbstract%3A%20%20%20We%20formalize%20the%20problem%20of%20online%20learning-unlearning%2C%20where%20a%20model%20is%0Aupdated%20sequentially%20in%20an%20online%20setting%20while%20accommodating%20unlearning%0Arequests%20between%20updates.%20After%20a%20data%20point%20is%20unlearned%2C%20all%20subsequent%0Aoutputs%20must%20be%20statistically%20indistinguishable%20from%20those%20of%20a%20model%20trained%0Awithout%20that%20point.%20We%20present%20two%20online%20learner-unlearner%20%28OLU%29%20algorithms%2C%0Aboth%20built%20upon%20online%20gradient%20descent%20%28OGD%29.%20The%20first%2C%20passive%20OLU%2C%0Aleverages%20OGD%27s%20contractive%20property%20and%20injects%20noise%20when%20unlearning%20occurs%2C%0Aincurring%20no%20additional%20computation.%20The%20second%2C%20active%20OLU%2C%20uses%20an%20offline%0Aunlearning%20algorithm%20that%20shifts%20the%20model%20toward%20a%20solution%20excluding%20the%0Adeleted%20data.%20Under%20standard%20convexity%20and%20smoothness%20assumptions%2C%20both%20methods%0Aachieve%20regret%20bounds%20comparable%20to%20those%20of%20standard%20OGD%2C%20demonstrating%20that%0Aone%20can%20maintain%20competitive%20regret%20bounds%20while%20providing%20unlearning%0Aguarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Learning%2520and%2520Unlearning%26entry.906535625%3DYaxi%2520Hu%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Amartya%2520Sanyal%26entry.1292438233%3D%2520%2520We%2520formalize%2520the%2520problem%2520of%2520online%2520learning-unlearning%252C%2520where%2520a%2520model%2520is%250Aupdated%2520sequentially%2520in%2520an%2520online%2520setting%2520while%2520accommodating%2520unlearning%250Arequests%2520between%2520updates.%2520After%2520a%2520data%2520point%2520is%2520unlearned%252C%2520all%2520subsequent%250Aoutputs%2520must%2520be%2520statistically%2520indistinguishable%2520from%2520those%2520of%2520a%2520model%2520trained%250Awithout%2520that%2520point.%2520We%2520present%2520two%2520online%2520learner-unlearner%2520%2528OLU%2529%2520algorithms%252C%250Aboth%2520built%2520upon%2520online%2520gradient%2520descent%2520%2528OGD%2529.%2520The%2520first%252C%2520passive%2520OLU%252C%250Aleverages%2520OGD%2527s%2520contractive%2520property%2520and%2520injects%2520noise%2520when%2520unlearning%2520occurs%252C%250Aincurring%2520no%2520additional%2520computation.%2520The%2520second%252C%2520active%2520OLU%252C%2520uses%2520an%2520offline%250Aunlearning%2520algorithm%2520that%2520shifts%2520the%2520model%2520toward%2520a%2520solution%2520excluding%2520the%250Adeleted%2520data.%2520Under%2520standard%2520convexity%2520and%2520smoothness%2520assumptions%252C%2520both%2520methods%250Aachieve%2520regret%2520bounds%2520comparable%2520to%2520those%2520of%2520standard%2520OGD%252C%2520demonstrating%2520that%250Aone%2520can%2520maintain%2520competitive%2520regret%2520bounds%2520while%2520providing%2520unlearning%250Aguarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Learning%20and%20Unlearning&entry.906535625=Yaxi%20Hu%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Amartya%20Sanyal&entry.1292438233=%20%20We%20formalize%20the%20problem%20of%20online%20learning-unlearning%2C%20where%20a%20model%20is%0Aupdated%20sequentially%20in%20an%20online%20setting%20while%20accommodating%20unlearning%0Arequests%20between%20updates.%20After%20a%20data%20point%20is%20unlearned%2C%20all%20subsequent%0Aoutputs%20must%20be%20statistically%20indistinguishable%20from%20those%20of%20a%20model%20trained%0Awithout%20that%20point.%20We%20present%20two%20online%20learner-unlearner%20%28OLU%29%20algorithms%2C%0Aboth%20built%20upon%20online%20gradient%20descent%20%28OGD%29.%20The%20first%2C%20passive%20OLU%2C%0Aleverages%20OGD%27s%20contractive%20property%20and%20injects%20noise%20when%20unlearning%20occurs%2C%0Aincurring%20no%20additional%20computation.%20The%20second%2C%20active%20OLU%2C%20uses%20an%20offline%0Aunlearning%20algorithm%20that%20shifts%20the%20model%20toward%20a%20solution%20excluding%20the%0Adeleted%20data.%20Under%20standard%20convexity%20and%20smoothness%20assumptions%2C%20both%20methods%0Aachieve%20regret%20bounds%20comparable%20to%20those%20of%20standard%20OGD%2C%20demonstrating%20that%0Aone%20can%20maintain%20competitive%20regret%20bounds%20while%20providing%20unlearning%0Aguarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08557v1&entry.124074799=Read"},
{"title": "SafeMate: A Modular RAG-Based Agent for Context-Aware Emergency Guidance", "author": "Junfeng Jiao and Jihyung Park and Yiming Xu and Kristen Sussman and Lucy Atkinson", "abstract": "  Despite the abundance of public safety documents and emergency protocols,\nmost individuals remain ill-equipped to interpret and act on such information\nduring crises. Traditional emergency decision support systems (EDSS) are\ndesigned for professionals and rely heavily on static documents like PDFs or\nSOPs, which are difficult for non-experts to navigate under stress. This gap\nbetween institutional knowledge and public accessibility poses a critical\nbarrier to effective emergency preparedness and response.\n  We introduce SafeMate, a retrieval-augmented AI assistant that delivers\naccurate, context-aware guidance to general users in both preparedness and\nactive emergency scenarios. Built on the Model Context Protocol (MCP), SafeMate\ndynamically routes user queries to tools for document retrieval, checklist\ngeneration, and structured summarization. It uses FAISS with cosine similarity\nto identify relevant content from trusted sources.\n", "link": "http://arxiv.org/abs/2505.02306v2", "date": "2025-05-13", "relevancy": 1.4906, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5471}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4935}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafeMate%3A%20A%20Modular%20RAG-Based%20Agent%20for%20Context-Aware%20Emergency%20Guidance&body=Title%3A%20SafeMate%3A%20A%20Modular%20RAG-Based%20Agent%20for%20Context-Aware%20Emergency%20Guidance%0AAuthor%3A%20Junfeng%20Jiao%20and%20Jihyung%20Park%20and%20Yiming%20Xu%20and%20Kristen%20Sussman%20and%20Lucy%20Atkinson%0AAbstract%3A%20%20%20Despite%20the%20abundance%20of%20public%20safety%20documents%20and%20emergency%20protocols%2C%0Amost%20individuals%20remain%20ill-equipped%20to%20interpret%20and%20act%20on%20such%20information%0Aduring%20crises.%20Traditional%20emergency%20decision%20support%20systems%20%28EDSS%29%20are%0Adesigned%20for%20professionals%20and%20rely%20heavily%20on%20static%20documents%20like%20PDFs%20or%0ASOPs%2C%20which%20are%20difficult%20for%20non-experts%20to%20navigate%20under%20stress.%20This%20gap%0Abetween%20institutional%20knowledge%20and%20public%20accessibility%20poses%20a%20critical%0Abarrier%20to%20effective%20emergency%20preparedness%20and%20response.%0A%20%20We%20introduce%20SafeMate%2C%20a%20retrieval-augmented%20AI%20assistant%20that%20delivers%0Aaccurate%2C%20context-aware%20guidance%20to%20general%20users%20in%20both%20preparedness%20and%0Aactive%20emergency%20scenarios.%20Built%20on%20the%20Model%20Context%20Protocol%20%28MCP%29%2C%20SafeMate%0Adynamically%20routes%20user%20queries%20to%20tools%20for%20document%20retrieval%2C%20checklist%0Ageneration%2C%20and%20structured%20summarization.%20It%20uses%20FAISS%20with%20cosine%20similarity%0Ato%20identify%20relevant%20content%20from%20trusted%20sources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02306v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeMate%253A%2520A%2520Modular%2520RAG-Based%2520Agent%2520for%2520Context-Aware%2520Emergency%2520Guidance%26entry.906535625%3DJunfeng%2520Jiao%2520and%2520Jihyung%2520Park%2520and%2520Yiming%2520Xu%2520and%2520Kristen%2520Sussman%2520and%2520Lucy%2520Atkinson%26entry.1292438233%3D%2520%2520Despite%2520the%2520abundance%2520of%2520public%2520safety%2520documents%2520and%2520emergency%2520protocols%252C%250Amost%2520individuals%2520remain%2520ill-equipped%2520to%2520interpret%2520and%2520act%2520on%2520such%2520information%250Aduring%2520crises.%2520Traditional%2520emergency%2520decision%2520support%2520systems%2520%2528EDSS%2529%2520are%250Adesigned%2520for%2520professionals%2520and%2520rely%2520heavily%2520on%2520static%2520documents%2520like%2520PDFs%2520or%250ASOPs%252C%2520which%2520are%2520difficult%2520for%2520non-experts%2520to%2520navigate%2520under%2520stress.%2520This%2520gap%250Abetween%2520institutional%2520knowledge%2520and%2520public%2520accessibility%2520poses%2520a%2520critical%250Abarrier%2520to%2520effective%2520emergency%2520preparedness%2520and%2520response.%250A%2520%2520We%2520introduce%2520SafeMate%252C%2520a%2520retrieval-augmented%2520AI%2520assistant%2520that%2520delivers%250Aaccurate%252C%2520context-aware%2520guidance%2520to%2520general%2520users%2520in%2520both%2520preparedness%2520and%250Aactive%2520emergency%2520scenarios.%2520Built%2520on%2520the%2520Model%2520Context%2520Protocol%2520%2528MCP%2529%252C%2520SafeMate%250Adynamically%2520routes%2520user%2520queries%2520to%2520tools%2520for%2520document%2520retrieval%252C%2520checklist%250Ageneration%252C%2520and%2520structured%2520summarization.%2520It%2520uses%2520FAISS%2520with%2520cosine%2520similarity%250Ato%2520identify%2520relevant%2520content%2520from%2520trusted%2520sources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02306v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafeMate%3A%20A%20Modular%20RAG-Based%20Agent%20for%20Context-Aware%20Emergency%20Guidance&entry.906535625=Junfeng%20Jiao%20and%20Jihyung%20Park%20and%20Yiming%20Xu%20and%20Kristen%20Sussman%20and%20Lucy%20Atkinson&entry.1292438233=%20%20Despite%20the%20abundance%20of%20public%20safety%20documents%20and%20emergency%20protocols%2C%0Amost%20individuals%20remain%20ill-equipped%20to%20interpret%20and%20act%20on%20such%20information%0Aduring%20crises.%20Traditional%20emergency%20decision%20support%20systems%20%28EDSS%29%20are%0Adesigned%20for%20professionals%20and%20rely%20heavily%20on%20static%20documents%20like%20PDFs%20or%0ASOPs%2C%20which%20are%20difficult%20for%20non-experts%20to%20navigate%20under%20stress.%20This%20gap%0Abetween%20institutional%20knowledge%20and%20public%20accessibility%20poses%20a%20critical%0Abarrier%20to%20effective%20emergency%20preparedness%20and%20response.%0A%20%20We%20introduce%20SafeMate%2C%20a%20retrieval-augmented%20AI%20assistant%20that%20delivers%0Aaccurate%2C%20context-aware%20guidance%20to%20general%20users%20in%20both%20preparedness%20and%0Aactive%20emergency%20scenarios.%20Built%20on%20the%20Model%20Context%20Protocol%20%28MCP%29%2C%20SafeMate%0Adynamically%20routes%20user%20queries%20to%20tools%20for%20document%20retrieval%2C%20checklist%0Ageneration%2C%20and%20structured%20summarization.%20It%20uses%20FAISS%20with%20cosine%20similarity%0Ato%20identify%20relevant%20content%20from%20trusted%20sources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02306v2&entry.124074799=Read"},
{"title": "Transfer Learning of Surrogate Models: Integrating Domain Warping and\n  Affine Transformations", "author": "Shuaiqun Pan and Diederick Vermetten and Manuel L\u00f3pez-Ib\u00e1\u00f1ez and Thomas B\u00e4ck and Hao Wang", "abstract": "  Surrogate models provide efficient alternatives to computationally demanding\nreal world processes but often require large datasets for effective training. A\npromising solution to this limitation is the transfer of pre-trained surrogate\nmodels to new tasks. Previous studies have investigated the transfer of\ndifferentiable and non-differentiable surrogate models, typically assuming an\naffine transformation between the source and target functions. This paper\nextends previous research by addressing a broader range of transformations,\nincluding linear and nonlinear variations. Specifically, we consider the\ncombination of an unknown input warping, such as one modeled by the beta\ncumulative distribution function, with an unspecified affine transformation.\nOur approach achieves transfer learning by employing a limited number of data\npoints from the target task to optimize these transformations, minimizing\nempirical loss on the transfer dataset. We validate the proposed method on the\nwidely used Black-Box Optimization Benchmark (BBOB) testbed and a real-world\ntransfer learning task from the automobile industry. The results underscore the\nsignificant advantages of the approach, revealing that the transferred\nsurrogate significantly outperforms both the original surrogate and the one\nbuilt from scratch using the transfer dataset, particularly in data-scarce\nscenarios.\n", "link": "http://arxiv.org/abs/2501.18344v2", "date": "2025-05-13", "relevancy": 1.5936, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5483}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5353}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20of%20Surrogate%20Models%3A%20Integrating%20Domain%20Warping%20and%0A%20%20Affine%20Transformations&body=Title%3A%20Transfer%20Learning%20of%20Surrogate%20Models%3A%20Integrating%20Domain%20Warping%20and%0A%20%20Affine%20Transformations%0AAuthor%3A%20Shuaiqun%20Pan%20and%20Diederick%20Vermetten%20and%20Manuel%20L%C3%B3pez-Ib%C3%A1%C3%B1ez%20and%20Thomas%20B%C3%A4ck%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Surrogate%20models%20provide%20efficient%20alternatives%20to%20computationally%20demanding%0Areal%20world%20processes%20but%20often%20require%20large%20datasets%20for%20effective%20training.%20A%0Apromising%20solution%20to%20this%20limitation%20is%20the%20transfer%20of%20pre-trained%20surrogate%0Amodels%20to%20new%20tasks.%20Previous%20studies%20have%20investigated%20the%20transfer%20of%0Adifferentiable%20and%20non-differentiable%20surrogate%20models%2C%20typically%20assuming%20an%0Aaffine%20transformation%20between%20the%20source%20and%20target%20functions.%20This%20paper%0Aextends%20previous%20research%20by%20addressing%20a%20broader%20range%20of%20transformations%2C%0Aincluding%20linear%20and%20nonlinear%20variations.%20Specifically%2C%20we%20consider%20the%0Acombination%20of%20an%20unknown%20input%20warping%2C%20such%20as%20one%20modeled%20by%20the%20beta%0Acumulative%20distribution%20function%2C%20with%20an%20unspecified%20affine%20transformation.%0AOur%20approach%20achieves%20transfer%20learning%20by%20employing%20a%20limited%20number%20of%20data%0Apoints%20from%20the%20target%20task%20to%20optimize%20these%20transformations%2C%20minimizing%0Aempirical%20loss%20on%20the%20transfer%20dataset.%20We%20validate%20the%20proposed%20method%20on%20the%0Awidely%20used%20Black-Box%20Optimization%20Benchmark%20%28BBOB%29%20testbed%20and%20a%20real-world%0Atransfer%20learning%20task%20from%20the%20automobile%20industry.%20The%20results%20underscore%20the%0Asignificant%20advantages%20of%20the%20approach%2C%20revealing%20that%20the%20transferred%0Asurrogate%20significantly%20outperforms%20both%20the%20original%20surrogate%20and%20the%20one%0Abuilt%20from%20scratch%20using%20the%20transfer%20dataset%2C%20particularly%20in%20data-scarce%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520of%2520Surrogate%2520Models%253A%2520Integrating%2520Domain%2520Warping%2520and%250A%2520%2520Affine%2520Transformations%26entry.906535625%3DShuaiqun%2520Pan%2520and%2520Diederick%2520Vermetten%2520and%2520Manuel%2520L%25C3%25B3pez-Ib%25C3%25A1%25C3%25B1ez%2520and%2520Thomas%2520B%25C3%25A4ck%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Surrogate%2520models%2520provide%2520efficient%2520alternatives%2520to%2520computationally%2520demanding%250Areal%2520world%2520processes%2520but%2520often%2520require%2520large%2520datasets%2520for%2520effective%2520training.%2520A%250Apromising%2520solution%2520to%2520this%2520limitation%2520is%2520the%2520transfer%2520of%2520pre-trained%2520surrogate%250Amodels%2520to%2520new%2520tasks.%2520Previous%2520studies%2520have%2520investigated%2520the%2520transfer%2520of%250Adifferentiable%2520and%2520non-differentiable%2520surrogate%2520models%252C%2520typically%2520assuming%2520an%250Aaffine%2520transformation%2520between%2520the%2520source%2520and%2520target%2520functions.%2520This%2520paper%250Aextends%2520previous%2520research%2520by%2520addressing%2520a%2520broader%2520range%2520of%2520transformations%252C%250Aincluding%2520linear%2520and%2520nonlinear%2520variations.%2520Specifically%252C%2520we%2520consider%2520the%250Acombination%2520of%2520an%2520unknown%2520input%2520warping%252C%2520such%2520as%2520one%2520modeled%2520by%2520the%2520beta%250Acumulative%2520distribution%2520function%252C%2520with%2520an%2520unspecified%2520affine%2520transformation.%250AOur%2520approach%2520achieves%2520transfer%2520learning%2520by%2520employing%2520a%2520limited%2520number%2520of%2520data%250Apoints%2520from%2520the%2520target%2520task%2520to%2520optimize%2520these%2520transformations%252C%2520minimizing%250Aempirical%2520loss%2520on%2520the%2520transfer%2520dataset.%2520We%2520validate%2520the%2520proposed%2520method%2520on%2520the%250Awidely%2520used%2520Black-Box%2520Optimization%2520Benchmark%2520%2528BBOB%2529%2520testbed%2520and%2520a%2520real-world%250Atransfer%2520learning%2520task%2520from%2520the%2520automobile%2520industry.%2520The%2520results%2520underscore%2520the%250Asignificant%2520advantages%2520of%2520the%2520approach%252C%2520revealing%2520that%2520the%2520transferred%250Asurrogate%2520significantly%2520outperforms%2520both%2520the%2520original%2520surrogate%2520and%2520the%2520one%250Abuilt%2520from%2520scratch%2520using%2520the%2520transfer%2520dataset%252C%2520particularly%2520in%2520data-scarce%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20of%20Surrogate%20Models%3A%20Integrating%20Domain%20Warping%20and%0A%20%20Affine%20Transformations&entry.906535625=Shuaiqun%20Pan%20and%20Diederick%20Vermetten%20and%20Manuel%20L%C3%B3pez-Ib%C3%A1%C3%B1ez%20and%20Thomas%20B%C3%A4ck%20and%20Hao%20Wang&entry.1292438233=%20%20Surrogate%20models%20provide%20efficient%20alternatives%20to%20computationally%20demanding%0Areal%20world%20processes%20but%20often%20require%20large%20datasets%20for%20effective%20training.%20A%0Apromising%20solution%20to%20this%20limitation%20is%20the%20transfer%20of%20pre-trained%20surrogate%0Amodels%20to%20new%20tasks.%20Previous%20studies%20have%20investigated%20the%20transfer%20of%0Adifferentiable%20and%20non-differentiable%20surrogate%20models%2C%20typically%20assuming%20an%0Aaffine%20transformation%20between%20the%20source%20and%20target%20functions.%20This%20paper%0Aextends%20previous%20research%20by%20addressing%20a%20broader%20range%20of%20transformations%2C%0Aincluding%20linear%20and%20nonlinear%20variations.%20Specifically%2C%20we%20consider%20the%0Acombination%20of%20an%20unknown%20input%20warping%2C%20such%20as%20one%20modeled%20by%20the%20beta%0Acumulative%20distribution%20function%2C%20with%20an%20unspecified%20affine%20transformation.%0AOur%20approach%20achieves%20transfer%20learning%20by%20employing%20a%20limited%20number%20of%20data%0Apoints%20from%20the%20target%20task%20to%20optimize%20these%20transformations%2C%20minimizing%0Aempirical%20loss%20on%20the%20transfer%20dataset.%20We%20validate%20the%20proposed%20method%20on%20the%0Awidely%20used%20Black-Box%20Optimization%20Benchmark%20%28BBOB%29%20testbed%20and%20a%20real-world%0Atransfer%20learning%20task%20from%20the%20automobile%20industry.%20The%20results%20underscore%20the%0Asignificant%20advantages%20of%20the%20approach%2C%20revealing%20that%20the%20transferred%0Asurrogate%20significantly%20outperforms%20both%20the%20original%20surrogate%20and%20the%20one%0Abuilt%20from%20scratch%20using%20the%20transfer%20dataset%2C%20particularly%20in%20data-scarce%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18344v2&entry.124074799=Read"},
{"title": "Aerial Path Online Planning for Urban Scene Updation", "author": "Mingfeng Tang and Ningna Wang and Ziyuan Xie and Jianwei Hu and Ke Xie and Xiaohu Guo and Hui Huang", "abstract": "  We present the first scene-update aerial path planning algorithm specifically\ndesigned for detecting and updating change areas in urban environments. While\nexisting methods for large-scale 3D urban scene reconstruction focus on\nachieving high accuracy and completeness, they are inefficient for scenarios\nrequiring periodic updates, as they often re-explore and reconstruct entire\nscenes, wasting significant time and resources on unchanged areas. To address\nthis limitation, our method leverages prior reconstructions and change\nprobability statistics to guide UAVs in detecting and focusing on areas likely\nto have changed. Our approach introduces a novel changeability heuristic to\nevaluate the likelihood of changes, driving the planning of two flight paths: a\nprior path informed by static priors and a dynamic real-time path that adapts\nto newly detected changes. The framework integrates surface sampling and\ncandidate view generation strategies, ensuring efficient coverage of change\nareas with minimal redundancy. Extensive experiments on real-world urban\ndatasets demonstrate that our method significantly reduces flight time and\ncomputational overhead, while maintaining high-quality updates comparable to\nfull-scene re-exploration and reconstruction. These contributions pave the way\nfor efficient, scalable, and adaptive UAV-based scene updates in complex urban\nenvironments.\n", "link": "http://arxiv.org/abs/2505.01486v3", "date": "2025-05-13", "relevancy": 1.5833, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5381}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5341}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aerial%20Path%20Online%20Planning%20for%20Urban%20Scene%20Updation&body=Title%3A%20Aerial%20Path%20Online%20Planning%20for%20Urban%20Scene%20Updation%0AAuthor%3A%20Mingfeng%20Tang%20and%20Ningna%20Wang%20and%20Ziyuan%20Xie%20and%20Jianwei%20Hu%20and%20Ke%20Xie%20and%20Xiaohu%20Guo%20and%20Hui%20Huang%0AAbstract%3A%20%20%20We%20present%20the%20first%20scene-update%20aerial%20path%20planning%20algorithm%20specifically%0Adesigned%20for%20detecting%20and%20updating%20change%20areas%20in%20urban%20environments.%20While%0Aexisting%20methods%20for%20large-scale%203D%20urban%20scene%20reconstruction%20focus%20on%0Aachieving%20high%20accuracy%20and%20completeness%2C%20they%20are%20inefficient%20for%20scenarios%0Arequiring%20periodic%20updates%2C%20as%20they%20often%20re-explore%20and%20reconstruct%20entire%0Ascenes%2C%20wasting%20significant%20time%20and%20resources%20on%20unchanged%20areas.%20To%20address%0Athis%20limitation%2C%20our%20method%20leverages%20prior%20reconstructions%20and%20change%0Aprobability%20statistics%20to%20guide%20UAVs%20in%20detecting%20and%20focusing%20on%20areas%20likely%0Ato%20have%20changed.%20Our%20approach%20introduces%20a%20novel%20changeability%20heuristic%20to%0Aevaluate%20the%20likelihood%20of%20changes%2C%20driving%20the%20planning%20of%20two%20flight%20paths%3A%20a%0Aprior%20path%20informed%20by%20static%20priors%20and%20a%20dynamic%20real-time%20path%20that%20adapts%0Ato%20newly%20detected%20changes.%20The%20framework%20integrates%20surface%20sampling%20and%0Acandidate%20view%20generation%20strategies%2C%20ensuring%20efficient%20coverage%20of%20change%0Aareas%20with%20minimal%20redundancy.%20Extensive%20experiments%20on%20real-world%20urban%0Adatasets%20demonstrate%20that%20our%20method%20significantly%20reduces%20flight%20time%20and%0Acomputational%20overhead%2C%20while%20maintaining%20high-quality%20updates%20comparable%20to%0Afull-scene%20re-exploration%20and%20reconstruction.%20These%20contributions%20pave%20the%20way%0Afor%20efficient%2C%20scalable%2C%20and%20adaptive%20UAV-based%20scene%20updates%20in%20complex%20urban%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01486v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAerial%2520Path%2520Online%2520Planning%2520for%2520Urban%2520Scene%2520Updation%26entry.906535625%3DMingfeng%2520Tang%2520and%2520Ningna%2520Wang%2520and%2520Ziyuan%2520Xie%2520and%2520Jianwei%2520Hu%2520and%2520Ke%2520Xie%2520and%2520Xiaohu%2520Guo%2520and%2520Hui%2520Huang%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520scene-update%2520aerial%2520path%2520planning%2520algorithm%2520specifically%250Adesigned%2520for%2520detecting%2520and%2520updating%2520change%2520areas%2520in%2520urban%2520environments.%2520While%250Aexisting%2520methods%2520for%2520large-scale%25203D%2520urban%2520scene%2520reconstruction%2520focus%2520on%250Aachieving%2520high%2520accuracy%2520and%2520completeness%252C%2520they%2520are%2520inefficient%2520for%2520scenarios%250Arequiring%2520periodic%2520updates%252C%2520as%2520they%2520often%2520re-explore%2520and%2520reconstruct%2520entire%250Ascenes%252C%2520wasting%2520significant%2520time%2520and%2520resources%2520on%2520unchanged%2520areas.%2520To%2520address%250Athis%2520limitation%252C%2520our%2520method%2520leverages%2520prior%2520reconstructions%2520and%2520change%250Aprobability%2520statistics%2520to%2520guide%2520UAVs%2520in%2520detecting%2520and%2520focusing%2520on%2520areas%2520likely%250Ato%2520have%2520changed.%2520Our%2520approach%2520introduces%2520a%2520novel%2520changeability%2520heuristic%2520to%250Aevaluate%2520the%2520likelihood%2520of%2520changes%252C%2520driving%2520the%2520planning%2520of%2520two%2520flight%2520paths%253A%2520a%250Aprior%2520path%2520informed%2520by%2520static%2520priors%2520and%2520a%2520dynamic%2520real-time%2520path%2520that%2520adapts%250Ato%2520newly%2520detected%2520changes.%2520The%2520framework%2520integrates%2520surface%2520sampling%2520and%250Acandidate%2520view%2520generation%2520strategies%252C%2520ensuring%2520efficient%2520coverage%2520of%2520change%250Aareas%2520with%2520minimal%2520redundancy.%2520Extensive%2520experiments%2520on%2520real-world%2520urban%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520significantly%2520reduces%2520flight%2520time%2520and%250Acomputational%2520overhead%252C%2520while%2520maintaining%2520high-quality%2520updates%2520comparable%2520to%250Afull-scene%2520re-exploration%2520and%2520reconstruction.%2520These%2520contributions%2520pave%2520the%2520way%250Afor%2520efficient%252C%2520scalable%252C%2520and%2520adaptive%2520UAV-based%2520scene%2520updates%2520in%2520complex%2520urban%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01486v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aerial%20Path%20Online%20Planning%20for%20Urban%20Scene%20Updation&entry.906535625=Mingfeng%20Tang%20and%20Ningna%20Wang%20and%20Ziyuan%20Xie%20and%20Jianwei%20Hu%20and%20Ke%20Xie%20and%20Xiaohu%20Guo%20and%20Hui%20Huang&entry.1292438233=%20%20We%20present%20the%20first%20scene-update%20aerial%20path%20planning%20algorithm%20specifically%0Adesigned%20for%20detecting%20and%20updating%20change%20areas%20in%20urban%20environments.%20While%0Aexisting%20methods%20for%20large-scale%203D%20urban%20scene%20reconstruction%20focus%20on%0Aachieving%20high%20accuracy%20and%20completeness%2C%20they%20are%20inefficient%20for%20scenarios%0Arequiring%20periodic%20updates%2C%20as%20they%20often%20re-explore%20and%20reconstruct%20entire%0Ascenes%2C%20wasting%20significant%20time%20and%20resources%20on%20unchanged%20areas.%20To%20address%0Athis%20limitation%2C%20our%20method%20leverages%20prior%20reconstructions%20and%20change%0Aprobability%20statistics%20to%20guide%20UAVs%20in%20detecting%20and%20focusing%20on%20areas%20likely%0Ato%20have%20changed.%20Our%20approach%20introduces%20a%20novel%20changeability%20heuristic%20to%0Aevaluate%20the%20likelihood%20of%20changes%2C%20driving%20the%20planning%20of%20two%20flight%20paths%3A%20a%0Aprior%20path%20informed%20by%20static%20priors%20and%20a%20dynamic%20real-time%20path%20that%20adapts%0Ato%20newly%20detected%20changes.%20The%20framework%20integrates%20surface%20sampling%20and%0Acandidate%20view%20generation%20strategies%2C%20ensuring%20efficient%20coverage%20of%20change%0Aareas%20with%20minimal%20redundancy.%20Extensive%20experiments%20on%20real-world%20urban%0Adatasets%20demonstrate%20that%20our%20method%20significantly%20reduces%20flight%20time%20and%0Acomputational%20overhead%2C%20while%20maintaining%20high-quality%20updates%20comparable%20to%0Afull-scene%20re-exploration%20and%20reconstruction.%20These%20contributions%20pave%20the%20way%0Afor%20efficient%2C%20scalable%2C%20and%20adaptive%20UAV-based%20scene%20updates%20in%20complex%20urban%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01486v3&entry.124074799=Read"},
{"title": "Continuous Temporal Learning of Probability Distributions via Neural\n  ODEs with Applications in Continuous Glucose Monitoring Data", "author": "Antonio \u00c1lvarez-L\u00f3pez and Marcos Matabuena", "abstract": "  Modeling the continuous--time dynamics of probability distributions from\ntime--dependent data samples is a fundamental problem in many fields, including\ndigital health. The aim is to analyze how the distribution of a biomarker, such\nas glucose, evolves over time and how these changes may reflect the progression\nof chronic diseases such as diabetes. In this paper, we propose a novel\nprobabilistic model based on a mixture of Gaussian distributions to capture how\nsamples from a continuous-time stochastic process evolve over the time. To\nmodel potential distribution shifts over time, we introduce a time-dependent\nfunction parameterized by a Neural Ordinary Differential Equation (Neural ODE)\nand estimate it non--parametrically using the Maximum Mean Discrepancy (MMD).\nThe proposed model is highly interpretable, detects subtle temporal shifts, and\nremains computationally efficient. Through simulation studies, we show that it\nperforms competitively in terms of estimation accuracy against\nstate-of-the-art, less interpretable methods such as normalized gradient--flows\nand non--parameteric kernel density estimators. Finally, we demonstrate the\nutility of our method on digital clinical--trial data, showing how the\ninterventions alters the time-dependent distribution of glucose levels and\nenabling a rigorous comparison of control and treatment groups from novel\nmathematical and clinical perspectives.\n", "link": "http://arxiv.org/abs/2505.08698v1", "date": "2025-05-13", "relevancy": 1.5146, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5078}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Temporal%20Learning%20of%20Probability%20Distributions%20via%20Neural%0A%20%20ODEs%20with%20Applications%20in%20Continuous%20Glucose%20Monitoring%20Data&body=Title%3A%20Continuous%20Temporal%20Learning%20of%20Probability%20Distributions%20via%20Neural%0A%20%20ODEs%20with%20Applications%20in%20Continuous%20Glucose%20Monitoring%20Data%0AAuthor%3A%20Antonio%20%C3%81lvarez-L%C3%B3pez%20and%20Marcos%20Matabuena%0AAbstract%3A%20%20%20Modeling%20the%20continuous--time%20dynamics%20of%20probability%20distributions%20from%0Atime--dependent%20data%20samples%20is%20a%20fundamental%20problem%20in%20many%20fields%2C%20including%0Adigital%20health.%20The%20aim%20is%20to%20analyze%20how%20the%20distribution%20of%20a%20biomarker%2C%20such%0Aas%20glucose%2C%20evolves%20over%20time%20and%20how%20these%20changes%20may%20reflect%20the%20progression%0Aof%20chronic%20diseases%20such%20as%20diabetes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aprobabilistic%20model%20based%20on%20a%20mixture%20of%20Gaussian%20distributions%20to%20capture%20how%0Asamples%20from%20a%20continuous-time%20stochastic%20process%20evolve%20over%20the%20time.%20To%0Amodel%20potential%20distribution%20shifts%20over%20time%2C%20we%20introduce%20a%20time-dependent%0Afunction%20parameterized%20by%20a%20Neural%20Ordinary%20Differential%20Equation%20%28Neural%20ODE%29%0Aand%20estimate%20it%20non--parametrically%20using%20the%20Maximum%20Mean%20Discrepancy%20%28MMD%29.%0AThe%20proposed%20model%20is%20highly%20interpretable%2C%20detects%20subtle%20temporal%20shifts%2C%20and%0Aremains%20computationally%20efficient.%20Through%20simulation%20studies%2C%20we%20show%20that%20it%0Aperforms%20competitively%20in%20terms%20of%20estimation%20accuracy%20against%0Astate-of-the-art%2C%20less%20interpretable%20methods%20such%20as%20normalized%20gradient--flows%0Aand%20non--parameteric%20kernel%20density%20estimators.%20Finally%2C%20we%20demonstrate%20the%0Autility%20of%20our%20method%20on%20digital%20clinical--trial%20data%2C%20showing%20how%20the%0Ainterventions%20alters%20the%20time-dependent%20distribution%20of%20glucose%20levels%20and%0Aenabling%20a%20rigorous%20comparison%20of%20control%20and%20treatment%20groups%20from%20novel%0Amathematical%20and%20clinical%20perspectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Temporal%2520Learning%2520of%2520Probability%2520Distributions%2520via%2520Neural%250A%2520%2520ODEs%2520with%2520Applications%2520in%2520Continuous%2520Glucose%2520Monitoring%2520Data%26entry.906535625%3DAntonio%2520%25C3%2581lvarez-L%25C3%25B3pez%2520and%2520Marcos%2520Matabuena%26entry.1292438233%3D%2520%2520Modeling%2520the%2520continuous--time%2520dynamics%2520of%2520probability%2520distributions%2520from%250Atime--dependent%2520data%2520samples%2520is%2520a%2520fundamental%2520problem%2520in%2520many%2520fields%252C%2520including%250Adigital%2520health.%2520The%2520aim%2520is%2520to%2520analyze%2520how%2520the%2520distribution%2520of%2520a%2520biomarker%252C%2520such%250Aas%2520glucose%252C%2520evolves%2520over%2520time%2520and%2520how%2520these%2520changes%2520may%2520reflect%2520the%2520progression%250Aof%2520chronic%2520diseases%2520such%2520as%2520diabetes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aprobabilistic%2520model%2520based%2520on%2520a%2520mixture%2520of%2520Gaussian%2520distributions%2520to%2520capture%2520how%250Asamples%2520from%2520a%2520continuous-time%2520stochastic%2520process%2520evolve%2520over%2520the%2520time.%2520To%250Amodel%2520potential%2520distribution%2520shifts%2520over%2520time%252C%2520we%2520introduce%2520a%2520time-dependent%250Afunction%2520parameterized%2520by%2520a%2520Neural%2520Ordinary%2520Differential%2520Equation%2520%2528Neural%2520ODE%2529%250Aand%2520estimate%2520it%2520non--parametrically%2520using%2520the%2520Maximum%2520Mean%2520Discrepancy%2520%2528MMD%2529.%250AThe%2520proposed%2520model%2520is%2520highly%2520interpretable%252C%2520detects%2520subtle%2520temporal%2520shifts%252C%2520and%250Aremains%2520computationally%2520efficient.%2520Through%2520simulation%2520studies%252C%2520we%2520show%2520that%2520it%250Aperforms%2520competitively%2520in%2520terms%2520of%2520estimation%2520accuracy%2520against%250Astate-of-the-art%252C%2520less%2520interpretable%2520methods%2520such%2520as%2520normalized%2520gradient--flows%250Aand%2520non--parameteric%2520kernel%2520density%2520estimators.%2520Finally%252C%2520we%2520demonstrate%2520the%250Autility%2520of%2520our%2520method%2520on%2520digital%2520clinical--trial%2520data%252C%2520showing%2520how%2520the%250Ainterventions%2520alters%2520the%2520time-dependent%2520distribution%2520of%2520glucose%2520levels%2520and%250Aenabling%2520a%2520rigorous%2520comparison%2520of%2520control%2520and%2520treatment%2520groups%2520from%2520novel%250Amathematical%2520and%2520clinical%2520perspectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Temporal%20Learning%20of%20Probability%20Distributions%20via%20Neural%0A%20%20ODEs%20with%20Applications%20in%20Continuous%20Glucose%20Monitoring%20Data&entry.906535625=Antonio%20%C3%81lvarez-L%C3%B3pez%20and%20Marcos%20Matabuena&entry.1292438233=%20%20Modeling%20the%20continuous--time%20dynamics%20of%20probability%20distributions%20from%0Atime--dependent%20data%20samples%20is%20a%20fundamental%20problem%20in%20many%20fields%2C%20including%0Adigital%20health.%20The%20aim%20is%20to%20analyze%20how%20the%20distribution%20of%20a%20biomarker%2C%20such%0Aas%20glucose%2C%20evolves%20over%20time%20and%20how%20these%20changes%20may%20reflect%20the%20progression%0Aof%20chronic%20diseases%20such%20as%20diabetes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aprobabilistic%20model%20based%20on%20a%20mixture%20of%20Gaussian%20distributions%20to%20capture%20how%0Asamples%20from%20a%20continuous-time%20stochastic%20process%20evolve%20over%20the%20time.%20To%0Amodel%20potential%20distribution%20shifts%20over%20time%2C%20we%20introduce%20a%20time-dependent%0Afunction%20parameterized%20by%20a%20Neural%20Ordinary%20Differential%20Equation%20%28Neural%20ODE%29%0Aand%20estimate%20it%20non--parametrically%20using%20the%20Maximum%20Mean%20Discrepancy%20%28MMD%29.%0AThe%20proposed%20model%20is%20highly%20interpretable%2C%20detects%20subtle%20temporal%20shifts%2C%20and%0Aremains%20computationally%20efficient.%20Through%20simulation%20studies%2C%20we%20show%20that%20it%0Aperforms%20competitively%20in%20terms%20of%20estimation%20accuracy%20against%0Astate-of-the-art%2C%20less%20interpretable%20methods%20such%20as%20normalized%20gradient--flows%0Aand%20non--parameteric%20kernel%20density%20estimators.%20Finally%2C%20we%20demonstrate%20the%0Autility%20of%20our%20method%20on%20digital%20clinical--trial%20data%2C%20showing%20how%20the%0Ainterventions%20alters%20the%20time-dependent%20distribution%20of%20glucose%20levels%20and%0Aenabling%20a%20rigorous%20comparison%20of%20control%20and%20treatment%20groups%20from%20novel%0Amathematical%20and%20clinical%20perspectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08698v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


