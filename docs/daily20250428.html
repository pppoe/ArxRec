<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250427.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "DCFormer: Efficient 3D Vision-Language Modeling with Decomposed\n  Convolutions", "author": "Gorkem Can Ates and Yu Xin and Kuang Gong and Wei Shao", "abstract": "  Vision-language models (VLMs) have been widely applied to 2D medical image\nanalysis due to their ability to align visual and textual representations.\nHowever, extending VLMs to 3D imaging remains computationally challenging.\nExisting 3D VLMs often rely on Vision Transformers (ViTs), which are\ncomputationally expensive due to the quadratic complexity of self-attention, or\non 3D convolutions, which require large numbers of parameters and FLOPs as\nkernel size increases. We introduce DCFormer, an efficient 3D image encoder\nthat factorizes 3D convolutions into three parallel 1D convolutions along the\ndepth, height, and width dimensions. This design preserves spatial information\nwhile significantly reducing computational cost. Integrated into a CLIP-based\nvision-language framework, DCFormer is trained and evaluated on CT-RATE, a\ndataset of 50,188 paired 3D chest CT volumes and radiology reports. In\nzero-shot and fine-tuned detection of 18 pathologies, as well as in image-text\nretrieval tasks, DCFormer consistently outperforms state-of-the-art 3D vision\nencoders, including CT-ViT, ViT, ConvNeXt, PoolFormer, and TransUNet. These\nresults highlight DCFormer's potential for scalable, clinically deployable 3D\nmedical VLMs. Our code is available at: https://github.com/mirthAI/DCFormer.\n", "link": "http://arxiv.org/abs/2502.05091v2", "date": "2025-04-25", "relevancy": 3.2107, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6559}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCFormer%3A%20Efficient%203D%20Vision-Language%20Modeling%20with%20Decomposed%0A%20%20Convolutions&body=Title%3A%20DCFormer%3A%20Efficient%203D%20Vision-Language%20Modeling%20with%20Decomposed%0A%20%20Convolutions%0AAuthor%3A%20Gorkem%20Can%20Ates%20and%20Yu%20Xin%20and%20Kuang%20Gong%20and%20Wei%20Shao%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20been%20widely%20applied%20to%202D%20medical%20image%0Aanalysis%20due%20to%20their%20ability%20to%20align%20visual%20and%20textual%20representations.%0AHowever%2C%20extending%20VLMs%20to%203D%20imaging%20remains%20computationally%20challenging.%0AExisting%203D%20VLMs%20often%20rely%20on%20Vision%20Transformers%20%28ViTs%29%2C%20which%20are%0Acomputationally%20expensive%20due%20to%20the%20quadratic%20complexity%20of%20self-attention%2C%20or%0Aon%203D%20convolutions%2C%20which%20require%20large%20numbers%20of%20parameters%20and%20FLOPs%20as%0Akernel%20size%20increases.%20We%20introduce%20DCFormer%2C%20an%20efficient%203D%20image%20encoder%0Athat%20factorizes%203D%20convolutions%20into%20three%20parallel%201D%20convolutions%20along%20the%0Adepth%2C%20height%2C%20and%20width%20dimensions.%20This%20design%20preserves%20spatial%20information%0Awhile%20significantly%20reducing%20computational%20cost.%20Integrated%20into%20a%20CLIP-based%0Avision-language%20framework%2C%20DCFormer%20is%20trained%20and%20evaluated%20on%20CT-RATE%2C%20a%0Adataset%20of%2050%2C188%20paired%203D%20chest%20CT%20volumes%20and%20radiology%20reports.%20In%0Azero-shot%20and%20fine-tuned%20detection%20of%2018%20pathologies%2C%20as%20well%20as%20in%20image-text%0Aretrieval%20tasks%2C%20DCFormer%20consistently%20outperforms%20state-of-the-art%203D%20vision%0Aencoders%2C%20including%20CT-ViT%2C%20ViT%2C%20ConvNeXt%2C%20PoolFormer%2C%20and%20TransUNet.%20These%0Aresults%20highlight%20DCFormer%27s%20potential%20for%20scalable%2C%20clinically%20deployable%203D%0Amedical%20VLMs.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/mirthAI/DCFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05091v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCFormer%253A%2520Efficient%25203D%2520Vision-Language%2520Modeling%2520with%2520Decomposed%250A%2520%2520Convolutions%26entry.906535625%3DGorkem%2520Can%2520Ates%2520and%2520Yu%2520Xin%2520and%2520Kuang%2520Gong%2520and%2520Wei%2520Shao%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520been%2520widely%2520applied%2520to%25202D%2520medical%2520image%250Aanalysis%2520due%2520to%2520their%2520ability%2520to%2520align%2520visual%2520and%2520textual%2520representations.%250AHowever%252C%2520extending%2520VLMs%2520to%25203D%2520imaging%2520remains%2520computationally%2520challenging.%250AExisting%25203D%2520VLMs%2520often%2520rely%2520on%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520which%2520are%250Acomputationally%2520expensive%2520due%2520to%2520the%2520quadratic%2520complexity%2520of%2520self-attention%252C%2520or%250Aon%25203D%2520convolutions%252C%2520which%2520require%2520large%2520numbers%2520of%2520parameters%2520and%2520FLOPs%2520as%250Akernel%2520size%2520increases.%2520We%2520introduce%2520DCFormer%252C%2520an%2520efficient%25203D%2520image%2520encoder%250Athat%2520factorizes%25203D%2520convolutions%2520into%2520three%2520parallel%25201D%2520convolutions%2520along%2520the%250Adepth%252C%2520height%252C%2520and%2520width%2520dimensions.%2520This%2520design%2520preserves%2520spatial%2520information%250Awhile%2520significantly%2520reducing%2520computational%2520cost.%2520Integrated%2520into%2520a%2520CLIP-based%250Avision-language%2520framework%252C%2520DCFormer%2520is%2520trained%2520and%2520evaluated%2520on%2520CT-RATE%252C%2520a%250Adataset%2520of%252050%252C188%2520paired%25203D%2520chest%2520CT%2520volumes%2520and%2520radiology%2520reports.%2520In%250Azero-shot%2520and%2520fine-tuned%2520detection%2520of%252018%2520pathologies%252C%2520as%2520well%2520as%2520in%2520image-text%250Aretrieval%2520tasks%252C%2520DCFormer%2520consistently%2520outperforms%2520state-of-the-art%25203D%2520vision%250Aencoders%252C%2520including%2520CT-ViT%252C%2520ViT%252C%2520ConvNeXt%252C%2520PoolFormer%252C%2520and%2520TransUNet.%2520These%250Aresults%2520highlight%2520DCFormer%2527s%2520potential%2520for%2520scalable%252C%2520clinically%2520deployable%25203D%250Amedical%2520VLMs.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/mirthAI/DCFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05091v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCFormer%3A%20Efficient%203D%20Vision-Language%20Modeling%20with%20Decomposed%0A%20%20Convolutions&entry.906535625=Gorkem%20Can%20Ates%20and%20Yu%20Xin%20and%20Kuang%20Gong%20and%20Wei%20Shao&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20been%20widely%20applied%20to%202D%20medical%20image%0Aanalysis%20due%20to%20their%20ability%20to%20align%20visual%20and%20textual%20representations.%0AHowever%2C%20extending%20VLMs%20to%203D%20imaging%20remains%20computationally%20challenging.%0AExisting%203D%20VLMs%20often%20rely%20on%20Vision%20Transformers%20%28ViTs%29%2C%20which%20are%0Acomputationally%20expensive%20due%20to%20the%20quadratic%20complexity%20of%20self-attention%2C%20or%0Aon%203D%20convolutions%2C%20which%20require%20large%20numbers%20of%20parameters%20and%20FLOPs%20as%0Akernel%20size%20increases.%20We%20introduce%20DCFormer%2C%20an%20efficient%203D%20image%20encoder%0Athat%20factorizes%203D%20convolutions%20into%20three%20parallel%201D%20convolutions%20along%20the%0Adepth%2C%20height%2C%20and%20width%20dimensions.%20This%20design%20preserves%20spatial%20information%0Awhile%20significantly%20reducing%20computational%20cost.%20Integrated%20into%20a%20CLIP-based%0Avision-language%20framework%2C%20DCFormer%20is%20trained%20and%20evaluated%20on%20CT-RATE%2C%20a%0Adataset%20of%2050%2C188%20paired%203D%20chest%20CT%20volumes%20and%20radiology%20reports.%20In%0Azero-shot%20and%20fine-tuned%20detection%20of%2018%20pathologies%2C%20as%20well%20as%20in%20image-text%0Aretrieval%20tasks%2C%20DCFormer%20consistently%20outperforms%20state-of-the-art%203D%20vision%0Aencoders%2C%20including%20CT-ViT%2C%20ViT%2C%20ConvNeXt%2C%20PoolFormer%2C%20and%20TransUNet.%20These%0Aresults%20highlight%20DCFormer%27s%20potential%20for%20scalable%2C%20clinically%20deployable%203D%0Amedical%20VLMs.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/mirthAI/DCFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05091v2&entry.124074799=Read"},
{"title": "Understanding Depth and Height Perception in Large Visual-Language\n  Models", "author": "Shehreen Azad and Yash Jain and Rishit Garg and Yogesh S Rawat and Vibhav Vineet", "abstract": "  Geometric understanding - including depth and height perception - is\nfundamental to intelligence and crucial for navigating our environment. Despite\nthe impressive capabilities of large Vision Language Models (VLMs), it remains\nunclear how well they possess the geometric understanding required for\npractical applications in visual perception. In this work, we focus on\nevaluating the geometric understanding of these models, specifically targeting\ntheir ability to perceive the depth and height of objects in an image. To\naddress this, we introduce GeoMeter, a suite of benchmark datasets -\nencompassing 2D and 3D scenarios - to rigorously evaluate these aspects. By\nbenchmarking 18 state-of-the-art VLMs, we found that although they excel in\nperceiving basic geometric properties like shape and size, they consistently\nstruggle with depth and height perception. Our analysis reveal that these\nchallenges stem from shortcomings in their depth and height reasoning\ncapabilities and inherent biases. This study aims to pave the way for\ndeveloping VLMs with enhanced geometric understanding by emphasizing depth and\nheight perception as critical components necessary for real-world applications.\n", "link": "http://arxiv.org/abs/2408.11748v5", "date": "2025-04-25", "relevancy": 3.1327, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6641}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6641}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Depth%20and%20Height%20Perception%20in%20Large%20Visual-Language%0A%20%20Models&body=Title%3A%20Understanding%20Depth%20and%20Height%20Perception%20in%20Large%20Visual-Language%0A%20%20Models%0AAuthor%3A%20Shehreen%20Azad%20and%20Yash%20Jain%20and%20Rishit%20Garg%20and%20Yogesh%20S%20Rawat%20and%20Vibhav%20Vineet%0AAbstract%3A%20%20%20Geometric%20understanding%20-%20including%20depth%20and%20height%20perception%20-%20is%0Afundamental%20to%20intelligence%20and%20crucial%20for%20navigating%20our%20environment.%20Despite%0Athe%20impressive%20capabilities%20of%20large%20Vision%20Language%20Models%20%28VLMs%29%2C%20it%20remains%0Aunclear%20how%20well%20they%20possess%20the%20geometric%20understanding%20required%20for%0Apractical%20applications%20in%20visual%20perception.%20In%20this%20work%2C%20we%20focus%20on%0Aevaluating%20the%20geometric%20understanding%20of%20these%20models%2C%20specifically%20targeting%0Atheir%20ability%20to%20perceive%20the%20depth%20and%20height%20of%20objects%20in%20an%20image.%20To%0Aaddress%20this%2C%20we%20introduce%20GeoMeter%2C%20a%20suite%20of%20benchmark%20datasets%20-%0Aencompassing%202D%20and%203D%20scenarios%20-%20to%20rigorously%20evaluate%20these%20aspects.%20By%0Abenchmarking%2018%20state-of-the-art%20VLMs%2C%20we%20found%20that%20although%20they%20excel%20in%0Aperceiving%20basic%20geometric%20properties%20like%20shape%20and%20size%2C%20they%20consistently%0Astruggle%20with%20depth%20and%20height%20perception.%20Our%20analysis%20reveal%20that%20these%0Achallenges%20stem%20from%20shortcomings%20in%20their%20depth%20and%20height%20reasoning%0Acapabilities%20and%20inherent%20biases.%20This%20study%20aims%20to%20pave%20the%20way%20for%0Adeveloping%20VLMs%20with%20enhanced%20geometric%20understanding%20by%20emphasizing%20depth%20and%0Aheight%20perception%20as%20critical%20components%20necessary%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11748v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Depth%2520and%2520Height%2520Perception%2520in%2520Large%2520Visual-Language%250A%2520%2520Models%26entry.906535625%3DShehreen%2520Azad%2520and%2520Yash%2520Jain%2520and%2520Rishit%2520Garg%2520and%2520Yogesh%2520S%2520Rawat%2520and%2520Vibhav%2520Vineet%26entry.1292438233%3D%2520%2520Geometric%2520understanding%2520-%2520including%2520depth%2520and%2520height%2520perception%2520-%2520is%250Afundamental%2520to%2520intelligence%2520and%2520crucial%2520for%2520navigating%2520our%2520environment.%2520Despite%250Athe%2520impressive%2520capabilities%2520of%2520large%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520it%2520remains%250Aunclear%2520how%2520well%2520they%2520possess%2520the%2520geometric%2520understanding%2520required%2520for%250Apractical%2520applications%2520in%2520visual%2520perception.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%250Aevaluating%2520the%2520geometric%2520understanding%2520of%2520these%2520models%252C%2520specifically%2520targeting%250Atheir%2520ability%2520to%2520perceive%2520the%2520depth%2520and%2520height%2520of%2520objects%2520in%2520an%2520image.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520GeoMeter%252C%2520a%2520suite%2520of%2520benchmark%2520datasets%2520-%250Aencompassing%25202D%2520and%25203D%2520scenarios%2520-%2520to%2520rigorously%2520evaluate%2520these%2520aspects.%2520By%250Abenchmarking%252018%2520state-of-the-art%2520VLMs%252C%2520we%2520found%2520that%2520although%2520they%2520excel%2520in%250Aperceiving%2520basic%2520geometric%2520properties%2520like%2520shape%2520and%2520size%252C%2520they%2520consistently%250Astruggle%2520with%2520depth%2520and%2520height%2520perception.%2520Our%2520analysis%2520reveal%2520that%2520these%250Achallenges%2520stem%2520from%2520shortcomings%2520in%2520their%2520depth%2520and%2520height%2520reasoning%250Acapabilities%2520and%2520inherent%2520biases.%2520This%2520study%2520aims%2520to%2520pave%2520the%2520way%2520for%250Adeveloping%2520VLMs%2520with%2520enhanced%2520geometric%2520understanding%2520by%2520emphasizing%2520depth%2520and%250Aheight%2520perception%2520as%2520critical%2520components%2520necessary%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11748v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Depth%20and%20Height%20Perception%20in%20Large%20Visual-Language%0A%20%20Models&entry.906535625=Shehreen%20Azad%20and%20Yash%20Jain%20and%20Rishit%20Garg%20and%20Yogesh%20S%20Rawat%20and%20Vibhav%20Vineet&entry.1292438233=%20%20Geometric%20understanding%20-%20including%20depth%20and%20height%20perception%20-%20is%0Afundamental%20to%20intelligence%20and%20crucial%20for%20navigating%20our%20environment.%20Despite%0Athe%20impressive%20capabilities%20of%20large%20Vision%20Language%20Models%20%28VLMs%29%2C%20it%20remains%0Aunclear%20how%20well%20they%20possess%20the%20geometric%20understanding%20required%20for%0Apractical%20applications%20in%20visual%20perception.%20In%20this%20work%2C%20we%20focus%20on%0Aevaluating%20the%20geometric%20understanding%20of%20these%20models%2C%20specifically%20targeting%0Atheir%20ability%20to%20perceive%20the%20depth%20and%20height%20of%20objects%20in%20an%20image.%20To%0Aaddress%20this%2C%20we%20introduce%20GeoMeter%2C%20a%20suite%20of%20benchmark%20datasets%20-%0Aencompassing%202D%20and%203D%20scenarios%20-%20to%20rigorously%20evaluate%20these%20aspects.%20By%0Abenchmarking%2018%20state-of-the-art%20VLMs%2C%20we%20found%20that%20although%20they%20excel%20in%0Aperceiving%20basic%20geometric%20properties%20like%20shape%20and%20size%2C%20they%20consistently%0Astruggle%20with%20depth%20and%20height%20perception.%20Our%20analysis%20reveal%20that%20these%0Achallenges%20stem%20from%20shortcomings%20in%20their%20depth%20and%20height%20reasoning%0Acapabilities%20and%20inherent%20biases.%20This%20study%20aims%20to%20pave%20the%20way%20for%0Adeveloping%20VLMs%20with%20enhanced%20geometric%20understanding%20by%20emphasizing%20depth%20and%0Aheight%20perception%20as%20critical%20components%20necessary%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11748v5&entry.124074799=Read"},
{"title": "RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny\n  Objects", "author": "Georgios Kouros and Minye Wu and Tinne Tuytelaars", "abstract": "  We introduce RGS-DR, a novel inverse rendering method for reconstructing and\nrendering glossy and reflective objects with support for flexible relighting\nand scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian\nSplatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D\nGaussian surfel representation to accurately estimate geometry and surface\nnormals, an essential property for high-quality inverse rendering. Our approach\nexplicitly models geometric and material properties through learnable\nprimitives rasterized into a deferred shading pipeline, effectively reducing\nrendering artifacts and preserving sharp reflections. By employing a\nmulti-level cube mipmap, RGS-DR accurately approximates environment lighting\nintegrals, facilitating high-quality reconstruction and relighting. A residual\npass with spherical-mipmap-based directional encoding further refines the\nappearance modeling. Experiments demonstrate that RGS-DR achieves high-quality\nreconstruction and rendering quality for shiny objects, often outperforming\nreconstruction-exclusive state-of-the-art methods incapable of relighting.\n", "link": "http://arxiv.org/abs/2504.18468v1", "date": "2025-04-25", "relevancy": 3.088, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6454}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6172}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGS-DR%3A%20Reflective%20Gaussian%20Surfels%20with%20Deferred%20Rendering%20for%20Shiny%0A%20%20Objects&body=Title%3A%20RGS-DR%3A%20Reflective%20Gaussian%20Surfels%20with%20Deferred%20Rendering%20for%20Shiny%0A%20%20Objects%0AAuthor%3A%20Georgios%20Kouros%20and%20Minye%20Wu%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20%20%20We%20introduce%20RGS-DR%2C%20a%20novel%20inverse%20rendering%20method%20for%20reconstructing%20and%0Arendering%20glossy%20and%20reflective%20objects%20with%20support%20for%20flexible%20relighting%0Aand%20scene%20editing.%20Unlike%20existing%20methods%20%28e.g.%2C%20NeRF%20and%203D%20Gaussian%0ASplatting%29%2C%20which%20struggle%20with%20view-dependent%20effects%2C%20RGS-DR%20utilizes%20a%202D%0AGaussian%20surfel%20representation%20to%20accurately%20estimate%20geometry%20and%20surface%0Anormals%2C%20an%20essential%20property%20for%20high-quality%20inverse%20rendering.%20Our%20approach%0Aexplicitly%20models%20geometric%20and%20material%20properties%20through%20learnable%0Aprimitives%20rasterized%20into%20a%20deferred%20shading%20pipeline%2C%20effectively%20reducing%0Arendering%20artifacts%20and%20preserving%20sharp%20reflections.%20By%20employing%20a%0Amulti-level%20cube%20mipmap%2C%20RGS-DR%20accurately%20approximates%20environment%20lighting%0Aintegrals%2C%20facilitating%20high-quality%20reconstruction%20and%20relighting.%20A%20residual%0Apass%20with%20spherical-mipmap-based%20directional%20encoding%20further%20refines%20the%0Aappearance%20modeling.%20Experiments%20demonstrate%20that%20RGS-DR%20achieves%20high-quality%0Areconstruction%20and%20rendering%20quality%20for%20shiny%20objects%2C%20often%20outperforming%0Areconstruction-exclusive%20state-of-the-art%20methods%20incapable%20of%20relighting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGS-DR%253A%2520Reflective%2520Gaussian%2520Surfels%2520with%2520Deferred%2520Rendering%2520for%2520Shiny%250A%2520%2520Objects%26entry.906535625%3DGeorgios%2520Kouros%2520and%2520Minye%2520Wu%2520and%2520Tinne%2520Tuytelaars%26entry.1292438233%3D%2520%2520We%2520introduce%2520RGS-DR%252C%2520a%2520novel%2520inverse%2520rendering%2520method%2520for%2520reconstructing%2520and%250Arendering%2520glossy%2520and%2520reflective%2520objects%2520with%2520support%2520for%2520flexible%2520relighting%250Aand%2520scene%2520editing.%2520Unlike%2520existing%2520methods%2520%2528e.g.%252C%2520NeRF%2520and%25203D%2520Gaussian%250ASplatting%2529%252C%2520which%2520struggle%2520with%2520view-dependent%2520effects%252C%2520RGS-DR%2520utilizes%2520a%25202D%250AGaussian%2520surfel%2520representation%2520to%2520accurately%2520estimate%2520geometry%2520and%2520surface%250Anormals%252C%2520an%2520essential%2520property%2520for%2520high-quality%2520inverse%2520rendering.%2520Our%2520approach%250Aexplicitly%2520models%2520geometric%2520and%2520material%2520properties%2520through%2520learnable%250Aprimitives%2520rasterized%2520into%2520a%2520deferred%2520shading%2520pipeline%252C%2520effectively%2520reducing%250Arendering%2520artifacts%2520and%2520preserving%2520sharp%2520reflections.%2520By%2520employing%2520a%250Amulti-level%2520cube%2520mipmap%252C%2520RGS-DR%2520accurately%2520approximates%2520environment%2520lighting%250Aintegrals%252C%2520facilitating%2520high-quality%2520reconstruction%2520and%2520relighting.%2520A%2520residual%250Apass%2520with%2520spherical-mipmap-based%2520directional%2520encoding%2520further%2520refines%2520the%250Aappearance%2520modeling.%2520Experiments%2520demonstrate%2520that%2520RGS-DR%2520achieves%2520high-quality%250Areconstruction%2520and%2520rendering%2520quality%2520for%2520shiny%2520objects%252C%2520often%2520outperforming%250Areconstruction-exclusive%2520state-of-the-art%2520methods%2520incapable%2520of%2520relighting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGS-DR%3A%20Reflective%20Gaussian%20Surfels%20with%20Deferred%20Rendering%20for%20Shiny%0A%20%20Objects&entry.906535625=Georgios%20Kouros%20and%20Minye%20Wu%20and%20Tinne%20Tuytelaars&entry.1292438233=%20%20We%20introduce%20RGS-DR%2C%20a%20novel%20inverse%20rendering%20method%20for%20reconstructing%20and%0Arendering%20glossy%20and%20reflective%20objects%20with%20support%20for%20flexible%20relighting%0Aand%20scene%20editing.%20Unlike%20existing%20methods%20%28e.g.%2C%20NeRF%20and%203D%20Gaussian%0ASplatting%29%2C%20which%20struggle%20with%20view-dependent%20effects%2C%20RGS-DR%20utilizes%20a%202D%0AGaussian%20surfel%20representation%20to%20accurately%20estimate%20geometry%20and%20surface%0Anormals%2C%20an%20essential%20property%20for%20high-quality%20inverse%20rendering.%20Our%20approach%0Aexplicitly%20models%20geometric%20and%20material%20properties%20through%20learnable%0Aprimitives%20rasterized%20into%20a%20deferred%20shading%20pipeline%2C%20effectively%20reducing%0Arendering%20artifacts%20and%20preserving%20sharp%20reflections.%20By%20employing%20a%0Amulti-level%20cube%20mipmap%2C%20RGS-DR%20accurately%20approximates%20environment%20lighting%0Aintegrals%2C%20facilitating%20high-quality%20reconstruction%20and%20relighting.%20A%20residual%0Apass%20with%20spherical-mipmap-based%20directional%20encoding%20further%20refines%20the%0Aappearance%20modeling.%20Experiments%20demonstrate%20that%20RGS-DR%20achieves%20high-quality%0Areconstruction%20and%20rendering%20quality%20for%20shiny%20objects%2C%20often%20outperforming%0Areconstruction-exclusive%20state-of-the-art%20methods%20incapable%20of%20relighting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18468v1&entry.124074799=Read"},
{"title": "SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse\n  Observations", "author": "Shuting Zhao and Linxin Bai and Liangjing Shao and Ye Zhang and Xinrong Chen", "abstract": "  The growing applications of AR/VR increase the demand for real-time full-body\npose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint\nsignals from the head and hands, reconstructing a full-body pose remains\nchallenging due to the unconstrained lower body. Recent advancements often rely\non conventional neural networks and generative models to improve performance in\nthis task, such as Transformers and diffusion models. However, these approaches\nstruggle to strike a balance between achieving precise pose reconstruction and\nmaintaining fast inference speed. To overcome these challenges, a lightweight\nand efficient model, SSD-Poser, is designed for robust full-body motion\nestimation from sparse observations. SSD-Poser incorporates a well-designed\nhybrid encoder, State Space Attention Encoders, to adapt the state space\nduality to complex motion poses and enable real-time realistic pose\nreconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate\njitter caused by variable-frequency motion signals, remarkably enhancing the\nmotion smoothness. Comprehensive experiments on the AMASS dataset demonstrate\nthat SSD-Poser achieves exceptional accuracy and computational efficiency,\nshowing outstanding inference efficiency compared to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2504.18332v1", "date": "2025-04-25", "relevancy": 3.053, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6271}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6044}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSD-Poser%3A%20Avatar%20Pose%20Estimation%20with%20State%20Space%20Duality%20from%20Sparse%0A%20%20Observations&body=Title%3A%20SSD-Poser%3A%20Avatar%20Pose%20Estimation%20with%20State%20Space%20Duality%20from%20Sparse%0A%20%20Observations%0AAuthor%3A%20Shuting%20Zhao%20and%20Linxin%20Bai%20and%20Liangjing%20Shao%20and%20Ye%20Zhang%20and%20Xinrong%20Chen%0AAbstract%3A%20%20%20The%20growing%20applications%20of%20AR/VR%20increase%20the%20demand%20for%20real-time%20full-body%0Apose%20estimation%20from%20Head-Mounted%20Displays%20%28HMDs%29.%20Although%20HMDs%20provide%20joint%0Asignals%20from%20the%20head%20and%20hands%2C%20reconstructing%20a%20full-body%20pose%20remains%0Achallenging%20due%20to%20the%20unconstrained%20lower%20body.%20Recent%20advancements%20often%20rely%0Aon%20conventional%20neural%20networks%20and%20generative%20models%20to%20improve%20performance%20in%0Athis%20task%2C%20such%20as%20Transformers%20and%20diffusion%20models.%20However%2C%20these%20approaches%0Astruggle%20to%20strike%20a%20balance%20between%20achieving%20precise%20pose%20reconstruction%20and%0Amaintaining%20fast%20inference%20speed.%20To%20overcome%20these%20challenges%2C%20a%20lightweight%0Aand%20efficient%20model%2C%20SSD-Poser%2C%20is%20designed%20for%20robust%20full-body%20motion%0Aestimation%20from%20sparse%20observations.%20SSD-Poser%20incorporates%20a%20well-designed%0Ahybrid%20encoder%2C%20State%20Space%20Attention%20Encoders%2C%20to%20adapt%20the%20state%20space%0Aduality%20to%20complex%20motion%20poses%20and%20enable%20real-time%20realistic%20pose%0Areconstruction.%20Moreover%2C%20a%20Frequency-Aware%20Decoder%20is%20introduced%20to%20mitigate%0Ajitter%20caused%20by%20variable-frequency%20motion%20signals%2C%20remarkably%20enhancing%20the%0Amotion%20smoothness.%20Comprehensive%20experiments%20on%20the%20AMASS%20dataset%20demonstrate%0Athat%20SSD-Poser%20achieves%20exceptional%20accuracy%20and%20computational%20efficiency%2C%0Ashowing%20outstanding%20inference%20efficiency%20compared%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSD-Poser%253A%2520Avatar%2520Pose%2520Estimation%2520with%2520State%2520Space%2520Duality%2520from%2520Sparse%250A%2520%2520Observations%26entry.906535625%3DShuting%2520Zhao%2520and%2520Linxin%2520Bai%2520and%2520Liangjing%2520Shao%2520and%2520Ye%2520Zhang%2520and%2520Xinrong%2520Chen%26entry.1292438233%3D%2520%2520The%2520growing%2520applications%2520of%2520AR/VR%2520increase%2520the%2520demand%2520for%2520real-time%2520full-body%250Apose%2520estimation%2520from%2520Head-Mounted%2520Displays%2520%2528HMDs%2529.%2520Although%2520HMDs%2520provide%2520joint%250Asignals%2520from%2520the%2520head%2520and%2520hands%252C%2520reconstructing%2520a%2520full-body%2520pose%2520remains%250Achallenging%2520due%2520to%2520the%2520unconstrained%2520lower%2520body.%2520Recent%2520advancements%2520often%2520rely%250Aon%2520conventional%2520neural%2520networks%2520and%2520generative%2520models%2520to%2520improve%2520performance%2520in%250Athis%2520task%252C%2520such%2520as%2520Transformers%2520and%2520diffusion%2520models.%2520However%252C%2520these%2520approaches%250Astruggle%2520to%2520strike%2520a%2520balance%2520between%2520achieving%2520precise%2520pose%2520reconstruction%2520and%250Amaintaining%2520fast%2520inference%2520speed.%2520To%2520overcome%2520these%2520challenges%252C%2520a%2520lightweight%250Aand%2520efficient%2520model%252C%2520SSD-Poser%252C%2520is%2520designed%2520for%2520robust%2520full-body%2520motion%250Aestimation%2520from%2520sparse%2520observations.%2520SSD-Poser%2520incorporates%2520a%2520well-designed%250Ahybrid%2520encoder%252C%2520State%2520Space%2520Attention%2520Encoders%252C%2520to%2520adapt%2520the%2520state%2520space%250Aduality%2520to%2520complex%2520motion%2520poses%2520and%2520enable%2520real-time%2520realistic%2520pose%250Areconstruction.%2520Moreover%252C%2520a%2520Frequency-Aware%2520Decoder%2520is%2520introduced%2520to%2520mitigate%250Ajitter%2520caused%2520by%2520variable-frequency%2520motion%2520signals%252C%2520remarkably%2520enhancing%2520the%250Amotion%2520smoothness.%2520Comprehensive%2520experiments%2520on%2520the%2520AMASS%2520dataset%2520demonstrate%250Athat%2520SSD-Poser%2520achieves%2520exceptional%2520accuracy%2520and%2520computational%2520efficiency%252C%250Ashowing%2520outstanding%2520inference%2520efficiency%2520compared%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSD-Poser%3A%20Avatar%20Pose%20Estimation%20with%20State%20Space%20Duality%20from%20Sparse%0A%20%20Observations&entry.906535625=Shuting%20Zhao%20and%20Linxin%20Bai%20and%20Liangjing%20Shao%20and%20Ye%20Zhang%20and%20Xinrong%20Chen&entry.1292438233=%20%20The%20growing%20applications%20of%20AR/VR%20increase%20the%20demand%20for%20real-time%20full-body%0Apose%20estimation%20from%20Head-Mounted%20Displays%20%28HMDs%29.%20Although%20HMDs%20provide%20joint%0Asignals%20from%20the%20head%20and%20hands%2C%20reconstructing%20a%20full-body%20pose%20remains%0Achallenging%20due%20to%20the%20unconstrained%20lower%20body.%20Recent%20advancements%20often%20rely%0Aon%20conventional%20neural%20networks%20and%20generative%20models%20to%20improve%20performance%20in%0Athis%20task%2C%20such%20as%20Transformers%20and%20diffusion%20models.%20However%2C%20these%20approaches%0Astruggle%20to%20strike%20a%20balance%20between%20achieving%20precise%20pose%20reconstruction%20and%0Amaintaining%20fast%20inference%20speed.%20To%20overcome%20these%20challenges%2C%20a%20lightweight%0Aand%20efficient%20model%2C%20SSD-Poser%2C%20is%20designed%20for%20robust%20full-body%20motion%0Aestimation%20from%20sparse%20observations.%20SSD-Poser%20incorporates%20a%20well-designed%0Ahybrid%20encoder%2C%20State%20Space%20Attention%20Encoders%2C%20to%20adapt%20the%20state%20space%0Aduality%20to%20complex%20motion%20poses%20and%20enable%20real-time%20realistic%20pose%0Areconstruction.%20Moreover%2C%20a%20Frequency-Aware%20Decoder%20is%20introduced%20to%20mitigate%0Ajitter%20caused%20by%20variable-frequency%20motion%20signals%2C%20remarkably%20enhancing%20the%0Amotion%20smoothness.%20Comprehensive%20experiments%20on%20the%20AMASS%20dataset%20demonstrate%0Athat%20SSD-Poser%20achieves%20exceptional%20accuracy%20and%20computational%20efficiency%2C%0Ashowing%20outstanding%20inference%20efficiency%20compared%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18332v1&entry.124074799=Read"},
{"title": "Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation", "author": "Shivam Duggal and Yushi Hu and Oscar Michel and Aniruddha Kembhavi and William T. Freeman and Noah A. Smith and Ranjay Krishna and Antonio Torralba and Ali Farhadi and Wei-Chiu Ma", "abstract": "  Despite the unprecedented progress in the field of 3D generation, current\nsystems still often fail to produce high-quality 3D assets that are visually\nappealing and geometrically and semantically consistent across multiple\nviewpoints. To effectively assess the quality of the generated 3D data, there\nis a need for a reliable 3D evaluation tool. Unfortunately, existing 3D\nevaluation metrics often overlook the geometric quality of generated assets or\nmerely rely on black-box multimodal large language models for coarse\nassessment. In this paper, we introduce Eval3D, a fine-grained, interpretable\nevaluation tool that can faithfully evaluate the quality of generated 3D assets\nbased on various distinct yet complementary criteria. Our key observation is\nthat many desired properties of 3D generation, such as semantic and geometric\nconsistency, can be effectively captured by measuring the consistency among\nvarious foundation models and tools. We thus leverage a diverse set of models\nand tools as probes to evaluate the inconsistency of generated 3D assets across\ndifferent aspects. Compared to prior work, Eval3D provides pixel-wise\nmeasurement, enables accurate 3D spatial feedback, and aligns more closely with\nhuman judgments. We comprehensively evaluate existing 3D generation models\nusing Eval3D and highlight the limitations and challenges of current models.\n", "link": "http://arxiv.org/abs/2504.18509v1", "date": "2025-04-25", "relevancy": 3.0394, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6134}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6134}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eval3D%3A%20Interpretable%20and%20Fine-grained%20Evaluation%20for%203D%20Generation&body=Title%3A%20Eval3D%3A%20Interpretable%20and%20Fine-grained%20Evaluation%20for%203D%20Generation%0AAuthor%3A%20Shivam%20Duggal%20and%20Yushi%20Hu%20and%20Oscar%20Michel%20and%20Aniruddha%20Kembhavi%20and%20William%20T.%20Freeman%20and%20Noah%20A.%20Smith%20and%20Ranjay%20Krishna%20and%20Antonio%20Torralba%20and%20Ali%20Farhadi%20and%20Wei-Chiu%20Ma%0AAbstract%3A%20%20%20Despite%20the%20unprecedented%20progress%20in%20the%20field%20of%203D%20generation%2C%20current%0Asystems%20still%20often%20fail%20to%20produce%20high-quality%203D%20assets%20that%20are%20visually%0Aappealing%20and%20geometrically%20and%20semantically%20consistent%20across%20multiple%0Aviewpoints.%20To%20effectively%20assess%20the%20quality%20of%20the%20generated%203D%20data%2C%20there%0Ais%20a%20need%20for%20a%20reliable%203D%20evaluation%20tool.%20Unfortunately%2C%20existing%203D%0Aevaluation%20metrics%20often%20overlook%20the%20geometric%20quality%20of%20generated%20assets%20or%0Amerely%20rely%20on%20black-box%20multimodal%20large%20language%20models%20for%20coarse%0Aassessment.%20In%20this%20paper%2C%20we%20introduce%20Eval3D%2C%20a%20fine-grained%2C%20interpretable%0Aevaluation%20tool%20that%20can%20faithfully%20evaluate%20the%20quality%20of%20generated%203D%20assets%0Abased%20on%20various%20distinct%20yet%20complementary%20criteria.%20Our%20key%20observation%20is%0Athat%20many%20desired%20properties%20of%203D%20generation%2C%20such%20as%20semantic%20and%20geometric%0Aconsistency%2C%20can%20be%20effectively%20captured%20by%20measuring%20the%20consistency%20among%0Avarious%20foundation%20models%20and%20tools.%20We%20thus%20leverage%20a%20diverse%20set%20of%20models%0Aand%20tools%20as%20probes%20to%20evaluate%20the%20inconsistency%20of%20generated%203D%20assets%20across%0Adifferent%20aspects.%20Compared%20to%20prior%20work%2C%20Eval3D%20provides%20pixel-wise%0Ameasurement%2C%20enables%20accurate%203D%20spatial%20feedback%2C%20and%20aligns%20more%20closely%20with%0Ahuman%20judgments.%20We%20comprehensively%20evaluate%20existing%203D%20generation%20models%0Ausing%20Eval3D%20and%20highlight%20the%20limitations%20and%20challenges%20of%20current%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEval3D%253A%2520Interpretable%2520and%2520Fine-grained%2520Evaluation%2520for%25203D%2520Generation%26entry.906535625%3DShivam%2520Duggal%2520and%2520Yushi%2520Hu%2520and%2520Oscar%2520Michel%2520and%2520Aniruddha%2520Kembhavi%2520and%2520William%2520T.%2520Freeman%2520and%2520Noah%2520A.%2520Smith%2520and%2520Ranjay%2520Krishna%2520and%2520Antonio%2520Torralba%2520and%2520Ali%2520Farhadi%2520and%2520Wei-Chiu%2520Ma%26entry.1292438233%3D%2520%2520Despite%2520the%2520unprecedented%2520progress%2520in%2520the%2520field%2520of%25203D%2520generation%252C%2520current%250Asystems%2520still%2520often%2520fail%2520to%2520produce%2520high-quality%25203D%2520assets%2520that%2520are%2520visually%250Aappealing%2520and%2520geometrically%2520and%2520semantically%2520consistent%2520across%2520multiple%250Aviewpoints.%2520To%2520effectively%2520assess%2520the%2520quality%2520of%2520the%2520generated%25203D%2520data%252C%2520there%250Ais%2520a%2520need%2520for%2520a%2520reliable%25203D%2520evaluation%2520tool.%2520Unfortunately%252C%2520existing%25203D%250Aevaluation%2520metrics%2520often%2520overlook%2520the%2520geometric%2520quality%2520of%2520generated%2520assets%2520or%250Amerely%2520rely%2520on%2520black-box%2520multimodal%2520large%2520language%2520models%2520for%2520coarse%250Aassessment.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Eval3D%252C%2520a%2520fine-grained%252C%2520interpretable%250Aevaluation%2520tool%2520that%2520can%2520faithfully%2520evaluate%2520the%2520quality%2520of%2520generated%25203D%2520assets%250Abased%2520on%2520various%2520distinct%2520yet%2520complementary%2520criteria.%2520Our%2520key%2520observation%2520is%250Athat%2520many%2520desired%2520properties%2520of%25203D%2520generation%252C%2520such%2520as%2520semantic%2520and%2520geometric%250Aconsistency%252C%2520can%2520be%2520effectively%2520captured%2520by%2520measuring%2520the%2520consistency%2520among%250Avarious%2520foundation%2520models%2520and%2520tools.%2520We%2520thus%2520leverage%2520a%2520diverse%2520set%2520of%2520models%250Aand%2520tools%2520as%2520probes%2520to%2520evaluate%2520the%2520inconsistency%2520of%2520generated%25203D%2520assets%2520across%250Adifferent%2520aspects.%2520Compared%2520to%2520prior%2520work%252C%2520Eval3D%2520provides%2520pixel-wise%250Ameasurement%252C%2520enables%2520accurate%25203D%2520spatial%2520feedback%252C%2520and%2520aligns%2520more%2520closely%2520with%250Ahuman%2520judgments.%2520We%2520comprehensively%2520evaluate%2520existing%25203D%2520generation%2520models%250Ausing%2520Eval3D%2520and%2520highlight%2520the%2520limitations%2520and%2520challenges%2520of%2520current%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eval3D%3A%20Interpretable%20and%20Fine-grained%20Evaluation%20for%203D%20Generation&entry.906535625=Shivam%20Duggal%20and%20Yushi%20Hu%20and%20Oscar%20Michel%20and%20Aniruddha%20Kembhavi%20and%20William%20T.%20Freeman%20and%20Noah%20A.%20Smith%20and%20Ranjay%20Krishna%20and%20Antonio%20Torralba%20and%20Ali%20Farhadi%20and%20Wei-Chiu%20Ma&entry.1292438233=%20%20Despite%20the%20unprecedented%20progress%20in%20the%20field%20of%203D%20generation%2C%20current%0Asystems%20still%20often%20fail%20to%20produce%20high-quality%203D%20assets%20that%20are%20visually%0Aappealing%20and%20geometrically%20and%20semantically%20consistent%20across%20multiple%0Aviewpoints.%20To%20effectively%20assess%20the%20quality%20of%20the%20generated%203D%20data%2C%20there%0Ais%20a%20need%20for%20a%20reliable%203D%20evaluation%20tool.%20Unfortunately%2C%20existing%203D%0Aevaluation%20metrics%20often%20overlook%20the%20geometric%20quality%20of%20generated%20assets%20or%0Amerely%20rely%20on%20black-box%20multimodal%20large%20language%20models%20for%20coarse%0Aassessment.%20In%20this%20paper%2C%20we%20introduce%20Eval3D%2C%20a%20fine-grained%2C%20interpretable%0Aevaluation%20tool%20that%20can%20faithfully%20evaluate%20the%20quality%20of%20generated%203D%20assets%0Abased%20on%20various%20distinct%20yet%20complementary%20criteria.%20Our%20key%20observation%20is%0Athat%20many%20desired%20properties%20of%203D%20generation%2C%20such%20as%20semantic%20and%20geometric%0Aconsistency%2C%20can%20be%20effectively%20captured%20by%20measuring%20the%20consistency%20among%0Avarious%20foundation%20models%20and%20tools.%20We%20thus%20leverage%20a%20diverse%20set%20of%20models%0Aand%20tools%20as%20probes%20to%20evaluate%20the%20inconsistency%20of%20generated%203D%20assets%20across%0Adifferent%20aspects.%20Compared%20to%20prior%20work%2C%20Eval3D%20provides%20pixel-wise%0Ameasurement%2C%20enables%20accurate%203D%20spatial%20feedback%2C%20and%20aligns%20more%20closely%20with%0Ahuman%20judgments.%20We%20comprehensively%20evaluate%20existing%203D%20generation%20models%0Ausing%20Eval3D%20and%20highlight%20the%20limitations%20and%20challenges%20of%20current%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18509v1&entry.124074799=Read"},
{"title": "Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized\n  Gaussian Splat", "author": "Jonathan Michaux and Seth Isaacson and Challen Enninful Adu and Adam Li and Rahul Kashyap Swayampakula and Parker Ewen and Sean Rice and Katherine A. Skinner and Ram Vasudevan", "abstract": "  Neural Radiance Fields and Gaussian Splatting have recently transformed\ncomputer vision by enabling photo-realistic representations of complex scenes.\nHowever, they have seen limited application in real-world robotics tasks such\nas trajectory optimization. This is due to the difficulty in reasoning about\ncollisions in radiance models and the computational complexity associated with\noperating in dense models. This paper addresses these challenges by proposing\nSPLANNING, a risk-aware trajectory optimizer operating in a Gaussian Splatting\nmodel. This paper first derives a method to rigorously upper-bound the\nprobability of collision between a robot and a radiance field. Then, this paper\nintroduces a normalized reformulation of Gaussian Splatting that enables\nefficient computation of this collision bound. Finally, this paper presents a\nmethod to optimize trajectories that avoid collisions in a Gaussian Splat.\nExperiments show that SPLANNING outperforms state-of-the-art methods in\ngenerating collision-free trajectories in cluttered environments. The proposed\nsystem is also tested on a real-world robot manipulator. A project page is\navailable at https://roahmlab.github.io/splanning.\n", "link": "http://arxiv.org/abs/2409.16915v2", "date": "2025-04-25", "relevancy": 2.9913, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6471}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5778}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Make%20a%20Splan%3A%20Risk-Aware%20Trajectory%20Optimization%20in%20a%20Normalized%0A%20%20Gaussian%20Splat&body=Title%3A%20Let%27s%20Make%20a%20Splan%3A%20Risk-Aware%20Trajectory%20Optimization%20in%20a%20Normalized%0A%20%20Gaussian%20Splat%0AAuthor%3A%20Jonathan%20Michaux%20and%20Seth%20Isaacson%20and%20Challen%20Enninful%20Adu%20and%20Adam%20Li%20and%20Rahul%20Kashyap%20Swayampakula%20and%20Parker%20Ewen%20and%20Sean%20Rice%20and%20Katherine%20A.%20Skinner%20and%20Ram%20Vasudevan%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20and%20Gaussian%20Splatting%20have%20recently%20transformed%0Acomputer%20vision%20by%20enabling%20photo-realistic%20representations%20of%20complex%20scenes.%0AHowever%2C%20they%20have%20seen%20limited%20application%20in%20real-world%20robotics%20tasks%20such%0Aas%20trajectory%20optimization.%20This%20is%20due%20to%20the%20difficulty%20in%20reasoning%20about%0Acollisions%20in%20radiance%20models%20and%20the%20computational%20complexity%20associated%20with%0Aoperating%20in%20dense%20models.%20This%20paper%20addresses%20these%20challenges%20by%20proposing%0ASPLANNING%2C%20a%20risk-aware%20trajectory%20optimizer%20operating%20in%20a%20Gaussian%20Splatting%0Amodel.%20This%20paper%20first%20derives%20a%20method%20to%20rigorously%20upper-bound%20the%0Aprobability%20of%20collision%20between%20a%20robot%20and%20a%20radiance%20field.%20Then%2C%20this%20paper%0Aintroduces%20a%20normalized%20reformulation%20of%20Gaussian%20Splatting%20that%20enables%0Aefficient%20computation%20of%20this%20collision%20bound.%20Finally%2C%20this%20paper%20presents%20a%0Amethod%20to%20optimize%20trajectories%20that%20avoid%20collisions%20in%20a%20Gaussian%20Splat.%0AExperiments%20show%20that%20SPLANNING%20outperforms%20state-of-the-art%20methods%20in%0Agenerating%20collision-free%20trajectories%20in%20cluttered%20environments.%20The%20proposed%0Asystem%20is%20also%20tested%20on%20a%20real-world%20robot%20manipulator.%20A%20project%20page%20is%0Aavailable%20at%20https%3A//roahmlab.github.io/splanning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16915v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Make%2520a%2520Splan%253A%2520Risk-Aware%2520Trajectory%2520Optimization%2520in%2520a%2520Normalized%250A%2520%2520Gaussian%2520Splat%26entry.906535625%3DJonathan%2520Michaux%2520and%2520Seth%2520Isaacson%2520and%2520Challen%2520Enninful%2520Adu%2520and%2520Adam%2520Li%2520and%2520Rahul%2520Kashyap%2520Swayampakula%2520and%2520Parker%2520Ewen%2520and%2520Sean%2520Rice%2520and%2520Katherine%2520A.%2520Skinner%2520and%2520Ram%2520Vasudevan%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520and%2520Gaussian%2520Splatting%2520have%2520recently%2520transformed%250Acomputer%2520vision%2520by%2520enabling%2520photo-realistic%2520representations%2520of%2520complex%2520scenes.%250AHowever%252C%2520they%2520have%2520seen%2520limited%2520application%2520in%2520real-world%2520robotics%2520tasks%2520such%250Aas%2520trajectory%2520optimization.%2520This%2520is%2520due%2520to%2520the%2520difficulty%2520in%2520reasoning%2520about%250Acollisions%2520in%2520radiance%2520models%2520and%2520the%2520computational%2520complexity%2520associated%2520with%250Aoperating%2520in%2520dense%2520models.%2520This%2520paper%2520addresses%2520these%2520challenges%2520by%2520proposing%250ASPLANNING%252C%2520a%2520risk-aware%2520trajectory%2520optimizer%2520operating%2520in%2520a%2520Gaussian%2520Splatting%250Amodel.%2520This%2520paper%2520first%2520derives%2520a%2520method%2520to%2520rigorously%2520upper-bound%2520the%250Aprobability%2520of%2520collision%2520between%2520a%2520robot%2520and%2520a%2520radiance%2520field.%2520Then%252C%2520this%2520paper%250Aintroduces%2520a%2520normalized%2520reformulation%2520of%2520Gaussian%2520Splatting%2520that%2520enables%250Aefficient%2520computation%2520of%2520this%2520collision%2520bound.%2520Finally%252C%2520this%2520paper%2520presents%2520a%250Amethod%2520to%2520optimize%2520trajectories%2520that%2520avoid%2520collisions%2520in%2520a%2520Gaussian%2520Splat.%250AExperiments%2520show%2520that%2520SPLANNING%2520outperforms%2520state-of-the-art%2520methods%2520in%250Agenerating%2520collision-free%2520trajectories%2520in%2520cluttered%2520environments.%2520The%2520proposed%250Asystem%2520is%2520also%2520tested%2520on%2520a%2520real-world%2520robot%2520manipulator.%2520A%2520project%2520page%2520is%250Aavailable%2520at%2520https%253A//roahmlab.github.io/splanning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16915v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Make%20a%20Splan%3A%20Risk-Aware%20Trajectory%20Optimization%20in%20a%20Normalized%0A%20%20Gaussian%20Splat&entry.906535625=Jonathan%20Michaux%20and%20Seth%20Isaacson%20and%20Challen%20Enninful%20Adu%20and%20Adam%20Li%20and%20Rahul%20Kashyap%20Swayampakula%20and%20Parker%20Ewen%20and%20Sean%20Rice%20and%20Katherine%20A.%20Skinner%20and%20Ram%20Vasudevan&entry.1292438233=%20%20Neural%20Radiance%20Fields%20and%20Gaussian%20Splatting%20have%20recently%20transformed%0Acomputer%20vision%20by%20enabling%20photo-realistic%20representations%20of%20complex%20scenes.%0AHowever%2C%20they%20have%20seen%20limited%20application%20in%20real-world%20robotics%20tasks%20such%0Aas%20trajectory%20optimization.%20This%20is%20due%20to%20the%20difficulty%20in%20reasoning%20about%0Acollisions%20in%20radiance%20models%20and%20the%20computational%20complexity%20associated%20with%0Aoperating%20in%20dense%20models.%20This%20paper%20addresses%20these%20challenges%20by%20proposing%0ASPLANNING%2C%20a%20risk-aware%20trajectory%20optimizer%20operating%20in%20a%20Gaussian%20Splatting%0Amodel.%20This%20paper%20first%20derives%20a%20method%20to%20rigorously%20upper-bound%20the%0Aprobability%20of%20collision%20between%20a%20robot%20and%20a%20radiance%20field.%20Then%2C%20this%20paper%0Aintroduces%20a%20normalized%20reformulation%20of%20Gaussian%20Splatting%20that%20enables%0Aefficient%20computation%20of%20this%20collision%20bound.%20Finally%2C%20this%20paper%20presents%20a%0Amethod%20to%20optimize%20trajectories%20that%20avoid%20collisions%20in%20a%20Gaussian%20Splat.%0AExperiments%20show%20that%20SPLANNING%20outperforms%20state-of-the-art%20methods%20in%0Agenerating%20collision-free%20trajectories%20in%20cluttered%20environments.%20The%20proposed%0Asystem%20is%20also%20tested%20on%20a%20real-world%20robot%20manipulator.%20A%20project%20page%20is%0Aavailable%20at%20https%3A//roahmlab.github.io/splanning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16915v2&entry.124074799=Read"},
{"title": "STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D\n  Gaussian Splatting", "author": "Yunze Deng and Haijun Xiong and Bin Feng and Xinggang Wang and Wenyu Liu", "abstract": "  Text-to-4D generation is rapidly developing and widely applied in various\nscenarios. However, existing methods often fail to incorporate adequate\nspatio-temporal modeling and prompt alignment within a unified framework,\nresulting in temporal inconsistencies, geometric distortions, or low-quality 4D\ncontent that deviates from the provided texts. Therefore, we propose STP4D, a\nnovel approach that aims to integrate comprehensive spatio-temporal-prompt\nconsistency modeling for high-quality text-to-4D generation. Specifically,\nSTP4D employs three carefully designed modules: Time-varying Prompt Embedding,\nGeometric Information Enhancement, and Temporal Extension Deformation, which\ncollaborate to accomplish this goal. Furthermore, STP4D is among the first\nmethods to exploit the Diffusion model to generate 4D Gaussians, combining the\nfine-grained modeling capabilities and the real-time rendering process of 4DGS\nwith the rapid inference speed of the Diffusion model. Extensive experiments\ndemonstrate that STP4D excels in generating high-fidelity 4D content with\nexceptional efficiency (approximately 4.6s per asset), surpassing existing\nmethods in both quality and speed.\n", "link": "http://arxiv.org/abs/2504.18318v1", "date": "2025-04-25", "relevancy": 2.8985, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6038}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5677}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STP4D%3A%20Spatio-Temporal-Prompt%20Consistent%20Modeling%20for%20Text-to-4D%0A%20%20Gaussian%20Splatting&body=Title%3A%20STP4D%3A%20Spatio-Temporal-Prompt%20Consistent%20Modeling%20for%20Text-to-4D%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Yunze%20Deng%20and%20Haijun%20Xiong%20and%20Bin%20Feng%20and%20Xinggang%20Wang%20and%20Wenyu%20Liu%0AAbstract%3A%20%20%20Text-to-4D%20generation%20is%20rapidly%20developing%20and%20widely%20applied%20in%20various%0Ascenarios.%20However%2C%20existing%20methods%20often%20fail%20to%20incorporate%20adequate%0Aspatio-temporal%20modeling%20and%20prompt%20alignment%20within%20a%20unified%20framework%2C%0Aresulting%20in%20temporal%20inconsistencies%2C%20geometric%20distortions%2C%20or%20low-quality%204D%0Acontent%20that%20deviates%20from%20the%20provided%20texts.%20Therefore%2C%20we%20propose%20STP4D%2C%20a%0Anovel%20approach%20that%20aims%20to%20integrate%20comprehensive%20spatio-temporal-prompt%0Aconsistency%20modeling%20for%20high-quality%20text-to-4D%20generation.%20Specifically%2C%0ASTP4D%20employs%20three%20carefully%20designed%20modules%3A%20Time-varying%20Prompt%20Embedding%2C%0AGeometric%20Information%20Enhancement%2C%20and%20Temporal%20Extension%20Deformation%2C%20which%0Acollaborate%20to%20accomplish%20this%20goal.%20Furthermore%2C%20STP4D%20is%20among%20the%20first%0Amethods%20to%20exploit%20the%20Diffusion%20model%20to%20generate%204D%20Gaussians%2C%20combining%20the%0Afine-grained%20modeling%20capabilities%20and%20the%20real-time%20rendering%20process%20of%204DGS%0Awith%20the%20rapid%20inference%20speed%20of%20the%20Diffusion%20model.%20Extensive%20experiments%0Ademonstrate%20that%20STP4D%20excels%20in%20generating%20high-fidelity%204D%20content%20with%0Aexceptional%20efficiency%20%28approximately%204.6s%20per%20asset%29%2C%20surpassing%20existing%0Amethods%20in%20both%20quality%20and%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTP4D%253A%2520Spatio-Temporal-Prompt%2520Consistent%2520Modeling%2520for%2520Text-to-4D%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DYunze%2520Deng%2520and%2520Haijun%2520Xiong%2520and%2520Bin%2520Feng%2520and%2520Xinggang%2520Wang%2520and%2520Wenyu%2520Liu%26entry.1292438233%3D%2520%2520Text-to-4D%2520generation%2520is%2520rapidly%2520developing%2520and%2520widely%2520applied%2520in%2520various%250Ascenarios.%2520However%252C%2520existing%2520methods%2520often%2520fail%2520to%2520incorporate%2520adequate%250Aspatio-temporal%2520modeling%2520and%2520prompt%2520alignment%2520within%2520a%2520unified%2520framework%252C%250Aresulting%2520in%2520temporal%2520inconsistencies%252C%2520geometric%2520distortions%252C%2520or%2520low-quality%25204D%250Acontent%2520that%2520deviates%2520from%2520the%2520provided%2520texts.%2520Therefore%252C%2520we%2520propose%2520STP4D%252C%2520a%250Anovel%2520approach%2520that%2520aims%2520to%2520integrate%2520comprehensive%2520spatio-temporal-prompt%250Aconsistency%2520modeling%2520for%2520high-quality%2520text-to-4D%2520generation.%2520Specifically%252C%250ASTP4D%2520employs%2520three%2520carefully%2520designed%2520modules%253A%2520Time-varying%2520Prompt%2520Embedding%252C%250AGeometric%2520Information%2520Enhancement%252C%2520and%2520Temporal%2520Extension%2520Deformation%252C%2520which%250Acollaborate%2520to%2520accomplish%2520this%2520goal.%2520Furthermore%252C%2520STP4D%2520is%2520among%2520the%2520first%250Amethods%2520to%2520exploit%2520the%2520Diffusion%2520model%2520to%2520generate%25204D%2520Gaussians%252C%2520combining%2520the%250Afine-grained%2520modeling%2520capabilities%2520and%2520the%2520real-time%2520rendering%2520process%2520of%25204DGS%250Awith%2520the%2520rapid%2520inference%2520speed%2520of%2520the%2520Diffusion%2520model.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520STP4D%2520excels%2520in%2520generating%2520high-fidelity%25204D%2520content%2520with%250Aexceptional%2520efficiency%2520%2528approximately%25204.6s%2520per%2520asset%2529%252C%2520surpassing%2520existing%250Amethods%2520in%2520both%2520quality%2520and%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STP4D%3A%20Spatio-Temporal-Prompt%20Consistent%20Modeling%20for%20Text-to-4D%0A%20%20Gaussian%20Splatting&entry.906535625=Yunze%20Deng%20and%20Haijun%20Xiong%20and%20Bin%20Feng%20and%20Xinggang%20Wang%20and%20Wenyu%20Liu&entry.1292438233=%20%20Text-to-4D%20generation%20is%20rapidly%20developing%20and%20widely%20applied%20in%20various%0Ascenarios.%20However%2C%20existing%20methods%20often%20fail%20to%20incorporate%20adequate%0Aspatio-temporal%20modeling%20and%20prompt%20alignment%20within%20a%20unified%20framework%2C%0Aresulting%20in%20temporal%20inconsistencies%2C%20geometric%20distortions%2C%20or%20low-quality%204D%0Acontent%20that%20deviates%20from%20the%20provided%20texts.%20Therefore%2C%20we%20propose%20STP4D%2C%20a%0Anovel%20approach%20that%20aims%20to%20integrate%20comprehensive%20spatio-temporal-prompt%0Aconsistency%20modeling%20for%20high-quality%20text-to-4D%20generation.%20Specifically%2C%0ASTP4D%20employs%20three%20carefully%20designed%20modules%3A%20Time-varying%20Prompt%20Embedding%2C%0AGeometric%20Information%20Enhancement%2C%20and%20Temporal%20Extension%20Deformation%2C%20which%0Acollaborate%20to%20accomplish%20this%20goal.%20Furthermore%2C%20STP4D%20is%20among%20the%20first%0Amethods%20to%20exploit%20the%20Diffusion%20model%20to%20generate%204D%20Gaussians%2C%20combining%20the%0Afine-grained%20modeling%20capabilities%20and%20the%20real-time%20rendering%20process%20of%204DGS%0Awith%20the%20rapid%20inference%20speed%20of%20the%20Diffusion%20model.%20Extensive%20experiments%0Ademonstrate%20that%20STP4D%20excels%20in%20generating%20high-fidelity%204D%20content%20with%0Aexceptional%20efficiency%20%28approximately%204.6s%20per%20asset%29%2C%20surpassing%20existing%0Amethods%20in%20both%20quality%20and%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18318v1&entry.124074799=Read"},
{"title": "StoryGPT-V: Large Language Models as Consistent Story Visualizers", "author": "Xiaoqian Shen and Mohamed Elhoseiny", "abstract": "  Recent generative models have demonstrated impressive capabilities in\ngenerating realistic and visually pleasing images grounded on textual prompts.\nNevertheless, a significant challenge remains in applying these models for the\nmore intricate task of story visualization. Since it requires resolving\npronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution,\nand ensuring consistent characters and background synthesis across frames. Yet,\nthe emerging Large Language Model (LLM) showcases robust reasoning abilities to\nnavigate through ambiguous references and process extensive sequences.\nTherefore, we introduce \\emph{StoryGPT-V}, which leverages the merits of the\nlatent diffusion (LDM) and LLM to produce images with consistent and\nhigh-quality characters grounded on given story descriptions. First, we train a\ncharacter-aware LDM, which takes character-augmented semantic embedding as\ninput and includes the supervision of the cross-attention map using character\nsegmentation masks, aiming to enhance character generation accuracy and\nfaithfulness. In the second stage, we enable an alignment between the output of\nLLM and the character-augmented embedding residing in the input space of the\nfirst-stage model. This harnesses the reasoning ability of LLM to address\nambiguous references and the comprehension capability to memorize the context.\nWe conduct comprehensive experiments on two visual story visualization\nbenchmarks. Our model reports superior quantitative results and consistently\ngenerates accurate characters of remarkable quality with low memory\nconsumption. Our code is publicly available at:\n\\href{https://xiaoqian-shen.github.io/StoryGPT-V}{https://xiaoqian-shen.github.io/StoryGPT-V}.\n", "link": "http://arxiv.org/abs/2312.02252v3", "date": "2025-04-25", "relevancy": 2.8823, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5813}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5813}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StoryGPT-V%3A%20Large%20Language%20Models%20as%20Consistent%20Story%20Visualizers&body=Title%3A%20StoryGPT-V%3A%20Large%20Language%20Models%20as%20Consistent%20Story%20Visualizers%0AAuthor%3A%20Xiaoqian%20Shen%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20Recent%20generative%20models%20have%20demonstrated%20impressive%20capabilities%20in%0Agenerating%20realistic%20and%20visually%20pleasing%20images%20grounded%20on%20textual%20prompts.%0ANevertheless%2C%20a%20significant%20challenge%20remains%20in%20applying%20these%20models%20for%20the%0Amore%20intricate%20task%20of%20story%20visualization.%20Since%20it%20requires%20resolving%0Apronouns%20%28he%2C%20she%2C%20they%29%20in%20the%20frame%20descriptions%2C%20i.e.%2C%20anaphora%20resolution%2C%0Aand%20ensuring%20consistent%20characters%20and%20background%20synthesis%20across%20frames.%20Yet%2C%0Athe%20emerging%20Large%20Language%20Model%20%28LLM%29%20showcases%20robust%20reasoning%20abilities%20to%0Anavigate%20through%20ambiguous%20references%20and%20process%20extensive%20sequences.%0ATherefore%2C%20we%20introduce%20%5Cemph%7BStoryGPT-V%7D%2C%20which%20leverages%20the%20merits%20of%20the%0Alatent%20diffusion%20%28LDM%29%20and%20LLM%20to%20produce%20images%20with%20consistent%20and%0Ahigh-quality%20characters%20grounded%20on%20given%20story%20descriptions.%20First%2C%20we%20train%20a%0Acharacter-aware%20LDM%2C%20which%20takes%20character-augmented%20semantic%20embedding%20as%0Ainput%20and%20includes%20the%20supervision%20of%20the%20cross-attention%20map%20using%20character%0Asegmentation%20masks%2C%20aiming%20to%20enhance%20character%20generation%20accuracy%20and%0Afaithfulness.%20In%20the%20second%20stage%2C%20we%20enable%20an%20alignment%20between%20the%20output%20of%0ALLM%20and%20the%20character-augmented%20embedding%20residing%20in%20the%20input%20space%20of%20the%0Afirst-stage%20model.%20This%20harnesses%20the%20reasoning%20ability%20of%20LLM%20to%20address%0Aambiguous%20references%20and%20the%20comprehension%20capability%20to%20memorize%20the%20context.%0AWe%20conduct%20comprehensive%20experiments%20on%20two%20visual%20story%20visualization%0Abenchmarks.%20Our%20model%20reports%20superior%20quantitative%20results%20and%20consistently%0Agenerates%20accurate%20characters%20of%20remarkable%20quality%20with%20low%20memory%0Aconsumption.%20Our%20code%20is%20publicly%20available%20at%3A%0A%5Chref%7Bhttps%3A//xiaoqian-shen.github.io/StoryGPT-V%7D%7Bhttps%3A//xiaoqian-shen.github.io/StoryGPT-V%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02252v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStoryGPT-V%253A%2520Large%2520Language%2520Models%2520as%2520Consistent%2520Story%2520Visualizers%26entry.906535625%3DXiaoqian%2520Shen%2520and%2520Mohamed%2520Elhoseiny%26entry.1292438233%3D%2520%2520Recent%2520generative%2520models%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%250Agenerating%2520realistic%2520and%2520visually%2520pleasing%2520images%2520grounded%2520on%2520textual%2520prompts.%250ANevertheless%252C%2520a%2520significant%2520challenge%2520remains%2520in%2520applying%2520these%2520models%2520for%2520the%250Amore%2520intricate%2520task%2520of%2520story%2520visualization.%2520Since%2520it%2520requires%2520resolving%250Apronouns%2520%2528he%252C%2520she%252C%2520they%2529%2520in%2520the%2520frame%2520descriptions%252C%2520i.e.%252C%2520anaphora%2520resolution%252C%250Aand%2520ensuring%2520consistent%2520characters%2520and%2520background%2520synthesis%2520across%2520frames.%2520Yet%252C%250Athe%2520emerging%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520showcases%2520robust%2520reasoning%2520abilities%2520to%250Anavigate%2520through%2520ambiguous%2520references%2520and%2520process%2520extensive%2520sequences.%250ATherefore%252C%2520we%2520introduce%2520%255Cemph%257BStoryGPT-V%257D%252C%2520which%2520leverages%2520the%2520merits%2520of%2520the%250Alatent%2520diffusion%2520%2528LDM%2529%2520and%2520LLM%2520to%2520produce%2520images%2520with%2520consistent%2520and%250Ahigh-quality%2520characters%2520grounded%2520on%2520given%2520story%2520descriptions.%2520First%252C%2520we%2520train%2520a%250Acharacter-aware%2520LDM%252C%2520which%2520takes%2520character-augmented%2520semantic%2520embedding%2520as%250Ainput%2520and%2520includes%2520the%2520supervision%2520of%2520the%2520cross-attention%2520map%2520using%2520character%250Asegmentation%2520masks%252C%2520aiming%2520to%2520enhance%2520character%2520generation%2520accuracy%2520and%250Afaithfulness.%2520In%2520the%2520second%2520stage%252C%2520we%2520enable%2520an%2520alignment%2520between%2520the%2520output%2520of%250ALLM%2520and%2520the%2520character-augmented%2520embedding%2520residing%2520in%2520the%2520input%2520space%2520of%2520the%250Afirst-stage%2520model.%2520This%2520harnesses%2520the%2520reasoning%2520ability%2520of%2520LLM%2520to%2520address%250Aambiguous%2520references%2520and%2520the%2520comprehension%2520capability%2520to%2520memorize%2520the%2520context.%250AWe%2520conduct%2520comprehensive%2520experiments%2520on%2520two%2520visual%2520story%2520visualization%250Abenchmarks.%2520Our%2520model%2520reports%2520superior%2520quantitative%2520results%2520and%2520consistently%250Agenerates%2520accurate%2520characters%2520of%2520remarkable%2520quality%2520with%2520low%2520memory%250Aconsumption.%2520Our%2520code%2520is%2520publicly%2520available%2520at%253A%250A%255Chref%257Bhttps%253A//xiaoqian-shen.github.io/StoryGPT-V%257D%257Bhttps%253A//xiaoqian-shen.github.io/StoryGPT-V%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02252v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StoryGPT-V%3A%20Large%20Language%20Models%20as%20Consistent%20Story%20Visualizers&entry.906535625=Xiaoqian%20Shen%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20Recent%20generative%20models%20have%20demonstrated%20impressive%20capabilities%20in%0Agenerating%20realistic%20and%20visually%20pleasing%20images%20grounded%20on%20textual%20prompts.%0ANevertheless%2C%20a%20significant%20challenge%20remains%20in%20applying%20these%20models%20for%20the%0Amore%20intricate%20task%20of%20story%20visualization.%20Since%20it%20requires%20resolving%0Apronouns%20%28he%2C%20she%2C%20they%29%20in%20the%20frame%20descriptions%2C%20i.e.%2C%20anaphora%20resolution%2C%0Aand%20ensuring%20consistent%20characters%20and%20background%20synthesis%20across%20frames.%20Yet%2C%0Athe%20emerging%20Large%20Language%20Model%20%28LLM%29%20showcases%20robust%20reasoning%20abilities%20to%0Anavigate%20through%20ambiguous%20references%20and%20process%20extensive%20sequences.%0ATherefore%2C%20we%20introduce%20%5Cemph%7BStoryGPT-V%7D%2C%20which%20leverages%20the%20merits%20of%20the%0Alatent%20diffusion%20%28LDM%29%20and%20LLM%20to%20produce%20images%20with%20consistent%20and%0Ahigh-quality%20characters%20grounded%20on%20given%20story%20descriptions.%20First%2C%20we%20train%20a%0Acharacter-aware%20LDM%2C%20which%20takes%20character-augmented%20semantic%20embedding%20as%0Ainput%20and%20includes%20the%20supervision%20of%20the%20cross-attention%20map%20using%20character%0Asegmentation%20masks%2C%20aiming%20to%20enhance%20character%20generation%20accuracy%20and%0Afaithfulness.%20In%20the%20second%20stage%2C%20we%20enable%20an%20alignment%20between%20the%20output%20of%0ALLM%20and%20the%20character-augmented%20embedding%20residing%20in%20the%20input%20space%20of%20the%0Afirst-stage%20model.%20This%20harnesses%20the%20reasoning%20ability%20of%20LLM%20to%20address%0Aambiguous%20references%20and%20the%20comprehension%20capability%20to%20memorize%20the%20context.%0AWe%20conduct%20comprehensive%20experiments%20on%20two%20visual%20story%20visualization%0Abenchmarks.%20Our%20model%20reports%20superior%20quantitative%20results%20and%20consistently%0Agenerates%20accurate%20characters%20of%20remarkable%20quality%20with%20low%20memory%0Aconsumption.%20Our%20code%20is%20publicly%20available%20at%3A%0A%5Chref%7Bhttps%3A//xiaoqian-shen.github.io/StoryGPT-V%7D%7Bhttps%3A//xiaoqian-shen.github.io/StoryGPT-V%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02252v3&entry.124074799=Read"},
{"title": "Unsupervised Visual Chain-of-Thought Reasoning via Preference\n  Optimization", "author": "Kesen Zhao and Beier Zhu and Qianru Sun and Hanwang Zhang", "abstract": "  Chain-of-thought (CoT) reasoning greatly improves the interpretability and\nproblem-solving abilities of multimodal large language models (MLLMs). However,\nexisting approaches are focused on text CoT, limiting their ability to leverage\nvisual cues. Visual CoT remains underexplored, and the only work is based on\nsupervised fine-tuning (SFT) that relies on extensive labeled bounding-box data\nand is hard to generalize to unseen cases. In this paper, we introduce\nUnsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT\nreasoning via preference optimization. UV-CoT performs preference comparisons\nbetween model-generated bounding boxes (one is preferred and the other is\ndis-preferred), eliminating the need for bounding-box annotations. We get such\npreference data by introducing an automatic data generation pipeline. Given an\nimage, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using\na template prompt and then answers the question using each bounded region as\ninput. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these\nrankings serve as supervision to train the target MLLM with UV-CoT by\nminimizing negative log-likelihood losses. By emulating human\nperception--identifying key regions and reasoning based on them--UV-CoT can\nimprove visual comprehension, particularly in spatial reasoning tasks where\ntextual descriptions alone fall short. Our experiments on six datasets\ndemonstrate the superiority of UV-CoT, compared to the state-of-the-art textual\nand visual CoT methods. Our zero-shot testing on four unseen datasets shows the\nstrong generalization of UV-CoT. The code is available in\nhttps://github.com/kesenzhao/UV-CoT.\n", "link": "http://arxiv.org/abs/2504.18397v1", "date": "2025-04-25", "relevancy": 2.8457, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5807}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5807}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Visual%20Chain-of-Thought%20Reasoning%20via%20Preference%0A%20%20Optimization&body=Title%3A%20Unsupervised%20Visual%20Chain-of-Thought%20Reasoning%20via%20Preference%0A%20%20Optimization%0AAuthor%3A%20Kesen%20Zhao%20and%20Beier%20Zhu%20and%20Qianru%20Sun%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20Chain-of-thought%20%28CoT%29%20reasoning%20greatly%20improves%20the%20interpretability%20and%0Aproblem-solving%20abilities%20of%20multimodal%20large%20language%20models%20%28MLLMs%29.%20However%2C%0Aexisting%20approaches%20are%20focused%20on%20text%20CoT%2C%20limiting%20their%20ability%20to%20leverage%0Avisual%20cues.%20Visual%20CoT%20remains%20underexplored%2C%20and%20the%20only%20work%20is%20based%20on%0Asupervised%20fine-tuning%20%28SFT%29%20that%20relies%20on%20extensive%20labeled%20bounding-box%20data%0Aand%20is%20hard%20to%20generalize%20to%20unseen%20cases.%20In%20this%20paper%2C%20we%20introduce%0AUnsupervised%20Visual%20CoT%20%28UV-CoT%29%2C%20a%20novel%20framework%20for%20image-level%20CoT%0Areasoning%20via%20preference%20optimization.%20UV-CoT%20performs%20preference%20comparisons%0Abetween%20model-generated%20bounding%20boxes%20%28one%20is%20preferred%20and%20the%20other%20is%0Adis-preferred%29%2C%20eliminating%20the%20need%20for%20bounding-box%20annotations.%20We%20get%20such%0Apreference%20data%20by%20introducing%20an%20automatic%20data%20generation%20pipeline.%20Given%20an%0Aimage%2C%20our%20target%20MLLM%20%28e.g.%2C%20LLaVA-1.5-7B%29%20generates%20seed%20bounding%20boxes%20using%0Aa%20template%20prompt%20and%20then%20answers%20the%20question%20using%20each%20bounded%20region%20as%0Ainput.%20An%20evaluator%20MLLM%20%28e.g.%2C%20OmniLLM-12B%29%20ranks%20the%20responses%2C%20and%20these%0Arankings%20serve%20as%20supervision%20to%20train%20the%20target%20MLLM%20with%20UV-CoT%20by%0Aminimizing%20negative%20log-likelihood%20losses.%20By%20emulating%20human%0Aperception--identifying%20key%20regions%20and%20reasoning%20based%20on%20them--UV-CoT%20can%0Aimprove%20visual%20comprehension%2C%20particularly%20in%20spatial%20reasoning%20tasks%20where%0Atextual%20descriptions%20alone%20fall%20short.%20Our%20experiments%20on%20six%20datasets%0Ademonstrate%20the%20superiority%20of%20UV-CoT%2C%20compared%20to%20the%20state-of-the-art%20textual%0Aand%20visual%20CoT%20methods.%20Our%20zero-shot%20testing%20on%20four%20unseen%20datasets%20shows%20the%0Astrong%20generalization%20of%20UV-CoT.%20The%20code%20is%20available%20in%0Ahttps%3A//github.com/kesenzhao/UV-CoT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Visual%2520Chain-of-Thought%2520Reasoning%2520via%2520Preference%250A%2520%2520Optimization%26entry.906535625%3DKesen%2520Zhao%2520and%2520Beier%2520Zhu%2520and%2520Qianru%2520Sun%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520Chain-of-thought%2520%2528CoT%2529%2520reasoning%2520greatly%2520improves%2520the%2520interpretability%2520and%250Aproblem-solving%2520abilities%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520However%252C%250Aexisting%2520approaches%2520are%2520focused%2520on%2520text%2520CoT%252C%2520limiting%2520their%2520ability%2520to%2520leverage%250Avisual%2520cues.%2520Visual%2520CoT%2520remains%2520underexplored%252C%2520and%2520the%2520only%2520work%2520is%2520based%2520on%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520that%2520relies%2520on%2520extensive%2520labeled%2520bounding-box%2520data%250Aand%2520is%2520hard%2520to%2520generalize%2520to%2520unseen%2520cases.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AUnsupervised%2520Visual%2520CoT%2520%2528UV-CoT%2529%252C%2520a%2520novel%2520framework%2520for%2520image-level%2520CoT%250Areasoning%2520via%2520preference%2520optimization.%2520UV-CoT%2520performs%2520preference%2520comparisons%250Abetween%2520model-generated%2520bounding%2520boxes%2520%2528one%2520is%2520preferred%2520and%2520the%2520other%2520is%250Adis-preferred%2529%252C%2520eliminating%2520the%2520need%2520for%2520bounding-box%2520annotations.%2520We%2520get%2520such%250Apreference%2520data%2520by%2520introducing%2520an%2520automatic%2520data%2520generation%2520pipeline.%2520Given%2520an%250Aimage%252C%2520our%2520target%2520MLLM%2520%2528e.g.%252C%2520LLaVA-1.5-7B%2529%2520generates%2520seed%2520bounding%2520boxes%2520using%250Aa%2520template%2520prompt%2520and%2520then%2520answers%2520the%2520question%2520using%2520each%2520bounded%2520region%2520as%250Ainput.%2520An%2520evaluator%2520MLLM%2520%2528e.g.%252C%2520OmniLLM-12B%2529%2520ranks%2520the%2520responses%252C%2520and%2520these%250Arankings%2520serve%2520as%2520supervision%2520to%2520train%2520the%2520target%2520MLLM%2520with%2520UV-CoT%2520by%250Aminimizing%2520negative%2520log-likelihood%2520losses.%2520By%2520emulating%2520human%250Aperception--identifying%2520key%2520regions%2520and%2520reasoning%2520based%2520on%2520them--UV-CoT%2520can%250Aimprove%2520visual%2520comprehension%252C%2520particularly%2520in%2520spatial%2520reasoning%2520tasks%2520where%250Atextual%2520descriptions%2520alone%2520fall%2520short.%2520Our%2520experiments%2520on%2520six%2520datasets%250Ademonstrate%2520the%2520superiority%2520of%2520UV-CoT%252C%2520compared%2520to%2520the%2520state-of-the-art%2520textual%250Aand%2520visual%2520CoT%2520methods.%2520Our%2520zero-shot%2520testing%2520on%2520four%2520unseen%2520datasets%2520shows%2520the%250Astrong%2520generalization%2520of%2520UV-CoT.%2520The%2520code%2520is%2520available%2520in%250Ahttps%253A//github.com/kesenzhao/UV-CoT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Visual%20Chain-of-Thought%20Reasoning%20via%20Preference%0A%20%20Optimization&entry.906535625=Kesen%20Zhao%20and%20Beier%20Zhu%20and%20Qianru%20Sun%20and%20Hanwang%20Zhang&entry.1292438233=%20%20Chain-of-thought%20%28CoT%29%20reasoning%20greatly%20improves%20the%20interpretability%20and%0Aproblem-solving%20abilities%20of%20multimodal%20large%20language%20models%20%28MLLMs%29.%20However%2C%0Aexisting%20approaches%20are%20focused%20on%20text%20CoT%2C%20limiting%20their%20ability%20to%20leverage%0Avisual%20cues.%20Visual%20CoT%20remains%20underexplored%2C%20and%20the%20only%20work%20is%20based%20on%0Asupervised%20fine-tuning%20%28SFT%29%20that%20relies%20on%20extensive%20labeled%20bounding-box%20data%0Aand%20is%20hard%20to%20generalize%20to%20unseen%20cases.%20In%20this%20paper%2C%20we%20introduce%0AUnsupervised%20Visual%20CoT%20%28UV-CoT%29%2C%20a%20novel%20framework%20for%20image-level%20CoT%0Areasoning%20via%20preference%20optimization.%20UV-CoT%20performs%20preference%20comparisons%0Abetween%20model-generated%20bounding%20boxes%20%28one%20is%20preferred%20and%20the%20other%20is%0Adis-preferred%29%2C%20eliminating%20the%20need%20for%20bounding-box%20annotations.%20We%20get%20such%0Apreference%20data%20by%20introducing%20an%20automatic%20data%20generation%20pipeline.%20Given%20an%0Aimage%2C%20our%20target%20MLLM%20%28e.g.%2C%20LLaVA-1.5-7B%29%20generates%20seed%20bounding%20boxes%20using%0Aa%20template%20prompt%20and%20then%20answers%20the%20question%20using%20each%20bounded%20region%20as%0Ainput.%20An%20evaluator%20MLLM%20%28e.g.%2C%20OmniLLM-12B%29%20ranks%20the%20responses%2C%20and%20these%0Arankings%20serve%20as%20supervision%20to%20train%20the%20target%20MLLM%20with%20UV-CoT%20by%0Aminimizing%20negative%20log-likelihood%20losses.%20By%20emulating%20human%0Aperception--identifying%20key%20regions%20and%20reasoning%20based%20on%20them--UV-CoT%20can%0Aimprove%20visual%20comprehension%2C%20particularly%20in%20spatial%20reasoning%20tasks%20where%0Atextual%20descriptions%20alone%20fall%20short.%20Our%20experiments%20on%20six%20datasets%0Ademonstrate%20the%20superiority%20of%20UV-CoT%2C%20compared%20to%20the%20state-of-the-art%20textual%0Aand%20visual%20CoT%20methods.%20Our%20zero-shot%20testing%20on%20four%20unseen%20datasets%20shows%20the%0Astrong%20generalization%20of%20UV-CoT.%20The%20code%20is%20available%20in%0Ahttps%3A//github.com/kesenzhao/UV-CoT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18397v1&entry.124074799=Read"},
{"title": "Spatial Reasoner: A 3D Inference Pipeline for XR Applications", "author": "Steven H\u00e4sler and Philipp Ackermann", "abstract": "  Modern extended reality XR systems provide rich analysis of image data and\nfusion of sensor input and demand AR/VR applications that can reason about 3D\nscenes in a semantic manner. We present a spatial reasoning framework that\nbridges geometric facts with symbolic predicates and relations to handle key\ntasks such as determining how 3D objects are arranged among each other ('on',\n'behind', 'near', etc.). Its foundation relies on oriented 3D bounding box\nrepresentations, enhanced by a comprehensive set of spatial predicates, ranging\nfrom topology and connectivity to directionality and orientation, expressed in\na formalism related to natural language. The derived predicates form a spatial\nknowledge graph and, in combination with a pipeline-based inference model,\nenable spatial queries and dynamic rule evaluation. Implementations for client-\nand server-side processing demonstrate the framework's capability to\nefficiently translate geometric data into actionable knowledge, ensuring\nscalable and technology-independent spatial reasoning in complex 3D\nenvironments. The Spatial Reasoner framework is fostering the creation of\nspatial ontologies, and seamlessly integrates with and therefore enriches\nmachine learning, natural language processing, and rule systems in XR\napplications.\n", "link": "http://arxiv.org/abs/2504.18380v1", "date": "2025-04-25", "relevancy": 2.8176, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Reasoner%3A%20A%203D%20Inference%20Pipeline%20for%20XR%20Applications&body=Title%3A%20Spatial%20Reasoner%3A%20A%203D%20Inference%20Pipeline%20for%20XR%20Applications%0AAuthor%3A%20Steven%20H%C3%A4sler%20and%20Philipp%20Ackermann%0AAbstract%3A%20%20%20Modern%20extended%20reality%20XR%20systems%20provide%20rich%20analysis%20of%20image%20data%20and%0Afusion%20of%20sensor%20input%20and%20demand%20AR/VR%20applications%20that%20can%20reason%20about%203D%0Ascenes%20in%20a%20semantic%20manner.%20We%20present%20a%20spatial%20reasoning%20framework%20that%0Abridges%20geometric%20facts%20with%20symbolic%20predicates%20and%20relations%20to%20handle%20key%0Atasks%20such%20as%20determining%20how%203D%20objects%20are%20arranged%20among%20each%20other%20%28%27on%27%2C%0A%27behind%27%2C%20%27near%27%2C%20etc.%29.%20Its%20foundation%20relies%20on%20oriented%203D%20bounding%20box%0Arepresentations%2C%20enhanced%20by%20a%20comprehensive%20set%20of%20spatial%20predicates%2C%20ranging%0Afrom%20topology%20and%20connectivity%20to%20directionality%20and%20orientation%2C%20expressed%20in%0Aa%20formalism%20related%20to%20natural%20language.%20The%20derived%20predicates%20form%20a%20spatial%0Aknowledge%20graph%20and%2C%20in%20combination%20with%20a%20pipeline-based%20inference%20model%2C%0Aenable%20spatial%20queries%20and%20dynamic%20rule%20evaluation.%20Implementations%20for%20client-%0Aand%20server-side%20processing%20demonstrate%20the%20framework%27s%20capability%20to%0Aefficiently%20translate%20geometric%20data%20into%20actionable%20knowledge%2C%20ensuring%0Ascalable%20and%20technology-independent%20spatial%20reasoning%20in%20complex%203D%0Aenvironments.%20The%20Spatial%20Reasoner%20framework%20is%20fostering%20the%20creation%20of%0Aspatial%20ontologies%2C%20and%20seamlessly%20integrates%20with%20and%20therefore%20enriches%0Amachine%20learning%2C%20natural%20language%20processing%2C%20and%20rule%20systems%20in%20XR%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Reasoner%253A%2520A%25203D%2520Inference%2520Pipeline%2520for%2520XR%2520Applications%26entry.906535625%3DSteven%2520H%25C3%25A4sler%2520and%2520Philipp%2520Ackermann%26entry.1292438233%3D%2520%2520Modern%2520extended%2520reality%2520XR%2520systems%2520provide%2520rich%2520analysis%2520of%2520image%2520data%2520and%250Afusion%2520of%2520sensor%2520input%2520and%2520demand%2520AR/VR%2520applications%2520that%2520can%2520reason%2520about%25203D%250Ascenes%2520in%2520a%2520semantic%2520manner.%2520We%2520present%2520a%2520spatial%2520reasoning%2520framework%2520that%250Abridges%2520geometric%2520facts%2520with%2520symbolic%2520predicates%2520and%2520relations%2520to%2520handle%2520key%250Atasks%2520such%2520as%2520determining%2520how%25203D%2520objects%2520are%2520arranged%2520among%2520each%2520other%2520%2528%2527on%2527%252C%250A%2527behind%2527%252C%2520%2527near%2527%252C%2520etc.%2529.%2520Its%2520foundation%2520relies%2520on%2520oriented%25203D%2520bounding%2520box%250Arepresentations%252C%2520enhanced%2520by%2520a%2520comprehensive%2520set%2520of%2520spatial%2520predicates%252C%2520ranging%250Afrom%2520topology%2520and%2520connectivity%2520to%2520directionality%2520and%2520orientation%252C%2520expressed%2520in%250Aa%2520formalism%2520related%2520to%2520natural%2520language.%2520The%2520derived%2520predicates%2520form%2520a%2520spatial%250Aknowledge%2520graph%2520and%252C%2520in%2520combination%2520with%2520a%2520pipeline-based%2520inference%2520model%252C%250Aenable%2520spatial%2520queries%2520and%2520dynamic%2520rule%2520evaluation.%2520Implementations%2520for%2520client-%250Aand%2520server-side%2520processing%2520demonstrate%2520the%2520framework%2527s%2520capability%2520to%250Aefficiently%2520translate%2520geometric%2520data%2520into%2520actionable%2520knowledge%252C%2520ensuring%250Ascalable%2520and%2520technology-independent%2520spatial%2520reasoning%2520in%2520complex%25203D%250Aenvironments.%2520The%2520Spatial%2520Reasoner%2520framework%2520is%2520fostering%2520the%2520creation%2520of%250Aspatial%2520ontologies%252C%2520and%2520seamlessly%2520integrates%2520with%2520and%2520therefore%2520enriches%250Amachine%2520learning%252C%2520natural%2520language%2520processing%252C%2520and%2520rule%2520systems%2520in%2520XR%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Reasoner%3A%20A%203D%20Inference%20Pipeline%20for%20XR%20Applications&entry.906535625=Steven%20H%C3%A4sler%20and%20Philipp%20Ackermann&entry.1292438233=%20%20Modern%20extended%20reality%20XR%20systems%20provide%20rich%20analysis%20of%20image%20data%20and%0Afusion%20of%20sensor%20input%20and%20demand%20AR/VR%20applications%20that%20can%20reason%20about%203D%0Ascenes%20in%20a%20semantic%20manner.%20We%20present%20a%20spatial%20reasoning%20framework%20that%0Abridges%20geometric%20facts%20with%20symbolic%20predicates%20and%20relations%20to%20handle%20key%0Atasks%20such%20as%20determining%20how%203D%20objects%20are%20arranged%20among%20each%20other%20%28%27on%27%2C%0A%27behind%27%2C%20%27near%27%2C%20etc.%29.%20Its%20foundation%20relies%20on%20oriented%203D%20bounding%20box%0Arepresentations%2C%20enhanced%20by%20a%20comprehensive%20set%20of%20spatial%20predicates%2C%20ranging%0Afrom%20topology%20and%20connectivity%20to%20directionality%20and%20orientation%2C%20expressed%20in%0Aa%20formalism%20related%20to%20natural%20language.%20The%20derived%20predicates%20form%20a%20spatial%0Aknowledge%20graph%20and%2C%20in%20combination%20with%20a%20pipeline-based%20inference%20model%2C%0Aenable%20spatial%20queries%20and%20dynamic%20rule%20evaluation.%20Implementations%20for%20client-%0Aand%20server-side%20processing%20demonstrate%20the%20framework%27s%20capability%20to%0Aefficiently%20translate%20geometric%20data%20into%20actionable%20knowledge%2C%20ensuring%0Ascalable%20and%20technology-independent%20spatial%20reasoning%20in%20complex%203D%0Aenvironments.%20The%20Spatial%20Reasoner%20framework%20is%20fostering%20the%20creation%20of%0Aspatial%20ontologies%2C%20and%20seamlessly%20integrates%20with%20and%20therefore%20enriches%0Amachine%20learning%2C%20natural%20language%20processing%2C%20and%20rule%20systems%20in%20XR%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18380v1&entry.124074799=Read"},
{"title": "Contrastive Learning and Adversarial Disentanglement for Task-Oriented\n  Semantic Communications", "author": "Omar Erak and Omar Alhussein and Wen Tong", "abstract": "  Task-oriented semantic communication systems have emerged as a promising\napproach to achieving efficient and intelligent data transmission, where only\ninformation relevant to a specific task is communicated. However, existing\nmethods struggle to fully disentangle task-relevant and task-irrelevant\ninformation, leading to privacy concerns and subpar performance. To address\nthis, we propose an information-bottleneck method, named CLAD (contrastive\nlearning and adversarial disentanglement). CLAD utilizes contrastive learning\nto effectively capture task-relevant features while employing adversarial\ndisentanglement to discard task-irrelevant information. Additionally, due to\nthe lack of reliable and reproducible methods to gain insight into the\ninformativeness and minimality of the encoded feature vectors, we introduce a\nnew technique to compute the information retention index (IRI), a comparative\nmetric used as a proxy for the mutual information between the encoded features\nand the input, reflecting the minimality of the encoded features. The IRI\nquantifies the minimality and informativeness of the encoded feature vectors\nacross different task-oriented communication techniques. Our extensive\nexperiments demonstrate that CLAD outperforms state-of-the-art baselines in\nterms of semantic extraction, task performance, privacy preservation, and IRI.\nCLAD achieves a predictive performance improvement of around 2.5-3%, along with\na 77-90% reduction in IRI and a 57-76% decrease in adversarial attribute\ninference attack accuracy.\n", "link": "http://arxiv.org/abs/2410.22784v2", "date": "2025-04-25", "relevancy": 2.7589, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Learning%20and%20Adversarial%20Disentanglement%20for%20Task-Oriented%0A%20%20Semantic%20Communications&body=Title%3A%20Contrastive%20Learning%20and%20Adversarial%20Disentanglement%20for%20Task-Oriented%0A%20%20Semantic%20Communications%0AAuthor%3A%20Omar%20Erak%20and%20Omar%20Alhussein%20and%20Wen%20Tong%0AAbstract%3A%20%20%20Task-oriented%20semantic%20communication%20systems%20have%20emerged%20as%20a%20promising%0Aapproach%20to%20achieving%20efficient%20and%20intelligent%20data%20transmission%2C%20where%20only%0Ainformation%20relevant%20to%20a%20specific%20task%20is%20communicated.%20However%2C%20existing%0Amethods%20struggle%20to%20fully%20disentangle%20task-relevant%20and%20task-irrelevant%0Ainformation%2C%20leading%20to%20privacy%20concerns%20and%20subpar%20performance.%20To%20address%0Athis%2C%20we%20propose%20an%20information-bottleneck%20method%2C%20named%20CLAD%20%28contrastive%0Alearning%20and%20adversarial%20disentanglement%29.%20CLAD%20utilizes%20contrastive%20learning%0Ato%20effectively%20capture%20task-relevant%20features%20while%20employing%20adversarial%0Adisentanglement%20to%20discard%20task-irrelevant%20information.%20Additionally%2C%20due%20to%0Athe%20lack%20of%20reliable%20and%20reproducible%20methods%20to%20gain%20insight%20into%20the%0Ainformativeness%20and%20minimality%20of%20the%20encoded%20feature%20vectors%2C%20we%20introduce%20a%0Anew%20technique%20to%20compute%20the%20information%20retention%20index%20%28IRI%29%2C%20a%20comparative%0Ametric%20used%20as%20a%20proxy%20for%20the%20mutual%20information%20between%20the%20encoded%20features%0Aand%20the%20input%2C%20reflecting%20the%20minimality%20of%20the%20encoded%20features.%20The%20IRI%0Aquantifies%20the%20minimality%20and%20informativeness%20of%20the%20encoded%20feature%20vectors%0Aacross%20different%20task-oriented%20communication%20techniques.%20Our%20extensive%0Aexperiments%20demonstrate%20that%20CLAD%20outperforms%20state-of-the-art%20baselines%20in%0Aterms%20of%20semantic%20extraction%2C%20task%20performance%2C%20privacy%20preservation%2C%20and%20IRI.%0ACLAD%20achieves%20a%20predictive%20performance%20improvement%20of%20around%202.5-3%25%2C%20along%20with%0Aa%2077-90%25%20reduction%20in%20IRI%20and%20a%2057-76%25%20decrease%20in%20adversarial%20attribute%0Ainference%20attack%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Learning%2520and%2520Adversarial%2520Disentanglement%2520for%2520Task-Oriented%250A%2520%2520Semantic%2520Communications%26entry.906535625%3DOmar%2520Erak%2520and%2520Omar%2520Alhussein%2520and%2520Wen%2520Tong%26entry.1292438233%3D%2520%2520Task-oriented%2520semantic%2520communication%2520systems%2520have%2520emerged%2520as%2520a%2520promising%250Aapproach%2520to%2520achieving%2520efficient%2520and%2520intelligent%2520data%2520transmission%252C%2520where%2520only%250Ainformation%2520relevant%2520to%2520a%2520specific%2520task%2520is%2520communicated.%2520However%252C%2520existing%250Amethods%2520struggle%2520to%2520fully%2520disentangle%2520task-relevant%2520and%2520task-irrelevant%250Ainformation%252C%2520leading%2520to%2520privacy%2520concerns%2520and%2520subpar%2520performance.%2520To%2520address%250Athis%252C%2520we%2520propose%2520an%2520information-bottleneck%2520method%252C%2520named%2520CLAD%2520%2528contrastive%250Alearning%2520and%2520adversarial%2520disentanglement%2529.%2520CLAD%2520utilizes%2520contrastive%2520learning%250Ato%2520effectively%2520capture%2520task-relevant%2520features%2520while%2520employing%2520adversarial%250Adisentanglement%2520to%2520discard%2520task-irrelevant%2520information.%2520Additionally%252C%2520due%2520to%250Athe%2520lack%2520of%2520reliable%2520and%2520reproducible%2520methods%2520to%2520gain%2520insight%2520into%2520the%250Ainformativeness%2520and%2520minimality%2520of%2520the%2520encoded%2520feature%2520vectors%252C%2520we%2520introduce%2520a%250Anew%2520technique%2520to%2520compute%2520the%2520information%2520retention%2520index%2520%2528IRI%2529%252C%2520a%2520comparative%250Ametric%2520used%2520as%2520a%2520proxy%2520for%2520the%2520mutual%2520information%2520between%2520the%2520encoded%2520features%250Aand%2520the%2520input%252C%2520reflecting%2520the%2520minimality%2520of%2520the%2520encoded%2520features.%2520The%2520IRI%250Aquantifies%2520the%2520minimality%2520and%2520informativeness%2520of%2520the%2520encoded%2520feature%2520vectors%250Aacross%2520different%2520task-oriented%2520communication%2520techniques.%2520Our%2520extensive%250Aexperiments%2520demonstrate%2520that%2520CLAD%2520outperforms%2520state-of-the-art%2520baselines%2520in%250Aterms%2520of%2520semantic%2520extraction%252C%2520task%2520performance%252C%2520privacy%2520preservation%252C%2520and%2520IRI.%250ACLAD%2520achieves%2520a%2520predictive%2520performance%2520improvement%2520of%2520around%25202.5-3%2525%252C%2520along%2520with%250Aa%252077-90%2525%2520reduction%2520in%2520IRI%2520and%2520a%252057-76%2525%2520decrease%2520in%2520adversarial%2520attribute%250Ainference%2520attack%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning%20and%20Adversarial%20Disentanglement%20for%20Task-Oriented%0A%20%20Semantic%20Communications&entry.906535625=Omar%20Erak%20and%20Omar%20Alhussein%20and%20Wen%20Tong&entry.1292438233=%20%20Task-oriented%20semantic%20communication%20systems%20have%20emerged%20as%20a%20promising%0Aapproach%20to%20achieving%20efficient%20and%20intelligent%20data%20transmission%2C%20where%20only%0Ainformation%20relevant%20to%20a%20specific%20task%20is%20communicated.%20However%2C%20existing%0Amethods%20struggle%20to%20fully%20disentangle%20task-relevant%20and%20task-irrelevant%0Ainformation%2C%20leading%20to%20privacy%20concerns%20and%20subpar%20performance.%20To%20address%0Athis%2C%20we%20propose%20an%20information-bottleneck%20method%2C%20named%20CLAD%20%28contrastive%0Alearning%20and%20adversarial%20disentanglement%29.%20CLAD%20utilizes%20contrastive%20learning%0Ato%20effectively%20capture%20task-relevant%20features%20while%20employing%20adversarial%0Adisentanglement%20to%20discard%20task-irrelevant%20information.%20Additionally%2C%20due%20to%0Athe%20lack%20of%20reliable%20and%20reproducible%20methods%20to%20gain%20insight%20into%20the%0Ainformativeness%20and%20minimality%20of%20the%20encoded%20feature%20vectors%2C%20we%20introduce%20a%0Anew%20technique%20to%20compute%20the%20information%20retention%20index%20%28IRI%29%2C%20a%20comparative%0Ametric%20used%20as%20a%20proxy%20for%20the%20mutual%20information%20between%20the%20encoded%20features%0Aand%20the%20input%2C%20reflecting%20the%20minimality%20of%20the%20encoded%20features.%20The%20IRI%0Aquantifies%20the%20minimality%20and%20informativeness%20of%20the%20encoded%20feature%20vectors%0Aacross%20different%20task-oriented%20communication%20techniques.%20Our%20extensive%0Aexperiments%20demonstrate%20that%20CLAD%20outperforms%20state-of-the-art%20baselines%20in%0Aterms%20of%20semantic%20extraction%2C%20task%20performance%2C%20privacy%20preservation%2C%20and%20IRI.%0ACLAD%20achieves%20a%20predictive%20performance%20improvement%20of%20around%202.5-3%25%2C%20along%20with%0Aa%2077-90%25%20reduction%20in%20IRI%20and%20a%2057-76%25%20decrease%20in%20adversarial%20attribute%0Ainference%20attack%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22784v2&entry.124074799=Read"},
{"title": "Bridge the Domains: Large Language Models Enhanced Cross-domain\n  Sequential Recommendation", "author": "Qidong Liu and Xiangyu Zhao and Yejing Wang and Zijian Zhang and Howard Zhong and Chong Chen and Xiang Li and Wei Huang and Feng Tian", "abstract": "  Cross-domain Sequential Recommendation (CDSR) aims to extract the preference\nfrom the user's historical interactions across various domains. Despite some\nprogress in CDSR, two problems set the barrier for further advancements, i.e.,\noverlap dilemma and transition complexity. The former means existing CDSR\nmethods severely rely on users who own interactions on all domains to learn\ncross-domain item relationships, compromising the practicability. The latter\nrefers to the difficulties in learning the complex transition patterns from the\nmixed behavior sequences. With powerful representation and reasoning abilities,\nLarge Language Models (LLMs) are promising to address these two problems by\nbridging the items and capturing the user's preferences from a semantic view.\nTherefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation\nmodel (LLM4CDSR). To obtain the semantic item relationships, we first propose\nan LLM-based unified representation module to represent items. Then, a\ntrainable adapter with contrastive regularization is designed to adapt the CDSR\ntask. Besides, a hierarchical LLMs profiling module is designed to summarize\nuser cross-domain preferences. Finally, these two modules are integrated into\nthe proposed tri-thread framework to derive recommendations. We have conducted\nextensive experiments on three public cross-domain datasets, validating the\neffectiveness of LLM4CDSR. We have released the code online.\n", "link": "http://arxiv.org/abs/2504.18383v1", "date": "2025-04-25", "relevancy": 2.7105, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridge%20the%20Domains%3A%20Large%20Language%20Models%20Enhanced%20Cross-domain%0A%20%20Sequential%20Recommendation&body=Title%3A%20Bridge%20the%20Domains%3A%20Large%20Language%20Models%20Enhanced%20Cross-domain%0A%20%20Sequential%20Recommendation%0AAuthor%3A%20Qidong%20Liu%20and%20Xiangyu%20Zhao%20and%20Yejing%20Wang%20and%20Zijian%20Zhang%20and%20Howard%20Zhong%20and%20Chong%20Chen%20and%20Xiang%20Li%20and%20Wei%20Huang%20and%20Feng%20Tian%0AAbstract%3A%20%20%20Cross-domain%20Sequential%20Recommendation%20%28CDSR%29%20aims%20to%20extract%20the%20preference%0Afrom%20the%20user%27s%20historical%20interactions%20across%20various%20domains.%20Despite%20some%0Aprogress%20in%20CDSR%2C%20two%20problems%20set%20the%20barrier%20for%20further%20advancements%2C%20i.e.%2C%0Aoverlap%20dilemma%20and%20transition%20complexity.%20The%20former%20means%20existing%20CDSR%0Amethods%20severely%20rely%20on%20users%20who%20own%20interactions%20on%20all%20domains%20to%20learn%0Across-domain%20item%20relationships%2C%20compromising%20the%20practicability.%20The%20latter%0Arefers%20to%20the%20difficulties%20in%20learning%20the%20complex%20transition%20patterns%20from%20the%0Amixed%20behavior%20sequences.%20With%20powerful%20representation%20and%20reasoning%20abilities%2C%0ALarge%20Language%20Models%20%28LLMs%29%20are%20promising%20to%20address%20these%20two%20problems%20by%0Abridging%20the%20items%20and%20capturing%20the%20user%27s%20preferences%20from%20a%20semantic%20view.%0ATherefore%2C%20we%20propose%20an%20LLMs%20Enhanced%20Cross-domain%20Sequential%20Recommendation%0Amodel%20%28LLM4CDSR%29.%20To%20obtain%20the%20semantic%20item%20relationships%2C%20we%20first%20propose%0Aan%20LLM-based%20unified%20representation%20module%20to%20represent%20items.%20Then%2C%20a%0Atrainable%20adapter%20with%20contrastive%20regularization%20is%20designed%20to%20adapt%20the%20CDSR%0Atask.%20Besides%2C%20a%20hierarchical%20LLMs%20profiling%20module%20is%20designed%20to%20summarize%0Auser%20cross-domain%20preferences.%20Finally%2C%20these%20two%20modules%20are%20integrated%20into%0Athe%20proposed%20tri-thread%20framework%20to%20derive%20recommendations.%20We%20have%20conducted%0Aextensive%20experiments%20on%20three%20public%20cross-domain%20datasets%2C%20validating%20the%0Aeffectiveness%20of%20LLM4CDSR.%20We%20have%20released%20the%20code%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridge%2520the%2520Domains%253A%2520Large%2520Language%2520Models%2520Enhanced%2520Cross-domain%250A%2520%2520Sequential%2520Recommendation%26entry.906535625%3DQidong%2520Liu%2520and%2520Xiangyu%2520Zhao%2520and%2520Yejing%2520Wang%2520and%2520Zijian%2520Zhang%2520and%2520Howard%2520Zhong%2520and%2520Chong%2520Chen%2520and%2520Xiang%2520Li%2520and%2520Wei%2520Huang%2520and%2520Feng%2520Tian%26entry.1292438233%3D%2520%2520Cross-domain%2520Sequential%2520Recommendation%2520%2528CDSR%2529%2520aims%2520to%2520extract%2520the%2520preference%250Afrom%2520the%2520user%2527s%2520historical%2520interactions%2520across%2520various%2520domains.%2520Despite%2520some%250Aprogress%2520in%2520CDSR%252C%2520two%2520problems%2520set%2520the%2520barrier%2520for%2520further%2520advancements%252C%2520i.e.%252C%250Aoverlap%2520dilemma%2520and%2520transition%2520complexity.%2520The%2520former%2520means%2520existing%2520CDSR%250Amethods%2520severely%2520rely%2520on%2520users%2520who%2520own%2520interactions%2520on%2520all%2520domains%2520to%2520learn%250Across-domain%2520item%2520relationships%252C%2520compromising%2520the%2520practicability.%2520The%2520latter%250Arefers%2520to%2520the%2520difficulties%2520in%2520learning%2520the%2520complex%2520transition%2520patterns%2520from%2520the%250Amixed%2520behavior%2520sequences.%2520With%2520powerful%2520representation%2520and%2520reasoning%2520abilities%252C%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520promising%2520to%2520address%2520these%2520two%2520problems%2520by%250Abridging%2520the%2520items%2520and%2520capturing%2520the%2520user%2527s%2520preferences%2520from%2520a%2520semantic%2520view.%250ATherefore%252C%2520we%2520propose%2520an%2520LLMs%2520Enhanced%2520Cross-domain%2520Sequential%2520Recommendation%250Amodel%2520%2528LLM4CDSR%2529.%2520To%2520obtain%2520the%2520semantic%2520item%2520relationships%252C%2520we%2520first%2520propose%250Aan%2520LLM-based%2520unified%2520representation%2520module%2520to%2520represent%2520items.%2520Then%252C%2520a%250Atrainable%2520adapter%2520with%2520contrastive%2520regularization%2520is%2520designed%2520to%2520adapt%2520the%2520CDSR%250Atask.%2520Besides%252C%2520a%2520hierarchical%2520LLMs%2520profiling%2520module%2520is%2520designed%2520to%2520summarize%250Auser%2520cross-domain%2520preferences.%2520Finally%252C%2520these%2520two%2520modules%2520are%2520integrated%2520into%250Athe%2520proposed%2520tri-thread%2520framework%2520to%2520derive%2520recommendations.%2520We%2520have%2520conducted%250Aextensive%2520experiments%2520on%2520three%2520public%2520cross-domain%2520datasets%252C%2520validating%2520the%250Aeffectiveness%2520of%2520LLM4CDSR.%2520We%2520have%2520released%2520the%2520code%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridge%20the%20Domains%3A%20Large%20Language%20Models%20Enhanced%20Cross-domain%0A%20%20Sequential%20Recommendation&entry.906535625=Qidong%20Liu%20and%20Xiangyu%20Zhao%20and%20Yejing%20Wang%20and%20Zijian%20Zhang%20and%20Howard%20Zhong%20and%20Chong%20Chen%20and%20Xiang%20Li%20and%20Wei%20Huang%20and%20Feng%20Tian&entry.1292438233=%20%20Cross-domain%20Sequential%20Recommendation%20%28CDSR%29%20aims%20to%20extract%20the%20preference%0Afrom%20the%20user%27s%20historical%20interactions%20across%20various%20domains.%20Despite%20some%0Aprogress%20in%20CDSR%2C%20two%20problems%20set%20the%20barrier%20for%20further%20advancements%2C%20i.e.%2C%0Aoverlap%20dilemma%20and%20transition%20complexity.%20The%20former%20means%20existing%20CDSR%0Amethods%20severely%20rely%20on%20users%20who%20own%20interactions%20on%20all%20domains%20to%20learn%0Across-domain%20item%20relationships%2C%20compromising%20the%20practicability.%20The%20latter%0Arefers%20to%20the%20difficulties%20in%20learning%20the%20complex%20transition%20patterns%20from%20the%0Amixed%20behavior%20sequences.%20With%20powerful%20representation%20and%20reasoning%20abilities%2C%0ALarge%20Language%20Models%20%28LLMs%29%20are%20promising%20to%20address%20these%20two%20problems%20by%0Abridging%20the%20items%20and%20capturing%20the%20user%27s%20preferences%20from%20a%20semantic%20view.%0ATherefore%2C%20we%20propose%20an%20LLMs%20Enhanced%20Cross-domain%20Sequential%20Recommendation%0Amodel%20%28LLM4CDSR%29.%20To%20obtain%20the%20semantic%20item%20relationships%2C%20we%20first%20propose%0Aan%20LLM-based%20unified%20representation%20module%20to%20represent%20items.%20Then%2C%20a%0Atrainable%20adapter%20with%20contrastive%20regularization%20is%20designed%20to%20adapt%20the%20CDSR%0Atask.%20Besides%2C%20a%20hierarchical%20LLMs%20profiling%20module%20is%20designed%20to%20summarize%0Auser%20cross-domain%20preferences.%20Finally%2C%20these%20two%20modules%20are%20integrated%20into%0Athe%20proposed%20tri-thread%20framework%20to%20derive%20recommendations.%20We%20have%20conducted%0Aextensive%20experiments%20on%20three%20public%20cross-domain%20datasets%2C%20validating%20the%0Aeffectiveness%20of%20LLM4CDSR.%20We%20have%20released%20the%20code%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18383v1&entry.124074799=Read"},
{"title": "LaRI: Layered Ray Intersections for Single-view 3D Geometric Reasoning", "author": "Rui Li and Biao Zhang and Zhenyu Li and Federico Tombari and Peter Wonka", "abstract": "  We present layered ray intersections (LaRI), a new method for unseen geometry\nreasoning from a single image. Unlike conventional depth estimation that is\nlimited to the visible surface, LaRI models multiple surfaces intersected by\nthe camera rays using layered point maps. Benefiting from the compact and\nlayered representation, LaRI enables complete, efficient, and view-aligned\ngeometric reasoning to unify object- and scene-level tasks. We further propose\nto predict the ray stopping index, which identifies valid intersecting pixels\nand layers from LaRI's output. We build a complete training data generation\npipeline for synthetic and real-world data, including 3D objects and scenes,\nwith necessary data cleaning steps and coordination between rendering engines.\nAs a generic method, LaRI's performance is validated in two scenarios: It\nyields comparable object-level results to the recent large generative model\nusing 4% of its training data and 17% of its parameters. Meanwhile, it achieves\nscene-level occluded geometry reasoning in only one feed-forward.\n", "link": "http://arxiv.org/abs/2504.18424v1", "date": "2025-04-25", "relevancy": 2.7087, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5348}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaRI%3A%20Layered%20Ray%20Intersections%20for%20Single-view%203D%20Geometric%20Reasoning&body=Title%3A%20LaRI%3A%20Layered%20Ray%20Intersections%20for%20Single-view%203D%20Geometric%20Reasoning%0AAuthor%3A%20Rui%20Li%20and%20Biao%20Zhang%20and%20Zhenyu%20Li%20and%20Federico%20Tombari%20and%20Peter%20Wonka%0AAbstract%3A%20%20%20We%20present%20layered%20ray%20intersections%20%28LaRI%29%2C%20a%20new%20method%20for%20unseen%20geometry%0Areasoning%20from%20a%20single%20image.%20Unlike%20conventional%20depth%20estimation%20that%20is%0Alimited%20to%20the%20visible%20surface%2C%20LaRI%20models%20multiple%20surfaces%20intersected%20by%0Athe%20camera%20rays%20using%20layered%20point%20maps.%20Benefiting%20from%20the%20compact%20and%0Alayered%20representation%2C%20LaRI%20enables%20complete%2C%20efficient%2C%20and%20view-aligned%0Ageometric%20reasoning%20to%20unify%20object-%20and%20scene-level%20tasks.%20We%20further%20propose%0Ato%20predict%20the%20ray%20stopping%20index%2C%20which%20identifies%20valid%20intersecting%20pixels%0Aand%20layers%20from%20LaRI%27s%20output.%20We%20build%20a%20complete%20training%20data%20generation%0Apipeline%20for%20synthetic%20and%20real-world%20data%2C%20including%203D%20objects%20and%20scenes%2C%0Awith%20necessary%20data%20cleaning%20steps%20and%20coordination%20between%20rendering%20engines.%0AAs%20a%20generic%20method%2C%20LaRI%27s%20performance%20is%20validated%20in%20two%20scenarios%3A%20It%0Ayields%20comparable%20object-level%20results%20to%20the%20recent%20large%20generative%20model%0Ausing%204%25%20of%20its%20training%20data%20and%2017%25%20of%20its%20parameters.%20Meanwhile%2C%20it%20achieves%0Ascene-level%20occluded%20geometry%20reasoning%20in%20only%20one%20feed-forward.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaRI%253A%2520Layered%2520Ray%2520Intersections%2520for%2520Single-view%25203D%2520Geometric%2520Reasoning%26entry.906535625%3DRui%2520Li%2520and%2520Biao%2520Zhang%2520and%2520Zhenyu%2520Li%2520and%2520Federico%2520Tombari%2520and%2520Peter%2520Wonka%26entry.1292438233%3D%2520%2520We%2520present%2520layered%2520ray%2520intersections%2520%2528LaRI%2529%252C%2520a%2520new%2520method%2520for%2520unseen%2520geometry%250Areasoning%2520from%2520a%2520single%2520image.%2520Unlike%2520conventional%2520depth%2520estimation%2520that%2520is%250Alimited%2520to%2520the%2520visible%2520surface%252C%2520LaRI%2520models%2520multiple%2520surfaces%2520intersected%2520by%250Athe%2520camera%2520rays%2520using%2520layered%2520point%2520maps.%2520Benefiting%2520from%2520the%2520compact%2520and%250Alayered%2520representation%252C%2520LaRI%2520enables%2520complete%252C%2520efficient%252C%2520and%2520view-aligned%250Ageometric%2520reasoning%2520to%2520unify%2520object-%2520and%2520scene-level%2520tasks.%2520We%2520further%2520propose%250Ato%2520predict%2520the%2520ray%2520stopping%2520index%252C%2520which%2520identifies%2520valid%2520intersecting%2520pixels%250Aand%2520layers%2520from%2520LaRI%2527s%2520output.%2520We%2520build%2520a%2520complete%2520training%2520data%2520generation%250Apipeline%2520for%2520synthetic%2520and%2520real-world%2520data%252C%2520including%25203D%2520objects%2520and%2520scenes%252C%250Awith%2520necessary%2520data%2520cleaning%2520steps%2520and%2520coordination%2520between%2520rendering%2520engines.%250AAs%2520a%2520generic%2520method%252C%2520LaRI%2527s%2520performance%2520is%2520validated%2520in%2520two%2520scenarios%253A%2520It%250Ayields%2520comparable%2520object-level%2520results%2520to%2520the%2520recent%2520large%2520generative%2520model%250Ausing%25204%2525%2520of%2520its%2520training%2520data%2520and%252017%2525%2520of%2520its%2520parameters.%2520Meanwhile%252C%2520it%2520achieves%250Ascene-level%2520occluded%2520geometry%2520reasoning%2520in%2520only%2520one%2520feed-forward.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaRI%3A%20Layered%20Ray%20Intersections%20for%20Single-view%203D%20Geometric%20Reasoning&entry.906535625=Rui%20Li%20and%20Biao%20Zhang%20and%20Zhenyu%20Li%20and%20Federico%20Tombari%20and%20Peter%20Wonka&entry.1292438233=%20%20We%20present%20layered%20ray%20intersections%20%28LaRI%29%2C%20a%20new%20method%20for%20unseen%20geometry%0Areasoning%20from%20a%20single%20image.%20Unlike%20conventional%20depth%20estimation%20that%20is%0Alimited%20to%20the%20visible%20surface%2C%20LaRI%20models%20multiple%20surfaces%20intersected%20by%0Athe%20camera%20rays%20using%20layered%20point%20maps.%20Benefiting%20from%20the%20compact%20and%0Alayered%20representation%2C%20LaRI%20enables%20complete%2C%20efficient%2C%20and%20view-aligned%0Ageometric%20reasoning%20to%20unify%20object-%20and%20scene-level%20tasks.%20We%20further%20propose%0Ato%20predict%20the%20ray%20stopping%20index%2C%20which%20identifies%20valid%20intersecting%20pixels%0Aand%20layers%20from%20LaRI%27s%20output.%20We%20build%20a%20complete%20training%20data%20generation%0Apipeline%20for%20synthetic%20and%20real-world%20data%2C%20including%203D%20objects%20and%20scenes%2C%0Awith%20necessary%20data%20cleaning%20steps%20and%20coordination%20between%20rendering%20engines.%0AAs%20a%20generic%20method%2C%20LaRI%27s%20performance%20is%20validated%20in%20two%20scenarios%3A%20It%0Ayields%20comparable%20object-level%20results%20to%20the%20recent%20large%20generative%20model%0Ausing%204%25%20of%20its%20training%20data%20and%2017%25%20of%20its%20parameters.%20Meanwhile%2C%20it%20achieves%0Ascene-level%20occluded%20geometry%20reasoning%20in%20only%20one%20feed-forward.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18424v1&entry.124074799=Read"},
{"title": "All for One, and One for All: UrbanSyn Dataset, the third Musketeer of\n  Synthetic Driving Scenes", "author": "Jose L. G\u00f3mez and Manuel Silva and Antonio Seoane and Agn\u00e8s Borr\u00e1s and Mario Noriega and Germ\u00e1n Ros and Jose A. Iglesias-Guitian and Antonio M. L\u00f3pez", "abstract": "  We introduce UrbanSyn, a photorealistic dataset acquired through\nsemi-procedurally generated synthetic urban driving scenarios. Developed using\nhigh-quality geometry and materials, UrbanSyn provides pixel-level ground\ntruth, including depth, semantic segmentation, and instance segmentation with\nobject bounding boxes and occlusion degree. It complements GTAV and Synscapes\ndatasets to form what we coin as the 'Three Musketeers'. We demonstrate the\nvalue of the Three Musketeers in unsupervised domain adaptation for image\nsemantic segmentation. Results on real-world datasets, Cityscapes, Mapillary\nVistas, and BDD100K, establish new benchmarks, largely attributed to UrbanSyn.\nWe make UrbanSyn openly and freely accessible (www.urbansyn.org).\n", "link": "http://arxiv.org/abs/2312.12176v2", "date": "2025-04-25", "relevancy": 2.6881, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5393}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5368}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All%20for%20One%2C%20and%20One%20for%20All%3A%20UrbanSyn%20Dataset%2C%20the%20third%20Musketeer%20of%0A%20%20Synthetic%20Driving%20Scenes&body=Title%3A%20All%20for%20One%2C%20and%20One%20for%20All%3A%20UrbanSyn%20Dataset%2C%20the%20third%20Musketeer%20of%0A%20%20Synthetic%20Driving%20Scenes%0AAuthor%3A%20Jose%20L.%20G%C3%B3mez%20and%20Manuel%20Silva%20and%20Antonio%20Seoane%20and%20Agn%C3%A8s%20Borr%C3%A1s%20and%20Mario%20Noriega%20and%20Germ%C3%A1n%20Ros%20and%20Jose%20A.%20Iglesias-Guitian%20and%20Antonio%20M.%20L%C3%B3pez%0AAbstract%3A%20%20%20We%20introduce%20UrbanSyn%2C%20a%20photorealistic%20dataset%20acquired%20through%0Asemi-procedurally%20generated%20synthetic%20urban%20driving%20scenarios.%20Developed%20using%0Ahigh-quality%20geometry%20and%20materials%2C%20UrbanSyn%20provides%20pixel-level%20ground%0Atruth%2C%20including%20depth%2C%20semantic%20segmentation%2C%20and%20instance%20segmentation%20with%0Aobject%20bounding%20boxes%20and%20occlusion%20degree.%20It%20complements%20GTAV%20and%20Synscapes%0Adatasets%20to%20form%20what%20we%20coin%20as%20the%20%27Three%20Musketeers%27.%20We%20demonstrate%20the%0Avalue%20of%20the%20Three%20Musketeers%20in%20unsupervised%20domain%20adaptation%20for%20image%0Asemantic%20segmentation.%20Results%20on%20real-world%20datasets%2C%20Cityscapes%2C%20Mapillary%0AVistas%2C%20and%20BDD100K%2C%20establish%20new%20benchmarks%2C%20largely%20attributed%20to%20UrbanSyn.%0AWe%20make%20UrbanSyn%20openly%20and%20freely%20accessible%20%28www.urbansyn.org%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll%2520for%2520One%252C%2520and%2520One%2520for%2520All%253A%2520UrbanSyn%2520Dataset%252C%2520the%2520third%2520Musketeer%2520of%250A%2520%2520Synthetic%2520Driving%2520Scenes%26entry.906535625%3DJose%2520L.%2520G%25C3%25B3mez%2520and%2520Manuel%2520Silva%2520and%2520Antonio%2520Seoane%2520and%2520Agn%25C3%25A8s%2520Borr%25C3%25A1s%2520and%2520Mario%2520Noriega%2520and%2520Germ%25C3%25A1n%2520Ros%2520and%2520Jose%2520A.%2520Iglesias-Guitian%2520and%2520Antonio%2520M.%2520L%25C3%25B3pez%26entry.1292438233%3D%2520%2520We%2520introduce%2520UrbanSyn%252C%2520a%2520photorealistic%2520dataset%2520acquired%2520through%250Asemi-procedurally%2520generated%2520synthetic%2520urban%2520driving%2520scenarios.%2520Developed%2520using%250Ahigh-quality%2520geometry%2520and%2520materials%252C%2520UrbanSyn%2520provides%2520pixel-level%2520ground%250Atruth%252C%2520including%2520depth%252C%2520semantic%2520segmentation%252C%2520and%2520instance%2520segmentation%2520with%250Aobject%2520bounding%2520boxes%2520and%2520occlusion%2520degree.%2520It%2520complements%2520GTAV%2520and%2520Synscapes%250Adatasets%2520to%2520form%2520what%2520we%2520coin%2520as%2520the%2520%2527Three%2520Musketeers%2527.%2520We%2520demonstrate%2520the%250Avalue%2520of%2520the%2520Three%2520Musketeers%2520in%2520unsupervised%2520domain%2520adaptation%2520for%2520image%250Asemantic%2520segmentation.%2520Results%2520on%2520real-world%2520datasets%252C%2520Cityscapes%252C%2520Mapillary%250AVistas%252C%2520and%2520BDD100K%252C%2520establish%2520new%2520benchmarks%252C%2520largely%2520attributed%2520to%2520UrbanSyn.%250AWe%2520make%2520UrbanSyn%2520openly%2520and%2520freely%2520accessible%2520%2528www.urbansyn.org%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All%20for%20One%2C%20and%20One%20for%20All%3A%20UrbanSyn%20Dataset%2C%20the%20third%20Musketeer%20of%0A%20%20Synthetic%20Driving%20Scenes&entry.906535625=Jose%20L.%20G%C3%B3mez%20and%20Manuel%20Silva%20and%20Antonio%20Seoane%20and%20Agn%C3%A8s%20Borr%C3%A1s%20and%20Mario%20Noriega%20and%20Germ%C3%A1n%20Ros%20and%20Jose%20A.%20Iglesias-Guitian%20and%20Antonio%20M.%20L%C3%B3pez&entry.1292438233=%20%20We%20introduce%20UrbanSyn%2C%20a%20photorealistic%20dataset%20acquired%20through%0Asemi-procedurally%20generated%20synthetic%20urban%20driving%20scenarios.%20Developed%20using%0Ahigh-quality%20geometry%20and%20materials%2C%20UrbanSyn%20provides%20pixel-level%20ground%0Atruth%2C%20including%20depth%2C%20semantic%20segmentation%2C%20and%20instance%20segmentation%20with%0Aobject%20bounding%20boxes%20and%20occlusion%20degree.%20It%20complements%20GTAV%20and%20Synscapes%0Adatasets%20to%20form%20what%20we%20coin%20as%20the%20%27Three%20Musketeers%27.%20We%20demonstrate%20the%0Avalue%20of%20the%20Three%20Musketeers%20in%20unsupervised%20domain%20adaptation%20for%20image%0Asemantic%20segmentation.%20Results%20on%20real-world%20datasets%2C%20Cityscapes%2C%20Mapillary%0AVistas%2C%20and%20BDD100K%2C%20establish%20new%20benchmarks%2C%20largely%20attributed%20to%20UrbanSyn.%0AWe%20make%20UrbanSyn%20openly%20and%20freely%20accessible%20%28www.urbansyn.org%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12176v2&entry.124074799=Read"},
{"title": "Automatically Generating UI Code from Screenshot: A\n  Divide-and-Conquer-Based Approach", "author": "Yuxuan Wan and Chaozheng Wang and Yi Dong and Wenxuan Wang and Shuqing Li and Yintong Huo and Michael R. Lyu", "abstract": "  Websites are critical in today's digital world, with over 1.11 billion\ncurrently active and approximately 252,000 new sites launched daily. Converting\nwebsite layout design into functional UI code is a time-consuming yet\nindispensable step of website development. Manual methods of converting visual\ndesigns into functional code present significant challenges, especially for\nnon-experts. To explore automatic design-to-code solutions, we first conduct a\nmotivating study on GPT-4o and identify three types of issues in generating UI\ncode: element omission, element distortion, and element misarrangement. We\nfurther reveal that a focus on smaller visual segments can help multimodal\nlarge language models (MLLMs) mitigate these failures in the generation\nprocess.\n  In this paper, we propose DCGen, a divide-and-conquer-based approach to\nautomate the translation of webpage design to UI code. DCGen starts by dividing\nscreenshots into manageable segments, generating code for each segment, and\nthen reassembling them into complete UI code for the entire screenshot. We\nconduct extensive testing with a dataset comprised of real-world websites and\nvarious MLLMs and demonstrate that DCGen achieves up to a 15% improvement in\nvisual similarity and 8% in code similarity for large input images. Human\nevaluations show that DCGen can help developers implement webpages\nsignificantly faster and more similar to the UI designs. To the best of our\nknowledge, DCGen is the first segment-aware MLLM-based approach for generating\nUI code directly from screenshots.\n", "link": "http://arxiv.org/abs/2406.16386v3", "date": "2025-04-25", "relevancy": 2.6842, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5825}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5211}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatically%20Generating%20UI%20Code%20from%20Screenshot%3A%20A%0A%20%20Divide-and-Conquer-Based%20Approach&body=Title%3A%20Automatically%20Generating%20UI%20Code%20from%20Screenshot%3A%20A%0A%20%20Divide-and-Conquer-Based%20Approach%0AAuthor%3A%20Yuxuan%20Wan%20and%20Chaozheng%20Wang%20and%20Yi%20Dong%20and%20Wenxuan%20Wang%20and%20Shuqing%20Li%20and%20Yintong%20Huo%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20Websites%20are%20critical%20in%20today%27s%20digital%20world%2C%20with%20over%201.11%20billion%0Acurrently%20active%20and%20approximately%20252%2C000%20new%20sites%20launched%20daily.%20Converting%0Awebsite%20layout%20design%20into%20functional%20UI%20code%20is%20a%20time-consuming%20yet%0Aindispensable%20step%20of%20website%20development.%20Manual%20methods%20of%20converting%20visual%0Adesigns%20into%20functional%20code%20present%20significant%20challenges%2C%20especially%20for%0Anon-experts.%20To%20explore%20automatic%20design-to-code%20solutions%2C%20we%20first%20conduct%20a%0Amotivating%20study%20on%20GPT-4o%20and%20identify%20three%20types%20of%20issues%20in%20generating%20UI%0Acode%3A%20element%20omission%2C%20element%20distortion%2C%20and%20element%20misarrangement.%20We%0Afurther%20reveal%20that%20a%20focus%20on%20smaller%20visual%20segments%20can%20help%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20mitigate%20these%20failures%20in%20the%20generation%0Aprocess.%0A%20%20In%20this%20paper%2C%20we%20propose%20DCGen%2C%20a%20divide-and-conquer-based%20approach%20to%0Aautomate%20the%20translation%20of%20webpage%20design%20to%20UI%20code.%20DCGen%20starts%20by%20dividing%0Ascreenshots%20into%20manageable%20segments%2C%20generating%20code%20for%20each%20segment%2C%20and%0Athen%20reassembling%20them%20into%20complete%20UI%20code%20for%20the%20entire%20screenshot.%20We%0Aconduct%20extensive%20testing%20with%20a%20dataset%20comprised%20of%20real-world%20websites%20and%0Avarious%20MLLMs%20and%20demonstrate%20that%20DCGen%20achieves%20up%20to%20a%2015%25%20improvement%20in%0Avisual%20similarity%20and%208%25%20in%20code%20similarity%20for%20large%20input%20images.%20Human%0Aevaluations%20show%20that%20DCGen%20can%20help%20developers%20implement%20webpages%0Asignificantly%20faster%20and%20more%20similar%20to%20the%20UI%20designs.%20To%20the%20best%20of%20our%0Aknowledge%2C%20DCGen%20is%20the%20first%20segment-aware%20MLLM-based%20approach%20for%20generating%0AUI%20code%20directly%20from%20screenshots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16386v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatically%2520Generating%2520UI%2520Code%2520from%2520Screenshot%253A%2520A%250A%2520%2520Divide-and-Conquer-Based%2520Approach%26entry.906535625%3DYuxuan%2520Wan%2520and%2520Chaozheng%2520Wang%2520and%2520Yi%2520Dong%2520and%2520Wenxuan%2520Wang%2520and%2520Shuqing%2520Li%2520and%2520Yintong%2520Huo%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520Websites%2520are%2520critical%2520in%2520today%2527s%2520digital%2520world%252C%2520with%2520over%25201.11%2520billion%250Acurrently%2520active%2520and%2520approximately%2520252%252C000%2520new%2520sites%2520launched%2520daily.%2520Converting%250Awebsite%2520layout%2520design%2520into%2520functional%2520UI%2520code%2520is%2520a%2520time-consuming%2520yet%250Aindispensable%2520step%2520of%2520website%2520development.%2520Manual%2520methods%2520of%2520converting%2520visual%250Adesigns%2520into%2520functional%2520code%2520present%2520significant%2520challenges%252C%2520especially%2520for%250Anon-experts.%2520To%2520explore%2520automatic%2520design-to-code%2520solutions%252C%2520we%2520first%2520conduct%2520a%250Amotivating%2520study%2520on%2520GPT-4o%2520and%2520identify%2520three%2520types%2520of%2520issues%2520in%2520generating%2520UI%250Acode%253A%2520element%2520omission%252C%2520element%2520distortion%252C%2520and%2520element%2520misarrangement.%2520We%250Afurther%2520reveal%2520that%2520a%2520focus%2520on%2520smaller%2520visual%2520segments%2520can%2520help%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%2520mitigate%2520these%2520failures%2520in%2520the%2520generation%250Aprocess.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520DCGen%252C%2520a%2520divide-and-conquer-based%2520approach%2520to%250Aautomate%2520the%2520translation%2520of%2520webpage%2520design%2520to%2520UI%2520code.%2520DCGen%2520starts%2520by%2520dividing%250Ascreenshots%2520into%2520manageable%2520segments%252C%2520generating%2520code%2520for%2520each%2520segment%252C%2520and%250Athen%2520reassembling%2520them%2520into%2520complete%2520UI%2520code%2520for%2520the%2520entire%2520screenshot.%2520We%250Aconduct%2520extensive%2520testing%2520with%2520a%2520dataset%2520comprised%2520of%2520real-world%2520websites%2520and%250Avarious%2520MLLMs%2520and%2520demonstrate%2520that%2520DCGen%2520achieves%2520up%2520to%2520a%252015%2525%2520improvement%2520in%250Avisual%2520similarity%2520and%25208%2525%2520in%2520code%2520similarity%2520for%2520large%2520input%2520images.%2520Human%250Aevaluations%2520show%2520that%2520DCGen%2520can%2520help%2520developers%2520implement%2520webpages%250Asignificantly%2520faster%2520and%2520more%2520similar%2520to%2520the%2520UI%2520designs.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520DCGen%2520is%2520the%2520first%2520segment-aware%2520MLLM-based%2520approach%2520for%2520generating%250AUI%2520code%2520directly%2520from%2520screenshots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16386v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatically%20Generating%20UI%20Code%20from%20Screenshot%3A%20A%0A%20%20Divide-and-Conquer-Based%20Approach&entry.906535625=Yuxuan%20Wan%20and%20Chaozheng%20Wang%20and%20Yi%20Dong%20and%20Wenxuan%20Wang%20and%20Shuqing%20Li%20and%20Yintong%20Huo%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20Websites%20are%20critical%20in%20today%27s%20digital%20world%2C%20with%20over%201.11%20billion%0Acurrently%20active%20and%20approximately%20252%2C000%20new%20sites%20launched%20daily.%20Converting%0Awebsite%20layout%20design%20into%20functional%20UI%20code%20is%20a%20time-consuming%20yet%0Aindispensable%20step%20of%20website%20development.%20Manual%20methods%20of%20converting%20visual%0Adesigns%20into%20functional%20code%20present%20significant%20challenges%2C%20especially%20for%0Anon-experts.%20To%20explore%20automatic%20design-to-code%20solutions%2C%20we%20first%20conduct%20a%0Amotivating%20study%20on%20GPT-4o%20and%20identify%20three%20types%20of%20issues%20in%20generating%20UI%0Acode%3A%20element%20omission%2C%20element%20distortion%2C%20and%20element%20misarrangement.%20We%0Afurther%20reveal%20that%20a%20focus%20on%20smaller%20visual%20segments%20can%20help%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20mitigate%20these%20failures%20in%20the%20generation%0Aprocess.%0A%20%20In%20this%20paper%2C%20we%20propose%20DCGen%2C%20a%20divide-and-conquer-based%20approach%20to%0Aautomate%20the%20translation%20of%20webpage%20design%20to%20UI%20code.%20DCGen%20starts%20by%20dividing%0Ascreenshots%20into%20manageable%20segments%2C%20generating%20code%20for%20each%20segment%2C%20and%0Athen%20reassembling%20them%20into%20complete%20UI%20code%20for%20the%20entire%20screenshot.%20We%0Aconduct%20extensive%20testing%20with%20a%20dataset%20comprised%20of%20real-world%20websites%20and%0Avarious%20MLLMs%20and%20demonstrate%20that%20DCGen%20achieves%20up%20to%20a%2015%25%20improvement%20in%0Avisual%20similarity%20and%208%25%20in%20code%20similarity%20for%20large%20input%20images.%20Human%0Aevaluations%20show%20that%20DCGen%20can%20help%20developers%20implement%20webpages%0Asignificantly%20faster%20and%20more%20similar%20to%20the%20UI%20designs.%20To%20the%20best%20of%20our%0Aknowledge%2C%20DCGen%20is%20the%20first%20segment-aware%20MLLM-based%20approach%20for%20generating%0AUI%20code%20directly%20from%20screenshots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16386v3&entry.124074799=Read"},
{"title": "TextTIGER: Text-based Intelligent Generation with Entity Prompt\n  Refinement for Text-to-Image Generation", "author": "Shintaro Ozaki and Kazuki Hayashi and Yusuke Sakai and Jingun Kwon and Hidetaka Kamigaito and Katsuhiko Hayashi and Manabu Okumura and Taro Watanabe", "abstract": "  Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance.\n", "link": "http://arxiv.org/abs/2504.18269v1", "date": "2025-04-25", "relevancy": 2.6763, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5367}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5357}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextTIGER%3A%20Text-based%20Intelligent%20Generation%20with%20Entity%20Prompt%0A%20%20Refinement%20for%20Text-to-Image%20Generation&body=Title%3A%20TextTIGER%3A%20Text-based%20Intelligent%20Generation%20with%20Entity%20Prompt%0A%20%20Refinement%20for%20Text-to-Image%20Generation%0AAuthor%3A%20Shintaro%20Ozaki%20and%20Kazuki%20Hayashi%20and%20Yusuke%20Sakai%20and%20Jingun%20Kwon%20and%20Hidetaka%20Kamigaito%20and%20Katsuhiko%20Hayashi%20and%20Manabu%20Okumura%20and%20Taro%20Watanabe%0AAbstract%3A%20%20%20Generating%20images%20from%20prompts%20containing%20specific%20entities%20requires%20models%0Ato%20retain%20as%20much%20entity-specific%20knowledge%20as%20possible.%20However%2C%20fully%0Amemorizing%20such%20knowledge%20is%20impractical%20due%20to%20the%20vast%20number%20of%20entities%20and%0Atheir%20continuous%20emergence.%20To%20address%20this%2C%20we%20propose%20Text-based%20Intelligent%0AGeneration%20with%20Entity%20prompt%20Refinement%20%28TextTIGER%29%2C%20which%20augments%20knowledge%0Aon%20entities%20included%20in%20the%20prompts%20and%20then%20summarizes%20the%20augmented%0Adescriptions%20using%20Large%20Language%20Models%20%28LLMs%29%20to%20mitigate%20performance%0Adegradation%20from%20longer%20inputs.%20To%20evaluate%20our%20method%2C%20we%20introduce%20WiT-Cub%0A%28WiT%20with%20Captions%20and%20Uncomplicated%20Background-explanations%29%2C%20a%20dataset%0Acomprising%20captions%2C%20images%2C%20and%20an%20entity%20list.%20Experiments%20on%20four%20image%0Ageneration%20models%20and%20five%20LLMs%20show%20that%20TextTIGER%20improves%20image%20generation%0Aperformance%20in%20standard%20metrics%20%28IS%2C%20FID%2C%20and%20CLIPScore%29%20compared%20to%0Acaption-only%20prompts.%20Additionally%2C%20multiple%20annotators%27%20evaluation%20confirms%0Athat%20the%20summarized%20descriptions%20are%20more%20informative%2C%20validating%20LLMs%27%20ability%0Ato%20generate%20concise%20yet%20rich%20descriptions.%20These%20findings%20demonstrate%20that%0Arefining%20prompts%20with%20augmented%20and%20summarized%20entity-related%20descriptions%0Aenhances%20image%20generation%20capabilities.%20The%20code%20and%20dataset%20will%20be%20available%0Aupon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextTIGER%253A%2520Text-based%2520Intelligent%2520Generation%2520with%2520Entity%2520Prompt%250A%2520%2520Refinement%2520for%2520Text-to-Image%2520Generation%26entry.906535625%3DShintaro%2520Ozaki%2520and%2520Kazuki%2520Hayashi%2520and%2520Yusuke%2520Sakai%2520and%2520Jingun%2520Kwon%2520and%2520Hidetaka%2520Kamigaito%2520and%2520Katsuhiko%2520Hayashi%2520and%2520Manabu%2520Okumura%2520and%2520Taro%2520Watanabe%26entry.1292438233%3D%2520%2520Generating%2520images%2520from%2520prompts%2520containing%2520specific%2520entities%2520requires%2520models%250Ato%2520retain%2520as%2520much%2520entity-specific%2520knowledge%2520as%2520possible.%2520However%252C%2520fully%250Amemorizing%2520such%2520knowledge%2520is%2520impractical%2520due%2520to%2520the%2520vast%2520number%2520of%2520entities%2520and%250Atheir%2520continuous%2520emergence.%2520To%2520address%2520this%252C%2520we%2520propose%2520Text-based%2520Intelligent%250AGeneration%2520with%2520Entity%2520prompt%2520Refinement%2520%2528TextTIGER%2529%252C%2520which%2520augments%2520knowledge%250Aon%2520entities%2520included%2520in%2520the%2520prompts%2520and%2520then%2520summarizes%2520the%2520augmented%250Adescriptions%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520mitigate%2520performance%250Adegradation%2520from%2520longer%2520inputs.%2520To%2520evaluate%2520our%2520method%252C%2520we%2520introduce%2520WiT-Cub%250A%2528WiT%2520with%2520Captions%2520and%2520Uncomplicated%2520Background-explanations%2529%252C%2520a%2520dataset%250Acomprising%2520captions%252C%2520images%252C%2520and%2520an%2520entity%2520list.%2520Experiments%2520on%2520four%2520image%250Ageneration%2520models%2520and%2520five%2520LLMs%2520show%2520that%2520TextTIGER%2520improves%2520image%2520generation%250Aperformance%2520in%2520standard%2520metrics%2520%2528IS%252C%2520FID%252C%2520and%2520CLIPScore%2529%2520compared%2520to%250Acaption-only%2520prompts.%2520Additionally%252C%2520multiple%2520annotators%2527%2520evaluation%2520confirms%250Athat%2520the%2520summarized%2520descriptions%2520are%2520more%2520informative%252C%2520validating%2520LLMs%2527%2520ability%250Ato%2520generate%2520concise%2520yet%2520rich%2520descriptions.%2520These%2520findings%2520demonstrate%2520that%250Arefining%2520prompts%2520with%2520augmented%2520and%2520summarized%2520entity-related%2520descriptions%250Aenhances%2520image%2520generation%2520capabilities.%2520The%2520code%2520and%2520dataset%2520will%2520be%2520available%250Aupon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextTIGER%3A%20Text-based%20Intelligent%20Generation%20with%20Entity%20Prompt%0A%20%20Refinement%20for%20Text-to-Image%20Generation&entry.906535625=Shintaro%20Ozaki%20and%20Kazuki%20Hayashi%20and%20Yusuke%20Sakai%20and%20Jingun%20Kwon%20and%20Hidetaka%20Kamigaito%20and%20Katsuhiko%20Hayashi%20and%20Manabu%20Okumura%20and%20Taro%20Watanabe&entry.1292438233=%20%20Generating%20images%20from%20prompts%20containing%20specific%20entities%20requires%20models%0Ato%20retain%20as%20much%20entity-specific%20knowledge%20as%20possible.%20However%2C%20fully%0Amemorizing%20such%20knowledge%20is%20impractical%20due%20to%20the%20vast%20number%20of%20entities%20and%0Atheir%20continuous%20emergence.%20To%20address%20this%2C%20we%20propose%20Text-based%20Intelligent%0AGeneration%20with%20Entity%20prompt%20Refinement%20%28TextTIGER%29%2C%20which%20augments%20knowledge%0Aon%20entities%20included%20in%20the%20prompts%20and%20then%20summarizes%20the%20augmented%0Adescriptions%20using%20Large%20Language%20Models%20%28LLMs%29%20to%20mitigate%20performance%0Adegradation%20from%20longer%20inputs.%20To%20evaluate%20our%20method%2C%20we%20introduce%20WiT-Cub%0A%28WiT%20with%20Captions%20and%20Uncomplicated%20Background-explanations%29%2C%20a%20dataset%0Acomprising%20captions%2C%20images%2C%20and%20an%20entity%20list.%20Experiments%20on%20four%20image%0Ageneration%20models%20and%20five%20LLMs%20show%20that%20TextTIGER%20improves%20image%20generation%0Aperformance%20in%20standard%20metrics%20%28IS%2C%20FID%2C%20and%20CLIPScore%29%20compared%20to%0Acaption-only%20prompts.%20Additionally%2C%20multiple%20annotators%27%20evaluation%20confirms%0Athat%20the%20summarized%20descriptions%20are%20more%20informative%2C%20validating%20LLMs%27%20ability%0Ato%20generate%20concise%20yet%20rich%20descriptions.%20These%20findings%20demonstrate%20that%0Arefining%20prompts%20with%20augmented%20and%20summarized%20entity-related%20descriptions%0Aenhances%20image%20generation%20capabilities.%20The%20code%20and%20dataset%20will%20be%20available%0Aupon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18269v1&entry.124074799=Read"},
{"title": "Repurposing the scientific literature with vision-language models", "author": "Anton Alyakin and Jaden Stryker and Daniel Alexander Alber and Karl L. Sangwon and Jin Vivian Lee and Brandon Duderstadt and Akshay Save and David Kurland and Spencer Frome and Shrutika Singh and Jeff Zhang and Eunice Yang and Ki Yun Park and Cordelia Orillac and Aly A. Valliani and Sean Neifert and Albert Liu and Aneek Patel and Christopher Livia and Darryl Lau and Ilya Laufer and Peter A. Rozman and Eveline Teresa Hidalgo and Howard Riina and Rui Feng and Todd Hollon and Yindalon Aphinyanaphongs and John G. Golfinos and Laura Snyder and Eric Leuthardt and Douglas Kondziolka and Eric Karl Oermann", "abstract": "  Leading vision-language models (VLMs) are trained on general Internet\ncontent, overlooking scientific journals' rich, domain-specific knowledge.\nTraining on specialty-specific literature could yield high-performance,\ntask-specific tools, enabling generative AI to match generalist models in\nspecialty publishing, educational, and clinical tasks. We created NeuroPubs, a\nmultimodal dataset of 23,000 Neurosurgery Publications articles (134M words,\n78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready\ngraphical abstracts (70% of 100 abstracts) and board-style questions\nindistinguishable from human-written ones (54% of 89,587 questions). We used\nthese questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded,\nrandomized controlled trial, our model demonstrated non-inferiority to then\nstate-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical\nutility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%,\np=0.3797). Our pilot study demonstrates how training generative AI models on\nspecialty-specific journal content - without large-scale internet data -\nresults in high-performance academic and clinical tools, enabling\ndomain-tailored AI across diverse fields.\n", "link": "http://arxiv.org/abs/2502.19546v2", "date": "2025-04-25", "relevancy": 2.6062, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5335}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5335}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Repurposing%20the%20scientific%20literature%20with%20vision-language%20models&body=Title%3A%20Repurposing%20the%20scientific%20literature%20with%20vision-language%20models%0AAuthor%3A%20Anton%20Alyakin%20and%20Jaden%20Stryker%20and%20Daniel%20Alexander%20Alber%20and%20Karl%20L.%20Sangwon%20and%20Jin%20Vivian%20Lee%20and%20Brandon%20Duderstadt%20and%20Akshay%20Save%20and%20David%20Kurland%20and%20Spencer%20Frome%20and%20Shrutika%20Singh%20and%20Jeff%20Zhang%20and%20Eunice%20Yang%20and%20Ki%20Yun%20Park%20and%20Cordelia%20Orillac%20and%20Aly%20A.%20Valliani%20and%20Sean%20Neifert%20and%20Albert%20Liu%20and%20Aneek%20Patel%20and%20Christopher%20Livia%20and%20Darryl%20Lau%20and%20Ilya%20Laufer%20and%20Peter%20A.%20Rozman%20and%20Eveline%20Teresa%20Hidalgo%20and%20Howard%20Riina%20and%20Rui%20Feng%20and%20Todd%20Hollon%20and%20Yindalon%20Aphinyanaphongs%20and%20John%20G.%20Golfinos%20and%20Laura%20Snyder%20and%20Eric%20Leuthardt%20and%20Douglas%20Kondziolka%20and%20Eric%20Karl%20Oermann%0AAbstract%3A%20%20%20Leading%20vision-language%20models%20%28VLMs%29%20are%20trained%20on%20general%20Internet%0Acontent%2C%20overlooking%20scientific%20journals%27%20rich%2C%20domain-specific%20knowledge.%0ATraining%20on%20specialty-specific%20literature%20could%20yield%20high-performance%2C%0Atask-specific%20tools%2C%20enabling%20generative%20AI%20to%20match%20generalist%20models%20in%0Aspecialty%20publishing%2C%20educational%2C%20and%20clinical%20tasks.%20We%20created%20NeuroPubs%2C%20a%0Amultimodal%20dataset%20of%2023%2C000%20Neurosurgery%20Publications%20articles%20%28134M%20words%2C%0A78K%20image-caption%20pairs%29.%20Using%20NeuroPubs%2C%20VLMs%20generated%20publication-ready%0Agraphical%20abstracts%20%2870%25%20of%20100%20abstracts%29%20and%20board-style%20questions%0Aindistinguishable%20from%20human-written%20ones%20%2854%25%20of%2089%2C587%20questions%29.%20We%20used%0Athese%20questions%20to%20train%20CNS-Obsidian%2C%20a%2034B-parameter%20VLM.%20In%20a%20blinded%2C%0Arandomized%20controlled%20trial%2C%20our%20model%20demonstrated%20non-inferiority%20to%20then%0Astate-of-the-art%20GPT-4o%20in%20neurosurgical%20differential%20diagnosis%20%28clinical%0Autility%2C%2040.62%25%20upvotes%20vs.%2057.89%25%2C%20p%3D0.1150%3B%20accuracy%2C%2059.38%25%20vs.%2065.79%25%2C%0Ap%3D0.3797%29.%20Our%20pilot%20study%20demonstrates%20how%20training%20generative%20AI%20models%20on%0Aspecialty-specific%20journal%20content%20-%20without%20large-scale%20internet%20data%20-%0Aresults%20in%20high-performance%20academic%20and%20clinical%20tools%2C%20enabling%0Adomain-tailored%20AI%20across%20diverse%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19546v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepurposing%2520the%2520scientific%2520literature%2520with%2520vision-language%2520models%26entry.906535625%3DAnton%2520Alyakin%2520and%2520Jaden%2520Stryker%2520and%2520Daniel%2520Alexander%2520Alber%2520and%2520Karl%2520L.%2520Sangwon%2520and%2520Jin%2520Vivian%2520Lee%2520and%2520Brandon%2520Duderstadt%2520and%2520Akshay%2520Save%2520and%2520David%2520Kurland%2520and%2520Spencer%2520Frome%2520and%2520Shrutika%2520Singh%2520and%2520Jeff%2520Zhang%2520and%2520Eunice%2520Yang%2520and%2520Ki%2520Yun%2520Park%2520and%2520Cordelia%2520Orillac%2520and%2520Aly%2520A.%2520Valliani%2520and%2520Sean%2520Neifert%2520and%2520Albert%2520Liu%2520and%2520Aneek%2520Patel%2520and%2520Christopher%2520Livia%2520and%2520Darryl%2520Lau%2520and%2520Ilya%2520Laufer%2520and%2520Peter%2520A.%2520Rozman%2520and%2520Eveline%2520Teresa%2520Hidalgo%2520and%2520Howard%2520Riina%2520and%2520Rui%2520Feng%2520and%2520Todd%2520Hollon%2520and%2520Yindalon%2520Aphinyanaphongs%2520and%2520John%2520G.%2520Golfinos%2520and%2520Laura%2520Snyder%2520and%2520Eric%2520Leuthardt%2520and%2520Douglas%2520Kondziolka%2520and%2520Eric%2520Karl%2520Oermann%26entry.1292438233%3D%2520%2520Leading%2520vision-language%2520models%2520%2528VLMs%2529%2520are%2520trained%2520on%2520general%2520Internet%250Acontent%252C%2520overlooking%2520scientific%2520journals%2527%2520rich%252C%2520domain-specific%2520knowledge.%250ATraining%2520on%2520specialty-specific%2520literature%2520could%2520yield%2520high-performance%252C%250Atask-specific%2520tools%252C%2520enabling%2520generative%2520AI%2520to%2520match%2520generalist%2520models%2520in%250Aspecialty%2520publishing%252C%2520educational%252C%2520and%2520clinical%2520tasks.%2520We%2520created%2520NeuroPubs%252C%2520a%250Amultimodal%2520dataset%2520of%252023%252C000%2520Neurosurgery%2520Publications%2520articles%2520%2528134M%2520words%252C%250A78K%2520image-caption%2520pairs%2529.%2520Using%2520NeuroPubs%252C%2520VLMs%2520generated%2520publication-ready%250Agraphical%2520abstracts%2520%252870%2525%2520of%2520100%2520abstracts%2529%2520and%2520board-style%2520questions%250Aindistinguishable%2520from%2520human-written%2520ones%2520%252854%2525%2520of%252089%252C587%2520questions%2529.%2520We%2520used%250Athese%2520questions%2520to%2520train%2520CNS-Obsidian%252C%2520a%252034B-parameter%2520VLM.%2520In%2520a%2520blinded%252C%250Arandomized%2520controlled%2520trial%252C%2520our%2520model%2520demonstrated%2520non-inferiority%2520to%2520then%250Astate-of-the-art%2520GPT-4o%2520in%2520neurosurgical%2520differential%2520diagnosis%2520%2528clinical%250Autility%252C%252040.62%2525%2520upvotes%2520vs.%252057.89%2525%252C%2520p%253D0.1150%253B%2520accuracy%252C%252059.38%2525%2520vs.%252065.79%2525%252C%250Ap%253D0.3797%2529.%2520Our%2520pilot%2520study%2520demonstrates%2520how%2520training%2520generative%2520AI%2520models%2520on%250Aspecialty-specific%2520journal%2520content%2520-%2520without%2520large-scale%2520internet%2520data%2520-%250Aresults%2520in%2520high-performance%2520academic%2520and%2520clinical%2520tools%252C%2520enabling%250Adomain-tailored%2520AI%2520across%2520diverse%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19546v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Repurposing%20the%20scientific%20literature%20with%20vision-language%20models&entry.906535625=Anton%20Alyakin%20and%20Jaden%20Stryker%20and%20Daniel%20Alexander%20Alber%20and%20Karl%20L.%20Sangwon%20and%20Jin%20Vivian%20Lee%20and%20Brandon%20Duderstadt%20and%20Akshay%20Save%20and%20David%20Kurland%20and%20Spencer%20Frome%20and%20Shrutika%20Singh%20and%20Jeff%20Zhang%20and%20Eunice%20Yang%20and%20Ki%20Yun%20Park%20and%20Cordelia%20Orillac%20and%20Aly%20A.%20Valliani%20and%20Sean%20Neifert%20and%20Albert%20Liu%20and%20Aneek%20Patel%20and%20Christopher%20Livia%20and%20Darryl%20Lau%20and%20Ilya%20Laufer%20and%20Peter%20A.%20Rozman%20and%20Eveline%20Teresa%20Hidalgo%20and%20Howard%20Riina%20and%20Rui%20Feng%20and%20Todd%20Hollon%20and%20Yindalon%20Aphinyanaphongs%20and%20John%20G.%20Golfinos%20and%20Laura%20Snyder%20and%20Eric%20Leuthardt%20and%20Douglas%20Kondziolka%20and%20Eric%20Karl%20Oermann&entry.1292438233=%20%20Leading%20vision-language%20models%20%28VLMs%29%20are%20trained%20on%20general%20Internet%0Acontent%2C%20overlooking%20scientific%20journals%27%20rich%2C%20domain-specific%20knowledge.%0ATraining%20on%20specialty-specific%20literature%20could%20yield%20high-performance%2C%0Atask-specific%20tools%2C%20enabling%20generative%20AI%20to%20match%20generalist%20models%20in%0Aspecialty%20publishing%2C%20educational%2C%20and%20clinical%20tasks.%20We%20created%20NeuroPubs%2C%20a%0Amultimodal%20dataset%20of%2023%2C000%20Neurosurgery%20Publications%20articles%20%28134M%20words%2C%0A78K%20image-caption%20pairs%29.%20Using%20NeuroPubs%2C%20VLMs%20generated%20publication-ready%0Agraphical%20abstracts%20%2870%25%20of%20100%20abstracts%29%20and%20board-style%20questions%0Aindistinguishable%20from%20human-written%20ones%20%2854%25%20of%2089%2C587%20questions%29.%20We%20used%0Athese%20questions%20to%20train%20CNS-Obsidian%2C%20a%2034B-parameter%20VLM.%20In%20a%20blinded%2C%0Arandomized%20controlled%20trial%2C%20our%20model%20demonstrated%20non-inferiority%20to%20then%0Astate-of-the-art%20GPT-4o%20in%20neurosurgical%20differential%20diagnosis%20%28clinical%0Autility%2C%2040.62%25%20upvotes%20vs.%2057.89%25%2C%20p%3D0.1150%3B%20accuracy%2C%2059.38%25%20vs.%2065.79%25%2C%0Ap%3D0.3797%29.%20Our%20pilot%20study%20demonstrates%20how%20training%20generative%20AI%20models%20on%0Aspecialty-specific%20journal%20content%20-%20without%20large-scale%20internet%20data%20-%0Aresults%20in%20high-performance%20academic%20and%20clinical%20tools%2C%20enabling%0Adomain-tailored%20AI%20across%20diverse%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19546v2&entry.124074799=Read"},
{"title": "Fast-Slow Thinking for Large Vision-Language Model Reasoning", "author": "Wenyi Xiao and Leilei Gan and Weilong Dai and Wanggui He and Ziwei Huang and Haoyuan Li and Fangxun Shu and Zhelun Yu and Peng Zhang and Hao Jiang and Fei Wu", "abstract": "  Recent advances in large vision-language models (LVLMs) have revealed an\n\\textit{overthinking} phenomenon, where models generate verbose reasoning\nacross all tasks regardless of questions. To address this issue, we present\n\\textbf{FAST}, a novel \\textbf{Fa}st-\\textbf{S}low \\textbf{T}hinking framework\nthat dynamically adapts reasoning depth based on question characteristics.\nThrough empirical analysis, we establish the feasibility of fast-slow thinking\nin LVLMs by investigating how response length and data distribution affect\nperformance. We develop FAST-GRPO with three components: model-based metrics\nfor question characterization, an adaptive thinking reward mechanism, and\ndifficulty-aware KL regularization. Experiments across seven reasoning\nbenchmarks demonstrate that FAST achieves state-of-the-art accuracy with over\n10\\% relative improvement compared to the base model, while reducing token\nusage by 32.7-67.3\\% compared to previous slow-thinking approaches, effectively\nbalancing reasoning length and accuracy.\n", "link": "http://arxiv.org/abs/2504.18458v1", "date": "2025-04-25", "relevancy": 2.6029, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast-Slow%20Thinking%20for%20Large%20Vision-Language%20Model%20Reasoning&body=Title%3A%20Fast-Slow%20Thinking%20for%20Large%20Vision-Language%20Model%20Reasoning%0AAuthor%3A%20Wenyi%20Xiao%20and%20Leilei%20Gan%20and%20Weilong%20Dai%20and%20Wanggui%20He%20and%20Ziwei%20Huang%20and%20Haoyuan%20Li%20and%20Fangxun%20Shu%20and%20Zhelun%20Yu%20and%20Peng%20Zhang%20and%20Hao%20Jiang%20and%20Fei%20Wu%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20vision-language%20models%20%28LVLMs%29%20have%20revealed%20an%0A%5Ctextit%7Boverthinking%7D%20phenomenon%2C%20where%20models%20generate%20verbose%20reasoning%0Aacross%20all%20tasks%20regardless%20of%20questions.%20To%20address%20this%20issue%2C%20we%20present%0A%5Ctextbf%7BFAST%7D%2C%20a%20novel%20%5Ctextbf%7BFa%7Dst-%5Ctextbf%7BS%7Dlow%20%5Ctextbf%7BT%7Dhinking%20framework%0Athat%20dynamically%20adapts%20reasoning%20depth%20based%20on%20question%20characteristics.%0AThrough%20empirical%20analysis%2C%20we%20establish%20the%20feasibility%20of%20fast-slow%20thinking%0Ain%20LVLMs%20by%20investigating%20how%20response%20length%20and%20data%20distribution%20affect%0Aperformance.%20We%20develop%20FAST-GRPO%20with%20three%20components%3A%20model-based%20metrics%0Afor%20question%20characterization%2C%20an%20adaptive%20thinking%20reward%20mechanism%2C%20and%0Adifficulty-aware%20KL%20regularization.%20Experiments%20across%20seven%20reasoning%0Abenchmarks%20demonstrate%20that%20FAST%20achieves%20state-of-the-art%20accuracy%20with%20over%0A10%5C%25%20relative%20improvement%20compared%20to%20the%20base%20model%2C%20while%20reducing%20token%0Ausage%20by%2032.7-67.3%5C%25%20compared%20to%20previous%20slow-thinking%20approaches%2C%20effectively%0Abalancing%20reasoning%20length%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast-Slow%2520Thinking%2520for%2520Large%2520Vision-Language%2520Model%2520Reasoning%26entry.906535625%3DWenyi%2520Xiao%2520and%2520Leilei%2520Gan%2520and%2520Weilong%2520Dai%2520and%2520Wanggui%2520He%2520and%2520Ziwei%2520Huang%2520and%2520Haoyuan%2520Li%2520and%2520Fangxun%2520Shu%2520and%2520Zhelun%2520Yu%2520and%2520Peng%2520Zhang%2520and%2520Hao%2520Jiang%2520and%2520Fei%2520Wu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520revealed%2520an%250A%255Ctextit%257Boverthinking%257D%2520phenomenon%252C%2520where%2520models%2520generate%2520verbose%2520reasoning%250Aacross%2520all%2520tasks%2520regardless%2520of%2520questions.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%250A%255Ctextbf%257BFAST%257D%252C%2520a%2520novel%2520%255Ctextbf%257BFa%257Dst-%255Ctextbf%257BS%257Dlow%2520%255Ctextbf%257BT%257Dhinking%2520framework%250Athat%2520dynamically%2520adapts%2520reasoning%2520depth%2520based%2520on%2520question%2520characteristics.%250AThrough%2520empirical%2520analysis%252C%2520we%2520establish%2520the%2520feasibility%2520of%2520fast-slow%2520thinking%250Ain%2520LVLMs%2520by%2520investigating%2520how%2520response%2520length%2520and%2520data%2520distribution%2520affect%250Aperformance.%2520We%2520develop%2520FAST-GRPO%2520with%2520three%2520components%253A%2520model-based%2520metrics%250Afor%2520question%2520characterization%252C%2520an%2520adaptive%2520thinking%2520reward%2520mechanism%252C%2520and%250Adifficulty-aware%2520KL%2520regularization.%2520Experiments%2520across%2520seven%2520reasoning%250Abenchmarks%2520demonstrate%2520that%2520FAST%2520achieves%2520state-of-the-art%2520accuracy%2520with%2520over%250A10%255C%2525%2520relative%2520improvement%2520compared%2520to%2520the%2520base%2520model%252C%2520while%2520reducing%2520token%250Ausage%2520by%252032.7-67.3%255C%2525%2520compared%2520to%2520previous%2520slow-thinking%2520approaches%252C%2520effectively%250Abalancing%2520reasoning%2520length%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast-Slow%20Thinking%20for%20Large%20Vision-Language%20Model%20Reasoning&entry.906535625=Wenyi%20Xiao%20and%20Leilei%20Gan%20and%20Weilong%20Dai%20and%20Wanggui%20He%20and%20Ziwei%20Huang%20and%20Haoyuan%20Li%20and%20Fangxun%20Shu%20and%20Zhelun%20Yu%20and%20Peng%20Zhang%20and%20Hao%20Jiang%20and%20Fei%20Wu&entry.1292438233=%20%20Recent%20advances%20in%20large%20vision-language%20models%20%28LVLMs%29%20have%20revealed%20an%0A%5Ctextit%7Boverthinking%7D%20phenomenon%2C%20where%20models%20generate%20verbose%20reasoning%0Aacross%20all%20tasks%20regardless%20of%20questions.%20To%20address%20this%20issue%2C%20we%20present%0A%5Ctextbf%7BFAST%7D%2C%20a%20novel%20%5Ctextbf%7BFa%7Dst-%5Ctextbf%7BS%7Dlow%20%5Ctextbf%7BT%7Dhinking%20framework%0Athat%20dynamically%20adapts%20reasoning%20depth%20based%20on%20question%20characteristics.%0AThrough%20empirical%20analysis%2C%20we%20establish%20the%20feasibility%20of%20fast-slow%20thinking%0Ain%20LVLMs%20by%20investigating%20how%20response%20length%20and%20data%20distribution%20affect%0Aperformance.%20We%20develop%20FAST-GRPO%20with%20three%20components%3A%20model-based%20metrics%0Afor%20question%20characterization%2C%20an%20adaptive%20thinking%20reward%20mechanism%2C%20and%0Adifficulty-aware%20KL%20regularization.%20Experiments%20across%20seven%20reasoning%0Abenchmarks%20demonstrate%20that%20FAST%20achieves%20state-of-the-art%20accuracy%20with%20over%0A10%5C%25%20relative%20improvement%20compared%20to%20the%20base%20model%2C%20while%20reducing%20token%0Ausage%20by%2032.7-67.3%5C%25%20compared%20to%20previous%20slow-thinking%20approaches%2C%20effectively%0Abalancing%20reasoning%20length%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18458v1&entry.124074799=Read"},
{"title": "SpINR: Neural Volumetric Reconstruction for FMCW Radars", "author": "Harshvardhan Takawale and Nirupam Roy", "abstract": "  In this paper, we introduce SpINR, a novel framework for volumetric\nreconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar data.\nTraditional radar imaging techniques, such as backprojection, often assume\nideal signal models and require dense aperture sampling, leading to limitations\nin resolution and generalization. To address these challenges, SpINR integrates\na fully differentiable forward model that operates natively in the frequency\ndomain with implicit neural representations (INRs). This integration leverages\nthe linear relationship between beat frequency and scatterer distance inherent\nin FMCW radar systems, facilitating more efficient and accurate learning of\nscene geometry. Additionally, by computing outputs for only the relevant\nfrequency bins, our forward model achieves greater computational efficiency\ncompared to time-domain approaches that process the entire signal before\ntransformation. Through extensive experiments, we demonstrate that SpINR\nsignificantly outperforms classical backprojection methods and existing\nlearning-based approaches, achieving higher resolution and more accurate\nreconstructions of complex scenes. This work represents the first application\nof neural volumetic reconstruction in the radar domain, offering a promising\ndirection for future research in radar-based imaging and perception systems.\n", "link": "http://arxiv.org/abs/2503.23313v2", "date": "2025-04-25", "relevancy": 2.5997, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5493}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5322}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpINR%3A%20Neural%20Volumetric%20Reconstruction%20for%20FMCW%20Radars&body=Title%3A%20SpINR%3A%20Neural%20Volumetric%20Reconstruction%20for%20FMCW%20Radars%0AAuthor%3A%20Harshvardhan%20Takawale%20and%20Nirupam%20Roy%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20SpINR%2C%20a%20novel%20framework%20for%20volumetric%0Areconstruction%20using%20Frequency-Modulated%20Continuous-Wave%20%28FMCW%29%20radar%20data.%0ATraditional%20radar%20imaging%20techniques%2C%20such%20as%20backprojection%2C%20often%20assume%0Aideal%20signal%20models%20and%20require%20dense%20aperture%20sampling%2C%20leading%20to%20limitations%0Ain%20resolution%20and%20generalization.%20To%20address%20these%20challenges%2C%20SpINR%20integrates%0Aa%20fully%20differentiable%20forward%20model%20that%20operates%20natively%20in%20the%20frequency%0Adomain%20with%20implicit%20neural%20representations%20%28INRs%29.%20This%20integration%20leverages%0Athe%20linear%20relationship%20between%20beat%20frequency%20and%20scatterer%20distance%20inherent%0Ain%20FMCW%20radar%20systems%2C%20facilitating%20more%20efficient%20and%20accurate%20learning%20of%0Ascene%20geometry.%20Additionally%2C%20by%20computing%20outputs%20for%20only%20the%20relevant%0Afrequency%20bins%2C%20our%20forward%20model%20achieves%20greater%20computational%20efficiency%0Acompared%20to%20time-domain%20approaches%20that%20process%20the%20entire%20signal%20before%0Atransformation.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20SpINR%0Asignificantly%20outperforms%20classical%20backprojection%20methods%20and%20existing%0Alearning-based%20approaches%2C%20achieving%20higher%20resolution%20and%20more%20accurate%0Areconstructions%20of%20complex%20scenes.%20This%20work%20represents%20the%20first%20application%0Aof%20neural%20volumetic%20reconstruction%20in%20the%20radar%20domain%2C%20offering%20a%20promising%0Adirection%20for%20future%20research%20in%20radar-based%20imaging%20and%20perception%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23313v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpINR%253A%2520Neural%2520Volumetric%2520Reconstruction%2520for%2520FMCW%2520Radars%26entry.906535625%3DHarshvardhan%2520Takawale%2520and%2520Nirupam%2520Roy%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SpINR%252C%2520a%2520novel%2520framework%2520for%2520volumetric%250Areconstruction%2520using%2520Frequency-Modulated%2520Continuous-Wave%2520%2528FMCW%2529%2520radar%2520data.%250ATraditional%2520radar%2520imaging%2520techniques%252C%2520such%2520as%2520backprojection%252C%2520often%2520assume%250Aideal%2520signal%2520models%2520and%2520require%2520dense%2520aperture%2520sampling%252C%2520leading%2520to%2520limitations%250Ain%2520resolution%2520and%2520generalization.%2520To%2520address%2520these%2520challenges%252C%2520SpINR%2520integrates%250Aa%2520fully%2520differentiable%2520forward%2520model%2520that%2520operates%2520natively%2520in%2520the%2520frequency%250Adomain%2520with%2520implicit%2520neural%2520representations%2520%2528INRs%2529.%2520This%2520integration%2520leverages%250Athe%2520linear%2520relationship%2520between%2520beat%2520frequency%2520and%2520scatterer%2520distance%2520inherent%250Ain%2520FMCW%2520radar%2520systems%252C%2520facilitating%2520more%2520efficient%2520and%2520accurate%2520learning%2520of%250Ascene%2520geometry.%2520Additionally%252C%2520by%2520computing%2520outputs%2520for%2520only%2520the%2520relevant%250Afrequency%2520bins%252C%2520our%2520forward%2520model%2520achieves%2520greater%2520computational%2520efficiency%250Acompared%2520to%2520time-domain%2520approaches%2520that%2520process%2520the%2520entire%2520signal%2520before%250Atransformation.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520SpINR%250Asignificantly%2520outperforms%2520classical%2520backprojection%2520methods%2520and%2520existing%250Alearning-based%2520approaches%252C%2520achieving%2520higher%2520resolution%2520and%2520more%2520accurate%250Areconstructions%2520of%2520complex%2520scenes.%2520This%2520work%2520represents%2520the%2520first%2520application%250Aof%2520neural%2520volumetic%2520reconstruction%2520in%2520the%2520radar%2520domain%252C%2520offering%2520a%2520promising%250Adirection%2520for%2520future%2520research%2520in%2520radar-based%2520imaging%2520and%2520perception%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23313v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpINR%3A%20Neural%20Volumetric%20Reconstruction%20for%20FMCW%20Radars&entry.906535625=Harshvardhan%20Takawale%20and%20Nirupam%20Roy&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20SpINR%2C%20a%20novel%20framework%20for%20volumetric%0Areconstruction%20using%20Frequency-Modulated%20Continuous-Wave%20%28FMCW%29%20radar%20data.%0ATraditional%20radar%20imaging%20techniques%2C%20such%20as%20backprojection%2C%20often%20assume%0Aideal%20signal%20models%20and%20require%20dense%20aperture%20sampling%2C%20leading%20to%20limitations%0Ain%20resolution%20and%20generalization.%20To%20address%20these%20challenges%2C%20SpINR%20integrates%0Aa%20fully%20differentiable%20forward%20model%20that%20operates%20natively%20in%20the%20frequency%0Adomain%20with%20implicit%20neural%20representations%20%28INRs%29.%20This%20integration%20leverages%0Athe%20linear%20relationship%20between%20beat%20frequency%20and%20scatterer%20distance%20inherent%0Ain%20FMCW%20radar%20systems%2C%20facilitating%20more%20efficient%20and%20accurate%20learning%20of%0Ascene%20geometry.%20Additionally%2C%20by%20computing%20outputs%20for%20only%20the%20relevant%0Afrequency%20bins%2C%20our%20forward%20model%20achieves%20greater%20computational%20efficiency%0Acompared%20to%20time-domain%20approaches%20that%20process%20the%20entire%20signal%20before%0Atransformation.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20SpINR%0Asignificantly%20outperforms%20classical%20backprojection%20methods%20and%20existing%0Alearning-based%20approaches%2C%20achieving%20higher%20resolution%20and%20more%20accurate%0Areconstructions%20of%20complex%20scenes.%20This%20work%20represents%20the%20first%20application%0Aof%20neural%20volumetic%20reconstruction%20in%20the%20radar%20domain%2C%20offering%20a%20promising%0Adirection%20for%20future%20research%20in%20radar-based%20imaging%20and%20perception%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23313v2&entry.124074799=Read"},
{"title": "Structure Learning in Gaussian Graphical Models from Glauber Dynamics", "author": "Vignesh Tirukkonda and Anirudh Rayas and Gautam Dasarathy", "abstract": "  Gaussian graphical model selection is an important paradigm with numerous\napplications, including biological network modeling, financial network\nmodeling, and social network analysis. Traditional approaches assume access to\nindependent and identically distributed (i.i.d) samples, which is often\nimpractical in real-world scenarios. In this paper, we address Gaussian\ngraphical model selection under observations from a more realistic dependent\nstochastic process known as Glauber dynamics. Glauber dynamics, also called the\nGibbs sampler, is a Markov chain that sequentially updates the variables of the\nunderlying model based on the statistics of the remaining model. Such models,\naside from frequently being employed to generate samples from complex\nmultivariate distributions, naturally arise in various settings, such as\nopinion consensus in social networks and clearing/stock-price dynamics in\nfinancial networks.\n  In contrast to the extensive body of existing work, we present the first\nalgorithm for Gaussian graphical model selection when data are sampled\naccording to the Glauber dynamics. We provide theoretical guarantees on the\ncomputational and statistical complexity of the proposed algorithm's structure\nlearning performance. Additionally, we provide information-theoretic lower\nbounds on the statistical complexity and show that our algorithm is nearly\nminimax optimal for a broad class of problems.\n", "link": "http://arxiv.org/abs/2412.18594v2", "date": "2025-04-25", "relevancy": 2.5886, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5594}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4969}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20Learning%20in%20Gaussian%20Graphical%20Models%20from%20Glauber%20Dynamics&body=Title%3A%20Structure%20Learning%20in%20Gaussian%20Graphical%20Models%20from%20Glauber%20Dynamics%0AAuthor%3A%20Vignesh%20Tirukkonda%20and%20Anirudh%20Rayas%20and%20Gautam%20Dasarathy%0AAbstract%3A%20%20%20Gaussian%20graphical%20model%20selection%20is%20an%20important%20paradigm%20with%20numerous%0Aapplications%2C%20including%20biological%20network%20modeling%2C%20financial%20network%0Amodeling%2C%20and%20social%20network%20analysis.%20Traditional%20approaches%20assume%20access%20to%0Aindependent%20and%20identically%20distributed%20%28i.i.d%29%20samples%2C%20which%20is%20often%0Aimpractical%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20address%20Gaussian%0Agraphical%20model%20selection%20under%20observations%20from%20a%20more%20realistic%20dependent%0Astochastic%20process%20known%20as%20Glauber%20dynamics.%20Glauber%20dynamics%2C%20also%20called%20the%0AGibbs%20sampler%2C%20is%20a%20Markov%20chain%20that%20sequentially%20updates%20the%20variables%20of%20the%0Aunderlying%20model%20based%20on%20the%20statistics%20of%20the%20remaining%20model.%20Such%20models%2C%0Aaside%20from%20frequently%20being%20employed%20to%20generate%20samples%20from%20complex%0Amultivariate%20distributions%2C%20naturally%20arise%20in%20various%20settings%2C%20such%20as%0Aopinion%20consensus%20in%20social%20networks%20and%20clearing/stock-price%20dynamics%20in%0Afinancial%20networks.%0A%20%20In%20contrast%20to%20the%20extensive%20body%20of%20existing%20work%2C%20we%20present%20the%20first%0Aalgorithm%20for%20Gaussian%20graphical%20model%20selection%20when%20data%20are%20sampled%0Aaccording%20to%20the%20Glauber%20dynamics.%20We%20provide%20theoretical%20guarantees%20on%20the%0Acomputational%20and%20statistical%20complexity%20of%20the%20proposed%20algorithm%27s%20structure%0Alearning%20performance.%20Additionally%2C%20we%20provide%20information-theoretic%20lower%0Abounds%20on%20the%20statistical%20complexity%20and%20show%20that%20our%20algorithm%20is%20nearly%0Aminimax%20optimal%20for%20a%20broad%20class%20of%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18594v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520Learning%2520in%2520Gaussian%2520Graphical%2520Models%2520from%2520Glauber%2520Dynamics%26entry.906535625%3DVignesh%2520Tirukkonda%2520and%2520Anirudh%2520Rayas%2520and%2520Gautam%2520Dasarathy%26entry.1292438233%3D%2520%2520Gaussian%2520graphical%2520model%2520selection%2520is%2520an%2520important%2520paradigm%2520with%2520numerous%250Aapplications%252C%2520including%2520biological%2520network%2520modeling%252C%2520financial%2520network%250Amodeling%252C%2520and%2520social%2520network%2520analysis.%2520Traditional%2520approaches%2520assume%2520access%2520to%250Aindependent%2520and%2520identically%2520distributed%2520%2528i.i.d%2529%2520samples%252C%2520which%2520is%2520often%250Aimpractical%2520in%2520real-world%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520address%2520Gaussian%250Agraphical%2520model%2520selection%2520under%2520observations%2520from%2520a%2520more%2520realistic%2520dependent%250Astochastic%2520process%2520known%2520as%2520Glauber%2520dynamics.%2520Glauber%2520dynamics%252C%2520also%2520called%2520the%250AGibbs%2520sampler%252C%2520is%2520a%2520Markov%2520chain%2520that%2520sequentially%2520updates%2520the%2520variables%2520of%2520the%250Aunderlying%2520model%2520based%2520on%2520the%2520statistics%2520of%2520the%2520remaining%2520model.%2520Such%2520models%252C%250Aaside%2520from%2520frequently%2520being%2520employed%2520to%2520generate%2520samples%2520from%2520complex%250Amultivariate%2520distributions%252C%2520naturally%2520arise%2520in%2520various%2520settings%252C%2520such%2520as%250Aopinion%2520consensus%2520in%2520social%2520networks%2520and%2520clearing/stock-price%2520dynamics%2520in%250Afinancial%2520networks.%250A%2520%2520In%2520contrast%2520to%2520the%2520extensive%2520body%2520of%2520existing%2520work%252C%2520we%2520present%2520the%2520first%250Aalgorithm%2520for%2520Gaussian%2520graphical%2520model%2520selection%2520when%2520data%2520are%2520sampled%250Aaccording%2520to%2520the%2520Glauber%2520dynamics.%2520We%2520provide%2520theoretical%2520guarantees%2520on%2520the%250Acomputational%2520and%2520statistical%2520complexity%2520of%2520the%2520proposed%2520algorithm%2527s%2520structure%250Alearning%2520performance.%2520Additionally%252C%2520we%2520provide%2520information-theoretic%2520lower%250Abounds%2520on%2520the%2520statistical%2520complexity%2520and%2520show%2520that%2520our%2520algorithm%2520is%2520nearly%250Aminimax%2520optimal%2520for%2520a%2520broad%2520class%2520of%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18594v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20Learning%20in%20Gaussian%20Graphical%20Models%20from%20Glauber%20Dynamics&entry.906535625=Vignesh%20Tirukkonda%20and%20Anirudh%20Rayas%20and%20Gautam%20Dasarathy&entry.1292438233=%20%20Gaussian%20graphical%20model%20selection%20is%20an%20important%20paradigm%20with%20numerous%0Aapplications%2C%20including%20biological%20network%20modeling%2C%20financial%20network%0Amodeling%2C%20and%20social%20network%20analysis.%20Traditional%20approaches%20assume%20access%20to%0Aindependent%20and%20identically%20distributed%20%28i.i.d%29%20samples%2C%20which%20is%20often%0Aimpractical%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20address%20Gaussian%0Agraphical%20model%20selection%20under%20observations%20from%20a%20more%20realistic%20dependent%0Astochastic%20process%20known%20as%20Glauber%20dynamics.%20Glauber%20dynamics%2C%20also%20called%20the%0AGibbs%20sampler%2C%20is%20a%20Markov%20chain%20that%20sequentially%20updates%20the%20variables%20of%20the%0Aunderlying%20model%20based%20on%20the%20statistics%20of%20the%20remaining%20model.%20Such%20models%2C%0Aaside%20from%20frequently%20being%20employed%20to%20generate%20samples%20from%20complex%0Amultivariate%20distributions%2C%20naturally%20arise%20in%20various%20settings%2C%20such%20as%0Aopinion%20consensus%20in%20social%20networks%20and%20clearing/stock-price%20dynamics%20in%0Afinancial%20networks.%0A%20%20In%20contrast%20to%20the%20extensive%20body%20of%20existing%20work%2C%20we%20present%20the%20first%0Aalgorithm%20for%20Gaussian%20graphical%20model%20selection%20when%20data%20are%20sampled%0Aaccording%20to%20the%20Glauber%20dynamics.%20We%20provide%20theoretical%20guarantees%20on%20the%0Acomputational%20and%20statistical%20complexity%20of%20the%20proposed%20algorithm%27s%20structure%0Alearning%20performance.%20Additionally%2C%20we%20provide%20information-theoretic%20lower%0Abounds%20on%20the%20statistical%20complexity%20and%20show%20that%20our%20algorithm%20is%20nearly%0Aminimax%20optimal%20for%20a%20broad%20class%20of%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18594v2&entry.124074799=Read"},
{"title": "CR-LSO: Convex Neural Architecture Optimization in the Latent Space of\n  Graph Variational Autoencoder with Input Convex Neural Networks", "author": "Xuan Rao and Bo Zhao and Derong Liu", "abstract": "  In neural architecture search (NAS) methods based on latent space\noptimization (LSO), a deep generative model is trained to embed discrete neural\narchitectures into a continuous latent space. In this case, different\noptimization algorithms that operate in the continuous space can be implemented\nto search neural architectures. However, the optimization of latent variables\nis challenging for gradient-based LSO since the mapping from the latent space\nto the architecture performance is generally non-convex. To tackle this\nproblem, this paper develops a convexity regularized latent space optimization\n(CR-LSO) method, which aims to regularize the learning process of latent space\nin order to obtain a convex architecture performance mapping. Specifically,\nCR-LSO trains a graph variational autoencoder (G-VAE) to learn the continuous\nrepresentations of discrete architectures. Simultaneously, the learning process\nof latent space is regularized by the guaranteed convexity of input convex\nneural networks (ICNNs). In this way, the G-VAE is forced to learn a convex\nmapping from the architecture representation to the architecture performance.\nHereafter, the CR-LSO approximates the performance mapping using the ICNN and\nleverages the estimated gradient to optimize neural architecture\nrepresentations. Experimental results on three popular NAS benchmarks show that\nCR-LSO achieves competitive evaluation results in terms of both computational\ncomplexity and architecture performance.\n", "link": "http://arxiv.org/abs/2211.05950v2", "date": "2025-04-25", "relevancy": 2.5828, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5582}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5102}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CR-LSO%3A%20Convex%20Neural%20Architecture%20Optimization%20in%20the%20Latent%20Space%20of%0A%20%20Graph%20Variational%20Autoencoder%20with%20Input%20Convex%20Neural%20Networks&body=Title%3A%20CR-LSO%3A%20Convex%20Neural%20Architecture%20Optimization%20in%20the%20Latent%20Space%20of%0A%20%20Graph%20Variational%20Autoencoder%20with%20Input%20Convex%20Neural%20Networks%0AAuthor%3A%20Xuan%20Rao%20and%20Bo%20Zhao%20and%20Derong%20Liu%0AAbstract%3A%20%20%20In%20neural%20architecture%20search%20%28NAS%29%20methods%20based%20on%20latent%20space%0Aoptimization%20%28LSO%29%2C%20a%20deep%20generative%20model%20is%20trained%20to%20embed%20discrete%20neural%0Aarchitectures%20into%20a%20continuous%20latent%20space.%20In%20this%20case%2C%20different%0Aoptimization%20algorithms%20that%20operate%20in%20the%20continuous%20space%20can%20be%20implemented%0Ato%20search%20neural%20architectures.%20However%2C%20the%20optimization%20of%20latent%20variables%0Ais%20challenging%20for%20gradient-based%20LSO%20since%20the%20mapping%20from%20the%20latent%20space%0Ato%20the%20architecture%20performance%20is%20generally%20non-convex.%20To%20tackle%20this%0Aproblem%2C%20this%20paper%20develops%20a%20convexity%20regularized%20latent%20space%20optimization%0A%28CR-LSO%29%20method%2C%20which%20aims%20to%20regularize%20the%20learning%20process%20of%20latent%20space%0Ain%20order%20to%20obtain%20a%20convex%20architecture%20performance%20mapping.%20Specifically%2C%0ACR-LSO%20trains%20a%20graph%20variational%20autoencoder%20%28G-VAE%29%20to%20learn%20the%20continuous%0Arepresentations%20of%20discrete%20architectures.%20Simultaneously%2C%20the%20learning%20process%0Aof%20latent%20space%20is%20regularized%20by%20the%20guaranteed%20convexity%20of%20input%20convex%0Aneural%20networks%20%28ICNNs%29.%20In%20this%20way%2C%20the%20G-VAE%20is%20forced%20to%20learn%20a%20convex%0Amapping%20from%20the%20architecture%20representation%20to%20the%20architecture%20performance.%0AHereafter%2C%20the%20CR-LSO%20approximates%20the%20performance%20mapping%20using%20the%20ICNN%20and%0Aleverages%20the%20estimated%20gradient%20to%20optimize%20neural%20architecture%0Arepresentations.%20Experimental%20results%20on%20three%20popular%20NAS%20benchmarks%20show%20that%0ACR-LSO%20achieves%20competitive%20evaluation%20results%20in%20terms%20of%20both%20computational%0Acomplexity%20and%20architecture%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.05950v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCR-LSO%253A%2520Convex%2520Neural%2520Architecture%2520Optimization%2520in%2520the%2520Latent%2520Space%2520of%250A%2520%2520Graph%2520Variational%2520Autoencoder%2520with%2520Input%2520Convex%2520Neural%2520Networks%26entry.906535625%3DXuan%2520Rao%2520and%2520Bo%2520Zhao%2520and%2520Derong%2520Liu%26entry.1292438233%3D%2520%2520In%2520neural%2520architecture%2520search%2520%2528NAS%2529%2520methods%2520based%2520on%2520latent%2520space%250Aoptimization%2520%2528LSO%2529%252C%2520a%2520deep%2520generative%2520model%2520is%2520trained%2520to%2520embed%2520discrete%2520neural%250Aarchitectures%2520into%2520a%2520continuous%2520latent%2520space.%2520In%2520this%2520case%252C%2520different%250Aoptimization%2520algorithms%2520that%2520operate%2520in%2520the%2520continuous%2520space%2520can%2520be%2520implemented%250Ato%2520search%2520neural%2520architectures.%2520However%252C%2520the%2520optimization%2520of%2520latent%2520variables%250Ais%2520challenging%2520for%2520gradient-based%2520LSO%2520since%2520the%2520mapping%2520from%2520the%2520latent%2520space%250Ato%2520the%2520architecture%2520performance%2520is%2520generally%2520non-convex.%2520To%2520tackle%2520this%250Aproblem%252C%2520this%2520paper%2520develops%2520a%2520convexity%2520regularized%2520latent%2520space%2520optimization%250A%2528CR-LSO%2529%2520method%252C%2520which%2520aims%2520to%2520regularize%2520the%2520learning%2520process%2520of%2520latent%2520space%250Ain%2520order%2520to%2520obtain%2520a%2520convex%2520architecture%2520performance%2520mapping.%2520Specifically%252C%250ACR-LSO%2520trains%2520a%2520graph%2520variational%2520autoencoder%2520%2528G-VAE%2529%2520to%2520learn%2520the%2520continuous%250Arepresentations%2520of%2520discrete%2520architectures.%2520Simultaneously%252C%2520the%2520learning%2520process%250Aof%2520latent%2520space%2520is%2520regularized%2520by%2520the%2520guaranteed%2520convexity%2520of%2520input%2520convex%250Aneural%2520networks%2520%2528ICNNs%2529.%2520In%2520this%2520way%252C%2520the%2520G-VAE%2520is%2520forced%2520to%2520learn%2520a%2520convex%250Amapping%2520from%2520the%2520architecture%2520representation%2520to%2520the%2520architecture%2520performance.%250AHereafter%252C%2520the%2520CR-LSO%2520approximates%2520the%2520performance%2520mapping%2520using%2520the%2520ICNN%2520and%250Aleverages%2520the%2520estimated%2520gradient%2520to%2520optimize%2520neural%2520architecture%250Arepresentations.%2520Experimental%2520results%2520on%2520three%2520popular%2520NAS%2520benchmarks%2520show%2520that%250ACR-LSO%2520achieves%2520competitive%2520evaluation%2520results%2520in%2520terms%2520of%2520both%2520computational%250Acomplexity%2520and%2520architecture%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.05950v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CR-LSO%3A%20Convex%20Neural%20Architecture%20Optimization%20in%20the%20Latent%20Space%20of%0A%20%20Graph%20Variational%20Autoencoder%20with%20Input%20Convex%20Neural%20Networks&entry.906535625=Xuan%20Rao%20and%20Bo%20Zhao%20and%20Derong%20Liu&entry.1292438233=%20%20In%20neural%20architecture%20search%20%28NAS%29%20methods%20based%20on%20latent%20space%0Aoptimization%20%28LSO%29%2C%20a%20deep%20generative%20model%20is%20trained%20to%20embed%20discrete%20neural%0Aarchitectures%20into%20a%20continuous%20latent%20space.%20In%20this%20case%2C%20different%0Aoptimization%20algorithms%20that%20operate%20in%20the%20continuous%20space%20can%20be%20implemented%0Ato%20search%20neural%20architectures.%20However%2C%20the%20optimization%20of%20latent%20variables%0Ais%20challenging%20for%20gradient-based%20LSO%20since%20the%20mapping%20from%20the%20latent%20space%0Ato%20the%20architecture%20performance%20is%20generally%20non-convex.%20To%20tackle%20this%0Aproblem%2C%20this%20paper%20develops%20a%20convexity%20regularized%20latent%20space%20optimization%0A%28CR-LSO%29%20method%2C%20which%20aims%20to%20regularize%20the%20learning%20process%20of%20latent%20space%0Ain%20order%20to%20obtain%20a%20convex%20architecture%20performance%20mapping.%20Specifically%2C%0ACR-LSO%20trains%20a%20graph%20variational%20autoencoder%20%28G-VAE%29%20to%20learn%20the%20continuous%0Arepresentations%20of%20discrete%20architectures.%20Simultaneously%2C%20the%20learning%20process%0Aof%20latent%20space%20is%20regularized%20by%20the%20guaranteed%20convexity%20of%20input%20convex%0Aneural%20networks%20%28ICNNs%29.%20In%20this%20way%2C%20the%20G-VAE%20is%20forced%20to%20learn%20a%20convex%0Amapping%20from%20the%20architecture%20representation%20to%20the%20architecture%20performance.%0AHereafter%2C%20the%20CR-LSO%20approximates%20the%20performance%20mapping%20using%20the%20ICNN%20and%0Aleverages%20the%20estimated%20gradient%20to%20optimize%20neural%20architecture%0Arepresentations.%20Experimental%20results%20on%20three%20popular%20NAS%20benchmarks%20show%20that%0ACR-LSO%20achieves%20competitive%20evaluation%20results%20in%20terms%20of%20both%20computational%0Acomplexity%20and%20architecture%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.05950v2&entry.124074799=Read"},
{"title": "A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time\n  Series via Visualization", "author": "Haoxin Liu and Chenghao Liu and B. Aditya Prakash", "abstract": "  Large language models (LLMs), with demonstrated reasoning abilities across\nmultiple domains, are largely underexplored for time-series reasoning (TsR),\nwhich is ubiquitous in the real world. In this work, we propose TimerBed, the\nfirst comprehensive testbed for evaluating LLMs' TsR performance. Specifically,\nTimerBed includes stratified reasoning patterns with real-world tasks,\ncomprehensive combinations of LLMs and reasoning strategies, and various\nsupervised models as comparison anchors. We perform extensive experiments with\nTimerBed, test multiple current beliefs, and verify the initial failures of\nLLMs in TsR, evidenced by the ineffectiveness of zero shot (ZST) and\nperformance degradation of few shot in-context learning (ICL). Further, we\nidentify one possible root cause: the numerical modeling of data. To address\nthis, we propose a prompt-based solution VL-Time, using visualization-modeled\ndata and language-guided reasoning. Experimental results demonstrate that\nVl-Time enables multimodal LLMs to be non-trivial ZST and powerful ICL\nreasoners for time series, achieving about 140% average performance improvement\nand 99% average token costs reduction.\n", "link": "http://arxiv.org/abs/2411.06018v2", "date": "2025-04-25", "relevancy": 2.5822, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5209}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Picture%20is%20Worth%20A%20Thousand%20Numbers%3A%20Enabling%20LLMs%20Reason%20about%20Time%0A%20%20Series%20via%20Visualization&body=Title%3A%20A%20Picture%20is%20Worth%20A%20Thousand%20Numbers%3A%20Enabling%20LLMs%20Reason%20about%20Time%0A%20%20Series%20via%20Visualization%0AAuthor%3A%20Haoxin%20Liu%20and%20Chenghao%20Liu%20and%20B.%20Aditya%20Prakash%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20with%20demonstrated%20reasoning%20abilities%20across%0Amultiple%20domains%2C%20are%20largely%20underexplored%20for%20time-series%20reasoning%20%28TsR%29%2C%0Awhich%20is%20ubiquitous%20in%20the%20real%20world.%20In%20this%20work%2C%20we%20propose%20TimerBed%2C%20the%0Afirst%20comprehensive%20testbed%20for%20evaluating%20LLMs%27%20TsR%20performance.%20Specifically%2C%0ATimerBed%20includes%20stratified%20reasoning%20patterns%20with%20real-world%20tasks%2C%0Acomprehensive%20combinations%20of%20LLMs%20and%20reasoning%20strategies%2C%20and%20various%0Asupervised%20models%20as%20comparison%20anchors.%20We%20perform%20extensive%20experiments%20with%0ATimerBed%2C%20test%20multiple%20current%20beliefs%2C%20and%20verify%20the%20initial%20failures%20of%0ALLMs%20in%20TsR%2C%20evidenced%20by%20the%20ineffectiveness%20of%20zero%20shot%20%28ZST%29%20and%0Aperformance%20degradation%20of%20few%20shot%20in-context%20learning%20%28ICL%29.%20Further%2C%20we%0Aidentify%20one%20possible%20root%20cause%3A%20the%20numerical%20modeling%20of%20data.%20To%20address%0Athis%2C%20we%20propose%20a%20prompt-based%20solution%20VL-Time%2C%20using%20visualization-modeled%0Adata%20and%20language-guided%20reasoning.%20Experimental%20results%20demonstrate%20that%0AVl-Time%20enables%20multimodal%20LLMs%20to%20be%20non-trivial%20ZST%20and%20powerful%20ICL%0Areasoners%20for%20time%20series%2C%20achieving%20about%20140%25%20average%20performance%20improvement%0Aand%2099%25%20average%20token%20costs%20reduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06018v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Picture%2520is%2520Worth%2520A%2520Thousand%2520Numbers%253A%2520Enabling%2520LLMs%2520Reason%2520about%2520Time%250A%2520%2520Series%2520via%2520Visualization%26entry.906535625%3DHaoxin%2520Liu%2520and%2520Chenghao%2520Liu%2520and%2520B.%2520Aditya%2520Prakash%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520with%2520demonstrated%2520reasoning%2520abilities%2520across%250Amultiple%2520domains%252C%2520are%2520largely%2520underexplored%2520for%2520time-series%2520reasoning%2520%2528TsR%2529%252C%250Awhich%2520is%2520ubiquitous%2520in%2520the%2520real%2520world.%2520In%2520this%2520work%252C%2520we%2520propose%2520TimerBed%252C%2520the%250Afirst%2520comprehensive%2520testbed%2520for%2520evaluating%2520LLMs%2527%2520TsR%2520performance.%2520Specifically%252C%250ATimerBed%2520includes%2520stratified%2520reasoning%2520patterns%2520with%2520real-world%2520tasks%252C%250Acomprehensive%2520combinations%2520of%2520LLMs%2520and%2520reasoning%2520strategies%252C%2520and%2520various%250Asupervised%2520models%2520as%2520comparison%2520anchors.%2520We%2520perform%2520extensive%2520experiments%2520with%250ATimerBed%252C%2520test%2520multiple%2520current%2520beliefs%252C%2520and%2520verify%2520the%2520initial%2520failures%2520of%250ALLMs%2520in%2520TsR%252C%2520evidenced%2520by%2520the%2520ineffectiveness%2520of%2520zero%2520shot%2520%2528ZST%2529%2520and%250Aperformance%2520degradation%2520of%2520few%2520shot%2520in-context%2520learning%2520%2528ICL%2529.%2520Further%252C%2520we%250Aidentify%2520one%2520possible%2520root%2520cause%253A%2520the%2520numerical%2520modeling%2520of%2520data.%2520To%2520address%250Athis%252C%2520we%2520propose%2520a%2520prompt-based%2520solution%2520VL-Time%252C%2520using%2520visualization-modeled%250Adata%2520and%2520language-guided%2520reasoning.%2520Experimental%2520results%2520demonstrate%2520that%250AVl-Time%2520enables%2520multimodal%2520LLMs%2520to%2520be%2520non-trivial%2520ZST%2520and%2520powerful%2520ICL%250Areasoners%2520for%2520time%2520series%252C%2520achieving%2520about%2520140%2525%2520average%2520performance%2520improvement%250Aand%252099%2525%2520average%2520token%2520costs%2520reduction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06018v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Picture%20is%20Worth%20A%20Thousand%20Numbers%3A%20Enabling%20LLMs%20Reason%20about%20Time%0A%20%20Series%20via%20Visualization&entry.906535625=Haoxin%20Liu%20and%20Chenghao%20Liu%20and%20B.%20Aditya%20Prakash&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20with%20demonstrated%20reasoning%20abilities%20across%0Amultiple%20domains%2C%20are%20largely%20underexplored%20for%20time-series%20reasoning%20%28TsR%29%2C%0Awhich%20is%20ubiquitous%20in%20the%20real%20world.%20In%20this%20work%2C%20we%20propose%20TimerBed%2C%20the%0Afirst%20comprehensive%20testbed%20for%20evaluating%20LLMs%27%20TsR%20performance.%20Specifically%2C%0ATimerBed%20includes%20stratified%20reasoning%20patterns%20with%20real-world%20tasks%2C%0Acomprehensive%20combinations%20of%20LLMs%20and%20reasoning%20strategies%2C%20and%20various%0Asupervised%20models%20as%20comparison%20anchors.%20We%20perform%20extensive%20experiments%20with%0ATimerBed%2C%20test%20multiple%20current%20beliefs%2C%20and%20verify%20the%20initial%20failures%20of%0ALLMs%20in%20TsR%2C%20evidenced%20by%20the%20ineffectiveness%20of%20zero%20shot%20%28ZST%29%20and%0Aperformance%20degradation%20of%20few%20shot%20in-context%20learning%20%28ICL%29.%20Further%2C%20we%0Aidentify%20one%20possible%20root%20cause%3A%20the%20numerical%20modeling%20of%20data.%20To%20address%0Athis%2C%20we%20propose%20a%20prompt-based%20solution%20VL-Time%2C%20using%20visualization-modeled%0Adata%20and%20language-guided%20reasoning.%20Experimental%20results%20demonstrate%20that%0AVl-Time%20enables%20multimodal%20LLMs%20to%20be%20non-trivial%20ZST%20and%20powerful%20ICL%0Areasoners%20for%20time%20series%2C%20achieving%20about%20140%25%20average%20performance%20improvement%0Aand%2099%25%20average%20token%20costs%20reduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06018v2&entry.124074799=Read"},
{"title": "Efficient Single-Pass Training for Multi-Turn Reasoning", "author": "Ritesh Goru and Shanay Mehta and Prateek Jain", "abstract": "  Training Large Language Models ( LLMs) to generate explicit reasoning before\nthey produce an answer has been shown to improve their performance across\nvarious tasks such as mathematics and coding. However, fine-tuning LLMs on\nmulti-turn reasoning datasets presents a unique challenge: LLMs must generate\nreasoning tokens that are excluded from subsequent inputs to the LLM. This\ndiscrepancy prevents us from processing an entire conversation in a single\nforward pass-an optimization readily available when we fine-tune on a\nmulti-turn non-reasoning dataset. This paper proposes a novel approach that\novercomes this limitation through response token duplication and a custom\nattention mask that enforces appropriate visibility constraints. Our approach\nsignificantly reduces the training time and allows efficient fine-tuning on\nmulti-turn reasoning datasets.\n", "link": "http://arxiv.org/abs/2504.18246v1", "date": "2025-04-25", "relevancy": 2.5585, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5517}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Single-Pass%20Training%20for%20Multi-Turn%20Reasoning&body=Title%3A%20Efficient%20Single-Pass%20Training%20for%20Multi-Turn%20Reasoning%0AAuthor%3A%20Ritesh%20Goru%20and%20Shanay%20Mehta%20and%20Prateek%20Jain%0AAbstract%3A%20%20%20Training%20Large%20Language%20Models%20%28%20LLMs%29%20to%20generate%20explicit%20reasoning%20before%0Athey%20produce%20an%20answer%20has%20been%20shown%20to%20improve%20their%20performance%20across%0Avarious%20tasks%20such%20as%20mathematics%20and%20coding.%20However%2C%20fine-tuning%20LLMs%20on%0Amulti-turn%20reasoning%20datasets%20presents%20a%20unique%20challenge%3A%20LLMs%20must%20generate%0Areasoning%20tokens%20that%20are%20excluded%20from%20subsequent%20inputs%20to%20the%20LLM.%20This%0Adiscrepancy%20prevents%20us%20from%20processing%20an%20entire%20conversation%20in%20a%20single%0Aforward%20pass-an%20optimization%20readily%20available%20when%20we%20fine-tune%20on%20a%0Amulti-turn%20non-reasoning%20dataset.%20This%20paper%20proposes%20a%20novel%20approach%20that%0Aovercomes%20this%20limitation%20through%20response%20token%20duplication%20and%20a%20custom%0Aattention%20mask%20that%20enforces%20appropriate%20visibility%20constraints.%20Our%20approach%0Asignificantly%20reduces%20the%20training%20time%20and%20allows%20efficient%20fine-tuning%20on%0Amulti-turn%20reasoning%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Single-Pass%2520Training%2520for%2520Multi-Turn%2520Reasoning%26entry.906535625%3DRitesh%2520Goru%2520and%2520Shanay%2520Mehta%2520and%2520Prateek%2520Jain%26entry.1292438233%3D%2520%2520Training%2520Large%2520Language%2520Models%2520%2528%2520LLMs%2529%2520to%2520generate%2520explicit%2520reasoning%2520before%250Athey%2520produce%2520an%2520answer%2520has%2520been%2520shown%2520to%2520improve%2520their%2520performance%2520across%250Avarious%2520tasks%2520such%2520as%2520mathematics%2520and%2520coding.%2520However%252C%2520fine-tuning%2520LLMs%2520on%250Amulti-turn%2520reasoning%2520datasets%2520presents%2520a%2520unique%2520challenge%253A%2520LLMs%2520must%2520generate%250Areasoning%2520tokens%2520that%2520are%2520excluded%2520from%2520subsequent%2520inputs%2520to%2520the%2520LLM.%2520This%250Adiscrepancy%2520prevents%2520us%2520from%2520processing%2520an%2520entire%2520conversation%2520in%2520a%2520single%250Aforward%2520pass-an%2520optimization%2520readily%2520available%2520when%2520we%2520fine-tune%2520on%2520a%250Amulti-turn%2520non-reasoning%2520dataset.%2520This%2520paper%2520proposes%2520a%2520novel%2520approach%2520that%250Aovercomes%2520this%2520limitation%2520through%2520response%2520token%2520duplication%2520and%2520a%2520custom%250Aattention%2520mask%2520that%2520enforces%2520appropriate%2520visibility%2520constraints.%2520Our%2520approach%250Asignificantly%2520reduces%2520the%2520training%2520time%2520and%2520allows%2520efficient%2520fine-tuning%2520on%250Amulti-turn%2520reasoning%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Single-Pass%20Training%20for%20Multi-Turn%20Reasoning&entry.906535625=Ritesh%20Goru%20and%20Shanay%20Mehta%20and%20Prateek%20Jain&entry.1292438233=%20%20Training%20Large%20Language%20Models%20%28%20LLMs%29%20to%20generate%20explicit%20reasoning%20before%0Athey%20produce%20an%20answer%20has%20been%20shown%20to%20improve%20their%20performance%20across%0Avarious%20tasks%20such%20as%20mathematics%20and%20coding.%20However%2C%20fine-tuning%20LLMs%20on%0Amulti-turn%20reasoning%20datasets%20presents%20a%20unique%20challenge%3A%20LLMs%20must%20generate%0Areasoning%20tokens%20that%20are%20excluded%20from%20subsequent%20inputs%20to%20the%20LLM.%20This%0Adiscrepancy%20prevents%20us%20from%20processing%20an%20entire%20conversation%20in%20a%20single%0Aforward%20pass-an%20optimization%20readily%20available%20when%20we%20fine-tune%20on%20a%0Amulti-turn%20non-reasoning%20dataset.%20This%20paper%20proposes%20a%20novel%20approach%20that%0Aovercomes%20this%20limitation%20through%20response%20token%20duplication%20and%20a%20custom%0Aattention%20mask%20that%20enforces%20appropriate%20visibility%20constraints.%20Our%20approach%0Asignificantly%20reduces%20the%20training%20time%20and%20allows%20efficient%20fine-tuning%20on%0Amulti-turn%20reasoning%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18246v1&entry.124074799=Read"},
{"title": "L4P: Low-Level 4D Vision Perception Unified", "author": "Abhishek Badki and Hang Su and Bowen Wen and Orazio Gallo", "abstract": "  The spatio-temporal relationship between the pixels of a video carries\ncritical information for low-level 4D perception tasks. A single model that\nreasons about it should be able to solve several such tasks well. Yet, most\nstate-of-the-art methods rely on architectures specialized for the task at\nhand. We present L4P, a feedforward, general-purpose architecture that solves\nlow-level 4D perception tasks in a unified framework. L4P leverages a\npre-trained ViT-based video encoder and combines it with per-task heads that\nare lightweight and therefore do not require extensive training. Despite its\ngeneral and feedforward formulation, our method matches or surpasses the\nperformance of existing specialized methods on both dense tasks, such as depth\nor optical flow estimation, and sparse tasks, such as 2D/3D tracking. Moreover,\nit solves all tasks at once in a time comparable to that of single-task\nmethods.\n", "link": "http://arxiv.org/abs/2502.13078v2", "date": "2025-04-25", "relevancy": 2.5289, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6378}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6378}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L4P%3A%20Low-Level%204D%20Vision%20Perception%20Unified&body=Title%3A%20L4P%3A%20Low-Level%204D%20Vision%20Perception%20Unified%0AAuthor%3A%20Abhishek%20Badki%20and%20Hang%20Su%20and%20Bowen%20Wen%20and%20Orazio%20Gallo%0AAbstract%3A%20%20%20The%20spatio-temporal%20relationship%20between%20the%20pixels%20of%20a%20video%20carries%0Acritical%20information%20for%20low-level%204D%20perception%20tasks.%20A%20single%20model%20that%0Areasons%20about%20it%20should%20be%20able%20to%20solve%20several%20such%20tasks%20well.%20Yet%2C%20most%0Astate-of-the-art%20methods%20rely%20on%20architectures%20specialized%20for%20the%20task%20at%0Ahand.%20We%20present%20L4P%2C%20a%20feedforward%2C%20general-purpose%20architecture%20that%20solves%0Alow-level%204D%20perception%20tasks%20in%20a%20unified%20framework.%20L4P%20leverages%20a%0Apre-trained%20ViT-based%20video%20encoder%20and%20combines%20it%20with%20per-task%20heads%20that%0Aare%20lightweight%20and%20therefore%20do%20not%20require%20extensive%20training.%20Despite%20its%0Ageneral%20and%20feedforward%20formulation%2C%20our%20method%20matches%20or%20surpasses%20the%0Aperformance%20of%20existing%20specialized%20methods%20on%20both%20dense%20tasks%2C%20such%20as%20depth%0Aor%20optical%20flow%20estimation%2C%20and%20sparse%20tasks%2C%20such%20as%202D/3D%20tracking.%20Moreover%2C%0Ait%20solves%20all%20tasks%20at%20once%20in%20a%20time%20comparable%20to%20that%20of%20single-task%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL4P%253A%2520Low-Level%25204D%2520Vision%2520Perception%2520Unified%26entry.906535625%3DAbhishek%2520Badki%2520and%2520Hang%2520Su%2520and%2520Bowen%2520Wen%2520and%2520Orazio%2520Gallo%26entry.1292438233%3D%2520%2520The%2520spatio-temporal%2520relationship%2520between%2520the%2520pixels%2520of%2520a%2520video%2520carries%250Acritical%2520information%2520for%2520low-level%25204D%2520perception%2520tasks.%2520A%2520single%2520model%2520that%250Areasons%2520about%2520it%2520should%2520be%2520able%2520to%2520solve%2520several%2520such%2520tasks%2520well.%2520Yet%252C%2520most%250Astate-of-the-art%2520methods%2520rely%2520on%2520architectures%2520specialized%2520for%2520the%2520task%2520at%250Ahand.%2520We%2520present%2520L4P%252C%2520a%2520feedforward%252C%2520general-purpose%2520architecture%2520that%2520solves%250Alow-level%25204D%2520perception%2520tasks%2520in%2520a%2520unified%2520framework.%2520L4P%2520leverages%2520a%250Apre-trained%2520ViT-based%2520video%2520encoder%2520and%2520combines%2520it%2520with%2520per-task%2520heads%2520that%250Aare%2520lightweight%2520and%2520therefore%2520do%2520not%2520require%2520extensive%2520training.%2520Despite%2520its%250Ageneral%2520and%2520feedforward%2520formulation%252C%2520our%2520method%2520matches%2520or%2520surpasses%2520the%250Aperformance%2520of%2520existing%2520specialized%2520methods%2520on%2520both%2520dense%2520tasks%252C%2520such%2520as%2520depth%250Aor%2520optical%2520flow%2520estimation%252C%2520and%2520sparse%2520tasks%252C%2520such%2520as%25202D/3D%2520tracking.%2520Moreover%252C%250Ait%2520solves%2520all%2520tasks%2520at%2520once%2520in%2520a%2520time%2520comparable%2520to%2520that%2520of%2520single-task%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L4P%3A%20Low-Level%204D%20Vision%20Perception%20Unified&entry.906535625=Abhishek%20Badki%20and%20Hang%20Su%20and%20Bowen%20Wen%20and%20Orazio%20Gallo&entry.1292438233=%20%20The%20spatio-temporal%20relationship%20between%20the%20pixels%20of%20a%20video%20carries%0Acritical%20information%20for%20low-level%204D%20perception%20tasks.%20A%20single%20model%20that%0Areasons%20about%20it%20should%20be%20able%20to%20solve%20several%20such%20tasks%20well.%20Yet%2C%20most%0Astate-of-the-art%20methods%20rely%20on%20architectures%20specialized%20for%20the%20task%20at%0Ahand.%20We%20present%20L4P%2C%20a%20feedforward%2C%20general-purpose%20architecture%20that%20solves%0Alow-level%204D%20perception%20tasks%20in%20a%20unified%20framework.%20L4P%20leverages%20a%0Apre-trained%20ViT-based%20video%20encoder%20and%20combines%20it%20with%20per-task%20heads%20that%0Aare%20lightweight%20and%20therefore%20do%20not%20require%20extensive%20training.%20Despite%20its%0Ageneral%20and%20feedforward%20formulation%2C%20our%20method%20matches%20or%20surpasses%20the%0Aperformance%20of%20existing%20specialized%20methods%20on%20both%20dense%20tasks%2C%20such%20as%20depth%0Aor%20optical%20flow%20estimation%2C%20and%20sparse%20tasks%2C%20such%20as%202D/3D%20tracking.%20Moreover%2C%0Ait%20solves%20all%20tasks%20at%20once%20in%20a%20time%20comparable%20to%20that%20of%20single-task%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13078v2&entry.124074799=Read"},
{"title": "Outlier-aware Tensor Robust Principal Component Analysis with\n  Self-guided Data Augmentation", "author": "Yangyang Xu and Kexin Li and Li Yang and You-Wei Wen", "abstract": "  Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique\nfor decomposing multi-dimensional data into a low-rank tensor and an outlier\ntensor, yet existing methods relying on sparse outlier assumptions often fail\nunder structured corruptions. In this paper, we propose a self-guided data\naugmentation approach that employs adaptive weighting to suppress outlier\ninfluence, reformulating the original TRPCA problem into a standard Tensor\nPrincipal Component Analysis (TPCA) problem. The proposed model involves an\noptimization-driven weighting scheme that dynamically identifies and\ndownweights outlier contributions during tensor augmentation. We develop an\nefficient proximal block coordinate descent algorithm with closed-form updates\nto solve the resulting optimization problem, ensuring computational efficiency.\nTheoretical convergence is guaranteed through a framework combining block\ncoordinate descent with majorization-minimization principles. Numerical\nexperiments on synthetic and real-world datasets, including face recovery,\nbackground subtraction, and hyperspectral denoising, demonstrate that our\nmethod effectively handles various corruption patterns. The results show the\nimprovements in both accuracy and computational efficiency compared to\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2504.18323v1", "date": "2025-04-25", "relevancy": 2.5289, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5233}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5072}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Outlier-aware%20Tensor%20Robust%20Principal%20Component%20Analysis%20with%0A%20%20Self-guided%20Data%20Augmentation&body=Title%3A%20Outlier-aware%20Tensor%20Robust%20Principal%20Component%20Analysis%20with%0A%20%20Self-guided%20Data%20Augmentation%0AAuthor%3A%20Yangyang%20Xu%20and%20Kexin%20Li%20and%20Li%20Yang%20and%20You-Wei%20Wen%0AAbstract%3A%20%20%20Tensor%20Robust%20Principal%20Component%20Analysis%20%28TRPCA%29%20is%20a%20fundamental%20technique%0Afor%20decomposing%20multi-dimensional%20data%20into%20a%20low-rank%20tensor%20and%20an%20outlier%0Atensor%2C%20yet%20existing%20methods%20relying%20on%20sparse%20outlier%20assumptions%20often%20fail%0Aunder%20structured%20corruptions.%20In%20this%20paper%2C%20we%20propose%20a%20self-guided%20data%0Aaugmentation%20approach%20that%20employs%20adaptive%20weighting%20to%20suppress%20outlier%0Ainfluence%2C%20reformulating%20the%20original%20TRPCA%20problem%20into%20a%20standard%20Tensor%0APrincipal%20Component%20Analysis%20%28TPCA%29%20problem.%20The%20proposed%20model%20involves%20an%0Aoptimization-driven%20weighting%20scheme%20that%20dynamically%20identifies%20and%0Adownweights%20outlier%20contributions%20during%20tensor%20augmentation.%20We%20develop%20an%0Aefficient%20proximal%20block%20coordinate%20descent%20algorithm%20with%20closed-form%20updates%0Ato%20solve%20the%20resulting%20optimization%20problem%2C%20ensuring%20computational%20efficiency.%0ATheoretical%20convergence%20is%20guaranteed%20through%20a%20framework%20combining%20block%0Acoordinate%20descent%20with%20majorization-minimization%20principles.%20Numerical%0Aexperiments%20on%20synthetic%20and%20real-world%20datasets%2C%20including%20face%20recovery%2C%0Abackground%20subtraction%2C%20and%20hyperspectral%20denoising%2C%20demonstrate%20that%20our%0Amethod%20effectively%20handles%20various%20corruption%20patterns.%20The%20results%20show%20the%0Aimprovements%20in%20both%20accuracy%20and%20computational%20efficiency%20compared%20to%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOutlier-aware%2520Tensor%2520Robust%2520Principal%2520Component%2520Analysis%2520with%250A%2520%2520Self-guided%2520Data%2520Augmentation%26entry.906535625%3DYangyang%2520Xu%2520and%2520Kexin%2520Li%2520and%2520Li%2520Yang%2520and%2520You-Wei%2520Wen%26entry.1292438233%3D%2520%2520Tensor%2520Robust%2520Principal%2520Component%2520Analysis%2520%2528TRPCA%2529%2520is%2520a%2520fundamental%2520technique%250Afor%2520decomposing%2520multi-dimensional%2520data%2520into%2520a%2520low-rank%2520tensor%2520and%2520an%2520outlier%250Atensor%252C%2520yet%2520existing%2520methods%2520relying%2520on%2520sparse%2520outlier%2520assumptions%2520often%2520fail%250Aunder%2520structured%2520corruptions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520self-guided%2520data%250Aaugmentation%2520approach%2520that%2520employs%2520adaptive%2520weighting%2520to%2520suppress%2520outlier%250Ainfluence%252C%2520reformulating%2520the%2520original%2520TRPCA%2520problem%2520into%2520a%2520standard%2520Tensor%250APrincipal%2520Component%2520Analysis%2520%2528TPCA%2529%2520problem.%2520The%2520proposed%2520model%2520involves%2520an%250Aoptimization-driven%2520weighting%2520scheme%2520that%2520dynamically%2520identifies%2520and%250Adownweights%2520outlier%2520contributions%2520during%2520tensor%2520augmentation.%2520We%2520develop%2520an%250Aefficient%2520proximal%2520block%2520coordinate%2520descent%2520algorithm%2520with%2520closed-form%2520updates%250Ato%2520solve%2520the%2520resulting%2520optimization%2520problem%252C%2520ensuring%2520computational%2520efficiency.%250ATheoretical%2520convergence%2520is%2520guaranteed%2520through%2520a%2520framework%2520combining%2520block%250Acoordinate%2520descent%2520with%2520majorization-minimization%2520principles.%2520Numerical%250Aexperiments%2520on%2520synthetic%2520and%2520real-world%2520datasets%252C%2520including%2520face%2520recovery%252C%250Abackground%2520subtraction%252C%2520and%2520hyperspectral%2520denoising%252C%2520demonstrate%2520that%2520our%250Amethod%2520effectively%2520handles%2520various%2520corruption%2520patterns.%2520The%2520results%2520show%2520the%250Aimprovements%2520in%2520both%2520accuracy%2520and%2520computational%2520efficiency%2520compared%2520to%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Outlier-aware%20Tensor%20Robust%20Principal%20Component%20Analysis%20with%0A%20%20Self-guided%20Data%20Augmentation&entry.906535625=Yangyang%20Xu%20and%20Kexin%20Li%20and%20Li%20Yang%20and%20You-Wei%20Wen&entry.1292438233=%20%20Tensor%20Robust%20Principal%20Component%20Analysis%20%28TRPCA%29%20is%20a%20fundamental%20technique%0Afor%20decomposing%20multi-dimensional%20data%20into%20a%20low-rank%20tensor%20and%20an%20outlier%0Atensor%2C%20yet%20existing%20methods%20relying%20on%20sparse%20outlier%20assumptions%20often%20fail%0Aunder%20structured%20corruptions.%20In%20this%20paper%2C%20we%20propose%20a%20self-guided%20data%0Aaugmentation%20approach%20that%20employs%20adaptive%20weighting%20to%20suppress%20outlier%0Ainfluence%2C%20reformulating%20the%20original%20TRPCA%20problem%20into%20a%20standard%20Tensor%0APrincipal%20Component%20Analysis%20%28TPCA%29%20problem.%20The%20proposed%20model%20involves%20an%0Aoptimization-driven%20weighting%20scheme%20that%20dynamically%20identifies%20and%0Adownweights%20outlier%20contributions%20during%20tensor%20augmentation.%20We%20develop%20an%0Aefficient%20proximal%20block%20coordinate%20descent%20algorithm%20with%20closed-form%20updates%0Ato%20solve%20the%20resulting%20optimization%20problem%2C%20ensuring%20computational%20efficiency.%0ATheoretical%20convergence%20is%20guaranteed%20through%20a%20framework%20combining%20block%0Acoordinate%20descent%20with%20majorization-minimization%20principles.%20Numerical%0Aexperiments%20on%20synthetic%20and%20real-world%20datasets%2C%20including%20face%20recovery%2C%0Abackground%20subtraction%2C%20and%20hyperspectral%20denoising%2C%20demonstrate%20that%20our%0Amethod%20effectively%20handles%20various%20corruption%20patterns.%20The%20results%20show%20the%0Aimprovements%20in%20both%20accuracy%20and%20computational%20efficiency%20compared%20to%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18323v1&entry.124074799=Read"},
{"title": "MeTHanol: Modularized Thinking Language Models with Intermediate Layer\n  Thinking, Decoding and Bootstrapping Reasoning", "author": "Ningyuan Xi and Xiaoyu Wang and Yetao Wu and Teng Chen and Qingqing Gu and Yue Zhao and Jinxian Qu and Zhonglin Jiang and Yong Chen and Luo Ji", "abstract": "  Large Language Model can reasonably understand and generate human expressions\nbut may lack of thorough thinking and reasoning mechanisms. Recently there have\nbeen several studies which enhance the thinking ability of language models but\nmost of them are not data-driven or training-based. In this paper, we are\nmotivated by the cognitive mechanism in the natural world, and design a novel\nmodel architecture called TaS which allows it to first consider the thoughts\nand then express the response based upon the query. We design several pipelines\nto annotate or generate the thought contents from prompt-response samples, then\nadd language heads in a middle layer which behaves as the thinking layer. We\ntrain the language model by the thoughts-augmented data and successfully let\nthe thinking layer automatically generate reasonable thoughts and finally\noutput more reasonable responses. Both qualitative examples and quantitative\nresults validate the effectiveness and performance of TaS. Our code is\navailable at https://anonymous.4open.science/r/TadE.\n", "link": "http://arxiv.org/abs/2409.12059v4", "date": "2025-04-25", "relevancy": 2.4952, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeTHanol%3A%20Modularized%20Thinking%20Language%20Models%20with%20Intermediate%20Layer%0A%20%20Thinking%2C%20Decoding%20and%20Bootstrapping%20Reasoning&body=Title%3A%20MeTHanol%3A%20Modularized%20Thinking%20Language%20Models%20with%20Intermediate%20Layer%0A%20%20Thinking%2C%20Decoding%20and%20Bootstrapping%20Reasoning%0AAuthor%3A%20Ningyuan%20Xi%20and%20Xiaoyu%20Wang%20and%20Yetao%20Wu%20and%20Teng%20Chen%20and%20Qingqing%20Gu%20and%20Yue%20Zhao%20and%20Jinxian%20Qu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji%0AAbstract%3A%20%20%20Large%20Language%20Model%20can%20reasonably%20understand%20and%20generate%20human%20expressions%0Abut%20may%20lack%20of%20thorough%20thinking%20and%20reasoning%20mechanisms.%20Recently%20there%20have%0Abeen%20several%20studies%20which%20enhance%20the%20thinking%20ability%20of%20language%20models%20but%0Amost%20of%20them%20are%20not%20data-driven%20or%20training-based.%20In%20this%20paper%2C%20we%20are%0Amotivated%20by%20the%20cognitive%20mechanism%20in%20the%20natural%20world%2C%20and%20design%20a%20novel%0Amodel%20architecture%20called%20TaS%20which%20allows%20it%20to%20first%20consider%20the%20thoughts%0Aand%20then%20express%20the%20response%20based%20upon%20the%20query.%20We%20design%20several%20pipelines%0Ato%20annotate%20or%20generate%20the%20thought%20contents%20from%20prompt-response%20samples%2C%20then%0Aadd%20language%20heads%20in%20a%20middle%20layer%20which%20behaves%20as%20the%20thinking%20layer.%20We%0Atrain%20the%20language%20model%20by%20the%20thoughts-augmented%20data%20and%20successfully%20let%0Athe%20thinking%20layer%20automatically%20generate%20reasonable%20thoughts%20and%20finally%0Aoutput%20more%20reasonable%20responses.%20Both%20qualitative%20examples%20and%20quantitative%0Aresults%20validate%20the%20effectiveness%20and%20performance%20of%20TaS.%20Our%20code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/TadE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12059v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeTHanol%253A%2520Modularized%2520Thinking%2520Language%2520Models%2520with%2520Intermediate%2520Layer%250A%2520%2520Thinking%252C%2520Decoding%2520and%2520Bootstrapping%2520Reasoning%26entry.906535625%3DNingyuan%2520Xi%2520and%2520Xiaoyu%2520Wang%2520and%2520Yetao%2520Wu%2520and%2520Teng%2520Chen%2520and%2520Qingqing%2520Gu%2520and%2520Yue%2520Zhao%2520and%2520Jinxian%2520Qu%2520and%2520Zhonglin%2520Jiang%2520and%2520Yong%2520Chen%2520and%2520Luo%2520Ji%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520can%2520reasonably%2520understand%2520and%2520generate%2520human%2520expressions%250Abut%2520may%2520lack%2520of%2520thorough%2520thinking%2520and%2520reasoning%2520mechanisms.%2520Recently%2520there%2520have%250Abeen%2520several%2520studies%2520which%2520enhance%2520the%2520thinking%2520ability%2520of%2520language%2520models%2520but%250Amost%2520of%2520them%2520are%2520not%2520data-driven%2520or%2520training-based.%2520In%2520this%2520paper%252C%2520we%2520are%250Amotivated%2520by%2520the%2520cognitive%2520mechanism%2520in%2520the%2520natural%2520world%252C%2520and%2520design%2520a%2520novel%250Amodel%2520architecture%2520called%2520TaS%2520which%2520allows%2520it%2520to%2520first%2520consider%2520the%2520thoughts%250Aand%2520then%2520express%2520the%2520response%2520based%2520upon%2520the%2520query.%2520We%2520design%2520several%2520pipelines%250Ato%2520annotate%2520or%2520generate%2520the%2520thought%2520contents%2520from%2520prompt-response%2520samples%252C%2520then%250Aadd%2520language%2520heads%2520in%2520a%2520middle%2520layer%2520which%2520behaves%2520as%2520the%2520thinking%2520layer.%2520We%250Atrain%2520the%2520language%2520model%2520by%2520the%2520thoughts-augmented%2520data%2520and%2520successfully%2520let%250Athe%2520thinking%2520layer%2520automatically%2520generate%2520reasonable%2520thoughts%2520and%2520finally%250Aoutput%2520more%2520reasonable%2520responses.%2520Both%2520qualitative%2520examples%2520and%2520quantitative%250Aresults%2520validate%2520the%2520effectiveness%2520and%2520performance%2520of%2520TaS.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//anonymous.4open.science/r/TadE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12059v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeTHanol%3A%20Modularized%20Thinking%20Language%20Models%20with%20Intermediate%20Layer%0A%20%20Thinking%2C%20Decoding%20and%20Bootstrapping%20Reasoning&entry.906535625=Ningyuan%20Xi%20and%20Xiaoyu%20Wang%20and%20Yetao%20Wu%20and%20Teng%20Chen%20and%20Qingqing%20Gu%20and%20Yue%20Zhao%20and%20Jinxian%20Qu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji&entry.1292438233=%20%20Large%20Language%20Model%20can%20reasonably%20understand%20and%20generate%20human%20expressions%0Abut%20may%20lack%20of%20thorough%20thinking%20and%20reasoning%20mechanisms.%20Recently%20there%20have%0Abeen%20several%20studies%20which%20enhance%20the%20thinking%20ability%20of%20language%20models%20but%0Amost%20of%20them%20are%20not%20data-driven%20or%20training-based.%20In%20this%20paper%2C%20we%20are%0Amotivated%20by%20the%20cognitive%20mechanism%20in%20the%20natural%20world%2C%20and%20design%20a%20novel%0Amodel%20architecture%20called%20TaS%20which%20allows%20it%20to%20first%20consider%20the%20thoughts%0Aand%20then%20express%20the%20response%20based%20upon%20the%20query.%20We%20design%20several%20pipelines%0Ato%20annotate%20or%20generate%20the%20thought%20contents%20from%20prompt-response%20samples%2C%20then%0Aadd%20language%20heads%20in%20a%20middle%20layer%20which%20behaves%20as%20the%20thinking%20layer.%20We%0Atrain%20the%20language%20model%20by%20the%20thoughts-augmented%20data%20and%20successfully%20let%0Athe%20thinking%20layer%20automatically%20generate%20reasonable%20thoughts%20and%20finally%0Aoutput%20more%20reasonable%20responses.%20Both%20qualitative%20examples%20and%20quantitative%0Aresults%20validate%20the%20effectiveness%20and%20performance%20of%20TaS.%20Our%20code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/TadE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12059v4&entry.124074799=Read"},
{"title": "Paradigm shift on Coding Productivity Using GenAI", "author": "Liang Yu", "abstract": "  Generative AI (GenAI) applications are transforming software engineering by\nenabling automated code co-creation. However, empirical evidence on GenAI's\nproductivity effects in industrial settings remains limited. This paper\ninvestigates the adoption of GenAI coding assistants (e.g., Codeium, Amazon Q)\nwithin telecommunications and FinTech domains. Through surveys and interviews\nwith industrial domain-experts, we identify primary productivity-influencing\nfactors, including task complexity, coding skills, domain knowledge, and GenAI\nintegration. Our findings indicate that GenAI tools enhance productivity in\nroutine coding tasks (e.g., refactoring and Javadoc generation) but face\nchallenges in complex, domain-specific activities due to limited\ncontext-awareness of codebases and insufficient support for customized design\nrules. We highlight new paradigms for coding transfer, emphasizing iterative\nprompt refinement, immersive development environment, and automated code\nevaluation as essential for effective GenAI usage.\n", "link": "http://arxiv.org/abs/2504.18404v1", "date": "2025-04-25", "relevancy": 2.4904, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5272}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5233}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Paradigm%20shift%20on%20Coding%20Productivity%20Using%20GenAI&body=Title%3A%20Paradigm%20shift%20on%20Coding%20Productivity%20Using%20GenAI%0AAuthor%3A%20Liang%20Yu%0AAbstract%3A%20%20%20Generative%20AI%20%28GenAI%29%20applications%20are%20transforming%20software%20engineering%20by%0Aenabling%20automated%20code%20co-creation.%20However%2C%20empirical%20evidence%20on%20GenAI%27s%0Aproductivity%20effects%20in%20industrial%20settings%20remains%20limited.%20This%20paper%0Ainvestigates%20the%20adoption%20of%20GenAI%20coding%20assistants%20%28e.g.%2C%20Codeium%2C%20Amazon%20Q%29%0Awithin%20telecommunications%20and%20FinTech%20domains.%20Through%20surveys%20and%20interviews%0Awith%20industrial%20domain-experts%2C%20we%20identify%20primary%20productivity-influencing%0Afactors%2C%20including%20task%20complexity%2C%20coding%20skills%2C%20domain%20knowledge%2C%20and%20GenAI%0Aintegration.%20Our%20findings%20indicate%20that%20GenAI%20tools%20enhance%20productivity%20in%0Aroutine%20coding%20tasks%20%28e.g.%2C%20refactoring%20and%20Javadoc%20generation%29%20but%20face%0Achallenges%20in%20complex%2C%20domain-specific%20activities%20due%20to%20limited%0Acontext-awareness%20of%20codebases%20and%20insufficient%20support%20for%20customized%20design%0Arules.%20We%20highlight%20new%20paradigms%20for%20coding%20transfer%2C%20emphasizing%20iterative%0Aprompt%20refinement%2C%20immersive%20development%20environment%2C%20and%20automated%20code%0Aevaluation%20as%20essential%20for%20effective%20GenAI%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParadigm%2520shift%2520on%2520Coding%2520Productivity%2520Using%2520GenAI%26entry.906535625%3DLiang%2520Yu%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GenAI%2529%2520applications%2520are%2520transforming%2520software%2520engineering%2520by%250Aenabling%2520automated%2520code%2520co-creation.%2520However%252C%2520empirical%2520evidence%2520on%2520GenAI%2527s%250Aproductivity%2520effects%2520in%2520industrial%2520settings%2520remains%2520limited.%2520This%2520paper%250Ainvestigates%2520the%2520adoption%2520of%2520GenAI%2520coding%2520assistants%2520%2528e.g.%252C%2520Codeium%252C%2520Amazon%2520Q%2529%250Awithin%2520telecommunications%2520and%2520FinTech%2520domains.%2520Through%2520surveys%2520and%2520interviews%250Awith%2520industrial%2520domain-experts%252C%2520we%2520identify%2520primary%2520productivity-influencing%250Afactors%252C%2520including%2520task%2520complexity%252C%2520coding%2520skills%252C%2520domain%2520knowledge%252C%2520and%2520GenAI%250Aintegration.%2520Our%2520findings%2520indicate%2520that%2520GenAI%2520tools%2520enhance%2520productivity%2520in%250Aroutine%2520coding%2520tasks%2520%2528e.g.%252C%2520refactoring%2520and%2520Javadoc%2520generation%2529%2520but%2520face%250Achallenges%2520in%2520complex%252C%2520domain-specific%2520activities%2520due%2520to%2520limited%250Acontext-awareness%2520of%2520codebases%2520and%2520insufficient%2520support%2520for%2520customized%2520design%250Arules.%2520We%2520highlight%2520new%2520paradigms%2520for%2520coding%2520transfer%252C%2520emphasizing%2520iterative%250Aprompt%2520refinement%252C%2520immersive%2520development%2520environment%252C%2520and%2520automated%2520code%250Aevaluation%2520as%2520essential%2520for%2520effective%2520GenAI%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Paradigm%20shift%20on%20Coding%20Productivity%20Using%20GenAI&entry.906535625=Liang%20Yu&entry.1292438233=%20%20Generative%20AI%20%28GenAI%29%20applications%20are%20transforming%20software%20engineering%20by%0Aenabling%20automated%20code%20co-creation.%20However%2C%20empirical%20evidence%20on%20GenAI%27s%0Aproductivity%20effects%20in%20industrial%20settings%20remains%20limited.%20This%20paper%0Ainvestigates%20the%20adoption%20of%20GenAI%20coding%20assistants%20%28e.g.%2C%20Codeium%2C%20Amazon%20Q%29%0Awithin%20telecommunications%20and%20FinTech%20domains.%20Through%20surveys%20and%20interviews%0Awith%20industrial%20domain-experts%2C%20we%20identify%20primary%20productivity-influencing%0Afactors%2C%20including%20task%20complexity%2C%20coding%20skills%2C%20domain%20knowledge%2C%20and%20GenAI%0Aintegration.%20Our%20findings%20indicate%20that%20GenAI%20tools%20enhance%20productivity%20in%0Aroutine%20coding%20tasks%20%28e.g.%2C%20refactoring%20and%20Javadoc%20generation%29%20but%20face%0Achallenges%20in%20complex%2C%20domain-specific%20activities%20due%20to%20limited%0Acontext-awareness%20of%20codebases%20and%20insufficient%20support%20for%20customized%20design%0Arules.%20We%20highlight%20new%20paradigms%20for%20coding%20transfer%2C%20emphasizing%20iterative%0Aprompt%20refinement%2C%20immersive%20development%20environment%2C%20and%20automated%20code%0Aevaluation%20as%20essential%20for%20effective%20GenAI%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18404v1&entry.124074799=Read"},
{"title": "Enhancing Pre-Trained Model-Based Class-Incremental Learning through\n  Neural Collapse", "author": "Kun He and Zijian Song and Shuoxi Zhang and John E. Hopcroft", "abstract": "  Class-Incremental Learning (CIL) is a critical capability for real-world\napplications, enabling learning systems to adapt to new tasks while retaining\nknowledge from previous ones. Recent advancements in pre-trained models (PTMs)\nhave significantly advanced the field of CIL, demonstrating superior\nperformance over traditional methods. However, understanding how features\nevolve and are distributed across incremental tasks remains an open challenge.\nIn this paper, we propose a novel approach to modeling feature evolution in\nPTM-based CIL through the lens of neural collapse (NC), a striking phenomenon\nobserved in the final phase of training, which leads to a well-separated,\nequiangular feature space. We explore the connection between NC and CIL\neffectiveness, showing that aligning feature distributions with the NC geometry\nenhances the ability to capture the dynamic behavior of continual learning.\nBased on this insight, we introduce Neural Collapse-inspired Pre-Trained\nModel-based CIL (NCPTM-CIL), a method that dynamically adjusts the feature\nspace to conform to the elegant NC structure, thereby enhancing the continual\nlearning process. Extensive experiments demonstrate that NCPTM-CIL outperforms\nstate-of-the-art methods across four benchmark datasets. Notably, when\ninitialized with ViT-B/16-IN1K, NCPTM-CIL surpasses the runner-up method by\n6.73% on VTAB, 1.25% on CIFAR-100, and 2.5% on OmniBenchmark.\n", "link": "http://arxiv.org/abs/2504.18437v1", "date": "2025-04-25", "relevancy": 2.4827, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5134}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.504}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Pre-Trained%20Model-Based%20Class-Incremental%20Learning%20through%0A%20%20Neural%20Collapse&body=Title%3A%20Enhancing%20Pre-Trained%20Model-Based%20Class-Incremental%20Learning%20through%0A%20%20Neural%20Collapse%0AAuthor%3A%20Kun%20He%20and%20Zijian%20Song%20and%20Shuoxi%20Zhang%20and%20John%20E.%20Hopcroft%0AAbstract%3A%20%20%20Class-Incremental%20Learning%20%28CIL%29%20is%20a%20critical%20capability%20for%20real-world%0Aapplications%2C%20enabling%20learning%20systems%20to%20adapt%20to%20new%20tasks%20while%20retaining%0Aknowledge%20from%20previous%20ones.%20Recent%20advancements%20in%20pre-trained%20models%20%28PTMs%29%0Ahave%20significantly%20advanced%20the%20field%20of%20CIL%2C%20demonstrating%20superior%0Aperformance%20over%20traditional%20methods.%20However%2C%20understanding%20how%20features%0Aevolve%20and%20are%20distributed%20across%20incremental%20tasks%20remains%20an%20open%20challenge.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20modeling%20feature%20evolution%20in%0APTM-based%20CIL%20through%20the%20lens%20of%20neural%20collapse%20%28NC%29%2C%20a%20striking%20phenomenon%0Aobserved%20in%20the%20final%20phase%20of%20training%2C%20which%20leads%20to%20a%20well-separated%2C%0Aequiangular%20feature%20space.%20We%20explore%20the%20connection%20between%20NC%20and%20CIL%0Aeffectiveness%2C%20showing%20that%20aligning%20feature%20distributions%20with%20the%20NC%20geometry%0Aenhances%20the%20ability%20to%20capture%20the%20dynamic%20behavior%20of%20continual%20learning.%0ABased%20on%20this%20insight%2C%20we%20introduce%20Neural%20Collapse-inspired%20Pre-Trained%0AModel-based%20CIL%20%28NCPTM-CIL%29%2C%20a%20method%20that%20dynamically%20adjusts%20the%20feature%0Aspace%20to%20conform%20to%20the%20elegant%20NC%20structure%2C%20thereby%20enhancing%20the%20continual%0Alearning%20process.%20Extensive%20experiments%20demonstrate%20that%20NCPTM-CIL%20outperforms%0Astate-of-the-art%20methods%20across%20four%20benchmark%20datasets.%20Notably%2C%20when%0Ainitialized%20with%20ViT-B/16-IN1K%2C%20NCPTM-CIL%20surpasses%20the%20runner-up%20method%20by%0A6.73%25%20on%20VTAB%2C%201.25%25%20on%20CIFAR-100%2C%20and%202.5%25%20on%20OmniBenchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Pre-Trained%2520Model-Based%2520Class-Incremental%2520Learning%2520through%250A%2520%2520Neural%2520Collapse%26entry.906535625%3DKun%2520He%2520and%2520Zijian%2520Song%2520and%2520Shuoxi%2520Zhang%2520and%2520John%2520E.%2520Hopcroft%26entry.1292438233%3D%2520%2520Class-Incremental%2520Learning%2520%2528CIL%2529%2520is%2520a%2520critical%2520capability%2520for%2520real-world%250Aapplications%252C%2520enabling%2520learning%2520systems%2520to%2520adapt%2520to%2520new%2520tasks%2520while%2520retaining%250Aknowledge%2520from%2520previous%2520ones.%2520Recent%2520advancements%2520in%2520pre-trained%2520models%2520%2528PTMs%2529%250Ahave%2520significantly%2520advanced%2520the%2520field%2520of%2520CIL%252C%2520demonstrating%2520superior%250Aperformance%2520over%2520traditional%2520methods.%2520However%252C%2520understanding%2520how%2520features%250Aevolve%2520and%2520are%2520distributed%2520across%2520incremental%2520tasks%2520remains%2520an%2520open%2520challenge.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520modeling%2520feature%2520evolution%2520in%250APTM-based%2520CIL%2520through%2520the%2520lens%2520of%2520neural%2520collapse%2520%2528NC%2529%252C%2520a%2520striking%2520phenomenon%250Aobserved%2520in%2520the%2520final%2520phase%2520of%2520training%252C%2520which%2520leads%2520to%2520a%2520well-separated%252C%250Aequiangular%2520feature%2520space.%2520We%2520explore%2520the%2520connection%2520between%2520NC%2520and%2520CIL%250Aeffectiveness%252C%2520showing%2520that%2520aligning%2520feature%2520distributions%2520with%2520the%2520NC%2520geometry%250Aenhances%2520the%2520ability%2520to%2520capture%2520the%2520dynamic%2520behavior%2520of%2520continual%2520learning.%250ABased%2520on%2520this%2520insight%252C%2520we%2520introduce%2520Neural%2520Collapse-inspired%2520Pre-Trained%250AModel-based%2520CIL%2520%2528NCPTM-CIL%2529%252C%2520a%2520method%2520that%2520dynamically%2520adjusts%2520the%2520feature%250Aspace%2520to%2520conform%2520to%2520the%2520elegant%2520NC%2520structure%252C%2520thereby%2520enhancing%2520the%2520continual%250Alearning%2520process.%2520Extensive%2520experiments%2520demonstrate%2520that%2520NCPTM-CIL%2520outperforms%250Astate-of-the-art%2520methods%2520across%2520four%2520benchmark%2520datasets.%2520Notably%252C%2520when%250Ainitialized%2520with%2520ViT-B/16-IN1K%252C%2520NCPTM-CIL%2520surpasses%2520the%2520runner-up%2520method%2520by%250A6.73%2525%2520on%2520VTAB%252C%25201.25%2525%2520on%2520CIFAR-100%252C%2520and%25202.5%2525%2520on%2520OmniBenchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Pre-Trained%20Model-Based%20Class-Incremental%20Learning%20through%0A%20%20Neural%20Collapse&entry.906535625=Kun%20He%20and%20Zijian%20Song%20and%20Shuoxi%20Zhang%20and%20John%20E.%20Hopcroft&entry.1292438233=%20%20Class-Incremental%20Learning%20%28CIL%29%20is%20a%20critical%20capability%20for%20real-world%0Aapplications%2C%20enabling%20learning%20systems%20to%20adapt%20to%20new%20tasks%20while%20retaining%0Aknowledge%20from%20previous%20ones.%20Recent%20advancements%20in%20pre-trained%20models%20%28PTMs%29%0Ahave%20significantly%20advanced%20the%20field%20of%20CIL%2C%20demonstrating%20superior%0Aperformance%20over%20traditional%20methods.%20However%2C%20understanding%20how%20features%0Aevolve%20and%20are%20distributed%20across%20incremental%20tasks%20remains%20an%20open%20challenge.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20modeling%20feature%20evolution%20in%0APTM-based%20CIL%20through%20the%20lens%20of%20neural%20collapse%20%28NC%29%2C%20a%20striking%20phenomenon%0Aobserved%20in%20the%20final%20phase%20of%20training%2C%20which%20leads%20to%20a%20well-separated%2C%0Aequiangular%20feature%20space.%20We%20explore%20the%20connection%20between%20NC%20and%20CIL%0Aeffectiveness%2C%20showing%20that%20aligning%20feature%20distributions%20with%20the%20NC%20geometry%0Aenhances%20the%20ability%20to%20capture%20the%20dynamic%20behavior%20of%20continual%20learning.%0ABased%20on%20this%20insight%2C%20we%20introduce%20Neural%20Collapse-inspired%20Pre-Trained%0AModel-based%20CIL%20%28NCPTM-CIL%29%2C%20a%20method%20that%20dynamically%20adjusts%20the%20feature%0Aspace%20to%20conform%20to%20the%20elegant%20NC%20structure%2C%20thereby%20enhancing%20the%20continual%0Alearning%20process.%20Extensive%20experiments%20demonstrate%20that%20NCPTM-CIL%20outperforms%0Astate-of-the-art%20methods%20across%20four%20benchmark%20datasets.%20Notably%2C%20when%0Ainitialized%20with%20ViT-B/16-IN1K%2C%20NCPTM-CIL%20surpasses%20the%20runner-up%20method%20by%0A6.73%25%20on%20VTAB%2C%201.25%25%20on%20CIFAR-100%2C%20and%202.5%25%20on%20OmniBenchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18437v1&entry.124074799=Read"},
{"title": "Efficient Learning on Large Graphs using a Densifying Regularity Lemma", "author": "Jonathan Kouchly and Ben Finkelshtein and Michael Bronstein and Ron Levie", "abstract": "  Learning on large graphs presents significant challenges, with traditional\nMessage Passing Neural Networks suffering from computational and memory costs\nscaling linearly with the number of edges. We introduce the Intersecting Block\nGraph (IBG), a low-rank factorization of large directed graphs based on\ncombinations of intersecting bipartite components, each consisting of a pair of\ncommunities, for source and target nodes. By giving less weight to non-edges,\nwe show how to efficiently approximate any graph, sparse or dense, by a dense\nIBG. Specifically, we prove a constructive version of the weak regularity\nlemma, showing that for any chosen accuracy, every graph, regardless of its\nsize or sparsity, can be approximated by a dense IBG whose rank depends only on\nthe accuracy. This dependence of the rank solely on the accuracy, and not on\nthe sparsity level, is in contrast to previous forms of the weak regularity\nlemma. We present a graph neural network architecture operating on the IBG\nrepresentation of the graph and demonstrating competitive performance on node\nclassification, spatio-temporal graph analysis, and knowledge graph completion,\nwhile having memory and computational complexity linear in the number of nodes\nrather than edges.\n", "link": "http://arxiv.org/abs/2504.18273v1", "date": "2025-04-25", "relevancy": 2.4438, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5031}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4823}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Learning%20on%20Large%20Graphs%20using%20a%20Densifying%20Regularity%20Lemma&body=Title%3A%20Efficient%20Learning%20on%20Large%20Graphs%20using%20a%20Densifying%20Regularity%20Lemma%0AAuthor%3A%20Jonathan%20Kouchly%20and%20Ben%20Finkelshtein%20and%20Michael%20Bronstein%20and%20Ron%20Levie%0AAbstract%3A%20%20%20Learning%20on%20large%20graphs%20presents%20significant%20challenges%2C%20with%20traditional%0AMessage%20Passing%20Neural%20Networks%20suffering%20from%20computational%20and%20memory%20costs%0Ascaling%20linearly%20with%20the%20number%20of%20edges.%20We%20introduce%20the%20Intersecting%20Block%0AGraph%20%28IBG%29%2C%20a%20low-rank%20factorization%20of%20large%20directed%20graphs%20based%20on%0Acombinations%20of%20intersecting%20bipartite%20components%2C%20each%20consisting%20of%20a%20pair%20of%0Acommunities%2C%20for%20source%20and%20target%20nodes.%20By%20giving%20less%20weight%20to%20non-edges%2C%0Awe%20show%20how%20to%20efficiently%20approximate%20any%20graph%2C%20sparse%20or%20dense%2C%20by%20a%20dense%0AIBG.%20Specifically%2C%20we%20prove%20a%20constructive%20version%20of%20the%20weak%20regularity%0Alemma%2C%20showing%20that%20for%20any%20chosen%20accuracy%2C%20every%20graph%2C%20regardless%20of%20its%0Asize%20or%20sparsity%2C%20can%20be%20approximated%20by%20a%20dense%20IBG%20whose%20rank%20depends%20only%20on%0Athe%20accuracy.%20This%20dependence%20of%20the%20rank%20solely%20on%20the%20accuracy%2C%20and%20not%20on%0Athe%20sparsity%20level%2C%20is%20in%20contrast%20to%20previous%20forms%20of%20the%20weak%20regularity%0Alemma.%20We%20present%20a%20graph%20neural%20network%20architecture%20operating%20on%20the%20IBG%0Arepresentation%20of%20the%20graph%20and%20demonstrating%20competitive%20performance%20on%20node%0Aclassification%2C%20spatio-temporal%20graph%20analysis%2C%20and%20knowledge%20graph%20completion%2C%0Awhile%20having%20memory%20and%20computational%20complexity%20linear%20in%20the%20number%20of%20nodes%0Arather%20than%20edges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Learning%2520on%2520Large%2520Graphs%2520using%2520a%2520Densifying%2520Regularity%2520Lemma%26entry.906535625%3DJonathan%2520Kouchly%2520and%2520Ben%2520Finkelshtein%2520and%2520Michael%2520Bronstein%2520and%2520Ron%2520Levie%26entry.1292438233%3D%2520%2520Learning%2520on%2520large%2520graphs%2520presents%2520significant%2520challenges%252C%2520with%2520traditional%250AMessage%2520Passing%2520Neural%2520Networks%2520suffering%2520from%2520computational%2520and%2520memory%2520costs%250Ascaling%2520linearly%2520with%2520the%2520number%2520of%2520edges.%2520We%2520introduce%2520the%2520Intersecting%2520Block%250AGraph%2520%2528IBG%2529%252C%2520a%2520low-rank%2520factorization%2520of%2520large%2520directed%2520graphs%2520based%2520on%250Acombinations%2520of%2520intersecting%2520bipartite%2520components%252C%2520each%2520consisting%2520of%2520a%2520pair%2520of%250Acommunities%252C%2520for%2520source%2520and%2520target%2520nodes.%2520By%2520giving%2520less%2520weight%2520to%2520non-edges%252C%250Awe%2520show%2520how%2520to%2520efficiently%2520approximate%2520any%2520graph%252C%2520sparse%2520or%2520dense%252C%2520by%2520a%2520dense%250AIBG.%2520Specifically%252C%2520we%2520prove%2520a%2520constructive%2520version%2520of%2520the%2520weak%2520regularity%250Alemma%252C%2520showing%2520that%2520for%2520any%2520chosen%2520accuracy%252C%2520every%2520graph%252C%2520regardless%2520of%2520its%250Asize%2520or%2520sparsity%252C%2520can%2520be%2520approximated%2520by%2520a%2520dense%2520IBG%2520whose%2520rank%2520depends%2520only%2520on%250Athe%2520accuracy.%2520This%2520dependence%2520of%2520the%2520rank%2520solely%2520on%2520the%2520accuracy%252C%2520and%2520not%2520on%250Athe%2520sparsity%2520level%252C%2520is%2520in%2520contrast%2520to%2520previous%2520forms%2520of%2520the%2520weak%2520regularity%250Alemma.%2520We%2520present%2520a%2520graph%2520neural%2520network%2520architecture%2520operating%2520on%2520the%2520IBG%250Arepresentation%2520of%2520the%2520graph%2520and%2520demonstrating%2520competitive%2520performance%2520on%2520node%250Aclassification%252C%2520spatio-temporal%2520graph%2520analysis%252C%2520and%2520knowledge%2520graph%2520completion%252C%250Awhile%2520having%2520memory%2520and%2520computational%2520complexity%2520linear%2520in%2520the%2520number%2520of%2520nodes%250Arather%2520than%2520edges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Learning%20on%20Large%20Graphs%20using%20a%20Densifying%20Regularity%20Lemma&entry.906535625=Jonathan%20Kouchly%20and%20Ben%20Finkelshtein%20and%20Michael%20Bronstein%20and%20Ron%20Levie&entry.1292438233=%20%20Learning%20on%20large%20graphs%20presents%20significant%20challenges%2C%20with%20traditional%0AMessage%20Passing%20Neural%20Networks%20suffering%20from%20computational%20and%20memory%20costs%0Ascaling%20linearly%20with%20the%20number%20of%20edges.%20We%20introduce%20the%20Intersecting%20Block%0AGraph%20%28IBG%29%2C%20a%20low-rank%20factorization%20of%20large%20directed%20graphs%20based%20on%0Acombinations%20of%20intersecting%20bipartite%20components%2C%20each%20consisting%20of%20a%20pair%20of%0Acommunities%2C%20for%20source%20and%20target%20nodes.%20By%20giving%20less%20weight%20to%20non-edges%2C%0Awe%20show%20how%20to%20efficiently%20approximate%20any%20graph%2C%20sparse%20or%20dense%2C%20by%20a%20dense%0AIBG.%20Specifically%2C%20we%20prove%20a%20constructive%20version%20of%20the%20weak%20regularity%0Alemma%2C%20showing%20that%20for%20any%20chosen%20accuracy%2C%20every%20graph%2C%20regardless%20of%20its%0Asize%20or%20sparsity%2C%20can%20be%20approximated%20by%20a%20dense%20IBG%20whose%20rank%20depends%20only%20on%0Athe%20accuracy.%20This%20dependence%20of%20the%20rank%20solely%20on%20the%20accuracy%2C%20and%20not%20on%0Athe%20sparsity%20level%2C%20is%20in%20contrast%20to%20previous%20forms%20of%20the%20weak%20regularity%0Alemma.%20We%20present%20a%20graph%20neural%20network%20architecture%20operating%20on%20the%20IBG%0Arepresentation%20of%20the%20graph%20and%20demonstrating%20competitive%20performance%20on%20node%0Aclassification%2C%20spatio-temporal%20graph%20analysis%2C%20and%20knowledge%20graph%20completion%2C%0Awhile%20having%20memory%20and%20computational%20complexity%20linear%20in%20the%20number%20of%20nodes%0Arather%20than%20edges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18273v1&entry.124074799=Read"},
{"title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "author": "Muskan Garg and Shaina Raza and Shebuti Rayana and Xingyi Liu and Sunghwan Sohn", "abstract": "  Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github\n", "link": "http://arxiv.org/abs/2504.17119v2", "date": "2025-04-25", "relevancy": 2.3869, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Rise%20of%20Small%20Language%20Models%20in%20Healthcare%3A%20A%20Comprehensive%20Survey&body=Title%3A%20The%20Rise%20of%20Small%20Language%20Models%20in%20Healthcare%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Muskan%20Garg%20and%20Shaina%20Raza%20and%20Shebuti%20Rayana%20and%20Xingyi%20Liu%20and%20Sunghwan%20Sohn%0AAbstract%3A%20%20%20Despite%20substantial%20progress%20in%20healthcare%20applications%20driven%20by%20large%0Alanguage%20models%20%28LLMs%29%2C%20growing%20concerns%20around%20data%20privacy%2C%20and%20limited%0Aresources%3B%20the%20small%20language%20models%20%28SLMs%29%20offer%20a%20scalable%20and%20clinically%0Aviable%20solution%20for%20efficient%20performance%20in%20resource-constrained%20environments%0Afor%20next-generation%20healthcare%20informatics.%20Our%20comprehensive%20survey%20presents%20a%0Ataxonomic%20framework%20to%20identify%20and%20categorize%20them%20for%20healthcare%0Aprofessionals%20and%20informaticians.%20The%20timeline%20of%20healthcare%20SLM%20contributions%0Aestablishes%20a%20foundational%20framework%20for%20analyzing%20models%20across%20three%0Adimensions%3A%20NLP%20tasks%2C%20stakeholder%20roles%2C%20and%20the%20continuum%20of%20care.%20We%20present%0Aa%20taxonomic%20framework%20to%20identify%20the%20architectural%20foundations%20for%20building%0Amodels%20from%20scratch%3B%20adapting%20SLMs%20to%20clinical%20precision%20through%20prompting%2C%0Ainstruction%20fine-tuning%2C%20and%20reasoning%3B%20and%20accessibility%20and%20sustainability%0Athrough%20compression%20techniques.%20Our%20primary%20objective%20is%20to%20offer%20a%0Acomprehensive%20survey%20for%20healthcare%20professionals%2C%20introducing%20recent%0Ainnovations%20in%20model%20optimization%20and%20equipping%20them%20with%20curated%20resources%20to%0Asupport%20future%20research%20and%20development%20in%20the%20field.%20Aiming%20to%20showcase%20the%0Agroundbreaking%20advancements%20in%20SLMs%20for%20healthcare%2C%20we%20present%20a%20comprehensive%0Acompilation%20of%20experimental%20results%20across%20widely%20studied%20NLP%20tasks%20in%0Ahealthcare%20to%20highlight%20the%20transformative%20potential%20of%20SLMs%20in%20healthcare.%20The%0Aupdated%20repository%20is%20available%20at%20Github%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17119v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Rise%2520of%2520Small%2520Language%2520Models%2520in%2520Healthcare%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DMuskan%2520Garg%2520and%2520Shaina%2520Raza%2520and%2520Shebuti%2520Rayana%2520and%2520Xingyi%2520Liu%2520and%2520Sunghwan%2520Sohn%26entry.1292438233%3D%2520%2520Despite%2520substantial%2520progress%2520in%2520healthcare%2520applications%2520driven%2520by%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520growing%2520concerns%2520around%2520data%2520privacy%252C%2520and%2520limited%250Aresources%253B%2520the%2520small%2520language%2520models%2520%2528SLMs%2529%2520offer%2520a%2520scalable%2520and%2520clinically%250Aviable%2520solution%2520for%2520efficient%2520performance%2520in%2520resource-constrained%2520environments%250Afor%2520next-generation%2520healthcare%2520informatics.%2520Our%2520comprehensive%2520survey%2520presents%2520a%250Ataxonomic%2520framework%2520to%2520identify%2520and%2520categorize%2520them%2520for%2520healthcare%250Aprofessionals%2520and%2520informaticians.%2520The%2520timeline%2520of%2520healthcare%2520SLM%2520contributions%250Aestablishes%2520a%2520foundational%2520framework%2520for%2520analyzing%2520models%2520across%2520three%250Adimensions%253A%2520NLP%2520tasks%252C%2520stakeholder%2520roles%252C%2520and%2520the%2520continuum%2520of%2520care.%2520We%2520present%250Aa%2520taxonomic%2520framework%2520to%2520identify%2520the%2520architectural%2520foundations%2520for%2520building%250Amodels%2520from%2520scratch%253B%2520adapting%2520SLMs%2520to%2520clinical%2520precision%2520through%2520prompting%252C%250Ainstruction%2520fine-tuning%252C%2520and%2520reasoning%253B%2520and%2520accessibility%2520and%2520sustainability%250Athrough%2520compression%2520techniques.%2520Our%2520primary%2520objective%2520is%2520to%2520offer%2520a%250Acomprehensive%2520survey%2520for%2520healthcare%2520professionals%252C%2520introducing%2520recent%250Ainnovations%2520in%2520model%2520optimization%2520and%2520equipping%2520them%2520with%2520curated%2520resources%2520to%250Asupport%2520future%2520research%2520and%2520development%2520in%2520the%2520field.%2520Aiming%2520to%2520showcase%2520the%250Agroundbreaking%2520advancements%2520in%2520SLMs%2520for%2520healthcare%252C%2520we%2520present%2520a%2520comprehensive%250Acompilation%2520of%2520experimental%2520results%2520across%2520widely%2520studied%2520NLP%2520tasks%2520in%250Ahealthcare%2520to%2520highlight%2520the%2520transformative%2520potential%2520of%2520SLMs%2520in%2520healthcare.%2520The%250Aupdated%2520repository%2520is%2520available%2520at%2520Github%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17119v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Rise%20of%20Small%20Language%20Models%20in%20Healthcare%3A%20A%20Comprehensive%20Survey&entry.906535625=Muskan%20Garg%20and%20Shaina%20Raza%20and%20Shebuti%20Rayana%20and%20Xingyi%20Liu%20and%20Sunghwan%20Sohn&entry.1292438233=%20%20Despite%20substantial%20progress%20in%20healthcare%20applications%20driven%20by%20large%0Alanguage%20models%20%28LLMs%29%2C%20growing%20concerns%20around%20data%20privacy%2C%20and%20limited%0Aresources%3B%20the%20small%20language%20models%20%28SLMs%29%20offer%20a%20scalable%20and%20clinically%0Aviable%20solution%20for%20efficient%20performance%20in%20resource-constrained%20environments%0Afor%20next-generation%20healthcare%20informatics.%20Our%20comprehensive%20survey%20presents%20a%0Ataxonomic%20framework%20to%20identify%20and%20categorize%20them%20for%20healthcare%0Aprofessionals%20and%20informaticians.%20The%20timeline%20of%20healthcare%20SLM%20contributions%0Aestablishes%20a%20foundational%20framework%20for%20analyzing%20models%20across%20three%0Adimensions%3A%20NLP%20tasks%2C%20stakeholder%20roles%2C%20and%20the%20continuum%20of%20care.%20We%20present%0Aa%20taxonomic%20framework%20to%20identify%20the%20architectural%20foundations%20for%20building%0Amodels%20from%20scratch%3B%20adapting%20SLMs%20to%20clinical%20precision%20through%20prompting%2C%0Ainstruction%20fine-tuning%2C%20and%20reasoning%3B%20and%20accessibility%20and%20sustainability%0Athrough%20compression%20techniques.%20Our%20primary%20objective%20is%20to%20offer%20a%0Acomprehensive%20survey%20for%20healthcare%20professionals%2C%20introducing%20recent%0Ainnovations%20in%20model%20optimization%20and%20equipping%20them%20with%20curated%20resources%20to%0Asupport%20future%20research%20and%20development%20in%20the%20field.%20Aiming%20to%20showcase%20the%0Agroundbreaking%20advancements%20in%20SLMs%20for%20healthcare%2C%20we%20present%20a%20comprehensive%0Acompilation%20of%20experimental%20results%20across%20widely%20studied%20NLP%20tasks%20in%0Ahealthcare%20to%20highlight%20the%20transformative%20potential%20of%20SLMs%20in%20healthcare.%20The%0Aupdated%20repository%20is%20available%20at%20Github%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17119v2&entry.124074799=Read"},
{"title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D\n  Dataset", "author": "Oussema Dhaouadi and Johannes Meier and Luca Wahl and Jacques Kaiser and Luca Scalerandi and Nick Wandelburg and Zhuolun Zhou and Nijanthan Berinpanathan and Holger Banzhaf and Daniel Cremers", "abstract": "  Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet,\ntraditional datasets are usually captured by fixed sensors mounted on a car and\nare susceptible to occlusion. Additionally, such an approach can precisely\nreconstruct the dynamic environment in the close vicinity of the measurement\nvehicle only, while neglecting objects that are further away. In this paper, we\nintroduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality,\nocclusion-free dataset of 6 degrees of freedom bounding box trajectories\nacquired through a novel monocular camera drone tracking pipeline. Our dataset\nincludes more than 175,000 trajectories of 14 types of traffic participants and\nsignificantly exceeds existing datasets in terms of diversity and scale,\ncontaining many unprecedented scenarios such as complex vehicle-pedestrian\ninteraction on highly populated urban streets and comprehensive parking\nmaneuvers from entry to exit. DSC3D dataset was captured in five various\nlocations in Europe and the United States and include: a parking lot, a crowded\ninner-city, a steep urban intersection, a federal highway, and a suburban\nintersection. Our 3D trajectory dataset aims to enhance autonomous driving\nsystems by providing detailed environmental 3D representations, which could\nlead to improved obstacle interactions and safety. We demonstrate its utility\nacross multiple applications including motion prediction, motion planning,\nscenario mining, and generative reactive traffic agents. Our interactive online\nvisualization platform and the complete dataset are publicly available at\nhttps://app.deepscenario.com, facilitating research in motion prediction,\nbehavior modeling, and safety validation.\n", "link": "http://arxiv.org/abs/2504.17371v2", "date": "2025-04-25", "relevancy": 2.3795, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6011}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5936}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Highly%20Accurate%20and%20Diverse%20Traffic%20Data%3A%20The%20DeepScenario%20Open%203D%0A%20%20Dataset&body=Title%3A%20Highly%20Accurate%20and%20Diverse%20Traffic%20Data%3A%20The%20DeepScenario%20Open%203D%0A%20%20Dataset%0AAuthor%3A%20Oussema%20Dhaouadi%20and%20Johannes%20Meier%20and%20Luca%20Wahl%20and%20Jacques%20Kaiser%20and%20Luca%20Scalerandi%20and%20Nick%20Wandelburg%20and%20Zhuolun%20Zhou%20and%20Nijanthan%20Berinpanathan%20and%20Holger%20Banzhaf%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Accurate%203D%20trajectory%20data%20is%20crucial%20for%20advancing%20autonomous%20driving.%20Yet%2C%0Atraditional%20datasets%20are%20usually%20captured%20by%20fixed%20sensors%20mounted%20on%20a%20car%20and%0Aare%20susceptible%20to%20occlusion.%20Additionally%2C%20such%20an%20approach%20can%20precisely%0Areconstruct%20the%20dynamic%20environment%20in%20the%20close%20vicinity%20of%20the%20measurement%0Avehicle%20only%2C%20while%20neglecting%20objects%20that%20are%20further%20away.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20DeepScenario%20Open%203D%20Dataset%20%28DSC3D%29%2C%20a%20high-quality%2C%0Aocclusion-free%20dataset%20of%206%20degrees%20of%20freedom%20bounding%20box%20trajectories%0Aacquired%20through%20a%20novel%20monocular%20camera%20drone%20tracking%20pipeline.%20Our%20dataset%0Aincludes%20more%20than%20175%2C000%20trajectories%20of%2014%20types%20of%20traffic%20participants%20and%0Asignificantly%20exceeds%20existing%20datasets%20in%20terms%20of%20diversity%20and%20scale%2C%0Acontaining%20many%20unprecedented%20scenarios%20such%20as%20complex%20vehicle-pedestrian%0Ainteraction%20on%20highly%20populated%20urban%20streets%20and%20comprehensive%20parking%0Amaneuvers%20from%20entry%20to%20exit.%20DSC3D%20dataset%20was%20captured%20in%20five%20various%0Alocations%20in%20Europe%20and%20the%20United%20States%20and%20include%3A%20a%20parking%20lot%2C%20a%20crowded%0Ainner-city%2C%20a%20steep%20urban%20intersection%2C%20a%20federal%20highway%2C%20and%20a%20suburban%0Aintersection.%20Our%203D%20trajectory%20dataset%20aims%20to%20enhance%20autonomous%20driving%0Asystems%20by%20providing%20detailed%20environmental%203D%20representations%2C%20which%20could%0Alead%20to%20improved%20obstacle%20interactions%20and%20safety.%20We%20demonstrate%20its%20utility%0Aacross%20multiple%20applications%20including%20motion%20prediction%2C%20motion%20planning%2C%0Ascenario%20mining%2C%20and%20generative%20reactive%20traffic%20agents.%20Our%20interactive%20online%0Avisualization%20platform%20and%20the%20complete%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//app.deepscenario.com%2C%20facilitating%20research%20in%20motion%20prediction%2C%0Abehavior%20modeling%2C%20and%20safety%20validation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17371v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHighly%2520Accurate%2520and%2520Diverse%2520Traffic%2520Data%253A%2520The%2520DeepScenario%2520Open%25203D%250A%2520%2520Dataset%26entry.906535625%3DOussema%2520Dhaouadi%2520and%2520Johannes%2520Meier%2520and%2520Luca%2520Wahl%2520and%2520Jacques%2520Kaiser%2520and%2520Luca%2520Scalerandi%2520and%2520Nick%2520Wandelburg%2520and%2520Zhuolun%2520Zhou%2520and%2520Nijanthan%2520Berinpanathan%2520and%2520Holger%2520Banzhaf%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Accurate%25203D%2520trajectory%2520data%2520is%2520crucial%2520for%2520advancing%2520autonomous%2520driving.%2520Yet%252C%250Atraditional%2520datasets%2520are%2520usually%2520captured%2520by%2520fixed%2520sensors%2520mounted%2520on%2520a%2520car%2520and%250Aare%2520susceptible%2520to%2520occlusion.%2520Additionally%252C%2520such%2520an%2520approach%2520can%2520precisely%250Areconstruct%2520the%2520dynamic%2520environment%2520in%2520the%2520close%2520vicinity%2520of%2520the%2520measurement%250Avehicle%2520only%252C%2520while%2520neglecting%2520objects%2520that%2520are%2520further%2520away.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520DeepScenario%2520Open%25203D%2520Dataset%2520%2528DSC3D%2529%252C%2520a%2520high-quality%252C%250Aocclusion-free%2520dataset%2520of%25206%2520degrees%2520of%2520freedom%2520bounding%2520box%2520trajectories%250Aacquired%2520through%2520a%2520novel%2520monocular%2520camera%2520drone%2520tracking%2520pipeline.%2520Our%2520dataset%250Aincludes%2520more%2520than%2520175%252C000%2520trajectories%2520of%252014%2520types%2520of%2520traffic%2520participants%2520and%250Asignificantly%2520exceeds%2520existing%2520datasets%2520in%2520terms%2520of%2520diversity%2520and%2520scale%252C%250Acontaining%2520many%2520unprecedented%2520scenarios%2520such%2520as%2520complex%2520vehicle-pedestrian%250Ainteraction%2520on%2520highly%2520populated%2520urban%2520streets%2520and%2520comprehensive%2520parking%250Amaneuvers%2520from%2520entry%2520to%2520exit.%2520DSC3D%2520dataset%2520was%2520captured%2520in%2520five%2520various%250Alocations%2520in%2520Europe%2520and%2520the%2520United%2520States%2520and%2520include%253A%2520a%2520parking%2520lot%252C%2520a%2520crowded%250Ainner-city%252C%2520a%2520steep%2520urban%2520intersection%252C%2520a%2520federal%2520highway%252C%2520and%2520a%2520suburban%250Aintersection.%2520Our%25203D%2520trajectory%2520dataset%2520aims%2520to%2520enhance%2520autonomous%2520driving%250Asystems%2520by%2520providing%2520detailed%2520environmental%25203D%2520representations%252C%2520which%2520could%250Alead%2520to%2520improved%2520obstacle%2520interactions%2520and%2520safety.%2520We%2520demonstrate%2520its%2520utility%250Aacross%2520multiple%2520applications%2520including%2520motion%2520prediction%252C%2520motion%2520planning%252C%250Ascenario%2520mining%252C%2520and%2520generative%2520reactive%2520traffic%2520agents.%2520Our%2520interactive%2520online%250Avisualization%2520platform%2520and%2520the%2520complete%2520dataset%2520are%2520publicly%2520available%2520at%250Ahttps%253A//app.deepscenario.com%252C%2520facilitating%2520research%2520in%2520motion%2520prediction%252C%250Abehavior%2520modeling%252C%2520and%2520safety%2520validation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17371v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Highly%20Accurate%20and%20Diverse%20Traffic%20Data%3A%20The%20DeepScenario%20Open%203D%0A%20%20Dataset&entry.906535625=Oussema%20Dhaouadi%20and%20Johannes%20Meier%20and%20Luca%20Wahl%20and%20Jacques%20Kaiser%20and%20Luca%20Scalerandi%20and%20Nick%20Wandelburg%20and%20Zhuolun%20Zhou%20and%20Nijanthan%20Berinpanathan%20and%20Holger%20Banzhaf%20and%20Daniel%20Cremers&entry.1292438233=%20%20Accurate%203D%20trajectory%20data%20is%20crucial%20for%20advancing%20autonomous%20driving.%20Yet%2C%0Atraditional%20datasets%20are%20usually%20captured%20by%20fixed%20sensors%20mounted%20on%20a%20car%20and%0Aare%20susceptible%20to%20occlusion.%20Additionally%2C%20such%20an%20approach%20can%20precisely%0Areconstruct%20the%20dynamic%20environment%20in%20the%20close%20vicinity%20of%20the%20measurement%0Avehicle%20only%2C%20while%20neglecting%20objects%20that%20are%20further%20away.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20DeepScenario%20Open%203D%20Dataset%20%28DSC3D%29%2C%20a%20high-quality%2C%0Aocclusion-free%20dataset%20of%206%20degrees%20of%20freedom%20bounding%20box%20trajectories%0Aacquired%20through%20a%20novel%20monocular%20camera%20drone%20tracking%20pipeline.%20Our%20dataset%0Aincludes%20more%20than%20175%2C000%20trajectories%20of%2014%20types%20of%20traffic%20participants%20and%0Asignificantly%20exceeds%20existing%20datasets%20in%20terms%20of%20diversity%20and%20scale%2C%0Acontaining%20many%20unprecedented%20scenarios%20such%20as%20complex%20vehicle-pedestrian%0Ainteraction%20on%20highly%20populated%20urban%20streets%20and%20comprehensive%20parking%0Amaneuvers%20from%20entry%20to%20exit.%20DSC3D%20dataset%20was%20captured%20in%20five%20various%0Alocations%20in%20Europe%20and%20the%20United%20States%20and%20include%3A%20a%20parking%20lot%2C%20a%20crowded%0Ainner-city%2C%20a%20steep%20urban%20intersection%2C%20a%20federal%20highway%2C%20and%20a%20suburban%0Aintersection.%20Our%203D%20trajectory%20dataset%20aims%20to%20enhance%20autonomous%20driving%0Asystems%20by%20providing%20detailed%20environmental%203D%20representations%2C%20which%20could%0Alead%20to%20improved%20obstacle%20interactions%20and%20safety.%20We%20demonstrate%20its%20utility%0Aacross%20multiple%20applications%20including%20motion%20prediction%2C%20motion%20planning%2C%0Ascenario%20mining%2C%20and%20generative%20reactive%20traffic%20agents.%20Our%20interactive%20online%0Avisualization%20platform%20and%20the%20complete%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//app.deepscenario.com%2C%20facilitating%20research%20in%20motion%20prediction%2C%0Abehavior%20modeling%2C%20and%20safety%20validation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17371v2&entry.124074799=Read"},
{"title": "TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for\n  Lazy Clients", "author": "Mengdi Wang and Anna Bodonhelyi and Efe Bozkir and Enkelejda Kasneci", "abstract": "  Federated learning is a distributed collaborative machine learning paradigm\nthat has gained strong momentum in recent years. In federated learning, a\ncentral server periodically coordinates models with clients and aggregates the\nmodels trained locally by clients without necessitating access to local data.\nDespite its potential, the implementation of federated learning continues to\nencounter several challenges, predominantly the slow convergence that is\nlargely due to data heterogeneity. The slow convergence becomes particularly\nproblematic in cross-device federated learning scenarios where clients may be\nstrongly limited by computing power and storage space, and hence counteracting\nmethods that induce additional computation or memory cost on the client side\nsuch as auxiliary objective terms and larger training iterations can be\nimpractical. In this paper, we propose a novel federated aggregation strategy,\nTurboSVM-FL, that poses no additional computation burden on the client side and\ncan significantly accelerate convergence for federated classification task,\nespecially when clients are \"lazy\" and train their models solely for few epochs\nfor next global aggregation. TurboSVM-FL extensively utilizes support vector\nmachine to conduct selective aggregation and max-margin spread-out\nregularization on class embeddings. We evaluate TurboSVM-FL on multiple\ndatasets including FEMNIST, CelebA, and Shakespeare using user-independent\nvalidation with non-iid data distribution. Our results show that TurboSVM-FL\ncan significantly outperform existing popular algorithms on convergence rate\nand reduce communication rounds while delivering better test metrics including\naccuracy, F1 score, and MCC.\n", "link": "http://arxiv.org/abs/2401.12012v6", "date": "2025-04-25", "relevancy": 2.3655, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4907}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4672}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TurboSVM-FL%3A%20Boosting%20Federated%20Learning%20through%20SVM%20Aggregation%20for%0A%20%20Lazy%20Clients&body=Title%3A%20TurboSVM-FL%3A%20Boosting%20Federated%20Learning%20through%20SVM%20Aggregation%20for%0A%20%20Lazy%20Clients%0AAuthor%3A%20Mengdi%20Wang%20and%20Anna%20Bodonhelyi%20and%20Efe%20Bozkir%20and%20Enkelejda%20Kasneci%0AAbstract%3A%20%20%20Federated%20learning%20is%20a%20distributed%20collaborative%20machine%20learning%20paradigm%0Athat%20has%20gained%20strong%20momentum%20in%20recent%20years.%20In%20federated%20learning%2C%20a%0Acentral%20server%20periodically%20coordinates%20models%20with%20clients%20and%20aggregates%20the%0Amodels%20trained%20locally%20by%20clients%20without%20necessitating%20access%20to%20local%20data.%0ADespite%20its%20potential%2C%20the%20implementation%20of%20federated%20learning%20continues%20to%0Aencounter%20several%20challenges%2C%20predominantly%20the%20slow%20convergence%20that%20is%0Alargely%20due%20to%20data%20heterogeneity.%20The%20slow%20convergence%20becomes%20particularly%0Aproblematic%20in%20cross-device%20federated%20learning%20scenarios%20where%20clients%20may%20be%0Astrongly%20limited%20by%20computing%20power%20and%20storage%20space%2C%20and%20hence%20counteracting%0Amethods%20that%20induce%20additional%20computation%20or%20memory%20cost%20on%20the%20client%20side%0Asuch%20as%20auxiliary%20objective%20terms%20and%20larger%20training%20iterations%20can%20be%0Aimpractical.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20federated%20aggregation%20strategy%2C%0ATurboSVM-FL%2C%20that%20poses%20no%20additional%20computation%20burden%20on%20the%20client%20side%20and%0Acan%20significantly%20accelerate%20convergence%20for%20federated%20classification%20task%2C%0Aespecially%20when%20clients%20are%20%22lazy%22%20and%20train%20their%20models%20solely%20for%20few%20epochs%0Afor%20next%20global%20aggregation.%20TurboSVM-FL%20extensively%20utilizes%20support%20vector%0Amachine%20to%20conduct%20selective%20aggregation%20and%20max-margin%20spread-out%0Aregularization%20on%20class%20embeddings.%20We%20evaluate%20TurboSVM-FL%20on%20multiple%0Adatasets%20including%20FEMNIST%2C%20CelebA%2C%20and%20Shakespeare%20using%20user-independent%0Avalidation%20with%20non-iid%20data%20distribution.%20Our%20results%20show%20that%20TurboSVM-FL%0Acan%20significantly%20outperform%20existing%20popular%20algorithms%20on%20convergence%20rate%0Aand%20reduce%20communication%20rounds%20while%20delivering%20better%20test%20metrics%20including%0Aaccuracy%2C%20F1%20score%2C%20and%20MCC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12012v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurboSVM-FL%253A%2520Boosting%2520Federated%2520Learning%2520through%2520SVM%2520Aggregation%2520for%250A%2520%2520Lazy%2520Clients%26entry.906535625%3DMengdi%2520Wang%2520and%2520Anna%2520Bodonhelyi%2520and%2520Efe%2520Bozkir%2520and%2520Enkelejda%2520Kasneci%26entry.1292438233%3D%2520%2520Federated%2520learning%2520is%2520a%2520distributed%2520collaborative%2520machine%2520learning%2520paradigm%250Athat%2520has%2520gained%2520strong%2520momentum%2520in%2520recent%2520years.%2520In%2520federated%2520learning%252C%2520a%250Acentral%2520server%2520periodically%2520coordinates%2520models%2520with%2520clients%2520and%2520aggregates%2520the%250Amodels%2520trained%2520locally%2520by%2520clients%2520without%2520necessitating%2520access%2520to%2520local%2520data.%250ADespite%2520its%2520potential%252C%2520the%2520implementation%2520of%2520federated%2520learning%2520continues%2520to%250Aencounter%2520several%2520challenges%252C%2520predominantly%2520the%2520slow%2520convergence%2520that%2520is%250Alargely%2520due%2520to%2520data%2520heterogeneity.%2520The%2520slow%2520convergence%2520becomes%2520particularly%250Aproblematic%2520in%2520cross-device%2520federated%2520learning%2520scenarios%2520where%2520clients%2520may%2520be%250Astrongly%2520limited%2520by%2520computing%2520power%2520and%2520storage%2520space%252C%2520and%2520hence%2520counteracting%250Amethods%2520that%2520induce%2520additional%2520computation%2520or%2520memory%2520cost%2520on%2520the%2520client%2520side%250Asuch%2520as%2520auxiliary%2520objective%2520terms%2520and%2520larger%2520training%2520iterations%2520can%2520be%250Aimpractical.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520federated%2520aggregation%2520strategy%252C%250ATurboSVM-FL%252C%2520that%2520poses%2520no%2520additional%2520computation%2520burden%2520on%2520the%2520client%2520side%2520and%250Acan%2520significantly%2520accelerate%2520convergence%2520for%2520federated%2520classification%2520task%252C%250Aespecially%2520when%2520clients%2520are%2520%2522lazy%2522%2520and%2520train%2520their%2520models%2520solely%2520for%2520few%2520epochs%250Afor%2520next%2520global%2520aggregation.%2520TurboSVM-FL%2520extensively%2520utilizes%2520support%2520vector%250Amachine%2520to%2520conduct%2520selective%2520aggregation%2520and%2520max-margin%2520spread-out%250Aregularization%2520on%2520class%2520embeddings.%2520We%2520evaluate%2520TurboSVM-FL%2520on%2520multiple%250Adatasets%2520including%2520FEMNIST%252C%2520CelebA%252C%2520and%2520Shakespeare%2520using%2520user-independent%250Avalidation%2520with%2520non-iid%2520data%2520distribution.%2520Our%2520results%2520show%2520that%2520TurboSVM-FL%250Acan%2520significantly%2520outperform%2520existing%2520popular%2520algorithms%2520on%2520convergence%2520rate%250Aand%2520reduce%2520communication%2520rounds%2520while%2520delivering%2520better%2520test%2520metrics%2520including%250Aaccuracy%252C%2520F1%2520score%252C%2520and%2520MCC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12012v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TurboSVM-FL%3A%20Boosting%20Federated%20Learning%20through%20SVM%20Aggregation%20for%0A%20%20Lazy%20Clients&entry.906535625=Mengdi%20Wang%20and%20Anna%20Bodonhelyi%20and%20Efe%20Bozkir%20and%20Enkelejda%20Kasneci&entry.1292438233=%20%20Federated%20learning%20is%20a%20distributed%20collaborative%20machine%20learning%20paradigm%0Athat%20has%20gained%20strong%20momentum%20in%20recent%20years.%20In%20federated%20learning%2C%20a%0Acentral%20server%20periodically%20coordinates%20models%20with%20clients%20and%20aggregates%20the%0Amodels%20trained%20locally%20by%20clients%20without%20necessitating%20access%20to%20local%20data.%0ADespite%20its%20potential%2C%20the%20implementation%20of%20federated%20learning%20continues%20to%0Aencounter%20several%20challenges%2C%20predominantly%20the%20slow%20convergence%20that%20is%0Alargely%20due%20to%20data%20heterogeneity.%20The%20slow%20convergence%20becomes%20particularly%0Aproblematic%20in%20cross-device%20federated%20learning%20scenarios%20where%20clients%20may%20be%0Astrongly%20limited%20by%20computing%20power%20and%20storage%20space%2C%20and%20hence%20counteracting%0Amethods%20that%20induce%20additional%20computation%20or%20memory%20cost%20on%20the%20client%20side%0Asuch%20as%20auxiliary%20objective%20terms%20and%20larger%20training%20iterations%20can%20be%0Aimpractical.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20federated%20aggregation%20strategy%2C%0ATurboSVM-FL%2C%20that%20poses%20no%20additional%20computation%20burden%20on%20the%20client%20side%20and%0Acan%20significantly%20accelerate%20convergence%20for%20federated%20classification%20task%2C%0Aespecially%20when%20clients%20are%20%22lazy%22%20and%20train%20their%20models%20solely%20for%20few%20epochs%0Afor%20next%20global%20aggregation.%20TurboSVM-FL%20extensively%20utilizes%20support%20vector%0Amachine%20to%20conduct%20selective%20aggregation%20and%20max-margin%20spread-out%0Aregularization%20on%20class%20embeddings.%20We%20evaluate%20TurboSVM-FL%20on%20multiple%0Adatasets%20including%20FEMNIST%2C%20CelebA%2C%20and%20Shakespeare%20using%20user-independent%0Avalidation%20with%20non-iid%20data%20distribution.%20Our%20results%20show%20that%20TurboSVM-FL%0Acan%20significantly%20outperform%20existing%20popular%20algorithms%20on%20convergence%20rate%0Aand%20reduce%20communication%20rounds%20while%20delivering%20better%20test%20metrics%20including%0Aaccuracy%2C%20F1%20score%2C%20and%20MCC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12012v6&entry.124074799=Read"},
{"title": "SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in\n  Ecology", "author": "Elena Plekhanova and Damien Robert and Johannes Dollinger and Emilia Arens and Philipp Brun and Jan Dirk Wegner and Niklaus Zimmermann", "abstract": "  With the exacerbation of the biodiversity and climate crises, macroecological\npursuits such as global biodiversity mapping become more urgent. Remote sensing\noffers a wealth of Earth observation data for ecological studies, but the\nscarcity of labeled datasets remains a major challenge. Recently,\nself-supervised learning has enabled learning representations from unlabeled\ndata, triggering the development of pretrained geospatial models with\ngeneralizable features. However, these models are often trained on datasets\nbiased toward areas of high human activity, leaving entire ecological regions\nunderrepresented. Additionally, while some datasets attempt to address\nseasonality through multi-date imagery, they typically follow calendar seasons\nrather than local phenological cycles. To better capture vegetation seasonality\nat a global scale, we propose a simple phenology-informed sampling strategy and\nintroduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we\ntrain an existing model with a season-contrastive objective. We compare\nrepresentations learned from SSL4Eco against other datasets on diverse\necological downstream tasks and demonstrate that our straightforward sampling\nmethod consistently improves representation quality, highlighting the\nimportance of dataset construction. The model pretrained on SSL4Eco reaches\nstate of the art performance on 7 out of 8 downstream tasks spanning\n(multi-label) classification and regression. We release our code, data, and\nmodel weights to support macroecological and computer vision research at\nhttps://github.com/PlekhanovaElena/ssl4eco.\n", "link": "http://arxiv.org/abs/2504.18256v1", "date": "2025-04-25", "relevancy": 2.3646, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSL4Eco%3A%20A%20Global%20Seasonal%20Dataset%20for%20Geospatial%20Foundation%20Models%20in%0A%20%20Ecology&body=Title%3A%20SSL4Eco%3A%20A%20Global%20Seasonal%20Dataset%20for%20Geospatial%20Foundation%20Models%20in%0A%20%20Ecology%0AAuthor%3A%20Elena%20Plekhanova%20and%20Damien%20Robert%20and%20Johannes%20Dollinger%20and%20Emilia%20Arens%20and%20Philipp%20Brun%20and%20Jan%20Dirk%20Wegner%20and%20Niklaus%20Zimmermann%0AAbstract%3A%20%20%20With%20the%20exacerbation%20of%20the%20biodiversity%20and%20climate%20crises%2C%20macroecological%0Apursuits%20such%20as%20global%20biodiversity%20mapping%20become%20more%20urgent.%20Remote%20sensing%0Aoffers%20a%20wealth%20of%20Earth%20observation%20data%20for%20ecological%20studies%2C%20but%20the%0Ascarcity%20of%20labeled%20datasets%20remains%20a%20major%20challenge.%20Recently%2C%0Aself-supervised%20learning%20has%20enabled%20learning%20representations%20from%20unlabeled%0Adata%2C%20triggering%20the%20development%20of%20pretrained%20geospatial%20models%20with%0Ageneralizable%20features.%20However%2C%20these%20models%20are%20often%20trained%20on%20datasets%0Abiased%20toward%20areas%20of%20high%20human%20activity%2C%20leaving%20entire%20ecological%20regions%0Aunderrepresented.%20Additionally%2C%20while%20some%20datasets%20attempt%20to%20address%0Aseasonality%20through%20multi-date%20imagery%2C%20they%20typically%20follow%20calendar%20seasons%0Arather%20than%20local%20phenological%20cycles.%20To%20better%20capture%20vegetation%20seasonality%0Aat%20a%20global%20scale%2C%20we%20propose%20a%20simple%20phenology-informed%20sampling%20strategy%20and%0Aintroduce%20corresponding%20SSL4Eco%2C%20a%20multi-date%20Sentinel-2%20dataset%2C%20on%20which%20we%0Atrain%20an%20existing%20model%20with%20a%20season-contrastive%20objective.%20We%20compare%0Arepresentations%20learned%20from%20SSL4Eco%20against%20other%20datasets%20on%20diverse%0Aecological%20downstream%20tasks%20and%20demonstrate%20that%20our%20straightforward%20sampling%0Amethod%20consistently%20improves%20representation%20quality%2C%20highlighting%20the%0Aimportance%20of%20dataset%20construction.%20The%20model%20pretrained%20on%20SSL4Eco%20reaches%0Astate%20of%20the%20art%20performance%20on%207%20out%20of%208%20downstream%20tasks%20spanning%0A%28multi-label%29%20classification%20and%20regression.%20We%20release%20our%20code%2C%20data%2C%20and%0Amodel%20weights%20to%20support%20macroecological%20and%20computer%20vision%20research%20at%0Ahttps%3A//github.com/PlekhanovaElena/ssl4eco.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSL4Eco%253A%2520A%2520Global%2520Seasonal%2520Dataset%2520for%2520Geospatial%2520Foundation%2520Models%2520in%250A%2520%2520Ecology%26entry.906535625%3DElena%2520Plekhanova%2520and%2520Damien%2520Robert%2520and%2520Johannes%2520Dollinger%2520and%2520Emilia%2520Arens%2520and%2520Philipp%2520Brun%2520and%2520Jan%2520Dirk%2520Wegner%2520and%2520Niklaus%2520Zimmermann%26entry.1292438233%3D%2520%2520With%2520the%2520exacerbation%2520of%2520the%2520biodiversity%2520and%2520climate%2520crises%252C%2520macroecological%250Apursuits%2520such%2520as%2520global%2520biodiversity%2520mapping%2520become%2520more%2520urgent.%2520Remote%2520sensing%250Aoffers%2520a%2520wealth%2520of%2520Earth%2520observation%2520data%2520for%2520ecological%2520studies%252C%2520but%2520the%250Ascarcity%2520of%2520labeled%2520datasets%2520remains%2520a%2520major%2520challenge.%2520Recently%252C%250Aself-supervised%2520learning%2520has%2520enabled%2520learning%2520representations%2520from%2520unlabeled%250Adata%252C%2520triggering%2520the%2520development%2520of%2520pretrained%2520geospatial%2520models%2520with%250Ageneralizable%2520features.%2520However%252C%2520these%2520models%2520are%2520often%2520trained%2520on%2520datasets%250Abiased%2520toward%2520areas%2520of%2520high%2520human%2520activity%252C%2520leaving%2520entire%2520ecological%2520regions%250Aunderrepresented.%2520Additionally%252C%2520while%2520some%2520datasets%2520attempt%2520to%2520address%250Aseasonality%2520through%2520multi-date%2520imagery%252C%2520they%2520typically%2520follow%2520calendar%2520seasons%250Arather%2520than%2520local%2520phenological%2520cycles.%2520To%2520better%2520capture%2520vegetation%2520seasonality%250Aat%2520a%2520global%2520scale%252C%2520we%2520propose%2520a%2520simple%2520phenology-informed%2520sampling%2520strategy%2520and%250Aintroduce%2520corresponding%2520SSL4Eco%252C%2520a%2520multi-date%2520Sentinel-2%2520dataset%252C%2520on%2520which%2520we%250Atrain%2520an%2520existing%2520model%2520with%2520a%2520season-contrastive%2520objective.%2520We%2520compare%250Arepresentations%2520learned%2520from%2520SSL4Eco%2520against%2520other%2520datasets%2520on%2520diverse%250Aecological%2520downstream%2520tasks%2520and%2520demonstrate%2520that%2520our%2520straightforward%2520sampling%250Amethod%2520consistently%2520improves%2520representation%2520quality%252C%2520highlighting%2520the%250Aimportance%2520of%2520dataset%2520construction.%2520The%2520model%2520pretrained%2520on%2520SSL4Eco%2520reaches%250Astate%2520of%2520the%2520art%2520performance%2520on%25207%2520out%2520of%25208%2520downstream%2520tasks%2520spanning%250A%2528multi-label%2529%2520classification%2520and%2520regression.%2520We%2520release%2520our%2520code%252C%2520data%252C%2520and%250Amodel%2520weights%2520to%2520support%2520macroecological%2520and%2520computer%2520vision%2520research%2520at%250Ahttps%253A//github.com/PlekhanovaElena/ssl4eco.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSL4Eco%3A%20A%20Global%20Seasonal%20Dataset%20for%20Geospatial%20Foundation%20Models%20in%0A%20%20Ecology&entry.906535625=Elena%20Plekhanova%20and%20Damien%20Robert%20and%20Johannes%20Dollinger%20and%20Emilia%20Arens%20and%20Philipp%20Brun%20and%20Jan%20Dirk%20Wegner%20and%20Niklaus%20Zimmermann&entry.1292438233=%20%20With%20the%20exacerbation%20of%20the%20biodiversity%20and%20climate%20crises%2C%20macroecological%0Apursuits%20such%20as%20global%20biodiversity%20mapping%20become%20more%20urgent.%20Remote%20sensing%0Aoffers%20a%20wealth%20of%20Earth%20observation%20data%20for%20ecological%20studies%2C%20but%20the%0Ascarcity%20of%20labeled%20datasets%20remains%20a%20major%20challenge.%20Recently%2C%0Aself-supervised%20learning%20has%20enabled%20learning%20representations%20from%20unlabeled%0Adata%2C%20triggering%20the%20development%20of%20pretrained%20geospatial%20models%20with%0Ageneralizable%20features.%20However%2C%20these%20models%20are%20often%20trained%20on%20datasets%0Abiased%20toward%20areas%20of%20high%20human%20activity%2C%20leaving%20entire%20ecological%20regions%0Aunderrepresented.%20Additionally%2C%20while%20some%20datasets%20attempt%20to%20address%0Aseasonality%20through%20multi-date%20imagery%2C%20they%20typically%20follow%20calendar%20seasons%0Arather%20than%20local%20phenological%20cycles.%20To%20better%20capture%20vegetation%20seasonality%0Aat%20a%20global%20scale%2C%20we%20propose%20a%20simple%20phenology-informed%20sampling%20strategy%20and%0Aintroduce%20corresponding%20SSL4Eco%2C%20a%20multi-date%20Sentinel-2%20dataset%2C%20on%20which%20we%0Atrain%20an%20existing%20model%20with%20a%20season-contrastive%20objective.%20We%20compare%0Arepresentations%20learned%20from%20SSL4Eco%20against%20other%20datasets%20on%20diverse%0Aecological%20downstream%20tasks%20and%20demonstrate%20that%20our%20straightforward%20sampling%0Amethod%20consistently%20improves%20representation%20quality%2C%20highlighting%20the%0Aimportance%20of%20dataset%20construction.%20The%20model%20pretrained%20on%20SSL4Eco%20reaches%0Astate%20of%20the%20art%20performance%20on%207%20out%20of%208%20downstream%20tasks%20spanning%0A%28multi-label%29%20classification%20and%20regression.%20We%20release%20our%20code%2C%20data%2C%20and%0Amodel%20weights%20to%20support%20macroecological%20and%20computer%20vision%20research%20at%0Ahttps%3A//github.com/PlekhanovaElena/ssl4eco.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18256v1&entry.124074799=Read"},
{"title": "Kimi-Audio Technical Report", "author": " KimiTeam and Ding Ding and Zeqian Ju and Yichong Leng and Songxiang Liu and Tong Liu and Zeyu Shang and Kai Shen and Wei Song and Xu Tan and Heyi Tang and Zhengtao Wang and Chu Wei and Yifei Xin and Xinran Xu and Jianwei Yu and Yutao Zhang and Xinyu Zhou and Y. Charles and Jun Chen and Yanru Chen and Yulun Du and Weiran He and Zhenxing Hu and Guokun Lai and Qingcheng Li and Yangyang Liu and Weidong Sun and Jianzhou Wang and Yuzhi Wang and Yuefeng Wu and Yuxin Wu and Dongchao Yang and Hao Yang and Ying Yang and Zhilin Yang and Aoxiong Yin and Ruibin Yuan and Yutong Zhang and Zaida Zhou", "abstract": "  We present Kimi-Audio, an open-source audio foundation model that excels in\naudio understanding, generation, and conversation. We detail the practices in\nbuilding Kimi-Audio, including model architecture, data curation, training\nrecipe, inference deployment, and evaluation. Specifically, we leverage a\n12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous\nfeatures as input and discrete tokens as output, and develop a chunk-wise\nstreaming detokenizer based on flow matching. We curate a pre-training dataset\nthat consists of more than 13 million hours of audio data covering a wide range\nof modalities including speech, sound, and music, and build a pipeline to\nconstruct high-quality and diverse post-training data. Initialized from a\npre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text\ndata with several carefully designed tasks, and then fine-tuned to support a\ndiverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio\nachieves state-of-the-art performance on a range of audio benchmarks including\nspeech recognition, audio understanding, audio question answering, and speech\nconversation. We release the codes, model checkpoints, as well as the\nevaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.\n", "link": "http://arxiv.org/abs/2504.18425v1", "date": "2025-04-25", "relevancy": 2.3578, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kimi-Audio%20Technical%20Report&body=Title%3A%20Kimi-Audio%20Technical%20Report%0AAuthor%3A%20%20KimiTeam%20and%20Ding%20Ding%20and%20Zeqian%20Ju%20and%20Yichong%20Leng%20and%20Songxiang%20Liu%20and%20Tong%20Liu%20and%20Zeyu%20Shang%20and%20Kai%20Shen%20and%20Wei%20Song%20and%20Xu%20Tan%20and%20Heyi%20Tang%20and%20Zhengtao%20Wang%20and%20Chu%20Wei%20and%20Yifei%20Xin%20and%20Xinran%20Xu%20and%20Jianwei%20Yu%20and%20Yutao%20Zhang%20and%20Xinyu%20Zhou%20and%20Y.%20Charles%20and%20Jun%20Chen%20and%20Yanru%20Chen%20and%20Yulun%20Du%20and%20Weiran%20He%20and%20Zhenxing%20Hu%20and%20Guokun%20Lai%20and%20Qingcheng%20Li%20and%20Yangyang%20Liu%20and%20Weidong%20Sun%20and%20Jianzhou%20Wang%20and%20Yuzhi%20Wang%20and%20Yuefeng%20Wu%20and%20Yuxin%20Wu%20and%20Dongchao%20Yang%20and%20Hao%20Yang%20and%20Ying%20Yang%20and%20Zhilin%20Yang%20and%20Aoxiong%20Yin%20and%20Ruibin%20Yuan%20and%20Yutong%20Zhang%20and%20Zaida%20Zhou%0AAbstract%3A%20%20%20We%20present%20Kimi-Audio%2C%20an%20open-source%20audio%20foundation%20model%20that%20excels%20in%0Aaudio%20understanding%2C%20generation%2C%20and%20conversation.%20We%20detail%20the%20practices%20in%0Abuilding%20Kimi-Audio%2C%20including%20model%20architecture%2C%20data%20curation%2C%20training%0Arecipe%2C%20inference%20deployment%2C%20and%20evaluation.%20Specifically%2C%20we%20leverage%20a%0A12.5Hz%20audio%20tokenizer%2C%20design%20a%20novel%20LLM-based%20architecture%20with%20continuous%0Afeatures%20as%20input%20and%20discrete%20tokens%20as%20output%2C%20and%20develop%20a%20chunk-wise%0Astreaming%20detokenizer%20based%20on%20flow%20matching.%20We%20curate%20a%20pre-training%20dataset%0Athat%20consists%20of%20more%20than%2013%20million%20hours%20of%20audio%20data%20covering%20a%20wide%20range%0Aof%20modalities%20including%20speech%2C%20sound%2C%20and%20music%2C%20and%20build%20a%20pipeline%20to%0Aconstruct%20high-quality%20and%20diverse%20post-training%20data.%20Initialized%20from%20a%0Apre-trained%20LLM%2C%20Kimi-Audio%20is%20continual%20pre-trained%20on%20both%20audio%20and%20text%0Adata%20with%20several%20carefully%20designed%20tasks%2C%20and%20then%20fine-tuned%20to%20support%20a%0Adiverse%20of%20audio-related%20tasks.%20Extensive%20evaluation%20shows%20that%20Kimi-Audio%0Aachieves%20state-of-the-art%20performance%20on%20a%20range%20of%20audio%20benchmarks%20including%0Aspeech%20recognition%2C%20audio%20understanding%2C%20audio%20question%20answering%2C%20and%20speech%0Aconversation.%20We%20release%20the%20codes%2C%20model%20checkpoints%2C%20as%20well%20as%20the%0Aevaluation%20toolkits%20in%20https%3A//github.com/MoonshotAI/Kimi-Audio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKimi-Audio%2520Technical%2520Report%26entry.906535625%3D%2520KimiTeam%2520and%2520Ding%2520Ding%2520and%2520Zeqian%2520Ju%2520and%2520Yichong%2520Leng%2520and%2520Songxiang%2520Liu%2520and%2520Tong%2520Liu%2520and%2520Zeyu%2520Shang%2520and%2520Kai%2520Shen%2520and%2520Wei%2520Song%2520and%2520Xu%2520Tan%2520and%2520Heyi%2520Tang%2520and%2520Zhengtao%2520Wang%2520and%2520Chu%2520Wei%2520and%2520Yifei%2520Xin%2520and%2520Xinran%2520Xu%2520and%2520Jianwei%2520Yu%2520and%2520Yutao%2520Zhang%2520and%2520Xinyu%2520Zhou%2520and%2520Y.%2520Charles%2520and%2520Jun%2520Chen%2520and%2520Yanru%2520Chen%2520and%2520Yulun%2520Du%2520and%2520Weiran%2520He%2520and%2520Zhenxing%2520Hu%2520and%2520Guokun%2520Lai%2520and%2520Qingcheng%2520Li%2520and%2520Yangyang%2520Liu%2520and%2520Weidong%2520Sun%2520and%2520Jianzhou%2520Wang%2520and%2520Yuzhi%2520Wang%2520and%2520Yuefeng%2520Wu%2520and%2520Yuxin%2520Wu%2520and%2520Dongchao%2520Yang%2520and%2520Hao%2520Yang%2520and%2520Ying%2520Yang%2520and%2520Zhilin%2520Yang%2520and%2520Aoxiong%2520Yin%2520and%2520Ruibin%2520Yuan%2520and%2520Yutong%2520Zhang%2520and%2520Zaida%2520Zhou%26entry.1292438233%3D%2520%2520We%2520present%2520Kimi-Audio%252C%2520an%2520open-source%2520audio%2520foundation%2520model%2520that%2520excels%2520in%250Aaudio%2520understanding%252C%2520generation%252C%2520and%2520conversation.%2520We%2520detail%2520the%2520practices%2520in%250Abuilding%2520Kimi-Audio%252C%2520including%2520model%2520architecture%252C%2520data%2520curation%252C%2520training%250Arecipe%252C%2520inference%2520deployment%252C%2520and%2520evaluation.%2520Specifically%252C%2520we%2520leverage%2520a%250A12.5Hz%2520audio%2520tokenizer%252C%2520design%2520a%2520novel%2520LLM-based%2520architecture%2520with%2520continuous%250Afeatures%2520as%2520input%2520and%2520discrete%2520tokens%2520as%2520output%252C%2520and%2520develop%2520a%2520chunk-wise%250Astreaming%2520detokenizer%2520based%2520on%2520flow%2520matching.%2520We%2520curate%2520a%2520pre-training%2520dataset%250Athat%2520consists%2520of%2520more%2520than%252013%2520million%2520hours%2520of%2520audio%2520data%2520covering%2520a%2520wide%2520range%250Aof%2520modalities%2520including%2520speech%252C%2520sound%252C%2520and%2520music%252C%2520and%2520build%2520a%2520pipeline%2520to%250Aconstruct%2520high-quality%2520and%2520diverse%2520post-training%2520data.%2520Initialized%2520from%2520a%250Apre-trained%2520LLM%252C%2520Kimi-Audio%2520is%2520continual%2520pre-trained%2520on%2520both%2520audio%2520and%2520text%250Adata%2520with%2520several%2520carefully%2520designed%2520tasks%252C%2520and%2520then%2520fine-tuned%2520to%2520support%2520a%250Adiverse%2520of%2520audio-related%2520tasks.%2520Extensive%2520evaluation%2520shows%2520that%2520Kimi-Audio%250Aachieves%2520state-of-the-art%2520performance%2520on%2520a%2520range%2520of%2520audio%2520benchmarks%2520including%250Aspeech%2520recognition%252C%2520audio%2520understanding%252C%2520audio%2520question%2520answering%252C%2520and%2520speech%250Aconversation.%2520We%2520release%2520the%2520codes%252C%2520model%2520checkpoints%252C%2520as%2520well%2520as%2520the%250Aevaluation%2520toolkits%2520in%2520https%253A//github.com/MoonshotAI/Kimi-Audio.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kimi-Audio%20Technical%20Report&entry.906535625=%20KimiTeam%20and%20Ding%20Ding%20and%20Zeqian%20Ju%20and%20Yichong%20Leng%20and%20Songxiang%20Liu%20and%20Tong%20Liu%20and%20Zeyu%20Shang%20and%20Kai%20Shen%20and%20Wei%20Song%20and%20Xu%20Tan%20and%20Heyi%20Tang%20and%20Zhengtao%20Wang%20and%20Chu%20Wei%20and%20Yifei%20Xin%20and%20Xinran%20Xu%20and%20Jianwei%20Yu%20and%20Yutao%20Zhang%20and%20Xinyu%20Zhou%20and%20Y.%20Charles%20and%20Jun%20Chen%20and%20Yanru%20Chen%20and%20Yulun%20Du%20and%20Weiran%20He%20and%20Zhenxing%20Hu%20and%20Guokun%20Lai%20and%20Qingcheng%20Li%20and%20Yangyang%20Liu%20and%20Weidong%20Sun%20and%20Jianzhou%20Wang%20and%20Yuzhi%20Wang%20and%20Yuefeng%20Wu%20and%20Yuxin%20Wu%20and%20Dongchao%20Yang%20and%20Hao%20Yang%20and%20Ying%20Yang%20and%20Zhilin%20Yang%20and%20Aoxiong%20Yin%20and%20Ruibin%20Yuan%20and%20Yutong%20Zhang%20and%20Zaida%20Zhou&entry.1292438233=%20%20We%20present%20Kimi-Audio%2C%20an%20open-source%20audio%20foundation%20model%20that%20excels%20in%0Aaudio%20understanding%2C%20generation%2C%20and%20conversation.%20We%20detail%20the%20practices%20in%0Abuilding%20Kimi-Audio%2C%20including%20model%20architecture%2C%20data%20curation%2C%20training%0Arecipe%2C%20inference%20deployment%2C%20and%20evaluation.%20Specifically%2C%20we%20leverage%20a%0A12.5Hz%20audio%20tokenizer%2C%20design%20a%20novel%20LLM-based%20architecture%20with%20continuous%0Afeatures%20as%20input%20and%20discrete%20tokens%20as%20output%2C%20and%20develop%20a%20chunk-wise%0Astreaming%20detokenizer%20based%20on%20flow%20matching.%20We%20curate%20a%20pre-training%20dataset%0Athat%20consists%20of%20more%20than%2013%20million%20hours%20of%20audio%20data%20covering%20a%20wide%20range%0Aof%20modalities%20including%20speech%2C%20sound%2C%20and%20music%2C%20and%20build%20a%20pipeline%20to%0Aconstruct%20high-quality%20and%20diverse%20post-training%20data.%20Initialized%20from%20a%0Apre-trained%20LLM%2C%20Kimi-Audio%20is%20continual%20pre-trained%20on%20both%20audio%20and%20text%0Adata%20with%20several%20carefully%20designed%20tasks%2C%20and%20then%20fine-tuned%20to%20support%20a%0Adiverse%20of%20audio-related%20tasks.%20Extensive%20evaluation%20shows%20that%20Kimi-Audio%0Aachieves%20state-of-the-art%20performance%20on%20a%20range%20of%20audio%20benchmarks%20including%0Aspeech%20recognition%2C%20audio%20understanding%2C%20audio%20question%20answering%2C%20and%20speech%0Aconversation.%20We%20release%20the%20codes%2C%20model%20checkpoints%2C%20as%20well%20as%20the%0Aevaluation%20toolkits%20in%20https%3A//github.com/MoonshotAI/Kimi-Audio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18425v1&entry.124074799=Read"},
{"title": "Interpretable Affordance Detection on 3D Point Clouds with Probabilistic\n  Prototypes", "author": "Maximilian Xiling Li and Korbinian Rudolf and Nils Blank and Rudolf Lioutikov", "abstract": "  Robotic agents need to understand how to interact with objects in their\nenvironment, both autonomously and during human-robot interactions. Affordance\ndetection on 3D point clouds, which identifies object regions that allow\nspecific interactions, has traditionally relied on deep learning models like\nPointNet++, DGCNN, or PointTransformerV3. However, these models operate as\nblack boxes, offering no insight into their decision-making processes.\nPrototypical Learning methods, such as ProtoPNet, provide an interpretable\nalternative to black-box models by employing a \"this looks like that\"\ncase-based reasoning approach. However, they have been primarily applied to\nimage-based tasks. In this work, we apply prototypical learning to models for\naffordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet\nbenchmark dataset show that prototypical models achieve competitive performance\nwith state-of-the-art black-box models and offer inherent interpretability.\nThis makes prototypical models a promising candidate for human-robot\ninteraction scenarios that require increased trust and safety.\n", "link": "http://arxiv.org/abs/2504.18355v1", "date": "2025-04-25", "relevancy": 2.3197, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Affordance%20Detection%20on%203D%20Point%20Clouds%20with%20Probabilistic%0A%20%20Prototypes&body=Title%3A%20Interpretable%20Affordance%20Detection%20on%203D%20Point%20Clouds%20with%20Probabilistic%0A%20%20Prototypes%0AAuthor%3A%20Maximilian%20Xiling%20Li%20and%20Korbinian%20Rudolf%20and%20Nils%20Blank%20and%20Rudolf%20Lioutikov%0AAbstract%3A%20%20%20Robotic%20agents%20need%20to%20understand%20how%20to%20interact%20with%20objects%20in%20their%0Aenvironment%2C%20both%20autonomously%20and%20during%20human-robot%20interactions.%20Affordance%0Adetection%20on%203D%20point%20clouds%2C%20which%20identifies%20object%20regions%20that%20allow%0Aspecific%20interactions%2C%20has%20traditionally%20relied%20on%20deep%20learning%20models%20like%0APointNet%2B%2B%2C%20DGCNN%2C%20or%20PointTransformerV3.%20However%2C%20these%20models%20operate%20as%0Ablack%20boxes%2C%20offering%20no%20insight%20into%20their%20decision-making%20processes.%0APrototypical%20Learning%20methods%2C%20such%20as%20ProtoPNet%2C%20provide%20an%20interpretable%0Aalternative%20to%20black-box%20models%20by%20employing%20a%20%22this%20looks%20like%20that%22%0Acase-based%20reasoning%20approach.%20However%2C%20they%20have%20been%20primarily%20applied%20to%0Aimage-based%20tasks.%20In%20this%20work%2C%20we%20apply%20prototypical%20learning%20to%20models%20for%0Aaffordance%20detection%20on%203D%20point%20clouds.%20Experiments%20on%20the%203D-AffordanceNet%0Abenchmark%20dataset%20show%20that%20prototypical%20models%20achieve%20competitive%20performance%0Awith%20state-of-the-art%20black-box%20models%20and%20offer%20inherent%20interpretability.%0AThis%20makes%20prototypical%20models%20a%20promising%20candidate%20for%20human-robot%0Ainteraction%20scenarios%20that%20require%20increased%20trust%20and%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Affordance%2520Detection%2520on%25203D%2520Point%2520Clouds%2520with%2520Probabilistic%250A%2520%2520Prototypes%26entry.906535625%3DMaximilian%2520Xiling%2520Li%2520and%2520Korbinian%2520Rudolf%2520and%2520Nils%2520Blank%2520and%2520Rudolf%2520Lioutikov%26entry.1292438233%3D%2520%2520Robotic%2520agents%2520need%2520to%2520understand%2520how%2520to%2520interact%2520with%2520objects%2520in%2520their%250Aenvironment%252C%2520both%2520autonomously%2520and%2520during%2520human-robot%2520interactions.%2520Affordance%250Adetection%2520on%25203D%2520point%2520clouds%252C%2520which%2520identifies%2520object%2520regions%2520that%2520allow%250Aspecific%2520interactions%252C%2520has%2520traditionally%2520relied%2520on%2520deep%2520learning%2520models%2520like%250APointNet%252B%252B%252C%2520DGCNN%252C%2520or%2520PointTransformerV3.%2520However%252C%2520these%2520models%2520operate%2520as%250Ablack%2520boxes%252C%2520offering%2520no%2520insight%2520into%2520their%2520decision-making%2520processes.%250APrototypical%2520Learning%2520methods%252C%2520such%2520as%2520ProtoPNet%252C%2520provide%2520an%2520interpretable%250Aalternative%2520to%2520black-box%2520models%2520by%2520employing%2520a%2520%2522this%2520looks%2520like%2520that%2522%250Acase-based%2520reasoning%2520approach.%2520However%252C%2520they%2520have%2520been%2520primarily%2520applied%2520to%250Aimage-based%2520tasks.%2520In%2520this%2520work%252C%2520we%2520apply%2520prototypical%2520learning%2520to%2520models%2520for%250Aaffordance%2520detection%2520on%25203D%2520point%2520clouds.%2520Experiments%2520on%2520the%25203D-AffordanceNet%250Abenchmark%2520dataset%2520show%2520that%2520prototypical%2520models%2520achieve%2520competitive%2520performance%250Awith%2520state-of-the-art%2520black-box%2520models%2520and%2520offer%2520inherent%2520interpretability.%250AThis%2520makes%2520prototypical%2520models%2520a%2520promising%2520candidate%2520for%2520human-robot%250Ainteraction%2520scenarios%2520that%2520require%2520increased%2520trust%2520and%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Affordance%20Detection%20on%203D%20Point%20Clouds%20with%20Probabilistic%0A%20%20Prototypes&entry.906535625=Maximilian%20Xiling%20Li%20and%20Korbinian%20Rudolf%20and%20Nils%20Blank%20and%20Rudolf%20Lioutikov&entry.1292438233=%20%20Robotic%20agents%20need%20to%20understand%20how%20to%20interact%20with%20objects%20in%20their%0Aenvironment%2C%20both%20autonomously%20and%20during%20human-robot%20interactions.%20Affordance%0Adetection%20on%203D%20point%20clouds%2C%20which%20identifies%20object%20regions%20that%20allow%0Aspecific%20interactions%2C%20has%20traditionally%20relied%20on%20deep%20learning%20models%20like%0APointNet%2B%2B%2C%20DGCNN%2C%20or%20PointTransformerV3.%20However%2C%20these%20models%20operate%20as%0Ablack%20boxes%2C%20offering%20no%20insight%20into%20their%20decision-making%20processes.%0APrototypical%20Learning%20methods%2C%20such%20as%20ProtoPNet%2C%20provide%20an%20interpretable%0Aalternative%20to%20black-box%20models%20by%20employing%20a%20%22this%20looks%20like%20that%22%0Acase-based%20reasoning%20approach.%20However%2C%20they%20have%20been%20primarily%20applied%20to%0Aimage-based%20tasks.%20In%20this%20work%2C%20we%20apply%20prototypical%20learning%20to%20models%20for%0Aaffordance%20detection%20on%203D%20point%20clouds.%20Experiments%20on%20the%203D-AffordanceNet%0Abenchmark%20dataset%20show%20that%20prototypical%20models%20achieve%20competitive%20performance%0Awith%20state-of-the-art%20black-box%20models%20and%20offer%20inherent%20interpretability.%0AThis%20makes%20prototypical%20models%20a%20promising%20candidate%20for%20human-robot%0Ainteraction%20scenarios%20that%20require%20increased%20trust%20and%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18355v1&entry.124074799=Read"},
{"title": "A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object\n  Detection", "author": "Carlo Sgaravatti and Roberto Basla and Riccardo Pieroni and Matteo Corno and Sergio M. Savaresi and Luca Magri and Giacomo Boracchi", "abstract": "  We present a new way to detect 3D objects from multimodal inputs, leveraging\nboth LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an\nRGB detection network and a 3D LiDAR detector. We exploit late fusion\nprinciples to reduce LiDAR False Positives, matching LiDAR detections with RGB\nones by projecting the LiDAR bounding boxes on the image. We rely on cascade\nfusion principles to recover LiDAR False Negatives leveraging epipolar\nconstraints and frustums generated by RGB detections of separate views. Our\nsolution can be plugged on top of any underlying single-modal detectors,\nenabling a flexible training process that can take advantage of pre-trained\nLiDAR and RGB detectors, or train the two branches separately. We evaluate our\nresults on the KITTI object detection benchmark, showing significant\nperformance improvements, especially for the detection of Pedestrians and\nCyclists.\n", "link": "http://arxiv.org/abs/2504.18419v1", "date": "2025-04-25", "relevancy": 2.3092, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6012}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.566}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Hybrid%20Late-Cascade%20Fusion%20Network%20for%20Enhanced%203D%20Object%0A%20%20Detection&body=Title%3A%20A%20Multimodal%20Hybrid%20Late-Cascade%20Fusion%20Network%20for%20Enhanced%203D%20Object%0A%20%20Detection%0AAuthor%3A%20Carlo%20Sgaravatti%20and%20Roberto%20Basla%20and%20Riccardo%20Pieroni%20and%20Matteo%20Corno%20and%20Sergio%20M.%20Savaresi%20and%20Luca%20Magri%20and%20Giacomo%20Boracchi%0AAbstract%3A%20%20%20We%20present%20a%20new%20way%20to%20detect%203D%20objects%20from%20multimodal%20inputs%2C%20leveraging%0Aboth%20LiDAR%20and%20RGB%20cameras%20in%20a%20hybrid%20late-cascade%20scheme%2C%20that%20combines%20an%0ARGB%20detection%20network%20and%20a%203D%20LiDAR%20detector.%20We%20exploit%20late%20fusion%0Aprinciples%20to%20reduce%20LiDAR%20False%20Positives%2C%20matching%20LiDAR%20detections%20with%20RGB%0Aones%20by%20projecting%20the%20LiDAR%20bounding%20boxes%20on%20the%20image.%20We%20rely%20on%20cascade%0Afusion%20principles%20to%20recover%20LiDAR%20False%20Negatives%20leveraging%20epipolar%0Aconstraints%20and%20frustums%20generated%20by%20RGB%20detections%20of%20separate%20views.%20Our%0Asolution%20can%20be%20plugged%20on%20top%20of%20any%20underlying%20single-modal%20detectors%2C%0Aenabling%20a%20flexible%20training%20process%20that%20can%20take%20advantage%20of%20pre-trained%0ALiDAR%20and%20RGB%20detectors%2C%20or%20train%20the%20two%20branches%20separately.%20We%20evaluate%20our%0Aresults%20on%20the%20KITTI%20object%20detection%20benchmark%2C%20showing%20significant%0Aperformance%20improvements%2C%20especially%20for%20the%20detection%20of%20Pedestrians%20and%0ACyclists.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multimodal%2520Hybrid%2520Late-Cascade%2520Fusion%2520Network%2520for%2520Enhanced%25203D%2520Object%250A%2520%2520Detection%26entry.906535625%3DCarlo%2520Sgaravatti%2520and%2520Roberto%2520Basla%2520and%2520Riccardo%2520Pieroni%2520and%2520Matteo%2520Corno%2520and%2520Sergio%2520M.%2520Savaresi%2520and%2520Luca%2520Magri%2520and%2520Giacomo%2520Boracchi%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520way%2520to%2520detect%25203D%2520objects%2520from%2520multimodal%2520inputs%252C%2520leveraging%250Aboth%2520LiDAR%2520and%2520RGB%2520cameras%2520in%2520a%2520hybrid%2520late-cascade%2520scheme%252C%2520that%2520combines%2520an%250ARGB%2520detection%2520network%2520and%2520a%25203D%2520LiDAR%2520detector.%2520We%2520exploit%2520late%2520fusion%250Aprinciples%2520to%2520reduce%2520LiDAR%2520False%2520Positives%252C%2520matching%2520LiDAR%2520detections%2520with%2520RGB%250Aones%2520by%2520projecting%2520the%2520LiDAR%2520bounding%2520boxes%2520on%2520the%2520image.%2520We%2520rely%2520on%2520cascade%250Afusion%2520principles%2520to%2520recover%2520LiDAR%2520False%2520Negatives%2520leveraging%2520epipolar%250Aconstraints%2520and%2520frustums%2520generated%2520by%2520RGB%2520detections%2520of%2520separate%2520views.%2520Our%250Asolution%2520can%2520be%2520plugged%2520on%2520top%2520of%2520any%2520underlying%2520single-modal%2520detectors%252C%250Aenabling%2520a%2520flexible%2520training%2520process%2520that%2520can%2520take%2520advantage%2520of%2520pre-trained%250ALiDAR%2520and%2520RGB%2520detectors%252C%2520or%2520train%2520the%2520two%2520branches%2520separately.%2520We%2520evaluate%2520our%250Aresults%2520on%2520the%2520KITTI%2520object%2520detection%2520benchmark%252C%2520showing%2520significant%250Aperformance%2520improvements%252C%2520especially%2520for%2520the%2520detection%2520of%2520Pedestrians%2520and%250ACyclists.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Hybrid%20Late-Cascade%20Fusion%20Network%20for%20Enhanced%203D%20Object%0A%20%20Detection&entry.906535625=Carlo%20Sgaravatti%20and%20Roberto%20Basla%20and%20Riccardo%20Pieroni%20and%20Matteo%20Corno%20and%20Sergio%20M.%20Savaresi%20and%20Luca%20Magri%20and%20Giacomo%20Boracchi&entry.1292438233=%20%20We%20present%20a%20new%20way%20to%20detect%203D%20objects%20from%20multimodal%20inputs%2C%20leveraging%0Aboth%20LiDAR%20and%20RGB%20cameras%20in%20a%20hybrid%20late-cascade%20scheme%2C%20that%20combines%20an%0ARGB%20detection%20network%20and%20a%203D%20LiDAR%20detector.%20We%20exploit%20late%20fusion%0Aprinciples%20to%20reduce%20LiDAR%20False%20Positives%2C%20matching%20LiDAR%20detections%20with%20RGB%0Aones%20by%20projecting%20the%20LiDAR%20bounding%20boxes%20on%20the%20image.%20We%20rely%20on%20cascade%0Afusion%20principles%20to%20recover%20LiDAR%20False%20Negatives%20leveraging%20epipolar%0Aconstraints%20and%20frustums%20generated%20by%20RGB%20detections%20of%20separate%20views.%20Our%0Asolution%20can%20be%20plugged%20on%20top%20of%20any%20underlying%20single-modal%20detectors%2C%0Aenabling%20a%20flexible%20training%20process%20that%20can%20take%20advantage%20of%20pre-trained%0ALiDAR%20and%20RGB%20detectors%2C%20or%20train%20the%20two%20branches%20separately.%20We%20evaluate%20our%0Aresults%20on%20the%20KITTI%20object%20detection%20benchmark%2C%20showing%20significant%0Aperformance%20improvements%2C%20especially%20for%20the%20detection%20of%20Pedestrians%20and%0ACyclists.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18419v1&entry.124074799=Read"},
{"title": "A Taylor Series Approach to Correction of Input Errors in Gaussian\n  Process Regression", "author": "Muzaffar Qureshi and Tochukwu Elijah Ogri and Zachary I. Bell and Wanjiku A. Makumi and Rushikesh Kamalapurkar", "abstract": "  Gaussian Processes (GPs) are widely recognized as powerful non-parametric\nmodels for regression and classification. Traditional GP frameworks\npredominantly operate under the assumption that the inputs are either\naccurately known or subject to zero-mean noise. However, several real-world\napplications such as mobile sensors have imperfect localization, leading to\ninputs with biased errors. These biases can typically be estimated through\nmeasurements collected over time using, for example, Kalman filters. To avoid\nrecomputation of the entire GP model when better estimates of the inputs used\nin the training data become available, we introduce a technique for updating a\ntrained GP model to incorporate updated estimates of the inputs. By leveraging\nthe differentiability of the mean and covariance functions derived from the\nsquared exponential kernel, a second-order correction algorithm is developed to\nupdate the trained GP models. Precomputed Jacobians and Hessians of kernels\nenable real-time refinement of the mean and covariance predictions. The\nefficacy of the developed approach is demonstrated using two simulation\nstudies, with error analyses revealing improvements in both predictive accuracy\nand uncertainty quantification.\n", "link": "http://arxiv.org/abs/2504.18463v1", "date": "2025-04-25", "relevancy": 2.3006, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4735}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4683}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Taylor%20Series%20Approach%20to%20Correction%20of%20Input%20Errors%20in%20Gaussian%0A%20%20Process%20Regression&body=Title%3A%20A%20Taylor%20Series%20Approach%20to%20Correction%20of%20Input%20Errors%20in%20Gaussian%0A%20%20Process%20Regression%0AAuthor%3A%20Muzaffar%20Qureshi%20and%20Tochukwu%20Elijah%20Ogri%20and%20Zachary%20I.%20Bell%20and%20Wanjiku%20A.%20Makumi%20and%20Rushikesh%20Kamalapurkar%0AAbstract%3A%20%20%20Gaussian%20Processes%20%28GPs%29%20are%20widely%20recognized%20as%20powerful%20non-parametric%0Amodels%20for%20regression%20and%20classification.%20Traditional%20GP%20frameworks%0Apredominantly%20operate%20under%20the%20assumption%20that%20the%20inputs%20are%20either%0Aaccurately%20known%20or%20subject%20to%20zero-mean%20noise.%20However%2C%20several%20real-world%0Aapplications%20such%20as%20mobile%20sensors%20have%20imperfect%20localization%2C%20leading%20to%0Ainputs%20with%20biased%20errors.%20These%20biases%20can%20typically%20be%20estimated%20through%0Ameasurements%20collected%20over%20time%20using%2C%20for%20example%2C%20Kalman%20filters.%20To%20avoid%0Arecomputation%20of%20the%20entire%20GP%20model%20when%20better%20estimates%20of%20the%20inputs%20used%0Ain%20the%20training%20data%20become%20available%2C%20we%20introduce%20a%20technique%20for%20updating%20a%0Atrained%20GP%20model%20to%20incorporate%20updated%20estimates%20of%20the%20inputs.%20By%20leveraging%0Athe%20differentiability%20of%20the%20mean%20and%20covariance%20functions%20derived%20from%20the%0Asquared%20exponential%20kernel%2C%20a%20second-order%20correction%20algorithm%20is%20developed%20to%0Aupdate%20the%20trained%20GP%20models.%20Precomputed%20Jacobians%20and%20Hessians%20of%20kernels%0Aenable%20real-time%20refinement%20of%20the%20mean%20and%20covariance%20predictions.%20The%0Aefficacy%20of%20the%20developed%20approach%20is%20demonstrated%20using%20two%20simulation%0Astudies%2C%20with%20error%20analyses%20revealing%20improvements%20in%20both%20predictive%20accuracy%0Aand%20uncertainty%20quantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Taylor%2520Series%2520Approach%2520to%2520Correction%2520of%2520Input%2520Errors%2520in%2520Gaussian%250A%2520%2520Process%2520Regression%26entry.906535625%3DMuzaffar%2520Qureshi%2520and%2520Tochukwu%2520Elijah%2520Ogri%2520and%2520Zachary%2520I.%2520Bell%2520and%2520Wanjiku%2520A.%2520Makumi%2520and%2520Rushikesh%2520Kamalapurkar%26entry.1292438233%3D%2520%2520Gaussian%2520Processes%2520%2528GPs%2529%2520are%2520widely%2520recognized%2520as%2520powerful%2520non-parametric%250Amodels%2520for%2520regression%2520and%2520classification.%2520Traditional%2520GP%2520frameworks%250Apredominantly%2520operate%2520under%2520the%2520assumption%2520that%2520the%2520inputs%2520are%2520either%250Aaccurately%2520known%2520or%2520subject%2520to%2520zero-mean%2520noise.%2520However%252C%2520several%2520real-world%250Aapplications%2520such%2520as%2520mobile%2520sensors%2520have%2520imperfect%2520localization%252C%2520leading%2520to%250Ainputs%2520with%2520biased%2520errors.%2520These%2520biases%2520can%2520typically%2520be%2520estimated%2520through%250Ameasurements%2520collected%2520over%2520time%2520using%252C%2520for%2520example%252C%2520Kalman%2520filters.%2520To%2520avoid%250Arecomputation%2520of%2520the%2520entire%2520GP%2520model%2520when%2520better%2520estimates%2520of%2520the%2520inputs%2520used%250Ain%2520the%2520training%2520data%2520become%2520available%252C%2520we%2520introduce%2520a%2520technique%2520for%2520updating%2520a%250Atrained%2520GP%2520model%2520to%2520incorporate%2520updated%2520estimates%2520of%2520the%2520inputs.%2520By%2520leveraging%250Athe%2520differentiability%2520of%2520the%2520mean%2520and%2520covariance%2520functions%2520derived%2520from%2520the%250Asquared%2520exponential%2520kernel%252C%2520a%2520second-order%2520correction%2520algorithm%2520is%2520developed%2520to%250Aupdate%2520the%2520trained%2520GP%2520models.%2520Precomputed%2520Jacobians%2520and%2520Hessians%2520of%2520kernels%250Aenable%2520real-time%2520refinement%2520of%2520the%2520mean%2520and%2520covariance%2520predictions.%2520The%250Aefficacy%2520of%2520the%2520developed%2520approach%2520is%2520demonstrated%2520using%2520two%2520simulation%250Astudies%252C%2520with%2520error%2520analyses%2520revealing%2520improvements%2520in%2520both%2520predictive%2520accuracy%250Aand%2520uncertainty%2520quantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Taylor%20Series%20Approach%20to%20Correction%20of%20Input%20Errors%20in%20Gaussian%0A%20%20Process%20Regression&entry.906535625=Muzaffar%20Qureshi%20and%20Tochukwu%20Elijah%20Ogri%20and%20Zachary%20I.%20Bell%20and%20Wanjiku%20A.%20Makumi%20and%20Rushikesh%20Kamalapurkar&entry.1292438233=%20%20Gaussian%20Processes%20%28GPs%29%20are%20widely%20recognized%20as%20powerful%20non-parametric%0Amodels%20for%20regression%20and%20classification.%20Traditional%20GP%20frameworks%0Apredominantly%20operate%20under%20the%20assumption%20that%20the%20inputs%20are%20either%0Aaccurately%20known%20or%20subject%20to%20zero-mean%20noise.%20However%2C%20several%20real-world%0Aapplications%20such%20as%20mobile%20sensors%20have%20imperfect%20localization%2C%20leading%20to%0Ainputs%20with%20biased%20errors.%20These%20biases%20can%20typically%20be%20estimated%20through%0Ameasurements%20collected%20over%20time%20using%2C%20for%20example%2C%20Kalman%20filters.%20To%20avoid%0Arecomputation%20of%20the%20entire%20GP%20model%20when%20better%20estimates%20of%20the%20inputs%20used%0Ain%20the%20training%20data%20become%20available%2C%20we%20introduce%20a%20technique%20for%20updating%20a%0Atrained%20GP%20model%20to%20incorporate%20updated%20estimates%20of%20the%20inputs.%20By%20leveraging%0Athe%20differentiability%20of%20the%20mean%20and%20covariance%20functions%20derived%20from%20the%0Asquared%20exponential%20kernel%2C%20a%20second-order%20correction%20algorithm%20is%20developed%20to%0Aupdate%20the%20trained%20GP%20models.%20Precomputed%20Jacobians%20and%20Hessians%20of%20kernels%0Aenable%20real-time%20refinement%20of%20the%20mean%20and%20covariance%20predictions.%20The%0Aefficacy%20of%20the%20developed%20approach%20is%20demonstrated%20using%20two%20simulation%0Astudies%2C%20with%20error%20analyses%20revealing%20improvements%20in%20both%20predictive%20accuracy%0Aand%20uncertainty%20quantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18463v1&entry.124074799=Read"},
{"title": "TDAvec: Computing Vector Summaries of Persistence Diagrams for\n  Topological Data Analysis in R and Python", "author": "Aleksei Luchinsky and Umar Islambekov", "abstract": "  Persistent homology is a widely-used tool in topological data analysis (TDA)\nfor understanding the underlying shape of complex data. By constructing a\nfiltration of simplicial complexes from data points, it captures topological\nfeatures such as connected components, loops, and voids across multiple scales.\nThese features are encoded in persistence diagrams (PDs), which provide a\nconcise summary of the data's topological structure. However, the non-Hilbert\nnature of the space of PDs poses challenges for their direct use in machine\nlearning applications. To address this, kernel methods and vectorization\ntechniques have been developed to transform PDs into\nmachine-learning-compatible formats. In this paper, we introduce a new software\npackage designed to streamline the vectorization of PDs, offering an intuitive\nworkflow and advanced functionalities. We demonstrate the necessity of the\npackage through practical examples and provide a detailed discussion on its\ncontributions to applied TDA. Definitions of all vectorization summaries used\nin the package are included in the appendix.\n", "link": "http://arxiv.org/abs/2411.17340v2", "date": "2025-04-25", "relevancy": 2.295, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4697}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4536}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TDAvec%3A%20Computing%20Vector%20Summaries%20of%20Persistence%20Diagrams%20for%0A%20%20Topological%20Data%20Analysis%20in%20R%20and%20Python&body=Title%3A%20TDAvec%3A%20Computing%20Vector%20Summaries%20of%20Persistence%20Diagrams%20for%0A%20%20Topological%20Data%20Analysis%20in%20R%20and%20Python%0AAuthor%3A%20Aleksei%20Luchinsky%20and%20Umar%20Islambekov%0AAbstract%3A%20%20%20Persistent%20homology%20is%20a%20widely-used%20tool%20in%20topological%20data%20analysis%20%28TDA%29%0Afor%20understanding%20the%20underlying%20shape%20of%20complex%20data.%20By%20constructing%20a%0Afiltration%20of%20simplicial%20complexes%20from%20data%20points%2C%20it%20captures%20topological%0Afeatures%20such%20as%20connected%20components%2C%20loops%2C%20and%20voids%20across%20multiple%20scales.%0AThese%20features%20are%20encoded%20in%20persistence%20diagrams%20%28PDs%29%2C%20which%20provide%20a%0Aconcise%20summary%20of%20the%20data%27s%20topological%20structure.%20However%2C%20the%20non-Hilbert%0Anature%20of%20the%20space%20of%20PDs%20poses%20challenges%20for%20their%20direct%20use%20in%20machine%0Alearning%20applications.%20To%20address%20this%2C%20kernel%20methods%20and%20vectorization%0Atechniques%20have%20been%20developed%20to%20transform%20PDs%20into%0Amachine-learning-compatible%20formats.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20software%0Apackage%20designed%20to%20streamline%20the%20vectorization%20of%20PDs%2C%20offering%20an%20intuitive%0Aworkflow%20and%20advanced%20functionalities.%20We%20demonstrate%20the%20necessity%20of%20the%0Apackage%20through%20practical%20examples%20and%20provide%20a%20detailed%20discussion%20on%20its%0Acontributions%20to%20applied%20TDA.%20Definitions%20of%20all%20vectorization%20summaries%20used%0Ain%20the%20package%20are%20included%20in%20the%20appendix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17340v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTDAvec%253A%2520Computing%2520Vector%2520Summaries%2520of%2520Persistence%2520Diagrams%2520for%250A%2520%2520Topological%2520Data%2520Analysis%2520in%2520R%2520and%2520Python%26entry.906535625%3DAleksei%2520Luchinsky%2520and%2520Umar%2520Islambekov%26entry.1292438233%3D%2520%2520Persistent%2520homology%2520is%2520a%2520widely-used%2520tool%2520in%2520topological%2520data%2520analysis%2520%2528TDA%2529%250Afor%2520understanding%2520the%2520underlying%2520shape%2520of%2520complex%2520data.%2520By%2520constructing%2520a%250Afiltration%2520of%2520simplicial%2520complexes%2520from%2520data%2520points%252C%2520it%2520captures%2520topological%250Afeatures%2520such%2520as%2520connected%2520components%252C%2520loops%252C%2520and%2520voids%2520across%2520multiple%2520scales.%250AThese%2520features%2520are%2520encoded%2520in%2520persistence%2520diagrams%2520%2528PDs%2529%252C%2520which%2520provide%2520a%250Aconcise%2520summary%2520of%2520the%2520data%2527s%2520topological%2520structure.%2520However%252C%2520the%2520non-Hilbert%250Anature%2520of%2520the%2520space%2520of%2520PDs%2520poses%2520challenges%2520for%2520their%2520direct%2520use%2520in%2520machine%250Alearning%2520applications.%2520To%2520address%2520this%252C%2520kernel%2520methods%2520and%2520vectorization%250Atechniques%2520have%2520been%2520developed%2520to%2520transform%2520PDs%2520into%250Amachine-learning-compatible%2520formats.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520software%250Apackage%2520designed%2520to%2520streamline%2520the%2520vectorization%2520of%2520PDs%252C%2520offering%2520an%2520intuitive%250Aworkflow%2520and%2520advanced%2520functionalities.%2520We%2520demonstrate%2520the%2520necessity%2520of%2520the%250Apackage%2520through%2520practical%2520examples%2520and%2520provide%2520a%2520detailed%2520discussion%2520on%2520its%250Acontributions%2520to%2520applied%2520TDA.%2520Definitions%2520of%2520all%2520vectorization%2520summaries%2520used%250Ain%2520the%2520package%2520are%2520included%2520in%2520the%2520appendix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17340v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TDAvec%3A%20Computing%20Vector%20Summaries%20of%20Persistence%20Diagrams%20for%0A%20%20Topological%20Data%20Analysis%20in%20R%20and%20Python&entry.906535625=Aleksei%20Luchinsky%20and%20Umar%20Islambekov&entry.1292438233=%20%20Persistent%20homology%20is%20a%20widely-used%20tool%20in%20topological%20data%20analysis%20%28TDA%29%0Afor%20understanding%20the%20underlying%20shape%20of%20complex%20data.%20By%20constructing%20a%0Afiltration%20of%20simplicial%20complexes%20from%20data%20points%2C%20it%20captures%20topological%0Afeatures%20such%20as%20connected%20components%2C%20loops%2C%20and%20voids%20across%20multiple%20scales.%0AThese%20features%20are%20encoded%20in%20persistence%20diagrams%20%28PDs%29%2C%20which%20provide%20a%0Aconcise%20summary%20of%20the%20data%27s%20topological%20structure.%20However%2C%20the%20non-Hilbert%0Anature%20of%20the%20space%20of%20PDs%20poses%20challenges%20for%20their%20direct%20use%20in%20machine%0Alearning%20applications.%20To%20address%20this%2C%20kernel%20methods%20and%20vectorization%0Atechniques%20have%20been%20developed%20to%20transform%20PDs%20into%0Amachine-learning-compatible%20formats.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20software%0Apackage%20designed%20to%20streamline%20the%20vectorization%20of%20PDs%2C%20offering%20an%20intuitive%0Aworkflow%20and%20advanced%20functionalities.%20We%20demonstrate%20the%20necessity%20of%20the%0Apackage%20through%20practical%20examples%20and%20provide%20a%20detailed%20discussion%20on%20its%0Acontributions%20to%20applied%20TDA.%20Definitions%20of%20all%20vectorization%20summaries%20used%0Ain%20the%20package%20are%20included%20in%20the%20appendix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17340v2&entry.124074799=Read"},
{"title": "Task-Oriented Communications for Visual Navigation with Edge-Aerial\n  Collaboration in Low Altitude Economy", "author": "Zhengru Fang and Zhenghao Liu and Jingjing Wang and Senkang Hu and Yu Guo and Yiqin Deng and Yuguang Fang", "abstract": "  To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles\n(UAVs) localization in urban areas where global positioning system (GPS)\nsignals are unavailable. Vision-based methods offer a viable alternative but\nface severe bandwidth, memory and processing constraints on lightweight UAVs.\nInspired by mammalian spatial cognition, we propose a task-oriented\ncommunication framework, where UAVs equipped with multi-camera systems extract\ncompact multi-view features and offload localization tasks to edge servers. We\nintroduce the Orthogonally-constrained Variational Information Bottleneck\nencoder (O-VIB), which incorporates automatic relevance determination (ARD) to\nprune non-informative features while enforcing orthogonality to minimize\nredundancy. This enables efficient and accurate localization with minimal\ntransmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows\nthat O-VIB achieves high-precision localization under stringent bandwidth\nbudgets. Code and dataset will be made publicly available:\ngithub.com/fangzr/TOC-Edge-Aerial.\n", "link": "http://arxiv.org/abs/2504.18317v1", "date": "2025-04-25", "relevancy": 2.2889, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.603}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5518}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Oriented%20Communications%20for%20Visual%20Navigation%20with%20Edge-Aerial%0A%20%20Collaboration%20in%20Low%20Altitude%20Economy&body=Title%3A%20Task-Oriented%20Communications%20for%20Visual%20Navigation%20with%20Edge-Aerial%0A%20%20Collaboration%20in%20Low%20Altitude%20Economy%0AAuthor%3A%20Zhengru%20Fang%20and%20Zhenghao%20Liu%20and%20Jingjing%20Wang%20and%20Senkang%20Hu%20and%20Yu%20Guo%20and%20Yiqin%20Deng%20and%20Yuguang%20Fang%0AAbstract%3A%20%20%20To%20support%20the%20Low%20Altitude%20Economy%20%28LAE%29%2C%20precise%20unmanned%20aerial%20vehicles%0A%28UAVs%29%20localization%20in%20urban%20areas%20where%20global%20positioning%20system%20%28GPS%29%0Asignals%20are%20unavailable.%20Vision-based%20methods%20offer%20a%20viable%20alternative%20but%0Aface%20severe%20bandwidth%2C%20memory%20and%20processing%20constraints%20on%20lightweight%20UAVs.%0AInspired%20by%20mammalian%20spatial%20cognition%2C%20we%20propose%20a%20task-oriented%0Acommunication%20framework%2C%20where%20UAVs%20equipped%20with%20multi-camera%20systems%20extract%0Acompact%20multi-view%20features%20and%20offload%20localization%20tasks%20to%20edge%20servers.%20We%0Aintroduce%20the%20Orthogonally-constrained%20Variational%20Information%20Bottleneck%0Aencoder%20%28O-VIB%29%2C%20which%20incorporates%20automatic%20relevance%20determination%20%28ARD%29%20to%0Aprune%20non-informative%20features%20while%20enforcing%20orthogonality%20to%20minimize%0Aredundancy.%20This%20enables%20efficient%20and%20accurate%20localization%20with%20minimal%0Atransmission%20cost.%20Extensive%20evaluation%20on%20a%20dedicated%20LAE%20UAV%20dataset%20shows%0Athat%20O-VIB%20achieves%20high-precision%20localization%20under%20stringent%20bandwidth%0Abudgets.%20Code%20and%20dataset%20will%20be%20made%20publicly%20available%3A%0Agithub.com/fangzr/TOC-Edge-Aerial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Oriented%2520Communications%2520for%2520Visual%2520Navigation%2520with%2520Edge-Aerial%250A%2520%2520Collaboration%2520in%2520Low%2520Altitude%2520Economy%26entry.906535625%3DZhengru%2520Fang%2520and%2520Zhenghao%2520Liu%2520and%2520Jingjing%2520Wang%2520and%2520Senkang%2520Hu%2520and%2520Yu%2520Guo%2520and%2520Yiqin%2520Deng%2520and%2520Yuguang%2520Fang%26entry.1292438233%3D%2520%2520To%2520support%2520the%2520Low%2520Altitude%2520Economy%2520%2528LAE%2529%252C%2520precise%2520unmanned%2520aerial%2520vehicles%250A%2528UAVs%2529%2520localization%2520in%2520urban%2520areas%2520where%2520global%2520positioning%2520system%2520%2528GPS%2529%250Asignals%2520are%2520unavailable.%2520Vision-based%2520methods%2520offer%2520a%2520viable%2520alternative%2520but%250Aface%2520severe%2520bandwidth%252C%2520memory%2520and%2520processing%2520constraints%2520on%2520lightweight%2520UAVs.%250AInspired%2520by%2520mammalian%2520spatial%2520cognition%252C%2520we%2520propose%2520a%2520task-oriented%250Acommunication%2520framework%252C%2520where%2520UAVs%2520equipped%2520with%2520multi-camera%2520systems%2520extract%250Acompact%2520multi-view%2520features%2520and%2520offload%2520localization%2520tasks%2520to%2520edge%2520servers.%2520We%250Aintroduce%2520the%2520Orthogonally-constrained%2520Variational%2520Information%2520Bottleneck%250Aencoder%2520%2528O-VIB%2529%252C%2520which%2520incorporates%2520automatic%2520relevance%2520determination%2520%2528ARD%2529%2520to%250Aprune%2520non-informative%2520features%2520while%2520enforcing%2520orthogonality%2520to%2520minimize%250Aredundancy.%2520This%2520enables%2520efficient%2520and%2520accurate%2520localization%2520with%2520minimal%250Atransmission%2520cost.%2520Extensive%2520evaluation%2520on%2520a%2520dedicated%2520LAE%2520UAV%2520dataset%2520shows%250Athat%2520O-VIB%2520achieves%2520high-precision%2520localization%2520under%2520stringent%2520bandwidth%250Abudgets.%2520Code%2520and%2520dataset%2520will%2520be%2520made%2520publicly%2520available%253A%250Agithub.com/fangzr/TOC-Edge-Aerial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Oriented%20Communications%20for%20Visual%20Navigation%20with%20Edge-Aerial%0A%20%20Collaboration%20in%20Low%20Altitude%20Economy&entry.906535625=Zhengru%20Fang%20and%20Zhenghao%20Liu%20and%20Jingjing%20Wang%20and%20Senkang%20Hu%20and%20Yu%20Guo%20and%20Yiqin%20Deng%20and%20Yuguang%20Fang&entry.1292438233=%20%20To%20support%20the%20Low%20Altitude%20Economy%20%28LAE%29%2C%20precise%20unmanned%20aerial%20vehicles%0A%28UAVs%29%20localization%20in%20urban%20areas%20where%20global%20positioning%20system%20%28GPS%29%0Asignals%20are%20unavailable.%20Vision-based%20methods%20offer%20a%20viable%20alternative%20but%0Aface%20severe%20bandwidth%2C%20memory%20and%20processing%20constraints%20on%20lightweight%20UAVs.%0AInspired%20by%20mammalian%20spatial%20cognition%2C%20we%20propose%20a%20task-oriented%0Acommunication%20framework%2C%20where%20UAVs%20equipped%20with%20multi-camera%20systems%20extract%0Acompact%20multi-view%20features%20and%20offload%20localization%20tasks%20to%20edge%20servers.%20We%0Aintroduce%20the%20Orthogonally-constrained%20Variational%20Information%20Bottleneck%0Aencoder%20%28O-VIB%29%2C%20which%20incorporates%20automatic%20relevance%20determination%20%28ARD%29%20to%0Aprune%20non-informative%20features%20while%20enforcing%20orthogonality%20to%20minimize%0Aredundancy.%20This%20enables%20efficient%20and%20accurate%20localization%20with%20minimal%0Atransmission%20cost.%20Extensive%20evaluation%20on%20a%20dedicated%20LAE%20UAV%20dataset%20shows%0Athat%20O-VIB%20achieves%20high-precision%20localization%20under%20stringent%20bandwidth%0Abudgets.%20Code%20and%20dataset%20will%20be%20made%20publicly%20available%3A%0Agithub.com/fangzr/TOC-Edge-Aerial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18317v1&entry.124074799=Read"},
{"title": "PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory\n  Planner", "author": "Kota Kondo and Claudius T. Tewari and Andrea Tagliabue and Jesus Tordesillas and Parker C. Lusk and Mason B. Peterson and Jonathan P. How", "abstract": "  In decentralized multiagent trajectory planners, agents need to communicate\nand exchange their positions to generate collision-free trajectories. However,\ndue to localization errors/uncertainties, trajectory deconfliction can fail\neven if trajectories are perfectly shared between agents. To address this\nissue, we first present PARM and PARM*, perception-aware, decentralized,\nasynchronous multiagent trajectory planners that enable a team of agents to\nnavigate uncertain environments while deconflicting trajectories and avoiding\nobstacles using perception information. PARM* differs from PARM as it is less\nconservative, using more computation to find closer-to-optimal solutions. While\nthese methods achieve state-of-the-art performance, they suffer from high\ncomputational costs as they need to solve large optimization problems onboard,\nmaking it difficult for agents to replan at high rates. To overcome this\nchallenge, we present our second key contribution, PRIMER, a learning-based\nplanner trained with imitation learning (IL) using PARM* as the expert\ndemonstrator. PRIMER leverages the low computational requirements at deployment\nof neural networks and achieves a computation speed up to 5500 times faster\nthan optimization-based approaches.\n", "link": "http://arxiv.org/abs/2406.10060v3", "date": "2025-04-25", "relevancy": 2.2752, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.583}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5775}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner&body=Title%3A%20PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner%0AAuthor%3A%20Kota%20Kondo%20and%20Claudius%20T.%20Tewari%20and%20Andrea%20Tagliabue%20and%20Jesus%20Tordesillas%20and%20Parker%20C.%20Lusk%20and%20Mason%20B.%20Peterson%20and%20Jonathan%20P.%20How%0AAbstract%3A%20%20%20In%20decentralized%20multiagent%20trajectory%20planners%2C%20agents%20need%20to%20communicate%0Aand%20exchange%20their%20positions%20to%20generate%20collision-free%20trajectories.%20However%2C%0Adue%20to%20localization%20errors/uncertainties%2C%20trajectory%20deconfliction%20can%20fail%0Aeven%20if%20trajectories%20are%20perfectly%20shared%20between%20agents.%20To%20address%20this%0Aissue%2C%20we%20first%20present%20PARM%20and%20PARM%2A%2C%20perception-aware%2C%20decentralized%2C%0Aasynchronous%20multiagent%20trajectory%20planners%20that%20enable%20a%20team%20of%20agents%20to%0Anavigate%20uncertain%20environments%20while%20deconflicting%20trajectories%20and%20avoiding%0Aobstacles%20using%20perception%20information.%20PARM%2A%20differs%20from%20PARM%20as%20it%20is%20less%0Aconservative%2C%20using%20more%20computation%20to%20find%20closer-to-optimal%20solutions.%20While%0Athese%20methods%20achieve%20state-of-the-art%20performance%2C%20they%20suffer%20from%20high%0Acomputational%20costs%20as%20they%20need%20to%20solve%20large%20optimization%20problems%20onboard%2C%0Amaking%20it%20difficult%20for%20agents%20to%20replan%20at%20high%20rates.%20To%20overcome%20this%0Achallenge%2C%20we%20present%20our%20second%20key%20contribution%2C%20PRIMER%2C%20a%20learning-based%0Aplanner%20trained%20with%20imitation%20learning%20%28IL%29%20using%20PARM%2A%20as%20the%20expert%0Ademonstrator.%20PRIMER%20leverages%20the%20low%20computational%20requirements%20at%20deployment%0Aof%20neural%20networks%20and%20achieves%20a%20computation%20speed%20up%20to%205500%20times%20faster%0Athan%20optimization-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10060v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRIMER%253A%2520Perception-Aware%2520Robust%2520Learning-based%2520Multiagent%2520Trajectory%250A%2520%2520Planner%26entry.906535625%3DKota%2520Kondo%2520and%2520Claudius%2520T.%2520Tewari%2520and%2520Andrea%2520Tagliabue%2520and%2520Jesus%2520Tordesillas%2520and%2520Parker%2520C.%2520Lusk%2520and%2520Mason%2520B.%2520Peterson%2520and%2520Jonathan%2520P.%2520How%26entry.1292438233%3D%2520%2520In%2520decentralized%2520multiagent%2520trajectory%2520planners%252C%2520agents%2520need%2520to%2520communicate%250Aand%2520exchange%2520their%2520positions%2520to%2520generate%2520collision-free%2520trajectories.%2520However%252C%250Adue%2520to%2520localization%2520errors/uncertainties%252C%2520trajectory%2520deconfliction%2520can%2520fail%250Aeven%2520if%2520trajectories%2520are%2520perfectly%2520shared%2520between%2520agents.%2520To%2520address%2520this%250Aissue%252C%2520we%2520first%2520present%2520PARM%2520and%2520PARM%252A%252C%2520perception-aware%252C%2520decentralized%252C%250Aasynchronous%2520multiagent%2520trajectory%2520planners%2520that%2520enable%2520a%2520team%2520of%2520agents%2520to%250Anavigate%2520uncertain%2520environments%2520while%2520deconflicting%2520trajectories%2520and%2520avoiding%250Aobstacles%2520using%2520perception%2520information.%2520PARM%252A%2520differs%2520from%2520PARM%2520as%2520it%2520is%2520less%250Aconservative%252C%2520using%2520more%2520computation%2520to%2520find%2520closer-to-optimal%2520solutions.%2520While%250Athese%2520methods%2520achieve%2520state-of-the-art%2520performance%252C%2520they%2520suffer%2520from%2520high%250Acomputational%2520costs%2520as%2520they%2520need%2520to%2520solve%2520large%2520optimization%2520problems%2520onboard%252C%250Amaking%2520it%2520difficult%2520for%2520agents%2520to%2520replan%2520at%2520high%2520rates.%2520To%2520overcome%2520this%250Achallenge%252C%2520we%2520present%2520our%2520second%2520key%2520contribution%252C%2520PRIMER%252C%2520a%2520learning-based%250Aplanner%2520trained%2520with%2520imitation%2520learning%2520%2528IL%2529%2520using%2520PARM%252A%2520as%2520the%2520expert%250Ademonstrator.%2520PRIMER%2520leverages%2520the%2520low%2520computational%2520requirements%2520at%2520deployment%250Aof%2520neural%2520networks%2520and%2520achieves%2520a%2520computation%2520speed%2520up%2520to%25205500%2520times%2520faster%250Athan%2520optimization-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10060v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner&entry.906535625=Kota%20Kondo%20and%20Claudius%20T.%20Tewari%20and%20Andrea%20Tagliabue%20and%20Jesus%20Tordesillas%20and%20Parker%20C.%20Lusk%20and%20Mason%20B.%20Peterson%20and%20Jonathan%20P.%20How&entry.1292438233=%20%20In%20decentralized%20multiagent%20trajectory%20planners%2C%20agents%20need%20to%20communicate%0Aand%20exchange%20their%20positions%20to%20generate%20collision-free%20trajectories.%20However%2C%0Adue%20to%20localization%20errors/uncertainties%2C%20trajectory%20deconfliction%20can%20fail%0Aeven%20if%20trajectories%20are%20perfectly%20shared%20between%20agents.%20To%20address%20this%0Aissue%2C%20we%20first%20present%20PARM%20and%20PARM%2A%2C%20perception-aware%2C%20decentralized%2C%0Aasynchronous%20multiagent%20trajectory%20planners%20that%20enable%20a%20team%20of%20agents%20to%0Anavigate%20uncertain%20environments%20while%20deconflicting%20trajectories%20and%20avoiding%0Aobstacles%20using%20perception%20information.%20PARM%2A%20differs%20from%20PARM%20as%20it%20is%20less%0Aconservative%2C%20using%20more%20computation%20to%20find%20closer-to-optimal%20solutions.%20While%0Athese%20methods%20achieve%20state-of-the-art%20performance%2C%20they%20suffer%20from%20high%0Acomputational%20costs%20as%20they%20need%20to%20solve%20large%20optimization%20problems%20onboard%2C%0Amaking%20it%20difficult%20for%20agents%20to%20replan%20at%20high%20rates.%20To%20overcome%20this%0Achallenge%2C%20we%20present%20our%20second%20key%20contribution%2C%20PRIMER%2C%20a%20learning-based%0Aplanner%20trained%20with%20imitation%20learning%20%28IL%29%20using%20PARM%2A%20as%20the%20expert%0Ademonstrator.%20PRIMER%20leverages%20the%20low%20computational%20requirements%20at%20deployment%0Aof%20neural%20networks%20and%20achieves%20a%20computation%20speed%20up%20to%205500%20times%20faster%0Athan%20optimization-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10060v3&entry.124074799=Read"},
{"title": "Testing Individual Fairness in Graph Neural Networks", "author": "Roya Nasiri", "abstract": "  The biases in artificial intelligence (AI) models can lead to automated\ndecision-making processes that discriminate against groups and/or individuals\nbased on sensitive properties such as gender and race. While there are many\nstudies on diagnosing and mitigating biases in various AI models, there is\nlittle research on individual fairness in Graph Neural Networks (GNNs). Unlike\ntraditional models, which treat data features independently and overlook their\ninter-relationships, GNNs are designed to capture graph-based structure where\nnodes are interconnected. This relational approach enables GNNs to model\ncomplex dependencies, but it also means that biases can propagate through these\nconnections, complicating the detection and mitigation of individual fairness\nviolations. This PhD project aims to develop a testing framework to assess and\nensure individual fairness in GNNs. It first systematically reviews the\nliterature on individual fairness, categorizing existing approaches to define,\nmeasure, test, and mitigate model biases, creating a taxonomy of individual\nfairness. Next, the project will develop a framework for testing and ensuring\nfairness in GNNs by adapting and extending current fairness testing and\nmitigation techniques. The framework will be evaluated through industrial case\nstudies, focusing on graph-based large language models.\n", "link": "http://arxiv.org/abs/2504.18353v1", "date": "2025-04-25", "relevancy": 2.2752, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4649}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4546}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Testing%20Individual%20Fairness%20in%20Graph%20Neural%20Networks&body=Title%3A%20Testing%20Individual%20Fairness%20in%20Graph%20Neural%20Networks%0AAuthor%3A%20Roya%20Nasiri%0AAbstract%3A%20%20%20The%20biases%20in%20artificial%20intelligence%20%28AI%29%20models%20can%20lead%20to%20automated%0Adecision-making%20processes%20that%20discriminate%20against%20groups%20and/or%20individuals%0Abased%20on%20sensitive%20properties%20such%20as%20gender%20and%20race.%20While%20there%20are%20many%0Astudies%20on%20diagnosing%20and%20mitigating%20biases%20in%20various%20AI%20models%2C%20there%20is%0Alittle%20research%20on%20individual%20fairness%20in%20Graph%20Neural%20Networks%20%28GNNs%29.%20Unlike%0Atraditional%20models%2C%20which%20treat%20data%20features%20independently%20and%20overlook%20their%0Ainter-relationships%2C%20GNNs%20are%20designed%20to%20capture%20graph-based%20structure%20where%0Anodes%20are%20interconnected.%20This%20relational%20approach%20enables%20GNNs%20to%20model%0Acomplex%20dependencies%2C%20but%20it%20also%20means%20that%20biases%20can%20propagate%20through%20these%0Aconnections%2C%20complicating%20the%20detection%20and%20mitigation%20of%20individual%20fairness%0Aviolations.%20This%20PhD%20project%20aims%20to%20develop%20a%20testing%20framework%20to%20assess%20and%0Aensure%20individual%20fairness%20in%20GNNs.%20It%20first%20systematically%20reviews%20the%0Aliterature%20on%20individual%20fairness%2C%20categorizing%20existing%20approaches%20to%20define%2C%0Ameasure%2C%20test%2C%20and%20mitigate%20model%20biases%2C%20creating%20a%20taxonomy%20of%20individual%0Afairness.%20Next%2C%20the%20project%20will%20develop%20a%20framework%20for%20testing%20and%20ensuring%0Afairness%20in%20GNNs%20by%20adapting%20and%20extending%20current%20fairness%20testing%20and%0Amitigation%20techniques.%20The%20framework%20will%20be%20evaluated%20through%20industrial%20case%0Astudies%2C%20focusing%20on%20graph-based%20large%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTesting%2520Individual%2520Fairness%2520in%2520Graph%2520Neural%2520Networks%26entry.906535625%3DRoya%2520Nasiri%26entry.1292438233%3D%2520%2520The%2520biases%2520in%2520artificial%2520intelligence%2520%2528AI%2529%2520models%2520can%2520lead%2520to%2520automated%250Adecision-making%2520processes%2520that%2520discriminate%2520against%2520groups%2520and/or%2520individuals%250Abased%2520on%2520sensitive%2520properties%2520such%2520as%2520gender%2520and%2520race.%2520While%2520there%2520are%2520many%250Astudies%2520on%2520diagnosing%2520and%2520mitigating%2520biases%2520in%2520various%2520AI%2520models%252C%2520there%2520is%250Alittle%2520research%2520on%2520individual%2520fairness%2520in%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520Unlike%250Atraditional%2520models%252C%2520which%2520treat%2520data%2520features%2520independently%2520and%2520overlook%2520their%250Ainter-relationships%252C%2520GNNs%2520are%2520designed%2520to%2520capture%2520graph-based%2520structure%2520where%250Anodes%2520are%2520interconnected.%2520This%2520relational%2520approach%2520enables%2520GNNs%2520to%2520model%250Acomplex%2520dependencies%252C%2520but%2520it%2520also%2520means%2520that%2520biases%2520can%2520propagate%2520through%2520these%250Aconnections%252C%2520complicating%2520the%2520detection%2520and%2520mitigation%2520of%2520individual%2520fairness%250Aviolations.%2520This%2520PhD%2520project%2520aims%2520to%2520develop%2520a%2520testing%2520framework%2520to%2520assess%2520and%250Aensure%2520individual%2520fairness%2520in%2520GNNs.%2520It%2520first%2520systematically%2520reviews%2520the%250Aliterature%2520on%2520individual%2520fairness%252C%2520categorizing%2520existing%2520approaches%2520to%2520define%252C%250Ameasure%252C%2520test%252C%2520and%2520mitigate%2520model%2520biases%252C%2520creating%2520a%2520taxonomy%2520of%2520individual%250Afairness.%2520Next%252C%2520the%2520project%2520will%2520develop%2520a%2520framework%2520for%2520testing%2520and%2520ensuring%250Afairness%2520in%2520GNNs%2520by%2520adapting%2520and%2520extending%2520current%2520fairness%2520testing%2520and%250Amitigation%2520techniques.%2520The%2520framework%2520will%2520be%2520evaluated%2520through%2520industrial%2520case%250Astudies%252C%2520focusing%2520on%2520graph-based%2520large%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Testing%20Individual%20Fairness%20in%20Graph%20Neural%20Networks&entry.906535625=Roya%20Nasiri&entry.1292438233=%20%20The%20biases%20in%20artificial%20intelligence%20%28AI%29%20models%20can%20lead%20to%20automated%0Adecision-making%20processes%20that%20discriminate%20against%20groups%20and/or%20individuals%0Abased%20on%20sensitive%20properties%20such%20as%20gender%20and%20race.%20While%20there%20are%20many%0Astudies%20on%20diagnosing%20and%20mitigating%20biases%20in%20various%20AI%20models%2C%20there%20is%0Alittle%20research%20on%20individual%20fairness%20in%20Graph%20Neural%20Networks%20%28GNNs%29.%20Unlike%0Atraditional%20models%2C%20which%20treat%20data%20features%20independently%20and%20overlook%20their%0Ainter-relationships%2C%20GNNs%20are%20designed%20to%20capture%20graph-based%20structure%20where%0Anodes%20are%20interconnected.%20This%20relational%20approach%20enables%20GNNs%20to%20model%0Acomplex%20dependencies%2C%20but%20it%20also%20means%20that%20biases%20can%20propagate%20through%20these%0Aconnections%2C%20complicating%20the%20detection%20and%20mitigation%20of%20individual%20fairness%0Aviolations.%20This%20PhD%20project%20aims%20to%20develop%20a%20testing%20framework%20to%20assess%20and%0Aensure%20individual%20fairness%20in%20GNNs.%20It%20first%20systematically%20reviews%20the%0Aliterature%20on%20individual%20fairness%2C%20categorizing%20existing%20approaches%20to%20define%2C%0Ameasure%2C%20test%2C%20and%20mitigate%20model%20biases%2C%20creating%20a%20taxonomy%20of%20individual%0Afairness.%20Next%2C%20the%20project%20will%20develop%20a%20framework%20for%20testing%20and%20ensuring%0Afairness%20in%20GNNs%20by%20adapting%20and%20extending%20current%20fairness%20testing%20and%0Amitigation%20techniques.%20The%20framework%20will%20be%20evaluated%20through%20industrial%20case%0Astudies%2C%20focusing%20on%20graph-based%20large%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18353v1&entry.124074799=Read"},
{"title": "NUDF: Neural Unsigned Distance Fields for high resolution 3D medical\n  image segmentation", "author": "Kristine S\u00f8rensen and Oscar Camara and Ole de Backer and Klaus Kofoed and Rasmus Paulsen", "abstract": "  Medical image segmentation is often considered as the task of labelling each\npixel or voxel as being inside or outside a given anatomy. Processing the\nimages at their original size and resolution often result in insuperable memory\nrequirements, but downsampling the images leads to a loss of important details.\nInstead of aiming to represent a smooth and continuous surface in a binary\nvoxel-grid, we propose to learn a Neural Unsigned Distance Field (NUDF)\ndirectly from the image. The small memory requirements of NUDF allow for high\nresolution processing, while the continuous nature of the distance field allows\nus to create high resolution 3D mesh models of shapes of any topology (i.e.\nopen surfaces). We evaluate our method on the task of left atrial appendage\n(LAA) segmentation from Computed Tomography (CT) images. The LAA is a complex\nand highly variable shape, being thus difficult to represent with traditional\nsegmentation methods using discrete labelmaps. With our proposed method, we are\nable to predict 3D mesh models that capture the details of the LAA and achieve\naccuracy in the order of the voxel spacing in the CT images.\n", "link": "http://arxiv.org/abs/2504.18344v1", "date": "2025-04-25", "relevancy": 2.257, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5962}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5591}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NUDF%3A%20Neural%20Unsigned%20Distance%20Fields%20for%20high%20resolution%203D%20medical%0A%20%20image%20segmentation&body=Title%3A%20NUDF%3A%20Neural%20Unsigned%20Distance%20Fields%20for%20high%20resolution%203D%20medical%0A%20%20image%20segmentation%0AAuthor%3A%20Kristine%20S%C3%B8rensen%20and%20Oscar%20Camara%20and%20Ole%20de%20Backer%20and%20Klaus%20Kofoed%20and%20Rasmus%20Paulsen%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20often%20considered%20as%20the%20task%20of%20labelling%20each%0Apixel%20or%20voxel%20as%20being%20inside%20or%20outside%20a%20given%20anatomy.%20Processing%20the%0Aimages%20at%20their%20original%20size%20and%20resolution%20often%20result%20in%20insuperable%20memory%0Arequirements%2C%20but%20downsampling%20the%20images%20leads%20to%20a%20loss%20of%20important%20details.%0AInstead%20of%20aiming%20to%20represent%20a%20smooth%20and%20continuous%20surface%20in%20a%20binary%0Avoxel-grid%2C%20we%20propose%20to%20learn%20a%20Neural%20Unsigned%20Distance%20Field%20%28NUDF%29%0Adirectly%20from%20the%20image.%20The%20small%20memory%20requirements%20of%20NUDF%20allow%20for%20high%0Aresolution%20processing%2C%20while%20the%20continuous%20nature%20of%20the%20distance%20field%20allows%0Aus%20to%20create%20high%20resolution%203D%20mesh%20models%20of%20shapes%20of%20any%20topology%20%28i.e.%0Aopen%20surfaces%29.%20We%20evaluate%20our%20method%20on%20the%20task%20of%20left%20atrial%20appendage%0A%28LAA%29%20segmentation%20from%20Computed%20Tomography%20%28CT%29%20images.%20The%20LAA%20is%20a%20complex%0Aand%20highly%20variable%20shape%2C%20being%20thus%20difficult%20to%20represent%20with%20traditional%0Asegmentation%20methods%20using%20discrete%20labelmaps.%20With%20our%20proposed%20method%2C%20we%20are%0Aable%20to%20predict%203D%20mesh%20models%20that%20capture%20the%20details%20of%20the%20LAA%20and%20achieve%0Aaccuracy%20in%20the%20order%20of%20the%20voxel%20spacing%20in%20the%20CT%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNUDF%253A%2520Neural%2520Unsigned%2520Distance%2520Fields%2520for%2520high%2520resolution%25203D%2520medical%250A%2520%2520image%2520segmentation%26entry.906535625%3DKristine%2520S%25C3%25B8rensen%2520and%2520Oscar%2520Camara%2520and%2520Ole%2520de%2520Backer%2520and%2520Klaus%2520Kofoed%2520and%2520Rasmus%2520Paulsen%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520often%2520considered%2520as%2520the%2520task%2520of%2520labelling%2520each%250Apixel%2520or%2520voxel%2520as%2520being%2520inside%2520or%2520outside%2520a%2520given%2520anatomy.%2520Processing%2520the%250Aimages%2520at%2520their%2520original%2520size%2520and%2520resolution%2520often%2520result%2520in%2520insuperable%2520memory%250Arequirements%252C%2520but%2520downsampling%2520the%2520images%2520leads%2520to%2520a%2520loss%2520of%2520important%2520details.%250AInstead%2520of%2520aiming%2520to%2520represent%2520a%2520smooth%2520and%2520continuous%2520surface%2520in%2520a%2520binary%250Avoxel-grid%252C%2520we%2520propose%2520to%2520learn%2520a%2520Neural%2520Unsigned%2520Distance%2520Field%2520%2528NUDF%2529%250Adirectly%2520from%2520the%2520image.%2520The%2520small%2520memory%2520requirements%2520of%2520NUDF%2520allow%2520for%2520high%250Aresolution%2520processing%252C%2520while%2520the%2520continuous%2520nature%2520of%2520the%2520distance%2520field%2520allows%250Aus%2520to%2520create%2520high%2520resolution%25203D%2520mesh%2520models%2520of%2520shapes%2520of%2520any%2520topology%2520%2528i.e.%250Aopen%2520surfaces%2529.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520task%2520of%2520left%2520atrial%2520appendage%250A%2528LAA%2529%2520segmentation%2520from%2520Computed%2520Tomography%2520%2528CT%2529%2520images.%2520The%2520LAA%2520is%2520a%2520complex%250Aand%2520highly%2520variable%2520shape%252C%2520being%2520thus%2520difficult%2520to%2520represent%2520with%2520traditional%250Asegmentation%2520methods%2520using%2520discrete%2520labelmaps.%2520With%2520our%2520proposed%2520method%252C%2520we%2520are%250Aable%2520to%2520predict%25203D%2520mesh%2520models%2520that%2520capture%2520the%2520details%2520of%2520the%2520LAA%2520and%2520achieve%250Aaccuracy%2520in%2520the%2520order%2520of%2520the%2520voxel%2520spacing%2520in%2520the%2520CT%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NUDF%3A%20Neural%20Unsigned%20Distance%20Fields%20for%20high%20resolution%203D%20medical%0A%20%20image%20segmentation&entry.906535625=Kristine%20S%C3%B8rensen%20and%20Oscar%20Camara%20and%20Ole%20de%20Backer%20and%20Klaus%20Kofoed%20and%20Rasmus%20Paulsen&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20often%20considered%20as%20the%20task%20of%20labelling%20each%0Apixel%20or%20voxel%20as%20being%20inside%20or%20outside%20a%20given%20anatomy.%20Processing%20the%0Aimages%20at%20their%20original%20size%20and%20resolution%20often%20result%20in%20insuperable%20memory%0Arequirements%2C%20but%20downsampling%20the%20images%20leads%20to%20a%20loss%20of%20important%20details.%0AInstead%20of%20aiming%20to%20represent%20a%20smooth%20and%20continuous%20surface%20in%20a%20binary%0Avoxel-grid%2C%20we%20propose%20to%20learn%20a%20Neural%20Unsigned%20Distance%20Field%20%28NUDF%29%0Adirectly%20from%20the%20image.%20The%20small%20memory%20requirements%20of%20NUDF%20allow%20for%20high%0Aresolution%20processing%2C%20while%20the%20continuous%20nature%20of%20the%20distance%20field%20allows%0Aus%20to%20create%20high%20resolution%203D%20mesh%20models%20of%20shapes%20of%20any%20topology%20%28i.e.%0Aopen%20surfaces%29.%20We%20evaluate%20our%20method%20on%20the%20task%20of%20left%20atrial%20appendage%0A%28LAA%29%20segmentation%20from%20Computed%20Tomography%20%28CT%29%20images.%20The%20LAA%20is%20a%20complex%0Aand%20highly%20variable%20shape%2C%20being%20thus%20difficult%20to%20represent%20with%20traditional%0Asegmentation%20methods%20using%20discrete%20labelmaps.%20With%20our%20proposed%20method%2C%20we%20are%0Aable%20to%20predict%203D%20mesh%20models%20that%20capture%20the%20details%20of%20the%20LAA%20and%20achieve%0Aaccuracy%20in%20the%20order%20of%20the%20voxel%20spacing%20in%20the%20CT%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18344v1&entry.124074799=Read"},
{"title": "LEAM: A Prompt-only Large Language Model-enabled Antenna Modeling Method", "author": "Tao Wu and Kexue Fu and Qiang Hua and Xinxin Liu and Muhammad Ali Imran and Bo Liu", "abstract": "  Antenna modeling is a time-consuming and complex process, decreasing the\nspeed of antenna analysis and design. In this paper, a large language model\n(LLM)- enabled antenna modeling method, called LEAM, is presented to address\nthis challenge. LEAM enables automatic antenna model generation based on\nlanguage descriptions via prompt input, images, descriptions from academic\npapers, patents, and technical reports (either one or multiple). The\neffectiveness of LEAM is demonstrated by three examples: a Vivaldi antenna\ngenerated from a complete user description, a slotted patch antenna generated\nfrom an incomplete user description and the operating frequency, and a monopole\nslotted antenna generated from images and descriptions scanned from the\nliterature. For all the examples, correct antenna models are generated in a few\nminutes. The code can be accessed via https://github.com/TaoWu974/LEAM.\n", "link": "http://arxiv.org/abs/2504.18271v1", "date": "2025-04-25", "relevancy": 2.2342, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4492}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEAM%3A%20A%20Prompt-only%20Large%20Language%20Model-enabled%20Antenna%20Modeling%20Method&body=Title%3A%20LEAM%3A%20A%20Prompt-only%20Large%20Language%20Model-enabled%20Antenna%20Modeling%20Method%0AAuthor%3A%20Tao%20Wu%20and%20Kexue%20Fu%20and%20Qiang%20Hua%20and%20Xinxin%20Liu%20and%20Muhammad%20Ali%20Imran%20and%20Bo%20Liu%0AAbstract%3A%20%20%20Antenna%20modeling%20is%20a%20time-consuming%20and%20complex%20process%2C%20decreasing%20the%0Aspeed%20of%20antenna%20analysis%20and%20design.%20In%20this%20paper%2C%20a%20large%20language%20model%0A%28LLM%29-%20enabled%20antenna%20modeling%20method%2C%20called%20LEAM%2C%20is%20presented%20to%20address%0Athis%20challenge.%20LEAM%20enables%20automatic%20antenna%20model%20generation%20based%20on%0Alanguage%20descriptions%20via%20prompt%20input%2C%20images%2C%20descriptions%20from%20academic%0Apapers%2C%20patents%2C%20and%20technical%20reports%20%28either%20one%20or%20multiple%29.%20The%0Aeffectiveness%20of%20LEAM%20is%20demonstrated%20by%20three%20examples%3A%20a%20Vivaldi%20antenna%0Agenerated%20from%20a%20complete%20user%20description%2C%20a%20slotted%20patch%20antenna%20generated%0Afrom%20an%20incomplete%20user%20description%20and%20the%20operating%20frequency%2C%20and%20a%20monopole%0Aslotted%20antenna%20generated%20from%20images%20and%20descriptions%20scanned%20from%20the%0Aliterature.%20For%20all%20the%20examples%2C%20correct%20antenna%20models%20are%20generated%20in%20a%20few%0Aminutes.%20The%20code%20can%20be%20accessed%20via%20https%3A//github.com/TaoWu974/LEAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEAM%253A%2520A%2520Prompt-only%2520Large%2520Language%2520Model-enabled%2520Antenna%2520Modeling%2520Method%26entry.906535625%3DTao%2520Wu%2520and%2520Kexue%2520Fu%2520and%2520Qiang%2520Hua%2520and%2520Xinxin%2520Liu%2520and%2520Muhammad%2520Ali%2520Imran%2520and%2520Bo%2520Liu%26entry.1292438233%3D%2520%2520Antenna%2520modeling%2520is%2520a%2520time-consuming%2520and%2520complex%2520process%252C%2520decreasing%2520the%250Aspeed%2520of%2520antenna%2520analysis%2520and%2520design.%2520In%2520this%2520paper%252C%2520a%2520large%2520language%2520model%250A%2528LLM%2529-%2520enabled%2520antenna%2520modeling%2520method%252C%2520called%2520LEAM%252C%2520is%2520presented%2520to%2520address%250Athis%2520challenge.%2520LEAM%2520enables%2520automatic%2520antenna%2520model%2520generation%2520based%2520on%250Alanguage%2520descriptions%2520via%2520prompt%2520input%252C%2520images%252C%2520descriptions%2520from%2520academic%250Apapers%252C%2520patents%252C%2520and%2520technical%2520reports%2520%2528either%2520one%2520or%2520multiple%2529.%2520The%250Aeffectiveness%2520of%2520LEAM%2520is%2520demonstrated%2520by%2520three%2520examples%253A%2520a%2520Vivaldi%2520antenna%250Agenerated%2520from%2520a%2520complete%2520user%2520description%252C%2520a%2520slotted%2520patch%2520antenna%2520generated%250Afrom%2520an%2520incomplete%2520user%2520description%2520and%2520the%2520operating%2520frequency%252C%2520and%2520a%2520monopole%250Aslotted%2520antenna%2520generated%2520from%2520images%2520and%2520descriptions%2520scanned%2520from%2520the%250Aliterature.%2520For%2520all%2520the%2520examples%252C%2520correct%2520antenna%2520models%2520are%2520generated%2520in%2520a%2520few%250Aminutes.%2520The%2520code%2520can%2520be%2520accessed%2520via%2520https%253A//github.com/TaoWu974/LEAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEAM%3A%20A%20Prompt-only%20Large%20Language%20Model-enabled%20Antenna%20Modeling%20Method&entry.906535625=Tao%20Wu%20and%20Kexue%20Fu%20and%20Qiang%20Hua%20and%20Xinxin%20Liu%20and%20Muhammad%20Ali%20Imran%20and%20Bo%20Liu&entry.1292438233=%20%20Antenna%20modeling%20is%20a%20time-consuming%20and%20complex%20process%2C%20decreasing%20the%0Aspeed%20of%20antenna%20analysis%20and%20design.%20In%20this%20paper%2C%20a%20large%20language%20model%0A%28LLM%29-%20enabled%20antenna%20modeling%20method%2C%20called%20LEAM%2C%20is%20presented%20to%20address%0Athis%20challenge.%20LEAM%20enables%20automatic%20antenna%20model%20generation%20based%20on%0Alanguage%20descriptions%20via%20prompt%20input%2C%20images%2C%20descriptions%20from%20academic%0Apapers%2C%20patents%2C%20and%20technical%20reports%20%28either%20one%20or%20multiple%29.%20The%0Aeffectiveness%20of%20LEAM%20is%20demonstrated%20by%20three%20examples%3A%20a%20Vivaldi%20antenna%0Agenerated%20from%20a%20complete%20user%20description%2C%20a%20slotted%20patch%20antenna%20generated%0Afrom%20an%20incomplete%20user%20description%20and%20the%20operating%20frequency%2C%20and%20a%20monopole%0Aslotted%20antenna%20generated%20from%20images%20and%20descriptions%20scanned%20from%20the%0Aliterature.%20For%20all%20the%20examples%2C%20correct%20antenna%20models%20are%20generated%20in%20a%20few%0Aminutes.%20The%20code%20can%20be%20accessed%20via%20https%3A//github.com/TaoWu974/LEAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18271v1&entry.124074799=Read"},
{"title": "Boxi: Design Decisions in the Context of Algorithmic Performance for\n  Robotics", "author": "Jonas Frey and Turcan Tuna and Lanke Frank Tarimo Fu and Cedric Weibel and Katharine Patterson and Benjamin Krummenacher and Matthias M\u00fcller and Julian Nubert and Maurice Fallon and Cesar Cadena and Marco Hutter", "abstract": "  Achieving robust autonomy in mobile robots operating in complex and\nunstructured environments requires a multimodal sensor suite capable of\ncapturing diverse and complementary information. However, designing such a\nsensor suite involves multiple critical design decisions, such as sensor\nselection, component placement, thermal and power limitations, compute\nrequirements, networking, synchronization, and calibration. While the\nimportance of these key aspects is widely recognized, they are often overlooked\nin academia or retained as proprietary knowledge within large corporations. To\nimprove this situation, we present Boxi, a tightly integrated sensor payload\nthat enables robust autonomy of robots in the wild. This paper discusses the\nimpact of payload design decisions made to optimize algorithmic performance for\ndownstream tasks, specifically focusing on state estimation and mapping. Boxi\nis equipped with a variety of sensors: two LiDARs, 10 RGB cameras including\nhigh-dynamic range, global shutter, and rolling shutter models, an RGB-D\ncamera, 7 inertial measurement units (IMUs) of varying precision, and a dual\nantenna RTK GNSS system. Our analysis shows that time synchronization,\ncalibration, and sensor modality have a crucial impact on the state estimation\nperformance. We frame this analysis in the context of cost considerations and\nenvironment-specific challenges. We also present a mobile sensor suite\n`cookbook` to serve as a comprehensive guideline, highlighting generalizable\nkey design considerations and lessons learned during the development of Boxi.\nFinally, we demonstrate the versatility of Boxi being used in a variety of\napplications in real-world scenarios, contributing to robust autonomy. More\ndetails and code: https://github.com/leggedrobotics/grand_tour_box\n", "link": "http://arxiv.org/abs/2504.18500v1", "date": "2025-04-25", "relevancy": 2.2224, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5921}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5525}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boxi%3A%20Design%20Decisions%20in%20the%20Context%20of%20Algorithmic%20Performance%20for%0A%20%20Robotics&body=Title%3A%20Boxi%3A%20Design%20Decisions%20in%20the%20Context%20of%20Algorithmic%20Performance%20for%0A%20%20Robotics%0AAuthor%3A%20Jonas%20Frey%20and%20Turcan%20Tuna%20and%20Lanke%20Frank%20Tarimo%20Fu%20and%20Cedric%20Weibel%20and%20Katharine%20Patterson%20and%20Benjamin%20Krummenacher%20and%20Matthias%20M%C3%BCller%20and%20Julian%20Nubert%20and%20Maurice%20Fallon%20and%20Cesar%20Cadena%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Achieving%20robust%20autonomy%20in%20mobile%20robots%20operating%20in%20complex%20and%0Aunstructured%20environments%20requires%20a%20multimodal%20sensor%20suite%20capable%20of%0Acapturing%20diverse%20and%20complementary%20information.%20However%2C%20designing%20such%20a%0Asensor%20suite%20involves%20multiple%20critical%20design%20decisions%2C%20such%20as%20sensor%0Aselection%2C%20component%20placement%2C%20thermal%20and%20power%20limitations%2C%20compute%0Arequirements%2C%20networking%2C%20synchronization%2C%20and%20calibration.%20While%20the%0Aimportance%20of%20these%20key%20aspects%20is%20widely%20recognized%2C%20they%20are%20often%20overlooked%0Ain%20academia%20or%20retained%20as%20proprietary%20knowledge%20within%20large%20corporations.%20To%0Aimprove%20this%20situation%2C%20we%20present%20Boxi%2C%20a%20tightly%20integrated%20sensor%20payload%0Athat%20enables%20robust%20autonomy%20of%20robots%20in%20the%20wild.%20This%20paper%20discusses%20the%0Aimpact%20of%20payload%20design%20decisions%20made%20to%20optimize%20algorithmic%20performance%20for%0Adownstream%20tasks%2C%20specifically%20focusing%20on%20state%20estimation%20and%20mapping.%20Boxi%0Ais%20equipped%20with%20a%20variety%20of%20sensors%3A%20two%20LiDARs%2C%2010%20RGB%20cameras%20including%0Ahigh-dynamic%20range%2C%20global%20shutter%2C%20and%20rolling%20shutter%20models%2C%20an%20RGB-D%0Acamera%2C%207%20inertial%20measurement%20units%20%28IMUs%29%20of%20varying%20precision%2C%20and%20a%20dual%0Aantenna%20RTK%20GNSS%20system.%20Our%20analysis%20shows%20that%20time%20synchronization%2C%0Acalibration%2C%20and%20sensor%20modality%20have%20a%20crucial%20impact%20on%20the%20state%20estimation%0Aperformance.%20We%20frame%20this%20analysis%20in%20the%20context%20of%20cost%20considerations%20and%0Aenvironment-specific%20challenges.%20We%20also%20present%20a%20mobile%20sensor%20suite%0A%60cookbook%60%20to%20serve%20as%20a%20comprehensive%20guideline%2C%20highlighting%20generalizable%0Akey%20design%20considerations%20and%20lessons%20learned%20during%20the%20development%20of%20Boxi.%0AFinally%2C%20we%20demonstrate%20the%20versatility%20of%20Boxi%20being%20used%20in%20a%20variety%20of%0Aapplications%20in%20real-world%20scenarios%2C%20contributing%20to%20robust%20autonomy.%20More%0Adetails%20and%20code%3A%20https%3A//github.com/leggedrobotics/grand_tour_box%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoxi%253A%2520Design%2520Decisions%2520in%2520the%2520Context%2520of%2520Algorithmic%2520Performance%2520for%250A%2520%2520Robotics%26entry.906535625%3DJonas%2520Frey%2520and%2520Turcan%2520Tuna%2520and%2520Lanke%2520Frank%2520Tarimo%2520Fu%2520and%2520Cedric%2520Weibel%2520and%2520Katharine%2520Patterson%2520and%2520Benjamin%2520Krummenacher%2520and%2520Matthias%2520M%25C3%25BCller%2520and%2520Julian%2520Nubert%2520and%2520Maurice%2520Fallon%2520and%2520Cesar%2520Cadena%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Achieving%2520robust%2520autonomy%2520in%2520mobile%2520robots%2520operating%2520in%2520complex%2520and%250Aunstructured%2520environments%2520requires%2520a%2520multimodal%2520sensor%2520suite%2520capable%2520of%250Acapturing%2520diverse%2520and%2520complementary%2520information.%2520However%252C%2520designing%2520such%2520a%250Asensor%2520suite%2520involves%2520multiple%2520critical%2520design%2520decisions%252C%2520such%2520as%2520sensor%250Aselection%252C%2520component%2520placement%252C%2520thermal%2520and%2520power%2520limitations%252C%2520compute%250Arequirements%252C%2520networking%252C%2520synchronization%252C%2520and%2520calibration.%2520While%2520the%250Aimportance%2520of%2520these%2520key%2520aspects%2520is%2520widely%2520recognized%252C%2520they%2520are%2520often%2520overlooked%250Ain%2520academia%2520or%2520retained%2520as%2520proprietary%2520knowledge%2520within%2520large%2520corporations.%2520To%250Aimprove%2520this%2520situation%252C%2520we%2520present%2520Boxi%252C%2520a%2520tightly%2520integrated%2520sensor%2520payload%250Athat%2520enables%2520robust%2520autonomy%2520of%2520robots%2520in%2520the%2520wild.%2520This%2520paper%2520discusses%2520the%250Aimpact%2520of%2520payload%2520design%2520decisions%2520made%2520to%2520optimize%2520algorithmic%2520performance%2520for%250Adownstream%2520tasks%252C%2520specifically%2520focusing%2520on%2520state%2520estimation%2520and%2520mapping.%2520Boxi%250Ais%2520equipped%2520with%2520a%2520variety%2520of%2520sensors%253A%2520two%2520LiDARs%252C%252010%2520RGB%2520cameras%2520including%250Ahigh-dynamic%2520range%252C%2520global%2520shutter%252C%2520and%2520rolling%2520shutter%2520models%252C%2520an%2520RGB-D%250Acamera%252C%25207%2520inertial%2520measurement%2520units%2520%2528IMUs%2529%2520of%2520varying%2520precision%252C%2520and%2520a%2520dual%250Aantenna%2520RTK%2520GNSS%2520system.%2520Our%2520analysis%2520shows%2520that%2520time%2520synchronization%252C%250Acalibration%252C%2520and%2520sensor%2520modality%2520have%2520a%2520crucial%2520impact%2520on%2520the%2520state%2520estimation%250Aperformance.%2520We%2520frame%2520this%2520analysis%2520in%2520the%2520context%2520of%2520cost%2520considerations%2520and%250Aenvironment-specific%2520challenges.%2520We%2520also%2520present%2520a%2520mobile%2520sensor%2520suite%250A%2560cookbook%2560%2520to%2520serve%2520as%2520a%2520comprehensive%2520guideline%252C%2520highlighting%2520generalizable%250Akey%2520design%2520considerations%2520and%2520lessons%2520learned%2520during%2520the%2520development%2520of%2520Boxi.%250AFinally%252C%2520we%2520demonstrate%2520the%2520versatility%2520of%2520Boxi%2520being%2520used%2520in%2520a%2520variety%2520of%250Aapplications%2520in%2520real-world%2520scenarios%252C%2520contributing%2520to%2520robust%2520autonomy.%2520More%250Adetails%2520and%2520code%253A%2520https%253A//github.com/leggedrobotics/grand_tour_box%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boxi%3A%20Design%20Decisions%20in%20the%20Context%20of%20Algorithmic%20Performance%20for%0A%20%20Robotics&entry.906535625=Jonas%20Frey%20and%20Turcan%20Tuna%20and%20Lanke%20Frank%20Tarimo%20Fu%20and%20Cedric%20Weibel%20and%20Katharine%20Patterson%20and%20Benjamin%20Krummenacher%20and%20Matthias%20M%C3%BCller%20and%20Julian%20Nubert%20and%20Maurice%20Fallon%20and%20Cesar%20Cadena%20and%20Marco%20Hutter&entry.1292438233=%20%20Achieving%20robust%20autonomy%20in%20mobile%20robots%20operating%20in%20complex%20and%0Aunstructured%20environments%20requires%20a%20multimodal%20sensor%20suite%20capable%20of%0Acapturing%20diverse%20and%20complementary%20information.%20However%2C%20designing%20such%20a%0Asensor%20suite%20involves%20multiple%20critical%20design%20decisions%2C%20such%20as%20sensor%0Aselection%2C%20component%20placement%2C%20thermal%20and%20power%20limitations%2C%20compute%0Arequirements%2C%20networking%2C%20synchronization%2C%20and%20calibration.%20While%20the%0Aimportance%20of%20these%20key%20aspects%20is%20widely%20recognized%2C%20they%20are%20often%20overlooked%0Ain%20academia%20or%20retained%20as%20proprietary%20knowledge%20within%20large%20corporations.%20To%0Aimprove%20this%20situation%2C%20we%20present%20Boxi%2C%20a%20tightly%20integrated%20sensor%20payload%0Athat%20enables%20robust%20autonomy%20of%20robots%20in%20the%20wild.%20This%20paper%20discusses%20the%0Aimpact%20of%20payload%20design%20decisions%20made%20to%20optimize%20algorithmic%20performance%20for%0Adownstream%20tasks%2C%20specifically%20focusing%20on%20state%20estimation%20and%20mapping.%20Boxi%0Ais%20equipped%20with%20a%20variety%20of%20sensors%3A%20two%20LiDARs%2C%2010%20RGB%20cameras%20including%0Ahigh-dynamic%20range%2C%20global%20shutter%2C%20and%20rolling%20shutter%20models%2C%20an%20RGB-D%0Acamera%2C%207%20inertial%20measurement%20units%20%28IMUs%29%20of%20varying%20precision%2C%20and%20a%20dual%0Aantenna%20RTK%20GNSS%20system.%20Our%20analysis%20shows%20that%20time%20synchronization%2C%0Acalibration%2C%20and%20sensor%20modality%20have%20a%20crucial%20impact%20on%20the%20state%20estimation%0Aperformance.%20We%20frame%20this%20analysis%20in%20the%20context%20of%20cost%20considerations%20and%0Aenvironment-specific%20challenges.%20We%20also%20present%20a%20mobile%20sensor%20suite%0A%60cookbook%60%20to%20serve%20as%20a%20comprehensive%20guideline%2C%20highlighting%20generalizable%0Akey%20design%20considerations%20and%20lessons%20learned%20during%20the%20development%20of%20Boxi.%0AFinally%2C%20we%20demonstrate%20the%20versatility%20of%20Boxi%20being%20used%20in%20a%20variety%20of%0Aapplications%20in%20real-world%20scenarios%2C%20contributing%20to%20robust%20autonomy.%20More%0Adetails%20and%20code%3A%20https%3A//github.com/leggedrobotics/grand_tour_box%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18500v1&entry.124074799=Read"},
{"title": "Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph\n  Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems", "author": "Vinay Sharma and R\u00e9mi Tanguy Oddon and Pietro Tesini and Jens Ravesloot and Cees Taal and Olga Fink", "abstract": "  Accurate real-time modeling of multi-body dynamical systems is essential for\nenabling digital twin applications across industries. While many data-driven\napproaches aim to learn system dynamics, jointly predicting internal loads and\nsystem trajectories remains a key challenge. This dual prediction is especially\nimportant for fault detection and predictive maintenance, where internal\nloads-such as contact forces-act as early indicators of faults, reflecting wear\nor misalignment before affecting motion. These forces also serve as inputs to\ndegradation models (e.g., crack growth), enabling damage prediction and\nremaining useful life estimation. We propose Equi-Euler GraphNet, a\nphysics-informed graph neural network (GNN) that simultaneously predicts\ninternal forces and global trajectories in multi-body systems. In this\nmesh-free framework, nodes represent system components and edges encode\ninteractions. Equi-Euler GraphNet introduces two inductive biases: (1) an\nequivariant message-passing scheme, interpreting edge messages as interaction\nforces consistent under Euclidean transformations; and (2) a temporal-aware\niterative node update mechanism, based on Euler integration, to capture\ninfluence of distant interactions over time. Tailored for cylindrical roller\nbearings, it decouples ring dynamics from constrained motion of rolling\nelements. Trained on high-fidelity multiphysics simulations, Equi-Euler\nGraphNet generalizes beyond the training distribution, accurately predicting\nloads and trajectories under unseen speeds, loads, and configurations. It\noutperforms state-of-the-art GNNs focused on trajectory prediction, delivering\nstable rollouts over thousands of time steps with minimal error accumulation.\nAchieving up to a 200x speedup over conventional solvers while maintaining\ncomparable accuracy, it serves as an efficient reduced-order model for digital\ntwins, design, and maintenance.\n", "link": "http://arxiv.org/abs/2504.13768v3", "date": "2025-04-25", "relevancy": 2.209, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5556}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equi-Euler%20GraphNet%3A%20An%20Equivariant%2C%20Temporal-Dynamics%20Informed%20Graph%0A%20%20Neural%20Network%20for%20Dual%20Force%20and%20Trajectory%20Prediction%20in%20Multi-Body%20Systems&body=Title%3A%20Equi-Euler%20GraphNet%3A%20An%20Equivariant%2C%20Temporal-Dynamics%20Informed%20Graph%0A%20%20Neural%20Network%20for%20Dual%20Force%20and%20Trajectory%20Prediction%20in%20Multi-Body%20Systems%0AAuthor%3A%20Vinay%20Sharma%20and%20R%C3%A9mi%20Tanguy%20Oddon%20and%20Pietro%20Tesini%20and%20Jens%20Ravesloot%20and%20Cees%20Taal%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Accurate%20real-time%20modeling%20of%20multi-body%20dynamical%20systems%20is%20essential%20for%0Aenabling%20digital%20twin%20applications%20across%20industries.%20While%20many%20data-driven%0Aapproaches%20aim%20to%20learn%20system%20dynamics%2C%20jointly%20predicting%20internal%20loads%20and%0Asystem%20trajectories%20remains%20a%20key%20challenge.%20This%20dual%20prediction%20is%20especially%0Aimportant%20for%20fault%20detection%20and%20predictive%20maintenance%2C%20where%20internal%0Aloads-such%20as%20contact%20forces-act%20as%20early%20indicators%20of%20faults%2C%20reflecting%20wear%0Aor%20misalignment%20before%20affecting%20motion.%20These%20forces%20also%20serve%20as%20inputs%20to%0Adegradation%20models%20%28e.g.%2C%20crack%20growth%29%2C%20enabling%20damage%20prediction%20and%0Aremaining%20useful%20life%20estimation.%20We%20propose%20Equi-Euler%20GraphNet%2C%20a%0Aphysics-informed%20graph%20neural%20network%20%28GNN%29%20that%20simultaneously%20predicts%0Ainternal%20forces%20and%20global%20trajectories%20in%20multi-body%20systems.%20In%20this%0Amesh-free%20framework%2C%20nodes%20represent%20system%20components%20and%20edges%20encode%0Ainteractions.%20Equi-Euler%20GraphNet%20introduces%20two%20inductive%20biases%3A%20%281%29%20an%0Aequivariant%20message-passing%20scheme%2C%20interpreting%20edge%20messages%20as%20interaction%0Aforces%20consistent%20under%20Euclidean%20transformations%3B%20and%20%282%29%20a%20temporal-aware%0Aiterative%20node%20update%20mechanism%2C%20based%20on%20Euler%20integration%2C%20to%20capture%0Ainfluence%20of%20distant%20interactions%20over%20time.%20Tailored%20for%20cylindrical%20roller%0Abearings%2C%20it%20decouples%20ring%20dynamics%20from%20constrained%20motion%20of%20rolling%0Aelements.%20Trained%20on%20high-fidelity%20multiphysics%20simulations%2C%20Equi-Euler%0AGraphNet%20generalizes%20beyond%20the%20training%20distribution%2C%20accurately%20predicting%0Aloads%20and%20trajectories%20under%20unseen%20speeds%2C%20loads%2C%20and%20configurations.%20It%0Aoutperforms%20state-of-the-art%20GNNs%20focused%20on%20trajectory%20prediction%2C%20delivering%0Astable%20rollouts%20over%20thousands%20of%20time%20steps%20with%20minimal%20error%20accumulation.%0AAchieving%20up%20to%20a%20200x%20speedup%20over%20conventional%20solvers%20while%20maintaining%0Acomparable%20accuracy%2C%20it%20serves%20as%20an%20efficient%20reduced-order%20model%20for%20digital%0Atwins%2C%20design%2C%20and%20maintenance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13768v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEqui-Euler%2520GraphNet%253A%2520An%2520Equivariant%252C%2520Temporal-Dynamics%2520Informed%2520Graph%250A%2520%2520Neural%2520Network%2520for%2520Dual%2520Force%2520and%2520Trajectory%2520Prediction%2520in%2520Multi-Body%2520Systems%26entry.906535625%3DVinay%2520Sharma%2520and%2520R%25C3%25A9mi%2520Tanguy%2520Oddon%2520and%2520Pietro%2520Tesini%2520and%2520Jens%2520Ravesloot%2520and%2520Cees%2520Taal%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Accurate%2520real-time%2520modeling%2520of%2520multi-body%2520dynamical%2520systems%2520is%2520essential%2520for%250Aenabling%2520digital%2520twin%2520applications%2520across%2520industries.%2520While%2520many%2520data-driven%250Aapproaches%2520aim%2520to%2520learn%2520system%2520dynamics%252C%2520jointly%2520predicting%2520internal%2520loads%2520and%250Asystem%2520trajectories%2520remains%2520a%2520key%2520challenge.%2520This%2520dual%2520prediction%2520is%2520especially%250Aimportant%2520for%2520fault%2520detection%2520and%2520predictive%2520maintenance%252C%2520where%2520internal%250Aloads-such%2520as%2520contact%2520forces-act%2520as%2520early%2520indicators%2520of%2520faults%252C%2520reflecting%2520wear%250Aor%2520misalignment%2520before%2520affecting%2520motion.%2520These%2520forces%2520also%2520serve%2520as%2520inputs%2520to%250Adegradation%2520models%2520%2528e.g.%252C%2520crack%2520growth%2529%252C%2520enabling%2520damage%2520prediction%2520and%250Aremaining%2520useful%2520life%2520estimation.%2520We%2520propose%2520Equi-Euler%2520GraphNet%252C%2520a%250Aphysics-informed%2520graph%2520neural%2520network%2520%2528GNN%2529%2520that%2520simultaneously%2520predicts%250Ainternal%2520forces%2520and%2520global%2520trajectories%2520in%2520multi-body%2520systems.%2520In%2520this%250Amesh-free%2520framework%252C%2520nodes%2520represent%2520system%2520components%2520and%2520edges%2520encode%250Ainteractions.%2520Equi-Euler%2520GraphNet%2520introduces%2520two%2520inductive%2520biases%253A%2520%25281%2529%2520an%250Aequivariant%2520message-passing%2520scheme%252C%2520interpreting%2520edge%2520messages%2520as%2520interaction%250Aforces%2520consistent%2520under%2520Euclidean%2520transformations%253B%2520and%2520%25282%2529%2520a%2520temporal-aware%250Aiterative%2520node%2520update%2520mechanism%252C%2520based%2520on%2520Euler%2520integration%252C%2520to%2520capture%250Ainfluence%2520of%2520distant%2520interactions%2520over%2520time.%2520Tailored%2520for%2520cylindrical%2520roller%250Abearings%252C%2520it%2520decouples%2520ring%2520dynamics%2520from%2520constrained%2520motion%2520of%2520rolling%250Aelements.%2520Trained%2520on%2520high-fidelity%2520multiphysics%2520simulations%252C%2520Equi-Euler%250AGraphNet%2520generalizes%2520beyond%2520the%2520training%2520distribution%252C%2520accurately%2520predicting%250Aloads%2520and%2520trajectories%2520under%2520unseen%2520speeds%252C%2520loads%252C%2520and%2520configurations.%2520It%250Aoutperforms%2520state-of-the-art%2520GNNs%2520focused%2520on%2520trajectory%2520prediction%252C%2520delivering%250Astable%2520rollouts%2520over%2520thousands%2520of%2520time%2520steps%2520with%2520minimal%2520error%2520accumulation.%250AAchieving%2520up%2520to%2520a%2520200x%2520speedup%2520over%2520conventional%2520solvers%2520while%2520maintaining%250Acomparable%2520accuracy%252C%2520it%2520serves%2520as%2520an%2520efficient%2520reduced-order%2520model%2520for%2520digital%250Atwins%252C%2520design%252C%2520and%2520maintenance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13768v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equi-Euler%20GraphNet%3A%20An%20Equivariant%2C%20Temporal-Dynamics%20Informed%20Graph%0A%20%20Neural%20Network%20for%20Dual%20Force%20and%20Trajectory%20Prediction%20in%20Multi-Body%20Systems&entry.906535625=Vinay%20Sharma%20and%20R%C3%A9mi%20Tanguy%20Oddon%20and%20Pietro%20Tesini%20and%20Jens%20Ravesloot%20and%20Cees%20Taal%20and%20Olga%20Fink&entry.1292438233=%20%20Accurate%20real-time%20modeling%20of%20multi-body%20dynamical%20systems%20is%20essential%20for%0Aenabling%20digital%20twin%20applications%20across%20industries.%20While%20many%20data-driven%0Aapproaches%20aim%20to%20learn%20system%20dynamics%2C%20jointly%20predicting%20internal%20loads%20and%0Asystem%20trajectories%20remains%20a%20key%20challenge.%20This%20dual%20prediction%20is%20especially%0Aimportant%20for%20fault%20detection%20and%20predictive%20maintenance%2C%20where%20internal%0Aloads-such%20as%20contact%20forces-act%20as%20early%20indicators%20of%20faults%2C%20reflecting%20wear%0Aor%20misalignment%20before%20affecting%20motion.%20These%20forces%20also%20serve%20as%20inputs%20to%0Adegradation%20models%20%28e.g.%2C%20crack%20growth%29%2C%20enabling%20damage%20prediction%20and%0Aremaining%20useful%20life%20estimation.%20We%20propose%20Equi-Euler%20GraphNet%2C%20a%0Aphysics-informed%20graph%20neural%20network%20%28GNN%29%20that%20simultaneously%20predicts%0Ainternal%20forces%20and%20global%20trajectories%20in%20multi-body%20systems.%20In%20this%0Amesh-free%20framework%2C%20nodes%20represent%20system%20components%20and%20edges%20encode%0Ainteractions.%20Equi-Euler%20GraphNet%20introduces%20two%20inductive%20biases%3A%20%281%29%20an%0Aequivariant%20message-passing%20scheme%2C%20interpreting%20edge%20messages%20as%20interaction%0Aforces%20consistent%20under%20Euclidean%20transformations%3B%20and%20%282%29%20a%20temporal-aware%0Aiterative%20node%20update%20mechanism%2C%20based%20on%20Euler%20integration%2C%20to%20capture%0Ainfluence%20of%20distant%20interactions%20over%20time.%20Tailored%20for%20cylindrical%20roller%0Abearings%2C%20it%20decouples%20ring%20dynamics%20from%20constrained%20motion%20of%20rolling%0Aelements.%20Trained%20on%20high-fidelity%20multiphysics%20simulations%2C%20Equi-Euler%0AGraphNet%20generalizes%20beyond%20the%20training%20distribution%2C%20accurately%20predicting%0Aloads%20and%20trajectories%20under%20unseen%20speeds%2C%20loads%2C%20and%20configurations.%20It%0Aoutperforms%20state-of-the-art%20GNNs%20focused%20on%20trajectory%20prediction%2C%20delivering%0Astable%20rollouts%20over%20thousands%20of%20time%20steps%20with%20minimal%20error%20accumulation.%0AAchieving%20up%20to%20a%20200x%20speedup%20over%20conventional%20solvers%20while%20maintaining%0Acomparable%20accuracy%2C%20it%20serves%20as%20an%20efficient%20reduced-order%20model%20for%20digital%0Atwins%2C%20design%2C%20and%20maintenance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13768v3&entry.124074799=Read"},
{"title": "Generalization Capability for Imitation Learning", "author": "Yixiao Wang", "abstract": "  Imitation learning holds the promise of equipping robots with versatile\nskills by learning from expert demonstrations. However, policies trained on\nfinite datasets often struggle to generalize beyond the training distribution.\nIn this work, we present a unified perspective on the generalization capability\nof imitation learning, grounded in both information theorey and data\ndistribution property. We first show that the generalization gap can be upper\nbounded by (i) the conditional information bottleneck on intermediate\nrepresentations and (ii) the mutual information between the model parameters\nand the training dataset. This characterization provides theoretical guidance\nfor designing effective training strategies in imitation learning, particularly\nin determining whether to freeze, fine-tune, or train large pretrained encoders\n(e.g., vision-language models or vision foundation models) from scratch to\nachieve better generalization. Furthermore, we demonstrate that high\nconditional entropy from input to output induces a flatter likelihood\nlandscape, thereby reducing the upper bound on the generalization gap. In\naddition, it shortens the stochastic gradient descent (SGD) escape time from\nsharp local minima, which may increase the likelihood of reaching global optima\nunder fixed optimization budgets. These insights explain why imitation learning\noften exhibits limited generalization and underscore the importance of not only\nscaling the diversity of input data but also enriching the variability of\noutput labels conditioned on the same input.\n", "link": "http://arxiv.org/abs/2504.18538v1", "date": "2025-04-25", "relevancy": 2.2043, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5609}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5537}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20Capability%20for%20Imitation%20Learning&body=Title%3A%20Generalization%20Capability%20for%20Imitation%20Learning%0AAuthor%3A%20Yixiao%20Wang%0AAbstract%3A%20%20%20Imitation%20learning%20holds%20the%20promise%20of%20equipping%20robots%20with%20versatile%0Askills%20by%20learning%20from%20expert%20demonstrations.%20However%2C%20policies%20trained%20on%0Afinite%20datasets%20often%20struggle%20to%20generalize%20beyond%20the%20training%20distribution.%0AIn%20this%20work%2C%20we%20present%20a%20unified%20perspective%20on%20the%20generalization%20capability%0Aof%20imitation%20learning%2C%20grounded%20in%20both%20information%20theorey%20and%20data%0Adistribution%20property.%20We%20first%20show%20that%20the%20generalization%20gap%20can%20be%20upper%0Abounded%20by%20%28i%29%20the%20conditional%20information%20bottleneck%20on%20intermediate%0Arepresentations%20and%20%28ii%29%20the%20mutual%20information%20between%20the%20model%20parameters%0Aand%20the%20training%20dataset.%20This%20characterization%20provides%20theoretical%20guidance%0Afor%20designing%20effective%20training%20strategies%20in%20imitation%20learning%2C%20particularly%0Ain%20determining%20whether%20to%20freeze%2C%20fine-tune%2C%20or%20train%20large%20pretrained%20encoders%0A%28e.g.%2C%20vision-language%20models%20or%20vision%20foundation%20models%29%20from%20scratch%20to%0Aachieve%20better%20generalization.%20Furthermore%2C%20we%20demonstrate%20that%20high%0Aconditional%20entropy%20from%20input%20to%20output%20induces%20a%20flatter%20likelihood%0Alandscape%2C%20thereby%20reducing%20the%20upper%20bound%20on%20the%20generalization%20gap.%20In%0Aaddition%2C%20it%20shortens%20the%20stochastic%20gradient%20descent%20%28SGD%29%20escape%20time%20from%0Asharp%20local%20minima%2C%20which%20may%20increase%20the%20likelihood%20of%20reaching%20global%20optima%0Aunder%20fixed%20optimization%20budgets.%20These%20insights%20explain%20why%20imitation%20learning%0Aoften%20exhibits%20limited%20generalization%20and%20underscore%20the%20importance%20of%20not%20only%0Ascaling%20the%20diversity%20of%20input%20data%20but%20also%20enriching%20the%20variability%20of%0Aoutput%20labels%20conditioned%20on%20the%20same%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520Capability%2520for%2520Imitation%2520Learning%26entry.906535625%3DYixiao%2520Wang%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520holds%2520the%2520promise%2520of%2520equipping%2520robots%2520with%2520versatile%250Askills%2520by%2520learning%2520from%2520expert%2520demonstrations.%2520However%252C%2520policies%2520trained%2520on%250Afinite%2520datasets%2520often%2520struggle%2520to%2520generalize%2520beyond%2520the%2520training%2520distribution.%250AIn%2520this%2520work%252C%2520we%2520present%2520a%2520unified%2520perspective%2520on%2520the%2520generalization%2520capability%250Aof%2520imitation%2520learning%252C%2520grounded%2520in%2520both%2520information%2520theorey%2520and%2520data%250Adistribution%2520property.%2520We%2520first%2520show%2520that%2520the%2520generalization%2520gap%2520can%2520be%2520upper%250Abounded%2520by%2520%2528i%2529%2520the%2520conditional%2520information%2520bottleneck%2520on%2520intermediate%250Arepresentations%2520and%2520%2528ii%2529%2520the%2520mutual%2520information%2520between%2520the%2520model%2520parameters%250Aand%2520the%2520training%2520dataset.%2520This%2520characterization%2520provides%2520theoretical%2520guidance%250Afor%2520designing%2520effective%2520training%2520strategies%2520in%2520imitation%2520learning%252C%2520particularly%250Ain%2520determining%2520whether%2520to%2520freeze%252C%2520fine-tune%252C%2520or%2520train%2520large%2520pretrained%2520encoders%250A%2528e.g.%252C%2520vision-language%2520models%2520or%2520vision%2520foundation%2520models%2529%2520from%2520scratch%2520to%250Aachieve%2520better%2520generalization.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520high%250Aconditional%2520entropy%2520from%2520input%2520to%2520output%2520induces%2520a%2520flatter%2520likelihood%250Alandscape%252C%2520thereby%2520reducing%2520the%2520upper%2520bound%2520on%2520the%2520generalization%2520gap.%2520In%250Aaddition%252C%2520it%2520shortens%2520the%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520escape%2520time%2520from%250Asharp%2520local%2520minima%252C%2520which%2520may%2520increase%2520the%2520likelihood%2520of%2520reaching%2520global%2520optima%250Aunder%2520fixed%2520optimization%2520budgets.%2520These%2520insights%2520explain%2520why%2520imitation%2520learning%250Aoften%2520exhibits%2520limited%2520generalization%2520and%2520underscore%2520the%2520importance%2520of%2520not%2520only%250Ascaling%2520the%2520diversity%2520of%2520input%2520data%2520but%2520also%2520enriching%2520the%2520variability%2520of%250Aoutput%2520labels%2520conditioned%2520on%2520the%2520same%2520input.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20Capability%20for%20Imitation%20Learning&entry.906535625=Yixiao%20Wang&entry.1292438233=%20%20Imitation%20learning%20holds%20the%20promise%20of%20equipping%20robots%20with%20versatile%0Askills%20by%20learning%20from%20expert%20demonstrations.%20However%2C%20policies%20trained%20on%0Afinite%20datasets%20often%20struggle%20to%20generalize%20beyond%20the%20training%20distribution.%0AIn%20this%20work%2C%20we%20present%20a%20unified%20perspective%20on%20the%20generalization%20capability%0Aof%20imitation%20learning%2C%20grounded%20in%20both%20information%20theorey%20and%20data%0Adistribution%20property.%20We%20first%20show%20that%20the%20generalization%20gap%20can%20be%20upper%0Abounded%20by%20%28i%29%20the%20conditional%20information%20bottleneck%20on%20intermediate%0Arepresentations%20and%20%28ii%29%20the%20mutual%20information%20between%20the%20model%20parameters%0Aand%20the%20training%20dataset.%20This%20characterization%20provides%20theoretical%20guidance%0Afor%20designing%20effective%20training%20strategies%20in%20imitation%20learning%2C%20particularly%0Ain%20determining%20whether%20to%20freeze%2C%20fine-tune%2C%20or%20train%20large%20pretrained%20encoders%0A%28e.g.%2C%20vision-language%20models%20or%20vision%20foundation%20models%29%20from%20scratch%20to%0Aachieve%20better%20generalization.%20Furthermore%2C%20we%20demonstrate%20that%20high%0Aconditional%20entropy%20from%20input%20to%20output%20induces%20a%20flatter%20likelihood%0Alandscape%2C%20thereby%20reducing%20the%20upper%20bound%20on%20the%20generalization%20gap.%20In%0Aaddition%2C%20it%20shortens%20the%20stochastic%20gradient%20descent%20%28SGD%29%20escape%20time%20from%0Asharp%20local%20minima%2C%20which%20may%20increase%20the%20likelihood%20of%20reaching%20global%20optima%0Aunder%20fixed%20optimization%20budgets.%20These%20insights%20explain%20why%20imitation%20learning%0Aoften%20exhibits%20limited%20generalization%20and%20underscore%20the%20importance%20of%20not%20only%0Ascaling%20the%20diversity%20of%20input%20data%20but%20also%20enriching%20the%20variability%20of%0Aoutput%20labels%20conditioned%20on%20the%20same%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18538v1&entry.124074799=Read"},
{"title": "Action-Minimization Meets Generative Modeling: Efficient Transition Path\n  Sampling with the Onsager-Machlup Functional", "author": "Sanjeev Raja and Martin \u0160\u00edpka and Michael Psenka and Tobias Kreiman and Michal Pavelka and Aditi S. Krishnapriyan", "abstract": "  Transition path sampling (TPS), which involves finding probable paths\nconnecting two points on an energy landscape, remains a challenge due to the\ncomplexity of real-world atomistic systems. Current machine learning approaches\nuse expensive, task-specific, and data-free training procedures, limiting their\nability to benefit from recent advances in atomistic machine learning, such as\nhigh-quality datasets and large-scale pre-trained models. In this work, we\naddress TPS by interpreting candidate paths as trajectories sampled from\nstochastic dynamics induced by the learned score function of pre-trained\ngenerative models, specifically denoising diffusion and flow matching. Under\nthese dynamics, finding high-likelihood transition paths becomes equivalent to\nminimizing the Onsager-Machlup (OM) action functional. This enables us to\nrepurpose pre-trained generative models for TPS in a zero-shot manner, in\ncontrast with bespoke, task-specific TPS models trained in previous work. We\ndemonstrate our approach on varied molecular systems, obtaining diverse,\nphysically realistic transition pathways and generalizing beyond the\npre-trained model's original training dataset. Our method can be easily\nincorporated into new generative models, making it practically relevant as\nmodels continue to scale and improve with increased data availability.\n", "link": "http://arxiv.org/abs/2504.18506v1", "date": "2025-04-25", "relevancy": 2.1925, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5557}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5548}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action-Minimization%20Meets%20Generative%20Modeling%3A%20Efficient%20Transition%20Path%0A%20%20Sampling%20with%20the%20Onsager-Machlup%20Functional&body=Title%3A%20Action-Minimization%20Meets%20Generative%20Modeling%3A%20Efficient%20Transition%20Path%0A%20%20Sampling%20with%20the%20Onsager-Machlup%20Functional%0AAuthor%3A%20Sanjeev%20Raja%20and%20Martin%20%C5%A0%C3%ADpka%20and%20Michael%20Psenka%20and%20Tobias%20Kreiman%20and%20Michal%20Pavelka%20and%20Aditi%20S.%20Krishnapriyan%0AAbstract%3A%20%20%20Transition%20path%20sampling%20%28TPS%29%2C%20which%20involves%20finding%20probable%20paths%0Aconnecting%20two%20points%20on%20an%20energy%20landscape%2C%20remains%20a%20challenge%20due%20to%20the%0Acomplexity%20of%20real-world%20atomistic%20systems.%20Current%20machine%20learning%20approaches%0Ause%20expensive%2C%20task-specific%2C%20and%20data-free%20training%20procedures%2C%20limiting%20their%0Aability%20to%20benefit%20from%20recent%20advances%20in%20atomistic%20machine%20learning%2C%20such%20as%0Ahigh-quality%20datasets%20and%20large-scale%20pre-trained%20models.%20In%20this%20work%2C%20we%0Aaddress%20TPS%20by%20interpreting%20candidate%20paths%20as%20trajectories%20sampled%20from%0Astochastic%20dynamics%20induced%20by%20the%20learned%20score%20function%20of%20pre-trained%0Agenerative%20models%2C%20specifically%20denoising%20diffusion%20and%20flow%20matching.%20Under%0Athese%20dynamics%2C%20finding%20high-likelihood%20transition%20paths%20becomes%20equivalent%20to%0Aminimizing%20the%20Onsager-Machlup%20%28OM%29%20action%20functional.%20This%20enables%20us%20to%0Arepurpose%20pre-trained%20generative%20models%20for%20TPS%20in%20a%20zero-shot%20manner%2C%20in%0Acontrast%20with%20bespoke%2C%20task-specific%20TPS%20models%20trained%20in%20previous%20work.%20We%0Ademonstrate%20our%20approach%20on%20varied%20molecular%20systems%2C%20obtaining%20diverse%2C%0Aphysically%20realistic%20transition%20pathways%20and%20generalizing%20beyond%20the%0Apre-trained%20model%27s%20original%20training%20dataset.%20Our%20method%20can%20be%20easily%0Aincorporated%20into%20new%20generative%20models%2C%20making%20it%20practically%20relevant%20as%0Amodels%20continue%20to%20scale%20and%20improve%20with%20increased%20data%20availability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction-Minimization%2520Meets%2520Generative%2520Modeling%253A%2520Efficient%2520Transition%2520Path%250A%2520%2520Sampling%2520with%2520the%2520Onsager-Machlup%2520Functional%26entry.906535625%3DSanjeev%2520Raja%2520and%2520Martin%2520%25C5%25A0%25C3%25ADpka%2520and%2520Michael%2520Psenka%2520and%2520Tobias%2520Kreiman%2520and%2520Michal%2520Pavelka%2520and%2520Aditi%2520S.%2520Krishnapriyan%26entry.1292438233%3D%2520%2520Transition%2520path%2520sampling%2520%2528TPS%2529%252C%2520which%2520involves%2520finding%2520probable%2520paths%250Aconnecting%2520two%2520points%2520on%2520an%2520energy%2520landscape%252C%2520remains%2520a%2520challenge%2520due%2520to%2520the%250Acomplexity%2520of%2520real-world%2520atomistic%2520systems.%2520Current%2520machine%2520learning%2520approaches%250Ause%2520expensive%252C%2520task-specific%252C%2520and%2520data-free%2520training%2520procedures%252C%2520limiting%2520their%250Aability%2520to%2520benefit%2520from%2520recent%2520advances%2520in%2520atomistic%2520machine%2520learning%252C%2520such%2520as%250Ahigh-quality%2520datasets%2520and%2520large-scale%2520pre-trained%2520models.%2520In%2520this%2520work%252C%2520we%250Aaddress%2520TPS%2520by%2520interpreting%2520candidate%2520paths%2520as%2520trajectories%2520sampled%2520from%250Astochastic%2520dynamics%2520induced%2520by%2520the%2520learned%2520score%2520function%2520of%2520pre-trained%250Agenerative%2520models%252C%2520specifically%2520denoising%2520diffusion%2520and%2520flow%2520matching.%2520Under%250Athese%2520dynamics%252C%2520finding%2520high-likelihood%2520transition%2520paths%2520becomes%2520equivalent%2520to%250Aminimizing%2520the%2520Onsager-Machlup%2520%2528OM%2529%2520action%2520functional.%2520This%2520enables%2520us%2520to%250Arepurpose%2520pre-trained%2520generative%2520models%2520for%2520TPS%2520in%2520a%2520zero-shot%2520manner%252C%2520in%250Acontrast%2520with%2520bespoke%252C%2520task-specific%2520TPS%2520models%2520trained%2520in%2520previous%2520work.%2520We%250Ademonstrate%2520our%2520approach%2520on%2520varied%2520molecular%2520systems%252C%2520obtaining%2520diverse%252C%250Aphysically%2520realistic%2520transition%2520pathways%2520and%2520generalizing%2520beyond%2520the%250Apre-trained%2520model%2527s%2520original%2520training%2520dataset.%2520Our%2520method%2520can%2520be%2520easily%250Aincorporated%2520into%2520new%2520generative%2520models%252C%2520making%2520it%2520practically%2520relevant%2520as%250Amodels%2520continue%2520to%2520scale%2520and%2520improve%2520with%2520increased%2520data%2520availability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action-Minimization%20Meets%20Generative%20Modeling%3A%20Efficient%20Transition%20Path%0A%20%20Sampling%20with%20the%20Onsager-Machlup%20Functional&entry.906535625=Sanjeev%20Raja%20and%20Martin%20%C5%A0%C3%ADpka%20and%20Michael%20Psenka%20and%20Tobias%20Kreiman%20and%20Michal%20Pavelka%20and%20Aditi%20S.%20Krishnapriyan&entry.1292438233=%20%20Transition%20path%20sampling%20%28TPS%29%2C%20which%20involves%20finding%20probable%20paths%0Aconnecting%20two%20points%20on%20an%20energy%20landscape%2C%20remains%20a%20challenge%20due%20to%20the%0Acomplexity%20of%20real-world%20atomistic%20systems.%20Current%20machine%20learning%20approaches%0Ause%20expensive%2C%20task-specific%2C%20and%20data-free%20training%20procedures%2C%20limiting%20their%0Aability%20to%20benefit%20from%20recent%20advances%20in%20atomistic%20machine%20learning%2C%20such%20as%0Ahigh-quality%20datasets%20and%20large-scale%20pre-trained%20models.%20In%20this%20work%2C%20we%0Aaddress%20TPS%20by%20interpreting%20candidate%20paths%20as%20trajectories%20sampled%20from%0Astochastic%20dynamics%20induced%20by%20the%20learned%20score%20function%20of%20pre-trained%0Agenerative%20models%2C%20specifically%20denoising%20diffusion%20and%20flow%20matching.%20Under%0Athese%20dynamics%2C%20finding%20high-likelihood%20transition%20paths%20becomes%20equivalent%20to%0Aminimizing%20the%20Onsager-Machlup%20%28OM%29%20action%20functional.%20This%20enables%20us%20to%0Arepurpose%20pre-trained%20generative%20models%20for%20TPS%20in%20a%20zero-shot%20manner%2C%20in%0Acontrast%20with%20bespoke%2C%20task-specific%20TPS%20models%20trained%20in%20previous%20work.%20We%0Ademonstrate%20our%20approach%20on%20varied%20molecular%20systems%2C%20obtaining%20diverse%2C%0Aphysically%20realistic%20transition%20pathways%20and%20generalizing%20beyond%20the%0Apre-trained%20model%27s%20original%20training%20dataset.%20Our%20method%20can%20be%20easily%0Aincorporated%20into%20new%20generative%20models%2C%20making%20it%20practically%20relevant%20as%0Amodels%20continue%20to%20scale%20and%20improve%20with%20increased%20data%20availability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18506v1&entry.124074799=Read"},
{"title": "A Multimodal Deep Learning Approach for White Matter Shape Prediction in\n  Diffusion MRI Tractography", "author": "Yui Lo and Yuqian Chen and Dongnan Liu and Leo Zekelman and Jarrett Rushmore and Yogesh Rathi and Nikos Makris and Alexandra J. Golby and Fan Zhang and Weidong Cai and Lauren J. O'Donnell", "abstract": "  Shape measures have emerged as promising descriptors of white matter\ntractography, offering complementary insights into anatomical variability and\nassociations with cognitive and clinical phenotypes. However, conventional\nmethods for computing shape measures are computationally expensive and\ntime-consuming for large-scale datasets due to reliance on voxel-based\nrepresentations. We propose Tract2Shape, a novel multimodal deep learning\nframework that leverages geometric (point cloud) and scalar (tabular) features\nto predict ten white matter tractography shape measures. To enhance model\nefficiency, we utilize a dimensionality reduction algorithm for the model to\npredict five primary shape components. The model is trained and evaluated on\ntwo independently acquired datasets, the HCP-YA dataset, and the PPMI dataset.\nWe evaluate the performance of Tract2Shape by training and testing it on the\nHCP-YA dataset and comparing the results with state-of-the-art models. To\nfurther assess its robustness and generalization ability, we also test\nTract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep\nlearning models across all ten shape measures, achieving the highest average\nPearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows\nthat both multimodal input and PCA contribute to performance gains. On the\nunseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low\nnMSE, demonstrating strong generalizability in cross-dataset evaluation.\nTract2Shape enables fast, accurate, and generalizable prediction of white\nmatter shape measures from tractography data, supporting scalable analysis\nacross datasets. This framework lays a promising foundation for future\nlarge-scale white matter shape analysis.\n", "link": "http://arxiv.org/abs/2504.18400v1", "date": "2025-04-25", "relevancy": 2.1915, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5937}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5174}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Deep%20Learning%20Approach%20for%20White%20Matter%20Shape%20Prediction%20in%0A%20%20Diffusion%20MRI%20Tractography&body=Title%3A%20A%20Multimodal%20Deep%20Learning%20Approach%20for%20White%20Matter%20Shape%20Prediction%20in%0A%20%20Diffusion%20MRI%20Tractography%0AAuthor%3A%20Yui%20Lo%20and%20Yuqian%20Chen%20and%20Dongnan%20Liu%20and%20Leo%20Zekelman%20and%20Jarrett%20Rushmore%20and%20Yogesh%20Rathi%20and%20Nikos%20Makris%20and%20Alexandra%20J.%20Golby%20and%20Fan%20Zhang%20and%20Weidong%20Cai%20and%20Lauren%20J.%20O%27Donnell%0AAbstract%3A%20%20%20Shape%20measures%20have%20emerged%20as%20promising%20descriptors%20of%20white%20matter%0Atractography%2C%20offering%20complementary%20insights%20into%20anatomical%20variability%20and%0Aassociations%20with%20cognitive%20and%20clinical%20phenotypes.%20However%2C%20conventional%0Amethods%20for%20computing%20shape%20measures%20are%20computationally%20expensive%20and%0Atime-consuming%20for%20large-scale%20datasets%20due%20to%20reliance%20on%20voxel-based%0Arepresentations.%20We%20propose%20Tract2Shape%2C%20a%20novel%20multimodal%20deep%20learning%0Aframework%20that%20leverages%20geometric%20%28point%20cloud%29%20and%20scalar%20%28tabular%29%20features%0Ato%20predict%20ten%20white%20matter%20tractography%20shape%20measures.%20To%20enhance%20model%0Aefficiency%2C%20we%20utilize%20a%20dimensionality%20reduction%20algorithm%20for%20the%20model%20to%0Apredict%20five%20primary%20shape%20components.%20The%20model%20is%20trained%20and%20evaluated%20on%0Atwo%20independently%20acquired%20datasets%2C%20the%20HCP-YA%20dataset%2C%20and%20the%20PPMI%20dataset.%0AWe%20evaluate%20the%20performance%20of%20Tract2Shape%20by%20training%20and%20testing%20it%20on%20the%0AHCP-YA%20dataset%20and%20comparing%20the%20results%20with%20state-of-the-art%20models.%20To%0Afurther%20assess%20its%20robustness%20and%20generalization%20ability%2C%20we%20also%20test%0ATract2Shape%20on%20the%20unseen%20PPMI%20dataset.%20Tract2Shape%20outperforms%20SOTA%20deep%0Alearning%20models%20across%20all%20ten%20shape%20measures%2C%20achieving%20the%20highest%20average%0APearson%27s%20r%20and%20the%20lowest%20nMSE%20on%20the%20HCP-YA%20dataset.%20The%20ablation%20study%20shows%0Athat%20both%20multimodal%20input%20and%20PCA%20contribute%20to%20performance%20gains.%20On%20the%0Aunseen%20testing%20PPMI%20dataset%2C%20Tract2Shape%20maintains%20a%20high%20Pearson%27s%20r%20and%20low%0AnMSE%2C%20demonstrating%20strong%20generalizability%20in%20cross-dataset%20evaluation.%0ATract2Shape%20enables%20fast%2C%20accurate%2C%20and%20generalizable%20prediction%20of%20white%0Amatter%20shape%20measures%20from%20tractography%20data%2C%20supporting%20scalable%20analysis%0Aacross%20datasets.%20This%20framework%20lays%20a%20promising%20foundation%20for%20future%0Alarge-scale%20white%20matter%20shape%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multimodal%2520Deep%2520Learning%2520Approach%2520for%2520White%2520Matter%2520Shape%2520Prediction%2520in%250A%2520%2520Diffusion%2520MRI%2520Tractography%26entry.906535625%3DYui%2520Lo%2520and%2520Yuqian%2520Chen%2520and%2520Dongnan%2520Liu%2520and%2520Leo%2520Zekelman%2520and%2520Jarrett%2520Rushmore%2520and%2520Yogesh%2520Rathi%2520and%2520Nikos%2520Makris%2520and%2520Alexandra%2520J.%2520Golby%2520and%2520Fan%2520Zhang%2520and%2520Weidong%2520Cai%2520and%2520Lauren%2520J.%2520O%2527Donnell%26entry.1292438233%3D%2520%2520Shape%2520measures%2520have%2520emerged%2520as%2520promising%2520descriptors%2520of%2520white%2520matter%250Atractography%252C%2520offering%2520complementary%2520insights%2520into%2520anatomical%2520variability%2520and%250Aassociations%2520with%2520cognitive%2520and%2520clinical%2520phenotypes.%2520However%252C%2520conventional%250Amethods%2520for%2520computing%2520shape%2520measures%2520are%2520computationally%2520expensive%2520and%250Atime-consuming%2520for%2520large-scale%2520datasets%2520due%2520to%2520reliance%2520on%2520voxel-based%250Arepresentations.%2520We%2520propose%2520Tract2Shape%252C%2520a%2520novel%2520multimodal%2520deep%2520learning%250Aframework%2520that%2520leverages%2520geometric%2520%2528point%2520cloud%2529%2520and%2520scalar%2520%2528tabular%2529%2520features%250Ato%2520predict%2520ten%2520white%2520matter%2520tractography%2520shape%2520measures.%2520To%2520enhance%2520model%250Aefficiency%252C%2520we%2520utilize%2520a%2520dimensionality%2520reduction%2520algorithm%2520for%2520the%2520model%2520to%250Apredict%2520five%2520primary%2520shape%2520components.%2520The%2520model%2520is%2520trained%2520and%2520evaluated%2520on%250Atwo%2520independently%2520acquired%2520datasets%252C%2520the%2520HCP-YA%2520dataset%252C%2520and%2520the%2520PPMI%2520dataset.%250AWe%2520evaluate%2520the%2520performance%2520of%2520Tract2Shape%2520by%2520training%2520and%2520testing%2520it%2520on%2520the%250AHCP-YA%2520dataset%2520and%2520comparing%2520the%2520results%2520with%2520state-of-the-art%2520models.%2520To%250Afurther%2520assess%2520its%2520robustness%2520and%2520generalization%2520ability%252C%2520we%2520also%2520test%250ATract2Shape%2520on%2520the%2520unseen%2520PPMI%2520dataset.%2520Tract2Shape%2520outperforms%2520SOTA%2520deep%250Alearning%2520models%2520across%2520all%2520ten%2520shape%2520measures%252C%2520achieving%2520the%2520highest%2520average%250APearson%2527s%2520r%2520and%2520the%2520lowest%2520nMSE%2520on%2520the%2520HCP-YA%2520dataset.%2520The%2520ablation%2520study%2520shows%250Athat%2520both%2520multimodal%2520input%2520and%2520PCA%2520contribute%2520to%2520performance%2520gains.%2520On%2520the%250Aunseen%2520testing%2520PPMI%2520dataset%252C%2520Tract2Shape%2520maintains%2520a%2520high%2520Pearson%2527s%2520r%2520and%2520low%250AnMSE%252C%2520demonstrating%2520strong%2520generalizability%2520in%2520cross-dataset%2520evaluation.%250ATract2Shape%2520enables%2520fast%252C%2520accurate%252C%2520and%2520generalizable%2520prediction%2520of%2520white%250Amatter%2520shape%2520measures%2520from%2520tractography%2520data%252C%2520supporting%2520scalable%2520analysis%250Aacross%2520datasets.%2520This%2520framework%2520lays%2520a%2520promising%2520foundation%2520for%2520future%250Alarge-scale%2520white%2520matter%2520shape%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Deep%20Learning%20Approach%20for%20White%20Matter%20Shape%20Prediction%20in%0A%20%20Diffusion%20MRI%20Tractography&entry.906535625=Yui%20Lo%20and%20Yuqian%20Chen%20and%20Dongnan%20Liu%20and%20Leo%20Zekelman%20and%20Jarrett%20Rushmore%20and%20Yogesh%20Rathi%20and%20Nikos%20Makris%20and%20Alexandra%20J.%20Golby%20and%20Fan%20Zhang%20and%20Weidong%20Cai%20and%20Lauren%20J.%20O%27Donnell&entry.1292438233=%20%20Shape%20measures%20have%20emerged%20as%20promising%20descriptors%20of%20white%20matter%0Atractography%2C%20offering%20complementary%20insights%20into%20anatomical%20variability%20and%0Aassociations%20with%20cognitive%20and%20clinical%20phenotypes.%20However%2C%20conventional%0Amethods%20for%20computing%20shape%20measures%20are%20computationally%20expensive%20and%0Atime-consuming%20for%20large-scale%20datasets%20due%20to%20reliance%20on%20voxel-based%0Arepresentations.%20We%20propose%20Tract2Shape%2C%20a%20novel%20multimodal%20deep%20learning%0Aframework%20that%20leverages%20geometric%20%28point%20cloud%29%20and%20scalar%20%28tabular%29%20features%0Ato%20predict%20ten%20white%20matter%20tractography%20shape%20measures.%20To%20enhance%20model%0Aefficiency%2C%20we%20utilize%20a%20dimensionality%20reduction%20algorithm%20for%20the%20model%20to%0Apredict%20five%20primary%20shape%20components.%20The%20model%20is%20trained%20and%20evaluated%20on%0Atwo%20independently%20acquired%20datasets%2C%20the%20HCP-YA%20dataset%2C%20and%20the%20PPMI%20dataset.%0AWe%20evaluate%20the%20performance%20of%20Tract2Shape%20by%20training%20and%20testing%20it%20on%20the%0AHCP-YA%20dataset%20and%20comparing%20the%20results%20with%20state-of-the-art%20models.%20To%0Afurther%20assess%20its%20robustness%20and%20generalization%20ability%2C%20we%20also%20test%0ATract2Shape%20on%20the%20unseen%20PPMI%20dataset.%20Tract2Shape%20outperforms%20SOTA%20deep%0Alearning%20models%20across%20all%20ten%20shape%20measures%2C%20achieving%20the%20highest%20average%0APearson%27s%20r%20and%20the%20lowest%20nMSE%20on%20the%20HCP-YA%20dataset.%20The%20ablation%20study%20shows%0Athat%20both%20multimodal%20input%20and%20PCA%20contribute%20to%20performance%20gains.%20On%20the%0Aunseen%20testing%20PPMI%20dataset%2C%20Tract2Shape%20maintains%20a%20high%20Pearson%27s%20r%20and%20low%0AnMSE%2C%20demonstrating%20strong%20generalizability%20in%20cross-dataset%20evaluation.%0ATract2Shape%20enables%20fast%2C%20accurate%2C%20and%20generalizable%20prediction%20of%20white%0Amatter%20shape%20measures%20from%20tractography%20data%2C%20supporting%20scalable%20analysis%0Aacross%20datasets.%20This%20framework%20lays%20a%20promising%20foundation%20for%20future%0Alarge-scale%20white%20matter%20shape%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18400v1&entry.124074799=Read"},
{"title": "Dense Geometry Supervision for Underwater Depth Estimation", "author": "Wenxiang Gua and Lin Qia", "abstract": "  The field of monocular depth estimation is continually evolving with the\nadvent of numerous innovative models and extensions. However, research on\nmonocular depth estimation methods specifically for underwater scenes remains\nlimited, compounded by a scarcity of relevant data and methodological support.\nThis paper proposes a novel approach to address the existing challenges in\ncurrent monocular depth estimation methods for underwater environments. We\nconstruct an economically efficient dataset suitable for underwater scenarios\nby employing multi-view depth estimation to generate supervisory signals and\ncorresponding enhanced underwater images. we introduces a texture-depth fusion\nmodule, designed according to the underwater optical imaging principles, which\naims to effectively exploit and integrate depth information from texture cues.\nExperimental results on the FLSea dataset demonstrate that our approach\nsignificantly improves the accuracy and adaptability of models in underwater\nsettings. This work offers a cost-effective solution for monocular underwater\ndepth estimation and holds considerable promise for practical applications.\n", "link": "http://arxiv.org/abs/2504.18233v1", "date": "2025-04-25", "relevancy": 2.1779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%20Geometry%20Supervision%20for%20Underwater%20Depth%20Estimation&body=Title%3A%20Dense%20Geometry%20Supervision%20for%20Underwater%20Depth%20Estimation%0AAuthor%3A%20Wenxiang%20Gua%20and%20Lin%20Qia%0AAbstract%3A%20%20%20The%20field%20of%20monocular%20depth%20estimation%20is%20continually%20evolving%20with%20the%0Aadvent%20of%20numerous%20innovative%20models%20and%20extensions.%20However%2C%20research%20on%0Amonocular%20depth%20estimation%20methods%20specifically%20for%20underwater%20scenes%20remains%0Alimited%2C%20compounded%20by%20a%20scarcity%20of%20relevant%20data%20and%20methodological%20support.%0AThis%20paper%20proposes%20a%20novel%20approach%20to%20address%20the%20existing%20challenges%20in%0Acurrent%20monocular%20depth%20estimation%20methods%20for%20underwater%20environments.%20We%0Aconstruct%20an%20economically%20efficient%20dataset%20suitable%20for%20underwater%20scenarios%0Aby%20employing%20multi-view%20depth%20estimation%20to%20generate%20supervisory%20signals%20and%0Acorresponding%20enhanced%20underwater%20images.%20we%20introduces%20a%20texture-depth%20fusion%0Amodule%2C%20designed%20according%20to%20the%20underwater%20optical%20imaging%20principles%2C%20which%0Aaims%20to%20effectively%20exploit%20and%20integrate%20depth%20information%20from%20texture%20cues.%0AExperimental%20results%20on%20the%20FLSea%20dataset%20demonstrate%20that%20our%20approach%0Asignificantly%20improves%20the%20accuracy%20and%20adaptability%20of%20models%20in%20underwater%0Asettings.%20This%20work%20offers%20a%20cost-effective%20solution%20for%20monocular%20underwater%0Adepth%20estimation%20and%20holds%20considerable%20promise%20for%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%2520Geometry%2520Supervision%2520for%2520Underwater%2520Depth%2520Estimation%26entry.906535625%3DWenxiang%2520Gua%2520and%2520Lin%2520Qia%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520monocular%2520depth%2520estimation%2520is%2520continually%2520evolving%2520with%2520the%250Aadvent%2520of%2520numerous%2520innovative%2520models%2520and%2520extensions.%2520However%252C%2520research%2520on%250Amonocular%2520depth%2520estimation%2520methods%2520specifically%2520for%2520underwater%2520scenes%2520remains%250Alimited%252C%2520compounded%2520by%2520a%2520scarcity%2520of%2520relevant%2520data%2520and%2520methodological%2520support.%250AThis%2520paper%2520proposes%2520a%2520novel%2520approach%2520to%2520address%2520the%2520existing%2520challenges%2520in%250Acurrent%2520monocular%2520depth%2520estimation%2520methods%2520for%2520underwater%2520environments.%2520We%250Aconstruct%2520an%2520economically%2520efficient%2520dataset%2520suitable%2520for%2520underwater%2520scenarios%250Aby%2520employing%2520multi-view%2520depth%2520estimation%2520to%2520generate%2520supervisory%2520signals%2520and%250Acorresponding%2520enhanced%2520underwater%2520images.%2520we%2520introduces%2520a%2520texture-depth%2520fusion%250Amodule%252C%2520designed%2520according%2520to%2520the%2520underwater%2520optical%2520imaging%2520principles%252C%2520which%250Aaims%2520to%2520effectively%2520exploit%2520and%2520integrate%2520depth%2520information%2520from%2520texture%2520cues.%250AExperimental%2520results%2520on%2520the%2520FLSea%2520dataset%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520improves%2520the%2520accuracy%2520and%2520adaptability%2520of%2520models%2520in%2520underwater%250Asettings.%2520This%2520work%2520offers%2520a%2520cost-effective%2520solution%2520for%2520monocular%2520underwater%250Adepth%2520estimation%2520and%2520holds%2520considerable%2520promise%2520for%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20Geometry%20Supervision%20for%20Underwater%20Depth%20Estimation&entry.906535625=Wenxiang%20Gua%20and%20Lin%20Qia&entry.1292438233=%20%20The%20field%20of%20monocular%20depth%20estimation%20is%20continually%20evolving%20with%20the%0Aadvent%20of%20numerous%20innovative%20models%20and%20extensions.%20However%2C%20research%20on%0Amonocular%20depth%20estimation%20methods%20specifically%20for%20underwater%20scenes%20remains%0Alimited%2C%20compounded%20by%20a%20scarcity%20of%20relevant%20data%20and%20methodological%20support.%0AThis%20paper%20proposes%20a%20novel%20approach%20to%20address%20the%20existing%20challenges%20in%0Acurrent%20monocular%20depth%20estimation%20methods%20for%20underwater%20environments.%20We%0Aconstruct%20an%20economically%20efficient%20dataset%20suitable%20for%20underwater%20scenarios%0Aby%20employing%20multi-view%20depth%20estimation%20to%20generate%20supervisory%20signals%20and%0Acorresponding%20enhanced%20underwater%20images.%20we%20introduces%20a%20texture-depth%20fusion%0Amodule%2C%20designed%20according%20to%20the%20underwater%20optical%20imaging%20principles%2C%20which%0Aaims%20to%20effectively%20exploit%20and%20integrate%20depth%20information%20from%20texture%20cues.%0AExperimental%20results%20on%20the%20FLSea%20dataset%20demonstrate%20that%20our%20approach%0Asignificantly%20improves%20the%20accuracy%20and%20adaptability%20of%20models%20in%20underwater%0Asettings.%20This%20work%20offers%20a%20cost-effective%20solution%20for%20monocular%20underwater%0Adepth%20estimation%20and%20holds%20considerable%20promise%20for%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18233v1&entry.124074799=Read"},
{"title": "Boosting-Enabled Robust System Identification of Partially Observed LTI\n  Systems Under Heavy-Tailed Noise", "author": "Vinay Kanakeri and Aritra Mitra", "abstract": "  We consider the problem of system identification of partially observed linear\ntime-invariant (LTI) systems. Given input-output data, we provide\nnon-asymptotic guarantees for identifying the system parameters under general\nheavy-tailed noise processes. Unlike previous works that assume Gaussian or\nsub-Gaussian noise, we consider significantly broader noise distributions that\nare required to admit only up to the second moment. For this setting, we\nleverage tools from robust statistics to propose a novel system identification\nalgorithm that exploits the idea of boosting. Despite the much weaker noise\nassumptions, we show that our proposed algorithm achieves sample complexity\nbounds that nearly match those derived under sub-Gaussian noise. In particular,\nwe establish that our bounds retain a logarithmic dependence on the prescribed\nfailure probability. Interestingly, we show that such bounds can be achieved by\nrequiring just a finite fourth moment on the excitatory input process.\n", "link": "http://arxiv.org/abs/2504.18444v1", "date": "2025-04-25", "relevancy": 2.1552, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4342}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4318}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting-Enabled%20Robust%20System%20Identification%20of%20Partially%20Observed%20LTI%0A%20%20Systems%20Under%20Heavy-Tailed%20Noise&body=Title%3A%20Boosting-Enabled%20Robust%20System%20Identification%20of%20Partially%20Observed%20LTI%0A%20%20Systems%20Under%20Heavy-Tailed%20Noise%0AAuthor%3A%20Vinay%20Kanakeri%20and%20Aritra%20Mitra%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20system%20identification%20of%20partially%20observed%20linear%0Atime-invariant%20%28LTI%29%20systems.%20Given%20input-output%20data%2C%20we%20provide%0Anon-asymptotic%20guarantees%20for%20identifying%20the%20system%20parameters%20under%20general%0Aheavy-tailed%20noise%20processes.%20Unlike%20previous%20works%20that%20assume%20Gaussian%20or%0Asub-Gaussian%20noise%2C%20we%20consider%20significantly%20broader%20noise%20distributions%20that%0Aare%20required%20to%20admit%20only%20up%20to%20the%20second%20moment.%20For%20this%20setting%2C%20we%0Aleverage%20tools%20from%20robust%20statistics%20to%20propose%20a%20novel%20system%20identification%0Aalgorithm%20that%20exploits%20the%20idea%20of%20boosting.%20Despite%20the%20much%20weaker%20noise%0Aassumptions%2C%20we%20show%20that%20our%20proposed%20algorithm%20achieves%20sample%20complexity%0Abounds%20that%20nearly%20match%20those%20derived%20under%20sub-Gaussian%20noise.%20In%20particular%2C%0Awe%20establish%20that%20our%20bounds%20retain%20a%20logarithmic%20dependence%20on%20the%20prescribed%0Afailure%20probability.%20Interestingly%2C%20we%20show%20that%20such%20bounds%20can%20be%20achieved%20by%0Arequiring%20just%20a%20finite%20fourth%20moment%20on%20the%20excitatory%20input%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting-Enabled%2520Robust%2520System%2520Identification%2520of%2520Partially%2520Observed%2520LTI%250A%2520%2520Systems%2520Under%2520Heavy-Tailed%2520Noise%26entry.906535625%3DVinay%2520Kanakeri%2520and%2520Aritra%2520Mitra%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520system%2520identification%2520of%2520partially%2520observed%2520linear%250Atime-invariant%2520%2528LTI%2529%2520systems.%2520Given%2520input-output%2520data%252C%2520we%2520provide%250Anon-asymptotic%2520guarantees%2520for%2520identifying%2520the%2520system%2520parameters%2520under%2520general%250Aheavy-tailed%2520noise%2520processes.%2520Unlike%2520previous%2520works%2520that%2520assume%2520Gaussian%2520or%250Asub-Gaussian%2520noise%252C%2520we%2520consider%2520significantly%2520broader%2520noise%2520distributions%2520that%250Aare%2520required%2520to%2520admit%2520only%2520up%2520to%2520the%2520second%2520moment.%2520For%2520this%2520setting%252C%2520we%250Aleverage%2520tools%2520from%2520robust%2520statistics%2520to%2520propose%2520a%2520novel%2520system%2520identification%250Aalgorithm%2520that%2520exploits%2520the%2520idea%2520of%2520boosting.%2520Despite%2520the%2520much%2520weaker%2520noise%250Aassumptions%252C%2520we%2520show%2520that%2520our%2520proposed%2520algorithm%2520achieves%2520sample%2520complexity%250Abounds%2520that%2520nearly%2520match%2520those%2520derived%2520under%2520sub-Gaussian%2520noise.%2520In%2520particular%252C%250Awe%2520establish%2520that%2520our%2520bounds%2520retain%2520a%2520logarithmic%2520dependence%2520on%2520the%2520prescribed%250Afailure%2520probability.%2520Interestingly%252C%2520we%2520show%2520that%2520such%2520bounds%2520can%2520be%2520achieved%2520by%250Arequiring%2520just%2520a%2520finite%2520fourth%2520moment%2520on%2520the%2520excitatory%2520input%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting-Enabled%20Robust%20System%20Identification%20of%20Partially%20Observed%20LTI%0A%20%20Systems%20Under%20Heavy-Tailed%20Noise&entry.906535625=Vinay%20Kanakeri%20and%20Aritra%20Mitra&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20system%20identification%20of%20partially%20observed%20linear%0Atime-invariant%20%28LTI%29%20systems.%20Given%20input-output%20data%2C%20we%20provide%0Anon-asymptotic%20guarantees%20for%20identifying%20the%20system%20parameters%20under%20general%0Aheavy-tailed%20noise%20processes.%20Unlike%20previous%20works%20that%20assume%20Gaussian%20or%0Asub-Gaussian%20noise%2C%20we%20consider%20significantly%20broader%20noise%20distributions%20that%0Aare%20required%20to%20admit%20only%20up%20to%20the%20second%20moment.%20For%20this%20setting%2C%20we%0Aleverage%20tools%20from%20robust%20statistics%20to%20propose%20a%20novel%20system%20identification%0Aalgorithm%20that%20exploits%20the%20idea%20of%20boosting.%20Despite%20the%20much%20weaker%20noise%0Aassumptions%2C%20we%20show%20that%20our%20proposed%20algorithm%20achieves%20sample%20complexity%0Abounds%20that%20nearly%20match%20those%20derived%20under%20sub-Gaussian%20noise.%20In%20particular%2C%0Awe%20establish%20that%20our%20bounds%20retain%20a%20logarithmic%20dependence%20on%20the%20prescribed%0Afailure%20probability.%20Interestingly%2C%20we%20show%20that%20such%20bounds%20can%20be%20achieved%20by%0Arequiring%20just%20a%20finite%20fourth%20moment%20on%20the%20excitatory%20input%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18444v1&entry.124074799=Read"},
{"title": "Revisiting Data Auditing in Large Vision-Language Models", "author": "Hongyu Zhu and Sichu Liang and Wenwen Wang and Boheng Li and Tongxin Yuan and Fangqi Li and ShiLin Wang and Zhuosheng Zhang", "abstract": "  With the surge of large language models (LLMs), Large Vision-Language Models\n(VLMs)--which integrate vision encoders with LLMs for accurate visual\ngrounding--have shown great potential in tasks like generalist agents and\nrobotic control. However, VLMs are typically trained on massive web-scraped\nimages, raising concerns over copyright infringement and privacy violations,\nand making data auditing increasingly urgent. Membership inference (MI), which\ndetermines whether a sample was used in training, has emerged as a key auditing\ntechnique, with promising results on open-source VLMs like LLaVA (AUC > 80%).\nIn this work, we revisit these advances and uncover a critical issue: current\nMI benchmarks suffer from distribution shifts between member and non-member\nimages, introducing shortcut cues that inflate MI performance. We further\nanalyze the nature of these shifts and propose a principled metric based on\noptimal transport to quantify the distribution discrepancy. To evaluate MI in\nrealistic settings, we construct new benchmarks with i.i.d. member and\nnon-member images. Existing MI methods fail under these unbiased conditions,\nperforming only marginally better than chance. Further, we explore the\ntheoretical upper bound of MI by probing the Bayes Optimality within the VLM's\nembedding space and find the irreducible error rate remains high. Despite this\npessimistic outlook, we analyze why MI for VLMs is particularly challenging and\nidentify three practical scenarios--fine-tuning, access to ground-truth texts,\nand set-based inference--where auditing becomes feasible. Our study presents a\nsystematic view of the limits and opportunities of MI for VLMs, providing\nguidance for future efforts in trustworthy data auditing.\n", "link": "http://arxiv.org/abs/2504.18349v1", "date": "2025-04-25", "relevancy": 2.1467, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Data%20Auditing%20in%20Large%20Vision-Language%20Models&body=Title%3A%20Revisiting%20Data%20Auditing%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Hongyu%20Zhu%20and%20Sichu%20Liang%20and%20Wenwen%20Wang%20and%20Boheng%20Li%20and%20Tongxin%20Yuan%20and%20Fangqi%20Li%20and%20ShiLin%20Wang%20and%20Zhuosheng%20Zhang%0AAbstract%3A%20%20%20With%20the%20surge%20of%20large%20language%20models%20%28LLMs%29%2C%20Large%20Vision-Language%20Models%0A%28VLMs%29--which%20integrate%20vision%20encoders%20with%20LLMs%20for%20accurate%20visual%0Agrounding--have%20shown%20great%20potential%20in%20tasks%20like%20generalist%20agents%20and%0Arobotic%20control.%20However%2C%20VLMs%20are%20typically%20trained%20on%20massive%20web-scraped%0Aimages%2C%20raising%20concerns%20over%20copyright%20infringement%20and%20privacy%20violations%2C%0Aand%20making%20data%20auditing%20increasingly%20urgent.%20Membership%20inference%20%28MI%29%2C%20which%0Adetermines%20whether%20a%20sample%20was%20used%20in%20training%2C%20has%20emerged%20as%20a%20key%20auditing%0Atechnique%2C%20with%20promising%20results%20on%20open-source%20VLMs%20like%20LLaVA%20%28AUC%20%3E%2080%25%29.%0AIn%20this%20work%2C%20we%20revisit%20these%20advances%20and%20uncover%20a%20critical%20issue%3A%20current%0AMI%20benchmarks%20suffer%20from%20distribution%20shifts%20between%20member%20and%20non-member%0Aimages%2C%20introducing%20shortcut%20cues%20that%20inflate%20MI%20performance.%20We%20further%0Aanalyze%20the%20nature%20of%20these%20shifts%20and%20propose%20a%20principled%20metric%20based%20on%0Aoptimal%20transport%20to%20quantify%20the%20distribution%20discrepancy.%20To%20evaluate%20MI%20in%0Arealistic%20settings%2C%20we%20construct%20new%20benchmarks%20with%20i.i.d.%20member%20and%0Anon-member%20images.%20Existing%20MI%20methods%20fail%20under%20these%20unbiased%20conditions%2C%0Aperforming%20only%20marginally%20better%20than%20chance.%20Further%2C%20we%20explore%20the%0Atheoretical%20upper%20bound%20of%20MI%20by%20probing%20the%20Bayes%20Optimality%20within%20the%20VLM%27s%0Aembedding%20space%20and%20find%20the%20irreducible%20error%20rate%20remains%20high.%20Despite%20this%0Apessimistic%20outlook%2C%20we%20analyze%20why%20MI%20for%20VLMs%20is%20particularly%20challenging%20and%0Aidentify%20three%20practical%20scenarios--fine-tuning%2C%20access%20to%20ground-truth%20texts%2C%0Aand%20set-based%20inference--where%20auditing%20becomes%20feasible.%20Our%20study%20presents%20a%0Asystematic%20view%20of%20the%20limits%20and%20opportunities%20of%20MI%20for%20VLMs%2C%20providing%0Aguidance%20for%20future%20efforts%20in%20trustworthy%20data%20auditing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Data%2520Auditing%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DHongyu%2520Zhu%2520and%2520Sichu%2520Liang%2520and%2520Wenwen%2520Wang%2520and%2520Boheng%2520Li%2520and%2520Tongxin%2520Yuan%2520and%2520Fangqi%2520Li%2520and%2520ShiLin%2520Wang%2520and%2520Zhuosheng%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520surge%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520Large%2520Vision-Language%2520Models%250A%2528VLMs%2529--which%2520integrate%2520vision%2520encoders%2520with%2520LLMs%2520for%2520accurate%2520visual%250Agrounding--have%2520shown%2520great%2520potential%2520in%2520tasks%2520like%2520generalist%2520agents%2520and%250Arobotic%2520control.%2520However%252C%2520VLMs%2520are%2520typically%2520trained%2520on%2520massive%2520web-scraped%250Aimages%252C%2520raising%2520concerns%2520over%2520copyright%2520infringement%2520and%2520privacy%2520violations%252C%250Aand%2520making%2520data%2520auditing%2520increasingly%2520urgent.%2520Membership%2520inference%2520%2528MI%2529%252C%2520which%250Adetermines%2520whether%2520a%2520sample%2520was%2520used%2520in%2520training%252C%2520has%2520emerged%2520as%2520a%2520key%2520auditing%250Atechnique%252C%2520with%2520promising%2520results%2520on%2520open-source%2520VLMs%2520like%2520LLaVA%2520%2528AUC%2520%253E%252080%2525%2529.%250AIn%2520this%2520work%252C%2520we%2520revisit%2520these%2520advances%2520and%2520uncover%2520a%2520critical%2520issue%253A%2520current%250AMI%2520benchmarks%2520suffer%2520from%2520distribution%2520shifts%2520between%2520member%2520and%2520non-member%250Aimages%252C%2520introducing%2520shortcut%2520cues%2520that%2520inflate%2520MI%2520performance.%2520We%2520further%250Aanalyze%2520the%2520nature%2520of%2520these%2520shifts%2520and%2520propose%2520a%2520principled%2520metric%2520based%2520on%250Aoptimal%2520transport%2520to%2520quantify%2520the%2520distribution%2520discrepancy.%2520To%2520evaluate%2520MI%2520in%250Arealistic%2520settings%252C%2520we%2520construct%2520new%2520benchmarks%2520with%2520i.i.d.%2520member%2520and%250Anon-member%2520images.%2520Existing%2520MI%2520methods%2520fail%2520under%2520these%2520unbiased%2520conditions%252C%250Aperforming%2520only%2520marginally%2520better%2520than%2520chance.%2520Further%252C%2520we%2520explore%2520the%250Atheoretical%2520upper%2520bound%2520of%2520MI%2520by%2520probing%2520the%2520Bayes%2520Optimality%2520within%2520the%2520VLM%2527s%250Aembedding%2520space%2520and%2520find%2520the%2520irreducible%2520error%2520rate%2520remains%2520high.%2520Despite%2520this%250Apessimistic%2520outlook%252C%2520we%2520analyze%2520why%2520MI%2520for%2520VLMs%2520is%2520particularly%2520challenging%2520and%250Aidentify%2520three%2520practical%2520scenarios--fine-tuning%252C%2520access%2520to%2520ground-truth%2520texts%252C%250Aand%2520set-based%2520inference--where%2520auditing%2520becomes%2520feasible.%2520Our%2520study%2520presents%2520a%250Asystematic%2520view%2520of%2520the%2520limits%2520and%2520opportunities%2520of%2520MI%2520for%2520VLMs%252C%2520providing%250Aguidance%2520for%2520future%2520efforts%2520in%2520trustworthy%2520data%2520auditing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Data%20Auditing%20in%20Large%20Vision-Language%20Models&entry.906535625=Hongyu%20Zhu%20and%20Sichu%20Liang%20and%20Wenwen%20Wang%20and%20Boheng%20Li%20and%20Tongxin%20Yuan%20and%20Fangqi%20Li%20and%20ShiLin%20Wang%20and%20Zhuosheng%20Zhang&entry.1292438233=%20%20With%20the%20surge%20of%20large%20language%20models%20%28LLMs%29%2C%20Large%20Vision-Language%20Models%0A%28VLMs%29--which%20integrate%20vision%20encoders%20with%20LLMs%20for%20accurate%20visual%0Agrounding--have%20shown%20great%20potential%20in%20tasks%20like%20generalist%20agents%20and%0Arobotic%20control.%20However%2C%20VLMs%20are%20typically%20trained%20on%20massive%20web-scraped%0Aimages%2C%20raising%20concerns%20over%20copyright%20infringement%20and%20privacy%20violations%2C%0Aand%20making%20data%20auditing%20increasingly%20urgent.%20Membership%20inference%20%28MI%29%2C%20which%0Adetermines%20whether%20a%20sample%20was%20used%20in%20training%2C%20has%20emerged%20as%20a%20key%20auditing%0Atechnique%2C%20with%20promising%20results%20on%20open-source%20VLMs%20like%20LLaVA%20%28AUC%20%3E%2080%25%29.%0AIn%20this%20work%2C%20we%20revisit%20these%20advances%20and%20uncover%20a%20critical%20issue%3A%20current%0AMI%20benchmarks%20suffer%20from%20distribution%20shifts%20between%20member%20and%20non-member%0Aimages%2C%20introducing%20shortcut%20cues%20that%20inflate%20MI%20performance.%20We%20further%0Aanalyze%20the%20nature%20of%20these%20shifts%20and%20propose%20a%20principled%20metric%20based%20on%0Aoptimal%20transport%20to%20quantify%20the%20distribution%20discrepancy.%20To%20evaluate%20MI%20in%0Arealistic%20settings%2C%20we%20construct%20new%20benchmarks%20with%20i.i.d.%20member%20and%0Anon-member%20images.%20Existing%20MI%20methods%20fail%20under%20these%20unbiased%20conditions%2C%0Aperforming%20only%20marginally%20better%20than%20chance.%20Further%2C%20we%20explore%20the%0Atheoretical%20upper%20bound%20of%20MI%20by%20probing%20the%20Bayes%20Optimality%20within%20the%20VLM%27s%0Aembedding%20space%20and%20find%20the%20irreducible%20error%20rate%20remains%20high.%20Despite%20this%0Apessimistic%20outlook%2C%20we%20analyze%20why%20MI%20for%20VLMs%20is%20particularly%20challenging%20and%0Aidentify%20three%20practical%20scenarios--fine-tuning%2C%20access%20to%20ground-truth%20texts%2C%0Aand%20set-based%20inference--where%20auditing%20becomes%20feasible.%20Our%20study%20presents%20a%0Asystematic%20view%20of%20the%20limits%20and%20opportunities%20of%20MI%20for%20VLMs%2C%20providing%0Aguidance%20for%20future%20efforts%20in%20trustworthy%20data%20auditing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18349v1&entry.124074799=Read"},
{"title": "Comparing Uncertainty Measurement and Mitigation Methods for Large\n  Language Models: A Systematic Review", "author": "Toghrul Abbasli and Kentaroh Toyoda and Yuan Wang and Leon Witt and Muhammad Asif Ali and Yukai Miao and Dan Li and Qingsong Wei", "abstract": "  Large Language Models (LLMs) have been transformative across many domains.\nHowever, hallucination -- confidently outputting incorrect information --\nremains one of the leading challenges for LLMs. This raises the question of how\nto accurately assess and quantify the uncertainty of LLMs. Extensive literature\non traditional models has explored Uncertainty Quantification (UQ) to measure\nuncertainty and employed calibration techniques to address the misalignment\nbetween uncertainty and accuracy. While some of these methods have been adapted\nfor LLMs, the literature lacks an in-depth analysis of their effectiveness and\ndoes not offer a comprehensive benchmark to enable insightful comparison among\nexisting solutions. In this work, we fill this gap via a systematic survey of\nrepresentative prior works on UQ and calibration for LLMs and introduce a\nrigorous benchmark. Using two widely used reliability datasets, we empirically\nevaluate six related methods, which justify the significant findings of our\nreview. Finally, we provide outlooks for key future directions and outline open\nchallenges. To the best of our knowledge, this survey is the first dedicated\nstudy to review the calibration methods and relevant metrics for LLMs.\n", "link": "http://arxiv.org/abs/2504.18346v1", "date": "2025-04-25", "relevancy": 2.1437, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5937}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Uncertainty%20Measurement%20and%20Mitigation%20Methods%20for%20Large%0A%20%20Language%20Models%3A%20A%20Systematic%20Review&body=Title%3A%20Comparing%20Uncertainty%20Measurement%20and%20Mitigation%20Methods%20for%20Large%0A%20%20Language%20Models%3A%20A%20Systematic%20Review%0AAuthor%3A%20Toghrul%20Abbasli%20and%20Kentaroh%20Toyoda%20and%20Yuan%20Wang%20and%20Leon%20Witt%20and%20Muhammad%20Asif%20Ali%20and%20Yukai%20Miao%20and%20Dan%20Li%20and%20Qingsong%20Wei%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20transformative%20across%20many%20domains.%0AHowever%2C%20hallucination%20--%20confidently%20outputting%20incorrect%20information%20--%0Aremains%20one%20of%20the%20leading%20challenges%20for%20LLMs.%20This%20raises%20the%20question%20of%20how%0Ato%20accurately%20assess%20and%20quantify%20the%20uncertainty%20of%20LLMs.%20Extensive%20literature%0Aon%20traditional%20models%20has%20explored%20Uncertainty%20Quantification%20%28UQ%29%20to%20measure%0Auncertainty%20and%20employed%20calibration%20techniques%20to%20address%20the%20misalignment%0Abetween%20uncertainty%20and%20accuracy.%20While%20some%20of%20these%20methods%20have%20been%20adapted%0Afor%20LLMs%2C%20the%20literature%20lacks%20an%20in-depth%20analysis%20of%20their%20effectiveness%20and%0Adoes%20not%20offer%20a%20comprehensive%20benchmark%20to%20enable%20insightful%20comparison%20among%0Aexisting%20solutions.%20In%20this%20work%2C%20we%20fill%20this%20gap%20via%20a%20systematic%20survey%20of%0Arepresentative%20prior%20works%20on%20UQ%20and%20calibration%20for%20LLMs%20and%20introduce%20a%0Arigorous%20benchmark.%20Using%20two%20widely%20used%20reliability%20datasets%2C%20we%20empirically%0Aevaluate%20six%20related%20methods%2C%20which%20justify%20the%20significant%20findings%20of%20our%0Areview.%20Finally%2C%20we%20provide%20outlooks%20for%20key%20future%20directions%20and%20outline%20open%0Achallenges.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20survey%20is%20the%20first%20dedicated%0Astudy%20to%20review%20the%20calibration%20methods%20and%20relevant%20metrics%20for%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Uncertainty%2520Measurement%2520and%2520Mitigation%2520Methods%2520for%2520Large%250A%2520%2520Language%2520Models%253A%2520A%2520Systematic%2520Review%26entry.906535625%3DToghrul%2520Abbasli%2520and%2520Kentaroh%2520Toyoda%2520and%2520Yuan%2520Wang%2520and%2520Leon%2520Witt%2520and%2520Muhammad%2520Asif%2520Ali%2520and%2520Yukai%2520Miao%2520and%2520Dan%2520Li%2520and%2520Qingsong%2520Wei%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520transformative%2520across%2520many%2520domains.%250AHowever%252C%2520hallucination%2520--%2520confidently%2520outputting%2520incorrect%2520information%2520--%250Aremains%2520one%2520of%2520the%2520leading%2520challenges%2520for%2520LLMs.%2520This%2520raises%2520the%2520question%2520of%2520how%250Ato%2520accurately%2520assess%2520and%2520quantify%2520the%2520uncertainty%2520of%2520LLMs.%2520Extensive%2520literature%250Aon%2520traditional%2520models%2520has%2520explored%2520Uncertainty%2520Quantification%2520%2528UQ%2529%2520to%2520measure%250Auncertainty%2520and%2520employed%2520calibration%2520techniques%2520to%2520address%2520the%2520misalignment%250Abetween%2520uncertainty%2520and%2520accuracy.%2520While%2520some%2520of%2520these%2520methods%2520have%2520been%2520adapted%250Afor%2520LLMs%252C%2520the%2520literature%2520lacks%2520an%2520in-depth%2520analysis%2520of%2520their%2520effectiveness%2520and%250Adoes%2520not%2520offer%2520a%2520comprehensive%2520benchmark%2520to%2520enable%2520insightful%2520comparison%2520among%250Aexisting%2520solutions.%2520In%2520this%2520work%252C%2520we%2520fill%2520this%2520gap%2520via%2520a%2520systematic%2520survey%2520of%250Arepresentative%2520prior%2520works%2520on%2520UQ%2520and%2520calibration%2520for%2520LLMs%2520and%2520introduce%2520a%250Arigorous%2520benchmark.%2520Using%2520two%2520widely%2520used%2520reliability%2520datasets%252C%2520we%2520empirically%250Aevaluate%2520six%2520related%2520methods%252C%2520which%2520justify%2520the%2520significant%2520findings%2520of%2520our%250Areview.%2520Finally%252C%2520we%2520provide%2520outlooks%2520for%2520key%2520future%2520directions%2520and%2520outline%2520open%250Achallenges.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520survey%2520is%2520the%2520first%2520dedicated%250Astudy%2520to%2520review%2520the%2520calibration%2520methods%2520and%2520relevant%2520metrics%2520for%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Uncertainty%20Measurement%20and%20Mitigation%20Methods%20for%20Large%0A%20%20Language%20Models%3A%20A%20Systematic%20Review&entry.906535625=Toghrul%20Abbasli%20and%20Kentaroh%20Toyoda%20and%20Yuan%20Wang%20and%20Leon%20Witt%20and%20Muhammad%20Asif%20Ali%20and%20Yukai%20Miao%20and%20Dan%20Li%20and%20Qingsong%20Wei&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20transformative%20across%20many%20domains.%0AHowever%2C%20hallucination%20--%20confidently%20outputting%20incorrect%20information%20--%0Aremains%20one%20of%20the%20leading%20challenges%20for%20LLMs.%20This%20raises%20the%20question%20of%20how%0Ato%20accurately%20assess%20and%20quantify%20the%20uncertainty%20of%20LLMs.%20Extensive%20literature%0Aon%20traditional%20models%20has%20explored%20Uncertainty%20Quantification%20%28UQ%29%20to%20measure%0Auncertainty%20and%20employed%20calibration%20techniques%20to%20address%20the%20misalignment%0Abetween%20uncertainty%20and%20accuracy.%20While%20some%20of%20these%20methods%20have%20been%20adapted%0Afor%20LLMs%2C%20the%20literature%20lacks%20an%20in-depth%20analysis%20of%20their%20effectiveness%20and%0Adoes%20not%20offer%20a%20comprehensive%20benchmark%20to%20enable%20insightful%20comparison%20among%0Aexisting%20solutions.%20In%20this%20work%2C%20we%20fill%20this%20gap%20via%20a%20systematic%20survey%20of%0Arepresentative%20prior%20works%20on%20UQ%20and%20calibration%20for%20LLMs%20and%20introduce%20a%0Arigorous%20benchmark.%20Using%20two%20widely%20used%20reliability%20datasets%2C%20we%20empirically%0Aevaluate%20six%20related%20methods%2C%20which%20justify%20the%20significant%20findings%20of%20our%0Areview.%20Finally%2C%20we%20provide%20outlooks%20for%20key%20future%20directions%20and%20outline%20open%0Achallenges.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20survey%20is%20the%20first%20dedicated%0Astudy%20to%20review%20the%20calibration%20methods%20and%20relevant%20metrics%20for%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18346v1&entry.124074799=Read"},
{"title": "AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series\n  Anomaly Detection", "author": "Tiange Huang and Yongjun Li", "abstract": "  Unsupervised multivariate time series anomaly detection (UMTSAD) plays a\ncritical role in various domains, including finance, networks, and sensor\nsystems. In recent years, due to the outstanding performance of deep learning\nin general sequential tasks, many models have been specialized for deep UMTSAD\ntasks and have achieved impressive results, particularly those based on the\nTransformer and self-attention mechanisms. However, the sequence anomaly\nassociation assumptions underlying these models are often limited to specific\npredefined patterns and scenarios, such as concentrated or peak anomaly\npatterns. These limitations hinder their ability to generalize to diverse\nanomaly situations, especially where the lack of labels poses significant\nchallenges. To address these issues, we propose AMAD, which integrates\n\\textbf{A}uto\\textbf{M}asked Attention for UMTS\\textbf{AD} scenarios. AMAD\nintroduces a novel structure based on the AutoMask mechanism and an attention\nmixup module, forming a simple yet generalized anomaly association\nrepresentation framework. This framework is further enhanced by a Max-Min\ntraining strategy and a Local-Global contrastive learning approach. By\ncombining multi-scale feature extraction with automatic relative association\nmodeling, AMAD provides a robust and adaptable solution to UMTSAD challenges.\nExtensive experimental results demonstrate that the proposed model achieving\ncompetitive performance results compared to SOTA benchmarks across a variety of\ndatasets.\n", "link": "http://arxiv.org/abs/2504.06643v3", "date": "2025-04-25", "relevancy": 2.1377, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5439}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5357}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMAD%3A%20AutoMasked%20Attention%20for%20Unsupervised%20Multivariate%20Time%20Series%0A%20%20Anomaly%20Detection&body=Title%3A%20AMAD%3A%20AutoMasked%20Attention%20for%20Unsupervised%20Multivariate%20Time%20Series%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Tiange%20Huang%20and%20Yongjun%20Li%0AAbstract%3A%20%20%20Unsupervised%20multivariate%20time%20series%20anomaly%20detection%20%28UMTSAD%29%20plays%20a%0Acritical%20role%20in%20various%20domains%2C%20including%20finance%2C%20networks%2C%20and%20sensor%0Asystems.%20In%20recent%20years%2C%20due%20to%20the%20outstanding%20performance%20of%20deep%20learning%0Ain%20general%20sequential%20tasks%2C%20many%20models%20have%20been%20specialized%20for%20deep%20UMTSAD%0Atasks%20and%20have%20achieved%20impressive%20results%2C%20particularly%20those%20based%20on%20the%0ATransformer%20and%20self-attention%20mechanisms.%20However%2C%20the%20sequence%20anomaly%0Aassociation%20assumptions%20underlying%20these%20models%20are%20often%20limited%20to%20specific%0Apredefined%20patterns%20and%20scenarios%2C%20such%20as%20concentrated%20or%20peak%20anomaly%0Apatterns.%20These%20limitations%20hinder%20their%20ability%20to%20generalize%20to%20diverse%0Aanomaly%20situations%2C%20especially%20where%20the%20lack%20of%20labels%20poses%20significant%0Achallenges.%20To%20address%20these%20issues%2C%20we%20propose%20AMAD%2C%20which%20integrates%0A%5Ctextbf%7BA%7Duto%5Ctextbf%7BM%7Dasked%20Attention%20for%20UMTS%5Ctextbf%7BAD%7D%20scenarios.%20AMAD%0Aintroduces%20a%20novel%20structure%20based%20on%20the%20AutoMask%20mechanism%20and%20an%20attention%0Amixup%20module%2C%20forming%20a%20simple%20yet%20generalized%20anomaly%20association%0Arepresentation%20framework.%20This%20framework%20is%20further%20enhanced%20by%20a%20Max-Min%0Atraining%20strategy%20and%20a%20Local-Global%20contrastive%20learning%20approach.%20By%0Acombining%20multi-scale%20feature%20extraction%20with%20automatic%20relative%20association%0Amodeling%2C%20AMAD%20provides%20a%20robust%20and%20adaptable%20solution%20to%20UMTSAD%20challenges.%0AExtensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20model%20achieving%0Acompetitive%20performance%20results%20compared%20to%20SOTA%20benchmarks%20across%20a%20variety%20of%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06643v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMAD%253A%2520AutoMasked%2520Attention%2520for%2520Unsupervised%2520Multivariate%2520Time%2520Series%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DTiange%2520Huang%2520and%2520Yongjun%2520Li%26entry.1292438233%3D%2520%2520Unsupervised%2520multivariate%2520time%2520series%2520anomaly%2520detection%2520%2528UMTSAD%2529%2520plays%2520a%250Acritical%2520role%2520in%2520various%2520domains%252C%2520including%2520finance%252C%2520networks%252C%2520and%2520sensor%250Asystems.%2520In%2520recent%2520years%252C%2520due%2520to%2520the%2520outstanding%2520performance%2520of%2520deep%2520learning%250Ain%2520general%2520sequential%2520tasks%252C%2520many%2520models%2520have%2520been%2520specialized%2520for%2520deep%2520UMTSAD%250Atasks%2520and%2520have%2520achieved%2520impressive%2520results%252C%2520particularly%2520those%2520based%2520on%2520the%250ATransformer%2520and%2520self-attention%2520mechanisms.%2520However%252C%2520the%2520sequence%2520anomaly%250Aassociation%2520assumptions%2520underlying%2520these%2520models%2520are%2520often%2520limited%2520to%2520specific%250Apredefined%2520patterns%2520and%2520scenarios%252C%2520such%2520as%2520concentrated%2520or%2520peak%2520anomaly%250Apatterns.%2520These%2520limitations%2520hinder%2520their%2520ability%2520to%2520generalize%2520to%2520diverse%250Aanomaly%2520situations%252C%2520especially%2520where%2520the%2520lack%2520of%2520labels%2520poses%2520significant%250Achallenges.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520AMAD%252C%2520which%2520integrates%250A%255Ctextbf%257BA%257Duto%255Ctextbf%257BM%257Dasked%2520Attention%2520for%2520UMTS%255Ctextbf%257BAD%257D%2520scenarios.%2520AMAD%250Aintroduces%2520a%2520novel%2520structure%2520based%2520on%2520the%2520AutoMask%2520mechanism%2520and%2520an%2520attention%250Amixup%2520module%252C%2520forming%2520a%2520simple%2520yet%2520generalized%2520anomaly%2520association%250Arepresentation%2520framework.%2520This%2520framework%2520is%2520further%2520enhanced%2520by%2520a%2520Max-Min%250Atraining%2520strategy%2520and%2520a%2520Local-Global%2520contrastive%2520learning%2520approach.%2520By%250Acombining%2520multi-scale%2520feature%2520extraction%2520with%2520automatic%2520relative%2520association%250Amodeling%252C%2520AMAD%2520provides%2520a%2520robust%2520and%2520adaptable%2520solution%2520to%2520UMTSAD%2520challenges.%250AExtensive%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520model%2520achieving%250Acompetitive%2520performance%2520results%2520compared%2520to%2520SOTA%2520benchmarks%2520across%2520a%2520variety%2520of%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06643v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMAD%3A%20AutoMasked%20Attention%20for%20Unsupervised%20Multivariate%20Time%20Series%0A%20%20Anomaly%20Detection&entry.906535625=Tiange%20Huang%20and%20Yongjun%20Li&entry.1292438233=%20%20Unsupervised%20multivariate%20time%20series%20anomaly%20detection%20%28UMTSAD%29%20plays%20a%0Acritical%20role%20in%20various%20domains%2C%20including%20finance%2C%20networks%2C%20and%20sensor%0Asystems.%20In%20recent%20years%2C%20due%20to%20the%20outstanding%20performance%20of%20deep%20learning%0Ain%20general%20sequential%20tasks%2C%20many%20models%20have%20been%20specialized%20for%20deep%20UMTSAD%0Atasks%20and%20have%20achieved%20impressive%20results%2C%20particularly%20those%20based%20on%20the%0ATransformer%20and%20self-attention%20mechanisms.%20However%2C%20the%20sequence%20anomaly%0Aassociation%20assumptions%20underlying%20these%20models%20are%20often%20limited%20to%20specific%0Apredefined%20patterns%20and%20scenarios%2C%20such%20as%20concentrated%20or%20peak%20anomaly%0Apatterns.%20These%20limitations%20hinder%20their%20ability%20to%20generalize%20to%20diverse%0Aanomaly%20situations%2C%20especially%20where%20the%20lack%20of%20labels%20poses%20significant%0Achallenges.%20To%20address%20these%20issues%2C%20we%20propose%20AMAD%2C%20which%20integrates%0A%5Ctextbf%7BA%7Duto%5Ctextbf%7BM%7Dasked%20Attention%20for%20UMTS%5Ctextbf%7BAD%7D%20scenarios.%20AMAD%0Aintroduces%20a%20novel%20structure%20based%20on%20the%20AutoMask%20mechanism%20and%20an%20attention%0Amixup%20module%2C%20forming%20a%20simple%20yet%20generalized%20anomaly%20association%0Arepresentation%20framework.%20This%20framework%20is%20further%20enhanced%20by%20a%20Max-Min%0Atraining%20strategy%20and%20a%20Local-Global%20contrastive%20learning%20approach.%20By%0Acombining%20multi-scale%20feature%20extraction%20with%20automatic%20relative%20association%0Amodeling%2C%20AMAD%20provides%20a%20robust%20and%20adaptable%20solution%20to%20UMTSAD%20challenges.%0AExtensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20model%20achieving%0Acompetitive%20performance%20results%20compared%20to%20SOTA%20benchmarks%20across%20a%20variety%20of%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06643v3&entry.124074799=Read"},
{"title": "HepatoGEN: Generating Hepatobiliary Phase MRI with Perceptual and\n  Adversarial Models", "author": "Jens Hooge and Gerard Sanroma-Guell and Faidra Stavropoulou and Alexander Ullmann and Gesine Knobloch and Mark Klemens and Carola Schmidt and Sabine Weckbach and Andreas Bolz", "abstract": "  Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays a\ncrucial role in the detection and characterization of focal liver lesions, with\nthe hepatobiliary phase (HBP) providing essential diagnostic information.\nHowever, acquiring HBP images requires prolonged scan times, which may\ncompromise patient comfort and scanner throughput. In this study, we propose a\ndeep learning based approach for synthesizing HBP images from earlier contrast\nphases (precontrast and transitional) and compare three generative models: a\nperceptual U-Net, a perceptual GAN (pGAN), and a denoising diffusion\nprobabilistic model (DDPM). We curated a multi-site DCE-MRI dataset from\ndiverse clinical settings and introduced a contrast evolution score (CES) to\nassess training data quality, enhancing model performance. Quantitative\nevaluation using pixel-wise and perceptual metrics, combined with qualitative\nassessment through blinded radiologist reviews, showed that pGAN achieved the\nbest quantitative performance but introduced heterogeneous contrast in\nout-of-distribution cases. In contrast, the U-Net produced consistent liver\nenhancement with fewer artifacts, while DDPM underperformed due to limited\npreservation of fine structural details. These findings demonstrate the\nfeasibility of synthetic HBP image generation as a means to reduce scan time\nwithout compromising diagnostic utility, highlighting the clinical potential of\ndeep learning for dynamic contrast enhancement in liver MRI. A project demo is\navailable at: https://jhooge.github.io/hepatogen\n", "link": "http://arxiv.org/abs/2504.18405v1", "date": "2025-04-25", "relevancy": 2.1355, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5427}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5329}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HepatoGEN%3A%20Generating%20Hepatobiliary%20Phase%20MRI%20with%20Perceptual%20and%0A%20%20Adversarial%20Models&body=Title%3A%20HepatoGEN%3A%20Generating%20Hepatobiliary%20Phase%20MRI%20with%20Perceptual%20and%0A%20%20Adversarial%20Models%0AAuthor%3A%20Jens%20Hooge%20and%20Gerard%20Sanroma-Guell%20and%20Faidra%20Stavropoulou%20and%20Alexander%20Ullmann%20and%20Gesine%20Knobloch%20and%20Mark%20Klemens%20and%20Carola%20Schmidt%20and%20Sabine%20Weckbach%20and%20Andreas%20Bolz%0AAbstract%3A%20%20%20Dynamic%20contrast-enhanced%20magnetic%20resonance%20imaging%20%28DCE-MRI%29%20plays%20a%0Acrucial%20role%20in%20the%20detection%20and%20characterization%20of%20focal%20liver%20lesions%2C%20with%0Athe%20hepatobiliary%20phase%20%28HBP%29%20providing%20essential%20diagnostic%20information.%0AHowever%2C%20acquiring%20HBP%20images%20requires%20prolonged%20scan%20times%2C%20which%20may%0Acompromise%20patient%20comfort%20and%20scanner%20throughput.%20In%20this%20study%2C%20we%20propose%20a%0Adeep%20learning%20based%20approach%20for%20synthesizing%20HBP%20images%20from%20earlier%20contrast%0Aphases%20%28precontrast%20and%20transitional%29%20and%20compare%20three%20generative%20models%3A%20a%0Aperceptual%20U-Net%2C%20a%20perceptual%20GAN%20%28pGAN%29%2C%20and%20a%20denoising%20diffusion%0Aprobabilistic%20model%20%28DDPM%29.%20We%20curated%20a%20multi-site%20DCE-MRI%20dataset%20from%0Adiverse%20clinical%20settings%20and%20introduced%20a%20contrast%20evolution%20score%20%28CES%29%20to%0Aassess%20training%20data%20quality%2C%20enhancing%20model%20performance.%20Quantitative%0Aevaluation%20using%20pixel-wise%20and%20perceptual%20metrics%2C%20combined%20with%20qualitative%0Aassessment%20through%20blinded%20radiologist%20reviews%2C%20showed%20that%20pGAN%20achieved%20the%0Abest%20quantitative%20performance%20but%20introduced%20heterogeneous%20contrast%20in%0Aout-of-distribution%20cases.%20In%20contrast%2C%20the%20U-Net%20produced%20consistent%20liver%0Aenhancement%20with%20fewer%20artifacts%2C%20while%20DDPM%20underperformed%20due%20to%20limited%0Apreservation%20of%20fine%20structural%20details.%20These%20findings%20demonstrate%20the%0Afeasibility%20of%20synthetic%20HBP%20image%20generation%20as%20a%20means%20to%20reduce%20scan%20time%0Awithout%20compromising%20diagnostic%20utility%2C%20highlighting%20the%20clinical%20potential%20of%0Adeep%20learning%20for%20dynamic%20contrast%20enhancement%20in%20liver%20MRI.%20A%20project%20demo%20is%0Aavailable%20at%3A%20https%3A//jhooge.github.io/hepatogen%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHepatoGEN%253A%2520Generating%2520Hepatobiliary%2520Phase%2520MRI%2520with%2520Perceptual%2520and%250A%2520%2520Adversarial%2520Models%26entry.906535625%3DJens%2520Hooge%2520and%2520Gerard%2520Sanroma-Guell%2520and%2520Faidra%2520Stavropoulou%2520and%2520Alexander%2520Ullmann%2520and%2520Gesine%2520Knobloch%2520and%2520Mark%2520Klemens%2520and%2520Carola%2520Schmidt%2520and%2520Sabine%2520Weckbach%2520and%2520Andreas%2520Bolz%26entry.1292438233%3D%2520%2520Dynamic%2520contrast-enhanced%2520magnetic%2520resonance%2520imaging%2520%2528DCE-MRI%2529%2520plays%2520a%250Acrucial%2520role%2520in%2520the%2520detection%2520and%2520characterization%2520of%2520focal%2520liver%2520lesions%252C%2520with%250Athe%2520hepatobiliary%2520phase%2520%2528HBP%2529%2520providing%2520essential%2520diagnostic%2520information.%250AHowever%252C%2520acquiring%2520HBP%2520images%2520requires%2520prolonged%2520scan%2520times%252C%2520which%2520may%250Acompromise%2520patient%2520comfort%2520and%2520scanner%2520throughput.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%250Adeep%2520learning%2520based%2520approach%2520for%2520synthesizing%2520HBP%2520images%2520from%2520earlier%2520contrast%250Aphases%2520%2528precontrast%2520and%2520transitional%2529%2520and%2520compare%2520three%2520generative%2520models%253A%2520a%250Aperceptual%2520U-Net%252C%2520a%2520perceptual%2520GAN%2520%2528pGAN%2529%252C%2520and%2520a%2520denoising%2520diffusion%250Aprobabilistic%2520model%2520%2528DDPM%2529.%2520We%2520curated%2520a%2520multi-site%2520DCE-MRI%2520dataset%2520from%250Adiverse%2520clinical%2520settings%2520and%2520introduced%2520a%2520contrast%2520evolution%2520score%2520%2528CES%2529%2520to%250Aassess%2520training%2520data%2520quality%252C%2520enhancing%2520model%2520performance.%2520Quantitative%250Aevaluation%2520using%2520pixel-wise%2520and%2520perceptual%2520metrics%252C%2520combined%2520with%2520qualitative%250Aassessment%2520through%2520blinded%2520radiologist%2520reviews%252C%2520showed%2520that%2520pGAN%2520achieved%2520the%250Abest%2520quantitative%2520performance%2520but%2520introduced%2520heterogeneous%2520contrast%2520in%250Aout-of-distribution%2520cases.%2520In%2520contrast%252C%2520the%2520U-Net%2520produced%2520consistent%2520liver%250Aenhancement%2520with%2520fewer%2520artifacts%252C%2520while%2520DDPM%2520underperformed%2520due%2520to%2520limited%250Apreservation%2520of%2520fine%2520structural%2520details.%2520These%2520findings%2520demonstrate%2520the%250Afeasibility%2520of%2520synthetic%2520HBP%2520image%2520generation%2520as%2520a%2520means%2520to%2520reduce%2520scan%2520time%250Awithout%2520compromising%2520diagnostic%2520utility%252C%2520highlighting%2520the%2520clinical%2520potential%2520of%250Adeep%2520learning%2520for%2520dynamic%2520contrast%2520enhancement%2520in%2520liver%2520MRI.%2520A%2520project%2520demo%2520is%250Aavailable%2520at%253A%2520https%253A//jhooge.github.io/hepatogen%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HepatoGEN%3A%20Generating%20Hepatobiliary%20Phase%20MRI%20with%20Perceptual%20and%0A%20%20Adversarial%20Models&entry.906535625=Jens%20Hooge%20and%20Gerard%20Sanroma-Guell%20and%20Faidra%20Stavropoulou%20and%20Alexander%20Ullmann%20and%20Gesine%20Knobloch%20and%20Mark%20Klemens%20and%20Carola%20Schmidt%20and%20Sabine%20Weckbach%20and%20Andreas%20Bolz&entry.1292438233=%20%20Dynamic%20contrast-enhanced%20magnetic%20resonance%20imaging%20%28DCE-MRI%29%20plays%20a%0Acrucial%20role%20in%20the%20detection%20and%20characterization%20of%20focal%20liver%20lesions%2C%20with%0Athe%20hepatobiliary%20phase%20%28HBP%29%20providing%20essential%20diagnostic%20information.%0AHowever%2C%20acquiring%20HBP%20images%20requires%20prolonged%20scan%20times%2C%20which%20may%0Acompromise%20patient%20comfort%20and%20scanner%20throughput.%20In%20this%20study%2C%20we%20propose%20a%0Adeep%20learning%20based%20approach%20for%20synthesizing%20HBP%20images%20from%20earlier%20contrast%0Aphases%20%28precontrast%20and%20transitional%29%20and%20compare%20three%20generative%20models%3A%20a%0Aperceptual%20U-Net%2C%20a%20perceptual%20GAN%20%28pGAN%29%2C%20and%20a%20denoising%20diffusion%0Aprobabilistic%20model%20%28DDPM%29.%20We%20curated%20a%20multi-site%20DCE-MRI%20dataset%20from%0Adiverse%20clinical%20settings%20and%20introduced%20a%20contrast%20evolution%20score%20%28CES%29%20to%0Aassess%20training%20data%20quality%2C%20enhancing%20model%20performance.%20Quantitative%0Aevaluation%20using%20pixel-wise%20and%20perceptual%20metrics%2C%20combined%20with%20qualitative%0Aassessment%20through%20blinded%20radiologist%20reviews%2C%20showed%20that%20pGAN%20achieved%20the%0Abest%20quantitative%20performance%20but%20introduced%20heterogeneous%20contrast%20in%0Aout-of-distribution%20cases.%20In%20contrast%2C%20the%20U-Net%20produced%20consistent%20liver%0Aenhancement%20with%20fewer%20artifacts%2C%20while%20DDPM%20underperformed%20due%20to%20limited%0Apreservation%20of%20fine%20structural%20details.%20These%20findings%20demonstrate%20the%0Afeasibility%20of%20synthetic%20HBP%20image%20generation%20as%20a%20means%20to%20reduce%20scan%20time%0Awithout%20compromising%20diagnostic%20utility%2C%20highlighting%20the%20clinical%20potential%20of%0Adeep%20learning%20for%20dynamic%20contrast%20enhancement%20in%20liver%20MRI.%20A%20project%20demo%20is%0Aavailable%20at%3A%20https%3A//jhooge.github.io/hepatogen%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18405v1&entry.124074799=Read"},
{"title": "E-VLC: A Real-World Dataset for Event-based Visible Light Communication\n  And Localization", "author": "Shintaro Shiba and Quan Kong and Norimasa Kobori", "abstract": "  Optical communication using modulated LEDs (e.g., visible light\ncommunication) is an emerging application for event cameras, thanks to their\nhigh spatio-temporal resolutions. Event cameras can be used simply to decode\nthe LED signals and also to localize the camera relative to the LED marker\npositions. However, there is no public dataset to benchmark the decoding and\nlocalization in various real-world settings. We present, to the best of our\nknowledge, the first public dataset that consists of an event camera, a frame\ncamera, and ground-truth poses that are precisely synchronized with hardware\ntriggers. It provides various camera motions with various sensitivities in\ndifferent scene brightness settings, both indoor and outdoor. Furthermore, we\npropose a novel method of localization that leverages the Contrast Maximization\nframework for motion estimation and compensation. The detailed analysis and\nexperimental results demonstrate the advantages of LED-based localization with\nevents over the conventional AR-marker--based one with frames, as well as the\nefficacy of the proposed method in localization. We hope that the proposed\ndataset serves as a future benchmark for both motion-related classical computer\nvision tasks and LED marker decoding tasks simultaneously, paving the way to\nbroadening applications of event cameras on mobile devices.\nhttps://woven-visionai.github.io/evlc-dataset\n", "link": "http://arxiv.org/abs/2504.18521v1", "date": "2025-04-25", "relevancy": 2.1343, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5431}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5309}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E-VLC%3A%20A%20Real-World%20Dataset%20for%20Event-based%20Visible%20Light%20Communication%0A%20%20And%20Localization&body=Title%3A%20E-VLC%3A%20A%20Real-World%20Dataset%20for%20Event-based%20Visible%20Light%20Communication%0A%20%20And%20Localization%0AAuthor%3A%20Shintaro%20Shiba%20and%20Quan%20Kong%20and%20Norimasa%20Kobori%0AAbstract%3A%20%20%20Optical%20communication%20using%20modulated%20LEDs%20%28e.g.%2C%20visible%20light%0Acommunication%29%20is%20an%20emerging%20application%20for%20event%20cameras%2C%20thanks%20to%20their%0Ahigh%20spatio-temporal%20resolutions.%20Event%20cameras%20can%20be%20used%20simply%20to%20decode%0Athe%20LED%20signals%20and%20also%20to%20localize%20the%20camera%20relative%20to%20the%20LED%20marker%0Apositions.%20However%2C%20there%20is%20no%20public%20dataset%20to%20benchmark%20the%20decoding%20and%0Alocalization%20in%20various%20real-world%20settings.%20We%20present%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20public%20dataset%20that%20consists%20of%20an%20event%20camera%2C%20a%20frame%0Acamera%2C%20and%20ground-truth%20poses%20that%20are%20precisely%20synchronized%20with%20hardware%0Atriggers.%20It%20provides%20various%20camera%20motions%20with%20various%20sensitivities%20in%0Adifferent%20scene%20brightness%20settings%2C%20both%20indoor%20and%20outdoor.%20Furthermore%2C%20we%0Apropose%20a%20novel%20method%20of%20localization%20that%20leverages%20the%20Contrast%20Maximization%0Aframework%20for%20motion%20estimation%20and%20compensation.%20The%20detailed%20analysis%20and%0Aexperimental%20results%20demonstrate%20the%20advantages%20of%20LED-based%20localization%20with%0Aevents%20over%20the%20conventional%20AR-marker--based%20one%20with%20frames%2C%20as%20well%20as%20the%0Aefficacy%20of%20the%20proposed%20method%20in%20localization.%20We%20hope%20that%20the%20proposed%0Adataset%20serves%20as%20a%20future%20benchmark%20for%20both%20motion-related%20classical%20computer%0Avision%20tasks%20and%20LED%20marker%20decoding%20tasks%20simultaneously%2C%20paving%20the%20way%20to%0Abroadening%20applications%20of%20event%20cameras%20on%20mobile%20devices.%0Ahttps%3A//woven-visionai.github.io/evlc-dataset%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE-VLC%253A%2520A%2520Real-World%2520Dataset%2520for%2520Event-based%2520Visible%2520Light%2520Communication%250A%2520%2520And%2520Localization%26entry.906535625%3DShintaro%2520Shiba%2520and%2520Quan%2520Kong%2520and%2520Norimasa%2520Kobori%26entry.1292438233%3D%2520%2520Optical%2520communication%2520using%2520modulated%2520LEDs%2520%2528e.g.%252C%2520visible%2520light%250Acommunication%2529%2520is%2520an%2520emerging%2520application%2520for%2520event%2520cameras%252C%2520thanks%2520to%2520their%250Ahigh%2520spatio-temporal%2520resolutions.%2520Event%2520cameras%2520can%2520be%2520used%2520simply%2520to%2520decode%250Athe%2520LED%2520signals%2520and%2520also%2520to%2520localize%2520the%2520camera%2520relative%2520to%2520the%2520LED%2520marker%250Apositions.%2520However%252C%2520there%2520is%2520no%2520public%2520dataset%2520to%2520benchmark%2520the%2520decoding%2520and%250Alocalization%2520in%2520various%2520real-world%2520settings.%2520We%2520present%252C%2520to%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520the%2520first%2520public%2520dataset%2520that%2520consists%2520of%2520an%2520event%2520camera%252C%2520a%2520frame%250Acamera%252C%2520and%2520ground-truth%2520poses%2520that%2520are%2520precisely%2520synchronized%2520with%2520hardware%250Atriggers.%2520It%2520provides%2520various%2520camera%2520motions%2520with%2520various%2520sensitivities%2520in%250Adifferent%2520scene%2520brightness%2520settings%252C%2520both%2520indoor%2520and%2520outdoor.%2520Furthermore%252C%2520we%250Apropose%2520a%2520novel%2520method%2520of%2520localization%2520that%2520leverages%2520the%2520Contrast%2520Maximization%250Aframework%2520for%2520motion%2520estimation%2520and%2520compensation.%2520The%2520detailed%2520analysis%2520and%250Aexperimental%2520results%2520demonstrate%2520the%2520advantages%2520of%2520LED-based%2520localization%2520with%250Aevents%2520over%2520the%2520conventional%2520AR-marker--based%2520one%2520with%2520frames%252C%2520as%2520well%2520as%2520the%250Aefficacy%2520of%2520the%2520proposed%2520method%2520in%2520localization.%2520We%2520hope%2520that%2520the%2520proposed%250Adataset%2520serves%2520as%2520a%2520future%2520benchmark%2520for%2520both%2520motion-related%2520classical%2520computer%250Avision%2520tasks%2520and%2520LED%2520marker%2520decoding%2520tasks%2520simultaneously%252C%2520paving%2520the%2520way%2520to%250Abroadening%2520applications%2520of%2520event%2520cameras%2520on%2520mobile%2520devices.%250Ahttps%253A//woven-visionai.github.io/evlc-dataset%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E-VLC%3A%20A%20Real-World%20Dataset%20for%20Event-based%20Visible%20Light%20Communication%0A%20%20And%20Localization&entry.906535625=Shintaro%20Shiba%20and%20Quan%20Kong%20and%20Norimasa%20Kobori&entry.1292438233=%20%20Optical%20communication%20using%20modulated%20LEDs%20%28e.g.%2C%20visible%20light%0Acommunication%29%20is%20an%20emerging%20application%20for%20event%20cameras%2C%20thanks%20to%20their%0Ahigh%20spatio-temporal%20resolutions.%20Event%20cameras%20can%20be%20used%20simply%20to%20decode%0Athe%20LED%20signals%20and%20also%20to%20localize%20the%20camera%20relative%20to%20the%20LED%20marker%0Apositions.%20However%2C%20there%20is%20no%20public%20dataset%20to%20benchmark%20the%20decoding%20and%0Alocalization%20in%20various%20real-world%20settings.%20We%20present%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20public%20dataset%20that%20consists%20of%20an%20event%20camera%2C%20a%20frame%0Acamera%2C%20and%20ground-truth%20poses%20that%20are%20precisely%20synchronized%20with%20hardware%0Atriggers.%20It%20provides%20various%20camera%20motions%20with%20various%20sensitivities%20in%0Adifferent%20scene%20brightness%20settings%2C%20both%20indoor%20and%20outdoor.%20Furthermore%2C%20we%0Apropose%20a%20novel%20method%20of%20localization%20that%20leverages%20the%20Contrast%20Maximization%0Aframework%20for%20motion%20estimation%20and%20compensation.%20The%20detailed%20analysis%20and%0Aexperimental%20results%20demonstrate%20the%20advantages%20of%20LED-based%20localization%20with%0Aevents%20over%20the%20conventional%20AR-marker--based%20one%20with%20frames%2C%20as%20well%20as%20the%0Aefficacy%20of%20the%20proposed%20method%20in%20localization.%20We%20hope%20that%20the%20proposed%0Adataset%20serves%20as%20a%20future%20benchmark%20for%20both%20motion-related%20classical%20computer%0Avision%20tasks%20and%20LED%20marker%20decoding%20tasks%20simultaneously%2C%20paving%20the%20way%20to%0Abroadening%20applications%20of%20event%20cameras%20on%20mobile%20devices.%0Ahttps%3A//woven-visionai.github.io/evlc-dataset%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18521v1&entry.124074799=Read"},
{"title": "VisTabNet: Adapting Vision Transformers for Tabular Data", "author": "Witold Wydma\u0144ski and Ulvi Movsum-zada and Jacek Tabor and Marek \u015amieja", "abstract": "  Although deep learning models have had great success in natural language\nprocessing and computer vision, we do not observe comparable improvements in\nthe case of tabular data, which is still the most common data type used in\nbiological, industrial and financial applications. In particular, it is\nchallenging to transfer large-scale pre-trained models to downstream tasks\ndefined on small tabular datasets. To address this, we propose VisTabNet -- a\ncross-modal transfer learning method, which allows for adapting Vision\nTransformer (ViT) with pre-trained weights to process tabular data. By\nprojecting tabular inputs to patch embeddings acceptable by ViT, we can\ndirectly apply a pre-trained Transformer Encoder to tabular inputs. This\napproach eliminates the conceptual cost of designing a suitable architecture\nfor processing tabular data, while reducing the computational cost of training\nthe model from scratch. Experimental results on multiple small tabular datasets\n(less than 1k samples) demonstrate VisTabNet's superiority, outperforming both\ntraditional ensemble methods and recent deep learning models. The proposed\nmethod goes beyond conventional transfer learning practice and shows that\npre-trained image models can be transferred to solve tabular problems,\nextending the boundaries of transfer learning. We share our example\nimplementation as a GitHub repository available at\nhttps://github.com/wwydmanski/VisTabNet.\n", "link": "http://arxiv.org/abs/2501.00057v2", "date": "2025-04-25", "relevancy": 2.113, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5651}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5351}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisTabNet%3A%20Adapting%20Vision%20Transformers%20for%20Tabular%20Data&body=Title%3A%20VisTabNet%3A%20Adapting%20Vision%20Transformers%20for%20Tabular%20Data%0AAuthor%3A%20Witold%20Wydma%C5%84ski%20and%20Ulvi%20Movsum-zada%20and%20Jacek%20Tabor%20and%20Marek%20%C5%9Amieja%0AAbstract%3A%20%20%20Although%20deep%20learning%20models%20have%20had%20great%20success%20in%20natural%20language%0Aprocessing%20and%20computer%20vision%2C%20we%20do%20not%20observe%20comparable%20improvements%20in%0Athe%20case%20of%20tabular%20data%2C%20which%20is%20still%20the%20most%20common%20data%20type%20used%20in%0Abiological%2C%20industrial%20and%20financial%20applications.%20In%20particular%2C%20it%20is%0Achallenging%20to%20transfer%20large-scale%20pre-trained%20models%20to%20downstream%20tasks%0Adefined%20on%20small%20tabular%20datasets.%20To%20address%20this%2C%20we%20propose%20VisTabNet%20--%20a%0Across-modal%20transfer%20learning%20method%2C%20which%20allows%20for%20adapting%20Vision%0ATransformer%20%28ViT%29%20with%20pre-trained%20weights%20to%20process%20tabular%20data.%20By%0Aprojecting%20tabular%20inputs%20to%20patch%20embeddings%20acceptable%20by%20ViT%2C%20we%20can%0Adirectly%20apply%20a%20pre-trained%20Transformer%20Encoder%20to%20tabular%20inputs.%20This%0Aapproach%20eliminates%20the%20conceptual%20cost%20of%20designing%20a%20suitable%20architecture%0Afor%20processing%20tabular%20data%2C%20while%20reducing%20the%20computational%20cost%20of%20training%0Athe%20model%20from%20scratch.%20Experimental%20results%20on%20multiple%20small%20tabular%20datasets%0A%28less%20than%201k%20samples%29%20demonstrate%20VisTabNet%27s%20superiority%2C%20outperforming%20both%0Atraditional%20ensemble%20methods%20and%20recent%20deep%20learning%20models.%20The%20proposed%0Amethod%20goes%20beyond%20conventional%20transfer%20learning%20practice%20and%20shows%20that%0Apre-trained%20image%20models%20can%20be%20transferred%20to%20solve%20tabular%20problems%2C%0Aextending%20the%20boundaries%20of%20transfer%20learning.%20We%20share%20our%20example%0Aimplementation%20as%20a%20GitHub%20repository%20available%20at%0Ahttps%3A//github.com/wwydmanski/VisTabNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00057v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisTabNet%253A%2520Adapting%2520Vision%2520Transformers%2520for%2520Tabular%2520Data%26entry.906535625%3DWitold%2520Wydma%25C5%2584ski%2520and%2520Ulvi%2520Movsum-zada%2520and%2520Jacek%2520Tabor%2520and%2520Marek%2520%25C5%259Amieja%26entry.1292438233%3D%2520%2520Although%2520deep%2520learning%2520models%2520have%2520had%2520great%2520success%2520in%2520natural%2520language%250Aprocessing%2520and%2520computer%2520vision%252C%2520we%2520do%2520not%2520observe%2520comparable%2520improvements%2520in%250Athe%2520case%2520of%2520tabular%2520data%252C%2520which%2520is%2520still%2520the%2520most%2520common%2520data%2520type%2520used%2520in%250Abiological%252C%2520industrial%2520and%2520financial%2520applications.%2520In%2520particular%252C%2520it%2520is%250Achallenging%2520to%2520transfer%2520large-scale%2520pre-trained%2520models%2520to%2520downstream%2520tasks%250Adefined%2520on%2520small%2520tabular%2520datasets.%2520To%2520address%2520this%252C%2520we%2520propose%2520VisTabNet%2520--%2520a%250Across-modal%2520transfer%2520learning%2520method%252C%2520which%2520allows%2520for%2520adapting%2520Vision%250ATransformer%2520%2528ViT%2529%2520with%2520pre-trained%2520weights%2520to%2520process%2520tabular%2520data.%2520By%250Aprojecting%2520tabular%2520inputs%2520to%2520patch%2520embeddings%2520acceptable%2520by%2520ViT%252C%2520we%2520can%250Adirectly%2520apply%2520a%2520pre-trained%2520Transformer%2520Encoder%2520to%2520tabular%2520inputs.%2520This%250Aapproach%2520eliminates%2520the%2520conceptual%2520cost%2520of%2520designing%2520a%2520suitable%2520architecture%250Afor%2520processing%2520tabular%2520data%252C%2520while%2520reducing%2520the%2520computational%2520cost%2520of%2520training%250Athe%2520model%2520from%2520scratch.%2520Experimental%2520results%2520on%2520multiple%2520small%2520tabular%2520datasets%250A%2528less%2520than%25201k%2520samples%2529%2520demonstrate%2520VisTabNet%2527s%2520superiority%252C%2520outperforming%2520both%250Atraditional%2520ensemble%2520methods%2520and%2520recent%2520deep%2520learning%2520models.%2520The%2520proposed%250Amethod%2520goes%2520beyond%2520conventional%2520transfer%2520learning%2520practice%2520and%2520shows%2520that%250Apre-trained%2520image%2520models%2520can%2520be%2520transferred%2520to%2520solve%2520tabular%2520problems%252C%250Aextending%2520the%2520boundaries%2520of%2520transfer%2520learning.%2520We%2520share%2520our%2520example%250Aimplementation%2520as%2520a%2520GitHub%2520repository%2520available%2520at%250Ahttps%253A//github.com/wwydmanski/VisTabNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00057v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisTabNet%3A%20Adapting%20Vision%20Transformers%20for%20Tabular%20Data&entry.906535625=Witold%20Wydma%C5%84ski%20and%20Ulvi%20Movsum-zada%20and%20Jacek%20Tabor%20and%20Marek%20%C5%9Amieja&entry.1292438233=%20%20Although%20deep%20learning%20models%20have%20had%20great%20success%20in%20natural%20language%0Aprocessing%20and%20computer%20vision%2C%20we%20do%20not%20observe%20comparable%20improvements%20in%0Athe%20case%20of%20tabular%20data%2C%20which%20is%20still%20the%20most%20common%20data%20type%20used%20in%0Abiological%2C%20industrial%20and%20financial%20applications.%20In%20particular%2C%20it%20is%0Achallenging%20to%20transfer%20large-scale%20pre-trained%20models%20to%20downstream%20tasks%0Adefined%20on%20small%20tabular%20datasets.%20To%20address%20this%2C%20we%20propose%20VisTabNet%20--%20a%0Across-modal%20transfer%20learning%20method%2C%20which%20allows%20for%20adapting%20Vision%0ATransformer%20%28ViT%29%20with%20pre-trained%20weights%20to%20process%20tabular%20data.%20By%0Aprojecting%20tabular%20inputs%20to%20patch%20embeddings%20acceptable%20by%20ViT%2C%20we%20can%0Adirectly%20apply%20a%20pre-trained%20Transformer%20Encoder%20to%20tabular%20inputs.%20This%0Aapproach%20eliminates%20the%20conceptual%20cost%20of%20designing%20a%20suitable%20architecture%0Afor%20processing%20tabular%20data%2C%20while%20reducing%20the%20computational%20cost%20of%20training%0Athe%20model%20from%20scratch.%20Experimental%20results%20on%20multiple%20small%20tabular%20datasets%0A%28less%20than%201k%20samples%29%20demonstrate%20VisTabNet%27s%20superiority%2C%20outperforming%20both%0Atraditional%20ensemble%20methods%20and%20recent%20deep%20learning%20models.%20The%20proposed%0Amethod%20goes%20beyond%20conventional%20transfer%20learning%20practice%20and%20shows%20that%0Apre-trained%20image%20models%20can%20be%20transferred%20to%20solve%20tabular%20problems%2C%0Aextending%20the%20boundaries%20of%20transfer%20learning.%20We%20share%20our%20example%0Aimplementation%20as%20a%20GitHub%20repository%20available%20at%0Ahttps%3A//github.com/wwydmanski/VisTabNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00057v2&entry.124074799=Read"},
{"title": "Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable\n  General-Purpose Deep Fusion", "author": "Minjie Deng and Yan Wei and Hao Zhai and An Wu and Yuncan Ouyang and Qianyao Peng", "abstract": "  In image fusion tasks, the absence of real fused images as priors presents a\nfundamental challenge. Most deep learning-based fusion methods rely on\nlarge-scale paired datasets to extract global weighting features from raw\nimages, thereby generating fused outputs that approximate real fused images. In\ncontrast to previous studies, this paper explores few-shot training of neural\nnetworks under the condition of having prior knowledge. We propose a novel\nfusion framework named GBFF, and a Granular Ball Significant Extraction\nalgorithm specifically designed for the few-shot prior setting. All pixel pairs\ninvolved in the fusion process are initially modeled as a Coarse-Grained\nGranular Ball. At the local level, Fine-Grained Granular Balls are used to\nslide through the brightness space to extract Non-Salient Pixel Pairs, and\nperform splitting operations to obtain Salient Pixel Pairs. Pixel-wise weights\nare then computed to generate a pseudo-supervised image. At the global level,\npixel pairs with significant contributions to the fusion process are\ncategorized into the Positive Region, while those whose contributions cannot be\naccurately determined are assigned to the Boundary Region. The Granular Ball\nperforms modality-aware adaptation based on the proportion of the positive\nregion, thereby adjusting the neural network's loss function and enabling it to\ncomplement the information of the boundary region. Extensive experiments\ndemonstrate the effectiveness of both the proposed algorithm and the underlying\ntheory. Compared with state-of-the-art (SOTA) methods, our approach shows\nstrong competitiveness in terms of both fusion time and image expressiveness.\nOur code is publicly available at:\n", "link": "http://arxiv.org/abs/2504.08937v3", "date": "2025-04-25", "relevancy": 2.1128, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5492}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5297}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Few-Shot%20Image%20Fusion%3A%20Granular%20Ball%20Priors%20Enable%0A%20%20General-Purpose%20Deep%20Fusion&body=Title%3A%20Rethinking%20Few-Shot%20Image%20Fusion%3A%20Granular%20Ball%20Priors%20Enable%0A%20%20General-Purpose%20Deep%20Fusion%0AAuthor%3A%20Minjie%20Deng%20and%20Yan%20Wei%20and%20Hao%20Zhai%20and%20An%20Wu%20and%20Yuncan%20Ouyang%20and%20Qianyao%20Peng%0AAbstract%3A%20%20%20In%20image%20fusion%20tasks%2C%20the%20absence%20of%20real%20fused%20images%20as%20priors%20presents%20a%0Afundamental%20challenge.%20Most%20deep%20learning-based%20fusion%20methods%20rely%20on%0Alarge-scale%20paired%20datasets%20to%20extract%20global%20weighting%20features%20from%20raw%0Aimages%2C%20thereby%20generating%20fused%20outputs%20that%20approximate%20real%20fused%20images.%20In%0Acontrast%20to%20previous%20studies%2C%20this%20paper%20explores%20few-shot%20training%20of%20neural%0Anetworks%20under%20the%20condition%20of%20having%20prior%20knowledge.%20We%20propose%20a%20novel%0Afusion%20framework%20named%20GBFF%2C%20and%20a%20Granular%20Ball%20Significant%20Extraction%0Aalgorithm%20specifically%20designed%20for%20the%20few-shot%20prior%20setting.%20All%20pixel%20pairs%0Ainvolved%20in%20the%20fusion%20process%20are%20initially%20modeled%20as%20a%20Coarse-Grained%0AGranular%20Ball.%20At%20the%20local%20level%2C%20Fine-Grained%20Granular%20Balls%20are%20used%20to%0Aslide%20through%20the%20brightness%20space%20to%20extract%20Non-Salient%20Pixel%20Pairs%2C%20and%0Aperform%20splitting%20operations%20to%20obtain%20Salient%20Pixel%20Pairs.%20Pixel-wise%20weights%0Aare%20then%20computed%20to%20generate%20a%20pseudo-supervised%20image.%20At%20the%20global%20level%2C%0Apixel%20pairs%20with%20significant%20contributions%20to%20the%20fusion%20process%20are%0Acategorized%20into%20the%20Positive%20Region%2C%20while%20those%20whose%20contributions%20cannot%20be%0Aaccurately%20determined%20are%20assigned%20to%20the%20Boundary%20Region.%20The%20Granular%20Ball%0Aperforms%20modality-aware%20adaptation%20based%20on%20the%20proportion%20of%20the%20positive%0Aregion%2C%20thereby%20adjusting%20the%20neural%20network%27s%20loss%20function%20and%20enabling%20it%20to%0Acomplement%20the%20information%20of%20the%20boundary%20region.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20both%20the%20proposed%20algorithm%20and%20the%20underlying%0Atheory.%20Compared%20with%20state-of-the-art%20%28SOTA%29%20methods%2C%20our%20approach%20shows%0Astrong%20competitiveness%20in%20terms%20of%20both%20fusion%20time%20and%20image%20expressiveness.%0AOur%20code%20is%20publicly%20available%20at%3A%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08937v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Few-Shot%2520Image%2520Fusion%253A%2520Granular%2520Ball%2520Priors%2520Enable%250A%2520%2520General-Purpose%2520Deep%2520Fusion%26entry.906535625%3DMinjie%2520Deng%2520and%2520Yan%2520Wei%2520and%2520Hao%2520Zhai%2520and%2520An%2520Wu%2520and%2520Yuncan%2520Ouyang%2520and%2520Qianyao%2520Peng%26entry.1292438233%3D%2520%2520In%2520image%2520fusion%2520tasks%252C%2520the%2520absence%2520of%2520real%2520fused%2520images%2520as%2520priors%2520presents%2520a%250Afundamental%2520challenge.%2520Most%2520deep%2520learning-based%2520fusion%2520methods%2520rely%2520on%250Alarge-scale%2520paired%2520datasets%2520to%2520extract%2520global%2520weighting%2520features%2520from%2520raw%250Aimages%252C%2520thereby%2520generating%2520fused%2520outputs%2520that%2520approximate%2520real%2520fused%2520images.%2520In%250Acontrast%2520to%2520previous%2520studies%252C%2520this%2520paper%2520explores%2520few-shot%2520training%2520of%2520neural%250Anetworks%2520under%2520the%2520condition%2520of%2520having%2520prior%2520knowledge.%2520We%2520propose%2520a%2520novel%250Afusion%2520framework%2520named%2520GBFF%252C%2520and%2520a%2520Granular%2520Ball%2520Significant%2520Extraction%250Aalgorithm%2520specifically%2520designed%2520for%2520the%2520few-shot%2520prior%2520setting.%2520All%2520pixel%2520pairs%250Ainvolved%2520in%2520the%2520fusion%2520process%2520are%2520initially%2520modeled%2520as%2520a%2520Coarse-Grained%250AGranular%2520Ball.%2520At%2520the%2520local%2520level%252C%2520Fine-Grained%2520Granular%2520Balls%2520are%2520used%2520to%250Aslide%2520through%2520the%2520brightness%2520space%2520to%2520extract%2520Non-Salient%2520Pixel%2520Pairs%252C%2520and%250Aperform%2520splitting%2520operations%2520to%2520obtain%2520Salient%2520Pixel%2520Pairs.%2520Pixel-wise%2520weights%250Aare%2520then%2520computed%2520to%2520generate%2520a%2520pseudo-supervised%2520image.%2520At%2520the%2520global%2520level%252C%250Apixel%2520pairs%2520with%2520significant%2520contributions%2520to%2520the%2520fusion%2520process%2520are%250Acategorized%2520into%2520the%2520Positive%2520Region%252C%2520while%2520those%2520whose%2520contributions%2520cannot%2520be%250Aaccurately%2520determined%2520are%2520assigned%2520to%2520the%2520Boundary%2520Region.%2520The%2520Granular%2520Ball%250Aperforms%2520modality-aware%2520adaptation%2520based%2520on%2520the%2520proportion%2520of%2520the%2520positive%250Aregion%252C%2520thereby%2520adjusting%2520the%2520neural%2520network%2527s%2520loss%2520function%2520and%2520enabling%2520it%2520to%250Acomplement%2520the%2520information%2520of%2520the%2520boundary%2520region.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520both%2520the%2520proposed%2520algorithm%2520and%2520the%2520underlying%250Atheory.%2520Compared%2520with%2520state-of-the-art%2520%2528SOTA%2529%2520methods%252C%2520our%2520approach%2520shows%250Astrong%2520competitiveness%2520in%2520terms%2520of%2520both%2520fusion%2520time%2520and%2520image%2520expressiveness.%250AOur%2520code%2520is%2520publicly%2520available%2520at%253A%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08937v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Few-Shot%20Image%20Fusion%3A%20Granular%20Ball%20Priors%20Enable%0A%20%20General-Purpose%20Deep%20Fusion&entry.906535625=Minjie%20Deng%20and%20Yan%20Wei%20and%20Hao%20Zhai%20and%20An%20Wu%20and%20Yuncan%20Ouyang%20and%20Qianyao%20Peng&entry.1292438233=%20%20In%20image%20fusion%20tasks%2C%20the%20absence%20of%20real%20fused%20images%20as%20priors%20presents%20a%0Afundamental%20challenge.%20Most%20deep%20learning-based%20fusion%20methods%20rely%20on%0Alarge-scale%20paired%20datasets%20to%20extract%20global%20weighting%20features%20from%20raw%0Aimages%2C%20thereby%20generating%20fused%20outputs%20that%20approximate%20real%20fused%20images.%20In%0Acontrast%20to%20previous%20studies%2C%20this%20paper%20explores%20few-shot%20training%20of%20neural%0Anetworks%20under%20the%20condition%20of%20having%20prior%20knowledge.%20We%20propose%20a%20novel%0Afusion%20framework%20named%20GBFF%2C%20and%20a%20Granular%20Ball%20Significant%20Extraction%0Aalgorithm%20specifically%20designed%20for%20the%20few-shot%20prior%20setting.%20All%20pixel%20pairs%0Ainvolved%20in%20the%20fusion%20process%20are%20initially%20modeled%20as%20a%20Coarse-Grained%0AGranular%20Ball.%20At%20the%20local%20level%2C%20Fine-Grained%20Granular%20Balls%20are%20used%20to%0Aslide%20through%20the%20brightness%20space%20to%20extract%20Non-Salient%20Pixel%20Pairs%2C%20and%0Aperform%20splitting%20operations%20to%20obtain%20Salient%20Pixel%20Pairs.%20Pixel-wise%20weights%0Aare%20then%20computed%20to%20generate%20a%20pseudo-supervised%20image.%20At%20the%20global%20level%2C%0Apixel%20pairs%20with%20significant%20contributions%20to%20the%20fusion%20process%20are%0Acategorized%20into%20the%20Positive%20Region%2C%20while%20those%20whose%20contributions%20cannot%20be%0Aaccurately%20determined%20are%20assigned%20to%20the%20Boundary%20Region.%20The%20Granular%20Ball%0Aperforms%20modality-aware%20adaptation%20based%20on%20the%20proportion%20of%20the%20positive%0Aregion%2C%20thereby%20adjusting%20the%20neural%20network%27s%20loss%20function%20and%20enabling%20it%20to%0Acomplement%20the%20information%20of%20the%20boundary%20region.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20both%20the%20proposed%20algorithm%20and%20the%20underlying%0Atheory.%20Compared%20with%20state-of-the-art%20%28SOTA%29%20methods%2C%20our%20approach%20shows%0Astrong%20competitiveness%20in%20terms%20of%20both%20fusion%20time%20and%20image%20expressiveness.%0AOur%20code%20is%20publicly%20available%20at%3A%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08937v3&entry.124074799=Read"},
{"title": "Optimal Control of Sensor-Induced Illusions on Robotic Agents", "author": "Lorenzo Medici and Steven M. LaValle and Basak Sakcak", "abstract": "  This paper presents a novel problem of creating and regulating localization\nand navigation illusions considering two agents: a receiver and a producer. A\nreceiver is moving on a plane localizing itself using the intensity of signals\nfrom three known towers observed at its position. Based on this position\nestimate, it follows a simple policy to reach its goal. The key idea is that a\nproducer alters the signal intensities to alter the position estimate of the\nreceiver while ensuring it reaches a different destination with the belief that\nit reached its goal. We provide a precise mathematical formulation of this\nproblem and show that it allows standard techniques from control theory to be\napplied to generate localization and navigation illusions that result in a\ndesired receiver behavior.\n", "link": "http://arxiv.org/abs/2504.18339v1", "date": "2025-04-25", "relevancy": 2.1003, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5337}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5251}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Control%20of%20Sensor-Induced%20Illusions%20on%20Robotic%20Agents&body=Title%3A%20Optimal%20Control%20of%20Sensor-Induced%20Illusions%20on%20Robotic%20Agents%0AAuthor%3A%20Lorenzo%20Medici%20and%20Steven%20M.%20LaValle%20and%20Basak%20Sakcak%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20problem%20of%20creating%20and%20regulating%20localization%0Aand%20navigation%20illusions%20considering%20two%20agents%3A%20a%20receiver%20and%20a%20producer.%20A%0Areceiver%20is%20moving%20on%20a%20plane%20localizing%20itself%20using%20the%20intensity%20of%20signals%0Afrom%20three%20known%20towers%20observed%20at%20its%20position.%20Based%20on%20this%20position%0Aestimate%2C%20it%20follows%20a%20simple%20policy%20to%20reach%20its%20goal.%20The%20key%20idea%20is%20that%20a%0Aproducer%20alters%20the%20signal%20intensities%20to%20alter%20the%20position%20estimate%20of%20the%0Areceiver%20while%20ensuring%20it%20reaches%20a%20different%20destination%20with%20the%20belief%20that%0Ait%20reached%20its%20goal.%20We%20provide%20a%20precise%20mathematical%20formulation%20of%20this%0Aproblem%20and%20show%20that%20it%20allows%20standard%20techniques%20from%20control%20theory%20to%20be%0Aapplied%20to%20generate%20localization%20and%20navigation%20illusions%20that%20result%20in%20a%0Adesired%20receiver%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Control%2520of%2520Sensor-Induced%2520Illusions%2520on%2520Robotic%2520Agents%26entry.906535625%3DLorenzo%2520Medici%2520and%2520Steven%2520M.%2520LaValle%2520and%2520Basak%2520Sakcak%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520problem%2520of%2520creating%2520and%2520regulating%2520localization%250Aand%2520navigation%2520illusions%2520considering%2520two%2520agents%253A%2520a%2520receiver%2520and%2520a%2520producer.%2520A%250Areceiver%2520is%2520moving%2520on%2520a%2520plane%2520localizing%2520itself%2520using%2520the%2520intensity%2520of%2520signals%250Afrom%2520three%2520known%2520towers%2520observed%2520at%2520its%2520position.%2520Based%2520on%2520this%2520position%250Aestimate%252C%2520it%2520follows%2520a%2520simple%2520policy%2520to%2520reach%2520its%2520goal.%2520The%2520key%2520idea%2520is%2520that%2520a%250Aproducer%2520alters%2520the%2520signal%2520intensities%2520to%2520alter%2520the%2520position%2520estimate%2520of%2520the%250Areceiver%2520while%2520ensuring%2520it%2520reaches%2520a%2520different%2520destination%2520with%2520the%2520belief%2520that%250Ait%2520reached%2520its%2520goal.%2520We%2520provide%2520a%2520precise%2520mathematical%2520formulation%2520of%2520this%250Aproblem%2520and%2520show%2520that%2520it%2520allows%2520standard%2520techniques%2520from%2520control%2520theory%2520to%2520be%250Aapplied%2520to%2520generate%2520localization%2520and%2520navigation%2520illusions%2520that%2520result%2520in%2520a%250Adesired%2520receiver%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Control%20of%20Sensor-Induced%20Illusions%20on%20Robotic%20Agents&entry.906535625=Lorenzo%20Medici%20and%20Steven%20M.%20LaValle%20and%20Basak%20Sakcak&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20problem%20of%20creating%20and%20regulating%20localization%0Aand%20navigation%20illusions%20considering%20two%20agents%3A%20a%20receiver%20and%20a%20producer.%20A%0Areceiver%20is%20moving%20on%20a%20plane%20localizing%20itself%20using%20the%20intensity%20of%20signals%0Afrom%20three%20known%20towers%20observed%20at%20its%20position.%20Based%20on%20this%20position%0Aestimate%2C%20it%20follows%20a%20simple%20policy%20to%20reach%20its%20goal.%20The%20key%20idea%20is%20that%20a%0Aproducer%20alters%20the%20signal%20intensities%20to%20alter%20the%20position%20estimate%20of%20the%0Areceiver%20while%20ensuring%20it%20reaches%20a%20different%20destination%20with%20the%20belief%20that%0Ait%20reached%20its%20goal.%20We%20provide%20a%20precise%20mathematical%20formulation%20of%20this%0Aproblem%20and%20show%20that%20it%20allows%20standard%20techniques%20from%20control%20theory%20to%20be%0Aapplied%20to%20generate%20localization%20and%20navigation%20illusions%20that%20result%20in%20a%0Adesired%20receiver%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18339v1&entry.124074799=Read"},
{"title": "Seeing Soundscapes: Audio-Visual Generation and Separation from\n  Soundscapes Using Audio-Visual Separator", "author": "Minjae Kang and Martim Brand\u00e3o", "abstract": "  Recent audio-visual generative models have made substantial progress in\ngenerating images from audio. However, existing approaches focus on generating\nimages from single-class audio and fail to generate images from mixed audio. To\naddress this, we propose an Audio-Visual Generation and Separation model\n(AV-GAS) for generating images from soundscapes (mixed audio containing\nmultiple classes). Our contribution is threefold: First, we propose a new\nchallenge in the audio-visual generation task, which is to generate an image\ngiven a multi-class audio input, and we propose a method that solves this task\nusing an audio-visual separator. Second, we introduce a new audio-visual\nseparation task, which involves generating separate images for each class\npresent in a mixed audio input. Lastly, we propose new evaluation metrics for\nthe audio-visual generation task: Class Representation Score (CRS) and a\nmodified R@K. Our model is trained and evaluated on the VGGSound dataset. We\nshow that our method outperforms the state-of-the-art, achieving 7% higher CRS\nand 4% higher R@2* in generating plausible images with mixed audio.\n", "link": "http://arxiv.org/abs/2504.18283v1", "date": "2025-04-25", "relevancy": 2.0995, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5511}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5285}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Soundscapes%3A%20Audio-Visual%20Generation%20and%20Separation%20from%0A%20%20Soundscapes%20Using%20Audio-Visual%20Separator&body=Title%3A%20Seeing%20Soundscapes%3A%20Audio-Visual%20Generation%20and%20Separation%20from%0A%20%20Soundscapes%20Using%20Audio-Visual%20Separator%0AAuthor%3A%20Minjae%20Kang%20and%20Martim%20Brand%C3%A3o%0AAbstract%3A%20%20%20Recent%20audio-visual%20generative%20models%20have%20made%20substantial%20progress%20in%0Agenerating%20images%20from%20audio.%20However%2C%20existing%20approaches%20focus%20on%20generating%0Aimages%20from%20single-class%20audio%20and%20fail%20to%20generate%20images%20from%20mixed%20audio.%20To%0Aaddress%20this%2C%20we%20propose%20an%20Audio-Visual%20Generation%20and%20Separation%20model%0A%28AV-GAS%29%20for%20generating%20images%20from%20soundscapes%20%28mixed%20audio%20containing%0Amultiple%20classes%29.%20Our%20contribution%20is%20threefold%3A%20First%2C%20we%20propose%20a%20new%0Achallenge%20in%20the%20audio-visual%20generation%20task%2C%20which%20is%20to%20generate%20an%20image%0Agiven%20a%20multi-class%20audio%20input%2C%20and%20we%20propose%20a%20method%20that%20solves%20this%20task%0Ausing%20an%20audio-visual%20separator.%20Second%2C%20we%20introduce%20a%20new%20audio-visual%0Aseparation%20task%2C%20which%20involves%20generating%20separate%20images%20for%20each%20class%0Apresent%20in%20a%20mixed%20audio%20input.%20Lastly%2C%20we%20propose%20new%20evaluation%20metrics%20for%0Athe%20audio-visual%20generation%20task%3A%20Class%20Representation%20Score%20%28CRS%29%20and%20a%0Amodified%20R%40K.%20Our%20model%20is%20trained%20and%20evaluated%20on%20the%20VGGSound%20dataset.%20We%0Ashow%20that%20our%20method%20outperforms%20the%20state-of-the-art%2C%20achieving%207%25%20higher%20CRS%0Aand%204%25%20higher%20R%402%2A%20in%20generating%20plausible%20images%20with%20mixed%20audio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Soundscapes%253A%2520Audio-Visual%2520Generation%2520and%2520Separation%2520from%250A%2520%2520Soundscapes%2520Using%2520Audio-Visual%2520Separator%26entry.906535625%3DMinjae%2520Kang%2520and%2520Martim%2520Brand%25C3%25A3o%26entry.1292438233%3D%2520%2520Recent%2520audio-visual%2520generative%2520models%2520have%2520made%2520substantial%2520progress%2520in%250Agenerating%2520images%2520from%2520audio.%2520However%252C%2520existing%2520approaches%2520focus%2520on%2520generating%250Aimages%2520from%2520single-class%2520audio%2520and%2520fail%2520to%2520generate%2520images%2520from%2520mixed%2520audio.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520an%2520Audio-Visual%2520Generation%2520and%2520Separation%2520model%250A%2528AV-GAS%2529%2520for%2520generating%2520images%2520from%2520soundscapes%2520%2528mixed%2520audio%2520containing%250Amultiple%2520classes%2529.%2520Our%2520contribution%2520is%2520threefold%253A%2520First%252C%2520we%2520propose%2520a%2520new%250Achallenge%2520in%2520the%2520audio-visual%2520generation%2520task%252C%2520which%2520is%2520to%2520generate%2520an%2520image%250Agiven%2520a%2520multi-class%2520audio%2520input%252C%2520and%2520we%2520propose%2520a%2520method%2520that%2520solves%2520this%2520task%250Ausing%2520an%2520audio-visual%2520separator.%2520Second%252C%2520we%2520introduce%2520a%2520new%2520audio-visual%250Aseparation%2520task%252C%2520which%2520involves%2520generating%2520separate%2520images%2520for%2520each%2520class%250Apresent%2520in%2520a%2520mixed%2520audio%2520input.%2520Lastly%252C%2520we%2520propose%2520new%2520evaluation%2520metrics%2520for%250Athe%2520audio-visual%2520generation%2520task%253A%2520Class%2520Representation%2520Score%2520%2528CRS%2529%2520and%2520a%250Amodified%2520R%2540K.%2520Our%2520model%2520is%2520trained%2520and%2520evaluated%2520on%2520the%2520VGGSound%2520dataset.%2520We%250Ashow%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%252C%2520achieving%25207%2525%2520higher%2520CRS%250Aand%25204%2525%2520higher%2520R%25402%252A%2520in%2520generating%2520plausible%2520images%2520with%2520mixed%2520audio.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Soundscapes%3A%20Audio-Visual%20Generation%20and%20Separation%20from%0A%20%20Soundscapes%20Using%20Audio-Visual%20Separator&entry.906535625=Minjae%20Kang%20and%20Martim%20Brand%C3%A3o&entry.1292438233=%20%20Recent%20audio-visual%20generative%20models%20have%20made%20substantial%20progress%20in%0Agenerating%20images%20from%20audio.%20However%2C%20existing%20approaches%20focus%20on%20generating%0Aimages%20from%20single-class%20audio%20and%20fail%20to%20generate%20images%20from%20mixed%20audio.%20To%0Aaddress%20this%2C%20we%20propose%20an%20Audio-Visual%20Generation%20and%20Separation%20model%0A%28AV-GAS%29%20for%20generating%20images%20from%20soundscapes%20%28mixed%20audio%20containing%0Amultiple%20classes%29.%20Our%20contribution%20is%20threefold%3A%20First%2C%20we%20propose%20a%20new%0Achallenge%20in%20the%20audio-visual%20generation%20task%2C%20which%20is%20to%20generate%20an%20image%0Agiven%20a%20multi-class%20audio%20input%2C%20and%20we%20propose%20a%20method%20that%20solves%20this%20task%0Ausing%20an%20audio-visual%20separator.%20Second%2C%20we%20introduce%20a%20new%20audio-visual%0Aseparation%20task%2C%20which%20involves%20generating%20separate%20images%20for%20each%20class%0Apresent%20in%20a%20mixed%20audio%20input.%20Lastly%2C%20we%20propose%20new%20evaluation%20metrics%20for%0Athe%20audio-visual%20generation%20task%3A%20Class%20Representation%20Score%20%28CRS%29%20and%20a%0Amodified%20R%40K.%20Our%20model%20is%20trained%20and%20evaluated%20on%20the%20VGGSound%20dataset.%20We%0Ashow%20that%20our%20method%20outperforms%20the%20state-of-the-art%2C%20achieving%207%25%20higher%20CRS%0Aand%204%25%20higher%20R%402%2A%20in%20generating%20plausible%20images%20with%20mixed%20audio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18283v1&entry.124074799=Read"},
{"title": "Hierarchical and Multimodal Data for Daily Activity Understanding", "author": "Ghazal Kaviani and Yavuz Yarici and Seulgi Kim and Mohit Prabhushankar and Ghassan AlRegib and Mashhour Solh and Ameya Patil", "abstract": "  Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/\n", "link": "http://arxiv.org/abs/2504.17696v2", "date": "2025-04-25", "relevancy": 2.0957, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5326}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20and%20Multimodal%20Data%20for%20Daily%20Activity%20Understanding&body=Title%3A%20Hierarchical%20and%20Multimodal%20Data%20for%20Daily%20Activity%20Understanding%0AAuthor%3A%20Ghazal%20Kaviani%20and%20Yavuz%20Yarici%20and%20Seulgi%20Kim%20and%20Mohit%20Prabhushankar%20and%20Ghassan%20AlRegib%20and%20Mashhour%20Solh%20and%20Ameya%20Patil%0AAbstract%3A%20%20%20Daily%20Activity%20Recordings%20for%20Artificial%20Intelligence%20%28DARai%2C%20pronounced%0A%22Dahr-ree%22%29%20is%20a%20multimodal%2C%20hierarchically%20annotated%20dataset%20constructed%20to%0Aunderstand%20human%20activities%20in%20real-world%20settings.%20DARai%20consists%20of%0Acontinuous%20scripted%20and%20unscripted%20recordings%20of%2050%20participants%20in%2010%0Adifferent%20environments%2C%20totaling%20over%20200%20hours%20of%20data%20from%2020%20sensors%0Aincluding%20multiple%20camera%20views%2C%20depth%20and%20radar%20sensors%2C%20wearable%20inertial%0Ameasurement%20units%20%28IMUs%29%2C%20electromyography%20%28EMG%29%2C%20insole%20pressure%20sensors%2C%0Abiomonitor%20sensors%2C%20and%20gaze%20tracker.%0A%20%20To%20capture%20the%20complexity%20in%20human%20activities%2C%20DARai%20is%20annotated%20at%20three%0Alevels%20of%20hierarchy%3A%20%28i%29%20high-level%20activities%20%28L1%29%20that%20are%20independent%20tasks%2C%0A%28ii%29%20lower-level%20actions%20%28L2%29%20that%20are%20patterns%20shared%20between%20activities%2C%20and%0A%28iii%29%20fine-grained%20procedures%20%28L3%29%20that%20detail%20the%20exact%20execution%20steps%20for%0Aactions.%20The%20dataset%20annotations%20and%20recordings%20are%20designed%20so%20that%2022.7%25%20of%0AL2%20actions%20are%20shared%20between%20L1%20activities%20and%2014.2%25%20of%20L3%20procedures%20are%0Ashared%20between%20L2%20actions.%20The%20overlap%20and%20unscripted%20nature%20of%20DARai%20allows%0Acounterfactual%20activities%20in%20the%20dataset.%0A%20%20Experiments%20with%20various%20machine%20learning%20models%20showcase%20the%20value%20of%20DARai%0Ain%20uncovering%20important%20challenges%20in%20human-centered%20applications.%0ASpecifically%2C%20we%20conduct%20unimodal%20and%20multimodal%20sensor%20fusion%20experiments%20for%0Arecognition%2C%20temporal%20localization%2C%20and%20future%20action%20anticipation%20across%20all%0Ahierarchical%20annotation%20levels.%20To%20highlight%20the%20limitations%20of%20individual%0Asensors%2C%20we%20also%20conduct%20domain-variant%20experiments%20that%20are%20enabled%20by%20DARai%27s%0Amulti-sensor%20and%20counterfactual%20activity%20design%20setup.%0A%20%20The%20code%2C%20documentation%2C%20and%20dataset%20are%20available%20at%20the%20dedicated%20DARai%0Awebsite%3A%0Ahttps%3A//alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17696v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520and%2520Multimodal%2520Data%2520for%2520Daily%2520Activity%2520Understanding%26entry.906535625%3DGhazal%2520Kaviani%2520and%2520Yavuz%2520Yarici%2520and%2520Seulgi%2520Kim%2520and%2520Mohit%2520Prabhushankar%2520and%2520Ghassan%2520AlRegib%2520and%2520Mashhour%2520Solh%2520and%2520Ameya%2520Patil%26entry.1292438233%3D%2520%2520Daily%2520Activity%2520Recordings%2520for%2520Artificial%2520Intelligence%2520%2528DARai%252C%2520pronounced%250A%2522Dahr-ree%2522%2529%2520is%2520a%2520multimodal%252C%2520hierarchically%2520annotated%2520dataset%2520constructed%2520to%250Aunderstand%2520human%2520activities%2520in%2520real-world%2520settings.%2520DARai%2520consists%2520of%250Acontinuous%2520scripted%2520and%2520unscripted%2520recordings%2520of%252050%2520participants%2520in%252010%250Adifferent%2520environments%252C%2520totaling%2520over%2520200%2520hours%2520of%2520data%2520from%252020%2520sensors%250Aincluding%2520multiple%2520camera%2520views%252C%2520depth%2520and%2520radar%2520sensors%252C%2520wearable%2520inertial%250Ameasurement%2520units%2520%2528IMUs%2529%252C%2520electromyography%2520%2528EMG%2529%252C%2520insole%2520pressure%2520sensors%252C%250Abiomonitor%2520sensors%252C%2520and%2520gaze%2520tracker.%250A%2520%2520To%2520capture%2520the%2520complexity%2520in%2520human%2520activities%252C%2520DARai%2520is%2520annotated%2520at%2520three%250Alevels%2520of%2520hierarchy%253A%2520%2528i%2529%2520high-level%2520activities%2520%2528L1%2529%2520that%2520are%2520independent%2520tasks%252C%250A%2528ii%2529%2520lower-level%2520actions%2520%2528L2%2529%2520that%2520are%2520patterns%2520shared%2520between%2520activities%252C%2520and%250A%2528iii%2529%2520fine-grained%2520procedures%2520%2528L3%2529%2520that%2520detail%2520the%2520exact%2520execution%2520steps%2520for%250Aactions.%2520The%2520dataset%2520annotations%2520and%2520recordings%2520are%2520designed%2520so%2520that%252022.7%2525%2520of%250AL2%2520actions%2520are%2520shared%2520between%2520L1%2520activities%2520and%252014.2%2525%2520of%2520L3%2520procedures%2520are%250Ashared%2520between%2520L2%2520actions.%2520The%2520overlap%2520and%2520unscripted%2520nature%2520of%2520DARai%2520allows%250Acounterfactual%2520activities%2520in%2520the%2520dataset.%250A%2520%2520Experiments%2520with%2520various%2520machine%2520learning%2520models%2520showcase%2520the%2520value%2520of%2520DARai%250Ain%2520uncovering%2520important%2520challenges%2520in%2520human-centered%2520applications.%250ASpecifically%252C%2520we%2520conduct%2520unimodal%2520and%2520multimodal%2520sensor%2520fusion%2520experiments%2520for%250Arecognition%252C%2520temporal%2520localization%252C%2520and%2520future%2520action%2520anticipation%2520across%2520all%250Ahierarchical%2520annotation%2520levels.%2520To%2520highlight%2520the%2520limitations%2520of%2520individual%250Asensors%252C%2520we%2520also%2520conduct%2520domain-variant%2520experiments%2520that%2520are%2520enabled%2520by%2520DARai%2527s%250Amulti-sensor%2520and%2520counterfactual%2520activity%2520design%2520setup.%250A%2520%2520The%2520code%252C%2520documentation%252C%2520and%2520dataset%2520are%2520available%2520at%2520the%2520dedicated%2520DARai%250Awebsite%253A%250Ahttps%253A//alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17696v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20and%20Multimodal%20Data%20for%20Daily%20Activity%20Understanding&entry.906535625=Ghazal%20Kaviani%20and%20Yavuz%20Yarici%20and%20Seulgi%20Kim%20and%20Mohit%20Prabhushankar%20and%20Ghassan%20AlRegib%20and%20Mashhour%20Solh%20and%20Ameya%20Patil&entry.1292438233=%20%20Daily%20Activity%20Recordings%20for%20Artificial%20Intelligence%20%28DARai%2C%20pronounced%0A%22Dahr-ree%22%29%20is%20a%20multimodal%2C%20hierarchically%20annotated%20dataset%20constructed%20to%0Aunderstand%20human%20activities%20in%20real-world%20settings.%20DARai%20consists%20of%0Acontinuous%20scripted%20and%20unscripted%20recordings%20of%2050%20participants%20in%2010%0Adifferent%20environments%2C%20totaling%20over%20200%20hours%20of%20data%20from%2020%20sensors%0Aincluding%20multiple%20camera%20views%2C%20depth%20and%20radar%20sensors%2C%20wearable%20inertial%0Ameasurement%20units%20%28IMUs%29%2C%20electromyography%20%28EMG%29%2C%20insole%20pressure%20sensors%2C%0Abiomonitor%20sensors%2C%20and%20gaze%20tracker.%0A%20%20To%20capture%20the%20complexity%20in%20human%20activities%2C%20DARai%20is%20annotated%20at%20three%0Alevels%20of%20hierarchy%3A%20%28i%29%20high-level%20activities%20%28L1%29%20that%20are%20independent%20tasks%2C%0A%28ii%29%20lower-level%20actions%20%28L2%29%20that%20are%20patterns%20shared%20between%20activities%2C%20and%0A%28iii%29%20fine-grained%20procedures%20%28L3%29%20that%20detail%20the%20exact%20execution%20steps%20for%0Aactions.%20The%20dataset%20annotations%20and%20recordings%20are%20designed%20so%20that%2022.7%25%20of%0AL2%20actions%20are%20shared%20between%20L1%20activities%20and%2014.2%25%20of%20L3%20procedures%20are%0Ashared%20between%20L2%20actions.%20The%20overlap%20and%20unscripted%20nature%20of%20DARai%20allows%0Acounterfactual%20activities%20in%20the%20dataset.%0A%20%20Experiments%20with%20various%20machine%20learning%20models%20showcase%20the%20value%20of%20DARai%0Ain%20uncovering%20important%20challenges%20in%20human-centered%20applications.%0ASpecifically%2C%20we%20conduct%20unimodal%20and%20multimodal%20sensor%20fusion%20experiments%20for%0Arecognition%2C%20temporal%20localization%2C%20and%20future%20action%20anticipation%20across%20all%0Ahierarchical%20annotation%20levels.%20To%20highlight%20the%20limitations%20of%20individual%0Asensors%2C%20we%20also%20conduct%20domain-variant%20experiments%20that%20are%20enabled%20by%20DARai%27s%0Amulti-sensor%20and%20counterfactual%20activity%20design%20setup.%0A%20%20The%20code%2C%20documentation%2C%20and%20dataset%20are%20available%20at%20the%20dedicated%20DARai%0Awebsite%3A%0Ahttps%3A//alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17696v2&entry.124074799=Read"},
{"title": "Depth-Constrained ASV Navigation with Deep RL and Limited Sensing", "author": "Amirhossein Zhalehmehrabi and Daniele Meli and Francesco Dal Santo and Francesco Trotti and Alessandro Farinelli", "abstract": "  Autonomous Surface Vehicles (ASVs) play a crucial role in maritime\noperations, yet their navigation in shallow-water environments remains\nchallenging due to dynamic disturbances and depth constraints. Traditional\nnavigation strategies struggle with limited sensor information, making safe and\nefficient operation difficult. In this paper, we propose a reinforcement\nlearning (RL) framework for ASV navigation under depth constraints, where the\nvehicle must reach a target while avoiding unsafe areas with only a single\ndepth measurement per timestep from a downward-facing Single Beam Echosounder\n(SBES). To enhance environmental awareness, we integrate Gaussian Process (GP)\nregression into the RL framework, enabling the agent to progressively estimate\na bathymetric depth map from sparse sonar readings. This approach improves\ndecision-making by providing a richer representation of the environment.\nFurthermore, we demonstrate effective sim-to-real transfer, ensuring that\ntrained policies generalize well to real-world aquatic conditions. Experimental\nresults validate our method's capability to improve ASV navigation performance\nwhile maintaining safety in challenging shallow-water environments.\n", "link": "http://arxiv.org/abs/2504.18253v1", "date": "2025-04-25", "relevancy": 2.0948, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5545}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5237}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Constrained%20ASV%20Navigation%20with%20Deep%20RL%20and%20Limited%20Sensing&body=Title%3A%20Depth-Constrained%20ASV%20Navigation%20with%20Deep%20RL%20and%20Limited%20Sensing%0AAuthor%3A%20Amirhossein%20Zhalehmehrabi%20and%20Daniele%20Meli%20and%20Francesco%20Dal%20Santo%20and%20Francesco%20Trotti%20and%20Alessandro%20Farinelli%0AAbstract%3A%20%20%20Autonomous%20Surface%20Vehicles%20%28ASVs%29%20play%20a%20crucial%20role%20in%20maritime%0Aoperations%2C%20yet%20their%20navigation%20in%20shallow-water%20environments%20remains%0Achallenging%20due%20to%20dynamic%20disturbances%20and%20depth%20constraints.%20Traditional%0Anavigation%20strategies%20struggle%20with%20limited%20sensor%20information%2C%20making%20safe%20and%0Aefficient%20operation%20difficult.%20In%20this%20paper%2C%20we%20propose%20a%20reinforcement%0Alearning%20%28RL%29%20framework%20for%20ASV%20navigation%20under%20depth%20constraints%2C%20where%20the%0Avehicle%20must%20reach%20a%20target%20while%20avoiding%20unsafe%20areas%20with%20only%20a%20single%0Adepth%20measurement%20per%20timestep%20from%20a%20downward-facing%20Single%20Beam%20Echosounder%0A%28SBES%29.%20To%20enhance%20environmental%20awareness%2C%20we%20integrate%20Gaussian%20Process%20%28GP%29%0Aregression%20into%20the%20RL%20framework%2C%20enabling%20the%20agent%20to%20progressively%20estimate%0Aa%20bathymetric%20depth%20map%20from%20sparse%20sonar%20readings.%20This%20approach%20improves%0Adecision-making%20by%20providing%20a%20richer%20representation%20of%20the%20environment.%0AFurthermore%2C%20we%20demonstrate%20effective%20sim-to-real%20transfer%2C%20ensuring%20that%0Atrained%20policies%20generalize%20well%20to%20real-world%20aquatic%20conditions.%20Experimental%0Aresults%20validate%20our%20method%27s%20capability%20to%20improve%20ASV%20navigation%20performance%0Awhile%20maintaining%20safety%20in%20challenging%20shallow-water%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Constrained%2520ASV%2520Navigation%2520with%2520Deep%2520RL%2520and%2520Limited%2520Sensing%26entry.906535625%3DAmirhossein%2520Zhalehmehrabi%2520and%2520Daniele%2520Meli%2520and%2520Francesco%2520Dal%2520Santo%2520and%2520Francesco%2520Trotti%2520and%2520Alessandro%2520Farinelli%26entry.1292438233%3D%2520%2520Autonomous%2520Surface%2520Vehicles%2520%2528ASVs%2529%2520play%2520a%2520crucial%2520role%2520in%2520maritime%250Aoperations%252C%2520yet%2520their%2520navigation%2520in%2520shallow-water%2520environments%2520remains%250Achallenging%2520due%2520to%2520dynamic%2520disturbances%2520and%2520depth%2520constraints.%2520Traditional%250Anavigation%2520strategies%2520struggle%2520with%2520limited%2520sensor%2520information%252C%2520making%2520safe%2520and%250Aefficient%2520operation%2520difficult.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520reinforcement%250Alearning%2520%2528RL%2529%2520framework%2520for%2520ASV%2520navigation%2520under%2520depth%2520constraints%252C%2520where%2520the%250Avehicle%2520must%2520reach%2520a%2520target%2520while%2520avoiding%2520unsafe%2520areas%2520with%2520only%2520a%2520single%250Adepth%2520measurement%2520per%2520timestep%2520from%2520a%2520downward-facing%2520Single%2520Beam%2520Echosounder%250A%2528SBES%2529.%2520To%2520enhance%2520environmental%2520awareness%252C%2520we%2520integrate%2520Gaussian%2520Process%2520%2528GP%2529%250Aregression%2520into%2520the%2520RL%2520framework%252C%2520enabling%2520the%2520agent%2520to%2520progressively%2520estimate%250Aa%2520bathymetric%2520depth%2520map%2520from%2520sparse%2520sonar%2520readings.%2520This%2520approach%2520improves%250Adecision-making%2520by%2520providing%2520a%2520richer%2520representation%2520of%2520the%2520environment.%250AFurthermore%252C%2520we%2520demonstrate%2520effective%2520sim-to-real%2520transfer%252C%2520ensuring%2520that%250Atrained%2520policies%2520generalize%2520well%2520to%2520real-world%2520aquatic%2520conditions.%2520Experimental%250Aresults%2520validate%2520our%2520method%2527s%2520capability%2520to%2520improve%2520ASV%2520navigation%2520performance%250Awhile%2520maintaining%2520safety%2520in%2520challenging%2520shallow-water%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Constrained%20ASV%20Navigation%20with%20Deep%20RL%20and%20Limited%20Sensing&entry.906535625=Amirhossein%20Zhalehmehrabi%20and%20Daniele%20Meli%20and%20Francesco%20Dal%20Santo%20and%20Francesco%20Trotti%20and%20Alessandro%20Farinelli&entry.1292438233=%20%20Autonomous%20Surface%20Vehicles%20%28ASVs%29%20play%20a%20crucial%20role%20in%20maritime%0Aoperations%2C%20yet%20their%20navigation%20in%20shallow-water%20environments%20remains%0Achallenging%20due%20to%20dynamic%20disturbances%20and%20depth%20constraints.%20Traditional%0Anavigation%20strategies%20struggle%20with%20limited%20sensor%20information%2C%20making%20safe%20and%0Aefficient%20operation%20difficult.%20In%20this%20paper%2C%20we%20propose%20a%20reinforcement%0Alearning%20%28RL%29%20framework%20for%20ASV%20navigation%20under%20depth%20constraints%2C%20where%20the%0Avehicle%20must%20reach%20a%20target%20while%20avoiding%20unsafe%20areas%20with%20only%20a%20single%0Adepth%20measurement%20per%20timestep%20from%20a%20downward-facing%20Single%20Beam%20Echosounder%0A%28SBES%29.%20To%20enhance%20environmental%20awareness%2C%20we%20integrate%20Gaussian%20Process%20%28GP%29%0Aregression%20into%20the%20RL%20framework%2C%20enabling%20the%20agent%20to%20progressively%20estimate%0Aa%20bathymetric%20depth%20map%20from%20sparse%20sonar%20readings.%20This%20approach%20improves%0Adecision-making%20by%20providing%20a%20richer%20representation%20of%20the%20environment.%0AFurthermore%2C%20we%20demonstrate%20effective%20sim-to-real%20transfer%2C%20ensuring%20that%0Atrained%20policies%20generalize%20well%20to%20real-world%20aquatic%20conditions.%20Experimental%0Aresults%20validate%20our%20method%27s%20capability%20to%20improve%20ASV%20navigation%20performance%0Awhile%20maintaining%20safety%20in%20challenging%20shallow-water%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18253v1&entry.124074799=Read"},
{"title": "Generative Auto-Bidding with Value-Guided Explorations", "author": "Jingtong Gao and Yewen Li and Shuai Mao and Peng Jiang and Nan Jiang and Yejing Wang and Qingpeng Cai and Fei Pan and Peng Jiang and Kun Gai and Bo An and Xiangyu Zhao", "abstract": "  Auto-bidding, with its strong capability to optimize bidding decisions within\ndynamic and competitive online environments, has become a pivotal strategy for\nadvertising platforms. Existing approaches typically employ rule-based\nstrategies or Reinforcement Learning (RL) techniques. However, rule-based\nstrategies lack the flexibility to adapt to time-varying market conditions, and\nRL-based methods struggle to capture essential historical dependencies and\nobservations within Markov Decision Process (MDP) frameworks. Furthermore,\nthese approaches often face challenges in ensuring strategy adaptability across\ndiverse advertising objectives. Additionally, as offline training methods are\nincreasingly adopted to facilitate the deployment and maintenance of stable\nonline strategies, the issues of documented behavioral patterns and behavioral\ncollapse resulting from training on fixed offline datasets become increasingly\nsignificant. To address these limitations, this paper introduces a novel\noffline Generative Auto-bidding framework with Value-Guided Explorations\n(GAVE). GAVE accommodates various advertising objectives through a score-based\nReturn-To-Go (RTG) module. Moreover, GAVE integrates an action exploration\nmechanism with an RTG-based evaluation method to explore novel actions while\nensuring stability-preserving updates. A learnable value function is also\ndesigned to guide the direction of action exploration and mitigate\nOut-of-Distribution (OOD) problems. Experimental results on two offline\ndatasets and real-world deployments demonstrate that GAVE outperforms\nstate-of-the-art baselines in both offline evaluations and online A/B tests. By\napplying the core methods of this framework, we proudly secured first place in\nthe NeurIPS 2024 competition, 'AIGB Track: Learning Auto-Bidding Agents with\nGenerative Models'.\n", "link": "http://arxiv.org/abs/2504.14587v2", "date": "2025-04-25", "relevancy": 2.0821, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.552}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4996}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Auto-Bidding%20with%20Value-Guided%20Explorations&body=Title%3A%20Generative%20Auto-Bidding%20with%20Value-Guided%20Explorations%0AAuthor%3A%20Jingtong%20Gao%20and%20Yewen%20Li%20and%20Shuai%20Mao%20and%20Peng%20Jiang%20and%20Nan%20Jiang%20and%20Yejing%20Wang%20and%20Qingpeng%20Cai%20and%20Fei%20Pan%20and%20Peng%20Jiang%20and%20Kun%20Gai%20and%20Bo%20An%20and%20Xiangyu%20Zhao%0AAbstract%3A%20%20%20Auto-bidding%2C%20with%20its%20strong%20capability%20to%20optimize%20bidding%20decisions%20within%0Adynamic%20and%20competitive%20online%20environments%2C%20has%20become%20a%20pivotal%20strategy%20for%0Aadvertising%20platforms.%20Existing%20approaches%20typically%20employ%20rule-based%0Astrategies%20or%20Reinforcement%20Learning%20%28RL%29%20techniques.%20However%2C%20rule-based%0Astrategies%20lack%20the%20flexibility%20to%20adapt%20to%20time-varying%20market%20conditions%2C%20and%0ARL-based%20methods%20struggle%20to%20capture%20essential%20historical%20dependencies%20and%0Aobservations%20within%20Markov%20Decision%20Process%20%28MDP%29%20frameworks.%20Furthermore%2C%0Athese%20approaches%20often%20face%20challenges%20in%20ensuring%20strategy%20adaptability%20across%0Adiverse%20advertising%20objectives.%20Additionally%2C%20as%20offline%20training%20methods%20are%0Aincreasingly%20adopted%20to%20facilitate%20the%20deployment%20and%20maintenance%20of%20stable%0Aonline%20strategies%2C%20the%20issues%20of%20documented%20behavioral%20patterns%20and%20behavioral%0Acollapse%20resulting%20from%20training%20on%20fixed%20offline%20datasets%20become%20increasingly%0Asignificant.%20To%20address%20these%20limitations%2C%20this%20paper%20introduces%20a%20novel%0Aoffline%20Generative%20Auto-bidding%20framework%20with%20Value-Guided%20Explorations%0A%28GAVE%29.%20GAVE%20accommodates%20various%20advertising%20objectives%20through%20a%20score-based%0AReturn-To-Go%20%28RTG%29%20module.%20Moreover%2C%20GAVE%20integrates%20an%20action%20exploration%0Amechanism%20with%20an%20RTG-based%20evaluation%20method%20to%20explore%20novel%20actions%20while%0Aensuring%20stability-preserving%20updates.%20A%20learnable%20value%20function%20is%20also%0Adesigned%20to%20guide%20the%20direction%20of%20action%20exploration%20and%20mitigate%0AOut-of-Distribution%20%28OOD%29%20problems.%20Experimental%20results%20on%20two%20offline%0Adatasets%20and%20real-world%20deployments%20demonstrate%20that%20GAVE%20outperforms%0Astate-of-the-art%20baselines%20in%20both%20offline%20evaluations%20and%20online%20A/B%20tests.%20By%0Aapplying%20the%20core%20methods%20of%20this%20framework%2C%20we%20proudly%20secured%20first%20place%20in%0Athe%20NeurIPS%202024%20competition%2C%20%27AIGB%20Track%3A%20Learning%20Auto-Bidding%20Agents%20with%0AGenerative%20Models%27.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Auto-Bidding%2520with%2520Value-Guided%2520Explorations%26entry.906535625%3DJingtong%2520Gao%2520and%2520Yewen%2520Li%2520and%2520Shuai%2520Mao%2520and%2520Peng%2520Jiang%2520and%2520Nan%2520Jiang%2520and%2520Yejing%2520Wang%2520and%2520Qingpeng%2520Cai%2520and%2520Fei%2520Pan%2520and%2520Peng%2520Jiang%2520and%2520Kun%2520Gai%2520and%2520Bo%2520An%2520and%2520Xiangyu%2520Zhao%26entry.1292438233%3D%2520%2520Auto-bidding%252C%2520with%2520its%2520strong%2520capability%2520to%2520optimize%2520bidding%2520decisions%2520within%250Adynamic%2520and%2520competitive%2520online%2520environments%252C%2520has%2520become%2520a%2520pivotal%2520strategy%2520for%250Aadvertising%2520platforms.%2520Existing%2520approaches%2520typically%2520employ%2520rule-based%250Astrategies%2520or%2520Reinforcement%2520Learning%2520%2528RL%2529%2520techniques.%2520However%252C%2520rule-based%250Astrategies%2520lack%2520the%2520flexibility%2520to%2520adapt%2520to%2520time-varying%2520market%2520conditions%252C%2520and%250ARL-based%2520methods%2520struggle%2520to%2520capture%2520essential%2520historical%2520dependencies%2520and%250Aobservations%2520within%2520Markov%2520Decision%2520Process%2520%2528MDP%2529%2520frameworks.%2520Furthermore%252C%250Athese%2520approaches%2520often%2520face%2520challenges%2520in%2520ensuring%2520strategy%2520adaptability%2520across%250Adiverse%2520advertising%2520objectives.%2520Additionally%252C%2520as%2520offline%2520training%2520methods%2520are%250Aincreasingly%2520adopted%2520to%2520facilitate%2520the%2520deployment%2520and%2520maintenance%2520of%2520stable%250Aonline%2520strategies%252C%2520the%2520issues%2520of%2520documented%2520behavioral%2520patterns%2520and%2520behavioral%250Acollapse%2520resulting%2520from%2520training%2520on%2520fixed%2520offline%2520datasets%2520become%2520increasingly%250Asignificant.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%2520introduces%2520a%2520novel%250Aoffline%2520Generative%2520Auto-bidding%2520framework%2520with%2520Value-Guided%2520Explorations%250A%2528GAVE%2529.%2520GAVE%2520accommodates%2520various%2520advertising%2520objectives%2520through%2520a%2520score-based%250AReturn-To-Go%2520%2528RTG%2529%2520module.%2520Moreover%252C%2520GAVE%2520integrates%2520an%2520action%2520exploration%250Amechanism%2520with%2520an%2520RTG-based%2520evaluation%2520method%2520to%2520explore%2520novel%2520actions%2520while%250Aensuring%2520stability-preserving%2520updates.%2520A%2520learnable%2520value%2520function%2520is%2520also%250Adesigned%2520to%2520guide%2520the%2520direction%2520of%2520action%2520exploration%2520and%2520mitigate%250AOut-of-Distribution%2520%2528OOD%2529%2520problems.%2520Experimental%2520results%2520on%2520two%2520offline%250Adatasets%2520and%2520real-world%2520deployments%2520demonstrate%2520that%2520GAVE%2520outperforms%250Astate-of-the-art%2520baselines%2520in%2520both%2520offline%2520evaluations%2520and%2520online%2520A/B%2520tests.%2520By%250Aapplying%2520the%2520core%2520methods%2520of%2520this%2520framework%252C%2520we%2520proudly%2520secured%2520first%2520place%2520in%250Athe%2520NeurIPS%25202024%2520competition%252C%2520%2527AIGB%2520Track%253A%2520Learning%2520Auto-Bidding%2520Agents%2520with%250AGenerative%2520Models%2527.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Auto-Bidding%20with%20Value-Guided%20Explorations&entry.906535625=Jingtong%20Gao%20and%20Yewen%20Li%20and%20Shuai%20Mao%20and%20Peng%20Jiang%20and%20Nan%20Jiang%20and%20Yejing%20Wang%20and%20Qingpeng%20Cai%20and%20Fei%20Pan%20and%20Peng%20Jiang%20and%20Kun%20Gai%20and%20Bo%20An%20and%20Xiangyu%20Zhao&entry.1292438233=%20%20Auto-bidding%2C%20with%20its%20strong%20capability%20to%20optimize%20bidding%20decisions%20within%0Adynamic%20and%20competitive%20online%20environments%2C%20has%20become%20a%20pivotal%20strategy%20for%0Aadvertising%20platforms.%20Existing%20approaches%20typically%20employ%20rule-based%0Astrategies%20or%20Reinforcement%20Learning%20%28RL%29%20techniques.%20However%2C%20rule-based%0Astrategies%20lack%20the%20flexibility%20to%20adapt%20to%20time-varying%20market%20conditions%2C%20and%0ARL-based%20methods%20struggle%20to%20capture%20essential%20historical%20dependencies%20and%0Aobservations%20within%20Markov%20Decision%20Process%20%28MDP%29%20frameworks.%20Furthermore%2C%0Athese%20approaches%20often%20face%20challenges%20in%20ensuring%20strategy%20adaptability%20across%0Adiverse%20advertising%20objectives.%20Additionally%2C%20as%20offline%20training%20methods%20are%0Aincreasingly%20adopted%20to%20facilitate%20the%20deployment%20and%20maintenance%20of%20stable%0Aonline%20strategies%2C%20the%20issues%20of%20documented%20behavioral%20patterns%20and%20behavioral%0Acollapse%20resulting%20from%20training%20on%20fixed%20offline%20datasets%20become%20increasingly%0Asignificant.%20To%20address%20these%20limitations%2C%20this%20paper%20introduces%20a%20novel%0Aoffline%20Generative%20Auto-bidding%20framework%20with%20Value-Guided%20Explorations%0A%28GAVE%29.%20GAVE%20accommodates%20various%20advertising%20objectives%20through%20a%20score-based%0AReturn-To-Go%20%28RTG%29%20module.%20Moreover%2C%20GAVE%20integrates%20an%20action%20exploration%0Amechanism%20with%20an%20RTG-based%20evaluation%20method%20to%20explore%20novel%20actions%20while%0Aensuring%20stability-preserving%20updates.%20A%20learnable%20value%20function%20is%20also%0Adesigned%20to%20guide%20the%20direction%20of%20action%20exploration%20and%20mitigate%0AOut-of-Distribution%20%28OOD%29%20problems.%20Experimental%20results%20on%20two%20offline%0Adatasets%20and%20real-world%20deployments%20demonstrate%20that%20GAVE%20outperforms%0Astate-of-the-art%20baselines%20in%20both%20offline%20evaluations%20and%20online%20A/B%20tests.%20By%0Aapplying%20the%20core%20methods%20of%20this%20framework%2C%20we%20proudly%20secured%20first%20place%20in%0Athe%20NeurIPS%202024%20competition%2C%20%27AIGB%20Track%3A%20Learning%20Auto-Bidding%20Agents%20with%0AGenerative%20Models%27.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14587v2&entry.124074799=Read"},
{"title": "TSCL:Multi-party loss Balancing scheme for deep learning Image\n  steganography based on Curriculum learning", "author": "Fengchun Liu. Tong Zhang and Chunying Zhang", "abstract": "  For deep learning-based image steganography frameworks, in order to ensure\nthe invisibility and recoverability of the information embedding, the loss\nfunction usually contains several losses such as embedding loss, recovery loss\nand steganalysis loss. In previous research works, fixed loss weights are\nusually chosen for training optimization, and this setting is not linked to the\nimportance of the steganography task itself and the training process. In this\npaper, we propose a Two-stage Curriculum Learning loss scheduler (TSCL) for\nbalancing multinomial losses in deep learning image steganography algorithms.\nTSCL consists of two phases: a priori curriculum control and loss dynamics\ncontrol. The first phase firstly focuses the model on learning the information\nembedding of the original image by controlling the loss weights in the\nmulti-party adversarial training; secondly, it makes the model shift its\nlearning focus to improving the decoding accuracy; and finally, it makes the\nmodel learn to generate a steganographic image that is resistant to\nsteganalysis. In the second stage, the learning speed of each training task is\nevaluated by calculating the loss drop of the before and after iteration rounds\nto balance the learning of each task. Experimental results on three large\npublic datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed TSCL\nstrategy improves the quality of steganography, decoding accuracy and security.\n", "link": "http://arxiv.org/abs/2504.18348v1", "date": "2025-04-25", "relevancy": 2.0731, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5335}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5183}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSCL%3AMulti-party%20loss%20Balancing%20scheme%20for%20deep%20learning%20Image%0A%20%20steganography%20based%20on%20Curriculum%20learning&body=Title%3A%20TSCL%3AMulti-party%20loss%20Balancing%20scheme%20for%20deep%20learning%20Image%0A%20%20steganography%20based%20on%20Curriculum%20learning%0AAuthor%3A%20Fengchun%20Liu.%20Tong%20Zhang%20and%20Chunying%20Zhang%0AAbstract%3A%20%20%20For%20deep%20learning-based%20image%20steganography%20frameworks%2C%20in%20order%20to%20ensure%0Athe%20invisibility%20and%20recoverability%20of%20the%20information%20embedding%2C%20the%20loss%0Afunction%20usually%20contains%20several%20losses%20such%20as%20embedding%20loss%2C%20recovery%20loss%0Aand%20steganalysis%20loss.%20In%20previous%20research%20works%2C%20fixed%20loss%20weights%20are%0Ausually%20chosen%20for%20training%20optimization%2C%20and%20this%20setting%20is%20not%20linked%20to%20the%0Aimportance%20of%20the%20steganography%20task%20itself%20and%20the%20training%20process.%20In%20this%0Apaper%2C%20we%20propose%20a%20Two-stage%20Curriculum%20Learning%20loss%20scheduler%20%28TSCL%29%20for%0Abalancing%20multinomial%20losses%20in%20deep%20learning%20image%20steganography%20algorithms.%0ATSCL%20consists%20of%20two%20phases%3A%20a%20priori%20curriculum%20control%20and%20loss%20dynamics%0Acontrol.%20The%20first%20phase%20firstly%20focuses%20the%20model%20on%20learning%20the%20information%0Aembedding%20of%20the%20original%20image%20by%20controlling%20the%20loss%20weights%20in%20the%0Amulti-party%20adversarial%20training%3B%20secondly%2C%20it%20makes%20the%20model%20shift%20its%0Alearning%20focus%20to%20improving%20the%20decoding%20accuracy%3B%20and%20finally%2C%20it%20makes%20the%0Amodel%20learn%20to%20generate%20a%20steganographic%20image%20that%20is%20resistant%20to%0Asteganalysis.%20In%20the%20second%20stage%2C%20the%20learning%20speed%20of%20each%20training%20task%20is%0Aevaluated%20by%20calculating%20the%20loss%20drop%20of%20the%20before%20and%20after%20iteration%20rounds%0Ato%20balance%20the%20learning%20of%20each%20task.%20Experimental%20results%20on%20three%20large%0Apublic%20datasets%2C%20ALASKA2%2C%20VOC2012%20and%20ImageNet%2C%20show%20that%20the%20proposed%20TSCL%0Astrategy%20improves%20the%20quality%20of%20steganography%2C%20decoding%20accuracy%20and%20security.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSCL%253AMulti-party%2520loss%2520Balancing%2520scheme%2520for%2520deep%2520learning%2520Image%250A%2520%2520steganography%2520based%2520on%2520Curriculum%2520learning%26entry.906535625%3DFengchun%2520Liu.%2520Tong%2520Zhang%2520and%2520Chunying%2520Zhang%26entry.1292438233%3D%2520%2520For%2520deep%2520learning-based%2520image%2520steganography%2520frameworks%252C%2520in%2520order%2520to%2520ensure%250Athe%2520invisibility%2520and%2520recoverability%2520of%2520the%2520information%2520embedding%252C%2520the%2520loss%250Afunction%2520usually%2520contains%2520several%2520losses%2520such%2520as%2520embedding%2520loss%252C%2520recovery%2520loss%250Aand%2520steganalysis%2520loss.%2520In%2520previous%2520research%2520works%252C%2520fixed%2520loss%2520weights%2520are%250Ausually%2520chosen%2520for%2520training%2520optimization%252C%2520and%2520this%2520setting%2520is%2520not%2520linked%2520to%2520the%250Aimportance%2520of%2520the%2520steganography%2520task%2520itself%2520and%2520the%2520training%2520process.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520Two-stage%2520Curriculum%2520Learning%2520loss%2520scheduler%2520%2528TSCL%2529%2520for%250Abalancing%2520multinomial%2520losses%2520in%2520deep%2520learning%2520image%2520steganography%2520algorithms.%250ATSCL%2520consists%2520of%2520two%2520phases%253A%2520a%2520priori%2520curriculum%2520control%2520and%2520loss%2520dynamics%250Acontrol.%2520The%2520first%2520phase%2520firstly%2520focuses%2520the%2520model%2520on%2520learning%2520the%2520information%250Aembedding%2520of%2520the%2520original%2520image%2520by%2520controlling%2520the%2520loss%2520weights%2520in%2520the%250Amulti-party%2520adversarial%2520training%253B%2520secondly%252C%2520it%2520makes%2520the%2520model%2520shift%2520its%250Alearning%2520focus%2520to%2520improving%2520the%2520decoding%2520accuracy%253B%2520and%2520finally%252C%2520it%2520makes%2520the%250Amodel%2520learn%2520to%2520generate%2520a%2520steganographic%2520image%2520that%2520is%2520resistant%2520to%250Asteganalysis.%2520In%2520the%2520second%2520stage%252C%2520the%2520learning%2520speed%2520of%2520each%2520training%2520task%2520is%250Aevaluated%2520by%2520calculating%2520the%2520loss%2520drop%2520of%2520the%2520before%2520and%2520after%2520iteration%2520rounds%250Ato%2520balance%2520the%2520learning%2520of%2520each%2520task.%2520Experimental%2520results%2520on%2520three%2520large%250Apublic%2520datasets%252C%2520ALASKA2%252C%2520VOC2012%2520and%2520ImageNet%252C%2520show%2520that%2520the%2520proposed%2520TSCL%250Astrategy%2520improves%2520the%2520quality%2520of%2520steganography%252C%2520decoding%2520accuracy%2520and%2520security.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSCL%3AMulti-party%20loss%20Balancing%20scheme%20for%20deep%20learning%20Image%0A%20%20steganography%20based%20on%20Curriculum%20learning&entry.906535625=Fengchun%20Liu.%20Tong%20Zhang%20and%20Chunying%20Zhang&entry.1292438233=%20%20For%20deep%20learning-based%20image%20steganography%20frameworks%2C%20in%20order%20to%20ensure%0Athe%20invisibility%20and%20recoverability%20of%20the%20information%20embedding%2C%20the%20loss%0Afunction%20usually%20contains%20several%20losses%20such%20as%20embedding%20loss%2C%20recovery%20loss%0Aand%20steganalysis%20loss.%20In%20previous%20research%20works%2C%20fixed%20loss%20weights%20are%0Ausually%20chosen%20for%20training%20optimization%2C%20and%20this%20setting%20is%20not%20linked%20to%20the%0Aimportance%20of%20the%20steganography%20task%20itself%20and%20the%20training%20process.%20In%20this%0Apaper%2C%20we%20propose%20a%20Two-stage%20Curriculum%20Learning%20loss%20scheduler%20%28TSCL%29%20for%0Abalancing%20multinomial%20losses%20in%20deep%20learning%20image%20steganography%20algorithms.%0ATSCL%20consists%20of%20two%20phases%3A%20a%20priori%20curriculum%20control%20and%20loss%20dynamics%0Acontrol.%20The%20first%20phase%20firstly%20focuses%20the%20model%20on%20learning%20the%20information%0Aembedding%20of%20the%20original%20image%20by%20controlling%20the%20loss%20weights%20in%20the%0Amulti-party%20adversarial%20training%3B%20secondly%2C%20it%20makes%20the%20model%20shift%20its%0Alearning%20focus%20to%20improving%20the%20decoding%20accuracy%3B%20and%20finally%2C%20it%20makes%20the%0Amodel%20learn%20to%20generate%20a%20steganographic%20image%20that%20is%20resistant%20to%0Asteganalysis.%20In%20the%20second%20stage%2C%20the%20learning%20speed%20of%20each%20training%20task%20is%0Aevaluated%20by%20calculating%20the%20loss%20drop%20of%20the%20before%20and%20after%20iteration%20rounds%0Ato%20balance%20the%20learning%20of%20each%20task.%20Experimental%20results%20on%20three%20large%0Apublic%20datasets%2C%20ALASKA2%2C%20VOC2012%20and%20ImageNet%2C%20show%20that%20the%20proposed%20TSCL%0Astrategy%20improves%20the%20quality%20of%20steganography%2C%20decoding%20accuracy%20and%20security.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18348v1&entry.124074799=Read"},
{"title": "Generative AI-Powered Plugin for Robust Federated Learning in\n  Heterogeneous IoT Networks", "author": "Youngjoon Lee and Jinu Gong and Joonhyuk Kang", "abstract": "  Federated learning enables edge devices to collaboratively train a global\nmodel while maintaining data privacy by keeping data localized. However, the\nNon-IID nature of data distribution across devices often hinders model\nconvergence and reduces performance. In this paper, we propose a novel plugin\nfor federated optimization techniques that approximates Non-IID data\ndistributions to IID through generative AI-enhanced data augmentation and\nbalanced sampling strategy. Key idea is to synthesize additional data for\nunderrepresented classes on each edge device, leveraging generative AI to\ncreate a more balanced dataset across the FL network. Additionally, a balanced\nsampling approach at the central server selectively includes only the most\nIID-like devices, accelerating convergence while maximizing the global model's\nperformance. Experimental results validate that our approach significantly\nimproves convergence speed and robustness against data imbalance, establishing\na flexible, privacy-preserving FL plugin that is applicable even in data-scarce\nenvironments.\n", "link": "http://arxiv.org/abs/2410.23824v2", "date": "2025-04-25", "relevancy": 2.0696, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5264}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.522}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI-Powered%20Plugin%20for%20Robust%20Federated%20Learning%20in%0A%20%20Heterogeneous%20IoT%20Networks&body=Title%3A%20Generative%20AI-Powered%20Plugin%20for%20Robust%20Federated%20Learning%20in%0A%20%20Heterogeneous%20IoT%20Networks%0AAuthor%3A%20Youngjoon%20Lee%20and%20Jinu%20Gong%20and%20Joonhyuk%20Kang%0AAbstract%3A%20%20%20Federated%20learning%20enables%20edge%20devices%20to%20collaboratively%20train%20a%20global%0Amodel%20while%20maintaining%20data%20privacy%20by%20keeping%20data%20localized.%20However%2C%20the%0ANon-IID%20nature%20of%20data%20distribution%20across%20devices%20often%20hinders%20model%0Aconvergence%20and%20reduces%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20plugin%0Afor%20federated%20optimization%20techniques%20that%20approximates%20Non-IID%20data%0Adistributions%20to%20IID%20through%20generative%20AI-enhanced%20data%20augmentation%20and%0Abalanced%20sampling%20strategy.%20Key%20idea%20is%20to%20synthesize%20additional%20data%20for%0Aunderrepresented%20classes%20on%20each%20edge%20device%2C%20leveraging%20generative%20AI%20to%0Acreate%20a%20more%20balanced%20dataset%20across%20the%20FL%20network.%20Additionally%2C%20a%20balanced%0Asampling%20approach%20at%20the%20central%20server%20selectively%20includes%20only%20the%20most%0AIID-like%20devices%2C%20accelerating%20convergence%20while%20maximizing%20the%20global%20model%27s%0Aperformance.%20Experimental%20results%20validate%20that%20our%20approach%20significantly%0Aimproves%20convergence%20speed%20and%20robustness%20against%20data%20imbalance%2C%20establishing%0Aa%20flexible%2C%20privacy-preserving%20FL%20plugin%20that%20is%20applicable%20even%20in%20data-scarce%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23824v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI-Powered%2520Plugin%2520for%2520Robust%2520Federated%2520Learning%2520in%250A%2520%2520Heterogeneous%2520IoT%2520Networks%26entry.906535625%3DYoungjoon%2520Lee%2520and%2520Jinu%2520Gong%2520and%2520Joonhyuk%2520Kang%26entry.1292438233%3D%2520%2520Federated%2520learning%2520enables%2520edge%2520devices%2520to%2520collaboratively%2520train%2520a%2520global%250Amodel%2520while%2520maintaining%2520data%2520privacy%2520by%2520keeping%2520data%2520localized.%2520However%252C%2520the%250ANon-IID%2520nature%2520of%2520data%2520distribution%2520across%2520devices%2520often%2520hinders%2520model%250Aconvergence%2520and%2520reduces%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520plugin%250Afor%2520federated%2520optimization%2520techniques%2520that%2520approximates%2520Non-IID%2520data%250Adistributions%2520to%2520IID%2520through%2520generative%2520AI-enhanced%2520data%2520augmentation%2520and%250Abalanced%2520sampling%2520strategy.%2520Key%2520idea%2520is%2520to%2520synthesize%2520additional%2520data%2520for%250Aunderrepresented%2520classes%2520on%2520each%2520edge%2520device%252C%2520leveraging%2520generative%2520AI%2520to%250Acreate%2520a%2520more%2520balanced%2520dataset%2520across%2520the%2520FL%2520network.%2520Additionally%252C%2520a%2520balanced%250Asampling%2520approach%2520at%2520the%2520central%2520server%2520selectively%2520includes%2520only%2520the%2520most%250AIID-like%2520devices%252C%2520accelerating%2520convergence%2520while%2520maximizing%2520the%2520global%2520model%2527s%250Aperformance.%2520Experimental%2520results%2520validate%2520that%2520our%2520approach%2520significantly%250Aimproves%2520convergence%2520speed%2520and%2520robustness%2520against%2520data%2520imbalance%252C%2520establishing%250Aa%2520flexible%252C%2520privacy-preserving%2520FL%2520plugin%2520that%2520is%2520applicable%2520even%2520in%2520data-scarce%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23824v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI-Powered%20Plugin%20for%20Robust%20Federated%20Learning%20in%0A%20%20Heterogeneous%20IoT%20Networks&entry.906535625=Youngjoon%20Lee%20and%20Jinu%20Gong%20and%20Joonhyuk%20Kang&entry.1292438233=%20%20Federated%20learning%20enables%20edge%20devices%20to%20collaboratively%20train%20a%20global%0Amodel%20while%20maintaining%20data%20privacy%20by%20keeping%20data%20localized.%20However%2C%20the%0ANon-IID%20nature%20of%20data%20distribution%20across%20devices%20often%20hinders%20model%0Aconvergence%20and%20reduces%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20plugin%0Afor%20federated%20optimization%20techniques%20that%20approximates%20Non-IID%20data%0Adistributions%20to%20IID%20through%20generative%20AI-enhanced%20data%20augmentation%20and%0Abalanced%20sampling%20strategy.%20Key%20idea%20is%20to%20synthesize%20additional%20data%20for%0Aunderrepresented%20classes%20on%20each%20edge%20device%2C%20leveraging%20generative%20AI%20to%0Acreate%20a%20more%20balanced%20dataset%20across%20the%20FL%20network.%20Additionally%2C%20a%20balanced%0Asampling%20approach%20at%20the%20central%20server%20selectively%20includes%20only%20the%20most%0AIID-like%20devices%2C%20accelerating%20convergence%20while%20maximizing%20the%20global%20model%27s%0Aperformance.%20Experimental%20results%20validate%20that%20our%20approach%20significantly%0Aimproves%20convergence%20speed%20and%20robustness%20against%20data%20imbalance%2C%20establishing%0Aa%20flexible%2C%20privacy-preserving%20FL%20plugin%20that%20is%20applicable%20even%20in%20data-scarce%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23824v2&entry.124074799=Read"},
{"title": "Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning\n  for Verifiable Report Generation", "author": "Peiyuan Jing and Kinhei Lee and Zhenxuan Zhang and Huichi Zhou and Zhengqing Yuan and Zhifan Gao and Lei Zhu and Giorgos Papanastasiou and Yingying Fang and Guang Yang", "abstract": "  Radiology report generation is critical for efficiency but current models\nlack the structured reasoning of experts, hindering clinical trust and\nexplainability by failing to link visual findings to precise anatomical\nlocations. This paper introduces BoxMed-RL, a groundbreaking unified training\nframework for generating spatially verifiable and explainable radiology\nreports. Built on a large vision-language model, BoxMed-RL revolutionizes\nreport generation through two integrated phases: (1) In the Pretraining Phase,\nwe refine the model via medical concept learning, using Chain-of-Thought\nsupervision to internalize the radiologist-like workflow, followed by spatially\nverifiable reinforcement, which applies reinforcement learning to align medical\nfindings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze\nthe pretrained weights and train a downstream adapter to ensure fluent and\nclinically credible reports. This framework precisely mimics radiologists'\nworkflow, compelling the model to connect high-level medical concepts with\ndefinitive anatomical evidence. Extensive experiments on public datasets\ndemonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR\nand ROUGE-L metrics compared to state-of-the-art methods. An average 5%\nimprovement in large language model-based metrics further underscores\nBoxMed-RL's robustness in generating high-quality radiology reports.\n", "link": "http://arxiv.org/abs/2504.18453v1", "date": "2025-04-25", "relevancy": 2.0548, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5117}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reason%20Like%20a%20Radiologist%3A%20Chain-of-Thought%20and%20Reinforcement%20Learning%0A%20%20for%20Verifiable%20Report%20Generation&body=Title%3A%20Reason%20Like%20a%20Radiologist%3A%20Chain-of-Thought%20and%20Reinforcement%20Learning%0A%20%20for%20Verifiable%20Report%20Generation%0AAuthor%3A%20Peiyuan%20Jing%20and%20Kinhei%20Lee%20and%20Zhenxuan%20Zhang%20and%20Huichi%20Zhou%20and%20Zhengqing%20Yuan%20and%20Zhifan%20Gao%20and%20Lei%20Zhu%20and%20Giorgos%20Papanastasiou%20and%20Yingying%20Fang%20and%20Guang%20Yang%0AAbstract%3A%20%20%20Radiology%20report%20generation%20is%20critical%20for%20efficiency%20but%20current%20models%0Alack%20the%20structured%20reasoning%20of%20experts%2C%20hindering%20clinical%20trust%20and%0Aexplainability%20by%20failing%20to%20link%20visual%20findings%20to%20precise%20anatomical%0Alocations.%20This%20paper%20introduces%20BoxMed-RL%2C%20a%20groundbreaking%20unified%20training%0Aframework%20for%20generating%20spatially%20verifiable%20and%20explainable%20radiology%0Areports.%20Built%20on%20a%20large%20vision-language%20model%2C%20BoxMed-RL%20revolutionizes%0Areport%20generation%20through%20two%20integrated%20phases%3A%20%281%29%20In%20the%20Pretraining%20Phase%2C%0Awe%20refine%20the%20model%20via%20medical%20concept%20learning%2C%20using%20Chain-of-Thought%0Asupervision%20to%20internalize%20the%20radiologist-like%20workflow%2C%20followed%20by%20spatially%0Averifiable%20reinforcement%2C%20which%20applies%20reinforcement%20learning%20to%20align%20medical%0Afindings%20with%20bounding%20boxes.%20%282%29%20In%20the%20Downstream%20Adapter%20Phase%2C%20we%20freeze%0Athe%20pretrained%20weights%20and%20train%20a%20downstream%20adapter%20to%20ensure%20fluent%20and%0Aclinically%20credible%20reports.%20This%20framework%20precisely%20mimics%20radiologists%27%0Aworkflow%2C%20compelling%20the%20model%20to%20connect%20high-level%20medical%20concepts%20with%0Adefinitive%20anatomical%20evidence.%20Extensive%20experiments%20on%20public%20datasets%0Ademonstrate%20that%20BoxMed-RL%20achieves%20an%20average%207%25%20improvement%20in%20both%20METEOR%0Aand%20ROUGE-L%20metrics%20compared%20to%20state-of-the-art%20methods.%20An%20average%205%25%0Aimprovement%20in%20large%20language%20model-based%20metrics%20further%20underscores%0ABoxMed-RL%27s%20robustness%20in%20generating%20high-quality%20radiology%20reports.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReason%2520Like%2520a%2520Radiologist%253A%2520Chain-of-Thought%2520and%2520Reinforcement%2520Learning%250A%2520%2520for%2520Verifiable%2520Report%2520Generation%26entry.906535625%3DPeiyuan%2520Jing%2520and%2520Kinhei%2520Lee%2520and%2520Zhenxuan%2520Zhang%2520and%2520Huichi%2520Zhou%2520and%2520Zhengqing%2520Yuan%2520and%2520Zhifan%2520Gao%2520and%2520Lei%2520Zhu%2520and%2520Giorgos%2520Papanastasiou%2520and%2520Yingying%2520Fang%2520and%2520Guang%2520Yang%26entry.1292438233%3D%2520%2520Radiology%2520report%2520generation%2520is%2520critical%2520for%2520efficiency%2520but%2520current%2520models%250Alack%2520the%2520structured%2520reasoning%2520of%2520experts%252C%2520hindering%2520clinical%2520trust%2520and%250Aexplainability%2520by%2520failing%2520to%2520link%2520visual%2520findings%2520to%2520precise%2520anatomical%250Alocations.%2520This%2520paper%2520introduces%2520BoxMed-RL%252C%2520a%2520groundbreaking%2520unified%2520training%250Aframework%2520for%2520generating%2520spatially%2520verifiable%2520and%2520explainable%2520radiology%250Areports.%2520Built%2520on%2520a%2520large%2520vision-language%2520model%252C%2520BoxMed-RL%2520revolutionizes%250Areport%2520generation%2520through%2520two%2520integrated%2520phases%253A%2520%25281%2529%2520In%2520the%2520Pretraining%2520Phase%252C%250Awe%2520refine%2520the%2520model%2520via%2520medical%2520concept%2520learning%252C%2520using%2520Chain-of-Thought%250Asupervision%2520to%2520internalize%2520the%2520radiologist-like%2520workflow%252C%2520followed%2520by%2520spatially%250Averifiable%2520reinforcement%252C%2520which%2520applies%2520reinforcement%2520learning%2520to%2520align%2520medical%250Afindings%2520with%2520bounding%2520boxes.%2520%25282%2529%2520In%2520the%2520Downstream%2520Adapter%2520Phase%252C%2520we%2520freeze%250Athe%2520pretrained%2520weights%2520and%2520train%2520a%2520downstream%2520adapter%2520to%2520ensure%2520fluent%2520and%250Aclinically%2520credible%2520reports.%2520This%2520framework%2520precisely%2520mimics%2520radiologists%2527%250Aworkflow%252C%2520compelling%2520the%2520model%2520to%2520connect%2520high-level%2520medical%2520concepts%2520with%250Adefinitive%2520anatomical%2520evidence.%2520Extensive%2520experiments%2520on%2520public%2520datasets%250Ademonstrate%2520that%2520BoxMed-RL%2520achieves%2520an%2520average%25207%2525%2520improvement%2520in%2520both%2520METEOR%250Aand%2520ROUGE-L%2520metrics%2520compared%2520to%2520state-of-the-art%2520methods.%2520An%2520average%25205%2525%250Aimprovement%2520in%2520large%2520language%2520model-based%2520metrics%2520further%2520underscores%250ABoxMed-RL%2527s%2520robustness%2520in%2520generating%2520high-quality%2520radiology%2520reports.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reason%20Like%20a%20Radiologist%3A%20Chain-of-Thought%20and%20Reinforcement%20Learning%0A%20%20for%20Verifiable%20Report%20Generation&entry.906535625=Peiyuan%20Jing%20and%20Kinhei%20Lee%20and%20Zhenxuan%20Zhang%20and%20Huichi%20Zhou%20and%20Zhengqing%20Yuan%20and%20Zhifan%20Gao%20and%20Lei%20Zhu%20and%20Giorgos%20Papanastasiou%20and%20Yingying%20Fang%20and%20Guang%20Yang&entry.1292438233=%20%20Radiology%20report%20generation%20is%20critical%20for%20efficiency%20but%20current%20models%0Alack%20the%20structured%20reasoning%20of%20experts%2C%20hindering%20clinical%20trust%20and%0Aexplainability%20by%20failing%20to%20link%20visual%20findings%20to%20precise%20anatomical%0Alocations.%20This%20paper%20introduces%20BoxMed-RL%2C%20a%20groundbreaking%20unified%20training%0Aframework%20for%20generating%20spatially%20verifiable%20and%20explainable%20radiology%0Areports.%20Built%20on%20a%20large%20vision-language%20model%2C%20BoxMed-RL%20revolutionizes%0Areport%20generation%20through%20two%20integrated%20phases%3A%20%281%29%20In%20the%20Pretraining%20Phase%2C%0Awe%20refine%20the%20model%20via%20medical%20concept%20learning%2C%20using%20Chain-of-Thought%0Asupervision%20to%20internalize%20the%20radiologist-like%20workflow%2C%20followed%20by%20spatially%0Averifiable%20reinforcement%2C%20which%20applies%20reinforcement%20learning%20to%20align%20medical%0Afindings%20with%20bounding%20boxes.%20%282%29%20In%20the%20Downstream%20Adapter%20Phase%2C%20we%20freeze%0Athe%20pretrained%20weights%20and%20train%20a%20downstream%20adapter%20to%20ensure%20fluent%20and%0Aclinically%20credible%20reports.%20This%20framework%20precisely%20mimics%20radiologists%27%0Aworkflow%2C%20compelling%20the%20model%20to%20connect%20high-level%20medical%20concepts%20with%0Adefinitive%20anatomical%20evidence.%20Extensive%20experiments%20on%20public%20datasets%0Ademonstrate%20that%20BoxMed-RL%20achieves%20an%20average%207%25%20improvement%20in%20both%20METEOR%0Aand%20ROUGE-L%20metrics%20compared%20to%20state-of-the-art%20methods.%20An%20average%205%25%0Aimprovement%20in%20large%20language%20model-based%20metrics%20further%20underscores%0ABoxMed-RL%27s%20robustness%20in%20generating%20high-quality%20radiology%20reports.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18453v1&entry.124074799=Read"},
{"title": "Enhanced Sampling, Public Dataset and Generative Model for Drug-Protein\n  Dissociation Dynamics", "author": "Maodong Li and Jiying Zhang and Bin Feng and Wenqi Zeng and Dechin Chen and Zhijun Pan and Yu Li and Zijing Liu and Yi Isaac Yang", "abstract": "  Drug-protein binding and dissociation dynamics are fundamental to\nunderstanding molecular interactions in biological systems. While many tools\nfor drug-protein interaction studies have emerged, especially artificial\nintelligence (AI)-based generative models, predictive tools on\nbinding/dissociation kinetics and dynamics are still limited. We propose a\nnovel research paradigm that combines molecular dynamics (MD) simulations,\nenhanced sampling, and AI generative models to address this issue. We propose\nan enhanced sampling strategy to efficiently implement the drug-protein\ndissociation process in MD simulations and estimate the free energy surface\n(FES). We constructed a program pipeline of MD simulations based on this\nsampling strategy, thus generating a dataset including 26,612 drug-protein\ndissociation trajectories containing about 13 million frames. We named this\ndissociation dynamics dataset DD-13M and used it to train a deep equivariant\ngenerative model UnbindingFlow, which can generate collision-free dissociation\ntrajectories. The DD-13M database and UnbindingFlow model represent a\nsignificant advancement in computational structural biology, and we anticipate\nits broad applicability in machine learning studies of drug-protein\ninteractions. Our ongoing efforts focus on expanding this methodology to\nencompass a broader spectrum of drug-protein complexes and exploring novel\napplications in pathway prediction.\n", "link": "http://arxiv.org/abs/2504.18367v1", "date": "2025-04-25", "relevancy": 2.0543, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5191}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5125}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Sampling%2C%20Public%20Dataset%20and%20Generative%20Model%20for%20Drug-Protein%0A%20%20Dissociation%20Dynamics&body=Title%3A%20Enhanced%20Sampling%2C%20Public%20Dataset%20and%20Generative%20Model%20for%20Drug-Protein%0A%20%20Dissociation%20Dynamics%0AAuthor%3A%20Maodong%20Li%20and%20Jiying%20Zhang%20and%20Bin%20Feng%20and%20Wenqi%20Zeng%20and%20Dechin%20Chen%20and%20Zhijun%20Pan%20and%20Yu%20Li%20and%20Zijing%20Liu%20and%20Yi%20Isaac%20Yang%0AAbstract%3A%20%20%20Drug-protein%20binding%20and%20dissociation%20dynamics%20are%20fundamental%20to%0Aunderstanding%20molecular%20interactions%20in%20biological%20systems.%20While%20many%20tools%0Afor%20drug-protein%20interaction%20studies%20have%20emerged%2C%20especially%20artificial%0Aintelligence%20%28AI%29-based%20generative%20models%2C%20predictive%20tools%20on%0Abinding/dissociation%20kinetics%20and%20dynamics%20are%20still%20limited.%20We%20propose%20a%0Anovel%20research%20paradigm%20that%20combines%20molecular%20dynamics%20%28MD%29%20simulations%2C%0Aenhanced%20sampling%2C%20and%20AI%20generative%20models%20to%20address%20this%20issue.%20We%20propose%0Aan%20enhanced%20sampling%20strategy%20to%20efficiently%20implement%20the%20drug-protein%0Adissociation%20process%20in%20MD%20simulations%20and%20estimate%20the%20free%20energy%20surface%0A%28FES%29.%20We%20constructed%20a%20program%20pipeline%20of%20MD%20simulations%20based%20on%20this%0Asampling%20strategy%2C%20thus%20generating%20a%20dataset%20including%2026%2C612%20drug-protein%0Adissociation%20trajectories%20containing%20about%2013%20million%20frames.%20We%20named%20this%0Adissociation%20dynamics%20dataset%20DD-13M%20and%20used%20it%20to%20train%20a%20deep%20equivariant%0Agenerative%20model%20UnbindingFlow%2C%20which%20can%20generate%20collision-free%20dissociation%0Atrajectories.%20The%20DD-13M%20database%20and%20UnbindingFlow%20model%20represent%20a%0Asignificant%20advancement%20in%20computational%20structural%20biology%2C%20and%20we%20anticipate%0Aits%20broad%20applicability%20in%20machine%20learning%20studies%20of%20drug-protein%0Ainteractions.%20Our%20ongoing%20efforts%20focus%20on%20expanding%20this%20methodology%20to%0Aencompass%20a%20broader%20spectrum%20of%20drug-protein%20complexes%20and%20exploring%20novel%0Aapplications%20in%20pathway%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Sampling%252C%2520Public%2520Dataset%2520and%2520Generative%2520Model%2520for%2520Drug-Protein%250A%2520%2520Dissociation%2520Dynamics%26entry.906535625%3DMaodong%2520Li%2520and%2520Jiying%2520Zhang%2520and%2520Bin%2520Feng%2520and%2520Wenqi%2520Zeng%2520and%2520Dechin%2520Chen%2520and%2520Zhijun%2520Pan%2520and%2520Yu%2520Li%2520and%2520Zijing%2520Liu%2520and%2520Yi%2520Isaac%2520Yang%26entry.1292438233%3D%2520%2520Drug-protein%2520binding%2520and%2520dissociation%2520dynamics%2520are%2520fundamental%2520to%250Aunderstanding%2520molecular%2520interactions%2520in%2520biological%2520systems.%2520While%2520many%2520tools%250Afor%2520drug-protein%2520interaction%2520studies%2520have%2520emerged%252C%2520especially%2520artificial%250Aintelligence%2520%2528AI%2529-based%2520generative%2520models%252C%2520predictive%2520tools%2520on%250Abinding/dissociation%2520kinetics%2520and%2520dynamics%2520are%2520still%2520limited.%2520We%2520propose%2520a%250Anovel%2520research%2520paradigm%2520that%2520combines%2520molecular%2520dynamics%2520%2528MD%2529%2520simulations%252C%250Aenhanced%2520sampling%252C%2520and%2520AI%2520generative%2520models%2520to%2520address%2520this%2520issue.%2520We%2520propose%250Aan%2520enhanced%2520sampling%2520strategy%2520to%2520efficiently%2520implement%2520the%2520drug-protein%250Adissociation%2520process%2520in%2520MD%2520simulations%2520and%2520estimate%2520the%2520free%2520energy%2520surface%250A%2528FES%2529.%2520We%2520constructed%2520a%2520program%2520pipeline%2520of%2520MD%2520simulations%2520based%2520on%2520this%250Asampling%2520strategy%252C%2520thus%2520generating%2520a%2520dataset%2520including%252026%252C612%2520drug-protein%250Adissociation%2520trajectories%2520containing%2520about%252013%2520million%2520frames.%2520We%2520named%2520this%250Adissociation%2520dynamics%2520dataset%2520DD-13M%2520and%2520used%2520it%2520to%2520train%2520a%2520deep%2520equivariant%250Agenerative%2520model%2520UnbindingFlow%252C%2520which%2520can%2520generate%2520collision-free%2520dissociation%250Atrajectories.%2520The%2520DD-13M%2520database%2520and%2520UnbindingFlow%2520model%2520represent%2520a%250Asignificant%2520advancement%2520in%2520computational%2520structural%2520biology%252C%2520and%2520we%2520anticipate%250Aits%2520broad%2520applicability%2520in%2520machine%2520learning%2520studies%2520of%2520drug-protein%250Ainteractions.%2520Our%2520ongoing%2520efforts%2520focus%2520on%2520expanding%2520this%2520methodology%2520to%250Aencompass%2520a%2520broader%2520spectrum%2520of%2520drug-protein%2520complexes%2520and%2520exploring%2520novel%250Aapplications%2520in%2520pathway%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Sampling%2C%20Public%20Dataset%20and%20Generative%20Model%20for%20Drug-Protein%0A%20%20Dissociation%20Dynamics&entry.906535625=Maodong%20Li%20and%20Jiying%20Zhang%20and%20Bin%20Feng%20and%20Wenqi%20Zeng%20and%20Dechin%20Chen%20and%20Zhijun%20Pan%20and%20Yu%20Li%20and%20Zijing%20Liu%20and%20Yi%20Isaac%20Yang&entry.1292438233=%20%20Drug-protein%20binding%20and%20dissociation%20dynamics%20are%20fundamental%20to%0Aunderstanding%20molecular%20interactions%20in%20biological%20systems.%20While%20many%20tools%0Afor%20drug-protein%20interaction%20studies%20have%20emerged%2C%20especially%20artificial%0Aintelligence%20%28AI%29-based%20generative%20models%2C%20predictive%20tools%20on%0Abinding/dissociation%20kinetics%20and%20dynamics%20are%20still%20limited.%20We%20propose%20a%0Anovel%20research%20paradigm%20that%20combines%20molecular%20dynamics%20%28MD%29%20simulations%2C%0Aenhanced%20sampling%2C%20and%20AI%20generative%20models%20to%20address%20this%20issue.%20We%20propose%0Aan%20enhanced%20sampling%20strategy%20to%20efficiently%20implement%20the%20drug-protein%0Adissociation%20process%20in%20MD%20simulations%20and%20estimate%20the%20free%20energy%20surface%0A%28FES%29.%20We%20constructed%20a%20program%20pipeline%20of%20MD%20simulations%20based%20on%20this%0Asampling%20strategy%2C%20thus%20generating%20a%20dataset%20including%2026%2C612%20drug-protein%0Adissociation%20trajectories%20containing%20about%2013%20million%20frames.%20We%20named%20this%0Adissociation%20dynamics%20dataset%20DD-13M%20and%20used%20it%20to%20train%20a%20deep%20equivariant%0Agenerative%20model%20UnbindingFlow%2C%20which%20can%20generate%20collision-free%20dissociation%0Atrajectories.%20The%20DD-13M%20database%20and%20UnbindingFlow%20model%20represent%20a%0Asignificant%20advancement%20in%20computational%20structural%20biology%2C%20and%20we%20anticipate%0Aits%20broad%20applicability%20in%20machine%20learning%20studies%20of%20drug-protein%0Ainteractions.%20Our%20ongoing%20efforts%20focus%20on%20expanding%20this%20methodology%20to%0Aencompass%20a%20broader%20spectrum%20of%20drug-protein%20complexes%20and%20exploring%20novel%0Aapplications%20in%20pathway%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18367v1&entry.124074799=Read"},
{"title": "Augmenting Perceptual Super-Resolution via Image Quality Predictors", "author": "Fengjia Zhang and Samrudhdhi B. Rangrej and Tristan Aumentado-Armstrong and Afsaneh Fazly and Alex Levinshtein", "abstract": "  Super-resolution (SR), a classical inverse problem in computer vision, is\ninherently ill-posed, inducing a distribution of plausible solutions for every\ninput. However, the desired result is not simply the expectation of this\ndistribution, which is the blurry image obtained by minimizing pixelwise error,\nbut rather the sample with the highest image quality. A variety of techniques,\nfrom perceptual metrics to adversarial losses, are employed to this end. In\nthis work, we explore an alternative: utilizing powerful non-reference image\nquality assessment (NR-IQA) models in the SR context. We begin with a\ncomprehensive analysis of NR-IQA metrics on human-derived SR data, identifying\nboth the accuracy (human alignment) and complementarity of different metrics.\nThen, we explore two methods of applying NR-IQA models to SR learning: (i)\naltering data sampling, by building on an existing multi-ground-truth SR\nframework, and (ii) directly optimizing a differentiable quality score. Our\nresults demonstrate a more human-centric perception-distortion tradeoff,\nfocusing less on non-perceptual pixel-wise distortion, instead improving the\nbalance between perceptual fidelity and human-tuned NR-IQA measures.\n", "link": "http://arxiv.org/abs/2504.18524v1", "date": "2025-04-25", "relevancy": 2.0485, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5277}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.513}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmenting%20Perceptual%20Super-Resolution%20via%20Image%20Quality%20Predictors&body=Title%3A%20Augmenting%20Perceptual%20Super-Resolution%20via%20Image%20Quality%20Predictors%0AAuthor%3A%20Fengjia%20Zhang%20and%20Samrudhdhi%20B.%20Rangrej%20and%20Tristan%20Aumentado-Armstrong%20and%20Afsaneh%20Fazly%20and%20Alex%20Levinshtein%0AAbstract%3A%20%20%20Super-resolution%20%28SR%29%2C%20a%20classical%20inverse%20problem%20in%20computer%20vision%2C%20is%0Ainherently%20ill-posed%2C%20inducing%20a%20distribution%20of%20plausible%20solutions%20for%20every%0Ainput.%20However%2C%20the%20desired%20result%20is%20not%20simply%20the%20expectation%20of%20this%0Adistribution%2C%20which%20is%20the%20blurry%20image%20obtained%20by%20minimizing%20pixelwise%20error%2C%0Abut%20rather%20the%20sample%20with%20the%20highest%20image%20quality.%20A%20variety%20of%20techniques%2C%0Afrom%20perceptual%20metrics%20to%20adversarial%20losses%2C%20are%20employed%20to%20this%20end.%20In%0Athis%20work%2C%20we%20explore%20an%20alternative%3A%20utilizing%20powerful%20non-reference%20image%0Aquality%20assessment%20%28NR-IQA%29%20models%20in%20the%20SR%20context.%20We%20begin%20with%20a%0Acomprehensive%20analysis%20of%20NR-IQA%20metrics%20on%20human-derived%20SR%20data%2C%20identifying%0Aboth%20the%20accuracy%20%28human%20alignment%29%20and%20complementarity%20of%20different%20metrics.%0AThen%2C%20we%20explore%20two%20methods%20of%20applying%20NR-IQA%20models%20to%20SR%20learning%3A%20%28i%29%0Aaltering%20data%20sampling%2C%20by%20building%20on%20an%20existing%20multi-ground-truth%20SR%0Aframework%2C%20and%20%28ii%29%20directly%20optimizing%20a%20differentiable%20quality%20score.%20Our%0Aresults%20demonstrate%20a%20more%20human-centric%20perception-distortion%20tradeoff%2C%0Afocusing%20less%20on%20non-perceptual%20pixel-wise%20distortion%2C%20instead%20improving%20the%0Abalance%20between%20perceptual%20fidelity%20and%20human-tuned%20NR-IQA%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmenting%2520Perceptual%2520Super-Resolution%2520via%2520Image%2520Quality%2520Predictors%26entry.906535625%3DFengjia%2520Zhang%2520and%2520Samrudhdhi%2520B.%2520Rangrej%2520and%2520Tristan%2520Aumentado-Armstrong%2520and%2520Afsaneh%2520Fazly%2520and%2520Alex%2520Levinshtein%26entry.1292438233%3D%2520%2520Super-resolution%2520%2528SR%2529%252C%2520a%2520classical%2520inverse%2520problem%2520in%2520computer%2520vision%252C%2520is%250Ainherently%2520ill-posed%252C%2520inducing%2520a%2520distribution%2520of%2520plausible%2520solutions%2520for%2520every%250Ainput.%2520However%252C%2520the%2520desired%2520result%2520is%2520not%2520simply%2520the%2520expectation%2520of%2520this%250Adistribution%252C%2520which%2520is%2520the%2520blurry%2520image%2520obtained%2520by%2520minimizing%2520pixelwise%2520error%252C%250Abut%2520rather%2520the%2520sample%2520with%2520the%2520highest%2520image%2520quality.%2520A%2520variety%2520of%2520techniques%252C%250Afrom%2520perceptual%2520metrics%2520to%2520adversarial%2520losses%252C%2520are%2520employed%2520to%2520this%2520end.%2520In%250Athis%2520work%252C%2520we%2520explore%2520an%2520alternative%253A%2520utilizing%2520powerful%2520non-reference%2520image%250Aquality%2520assessment%2520%2528NR-IQA%2529%2520models%2520in%2520the%2520SR%2520context.%2520We%2520begin%2520with%2520a%250Acomprehensive%2520analysis%2520of%2520NR-IQA%2520metrics%2520on%2520human-derived%2520SR%2520data%252C%2520identifying%250Aboth%2520the%2520accuracy%2520%2528human%2520alignment%2529%2520and%2520complementarity%2520of%2520different%2520metrics.%250AThen%252C%2520we%2520explore%2520two%2520methods%2520of%2520applying%2520NR-IQA%2520models%2520to%2520SR%2520learning%253A%2520%2528i%2529%250Aaltering%2520data%2520sampling%252C%2520by%2520building%2520on%2520an%2520existing%2520multi-ground-truth%2520SR%250Aframework%252C%2520and%2520%2528ii%2529%2520directly%2520optimizing%2520a%2520differentiable%2520quality%2520score.%2520Our%250Aresults%2520demonstrate%2520a%2520more%2520human-centric%2520perception-distortion%2520tradeoff%252C%250Afocusing%2520less%2520on%2520non-perceptual%2520pixel-wise%2520distortion%252C%2520instead%2520improving%2520the%250Abalance%2520between%2520perceptual%2520fidelity%2520and%2520human-tuned%2520NR-IQA%2520measures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmenting%20Perceptual%20Super-Resolution%20via%20Image%20Quality%20Predictors&entry.906535625=Fengjia%20Zhang%20and%20Samrudhdhi%20B.%20Rangrej%20and%20Tristan%20Aumentado-Armstrong%20and%20Afsaneh%20Fazly%20and%20Alex%20Levinshtein&entry.1292438233=%20%20Super-resolution%20%28SR%29%2C%20a%20classical%20inverse%20problem%20in%20computer%20vision%2C%20is%0Ainherently%20ill-posed%2C%20inducing%20a%20distribution%20of%20plausible%20solutions%20for%20every%0Ainput.%20However%2C%20the%20desired%20result%20is%20not%20simply%20the%20expectation%20of%20this%0Adistribution%2C%20which%20is%20the%20blurry%20image%20obtained%20by%20minimizing%20pixelwise%20error%2C%0Abut%20rather%20the%20sample%20with%20the%20highest%20image%20quality.%20A%20variety%20of%20techniques%2C%0Afrom%20perceptual%20metrics%20to%20adversarial%20losses%2C%20are%20employed%20to%20this%20end.%20In%0Athis%20work%2C%20we%20explore%20an%20alternative%3A%20utilizing%20powerful%20non-reference%20image%0Aquality%20assessment%20%28NR-IQA%29%20models%20in%20the%20SR%20context.%20We%20begin%20with%20a%0Acomprehensive%20analysis%20of%20NR-IQA%20metrics%20on%20human-derived%20SR%20data%2C%20identifying%0Aboth%20the%20accuracy%20%28human%20alignment%29%20and%20complementarity%20of%20different%20metrics.%0AThen%2C%20we%20explore%20two%20methods%20of%20applying%20NR-IQA%20models%20to%20SR%20learning%3A%20%28i%29%0Aaltering%20data%20sampling%2C%20by%20building%20on%20an%20existing%20multi-ground-truth%20SR%0Aframework%2C%20and%20%28ii%29%20directly%20optimizing%20a%20differentiable%20quality%20score.%20Our%0Aresults%20demonstrate%20a%20more%20human-centric%20perception-distortion%20tradeoff%2C%0Afocusing%20less%20on%20non-perceptual%20pixel-wise%20distortion%2C%20instead%20improving%20the%0Abalance%20between%20perceptual%20fidelity%20and%20human-tuned%20NR-IQA%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18524v1&entry.124074799=Read"},
{"title": "Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A\n  Comparative Analysis", "author": "Christian Pionzewski and Rebecca Rademacher and J\u00e9r\u00f4me Rutinowski and Antonia Ponikarov and Stephan Matzke and Tim Chilla and Pia Schreynemackers and Alice Kirchheim", "abstract": "  This contribution explores the impact of synthetic training data usage and\nthe prediction of material wear and aging in the context of re-identification.\nDifferent experimental setups and gallery set expanding strategies are tested,\nanalyzing their impact on performance over time for aging re-identification\nsubjects. Using a continuously updating gallery, we were able to increase our\nmean Rank-1 accuracy by 24%, as material aging was taken into account step by\nstep. In addition, using models trained with 10% artificial training data,\nRank-1 accuracy could be increased by up to 13%, in comparison to a model\ntrained on only real-world data, significantly boosting generalized performance\non hold-out data. Finally, this work introduces a novel, open-source\nre-identification dataset, pallet-block-2696. This dataset contains 2,696\nimages of Euro pallets, taken over a period of 4 months. During this time,\nnatural aging processes occurred and some of the pallets were damaged during\ntheir usage. These wear and tear processes significantly changed the appearance\nof the pallets, providing a dataset that can be used to generate synthetically\naged pallets or other wooden materials.\n", "link": "http://arxiv.org/abs/2504.18286v1", "date": "2025-04-25", "relevancy": 2.0446, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5147}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5098}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Long-Term%20Re-Identification%20Robustness%20Using%20Synthetic%20Data%3A%20A%0A%20%20Comparative%20Analysis&body=Title%3A%20Enhancing%20Long-Term%20Re-Identification%20Robustness%20Using%20Synthetic%20Data%3A%20A%0A%20%20Comparative%20Analysis%0AAuthor%3A%20Christian%20Pionzewski%20and%20Rebecca%20Rademacher%20and%20J%C3%A9r%C3%B4me%20Rutinowski%20and%20Antonia%20Ponikarov%20and%20Stephan%20Matzke%20and%20Tim%20Chilla%20and%20Pia%20Schreynemackers%20and%20Alice%20Kirchheim%0AAbstract%3A%20%20%20This%20contribution%20explores%20the%20impact%20of%20synthetic%20training%20data%20usage%20and%0Athe%20prediction%20of%20material%20wear%20and%20aging%20in%20the%20context%20of%20re-identification.%0ADifferent%20experimental%20setups%20and%20gallery%20set%20expanding%20strategies%20are%20tested%2C%0Aanalyzing%20their%20impact%20on%20performance%20over%20time%20for%20aging%20re-identification%0Asubjects.%20Using%20a%20continuously%20updating%20gallery%2C%20we%20were%20able%20to%20increase%20our%0Amean%20Rank-1%20accuracy%20by%2024%25%2C%20as%20material%20aging%20was%20taken%20into%20account%20step%20by%0Astep.%20In%20addition%2C%20using%20models%20trained%20with%2010%25%20artificial%20training%20data%2C%0ARank-1%20accuracy%20could%20be%20increased%20by%20up%20to%2013%25%2C%20in%20comparison%20to%20a%20model%0Atrained%20on%20only%20real-world%20data%2C%20significantly%20boosting%20generalized%20performance%0Aon%20hold-out%20data.%20Finally%2C%20this%20work%20introduces%20a%20novel%2C%20open-source%0Are-identification%20dataset%2C%20pallet-block-2696.%20This%20dataset%20contains%202%2C696%0Aimages%20of%20Euro%20pallets%2C%20taken%20over%20a%20period%20of%204%20months.%20During%20this%20time%2C%0Anatural%20aging%20processes%20occurred%20and%20some%20of%20the%20pallets%20were%20damaged%20during%0Atheir%20usage.%20These%20wear%20and%20tear%20processes%20significantly%20changed%20the%20appearance%0Aof%20the%20pallets%2C%20providing%20a%20dataset%20that%20can%20be%20used%20to%20generate%20synthetically%0Aaged%20pallets%20or%20other%20wooden%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Long-Term%2520Re-Identification%2520Robustness%2520Using%2520Synthetic%2520Data%253A%2520A%250A%2520%2520Comparative%2520Analysis%26entry.906535625%3DChristian%2520Pionzewski%2520and%2520Rebecca%2520Rademacher%2520and%2520J%25C3%25A9r%25C3%25B4me%2520Rutinowski%2520and%2520Antonia%2520Ponikarov%2520and%2520Stephan%2520Matzke%2520and%2520Tim%2520Chilla%2520and%2520Pia%2520Schreynemackers%2520and%2520Alice%2520Kirchheim%26entry.1292438233%3D%2520%2520This%2520contribution%2520explores%2520the%2520impact%2520of%2520synthetic%2520training%2520data%2520usage%2520and%250Athe%2520prediction%2520of%2520material%2520wear%2520and%2520aging%2520in%2520the%2520context%2520of%2520re-identification.%250ADifferent%2520experimental%2520setups%2520and%2520gallery%2520set%2520expanding%2520strategies%2520are%2520tested%252C%250Aanalyzing%2520their%2520impact%2520on%2520performance%2520over%2520time%2520for%2520aging%2520re-identification%250Asubjects.%2520Using%2520a%2520continuously%2520updating%2520gallery%252C%2520we%2520were%2520able%2520to%2520increase%2520our%250Amean%2520Rank-1%2520accuracy%2520by%252024%2525%252C%2520as%2520material%2520aging%2520was%2520taken%2520into%2520account%2520step%2520by%250Astep.%2520In%2520addition%252C%2520using%2520models%2520trained%2520with%252010%2525%2520artificial%2520training%2520data%252C%250ARank-1%2520accuracy%2520could%2520be%2520increased%2520by%2520up%2520to%252013%2525%252C%2520in%2520comparison%2520to%2520a%2520model%250Atrained%2520on%2520only%2520real-world%2520data%252C%2520significantly%2520boosting%2520generalized%2520performance%250Aon%2520hold-out%2520data.%2520Finally%252C%2520this%2520work%2520introduces%2520a%2520novel%252C%2520open-source%250Are-identification%2520dataset%252C%2520pallet-block-2696.%2520This%2520dataset%2520contains%25202%252C696%250Aimages%2520of%2520Euro%2520pallets%252C%2520taken%2520over%2520a%2520period%2520of%25204%2520months.%2520During%2520this%2520time%252C%250Anatural%2520aging%2520processes%2520occurred%2520and%2520some%2520of%2520the%2520pallets%2520were%2520damaged%2520during%250Atheir%2520usage.%2520These%2520wear%2520and%2520tear%2520processes%2520significantly%2520changed%2520the%2520appearance%250Aof%2520the%2520pallets%252C%2520providing%2520a%2520dataset%2520that%2520can%2520be%2520used%2520to%2520generate%2520synthetically%250Aaged%2520pallets%2520or%2520other%2520wooden%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Long-Term%20Re-Identification%20Robustness%20Using%20Synthetic%20Data%3A%20A%0A%20%20Comparative%20Analysis&entry.906535625=Christian%20Pionzewski%20and%20Rebecca%20Rademacher%20and%20J%C3%A9r%C3%B4me%20Rutinowski%20and%20Antonia%20Ponikarov%20and%20Stephan%20Matzke%20and%20Tim%20Chilla%20and%20Pia%20Schreynemackers%20and%20Alice%20Kirchheim&entry.1292438233=%20%20This%20contribution%20explores%20the%20impact%20of%20synthetic%20training%20data%20usage%20and%0Athe%20prediction%20of%20material%20wear%20and%20aging%20in%20the%20context%20of%20re-identification.%0ADifferent%20experimental%20setups%20and%20gallery%20set%20expanding%20strategies%20are%20tested%2C%0Aanalyzing%20their%20impact%20on%20performance%20over%20time%20for%20aging%20re-identification%0Asubjects.%20Using%20a%20continuously%20updating%20gallery%2C%20we%20were%20able%20to%20increase%20our%0Amean%20Rank-1%20accuracy%20by%2024%25%2C%20as%20material%20aging%20was%20taken%20into%20account%20step%20by%0Astep.%20In%20addition%2C%20using%20models%20trained%20with%2010%25%20artificial%20training%20data%2C%0ARank-1%20accuracy%20could%20be%20increased%20by%20up%20to%2013%25%2C%20in%20comparison%20to%20a%20model%0Atrained%20on%20only%20real-world%20data%2C%20significantly%20boosting%20generalized%20performance%0Aon%20hold-out%20data.%20Finally%2C%20this%20work%20introduces%20a%20novel%2C%20open-source%0Are-identification%20dataset%2C%20pallet-block-2696.%20This%20dataset%20contains%202%2C696%0Aimages%20of%20Euro%20pallets%2C%20taken%20over%20a%20period%20of%204%20months.%20During%20this%20time%2C%0Anatural%20aging%20processes%20occurred%20and%20some%20of%20the%20pallets%20were%20damaged%20during%0Atheir%20usage.%20These%20wear%20and%20tear%20processes%20significantly%20changed%20the%20appearance%0Aof%20the%20pallets%2C%20providing%20a%20dataset%20that%20can%20be%20used%20to%20generate%20synthetically%0Aaged%20pallets%20or%20other%20wooden%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18286v1&entry.124074799=Read"},
{"title": "Generative Evaluation of Complex Reasoning in Large Language Models", "author": "Haowei Lin and Xiangyu Wang and Ruilin Yan and Baizhou Huang and Haotian Ye and Jianhua Zhu and Zihao Wang and James Zou and Jianzhu Ma and Yitao Liang", "abstract": "  With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.\n", "link": "http://arxiv.org/abs/2504.02810v2", "date": "2025-04-25", "relevancy": 2.0442, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Evaluation%20of%20Complex%20Reasoning%20in%20Large%20Language%20Models&body=Title%3A%20Generative%20Evaluation%20of%20Complex%20Reasoning%20in%20Large%20Language%20Models%0AAuthor%3A%20Haowei%20Lin%20and%20Xiangyu%20Wang%20and%20Ruilin%20Yan%20and%20Baizhou%20Huang%20and%20Haotian%20Ye%20and%20Jianhua%20Zhu%20and%20Zihao%20Wang%20and%20James%20Zou%20and%20Jianzhu%20Ma%20and%20Yitao%20Liang%0AAbstract%3A%20%20%20With%20powerful%20large%20language%20models%20%28LLMs%29%20demonstrating%20superhuman%20reasoning%0Acapabilities%2C%20a%20critical%20question%20arises%3A%20Do%20LLMs%20genuinely%20reason%2C%20or%20do%20they%0Amerely%20recall%20answers%20from%20their%20extensive%2C%20web-scraped%20training%20datasets%3F%0APublicly%20released%20benchmarks%20inevitably%20become%20contaminated%20once%20incorporated%0Ainto%20subsequent%20LLM%20training%20sets%2C%20undermining%20their%20reliability%20as%20faithful%0Aassessments.%20To%20address%20this%2C%20we%20introduce%20KUMO%2C%20a%20generative%20evaluation%0Aframework%20designed%20specifically%20for%20assessing%20reasoning%20in%20LLMs.%20KUMO%0Asynergistically%20combines%20LLMs%20with%20symbolic%20engines%20to%20dynamically%20produce%0Adiverse%2C%20multi-turn%20reasoning%20tasks%20that%20are%20partially%20observable%20and%0Aadjustable%20in%20difficulty.%20Through%20an%20automated%20pipeline%2C%20KUMO%20continuously%0Agenerates%20novel%20tasks%20across%20open-ended%20domains%2C%20compelling%20models%20to%0Ademonstrate%20genuine%20generalization%20rather%20than%20memorization.%20We%20evaluated%2023%0Astate-of-the-art%20LLMs%20on%205%2C000%20tasks%20across%20100%20domains%20created%20by%20KUMO%2C%0Abenchmarking%20their%20reasoning%20abilities%20against%20university%20students.%20Our%0Afindings%20reveal%20that%20many%20LLMs%20have%20outperformed%20university-level%20performance%0Aon%20easy%20reasoning%20tasks%2C%20and%20reasoning-scaled%20LLMs%20reach%20university-level%0Aperformance%20on%20complex%20reasoning%20challenges.%20Moreover%2C%20LLM%20performance%20on%20KUMO%0Atasks%20correlates%20strongly%20with%20results%20on%20newly%20released%20real-world%20reasoning%0Abenchmarks%2C%20underscoring%20KUMO%27s%20value%20as%20a%20robust%2C%20enduring%20assessment%20tool%20for%0Agenuine%20LLM%20reasoning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Evaluation%2520of%2520Complex%2520Reasoning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DHaowei%2520Lin%2520and%2520Xiangyu%2520Wang%2520and%2520Ruilin%2520Yan%2520and%2520Baizhou%2520Huang%2520and%2520Haotian%2520Ye%2520and%2520Jianhua%2520Zhu%2520and%2520Zihao%2520Wang%2520and%2520James%2520Zou%2520and%2520Jianzhu%2520Ma%2520and%2520Yitao%2520Liang%26entry.1292438233%3D%2520%2520With%2520powerful%2520large%2520language%2520models%2520%2528LLMs%2529%2520demonstrating%2520superhuman%2520reasoning%250Acapabilities%252C%2520a%2520critical%2520question%2520arises%253A%2520Do%2520LLMs%2520genuinely%2520reason%252C%2520or%2520do%2520they%250Amerely%2520recall%2520answers%2520from%2520their%2520extensive%252C%2520web-scraped%2520training%2520datasets%253F%250APublicly%2520released%2520benchmarks%2520inevitably%2520become%2520contaminated%2520once%2520incorporated%250Ainto%2520subsequent%2520LLM%2520training%2520sets%252C%2520undermining%2520their%2520reliability%2520as%2520faithful%250Aassessments.%2520To%2520address%2520this%252C%2520we%2520introduce%2520KUMO%252C%2520a%2520generative%2520evaluation%250Aframework%2520designed%2520specifically%2520for%2520assessing%2520reasoning%2520in%2520LLMs.%2520KUMO%250Asynergistically%2520combines%2520LLMs%2520with%2520symbolic%2520engines%2520to%2520dynamically%2520produce%250Adiverse%252C%2520multi-turn%2520reasoning%2520tasks%2520that%2520are%2520partially%2520observable%2520and%250Aadjustable%2520in%2520difficulty.%2520Through%2520an%2520automated%2520pipeline%252C%2520KUMO%2520continuously%250Agenerates%2520novel%2520tasks%2520across%2520open-ended%2520domains%252C%2520compelling%2520models%2520to%250Ademonstrate%2520genuine%2520generalization%2520rather%2520than%2520memorization.%2520We%2520evaluated%252023%250Astate-of-the-art%2520LLMs%2520on%25205%252C000%2520tasks%2520across%2520100%2520domains%2520created%2520by%2520KUMO%252C%250Abenchmarking%2520their%2520reasoning%2520abilities%2520against%2520university%2520students.%2520Our%250Afindings%2520reveal%2520that%2520many%2520LLMs%2520have%2520outperformed%2520university-level%2520performance%250Aon%2520easy%2520reasoning%2520tasks%252C%2520and%2520reasoning-scaled%2520LLMs%2520reach%2520university-level%250Aperformance%2520on%2520complex%2520reasoning%2520challenges.%2520Moreover%252C%2520LLM%2520performance%2520on%2520KUMO%250Atasks%2520correlates%2520strongly%2520with%2520results%2520on%2520newly%2520released%2520real-world%2520reasoning%250Abenchmarks%252C%2520underscoring%2520KUMO%2527s%2520value%2520as%2520a%2520robust%252C%2520enduring%2520assessment%2520tool%2520for%250Agenuine%2520LLM%2520reasoning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Evaluation%20of%20Complex%20Reasoning%20in%20Large%20Language%20Models&entry.906535625=Haowei%20Lin%20and%20Xiangyu%20Wang%20and%20Ruilin%20Yan%20and%20Baizhou%20Huang%20and%20Haotian%20Ye%20and%20Jianhua%20Zhu%20and%20Zihao%20Wang%20and%20James%20Zou%20and%20Jianzhu%20Ma%20and%20Yitao%20Liang&entry.1292438233=%20%20With%20powerful%20large%20language%20models%20%28LLMs%29%20demonstrating%20superhuman%20reasoning%0Acapabilities%2C%20a%20critical%20question%20arises%3A%20Do%20LLMs%20genuinely%20reason%2C%20or%20do%20they%0Amerely%20recall%20answers%20from%20their%20extensive%2C%20web-scraped%20training%20datasets%3F%0APublicly%20released%20benchmarks%20inevitably%20become%20contaminated%20once%20incorporated%0Ainto%20subsequent%20LLM%20training%20sets%2C%20undermining%20their%20reliability%20as%20faithful%0Aassessments.%20To%20address%20this%2C%20we%20introduce%20KUMO%2C%20a%20generative%20evaluation%0Aframework%20designed%20specifically%20for%20assessing%20reasoning%20in%20LLMs.%20KUMO%0Asynergistically%20combines%20LLMs%20with%20symbolic%20engines%20to%20dynamically%20produce%0Adiverse%2C%20multi-turn%20reasoning%20tasks%20that%20are%20partially%20observable%20and%0Aadjustable%20in%20difficulty.%20Through%20an%20automated%20pipeline%2C%20KUMO%20continuously%0Agenerates%20novel%20tasks%20across%20open-ended%20domains%2C%20compelling%20models%20to%0Ademonstrate%20genuine%20generalization%20rather%20than%20memorization.%20We%20evaluated%2023%0Astate-of-the-art%20LLMs%20on%205%2C000%20tasks%20across%20100%20domains%20created%20by%20KUMO%2C%0Abenchmarking%20their%20reasoning%20abilities%20against%20university%20students.%20Our%0Afindings%20reveal%20that%20many%20LLMs%20have%20outperformed%20university-level%20performance%0Aon%20easy%20reasoning%20tasks%2C%20and%20reasoning-scaled%20LLMs%20reach%20university-level%0Aperformance%20on%20complex%20reasoning%20challenges.%20Moreover%2C%20LLM%20performance%20on%20KUMO%0Atasks%20correlates%20strongly%20with%20results%20on%20newly%20released%20real-world%20reasoning%0Abenchmarks%2C%20underscoring%20KUMO%27s%20value%20as%20a%20robust%2C%20enduring%20assessment%20tool%20for%0Agenuine%20LLM%20reasoning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02810v2&entry.124074799=Read"},
{"title": "Nearly isotropic segmentation for medial temporal lobe subregions in\n  multi-modality MRI", "author": "Yue Li and Pulkit Khandelwal and Long Xie and Laura E. M. Wisse and Nidhi Mundada and Christopher A. Brown and Emily McGrew and Amanda Denning and Sandhitsu R. Das and David A. Wolk and Paul A. Yushkevich", "abstract": "  Morphometry of medial temporal lobe (MTL) subregions in brain MRI is\nsensitive biomarker to Alzheimers Disease and other related conditions. While\nT2-weighted (T2w) MRI with high in-plane resolution is widely used to segment\nhippocampal subfields due to its higher contrast in hippocampus, its lower\nout-of-plane resolution reduces the accuracy of subregion thickness\nmeasurements. To address this issue, we developed a nearly isotropic\nsegmentation pipeline that incorporates image and label upsampling and\nhigh-resolution segmentation in T2w MRI. First, a high-resolution atlas was\ncreated based on an existing anisotropic atlas derived from 29 individuals.\nBoth T1-weighted and T2w images in the atlas were upsampled from their original\nresolution to a nearly isotropic resolution 0.4x0.4x0.52mm3 using a non-local\nmeans approach. Manual segmentations within the atlas were also upsampled to\nmatch this resolution using a UNet-based neural network, which was trained on a\ncohort consisting of both high-resolution ex vivo and low-resolution\nanisotropic in vivo MRI with manual segmentations. Second, a multi-modality\ndeep learning-based segmentation model was trained within this nearly isotropic\natlas. Finally, experiments showed the nearly isotropic subregion segmentation\nimproved the accuracy of cortical thickness as an imaging biomarker for\nneurodegeneration in T2w MRI.\n", "link": "http://arxiv.org/abs/2504.18442v1", "date": "2025-04-25", "relevancy": 2.043, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5564}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4791}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nearly%20isotropic%20segmentation%20for%20medial%20temporal%20lobe%20subregions%20in%0A%20%20multi-modality%20MRI&body=Title%3A%20Nearly%20isotropic%20segmentation%20for%20medial%20temporal%20lobe%20subregions%20in%0A%20%20multi-modality%20MRI%0AAuthor%3A%20Yue%20Li%20and%20Pulkit%20Khandelwal%20and%20Long%20Xie%20and%20Laura%20E.%20M.%20Wisse%20and%20Nidhi%20Mundada%20and%20Christopher%20A.%20Brown%20and%20Emily%20McGrew%20and%20Amanda%20Denning%20and%20Sandhitsu%20R.%20Das%20and%20David%20A.%20Wolk%20and%20Paul%20A.%20Yushkevich%0AAbstract%3A%20%20%20Morphometry%20of%20medial%20temporal%20lobe%20%28MTL%29%20subregions%20in%20brain%20MRI%20is%0Asensitive%20biomarker%20to%20Alzheimers%20Disease%20and%20other%20related%20conditions.%20While%0AT2-weighted%20%28T2w%29%20MRI%20with%20high%20in-plane%20resolution%20is%20widely%20used%20to%20segment%0Ahippocampal%20subfields%20due%20to%20its%20higher%20contrast%20in%20hippocampus%2C%20its%20lower%0Aout-of-plane%20resolution%20reduces%20the%20accuracy%20of%20subregion%20thickness%0Ameasurements.%20To%20address%20this%20issue%2C%20we%20developed%20a%20nearly%20isotropic%0Asegmentation%20pipeline%20that%20incorporates%20image%20and%20label%20upsampling%20and%0Ahigh-resolution%20segmentation%20in%20T2w%20MRI.%20First%2C%20a%20high-resolution%20atlas%20was%0Acreated%20based%20on%20an%20existing%20anisotropic%20atlas%20derived%20from%2029%20individuals.%0ABoth%20T1-weighted%20and%20T2w%20images%20in%20the%20atlas%20were%20upsampled%20from%20their%20original%0Aresolution%20to%20a%20nearly%20isotropic%20resolution%200.4x0.4x0.52mm3%20using%20a%20non-local%0Ameans%20approach.%20Manual%20segmentations%20within%20the%20atlas%20were%20also%20upsampled%20to%0Amatch%20this%20resolution%20using%20a%20UNet-based%20neural%20network%2C%20which%20was%20trained%20on%20a%0Acohort%20consisting%20of%20both%20high-resolution%20ex%20vivo%20and%20low-resolution%0Aanisotropic%20in%20vivo%20MRI%20with%20manual%20segmentations.%20Second%2C%20a%20multi-modality%0Adeep%20learning-based%20segmentation%20model%20was%20trained%20within%20this%20nearly%20isotropic%0Aatlas.%20Finally%2C%20experiments%20showed%20the%20nearly%20isotropic%20subregion%20segmentation%0Aimproved%20the%20accuracy%20of%20cortical%20thickness%20as%20an%20imaging%20biomarker%20for%0Aneurodegeneration%20in%20T2w%20MRI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNearly%2520isotropic%2520segmentation%2520for%2520medial%2520temporal%2520lobe%2520subregions%2520in%250A%2520%2520multi-modality%2520MRI%26entry.906535625%3DYue%2520Li%2520and%2520Pulkit%2520Khandelwal%2520and%2520Long%2520Xie%2520and%2520Laura%2520E.%2520M.%2520Wisse%2520and%2520Nidhi%2520Mundada%2520and%2520Christopher%2520A.%2520Brown%2520and%2520Emily%2520McGrew%2520and%2520Amanda%2520Denning%2520and%2520Sandhitsu%2520R.%2520Das%2520and%2520David%2520A.%2520Wolk%2520and%2520Paul%2520A.%2520Yushkevich%26entry.1292438233%3D%2520%2520Morphometry%2520of%2520medial%2520temporal%2520lobe%2520%2528MTL%2529%2520subregions%2520in%2520brain%2520MRI%2520is%250Asensitive%2520biomarker%2520to%2520Alzheimers%2520Disease%2520and%2520other%2520related%2520conditions.%2520While%250AT2-weighted%2520%2528T2w%2529%2520MRI%2520with%2520high%2520in-plane%2520resolution%2520is%2520widely%2520used%2520to%2520segment%250Ahippocampal%2520subfields%2520due%2520to%2520its%2520higher%2520contrast%2520in%2520hippocampus%252C%2520its%2520lower%250Aout-of-plane%2520resolution%2520reduces%2520the%2520accuracy%2520of%2520subregion%2520thickness%250Ameasurements.%2520To%2520address%2520this%2520issue%252C%2520we%2520developed%2520a%2520nearly%2520isotropic%250Asegmentation%2520pipeline%2520that%2520incorporates%2520image%2520and%2520label%2520upsampling%2520and%250Ahigh-resolution%2520segmentation%2520in%2520T2w%2520MRI.%2520First%252C%2520a%2520high-resolution%2520atlas%2520was%250Acreated%2520based%2520on%2520an%2520existing%2520anisotropic%2520atlas%2520derived%2520from%252029%2520individuals.%250ABoth%2520T1-weighted%2520and%2520T2w%2520images%2520in%2520the%2520atlas%2520were%2520upsampled%2520from%2520their%2520original%250Aresolution%2520to%2520a%2520nearly%2520isotropic%2520resolution%25200.4x0.4x0.52mm3%2520using%2520a%2520non-local%250Ameans%2520approach.%2520Manual%2520segmentations%2520within%2520the%2520atlas%2520were%2520also%2520upsampled%2520to%250Amatch%2520this%2520resolution%2520using%2520a%2520UNet-based%2520neural%2520network%252C%2520which%2520was%2520trained%2520on%2520a%250Acohort%2520consisting%2520of%2520both%2520high-resolution%2520ex%2520vivo%2520and%2520low-resolution%250Aanisotropic%2520in%2520vivo%2520MRI%2520with%2520manual%2520segmentations.%2520Second%252C%2520a%2520multi-modality%250Adeep%2520learning-based%2520segmentation%2520model%2520was%2520trained%2520within%2520this%2520nearly%2520isotropic%250Aatlas.%2520Finally%252C%2520experiments%2520showed%2520the%2520nearly%2520isotropic%2520subregion%2520segmentation%250Aimproved%2520the%2520accuracy%2520of%2520cortical%2520thickness%2520as%2520an%2520imaging%2520biomarker%2520for%250Aneurodegeneration%2520in%2520T2w%2520MRI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nearly%20isotropic%20segmentation%20for%20medial%20temporal%20lobe%20subregions%20in%0A%20%20multi-modality%20MRI&entry.906535625=Yue%20Li%20and%20Pulkit%20Khandelwal%20and%20Long%20Xie%20and%20Laura%20E.%20M.%20Wisse%20and%20Nidhi%20Mundada%20and%20Christopher%20A.%20Brown%20and%20Emily%20McGrew%20and%20Amanda%20Denning%20and%20Sandhitsu%20R.%20Das%20and%20David%20A.%20Wolk%20and%20Paul%20A.%20Yushkevich&entry.1292438233=%20%20Morphometry%20of%20medial%20temporal%20lobe%20%28MTL%29%20subregions%20in%20brain%20MRI%20is%0Asensitive%20biomarker%20to%20Alzheimers%20Disease%20and%20other%20related%20conditions.%20While%0AT2-weighted%20%28T2w%29%20MRI%20with%20high%20in-plane%20resolution%20is%20widely%20used%20to%20segment%0Ahippocampal%20subfields%20due%20to%20its%20higher%20contrast%20in%20hippocampus%2C%20its%20lower%0Aout-of-plane%20resolution%20reduces%20the%20accuracy%20of%20subregion%20thickness%0Ameasurements.%20To%20address%20this%20issue%2C%20we%20developed%20a%20nearly%20isotropic%0Asegmentation%20pipeline%20that%20incorporates%20image%20and%20label%20upsampling%20and%0Ahigh-resolution%20segmentation%20in%20T2w%20MRI.%20First%2C%20a%20high-resolution%20atlas%20was%0Acreated%20based%20on%20an%20existing%20anisotropic%20atlas%20derived%20from%2029%20individuals.%0ABoth%20T1-weighted%20and%20T2w%20images%20in%20the%20atlas%20were%20upsampled%20from%20their%20original%0Aresolution%20to%20a%20nearly%20isotropic%20resolution%200.4x0.4x0.52mm3%20using%20a%20non-local%0Ameans%20approach.%20Manual%20segmentations%20within%20the%20atlas%20were%20also%20upsampled%20to%0Amatch%20this%20resolution%20using%20a%20UNet-based%20neural%20network%2C%20which%20was%20trained%20on%20a%0Acohort%20consisting%20of%20both%20high-resolution%20ex%20vivo%20and%20low-resolution%0Aanisotropic%20in%20vivo%20MRI%20with%20manual%20segmentations.%20Second%2C%20a%20multi-modality%0Adeep%20learning-based%20segmentation%20model%20was%20trained%20within%20this%20nearly%20isotropic%0Aatlas.%20Finally%2C%20experiments%20showed%20the%20nearly%20isotropic%20subregion%20segmentation%0Aimproved%20the%20accuracy%20of%20cortical%20thickness%20as%20an%20imaging%20biomarker%20for%0Aneurodegeneration%20in%20T2w%20MRI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18442v1&entry.124074799=Read"},
{"title": "Studying Small Language Models with Susceptibilities", "author": "Garrett Baker and George Wang and Jesse Hoogland and Daniel Murfet", "abstract": "  We develop a linear response framework for interpretability that treats a\nneural network as a Bayesian statistical mechanical system. A small, controlled\nperturbation of the data distribution, for example shifting the Pile toward\nGitHub or legal text, induces a first-order change in the posterior expectation\nof an observable localized on a chosen component of the network. The resulting\nsusceptibility can be estimated efficiently with local SGLD samples and\nfactorizes into signed, per-token contributions that serve as attribution\nscores. Building a set of perturbations (probes) yields a response matrix whose\nlow-rank structure separates functional modules such as multigram and induction\nheads in a 3M-parameter transformer. Susceptibilities link local learning\ncoefficients from singular learning theory with linear-response theory, and\nquantify how local loss landscape geometry deforms under shifts in the data\ndistribution.\n", "link": "http://arxiv.org/abs/2504.18274v1", "date": "2025-04-25", "relevancy": 2.0362, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5147}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Studying%20Small%20Language%20Models%20with%20Susceptibilities&body=Title%3A%20Studying%20Small%20Language%20Models%20with%20Susceptibilities%0AAuthor%3A%20Garrett%20Baker%20and%20George%20Wang%20and%20Jesse%20Hoogland%20and%20Daniel%20Murfet%0AAbstract%3A%20%20%20We%20develop%20a%20linear%20response%20framework%20for%20interpretability%20that%20treats%20a%0Aneural%20network%20as%20a%20Bayesian%20statistical%20mechanical%20system.%20A%20small%2C%20controlled%0Aperturbation%20of%20the%20data%20distribution%2C%20for%20example%20shifting%20the%20Pile%20toward%0AGitHub%20or%20legal%20text%2C%20induces%20a%20first-order%20change%20in%20the%20posterior%20expectation%0Aof%20an%20observable%20localized%20on%20a%20chosen%20component%20of%20the%20network.%20The%20resulting%0Asusceptibility%20can%20be%20estimated%20efficiently%20with%20local%20SGLD%20samples%20and%0Afactorizes%20into%20signed%2C%20per-token%20contributions%20that%20serve%20as%20attribution%0Ascores.%20Building%20a%20set%20of%20perturbations%20%28probes%29%20yields%20a%20response%20matrix%20whose%0Alow-rank%20structure%20separates%20functional%20modules%20such%20as%20multigram%20and%20induction%0Aheads%20in%20a%203M-parameter%20transformer.%20Susceptibilities%20link%20local%20learning%0Acoefficients%20from%20singular%20learning%20theory%20with%20linear-response%20theory%2C%20and%0Aquantify%20how%20local%20loss%20landscape%20geometry%20deforms%20under%20shifts%20in%20the%20data%0Adistribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudying%2520Small%2520Language%2520Models%2520with%2520Susceptibilities%26entry.906535625%3DGarrett%2520Baker%2520and%2520George%2520Wang%2520and%2520Jesse%2520Hoogland%2520and%2520Daniel%2520Murfet%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520linear%2520response%2520framework%2520for%2520interpretability%2520that%2520treats%2520a%250Aneural%2520network%2520as%2520a%2520Bayesian%2520statistical%2520mechanical%2520system.%2520A%2520small%252C%2520controlled%250Aperturbation%2520of%2520the%2520data%2520distribution%252C%2520for%2520example%2520shifting%2520the%2520Pile%2520toward%250AGitHub%2520or%2520legal%2520text%252C%2520induces%2520a%2520first-order%2520change%2520in%2520the%2520posterior%2520expectation%250Aof%2520an%2520observable%2520localized%2520on%2520a%2520chosen%2520component%2520of%2520the%2520network.%2520The%2520resulting%250Asusceptibility%2520can%2520be%2520estimated%2520efficiently%2520with%2520local%2520SGLD%2520samples%2520and%250Afactorizes%2520into%2520signed%252C%2520per-token%2520contributions%2520that%2520serve%2520as%2520attribution%250Ascores.%2520Building%2520a%2520set%2520of%2520perturbations%2520%2528probes%2529%2520yields%2520a%2520response%2520matrix%2520whose%250Alow-rank%2520structure%2520separates%2520functional%2520modules%2520such%2520as%2520multigram%2520and%2520induction%250Aheads%2520in%2520a%25203M-parameter%2520transformer.%2520Susceptibilities%2520link%2520local%2520learning%250Acoefficients%2520from%2520singular%2520learning%2520theory%2520with%2520linear-response%2520theory%252C%2520and%250Aquantify%2520how%2520local%2520loss%2520landscape%2520geometry%2520deforms%2520under%2520shifts%2520in%2520the%2520data%250Adistribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Studying%20Small%20Language%20Models%20with%20Susceptibilities&entry.906535625=Garrett%20Baker%20and%20George%20Wang%20and%20Jesse%20Hoogland%20and%20Daniel%20Murfet&entry.1292438233=%20%20We%20develop%20a%20linear%20response%20framework%20for%20interpretability%20that%20treats%20a%0Aneural%20network%20as%20a%20Bayesian%20statistical%20mechanical%20system.%20A%20small%2C%20controlled%0Aperturbation%20of%20the%20data%20distribution%2C%20for%20example%20shifting%20the%20Pile%20toward%0AGitHub%20or%20legal%20text%2C%20induces%20a%20first-order%20change%20in%20the%20posterior%20expectation%0Aof%20an%20observable%20localized%20on%20a%20chosen%20component%20of%20the%20network.%20The%20resulting%0Asusceptibility%20can%20be%20estimated%20efficiently%20with%20local%20SGLD%20samples%20and%0Afactorizes%20into%20signed%2C%20per-token%20contributions%20that%20serve%20as%20attribution%0Ascores.%20Building%20a%20set%20of%20perturbations%20%28probes%29%20yields%20a%20response%20matrix%20whose%0Alow-rank%20structure%20separates%20functional%20modules%20such%20as%20multigram%20and%20induction%0Aheads%20in%20a%203M-parameter%20transformer.%20Susceptibilities%20link%20local%20learning%0Acoefficients%20from%20singular%20learning%20theory%20with%20linear-response%20theory%2C%20and%0Aquantify%20how%20local%20loss%20landscape%20geometry%20deforms%20under%20shifts%20in%20the%20data%0Adistribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18274v1&entry.124074799=Read"},
{"title": "Decoding complexity: how machine learning is redefining scientific\n  discovery", "author": "Ricardo Vinuesa and Paola Cinnella and Jean Rabault and Hossein Azizpour and Stefan Bauer and Bingni W. Brunton and Arne Elofsson and Elias Jarlebring and Hedvig Kjellstrom and Stefano Markidis and David Marlevi and Javier Garcia-Martinez and Steven L. Brunton", "abstract": "  As modern scientific instruments generate vast amounts of data and the volume\nof information in the scientific literature continues to grow, machine learning\n(ML) has become an essential tool for organising, analysing, and interpreting\nthese complex datasets. This paper explores the transformative role of ML in\naccelerating breakthroughs across a range of scientific disciplines. By\npresenting key examples -- such as brain mapping and exoplanet detection -- we\ndemonstrate how ML is reshaping scientific research. We also explore different\nscenarios where different levels of knowledge of the underlying phenomenon are\navailable, identifying strategies to overcome limitations and unlock the full\npotential of ML. Despite its advances, the growing reliance on ML poses\nchallenges for research applications and rigorous validation of discoveries. We\nargue that even with these challenges, ML is poised to disrupt traditional\nmethodologies and advance the boundaries of knowledge by enabling researchers\nto tackle increasingly complex problems. Thus, the scientific community can\nmove beyond the necessary traditional oversimplifications to embrace the full\ncomplexity of natural systems, ultimately paving the way for interdisciplinary\nbreakthroughs and innovative solutions to humanity's most pressing challenges.\n", "link": "http://arxiv.org/abs/2405.04161v2", "date": "2025-04-25", "relevancy": 2.036, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5199}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5199}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20complexity%3A%20how%20machine%20learning%20is%20redefining%20scientific%0A%20%20discovery&body=Title%3A%20Decoding%20complexity%3A%20how%20machine%20learning%20is%20redefining%20scientific%0A%20%20discovery%0AAuthor%3A%20Ricardo%20Vinuesa%20and%20Paola%20Cinnella%20and%20Jean%20Rabault%20and%20Hossein%20Azizpour%20and%20Stefan%20Bauer%20and%20Bingni%20W.%20Brunton%20and%20Arne%20Elofsson%20and%20Elias%20Jarlebring%20and%20Hedvig%20Kjellstrom%20and%20Stefano%20Markidis%20and%20David%20Marlevi%20and%20Javier%20Garcia-Martinez%20and%20Steven%20L.%20Brunton%0AAbstract%3A%20%20%20As%20modern%20scientific%20instruments%20generate%20vast%20amounts%20of%20data%20and%20the%20volume%0Aof%20information%20in%20the%20scientific%20literature%20continues%20to%20grow%2C%20machine%20learning%0A%28ML%29%20has%20become%20an%20essential%20tool%20for%20organising%2C%20analysing%2C%20and%20interpreting%0Athese%20complex%20datasets.%20This%20paper%20explores%20the%20transformative%20role%20of%20ML%20in%0Aaccelerating%20breakthroughs%20across%20a%20range%20of%20scientific%20disciplines.%20By%0Apresenting%20key%20examples%20--%20such%20as%20brain%20mapping%20and%20exoplanet%20detection%20--%20we%0Ademonstrate%20how%20ML%20is%20reshaping%20scientific%20research.%20We%20also%20explore%20different%0Ascenarios%20where%20different%20levels%20of%20knowledge%20of%20the%20underlying%20phenomenon%20are%0Aavailable%2C%20identifying%20strategies%20to%20overcome%20limitations%20and%20unlock%20the%20full%0Apotential%20of%20ML.%20Despite%20its%20advances%2C%20the%20growing%20reliance%20on%20ML%20poses%0Achallenges%20for%20research%20applications%20and%20rigorous%20validation%20of%20discoveries.%20We%0Aargue%20that%20even%20with%20these%20challenges%2C%20ML%20is%20poised%20to%20disrupt%20traditional%0Amethodologies%20and%20advance%20the%20boundaries%20of%20knowledge%20by%20enabling%20researchers%0Ato%20tackle%20increasingly%20complex%20problems.%20Thus%2C%20the%20scientific%20community%20can%0Amove%20beyond%20the%20necessary%20traditional%20oversimplifications%20to%20embrace%20the%20full%0Acomplexity%20of%20natural%20systems%2C%20ultimately%20paving%20the%20way%20for%20interdisciplinary%0Abreakthroughs%20and%20innovative%20solutions%20to%20humanity%27s%20most%20pressing%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04161v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520complexity%253A%2520how%2520machine%2520learning%2520is%2520redefining%2520scientific%250A%2520%2520discovery%26entry.906535625%3DRicardo%2520Vinuesa%2520and%2520Paola%2520Cinnella%2520and%2520Jean%2520Rabault%2520and%2520Hossein%2520Azizpour%2520and%2520Stefan%2520Bauer%2520and%2520Bingni%2520W.%2520Brunton%2520and%2520Arne%2520Elofsson%2520and%2520Elias%2520Jarlebring%2520and%2520Hedvig%2520Kjellstrom%2520and%2520Stefano%2520Markidis%2520and%2520David%2520Marlevi%2520and%2520Javier%2520Garcia-Martinez%2520and%2520Steven%2520L.%2520Brunton%26entry.1292438233%3D%2520%2520As%2520modern%2520scientific%2520instruments%2520generate%2520vast%2520amounts%2520of%2520data%2520and%2520the%2520volume%250Aof%2520information%2520in%2520the%2520scientific%2520literature%2520continues%2520to%2520grow%252C%2520machine%2520learning%250A%2528ML%2529%2520has%2520become%2520an%2520essential%2520tool%2520for%2520organising%252C%2520analysing%252C%2520and%2520interpreting%250Athese%2520complex%2520datasets.%2520This%2520paper%2520explores%2520the%2520transformative%2520role%2520of%2520ML%2520in%250Aaccelerating%2520breakthroughs%2520across%2520a%2520range%2520of%2520scientific%2520disciplines.%2520By%250Apresenting%2520key%2520examples%2520--%2520such%2520as%2520brain%2520mapping%2520and%2520exoplanet%2520detection%2520--%2520we%250Ademonstrate%2520how%2520ML%2520is%2520reshaping%2520scientific%2520research.%2520We%2520also%2520explore%2520different%250Ascenarios%2520where%2520different%2520levels%2520of%2520knowledge%2520of%2520the%2520underlying%2520phenomenon%2520are%250Aavailable%252C%2520identifying%2520strategies%2520to%2520overcome%2520limitations%2520and%2520unlock%2520the%2520full%250Apotential%2520of%2520ML.%2520Despite%2520its%2520advances%252C%2520the%2520growing%2520reliance%2520on%2520ML%2520poses%250Achallenges%2520for%2520research%2520applications%2520and%2520rigorous%2520validation%2520of%2520discoveries.%2520We%250Aargue%2520that%2520even%2520with%2520these%2520challenges%252C%2520ML%2520is%2520poised%2520to%2520disrupt%2520traditional%250Amethodologies%2520and%2520advance%2520the%2520boundaries%2520of%2520knowledge%2520by%2520enabling%2520researchers%250Ato%2520tackle%2520increasingly%2520complex%2520problems.%2520Thus%252C%2520the%2520scientific%2520community%2520can%250Amove%2520beyond%2520the%2520necessary%2520traditional%2520oversimplifications%2520to%2520embrace%2520the%2520full%250Acomplexity%2520of%2520natural%2520systems%252C%2520ultimately%2520paving%2520the%2520way%2520for%2520interdisciplinary%250Abreakthroughs%2520and%2520innovative%2520solutions%2520to%2520humanity%2527s%2520most%2520pressing%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04161v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20complexity%3A%20how%20machine%20learning%20is%20redefining%20scientific%0A%20%20discovery&entry.906535625=Ricardo%20Vinuesa%20and%20Paola%20Cinnella%20and%20Jean%20Rabault%20and%20Hossein%20Azizpour%20and%20Stefan%20Bauer%20and%20Bingni%20W.%20Brunton%20and%20Arne%20Elofsson%20and%20Elias%20Jarlebring%20and%20Hedvig%20Kjellstrom%20and%20Stefano%20Markidis%20and%20David%20Marlevi%20and%20Javier%20Garcia-Martinez%20and%20Steven%20L.%20Brunton&entry.1292438233=%20%20As%20modern%20scientific%20instruments%20generate%20vast%20amounts%20of%20data%20and%20the%20volume%0Aof%20information%20in%20the%20scientific%20literature%20continues%20to%20grow%2C%20machine%20learning%0A%28ML%29%20has%20become%20an%20essential%20tool%20for%20organising%2C%20analysing%2C%20and%20interpreting%0Athese%20complex%20datasets.%20This%20paper%20explores%20the%20transformative%20role%20of%20ML%20in%0Aaccelerating%20breakthroughs%20across%20a%20range%20of%20scientific%20disciplines.%20By%0Apresenting%20key%20examples%20--%20such%20as%20brain%20mapping%20and%20exoplanet%20detection%20--%20we%0Ademonstrate%20how%20ML%20is%20reshaping%20scientific%20research.%20We%20also%20explore%20different%0Ascenarios%20where%20different%20levels%20of%20knowledge%20of%20the%20underlying%20phenomenon%20are%0Aavailable%2C%20identifying%20strategies%20to%20overcome%20limitations%20and%20unlock%20the%20full%0Apotential%20of%20ML.%20Despite%20its%20advances%2C%20the%20growing%20reliance%20on%20ML%20poses%0Achallenges%20for%20research%20applications%20and%20rigorous%20validation%20of%20discoveries.%20We%0Aargue%20that%20even%20with%20these%20challenges%2C%20ML%20is%20poised%20to%20disrupt%20traditional%0Amethodologies%20and%20advance%20the%20boundaries%20of%20knowledge%20by%20enabling%20researchers%0Ato%20tackle%20increasingly%20complex%20problems.%20Thus%2C%20the%20scientific%20community%20can%0Amove%20beyond%20the%20necessary%20traditional%20oversimplifications%20to%20embrace%20the%20full%0Acomplexity%20of%20natural%20systems%2C%20ultimately%20paving%20the%20way%20for%20interdisciplinary%0Abreakthroughs%20and%20innovative%20solutions%20to%20humanity%27s%20most%20pressing%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04161v2&entry.124074799=Read"},
{"title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to\n  Controllable Language Generation", "author": "Gwen Yidou Weng and Benjie Wang and Guy Van den Broeck", "abstract": "  As large language models (LMs) advance, there is an increasing need to\ncontrol their outputs to align with human values (e.g., detoxification) or\ndesired attributes (e.g., personalization, topic). However, autoregressive\nmodels focus on next-token predictions and struggle with global properties that\nrequire looking ahead. Existing solutions either tune or post-train LMs for\neach new attribute - expensive and inflexible - or approximate the Expected\nAttribute Probability (EAP) of future sequences by sampling or training, which\nis slow and unreliable for rare attributes. We introduce TRACE (Tractable\nProbabilistic Reasoning for Adaptable Controllable gEneration), a novel\nframework that efficiently computes EAP and adapts to new attributes through\ntractable probabilistic reasoning and lightweight control. TRACE distills a\nHidden Markov Model (HMM) from an LM and pairs it with a small classifier to\nestimate attribute probabilities, enabling exact EAP computation over the HMM's\npredicted futures. This EAP is then used to reweigh the LM's next-token\nprobabilities for globally compliant continuations. Empirically, TRACE achieves\nstate-of-the-art results in detoxification with only 10% decoding overhead,\nadapts to 76 low-resource personalized LLMs within seconds, and seamlessly\nextends to composite attributes.\n", "link": "http://arxiv.org/abs/2504.18535v1", "date": "2025-04-25", "relevancy": 2.0314, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5287}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5157}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRACE%20Back%20from%20the%20Future%3A%20A%20Probabilistic%20Reasoning%20Approach%20to%0A%20%20Controllable%20Language%20Generation&body=Title%3A%20TRACE%20Back%20from%20the%20Future%3A%20A%20Probabilistic%20Reasoning%20Approach%20to%0A%20%20Controllable%20Language%20Generation%0AAuthor%3A%20Gwen%20Yidou%20Weng%20and%20Benjie%20Wang%20and%20Guy%20Van%20den%20Broeck%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LMs%29%20advance%2C%20there%20is%20an%20increasing%20need%20to%0Acontrol%20their%20outputs%20to%20align%20with%20human%20values%20%28e.g.%2C%20detoxification%29%20or%0Adesired%20attributes%20%28e.g.%2C%20personalization%2C%20topic%29.%20However%2C%20autoregressive%0Amodels%20focus%20on%20next-token%20predictions%20and%20struggle%20with%20global%20properties%20that%0Arequire%20looking%20ahead.%20Existing%20solutions%20either%20tune%20or%20post-train%20LMs%20for%0Aeach%20new%20attribute%20-%20expensive%20and%20inflexible%20-%20or%20approximate%20the%20Expected%0AAttribute%20Probability%20%28EAP%29%20of%20future%20sequences%20by%20sampling%20or%20training%2C%20which%0Ais%20slow%20and%20unreliable%20for%20rare%20attributes.%20We%20introduce%20TRACE%20%28Tractable%0AProbabilistic%20Reasoning%20for%20Adaptable%20Controllable%20gEneration%29%2C%20a%20novel%0Aframework%20that%20efficiently%20computes%20EAP%20and%20adapts%20to%20new%20attributes%20through%0Atractable%20probabilistic%20reasoning%20and%20lightweight%20control.%20TRACE%20distills%20a%0AHidden%20Markov%20Model%20%28HMM%29%20from%20an%20LM%20and%20pairs%20it%20with%20a%20small%20classifier%20to%0Aestimate%20attribute%20probabilities%2C%20enabling%20exact%20EAP%20computation%20over%20the%20HMM%27s%0Apredicted%20futures.%20This%20EAP%20is%20then%20used%20to%20reweigh%20the%20LM%27s%20next-token%0Aprobabilities%20for%20globally%20compliant%20continuations.%20Empirically%2C%20TRACE%20achieves%0Astate-of-the-art%20results%20in%20detoxification%20with%20only%2010%25%20decoding%20overhead%2C%0Aadapts%20to%2076%20low-resource%20personalized%20LLMs%20within%20seconds%2C%20and%20seamlessly%0Aextends%20to%20composite%20attributes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRACE%2520Back%2520from%2520the%2520Future%253A%2520A%2520Probabilistic%2520Reasoning%2520Approach%2520to%250A%2520%2520Controllable%2520Language%2520Generation%26entry.906535625%3DGwen%2520Yidou%2520Weng%2520and%2520Benjie%2520Wang%2520and%2520Guy%2520Van%2520den%2520Broeck%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LMs%2529%2520advance%252C%2520there%2520is%2520an%2520increasing%2520need%2520to%250Acontrol%2520their%2520outputs%2520to%2520align%2520with%2520human%2520values%2520%2528e.g.%252C%2520detoxification%2529%2520or%250Adesired%2520attributes%2520%2528e.g.%252C%2520personalization%252C%2520topic%2529.%2520However%252C%2520autoregressive%250Amodels%2520focus%2520on%2520next-token%2520predictions%2520and%2520struggle%2520with%2520global%2520properties%2520that%250Arequire%2520looking%2520ahead.%2520Existing%2520solutions%2520either%2520tune%2520or%2520post-train%2520LMs%2520for%250Aeach%2520new%2520attribute%2520-%2520expensive%2520and%2520inflexible%2520-%2520or%2520approximate%2520the%2520Expected%250AAttribute%2520Probability%2520%2528EAP%2529%2520of%2520future%2520sequences%2520by%2520sampling%2520or%2520training%252C%2520which%250Ais%2520slow%2520and%2520unreliable%2520for%2520rare%2520attributes.%2520We%2520introduce%2520TRACE%2520%2528Tractable%250AProbabilistic%2520Reasoning%2520for%2520Adaptable%2520Controllable%2520gEneration%2529%252C%2520a%2520novel%250Aframework%2520that%2520efficiently%2520computes%2520EAP%2520and%2520adapts%2520to%2520new%2520attributes%2520through%250Atractable%2520probabilistic%2520reasoning%2520and%2520lightweight%2520control.%2520TRACE%2520distills%2520a%250AHidden%2520Markov%2520Model%2520%2528HMM%2529%2520from%2520an%2520LM%2520and%2520pairs%2520it%2520with%2520a%2520small%2520classifier%2520to%250Aestimate%2520attribute%2520probabilities%252C%2520enabling%2520exact%2520EAP%2520computation%2520over%2520the%2520HMM%2527s%250Apredicted%2520futures.%2520This%2520EAP%2520is%2520then%2520used%2520to%2520reweigh%2520the%2520LM%2527s%2520next-token%250Aprobabilities%2520for%2520globally%2520compliant%2520continuations.%2520Empirically%252C%2520TRACE%2520achieves%250Astate-of-the-art%2520results%2520in%2520detoxification%2520with%2520only%252010%2525%2520decoding%2520overhead%252C%250Aadapts%2520to%252076%2520low-resource%2520personalized%2520LLMs%2520within%2520seconds%252C%2520and%2520seamlessly%250Aextends%2520to%2520composite%2520attributes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRACE%20Back%20from%20the%20Future%3A%20A%20Probabilistic%20Reasoning%20Approach%20to%0A%20%20Controllable%20Language%20Generation&entry.906535625=Gwen%20Yidou%20Weng%20and%20Benjie%20Wang%20and%20Guy%20Van%20den%20Broeck&entry.1292438233=%20%20As%20large%20language%20models%20%28LMs%29%20advance%2C%20there%20is%20an%20increasing%20need%20to%0Acontrol%20their%20outputs%20to%20align%20with%20human%20values%20%28e.g.%2C%20detoxification%29%20or%0Adesired%20attributes%20%28e.g.%2C%20personalization%2C%20topic%29.%20However%2C%20autoregressive%0Amodels%20focus%20on%20next-token%20predictions%20and%20struggle%20with%20global%20properties%20that%0Arequire%20looking%20ahead.%20Existing%20solutions%20either%20tune%20or%20post-train%20LMs%20for%0Aeach%20new%20attribute%20-%20expensive%20and%20inflexible%20-%20or%20approximate%20the%20Expected%0AAttribute%20Probability%20%28EAP%29%20of%20future%20sequences%20by%20sampling%20or%20training%2C%20which%0Ais%20slow%20and%20unreliable%20for%20rare%20attributes.%20We%20introduce%20TRACE%20%28Tractable%0AProbabilistic%20Reasoning%20for%20Adaptable%20Controllable%20gEneration%29%2C%20a%20novel%0Aframework%20that%20efficiently%20computes%20EAP%20and%20adapts%20to%20new%20attributes%20through%0Atractable%20probabilistic%20reasoning%20and%20lightweight%20control.%20TRACE%20distills%20a%0AHidden%20Markov%20Model%20%28HMM%29%20from%20an%20LM%20and%20pairs%20it%20with%20a%20small%20classifier%20to%0Aestimate%20attribute%20probabilities%2C%20enabling%20exact%20EAP%20computation%20over%20the%20HMM%27s%0Apredicted%20futures.%20This%20EAP%20is%20then%20used%20to%20reweigh%20the%20LM%27s%20next-token%0Aprobabilities%20for%20globally%20compliant%20continuations.%20Empirically%2C%20TRACE%20achieves%0Astate-of-the-art%20results%20in%20detoxification%20with%20only%2010%25%20decoding%20overhead%2C%0Aadapts%20to%2076%20low-resource%20personalized%20LLMs%20within%20seconds%2C%20and%20seamlessly%0Aextends%20to%20composite%20attributes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18535v1&entry.124074799=Read"},
{"title": "ElChat: Adapting Chat Language Models Using Only Target Unlabeled\n  Language Data", "author": "Atsuki Yamaguchi and Terufumi Morishita and Aline Villavicencio and Nikolaos Aletras", "abstract": "  Vocabulary expansion (VE) is the de-facto approach to language adaptation of\nlarge language models (LLMs) by adding new tokens and continuing pre-training\non target data. While this is effective for base models trained on unlabeled\ndata, it poses challenges for chat models trained to follow instructions\nthrough labeled conversation data. Directly adapting the latter with VE on\ntarget unlabeled data may result in forgetting chat abilities. While ideal,\ntarget chat data is often unavailable or costly to create for low-resource\nlanguages, and machine-translated alternatives are not always effective. To\naddress this issue, previous work proposed using a base and chat model from the\nsame family. This method first adapts the base LLM with VE on target unlabeled\ndata and then converts it to a chat model by adding a chat vector (CV) derived\nfrom the weight difference between the source base and chat models. We propose\nElChat, a new language adaptation method for chat LLMs that adapts a chat model\ndirectly on target unlabeled data, without a base model. It elicits chat\nabilities by injecting information from the source chat model. ElChat offers\nmore robust and competitive target language and safety performance while\nachieving superior English, chat, and instruction-following abilities compared\nto CV.\n", "link": "http://arxiv.org/abs/2412.11704v3", "date": "2025-04-25", "relevancy": 2.0265, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ElChat%3A%20Adapting%20Chat%20Language%20Models%20Using%20Only%20Target%20Unlabeled%0A%20%20Language%20Data&body=Title%3A%20ElChat%3A%20Adapting%20Chat%20Language%20Models%20Using%20Only%20Target%20Unlabeled%0A%20%20Language%20Data%0AAuthor%3A%20Atsuki%20Yamaguchi%20and%20Terufumi%20Morishita%20and%20Aline%20Villavicencio%20and%20Nikolaos%20Aletras%0AAbstract%3A%20%20%20Vocabulary%20expansion%20%28VE%29%20is%20the%20de-facto%20approach%20to%20language%20adaptation%20of%0Alarge%20language%20models%20%28LLMs%29%20by%20adding%20new%20tokens%20and%20continuing%20pre-training%0Aon%20target%20data.%20While%20this%20is%20effective%20for%20base%20models%20trained%20on%20unlabeled%0Adata%2C%20it%20poses%20challenges%20for%20chat%20models%20trained%20to%20follow%20instructions%0Athrough%20labeled%20conversation%20data.%20Directly%20adapting%20the%20latter%20with%20VE%20on%0Atarget%20unlabeled%20data%20may%20result%20in%20forgetting%20chat%20abilities.%20While%20ideal%2C%0Atarget%20chat%20data%20is%20often%20unavailable%20or%20costly%20to%20create%20for%20low-resource%0Alanguages%2C%20and%20machine-translated%20alternatives%20are%20not%20always%20effective.%20To%0Aaddress%20this%20issue%2C%20previous%20work%20proposed%20using%20a%20base%20and%20chat%20model%20from%20the%0Asame%20family.%20This%20method%20first%20adapts%20the%20base%20LLM%20with%20VE%20on%20target%20unlabeled%0Adata%20and%20then%20converts%20it%20to%20a%20chat%20model%20by%20adding%20a%20chat%20vector%20%28CV%29%20derived%0Afrom%20the%20weight%20difference%20between%20the%20source%20base%20and%20chat%20models.%20We%20propose%0AElChat%2C%20a%20new%20language%20adaptation%20method%20for%20chat%20LLMs%20that%20adapts%20a%20chat%20model%0Adirectly%20on%20target%20unlabeled%20data%2C%20without%20a%20base%20model.%20It%20elicits%20chat%0Aabilities%20by%20injecting%20information%20from%20the%20source%20chat%20model.%20ElChat%20offers%0Amore%20robust%20and%20competitive%20target%20language%20and%20safety%20performance%20while%0Aachieving%20superior%20English%2C%20chat%2C%20and%20instruction-following%20abilities%20compared%0Ato%20CV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11704v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElChat%253A%2520Adapting%2520Chat%2520Language%2520Models%2520Using%2520Only%2520Target%2520Unlabeled%250A%2520%2520Language%2520Data%26entry.906535625%3DAtsuki%2520Yamaguchi%2520and%2520Terufumi%2520Morishita%2520and%2520Aline%2520Villavicencio%2520and%2520Nikolaos%2520Aletras%26entry.1292438233%3D%2520%2520Vocabulary%2520expansion%2520%2528VE%2529%2520is%2520the%2520de-facto%2520approach%2520to%2520language%2520adaptation%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520by%2520adding%2520new%2520tokens%2520and%2520continuing%2520pre-training%250Aon%2520target%2520data.%2520While%2520this%2520is%2520effective%2520for%2520base%2520models%2520trained%2520on%2520unlabeled%250Adata%252C%2520it%2520poses%2520challenges%2520for%2520chat%2520models%2520trained%2520to%2520follow%2520instructions%250Athrough%2520labeled%2520conversation%2520data.%2520Directly%2520adapting%2520the%2520latter%2520with%2520VE%2520on%250Atarget%2520unlabeled%2520data%2520may%2520result%2520in%2520forgetting%2520chat%2520abilities.%2520While%2520ideal%252C%250Atarget%2520chat%2520data%2520is%2520often%2520unavailable%2520or%2520costly%2520to%2520create%2520for%2520low-resource%250Alanguages%252C%2520and%2520machine-translated%2520alternatives%2520are%2520not%2520always%2520effective.%2520To%250Aaddress%2520this%2520issue%252C%2520previous%2520work%2520proposed%2520using%2520a%2520base%2520and%2520chat%2520model%2520from%2520the%250Asame%2520family.%2520This%2520method%2520first%2520adapts%2520the%2520base%2520LLM%2520with%2520VE%2520on%2520target%2520unlabeled%250Adata%2520and%2520then%2520converts%2520it%2520to%2520a%2520chat%2520model%2520by%2520adding%2520a%2520chat%2520vector%2520%2528CV%2529%2520derived%250Afrom%2520the%2520weight%2520difference%2520between%2520the%2520source%2520base%2520and%2520chat%2520models.%2520We%2520propose%250AElChat%252C%2520a%2520new%2520language%2520adaptation%2520method%2520for%2520chat%2520LLMs%2520that%2520adapts%2520a%2520chat%2520model%250Adirectly%2520on%2520target%2520unlabeled%2520data%252C%2520without%2520a%2520base%2520model.%2520It%2520elicits%2520chat%250Aabilities%2520by%2520injecting%2520information%2520from%2520the%2520source%2520chat%2520model.%2520ElChat%2520offers%250Amore%2520robust%2520and%2520competitive%2520target%2520language%2520and%2520safety%2520performance%2520while%250Aachieving%2520superior%2520English%252C%2520chat%252C%2520and%2520instruction-following%2520abilities%2520compared%250Ato%2520CV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11704v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ElChat%3A%20Adapting%20Chat%20Language%20Models%20Using%20Only%20Target%20Unlabeled%0A%20%20Language%20Data&entry.906535625=Atsuki%20Yamaguchi%20and%20Terufumi%20Morishita%20and%20Aline%20Villavicencio%20and%20Nikolaos%20Aletras&entry.1292438233=%20%20Vocabulary%20expansion%20%28VE%29%20is%20the%20de-facto%20approach%20to%20language%20adaptation%20of%0Alarge%20language%20models%20%28LLMs%29%20by%20adding%20new%20tokens%20and%20continuing%20pre-training%0Aon%20target%20data.%20While%20this%20is%20effective%20for%20base%20models%20trained%20on%20unlabeled%0Adata%2C%20it%20poses%20challenges%20for%20chat%20models%20trained%20to%20follow%20instructions%0Athrough%20labeled%20conversation%20data.%20Directly%20adapting%20the%20latter%20with%20VE%20on%0Atarget%20unlabeled%20data%20may%20result%20in%20forgetting%20chat%20abilities.%20While%20ideal%2C%0Atarget%20chat%20data%20is%20often%20unavailable%20or%20costly%20to%20create%20for%20low-resource%0Alanguages%2C%20and%20machine-translated%20alternatives%20are%20not%20always%20effective.%20To%0Aaddress%20this%20issue%2C%20previous%20work%20proposed%20using%20a%20base%20and%20chat%20model%20from%20the%0Asame%20family.%20This%20method%20first%20adapts%20the%20base%20LLM%20with%20VE%20on%20target%20unlabeled%0Adata%20and%20then%20converts%20it%20to%20a%20chat%20model%20by%20adding%20a%20chat%20vector%20%28CV%29%20derived%0Afrom%20the%20weight%20difference%20between%20the%20source%20base%20and%20chat%20models.%20We%20propose%0AElChat%2C%20a%20new%20language%20adaptation%20method%20for%20chat%20LLMs%20that%20adapts%20a%20chat%20model%0Adirectly%20on%20target%20unlabeled%20data%2C%20without%20a%20base%20model.%20It%20elicits%20chat%0Aabilities%20by%20injecting%20information%20from%20the%20source%20chat%20model.%20ElChat%20offers%0Amore%20robust%20and%20competitive%20target%20language%20and%20safety%20performance%20while%0Aachieving%20superior%20English%2C%20chat%2C%20and%20instruction-following%20abilities%20compared%0Ato%20CV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11704v3&entry.124074799=Read"},
{"title": "Generalization Guarantees for Multi-View Representation Learning and\n  Application to Regularization via Gaussian Product Mixture Prior", "author": "Milad Sefidgaran and Abdellatif Zaidi and Piotr Krasnowski", "abstract": "  We study the problem of distributed multi-view representation learning. In\nthis problem, $K$ agents observe each one distinct, possibly statistically\ncorrelated, view and independently extracts from it a suitable representation\nin a manner that a decoder that gets all $K$ representations estimates\ncorrectly the hidden label. In the absence of any explicit coordination between\nthe agents, a central question is: what should each agent extract from its view\nthat is necessary and sufficient for a correct estimation at the decoder? In\nthis paper, we investigate this question from a generalization error\nperspective. First, we establish several generalization bounds in terms of the\nrelative entropy between the distribution of the representations extracted from\ntraining and \"test\" datasets and a data-dependent symmetric prior, i.e., the\nMinimum Description Length (MDL) of the latent variables for all views and\ntraining and test datasets. Then, we use the obtained bounds to devise a\nregularizer; and investigate in depth the question of the selection of a\nsuitable prior. In particular, we show and conduct experiments that illustrate\nthat our data-dependent Gaussian mixture priors with judiciously chosen weights\nlead to good performance. For single-view settings (i.e., $K=1$), our\nexperimental results are shown to outperform existing prior art Variational\nInformation Bottleneck (VIB) and Category-Dependent VIB (CDVIB) approaches.\nInterestingly, we show that a weighted attention mechanism emerges naturally in\nthis setting. Finally, for the multi-view setting, we show that the selection\nof the joint prior as a Gaussians product mixture induces a Gaussian mixture\nmarginal prior for each marginal view and implicitly encourages the agents to\nextract and output redundant features, a finding which is somewhat\ncounter-intuitive.\n", "link": "http://arxiv.org/abs/2504.18455v1", "date": "2025-04-25", "relevancy": 2.0234, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5123}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5061}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20Guarantees%20for%20Multi-View%20Representation%20Learning%20and%0A%20%20Application%20to%20Regularization%20via%20Gaussian%20Product%20Mixture%20Prior&body=Title%3A%20Generalization%20Guarantees%20for%20Multi-View%20Representation%20Learning%20and%0A%20%20Application%20to%20Regularization%20via%20Gaussian%20Product%20Mixture%20Prior%0AAuthor%3A%20Milad%20Sefidgaran%20and%20Abdellatif%20Zaidi%20and%20Piotr%20Krasnowski%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20distributed%20multi-view%20representation%20learning.%20In%0Athis%20problem%2C%20%24K%24%20agents%20observe%20each%20one%20distinct%2C%20possibly%20statistically%0Acorrelated%2C%20view%20and%20independently%20extracts%20from%20it%20a%20suitable%20representation%0Ain%20a%20manner%20that%20a%20decoder%20that%20gets%20all%20%24K%24%20representations%20estimates%0Acorrectly%20the%20hidden%20label.%20In%20the%20absence%20of%20any%20explicit%20coordination%20between%0Athe%20agents%2C%20a%20central%20question%20is%3A%20what%20should%20each%20agent%20extract%20from%20its%20view%0Athat%20is%20necessary%20and%20sufficient%20for%20a%20correct%20estimation%20at%20the%20decoder%3F%20In%0Athis%20paper%2C%20we%20investigate%20this%20question%20from%20a%20generalization%20error%0Aperspective.%20First%2C%20we%20establish%20several%20generalization%20bounds%20in%20terms%20of%20the%0Arelative%20entropy%20between%20the%20distribution%20of%20the%20representations%20extracted%20from%0Atraining%20and%20%22test%22%20datasets%20and%20a%20data-dependent%20symmetric%20prior%2C%20i.e.%2C%20the%0AMinimum%20Description%20Length%20%28MDL%29%20of%20the%20latent%20variables%20for%20all%20views%20and%0Atraining%20and%20test%20datasets.%20Then%2C%20we%20use%20the%20obtained%20bounds%20to%20devise%20a%0Aregularizer%3B%20and%20investigate%20in%20depth%20the%20question%20of%20the%20selection%20of%20a%0Asuitable%20prior.%20In%20particular%2C%20we%20show%20and%20conduct%20experiments%20that%20illustrate%0Athat%20our%20data-dependent%20Gaussian%20mixture%20priors%20with%20judiciously%20chosen%20weights%0Alead%20to%20good%20performance.%20For%20single-view%20settings%20%28i.e.%2C%20%24K%3D1%24%29%2C%20our%0Aexperimental%20results%20are%20shown%20to%20outperform%20existing%20prior%20art%20Variational%0AInformation%20Bottleneck%20%28VIB%29%20and%20Category-Dependent%20VIB%20%28CDVIB%29%20approaches.%0AInterestingly%2C%20we%20show%20that%20a%20weighted%20attention%20mechanism%20emerges%20naturally%20in%0Athis%20setting.%20Finally%2C%20for%20the%20multi-view%20setting%2C%20we%20show%20that%20the%20selection%0Aof%20the%20joint%20prior%20as%20a%20Gaussians%20product%20mixture%20induces%20a%20Gaussian%20mixture%0Amarginal%20prior%20for%20each%20marginal%20view%20and%20implicitly%20encourages%20the%20agents%20to%0Aextract%20and%20output%20redundant%20features%2C%20a%20finding%20which%20is%20somewhat%0Acounter-intuitive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520Guarantees%2520for%2520Multi-View%2520Representation%2520Learning%2520and%250A%2520%2520Application%2520to%2520Regularization%2520via%2520Gaussian%2520Product%2520Mixture%2520Prior%26entry.906535625%3DMilad%2520Sefidgaran%2520and%2520Abdellatif%2520Zaidi%2520and%2520Piotr%2520Krasnowski%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520distributed%2520multi-view%2520representation%2520learning.%2520In%250Athis%2520problem%252C%2520%2524K%2524%2520agents%2520observe%2520each%2520one%2520distinct%252C%2520possibly%2520statistically%250Acorrelated%252C%2520view%2520and%2520independently%2520extracts%2520from%2520it%2520a%2520suitable%2520representation%250Ain%2520a%2520manner%2520that%2520a%2520decoder%2520that%2520gets%2520all%2520%2524K%2524%2520representations%2520estimates%250Acorrectly%2520the%2520hidden%2520label.%2520In%2520the%2520absence%2520of%2520any%2520explicit%2520coordination%2520between%250Athe%2520agents%252C%2520a%2520central%2520question%2520is%253A%2520what%2520should%2520each%2520agent%2520extract%2520from%2520its%2520view%250Athat%2520is%2520necessary%2520and%2520sufficient%2520for%2520a%2520correct%2520estimation%2520at%2520the%2520decoder%253F%2520In%250Athis%2520paper%252C%2520we%2520investigate%2520this%2520question%2520from%2520a%2520generalization%2520error%250Aperspective.%2520First%252C%2520we%2520establish%2520several%2520generalization%2520bounds%2520in%2520terms%2520of%2520the%250Arelative%2520entropy%2520between%2520the%2520distribution%2520of%2520the%2520representations%2520extracted%2520from%250Atraining%2520and%2520%2522test%2522%2520datasets%2520and%2520a%2520data-dependent%2520symmetric%2520prior%252C%2520i.e.%252C%2520the%250AMinimum%2520Description%2520Length%2520%2528MDL%2529%2520of%2520the%2520latent%2520variables%2520for%2520all%2520views%2520and%250Atraining%2520and%2520test%2520datasets.%2520Then%252C%2520we%2520use%2520the%2520obtained%2520bounds%2520to%2520devise%2520a%250Aregularizer%253B%2520and%2520investigate%2520in%2520depth%2520the%2520question%2520of%2520the%2520selection%2520of%2520a%250Asuitable%2520prior.%2520In%2520particular%252C%2520we%2520show%2520and%2520conduct%2520experiments%2520that%2520illustrate%250Athat%2520our%2520data-dependent%2520Gaussian%2520mixture%2520priors%2520with%2520judiciously%2520chosen%2520weights%250Alead%2520to%2520good%2520performance.%2520For%2520single-view%2520settings%2520%2528i.e.%252C%2520%2524K%253D1%2524%2529%252C%2520our%250Aexperimental%2520results%2520are%2520shown%2520to%2520outperform%2520existing%2520prior%2520art%2520Variational%250AInformation%2520Bottleneck%2520%2528VIB%2529%2520and%2520Category-Dependent%2520VIB%2520%2528CDVIB%2529%2520approaches.%250AInterestingly%252C%2520we%2520show%2520that%2520a%2520weighted%2520attention%2520mechanism%2520emerges%2520naturally%2520in%250Athis%2520setting.%2520Finally%252C%2520for%2520the%2520multi-view%2520setting%252C%2520we%2520show%2520that%2520the%2520selection%250Aof%2520the%2520joint%2520prior%2520as%2520a%2520Gaussians%2520product%2520mixture%2520induces%2520a%2520Gaussian%2520mixture%250Amarginal%2520prior%2520for%2520each%2520marginal%2520view%2520and%2520implicitly%2520encourages%2520the%2520agents%2520to%250Aextract%2520and%2520output%2520redundant%2520features%252C%2520a%2520finding%2520which%2520is%2520somewhat%250Acounter-intuitive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20Guarantees%20for%20Multi-View%20Representation%20Learning%20and%0A%20%20Application%20to%20Regularization%20via%20Gaussian%20Product%20Mixture%20Prior&entry.906535625=Milad%20Sefidgaran%20and%20Abdellatif%20Zaidi%20and%20Piotr%20Krasnowski&entry.1292438233=%20%20We%20study%20the%20problem%20of%20distributed%20multi-view%20representation%20learning.%20In%0Athis%20problem%2C%20%24K%24%20agents%20observe%20each%20one%20distinct%2C%20possibly%20statistically%0Acorrelated%2C%20view%20and%20independently%20extracts%20from%20it%20a%20suitable%20representation%0Ain%20a%20manner%20that%20a%20decoder%20that%20gets%20all%20%24K%24%20representations%20estimates%0Acorrectly%20the%20hidden%20label.%20In%20the%20absence%20of%20any%20explicit%20coordination%20between%0Athe%20agents%2C%20a%20central%20question%20is%3A%20what%20should%20each%20agent%20extract%20from%20its%20view%0Athat%20is%20necessary%20and%20sufficient%20for%20a%20correct%20estimation%20at%20the%20decoder%3F%20In%0Athis%20paper%2C%20we%20investigate%20this%20question%20from%20a%20generalization%20error%0Aperspective.%20First%2C%20we%20establish%20several%20generalization%20bounds%20in%20terms%20of%20the%0Arelative%20entropy%20between%20the%20distribution%20of%20the%20representations%20extracted%20from%0Atraining%20and%20%22test%22%20datasets%20and%20a%20data-dependent%20symmetric%20prior%2C%20i.e.%2C%20the%0AMinimum%20Description%20Length%20%28MDL%29%20of%20the%20latent%20variables%20for%20all%20views%20and%0Atraining%20and%20test%20datasets.%20Then%2C%20we%20use%20the%20obtained%20bounds%20to%20devise%20a%0Aregularizer%3B%20and%20investigate%20in%20depth%20the%20question%20of%20the%20selection%20of%20a%0Asuitable%20prior.%20In%20particular%2C%20we%20show%20and%20conduct%20experiments%20that%20illustrate%0Athat%20our%20data-dependent%20Gaussian%20mixture%20priors%20with%20judiciously%20chosen%20weights%0Alead%20to%20good%20performance.%20For%20single-view%20settings%20%28i.e.%2C%20%24K%3D1%24%29%2C%20our%0Aexperimental%20results%20are%20shown%20to%20outperform%20existing%20prior%20art%20Variational%0AInformation%20Bottleneck%20%28VIB%29%20and%20Category-Dependent%20VIB%20%28CDVIB%29%20approaches.%0AInterestingly%2C%20we%20show%20that%20a%20weighted%20attention%20mechanism%20emerges%20naturally%20in%0Athis%20setting.%20Finally%2C%20for%20the%20multi-view%20setting%2C%20we%20show%20that%20the%20selection%0Aof%20the%20joint%20prior%20as%20a%20Gaussians%20product%20mixture%20induces%20a%20Gaussian%20mixture%0Amarginal%20prior%20for%20each%20marginal%20view%20and%20implicitly%20encourages%20the%20agents%20to%0Aextract%20and%20output%20redundant%20features%2C%20a%20finding%20which%20is%20somewhat%0Acounter-intuitive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18455v1&entry.124074799=Read"},
{"title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning", "author": " Chris and Yichen Wei and Yi Peng and Xiaokun Wang and Weijie Qiu and Wei Shen and Tianyidan Xie and Jiangbo Pei and Jianhao Zhang and Yunzhuo Hao and Xuchen Song and Yang Liu and Yahui Zhou", "abstract": "  We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that jointly leverages the\nMixed Preference Optimization (MPO) and the Group Relative Policy Optimization\n(GRPO), which harmonizes reward-model guidance with rule-based strategies,\nthereby addressing the long-standing challenge of balancing sophisticated\nreasoning capabilities with broad generalization. To further enhance training\nefficiency, we introduce the Selective Sample Buffer (SSB) mechanism, which\neffectively counters the ``Vanishing Advantages'' dilemma inherent in GRPO by\nprioritizing high-value samples throughout the optimization process. Notably,\nwe observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 78.9 on AIME2024, 63.6 on LiveCodeBench, and\n73.6 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI-o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.\n", "link": "http://arxiv.org/abs/2504.16656v2", "date": "2025-04-25", "relevancy": 2.0194, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skywork%20R1V2%3A%20Multimodal%20Hybrid%20Reinforcement%20Learning%20for%20Reasoning&body=Title%3A%20Skywork%20R1V2%3A%20Multimodal%20Hybrid%20Reinforcement%20Learning%20for%20Reasoning%0AAuthor%3A%20%20Chris%20and%20Yichen%20Wei%20and%20Yi%20Peng%20and%20Xiaokun%20Wang%20and%20Weijie%20Qiu%20and%20Wei%20Shen%20and%20Tianyidan%20Xie%20and%20Jiangbo%20Pei%20and%20Jianhao%20Zhang%20and%20Yunzhuo%20Hao%20and%20Xuchen%20Song%20and%20Yang%20Liu%20and%20Yahui%20Zhou%0AAbstract%3A%20%20%20We%20present%20Skywork%20R1V2%2C%20a%20next-generation%20multimodal%20reasoning%20model%20and%20a%0Amajor%20leap%20forward%20from%20its%20predecessor%2C%20Skywork%20R1V.%20At%20its%20core%2C%20R1V2%0Aintroduces%20a%20hybrid%20reinforcement%20learning%20paradigm%20that%20jointly%20leverages%20the%0AMixed%20Preference%20Optimization%20%28MPO%29%20and%20the%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20which%20harmonizes%20reward-model%20guidance%20with%20rule-based%20strategies%2C%0Athereby%20addressing%20the%20long-standing%20challenge%20of%20balancing%20sophisticated%0Areasoning%20capabilities%20with%20broad%20generalization.%20To%20further%20enhance%20training%0Aefficiency%2C%20we%20introduce%20the%20Selective%20Sample%20Buffer%20%28SSB%29%20mechanism%2C%20which%0Aeffectively%20counters%20the%20%60%60Vanishing%20Advantages%27%27%20dilemma%20inherent%20in%20GRPO%20by%0Aprioritizing%20high-value%20samples%20throughout%20the%20optimization%20process.%20Notably%2C%0Awe%20observe%20that%20excessive%20reinforcement%20signals%20can%20induce%20visual%0Ahallucinations--a%20phenomenon%20we%20systematically%20monitor%20and%20mitigate%20through%0Acalibrated%20reward%20thresholds%20throughout%20the%20training%20process.%20Empirical%20results%0Aaffirm%20the%20exceptional%20capability%20of%20R1V2%2C%20with%20benchmark-leading%20performances%0Asuch%20as%2062.6%20on%20OlympiadBench%2C%2078.9%20on%20AIME2024%2C%2063.6%20on%20LiveCodeBench%2C%20and%0A73.6%20on%20MMMU.%20These%20results%20underscore%20R1V2%27s%20superiority%20over%20existing%0Aopen-source%20models%20and%20demonstrate%20significant%20progress%20in%20closing%20the%0Aperformance%20gap%20with%20premier%20proprietary%20systems%2C%20including%20Gemini%202.5%20and%0AOpenAI-o4-mini.%20The%20Skywork%20R1V2%20model%20weights%20have%20been%20publicly%20released%20to%0Apromote%20openness%20and%20reproducibility%0Ahttps%3A//huggingface.co/Skywork/Skywork-R1V2-38B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16656v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkywork%2520R1V2%253A%2520Multimodal%2520Hybrid%2520Reinforcement%2520Learning%2520for%2520Reasoning%26entry.906535625%3D%2520Chris%2520and%2520Yichen%2520Wei%2520and%2520Yi%2520Peng%2520and%2520Xiaokun%2520Wang%2520and%2520Weijie%2520Qiu%2520and%2520Wei%2520Shen%2520and%2520Tianyidan%2520Xie%2520and%2520Jiangbo%2520Pei%2520and%2520Jianhao%2520Zhang%2520and%2520Yunzhuo%2520Hao%2520and%2520Xuchen%2520Song%2520and%2520Yang%2520Liu%2520and%2520Yahui%2520Zhou%26entry.1292438233%3D%2520%2520We%2520present%2520Skywork%2520R1V2%252C%2520a%2520next-generation%2520multimodal%2520reasoning%2520model%2520and%2520a%250Amajor%2520leap%2520forward%2520from%2520its%2520predecessor%252C%2520Skywork%2520R1V.%2520At%2520its%2520core%252C%2520R1V2%250Aintroduces%2520a%2520hybrid%2520reinforcement%2520learning%2520paradigm%2520that%2520jointly%2520leverages%2520the%250AMixed%2520Preference%2520Optimization%2520%2528MPO%2529%2520and%2520the%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%252C%2520which%2520harmonizes%2520reward-model%2520guidance%2520with%2520rule-based%2520strategies%252C%250Athereby%2520addressing%2520the%2520long-standing%2520challenge%2520of%2520balancing%2520sophisticated%250Areasoning%2520capabilities%2520with%2520broad%2520generalization.%2520To%2520further%2520enhance%2520training%250Aefficiency%252C%2520we%2520introduce%2520the%2520Selective%2520Sample%2520Buffer%2520%2528SSB%2529%2520mechanism%252C%2520which%250Aeffectively%2520counters%2520the%2520%2560%2560Vanishing%2520Advantages%2527%2527%2520dilemma%2520inherent%2520in%2520GRPO%2520by%250Aprioritizing%2520high-value%2520samples%2520throughout%2520the%2520optimization%2520process.%2520Notably%252C%250Awe%2520observe%2520that%2520excessive%2520reinforcement%2520signals%2520can%2520induce%2520visual%250Ahallucinations--a%2520phenomenon%2520we%2520systematically%2520monitor%2520and%2520mitigate%2520through%250Acalibrated%2520reward%2520thresholds%2520throughout%2520the%2520training%2520process.%2520Empirical%2520results%250Aaffirm%2520the%2520exceptional%2520capability%2520of%2520R1V2%252C%2520with%2520benchmark-leading%2520performances%250Asuch%2520as%252062.6%2520on%2520OlympiadBench%252C%252078.9%2520on%2520AIME2024%252C%252063.6%2520on%2520LiveCodeBench%252C%2520and%250A73.6%2520on%2520MMMU.%2520These%2520results%2520underscore%2520R1V2%2527s%2520superiority%2520over%2520existing%250Aopen-source%2520models%2520and%2520demonstrate%2520significant%2520progress%2520in%2520closing%2520the%250Aperformance%2520gap%2520with%2520premier%2520proprietary%2520systems%252C%2520including%2520Gemini%25202.5%2520and%250AOpenAI-o4-mini.%2520The%2520Skywork%2520R1V2%2520model%2520weights%2520have%2520been%2520publicly%2520released%2520to%250Apromote%2520openness%2520and%2520reproducibility%250Ahttps%253A//huggingface.co/Skywork/Skywork-R1V2-38B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16656v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skywork%20R1V2%3A%20Multimodal%20Hybrid%20Reinforcement%20Learning%20for%20Reasoning&entry.906535625=%20Chris%20and%20Yichen%20Wei%20and%20Yi%20Peng%20and%20Xiaokun%20Wang%20and%20Weijie%20Qiu%20and%20Wei%20Shen%20and%20Tianyidan%20Xie%20and%20Jiangbo%20Pei%20and%20Jianhao%20Zhang%20and%20Yunzhuo%20Hao%20and%20Xuchen%20Song%20and%20Yang%20Liu%20and%20Yahui%20Zhou&entry.1292438233=%20%20We%20present%20Skywork%20R1V2%2C%20a%20next-generation%20multimodal%20reasoning%20model%20and%20a%0Amajor%20leap%20forward%20from%20its%20predecessor%2C%20Skywork%20R1V.%20At%20its%20core%2C%20R1V2%0Aintroduces%20a%20hybrid%20reinforcement%20learning%20paradigm%20that%20jointly%20leverages%20the%0AMixed%20Preference%20Optimization%20%28MPO%29%20and%20the%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20which%20harmonizes%20reward-model%20guidance%20with%20rule-based%20strategies%2C%0Athereby%20addressing%20the%20long-standing%20challenge%20of%20balancing%20sophisticated%0Areasoning%20capabilities%20with%20broad%20generalization.%20To%20further%20enhance%20training%0Aefficiency%2C%20we%20introduce%20the%20Selective%20Sample%20Buffer%20%28SSB%29%20mechanism%2C%20which%0Aeffectively%20counters%20the%20%60%60Vanishing%20Advantages%27%27%20dilemma%20inherent%20in%20GRPO%20by%0Aprioritizing%20high-value%20samples%20throughout%20the%20optimization%20process.%20Notably%2C%0Awe%20observe%20that%20excessive%20reinforcement%20signals%20can%20induce%20visual%0Ahallucinations--a%20phenomenon%20we%20systematically%20monitor%20and%20mitigate%20through%0Acalibrated%20reward%20thresholds%20throughout%20the%20training%20process.%20Empirical%20results%0Aaffirm%20the%20exceptional%20capability%20of%20R1V2%2C%20with%20benchmark-leading%20performances%0Asuch%20as%2062.6%20on%20OlympiadBench%2C%2078.9%20on%20AIME2024%2C%2063.6%20on%20LiveCodeBench%2C%20and%0A73.6%20on%20MMMU.%20These%20results%20underscore%20R1V2%27s%20superiority%20over%20existing%0Aopen-source%20models%20and%20demonstrate%20significant%20progress%20in%20closing%20the%0Aperformance%20gap%20with%20premier%20proprietary%20systems%2C%20including%20Gemini%202.5%20and%0AOpenAI-o4-mini.%20The%20Skywork%20R1V2%20model%20weights%20have%20been%20publicly%20released%20to%0Apromote%20openness%20and%20reproducibility%0Ahttps%3A//huggingface.co/Skywork/Skywork-R1V2-38B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16656v2&entry.124074799=Read"},
{"title": "Switch-Based Multi-Part Neural Network", "author": "Surajit Majumder and Paritosh Ranjan and Prodip Roy and Bhuban Padhan", "abstract": "  This paper introduces decentralized and modular neural network framework\ndesigned to enhance the scalability, interpretability, and performance of\nartificial intelligence (AI) systems. At the heart of this framework is a\ndynamic switch mechanism that governs the selective activation and training of\nindividual neurons based on input characteristics, allowing neurons to\nspecialize in distinct segments of the data domain. This approach enables\nneurons to learn from disjoint subsets of data, mimicking biological brain\nfunction by promoting task specialization and improving the interpretability of\nneural network behavior. Furthermore, the paper explores the application of\nfederated learning and decentralized training for real-world AI deployments,\nparticularly in edge computing and distributed environments. By simulating\nlocalized training on non-overlapping data subsets, we demonstrate how modular\nnetworks can be efficiently trained and evaluated. The proposed framework also\naddresses scalability, enabling AI systems to handle large datasets and\ndistributed processing while preserving model transparency and\ninterpretability. Finally, we discuss the potential of this approach in\nadvancing the design of scalable, privacy-preserving, and efficient AI systems\nfor diverse applications.\n", "link": "http://arxiv.org/abs/2504.18241v1", "date": "2025-04-25", "relevancy": 2.0077, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5452}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4873}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Switch-Based%20Multi-Part%20Neural%20Network&body=Title%3A%20Switch-Based%20Multi-Part%20Neural%20Network%0AAuthor%3A%20Surajit%20Majumder%20and%20Paritosh%20Ranjan%20and%20Prodip%20Roy%20and%20Bhuban%20Padhan%0AAbstract%3A%20%20%20This%20paper%20introduces%20decentralized%20and%20modular%20neural%20network%20framework%0Adesigned%20to%20enhance%20the%20scalability%2C%20interpretability%2C%20and%20performance%20of%0Aartificial%20intelligence%20%28AI%29%20systems.%20At%20the%20heart%20of%20this%20framework%20is%20a%0Adynamic%20switch%20mechanism%20that%20governs%20the%20selective%20activation%20and%20training%20of%0Aindividual%20neurons%20based%20on%20input%20characteristics%2C%20allowing%20neurons%20to%0Aspecialize%20in%20distinct%20segments%20of%20the%20data%20domain.%20This%20approach%20enables%0Aneurons%20to%20learn%20from%20disjoint%20subsets%20of%20data%2C%20mimicking%20biological%20brain%0Afunction%20by%20promoting%20task%20specialization%20and%20improving%20the%20interpretability%20of%0Aneural%20network%20behavior.%20Furthermore%2C%20the%20paper%20explores%20the%20application%20of%0Afederated%20learning%20and%20decentralized%20training%20for%20real-world%20AI%20deployments%2C%0Aparticularly%20in%20edge%20computing%20and%20distributed%20environments.%20By%20simulating%0Alocalized%20training%20on%20non-overlapping%20data%20subsets%2C%20we%20demonstrate%20how%20modular%0Anetworks%20can%20be%20efficiently%20trained%20and%20evaluated.%20The%20proposed%20framework%20also%0Aaddresses%20scalability%2C%20enabling%20AI%20systems%20to%20handle%20large%20datasets%20and%0Adistributed%20processing%20while%20preserving%20model%20transparency%20and%0Ainterpretability.%20Finally%2C%20we%20discuss%20the%20potential%20of%20this%20approach%20in%0Aadvancing%20the%20design%20of%20scalable%2C%20privacy-preserving%2C%20and%20efficient%20AI%20systems%0Afor%20diverse%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwitch-Based%2520Multi-Part%2520Neural%2520Network%26entry.906535625%3DSurajit%2520Majumder%2520and%2520Paritosh%2520Ranjan%2520and%2520Prodip%2520Roy%2520and%2520Bhuban%2520Padhan%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520decentralized%2520and%2520modular%2520neural%2520network%2520framework%250Adesigned%2520to%2520enhance%2520the%2520scalability%252C%2520interpretability%252C%2520and%2520performance%2520of%250Aartificial%2520intelligence%2520%2528AI%2529%2520systems.%2520At%2520the%2520heart%2520of%2520this%2520framework%2520is%2520a%250Adynamic%2520switch%2520mechanism%2520that%2520governs%2520the%2520selective%2520activation%2520and%2520training%2520of%250Aindividual%2520neurons%2520based%2520on%2520input%2520characteristics%252C%2520allowing%2520neurons%2520to%250Aspecialize%2520in%2520distinct%2520segments%2520of%2520the%2520data%2520domain.%2520This%2520approach%2520enables%250Aneurons%2520to%2520learn%2520from%2520disjoint%2520subsets%2520of%2520data%252C%2520mimicking%2520biological%2520brain%250Afunction%2520by%2520promoting%2520task%2520specialization%2520and%2520improving%2520the%2520interpretability%2520of%250Aneural%2520network%2520behavior.%2520Furthermore%252C%2520the%2520paper%2520explores%2520the%2520application%2520of%250Afederated%2520learning%2520and%2520decentralized%2520training%2520for%2520real-world%2520AI%2520deployments%252C%250Aparticularly%2520in%2520edge%2520computing%2520and%2520distributed%2520environments.%2520By%2520simulating%250Alocalized%2520training%2520on%2520non-overlapping%2520data%2520subsets%252C%2520we%2520demonstrate%2520how%2520modular%250Anetworks%2520can%2520be%2520efficiently%2520trained%2520and%2520evaluated.%2520The%2520proposed%2520framework%2520also%250Aaddresses%2520scalability%252C%2520enabling%2520AI%2520systems%2520to%2520handle%2520large%2520datasets%2520and%250Adistributed%2520processing%2520while%2520preserving%2520model%2520transparency%2520and%250Ainterpretability.%2520Finally%252C%2520we%2520discuss%2520the%2520potential%2520of%2520this%2520approach%2520in%250Aadvancing%2520the%2520design%2520of%2520scalable%252C%2520privacy-preserving%252C%2520and%2520efficient%2520AI%2520systems%250Afor%2520diverse%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Switch-Based%20Multi-Part%20Neural%20Network&entry.906535625=Surajit%20Majumder%20and%20Paritosh%20Ranjan%20and%20Prodip%20Roy%20and%20Bhuban%20Padhan&entry.1292438233=%20%20This%20paper%20introduces%20decentralized%20and%20modular%20neural%20network%20framework%0Adesigned%20to%20enhance%20the%20scalability%2C%20interpretability%2C%20and%20performance%20of%0Aartificial%20intelligence%20%28AI%29%20systems.%20At%20the%20heart%20of%20this%20framework%20is%20a%0Adynamic%20switch%20mechanism%20that%20governs%20the%20selective%20activation%20and%20training%20of%0Aindividual%20neurons%20based%20on%20input%20characteristics%2C%20allowing%20neurons%20to%0Aspecialize%20in%20distinct%20segments%20of%20the%20data%20domain.%20This%20approach%20enables%0Aneurons%20to%20learn%20from%20disjoint%20subsets%20of%20data%2C%20mimicking%20biological%20brain%0Afunction%20by%20promoting%20task%20specialization%20and%20improving%20the%20interpretability%20of%0Aneural%20network%20behavior.%20Furthermore%2C%20the%20paper%20explores%20the%20application%20of%0Afederated%20learning%20and%20decentralized%20training%20for%20real-world%20AI%20deployments%2C%0Aparticularly%20in%20edge%20computing%20and%20distributed%20environments.%20By%20simulating%0Alocalized%20training%20on%20non-overlapping%20data%20subsets%2C%20we%20demonstrate%20how%20modular%0Anetworks%20can%20be%20efficiently%20trained%20and%20evaluated.%20The%20proposed%20framework%20also%0Aaddresses%20scalability%2C%20enabling%20AI%20systems%20to%20handle%20large%20datasets%20and%0Adistributed%20processing%20while%20preserving%20model%20transparency%20and%0Ainterpretability.%20Finally%2C%20we%20discuss%20the%20potential%20of%20this%20approach%20in%0Aadvancing%20the%20design%20of%20scalable%2C%20privacy-preserving%2C%20and%20efficient%20AI%20systems%0Afor%20diverse%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18241v1&entry.124074799=Read"},
{"title": "Enhancing System Self-Awareness and Trust of AI: A Case Study in\n  Trajectory Prediction and Planning", "author": "Lars Ullrich and Zurab Mujirishvili and Knut Graichen", "abstract": "  In the trajectory planning of automated driving, data-driven statistical\nartificial intelligence (AI) methods are increasingly established for\npredicting the emergent behavior of other road users. While these methods\nachieve exceptional performance in defined datasets, they usually rely on the\nindependent and identically distributed (i.i.d.) assumption and thus tend to be\nvulnerable to distribution shifts that occur in the real world. In addition,\nthese methods lack explainability due to their black box nature, which poses\nfurther challenges in terms of the approval process and social trustworthiness.\nTherefore, in order to use the capabilities of data-driven statistical AI\nmethods in a reliable and trustworthy manner, the concept of TrustMHE is\nintroduced and investigated in this paper. TrustMHE represents a complementary\napproach, independent of the underlying AI systems, that combines AI-driven\nout-of-distribution detection with control-driven moving horizon estimation\n(MHE) to enable not only detection and monitoring, but also intervention. The\neffectiveness of the proposed TrustMHE is evaluated and proven in three\nsimulation scenarios.\n", "link": "http://arxiv.org/abs/2504.18421v1", "date": "2025-04-25", "relevancy": 2.0048, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5448}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5018}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20System%20Self-Awareness%20and%20Trust%20of%20AI%3A%20A%20Case%20Study%20in%0A%20%20Trajectory%20Prediction%20and%20Planning&body=Title%3A%20Enhancing%20System%20Self-Awareness%20and%20Trust%20of%20AI%3A%20A%20Case%20Study%20in%0A%20%20Trajectory%20Prediction%20and%20Planning%0AAuthor%3A%20Lars%20Ullrich%20and%20Zurab%20Mujirishvili%20and%20Knut%20Graichen%0AAbstract%3A%20%20%20In%20the%20trajectory%20planning%20of%20automated%20driving%2C%20data-driven%20statistical%0Aartificial%20intelligence%20%28AI%29%20methods%20are%20increasingly%20established%20for%0Apredicting%20the%20emergent%20behavior%20of%20other%20road%20users.%20While%20these%20methods%0Aachieve%20exceptional%20performance%20in%20defined%20datasets%2C%20they%20usually%20rely%20on%20the%0Aindependent%20and%20identically%20distributed%20%28i.i.d.%29%20assumption%20and%20thus%20tend%20to%20be%0Avulnerable%20to%20distribution%20shifts%20that%20occur%20in%20the%20real%20world.%20In%20addition%2C%0Athese%20methods%20lack%20explainability%20due%20to%20their%20black%20box%20nature%2C%20which%20poses%0Afurther%20challenges%20in%20terms%20of%20the%20approval%20process%20and%20social%20trustworthiness.%0ATherefore%2C%20in%20order%20to%20use%20the%20capabilities%20of%20data-driven%20statistical%20AI%0Amethods%20in%20a%20reliable%20and%20trustworthy%20manner%2C%20the%20concept%20of%20TrustMHE%20is%0Aintroduced%20and%20investigated%20in%20this%20paper.%20TrustMHE%20represents%20a%20complementary%0Aapproach%2C%20independent%20of%20the%20underlying%20AI%20systems%2C%20that%20combines%20AI-driven%0Aout-of-distribution%20detection%20with%20control-driven%20moving%20horizon%20estimation%0A%28MHE%29%20to%20enable%20not%20only%20detection%20and%20monitoring%2C%20but%20also%20intervention.%20The%0Aeffectiveness%20of%20the%20proposed%20TrustMHE%20is%20evaluated%20and%20proven%20in%20three%0Asimulation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520System%2520Self-Awareness%2520and%2520Trust%2520of%2520AI%253A%2520A%2520Case%2520Study%2520in%250A%2520%2520Trajectory%2520Prediction%2520and%2520Planning%26entry.906535625%3DLars%2520Ullrich%2520and%2520Zurab%2520Mujirishvili%2520and%2520Knut%2520Graichen%26entry.1292438233%3D%2520%2520In%2520the%2520trajectory%2520planning%2520of%2520automated%2520driving%252C%2520data-driven%2520statistical%250Aartificial%2520intelligence%2520%2528AI%2529%2520methods%2520are%2520increasingly%2520established%2520for%250Apredicting%2520the%2520emergent%2520behavior%2520of%2520other%2520road%2520users.%2520While%2520these%2520methods%250Aachieve%2520exceptional%2520performance%2520in%2520defined%2520datasets%252C%2520they%2520usually%2520rely%2520on%2520the%250Aindependent%2520and%2520identically%2520distributed%2520%2528i.i.d.%2529%2520assumption%2520and%2520thus%2520tend%2520to%2520be%250Avulnerable%2520to%2520distribution%2520shifts%2520that%2520occur%2520in%2520the%2520real%2520world.%2520In%2520addition%252C%250Athese%2520methods%2520lack%2520explainability%2520due%2520to%2520their%2520black%2520box%2520nature%252C%2520which%2520poses%250Afurther%2520challenges%2520in%2520terms%2520of%2520the%2520approval%2520process%2520and%2520social%2520trustworthiness.%250ATherefore%252C%2520in%2520order%2520to%2520use%2520the%2520capabilities%2520of%2520data-driven%2520statistical%2520AI%250Amethods%2520in%2520a%2520reliable%2520and%2520trustworthy%2520manner%252C%2520the%2520concept%2520of%2520TrustMHE%2520is%250Aintroduced%2520and%2520investigated%2520in%2520this%2520paper.%2520TrustMHE%2520represents%2520a%2520complementary%250Aapproach%252C%2520independent%2520of%2520the%2520underlying%2520AI%2520systems%252C%2520that%2520combines%2520AI-driven%250Aout-of-distribution%2520detection%2520with%2520control-driven%2520moving%2520horizon%2520estimation%250A%2528MHE%2529%2520to%2520enable%2520not%2520only%2520detection%2520and%2520monitoring%252C%2520but%2520also%2520intervention.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520TrustMHE%2520is%2520evaluated%2520and%2520proven%2520in%2520three%250Asimulation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20System%20Self-Awareness%20and%20Trust%20of%20AI%3A%20A%20Case%20Study%20in%0A%20%20Trajectory%20Prediction%20and%20Planning&entry.906535625=Lars%20Ullrich%20and%20Zurab%20Mujirishvili%20and%20Knut%20Graichen&entry.1292438233=%20%20In%20the%20trajectory%20planning%20of%20automated%20driving%2C%20data-driven%20statistical%0Aartificial%20intelligence%20%28AI%29%20methods%20are%20increasingly%20established%20for%0Apredicting%20the%20emergent%20behavior%20of%20other%20road%20users.%20While%20these%20methods%0Aachieve%20exceptional%20performance%20in%20defined%20datasets%2C%20they%20usually%20rely%20on%20the%0Aindependent%20and%20identically%20distributed%20%28i.i.d.%29%20assumption%20and%20thus%20tend%20to%20be%0Avulnerable%20to%20distribution%20shifts%20that%20occur%20in%20the%20real%20world.%20In%20addition%2C%0Athese%20methods%20lack%20explainability%20due%20to%20their%20black%20box%20nature%2C%20which%20poses%0Afurther%20challenges%20in%20terms%20of%20the%20approval%20process%20and%20social%20trustworthiness.%0ATherefore%2C%20in%20order%20to%20use%20the%20capabilities%20of%20data-driven%20statistical%20AI%0Amethods%20in%20a%20reliable%20and%20trustworthy%20manner%2C%20the%20concept%20of%20TrustMHE%20is%0Aintroduced%20and%20investigated%20in%20this%20paper.%20TrustMHE%20represents%20a%20complementary%0Aapproach%2C%20independent%20of%20the%20underlying%20AI%20systems%2C%20that%20combines%20AI-driven%0Aout-of-distribution%20detection%20with%20control-driven%20moving%20horizon%20estimation%0A%28MHE%29%20to%20enable%20not%20only%20detection%20and%20monitoring%2C%20but%20also%20intervention.%20The%0Aeffectiveness%20of%20the%20proposed%20TrustMHE%20is%20evaluated%20and%20proven%20in%20three%0Asimulation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18421v1&entry.124074799=Read"},
{"title": "The Moral Mind(s) of Large Language Models", "author": "Avner Seror", "abstract": "  As large language models (LLMs) increasingly participate in tasks with\nethical and societal stakes, a critical question arises: do they exhibit an\nemergent \"moral mind\" - a consistent structure of moral preferences guiding\ntheir decisions - and to what extent is this structure shared across models? To\ninvestigate this, we applied tools from revealed preference theory to nearly 40\nleading LLMs, presenting each with many structured moral dilemmas spanning five\nfoundational dimensions of ethical reasoning. Using a probabilistic rationality\ntest, we found that at least one model from each major provider exhibited\nbehavior consistent with approximately stable moral preferences, acting as if\nguided by an underlying utility function. We then estimated these utility\nfunctions and found that most models cluster around neutral moral stances. To\nfurther characterize heterogeneity, we employed a non-parametric permutation\napproach, constructing a probabilistic similarity network based on revealed\npreference patterns. The results reveal a shared core in LLMs' moral reasoning,\nbut also meaningful variation: some models show flexible reasoning across\nperspectives, while others adhere to more rigid ethical profiles. These\nfindings provide a new empirical lens for evaluating moral consistency in LLMs\nand offer a framework for benchmarking ethical alignment across AI systems.\n", "link": "http://arxiv.org/abs/2412.04476v3", "date": "2025-04-25", "relevancy": 1.9716, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4966}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Moral%20Mind%28s%29%20of%20Large%20Language%20Models&body=Title%3A%20The%20Moral%20Mind%28s%29%20of%20Large%20Language%20Models%0AAuthor%3A%20Avner%20Seror%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20increasingly%20participate%20in%20tasks%20with%0Aethical%20and%20societal%20stakes%2C%20a%20critical%20question%20arises%3A%20do%20they%20exhibit%20an%0Aemergent%20%22moral%20mind%22%20-%20a%20consistent%20structure%20of%20moral%20preferences%20guiding%0Atheir%20decisions%20-%20and%20to%20what%20extent%20is%20this%20structure%20shared%20across%20models%3F%20To%0Ainvestigate%20this%2C%20we%20applied%20tools%20from%20revealed%20preference%20theory%20to%20nearly%2040%0Aleading%20LLMs%2C%20presenting%20each%20with%20many%20structured%20moral%20dilemmas%20spanning%20five%0Afoundational%20dimensions%20of%20ethical%20reasoning.%20Using%20a%20probabilistic%20rationality%0Atest%2C%20we%20found%20that%20at%20least%20one%20model%20from%20each%20major%20provider%20exhibited%0Abehavior%20consistent%20with%20approximately%20stable%20moral%20preferences%2C%20acting%20as%20if%0Aguided%20by%20an%20underlying%20utility%20function.%20We%20then%20estimated%20these%20utility%0Afunctions%20and%20found%20that%20most%20models%20cluster%20around%20neutral%20moral%20stances.%20To%0Afurther%20characterize%20heterogeneity%2C%20we%20employed%20a%20non-parametric%20permutation%0Aapproach%2C%20constructing%20a%20probabilistic%20similarity%20network%20based%20on%20revealed%0Apreference%20patterns.%20The%20results%20reveal%20a%20shared%20core%20in%20LLMs%27%20moral%20reasoning%2C%0Abut%20also%20meaningful%20variation%3A%20some%20models%20show%20flexible%20reasoning%20across%0Aperspectives%2C%20while%20others%20adhere%20to%20more%20rigid%20ethical%20profiles.%20These%0Afindings%20provide%20a%20new%20empirical%20lens%20for%20evaluating%20moral%20consistency%20in%20LLMs%0Aand%20offer%20a%20framework%20for%20benchmarking%20ethical%20alignment%20across%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04476v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Moral%2520Mind%2528s%2529%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DAvner%2520Seror%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520participate%2520in%2520tasks%2520with%250Aethical%2520and%2520societal%2520stakes%252C%2520a%2520critical%2520question%2520arises%253A%2520do%2520they%2520exhibit%2520an%250Aemergent%2520%2522moral%2520mind%2522%2520-%2520a%2520consistent%2520structure%2520of%2520moral%2520preferences%2520guiding%250Atheir%2520decisions%2520-%2520and%2520to%2520what%2520extent%2520is%2520this%2520structure%2520shared%2520across%2520models%253F%2520To%250Ainvestigate%2520this%252C%2520we%2520applied%2520tools%2520from%2520revealed%2520preference%2520theory%2520to%2520nearly%252040%250Aleading%2520LLMs%252C%2520presenting%2520each%2520with%2520many%2520structured%2520moral%2520dilemmas%2520spanning%2520five%250Afoundational%2520dimensions%2520of%2520ethical%2520reasoning.%2520Using%2520a%2520probabilistic%2520rationality%250Atest%252C%2520we%2520found%2520that%2520at%2520least%2520one%2520model%2520from%2520each%2520major%2520provider%2520exhibited%250Abehavior%2520consistent%2520with%2520approximately%2520stable%2520moral%2520preferences%252C%2520acting%2520as%2520if%250Aguided%2520by%2520an%2520underlying%2520utility%2520function.%2520We%2520then%2520estimated%2520these%2520utility%250Afunctions%2520and%2520found%2520that%2520most%2520models%2520cluster%2520around%2520neutral%2520moral%2520stances.%2520To%250Afurther%2520characterize%2520heterogeneity%252C%2520we%2520employed%2520a%2520non-parametric%2520permutation%250Aapproach%252C%2520constructing%2520a%2520probabilistic%2520similarity%2520network%2520based%2520on%2520revealed%250Apreference%2520patterns.%2520The%2520results%2520reveal%2520a%2520shared%2520core%2520in%2520LLMs%2527%2520moral%2520reasoning%252C%250Abut%2520also%2520meaningful%2520variation%253A%2520some%2520models%2520show%2520flexible%2520reasoning%2520across%250Aperspectives%252C%2520while%2520others%2520adhere%2520to%2520more%2520rigid%2520ethical%2520profiles.%2520These%250Afindings%2520provide%2520a%2520new%2520empirical%2520lens%2520for%2520evaluating%2520moral%2520consistency%2520in%2520LLMs%250Aand%2520offer%2520a%2520framework%2520for%2520benchmarking%2520ethical%2520alignment%2520across%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04476v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Moral%20Mind%28s%29%20of%20Large%20Language%20Models&entry.906535625=Avner%20Seror&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20increasingly%20participate%20in%20tasks%20with%0Aethical%20and%20societal%20stakes%2C%20a%20critical%20question%20arises%3A%20do%20they%20exhibit%20an%0Aemergent%20%22moral%20mind%22%20-%20a%20consistent%20structure%20of%20moral%20preferences%20guiding%0Atheir%20decisions%20-%20and%20to%20what%20extent%20is%20this%20structure%20shared%20across%20models%3F%20To%0Ainvestigate%20this%2C%20we%20applied%20tools%20from%20revealed%20preference%20theory%20to%20nearly%2040%0Aleading%20LLMs%2C%20presenting%20each%20with%20many%20structured%20moral%20dilemmas%20spanning%20five%0Afoundational%20dimensions%20of%20ethical%20reasoning.%20Using%20a%20probabilistic%20rationality%0Atest%2C%20we%20found%20that%20at%20least%20one%20model%20from%20each%20major%20provider%20exhibited%0Abehavior%20consistent%20with%20approximately%20stable%20moral%20preferences%2C%20acting%20as%20if%0Aguided%20by%20an%20underlying%20utility%20function.%20We%20then%20estimated%20these%20utility%0Afunctions%20and%20found%20that%20most%20models%20cluster%20around%20neutral%20moral%20stances.%20To%0Afurther%20characterize%20heterogeneity%2C%20we%20employed%20a%20non-parametric%20permutation%0Aapproach%2C%20constructing%20a%20probabilistic%20similarity%20network%20based%20on%20revealed%0Apreference%20patterns.%20The%20results%20reveal%20a%20shared%20core%20in%20LLMs%27%20moral%20reasoning%2C%0Abut%20also%20meaningful%20variation%3A%20some%20models%20show%20flexible%20reasoning%20across%0Aperspectives%2C%20while%20others%20adhere%20to%20more%20rigid%20ethical%20profiles.%20These%0Afindings%20provide%20a%20new%20empirical%20lens%20for%20evaluating%20moral%20consistency%20in%20LLMs%0Aand%20offer%20a%20framework%20for%20benchmarking%20ethical%20alignment%20across%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04476v3&entry.124074799=Read"},
{"title": "Towards a deep learning approach for classifying treatment response in\n  glioblastomas", "author": "Ana Matoso and Catarina Passarinho and Marta P. Loureiro and Jos\u00e9 Maria Moreira and Patr\u00edcia Figueiredo and Rita G. Nunes", "abstract": "  Glioblastomas are the most aggressive type of glioma, having a 5-year\nsurvival rate of 6.9%. Treatment typically involves surgery, followed by\nradiotherapy and chemotherapy, and frequent magnetic resonance imaging (MRI)\nscans to monitor disease progression. To assess treatment response,\nradiologists use the Response Assessment in Neuro-Oncology (RANO) criteria to\ncategorize the tumor into one of four labels based on imaging and clinical\nfeatures: complete response, partial response, stable disease, and progressive\ndisease. This assessment is very complex and time-consuming. Since deep\nlearning (DL) has been widely used to tackle classification problems, this work\naimed to implement the first DL pipeline for the classification of RANO\ncriteria based on two consecutive MRI acquisitions. The models were trained and\ntested on the open dataset LUMIERE. Five approaches were tested: 1) subtraction\nof input images, 2) different combinations of modalities, 3) different model\narchitectures, 4) different pretraining tasks, and 5) adding clinical data. The\npipeline that achieved the best performance used a Densenet264 considering only\nT1-weighted, T2-weighted, and Fluid Attenuated Inversion Recovery (FLAIR)\nimages as input without any pretraining. A median Balanced Accuracy of 50.96%\nwas achieved. Additionally, explainability methods were applied. Using Saliency\nMaps, the tumor region was often successfully highlighted. In contrast,\nGrad-CAM typically failed to highlight the tumor region, with some exceptions\nobserved in the Complete Response and Progressive Disease classes, where it\neffectively identified the tumor region. These results set a benchmark for\nfuture studies on glioblastoma treatment response assessment based on the RANO\ncriteria while emphasizing the heterogeneity of factors that might play a role\nwhen assessing the tumor's response to treatment.\n", "link": "http://arxiv.org/abs/2504.18268v1", "date": "2025-04-25", "relevancy": 1.9617, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4921}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4916}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20deep%20learning%20approach%20for%20classifying%20treatment%20response%20in%0A%20%20glioblastomas&body=Title%3A%20Towards%20a%20deep%20learning%20approach%20for%20classifying%20treatment%20response%20in%0A%20%20glioblastomas%0AAuthor%3A%20Ana%20Matoso%20and%20Catarina%20Passarinho%20and%20Marta%20P.%20Loureiro%20and%20Jos%C3%A9%20Maria%20Moreira%20and%20Patr%C3%ADcia%20Figueiredo%20and%20Rita%20G.%20Nunes%0AAbstract%3A%20%20%20Glioblastomas%20are%20the%20most%20aggressive%20type%20of%20glioma%2C%20having%20a%205-year%0Asurvival%20rate%20of%206.9%25.%20Treatment%20typically%20involves%20surgery%2C%20followed%20by%0Aradiotherapy%20and%20chemotherapy%2C%20and%20frequent%20magnetic%20resonance%20imaging%20%28MRI%29%0Ascans%20to%20monitor%20disease%20progression.%20To%20assess%20treatment%20response%2C%0Aradiologists%20use%20the%20Response%20Assessment%20in%20Neuro-Oncology%20%28RANO%29%20criteria%20to%0Acategorize%20the%20tumor%20into%20one%20of%20four%20labels%20based%20on%20imaging%20and%20clinical%0Afeatures%3A%20complete%20response%2C%20partial%20response%2C%20stable%20disease%2C%20and%20progressive%0Adisease.%20This%20assessment%20is%20very%20complex%20and%20time-consuming.%20Since%20deep%0Alearning%20%28DL%29%20has%20been%20widely%20used%20to%20tackle%20classification%20problems%2C%20this%20work%0Aaimed%20to%20implement%20the%20first%20DL%20pipeline%20for%20the%20classification%20of%20RANO%0Acriteria%20based%20on%20two%20consecutive%20MRI%20acquisitions.%20The%20models%20were%20trained%20and%0Atested%20on%20the%20open%20dataset%20LUMIERE.%20Five%20approaches%20were%20tested%3A%201%29%20subtraction%0Aof%20input%20images%2C%202%29%20different%20combinations%20of%20modalities%2C%203%29%20different%20model%0Aarchitectures%2C%204%29%20different%20pretraining%20tasks%2C%20and%205%29%20adding%20clinical%20data.%20The%0Apipeline%20that%20achieved%20the%20best%20performance%20used%20a%20Densenet264%20considering%20only%0AT1-weighted%2C%20T2-weighted%2C%20and%20Fluid%20Attenuated%20Inversion%20Recovery%20%28FLAIR%29%0Aimages%20as%20input%20without%20any%20pretraining.%20A%20median%20Balanced%20Accuracy%20of%2050.96%25%0Awas%20achieved.%20Additionally%2C%20explainability%20methods%20were%20applied.%20Using%20Saliency%0AMaps%2C%20the%20tumor%20region%20was%20often%20successfully%20highlighted.%20In%20contrast%2C%0AGrad-CAM%20typically%20failed%20to%20highlight%20the%20tumor%20region%2C%20with%20some%20exceptions%0Aobserved%20in%20the%20Complete%20Response%20and%20Progressive%20Disease%20classes%2C%20where%20it%0Aeffectively%20identified%20the%20tumor%20region.%20These%20results%20set%20a%20benchmark%20for%0Afuture%20studies%20on%20glioblastoma%20treatment%20response%20assessment%20based%20on%20the%20RANO%0Acriteria%20while%20emphasizing%20the%20heterogeneity%20of%20factors%20that%20might%20play%20a%20role%0Awhen%20assessing%20the%20tumor%27s%20response%20to%20treatment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520deep%2520learning%2520approach%2520for%2520classifying%2520treatment%2520response%2520in%250A%2520%2520glioblastomas%26entry.906535625%3DAna%2520Matoso%2520and%2520Catarina%2520Passarinho%2520and%2520Marta%2520P.%2520Loureiro%2520and%2520Jos%25C3%25A9%2520Maria%2520Moreira%2520and%2520Patr%25C3%25ADcia%2520Figueiredo%2520and%2520Rita%2520G.%2520Nunes%26entry.1292438233%3D%2520%2520Glioblastomas%2520are%2520the%2520most%2520aggressive%2520type%2520of%2520glioma%252C%2520having%2520a%25205-year%250Asurvival%2520rate%2520of%25206.9%2525.%2520Treatment%2520typically%2520involves%2520surgery%252C%2520followed%2520by%250Aradiotherapy%2520and%2520chemotherapy%252C%2520and%2520frequent%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%250Ascans%2520to%2520monitor%2520disease%2520progression.%2520To%2520assess%2520treatment%2520response%252C%250Aradiologists%2520use%2520the%2520Response%2520Assessment%2520in%2520Neuro-Oncology%2520%2528RANO%2529%2520criteria%2520to%250Acategorize%2520the%2520tumor%2520into%2520one%2520of%2520four%2520labels%2520based%2520on%2520imaging%2520and%2520clinical%250Afeatures%253A%2520complete%2520response%252C%2520partial%2520response%252C%2520stable%2520disease%252C%2520and%2520progressive%250Adisease.%2520This%2520assessment%2520is%2520very%2520complex%2520and%2520time-consuming.%2520Since%2520deep%250Alearning%2520%2528DL%2529%2520has%2520been%2520widely%2520used%2520to%2520tackle%2520classification%2520problems%252C%2520this%2520work%250Aaimed%2520to%2520implement%2520the%2520first%2520DL%2520pipeline%2520for%2520the%2520classification%2520of%2520RANO%250Acriteria%2520based%2520on%2520two%2520consecutive%2520MRI%2520acquisitions.%2520The%2520models%2520were%2520trained%2520and%250Atested%2520on%2520the%2520open%2520dataset%2520LUMIERE.%2520Five%2520approaches%2520were%2520tested%253A%25201%2529%2520subtraction%250Aof%2520input%2520images%252C%25202%2529%2520different%2520combinations%2520of%2520modalities%252C%25203%2529%2520different%2520model%250Aarchitectures%252C%25204%2529%2520different%2520pretraining%2520tasks%252C%2520and%25205%2529%2520adding%2520clinical%2520data.%2520The%250Apipeline%2520that%2520achieved%2520the%2520best%2520performance%2520used%2520a%2520Densenet264%2520considering%2520only%250AT1-weighted%252C%2520T2-weighted%252C%2520and%2520Fluid%2520Attenuated%2520Inversion%2520Recovery%2520%2528FLAIR%2529%250Aimages%2520as%2520input%2520without%2520any%2520pretraining.%2520A%2520median%2520Balanced%2520Accuracy%2520of%252050.96%2525%250Awas%2520achieved.%2520Additionally%252C%2520explainability%2520methods%2520were%2520applied.%2520Using%2520Saliency%250AMaps%252C%2520the%2520tumor%2520region%2520was%2520often%2520successfully%2520highlighted.%2520In%2520contrast%252C%250AGrad-CAM%2520typically%2520failed%2520to%2520highlight%2520the%2520tumor%2520region%252C%2520with%2520some%2520exceptions%250Aobserved%2520in%2520the%2520Complete%2520Response%2520and%2520Progressive%2520Disease%2520classes%252C%2520where%2520it%250Aeffectively%2520identified%2520the%2520tumor%2520region.%2520These%2520results%2520set%2520a%2520benchmark%2520for%250Afuture%2520studies%2520on%2520glioblastoma%2520treatment%2520response%2520assessment%2520based%2520on%2520the%2520RANO%250Acriteria%2520while%2520emphasizing%2520the%2520heterogeneity%2520of%2520factors%2520that%2520might%2520play%2520a%2520role%250Awhen%2520assessing%2520the%2520tumor%2527s%2520response%2520to%2520treatment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20deep%20learning%20approach%20for%20classifying%20treatment%20response%20in%0A%20%20glioblastomas&entry.906535625=Ana%20Matoso%20and%20Catarina%20Passarinho%20and%20Marta%20P.%20Loureiro%20and%20Jos%C3%A9%20Maria%20Moreira%20and%20Patr%C3%ADcia%20Figueiredo%20and%20Rita%20G.%20Nunes&entry.1292438233=%20%20Glioblastomas%20are%20the%20most%20aggressive%20type%20of%20glioma%2C%20having%20a%205-year%0Asurvival%20rate%20of%206.9%25.%20Treatment%20typically%20involves%20surgery%2C%20followed%20by%0Aradiotherapy%20and%20chemotherapy%2C%20and%20frequent%20magnetic%20resonance%20imaging%20%28MRI%29%0Ascans%20to%20monitor%20disease%20progression.%20To%20assess%20treatment%20response%2C%0Aradiologists%20use%20the%20Response%20Assessment%20in%20Neuro-Oncology%20%28RANO%29%20criteria%20to%0Acategorize%20the%20tumor%20into%20one%20of%20four%20labels%20based%20on%20imaging%20and%20clinical%0Afeatures%3A%20complete%20response%2C%20partial%20response%2C%20stable%20disease%2C%20and%20progressive%0Adisease.%20This%20assessment%20is%20very%20complex%20and%20time-consuming.%20Since%20deep%0Alearning%20%28DL%29%20has%20been%20widely%20used%20to%20tackle%20classification%20problems%2C%20this%20work%0Aaimed%20to%20implement%20the%20first%20DL%20pipeline%20for%20the%20classification%20of%20RANO%0Acriteria%20based%20on%20two%20consecutive%20MRI%20acquisitions.%20The%20models%20were%20trained%20and%0Atested%20on%20the%20open%20dataset%20LUMIERE.%20Five%20approaches%20were%20tested%3A%201%29%20subtraction%0Aof%20input%20images%2C%202%29%20different%20combinations%20of%20modalities%2C%203%29%20different%20model%0Aarchitectures%2C%204%29%20different%20pretraining%20tasks%2C%20and%205%29%20adding%20clinical%20data.%20The%0Apipeline%20that%20achieved%20the%20best%20performance%20used%20a%20Densenet264%20considering%20only%0AT1-weighted%2C%20T2-weighted%2C%20and%20Fluid%20Attenuated%20Inversion%20Recovery%20%28FLAIR%29%0Aimages%20as%20input%20without%20any%20pretraining.%20A%20median%20Balanced%20Accuracy%20of%2050.96%25%0Awas%20achieved.%20Additionally%2C%20explainability%20methods%20were%20applied.%20Using%20Saliency%0AMaps%2C%20the%20tumor%20region%20was%20often%20successfully%20highlighted.%20In%20contrast%2C%0AGrad-CAM%20typically%20failed%20to%20highlight%20the%20tumor%20region%2C%20with%20some%20exceptions%0Aobserved%20in%20the%20Complete%20Response%20and%20Progressive%20Disease%20classes%2C%20where%20it%0Aeffectively%20identified%20the%20tumor%20region.%20These%20results%20set%20a%20benchmark%20for%0Afuture%20studies%20on%20glioblastoma%20treatment%20response%20assessment%20based%20on%20the%20RANO%0Acriteria%20while%20emphasizing%20the%20heterogeneity%20of%20factors%20that%20might%20play%20a%20role%0Awhen%20assessing%20the%20tumor%27s%20response%20to%20treatment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18268v1&entry.124074799=Read"},
{"title": "An Axiomatic Assessment of Entropy- and Variance-based Uncertainty\n  Quantification in Regression", "author": "Christopher B\u00fclte and Yusuf Sale and Timo L\u00f6hr and Paul Hofman and Gitta Kutyniok and Eyke H\u00fcllermeier", "abstract": "  Uncertainty quantification (UQ) is crucial in machine learning, yet most\n(axiomatic) studies of uncertainty measures focus on classification, leaving a\ngap in regression settings with limited formal justification and evaluations.\nIn this work, we introduce a set of axioms to rigorously assess measures of\naleatoric, epistemic, and total uncertainty in supervised regression. By\nutilizing a predictive exponential family, we can generalize commonly used\napproaches for uncertainty representation and corresponding uncertainty\nmeasures. More specifically, we analyze the widely used entropy- and\nvariance-based measures regarding limitations and challenges. Our findings\nprovide a principled foundation for UQ in regression, offering theoretical\ninsights and practical guidelines for reliable uncertainty assessment.\n", "link": "http://arxiv.org/abs/2504.18433v1", "date": "2025-04-25", "relevancy": 1.9549, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5031}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4966}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Axiomatic%20Assessment%20of%20Entropy-%20and%20Variance-based%20Uncertainty%0A%20%20Quantification%20in%20Regression&body=Title%3A%20An%20Axiomatic%20Assessment%20of%20Entropy-%20and%20Variance-based%20Uncertainty%0A%20%20Quantification%20in%20Regression%0AAuthor%3A%20Christopher%20B%C3%BClte%20and%20Yusuf%20Sale%20and%20Timo%20L%C3%B6hr%20and%20Paul%20Hofman%20and%20Gitta%20Kutyniok%20and%20Eyke%20H%C3%BCllermeier%0AAbstract%3A%20%20%20Uncertainty%20quantification%20%28UQ%29%20is%20crucial%20in%20machine%20learning%2C%20yet%20most%0A%28axiomatic%29%20studies%20of%20uncertainty%20measures%20focus%20on%20classification%2C%20leaving%20a%0Agap%20in%20regression%20settings%20with%20limited%20formal%20justification%20and%20evaluations.%0AIn%20this%20work%2C%20we%20introduce%20a%20set%20of%20axioms%20to%20rigorously%20assess%20measures%20of%0Aaleatoric%2C%20epistemic%2C%20and%20total%20uncertainty%20in%20supervised%20regression.%20By%0Autilizing%20a%20predictive%20exponential%20family%2C%20we%20can%20generalize%20commonly%20used%0Aapproaches%20for%20uncertainty%20representation%20and%20corresponding%20uncertainty%0Ameasures.%20More%20specifically%2C%20we%20analyze%20the%20widely%20used%20entropy-%20and%0Avariance-based%20measures%20regarding%20limitations%20and%20challenges.%20Our%20findings%0Aprovide%20a%20principled%20foundation%20for%20UQ%20in%20regression%2C%20offering%20theoretical%0Ainsights%20and%20practical%20guidelines%20for%20reliable%20uncertainty%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Axiomatic%2520Assessment%2520of%2520Entropy-%2520and%2520Variance-based%2520Uncertainty%250A%2520%2520Quantification%2520in%2520Regression%26entry.906535625%3DChristopher%2520B%25C3%25BClte%2520and%2520Yusuf%2520Sale%2520and%2520Timo%2520L%25C3%25B6hr%2520and%2520Paul%2520Hofman%2520and%2520Gitta%2520Kutyniok%2520and%2520Eyke%2520H%25C3%25BCllermeier%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520%2528UQ%2529%2520is%2520crucial%2520in%2520machine%2520learning%252C%2520yet%2520most%250A%2528axiomatic%2529%2520studies%2520of%2520uncertainty%2520measures%2520focus%2520on%2520classification%252C%2520leaving%2520a%250Agap%2520in%2520regression%2520settings%2520with%2520limited%2520formal%2520justification%2520and%2520evaluations.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520a%2520set%2520of%2520axioms%2520to%2520rigorously%2520assess%2520measures%2520of%250Aaleatoric%252C%2520epistemic%252C%2520and%2520total%2520uncertainty%2520in%2520supervised%2520regression.%2520By%250Autilizing%2520a%2520predictive%2520exponential%2520family%252C%2520we%2520can%2520generalize%2520commonly%2520used%250Aapproaches%2520for%2520uncertainty%2520representation%2520and%2520corresponding%2520uncertainty%250Ameasures.%2520More%2520specifically%252C%2520we%2520analyze%2520the%2520widely%2520used%2520entropy-%2520and%250Avariance-based%2520measures%2520regarding%2520limitations%2520and%2520challenges.%2520Our%2520findings%250Aprovide%2520a%2520principled%2520foundation%2520for%2520UQ%2520in%2520regression%252C%2520offering%2520theoretical%250Ainsights%2520and%2520practical%2520guidelines%2520for%2520reliable%2520uncertainty%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Axiomatic%20Assessment%20of%20Entropy-%20and%20Variance-based%20Uncertainty%0A%20%20Quantification%20in%20Regression&entry.906535625=Christopher%20B%C3%BClte%20and%20Yusuf%20Sale%20and%20Timo%20L%C3%B6hr%20and%20Paul%20Hofman%20and%20Gitta%20Kutyniok%20and%20Eyke%20H%C3%BCllermeier&entry.1292438233=%20%20Uncertainty%20quantification%20%28UQ%29%20is%20crucial%20in%20machine%20learning%2C%20yet%20most%0A%28axiomatic%29%20studies%20of%20uncertainty%20measures%20focus%20on%20classification%2C%20leaving%20a%0Agap%20in%20regression%20settings%20with%20limited%20formal%20justification%20and%20evaluations.%0AIn%20this%20work%2C%20we%20introduce%20a%20set%20of%20axioms%20to%20rigorously%20assess%20measures%20of%0Aaleatoric%2C%20epistemic%2C%20and%20total%20uncertainty%20in%20supervised%20regression.%20By%0Autilizing%20a%20predictive%20exponential%20family%2C%20we%20can%20generalize%20commonly%20used%0Aapproaches%20for%20uncertainty%20representation%20and%20corresponding%20uncertainty%0Ameasures.%20More%20specifically%2C%20we%20analyze%20the%20widely%20used%20entropy-%20and%0Avariance-based%20measures%20regarding%20limitations%20and%20challenges.%20Our%20findings%0Aprovide%20a%20principled%20foundation%20for%20UQ%20in%20regression%2C%20offering%20theoretical%0Ainsights%20and%20practical%20guidelines%20for%20reliable%20uncertainty%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18433v1&entry.124074799=Read"},
{"title": "Pushing the boundary on Natural Language Inference", "author": "Pablo Miralles-Gonz\u00e1lez and Javier Huertas-Tato and Alejandro Mart\u00edn and David Camacho", "abstract": "  Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality.\n", "link": "http://arxiv.org/abs/2504.18376v1", "date": "2025-04-25", "relevancy": 1.953, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pushing%20the%20boundary%20on%20Natural%20Language%20Inference&body=Title%3A%20Pushing%20the%20boundary%20on%20Natural%20Language%20Inference%0AAuthor%3A%20Pablo%20Miralles-Gonz%C3%A1lez%20and%20Javier%20Huertas-Tato%20and%20Alejandro%20Mart%C3%ADn%20and%20David%20Camacho%0AAbstract%3A%20%20%20Natural%20Language%20Inference%20%28NLI%29%20is%20a%20central%20task%20in%20natural%20language%0Aunderstanding%20with%20applications%20in%20fact-checking%2C%20question%20answering%2C%20and%0Ainformation%20retrieval.%20Despite%20its%20importance%2C%20current%20NLI%20systems%20heavily%20rely%0Aon%20supervised%20learning%20with%20datasets%20that%20often%20contain%20annotation%20artifacts%0Aand%20biases%2C%20limiting%20generalization%20and%20real-world%20applicability.%20In%20this%20work%2C%0Awe%20apply%20a%20reinforcement%20learning-based%20approach%20using%20Group%20Relative%20Policy%0AOptimization%20%28GRPO%29%20for%20Chain-of-Thought%20%28CoT%29%20learning%20in%20NLI%2C%20eliminating%20the%0Aneed%20for%20labeled%20rationales%20and%20enabling%20this%20type%20of%20training%20on%20more%0Achallenging%20datasets%20such%20as%20ANLI.%20We%20fine-tune%207B%2C%2014B%2C%20and%2032B%20language%0Amodels%20using%20parameter-efficient%20techniques%20%28LoRA%20and%20QLoRA%29%2C%20demonstrating%0Astrong%20performance%20across%20standard%20and%20adversarial%20NLI%20benchmarks.%20Our%2032B%0AAWQ-quantized%20model%20surpasses%20state-of-the-art%20results%20on%207%20out%20of%2011%0Aadversarial%20sets%24%5Cunicode%7Bx2013%7D%24or%20on%20all%20of%20them%20considering%20our%0Areplication%24%5Cunicode%7Bx2013%7D%24within%20a%2022GB%20memory%20footprint%2C%20showing%20that%20robust%0Areasoning%20can%20be%20retained%20under%20aggressive%20quantization.%20This%20work%20provides%20a%0Ascalable%20and%20practical%20framework%20for%20building%20robust%20NLI%20systems%20without%0Asacrificing%20inference%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPushing%2520the%2520boundary%2520on%2520Natural%2520Language%2520Inference%26entry.906535625%3DPablo%2520Miralles-Gonz%25C3%25A1lez%2520and%2520Javier%2520Huertas-Tato%2520and%2520Alejandro%2520Mart%25C3%25ADn%2520and%2520David%2520Camacho%26entry.1292438233%3D%2520%2520Natural%2520Language%2520Inference%2520%2528NLI%2529%2520is%2520a%2520central%2520task%2520in%2520natural%2520language%250Aunderstanding%2520with%2520applications%2520in%2520fact-checking%252C%2520question%2520answering%252C%2520and%250Ainformation%2520retrieval.%2520Despite%2520its%2520importance%252C%2520current%2520NLI%2520systems%2520heavily%2520rely%250Aon%2520supervised%2520learning%2520with%2520datasets%2520that%2520often%2520contain%2520annotation%2520artifacts%250Aand%2520biases%252C%2520limiting%2520generalization%2520and%2520real-world%2520applicability.%2520In%2520this%2520work%252C%250Awe%2520apply%2520a%2520reinforcement%2520learning-based%2520approach%2520using%2520Group%2520Relative%2520Policy%250AOptimization%2520%2528GRPO%2529%2520for%2520Chain-of-Thought%2520%2528CoT%2529%2520learning%2520in%2520NLI%252C%2520eliminating%2520the%250Aneed%2520for%2520labeled%2520rationales%2520and%2520enabling%2520this%2520type%2520of%2520training%2520on%2520more%250Achallenging%2520datasets%2520such%2520as%2520ANLI.%2520We%2520fine-tune%25207B%252C%252014B%252C%2520and%252032B%2520language%250Amodels%2520using%2520parameter-efficient%2520techniques%2520%2528LoRA%2520and%2520QLoRA%2529%252C%2520demonstrating%250Astrong%2520performance%2520across%2520standard%2520and%2520adversarial%2520NLI%2520benchmarks.%2520Our%252032B%250AAWQ-quantized%2520model%2520surpasses%2520state-of-the-art%2520results%2520on%25207%2520out%2520of%252011%250Aadversarial%2520sets%2524%255Cunicode%257Bx2013%257D%2524or%2520on%2520all%2520of%2520them%2520considering%2520our%250Areplication%2524%255Cunicode%257Bx2013%257D%2524within%2520a%252022GB%2520memory%2520footprint%252C%2520showing%2520that%2520robust%250Areasoning%2520can%2520be%2520retained%2520under%2520aggressive%2520quantization.%2520This%2520work%2520provides%2520a%250Ascalable%2520and%2520practical%2520framework%2520for%2520building%2520robust%2520NLI%2520systems%2520without%250Asacrificing%2520inference%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pushing%20the%20boundary%20on%20Natural%20Language%20Inference&entry.906535625=Pablo%20Miralles-Gonz%C3%A1lez%20and%20Javier%20Huertas-Tato%20and%20Alejandro%20Mart%C3%ADn%20and%20David%20Camacho&entry.1292438233=%20%20Natural%20Language%20Inference%20%28NLI%29%20is%20a%20central%20task%20in%20natural%20language%0Aunderstanding%20with%20applications%20in%20fact-checking%2C%20question%20answering%2C%20and%0Ainformation%20retrieval.%20Despite%20its%20importance%2C%20current%20NLI%20systems%20heavily%20rely%0Aon%20supervised%20learning%20with%20datasets%20that%20often%20contain%20annotation%20artifacts%0Aand%20biases%2C%20limiting%20generalization%20and%20real-world%20applicability.%20In%20this%20work%2C%0Awe%20apply%20a%20reinforcement%20learning-based%20approach%20using%20Group%20Relative%20Policy%0AOptimization%20%28GRPO%29%20for%20Chain-of-Thought%20%28CoT%29%20learning%20in%20NLI%2C%20eliminating%20the%0Aneed%20for%20labeled%20rationales%20and%20enabling%20this%20type%20of%20training%20on%20more%0Achallenging%20datasets%20such%20as%20ANLI.%20We%20fine-tune%207B%2C%2014B%2C%20and%2032B%20language%0Amodels%20using%20parameter-efficient%20techniques%20%28LoRA%20and%20QLoRA%29%2C%20demonstrating%0Astrong%20performance%20across%20standard%20and%20adversarial%20NLI%20benchmarks.%20Our%2032B%0AAWQ-quantized%20model%20surpasses%20state-of-the-art%20results%20on%207%20out%20of%2011%0Aadversarial%20sets%24%5Cunicode%7Bx2013%7D%24or%20on%20all%20of%20them%20considering%20our%0Areplication%24%5Cunicode%7Bx2013%7D%24within%20a%2022GB%20memory%20footprint%2C%20showing%20that%20robust%0Areasoning%20can%20be%20retained%20under%20aggressive%20quantization.%20This%20work%20provides%20a%0Ascalable%20and%20practical%20framework%20for%20building%20robust%20NLI%20systems%20without%0Asacrificing%20inference%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18376v1&entry.124074799=Read"},
{"title": "NoiseController: Towards Consistent Multi-view Video Generation via\n  Noise Decomposition and Collaboration", "author": "Haotian Dong and Xin Wang and Di Lin and Yipeng Wu and Qin Chen and Ruonan Liu and Kairui Yang and Ping Li and Qing Guo", "abstract": "  High-quality video generation is crucial for many fields, including the film\nindustry and autonomous driving. However, generating videos with spatiotemporal\nconsistencies remains challenging. Current methods typically utilize attention\nmechanisms or modify noise to achieve consistent videos, neglecting global\nspatiotemporal information that could help ensure spatial and temporal\nconsistency during video generation. In this paper, we propose the\nNoiseController, consisting of Multi-Level Noise Decomposition, Multi-Frame\nNoise Collaboration, and Joint Denoising, to enhance spatiotemporal\nconsistencies in video generation. In multi-level noise decomposition, we first\ndecompose initial noises into scene-level foreground/background noises,\ncapturing distinct motion properties to model multi-view foreground/background\nvariations. Furthermore, each scene-level noise is further decomposed into\nindividual-level shared and residual components. The shared noise preserves\nconsistency, while the residual component maintains diversity. In multi-frame\nnoise collaboration, we introduce an inter-view spatiotemporal collaboration\nmatrix and an intra-view impact collaboration matrix , which captures mutual\ncross-view effects and historical cross-frame impacts to enhance video quality.\nThe joint denoising contains two parallel denoising U-Nets to remove each\nscene-level noise, mutually enhancing video generation. We evaluate our\nNoiseController on public datasets focusing on video generation and downstream\ntasks, demonstrating its state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2504.18448v1", "date": "2025-04-25", "relevancy": 1.9409, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6742}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6335}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoiseController%3A%20Towards%20Consistent%20Multi-view%20Video%20Generation%20via%0A%20%20Noise%20Decomposition%20and%20Collaboration&body=Title%3A%20NoiseController%3A%20Towards%20Consistent%20Multi-view%20Video%20Generation%20via%0A%20%20Noise%20Decomposition%20and%20Collaboration%0AAuthor%3A%20Haotian%20Dong%20and%20Xin%20Wang%20and%20Di%20Lin%20and%20Yipeng%20Wu%20and%20Qin%20Chen%20and%20Ruonan%20Liu%20and%20Kairui%20Yang%20and%20Ping%20Li%20and%20Qing%20Guo%0AAbstract%3A%20%20%20High-quality%20video%20generation%20is%20crucial%20for%20many%20fields%2C%20including%20the%20film%0Aindustry%20and%20autonomous%20driving.%20However%2C%20generating%20videos%20with%20spatiotemporal%0Aconsistencies%20remains%20challenging.%20Current%20methods%20typically%20utilize%20attention%0Amechanisms%20or%20modify%20noise%20to%20achieve%20consistent%20videos%2C%20neglecting%20global%0Aspatiotemporal%20information%20that%20could%20help%20ensure%20spatial%20and%20temporal%0Aconsistency%20during%20video%20generation.%20In%20this%20paper%2C%20we%20propose%20the%0ANoiseController%2C%20consisting%20of%20Multi-Level%20Noise%20Decomposition%2C%20Multi-Frame%0ANoise%20Collaboration%2C%20and%20Joint%20Denoising%2C%20to%20enhance%20spatiotemporal%0Aconsistencies%20in%20video%20generation.%20In%20multi-level%20noise%20decomposition%2C%20we%20first%0Adecompose%20initial%20noises%20into%20scene-level%20foreground/background%20noises%2C%0Acapturing%20distinct%20motion%20properties%20to%20model%20multi-view%20foreground/background%0Avariations.%20Furthermore%2C%20each%20scene-level%20noise%20is%20further%20decomposed%20into%0Aindividual-level%20shared%20and%20residual%20components.%20The%20shared%20noise%20preserves%0Aconsistency%2C%20while%20the%20residual%20component%20maintains%20diversity.%20In%20multi-frame%0Anoise%20collaboration%2C%20we%20introduce%20an%20inter-view%20spatiotemporal%20collaboration%0Amatrix%20and%20an%20intra-view%20impact%20collaboration%20matrix%20%2C%20which%20captures%20mutual%0Across-view%20effects%20and%20historical%20cross-frame%20impacts%20to%20enhance%20video%20quality.%0AThe%20joint%20denoising%20contains%20two%20parallel%20denoising%20U-Nets%20to%20remove%20each%0Ascene-level%20noise%2C%20mutually%20enhancing%20video%20generation.%20We%20evaluate%20our%0ANoiseController%20on%20public%20datasets%20focusing%20on%20video%20generation%20and%20downstream%0Atasks%2C%20demonstrating%20its%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoiseController%253A%2520Towards%2520Consistent%2520Multi-view%2520Video%2520Generation%2520via%250A%2520%2520Noise%2520Decomposition%2520and%2520Collaboration%26entry.906535625%3DHaotian%2520Dong%2520and%2520Xin%2520Wang%2520and%2520Di%2520Lin%2520and%2520Yipeng%2520Wu%2520and%2520Qin%2520Chen%2520and%2520Ruonan%2520Liu%2520and%2520Kairui%2520Yang%2520and%2520Ping%2520Li%2520and%2520Qing%2520Guo%26entry.1292438233%3D%2520%2520High-quality%2520video%2520generation%2520is%2520crucial%2520for%2520many%2520fields%252C%2520including%2520the%2520film%250Aindustry%2520and%2520autonomous%2520driving.%2520However%252C%2520generating%2520videos%2520with%2520spatiotemporal%250Aconsistencies%2520remains%2520challenging.%2520Current%2520methods%2520typically%2520utilize%2520attention%250Amechanisms%2520or%2520modify%2520noise%2520to%2520achieve%2520consistent%2520videos%252C%2520neglecting%2520global%250Aspatiotemporal%2520information%2520that%2520could%2520help%2520ensure%2520spatial%2520and%2520temporal%250Aconsistency%2520during%2520video%2520generation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%250ANoiseController%252C%2520consisting%2520of%2520Multi-Level%2520Noise%2520Decomposition%252C%2520Multi-Frame%250ANoise%2520Collaboration%252C%2520and%2520Joint%2520Denoising%252C%2520to%2520enhance%2520spatiotemporal%250Aconsistencies%2520in%2520video%2520generation.%2520In%2520multi-level%2520noise%2520decomposition%252C%2520we%2520first%250Adecompose%2520initial%2520noises%2520into%2520scene-level%2520foreground/background%2520noises%252C%250Acapturing%2520distinct%2520motion%2520properties%2520to%2520model%2520multi-view%2520foreground/background%250Avariations.%2520Furthermore%252C%2520each%2520scene-level%2520noise%2520is%2520further%2520decomposed%2520into%250Aindividual-level%2520shared%2520and%2520residual%2520components.%2520The%2520shared%2520noise%2520preserves%250Aconsistency%252C%2520while%2520the%2520residual%2520component%2520maintains%2520diversity.%2520In%2520multi-frame%250Anoise%2520collaboration%252C%2520we%2520introduce%2520an%2520inter-view%2520spatiotemporal%2520collaboration%250Amatrix%2520and%2520an%2520intra-view%2520impact%2520collaboration%2520matrix%2520%252C%2520which%2520captures%2520mutual%250Across-view%2520effects%2520and%2520historical%2520cross-frame%2520impacts%2520to%2520enhance%2520video%2520quality.%250AThe%2520joint%2520denoising%2520contains%2520two%2520parallel%2520denoising%2520U-Nets%2520to%2520remove%2520each%250Ascene-level%2520noise%252C%2520mutually%2520enhancing%2520video%2520generation.%2520We%2520evaluate%2520our%250ANoiseController%2520on%2520public%2520datasets%2520focusing%2520on%2520video%2520generation%2520and%2520downstream%250Atasks%252C%2520demonstrating%2520its%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoiseController%3A%20Towards%20Consistent%20Multi-view%20Video%20Generation%20via%0A%20%20Noise%20Decomposition%20and%20Collaboration&entry.906535625=Haotian%20Dong%20and%20Xin%20Wang%20and%20Di%20Lin%20and%20Yipeng%20Wu%20and%20Qin%20Chen%20and%20Ruonan%20Liu%20and%20Kairui%20Yang%20and%20Ping%20Li%20and%20Qing%20Guo&entry.1292438233=%20%20High-quality%20video%20generation%20is%20crucial%20for%20many%20fields%2C%20including%20the%20film%0Aindustry%20and%20autonomous%20driving.%20However%2C%20generating%20videos%20with%20spatiotemporal%0Aconsistencies%20remains%20challenging.%20Current%20methods%20typically%20utilize%20attention%0Amechanisms%20or%20modify%20noise%20to%20achieve%20consistent%20videos%2C%20neglecting%20global%0Aspatiotemporal%20information%20that%20could%20help%20ensure%20spatial%20and%20temporal%0Aconsistency%20during%20video%20generation.%20In%20this%20paper%2C%20we%20propose%20the%0ANoiseController%2C%20consisting%20of%20Multi-Level%20Noise%20Decomposition%2C%20Multi-Frame%0ANoise%20Collaboration%2C%20and%20Joint%20Denoising%2C%20to%20enhance%20spatiotemporal%0Aconsistencies%20in%20video%20generation.%20In%20multi-level%20noise%20decomposition%2C%20we%20first%0Adecompose%20initial%20noises%20into%20scene-level%20foreground/background%20noises%2C%0Acapturing%20distinct%20motion%20properties%20to%20model%20multi-view%20foreground/background%0Avariations.%20Furthermore%2C%20each%20scene-level%20noise%20is%20further%20decomposed%20into%0Aindividual-level%20shared%20and%20residual%20components.%20The%20shared%20noise%20preserves%0Aconsistency%2C%20while%20the%20residual%20component%20maintains%20diversity.%20In%20multi-frame%0Anoise%20collaboration%2C%20we%20introduce%20an%20inter-view%20spatiotemporal%20collaboration%0Amatrix%20and%20an%20intra-view%20impact%20collaboration%20matrix%20%2C%20which%20captures%20mutual%0Across-view%20effects%20and%20historical%20cross-frame%20impacts%20to%20enhance%20video%20quality.%0AThe%20joint%20denoising%20contains%20two%20parallel%20denoising%20U-Nets%20to%20remove%20each%0Ascene-level%20noise%2C%20mutually%20enhancing%20video%20generation.%20We%20evaluate%20our%0ANoiseController%20on%20public%20datasets%20focusing%20on%20video%20generation%20and%20downstream%0Atasks%2C%20demonstrating%20its%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18448v1&entry.124074799=Read"},
{"title": "LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection", "author": "Rajesh Yarra", "abstract": "  Despite the transformative impact of Artificial Intelligence (AI) across\nvarious sectors, cyber security continues to rely on traditional static and\ndynamic analysis tools, hampered by high false positive rates and superficial\ncode comprehension. While generative AI offers promising automation\ncapabilities for software development, leveraging Large Language Models (LLMs)\nfor vulnerability detection presents unique challenges. This paper explores the\npotential and limitations of LLMs in identifying vulnerabilities, acknowledging\ninherent weaknesses such as hallucinations, limited context length, and\nknowledge cut-offs. Previous attempts employing machine learning models for\nvulnerability detection have proven ineffective due to limited real-world\napplicability, feature engineering challenges, lack of contextual\nunderstanding, and the complexities of training models to keep pace with the\nevolving threat landscape. Therefore, we propose a robust AI-driven approach\nfocused on mitigating these limitations and ensuring the quality and\nreliability of LLM based vulnerability detection. Through innovative\nmethodologies combining Retrieval-Augmented Generation (RAG) and\nMixtureof-Agents (MoA), this research seeks to leverage the strengths of LLMs\nwhile addressing their weaknesses, ultimately paving the way for dependable and\nefficient AI-powered solutions in securing the ever-evolving software\nlandscape.\n", "link": "http://arxiv.org/abs/2504.18423v1", "date": "2025-04-25", "relevancy": 1.9374, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5225}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4868}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMpatronous%3A%20Harnessing%20the%20Power%20of%20LLMs%20For%20Vulnerability%20Detection&body=Title%3A%20LLMpatronous%3A%20Harnessing%20the%20Power%20of%20LLMs%20For%20Vulnerability%20Detection%0AAuthor%3A%20Rajesh%20Yarra%0AAbstract%3A%20%20%20Despite%20the%20transformative%20impact%20of%20Artificial%20Intelligence%20%28AI%29%20across%0Avarious%20sectors%2C%20cyber%20security%20continues%20to%20rely%20on%20traditional%20static%20and%0Adynamic%20analysis%20tools%2C%20hampered%20by%20high%20false%20positive%20rates%20and%20superficial%0Acode%20comprehension.%20While%20generative%20AI%20offers%20promising%20automation%0Acapabilities%20for%20software%20development%2C%20leveraging%20Large%20Language%20Models%20%28LLMs%29%0Afor%20vulnerability%20detection%20presents%20unique%20challenges.%20This%20paper%20explores%20the%0Apotential%20and%20limitations%20of%20LLMs%20in%20identifying%20vulnerabilities%2C%20acknowledging%0Ainherent%20weaknesses%20such%20as%20hallucinations%2C%20limited%20context%20length%2C%20and%0Aknowledge%20cut-offs.%20Previous%20attempts%20employing%20machine%20learning%20models%20for%0Avulnerability%20detection%20have%20proven%20ineffective%20due%20to%20limited%20real-world%0Aapplicability%2C%20feature%20engineering%20challenges%2C%20lack%20of%20contextual%0Aunderstanding%2C%20and%20the%20complexities%20of%20training%20models%20to%20keep%20pace%20with%20the%0Aevolving%20threat%20landscape.%20Therefore%2C%20we%20propose%20a%20robust%20AI-driven%20approach%0Afocused%20on%20mitigating%20these%20limitations%20and%20ensuring%20the%20quality%20and%0Areliability%20of%20LLM%20based%20vulnerability%20detection.%20Through%20innovative%0Amethodologies%20combining%20Retrieval-Augmented%20Generation%20%28RAG%29%20and%0AMixtureof-Agents%20%28MoA%29%2C%20this%20research%20seeks%20to%20leverage%20the%20strengths%20of%20LLMs%0Awhile%20addressing%20their%20weaknesses%2C%20ultimately%20paving%20the%20way%20for%20dependable%20and%0Aefficient%20AI-powered%20solutions%20in%20securing%20the%20ever-evolving%20software%0Alandscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMpatronous%253A%2520Harnessing%2520the%2520Power%2520of%2520LLMs%2520For%2520Vulnerability%2520Detection%26entry.906535625%3DRajesh%2520Yarra%26entry.1292438233%3D%2520%2520Despite%2520the%2520transformative%2520impact%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520across%250Avarious%2520sectors%252C%2520cyber%2520security%2520continues%2520to%2520rely%2520on%2520traditional%2520static%2520and%250Adynamic%2520analysis%2520tools%252C%2520hampered%2520by%2520high%2520false%2520positive%2520rates%2520and%2520superficial%250Acode%2520comprehension.%2520While%2520generative%2520AI%2520offers%2520promising%2520automation%250Acapabilities%2520for%2520software%2520development%252C%2520leveraging%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Afor%2520vulnerability%2520detection%2520presents%2520unique%2520challenges.%2520This%2520paper%2520explores%2520the%250Apotential%2520and%2520limitations%2520of%2520LLMs%2520in%2520identifying%2520vulnerabilities%252C%2520acknowledging%250Ainherent%2520weaknesses%2520such%2520as%2520hallucinations%252C%2520limited%2520context%2520length%252C%2520and%250Aknowledge%2520cut-offs.%2520Previous%2520attempts%2520employing%2520machine%2520learning%2520models%2520for%250Avulnerability%2520detection%2520have%2520proven%2520ineffective%2520due%2520to%2520limited%2520real-world%250Aapplicability%252C%2520feature%2520engineering%2520challenges%252C%2520lack%2520of%2520contextual%250Aunderstanding%252C%2520and%2520the%2520complexities%2520of%2520training%2520models%2520to%2520keep%2520pace%2520with%2520the%250Aevolving%2520threat%2520landscape.%2520Therefore%252C%2520we%2520propose%2520a%2520robust%2520AI-driven%2520approach%250Afocused%2520on%2520mitigating%2520these%2520limitations%2520and%2520ensuring%2520the%2520quality%2520and%250Areliability%2520of%2520LLM%2520based%2520vulnerability%2520detection.%2520Through%2520innovative%250Amethodologies%2520combining%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520and%250AMixtureof-Agents%2520%2528MoA%2529%252C%2520this%2520research%2520seeks%2520to%2520leverage%2520the%2520strengths%2520of%2520LLMs%250Awhile%2520addressing%2520their%2520weaknesses%252C%2520ultimately%2520paving%2520the%2520way%2520for%2520dependable%2520and%250Aefficient%2520AI-powered%2520solutions%2520in%2520securing%2520the%2520ever-evolving%2520software%250Alandscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMpatronous%3A%20Harnessing%20the%20Power%20of%20LLMs%20For%20Vulnerability%20Detection&entry.906535625=Rajesh%20Yarra&entry.1292438233=%20%20Despite%20the%20transformative%20impact%20of%20Artificial%20Intelligence%20%28AI%29%20across%0Avarious%20sectors%2C%20cyber%20security%20continues%20to%20rely%20on%20traditional%20static%20and%0Adynamic%20analysis%20tools%2C%20hampered%20by%20high%20false%20positive%20rates%20and%20superficial%0Acode%20comprehension.%20While%20generative%20AI%20offers%20promising%20automation%0Acapabilities%20for%20software%20development%2C%20leveraging%20Large%20Language%20Models%20%28LLMs%29%0Afor%20vulnerability%20detection%20presents%20unique%20challenges.%20This%20paper%20explores%20the%0Apotential%20and%20limitations%20of%20LLMs%20in%20identifying%20vulnerabilities%2C%20acknowledging%0Ainherent%20weaknesses%20such%20as%20hallucinations%2C%20limited%20context%20length%2C%20and%0Aknowledge%20cut-offs.%20Previous%20attempts%20employing%20machine%20learning%20models%20for%0Avulnerability%20detection%20have%20proven%20ineffective%20due%20to%20limited%20real-world%0Aapplicability%2C%20feature%20engineering%20challenges%2C%20lack%20of%20contextual%0Aunderstanding%2C%20and%20the%20complexities%20of%20training%20models%20to%20keep%20pace%20with%20the%0Aevolving%20threat%20landscape.%20Therefore%2C%20we%20propose%20a%20robust%20AI-driven%20approach%0Afocused%20on%20mitigating%20these%20limitations%20and%20ensuring%20the%20quality%20and%0Areliability%20of%20LLM%20based%20vulnerability%20detection.%20Through%20innovative%0Amethodologies%20combining%20Retrieval-Augmented%20Generation%20%28RAG%29%20and%0AMixtureof-Agents%20%28MoA%29%2C%20this%20research%20seeks%20to%20leverage%20the%20strengths%20of%20LLMs%0Awhile%20addressing%20their%20weaknesses%2C%20ultimately%20paving%20the%20way%20for%20dependable%20and%0Aefficient%20AI-powered%20solutions%20in%20securing%20the%20ever-evolving%20software%0Alandscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18423v1&entry.124074799=Read"},
{"title": "Explainable AI for UAV Mobility Management: A Deep Q-Network Approach\n  for Handover Minimization", "author": "Irshad A. Meer and Bruno H\u00f6rmann and Mustafa Ozger and Fabien Geyer and Alberto Viseras and Dominic Schupke and Cicek Cavdar", "abstract": "  The integration of unmanned aerial vehicles (UAVs) into cellular networks\npresents significant mobility management challenges, primarily due to frequent\nhandovers caused by probabilistic line-of-sight conditions with multiple ground\nbase stations (BSs). To tackle these challenges, reinforcement learning\n(RL)-based methods, particularly deep Q-networks (DQN), have been employed to\noptimize handover decisions dynamically. However, a major drawback of these\nlearning-based approaches is their black-box nature, which limits\ninterpretability in the decision-making process. This paper introduces an\nexplainable AI (XAI) framework that incorporates Shapley Additive Explanations\n(SHAP) to provide deeper insights into how various state parameters influence\nhandover decisions in a DQN-based mobility management system. By quantifying\nthe impact of key features such as reference signal received power (RSRP),\nreference signal received quality (RSRQ), buffer status, and UAV position, our\napproach enhances the interpretability and reliability of RL-based handover\nsolutions. To validate and compare our framework, we utilize real-world network\nperformance data collected from UAV flight trials. Simulation results show that\nour method provides intuitive explanations for policy decisions, effectively\nbridging the gap between AI-driven models and human decision-makers.\n", "link": "http://arxiv.org/abs/2504.18371v1", "date": "2025-04-25", "relevancy": 1.9303, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5088}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.484}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20AI%20for%20UAV%20Mobility%20Management%3A%20A%20Deep%20Q-Network%20Approach%0A%20%20for%20Handover%20Minimization&body=Title%3A%20Explainable%20AI%20for%20UAV%20Mobility%20Management%3A%20A%20Deep%20Q-Network%20Approach%0A%20%20for%20Handover%20Minimization%0AAuthor%3A%20Irshad%20A.%20Meer%20and%20Bruno%20H%C3%B6rmann%20and%20Mustafa%20Ozger%20and%20Fabien%20Geyer%20and%20Alberto%20Viseras%20and%20Dominic%20Schupke%20and%20Cicek%20Cavdar%0AAbstract%3A%20%20%20The%20integration%20of%20unmanned%20aerial%20vehicles%20%28UAVs%29%20into%20cellular%20networks%0Apresents%20significant%20mobility%20management%20challenges%2C%20primarily%20due%20to%20frequent%0Ahandovers%20caused%20by%20probabilistic%20line-of-sight%20conditions%20with%20multiple%20ground%0Abase%20stations%20%28BSs%29.%20To%20tackle%20these%20challenges%2C%20reinforcement%20learning%0A%28RL%29-based%20methods%2C%20particularly%20deep%20Q-networks%20%28DQN%29%2C%20have%20been%20employed%20to%0Aoptimize%20handover%20decisions%20dynamically.%20However%2C%20a%20major%20drawback%20of%20these%0Alearning-based%20approaches%20is%20their%20black-box%20nature%2C%20which%20limits%0Ainterpretability%20in%20the%20decision-making%20process.%20This%20paper%20introduces%20an%0Aexplainable%20AI%20%28XAI%29%20framework%20that%20incorporates%20Shapley%20Additive%20Explanations%0A%28SHAP%29%20to%20provide%20deeper%20insights%20into%20how%20various%20state%20parameters%20influence%0Ahandover%20decisions%20in%20a%20DQN-based%20mobility%20management%20system.%20By%20quantifying%0Athe%20impact%20of%20key%20features%20such%20as%20reference%20signal%20received%20power%20%28RSRP%29%2C%0Areference%20signal%20received%20quality%20%28RSRQ%29%2C%20buffer%20status%2C%20and%20UAV%20position%2C%20our%0Aapproach%20enhances%20the%20interpretability%20and%20reliability%20of%20RL-based%20handover%0Asolutions.%20To%20validate%20and%20compare%20our%20framework%2C%20we%20utilize%20real-world%20network%0Aperformance%20data%20collected%20from%20UAV%20flight%20trials.%20Simulation%20results%20show%20that%0Aour%20method%20provides%20intuitive%20explanations%20for%20policy%20decisions%2C%20effectively%0Abridging%20the%20gap%20between%20AI-driven%20models%20and%20human%20decision-makers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520AI%2520for%2520UAV%2520Mobility%2520Management%253A%2520A%2520Deep%2520Q-Network%2520Approach%250A%2520%2520for%2520Handover%2520Minimization%26entry.906535625%3DIrshad%2520A.%2520Meer%2520and%2520Bruno%2520H%25C3%25B6rmann%2520and%2520Mustafa%2520Ozger%2520and%2520Fabien%2520Geyer%2520and%2520Alberto%2520Viseras%2520and%2520Dominic%2520Schupke%2520and%2520Cicek%2520Cavdar%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520into%2520cellular%2520networks%250Apresents%2520significant%2520mobility%2520management%2520challenges%252C%2520primarily%2520due%2520to%2520frequent%250Ahandovers%2520caused%2520by%2520probabilistic%2520line-of-sight%2520conditions%2520with%2520multiple%2520ground%250Abase%2520stations%2520%2528BSs%2529.%2520To%2520tackle%2520these%2520challenges%252C%2520reinforcement%2520learning%250A%2528RL%2529-based%2520methods%252C%2520particularly%2520deep%2520Q-networks%2520%2528DQN%2529%252C%2520have%2520been%2520employed%2520to%250Aoptimize%2520handover%2520decisions%2520dynamically.%2520However%252C%2520a%2520major%2520drawback%2520of%2520these%250Alearning-based%2520approaches%2520is%2520their%2520black-box%2520nature%252C%2520which%2520limits%250Ainterpretability%2520in%2520the%2520decision-making%2520process.%2520This%2520paper%2520introduces%2520an%250Aexplainable%2520AI%2520%2528XAI%2529%2520framework%2520that%2520incorporates%2520Shapley%2520Additive%2520Explanations%250A%2528SHAP%2529%2520to%2520provide%2520deeper%2520insights%2520into%2520how%2520various%2520state%2520parameters%2520influence%250Ahandover%2520decisions%2520in%2520a%2520DQN-based%2520mobility%2520management%2520system.%2520By%2520quantifying%250Athe%2520impact%2520of%2520key%2520features%2520such%2520as%2520reference%2520signal%2520received%2520power%2520%2528RSRP%2529%252C%250Areference%2520signal%2520received%2520quality%2520%2528RSRQ%2529%252C%2520buffer%2520status%252C%2520and%2520UAV%2520position%252C%2520our%250Aapproach%2520enhances%2520the%2520interpretability%2520and%2520reliability%2520of%2520RL-based%2520handover%250Asolutions.%2520To%2520validate%2520and%2520compare%2520our%2520framework%252C%2520we%2520utilize%2520real-world%2520network%250Aperformance%2520data%2520collected%2520from%2520UAV%2520flight%2520trials.%2520Simulation%2520results%2520show%2520that%250Aour%2520method%2520provides%2520intuitive%2520explanations%2520for%2520policy%2520decisions%252C%2520effectively%250Abridging%2520the%2520gap%2520between%2520AI-driven%2520models%2520and%2520human%2520decision-makers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20AI%20for%20UAV%20Mobility%20Management%3A%20A%20Deep%20Q-Network%20Approach%0A%20%20for%20Handover%20Minimization&entry.906535625=Irshad%20A.%20Meer%20and%20Bruno%20H%C3%B6rmann%20and%20Mustafa%20Ozger%20and%20Fabien%20Geyer%20and%20Alberto%20Viseras%20and%20Dominic%20Schupke%20and%20Cicek%20Cavdar&entry.1292438233=%20%20The%20integration%20of%20unmanned%20aerial%20vehicles%20%28UAVs%29%20into%20cellular%20networks%0Apresents%20significant%20mobility%20management%20challenges%2C%20primarily%20due%20to%20frequent%0Ahandovers%20caused%20by%20probabilistic%20line-of-sight%20conditions%20with%20multiple%20ground%0Abase%20stations%20%28BSs%29.%20To%20tackle%20these%20challenges%2C%20reinforcement%20learning%0A%28RL%29-based%20methods%2C%20particularly%20deep%20Q-networks%20%28DQN%29%2C%20have%20been%20employed%20to%0Aoptimize%20handover%20decisions%20dynamically.%20However%2C%20a%20major%20drawback%20of%20these%0Alearning-based%20approaches%20is%20their%20black-box%20nature%2C%20which%20limits%0Ainterpretability%20in%20the%20decision-making%20process.%20This%20paper%20introduces%20an%0Aexplainable%20AI%20%28XAI%29%20framework%20that%20incorporates%20Shapley%20Additive%20Explanations%0A%28SHAP%29%20to%20provide%20deeper%20insights%20into%20how%20various%20state%20parameters%20influence%0Ahandover%20decisions%20in%20a%20DQN-based%20mobility%20management%20system.%20By%20quantifying%0Athe%20impact%20of%20key%20features%20such%20as%20reference%20signal%20received%20power%20%28RSRP%29%2C%0Areference%20signal%20received%20quality%20%28RSRQ%29%2C%20buffer%20status%2C%20and%20UAV%20position%2C%20our%0Aapproach%20enhances%20the%20interpretability%20and%20reliability%20of%20RL-based%20handover%0Asolutions.%20To%20validate%20and%20compare%20our%20framework%2C%20we%20utilize%20real-world%20network%0Aperformance%20data%20collected%20from%20UAV%20flight%20trials.%20Simulation%20results%20show%20that%0Aour%20method%20provides%20intuitive%20explanations%20for%20policy%20decisions%2C%20effectively%0Abridging%20the%20gap%20between%20AI-driven%20models%20and%20human%20decision-makers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18371v1&entry.124074799=Read"},
{"title": "A comprehensive review of classifier probability calibration metrics", "author": "Richard Oliver Lane", "abstract": "  Probabilities or confidence values produced by artificial intelligence (AI)\nand machine learning (ML) models often do not reflect their true accuracy, with\nsome models being under or over confident in their predictions. For example, if\na model is 80% sure of an outcome, is it correct 80% of the time? Probability\ncalibration metrics measure the discrepancy between confidence and accuracy,\nproviding an independent assessment of model calibration performance that\ncomplements traditional accuracy metrics. Understanding calibration is\nimportant when the outputs of multiple systems are combined, for assurance in\nsafety or business-critical contexts, and for building user trust in models.\nThis paper provides a comprehensive review of probability calibration metrics\nfor classifier and object detection models, organising them according to a\nnumber of different categorisations to highlight their relationships. We\nidentify 82 major metrics, which can be grouped into four classifier families\n(point-based, bin-based, kernel or curve-based, and cumulative) and an object\ndetection family. For each metric, we provide equations where available,\nfacilitating implementation and comparison by future researchers.\n", "link": "http://arxiv.org/abs/2504.18278v1", "date": "2025-04-25", "relevancy": 1.9234, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5199}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4868}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20comprehensive%20review%20of%20classifier%20probability%20calibration%20metrics&body=Title%3A%20A%20comprehensive%20review%20of%20classifier%20probability%20calibration%20metrics%0AAuthor%3A%20Richard%20Oliver%20Lane%0AAbstract%3A%20%20%20Probabilities%20or%20confidence%20values%20produced%20by%20artificial%20intelligence%20%28AI%29%0Aand%20machine%20learning%20%28ML%29%20models%20often%20do%20not%20reflect%20their%20true%20accuracy%2C%20with%0Asome%20models%20being%20under%20or%20over%20confident%20in%20their%20predictions.%20For%20example%2C%20if%0Aa%20model%20is%2080%25%20sure%20of%20an%20outcome%2C%20is%20it%20correct%2080%25%20of%20the%20time%3F%20Probability%0Acalibration%20metrics%20measure%20the%20discrepancy%20between%20confidence%20and%20accuracy%2C%0Aproviding%20an%20independent%20assessment%20of%20model%20calibration%20performance%20that%0Acomplements%20traditional%20accuracy%20metrics.%20Understanding%20calibration%20is%0Aimportant%20when%20the%20outputs%20of%20multiple%20systems%20are%20combined%2C%20for%20assurance%20in%0Asafety%20or%20business-critical%20contexts%2C%20and%20for%20building%20user%20trust%20in%20models.%0AThis%20paper%20provides%20a%20comprehensive%20review%20of%20probability%20calibration%20metrics%0Afor%20classifier%20and%20object%20detection%20models%2C%20organising%20them%20according%20to%20a%0Anumber%20of%20different%20categorisations%20to%20highlight%20their%20relationships.%20We%0Aidentify%2082%20major%20metrics%2C%20which%20can%20be%20grouped%20into%20four%20classifier%20families%0A%28point-based%2C%20bin-based%2C%20kernel%20or%20curve-based%2C%20and%20cumulative%29%20and%20an%20object%0Adetection%20family.%20For%20each%20metric%2C%20we%20provide%20equations%20where%20available%2C%0Afacilitating%20implementation%20and%20comparison%20by%20future%20researchers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520comprehensive%2520review%2520of%2520classifier%2520probability%2520calibration%2520metrics%26entry.906535625%3DRichard%2520Oliver%2520Lane%26entry.1292438233%3D%2520%2520Probabilities%2520or%2520confidence%2520values%2520produced%2520by%2520artificial%2520intelligence%2520%2528AI%2529%250Aand%2520machine%2520learning%2520%2528ML%2529%2520models%2520often%2520do%2520not%2520reflect%2520their%2520true%2520accuracy%252C%2520with%250Asome%2520models%2520being%2520under%2520or%2520over%2520confident%2520in%2520their%2520predictions.%2520For%2520example%252C%2520if%250Aa%2520model%2520is%252080%2525%2520sure%2520of%2520an%2520outcome%252C%2520is%2520it%2520correct%252080%2525%2520of%2520the%2520time%253F%2520Probability%250Acalibration%2520metrics%2520measure%2520the%2520discrepancy%2520between%2520confidence%2520and%2520accuracy%252C%250Aproviding%2520an%2520independent%2520assessment%2520of%2520model%2520calibration%2520performance%2520that%250Acomplements%2520traditional%2520accuracy%2520metrics.%2520Understanding%2520calibration%2520is%250Aimportant%2520when%2520the%2520outputs%2520of%2520multiple%2520systems%2520are%2520combined%252C%2520for%2520assurance%2520in%250Asafety%2520or%2520business-critical%2520contexts%252C%2520and%2520for%2520building%2520user%2520trust%2520in%2520models.%250AThis%2520paper%2520provides%2520a%2520comprehensive%2520review%2520of%2520probability%2520calibration%2520metrics%250Afor%2520classifier%2520and%2520object%2520detection%2520models%252C%2520organising%2520them%2520according%2520to%2520a%250Anumber%2520of%2520different%2520categorisations%2520to%2520highlight%2520their%2520relationships.%2520We%250Aidentify%252082%2520major%2520metrics%252C%2520which%2520can%2520be%2520grouped%2520into%2520four%2520classifier%2520families%250A%2528point-based%252C%2520bin-based%252C%2520kernel%2520or%2520curve-based%252C%2520and%2520cumulative%2529%2520and%2520an%2520object%250Adetection%2520family.%2520For%2520each%2520metric%252C%2520we%2520provide%2520equations%2520where%2520available%252C%250Afacilitating%2520implementation%2520and%2520comparison%2520by%2520future%2520researchers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20comprehensive%20review%20of%20classifier%20probability%20calibration%20metrics&entry.906535625=Richard%20Oliver%20Lane&entry.1292438233=%20%20Probabilities%20or%20confidence%20values%20produced%20by%20artificial%20intelligence%20%28AI%29%0Aand%20machine%20learning%20%28ML%29%20models%20often%20do%20not%20reflect%20their%20true%20accuracy%2C%20with%0Asome%20models%20being%20under%20or%20over%20confident%20in%20their%20predictions.%20For%20example%2C%20if%0Aa%20model%20is%2080%25%20sure%20of%20an%20outcome%2C%20is%20it%20correct%2080%25%20of%20the%20time%3F%20Probability%0Acalibration%20metrics%20measure%20the%20discrepancy%20between%20confidence%20and%20accuracy%2C%0Aproviding%20an%20independent%20assessment%20of%20model%20calibration%20performance%20that%0Acomplements%20traditional%20accuracy%20metrics.%20Understanding%20calibration%20is%0Aimportant%20when%20the%20outputs%20of%20multiple%20systems%20are%20combined%2C%20for%20assurance%20in%0Asafety%20or%20business-critical%20contexts%2C%20and%20for%20building%20user%20trust%20in%20models.%0AThis%20paper%20provides%20a%20comprehensive%20review%20of%20probability%20calibration%20metrics%0Afor%20classifier%20and%20object%20detection%20models%2C%20organising%20them%20according%20to%20a%0Anumber%20of%20different%20categorisations%20to%20highlight%20their%20relationships.%20We%0Aidentify%2082%20major%20metrics%2C%20which%20can%20be%20grouped%20into%20four%20classifier%20families%0A%28point-based%2C%20bin-based%2C%20kernel%20or%20curve-based%2C%20and%20cumulative%29%20and%20an%20object%0Adetection%20family.%20For%20each%20metric%2C%20we%20provide%20equations%20where%20available%2C%0Afacilitating%20implementation%20and%20comparison%20by%20future%20researchers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18278v1&entry.124074799=Read"},
{"title": "SSA-UNet: Advanced Precipitation Nowcasting via Channel Shuffling", "author": "Marco Turzi and Siamak Mehrkanoon", "abstract": "  Weather forecasting is essential for facilitating diverse socio-economic\nactivity and environmental conservation initiatives. Deep learning techniques\nare increasingly being explored as complementary approaches to Numerical\nWeather Prediction (NWP) models, offering potential benefits such as reduced\ncomplexity and enhanced adaptability in specific applications. This work\npresents a novel design, Small Shuffled Attention UNet (SSA-UNet), which\nenhances SmaAt-UNet's architecture by including a shuffle channeling mechanism\nto optimize performance and diminish complexity. To assess its efficacy, this\narchitecture and its reduced variant are examined and trained on two datasets:\na Dutch precipitation dataset from 2016 to 2019, and a French cloud cover\ndataset containing radar images from 2017 to 2018. Three output configurations\nof the proposed architecture are evaluated, yielding outputs of 1, 6, and 12\nprecipitation maps, respectively. To better understand how this model operates\nand produces its predictions, a gradient-based approach called Grad-CAM is used\nto analyze the outputs generated. The analysis of heatmaps generated by\nGrad-CAM facilitated the identification of regions within the input maps that\nthe model considers most informative for generating its predictions. The\nimplementation of SSA-UNet can be found on our\nGithub\\footnote{\\href{https://github.com/MarcoTurzi/SSA-UNet}{https://github.com/MarcoTurzi/SSA-UNet}}\n", "link": "http://arxiv.org/abs/2504.18309v1", "date": "2025-04-25", "relevancy": 1.9162, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4978}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4941}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSA-UNet%3A%20Advanced%20Precipitation%20Nowcasting%20via%20Channel%20Shuffling&body=Title%3A%20SSA-UNet%3A%20Advanced%20Precipitation%20Nowcasting%20via%20Channel%20Shuffling%0AAuthor%3A%20Marco%20Turzi%20and%20Siamak%20Mehrkanoon%0AAbstract%3A%20%20%20Weather%20forecasting%20is%20essential%20for%20facilitating%20diverse%20socio-economic%0Aactivity%20and%20environmental%20conservation%20initiatives.%20Deep%20learning%20techniques%0Aare%20increasingly%20being%20explored%20as%20complementary%20approaches%20to%20Numerical%0AWeather%20Prediction%20%28NWP%29%20models%2C%20offering%20potential%20benefits%20such%20as%20reduced%0Acomplexity%20and%20enhanced%20adaptability%20in%20specific%20applications.%20This%20work%0Apresents%20a%20novel%20design%2C%20Small%20Shuffled%20Attention%20UNet%20%28SSA-UNet%29%2C%20which%0Aenhances%20SmaAt-UNet%27s%20architecture%20by%20including%20a%20shuffle%20channeling%20mechanism%0Ato%20optimize%20performance%20and%20diminish%20complexity.%20To%20assess%20its%20efficacy%2C%20this%0Aarchitecture%20and%20its%20reduced%20variant%20are%20examined%20and%20trained%20on%20two%20datasets%3A%0Aa%20Dutch%20precipitation%20dataset%20from%202016%20to%202019%2C%20and%20a%20French%20cloud%20cover%0Adataset%20containing%20radar%20images%20from%202017%20to%202018.%20Three%20output%20configurations%0Aof%20the%20proposed%20architecture%20are%20evaluated%2C%20yielding%20outputs%20of%201%2C%206%2C%20and%2012%0Aprecipitation%20maps%2C%20respectively.%20To%20better%20understand%20how%20this%20model%20operates%0Aand%20produces%20its%20predictions%2C%20a%20gradient-based%20approach%20called%20Grad-CAM%20is%20used%0Ato%20analyze%20the%20outputs%20generated.%20The%20analysis%20of%20heatmaps%20generated%20by%0AGrad-CAM%20facilitated%20the%20identification%20of%20regions%20within%20the%20input%20maps%20that%0Athe%20model%20considers%20most%20informative%20for%20generating%20its%20predictions.%20The%0Aimplementation%20of%20SSA-UNet%20can%20be%20found%20on%20our%0AGithub%5Cfootnote%7B%5Chref%7Bhttps%3A//github.com/MarcoTurzi/SSA-UNet%7D%7Bhttps%3A//github.com/MarcoTurzi/SSA-UNet%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSA-UNet%253A%2520Advanced%2520Precipitation%2520Nowcasting%2520via%2520Channel%2520Shuffling%26entry.906535625%3DMarco%2520Turzi%2520and%2520Siamak%2520Mehrkanoon%26entry.1292438233%3D%2520%2520Weather%2520forecasting%2520is%2520essential%2520for%2520facilitating%2520diverse%2520socio-economic%250Aactivity%2520and%2520environmental%2520conservation%2520initiatives.%2520Deep%2520learning%2520techniques%250Aare%2520increasingly%2520being%2520explored%2520as%2520complementary%2520approaches%2520to%2520Numerical%250AWeather%2520Prediction%2520%2528NWP%2529%2520models%252C%2520offering%2520potential%2520benefits%2520such%2520as%2520reduced%250Acomplexity%2520and%2520enhanced%2520adaptability%2520in%2520specific%2520applications.%2520This%2520work%250Apresents%2520a%2520novel%2520design%252C%2520Small%2520Shuffled%2520Attention%2520UNet%2520%2528SSA-UNet%2529%252C%2520which%250Aenhances%2520SmaAt-UNet%2527s%2520architecture%2520by%2520including%2520a%2520shuffle%2520channeling%2520mechanism%250Ato%2520optimize%2520performance%2520and%2520diminish%2520complexity.%2520To%2520assess%2520its%2520efficacy%252C%2520this%250Aarchitecture%2520and%2520its%2520reduced%2520variant%2520are%2520examined%2520and%2520trained%2520on%2520two%2520datasets%253A%250Aa%2520Dutch%2520precipitation%2520dataset%2520from%25202016%2520to%25202019%252C%2520and%2520a%2520French%2520cloud%2520cover%250Adataset%2520containing%2520radar%2520images%2520from%25202017%2520to%25202018.%2520Three%2520output%2520configurations%250Aof%2520the%2520proposed%2520architecture%2520are%2520evaluated%252C%2520yielding%2520outputs%2520of%25201%252C%25206%252C%2520and%252012%250Aprecipitation%2520maps%252C%2520respectively.%2520To%2520better%2520understand%2520how%2520this%2520model%2520operates%250Aand%2520produces%2520its%2520predictions%252C%2520a%2520gradient-based%2520approach%2520called%2520Grad-CAM%2520is%2520used%250Ato%2520analyze%2520the%2520outputs%2520generated.%2520The%2520analysis%2520of%2520heatmaps%2520generated%2520by%250AGrad-CAM%2520facilitated%2520the%2520identification%2520of%2520regions%2520within%2520the%2520input%2520maps%2520that%250Athe%2520model%2520considers%2520most%2520informative%2520for%2520generating%2520its%2520predictions.%2520The%250Aimplementation%2520of%2520SSA-UNet%2520can%2520be%2520found%2520on%2520our%250AGithub%255Cfootnote%257B%255Chref%257Bhttps%253A//github.com/MarcoTurzi/SSA-UNet%257D%257Bhttps%253A//github.com/MarcoTurzi/SSA-UNet%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSA-UNet%3A%20Advanced%20Precipitation%20Nowcasting%20via%20Channel%20Shuffling&entry.906535625=Marco%20Turzi%20and%20Siamak%20Mehrkanoon&entry.1292438233=%20%20Weather%20forecasting%20is%20essential%20for%20facilitating%20diverse%20socio-economic%0Aactivity%20and%20environmental%20conservation%20initiatives.%20Deep%20learning%20techniques%0Aare%20increasingly%20being%20explored%20as%20complementary%20approaches%20to%20Numerical%0AWeather%20Prediction%20%28NWP%29%20models%2C%20offering%20potential%20benefits%20such%20as%20reduced%0Acomplexity%20and%20enhanced%20adaptability%20in%20specific%20applications.%20This%20work%0Apresents%20a%20novel%20design%2C%20Small%20Shuffled%20Attention%20UNet%20%28SSA-UNet%29%2C%20which%0Aenhances%20SmaAt-UNet%27s%20architecture%20by%20including%20a%20shuffle%20channeling%20mechanism%0Ato%20optimize%20performance%20and%20diminish%20complexity.%20To%20assess%20its%20efficacy%2C%20this%0Aarchitecture%20and%20its%20reduced%20variant%20are%20examined%20and%20trained%20on%20two%20datasets%3A%0Aa%20Dutch%20precipitation%20dataset%20from%202016%20to%202019%2C%20and%20a%20French%20cloud%20cover%0Adataset%20containing%20radar%20images%20from%202017%20to%202018.%20Three%20output%20configurations%0Aof%20the%20proposed%20architecture%20are%20evaluated%2C%20yielding%20outputs%20of%201%2C%206%2C%20and%2012%0Aprecipitation%20maps%2C%20respectively.%20To%20better%20understand%20how%20this%20model%20operates%0Aand%20produces%20its%20predictions%2C%20a%20gradient-based%20approach%20called%20Grad-CAM%20is%20used%0Ato%20analyze%20the%20outputs%20generated.%20The%20analysis%20of%20heatmaps%20generated%20by%0AGrad-CAM%20facilitated%20the%20identification%20of%20regions%20within%20the%20input%20maps%20that%0Athe%20model%20considers%20most%20informative%20for%20generating%20its%20predictions.%20The%0Aimplementation%20of%20SSA-UNet%20can%20be%20found%20on%20our%0AGithub%5Cfootnote%7B%5Chref%7Bhttps%3A//github.com/MarcoTurzi/SSA-UNet%7D%7Bhttps%3A//github.com/MarcoTurzi/SSA-UNet%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18309v1&entry.124074799=Read"},
{"title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs", "author": "Hongyu Wang and Shuming Ma and Furu Wei", "abstract": "  Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.\n", "link": "http://arxiv.org/abs/2504.18415v1", "date": "2025-04-25", "relevancy": 1.9133, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4929}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4897}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BitNet%20v2%3A%20Native%204-bit%20Activations%20with%20Hadamard%20Transformation%20for%0A%20%201-bit%20LLMs&body=Title%3A%20BitNet%20v2%3A%20Native%204-bit%20Activations%20with%20Hadamard%20Transformation%20for%0A%20%201-bit%20LLMs%0AAuthor%3A%20Hongyu%20Wang%20and%20Shuming%20Ma%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Efficient%20deployment%20of%201-bit%20Large%20Language%20Models%20%28LLMs%29%20is%20hindered%20by%0Aactivation%20outliers%2C%20which%20complicate%20quantization%20to%20low%20bit-widths.%20We%0Aintroduce%20BitNet%20v2%2C%20a%20novel%20framework%20enabling%20native%204-bit%20activation%0Aquantization%20for%201-bit%20LLMs.%20To%20tackle%20outliers%20in%20attention%20and%20feed-forward%0Anetwork%20activations%2C%20we%20propose%20H-BitLinear%2C%20a%20module%20applying%20an%20online%0AHadamard%20transformation%20prior%20to%20activation%20quantization.%20This%20transformation%0Asmooths%20sharp%20activation%20distributions%20into%20more%20Gaussian-like%20forms%2C%20suitable%0Afor%20low-bit%20representation.%20Experiments%20show%20BitNet%20v2%20trained%20from%20scratch%0Awith%208-bit%20activations%20matches%20BitNet%20b1.58%20performance.%20Crucially%2C%20BitNet%20v2%0Aachieves%20minimal%20performance%20degradation%20when%20trained%20with%20native%204-bit%0Aactivations%2C%20significantly%20reducing%20memory%20footprint%20and%20computational%20cost%20for%0Abatched%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBitNet%2520v2%253A%2520Native%25204-bit%2520Activations%2520with%2520Hadamard%2520Transformation%2520for%250A%2520%25201-bit%2520LLMs%26entry.906535625%3DHongyu%2520Wang%2520and%2520Shuming%2520Ma%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520Efficient%2520deployment%2520of%25201-bit%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520hindered%2520by%250Aactivation%2520outliers%252C%2520which%2520complicate%2520quantization%2520to%2520low%2520bit-widths.%2520We%250Aintroduce%2520BitNet%2520v2%252C%2520a%2520novel%2520framework%2520enabling%2520native%25204-bit%2520activation%250Aquantization%2520for%25201-bit%2520LLMs.%2520To%2520tackle%2520outliers%2520in%2520attention%2520and%2520feed-forward%250Anetwork%2520activations%252C%2520we%2520propose%2520H-BitLinear%252C%2520a%2520module%2520applying%2520an%2520online%250AHadamard%2520transformation%2520prior%2520to%2520activation%2520quantization.%2520This%2520transformation%250Asmooths%2520sharp%2520activation%2520distributions%2520into%2520more%2520Gaussian-like%2520forms%252C%2520suitable%250Afor%2520low-bit%2520representation.%2520Experiments%2520show%2520BitNet%2520v2%2520trained%2520from%2520scratch%250Awith%25208-bit%2520activations%2520matches%2520BitNet%2520b1.58%2520performance.%2520Crucially%252C%2520BitNet%2520v2%250Aachieves%2520minimal%2520performance%2520degradation%2520when%2520trained%2520with%2520native%25204-bit%250Aactivations%252C%2520significantly%2520reducing%2520memory%2520footprint%2520and%2520computational%2520cost%2520for%250Abatched%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BitNet%20v2%3A%20Native%204-bit%20Activations%20with%20Hadamard%20Transformation%20for%0A%20%201-bit%20LLMs&entry.906535625=Hongyu%20Wang%20and%20Shuming%20Ma%20and%20Furu%20Wei&entry.1292438233=%20%20Efficient%20deployment%20of%201-bit%20Large%20Language%20Models%20%28LLMs%29%20is%20hindered%20by%0Aactivation%20outliers%2C%20which%20complicate%20quantization%20to%20low%20bit-widths.%20We%0Aintroduce%20BitNet%20v2%2C%20a%20novel%20framework%20enabling%20native%204-bit%20activation%0Aquantization%20for%201-bit%20LLMs.%20To%20tackle%20outliers%20in%20attention%20and%20feed-forward%0Anetwork%20activations%2C%20we%20propose%20H-BitLinear%2C%20a%20module%20applying%20an%20online%0AHadamard%20transformation%20prior%20to%20activation%20quantization.%20This%20transformation%0Asmooths%20sharp%20activation%20distributions%20into%20more%20Gaussian-like%20forms%2C%20suitable%0Afor%20low-bit%20representation.%20Experiments%20show%20BitNet%20v2%20trained%20from%20scratch%0Awith%208-bit%20activations%20matches%20BitNet%20b1.58%20performance.%20Crucially%2C%20BitNet%20v2%0Aachieves%20minimal%20performance%20degradation%20when%20trained%20with%20native%204-bit%0Aactivations%2C%20significantly%20reducing%20memory%20footprint%20and%20computational%20cost%20for%0Abatched%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18415v1&entry.124074799=Read"},
{"title": "Kernel-Based Optimal Control: An Infinitesimal Generator Approach", "author": "Petar Bevanda and Nicolas Hoischen and Tobias Wittmann and Jan Br\u00fcdigam and Sandra Hirche and Boris Houska", "abstract": "  This paper presents a novel operator-theoretic approach for optimal control\nof nonlinear stochastic systems within reproducing kernel Hilbert spaces. Our\nlearning framework leverages data samples of system dynamics and stage cost\nfunctions, with only control penalties and constraints provided. The proposed\nmethod directly learns the infinitesimal generator of a controlled stochastic\ndiffusion in an infinite-dimensional hypothesis space. We demonstrate that our\napproach seamlessly integrates with modern convex operator-theoretic\nHamilton-Jacobi-Bellman recursions, enabling a data-driven solution to the\noptimal control problems. Furthermore, our learning framework includes\nnonparametric estimators for uncontrolled infinitesimal generators as a special\ncase. Numerical experiments, ranging from synthetic differential equations to\nsimulated robotic systems, showcase the advantages of our approach compared to\nboth modern data-driven and classical nonlinear programming methods for optimal\ncontrol.\n", "link": "http://arxiv.org/abs/2412.01591v3", "date": "2025-04-25", "relevancy": 1.9068, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.499}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4739}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel-Based%20Optimal%20Control%3A%20An%20Infinitesimal%20Generator%20Approach&body=Title%3A%20Kernel-Based%20Optimal%20Control%3A%20An%20Infinitesimal%20Generator%20Approach%0AAuthor%3A%20Petar%20Bevanda%20and%20Nicolas%20Hoischen%20and%20Tobias%20Wittmann%20and%20Jan%20Br%C3%BCdigam%20and%20Sandra%20Hirche%20and%20Boris%20Houska%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20operator-theoretic%20approach%20for%20optimal%20control%0Aof%20nonlinear%20stochastic%20systems%20within%20reproducing%20kernel%20Hilbert%20spaces.%20Our%0Alearning%20framework%20leverages%20data%20samples%20of%20system%20dynamics%20and%20stage%20cost%0Afunctions%2C%20with%20only%20control%20penalties%20and%20constraints%20provided.%20The%20proposed%0Amethod%20directly%20learns%20the%20infinitesimal%20generator%20of%20a%20controlled%20stochastic%0Adiffusion%20in%20an%20infinite-dimensional%20hypothesis%20space.%20We%20demonstrate%20that%20our%0Aapproach%20seamlessly%20integrates%20with%20modern%20convex%20operator-theoretic%0AHamilton-Jacobi-Bellman%20recursions%2C%20enabling%20a%20data-driven%20solution%20to%20the%0Aoptimal%20control%20problems.%20Furthermore%2C%20our%20learning%20framework%20includes%0Anonparametric%20estimators%20for%20uncontrolled%20infinitesimal%20generators%20as%20a%20special%0Acase.%20Numerical%20experiments%2C%20ranging%20from%20synthetic%20differential%20equations%20to%0Asimulated%20robotic%20systems%2C%20showcase%20the%20advantages%20of%20our%20approach%20compared%20to%0Aboth%20modern%20data-driven%20and%20classical%20nonlinear%20programming%20methods%20for%20optimal%0Acontrol.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01591v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel-Based%2520Optimal%2520Control%253A%2520An%2520Infinitesimal%2520Generator%2520Approach%26entry.906535625%3DPetar%2520Bevanda%2520and%2520Nicolas%2520Hoischen%2520and%2520Tobias%2520Wittmann%2520and%2520Jan%2520Br%25C3%25BCdigam%2520and%2520Sandra%2520Hirche%2520and%2520Boris%2520Houska%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520operator-theoretic%2520approach%2520for%2520optimal%2520control%250Aof%2520nonlinear%2520stochastic%2520systems%2520within%2520reproducing%2520kernel%2520Hilbert%2520spaces.%2520Our%250Alearning%2520framework%2520leverages%2520data%2520samples%2520of%2520system%2520dynamics%2520and%2520stage%2520cost%250Afunctions%252C%2520with%2520only%2520control%2520penalties%2520and%2520constraints%2520provided.%2520The%2520proposed%250Amethod%2520directly%2520learns%2520the%2520infinitesimal%2520generator%2520of%2520a%2520controlled%2520stochastic%250Adiffusion%2520in%2520an%2520infinite-dimensional%2520hypothesis%2520space.%2520We%2520demonstrate%2520that%2520our%250Aapproach%2520seamlessly%2520integrates%2520with%2520modern%2520convex%2520operator-theoretic%250AHamilton-Jacobi-Bellman%2520recursions%252C%2520enabling%2520a%2520data-driven%2520solution%2520to%2520the%250Aoptimal%2520control%2520problems.%2520Furthermore%252C%2520our%2520learning%2520framework%2520includes%250Anonparametric%2520estimators%2520for%2520uncontrolled%2520infinitesimal%2520generators%2520as%2520a%2520special%250Acase.%2520Numerical%2520experiments%252C%2520ranging%2520from%2520synthetic%2520differential%2520equations%2520to%250Asimulated%2520robotic%2520systems%252C%2520showcase%2520the%2520advantages%2520of%2520our%2520approach%2520compared%2520to%250Aboth%2520modern%2520data-driven%2520and%2520classical%2520nonlinear%2520programming%2520methods%2520for%2520optimal%250Acontrol.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01591v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel-Based%20Optimal%20Control%3A%20An%20Infinitesimal%20Generator%20Approach&entry.906535625=Petar%20Bevanda%20and%20Nicolas%20Hoischen%20and%20Tobias%20Wittmann%20and%20Jan%20Br%C3%BCdigam%20and%20Sandra%20Hirche%20and%20Boris%20Houska&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20operator-theoretic%20approach%20for%20optimal%20control%0Aof%20nonlinear%20stochastic%20systems%20within%20reproducing%20kernel%20Hilbert%20spaces.%20Our%0Alearning%20framework%20leverages%20data%20samples%20of%20system%20dynamics%20and%20stage%20cost%0Afunctions%2C%20with%20only%20control%20penalties%20and%20constraints%20provided.%20The%20proposed%0Amethod%20directly%20learns%20the%20infinitesimal%20generator%20of%20a%20controlled%20stochastic%0Adiffusion%20in%20an%20infinite-dimensional%20hypothesis%20space.%20We%20demonstrate%20that%20our%0Aapproach%20seamlessly%20integrates%20with%20modern%20convex%20operator-theoretic%0AHamilton-Jacobi-Bellman%20recursions%2C%20enabling%20a%20data-driven%20solution%20to%20the%0Aoptimal%20control%20problems.%20Furthermore%2C%20our%20learning%20framework%20includes%0Anonparametric%20estimators%20for%20uncontrolled%20infinitesimal%20generators%20as%20a%20special%0Acase.%20Numerical%20experiments%2C%20ranging%20from%20synthetic%20differential%20equations%20to%0Asimulated%20robotic%20systems%2C%20showcase%20the%20advantages%20of%20our%20approach%20compared%20to%0Aboth%20modern%20data-driven%20and%20classical%20nonlinear%20programming%20methods%20for%20optimal%0Acontrol.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01591v3&entry.124074799=Read"},
{"title": "EquiNO: A Physics-Informed Neural Operator for Multiscale Simulations", "author": "Hamidreza Eivazi and Jendrik-Alexander Tr\u00f6ger and Stefan Wittek and Stefan Hartmann and Andreas Rausch", "abstract": "  Multiscale problems are ubiquitous in physics. Numerical simulations of such\nproblems by solving partial differential equations (PDEs) at high resolution\nare computationally too expensive for many-query scenarios, e.g., uncertainty\nquantification, remeshing applications, topology optimization, and so forth.\nThis limitation has motivated the application of data-driven surrogate models,\nwhere the microscale computations are $\\textit{substituted}$ with a surrogate,\nusually acting as a black-box mapping between macroscale quantities. These\nmodels offer significant speedups but struggle with incorporating microscale\nphysical constraints, such as the balance of linear momentum and constitutive\nmodels. In this contribution, we propose Equilibrium Neural Operator (EquiNO)\nas a $\\textit{complementary}$ physics-informed PDE surrogate for predicting\nmicroscale physics and compare it with variational physics-informed neural and\noperator networks. Our framework, applicable to the so-called multiscale\nFE$^{\\,2}\\,$ computations, introduces the FE-OL approach by integrating the\nfinite element (FE) method with operator learning (OL). We apply the proposed\nFE-OL approach to quasi-static problems of solid mechanics. The results\ndemonstrate that FE-OL can yield accurate solutions even when confronted with a\nrestricted dataset during model development. Our results show that EquiNO\nachieves speedup factors exceeding 8000-fold compared to traditional methods\nand offers an optimal balance between data-driven and physics-based strategies.\n", "link": "http://arxiv.org/abs/2504.07976v2", "date": "2025-04-25", "relevancy": 1.9058, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4886}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4783}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EquiNO%3A%20A%20Physics-Informed%20Neural%20Operator%20for%20Multiscale%20Simulations&body=Title%3A%20EquiNO%3A%20A%20Physics-Informed%20Neural%20Operator%20for%20Multiscale%20Simulations%0AAuthor%3A%20Hamidreza%20Eivazi%20and%20Jendrik-Alexander%20Tr%C3%B6ger%20and%20Stefan%20Wittek%20and%20Stefan%20Hartmann%20and%20Andreas%20Rausch%0AAbstract%3A%20%20%20Multiscale%20problems%20are%20ubiquitous%20in%20physics.%20Numerical%20simulations%20of%20such%0Aproblems%20by%20solving%20partial%20differential%20equations%20%28PDEs%29%20at%20high%20resolution%0Aare%20computationally%20too%20expensive%20for%20many-query%20scenarios%2C%20e.g.%2C%20uncertainty%0Aquantification%2C%20remeshing%20applications%2C%20topology%20optimization%2C%20and%20so%20forth.%0AThis%20limitation%20has%20motivated%20the%20application%20of%20data-driven%20surrogate%20models%2C%0Awhere%20the%20microscale%20computations%20are%20%24%5Ctextit%7Bsubstituted%7D%24%20with%20a%20surrogate%2C%0Ausually%20acting%20as%20a%20black-box%20mapping%20between%20macroscale%20quantities.%20These%0Amodels%20offer%20significant%20speedups%20but%20struggle%20with%20incorporating%20microscale%0Aphysical%20constraints%2C%20such%20as%20the%20balance%20of%20linear%20momentum%20and%20constitutive%0Amodels.%20In%20this%20contribution%2C%20we%20propose%20Equilibrium%20Neural%20Operator%20%28EquiNO%29%0Aas%20a%20%24%5Ctextit%7Bcomplementary%7D%24%20physics-informed%20PDE%20surrogate%20for%20predicting%0Amicroscale%20physics%20and%20compare%20it%20with%20variational%20physics-informed%20neural%20and%0Aoperator%20networks.%20Our%20framework%2C%20applicable%20to%20the%20so-called%20multiscale%0AFE%24%5E%7B%5C%2C2%7D%5C%2C%24%20computations%2C%20introduces%20the%20FE-OL%20approach%20by%20integrating%20the%0Afinite%20element%20%28FE%29%20method%20with%20operator%20learning%20%28OL%29.%20We%20apply%20the%20proposed%0AFE-OL%20approach%20to%20quasi-static%20problems%20of%20solid%20mechanics.%20The%20results%0Ademonstrate%20that%20FE-OL%20can%20yield%20accurate%20solutions%20even%20when%20confronted%20with%20a%0Arestricted%20dataset%20during%20model%20development.%20Our%20results%20show%20that%20EquiNO%0Aachieves%20speedup%20factors%20exceeding%208000-fold%20compared%20to%20traditional%20methods%0Aand%20offers%20an%20optimal%20balance%20between%20data-driven%20and%20physics-based%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07976v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquiNO%253A%2520A%2520Physics-Informed%2520Neural%2520Operator%2520for%2520Multiscale%2520Simulations%26entry.906535625%3DHamidreza%2520Eivazi%2520and%2520Jendrik-Alexander%2520Tr%25C3%25B6ger%2520and%2520Stefan%2520Wittek%2520and%2520Stefan%2520Hartmann%2520and%2520Andreas%2520Rausch%26entry.1292438233%3D%2520%2520Multiscale%2520problems%2520are%2520ubiquitous%2520in%2520physics.%2520Numerical%2520simulations%2520of%2520such%250Aproblems%2520by%2520solving%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520at%2520high%2520resolution%250Aare%2520computationally%2520too%2520expensive%2520for%2520many-query%2520scenarios%252C%2520e.g.%252C%2520uncertainty%250Aquantification%252C%2520remeshing%2520applications%252C%2520topology%2520optimization%252C%2520and%2520so%2520forth.%250AThis%2520limitation%2520has%2520motivated%2520the%2520application%2520of%2520data-driven%2520surrogate%2520models%252C%250Awhere%2520the%2520microscale%2520computations%2520are%2520%2524%255Ctextit%257Bsubstituted%257D%2524%2520with%2520a%2520surrogate%252C%250Ausually%2520acting%2520as%2520a%2520black-box%2520mapping%2520between%2520macroscale%2520quantities.%2520These%250Amodels%2520offer%2520significant%2520speedups%2520but%2520struggle%2520with%2520incorporating%2520microscale%250Aphysical%2520constraints%252C%2520such%2520as%2520the%2520balance%2520of%2520linear%2520momentum%2520and%2520constitutive%250Amodels.%2520In%2520this%2520contribution%252C%2520we%2520propose%2520Equilibrium%2520Neural%2520Operator%2520%2528EquiNO%2529%250Aas%2520a%2520%2524%255Ctextit%257Bcomplementary%257D%2524%2520physics-informed%2520PDE%2520surrogate%2520for%2520predicting%250Amicroscale%2520physics%2520and%2520compare%2520it%2520with%2520variational%2520physics-informed%2520neural%2520and%250Aoperator%2520networks.%2520Our%2520framework%252C%2520applicable%2520to%2520the%2520so-called%2520multiscale%250AFE%2524%255E%257B%255C%252C2%257D%255C%252C%2524%2520computations%252C%2520introduces%2520the%2520FE-OL%2520approach%2520by%2520integrating%2520the%250Afinite%2520element%2520%2528FE%2529%2520method%2520with%2520operator%2520learning%2520%2528OL%2529.%2520We%2520apply%2520the%2520proposed%250AFE-OL%2520approach%2520to%2520quasi-static%2520problems%2520of%2520solid%2520mechanics.%2520The%2520results%250Ademonstrate%2520that%2520FE-OL%2520can%2520yield%2520accurate%2520solutions%2520even%2520when%2520confronted%2520with%2520a%250Arestricted%2520dataset%2520during%2520model%2520development.%2520Our%2520results%2520show%2520that%2520EquiNO%250Aachieves%2520speedup%2520factors%2520exceeding%25208000-fold%2520compared%2520to%2520traditional%2520methods%250Aand%2520offers%2520an%2520optimal%2520balance%2520between%2520data-driven%2520and%2520physics-based%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07976v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EquiNO%3A%20A%20Physics-Informed%20Neural%20Operator%20for%20Multiscale%20Simulations&entry.906535625=Hamidreza%20Eivazi%20and%20Jendrik-Alexander%20Tr%C3%B6ger%20and%20Stefan%20Wittek%20and%20Stefan%20Hartmann%20and%20Andreas%20Rausch&entry.1292438233=%20%20Multiscale%20problems%20are%20ubiquitous%20in%20physics.%20Numerical%20simulations%20of%20such%0Aproblems%20by%20solving%20partial%20differential%20equations%20%28PDEs%29%20at%20high%20resolution%0Aare%20computationally%20too%20expensive%20for%20many-query%20scenarios%2C%20e.g.%2C%20uncertainty%0Aquantification%2C%20remeshing%20applications%2C%20topology%20optimization%2C%20and%20so%20forth.%0AThis%20limitation%20has%20motivated%20the%20application%20of%20data-driven%20surrogate%20models%2C%0Awhere%20the%20microscale%20computations%20are%20%24%5Ctextit%7Bsubstituted%7D%24%20with%20a%20surrogate%2C%0Ausually%20acting%20as%20a%20black-box%20mapping%20between%20macroscale%20quantities.%20These%0Amodels%20offer%20significant%20speedups%20but%20struggle%20with%20incorporating%20microscale%0Aphysical%20constraints%2C%20such%20as%20the%20balance%20of%20linear%20momentum%20and%20constitutive%0Amodels.%20In%20this%20contribution%2C%20we%20propose%20Equilibrium%20Neural%20Operator%20%28EquiNO%29%0Aas%20a%20%24%5Ctextit%7Bcomplementary%7D%24%20physics-informed%20PDE%20surrogate%20for%20predicting%0Amicroscale%20physics%20and%20compare%20it%20with%20variational%20physics-informed%20neural%20and%0Aoperator%20networks.%20Our%20framework%2C%20applicable%20to%20the%20so-called%20multiscale%0AFE%24%5E%7B%5C%2C2%7D%5C%2C%24%20computations%2C%20introduces%20the%20FE-OL%20approach%20by%20integrating%20the%0Afinite%20element%20%28FE%29%20method%20with%20operator%20learning%20%28OL%29.%20We%20apply%20the%20proposed%0AFE-OL%20approach%20to%20quasi-static%20problems%20of%20solid%20mechanics.%20The%20results%0Ademonstrate%20that%20FE-OL%20can%20yield%20accurate%20solutions%20even%20when%20confronted%20with%20a%0Arestricted%20dataset%20during%20model%20development.%20Our%20results%20show%20that%20EquiNO%0Aachieves%20speedup%20factors%20exceeding%208000-fold%20compared%20to%20traditional%20methods%0Aand%20offers%20an%20optimal%20balance%20between%20data-driven%20and%20physics-based%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07976v2&entry.124074799=Read"},
{"title": "HALO: Hardware-aware quantization with low critical-path-delay weights\n  for LLM acceleration", "author": "Rohan Juneja and Shivam Aggarwal and Safeen Huda and Tulika Mitra and Li-Shiuan Peh", "abstract": "  Quantization is critical for efficiently deploying large language models\n(LLMs). Yet conventional methods remain hardware-agnostic, limited to bit-width\nconstraints, and do not account for intrinsic circuit characteristics such as\nthe timing behaviors and energy profiles of Multiply-Accumulate (MAC) units.\nThis disconnect from circuit-level behavior limits the ability to exploit\navailable timing margins and energy-saving opportunities, reducing the overall\nefficiency of deployment on modern accelerators.\n  To address these limitations, we propose HALO, a versatile framework for\nHardware-Aware Post-Training Quantization (PTQ). Unlike traditional methods,\nHALO explicitly incorporates detailed hardware characteristics, including\ncritical-path timing and power consumption, into its quantization approach.\nHALO strategically selects weights with low critical-path-delays enabling\nhigher operational frequencies and dynamic frequency scaling without disrupting\nthe architecture's dataflow. Remarkably, HALO achieves these improvements with\nonly a few dynamic voltage and frequency scaling (DVFS) adjustments, ensuring\nsimplicity and practicality in deployment. Additionally, by reducing switching\nactivity within the MAC units, HALO effectively lowers energy consumption.\nEvaluations on accelerators such as Tensor Processing Units (TPUs) and Graphics\nProcessing Units (GPUs) demonstrate that HALO significantly enhances inference\nefficiency, achieving average performance improvements of 270% and energy\nsavings of 51% over baseline quantization methods, all with minimal impact on\naccuracy.\n", "link": "http://arxiv.org/abs/2502.19662v2", "date": "2025-04-25", "relevancy": 1.445, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5008}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4743}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HALO%3A%20Hardware-aware%20quantization%20with%20low%20critical-path-delay%20weights%0A%20%20for%20LLM%20acceleration&body=Title%3A%20HALO%3A%20Hardware-aware%20quantization%20with%20low%20critical-path-delay%20weights%0A%20%20for%20LLM%20acceleration%0AAuthor%3A%20Rohan%20Juneja%20and%20Shivam%20Aggarwal%20and%20Safeen%20Huda%20and%20Tulika%20Mitra%20and%20Li-Shiuan%20Peh%0AAbstract%3A%20%20%20Quantization%20is%20critical%20for%20efficiently%20deploying%20large%20language%20models%0A%28LLMs%29.%20Yet%20conventional%20methods%20remain%20hardware-agnostic%2C%20limited%20to%20bit-width%0Aconstraints%2C%20and%20do%20not%20account%20for%20intrinsic%20circuit%20characteristics%20such%20as%0Athe%20timing%20behaviors%20and%20energy%20profiles%20of%20Multiply-Accumulate%20%28MAC%29%20units.%0AThis%20disconnect%20from%20circuit-level%20behavior%20limits%20the%20ability%20to%20exploit%0Aavailable%20timing%20margins%20and%20energy-saving%20opportunities%2C%20reducing%20the%20overall%0Aefficiency%20of%20deployment%20on%20modern%20accelerators.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20HALO%2C%20a%20versatile%20framework%20for%0AHardware-Aware%20Post-Training%20Quantization%20%28PTQ%29.%20Unlike%20traditional%20methods%2C%0AHALO%20explicitly%20incorporates%20detailed%20hardware%20characteristics%2C%20including%0Acritical-path%20timing%20and%20power%20consumption%2C%20into%20its%20quantization%20approach.%0AHALO%20strategically%20selects%20weights%20with%20low%20critical-path-delays%20enabling%0Ahigher%20operational%20frequencies%20and%20dynamic%20frequency%20scaling%20without%20disrupting%0Athe%20architecture%27s%20dataflow.%20Remarkably%2C%20HALO%20achieves%20these%20improvements%20with%0Aonly%20a%20few%20dynamic%20voltage%20and%20frequency%20scaling%20%28DVFS%29%20adjustments%2C%20ensuring%0Asimplicity%20and%20practicality%20in%20deployment.%20Additionally%2C%20by%20reducing%20switching%0Aactivity%20within%20the%20MAC%20units%2C%20HALO%20effectively%20lowers%20energy%20consumption.%0AEvaluations%20on%20accelerators%20such%20as%20Tensor%20Processing%20Units%20%28TPUs%29%20and%20Graphics%0AProcessing%20Units%20%28GPUs%29%20demonstrate%20that%20HALO%20significantly%20enhances%20inference%0Aefficiency%2C%20achieving%20average%20performance%20improvements%20of%20270%25%20and%20energy%0Asavings%20of%2051%25%20over%20baseline%20quantization%20methods%2C%20all%20with%20minimal%20impact%20on%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19662v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHALO%253A%2520Hardware-aware%2520quantization%2520with%2520low%2520critical-path-delay%2520weights%250A%2520%2520for%2520LLM%2520acceleration%26entry.906535625%3DRohan%2520Juneja%2520and%2520Shivam%2520Aggarwal%2520and%2520Safeen%2520Huda%2520and%2520Tulika%2520Mitra%2520and%2520Li-Shiuan%2520Peh%26entry.1292438233%3D%2520%2520Quantization%2520is%2520critical%2520for%2520efficiently%2520deploying%2520large%2520language%2520models%250A%2528LLMs%2529.%2520Yet%2520conventional%2520methods%2520remain%2520hardware-agnostic%252C%2520limited%2520to%2520bit-width%250Aconstraints%252C%2520and%2520do%2520not%2520account%2520for%2520intrinsic%2520circuit%2520characteristics%2520such%2520as%250Athe%2520timing%2520behaviors%2520and%2520energy%2520profiles%2520of%2520Multiply-Accumulate%2520%2528MAC%2529%2520units.%250AThis%2520disconnect%2520from%2520circuit-level%2520behavior%2520limits%2520the%2520ability%2520to%2520exploit%250Aavailable%2520timing%2520margins%2520and%2520energy-saving%2520opportunities%252C%2520reducing%2520the%2520overall%250Aefficiency%2520of%2520deployment%2520on%2520modern%2520accelerators.%250A%2520%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520HALO%252C%2520a%2520versatile%2520framework%2520for%250AHardware-Aware%2520Post-Training%2520Quantization%2520%2528PTQ%2529.%2520Unlike%2520traditional%2520methods%252C%250AHALO%2520explicitly%2520incorporates%2520detailed%2520hardware%2520characteristics%252C%2520including%250Acritical-path%2520timing%2520and%2520power%2520consumption%252C%2520into%2520its%2520quantization%2520approach.%250AHALO%2520strategically%2520selects%2520weights%2520with%2520low%2520critical-path-delays%2520enabling%250Ahigher%2520operational%2520frequencies%2520and%2520dynamic%2520frequency%2520scaling%2520without%2520disrupting%250Athe%2520architecture%2527s%2520dataflow.%2520Remarkably%252C%2520HALO%2520achieves%2520these%2520improvements%2520with%250Aonly%2520a%2520few%2520dynamic%2520voltage%2520and%2520frequency%2520scaling%2520%2528DVFS%2529%2520adjustments%252C%2520ensuring%250Asimplicity%2520and%2520practicality%2520in%2520deployment.%2520Additionally%252C%2520by%2520reducing%2520switching%250Aactivity%2520within%2520the%2520MAC%2520units%252C%2520HALO%2520effectively%2520lowers%2520energy%2520consumption.%250AEvaluations%2520on%2520accelerators%2520such%2520as%2520Tensor%2520Processing%2520Units%2520%2528TPUs%2529%2520and%2520Graphics%250AProcessing%2520Units%2520%2528GPUs%2529%2520demonstrate%2520that%2520HALO%2520significantly%2520enhances%2520inference%250Aefficiency%252C%2520achieving%2520average%2520performance%2520improvements%2520of%2520270%2525%2520and%2520energy%250Asavings%2520of%252051%2525%2520over%2520baseline%2520quantization%2520methods%252C%2520all%2520with%2520minimal%2520impact%2520on%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19662v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HALO%3A%20Hardware-aware%20quantization%20with%20low%20critical-path-delay%20weights%0A%20%20for%20LLM%20acceleration&entry.906535625=Rohan%20Juneja%20and%20Shivam%20Aggarwal%20and%20Safeen%20Huda%20and%20Tulika%20Mitra%20and%20Li-Shiuan%20Peh&entry.1292438233=%20%20Quantization%20is%20critical%20for%20efficiently%20deploying%20large%20language%20models%0A%28LLMs%29.%20Yet%20conventional%20methods%20remain%20hardware-agnostic%2C%20limited%20to%20bit-width%0Aconstraints%2C%20and%20do%20not%20account%20for%20intrinsic%20circuit%20characteristics%20such%20as%0Athe%20timing%20behaviors%20and%20energy%20profiles%20of%20Multiply-Accumulate%20%28MAC%29%20units.%0AThis%20disconnect%20from%20circuit-level%20behavior%20limits%20the%20ability%20to%20exploit%0Aavailable%20timing%20margins%20and%20energy-saving%20opportunities%2C%20reducing%20the%20overall%0Aefficiency%20of%20deployment%20on%20modern%20accelerators.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20HALO%2C%20a%20versatile%20framework%20for%0AHardware-Aware%20Post-Training%20Quantization%20%28PTQ%29.%20Unlike%20traditional%20methods%2C%0AHALO%20explicitly%20incorporates%20detailed%20hardware%20characteristics%2C%20including%0Acritical-path%20timing%20and%20power%20consumption%2C%20into%20its%20quantization%20approach.%0AHALO%20strategically%20selects%20weights%20with%20low%20critical-path-delays%20enabling%0Ahigher%20operational%20frequencies%20and%20dynamic%20frequency%20scaling%20without%20disrupting%0Athe%20architecture%27s%20dataflow.%20Remarkably%2C%20HALO%20achieves%20these%20improvements%20with%0Aonly%20a%20few%20dynamic%20voltage%20and%20frequency%20scaling%20%28DVFS%29%20adjustments%2C%20ensuring%0Asimplicity%20and%20practicality%20in%20deployment.%20Additionally%2C%20by%20reducing%20switching%0Aactivity%20within%20the%20MAC%20units%2C%20HALO%20effectively%20lowers%20energy%20consumption.%0AEvaluations%20on%20accelerators%20such%20as%20Tensor%20Processing%20Units%20%28TPUs%29%20and%20Graphics%0AProcessing%20Units%20%28GPUs%29%20demonstrate%20that%20HALO%20significantly%20enhances%20inference%0Aefficiency%2C%20achieving%20average%20performance%20improvements%20of%20270%25%20and%20energy%0Asavings%20of%2051%25%20over%20baseline%20quantization%20methods%2C%20all%20with%20minimal%20impact%20on%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19662v2&entry.124074799=Read"},
{"title": "Can We Govern the Agent-to-Agent Economy?", "author": "Tomer Jordi Chaffer", "abstract": "  Current approaches to AI governance often fall short in anticipating a future\nwhere AI agents manage critical tasks, such as financial operations,\nadministrative functions, and beyond. While cryptocurrencies could serve as the\nfoundation for monetizing value exchange in a collaboration and delegation\ndynamic among AI agents, a critical question remains: how can humans ensure\nmeaningful oversight and control as a future economy of AI agents scales and\nevolves? In this philosophical exploration, we highlight emerging concepts in\nthe industry to inform research and development efforts in anticipation of a\nfuture decentralized agentic economy.\n", "link": "http://arxiv.org/abs/2501.16606v2", "date": "2025-04-25", "relevancy": 1.8592, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4818}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4656}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20We%20Govern%20the%20Agent-to-Agent%20Economy%3F&body=Title%3A%20Can%20We%20Govern%20the%20Agent-to-Agent%20Economy%3F%0AAuthor%3A%20Tomer%20Jordi%20Chaffer%0AAbstract%3A%20%20%20Current%20approaches%20to%20AI%20governance%20often%20fall%20short%20in%20anticipating%20a%20future%0Awhere%20AI%20agents%20manage%20critical%20tasks%2C%20such%20as%20financial%20operations%2C%0Aadministrative%20functions%2C%20and%20beyond.%20While%20cryptocurrencies%20could%20serve%20as%20the%0Afoundation%20for%20monetizing%20value%20exchange%20in%20a%20collaboration%20and%20delegation%0Adynamic%20among%20AI%20agents%2C%20a%20critical%20question%20remains%3A%20how%20can%20humans%20ensure%0Ameaningful%20oversight%20and%20control%20as%20a%20future%20economy%20of%20AI%20agents%20scales%20and%0Aevolves%3F%20In%20this%20philosophical%20exploration%2C%20we%20highlight%20emerging%20concepts%20in%0Athe%20industry%20to%20inform%20research%20and%20development%20efforts%20in%20anticipation%20of%20a%0Afuture%20decentralized%20agentic%20economy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16606v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520We%2520Govern%2520the%2520Agent-to-Agent%2520Economy%253F%26entry.906535625%3DTomer%2520Jordi%2520Chaffer%26entry.1292438233%3D%2520%2520Current%2520approaches%2520to%2520AI%2520governance%2520often%2520fall%2520short%2520in%2520anticipating%2520a%2520future%250Awhere%2520AI%2520agents%2520manage%2520critical%2520tasks%252C%2520such%2520as%2520financial%2520operations%252C%250Aadministrative%2520functions%252C%2520and%2520beyond.%2520While%2520cryptocurrencies%2520could%2520serve%2520as%2520the%250Afoundation%2520for%2520monetizing%2520value%2520exchange%2520in%2520a%2520collaboration%2520and%2520delegation%250Adynamic%2520among%2520AI%2520agents%252C%2520a%2520critical%2520question%2520remains%253A%2520how%2520can%2520humans%2520ensure%250Ameaningful%2520oversight%2520and%2520control%2520as%2520a%2520future%2520economy%2520of%2520AI%2520agents%2520scales%2520and%250Aevolves%253F%2520In%2520this%2520philosophical%2520exploration%252C%2520we%2520highlight%2520emerging%2520concepts%2520in%250Athe%2520industry%2520to%2520inform%2520research%2520and%2520development%2520efforts%2520in%2520anticipation%2520of%2520a%250Afuture%2520decentralized%2520agentic%2520economy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16606v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20We%20Govern%20the%20Agent-to-Agent%20Economy%3F&entry.906535625=Tomer%20Jordi%20Chaffer&entry.1292438233=%20%20Current%20approaches%20to%20AI%20governance%20often%20fall%20short%20in%20anticipating%20a%20future%0Awhere%20AI%20agents%20manage%20critical%20tasks%2C%20such%20as%20financial%20operations%2C%0Aadministrative%20functions%2C%20and%20beyond.%20While%20cryptocurrencies%20could%20serve%20as%20the%0Afoundation%20for%20monetizing%20value%20exchange%20in%20a%20collaboration%20and%20delegation%0Adynamic%20among%20AI%20agents%2C%20a%20critical%20question%20remains%3A%20how%20can%20humans%20ensure%0Ameaningful%20oversight%20and%20control%20as%20a%20future%20economy%20of%20AI%20agents%20scales%20and%0Aevolves%3F%20In%20this%20philosophical%20exploration%2C%20we%20highlight%20emerging%20concepts%20in%0Athe%20industry%20to%20inform%20research%20and%20development%20efforts%20in%20anticipation%20of%20a%0Afuture%20decentralized%20agentic%20economy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16606v2&entry.124074799=Read"},
{"title": "Event-Based Eye Tracking. 2025 Event-based Vision Workshop", "author": "Qinyu Chen and Chang Gao and Min Liu and Daniele Perrone and Yan Ru Pei and Zuowen Wang and Zhuo Zou and Shihang Tan and Tao Han and Guorui Lu and Zhen Xu and Junyuan Ding and Ziteng Wang and Zongwei Wu and Han Han and Yuliang Wu and Jinze Chen and Wei Zhai and Yang Cao and Zheng-jun Zha and Nuwan Bandara and Thivya Kandappu and Archan Misra and Xiaopeng Lin and Hongxiang Huang and Hongwei Ren and Bojun Cheng and Hoang M. Truong and Vinh-Thuan Ly and Huy G. Tran and Thuan-Phat Nguyen and Tram T. Doan", "abstract": "  This survey serves as a review for the 2025 Event-Based Eye Tracking\nChallenge organized as part of the 2025 CVPR event-based vision workshop. This\nchallenge focuses on the task of predicting the pupil center by processing\nevent camera recorded eye movement. We review and summarize the innovative\nmethods from teams rank the top in the challenge to advance future event-based\neye tracking research. In each method, accuracy, model size, and number of\noperations are reported. In this survey, we also discuss event-based eye\ntracking from the perspective of hardware design.\n", "link": "http://arxiv.org/abs/2504.18249v1", "date": "2025-04-25", "relevancy": 1.3593, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4584}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4529}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-Based%20Eye%20Tracking.%202025%20Event-based%20Vision%20Workshop&body=Title%3A%20Event-Based%20Eye%20Tracking.%202025%20Event-based%20Vision%20Workshop%0AAuthor%3A%20Qinyu%20Chen%20and%20Chang%20Gao%20and%20Min%20Liu%20and%20Daniele%20Perrone%20and%20Yan%20Ru%20Pei%20and%20Zuowen%20Wang%20and%20Zhuo%20Zou%20and%20Shihang%20Tan%20and%20Tao%20Han%20and%20Guorui%20Lu%20and%20Zhen%20Xu%20and%20Junyuan%20Ding%20and%20Ziteng%20Wang%20and%20Zongwei%20Wu%20and%20Han%20Han%20and%20Yuliang%20Wu%20and%20Jinze%20Chen%20and%20Wei%20Zhai%20and%20Yang%20Cao%20and%20Zheng-jun%20Zha%20and%20Nuwan%20Bandara%20and%20Thivya%20Kandappu%20and%20Archan%20Misra%20and%20Xiaopeng%20Lin%20and%20Hongxiang%20Huang%20and%20Hongwei%20Ren%20and%20Bojun%20Cheng%20and%20Hoang%20M.%20Truong%20and%20Vinh-Thuan%20Ly%20and%20Huy%20G.%20Tran%20and%20Thuan-Phat%20Nguyen%20and%20Tram%20T.%20Doan%0AAbstract%3A%20%20%20This%20survey%20serves%20as%20a%20review%20for%20the%202025%20Event-Based%20Eye%20Tracking%0AChallenge%20organized%20as%20part%20of%20the%202025%20CVPR%20event-based%20vision%20workshop.%20This%0Achallenge%20focuses%20on%20the%20task%20of%20predicting%20the%20pupil%20center%20by%20processing%0Aevent%20camera%20recorded%20eye%20movement.%20We%20review%20and%20summarize%20the%20innovative%0Amethods%20from%20teams%20rank%20the%20top%20in%20the%20challenge%20to%20advance%20future%20event-based%0Aeye%20tracking%20research.%20In%20each%20method%2C%20accuracy%2C%20model%20size%2C%20and%20number%20of%0Aoperations%20are%20reported.%20In%20this%20survey%2C%20we%20also%20discuss%20event-based%20eye%0Atracking%20from%20the%20perspective%20of%20hardware%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-Based%2520Eye%2520Tracking.%25202025%2520Event-based%2520Vision%2520Workshop%26entry.906535625%3DQinyu%2520Chen%2520and%2520Chang%2520Gao%2520and%2520Min%2520Liu%2520and%2520Daniele%2520Perrone%2520and%2520Yan%2520Ru%2520Pei%2520and%2520Zuowen%2520Wang%2520and%2520Zhuo%2520Zou%2520and%2520Shihang%2520Tan%2520and%2520Tao%2520Han%2520and%2520Guorui%2520Lu%2520and%2520Zhen%2520Xu%2520and%2520Junyuan%2520Ding%2520and%2520Ziteng%2520Wang%2520and%2520Zongwei%2520Wu%2520and%2520Han%2520Han%2520and%2520Yuliang%2520Wu%2520and%2520Jinze%2520Chen%2520and%2520Wei%2520Zhai%2520and%2520Yang%2520Cao%2520and%2520Zheng-jun%2520Zha%2520and%2520Nuwan%2520Bandara%2520and%2520Thivya%2520Kandappu%2520and%2520Archan%2520Misra%2520and%2520Xiaopeng%2520Lin%2520and%2520Hongxiang%2520Huang%2520and%2520Hongwei%2520Ren%2520and%2520Bojun%2520Cheng%2520and%2520Hoang%2520M.%2520Truong%2520and%2520Vinh-Thuan%2520Ly%2520and%2520Huy%2520G.%2520Tran%2520and%2520Thuan-Phat%2520Nguyen%2520and%2520Tram%2520T.%2520Doan%26entry.1292438233%3D%2520%2520This%2520survey%2520serves%2520as%2520a%2520review%2520for%2520the%25202025%2520Event-Based%2520Eye%2520Tracking%250AChallenge%2520organized%2520as%2520part%2520of%2520the%25202025%2520CVPR%2520event-based%2520vision%2520workshop.%2520This%250Achallenge%2520focuses%2520on%2520the%2520task%2520of%2520predicting%2520the%2520pupil%2520center%2520by%2520processing%250Aevent%2520camera%2520recorded%2520eye%2520movement.%2520We%2520review%2520and%2520summarize%2520the%2520innovative%250Amethods%2520from%2520teams%2520rank%2520the%2520top%2520in%2520the%2520challenge%2520to%2520advance%2520future%2520event-based%250Aeye%2520tracking%2520research.%2520In%2520each%2520method%252C%2520accuracy%252C%2520model%2520size%252C%2520and%2520number%2520of%250Aoperations%2520are%2520reported.%2520In%2520this%2520survey%252C%2520we%2520also%2520discuss%2520event-based%2520eye%250Atracking%2520from%2520the%2520perspective%2520of%2520hardware%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-Based%20Eye%20Tracking.%202025%20Event-based%20Vision%20Workshop&entry.906535625=Qinyu%20Chen%20and%20Chang%20Gao%20and%20Min%20Liu%20and%20Daniele%20Perrone%20and%20Yan%20Ru%20Pei%20and%20Zuowen%20Wang%20and%20Zhuo%20Zou%20and%20Shihang%20Tan%20and%20Tao%20Han%20and%20Guorui%20Lu%20and%20Zhen%20Xu%20and%20Junyuan%20Ding%20and%20Ziteng%20Wang%20and%20Zongwei%20Wu%20and%20Han%20Han%20and%20Yuliang%20Wu%20and%20Jinze%20Chen%20and%20Wei%20Zhai%20and%20Yang%20Cao%20and%20Zheng-jun%20Zha%20and%20Nuwan%20Bandara%20and%20Thivya%20Kandappu%20and%20Archan%20Misra%20and%20Xiaopeng%20Lin%20and%20Hongxiang%20Huang%20and%20Hongwei%20Ren%20and%20Bojun%20Cheng%20and%20Hoang%20M.%20Truong%20and%20Vinh-Thuan%20Ly%20and%20Huy%20G.%20Tran%20and%20Thuan-Phat%20Nguyen%20and%20Tram%20T.%20Doan&entry.1292438233=%20%20This%20survey%20serves%20as%20a%20review%20for%20the%202025%20Event-Based%20Eye%20Tracking%0AChallenge%20organized%20as%20part%20of%20the%202025%20CVPR%20event-based%20vision%20workshop.%20This%0Achallenge%20focuses%20on%20the%20task%20of%20predicting%20the%20pupil%20center%20by%20processing%0Aevent%20camera%20recorded%20eye%20movement.%20We%20review%20and%20summarize%20the%20innovative%0Amethods%20from%20teams%20rank%20the%20top%20in%20the%20challenge%20to%20advance%20future%20event-based%0Aeye%20tracking%20research.%20In%20each%20method%2C%20accuracy%2C%20model%20size%2C%20and%20number%20of%0Aoperations%20are%20reported.%20In%20this%20survey%2C%20we%20also%20discuss%20event-based%20eye%0Atracking%20from%20the%20perspective%20of%20hardware%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18249v1&entry.124074799=Read"},
{"title": "An Improved ResNet50 Model for Predicting Pavement Condition Index (PCI)\n  Directly from Pavement Images", "author": "Andrews Danyo and Anthony Dontoh and Armstrong Aboah", "abstract": "  Accurately predicting the Pavement Condition Index (PCI), a measure of\nroadway conditions, from pavement images is crucial for infrastructure\nmaintenance. This study proposes an enhanced version of the Residual Network\n(ResNet50) architecture, integrated with a Convolutional Block Attention Module\n(CBAM), to predict PCI directly from pavement images without additional\nannotations. By incorporating CBAM, the model autonomously prioritizes critical\nfeatures within the images, improving prediction accuracy. Compared to the\noriginal baseline ResNet50 and DenseNet161 architectures, the enhanced\nResNet50-CBAM model achieved a significantly lower mean absolute percentage\nerror (MAPE) of 58.16%, compared to the baseline models that achieved 70.76%\nand 65.48% respectively. These results highlight the potential of using\nattention mechanisms to refine feature extraction, ultimately enabling more\naccurate and efficient assessments of pavement conditions. This study\nemphasizes the importance of targeted feature refinement in advancing automated\npavement analysis through attention mechanisms.\n", "link": "http://arxiv.org/abs/2504.18490v1", "date": "2025-04-25", "relevancy": 1.4274, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4777}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4755}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Improved%20ResNet50%20Model%20for%20Predicting%20Pavement%20Condition%20Index%20%28PCI%29%0A%20%20Directly%20from%20Pavement%20Images&body=Title%3A%20An%20Improved%20ResNet50%20Model%20for%20Predicting%20Pavement%20Condition%20Index%20%28PCI%29%0A%20%20Directly%20from%20Pavement%20Images%0AAuthor%3A%20Andrews%20Danyo%20and%20Anthony%20Dontoh%20and%20Armstrong%20Aboah%0AAbstract%3A%20%20%20Accurately%20predicting%20the%20Pavement%20Condition%20Index%20%28PCI%29%2C%20a%20measure%20of%0Aroadway%20conditions%2C%20from%20pavement%20images%20is%20crucial%20for%20infrastructure%0Amaintenance.%20This%20study%20proposes%20an%20enhanced%20version%20of%20the%20Residual%20Network%0A%28ResNet50%29%20architecture%2C%20integrated%20with%20a%20Convolutional%20Block%20Attention%20Module%0A%28CBAM%29%2C%20to%20predict%20PCI%20directly%20from%20pavement%20images%20without%20additional%0Aannotations.%20By%20incorporating%20CBAM%2C%20the%20model%20autonomously%20prioritizes%20critical%0Afeatures%20within%20the%20images%2C%20improving%20prediction%20accuracy.%20Compared%20to%20the%0Aoriginal%20baseline%20ResNet50%20and%20DenseNet161%20architectures%2C%20the%20enhanced%0AResNet50-CBAM%20model%20achieved%20a%20significantly%20lower%20mean%20absolute%20percentage%0Aerror%20%28MAPE%29%20of%2058.16%25%2C%20compared%20to%20the%20baseline%20models%20that%20achieved%2070.76%25%0Aand%2065.48%25%20respectively.%20These%20results%20highlight%20the%20potential%20of%20using%0Aattention%20mechanisms%20to%20refine%20feature%20extraction%2C%20ultimately%20enabling%20more%0Aaccurate%20and%20efficient%20assessments%20of%20pavement%20conditions.%20This%20study%0Aemphasizes%20the%20importance%20of%20targeted%20feature%20refinement%20in%20advancing%20automated%0Apavement%20analysis%20through%20attention%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Improved%2520ResNet50%2520Model%2520for%2520Predicting%2520Pavement%2520Condition%2520Index%2520%2528PCI%2529%250A%2520%2520Directly%2520from%2520Pavement%2520Images%26entry.906535625%3DAndrews%2520Danyo%2520and%2520Anthony%2520Dontoh%2520and%2520Armstrong%2520Aboah%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520the%2520Pavement%2520Condition%2520Index%2520%2528PCI%2529%252C%2520a%2520measure%2520of%250Aroadway%2520conditions%252C%2520from%2520pavement%2520images%2520is%2520crucial%2520for%2520infrastructure%250Amaintenance.%2520This%2520study%2520proposes%2520an%2520enhanced%2520version%2520of%2520the%2520Residual%2520Network%250A%2528ResNet50%2529%2520architecture%252C%2520integrated%2520with%2520a%2520Convolutional%2520Block%2520Attention%2520Module%250A%2528CBAM%2529%252C%2520to%2520predict%2520PCI%2520directly%2520from%2520pavement%2520images%2520without%2520additional%250Aannotations.%2520By%2520incorporating%2520CBAM%252C%2520the%2520model%2520autonomously%2520prioritizes%2520critical%250Afeatures%2520within%2520the%2520images%252C%2520improving%2520prediction%2520accuracy.%2520Compared%2520to%2520the%250Aoriginal%2520baseline%2520ResNet50%2520and%2520DenseNet161%2520architectures%252C%2520the%2520enhanced%250AResNet50-CBAM%2520model%2520achieved%2520a%2520significantly%2520lower%2520mean%2520absolute%2520percentage%250Aerror%2520%2528MAPE%2529%2520of%252058.16%2525%252C%2520compared%2520to%2520the%2520baseline%2520models%2520that%2520achieved%252070.76%2525%250Aand%252065.48%2525%2520respectively.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520using%250Aattention%2520mechanisms%2520to%2520refine%2520feature%2520extraction%252C%2520ultimately%2520enabling%2520more%250Aaccurate%2520and%2520efficient%2520assessments%2520of%2520pavement%2520conditions.%2520This%2520study%250Aemphasizes%2520the%2520importance%2520of%2520targeted%2520feature%2520refinement%2520in%2520advancing%2520automated%250Apavement%2520analysis%2520through%2520attention%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Improved%20ResNet50%20Model%20for%20Predicting%20Pavement%20Condition%20Index%20%28PCI%29%0A%20%20Directly%20from%20Pavement%20Images&entry.906535625=Andrews%20Danyo%20and%20Anthony%20Dontoh%20and%20Armstrong%20Aboah&entry.1292438233=%20%20Accurately%20predicting%20the%20Pavement%20Condition%20Index%20%28PCI%29%2C%20a%20measure%20of%0Aroadway%20conditions%2C%20from%20pavement%20images%20is%20crucial%20for%20infrastructure%0Amaintenance.%20This%20study%20proposes%20an%20enhanced%20version%20of%20the%20Residual%20Network%0A%28ResNet50%29%20architecture%2C%20integrated%20with%20a%20Convolutional%20Block%20Attention%20Module%0A%28CBAM%29%2C%20to%20predict%20PCI%20directly%20from%20pavement%20images%20without%20additional%0Aannotations.%20By%20incorporating%20CBAM%2C%20the%20model%20autonomously%20prioritizes%20critical%0Afeatures%20within%20the%20images%2C%20improving%20prediction%20accuracy.%20Compared%20to%20the%0Aoriginal%20baseline%20ResNet50%20and%20DenseNet161%20architectures%2C%20the%20enhanced%0AResNet50-CBAM%20model%20achieved%20a%20significantly%20lower%20mean%20absolute%20percentage%0Aerror%20%28MAPE%29%20of%2058.16%25%2C%20compared%20to%20the%20baseline%20models%20that%20achieved%2070.76%25%0Aand%2065.48%25%20respectively.%20These%20results%20highlight%20the%20potential%20of%20using%0Aattention%20mechanisms%20to%20refine%20feature%20extraction%2C%20ultimately%20enabling%20more%0Aaccurate%20and%20efficient%20assessments%20of%20pavement%20conditions.%20This%20study%0Aemphasizes%20the%20importance%20of%20targeted%20feature%20refinement%20in%20advancing%20automated%0Apavement%20analysis%20through%20attention%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18490v1&entry.124074799=Read"},
{"title": "Tensor Networks for Explainable Machine Learning in Cybersecurity", "author": "Borja Aizpurua and Samuel Palmer and Roman Orus", "abstract": "  In this paper we show how tensor networks help in developing explainability\nof machine learning algorithms. Specifically, we develop an unsupervised\nclustering algorithm based on Matrix Product States (MPS) and apply it in the\ncontext of a real use-case of adversary-generated threat intelligence. Our\ninvestigation proves that MPS rival traditional deep learning models such as\nautoencoders and GANs in terms of performance, while providing much richer\nmodel interpretability. Our approach naturally facilitates the extraction of\nfeature-wise probabilities, Von Neumann Entropy, and mutual information,\noffering a compelling narrative for classification of anomalies and fostering\nan unprecedented level of transparency and interpretability, something\nfundamental to understand the rationale behind artificial intelligence\ndecisions.\n", "link": "http://arxiv.org/abs/2401.00867v4", "date": "2025-04-25", "relevancy": 1.297, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4496}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4283}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensor%20Networks%20for%20Explainable%20Machine%20Learning%20in%20Cybersecurity&body=Title%3A%20Tensor%20Networks%20for%20Explainable%20Machine%20Learning%20in%20Cybersecurity%0AAuthor%3A%20Borja%20Aizpurua%20and%20Samuel%20Palmer%20and%20Roman%20Orus%0AAbstract%3A%20%20%20In%20this%20paper%20we%20show%20how%20tensor%20networks%20help%20in%20developing%20explainability%0Aof%20machine%20learning%20algorithms.%20Specifically%2C%20we%20develop%20an%20unsupervised%0Aclustering%20algorithm%20based%20on%20Matrix%20Product%20States%20%28MPS%29%20and%20apply%20it%20in%20the%0Acontext%20of%20a%20real%20use-case%20of%20adversary-generated%20threat%20intelligence.%20Our%0Ainvestigation%20proves%20that%20MPS%20rival%20traditional%20deep%20learning%20models%20such%20as%0Aautoencoders%20and%20GANs%20in%20terms%20of%20performance%2C%20while%20providing%20much%20richer%0Amodel%20interpretability.%20Our%20approach%20naturally%20facilitates%20the%20extraction%20of%0Afeature-wise%20probabilities%2C%20Von%20Neumann%20Entropy%2C%20and%20mutual%20information%2C%0Aoffering%20a%20compelling%20narrative%20for%20classification%20of%20anomalies%20and%20fostering%0Aan%20unprecedented%20level%20of%20transparency%20and%20interpretability%2C%20something%0Afundamental%20to%20understand%20the%20rationale%20behind%20artificial%20intelligence%0Adecisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00867v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensor%2520Networks%2520for%2520Explainable%2520Machine%2520Learning%2520in%2520Cybersecurity%26entry.906535625%3DBorja%2520Aizpurua%2520and%2520Samuel%2520Palmer%2520and%2520Roman%2520Orus%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520show%2520how%2520tensor%2520networks%2520help%2520in%2520developing%2520explainability%250Aof%2520machine%2520learning%2520algorithms.%2520Specifically%252C%2520we%2520develop%2520an%2520unsupervised%250Aclustering%2520algorithm%2520based%2520on%2520Matrix%2520Product%2520States%2520%2528MPS%2529%2520and%2520apply%2520it%2520in%2520the%250Acontext%2520of%2520a%2520real%2520use-case%2520of%2520adversary-generated%2520threat%2520intelligence.%2520Our%250Ainvestigation%2520proves%2520that%2520MPS%2520rival%2520traditional%2520deep%2520learning%2520models%2520such%2520as%250Aautoencoders%2520and%2520GANs%2520in%2520terms%2520of%2520performance%252C%2520while%2520providing%2520much%2520richer%250Amodel%2520interpretability.%2520Our%2520approach%2520naturally%2520facilitates%2520the%2520extraction%2520of%250Afeature-wise%2520probabilities%252C%2520Von%2520Neumann%2520Entropy%252C%2520and%2520mutual%2520information%252C%250Aoffering%2520a%2520compelling%2520narrative%2520for%2520classification%2520of%2520anomalies%2520and%2520fostering%250Aan%2520unprecedented%2520level%2520of%2520transparency%2520and%2520interpretability%252C%2520something%250Afundamental%2520to%2520understand%2520the%2520rationale%2520behind%2520artificial%2520intelligence%250Adecisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00867v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensor%20Networks%20for%20Explainable%20Machine%20Learning%20in%20Cybersecurity&entry.906535625=Borja%20Aizpurua%20and%20Samuel%20Palmer%20and%20Roman%20Orus&entry.1292438233=%20%20In%20this%20paper%20we%20show%20how%20tensor%20networks%20help%20in%20developing%20explainability%0Aof%20machine%20learning%20algorithms.%20Specifically%2C%20we%20develop%20an%20unsupervised%0Aclustering%20algorithm%20based%20on%20Matrix%20Product%20States%20%28MPS%29%20and%20apply%20it%20in%20the%0Acontext%20of%20a%20real%20use-case%20of%20adversary-generated%20threat%20intelligence.%20Our%0Ainvestigation%20proves%20that%20MPS%20rival%20traditional%20deep%20learning%20models%20such%20as%0Aautoencoders%20and%20GANs%20in%20terms%20of%20performance%2C%20while%20providing%20much%20richer%0Amodel%20interpretability.%20Our%20approach%20naturally%20facilitates%20the%20extraction%20of%0Afeature-wise%20probabilities%2C%20Von%20Neumann%20Entropy%2C%20and%20mutual%20information%2C%0Aoffering%20a%20compelling%20narrative%20for%20classification%20of%20anomalies%20and%20fostering%0Aan%20unprecedented%20level%20of%20transparency%20and%20interpretability%2C%20something%0Afundamental%20to%20understand%20the%20rationale%20behind%20artificial%20intelligence%0Adecisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00867v4&entry.124074799=Read"},
{"title": "Transforming Hyperspectral Images Into Chemical Maps: An End-to-End Deep\n  Learning Approach", "author": "Ole-Christian Galbo Engstr\u00f8m and Michela Albano-Gaglio and Erik Schou Dreier and Yamine Bouzembrak and Maria Font-i-Furnols and Puneet Mishra and Kim Steenstrup Pedersen", "abstract": "  Current approaches to chemical map generation from hyperspectral images are\nbased on models such as partial least squares (PLS) regression, generating\npixel-wise predictions that do not consider spatial context and suffer from a\nhigh degree of noise. This study proposes an end-to-end deep learning approach\nusing a modified version of U-Net and a custom loss function to directly obtain\nchemical maps from hyperspectral images, skipping all intermediate steps\nrequired for traditional pixel-wise analysis. We compare the U-Net with the\ntraditional PLS regression on a real dataset of pork belly samples with\nassociated mean fat reference values. The U-Net obtains a test set root mean\nsquared error of between 9% and 13% lower than that of PLS regression on the\ntask of mean fat prediction. At the same time, U-Net generates fine detail\nchemical maps where 99.91% of the variance is spatially correlated. Conversely,\nonly 2.53% of the variance in the PLS-generated chemical maps is spatially\ncorrelated, indicating that each pixel-wise prediction is largely independent\nof neighboring pixels. Additionally, while the PLS-generated chemical maps\ncontain predictions far beyond the physically possible range of 0-100%, U-Net\nlearns to stay inside this range. Thus, the findings of this study indicate\nthat U-Net is superior to PLS for chemical map generation.\n", "link": "http://arxiv.org/abs/2504.14131v2", "date": "2025-04-25", "relevancy": 1.5964, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5526}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5073}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transforming%20Hyperspectral%20Images%20Into%20Chemical%20Maps%3A%20An%20End-to-End%20Deep%0A%20%20Learning%20Approach&body=Title%3A%20Transforming%20Hyperspectral%20Images%20Into%20Chemical%20Maps%3A%20An%20End-to-End%20Deep%0A%20%20Learning%20Approach%0AAuthor%3A%20Ole-Christian%20Galbo%20Engstr%C3%B8m%20and%20Michela%20Albano-Gaglio%20and%20Erik%20Schou%20Dreier%20and%20Yamine%20Bouzembrak%20and%20Maria%20Font-i-Furnols%20and%20Puneet%20Mishra%20and%20Kim%20Steenstrup%20Pedersen%0AAbstract%3A%20%20%20Current%20approaches%20to%20chemical%20map%20generation%20from%20hyperspectral%20images%20are%0Abased%20on%20models%20such%20as%20partial%20least%20squares%20%28PLS%29%20regression%2C%20generating%0Apixel-wise%20predictions%20that%20do%20not%20consider%20spatial%20context%20and%20suffer%20from%20a%0Ahigh%20degree%20of%20noise.%20This%20study%20proposes%20an%20end-to-end%20deep%20learning%20approach%0Ausing%20a%20modified%20version%20of%20U-Net%20and%20a%20custom%20loss%20function%20to%20directly%20obtain%0Achemical%20maps%20from%20hyperspectral%20images%2C%20skipping%20all%20intermediate%20steps%0Arequired%20for%20traditional%20pixel-wise%20analysis.%20We%20compare%20the%20U-Net%20with%20the%0Atraditional%20PLS%20regression%20on%20a%20real%20dataset%20of%20pork%20belly%20samples%20with%0Aassociated%20mean%20fat%20reference%20values.%20The%20U-Net%20obtains%20a%20test%20set%20root%20mean%0Asquared%20error%20of%20between%209%25%20and%2013%25%20lower%20than%20that%20of%20PLS%20regression%20on%20the%0Atask%20of%20mean%20fat%20prediction.%20At%20the%20same%20time%2C%20U-Net%20generates%20fine%20detail%0Achemical%20maps%20where%2099.91%25%20of%20the%20variance%20is%20spatially%20correlated.%20Conversely%2C%0Aonly%202.53%25%20of%20the%20variance%20in%20the%20PLS-generated%20chemical%20maps%20is%20spatially%0Acorrelated%2C%20indicating%20that%20each%20pixel-wise%20prediction%20is%20largely%20independent%0Aof%20neighboring%20pixels.%20Additionally%2C%20while%20the%20PLS-generated%20chemical%20maps%0Acontain%20predictions%20far%20beyond%20the%20physically%20possible%20range%20of%200-100%25%2C%20U-Net%0Alearns%20to%20stay%20inside%20this%20range.%20Thus%2C%20the%20findings%20of%20this%20study%20indicate%0Athat%20U-Net%20is%20superior%20to%20PLS%20for%20chemical%20map%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransforming%2520Hyperspectral%2520Images%2520Into%2520Chemical%2520Maps%253A%2520An%2520End-to-End%2520Deep%250A%2520%2520Learning%2520Approach%26entry.906535625%3DOle-Christian%2520Galbo%2520Engstr%25C3%25B8m%2520and%2520Michela%2520Albano-Gaglio%2520and%2520Erik%2520Schou%2520Dreier%2520and%2520Yamine%2520Bouzembrak%2520and%2520Maria%2520Font-i-Furnols%2520and%2520Puneet%2520Mishra%2520and%2520Kim%2520Steenstrup%2520Pedersen%26entry.1292438233%3D%2520%2520Current%2520approaches%2520to%2520chemical%2520map%2520generation%2520from%2520hyperspectral%2520images%2520are%250Abased%2520on%2520models%2520such%2520as%2520partial%2520least%2520squares%2520%2528PLS%2529%2520regression%252C%2520generating%250Apixel-wise%2520predictions%2520that%2520do%2520not%2520consider%2520spatial%2520context%2520and%2520suffer%2520from%2520a%250Ahigh%2520degree%2520of%2520noise.%2520This%2520study%2520proposes%2520an%2520end-to-end%2520deep%2520learning%2520approach%250Ausing%2520a%2520modified%2520version%2520of%2520U-Net%2520and%2520a%2520custom%2520loss%2520function%2520to%2520directly%2520obtain%250Achemical%2520maps%2520from%2520hyperspectral%2520images%252C%2520skipping%2520all%2520intermediate%2520steps%250Arequired%2520for%2520traditional%2520pixel-wise%2520analysis.%2520We%2520compare%2520the%2520U-Net%2520with%2520the%250Atraditional%2520PLS%2520regression%2520on%2520a%2520real%2520dataset%2520of%2520pork%2520belly%2520samples%2520with%250Aassociated%2520mean%2520fat%2520reference%2520values.%2520The%2520U-Net%2520obtains%2520a%2520test%2520set%2520root%2520mean%250Asquared%2520error%2520of%2520between%25209%2525%2520and%252013%2525%2520lower%2520than%2520that%2520of%2520PLS%2520regression%2520on%2520the%250Atask%2520of%2520mean%2520fat%2520prediction.%2520At%2520the%2520same%2520time%252C%2520U-Net%2520generates%2520fine%2520detail%250Achemical%2520maps%2520where%252099.91%2525%2520of%2520the%2520variance%2520is%2520spatially%2520correlated.%2520Conversely%252C%250Aonly%25202.53%2525%2520of%2520the%2520variance%2520in%2520the%2520PLS-generated%2520chemical%2520maps%2520is%2520spatially%250Acorrelated%252C%2520indicating%2520that%2520each%2520pixel-wise%2520prediction%2520is%2520largely%2520independent%250Aof%2520neighboring%2520pixels.%2520Additionally%252C%2520while%2520the%2520PLS-generated%2520chemical%2520maps%250Acontain%2520predictions%2520far%2520beyond%2520the%2520physically%2520possible%2520range%2520of%25200-100%2525%252C%2520U-Net%250Alearns%2520to%2520stay%2520inside%2520this%2520range.%2520Thus%252C%2520the%2520findings%2520of%2520this%2520study%2520indicate%250Athat%2520U-Net%2520is%2520superior%2520to%2520PLS%2520for%2520chemical%2520map%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transforming%20Hyperspectral%20Images%20Into%20Chemical%20Maps%3A%20An%20End-to-End%20Deep%0A%20%20Learning%20Approach&entry.906535625=Ole-Christian%20Galbo%20Engstr%C3%B8m%20and%20Michela%20Albano-Gaglio%20and%20Erik%20Schou%20Dreier%20and%20Yamine%20Bouzembrak%20and%20Maria%20Font-i-Furnols%20and%20Puneet%20Mishra%20and%20Kim%20Steenstrup%20Pedersen&entry.1292438233=%20%20Current%20approaches%20to%20chemical%20map%20generation%20from%20hyperspectral%20images%20are%0Abased%20on%20models%20such%20as%20partial%20least%20squares%20%28PLS%29%20regression%2C%20generating%0Apixel-wise%20predictions%20that%20do%20not%20consider%20spatial%20context%20and%20suffer%20from%20a%0Ahigh%20degree%20of%20noise.%20This%20study%20proposes%20an%20end-to-end%20deep%20learning%20approach%0Ausing%20a%20modified%20version%20of%20U-Net%20and%20a%20custom%20loss%20function%20to%20directly%20obtain%0Achemical%20maps%20from%20hyperspectral%20images%2C%20skipping%20all%20intermediate%20steps%0Arequired%20for%20traditional%20pixel-wise%20analysis.%20We%20compare%20the%20U-Net%20with%20the%0Atraditional%20PLS%20regression%20on%20a%20real%20dataset%20of%20pork%20belly%20samples%20with%0Aassociated%20mean%20fat%20reference%20values.%20The%20U-Net%20obtains%20a%20test%20set%20root%20mean%0Asquared%20error%20of%20between%209%25%20and%2013%25%20lower%20than%20that%20of%20PLS%20regression%20on%20the%0Atask%20of%20mean%20fat%20prediction.%20At%20the%20same%20time%2C%20U-Net%20generates%20fine%20detail%0Achemical%20maps%20where%2099.91%25%20of%20the%20variance%20is%20spatially%20correlated.%20Conversely%2C%0Aonly%202.53%25%20of%20the%20variance%20in%20the%20PLS-generated%20chemical%20maps%20is%20spatially%0Acorrelated%2C%20indicating%20that%20each%20pixel-wise%20prediction%20is%20largely%20independent%0Aof%20neighboring%20pixels.%20Additionally%2C%20while%20the%20PLS-generated%20chemical%20maps%0Acontain%20predictions%20far%20beyond%20the%20physically%20possible%20range%20of%200-100%25%2C%20U-Net%0Alearns%20to%20stay%20inside%20this%20range.%20Thus%2C%20the%20findings%20of%20this%20study%20indicate%0Athat%20U-Net%20is%20superior%20to%20PLS%20for%20chemical%20map%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14131v2&entry.124074799=Read"},
{"title": "PHEATPRUNER: Interpretable Data-centric Feature Selection for\n  Multivariate Time Series Classification through Persistent Homology", "author": "Anh-Duy Pham and Olivier Basole Kashongwe and Martin Atzmueller and Tim R\u00f6mer", "abstract": "  Balancing performance and interpretability in multivariate time series\nclassification is a significant challenge due to data complexity and high\ndimensionality. This paper introduces PHeatPruner, a method integrating\npersistent homology and sheaf theory to address these challenges. Persistent\nhomology facilitates the pruning of up to 45% of the applied variables while\nmaintaining or enhancing the accuracy of models such as Random Forest,\nCatBoost, XGBoost, and LightGBM, all without depending on posterior\nprobabilities or supervised optimization algorithms. Concurrently, sheaf theory\ncontributes explanatory vectors that provide deeper insights into the data's\nstructural nuances. The approach was validated using the UEA Archive and a\nmastitis detection dataset for dairy cows. The results demonstrate that\nPHeatPruner effectively preserves model accuracy. Furthermore, our results\nhighlight PHeatPruner's key features, i.e. simplifying complex data and\noffering actionable insights without increasing processing time or complexity.\nThis method bridges the gap between complexity reduction and interpretability,\nsuggesting promising applications in various fields.\n", "link": "http://arxiv.org/abs/2504.18329v1", "date": "2025-04-25", "relevancy": 1.8171, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4686}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4573}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PHEATPRUNER%3A%20Interpretable%20Data-centric%20Feature%20Selection%20for%0A%20%20Multivariate%20Time%20Series%20Classification%20through%20Persistent%20Homology&body=Title%3A%20PHEATPRUNER%3A%20Interpretable%20Data-centric%20Feature%20Selection%20for%0A%20%20Multivariate%20Time%20Series%20Classification%20through%20Persistent%20Homology%0AAuthor%3A%20Anh-Duy%20Pham%20and%20Olivier%20Basole%20Kashongwe%20and%20Martin%20Atzmueller%20and%20Tim%20R%C3%B6mer%0AAbstract%3A%20%20%20Balancing%20performance%20and%20interpretability%20in%20multivariate%20time%20series%0Aclassification%20is%20a%20significant%20challenge%20due%20to%20data%20complexity%20and%20high%0Adimensionality.%20This%20paper%20introduces%20PHeatPruner%2C%20a%20method%20integrating%0Apersistent%20homology%20and%20sheaf%20theory%20to%20address%20these%20challenges.%20Persistent%0Ahomology%20facilitates%20the%20pruning%20of%20up%20to%2045%25%20of%20the%20applied%20variables%20while%0Amaintaining%20or%20enhancing%20the%20accuracy%20of%20models%20such%20as%20Random%20Forest%2C%0ACatBoost%2C%20XGBoost%2C%20and%20LightGBM%2C%20all%20without%20depending%20on%20posterior%0Aprobabilities%20or%20supervised%20optimization%20algorithms.%20Concurrently%2C%20sheaf%20theory%0Acontributes%20explanatory%20vectors%20that%20provide%20deeper%20insights%20into%20the%20data%27s%0Astructural%20nuances.%20The%20approach%20was%20validated%20using%20the%20UEA%20Archive%20and%20a%0Amastitis%20detection%20dataset%20for%20dairy%20cows.%20The%20results%20demonstrate%20that%0APHeatPruner%20effectively%20preserves%20model%20accuracy.%20Furthermore%2C%20our%20results%0Ahighlight%20PHeatPruner%27s%20key%20features%2C%20i.e.%20simplifying%20complex%20data%20and%0Aoffering%20actionable%20insights%20without%20increasing%20processing%20time%20or%20complexity.%0AThis%20method%20bridges%20the%20gap%20between%20complexity%20reduction%20and%20interpretability%2C%0Asuggesting%20promising%20applications%20in%20various%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPHEATPRUNER%253A%2520Interpretable%2520Data-centric%2520Feature%2520Selection%2520for%250A%2520%2520Multivariate%2520Time%2520Series%2520Classification%2520through%2520Persistent%2520Homology%26entry.906535625%3DAnh-Duy%2520Pham%2520and%2520Olivier%2520Basole%2520Kashongwe%2520and%2520Martin%2520Atzmueller%2520and%2520Tim%2520R%25C3%25B6mer%26entry.1292438233%3D%2520%2520Balancing%2520performance%2520and%2520interpretability%2520in%2520multivariate%2520time%2520series%250Aclassification%2520is%2520a%2520significant%2520challenge%2520due%2520to%2520data%2520complexity%2520and%2520high%250Adimensionality.%2520This%2520paper%2520introduces%2520PHeatPruner%252C%2520a%2520method%2520integrating%250Apersistent%2520homology%2520and%2520sheaf%2520theory%2520to%2520address%2520these%2520challenges.%2520Persistent%250Ahomology%2520facilitates%2520the%2520pruning%2520of%2520up%2520to%252045%2525%2520of%2520the%2520applied%2520variables%2520while%250Amaintaining%2520or%2520enhancing%2520the%2520accuracy%2520of%2520models%2520such%2520as%2520Random%2520Forest%252C%250ACatBoost%252C%2520XGBoost%252C%2520and%2520LightGBM%252C%2520all%2520without%2520depending%2520on%2520posterior%250Aprobabilities%2520or%2520supervised%2520optimization%2520algorithms.%2520Concurrently%252C%2520sheaf%2520theory%250Acontributes%2520explanatory%2520vectors%2520that%2520provide%2520deeper%2520insights%2520into%2520the%2520data%2527s%250Astructural%2520nuances.%2520The%2520approach%2520was%2520validated%2520using%2520the%2520UEA%2520Archive%2520and%2520a%250Amastitis%2520detection%2520dataset%2520for%2520dairy%2520cows.%2520The%2520results%2520demonstrate%2520that%250APHeatPruner%2520effectively%2520preserves%2520model%2520accuracy.%2520Furthermore%252C%2520our%2520results%250Ahighlight%2520PHeatPruner%2527s%2520key%2520features%252C%2520i.e.%2520simplifying%2520complex%2520data%2520and%250Aoffering%2520actionable%2520insights%2520without%2520increasing%2520processing%2520time%2520or%2520complexity.%250AThis%2520method%2520bridges%2520the%2520gap%2520between%2520complexity%2520reduction%2520and%2520interpretability%252C%250Asuggesting%2520promising%2520applications%2520in%2520various%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PHEATPRUNER%3A%20Interpretable%20Data-centric%20Feature%20Selection%20for%0A%20%20Multivariate%20Time%20Series%20Classification%20through%20Persistent%20Homology&entry.906535625=Anh-Duy%20Pham%20and%20Olivier%20Basole%20Kashongwe%20and%20Martin%20Atzmueller%20and%20Tim%20R%C3%B6mer&entry.1292438233=%20%20Balancing%20performance%20and%20interpretability%20in%20multivariate%20time%20series%0Aclassification%20is%20a%20significant%20challenge%20due%20to%20data%20complexity%20and%20high%0Adimensionality.%20This%20paper%20introduces%20PHeatPruner%2C%20a%20method%20integrating%0Apersistent%20homology%20and%20sheaf%20theory%20to%20address%20these%20challenges.%20Persistent%0Ahomology%20facilitates%20the%20pruning%20of%20up%20to%2045%25%20of%20the%20applied%20variables%20while%0Amaintaining%20or%20enhancing%20the%20accuracy%20of%20models%20such%20as%20Random%20Forest%2C%0ACatBoost%2C%20XGBoost%2C%20and%20LightGBM%2C%20all%20without%20depending%20on%20posterior%0Aprobabilities%20or%20supervised%20optimization%20algorithms.%20Concurrently%2C%20sheaf%20theory%0Acontributes%20explanatory%20vectors%20that%20provide%20deeper%20insights%20into%20the%20data%27s%0Astructural%20nuances.%20The%20approach%20was%20validated%20using%20the%20UEA%20Archive%20and%20a%0Amastitis%20detection%20dataset%20for%20dairy%20cows.%20The%20results%20demonstrate%20that%0APHeatPruner%20effectively%20preserves%20model%20accuracy.%20Furthermore%2C%20our%20results%0Ahighlight%20PHeatPruner%27s%20key%20features%2C%20i.e.%20simplifying%20complex%20data%20and%0Aoffering%20actionable%20insights%20without%20increasing%20processing%20time%20or%20complexity.%0AThis%20method%20bridges%20the%20gap%20between%20complexity%20reduction%20and%20interpretability%2C%0Asuggesting%20promising%20applications%20in%20various%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18329v1&entry.124074799=Read"},
{"title": "DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval\n  for Multi-Hop Question Answering", "author": "Rong Cheng and Jinyi Liu and YAN ZHENG and Fei Ni and Jiazhen Du and Hangyu Mao and Fuzheng Zhang and Bo Wang and Jianye HAO", "abstract": "  Multi-Hop Question Answering (MHQA) tasks permeate real-world applications,\nposing challenges in orchestrating multi-step reasoning across diverse\nknowledge domains. While existing approaches have been improved with iterative\nretrieval, they still struggle to identify and organize dynamic knowledge. To\naddress this, we propose DualRAG, a synergistic dual-process framework that\nseamlessly integrates reasoning and retrieval. DualRAG operates through two\ntightly coupled processes: Reasoning-augmented Querying (RaQ) and progressive\nKnowledge Aggregation (pKA). They work in concert: as RaQ navigates the\nreasoning path and generates targeted queries, pKA ensures that newly acquired\nknowledge is systematically integrated to support coherent reasoning. This\ncreates a virtuous cycle of knowledge enrichment and reasoning refinement.\nThrough targeted fine-tuning, DualRAG preserves its sophisticated reasoning and\nretrieval capabilities even in smaller-scale models, demonstrating its\nversatility and core advantages across different scales. Extensive experiments\ndemonstrate that this dual-process approach substantially improves answer\naccuracy and coherence, approaching, and in some cases surpassing, the\nperformance achieved with oracle knowledge access. These results establish\nDualRAG as a robust and efficient solution for complex multi-hop reasoning\ntasks.\n", "link": "http://arxiv.org/abs/2504.18243v1", "date": "2025-04-25", "relevancy": 1.4208, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4922}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4853}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualRAG%3A%20A%20Dual-Process%20Approach%20to%20Integrate%20Reasoning%20and%20Retrieval%0A%20%20for%20Multi-Hop%20Question%20Answering&body=Title%3A%20DualRAG%3A%20A%20Dual-Process%20Approach%20to%20Integrate%20Reasoning%20and%20Retrieval%0A%20%20for%20Multi-Hop%20Question%20Answering%0AAuthor%3A%20Rong%20Cheng%20and%20Jinyi%20Liu%20and%20YAN%20ZHENG%20and%20Fei%20Ni%20and%20Jiazhen%20Du%20and%20Hangyu%20Mao%20and%20Fuzheng%20Zhang%20and%20Bo%20Wang%20and%20Jianye%20HAO%0AAbstract%3A%20%20%20Multi-Hop%20Question%20Answering%20%28MHQA%29%20tasks%20permeate%20real-world%20applications%2C%0Aposing%20challenges%20in%20orchestrating%20multi-step%20reasoning%20across%20diverse%0Aknowledge%20domains.%20While%20existing%20approaches%20have%20been%20improved%20with%20iterative%0Aretrieval%2C%20they%20still%20struggle%20to%20identify%20and%20organize%20dynamic%20knowledge.%20To%0Aaddress%20this%2C%20we%20propose%20DualRAG%2C%20a%20synergistic%20dual-process%20framework%20that%0Aseamlessly%20integrates%20reasoning%20and%20retrieval.%20DualRAG%20operates%20through%20two%0Atightly%20coupled%20processes%3A%20Reasoning-augmented%20Querying%20%28RaQ%29%20and%20progressive%0AKnowledge%20Aggregation%20%28pKA%29.%20They%20work%20in%20concert%3A%20as%20RaQ%20navigates%20the%0Areasoning%20path%20and%20generates%20targeted%20queries%2C%20pKA%20ensures%20that%20newly%20acquired%0Aknowledge%20is%20systematically%20integrated%20to%20support%20coherent%20reasoning.%20This%0Acreates%20a%20virtuous%20cycle%20of%20knowledge%20enrichment%20and%20reasoning%20refinement.%0AThrough%20targeted%20fine-tuning%2C%20DualRAG%20preserves%20its%20sophisticated%20reasoning%20and%0Aretrieval%20capabilities%20even%20in%20smaller-scale%20models%2C%20demonstrating%20its%0Aversatility%20and%20core%20advantages%20across%20different%20scales.%20Extensive%20experiments%0Ademonstrate%20that%20this%20dual-process%20approach%20substantially%20improves%20answer%0Aaccuracy%20and%20coherence%2C%20approaching%2C%20and%20in%20some%20cases%20surpassing%2C%20the%0Aperformance%20achieved%20with%20oracle%20knowledge%20access.%20These%20results%20establish%0ADualRAG%20as%20a%20robust%20and%20efficient%20solution%20for%20complex%20multi-hop%20reasoning%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualRAG%253A%2520A%2520Dual-Process%2520Approach%2520to%2520Integrate%2520Reasoning%2520and%2520Retrieval%250A%2520%2520for%2520Multi-Hop%2520Question%2520Answering%26entry.906535625%3DRong%2520Cheng%2520and%2520Jinyi%2520Liu%2520and%2520YAN%2520ZHENG%2520and%2520Fei%2520Ni%2520and%2520Jiazhen%2520Du%2520and%2520Hangyu%2520Mao%2520and%2520Fuzheng%2520Zhang%2520and%2520Bo%2520Wang%2520and%2520Jianye%2520HAO%26entry.1292438233%3D%2520%2520Multi-Hop%2520Question%2520Answering%2520%2528MHQA%2529%2520tasks%2520permeate%2520real-world%2520applications%252C%250Aposing%2520challenges%2520in%2520orchestrating%2520multi-step%2520reasoning%2520across%2520diverse%250Aknowledge%2520domains.%2520While%2520existing%2520approaches%2520have%2520been%2520improved%2520with%2520iterative%250Aretrieval%252C%2520they%2520still%2520struggle%2520to%2520identify%2520and%2520organize%2520dynamic%2520knowledge.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520DualRAG%252C%2520a%2520synergistic%2520dual-process%2520framework%2520that%250Aseamlessly%2520integrates%2520reasoning%2520and%2520retrieval.%2520DualRAG%2520operates%2520through%2520two%250Atightly%2520coupled%2520processes%253A%2520Reasoning-augmented%2520Querying%2520%2528RaQ%2529%2520and%2520progressive%250AKnowledge%2520Aggregation%2520%2528pKA%2529.%2520They%2520work%2520in%2520concert%253A%2520as%2520RaQ%2520navigates%2520the%250Areasoning%2520path%2520and%2520generates%2520targeted%2520queries%252C%2520pKA%2520ensures%2520that%2520newly%2520acquired%250Aknowledge%2520is%2520systematically%2520integrated%2520to%2520support%2520coherent%2520reasoning.%2520This%250Acreates%2520a%2520virtuous%2520cycle%2520of%2520knowledge%2520enrichment%2520and%2520reasoning%2520refinement.%250AThrough%2520targeted%2520fine-tuning%252C%2520DualRAG%2520preserves%2520its%2520sophisticated%2520reasoning%2520and%250Aretrieval%2520capabilities%2520even%2520in%2520smaller-scale%2520models%252C%2520demonstrating%2520its%250Aversatility%2520and%2520core%2520advantages%2520across%2520different%2520scales.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520this%2520dual-process%2520approach%2520substantially%2520improves%2520answer%250Aaccuracy%2520and%2520coherence%252C%2520approaching%252C%2520and%2520in%2520some%2520cases%2520surpassing%252C%2520the%250Aperformance%2520achieved%2520with%2520oracle%2520knowledge%2520access.%2520These%2520results%2520establish%250ADualRAG%2520as%2520a%2520robust%2520and%2520efficient%2520solution%2520for%2520complex%2520multi-hop%2520reasoning%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualRAG%3A%20A%20Dual-Process%20Approach%20to%20Integrate%20Reasoning%20and%20Retrieval%0A%20%20for%20Multi-Hop%20Question%20Answering&entry.906535625=Rong%20Cheng%20and%20Jinyi%20Liu%20and%20YAN%20ZHENG%20and%20Fei%20Ni%20and%20Jiazhen%20Du%20and%20Hangyu%20Mao%20and%20Fuzheng%20Zhang%20and%20Bo%20Wang%20and%20Jianye%20HAO&entry.1292438233=%20%20Multi-Hop%20Question%20Answering%20%28MHQA%29%20tasks%20permeate%20real-world%20applications%2C%0Aposing%20challenges%20in%20orchestrating%20multi-step%20reasoning%20across%20diverse%0Aknowledge%20domains.%20While%20existing%20approaches%20have%20been%20improved%20with%20iterative%0Aretrieval%2C%20they%20still%20struggle%20to%20identify%20and%20organize%20dynamic%20knowledge.%20To%0Aaddress%20this%2C%20we%20propose%20DualRAG%2C%20a%20synergistic%20dual-process%20framework%20that%0Aseamlessly%20integrates%20reasoning%20and%20retrieval.%20DualRAG%20operates%20through%20two%0Atightly%20coupled%20processes%3A%20Reasoning-augmented%20Querying%20%28RaQ%29%20and%20progressive%0AKnowledge%20Aggregation%20%28pKA%29.%20They%20work%20in%20concert%3A%20as%20RaQ%20navigates%20the%0Areasoning%20path%20and%20generates%20targeted%20queries%2C%20pKA%20ensures%20that%20newly%20acquired%0Aknowledge%20is%20systematically%20integrated%20to%20support%20coherent%20reasoning.%20This%0Acreates%20a%20virtuous%20cycle%20of%20knowledge%20enrichment%20and%20reasoning%20refinement.%0AThrough%20targeted%20fine-tuning%2C%20DualRAG%20preserves%20its%20sophisticated%20reasoning%20and%0Aretrieval%20capabilities%20even%20in%20smaller-scale%20models%2C%20demonstrating%20its%0Aversatility%20and%20core%20advantages%20across%20different%20scales.%20Extensive%20experiments%0Ademonstrate%20that%20this%20dual-process%20approach%20substantially%20improves%20answer%0Aaccuracy%20and%20coherence%2C%20approaching%2C%20and%20in%20some%20cases%20surpassing%2C%20the%0Aperformance%20achieved%20with%20oracle%20knowledge%20access.%20These%20results%20establish%0ADualRAG%20as%20a%20robust%20and%20efficient%20solution%20for%20complex%20multi-hop%20reasoning%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18243v1&entry.124074799=Read"},
{"title": "Learning to fuse: dynamic integration of multi-source data for accurate\n  battery lifespan prediction", "author": "He Shanxuan and Lin Zuhong and Yu Bolun and Gao Xu and Long Biao and Yao Jingjing", "abstract": "  Accurate prediction of lithium-ion battery lifespan is vital for ensuring\noperational reliability and reducing maintenance costs in applications like\nelectric vehicles and smart grids. This study presents a hybrid learning\nframework for precise battery lifespan prediction, integrating dynamic\nmulti-source data fusion with a stacked ensemble (SE) modeling approach. By\nleveraging heterogeneous datasets from the National Aeronautics and Space\nAdministration (NASA), Center for Advanced Life Cycle Engineering (CALCE),\nMIT-Stanford-Toyota Research Institute (TRC), and nickel cobalt aluminum (NCA)\nchemistries, an entropy-based dynamic weighting mechanism mitigates variability\nacross heterogeneous datasets. The SE model combines Ridge regression, long\nshort-term memory (LSTM) networks, and eXtreme Gradient Boosting (XGBoost),\neffectively capturing temporal dependencies and nonlinear degradation patterns.\nIt achieves a mean absolute error (MAE) of 0.0058, root mean square error\n(RMSE) of 0.0092, and coefficient of determination (R2) of 0.9839,\noutperforming established baseline models with a 46.2% improvement in R2 and an\n83.2% reduction in RMSE. Shapley additive explanations (SHAP) analysis\nidentifies differential discharge capacity (Qdlin) and temperature of\nmeasurement (Temp_m) as critical aging indicators. This scalable, interpretable\nframework enhances battery health management, supporting optimized maintenance\nand safety across diverse energy storage systems, thereby contributing to\nimproved battery health management in energy storage systems.\n", "link": "http://arxiv.org/abs/2504.18230v1", "date": "2025-04-25", "relevancy": 1.4263, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5068}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4783}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20fuse%3A%20dynamic%20integration%20of%20multi-source%20data%20for%20accurate%0A%20%20battery%20lifespan%20prediction&body=Title%3A%20Learning%20to%20fuse%3A%20dynamic%20integration%20of%20multi-source%20data%20for%20accurate%0A%20%20battery%20lifespan%20prediction%0AAuthor%3A%20He%20Shanxuan%20and%20Lin%20Zuhong%20and%20Yu%20Bolun%20and%20Gao%20Xu%20and%20Long%20Biao%20and%20Yao%20Jingjing%0AAbstract%3A%20%20%20Accurate%20prediction%20of%20lithium-ion%20battery%20lifespan%20is%20vital%20for%20ensuring%0Aoperational%20reliability%20and%20reducing%20maintenance%20costs%20in%20applications%20like%0Aelectric%20vehicles%20and%20smart%20grids.%20This%20study%20presents%20a%20hybrid%20learning%0Aframework%20for%20precise%20battery%20lifespan%20prediction%2C%20integrating%20dynamic%0Amulti-source%20data%20fusion%20with%20a%20stacked%20ensemble%20%28SE%29%20modeling%20approach.%20By%0Aleveraging%20heterogeneous%20datasets%20from%20the%20National%20Aeronautics%20and%20Space%0AAdministration%20%28NASA%29%2C%20Center%20for%20Advanced%20Life%20Cycle%20Engineering%20%28CALCE%29%2C%0AMIT-Stanford-Toyota%20Research%20Institute%20%28TRC%29%2C%20and%20nickel%20cobalt%20aluminum%20%28NCA%29%0Achemistries%2C%20an%20entropy-based%20dynamic%20weighting%20mechanism%20mitigates%20variability%0Aacross%20heterogeneous%20datasets.%20The%20SE%20model%20combines%20Ridge%20regression%2C%20long%0Ashort-term%20memory%20%28LSTM%29%20networks%2C%20and%20eXtreme%20Gradient%20Boosting%20%28XGBoost%29%2C%0Aeffectively%20capturing%20temporal%20dependencies%20and%20nonlinear%20degradation%20patterns.%0AIt%20achieves%20a%20mean%20absolute%20error%20%28MAE%29%20of%200.0058%2C%20root%20mean%20square%20error%0A%28RMSE%29%20of%200.0092%2C%20and%20coefficient%20of%20determination%20%28R2%29%20of%200.9839%2C%0Aoutperforming%20established%20baseline%20models%20with%20a%2046.2%25%20improvement%20in%20R2%20and%20an%0A83.2%25%20reduction%20in%20RMSE.%20Shapley%20additive%20explanations%20%28SHAP%29%20analysis%0Aidentifies%20differential%20discharge%20capacity%20%28Qdlin%29%20and%20temperature%20of%0Ameasurement%20%28Temp_m%29%20as%20critical%20aging%20indicators.%20This%20scalable%2C%20interpretable%0Aframework%20enhances%20battery%20health%20management%2C%20supporting%20optimized%20maintenance%0Aand%20safety%20across%20diverse%20energy%20storage%20systems%2C%20thereby%20contributing%20to%0Aimproved%20battery%20health%20management%20in%20energy%20storage%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520fuse%253A%2520dynamic%2520integration%2520of%2520multi-source%2520data%2520for%2520accurate%250A%2520%2520battery%2520lifespan%2520prediction%26entry.906535625%3DHe%2520Shanxuan%2520and%2520Lin%2520Zuhong%2520and%2520Yu%2520Bolun%2520and%2520Gao%2520Xu%2520and%2520Long%2520Biao%2520and%2520Yao%2520Jingjing%26entry.1292438233%3D%2520%2520Accurate%2520prediction%2520of%2520lithium-ion%2520battery%2520lifespan%2520is%2520vital%2520for%2520ensuring%250Aoperational%2520reliability%2520and%2520reducing%2520maintenance%2520costs%2520in%2520applications%2520like%250Aelectric%2520vehicles%2520and%2520smart%2520grids.%2520This%2520study%2520presents%2520a%2520hybrid%2520learning%250Aframework%2520for%2520precise%2520battery%2520lifespan%2520prediction%252C%2520integrating%2520dynamic%250Amulti-source%2520data%2520fusion%2520with%2520a%2520stacked%2520ensemble%2520%2528SE%2529%2520modeling%2520approach.%2520By%250Aleveraging%2520heterogeneous%2520datasets%2520from%2520the%2520National%2520Aeronautics%2520and%2520Space%250AAdministration%2520%2528NASA%2529%252C%2520Center%2520for%2520Advanced%2520Life%2520Cycle%2520Engineering%2520%2528CALCE%2529%252C%250AMIT-Stanford-Toyota%2520Research%2520Institute%2520%2528TRC%2529%252C%2520and%2520nickel%2520cobalt%2520aluminum%2520%2528NCA%2529%250Achemistries%252C%2520an%2520entropy-based%2520dynamic%2520weighting%2520mechanism%2520mitigates%2520variability%250Aacross%2520heterogeneous%2520datasets.%2520The%2520SE%2520model%2520combines%2520Ridge%2520regression%252C%2520long%250Ashort-term%2520memory%2520%2528LSTM%2529%2520networks%252C%2520and%2520eXtreme%2520Gradient%2520Boosting%2520%2528XGBoost%2529%252C%250Aeffectively%2520capturing%2520temporal%2520dependencies%2520and%2520nonlinear%2520degradation%2520patterns.%250AIt%2520achieves%2520a%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520of%25200.0058%252C%2520root%2520mean%2520square%2520error%250A%2528RMSE%2529%2520of%25200.0092%252C%2520and%2520coefficient%2520of%2520determination%2520%2528R2%2529%2520of%25200.9839%252C%250Aoutperforming%2520established%2520baseline%2520models%2520with%2520a%252046.2%2525%2520improvement%2520in%2520R2%2520and%2520an%250A83.2%2525%2520reduction%2520in%2520RMSE.%2520Shapley%2520additive%2520explanations%2520%2528SHAP%2529%2520analysis%250Aidentifies%2520differential%2520discharge%2520capacity%2520%2528Qdlin%2529%2520and%2520temperature%2520of%250Ameasurement%2520%2528Temp_m%2529%2520as%2520critical%2520aging%2520indicators.%2520This%2520scalable%252C%2520interpretable%250Aframework%2520enhances%2520battery%2520health%2520management%252C%2520supporting%2520optimized%2520maintenance%250Aand%2520safety%2520across%2520diverse%2520energy%2520storage%2520systems%252C%2520thereby%2520contributing%2520to%250Aimproved%2520battery%2520health%2520management%2520in%2520energy%2520storage%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20fuse%3A%20dynamic%20integration%20of%20multi-source%20data%20for%20accurate%0A%20%20battery%20lifespan%20prediction&entry.906535625=He%20Shanxuan%20and%20Lin%20Zuhong%20and%20Yu%20Bolun%20and%20Gao%20Xu%20and%20Long%20Biao%20and%20Yao%20Jingjing&entry.1292438233=%20%20Accurate%20prediction%20of%20lithium-ion%20battery%20lifespan%20is%20vital%20for%20ensuring%0Aoperational%20reliability%20and%20reducing%20maintenance%20costs%20in%20applications%20like%0Aelectric%20vehicles%20and%20smart%20grids.%20This%20study%20presents%20a%20hybrid%20learning%0Aframework%20for%20precise%20battery%20lifespan%20prediction%2C%20integrating%20dynamic%0Amulti-source%20data%20fusion%20with%20a%20stacked%20ensemble%20%28SE%29%20modeling%20approach.%20By%0Aleveraging%20heterogeneous%20datasets%20from%20the%20National%20Aeronautics%20and%20Space%0AAdministration%20%28NASA%29%2C%20Center%20for%20Advanced%20Life%20Cycle%20Engineering%20%28CALCE%29%2C%0AMIT-Stanford-Toyota%20Research%20Institute%20%28TRC%29%2C%20and%20nickel%20cobalt%20aluminum%20%28NCA%29%0Achemistries%2C%20an%20entropy-based%20dynamic%20weighting%20mechanism%20mitigates%20variability%0Aacross%20heterogeneous%20datasets.%20The%20SE%20model%20combines%20Ridge%20regression%2C%20long%0Ashort-term%20memory%20%28LSTM%29%20networks%2C%20and%20eXtreme%20Gradient%20Boosting%20%28XGBoost%29%2C%0Aeffectively%20capturing%20temporal%20dependencies%20and%20nonlinear%20degradation%20patterns.%0AIt%20achieves%20a%20mean%20absolute%20error%20%28MAE%29%20of%200.0058%2C%20root%20mean%20square%20error%0A%28RMSE%29%20of%200.0092%2C%20and%20coefficient%20of%20determination%20%28R2%29%20of%200.9839%2C%0Aoutperforming%20established%20baseline%20models%20with%20a%2046.2%25%20improvement%20in%20R2%20and%20an%0A83.2%25%20reduction%20in%20RMSE.%20Shapley%20additive%20explanations%20%28SHAP%29%20analysis%0Aidentifies%20differential%20discharge%20capacity%20%28Qdlin%29%20and%20temperature%20of%0Ameasurement%20%28Temp_m%29%20as%20critical%20aging%20indicators.%20This%20scalable%2C%20interpretable%0Aframework%20enhances%20battery%20health%20management%2C%20supporting%20optimized%20maintenance%0Aand%20safety%20across%20diverse%20energy%20storage%20systems%2C%20thereby%20contributing%20to%0Aimproved%20battery%20health%20management%20in%20energy%20storage%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18230v1&entry.124074799=Read"},
{"title": "Enhancing Strawberry Yield Forecasting with Backcasted IoT Sensor Data\n  and Machine Learning", "author": "Tewodros Alemu Ayall and Andy Li and Matthew Beddows and Milan Markovic and Georgios Leontidis", "abstract": "  Due to rapid population growth globally, digitally-enabled agricultural\nsectors are crucial for sustainable food production and making informed\ndecisions about resource management for farmers and various stakeholders. The\ndeployment of Internet of Things (IoT) technologies that collect real-time\nobservations of various environmental (e.g., temperature, humidity, etc.) and\noperational factors (e.g., irrigation) influencing production is often seen as\na critical step to enable additional novel downstream tasks, such as AI-based\nyield forecasting. However, since AI models require large amounts of data, this\ncreates practical challenges in a real-world dynamic farm setting where IoT\nobservations would need to be collected over a number of seasons. In this\nstudy, we deployed IoT sensors in strawberry production polytunnels for two\ngrowing seasons to collect environmental data, including water usage, external\nand internal temperature, external and internal humidity, soil moisture, soil\ntemperature, and photosynthetically active radiation. The sensor observations\nwere combined with manually provided yield records spanning a period of four\nseasons. To bridge the gap of missing IoT observations for two additional\nseasons, we propose an AI-based backcasting approach to generate synthetic\nsensor observations using historical weather data from a nearby weather station\nand the existing polytunnel observations. We built an AI-based yield\nforecasting model to evaluate our approach using the combination of real and\nsynthetic observations. Our results demonstrated that incorporating synthetic\ndata improved yield forecasting accuracy, with models incorporating synthetic\ndata outperforming those trained only on historical yield, weather records, and\nreal sensor data.\n", "link": "http://arxiv.org/abs/2504.18451v1", "date": "2025-04-25", "relevancy": 0.8867, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4652}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4404}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Strawberry%20Yield%20Forecasting%20with%20Backcasted%20IoT%20Sensor%20Data%0A%20%20and%20Machine%20Learning&body=Title%3A%20Enhancing%20Strawberry%20Yield%20Forecasting%20with%20Backcasted%20IoT%20Sensor%20Data%0A%20%20and%20Machine%20Learning%0AAuthor%3A%20Tewodros%20Alemu%20Ayall%20and%20Andy%20Li%20and%20Matthew%20Beddows%20and%20Milan%20Markovic%20and%20Georgios%20Leontidis%0AAbstract%3A%20%20%20Due%20to%20rapid%20population%20growth%20globally%2C%20digitally-enabled%20agricultural%0Asectors%20are%20crucial%20for%20sustainable%20food%20production%20and%20making%20informed%0Adecisions%20about%20resource%20management%20for%20farmers%20and%20various%20stakeholders.%20The%0Adeployment%20of%20Internet%20of%20Things%20%28IoT%29%20technologies%20that%20collect%20real-time%0Aobservations%20of%20various%20environmental%20%28e.g.%2C%20temperature%2C%20humidity%2C%20etc.%29%20and%0Aoperational%20factors%20%28e.g.%2C%20irrigation%29%20influencing%20production%20is%20often%20seen%20as%0Aa%20critical%20step%20to%20enable%20additional%20novel%20downstream%20tasks%2C%20such%20as%20AI-based%0Ayield%20forecasting.%20However%2C%20since%20AI%20models%20require%20large%20amounts%20of%20data%2C%20this%0Acreates%20practical%20challenges%20in%20a%20real-world%20dynamic%20farm%20setting%20where%20IoT%0Aobservations%20would%20need%20to%20be%20collected%20over%20a%20number%20of%20seasons.%20In%20this%0Astudy%2C%20we%20deployed%20IoT%20sensors%20in%20strawberry%20production%20polytunnels%20for%20two%0Agrowing%20seasons%20to%20collect%20environmental%20data%2C%20including%20water%20usage%2C%20external%0Aand%20internal%20temperature%2C%20external%20and%20internal%20humidity%2C%20soil%20moisture%2C%20soil%0Atemperature%2C%20and%20photosynthetically%20active%20radiation.%20The%20sensor%20observations%0Awere%20combined%20with%20manually%20provided%20yield%20records%20spanning%20a%20period%20of%20four%0Aseasons.%20To%20bridge%20the%20gap%20of%20missing%20IoT%20observations%20for%20two%20additional%0Aseasons%2C%20we%20propose%20an%20AI-based%20backcasting%20approach%20to%20generate%20synthetic%0Asensor%20observations%20using%20historical%20weather%20data%20from%20a%20nearby%20weather%20station%0Aand%20the%20existing%20polytunnel%20observations.%20We%20built%20an%20AI-based%20yield%0Aforecasting%20model%20to%20evaluate%20our%20approach%20using%20the%20combination%20of%20real%20and%0Asynthetic%20observations.%20Our%20results%20demonstrated%20that%20incorporating%20synthetic%0Adata%20improved%20yield%20forecasting%20accuracy%2C%20with%20models%20incorporating%20synthetic%0Adata%20outperforming%20those%20trained%20only%20on%20historical%20yield%2C%20weather%20records%2C%20and%0Areal%20sensor%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Strawberry%2520Yield%2520Forecasting%2520with%2520Backcasted%2520IoT%2520Sensor%2520Data%250A%2520%2520and%2520Machine%2520Learning%26entry.906535625%3DTewodros%2520Alemu%2520Ayall%2520and%2520Andy%2520Li%2520and%2520Matthew%2520Beddows%2520and%2520Milan%2520Markovic%2520and%2520Georgios%2520Leontidis%26entry.1292438233%3D%2520%2520Due%2520to%2520rapid%2520population%2520growth%2520globally%252C%2520digitally-enabled%2520agricultural%250Asectors%2520are%2520crucial%2520for%2520sustainable%2520food%2520production%2520and%2520making%2520informed%250Adecisions%2520about%2520resource%2520management%2520for%2520farmers%2520and%2520various%2520stakeholders.%2520The%250Adeployment%2520of%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520technologies%2520that%2520collect%2520real-time%250Aobservations%2520of%2520various%2520environmental%2520%2528e.g.%252C%2520temperature%252C%2520humidity%252C%2520etc.%2529%2520and%250Aoperational%2520factors%2520%2528e.g.%252C%2520irrigation%2529%2520influencing%2520production%2520is%2520often%2520seen%2520as%250Aa%2520critical%2520step%2520to%2520enable%2520additional%2520novel%2520downstream%2520tasks%252C%2520such%2520as%2520AI-based%250Ayield%2520forecasting.%2520However%252C%2520since%2520AI%2520models%2520require%2520large%2520amounts%2520of%2520data%252C%2520this%250Acreates%2520practical%2520challenges%2520in%2520a%2520real-world%2520dynamic%2520farm%2520setting%2520where%2520IoT%250Aobservations%2520would%2520need%2520to%2520be%2520collected%2520over%2520a%2520number%2520of%2520seasons.%2520In%2520this%250Astudy%252C%2520we%2520deployed%2520IoT%2520sensors%2520in%2520strawberry%2520production%2520polytunnels%2520for%2520two%250Agrowing%2520seasons%2520to%2520collect%2520environmental%2520data%252C%2520including%2520water%2520usage%252C%2520external%250Aand%2520internal%2520temperature%252C%2520external%2520and%2520internal%2520humidity%252C%2520soil%2520moisture%252C%2520soil%250Atemperature%252C%2520and%2520photosynthetically%2520active%2520radiation.%2520The%2520sensor%2520observations%250Awere%2520combined%2520with%2520manually%2520provided%2520yield%2520records%2520spanning%2520a%2520period%2520of%2520four%250Aseasons.%2520To%2520bridge%2520the%2520gap%2520of%2520missing%2520IoT%2520observations%2520for%2520two%2520additional%250Aseasons%252C%2520we%2520propose%2520an%2520AI-based%2520backcasting%2520approach%2520to%2520generate%2520synthetic%250Asensor%2520observations%2520using%2520historical%2520weather%2520data%2520from%2520a%2520nearby%2520weather%2520station%250Aand%2520the%2520existing%2520polytunnel%2520observations.%2520We%2520built%2520an%2520AI-based%2520yield%250Aforecasting%2520model%2520to%2520evaluate%2520our%2520approach%2520using%2520the%2520combination%2520of%2520real%2520and%250Asynthetic%2520observations.%2520Our%2520results%2520demonstrated%2520that%2520incorporating%2520synthetic%250Adata%2520improved%2520yield%2520forecasting%2520accuracy%252C%2520with%2520models%2520incorporating%2520synthetic%250Adata%2520outperforming%2520those%2520trained%2520only%2520on%2520historical%2520yield%252C%2520weather%2520records%252C%2520and%250Areal%2520sensor%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Strawberry%20Yield%20Forecasting%20with%20Backcasted%20IoT%20Sensor%20Data%0A%20%20and%20Machine%20Learning&entry.906535625=Tewodros%20Alemu%20Ayall%20and%20Andy%20Li%20and%20Matthew%20Beddows%20and%20Milan%20Markovic%20and%20Georgios%20Leontidis&entry.1292438233=%20%20Due%20to%20rapid%20population%20growth%20globally%2C%20digitally-enabled%20agricultural%0Asectors%20are%20crucial%20for%20sustainable%20food%20production%20and%20making%20informed%0Adecisions%20about%20resource%20management%20for%20farmers%20and%20various%20stakeholders.%20The%0Adeployment%20of%20Internet%20of%20Things%20%28IoT%29%20technologies%20that%20collect%20real-time%0Aobservations%20of%20various%20environmental%20%28e.g.%2C%20temperature%2C%20humidity%2C%20etc.%29%20and%0Aoperational%20factors%20%28e.g.%2C%20irrigation%29%20influencing%20production%20is%20often%20seen%20as%0Aa%20critical%20step%20to%20enable%20additional%20novel%20downstream%20tasks%2C%20such%20as%20AI-based%0Ayield%20forecasting.%20However%2C%20since%20AI%20models%20require%20large%20amounts%20of%20data%2C%20this%0Acreates%20practical%20challenges%20in%20a%20real-world%20dynamic%20farm%20setting%20where%20IoT%0Aobservations%20would%20need%20to%20be%20collected%20over%20a%20number%20of%20seasons.%20In%20this%0Astudy%2C%20we%20deployed%20IoT%20sensors%20in%20strawberry%20production%20polytunnels%20for%20two%0Agrowing%20seasons%20to%20collect%20environmental%20data%2C%20including%20water%20usage%2C%20external%0Aand%20internal%20temperature%2C%20external%20and%20internal%20humidity%2C%20soil%20moisture%2C%20soil%0Atemperature%2C%20and%20photosynthetically%20active%20radiation.%20The%20sensor%20observations%0Awere%20combined%20with%20manually%20provided%20yield%20records%20spanning%20a%20period%20of%20four%0Aseasons.%20To%20bridge%20the%20gap%20of%20missing%20IoT%20observations%20for%20two%20additional%0Aseasons%2C%20we%20propose%20an%20AI-based%20backcasting%20approach%20to%20generate%20synthetic%0Asensor%20observations%20using%20historical%20weather%20data%20from%20a%20nearby%20weather%20station%0Aand%20the%20existing%20polytunnel%20observations.%20We%20built%20an%20AI-based%20yield%0Aforecasting%20model%20to%20evaluate%20our%20approach%20using%20the%20combination%20of%20real%20and%0Asynthetic%20observations.%20Our%20results%20demonstrated%20that%20incorporating%20synthetic%0Adata%20improved%20yield%20forecasting%20accuracy%2C%20with%20models%20incorporating%20synthetic%0Adata%20outperforming%20those%20trained%20only%20on%20historical%20yield%2C%20weather%20records%2C%20and%0Areal%20sensor%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18451v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


