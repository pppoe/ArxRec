<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250120.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GaussianVideo: Efficient Video Representation Through 2D Gaussian\n  Splatting", "author": "Longan Wang and Yuang Shi and Wei Tsang Ooi", "abstract": "  3D Gaussian splats have emerged as a revolutionary, effective, learned\nrepresentation for static 3D scenes. In this work, we explore using 2D Gaussian\nsplats as a new primitive for representing videos. We propose GaussianVideo, an\napproach to learning a set of 2D Gaussian splats that can effectively represent\nvideo frames. GaussianVideo incorporates the following techniques: (i) To\nexploit temporal redundancy among adjacent frames, which can speed up training\nand improve the compression efficiency, we predict the Gaussian splats of a\nframe based on its previous frame; (ii) To control the trade-offs between file\nsize and quality, we remove Gaussian splats with low contribution to the video\nquality; (iii) To capture dynamics in videos, we randomly add Gaussian splats\nto fit content with large motion or newly-appeared objects; (iv) To handle\nsignificant changes in the scene, we detect key frames based on loss\ndifferences during the learning process. Experiment results show that\nGaussianVideo achieves good rate-distortion trade-offs, comparable to\nstate-of-the-art video codecs such as AV1 and VVC, and a rendering speed of\n1500 fps for a 1920x1080 video.\n", "link": "http://arxiv.org/abs/2501.12060v1", "date": "2025-01-21", "relevancy": 3.2423, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.667}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6479}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianVideo%3A%20Efficient%20Video%20Representation%20Through%202D%20Gaussian%0A%20%20Splatting&body=Title%3A%20GaussianVideo%3A%20Efficient%20Video%20Representation%20Through%202D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Longan%20Wang%20and%20Yuang%20Shi%20and%20Wei%20Tsang%20Ooi%0AAbstract%3A%20%20%203D%20Gaussian%20splats%20have%20emerged%20as%20a%20revolutionary%2C%20effective%2C%20learned%0Arepresentation%20for%20static%203D%20scenes.%20In%20this%20work%2C%20we%20explore%20using%202D%20Gaussian%0Asplats%20as%20a%20new%20primitive%20for%20representing%20videos.%20We%20propose%20GaussianVideo%2C%20an%0Aapproach%20to%20learning%20a%20set%20of%202D%20Gaussian%20splats%20that%20can%20effectively%20represent%0Avideo%20frames.%20GaussianVideo%20incorporates%20the%20following%20techniques%3A%20%28i%29%20To%0Aexploit%20temporal%20redundancy%20among%20adjacent%20frames%2C%20which%20can%20speed%20up%20training%0Aand%20improve%20the%20compression%20efficiency%2C%20we%20predict%20the%20Gaussian%20splats%20of%20a%0Aframe%20based%20on%20its%20previous%20frame%3B%20%28ii%29%20To%20control%20the%20trade-offs%20between%20file%0Asize%20and%20quality%2C%20we%20remove%20Gaussian%20splats%20with%20low%20contribution%20to%20the%20video%0Aquality%3B%20%28iii%29%20To%20capture%20dynamics%20in%20videos%2C%20we%20randomly%20add%20Gaussian%20splats%0Ato%20fit%20content%20with%20large%20motion%20or%20newly-appeared%20objects%3B%20%28iv%29%20To%20handle%0Asignificant%20changes%20in%20the%20scene%2C%20we%20detect%20key%20frames%20based%20on%20loss%0Adifferences%20during%20the%20learning%20process.%20Experiment%20results%20show%20that%0AGaussianVideo%20achieves%20good%20rate-distortion%20trade-offs%2C%20comparable%20to%0Astate-of-the-art%20video%20codecs%20such%20as%20AV1%20and%20VVC%2C%20and%20a%20rendering%20speed%20of%0A1500%20fps%20for%20a%201920x1080%20video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianVideo%253A%2520Efficient%2520Video%2520Representation%2520Through%25202D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DLongan%2520Wang%2520and%2520Yuang%2520Shi%2520and%2520Wei%2520Tsang%2520Ooi%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splats%2520have%2520emerged%2520as%2520a%2520revolutionary%252C%2520effective%252C%2520learned%250Arepresentation%2520for%2520static%25203D%2520scenes.%2520In%2520this%2520work%252C%2520we%2520explore%2520using%25202D%2520Gaussian%250Asplats%2520as%2520a%2520new%2520primitive%2520for%2520representing%2520videos.%2520We%2520propose%2520GaussianVideo%252C%2520an%250Aapproach%2520to%2520learning%2520a%2520set%2520of%25202D%2520Gaussian%2520splats%2520that%2520can%2520effectively%2520represent%250Avideo%2520frames.%2520GaussianVideo%2520incorporates%2520the%2520following%2520techniques%253A%2520%2528i%2529%2520To%250Aexploit%2520temporal%2520redundancy%2520among%2520adjacent%2520frames%252C%2520which%2520can%2520speed%2520up%2520training%250Aand%2520improve%2520the%2520compression%2520efficiency%252C%2520we%2520predict%2520the%2520Gaussian%2520splats%2520of%2520a%250Aframe%2520based%2520on%2520its%2520previous%2520frame%253B%2520%2528ii%2529%2520To%2520control%2520the%2520trade-offs%2520between%2520file%250Asize%2520and%2520quality%252C%2520we%2520remove%2520Gaussian%2520splats%2520with%2520low%2520contribution%2520to%2520the%2520video%250Aquality%253B%2520%2528iii%2529%2520To%2520capture%2520dynamics%2520in%2520videos%252C%2520we%2520randomly%2520add%2520Gaussian%2520splats%250Ato%2520fit%2520content%2520with%2520large%2520motion%2520or%2520newly-appeared%2520objects%253B%2520%2528iv%2529%2520To%2520handle%250Asignificant%2520changes%2520in%2520the%2520scene%252C%2520we%2520detect%2520key%2520frames%2520based%2520on%2520loss%250Adifferences%2520during%2520the%2520learning%2520process.%2520Experiment%2520results%2520show%2520that%250AGaussianVideo%2520achieves%2520good%2520rate-distortion%2520trade-offs%252C%2520comparable%2520to%250Astate-of-the-art%2520video%2520codecs%2520such%2520as%2520AV1%2520and%2520VVC%252C%2520and%2520a%2520rendering%2520speed%2520of%250A1500%2520fps%2520for%2520a%25201920x1080%2520video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianVideo%3A%20Efficient%20Video%20Representation%20Through%202D%20Gaussian%0A%20%20Splatting&entry.906535625=Longan%20Wang%20and%20Yuang%20Shi%20and%20Wei%20Tsang%20Ooi&entry.1292438233=%20%203D%20Gaussian%20splats%20have%20emerged%20as%20a%20revolutionary%2C%20effective%2C%20learned%0Arepresentation%20for%20static%203D%20scenes.%20In%20this%20work%2C%20we%20explore%20using%202D%20Gaussian%0Asplats%20as%20a%20new%20primitive%20for%20representing%20videos.%20We%20propose%20GaussianVideo%2C%20an%0Aapproach%20to%20learning%20a%20set%20of%202D%20Gaussian%20splats%20that%20can%20effectively%20represent%0Avideo%20frames.%20GaussianVideo%20incorporates%20the%20following%20techniques%3A%20%28i%29%20To%0Aexploit%20temporal%20redundancy%20among%20adjacent%20frames%2C%20which%20can%20speed%20up%20training%0Aand%20improve%20the%20compression%20efficiency%2C%20we%20predict%20the%20Gaussian%20splats%20of%20a%0Aframe%20based%20on%20its%20previous%20frame%3B%20%28ii%29%20To%20control%20the%20trade-offs%20between%20file%0Asize%20and%20quality%2C%20we%20remove%20Gaussian%20splats%20with%20low%20contribution%20to%20the%20video%0Aquality%3B%20%28iii%29%20To%20capture%20dynamics%20in%20videos%2C%20we%20randomly%20add%20Gaussian%20splats%0Ato%20fit%20content%20with%20large%20motion%20or%20newly-appeared%20objects%3B%20%28iv%29%20To%20handle%0Asignificant%20changes%20in%20the%20scene%2C%20we%20detect%20key%20frames%20based%20on%20loss%0Adifferences%20during%20the%20learning%20process.%20Experiment%20results%20show%20that%0AGaussianVideo%20achieves%20good%20rate-distortion%20trade-offs%2C%20comparable%20to%0Astate-of-the-art%20video%20codecs%20such%20as%20AV1%20and%20VVC%2C%20and%20a%20rendering%20speed%20of%0A1500%20fps%20for%20a%201920x1080%20video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12060v1&entry.124074799=Read"},
{"title": "HAC++: Towards 100X Compression of 3D Gaussian Splatting", "author": "Yihang Chen and Qianyi Wu and Weiyao Lin and Mehrtash Harandi and Jianfei Cai", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel\nview synthesis, boasting rapid rendering speed with high fidelity. However, the\nsubstantial Gaussians and their associated attributes necessitate effective\ncompression techniques. Nevertheless, the sparse and unorganized nature of the\npoint cloud of Gaussians (or anchors in our paper) presents challenges for\ncompression. To achieve a compact size, we propose HAC++, which leverages the\nrelationships between unorganized anchors and a structured hash grid, utilizing\ntheir mutual information for context modeling. Additionally, HAC++ captures\nintra-anchor contextual relationships to further enhance compression\nperformance. To facilitate entropy coding, we utilize Gaussian distributions to\nprecisely estimate the probability of each quantized attribute, where an\nadaptive quantization module is proposed to enable high-precision quantization\nof these attributes for improved fidelity restoration. Moreover, we incorporate\nan adaptive masking strategy to eliminate invalid Gaussians and anchors.\nOverall, HAC++ achieves a remarkable size reduction of over 100X compared to\nvanilla 3DGS when averaged on all datasets, while simultaneously improving\nfidelity. It also delivers more than 20X size reduction compared to\nScaffold-GS. Our code is available at\nhttps://github.com/YihangChen-ee/HAC-plus.\n", "link": "http://arxiv.org/abs/2501.12255v1", "date": "2025-01-21", "relevancy": 3.2301, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6576}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6541}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAC%2B%2B%3A%20Towards%20100X%20Compression%20of%203D%20Gaussian%20Splatting&body=Title%3A%20HAC%2B%2B%3A%20Towards%20100X%20Compression%20of%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Weiyao%20Lin%20and%20Mehrtash%20Harandi%20and%20Jianfei%20Cai%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20framework%20for%20novel%0Aview%20synthesis%2C%20boasting%20rapid%20rendering%20speed%20with%20high%20fidelity.%20However%2C%20the%0Asubstantial%20Gaussians%20and%20their%20associated%20attributes%20necessitate%20effective%0Acompression%20techniques.%20Nevertheless%2C%20the%20sparse%20and%20unorganized%20nature%20of%20the%0Apoint%20cloud%20of%20Gaussians%20%28or%20anchors%20in%20our%20paper%29%20presents%20challenges%20for%0Acompression.%20To%20achieve%20a%20compact%20size%2C%20we%20propose%20HAC%2B%2B%2C%20which%20leverages%20the%0Arelationships%20between%20unorganized%20anchors%20and%20a%20structured%20hash%20grid%2C%20utilizing%0Atheir%20mutual%20information%20for%20context%20modeling.%20Additionally%2C%20HAC%2B%2B%20captures%0Aintra-anchor%20contextual%20relationships%20to%20further%20enhance%20compression%0Aperformance.%20To%20facilitate%20entropy%20coding%2C%20we%20utilize%20Gaussian%20distributions%20to%0Aprecisely%20estimate%20the%20probability%20of%20each%20quantized%20attribute%2C%20where%20an%0Aadaptive%20quantization%20module%20is%20proposed%20to%20enable%20high-precision%20quantization%0Aof%20these%20attributes%20for%20improved%20fidelity%20restoration.%20Moreover%2C%20we%20incorporate%0Aan%20adaptive%20masking%20strategy%20to%20eliminate%20invalid%20Gaussians%20and%20anchors.%0AOverall%2C%20HAC%2B%2B%20achieves%20a%20remarkable%20size%20reduction%20of%20over%20100X%20compared%20to%0Avanilla%203DGS%20when%20averaged%20on%20all%20datasets%2C%20while%20simultaneously%20improving%0Afidelity.%20It%20also%20delivers%20more%20than%2020X%20size%20reduction%20compared%20to%0AScaffold-GS.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/YihangChen-ee/HAC-plus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAC%252B%252B%253A%2520Towards%2520100X%2520Compression%2520of%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYihang%2520Chen%2520and%2520Qianyi%2520Wu%2520and%2520Weiyao%2520Lin%2520and%2520Mehrtash%2520Harandi%2520and%2520Jianfei%2520Cai%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520promising%2520framework%2520for%2520novel%250Aview%2520synthesis%252C%2520boasting%2520rapid%2520rendering%2520speed%2520with%2520high%2520fidelity.%2520However%252C%2520the%250Asubstantial%2520Gaussians%2520and%2520their%2520associated%2520attributes%2520necessitate%2520effective%250Acompression%2520techniques.%2520Nevertheless%252C%2520the%2520sparse%2520and%2520unorganized%2520nature%2520of%2520the%250Apoint%2520cloud%2520of%2520Gaussians%2520%2528or%2520anchors%2520in%2520our%2520paper%2529%2520presents%2520challenges%2520for%250Acompression.%2520To%2520achieve%2520a%2520compact%2520size%252C%2520we%2520propose%2520HAC%252B%252B%252C%2520which%2520leverages%2520the%250Arelationships%2520between%2520unorganized%2520anchors%2520and%2520a%2520structured%2520hash%2520grid%252C%2520utilizing%250Atheir%2520mutual%2520information%2520for%2520context%2520modeling.%2520Additionally%252C%2520HAC%252B%252B%2520captures%250Aintra-anchor%2520contextual%2520relationships%2520to%2520further%2520enhance%2520compression%250Aperformance.%2520To%2520facilitate%2520entropy%2520coding%252C%2520we%2520utilize%2520Gaussian%2520distributions%2520to%250Aprecisely%2520estimate%2520the%2520probability%2520of%2520each%2520quantized%2520attribute%252C%2520where%2520an%250Aadaptive%2520quantization%2520module%2520is%2520proposed%2520to%2520enable%2520high-precision%2520quantization%250Aof%2520these%2520attributes%2520for%2520improved%2520fidelity%2520restoration.%2520Moreover%252C%2520we%2520incorporate%250Aan%2520adaptive%2520masking%2520strategy%2520to%2520eliminate%2520invalid%2520Gaussians%2520and%2520anchors.%250AOverall%252C%2520HAC%252B%252B%2520achieves%2520a%2520remarkable%2520size%2520reduction%2520of%2520over%2520100X%2520compared%2520to%250Avanilla%25203DGS%2520when%2520averaged%2520on%2520all%2520datasets%252C%2520while%2520simultaneously%2520improving%250Afidelity.%2520It%2520also%2520delivers%2520more%2520than%252020X%2520size%2520reduction%2520compared%2520to%250AScaffold-GS.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/YihangChen-ee/HAC-plus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAC%2B%2B%3A%20Towards%20100X%20Compression%20of%203D%20Gaussian%20Splatting&entry.906535625=Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Weiyao%20Lin%20and%20Mehrtash%20Harandi%20and%20Jianfei%20Cai&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20framework%20for%20novel%0Aview%20synthesis%2C%20boasting%20rapid%20rendering%20speed%20with%20high%20fidelity.%20However%2C%20the%0Asubstantial%20Gaussians%20and%20their%20associated%20attributes%20necessitate%20effective%0Acompression%20techniques.%20Nevertheless%2C%20the%20sparse%20and%20unorganized%20nature%20of%20the%0Apoint%20cloud%20of%20Gaussians%20%28or%20anchors%20in%20our%20paper%29%20presents%20challenges%20for%0Acompression.%20To%20achieve%20a%20compact%20size%2C%20we%20propose%20HAC%2B%2B%2C%20which%20leverages%20the%0Arelationships%20between%20unorganized%20anchors%20and%20a%20structured%20hash%20grid%2C%20utilizing%0Atheir%20mutual%20information%20for%20context%20modeling.%20Additionally%2C%20HAC%2B%2B%20captures%0Aintra-anchor%20contextual%20relationships%20to%20further%20enhance%20compression%0Aperformance.%20To%20facilitate%20entropy%20coding%2C%20we%20utilize%20Gaussian%20distributions%20to%0Aprecisely%20estimate%20the%20probability%20of%20each%20quantized%20attribute%2C%20where%20an%0Aadaptive%20quantization%20module%20is%20proposed%20to%20enable%20high-precision%20quantization%0Aof%20these%20attributes%20for%20improved%20fidelity%20restoration.%20Moreover%2C%20we%20incorporate%0Aan%20adaptive%20masking%20strategy%20to%20eliminate%20invalid%20Gaussians%20and%20anchors.%0AOverall%2C%20HAC%2B%2B%20achieves%20a%20remarkable%20size%20reduction%20of%20over%20100X%20compared%20to%0Avanilla%203DGS%20when%20averaged%20on%20all%20datasets%2C%20while%20simultaneously%20improving%0Afidelity.%20It%20also%20delivers%20more%20than%2020X%20size%20reduction%20compared%20to%0AScaffold-GS.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/YihangChen-ee/HAC-plus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12255v1&entry.124074799=Read"},
{"title": "InternVideo2.5: Empowering Video MLLMs with Long and Rich Context\n  Modeling", "author": "Yi Wang and Xinhao Li and Ziang Yan and Yinan He and Jiashuo Yu and Xiangyu Zeng and Chenting Wang and Changlian Ma and Haian Huang and Jianfei Gao and Min Dou and Kai Chen and Wenhai Wang and Yu Qiao and Yali Wang and Limin Wang", "abstract": "  This paper aims to improve the performance of video multimodal large language\nmodels (MLLM) via long and rich context (LRC) modeling. As a result, we develop\na new version of InternVideo2.5 with a focus on enhancing the original MLLMs'\nability to perceive fine-grained details and capture long-form temporal\nstructure in videos. Specifically, our approach incorporates dense vision task\nannotations into MLLMs using direct preference optimization and develops\ncompact spatiotemporal representations through adaptive hierarchical token\ncompression. Experimental results demonstrate this unique design of LRC greatly\nimproves the results of video MLLM in mainstream video understanding benchmarks\n(short & long), enabling the MLLM to memorize significantly longer video inputs\n(at least 6x longer than the original), and master specialized vision\ncapabilities like object tracking and segmentation. Our work highlights the\nimportance of multimodal context richness (length and fineness) in empowering\nMLLM's innate abilites (focus and memory), providing new insights for future\nresearch on video MLLM. Code and models are available at\nhttps://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5\n", "link": "http://arxiv.org/abs/2501.12386v1", "date": "2025-01-21", "relevancy": 3.0906, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6372}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6372}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternVideo2.5%3A%20Empowering%20Video%20MLLMs%20with%20Long%20and%20Rich%20Context%0A%20%20Modeling&body=Title%3A%20InternVideo2.5%3A%20Empowering%20Video%20MLLMs%20with%20Long%20and%20Rich%20Context%0A%20%20Modeling%0AAuthor%3A%20Yi%20Wang%20and%20Xinhao%20Li%20and%20Ziang%20Yan%20and%20Yinan%20He%20and%20Jiashuo%20Yu%20and%20Xiangyu%20Zeng%20and%20Chenting%20Wang%20and%20Changlian%20Ma%20and%20Haian%20Huang%20and%20Jianfei%20Gao%20and%20Min%20Dou%20and%20Kai%20Chen%20and%20Wenhai%20Wang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20improve%20the%20performance%20of%20video%20multimodal%20large%20language%0Amodels%20%28MLLM%29%20via%20long%20and%20rich%20context%20%28LRC%29%20modeling.%20As%20a%20result%2C%20we%20develop%0Aa%20new%20version%20of%20InternVideo2.5%20with%20a%20focus%20on%20enhancing%20the%20original%20MLLMs%27%0Aability%20to%20perceive%20fine-grained%20details%20and%20capture%20long-form%20temporal%0Astructure%20in%20videos.%20Specifically%2C%20our%20approach%20incorporates%20dense%20vision%20task%0Aannotations%20into%20MLLMs%20using%20direct%20preference%20optimization%20and%20develops%0Acompact%20spatiotemporal%20representations%20through%20adaptive%20hierarchical%20token%0Acompression.%20Experimental%20results%20demonstrate%20this%20unique%20design%20of%20LRC%20greatly%0Aimproves%20the%20results%20of%20video%20MLLM%20in%20mainstream%20video%20understanding%20benchmarks%0A%28short%20%26%20long%29%2C%20enabling%20the%20MLLM%20to%20memorize%20significantly%20longer%20video%20inputs%0A%28at%20least%206x%20longer%20than%20the%20original%29%2C%20and%20master%20specialized%20vision%0Acapabilities%20like%20object%20tracking%20and%20segmentation.%20Our%20work%20highlights%20the%0Aimportance%20of%20multimodal%20context%20richness%20%28length%20and%20fineness%29%20in%20empowering%0AMLLM%27s%20innate%20abilites%20%28focus%20and%20memory%29%2C%20providing%20new%20insights%20for%20future%0Aresearch%20on%20video%20MLLM.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternVideo2.5%253A%2520Empowering%2520Video%2520MLLMs%2520with%2520Long%2520and%2520Rich%2520Context%250A%2520%2520Modeling%26entry.906535625%3DYi%2520Wang%2520and%2520Xinhao%2520Li%2520and%2520Ziang%2520Yan%2520and%2520Yinan%2520He%2520and%2520Jiashuo%2520Yu%2520and%2520Xiangyu%2520Zeng%2520and%2520Chenting%2520Wang%2520and%2520Changlian%2520Ma%2520and%2520Haian%2520Huang%2520and%2520Jianfei%2520Gao%2520and%2520Min%2520Dou%2520and%2520Kai%2520Chen%2520and%2520Wenhai%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Yali%2520Wang%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520improve%2520the%2520performance%2520of%2520video%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLM%2529%2520via%2520long%2520and%2520rich%2520context%2520%2528LRC%2529%2520modeling.%2520As%2520a%2520result%252C%2520we%2520develop%250Aa%2520new%2520version%2520of%2520InternVideo2.5%2520with%2520a%2520focus%2520on%2520enhancing%2520the%2520original%2520MLLMs%2527%250Aability%2520to%2520perceive%2520fine-grained%2520details%2520and%2520capture%2520long-form%2520temporal%250Astructure%2520in%2520videos.%2520Specifically%252C%2520our%2520approach%2520incorporates%2520dense%2520vision%2520task%250Aannotations%2520into%2520MLLMs%2520using%2520direct%2520preference%2520optimization%2520and%2520develops%250Acompact%2520spatiotemporal%2520representations%2520through%2520adaptive%2520hierarchical%2520token%250Acompression.%2520Experimental%2520results%2520demonstrate%2520this%2520unique%2520design%2520of%2520LRC%2520greatly%250Aimproves%2520the%2520results%2520of%2520video%2520MLLM%2520in%2520mainstream%2520video%2520understanding%2520benchmarks%250A%2528short%2520%2526%2520long%2529%252C%2520enabling%2520the%2520MLLM%2520to%2520memorize%2520significantly%2520longer%2520video%2520inputs%250A%2528at%2520least%25206x%2520longer%2520than%2520the%2520original%2529%252C%2520and%2520master%2520specialized%2520vision%250Acapabilities%2520like%2520object%2520tracking%2520and%2520segmentation.%2520Our%2520work%2520highlights%2520the%250Aimportance%2520of%2520multimodal%2520context%2520richness%2520%2528length%2520and%2520fineness%2529%2520in%2520empowering%250AMLLM%2527s%2520innate%2520abilites%2520%2528focus%2520and%2520memory%2529%252C%2520providing%2520new%2520insights%2520for%2520future%250Aresearch%2520on%2520video%2520MLLM.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternVideo2.5%3A%20Empowering%20Video%20MLLMs%20with%20Long%20and%20Rich%20Context%0A%20%20Modeling&entry.906535625=Yi%20Wang%20and%20Xinhao%20Li%20and%20Ziang%20Yan%20and%20Yinan%20He%20and%20Jiashuo%20Yu%20and%20Xiangyu%20Zeng%20and%20Chenting%20Wang%20and%20Changlian%20Ma%20and%20Haian%20Huang%20and%20Jianfei%20Gao%20and%20Min%20Dou%20and%20Kai%20Chen%20and%20Wenhai%20Wang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang&entry.1292438233=%20%20This%20paper%20aims%20to%20improve%20the%20performance%20of%20video%20multimodal%20large%20language%0Amodels%20%28MLLM%29%20via%20long%20and%20rich%20context%20%28LRC%29%20modeling.%20As%20a%20result%2C%20we%20develop%0Aa%20new%20version%20of%20InternVideo2.5%20with%20a%20focus%20on%20enhancing%20the%20original%20MLLMs%27%0Aability%20to%20perceive%20fine-grained%20details%20and%20capture%20long-form%20temporal%0Astructure%20in%20videos.%20Specifically%2C%20our%20approach%20incorporates%20dense%20vision%20task%0Aannotations%20into%20MLLMs%20using%20direct%20preference%20optimization%20and%20develops%0Acompact%20spatiotemporal%20representations%20through%20adaptive%20hierarchical%20token%0Acompression.%20Experimental%20results%20demonstrate%20this%20unique%20design%20of%20LRC%20greatly%0Aimproves%20the%20results%20of%20video%20MLLM%20in%20mainstream%20video%20understanding%20benchmarks%0A%28short%20%26%20long%29%2C%20enabling%20the%20MLLM%20to%20memorize%20significantly%20longer%20video%20inputs%0A%28at%20least%206x%20longer%20than%20the%20original%29%2C%20and%20master%20specialized%20vision%0Acapabilities%20like%20object%20tracking%20and%20segmentation.%20Our%20work%20highlights%20the%0Aimportance%20of%20multimodal%20context%20richness%20%28length%20and%20fineness%29%20in%20empowering%0AMLLM%27s%20innate%20abilites%20%28focus%20and%20memory%29%2C%20providing%20new%20insights%20for%20future%0Aresearch%20on%20video%20MLLM.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12386v1&entry.124074799=Read"},
{"title": "DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial\n  Basis Functions", "author": "Vishagar Arunan and Saeedha Nazar and Hashiru Pramuditha and Vinasirajan Viruthshaan and Sameera Ramasinghe and Simon Lucey and Ranga Rodrigo", "abstract": "  Splatting-based 3D reconstruction methods have gained popularity with the\nadvent of 3D Gaussian Splatting, efficiently synthesizing high-quality novel\nviews. These methods commonly resort to using exponential family functions,\nsuch as the Gaussian function, as reconstruction kernels due to their\nanisotropic nature, ease of projection, and differentiability in rasterization.\nHowever, the field remains restricted to variations within the exponential\nfamily, leaving generalized reconstruction kernels largely underexplored,\npartly due to the lack of easy integrability in 3D to 2D projections. In this\nlight, we show that a class of decaying anisotropic radial basis functions\n(DARBFs), which are non-negative functions of the Mahalanobis distance,\nsupports splatting by approximating the Gaussian function's closed-form\nintegration advantage. With this fresh perspective, we demonstrate up to 34%\nfaster convergence during training and a 15% reduction in memory consumption\nacross various DARB reconstruction kernels, while maintaining comparable PSNR,\nSSIM, and LPIPS results. We will make the code available.\n", "link": "http://arxiv.org/abs/2501.12369v1", "date": "2025-01-21", "relevancy": 3.012, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6475}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5851}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DARB-Splatting%3A%20Generalizing%20Splatting%20with%20Decaying%20Anisotropic%20Radial%0A%20%20Basis%20Functions&body=Title%3A%20DARB-Splatting%3A%20Generalizing%20Splatting%20with%20Decaying%20Anisotropic%20Radial%0A%20%20Basis%20Functions%0AAuthor%3A%20Vishagar%20Arunan%20and%20Saeedha%20Nazar%20and%20Hashiru%20Pramuditha%20and%20Vinasirajan%20Viruthshaan%20and%20Sameera%20Ramasinghe%20and%20Simon%20Lucey%20and%20Ranga%20Rodrigo%0AAbstract%3A%20%20%20Splatting-based%203D%20reconstruction%20methods%20have%20gained%20popularity%20with%20the%0Aadvent%20of%203D%20Gaussian%20Splatting%2C%20efficiently%20synthesizing%20high-quality%20novel%0Aviews.%20These%20methods%20commonly%20resort%20to%20using%20exponential%20family%20functions%2C%0Asuch%20as%20the%20Gaussian%20function%2C%20as%20reconstruction%20kernels%20due%20to%20their%0Aanisotropic%20nature%2C%20ease%20of%20projection%2C%20and%20differentiability%20in%20rasterization.%0AHowever%2C%20the%20field%20remains%20restricted%20to%20variations%20within%20the%20exponential%0Afamily%2C%20leaving%20generalized%20reconstruction%20kernels%20largely%20underexplored%2C%0Apartly%20due%20to%20the%20lack%20of%20easy%20integrability%20in%203D%20to%202D%20projections.%20In%20this%0Alight%2C%20we%20show%20that%20a%20class%20of%20decaying%20anisotropic%20radial%20basis%20functions%0A%28DARBFs%29%2C%20which%20are%20non-negative%20functions%20of%20the%20Mahalanobis%20distance%2C%0Asupports%20splatting%20by%20approximating%20the%20Gaussian%20function%27s%20closed-form%0Aintegration%20advantage.%20With%20this%20fresh%20perspective%2C%20we%20demonstrate%20up%20to%2034%25%0Afaster%20convergence%20during%20training%20and%20a%2015%25%20reduction%20in%20memory%20consumption%0Aacross%20various%20DARB%20reconstruction%20kernels%2C%20while%20maintaining%20comparable%20PSNR%2C%0ASSIM%2C%20and%20LPIPS%20results.%20We%20will%20make%20the%20code%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDARB-Splatting%253A%2520Generalizing%2520Splatting%2520with%2520Decaying%2520Anisotropic%2520Radial%250A%2520%2520Basis%2520Functions%26entry.906535625%3DVishagar%2520Arunan%2520and%2520Saeedha%2520Nazar%2520and%2520Hashiru%2520Pramuditha%2520and%2520Vinasirajan%2520Viruthshaan%2520and%2520Sameera%2520Ramasinghe%2520and%2520Simon%2520Lucey%2520and%2520Ranga%2520Rodrigo%26entry.1292438233%3D%2520%2520Splatting-based%25203D%2520reconstruction%2520methods%2520have%2520gained%2520popularity%2520with%2520the%250Aadvent%2520of%25203D%2520Gaussian%2520Splatting%252C%2520efficiently%2520synthesizing%2520high-quality%2520novel%250Aviews.%2520These%2520methods%2520commonly%2520resort%2520to%2520using%2520exponential%2520family%2520functions%252C%250Asuch%2520as%2520the%2520Gaussian%2520function%252C%2520as%2520reconstruction%2520kernels%2520due%2520to%2520their%250Aanisotropic%2520nature%252C%2520ease%2520of%2520projection%252C%2520and%2520differentiability%2520in%2520rasterization.%250AHowever%252C%2520the%2520field%2520remains%2520restricted%2520to%2520variations%2520within%2520the%2520exponential%250Afamily%252C%2520leaving%2520generalized%2520reconstruction%2520kernels%2520largely%2520underexplored%252C%250Apartly%2520due%2520to%2520the%2520lack%2520of%2520easy%2520integrability%2520in%25203D%2520to%25202D%2520projections.%2520In%2520this%250Alight%252C%2520we%2520show%2520that%2520a%2520class%2520of%2520decaying%2520anisotropic%2520radial%2520basis%2520functions%250A%2528DARBFs%2529%252C%2520which%2520are%2520non-negative%2520functions%2520of%2520the%2520Mahalanobis%2520distance%252C%250Asupports%2520splatting%2520by%2520approximating%2520the%2520Gaussian%2520function%2527s%2520closed-form%250Aintegration%2520advantage.%2520With%2520this%2520fresh%2520perspective%252C%2520we%2520demonstrate%2520up%2520to%252034%2525%250Afaster%2520convergence%2520during%2520training%2520and%2520a%252015%2525%2520reduction%2520in%2520memory%2520consumption%250Aacross%2520various%2520DARB%2520reconstruction%2520kernels%252C%2520while%2520maintaining%2520comparable%2520PSNR%252C%250ASSIM%252C%2520and%2520LPIPS%2520results.%2520We%2520will%2520make%2520the%2520code%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DARB-Splatting%3A%20Generalizing%20Splatting%20with%20Decaying%20Anisotropic%20Radial%0A%20%20Basis%20Functions&entry.906535625=Vishagar%20Arunan%20and%20Saeedha%20Nazar%20and%20Hashiru%20Pramuditha%20and%20Vinasirajan%20Viruthshaan%20and%20Sameera%20Ramasinghe%20and%20Simon%20Lucey%20and%20Ranga%20Rodrigo&entry.1292438233=%20%20Splatting-based%203D%20reconstruction%20methods%20have%20gained%20popularity%20with%20the%0Aadvent%20of%203D%20Gaussian%20Splatting%2C%20efficiently%20synthesizing%20high-quality%20novel%0Aviews.%20These%20methods%20commonly%20resort%20to%20using%20exponential%20family%20functions%2C%0Asuch%20as%20the%20Gaussian%20function%2C%20as%20reconstruction%20kernels%20due%20to%20their%0Aanisotropic%20nature%2C%20ease%20of%20projection%2C%20and%20differentiability%20in%20rasterization.%0AHowever%2C%20the%20field%20remains%20restricted%20to%20variations%20within%20the%20exponential%0Afamily%2C%20leaving%20generalized%20reconstruction%20kernels%20largely%20underexplored%2C%0Apartly%20due%20to%20the%20lack%20of%20easy%20integrability%20in%203D%20to%202D%20projections.%20In%20this%0Alight%2C%20we%20show%20that%20a%20class%20of%20decaying%20anisotropic%20radial%20basis%20functions%0A%28DARBFs%29%2C%20which%20are%20non-negative%20functions%20of%20the%20Mahalanobis%20distance%2C%0Asupports%20splatting%20by%20approximating%20the%20Gaussian%20function%27s%20closed-form%0Aintegration%20advantage.%20With%20this%20fresh%20perspective%2C%20we%20demonstrate%20up%20to%2034%25%0Afaster%20convergence%20during%20training%20and%20a%2015%25%20reduction%20in%20memory%20consumption%0Aacross%20various%20DARB%20reconstruction%20kernels%2C%20while%20maintaining%20comparable%20PSNR%2C%0ASSIM%2C%20and%20LPIPS%20results.%20We%20will%20make%20the%20code%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12369v1&entry.124074799=Read"},
{"title": "Explainability for Vision Foundation Models: A Survey", "author": "R\u00e9mi Kazmierczak and Elo\u00efse Berthier and Goran Frehse and Gianni Franchi", "abstract": "  As artificial intelligence systems become increasingly integrated into daily\nlife, the field of explainability has gained significant attention. This trend\nis particularly driven by the complexity of modern AI models and their\ndecision-making processes. The advent of foundation models, characterized by\ntheir extensive generalization capabilities and emergent uses, has further\ncomplicated this landscape. Foundation models occupy an ambiguous position in\nthe explainability domain: their complexity makes them inherently challenging\nto interpret, yet they are increasingly leveraged as tools to construct\nexplainable models. In this survey, we explore the intersection of foundation\nmodels and eXplainable AI (XAI) in the vision domain. We begin by compiling a\ncomprehensive corpus of papers that bridge these fields. Next, we categorize\nthese works based on their architectural characteristics. We then discuss the\nchallenges faced by current research in integrating XAI within foundation\nmodels. Furthermore, we review common evaluation methodologies for these\ncombined approaches. Finally, we present key observations and insights from our\nsurvey, offering directions for future research in this rapidly evolving field.\n", "link": "http://arxiv.org/abs/2501.12203v1", "date": "2025-01-21", "relevancy": 2.9049, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6122}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6122}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainability%20for%20Vision%20Foundation%20Models%3A%20A%20Survey&body=Title%3A%20Explainability%20for%20Vision%20Foundation%20Models%3A%20A%20Survey%0AAuthor%3A%20R%C3%A9mi%20Kazmierczak%20and%20Elo%C3%AFse%20Berthier%20and%20Goran%20Frehse%20and%20Gianni%20Franchi%0AAbstract%3A%20%20%20As%20artificial%20intelligence%20systems%20become%20increasingly%20integrated%20into%20daily%0Alife%2C%20the%20field%20of%20explainability%20has%20gained%20significant%20attention.%20This%20trend%0Ais%20particularly%20driven%20by%20the%20complexity%20of%20modern%20AI%20models%20and%20their%0Adecision-making%20processes.%20The%20advent%20of%20foundation%20models%2C%20characterized%20by%0Atheir%20extensive%20generalization%20capabilities%20and%20emergent%20uses%2C%20has%20further%0Acomplicated%20this%20landscape.%20Foundation%20models%20occupy%20an%20ambiguous%20position%20in%0Athe%20explainability%20domain%3A%20their%20complexity%20makes%20them%20inherently%20challenging%0Ato%20interpret%2C%20yet%20they%20are%20increasingly%20leveraged%20as%20tools%20to%20construct%0Aexplainable%20models.%20In%20this%20survey%2C%20we%20explore%20the%20intersection%20of%20foundation%0Amodels%20and%20eXplainable%20AI%20%28XAI%29%20in%20the%20vision%20domain.%20We%20begin%20by%20compiling%20a%0Acomprehensive%20corpus%20of%20papers%20that%20bridge%20these%20fields.%20Next%2C%20we%20categorize%0Athese%20works%20based%20on%20their%20architectural%20characteristics.%20We%20then%20discuss%20the%0Achallenges%20faced%20by%20current%20research%20in%20integrating%20XAI%20within%20foundation%0Amodels.%20Furthermore%2C%20we%20review%20common%20evaluation%20methodologies%20for%20these%0Acombined%20approaches.%20Finally%2C%20we%20present%20key%20observations%20and%20insights%20from%20our%0Asurvey%2C%20offering%20directions%20for%20future%20research%20in%20this%20rapidly%20evolving%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainability%2520for%2520Vision%2520Foundation%2520Models%253A%2520A%2520Survey%26entry.906535625%3DR%25C3%25A9mi%2520Kazmierczak%2520and%2520Elo%25C3%25AFse%2520Berthier%2520and%2520Goran%2520Frehse%2520and%2520Gianni%2520Franchi%26entry.1292438233%3D%2520%2520As%2520artificial%2520intelligence%2520systems%2520become%2520increasingly%2520integrated%2520into%2520daily%250Alife%252C%2520the%2520field%2520of%2520explainability%2520has%2520gained%2520significant%2520attention.%2520This%2520trend%250Ais%2520particularly%2520driven%2520by%2520the%2520complexity%2520of%2520modern%2520AI%2520models%2520and%2520their%250Adecision-making%2520processes.%2520The%2520advent%2520of%2520foundation%2520models%252C%2520characterized%2520by%250Atheir%2520extensive%2520generalization%2520capabilities%2520and%2520emergent%2520uses%252C%2520has%2520further%250Acomplicated%2520this%2520landscape.%2520Foundation%2520models%2520occupy%2520an%2520ambiguous%2520position%2520in%250Athe%2520explainability%2520domain%253A%2520their%2520complexity%2520makes%2520them%2520inherently%2520challenging%250Ato%2520interpret%252C%2520yet%2520they%2520are%2520increasingly%2520leveraged%2520as%2520tools%2520to%2520construct%250Aexplainable%2520models.%2520In%2520this%2520survey%252C%2520we%2520explore%2520the%2520intersection%2520of%2520foundation%250Amodels%2520and%2520eXplainable%2520AI%2520%2528XAI%2529%2520in%2520the%2520vision%2520domain.%2520We%2520begin%2520by%2520compiling%2520a%250Acomprehensive%2520corpus%2520of%2520papers%2520that%2520bridge%2520these%2520fields.%2520Next%252C%2520we%2520categorize%250Athese%2520works%2520based%2520on%2520their%2520architectural%2520characteristics.%2520We%2520then%2520discuss%2520the%250Achallenges%2520faced%2520by%2520current%2520research%2520in%2520integrating%2520XAI%2520within%2520foundation%250Amodels.%2520Furthermore%252C%2520we%2520review%2520common%2520evaluation%2520methodologies%2520for%2520these%250Acombined%2520approaches.%2520Finally%252C%2520we%2520present%2520key%2520observations%2520and%2520insights%2520from%2520our%250Asurvey%252C%2520offering%2520directions%2520for%2520future%2520research%2520in%2520this%2520rapidly%2520evolving%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainability%20for%20Vision%20Foundation%20Models%3A%20A%20Survey&entry.906535625=R%C3%A9mi%20Kazmierczak%20and%20Elo%C3%AFse%20Berthier%20and%20Goran%20Frehse%20and%20Gianni%20Franchi&entry.1292438233=%20%20As%20artificial%20intelligence%20systems%20become%20increasingly%20integrated%20into%20daily%0Alife%2C%20the%20field%20of%20explainability%20has%20gained%20significant%20attention.%20This%20trend%0Ais%20particularly%20driven%20by%20the%20complexity%20of%20modern%20AI%20models%20and%20their%0Adecision-making%20processes.%20The%20advent%20of%20foundation%20models%2C%20characterized%20by%0Atheir%20extensive%20generalization%20capabilities%20and%20emergent%20uses%2C%20has%20further%0Acomplicated%20this%20landscape.%20Foundation%20models%20occupy%20an%20ambiguous%20position%20in%0Athe%20explainability%20domain%3A%20their%20complexity%20makes%20them%20inherently%20challenging%0Ato%20interpret%2C%20yet%20they%20are%20increasingly%20leveraged%20as%20tools%20to%20construct%0Aexplainable%20models.%20In%20this%20survey%2C%20we%20explore%20the%20intersection%20of%20foundation%0Amodels%20and%20eXplainable%20AI%20%28XAI%29%20in%20the%20vision%20domain.%20We%20begin%20by%20compiling%20a%0Acomprehensive%20corpus%20of%20papers%20that%20bridge%20these%20fields.%20Next%2C%20we%20categorize%0Athese%20works%20based%20on%20their%20architectural%20characteristics.%20We%20then%20discuss%20the%0Achallenges%20faced%20by%20current%20research%20in%20integrating%20XAI%20within%20foundation%0Amodels.%20Furthermore%2C%20we%20review%20common%20evaluation%20methodologies%20for%20these%0Acombined%20approaches.%20Finally%2C%20we%20present%20key%20observations%20and%20insights%20from%20our%0Asurvey%2C%20offering%20directions%20for%20future%20research%20in%20this%20rapidly%20evolving%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12203v1&entry.124074799=Read"},
{"title": "Unified 3D MRI Representations via Sequence-Invariant Contrastive\n  Learning", "author": "Liam Chalcroft and Jenny Cronin and Cathy J. Price and John Ashburner", "abstract": "  Self-supervised deep learning has accelerated 2D natural image analysis but\nremains difficult to translate into 3D MRI, where data are scarce and\npre-trained 2D backbones cannot capture volumetric context. We present a\nsequence-invariant self-supervised framework leveraging quantitative MRI\n(qMRI). By simulating multiple MRI contrasts from a single 3D qMRI scan and\nenforcing consistent representations across these contrasts, we learn\nanatomy-centric rather than sequence-specific features. This yields a robust 3D\nencoder that performs strongly across varied tasks and protocols. Experiments\non healthy brain segmentation (IXI), stroke lesion segmentation (ARC), and MRI\ndenoising show significant gains over baseline SSL approaches, especially in\nlow-data settings (up to +8.3% Dice, +4.2 dB PSNR). Our model also generalises\neffectively to unseen sites, demonstrating potential for more scalable and\nclinically reliable volumetric analysis. All code and trained models are\npublicly available.\n", "link": "http://arxiv.org/abs/2501.12057v1", "date": "2025-01-21", "relevancy": 2.8929, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5784}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%203D%20MRI%20Representations%20via%20Sequence-Invariant%20Contrastive%0A%20%20Learning&body=Title%3A%20Unified%203D%20MRI%20Representations%20via%20Sequence-Invariant%20Contrastive%0A%20%20Learning%0AAuthor%3A%20Liam%20Chalcroft%20and%20Jenny%20Cronin%20and%20Cathy%20J.%20Price%20and%20John%20Ashburner%0AAbstract%3A%20%20%20Self-supervised%20deep%20learning%20has%20accelerated%202D%20natural%20image%20analysis%20but%0Aremains%20difficult%20to%20translate%20into%203D%20MRI%2C%20where%20data%20are%20scarce%20and%0Apre-trained%202D%20backbones%20cannot%20capture%20volumetric%20context.%20We%20present%20a%0Asequence-invariant%20self-supervised%20framework%20leveraging%20quantitative%20MRI%0A%28qMRI%29.%20By%20simulating%20multiple%20MRI%20contrasts%20from%20a%20single%203D%20qMRI%20scan%20and%0Aenforcing%20consistent%20representations%20across%20these%20contrasts%2C%20we%20learn%0Aanatomy-centric%20rather%20than%20sequence-specific%20features.%20This%20yields%20a%20robust%203D%0Aencoder%20that%20performs%20strongly%20across%20varied%20tasks%20and%20protocols.%20Experiments%0Aon%20healthy%20brain%20segmentation%20%28IXI%29%2C%20stroke%20lesion%20segmentation%20%28ARC%29%2C%20and%20MRI%0Adenoising%20show%20significant%20gains%20over%20baseline%20SSL%20approaches%2C%20especially%20in%0Alow-data%20settings%20%28up%20to%20%2B8.3%25%20Dice%2C%20%2B4.2%20dB%20PSNR%29.%20Our%20model%20also%20generalises%0Aeffectively%20to%20unseen%20sites%2C%20demonstrating%20potential%20for%20more%20scalable%20and%0Aclinically%20reliable%20volumetric%20analysis.%20All%20code%20and%20trained%20models%20are%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%25203D%2520MRI%2520Representations%2520via%2520Sequence-Invariant%2520Contrastive%250A%2520%2520Learning%26entry.906535625%3DLiam%2520Chalcroft%2520and%2520Jenny%2520Cronin%2520and%2520Cathy%2520J.%2520Price%2520and%2520John%2520Ashburner%26entry.1292438233%3D%2520%2520Self-supervised%2520deep%2520learning%2520has%2520accelerated%25202D%2520natural%2520image%2520analysis%2520but%250Aremains%2520difficult%2520to%2520translate%2520into%25203D%2520MRI%252C%2520where%2520data%2520are%2520scarce%2520and%250Apre-trained%25202D%2520backbones%2520cannot%2520capture%2520volumetric%2520context.%2520We%2520present%2520a%250Asequence-invariant%2520self-supervised%2520framework%2520leveraging%2520quantitative%2520MRI%250A%2528qMRI%2529.%2520By%2520simulating%2520multiple%2520MRI%2520contrasts%2520from%2520a%2520single%25203D%2520qMRI%2520scan%2520and%250Aenforcing%2520consistent%2520representations%2520across%2520these%2520contrasts%252C%2520we%2520learn%250Aanatomy-centric%2520rather%2520than%2520sequence-specific%2520features.%2520This%2520yields%2520a%2520robust%25203D%250Aencoder%2520that%2520performs%2520strongly%2520across%2520varied%2520tasks%2520and%2520protocols.%2520Experiments%250Aon%2520healthy%2520brain%2520segmentation%2520%2528IXI%2529%252C%2520stroke%2520lesion%2520segmentation%2520%2528ARC%2529%252C%2520and%2520MRI%250Adenoising%2520show%2520significant%2520gains%2520over%2520baseline%2520SSL%2520approaches%252C%2520especially%2520in%250Alow-data%2520settings%2520%2528up%2520to%2520%252B8.3%2525%2520Dice%252C%2520%252B4.2%2520dB%2520PSNR%2529.%2520Our%2520model%2520also%2520generalises%250Aeffectively%2520to%2520unseen%2520sites%252C%2520demonstrating%2520potential%2520for%2520more%2520scalable%2520and%250Aclinically%2520reliable%2520volumetric%2520analysis.%2520All%2520code%2520and%2520trained%2520models%2520are%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%203D%20MRI%20Representations%20via%20Sequence-Invariant%20Contrastive%0A%20%20Learning&entry.906535625=Liam%20Chalcroft%20and%20Jenny%20Cronin%20and%20Cathy%20J.%20Price%20and%20John%20Ashburner&entry.1292438233=%20%20Self-supervised%20deep%20learning%20has%20accelerated%202D%20natural%20image%20analysis%20but%0Aremains%20difficult%20to%20translate%20into%203D%20MRI%2C%20where%20data%20are%20scarce%20and%0Apre-trained%202D%20backbones%20cannot%20capture%20volumetric%20context.%20We%20present%20a%0Asequence-invariant%20self-supervised%20framework%20leveraging%20quantitative%20MRI%0A%28qMRI%29.%20By%20simulating%20multiple%20MRI%20contrasts%20from%20a%20single%203D%20qMRI%20scan%20and%0Aenforcing%20consistent%20representations%20across%20these%20contrasts%2C%20we%20learn%0Aanatomy-centric%20rather%20than%20sequence-specific%20features.%20This%20yields%20a%20robust%203D%0Aencoder%20that%20performs%20strongly%20across%20varied%20tasks%20and%20protocols.%20Experiments%0Aon%20healthy%20brain%20segmentation%20%28IXI%29%2C%20stroke%20lesion%20segmentation%20%28ARC%29%2C%20and%20MRI%0Adenoising%20show%20significant%20gains%20over%20baseline%20SSL%20approaches%2C%20especially%20in%0Alow-data%20settings%20%28up%20to%20%2B8.3%25%20Dice%2C%20%2B4.2%20dB%20PSNR%29.%20Our%20model%20also%20generalises%0Aeffectively%20to%20unseen%20sites%2C%20demonstrating%20potential%20for%20more%20scalable%20and%0Aclinically%20reliable%20volumetric%20analysis.%20All%20code%20and%20trained%20models%20are%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12057v1&entry.124074799=Read"},
{"title": "BlanketGen2-Fit3D: Synthetic Blanket Augmentation Towards Improving\n  Real-World In-Bed Blanket Occluded Human Pose Estimation", "author": "Tam\u00e1s Kar\u00e1csony and Jo\u00e3o Carmona and Jo\u00e3o Paulo Silva Cunha", "abstract": "  Human Pose Estimation (HPE) from monocular RGB images is crucial for clinical\nin-bed skeleton-based action recognition, however, it poses unique challenges\nfor HPE models due to the frequent presence of blankets occluding the person,\nwhile labeled HPE data in this scenario is scarce. To address this we introduce\nBlanketGen2-Fit3D (BG2-Fit3D), an augmentation of Fit3D dataset that contains\n1,217,312 frames with synthetic photo-realistic blankets. To generate it we\nused BlanketGen2, our new and improved version of our BlanketGen pipeline that\nsimulates synthetic blankets using ground-truth Skinned Multi-Person Linear\nmodel (SMPL) meshes and then renders them as transparent images that can be\nlayered on top of the original frames. This dataset was used in combination\nwith the original Fit3D to finetune the ViTPose-B HPE model, to evaluate\nsynthetic blanket augmentation effectiveness. The trained models were further\nevaluated on a real-world blanket occluded in-bed HPE dataset (SLP dataset).\nComparing architectures trained on only Fit3D with the ones trained with our\nsynthetic blanket augmentation the later improved pose estimation performance\non BG2-Fit3D, the synthetic blanket occluded dataset significantly to (0.977\nPercentage of Correct Keypoints (PCK), 0.149 Normalized Mean Error (NME)) with\nan absolute 4.4% PCK increase. Furthermore, the test results on SLP\ndemonstrated the utility of synthetic data augmentation by improving\nperformance by an absolute 2.3% PCK, on real-world images with the poses\noccluded by real blankets. These results show synthetic blanket augmentation\nhas the potential to improve in-bed blanket occluded HPE from RGB images. The\ndataset as well as the code will be made available to the public.\n", "link": "http://arxiv.org/abs/2501.12318v1", "date": "2025-01-21", "relevancy": 2.8241, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6027}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5735}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BlanketGen2-Fit3D%3A%20Synthetic%20Blanket%20Augmentation%20Towards%20Improving%0A%20%20Real-World%20In-Bed%20Blanket%20Occluded%20Human%20Pose%20Estimation&body=Title%3A%20BlanketGen2-Fit3D%3A%20Synthetic%20Blanket%20Augmentation%20Towards%20Improving%0A%20%20Real-World%20In-Bed%20Blanket%20Occluded%20Human%20Pose%20Estimation%0AAuthor%3A%20Tam%C3%A1s%20Kar%C3%A1csony%20and%20Jo%C3%A3o%20Carmona%20and%20Jo%C3%A3o%20Paulo%20Silva%20Cunha%0AAbstract%3A%20%20%20Human%20Pose%20Estimation%20%28HPE%29%20from%20monocular%20RGB%20images%20is%20crucial%20for%20clinical%0Ain-bed%20skeleton-based%20action%20recognition%2C%20however%2C%20it%20poses%20unique%20challenges%0Afor%20HPE%20models%20due%20to%20the%20frequent%20presence%20of%20blankets%20occluding%20the%20person%2C%0Awhile%20labeled%20HPE%20data%20in%20this%20scenario%20is%20scarce.%20To%20address%20this%20we%20introduce%0ABlanketGen2-Fit3D%20%28BG2-Fit3D%29%2C%20an%20augmentation%20of%20Fit3D%20dataset%20that%20contains%0A1%2C217%2C312%20frames%20with%20synthetic%20photo-realistic%20blankets.%20To%20generate%20it%20we%0Aused%20BlanketGen2%2C%20our%20new%20and%20improved%20version%20of%20our%20BlanketGen%20pipeline%20that%0Asimulates%20synthetic%20blankets%20using%20ground-truth%20Skinned%20Multi-Person%20Linear%0Amodel%20%28SMPL%29%20meshes%20and%20then%20renders%20them%20as%20transparent%20images%20that%20can%20be%0Alayered%20on%20top%20of%20the%20original%20frames.%20This%20dataset%20was%20used%20in%20combination%0Awith%20the%20original%20Fit3D%20to%20finetune%20the%20ViTPose-B%20HPE%20model%2C%20to%20evaluate%0Asynthetic%20blanket%20augmentation%20effectiveness.%20The%20trained%20models%20were%20further%0Aevaluated%20on%20a%20real-world%20blanket%20occluded%20in-bed%20HPE%20dataset%20%28SLP%20dataset%29.%0AComparing%20architectures%20trained%20on%20only%20Fit3D%20with%20the%20ones%20trained%20with%20our%0Asynthetic%20blanket%20augmentation%20the%20later%20improved%20pose%20estimation%20performance%0Aon%20BG2-Fit3D%2C%20the%20synthetic%20blanket%20occluded%20dataset%20significantly%20to%20%280.977%0APercentage%20of%20Correct%20Keypoints%20%28PCK%29%2C%200.149%20Normalized%20Mean%20Error%20%28NME%29%29%20with%0Aan%20absolute%204.4%25%20PCK%20increase.%20Furthermore%2C%20the%20test%20results%20on%20SLP%0Ademonstrated%20the%20utility%20of%20synthetic%20data%20augmentation%20by%20improving%0Aperformance%20by%20an%20absolute%202.3%25%20PCK%2C%20on%20real-world%20images%20with%20the%20poses%0Aoccluded%20by%20real%20blankets.%20These%20results%20show%20synthetic%20blanket%20augmentation%0Ahas%20the%20potential%20to%20improve%20in-bed%20blanket%20occluded%20HPE%20from%20RGB%20images.%20The%0Adataset%20as%20well%20as%20the%20code%20will%20be%20made%20available%20to%20the%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlanketGen2-Fit3D%253A%2520Synthetic%2520Blanket%2520Augmentation%2520Towards%2520Improving%250A%2520%2520Real-World%2520In-Bed%2520Blanket%2520Occluded%2520Human%2520Pose%2520Estimation%26entry.906535625%3DTam%25C3%25A1s%2520Kar%25C3%25A1csony%2520and%2520Jo%25C3%25A3o%2520Carmona%2520and%2520Jo%25C3%25A3o%2520Paulo%2520Silva%2520Cunha%26entry.1292438233%3D%2520%2520Human%2520Pose%2520Estimation%2520%2528HPE%2529%2520from%2520monocular%2520RGB%2520images%2520is%2520crucial%2520for%2520clinical%250Ain-bed%2520skeleton-based%2520action%2520recognition%252C%2520however%252C%2520it%2520poses%2520unique%2520challenges%250Afor%2520HPE%2520models%2520due%2520to%2520the%2520frequent%2520presence%2520of%2520blankets%2520occluding%2520the%2520person%252C%250Awhile%2520labeled%2520HPE%2520data%2520in%2520this%2520scenario%2520is%2520scarce.%2520To%2520address%2520this%2520we%2520introduce%250ABlanketGen2-Fit3D%2520%2528BG2-Fit3D%2529%252C%2520an%2520augmentation%2520of%2520Fit3D%2520dataset%2520that%2520contains%250A1%252C217%252C312%2520frames%2520with%2520synthetic%2520photo-realistic%2520blankets.%2520To%2520generate%2520it%2520we%250Aused%2520BlanketGen2%252C%2520our%2520new%2520and%2520improved%2520version%2520of%2520our%2520BlanketGen%2520pipeline%2520that%250Asimulates%2520synthetic%2520blankets%2520using%2520ground-truth%2520Skinned%2520Multi-Person%2520Linear%250Amodel%2520%2528SMPL%2529%2520meshes%2520and%2520then%2520renders%2520them%2520as%2520transparent%2520images%2520that%2520can%2520be%250Alayered%2520on%2520top%2520of%2520the%2520original%2520frames.%2520This%2520dataset%2520was%2520used%2520in%2520combination%250Awith%2520the%2520original%2520Fit3D%2520to%2520finetune%2520the%2520ViTPose-B%2520HPE%2520model%252C%2520to%2520evaluate%250Asynthetic%2520blanket%2520augmentation%2520effectiveness.%2520The%2520trained%2520models%2520were%2520further%250Aevaluated%2520on%2520a%2520real-world%2520blanket%2520occluded%2520in-bed%2520HPE%2520dataset%2520%2528SLP%2520dataset%2529.%250AComparing%2520architectures%2520trained%2520on%2520only%2520Fit3D%2520with%2520the%2520ones%2520trained%2520with%2520our%250Asynthetic%2520blanket%2520augmentation%2520the%2520later%2520improved%2520pose%2520estimation%2520performance%250Aon%2520BG2-Fit3D%252C%2520the%2520synthetic%2520blanket%2520occluded%2520dataset%2520significantly%2520to%2520%25280.977%250APercentage%2520of%2520Correct%2520Keypoints%2520%2528PCK%2529%252C%25200.149%2520Normalized%2520Mean%2520Error%2520%2528NME%2529%2529%2520with%250Aan%2520absolute%25204.4%2525%2520PCK%2520increase.%2520Furthermore%252C%2520the%2520test%2520results%2520on%2520SLP%250Ademonstrated%2520the%2520utility%2520of%2520synthetic%2520data%2520augmentation%2520by%2520improving%250Aperformance%2520by%2520an%2520absolute%25202.3%2525%2520PCK%252C%2520on%2520real-world%2520images%2520with%2520the%2520poses%250Aoccluded%2520by%2520real%2520blankets.%2520These%2520results%2520show%2520synthetic%2520blanket%2520augmentation%250Ahas%2520the%2520potential%2520to%2520improve%2520in-bed%2520blanket%2520occluded%2520HPE%2520from%2520RGB%2520images.%2520The%250Adataset%2520as%2520well%2520as%2520the%2520code%2520will%2520be%2520made%2520available%2520to%2520the%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BlanketGen2-Fit3D%3A%20Synthetic%20Blanket%20Augmentation%20Towards%20Improving%0A%20%20Real-World%20In-Bed%20Blanket%20Occluded%20Human%20Pose%20Estimation&entry.906535625=Tam%C3%A1s%20Kar%C3%A1csony%20and%20Jo%C3%A3o%20Carmona%20and%20Jo%C3%A3o%20Paulo%20Silva%20Cunha&entry.1292438233=%20%20Human%20Pose%20Estimation%20%28HPE%29%20from%20monocular%20RGB%20images%20is%20crucial%20for%20clinical%0Ain-bed%20skeleton-based%20action%20recognition%2C%20however%2C%20it%20poses%20unique%20challenges%0Afor%20HPE%20models%20due%20to%20the%20frequent%20presence%20of%20blankets%20occluding%20the%20person%2C%0Awhile%20labeled%20HPE%20data%20in%20this%20scenario%20is%20scarce.%20To%20address%20this%20we%20introduce%0ABlanketGen2-Fit3D%20%28BG2-Fit3D%29%2C%20an%20augmentation%20of%20Fit3D%20dataset%20that%20contains%0A1%2C217%2C312%20frames%20with%20synthetic%20photo-realistic%20blankets.%20To%20generate%20it%20we%0Aused%20BlanketGen2%2C%20our%20new%20and%20improved%20version%20of%20our%20BlanketGen%20pipeline%20that%0Asimulates%20synthetic%20blankets%20using%20ground-truth%20Skinned%20Multi-Person%20Linear%0Amodel%20%28SMPL%29%20meshes%20and%20then%20renders%20them%20as%20transparent%20images%20that%20can%20be%0Alayered%20on%20top%20of%20the%20original%20frames.%20This%20dataset%20was%20used%20in%20combination%0Awith%20the%20original%20Fit3D%20to%20finetune%20the%20ViTPose-B%20HPE%20model%2C%20to%20evaluate%0Asynthetic%20blanket%20augmentation%20effectiveness.%20The%20trained%20models%20were%20further%0Aevaluated%20on%20a%20real-world%20blanket%20occluded%20in-bed%20HPE%20dataset%20%28SLP%20dataset%29.%0AComparing%20architectures%20trained%20on%20only%20Fit3D%20with%20the%20ones%20trained%20with%20our%0Asynthetic%20blanket%20augmentation%20the%20later%20improved%20pose%20estimation%20performance%0Aon%20BG2-Fit3D%2C%20the%20synthetic%20blanket%20occluded%20dataset%20significantly%20to%20%280.977%0APercentage%20of%20Correct%20Keypoints%20%28PCK%29%2C%200.149%20Normalized%20Mean%20Error%20%28NME%29%29%20with%0Aan%20absolute%204.4%25%20PCK%20increase.%20Furthermore%2C%20the%20test%20results%20on%20SLP%0Ademonstrated%20the%20utility%20of%20synthetic%20data%20augmentation%20by%20improving%0Aperformance%20by%20an%20absolute%202.3%25%20PCK%2C%20on%20real-world%20images%20with%20the%20poses%0Aoccluded%20by%20real%20blankets.%20These%20results%20show%20synthetic%20blanket%20augmentation%0Ahas%20the%20potential%20to%20improve%20in-bed%20blanket%20occluded%20HPE%20from%20RGB%20images.%20The%0Adataset%20as%20well%20as%20the%20code%20will%20be%20made%20available%20to%20the%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12318v1&entry.124074799=Read"},
{"title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction", "author": "Chaoyou Fu and Haojia Lin and Xiong Wang and Yi-Fan Zhang and Yunhang Shen and Xiaoyu Liu and Haoyu Cao and Zuwei Long and Heting Gao and Ke Li and Long Ma and Xiawu Zheng and Rongrong Ji and Xing Sun and Caifeng Shan and Ran He", "abstract": "  Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.\n", "link": "http://arxiv.org/abs/2501.01957v3", "date": "2025-01-21", "relevancy": 2.7807, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VITA-1.5%3A%20Towards%20GPT-4o%20Level%20Real-Time%20Vision%20and%20Speech%20Interaction&body=Title%3A%20VITA-1.5%3A%20Towards%20GPT-4o%20Level%20Real-Time%20Vision%20and%20Speech%20Interaction%0AAuthor%3A%20Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Xiong%20Wang%20and%20Yi-Fan%20Zhang%20and%20Yunhang%20Shen%20and%20Xiaoyu%20Liu%20and%20Haoyu%20Cao%20and%20Zuwei%20Long%20and%20Heting%20Gao%20and%20Ke%20Li%20and%20Long%20Ma%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Xing%20Sun%20and%20Caifeng%20Shan%20and%20Ran%20He%0AAbstract%3A%20%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20typically%20focused%20on%0Aintegrating%20visual%20and%20textual%20modalities%2C%20with%20less%20emphasis%20placed%20on%20the%0Arole%20of%20speech%20in%20enhancing%20interaction.%20However%2C%20speech%20plays%20a%20crucial%20role%0Ain%20multimodal%20dialogue%20systems%2C%20and%20implementing%20high-performance%20in%20both%0Avision%20and%20speech%20tasks%20remains%20a%20significant%20challenge%20due%20to%20the%20fundamental%0Amodality%20differences.%20In%20this%20paper%2C%20we%20propose%20a%20carefully%20designed%0Amulti-stage%20training%20methodology%20that%20progressively%20trains%20LLM%20to%20understand%0Aboth%20visual%20and%20speech%20information%2C%20ultimately%20enabling%20fluent%20vision%20and%0Aspeech%20interaction.%20Our%20approach%20not%20only%20preserves%20strong%20vision-language%0Acapacity%2C%20but%20also%20enables%20efficient%20speech-to-speech%20dialogue%20capabilities%0Awithout%20separate%20ASR%20and%20TTS%20modules%2C%20significantly%20accelerating%20multimodal%0Aend-to-end%20response%20speed.%20By%20comparing%20our%20method%20against%20state-of-the-art%0Acounterparts%20across%20benchmarks%20for%20image%2C%20video%2C%20and%20speech%20tasks%2C%20we%0Ademonstrate%20that%20our%20model%20is%20equipped%20with%20both%20strong%20visual%20and%20speech%0Acapabilities%2C%20making%20near%20real-time%20vision%20and%20speech%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01957v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVITA-1.5%253A%2520Towards%2520GPT-4o%2520Level%2520Real-Time%2520Vision%2520and%2520Speech%2520Interaction%26entry.906535625%3DChaoyou%2520Fu%2520and%2520Haojia%2520Lin%2520and%2520Xiong%2520Wang%2520and%2520Yi-Fan%2520Zhang%2520and%2520Yunhang%2520Shen%2520and%2520Xiaoyu%2520Liu%2520and%2520Haoyu%2520Cao%2520and%2520Zuwei%2520Long%2520and%2520Heting%2520Gao%2520and%2520Ke%2520Li%2520and%2520Long%2520Ma%2520and%2520Xiawu%2520Zheng%2520and%2520Rongrong%2520Ji%2520and%2520Xing%2520Sun%2520and%2520Caifeng%2520Shan%2520and%2520Ran%2520He%26entry.1292438233%3D%2520%2520Recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520typically%2520focused%2520on%250Aintegrating%2520visual%2520and%2520textual%2520modalities%252C%2520with%2520less%2520emphasis%2520placed%2520on%2520the%250Arole%2520of%2520speech%2520in%2520enhancing%2520interaction.%2520However%252C%2520speech%2520plays%2520a%2520crucial%2520role%250Ain%2520multimodal%2520dialogue%2520systems%252C%2520and%2520implementing%2520high-performance%2520in%2520both%250Avision%2520and%2520speech%2520tasks%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520fundamental%250Amodality%2520differences.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520carefully%2520designed%250Amulti-stage%2520training%2520methodology%2520that%2520progressively%2520trains%2520LLM%2520to%2520understand%250Aboth%2520visual%2520and%2520speech%2520information%252C%2520ultimately%2520enabling%2520fluent%2520vision%2520and%250Aspeech%2520interaction.%2520Our%2520approach%2520not%2520only%2520preserves%2520strong%2520vision-language%250Acapacity%252C%2520but%2520also%2520enables%2520efficient%2520speech-to-speech%2520dialogue%2520capabilities%250Awithout%2520separate%2520ASR%2520and%2520TTS%2520modules%252C%2520significantly%2520accelerating%2520multimodal%250Aend-to-end%2520response%2520speed.%2520By%2520comparing%2520our%2520method%2520against%2520state-of-the-art%250Acounterparts%2520across%2520benchmarks%2520for%2520image%252C%2520video%252C%2520and%2520speech%2520tasks%252C%2520we%250Ademonstrate%2520that%2520our%2520model%2520is%2520equipped%2520with%2520both%2520strong%2520visual%2520and%2520speech%250Acapabilities%252C%2520making%2520near%2520real-time%2520vision%2520and%2520speech%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01957v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VITA-1.5%3A%20Towards%20GPT-4o%20Level%20Real-Time%20Vision%20and%20Speech%20Interaction&entry.906535625=Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Xiong%20Wang%20and%20Yi-Fan%20Zhang%20and%20Yunhang%20Shen%20and%20Xiaoyu%20Liu%20and%20Haoyu%20Cao%20and%20Zuwei%20Long%20and%20Heting%20Gao%20and%20Ke%20Li%20and%20Long%20Ma%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Xing%20Sun%20and%20Caifeng%20Shan%20and%20Ran%20He&entry.1292438233=%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20typically%20focused%20on%0Aintegrating%20visual%20and%20textual%20modalities%2C%20with%20less%20emphasis%20placed%20on%20the%0Arole%20of%20speech%20in%20enhancing%20interaction.%20However%2C%20speech%20plays%20a%20crucial%20role%0Ain%20multimodal%20dialogue%20systems%2C%20and%20implementing%20high-performance%20in%20both%0Avision%20and%20speech%20tasks%20remains%20a%20significant%20challenge%20due%20to%20the%20fundamental%0Amodality%20differences.%20In%20this%20paper%2C%20we%20propose%20a%20carefully%20designed%0Amulti-stage%20training%20methodology%20that%20progressively%20trains%20LLM%20to%20understand%0Aboth%20visual%20and%20speech%20information%2C%20ultimately%20enabling%20fluent%20vision%20and%0Aspeech%20interaction.%20Our%20approach%20not%20only%20preserves%20strong%20vision-language%0Acapacity%2C%20but%20also%20enables%20efficient%20speech-to-speech%20dialogue%20capabilities%0Awithout%20separate%20ASR%20and%20TTS%20modules%2C%20significantly%20accelerating%20multimodal%0Aend-to-end%20response%20speed.%20By%20comparing%20our%20method%20against%20state-of-the-art%0Acounterparts%20across%20benchmarks%20for%20image%2C%20video%2C%20and%20speech%20tasks%2C%20we%0Ademonstrate%20that%20our%20model%20is%20equipped%20with%20both%20strong%20visual%20and%20speech%0Acapabilities%2C%20making%20near%20real-time%20vision%20and%20speech%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01957v3&entry.124074799=Read"},
{"title": "DSTSA-GCN: Advancing Skeleton-Based Gesture Recognition with\n  Semantic-Aware Spatio-Temporal Topology Modeling", "author": "Hu Cui and Renjing Huang and Ruoyu Zhang and Tessai Hayama", "abstract": "  Graph convolutional networks (GCNs) have emerged as a powerful tool for\nskeleton-based action and gesture recognition, thanks to their ability to model\nspatial and temporal dependencies in skeleton data. However, existing GCN-based\nmethods face critical limitations: (1) they lack effective spatio-temporal\ntopology modeling that captures dynamic variations in skeletal motion, and (2)\nthey struggle to model multiscale structural relationships beyond local joint\nconnectivity. To address these issues, we propose a novel framework called\nDynamic Spatial-Temporal Semantic Awareness Graph Convolutional Network\n(DSTSA-GCN). DSTSA-GCN introduces three key modules: Group Channel-wise Graph\nConvolution (GC-GC), Group Temporal-wise Graph Convolution (GT-GC), and\nMulti-Scale Temporal Convolution (MS-TCN). GC-GC and GT-GC operate in parallel\nto independently model channel-specific and frame-specific correlations,\nenabling robust topology learning that accounts for temporal variations.\nAdditionally, both modules employ a grouping strategy to adaptively capture\nmultiscale structural relationships. Complementing this, MS-TCN enhances\ntemporal modeling through group-wise temporal convolutions with diverse\nreceptive fields. Extensive experiments demonstrate that DSTSA-GCN\nsignificantly improves the topology modeling capabilities of GCNs, achieving\nstate-of-the-art performance on benchmark datasets for gesture and action\nrecognition, including SHREC17 Track, DHG-14\\/28, NTU-RGB+D, and NTU-RGB+D-120.\n", "link": "http://arxiv.org/abs/2501.12086v1", "date": "2025-01-21", "relevancy": 2.7568, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5649}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5511}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSTSA-GCN%3A%20Advancing%20Skeleton-Based%20Gesture%20Recognition%20with%0A%20%20Semantic-Aware%20Spatio-Temporal%20Topology%20Modeling&body=Title%3A%20DSTSA-GCN%3A%20Advancing%20Skeleton-Based%20Gesture%20Recognition%20with%0A%20%20Semantic-Aware%20Spatio-Temporal%20Topology%20Modeling%0AAuthor%3A%20Hu%20Cui%20and%20Renjing%20Huang%20and%20Ruoyu%20Zhang%20and%20Tessai%20Hayama%0AAbstract%3A%20%20%20Graph%20convolutional%20networks%20%28GCNs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%0Askeleton-based%20action%20and%20gesture%20recognition%2C%20thanks%20to%20their%20ability%20to%20model%0Aspatial%20and%20temporal%20dependencies%20in%20skeleton%20data.%20However%2C%20existing%20GCN-based%0Amethods%20face%20critical%20limitations%3A%20%281%29%20they%20lack%20effective%20spatio-temporal%0Atopology%20modeling%20that%20captures%20dynamic%20variations%20in%20skeletal%20motion%2C%20and%20%282%29%0Athey%20struggle%20to%20model%20multiscale%20structural%20relationships%20beyond%20local%20joint%0Aconnectivity.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20framework%20called%0ADynamic%20Spatial-Temporal%20Semantic%20Awareness%20Graph%20Convolutional%20Network%0A%28DSTSA-GCN%29.%20DSTSA-GCN%20introduces%20three%20key%20modules%3A%20Group%20Channel-wise%20Graph%0AConvolution%20%28GC-GC%29%2C%20Group%20Temporal-wise%20Graph%20Convolution%20%28GT-GC%29%2C%20and%0AMulti-Scale%20Temporal%20Convolution%20%28MS-TCN%29.%20GC-GC%20and%20GT-GC%20operate%20in%20parallel%0Ato%20independently%20model%20channel-specific%20and%20frame-specific%20correlations%2C%0Aenabling%20robust%20topology%20learning%20that%20accounts%20for%20temporal%20variations.%0AAdditionally%2C%20both%20modules%20employ%20a%20grouping%20strategy%20to%20adaptively%20capture%0Amultiscale%20structural%20relationships.%20Complementing%20this%2C%20MS-TCN%20enhances%0Atemporal%20modeling%20through%20group-wise%20temporal%20convolutions%20with%20diverse%0Areceptive%20fields.%20Extensive%20experiments%20demonstrate%20that%20DSTSA-GCN%0Asignificantly%20improves%20the%20topology%20modeling%20capabilities%20of%20GCNs%2C%20achieving%0Astate-of-the-art%20performance%20on%20benchmark%20datasets%20for%20gesture%20and%20action%0Arecognition%2C%20including%20SHREC17%20Track%2C%20DHG-14%5C/28%2C%20NTU-RGB%2BD%2C%20and%20NTU-RGB%2BD-120.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSTSA-GCN%253A%2520Advancing%2520Skeleton-Based%2520Gesture%2520Recognition%2520with%250A%2520%2520Semantic-Aware%2520Spatio-Temporal%2520Topology%2520Modeling%26entry.906535625%3DHu%2520Cui%2520and%2520Renjing%2520Huang%2520and%2520Ruoyu%2520Zhang%2520and%2520Tessai%2520Hayama%26entry.1292438233%3D%2520%2520Graph%2520convolutional%2520networks%2520%2528GCNs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%250Askeleton-based%2520action%2520and%2520gesture%2520recognition%252C%2520thanks%2520to%2520their%2520ability%2520to%2520model%250Aspatial%2520and%2520temporal%2520dependencies%2520in%2520skeleton%2520data.%2520However%252C%2520existing%2520GCN-based%250Amethods%2520face%2520critical%2520limitations%253A%2520%25281%2529%2520they%2520lack%2520effective%2520spatio-temporal%250Atopology%2520modeling%2520that%2520captures%2520dynamic%2520variations%2520in%2520skeletal%2520motion%252C%2520and%2520%25282%2529%250Athey%2520struggle%2520to%2520model%2520multiscale%2520structural%2520relationships%2520beyond%2520local%2520joint%250Aconnectivity.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520framework%2520called%250ADynamic%2520Spatial-Temporal%2520Semantic%2520Awareness%2520Graph%2520Convolutional%2520Network%250A%2528DSTSA-GCN%2529.%2520DSTSA-GCN%2520introduces%2520three%2520key%2520modules%253A%2520Group%2520Channel-wise%2520Graph%250AConvolution%2520%2528GC-GC%2529%252C%2520Group%2520Temporal-wise%2520Graph%2520Convolution%2520%2528GT-GC%2529%252C%2520and%250AMulti-Scale%2520Temporal%2520Convolution%2520%2528MS-TCN%2529.%2520GC-GC%2520and%2520GT-GC%2520operate%2520in%2520parallel%250Ato%2520independently%2520model%2520channel-specific%2520and%2520frame-specific%2520correlations%252C%250Aenabling%2520robust%2520topology%2520learning%2520that%2520accounts%2520for%2520temporal%2520variations.%250AAdditionally%252C%2520both%2520modules%2520employ%2520a%2520grouping%2520strategy%2520to%2520adaptively%2520capture%250Amultiscale%2520structural%2520relationships.%2520Complementing%2520this%252C%2520MS-TCN%2520enhances%250Atemporal%2520modeling%2520through%2520group-wise%2520temporal%2520convolutions%2520with%2520diverse%250Areceptive%2520fields.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DSTSA-GCN%250Asignificantly%2520improves%2520the%2520topology%2520modeling%2520capabilities%2520of%2520GCNs%252C%2520achieving%250Astate-of-the-art%2520performance%2520on%2520benchmark%2520datasets%2520for%2520gesture%2520and%2520action%250Arecognition%252C%2520including%2520SHREC17%2520Track%252C%2520DHG-14%255C/28%252C%2520NTU-RGB%252BD%252C%2520and%2520NTU-RGB%252BD-120.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSTSA-GCN%3A%20Advancing%20Skeleton-Based%20Gesture%20Recognition%20with%0A%20%20Semantic-Aware%20Spatio-Temporal%20Topology%20Modeling&entry.906535625=Hu%20Cui%20and%20Renjing%20Huang%20and%20Ruoyu%20Zhang%20and%20Tessai%20Hayama&entry.1292438233=%20%20Graph%20convolutional%20networks%20%28GCNs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%0Askeleton-based%20action%20and%20gesture%20recognition%2C%20thanks%20to%20their%20ability%20to%20model%0Aspatial%20and%20temporal%20dependencies%20in%20skeleton%20data.%20However%2C%20existing%20GCN-based%0Amethods%20face%20critical%20limitations%3A%20%281%29%20they%20lack%20effective%20spatio-temporal%0Atopology%20modeling%20that%20captures%20dynamic%20variations%20in%20skeletal%20motion%2C%20and%20%282%29%0Athey%20struggle%20to%20model%20multiscale%20structural%20relationships%20beyond%20local%20joint%0Aconnectivity.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20framework%20called%0ADynamic%20Spatial-Temporal%20Semantic%20Awareness%20Graph%20Convolutional%20Network%0A%28DSTSA-GCN%29.%20DSTSA-GCN%20introduces%20three%20key%20modules%3A%20Group%20Channel-wise%20Graph%0AConvolution%20%28GC-GC%29%2C%20Group%20Temporal-wise%20Graph%20Convolution%20%28GT-GC%29%2C%20and%0AMulti-Scale%20Temporal%20Convolution%20%28MS-TCN%29.%20GC-GC%20and%20GT-GC%20operate%20in%20parallel%0Ato%20independently%20model%20channel-specific%20and%20frame-specific%20correlations%2C%0Aenabling%20robust%20topology%20learning%20that%20accounts%20for%20temporal%20variations.%0AAdditionally%2C%20both%20modules%20employ%20a%20grouping%20strategy%20to%20adaptively%20capture%0Amultiscale%20structural%20relationships.%20Complementing%20this%2C%20MS-TCN%20enhances%0Atemporal%20modeling%20through%20group-wise%20temporal%20convolutions%20with%20diverse%0Areceptive%20fields.%20Extensive%20experiments%20demonstrate%20that%20DSTSA-GCN%0Asignificantly%20improves%20the%20topology%20modeling%20capabilities%20of%20GCNs%2C%20achieving%0Astate-of-the-art%20performance%20on%20benchmark%20datasets%20for%20gesture%20and%20action%0Arecognition%2C%20including%20SHREC17%20Track%2C%20DHG-14%5C/28%2C%20NTU-RGB%2BD%2C%20and%20NTU-RGB%2BD-120.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12086v1&entry.124074799=Read"},
{"title": "Fixing Imbalanced Attention to Mitigate In-Context Hallucination of\n  Large Vision-Language Model", "author": "Kazi Hasan Ibn Arif and Sajib Acharjee Dip and Khizar Hussain and Lang Zhang and Chris Thomas", "abstract": "  Large Vision Language Models (LVLMs) have demonstrated remarkable\ncapabilities in understanding and describing visual content, achieving\nstate-of-the-art performance across various vision-language tasks. However,\nthese models frequently exhibit hallucination behavior, where they generate\ndescriptions containing objects or details absent in the input image. Our work\ninvestigates this phenomenon by analyzing attention patterns across transformer\nlayers and heads, revealing that hallucinations often stem from progressive\ndegradation of visual grounding in deeper layers. We propose a novel attention\nmodification approach that combines selective token emphasis and head-specific\nmodulation to maintain visual grounding throughout the generation process. Our\nmethod introduces two key components: (1) a dual-stream token selection\nmechanism that identifies and prioritizes both locally informative and\nspatially significant visual tokens, and (2) an attention head-specific\nmodulation strategy that differentially amplifies visual information processing\nbased on measured visual sensitivity of individual attention heads. Through\nextensive experimentation on the MSCOCO dataset, we demonstrate that our\napproach reduces hallucination rates by up to 62.3\\% compared to baseline\nmodels while maintaining comparable task performance. Our analysis reveals that\nselectively modulating tokens across attention heads with varying levels of\nvisual sensitivity can significantly improve visual grounding without requiring\nmodel retraining.\n", "link": "http://arxiv.org/abs/2501.12206v1", "date": "2025-01-21", "relevancy": 2.7564, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fixing%20Imbalanced%20Attention%20to%20Mitigate%20In-Context%20Hallucination%20of%0A%20%20Large%20Vision-Language%20Model&body=Title%3A%20Fixing%20Imbalanced%20Attention%20to%20Mitigate%20In-Context%20Hallucination%20of%0A%20%20Large%20Vision-Language%20Model%0AAuthor%3A%20Kazi%20Hasan%20Ibn%20Arif%20and%20Sajib%20Acharjee%20Dip%20and%20Khizar%20Hussain%20and%20Lang%20Zhang%20and%20Chris%20Thomas%0AAbstract%3A%20%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20understanding%20and%20describing%20visual%20content%2C%20achieving%0Astate-of-the-art%20performance%20across%20various%20vision-language%20tasks.%20However%2C%0Athese%20models%20frequently%20exhibit%20hallucination%20behavior%2C%20where%20they%20generate%0Adescriptions%20containing%20objects%20or%20details%20absent%20in%20the%20input%20image.%20Our%20work%0Ainvestigates%20this%20phenomenon%20by%20analyzing%20attention%20patterns%20across%20transformer%0Alayers%20and%20heads%2C%20revealing%20that%20hallucinations%20often%20stem%20from%20progressive%0Adegradation%20of%20visual%20grounding%20in%20deeper%20layers.%20We%20propose%20a%20novel%20attention%0Amodification%20approach%20that%20combines%20selective%20token%20emphasis%20and%20head-specific%0Amodulation%20to%20maintain%20visual%20grounding%20throughout%20the%20generation%20process.%20Our%0Amethod%20introduces%20two%20key%20components%3A%20%281%29%20a%20dual-stream%20token%20selection%0Amechanism%20that%20identifies%20and%20prioritizes%20both%20locally%20informative%20and%0Aspatially%20significant%20visual%20tokens%2C%20and%20%282%29%20an%20attention%20head-specific%0Amodulation%20strategy%20that%20differentially%20amplifies%20visual%20information%20processing%0Abased%20on%20measured%20visual%20sensitivity%20of%20individual%20attention%20heads.%20Through%0Aextensive%20experimentation%20on%20the%20MSCOCO%20dataset%2C%20we%20demonstrate%20that%20our%0Aapproach%20reduces%20hallucination%20rates%20by%20up%20to%2062.3%5C%25%20compared%20to%20baseline%0Amodels%20while%20maintaining%20comparable%20task%20performance.%20Our%20analysis%20reveals%20that%0Aselectively%20modulating%20tokens%20across%20attention%20heads%20with%20varying%20levels%20of%0Avisual%20sensitivity%20can%20significantly%20improve%20visual%20grounding%20without%20requiring%0Amodel%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFixing%2520Imbalanced%2520Attention%2520to%2520Mitigate%2520In-Context%2520Hallucination%2520of%250A%2520%2520Large%2520Vision-Language%2520Model%26entry.906535625%3DKazi%2520Hasan%2520Ibn%2520Arif%2520and%2520Sajib%2520Acharjee%2520Dip%2520and%2520Khizar%2520Hussain%2520and%2520Lang%2520Zhang%2520and%2520Chris%2520Thomas%26entry.1292438233%3D%2520%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520understanding%2520and%2520describing%2520visual%2520content%252C%2520achieving%250Astate-of-the-art%2520performance%2520across%2520various%2520vision-language%2520tasks.%2520However%252C%250Athese%2520models%2520frequently%2520exhibit%2520hallucination%2520behavior%252C%2520where%2520they%2520generate%250Adescriptions%2520containing%2520objects%2520or%2520details%2520absent%2520in%2520the%2520input%2520image.%2520Our%2520work%250Ainvestigates%2520this%2520phenomenon%2520by%2520analyzing%2520attention%2520patterns%2520across%2520transformer%250Alayers%2520and%2520heads%252C%2520revealing%2520that%2520hallucinations%2520often%2520stem%2520from%2520progressive%250Adegradation%2520of%2520visual%2520grounding%2520in%2520deeper%2520layers.%2520We%2520propose%2520a%2520novel%2520attention%250Amodification%2520approach%2520that%2520combines%2520selective%2520token%2520emphasis%2520and%2520head-specific%250Amodulation%2520to%2520maintain%2520visual%2520grounding%2520throughout%2520the%2520generation%2520process.%2520Our%250Amethod%2520introduces%2520two%2520key%2520components%253A%2520%25281%2529%2520a%2520dual-stream%2520token%2520selection%250Amechanism%2520that%2520identifies%2520and%2520prioritizes%2520both%2520locally%2520informative%2520and%250Aspatially%2520significant%2520visual%2520tokens%252C%2520and%2520%25282%2529%2520an%2520attention%2520head-specific%250Amodulation%2520strategy%2520that%2520differentially%2520amplifies%2520visual%2520information%2520processing%250Abased%2520on%2520measured%2520visual%2520sensitivity%2520of%2520individual%2520attention%2520heads.%2520Through%250Aextensive%2520experimentation%2520on%2520the%2520MSCOCO%2520dataset%252C%2520we%2520demonstrate%2520that%2520our%250Aapproach%2520reduces%2520hallucination%2520rates%2520by%2520up%2520to%252062.3%255C%2525%2520compared%2520to%2520baseline%250Amodels%2520while%2520maintaining%2520comparable%2520task%2520performance.%2520Our%2520analysis%2520reveals%2520that%250Aselectively%2520modulating%2520tokens%2520across%2520attention%2520heads%2520with%2520varying%2520levels%2520of%250Avisual%2520sensitivity%2520can%2520significantly%2520improve%2520visual%2520grounding%2520without%2520requiring%250Amodel%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fixing%20Imbalanced%20Attention%20to%20Mitigate%20In-Context%20Hallucination%20of%0A%20%20Large%20Vision-Language%20Model&entry.906535625=Kazi%20Hasan%20Ibn%20Arif%20and%20Sajib%20Acharjee%20Dip%20and%20Khizar%20Hussain%20and%20Lang%20Zhang%20and%20Chris%20Thomas&entry.1292438233=%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20understanding%20and%20describing%20visual%20content%2C%20achieving%0Astate-of-the-art%20performance%20across%20various%20vision-language%20tasks.%20However%2C%0Athese%20models%20frequently%20exhibit%20hallucination%20behavior%2C%20where%20they%20generate%0Adescriptions%20containing%20objects%20or%20details%20absent%20in%20the%20input%20image.%20Our%20work%0Ainvestigates%20this%20phenomenon%20by%20analyzing%20attention%20patterns%20across%20transformer%0Alayers%20and%20heads%2C%20revealing%20that%20hallucinations%20often%20stem%20from%20progressive%0Adegradation%20of%20visual%20grounding%20in%20deeper%20layers.%20We%20propose%20a%20novel%20attention%0Amodification%20approach%20that%20combines%20selective%20token%20emphasis%20and%20head-specific%0Amodulation%20to%20maintain%20visual%20grounding%20throughout%20the%20generation%20process.%20Our%0Amethod%20introduces%20two%20key%20components%3A%20%281%29%20a%20dual-stream%20token%20selection%0Amechanism%20that%20identifies%20and%20prioritizes%20both%20locally%20informative%20and%0Aspatially%20significant%20visual%20tokens%2C%20and%20%282%29%20an%20attention%20head-specific%0Amodulation%20strategy%20that%20differentially%20amplifies%20visual%20information%20processing%0Abased%20on%20measured%20visual%20sensitivity%20of%20individual%20attention%20heads.%20Through%0Aextensive%20experimentation%20on%20the%20MSCOCO%20dataset%2C%20we%20demonstrate%20that%20our%0Aapproach%20reduces%20hallucination%20rates%20by%20up%20to%2062.3%5C%25%20compared%20to%20baseline%0Amodels%20while%20maintaining%20comparable%20task%20performance.%20Our%20analysis%20reveals%20that%0Aselectively%20modulating%20tokens%20across%20attention%20heads%20with%20varying%20levels%20of%0Avisual%20sensitivity%20can%20significantly%20improve%20visual%20grounding%20without%20requiring%0Amodel%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12206v1&entry.124074799=Read"},
{"title": "FoundationStereo: Zero-Shot Stereo Matching", "author": "Bowen Wen and Matthew Trepte and Joseph Aribido and Jan Kautz and Orazio Gallo and Stan Birchfield", "abstract": "  Tremendous progress has been made in deep stereo matching to excel on\nbenchmark datasets through per-domain fine-tuning. However, achieving strong\nzero-shot generalization - a hallmark of foundation models in other computer\nvision tasks - remains challenging for stereo matching. We introduce\nFoundationStereo, a foundation model for stereo depth estimation designed to\nachieve strong zero-shot generalization. To this end, we first construct a\nlarge-scale (1M stereo pairs) synthetic training dataset featuring large\ndiversity and high photorealism, followed by an automatic self-curation\npipeline to remove ambiguous samples. We then design a number of network\narchitecture components to enhance scalability, including a side-tuning feature\nbackbone that adapts rich monocular priors from vision foundation models to\nmitigate the sim-to-real gap, and long-range context reasoning for effective\ncost volume filtering. Together, these components lead to strong robustness and\naccuracy across domains, establishing a new standard in zero-shot stereo depth\nestimation. Project page: https://nvlabs.github.io/FoundationStereo/\n", "link": "http://arxiv.org/abs/2501.09898v2", "date": "2025-01-21", "relevancy": 2.7467, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FoundationStereo%3A%20Zero-Shot%20Stereo%20Matching&body=Title%3A%20FoundationStereo%3A%20Zero-Shot%20Stereo%20Matching%0AAuthor%3A%20Bowen%20Wen%20and%20Matthew%20Trepte%20and%20Joseph%20Aribido%20and%20Jan%20Kautz%20and%20Orazio%20Gallo%20and%20Stan%20Birchfield%0AAbstract%3A%20%20%20Tremendous%20progress%20has%20been%20made%20in%20deep%20stereo%20matching%20to%20excel%20on%0Abenchmark%20datasets%20through%20per-domain%20fine-tuning.%20However%2C%20achieving%20strong%0Azero-shot%20generalization%20-%20a%20hallmark%20of%20foundation%20models%20in%20other%20computer%0Avision%20tasks%20-%20remains%20challenging%20for%20stereo%20matching.%20We%20introduce%0AFoundationStereo%2C%20a%20foundation%20model%20for%20stereo%20depth%20estimation%20designed%20to%0Aachieve%20strong%20zero-shot%20generalization.%20To%20this%20end%2C%20we%20first%20construct%20a%0Alarge-scale%20%281M%20stereo%20pairs%29%20synthetic%20training%20dataset%20featuring%20large%0Adiversity%20and%20high%20photorealism%2C%20followed%20by%20an%20automatic%20self-curation%0Apipeline%20to%20remove%20ambiguous%20samples.%20We%20then%20design%20a%20number%20of%20network%0Aarchitecture%20components%20to%20enhance%20scalability%2C%20including%20a%20side-tuning%20feature%0Abackbone%20that%20adapts%20rich%20monocular%20priors%20from%20vision%20foundation%20models%20to%0Amitigate%20the%20sim-to-real%20gap%2C%20and%20long-range%20context%20reasoning%20for%20effective%0Acost%20volume%20filtering.%20Together%2C%20these%20components%20lead%20to%20strong%20robustness%20and%0Aaccuracy%20across%20domains%2C%20establishing%20a%20new%20standard%20in%20zero-shot%20stereo%20depth%0Aestimation.%20Project%20page%3A%20https%3A//nvlabs.github.io/FoundationStereo/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09898v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundationStereo%253A%2520Zero-Shot%2520Stereo%2520Matching%26entry.906535625%3DBowen%2520Wen%2520and%2520Matthew%2520Trepte%2520and%2520Joseph%2520Aribido%2520and%2520Jan%2520Kautz%2520and%2520Orazio%2520Gallo%2520and%2520Stan%2520Birchfield%26entry.1292438233%3D%2520%2520Tremendous%2520progress%2520has%2520been%2520made%2520in%2520deep%2520stereo%2520matching%2520to%2520excel%2520on%250Abenchmark%2520datasets%2520through%2520per-domain%2520fine-tuning.%2520However%252C%2520achieving%2520strong%250Azero-shot%2520generalization%2520-%2520a%2520hallmark%2520of%2520foundation%2520models%2520in%2520other%2520computer%250Avision%2520tasks%2520-%2520remains%2520challenging%2520for%2520stereo%2520matching.%2520We%2520introduce%250AFoundationStereo%252C%2520a%2520foundation%2520model%2520for%2520stereo%2520depth%2520estimation%2520designed%2520to%250Aachieve%2520strong%2520zero-shot%2520generalization.%2520To%2520this%2520end%252C%2520we%2520first%2520construct%2520a%250Alarge-scale%2520%25281M%2520stereo%2520pairs%2529%2520synthetic%2520training%2520dataset%2520featuring%2520large%250Adiversity%2520and%2520high%2520photorealism%252C%2520followed%2520by%2520an%2520automatic%2520self-curation%250Apipeline%2520to%2520remove%2520ambiguous%2520samples.%2520We%2520then%2520design%2520a%2520number%2520of%2520network%250Aarchitecture%2520components%2520to%2520enhance%2520scalability%252C%2520including%2520a%2520side-tuning%2520feature%250Abackbone%2520that%2520adapts%2520rich%2520monocular%2520priors%2520from%2520vision%2520foundation%2520models%2520to%250Amitigate%2520the%2520sim-to-real%2520gap%252C%2520and%2520long-range%2520context%2520reasoning%2520for%2520effective%250Acost%2520volume%2520filtering.%2520Together%252C%2520these%2520components%2520lead%2520to%2520strong%2520robustness%2520and%250Aaccuracy%2520across%2520domains%252C%2520establishing%2520a%2520new%2520standard%2520in%2520zero-shot%2520stereo%2520depth%250Aestimation.%2520Project%2520page%253A%2520https%253A//nvlabs.github.io/FoundationStereo/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09898v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoundationStereo%3A%20Zero-Shot%20Stereo%20Matching&entry.906535625=Bowen%20Wen%20and%20Matthew%20Trepte%20and%20Joseph%20Aribido%20and%20Jan%20Kautz%20and%20Orazio%20Gallo%20and%20Stan%20Birchfield&entry.1292438233=%20%20Tremendous%20progress%20has%20been%20made%20in%20deep%20stereo%20matching%20to%20excel%20on%0Abenchmark%20datasets%20through%20per-domain%20fine-tuning.%20However%2C%20achieving%20strong%0Azero-shot%20generalization%20-%20a%20hallmark%20of%20foundation%20models%20in%20other%20computer%0Avision%20tasks%20-%20remains%20challenging%20for%20stereo%20matching.%20We%20introduce%0AFoundationStereo%2C%20a%20foundation%20model%20for%20stereo%20depth%20estimation%20designed%20to%0Aachieve%20strong%20zero-shot%20generalization.%20To%20this%20end%2C%20we%20first%20construct%20a%0Alarge-scale%20%281M%20stereo%20pairs%29%20synthetic%20training%20dataset%20featuring%20large%0Adiversity%20and%20high%20photorealism%2C%20followed%20by%20an%20automatic%20self-curation%0Apipeline%20to%20remove%20ambiguous%20samples.%20We%20then%20design%20a%20number%20of%20network%0Aarchitecture%20components%20to%20enhance%20scalability%2C%20including%20a%20side-tuning%20feature%0Abackbone%20that%20adapts%20rich%20monocular%20priors%20from%20vision%20foundation%20models%20to%0Amitigate%20the%20sim-to-real%20gap%2C%20and%20long-range%20context%20reasoning%20for%20effective%0Acost%20volume%20filtering.%20Together%2C%20these%20components%20lead%20to%20strong%20robustness%20and%0Aaccuracy%20across%20domains%2C%20establishing%20a%20new%20standard%20in%20zero-shot%20stereo%20depth%0Aestimation.%20Project%20page%3A%20https%3A//nvlabs.github.io/FoundationStereo/%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09898v2&entry.124074799=Read"},
{"title": "Yi: Open Foundation Models by 01.AI", "author": "01. AI and  : and Alex Young and Bei Chen and Chao Li and Chengen Huang and Ge Zhang and Guanwei Zhang and Guoyin Wang and Heng Li and Jiangcheng Zhu and Jianqun Chen and Jing Chang and Kaidong Yu and Peng Liu and Qiang Liu and Shawn Yue and Senbin Yang and Shiming Yang and Wen Xie and Wenhao Huang and Xiaohui Hu and Xiaoyi Ren and Xinyao Niu and Pengcheng Nie and Yanpeng Li and Yuchi Xu and Yudong Liu and Yue Wang and Yuxuan Cai and Zhenyu Gu and Zhiyuan Liu and Zonghong Dai", "abstract": "  We introduce the Yi model family, a series of language and multimodal models\nthat demonstrate strong multi-dimensional capabilities. The Yi model family is\nbased on 6B and 34B pretrained language models, then we extend them to chat\nmodels, 200K long context models, depth-upscaled models, and vision-language\nmodels. Our base models achieve strong performance on a wide range of\nbenchmarks like MMLU, and our finetuned chat models deliver strong human\npreference rate on major evaluation platforms like AlpacaEval and Chatbot\nArena. Building upon our scalable super-computing infrastructure and the\nclassical transformer architecture, we attribute the performance of Yi models\nprimarily to its data quality resulting from our data-engineering efforts. For\npretraining, we construct 3.1 trillion tokens of English and Chinese corpora\nusing a cascaded data deduplication and quality filtering pipeline. For\nfinetuning, we polish a small scale (less than 10K) instruction dataset over\nmultiple iterations such that every single instance has been verified directly\nby our machine learning engineers. For vision-language, we combine the chat\nlanguage model with a vision transformer encoder and train the model to align\nvisual representations to the semantic space of the language model. We further\nextend the context length to 200K through lightweight continual pretraining and\ndemonstrate strong needle-in-a-haystack retrieval performance. We show that\nextending the depth of the pretrained checkpoint through continual pretraining\nfurther improves performance. We believe that given our current results,\ncontinuing to scale up model parameters using thoroughly optimized data will\nlead to even stronger frontier models.\n", "link": "http://arxiv.org/abs/2403.04652v3", "date": "2025-01-21", "relevancy": 2.7452, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Yi%3A%20Open%20Foundation%20Models%20by%2001.AI&body=Title%3A%20Yi%3A%20Open%20Foundation%20Models%20by%2001.AI%0AAuthor%3A%2001.%20AI%20and%20%20%3A%20and%20Alex%20Young%20and%20Bei%20Chen%20and%20Chao%20Li%20and%20Chengen%20Huang%20and%20Ge%20Zhang%20and%20Guanwei%20Zhang%20and%20Guoyin%20Wang%20and%20Heng%20Li%20and%20Jiangcheng%20Zhu%20and%20Jianqun%20Chen%20and%20Jing%20Chang%20and%20Kaidong%20Yu%20and%20Peng%20Liu%20and%20Qiang%20Liu%20and%20Shawn%20Yue%20and%20Senbin%20Yang%20and%20Shiming%20Yang%20and%20Wen%20Xie%20and%20Wenhao%20Huang%20and%20Xiaohui%20Hu%20and%20Xiaoyi%20Ren%20and%20Xinyao%20Niu%20and%20Pengcheng%20Nie%20and%20Yanpeng%20Li%20and%20Yuchi%20Xu%20and%20Yudong%20Liu%20and%20Yue%20Wang%20and%20Yuxuan%20Cai%20and%20Zhenyu%20Gu%20and%20Zhiyuan%20Liu%20and%20Zonghong%20Dai%0AAbstract%3A%20%20%20We%20introduce%20the%20Yi%20model%20family%2C%20a%20series%20of%20language%20and%20multimodal%20models%0Athat%20demonstrate%20strong%20multi-dimensional%20capabilities.%20The%20Yi%20model%20family%20is%0Abased%20on%206B%20and%2034B%20pretrained%20language%20models%2C%20then%20we%20extend%20them%20to%20chat%0Amodels%2C%20200K%20long%20context%20models%2C%20depth-upscaled%20models%2C%20and%20vision-language%0Amodels.%20Our%20base%20models%20achieve%20strong%20performance%20on%20a%20wide%20range%20of%0Abenchmarks%20like%20MMLU%2C%20and%20our%20finetuned%20chat%20models%20deliver%20strong%20human%0Apreference%20rate%20on%20major%20evaluation%20platforms%20like%20AlpacaEval%20and%20Chatbot%0AArena.%20Building%20upon%20our%20scalable%20super-computing%20infrastructure%20and%20the%0Aclassical%20transformer%20architecture%2C%20we%20attribute%20the%20performance%20of%20Yi%20models%0Aprimarily%20to%20its%20data%20quality%20resulting%20from%20our%20data-engineering%20efforts.%20For%0Apretraining%2C%20we%20construct%203.1%20trillion%20tokens%20of%20English%20and%20Chinese%20corpora%0Ausing%20a%20cascaded%20data%20deduplication%20and%20quality%20filtering%20pipeline.%20For%0Afinetuning%2C%20we%20polish%20a%20small%20scale%20%28less%20than%2010K%29%20instruction%20dataset%20over%0Amultiple%20iterations%20such%20that%20every%20single%20instance%20has%20been%20verified%20directly%0Aby%20our%20machine%20learning%20engineers.%20For%20vision-language%2C%20we%20combine%20the%20chat%0Alanguage%20model%20with%20a%20vision%20transformer%20encoder%20and%20train%20the%20model%20to%20align%0Avisual%20representations%20to%20the%20semantic%20space%20of%20the%20language%20model.%20We%20further%0Aextend%20the%20context%20length%20to%20200K%20through%20lightweight%20continual%20pretraining%20and%0Ademonstrate%20strong%20needle-in-a-haystack%20retrieval%20performance.%20We%20show%20that%0Aextending%20the%20depth%20of%20the%20pretrained%20checkpoint%20through%20continual%20pretraining%0Afurther%20improves%20performance.%20We%20believe%20that%20given%20our%20current%20results%2C%0Acontinuing%20to%20scale%20up%20model%20parameters%20using%20thoroughly%20optimized%20data%20will%0Alead%20to%20even%20stronger%20frontier%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04652v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYi%253A%2520Open%2520Foundation%2520Models%2520by%252001.AI%26entry.906535625%3D01.%2520AI%2520and%2520%2520%253A%2520and%2520Alex%2520Young%2520and%2520Bei%2520Chen%2520and%2520Chao%2520Li%2520and%2520Chengen%2520Huang%2520and%2520Ge%2520Zhang%2520and%2520Guanwei%2520Zhang%2520and%2520Guoyin%2520Wang%2520and%2520Heng%2520Li%2520and%2520Jiangcheng%2520Zhu%2520and%2520Jianqun%2520Chen%2520and%2520Jing%2520Chang%2520and%2520Kaidong%2520Yu%2520and%2520Peng%2520Liu%2520and%2520Qiang%2520Liu%2520and%2520Shawn%2520Yue%2520and%2520Senbin%2520Yang%2520and%2520Shiming%2520Yang%2520and%2520Wen%2520Xie%2520and%2520Wenhao%2520Huang%2520and%2520Xiaohui%2520Hu%2520and%2520Xiaoyi%2520Ren%2520and%2520Xinyao%2520Niu%2520and%2520Pengcheng%2520Nie%2520and%2520Yanpeng%2520Li%2520and%2520Yuchi%2520Xu%2520and%2520Yudong%2520Liu%2520and%2520Yue%2520Wang%2520and%2520Yuxuan%2520Cai%2520and%2520Zhenyu%2520Gu%2520and%2520Zhiyuan%2520Liu%2520and%2520Zonghong%2520Dai%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Yi%2520model%2520family%252C%2520a%2520series%2520of%2520language%2520and%2520multimodal%2520models%250Athat%2520demonstrate%2520strong%2520multi-dimensional%2520capabilities.%2520The%2520Yi%2520model%2520family%2520is%250Abased%2520on%25206B%2520and%252034B%2520pretrained%2520language%2520models%252C%2520then%2520we%2520extend%2520them%2520to%2520chat%250Amodels%252C%2520200K%2520long%2520context%2520models%252C%2520depth-upscaled%2520models%252C%2520and%2520vision-language%250Amodels.%2520Our%2520base%2520models%2520achieve%2520strong%2520performance%2520on%2520a%2520wide%2520range%2520of%250Abenchmarks%2520like%2520MMLU%252C%2520and%2520our%2520finetuned%2520chat%2520models%2520deliver%2520strong%2520human%250Apreference%2520rate%2520on%2520major%2520evaluation%2520platforms%2520like%2520AlpacaEval%2520and%2520Chatbot%250AArena.%2520Building%2520upon%2520our%2520scalable%2520super-computing%2520infrastructure%2520and%2520the%250Aclassical%2520transformer%2520architecture%252C%2520we%2520attribute%2520the%2520performance%2520of%2520Yi%2520models%250Aprimarily%2520to%2520its%2520data%2520quality%2520resulting%2520from%2520our%2520data-engineering%2520efforts.%2520For%250Apretraining%252C%2520we%2520construct%25203.1%2520trillion%2520tokens%2520of%2520English%2520and%2520Chinese%2520corpora%250Ausing%2520a%2520cascaded%2520data%2520deduplication%2520and%2520quality%2520filtering%2520pipeline.%2520For%250Afinetuning%252C%2520we%2520polish%2520a%2520small%2520scale%2520%2528less%2520than%252010K%2529%2520instruction%2520dataset%2520over%250Amultiple%2520iterations%2520such%2520that%2520every%2520single%2520instance%2520has%2520been%2520verified%2520directly%250Aby%2520our%2520machine%2520learning%2520engineers.%2520For%2520vision-language%252C%2520we%2520combine%2520the%2520chat%250Alanguage%2520model%2520with%2520a%2520vision%2520transformer%2520encoder%2520and%2520train%2520the%2520model%2520to%2520align%250Avisual%2520representations%2520to%2520the%2520semantic%2520space%2520of%2520the%2520language%2520model.%2520We%2520further%250Aextend%2520the%2520context%2520length%2520to%2520200K%2520through%2520lightweight%2520continual%2520pretraining%2520and%250Ademonstrate%2520strong%2520needle-in-a-haystack%2520retrieval%2520performance.%2520We%2520show%2520that%250Aextending%2520the%2520depth%2520of%2520the%2520pretrained%2520checkpoint%2520through%2520continual%2520pretraining%250Afurther%2520improves%2520performance.%2520We%2520believe%2520that%2520given%2520our%2520current%2520results%252C%250Acontinuing%2520to%2520scale%2520up%2520model%2520parameters%2520using%2520thoroughly%2520optimized%2520data%2520will%250Alead%2520to%2520even%2520stronger%2520frontier%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04652v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Yi%3A%20Open%20Foundation%20Models%20by%2001.AI&entry.906535625=01.%20AI%20and%20%20%3A%20and%20Alex%20Young%20and%20Bei%20Chen%20and%20Chao%20Li%20and%20Chengen%20Huang%20and%20Ge%20Zhang%20and%20Guanwei%20Zhang%20and%20Guoyin%20Wang%20and%20Heng%20Li%20and%20Jiangcheng%20Zhu%20and%20Jianqun%20Chen%20and%20Jing%20Chang%20and%20Kaidong%20Yu%20and%20Peng%20Liu%20and%20Qiang%20Liu%20and%20Shawn%20Yue%20and%20Senbin%20Yang%20and%20Shiming%20Yang%20and%20Wen%20Xie%20and%20Wenhao%20Huang%20and%20Xiaohui%20Hu%20and%20Xiaoyi%20Ren%20and%20Xinyao%20Niu%20and%20Pengcheng%20Nie%20and%20Yanpeng%20Li%20and%20Yuchi%20Xu%20and%20Yudong%20Liu%20and%20Yue%20Wang%20and%20Yuxuan%20Cai%20and%20Zhenyu%20Gu%20and%20Zhiyuan%20Liu%20and%20Zonghong%20Dai&entry.1292438233=%20%20We%20introduce%20the%20Yi%20model%20family%2C%20a%20series%20of%20language%20and%20multimodal%20models%0Athat%20demonstrate%20strong%20multi-dimensional%20capabilities.%20The%20Yi%20model%20family%20is%0Abased%20on%206B%20and%2034B%20pretrained%20language%20models%2C%20then%20we%20extend%20them%20to%20chat%0Amodels%2C%20200K%20long%20context%20models%2C%20depth-upscaled%20models%2C%20and%20vision-language%0Amodels.%20Our%20base%20models%20achieve%20strong%20performance%20on%20a%20wide%20range%20of%0Abenchmarks%20like%20MMLU%2C%20and%20our%20finetuned%20chat%20models%20deliver%20strong%20human%0Apreference%20rate%20on%20major%20evaluation%20platforms%20like%20AlpacaEval%20and%20Chatbot%0AArena.%20Building%20upon%20our%20scalable%20super-computing%20infrastructure%20and%20the%0Aclassical%20transformer%20architecture%2C%20we%20attribute%20the%20performance%20of%20Yi%20models%0Aprimarily%20to%20its%20data%20quality%20resulting%20from%20our%20data-engineering%20efforts.%20For%0Apretraining%2C%20we%20construct%203.1%20trillion%20tokens%20of%20English%20and%20Chinese%20corpora%0Ausing%20a%20cascaded%20data%20deduplication%20and%20quality%20filtering%20pipeline.%20For%0Afinetuning%2C%20we%20polish%20a%20small%20scale%20%28less%20than%2010K%29%20instruction%20dataset%20over%0Amultiple%20iterations%20such%20that%20every%20single%20instance%20has%20been%20verified%20directly%0Aby%20our%20machine%20learning%20engineers.%20For%20vision-language%2C%20we%20combine%20the%20chat%0Alanguage%20model%20with%20a%20vision%20transformer%20encoder%20and%20train%20the%20model%20to%20align%0Avisual%20representations%20to%20the%20semantic%20space%20of%20the%20language%20model.%20We%20further%0Aextend%20the%20context%20length%20to%20200K%20through%20lightweight%20continual%20pretraining%20and%0Ademonstrate%20strong%20needle-in-a-haystack%20retrieval%20performance.%20We%20show%20that%0Aextending%20the%20depth%20of%20the%20pretrained%20checkpoint%20through%20continual%20pretraining%0Afurther%20improves%20performance.%20We%20believe%20that%20given%20our%20current%20results%2C%0Acontinuing%20to%20scale%20up%20model%20parameters%20using%20thoroughly%20optimized%20data%20will%0Alead%20to%20even%20stronger%20frontier%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04652v3&entry.124074799=Read"},
{"title": "InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward\n  Model", "author": "Yuhang Zang and Xiaoyi Dong and Pan Zhang and Yuhang Cao and Ziyu Liu and Shengyuan Ding and Shenxi Wu and Yubo Ma and Haodong Duan and Wenwei Zhang and Kai Chen and Dahua Lin and Jiaqi Wang", "abstract": "  Despite the promising performance of Large Vision Language Models (LVLMs) in\nvisual understanding, they occasionally generate incorrect outputs. While\nreward models (RMs) with reinforcement learning or test-time scaling offer the\npotential for improving generation quality, a critical gap remains: publicly\navailable multi-modal RMs for LVLMs are scarce, and the implementation details\nof proprietary models are often unclear. We bridge this gap with\nInternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective\nmulti-modal reward model that aligns LVLMs with human preferences. To ensure\nthe robustness and versatility of IXC-2.5-Reward, we set up a high-quality\nmulti-modal preference corpus spanning text, image, and video inputs across\ndiverse domains, such as instruction following, general understanding,\ntext-rich documents, mathematical reasoning, and video understanding.\nIXC-2.5-Reward achieves excellent results on the latest multi-modal reward\nmodel benchmark and shows competitive performance on text-only reward model\nbenchmarks. We further demonstrate three key applications of IXC-2.5-Reward:\n(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward\nwith Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows\nconsistent improvements in instruction following and multi-modal open-ended\ndialogue; (2) Selecting the best response from candidate responses for\ntest-time scaling; and (3) Filtering outlier or noisy samples from existing\nimage and video instruction tuning training data. To ensure reproducibility and\nfacilitate further research, we have open-sourced all model weights and\ntraining recipes at https://github.com/InternLM/InternLM-XComposer\n", "link": "http://arxiv.org/abs/2501.12368v1", "date": "2025-01-21", "relevancy": 2.6951, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternLM-XComposer2.5-Reward%3A%20A%20Simple%20Yet%20Effective%20Multi-Modal%20Reward%0A%20%20Model&body=Title%3A%20InternLM-XComposer2.5-Reward%3A%20A%20Simple%20Yet%20Effective%20Multi-Modal%20Reward%0A%20%20Model%0AAuthor%3A%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Cao%20and%20Ziyu%20Liu%20and%20Shengyuan%20Ding%20and%20Shenxi%20Wu%20and%20Yubo%20Ma%20and%20Haodong%20Duan%20and%20Wenwei%20Zhang%20and%20Kai%20Chen%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Despite%20the%20promising%20performance%20of%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20in%0Avisual%20understanding%2C%20they%20occasionally%20generate%20incorrect%20outputs.%20While%0Areward%20models%20%28RMs%29%20with%20reinforcement%20learning%20or%20test-time%20scaling%20offer%20the%0Apotential%20for%20improving%20generation%20quality%2C%20a%20critical%20gap%20remains%3A%20publicly%0Aavailable%20multi-modal%20RMs%20for%20LVLMs%20are%20scarce%2C%20and%20the%20implementation%20details%0Aof%20proprietary%20models%20are%20often%20unclear.%20We%20bridge%20this%20gap%20with%0AInternLM-XComposer2.5-Reward%20%28IXC-2.5-Reward%29%2C%20a%20simple%20yet%20effective%0Amulti-modal%20reward%20model%20that%20aligns%20LVLMs%20with%20human%20preferences.%20To%20ensure%0Athe%20robustness%20and%20versatility%20of%20IXC-2.5-Reward%2C%20we%20set%20up%20a%20high-quality%0Amulti-modal%20preference%20corpus%20spanning%20text%2C%20image%2C%20and%20video%20inputs%20across%0Adiverse%20domains%2C%20such%20as%20instruction%20following%2C%20general%20understanding%2C%0Atext-rich%20documents%2C%20mathematical%20reasoning%2C%20and%20video%20understanding.%0AIXC-2.5-Reward%20achieves%20excellent%20results%20on%20the%20latest%20multi-modal%20reward%0Amodel%20benchmark%20and%20shows%20competitive%20performance%20on%20text-only%20reward%20model%0Abenchmarks.%20We%20further%20demonstrate%20three%20key%20applications%20of%20IXC-2.5-Reward%3A%0A%281%29%20Providing%20a%20supervisory%20signal%20for%20RL%20training.%20We%20integrate%20IXC-2.5-Reward%0Awith%20Proximal%20Policy%20Optimization%20%28PPO%29%20yields%20IXC-2.5-Chat%2C%20which%20shows%0Aconsistent%20improvements%20in%20instruction%20following%20and%20multi-modal%20open-ended%0Adialogue%3B%20%282%29%20Selecting%20the%20best%20response%20from%20candidate%20responses%20for%0Atest-time%20scaling%3B%20and%20%283%29%20Filtering%20outlier%20or%20noisy%20samples%20from%20existing%0Aimage%20and%20video%20instruction%20tuning%20training%20data.%20To%20ensure%20reproducibility%20and%0Afacilitate%20further%20research%2C%20we%20have%20open-sourced%20all%20model%20weights%20and%0Atraining%20recipes%20at%20https%3A//github.com/InternLM/InternLM-XComposer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternLM-XComposer2.5-Reward%253A%2520A%2520Simple%2520Yet%2520Effective%2520Multi-Modal%2520Reward%250A%2520%2520Model%26entry.906535625%3DYuhang%2520Zang%2520and%2520Xiaoyi%2520Dong%2520and%2520Pan%2520Zhang%2520and%2520Yuhang%2520Cao%2520and%2520Ziyu%2520Liu%2520and%2520Shengyuan%2520Ding%2520and%2520Shenxi%2520Wu%2520and%2520Yubo%2520Ma%2520and%2520Haodong%2520Duan%2520and%2520Wenwei%2520Zhang%2520and%2520Kai%2520Chen%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520the%2520promising%2520performance%2520of%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520in%250Avisual%2520understanding%252C%2520they%2520occasionally%2520generate%2520incorrect%2520outputs.%2520While%250Areward%2520models%2520%2528RMs%2529%2520with%2520reinforcement%2520learning%2520or%2520test-time%2520scaling%2520offer%2520the%250Apotential%2520for%2520improving%2520generation%2520quality%252C%2520a%2520critical%2520gap%2520remains%253A%2520publicly%250Aavailable%2520multi-modal%2520RMs%2520for%2520LVLMs%2520are%2520scarce%252C%2520and%2520the%2520implementation%2520details%250Aof%2520proprietary%2520models%2520are%2520often%2520unclear.%2520We%2520bridge%2520this%2520gap%2520with%250AInternLM-XComposer2.5-Reward%2520%2528IXC-2.5-Reward%2529%252C%2520a%2520simple%2520yet%2520effective%250Amulti-modal%2520reward%2520model%2520that%2520aligns%2520LVLMs%2520with%2520human%2520preferences.%2520To%2520ensure%250Athe%2520robustness%2520and%2520versatility%2520of%2520IXC-2.5-Reward%252C%2520we%2520set%2520up%2520a%2520high-quality%250Amulti-modal%2520preference%2520corpus%2520spanning%2520text%252C%2520image%252C%2520and%2520video%2520inputs%2520across%250Adiverse%2520domains%252C%2520such%2520as%2520instruction%2520following%252C%2520general%2520understanding%252C%250Atext-rich%2520documents%252C%2520mathematical%2520reasoning%252C%2520and%2520video%2520understanding.%250AIXC-2.5-Reward%2520achieves%2520excellent%2520results%2520on%2520the%2520latest%2520multi-modal%2520reward%250Amodel%2520benchmark%2520and%2520shows%2520competitive%2520performance%2520on%2520text-only%2520reward%2520model%250Abenchmarks.%2520We%2520further%2520demonstrate%2520three%2520key%2520applications%2520of%2520IXC-2.5-Reward%253A%250A%25281%2529%2520Providing%2520a%2520supervisory%2520signal%2520for%2520RL%2520training.%2520We%2520integrate%2520IXC-2.5-Reward%250Awith%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520yields%2520IXC-2.5-Chat%252C%2520which%2520shows%250Aconsistent%2520improvements%2520in%2520instruction%2520following%2520and%2520multi-modal%2520open-ended%250Adialogue%253B%2520%25282%2529%2520Selecting%2520the%2520best%2520response%2520from%2520candidate%2520responses%2520for%250Atest-time%2520scaling%253B%2520and%2520%25283%2529%2520Filtering%2520outlier%2520or%2520noisy%2520samples%2520from%2520existing%250Aimage%2520and%2520video%2520instruction%2520tuning%2520training%2520data.%2520To%2520ensure%2520reproducibility%2520and%250Afacilitate%2520further%2520research%252C%2520we%2520have%2520open-sourced%2520all%2520model%2520weights%2520and%250Atraining%2520recipes%2520at%2520https%253A//github.com/InternLM/InternLM-XComposer%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternLM-XComposer2.5-Reward%3A%20A%20Simple%20Yet%20Effective%20Multi-Modal%20Reward%0A%20%20Model&entry.906535625=Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Cao%20and%20Ziyu%20Liu%20and%20Shengyuan%20Ding%20and%20Shenxi%20Wu%20and%20Yubo%20Ma%20and%20Haodong%20Duan%20and%20Wenwei%20Zhang%20and%20Kai%20Chen%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Despite%20the%20promising%20performance%20of%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20in%0Avisual%20understanding%2C%20they%20occasionally%20generate%20incorrect%20outputs.%20While%0Areward%20models%20%28RMs%29%20with%20reinforcement%20learning%20or%20test-time%20scaling%20offer%20the%0Apotential%20for%20improving%20generation%20quality%2C%20a%20critical%20gap%20remains%3A%20publicly%0Aavailable%20multi-modal%20RMs%20for%20LVLMs%20are%20scarce%2C%20and%20the%20implementation%20details%0Aof%20proprietary%20models%20are%20often%20unclear.%20We%20bridge%20this%20gap%20with%0AInternLM-XComposer2.5-Reward%20%28IXC-2.5-Reward%29%2C%20a%20simple%20yet%20effective%0Amulti-modal%20reward%20model%20that%20aligns%20LVLMs%20with%20human%20preferences.%20To%20ensure%0Athe%20robustness%20and%20versatility%20of%20IXC-2.5-Reward%2C%20we%20set%20up%20a%20high-quality%0Amulti-modal%20preference%20corpus%20spanning%20text%2C%20image%2C%20and%20video%20inputs%20across%0Adiverse%20domains%2C%20such%20as%20instruction%20following%2C%20general%20understanding%2C%0Atext-rich%20documents%2C%20mathematical%20reasoning%2C%20and%20video%20understanding.%0AIXC-2.5-Reward%20achieves%20excellent%20results%20on%20the%20latest%20multi-modal%20reward%0Amodel%20benchmark%20and%20shows%20competitive%20performance%20on%20text-only%20reward%20model%0Abenchmarks.%20We%20further%20demonstrate%20three%20key%20applications%20of%20IXC-2.5-Reward%3A%0A%281%29%20Providing%20a%20supervisory%20signal%20for%20RL%20training.%20We%20integrate%20IXC-2.5-Reward%0Awith%20Proximal%20Policy%20Optimization%20%28PPO%29%20yields%20IXC-2.5-Chat%2C%20which%20shows%0Aconsistent%20improvements%20in%20instruction%20following%20and%20multi-modal%20open-ended%0Adialogue%3B%20%282%29%20Selecting%20the%20best%20response%20from%20candidate%20responses%20for%0Atest-time%20scaling%3B%20and%20%283%29%20Filtering%20outlier%20or%20noisy%20samples%20from%20existing%0Aimage%20and%20video%20instruction%20tuning%20training%20data.%20To%20ensure%20reproducibility%20and%0Afacilitate%20further%20research%2C%20we%20have%20open-sourced%20all%20model%20weights%20and%0Atraining%20recipes%20at%20https%3A//github.com/InternLM/InternLM-XComposer%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12368v1&entry.124074799=Read"},
{"title": "Teacher Encoder-Student Decoder Denoising Guided Segmentation Network\n  for Anomaly Detection", "author": "ShiXuan Song and Hao Chen and Shu Hu and Xin Wang and Jinrong Hu and Xi Wu", "abstract": "  Visual anomaly detection is a highly challenging task, often categorized as a\none-class classification and segmentation problem. Recent studies have\ndemonstrated that the student-teacher (S-T) framework effectively addresses\nthis challenge. However, most S-T frameworks rely solely on pre-trained teacher\nnetworks to guide student networks in learning multi-scale similar features,\noverlooking the potential of the student networks to enhance learning through\nmulti-scale feature fusion. In this study, we propose a novel model named\nPFADSeg, which integrates a pre-trained teacher network, a denoising student\nnetwork with multi-scale feature fusion, and a guided anomaly segmentation\nnetwork into a unified framework. By adopting a unique teacher-encoder and\nstudent-decoder denoising mode, the model improves the student network's\nability to learn from teacher network features. Furthermore, an adaptive\nfeature fusion mechanism is introduced to train a self-supervised segmentation\nnetwork that synthesizes anomaly masks autonomously, significantly increasing\ndetection performance. Evaluated on the MVTec AD dataset, PFADSeg achieves\nstate-of-the-art results with an image-level AUC of 98.9%, a pixel-level mean\nprecision of 76.4%, and an instance-level mean precision of 78.7%.\n", "link": "http://arxiv.org/abs/2501.12104v1", "date": "2025-01-21", "relevancy": 2.6749, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teacher%20Encoder-Student%20Decoder%20Denoising%20Guided%20Segmentation%20Network%0A%20%20for%20Anomaly%20Detection&body=Title%3A%20Teacher%20Encoder-Student%20Decoder%20Denoising%20Guided%20Segmentation%20Network%0A%20%20for%20Anomaly%20Detection%0AAuthor%3A%20ShiXuan%20Song%20and%20Hao%20Chen%20and%20Shu%20Hu%20and%20Xin%20Wang%20and%20Jinrong%20Hu%20and%20Xi%20Wu%0AAbstract%3A%20%20%20Visual%20anomaly%20detection%20is%20a%20highly%20challenging%20task%2C%20often%20categorized%20as%20a%0Aone-class%20classification%20and%20segmentation%20problem.%20Recent%20studies%20have%0Ademonstrated%20that%20the%20student-teacher%20%28S-T%29%20framework%20effectively%20addresses%0Athis%20challenge.%20However%2C%20most%20S-T%20frameworks%20rely%20solely%20on%20pre-trained%20teacher%0Anetworks%20to%20guide%20student%20networks%20in%20learning%20multi-scale%20similar%20features%2C%0Aoverlooking%20the%20potential%20of%20the%20student%20networks%20to%20enhance%20learning%20through%0Amulti-scale%20feature%20fusion.%20In%20this%20study%2C%20we%20propose%20a%20novel%20model%20named%0APFADSeg%2C%20which%20integrates%20a%20pre-trained%20teacher%20network%2C%20a%20denoising%20student%0Anetwork%20with%20multi-scale%20feature%20fusion%2C%20and%20a%20guided%20anomaly%20segmentation%0Anetwork%20into%20a%20unified%20framework.%20By%20adopting%20a%20unique%20teacher-encoder%20and%0Astudent-decoder%20denoising%20mode%2C%20the%20model%20improves%20the%20student%20network%27s%0Aability%20to%20learn%20from%20teacher%20network%20features.%20Furthermore%2C%20an%20adaptive%0Afeature%20fusion%20mechanism%20is%20introduced%20to%20train%20a%20self-supervised%20segmentation%0Anetwork%20that%20synthesizes%20anomaly%20masks%20autonomously%2C%20significantly%20increasing%0Adetection%20performance.%20Evaluated%20on%20the%20MVTec%20AD%20dataset%2C%20PFADSeg%20achieves%0Astate-of-the-art%20results%20with%20an%20image-level%20AUC%20of%2098.9%25%2C%20a%20pixel-level%20mean%0Aprecision%20of%2076.4%25%2C%20and%20an%20instance-level%20mean%20precision%20of%2078.7%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeacher%2520Encoder-Student%2520Decoder%2520Denoising%2520Guided%2520Segmentation%2520Network%250A%2520%2520for%2520Anomaly%2520Detection%26entry.906535625%3DShiXuan%2520Song%2520and%2520Hao%2520Chen%2520and%2520Shu%2520Hu%2520and%2520Xin%2520Wang%2520and%2520Jinrong%2520Hu%2520and%2520Xi%2520Wu%26entry.1292438233%3D%2520%2520Visual%2520anomaly%2520detection%2520is%2520a%2520highly%2520challenging%2520task%252C%2520often%2520categorized%2520as%2520a%250Aone-class%2520classification%2520and%2520segmentation%2520problem.%2520Recent%2520studies%2520have%250Ademonstrated%2520that%2520the%2520student-teacher%2520%2528S-T%2529%2520framework%2520effectively%2520addresses%250Athis%2520challenge.%2520However%252C%2520most%2520S-T%2520frameworks%2520rely%2520solely%2520on%2520pre-trained%2520teacher%250Anetworks%2520to%2520guide%2520student%2520networks%2520in%2520learning%2520multi-scale%2520similar%2520features%252C%250Aoverlooking%2520the%2520potential%2520of%2520the%2520student%2520networks%2520to%2520enhance%2520learning%2520through%250Amulti-scale%2520feature%2520fusion.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520model%2520named%250APFADSeg%252C%2520which%2520integrates%2520a%2520pre-trained%2520teacher%2520network%252C%2520a%2520denoising%2520student%250Anetwork%2520with%2520multi-scale%2520feature%2520fusion%252C%2520and%2520a%2520guided%2520anomaly%2520segmentation%250Anetwork%2520into%2520a%2520unified%2520framework.%2520By%2520adopting%2520a%2520unique%2520teacher-encoder%2520and%250Astudent-decoder%2520denoising%2520mode%252C%2520the%2520model%2520improves%2520the%2520student%2520network%2527s%250Aability%2520to%2520learn%2520from%2520teacher%2520network%2520features.%2520Furthermore%252C%2520an%2520adaptive%250Afeature%2520fusion%2520mechanism%2520is%2520introduced%2520to%2520train%2520a%2520self-supervised%2520segmentation%250Anetwork%2520that%2520synthesizes%2520anomaly%2520masks%2520autonomously%252C%2520significantly%2520increasing%250Adetection%2520performance.%2520Evaluated%2520on%2520the%2520MVTec%2520AD%2520dataset%252C%2520PFADSeg%2520achieves%250Astate-of-the-art%2520results%2520with%2520an%2520image-level%2520AUC%2520of%252098.9%2525%252C%2520a%2520pixel-level%2520mean%250Aprecision%2520of%252076.4%2525%252C%2520and%2520an%2520instance-level%2520mean%2520precision%2520of%252078.7%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teacher%20Encoder-Student%20Decoder%20Denoising%20Guided%20Segmentation%20Network%0A%20%20for%20Anomaly%20Detection&entry.906535625=ShiXuan%20Song%20and%20Hao%20Chen%20and%20Shu%20Hu%20and%20Xin%20Wang%20and%20Jinrong%20Hu%20and%20Xi%20Wu&entry.1292438233=%20%20Visual%20anomaly%20detection%20is%20a%20highly%20challenging%20task%2C%20often%20categorized%20as%20a%0Aone-class%20classification%20and%20segmentation%20problem.%20Recent%20studies%20have%0Ademonstrated%20that%20the%20student-teacher%20%28S-T%29%20framework%20effectively%20addresses%0Athis%20challenge.%20However%2C%20most%20S-T%20frameworks%20rely%20solely%20on%20pre-trained%20teacher%0Anetworks%20to%20guide%20student%20networks%20in%20learning%20multi-scale%20similar%20features%2C%0Aoverlooking%20the%20potential%20of%20the%20student%20networks%20to%20enhance%20learning%20through%0Amulti-scale%20feature%20fusion.%20In%20this%20study%2C%20we%20propose%20a%20novel%20model%20named%0APFADSeg%2C%20which%20integrates%20a%20pre-trained%20teacher%20network%2C%20a%20denoising%20student%0Anetwork%20with%20multi-scale%20feature%20fusion%2C%20and%20a%20guided%20anomaly%20segmentation%0Anetwork%20into%20a%20unified%20framework.%20By%20adopting%20a%20unique%20teacher-encoder%20and%0Astudent-decoder%20denoising%20mode%2C%20the%20model%20improves%20the%20student%20network%27s%0Aability%20to%20learn%20from%20teacher%20network%20features.%20Furthermore%2C%20an%20adaptive%0Afeature%20fusion%20mechanism%20is%20introduced%20to%20train%20a%20self-supervised%20segmentation%0Anetwork%20that%20synthesizes%20anomaly%20masks%20autonomously%2C%20significantly%20increasing%0Adetection%20performance.%20Evaluated%20on%20the%20MVTec%20AD%20dataset%2C%20PFADSeg%20achieves%0Astate-of-the-art%20results%20with%20an%20image-level%20AUC%20of%2098.9%25%2C%20a%20pixel-level%20mean%0Aprecision%20of%2076.4%25%2C%20and%20an%20instance-level%20mean%20precision%20of%2078.7%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12104v1&entry.124074799=Read"},
{"title": "RadaRays: Real-time Simulation of Rotating FMCW Radar for Mobile\n  Robotics via Hardware-accelerated Ray Tracing", "author": "Alexander Mock and Martin Magnusson and Joachim Hertzberg", "abstract": "  RadaRays allows for the accurate modeling and simulation of rotating FMCW\nradar sensors in complex environments, including the simulation of reflection,\nrefraction, and scattering of radar waves. Our software is able to handle large\nnumbers of objects and materials in real-time, making it suitable for use in a\nvariety of mobile robotics applications. We demonstrate the effectiveness of\nRadaRays through a series of experiments and show that it can more accurately\nreproduce the behavior of FMCW radar sensors in a variety of environments,\ncompared to the ray casting-based lidar-like simulations that are commonly used\nin simulators for autonomous driving such as CARLA. Our experiments\nadditionally serve as a valuable reference point for researchers to evaluate\ntheir own radar simulations. By using RadaRays, developers can significantly\nreduce the time and cost associated with prototyping and testing FMCW\nradar-based algorithms. We also provide a Gazebo plugin that makes our work\naccessible to the mobile robotics community.\n", "link": "http://arxiv.org/abs/2310.03505v2", "date": "2025-01-21", "relevancy": 2.6515, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5624}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5142}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadaRays%3A%20Real-time%20Simulation%20of%20Rotating%20FMCW%20Radar%20for%20Mobile%0A%20%20Robotics%20via%20Hardware-accelerated%20Ray%20Tracing&body=Title%3A%20RadaRays%3A%20Real-time%20Simulation%20of%20Rotating%20FMCW%20Radar%20for%20Mobile%0A%20%20Robotics%20via%20Hardware-accelerated%20Ray%20Tracing%0AAuthor%3A%20Alexander%20Mock%20and%20Martin%20Magnusson%20and%20Joachim%20Hertzberg%0AAbstract%3A%20%20%20RadaRays%20allows%20for%20the%20accurate%20modeling%20and%20simulation%20of%20rotating%20FMCW%0Aradar%20sensors%20in%20complex%20environments%2C%20including%20the%20simulation%20of%20reflection%2C%0Arefraction%2C%20and%20scattering%20of%20radar%20waves.%20Our%20software%20is%20able%20to%20handle%20large%0Anumbers%20of%20objects%20and%20materials%20in%20real-time%2C%20making%20it%20suitable%20for%20use%20in%20a%0Avariety%20of%20mobile%20robotics%20applications.%20We%20demonstrate%20the%20effectiveness%20of%0ARadaRays%20through%20a%20series%20of%20experiments%20and%20show%20that%20it%20can%20more%20accurately%0Areproduce%20the%20behavior%20of%20FMCW%20radar%20sensors%20in%20a%20variety%20of%20environments%2C%0Acompared%20to%20the%20ray%20casting-based%20lidar-like%20simulations%20that%20are%20commonly%20used%0Ain%20simulators%20for%20autonomous%20driving%20such%20as%20CARLA.%20Our%20experiments%0Aadditionally%20serve%20as%20a%20valuable%20reference%20point%20for%20researchers%20to%20evaluate%0Atheir%20own%20radar%20simulations.%20By%20using%20RadaRays%2C%20developers%20can%20significantly%0Areduce%20the%20time%20and%20cost%20associated%20with%20prototyping%20and%20testing%20FMCW%0Aradar-based%20algorithms.%20We%20also%20provide%20a%20Gazebo%20plugin%20that%20makes%20our%20work%0Aaccessible%20to%20the%20mobile%20robotics%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03505v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadaRays%253A%2520Real-time%2520Simulation%2520of%2520Rotating%2520FMCW%2520Radar%2520for%2520Mobile%250A%2520%2520Robotics%2520via%2520Hardware-accelerated%2520Ray%2520Tracing%26entry.906535625%3DAlexander%2520Mock%2520and%2520Martin%2520Magnusson%2520and%2520Joachim%2520Hertzberg%26entry.1292438233%3D%2520%2520RadaRays%2520allows%2520for%2520the%2520accurate%2520modeling%2520and%2520simulation%2520of%2520rotating%2520FMCW%250Aradar%2520sensors%2520in%2520complex%2520environments%252C%2520including%2520the%2520simulation%2520of%2520reflection%252C%250Arefraction%252C%2520and%2520scattering%2520of%2520radar%2520waves.%2520Our%2520software%2520is%2520able%2520to%2520handle%2520large%250Anumbers%2520of%2520objects%2520and%2520materials%2520in%2520real-time%252C%2520making%2520it%2520suitable%2520for%2520use%2520in%2520a%250Avariety%2520of%2520mobile%2520robotics%2520applications.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%250ARadaRays%2520through%2520a%2520series%2520of%2520experiments%2520and%2520show%2520that%2520it%2520can%2520more%2520accurately%250Areproduce%2520the%2520behavior%2520of%2520FMCW%2520radar%2520sensors%2520in%2520a%2520variety%2520of%2520environments%252C%250Acompared%2520to%2520the%2520ray%2520casting-based%2520lidar-like%2520simulations%2520that%2520are%2520commonly%2520used%250Ain%2520simulators%2520for%2520autonomous%2520driving%2520such%2520as%2520CARLA.%2520Our%2520experiments%250Aadditionally%2520serve%2520as%2520a%2520valuable%2520reference%2520point%2520for%2520researchers%2520to%2520evaluate%250Atheir%2520own%2520radar%2520simulations.%2520By%2520using%2520RadaRays%252C%2520developers%2520can%2520significantly%250Areduce%2520the%2520time%2520and%2520cost%2520associated%2520with%2520prototyping%2520and%2520testing%2520FMCW%250Aradar-based%2520algorithms.%2520We%2520also%2520provide%2520a%2520Gazebo%2520plugin%2520that%2520makes%2520our%2520work%250Aaccessible%2520to%2520the%2520mobile%2520robotics%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03505v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadaRays%3A%20Real-time%20Simulation%20of%20Rotating%20FMCW%20Radar%20for%20Mobile%0A%20%20Robotics%20via%20Hardware-accelerated%20Ray%20Tracing&entry.906535625=Alexander%20Mock%20and%20Martin%20Magnusson%20and%20Joachim%20Hertzberg&entry.1292438233=%20%20RadaRays%20allows%20for%20the%20accurate%20modeling%20and%20simulation%20of%20rotating%20FMCW%0Aradar%20sensors%20in%20complex%20environments%2C%20including%20the%20simulation%20of%20reflection%2C%0Arefraction%2C%20and%20scattering%20of%20radar%20waves.%20Our%20software%20is%20able%20to%20handle%20large%0Anumbers%20of%20objects%20and%20materials%20in%20real-time%2C%20making%20it%20suitable%20for%20use%20in%20a%0Avariety%20of%20mobile%20robotics%20applications.%20We%20demonstrate%20the%20effectiveness%20of%0ARadaRays%20through%20a%20series%20of%20experiments%20and%20show%20that%20it%20can%20more%20accurately%0Areproduce%20the%20behavior%20of%20FMCW%20radar%20sensors%20in%20a%20variety%20of%20environments%2C%0Acompared%20to%20the%20ray%20casting-based%20lidar-like%20simulations%20that%20are%20commonly%20used%0Ain%20simulators%20for%20autonomous%20driving%20such%20as%20CARLA.%20Our%20experiments%0Aadditionally%20serve%20as%20a%20valuable%20reference%20point%20for%20researchers%20to%20evaluate%0Atheir%20own%20radar%20simulations.%20By%20using%20RadaRays%2C%20developers%20can%20significantly%0Areduce%20the%20time%20and%20cost%20associated%20with%20prototyping%20and%20testing%20FMCW%0Aradar-based%20algorithms.%20We%20also%20provide%20a%20Gazebo%20plugin%20that%20makes%20our%20work%0Aaccessible%20to%20the%20mobile%20robotics%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03505v2&entry.124074799=Read"},
{"title": "Continuous 3D Perception Model with Persistent State", "author": "Qianqian Wang and Yifei Zhang and Aleksander Holynski and Alexei A. Efros and Angjoo Kanazawa", "abstract": "  We present a unified framework capable of solving a broad range of 3D tasks.\nOur approach features a stateful recurrent model that continuously updates its\nstate representation with each new observation. Given a stream of images, this\nevolving state can be used to generate metric-scale pointmaps (per-pixel 3D\npoints) for each new input in an online fashion. These pointmaps reside within\na common coordinate system, and can be accumulated into a coherent, dense scene\nreconstruction that updates as new images arrive. Our model, called CUT3R\n(Continuous Updating Transformer for 3D Reconstruction), captures rich priors\nof real-world scenes: not only can it predict accurate pointmaps from image\nobservations, but it can also infer unseen regions of the scene by probing at\nvirtual, unobserved views. Our method is simple yet highly flexible, naturally\naccepting varying lengths of images that may be either video streams or\nunordered photo collections, containing both static and dynamic content. We\nevaluate our method on various 3D/4D tasks and demonstrate competitive or\nstate-of-the-art performance in each. Project Page: https://cut3r.github.io/\n", "link": "http://arxiv.org/abs/2501.12387v1", "date": "2025-01-21", "relevancy": 2.6423, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6632}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6632}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%203D%20Perception%20Model%20with%20Persistent%20State&body=Title%3A%20Continuous%203D%20Perception%20Model%20with%20Persistent%20State%0AAuthor%3A%20Qianqian%20Wang%20and%20Yifei%20Zhang%20and%20Aleksander%20Holynski%20and%20Alexei%20A.%20Efros%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20We%20present%20a%20unified%20framework%20capable%20of%20solving%20a%20broad%20range%20of%203D%20tasks.%0AOur%20approach%20features%20a%20stateful%20recurrent%20model%20that%20continuously%20updates%20its%0Astate%20representation%20with%20each%20new%20observation.%20Given%20a%20stream%20of%20images%2C%20this%0Aevolving%20state%20can%20be%20used%20to%20generate%20metric-scale%20pointmaps%20%28per-pixel%203D%0Apoints%29%20for%20each%20new%20input%20in%20an%20online%20fashion.%20These%20pointmaps%20reside%20within%0Aa%20common%20coordinate%20system%2C%20and%20can%20be%20accumulated%20into%20a%20coherent%2C%20dense%20scene%0Areconstruction%20that%20updates%20as%20new%20images%20arrive.%20Our%20model%2C%20called%20CUT3R%0A%28Continuous%20Updating%20Transformer%20for%203D%20Reconstruction%29%2C%20captures%20rich%20priors%0Aof%20real-world%20scenes%3A%20not%20only%20can%20it%20predict%20accurate%20pointmaps%20from%20image%0Aobservations%2C%20but%20it%20can%20also%20infer%20unseen%20regions%20of%20the%20scene%20by%20probing%20at%0Avirtual%2C%20unobserved%20views.%20Our%20method%20is%20simple%20yet%20highly%20flexible%2C%20naturally%0Aaccepting%20varying%20lengths%20of%20images%20that%20may%20be%20either%20video%20streams%20or%0Aunordered%20photo%20collections%2C%20containing%20both%20static%20and%20dynamic%20content.%20We%0Aevaluate%20our%20method%20on%20various%203D/4D%20tasks%20and%20demonstrate%20competitive%20or%0Astate-of-the-art%20performance%20in%20each.%20Project%20Page%3A%20https%3A//cut3r.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%25203D%2520Perception%2520Model%2520with%2520Persistent%2520State%26entry.906535625%3DQianqian%2520Wang%2520and%2520Yifei%2520Zhang%2520and%2520Aleksander%2520Holynski%2520and%2520Alexei%2520A.%2520Efros%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520unified%2520framework%2520capable%2520of%2520solving%2520a%2520broad%2520range%2520of%25203D%2520tasks.%250AOur%2520approach%2520features%2520a%2520stateful%2520recurrent%2520model%2520that%2520continuously%2520updates%2520its%250Astate%2520representation%2520with%2520each%2520new%2520observation.%2520Given%2520a%2520stream%2520of%2520images%252C%2520this%250Aevolving%2520state%2520can%2520be%2520used%2520to%2520generate%2520metric-scale%2520pointmaps%2520%2528per-pixel%25203D%250Apoints%2529%2520for%2520each%2520new%2520input%2520in%2520an%2520online%2520fashion.%2520These%2520pointmaps%2520reside%2520within%250Aa%2520common%2520coordinate%2520system%252C%2520and%2520can%2520be%2520accumulated%2520into%2520a%2520coherent%252C%2520dense%2520scene%250Areconstruction%2520that%2520updates%2520as%2520new%2520images%2520arrive.%2520Our%2520model%252C%2520called%2520CUT3R%250A%2528Continuous%2520Updating%2520Transformer%2520for%25203D%2520Reconstruction%2529%252C%2520captures%2520rich%2520priors%250Aof%2520real-world%2520scenes%253A%2520not%2520only%2520can%2520it%2520predict%2520accurate%2520pointmaps%2520from%2520image%250Aobservations%252C%2520but%2520it%2520can%2520also%2520infer%2520unseen%2520regions%2520of%2520the%2520scene%2520by%2520probing%2520at%250Avirtual%252C%2520unobserved%2520views.%2520Our%2520method%2520is%2520simple%2520yet%2520highly%2520flexible%252C%2520naturally%250Aaccepting%2520varying%2520lengths%2520of%2520images%2520that%2520may%2520be%2520either%2520video%2520streams%2520or%250Aunordered%2520photo%2520collections%252C%2520containing%2520both%2520static%2520and%2520dynamic%2520content.%2520We%250Aevaluate%2520our%2520method%2520on%2520various%25203D/4D%2520tasks%2520and%2520demonstrate%2520competitive%2520or%250Astate-of-the-art%2520performance%2520in%2520each.%2520Project%2520Page%253A%2520https%253A//cut3r.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%203D%20Perception%20Model%20with%20Persistent%20State&entry.906535625=Qianqian%20Wang%20and%20Yifei%20Zhang%20and%20Aleksander%20Holynski%20and%20Alexei%20A.%20Efros%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20We%20present%20a%20unified%20framework%20capable%20of%20solving%20a%20broad%20range%20of%203D%20tasks.%0AOur%20approach%20features%20a%20stateful%20recurrent%20model%20that%20continuously%20updates%20its%0Astate%20representation%20with%20each%20new%20observation.%20Given%20a%20stream%20of%20images%2C%20this%0Aevolving%20state%20can%20be%20used%20to%20generate%20metric-scale%20pointmaps%20%28per-pixel%203D%0Apoints%29%20for%20each%20new%20input%20in%20an%20online%20fashion.%20These%20pointmaps%20reside%20within%0Aa%20common%20coordinate%20system%2C%20and%20can%20be%20accumulated%20into%20a%20coherent%2C%20dense%20scene%0Areconstruction%20that%20updates%20as%20new%20images%20arrive.%20Our%20model%2C%20called%20CUT3R%0A%28Continuous%20Updating%20Transformer%20for%203D%20Reconstruction%29%2C%20captures%20rich%20priors%0Aof%20real-world%20scenes%3A%20not%20only%20can%20it%20predict%20accurate%20pointmaps%20from%20image%0Aobservations%2C%20but%20it%20can%20also%20infer%20unseen%20regions%20of%20the%20scene%20by%20probing%20at%0Avirtual%2C%20unobserved%20views.%20Our%20method%20is%20simple%20yet%20highly%20flexible%2C%20naturally%0Aaccepting%20varying%20lengths%20of%20images%20that%20may%20be%20either%20video%20streams%20or%0Aunordered%20photo%20collections%2C%20containing%20both%20static%20and%20dynamic%20content.%20We%0Aevaluate%20our%20method%20on%20various%203D/4D%20tasks%20and%20demonstrate%20competitive%20or%0Astate-of-the-art%20performance%20in%20each.%20Project%20Page%3A%20https%3A//cut3r.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12387v1&entry.124074799=Read"},
{"title": "SVGS-DSGAT: An IoT-Enabled Innovation in Underwater Robotic Object\n  Detection Technology", "author": "Dongli Wu and Ling Luo", "abstract": "  With the advancement of Internet of Things (IoT) technology, underwater\ntarget detection and tracking have become increasingly important for ocean\nmonitoring and resource management. Existing methods often fall short in\nhandling high-noise and low-contrast images in complex underwater environments,\nlacking precision and robustness. This paper introduces a novel SVGS-DSGAT\nmodel that combines GraphSage, SVAM, and DSGAT modules, enhancing feature\nextraction and target detection capabilities through graph neural networks and\nattention mechanisms. The model integrates IoT technology to facilitate\nreal-time data collection and processing, optimizing resource allocation and\nmodel responsiveness. Experimental results demonstrate that the SVGS-DSGAT\nmodel achieves an mAP of 40.8% on the URPC 2020 dataset and 41.5% on the\nSeaDronesSee dataset, significantly outperforming existing mainstream models.\nThis IoT-enhanced approach not only excels in high-noise and complex\nbackgrounds but also improves the overall efficiency and scalability of the\nsystem. This research provides an effective IoT solution for underwater target\ndetection technology, offering significant practical application value and\nbroad development prospects.\n", "link": "http://arxiv.org/abs/2501.12169v1", "date": "2025-01-21", "relevancy": 2.6302, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5311}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5257}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVGS-DSGAT%3A%20An%20IoT-Enabled%20Innovation%20in%20Underwater%20Robotic%20Object%0A%20%20Detection%20Technology&body=Title%3A%20SVGS-DSGAT%3A%20An%20IoT-Enabled%20Innovation%20in%20Underwater%20Robotic%20Object%0A%20%20Detection%20Technology%0AAuthor%3A%20Dongli%20Wu%20and%20Ling%20Luo%0AAbstract%3A%20%20%20With%20the%20advancement%20of%20Internet%20of%20Things%20%28IoT%29%20technology%2C%20underwater%0Atarget%20detection%20and%20tracking%20have%20become%20increasingly%20important%20for%20ocean%0Amonitoring%20and%20resource%20management.%20Existing%20methods%20often%20fall%20short%20in%0Ahandling%20high-noise%20and%20low-contrast%20images%20in%20complex%20underwater%20environments%2C%0Alacking%20precision%20and%20robustness.%20This%20paper%20introduces%20a%20novel%20SVGS-DSGAT%0Amodel%20that%20combines%20GraphSage%2C%20SVAM%2C%20and%20DSGAT%20modules%2C%20enhancing%20feature%0Aextraction%20and%20target%20detection%20capabilities%20through%20graph%20neural%20networks%20and%0Aattention%20mechanisms.%20The%20model%20integrates%20IoT%20technology%20to%20facilitate%0Areal-time%20data%20collection%20and%20processing%2C%20optimizing%20resource%20allocation%20and%0Amodel%20responsiveness.%20Experimental%20results%20demonstrate%20that%20the%20SVGS-DSGAT%0Amodel%20achieves%20an%20mAP%20of%2040.8%25%20on%20the%20URPC%202020%20dataset%20and%2041.5%25%20on%20the%0ASeaDronesSee%20dataset%2C%20significantly%20outperforming%20existing%20mainstream%20models.%0AThis%20IoT-enhanced%20approach%20not%20only%20excels%20in%20high-noise%20and%20complex%0Abackgrounds%20but%20also%20improves%20the%20overall%20efficiency%20and%20scalability%20of%20the%0Asystem.%20This%20research%20provides%20an%20effective%20IoT%20solution%20for%20underwater%20target%0Adetection%20technology%2C%20offering%20significant%20practical%20application%20value%20and%0Abroad%20development%20prospects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVGS-DSGAT%253A%2520An%2520IoT-Enabled%2520Innovation%2520in%2520Underwater%2520Robotic%2520Object%250A%2520%2520Detection%2520Technology%26entry.906535625%3DDongli%2520Wu%2520and%2520Ling%2520Luo%26entry.1292438233%3D%2520%2520With%2520the%2520advancement%2520of%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520technology%252C%2520underwater%250Atarget%2520detection%2520and%2520tracking%2520have%2520become%2520increasingly%2520important%2520for%2520ocean%250Amonitoring%2520and%2520resource%2520management.%2520Existing%2520methods%2520often%2520fall%2520short%2520in%250Ahandling%2520high-noise%2520and%2520low-contrast%2520images%2520in%2520complex%2520underwater%2520environments%252C%250Alacking%2520precision%2520and%2520robustness.%2520This%2520paper%2520introduces%2520a%2520novel%2520SVGS-DSGAT%250Amodel%2520that%2520combines%2520GraphSage%252C%2520SVAM%252C%2520and%2520DSGAT%2520modules%252C%2520enhancing%2520feature%250Aextraction%2520and%2520target%2520detection%2520capabilities%2520through%2520graph%2520neural%2520networks%2520and%250Aattention%2520mechanisms.%2520The%2520model%2520integrates%2520IoT%2520technology%2520to%2520facilitate%250Areal-time%2520data%2520collection%2520and%2520processing%252C%2520optimizing%2520resource%2520allocation%2520and%250Amodel%2520responsiveness.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520SVGS-DSGAT%250Amodel%2520achieves%2520an%2520mAP%2520of%252040.8%2525%2520on%2520the%2520URPC%25202020%2520dataset%2520and%252041.5%2525%2520on%2520the%250ASeaDronesSee%2520dataset%252C%2520significantly%2520outperforming%2520existing%2520mainstream%2520models.%250AThis%2520IoT-enhanced%2520approach%2520not%2520only%2520excels%2520in%2520high-noise%2520and%2520complex%250Abackgrounds%2520but%2520also%2520improves%2520the%2520overall%2520efficiency%2520and%2520scalability%2520of%2520the%250Asystem.%2520This%2520research%2520provides%2520an%2520effective%2520IoT%2520solution%2520for%2520underwater%2520target%250Adetection%2520technology%252C%2520offering%2520significant%2520practical%2520application%2520value%2520and%250Abroad%2520development%2520prospects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVGS-DSGAT%3A%20An%20IoT-Enabled%20Innovation%20in%20Underwater%20Robotic%20Object%0A%20%20Detection%20Technology&entry.906535625=Dongli%20Wu%20and%20Ling%20Luo&entry.1292438233=%20%20With%20the%20advancement%20of%20Internet%20of%20Things%20%28IoT%29%20technology%2C%20underwater%0Atarget%20detection%20and%20tracking%20have%20become%20increasingly%20important%20for%20ocean%0Amonitoring%20and%20resource%20management.%20Existing%20methods%20often%20fall%20short%20in%0Ahandling%20high-noise%20and%20low-contrast%20images%20in%20complex%20underwater%20environments%2C%0Alacking%20precision%20and%20robustness.%20This%20paper%20introduces%20a%20novel%20SVGS-DSGAT%0Amodel%20that%20combines%20GraphSage%2C%20SVAM%2C%20and%20DSGAT%20modules%2C%20enhancing%20feature%0Aextraction%20and%20target%20detection%20capabilities%20through%20graph%20neural%20networks%20and%0Aattention%20mechanisms.%20The%20model%20integrates%20IoT%20technology%20to%20facilitate%0Areal-time%20data%20collection%20and%20processing%2C%20optimizing%20resource%20allocation%20and%0Amodel%20responsiveness.%20Experimental%20results%20demonstrate%20that%20the%20SVGS-DSGAT%0Amodel%20achieves%20an%20mAP%20of%2040.8%25%20on%20the%20URPC%202020%20dataset%20and%2041.5%25%20on%20the%0ASeaDronesSee%20dataset%2C%20significantly%20outperforming%20existing%20mainstream%20models.%0AThis%20IoT-enhanced%20approach%20not%20only%20excels%20in%20high-noise%20and%20complex%0Abackgrounds%20but%20also%20improves%20the%20overall%20efficiency%20and%20scalability%20of%20the%0Asystem.%20This%20research%20provides%20an%20effective%20IoT%20solution%20for%20underwater%20target%0Adetection%20technology%2C%20offering%20significant%20practical%20application%20value%20and%0Abroad%20development%20prospects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12169v1&entry.124074799=Read"},
{"title": "A Hybrid Supervised and Self-Supervised Graph Neural Network for\n  Edge-Centric Applications", "author": "Eugenio Borzone and Leandro Di Persia and Matias Gerard", "abstract": "  This paper presents a novel graph-based deep learning model for tasks\ninvolving relations between two nodes (edge-centric tasks), where the focus\nlies on predicting relationships and interactions between pairs of nodes rather\nthan node properties themselves. This model combines supervised and\nself-supervised learning, taking into account for the loss function the\nembeddings learned and patterns with and without ground truth. Additionally it\nincorporates an attention mechanism that leverages both node and edge features.\nThe architecture, trained end-to-end, comprises two primary components:\nembedding generation and prediction. First, a graph neural network (GNN)\ntransform raw node features into dense, low-dimensional embeddings,\nincorporating edge attributes. Then, a feedforward neural model processes the\nnode embeddings to produce the final output. Experiments demonstrate that our\nmodel matches or exceeds existing methods for protein-protein interactions\nprediction and Gene Ontology (GO) terms prediction. The model also performs\neffectively with one-hot encoding for node features, providing a solution for\nthe previously unsolved problem of predicting similarity between compounds with\nunknown structures.\n", "link": "http://arxiv.org/abs/2501.12309v1", "date": "2025-01-21", "relevancy": 2.6111, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5281}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Supervised%20and%20Self-Supervised%20Graph%20Neural%20Network%20for%0A%20%20Edge-Centric%20Applications&body=Title%3A%20A%20Hybrid%20Supervised%20and%20Self-Supervised%20Graph%20Neural%20Network%20for%0A%20%20Edge-Centric%20Applications%0AAuthor%3A%20Eugenio%20Borzone%20and%20Leandro%20Di%20Persia%20and%20Matias%20Gerard%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20graph-based%20deep%20learning%20model%20for%20tasks%0Ainvolving%20relations%20between%20two%20nodes%20%28edge-centric%20tasks%29%2C%20where%20the%20focus%0Alies%20on%20predicting%20relationships%20and%20interactions%20between%20pairs%20of%20nodes%20rather%0Athan%20node%20properties%20themselves.%20This%20model%20combines%20supervised%20and%0Aself-supervised%20learning%2C%20taking%20into%20account%20for%20the%20loss%20function%20the%0Aembeddings%20learned%20and%20patterns%20with%20and%20without%20ground%20truth.%20Additionally%20it%0Aincorporates%20an%20attention%20mechanism%20that%20leverages%20both%20node%20and%20edge%20features.%0AThe%20architecture%2C%20trained%20end-to-end%2C%20comprises%20two%20primary%20components%3A%0Aembedding%20generation%20and%20prediction.%20First%2C%20a%20graph%20neural%20network%20%28GNN%29%0Atransform%20raw%20node%20features%20into%20dense%2C%20low-dimensional%20embeddings%2C%0Aincorporating%20edge%20attributes.%20Then%2C%20a%20feedforward%20neural%20model%20processes%20the%0Anode%20embeddings%20to%20produce%20the%20final%20output.%20Experiments%20demonstrate%20that%20our%0Amodel%20matches%20or%20exceeds%20existing%20methods%20for%20protein-protein%20interactions%0Aprediction%20and%20Gene%20Ontology%20%28GO%29%20terms%20prediction.%20The%20model%20also%20performs%0Aeffectively%20with%20one-hot%20encoding%20for%20node%20features%2C%20providing%20a%20solution%20for%0Athe%20previously%20unsolved%20problem%20of%20predicting%20similarity%20between%20compounds%20with%0Aunknown%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Supervised%2520and%2520Self-Supervised%2520Graph%2520Neural%2520Network%2520for%250A%2520%2520Edge-Centric%2520Applications%26entry.906535625%3DEugenio%2520Borzone%2520and%2520Leandro%2520Di%2520Persia%2520and%2520Matias%2520Gerard%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520graph-based%2520deep%2520learning%2520model%2520for%2520tasks%250Ainvolving%2520relations%2520between%2520two%2520nodes%2520%2528edge-centric%2520tasks%2529%252C%2520where%2520the%2520focus%250Alies%2520on%2520predicting%2520relationships%2520and%2520interactions%2520between%2520pairs%2520of%2520nodes%2520rather%250Athan%2520node%2520properties%2520themselves.%2520This%2520model%2520combines%2520supervised%2520and%250Aself-supervised%2520learning%252C%2520taking%2520into%2520account%2520for%2520the%2520loss%2520function%2520the%250Aembeddings%2520learned%2520and%2520patterns%2520with%2520and%2520without%2520ground%2520truth.%2520Additionally%2520it%250Aincorporates%2520an%2520attention%2520mechanism%2520that%2520leverages%2520both%2520node%2520and%2520edge%2520features.%250AThe%2520architecture%252C%2520trained%2520end-to-end%252C%2520comprises%2520two%2520primary%2520components%253A%250Aembedding%2520generation%2520and%2520prediction.%2520First%252C%2520a%2520graph%2520neural%2520network%2520%2528GNN%2529%250Atransform%2520raw%2520node%2520features%2520into%2520dense%252C%2520low-dimensional%2520embeddings%252C%250Aincorporating%2520edge%2520attributes.%2520Then%252C%2520a%2520feedforward%2520neural%2520model%2520processes%2520the%250Anode%2520embeddings%2520to%2520produce%2520the%2520final%2520output.%2520Experiments%2520demonstrate%2520that%2520our%250Amodel%2520matches%2520or%2520exceeds%2520existing%2520methods%2520for%2520protein-protein%2520interactions%250Aprediction%2520and%2520Gene%2520Ontology%2520%2528GO%2529%2520terms%2520prediction.%2520The%2520model%2520also%2520performs%250Aeffectively%2520with%2520one-hot%2520encoding%2520for%2520node%2520features%252C%2520providing%2520a%2520solution%2520for%250Athe%2520previously%2520unsolved%2520problem%2520of%2520predicting%2520similarity%2520between%2520compounds%2520with%250Aunknown%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Supervised%20and%20Self-Supervised%20Graph%20Neural%20Network%20for%0A%20%20Edge-Centric%20Applications&entry.906535625=Eugenio%20Borzone%20and%20Leandro%20Di%20Persia%20and%20Matias%20Gerard&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20graph-based%20deep%20learning%20model%20for%20tasks%0Ainvolving%20relations%20between%20two%20nodes%20%28edge-centric%20tasks%29%2C%20where%20the%20focus%0Alies%20on%20predicting%20relationships%20and%20interactions%20between%20pairs%20of%20nodes%20rather%0Athan%20node%20properties%20themselves.%20This%20model%20combines%20supervised%20and%0Aself-supervised%20learning%2C%20taking%20into%20account%20for%20the%20loss%20function%20the%0Aembeddings%20learned%20and%20patterns%20with%20and%20without%20ground%20truth.%20Additionally%20it%0Aincorporates%20an%20attention%20mechanism%20that%20leverages%20both%20node%20and%20edge%20features.%0AThe%20architecture%2C%20trained%20end-to-end%2C%20comprises%20two%20primary%20components%3A%0Aembedding%20generation%20and%20prediction.%20First%2C%20a%20graph%20neural%20network%20%28GNN%29%0Atransform%20raw%20node%20features%20into%20dense%2C%20low-dimensional%20embeddings%2C%0Aincorporating%20edge%20attributes.%20Then%2C%20a%20feedforward%20neural%20model%20processes%20the%0Anode%20embeddings%20to%20produce%20the%20final%20output.%20Experiments%20demonstrate%20that%20our%0Amodel%20matches%20or%20exceeds%20existing%20methods%20for%20protein-protein%20interactions%0Aprediction%20and%20Gene%20Ontology%20%28GO%29%20terms%20prediction.%20The%20model%20also%20performs%0Aeffectively%20with%20one-hot%20encoding%20for%20node%20features%2C%20providing%20a%20solution%20for%0Athe%20previously%20unsolved%20problem%20of%20predicting%20similarity%20between%20compounds%20with%0Aunknown%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12309v1&entry.124074799=Read"},
{"title": "RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression", "author": "Uri Gadot and Assaf Shocher and Shie Mannor and Gal Chechik and Assaf Hallak", "abstract": "  Video encoders optimize compression for human perception by minimizing\nreconstruction error under bit-rate constraints. In many modern applications\nsuch as autonomous driving, an overwhelming majority of videos serve as input\nfor AI systems performing tasks like object recognition or segmentation, rather\nthan being watched by humans. It is therefore useful to optimize the encoder\nfor a downstream task instead of for perceptual image quality. However, a major\nchallenge is how to combine such downstream optimization with existing standard\nvideo encoders, which are highly efficient and popular. Here, we address this\nchallenge by controlling the Quantization Parameters (QPs) at the macro-block\nlevel to optimize the downstream task. This granular control allows us to\nprioritize encoding for task-relevant regions within each frame. We formulate\nthis optimization problem as a Reinforcement Learning (RL) task, where the\nagent learns to balance long-term implications of choosing QPs on both task\nperformance and bit-rate constraints. Notably, our policy does not require the\ndownstream task as an input during inference, making it suitable for streaming\napplications and edge devices such as vehicles. We demonstrate significant\nimprovements in two tasks, car detection, and ROI (saliency) encoding. Our\napproach improves task performance for a given bit rate compared to traditional\ntask agnostic encoding methods, paving the way for more efficient task-aware\nvideo compression.\n", "link": "http://arxiv.org/abs/2501.12216v1", "date": "2025-01-21", "relevancy": 2.5998, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.52}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5199}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL-RC-DoT%3A%20A%20Block-level%20RL%20agent%20for%20Task-Aware%20Video%20Compression&body=Title%3A%20RL-RC-DoT%3A%20A%20Block-level%20RL%20agent%20for%20Task-Aware%20Video%20Compression%0AAuthor%3A%20Uri%20Gadot%20and%20Assaf%20Shocher%20and%20Shie%20Mannor%20and%20Gal%20Chechik%20and%20Assaf%20Hallak%0AAbstract%3A%20%20%20Video%20encoders%20optimize%20compression%20for%20human%20perception%20by%20minimizing%0Areconstruction%20error%20under%20bit-rate%20constraints.%20In%20many%20modern%20applications%0Asuch%20as%20autonomous%20driving%2C%20an%20overwhelming%20majority%20of%20videos%20serve%20as%20input%0Afor%20AI%20systems%20performing%20tasks%20like%20object%20recognition%20or%20segmentation%2C%20rather%0Athan%20being%20watched%20by%20humans.%20It%20is%20therefore%20useful%20to%20optimize%20the%20encoder%0Afor%20a%20downstream%20task%20instead%20of%20for%20perceptual%20image%20quality.%20However%2C%20a%20major%0Achallenge%20is%20how%20to%20combine%20such%20downstream%20optimization%20with%20existing%20standard%0Avideo%20encoders%2C%20which%20are%20highly%20efficient%20and%20popular.%20Here%2C%20we%20address%20this%0Achallenge%20by%20controlling%20the%20Quantization%20Parameters%20%28QPs%29%20at%20the%20macro-block%0Alevel%20to%20optimize%20the%20downstream%20task.%20This%20granular%20control%20allows%20us%20to%0Aprioritize%20encoding%20for%20task-relevant%20regions%20within%20each%20frame.%20We%20formulate%0Athis%20optimization%20problem%20as%20a%20Reinforcement%20Learning%20%28RL%29%20task%2C%20where%20the%0Aagent%20learns%20to%20balance%20long-term%20implications%20of%20choosing%20QPs%20on%20both%20task%0Aperformance%20and%20bit-rate%20constraints.%20Notably%2C%20our%20policy%20does%20not%20require%20the%0Adownstream%20task%20as%20an%20input%20during%20inference%2C%20making%20it%20suitable%20for%20streaming%0Aapplications%20and%20edge%20devices%20such%20as%20vehicles.%20We%20demonstrate%20significant%0Aimprovements%20in%20two%20tasks%2C%20car%20detection%2C%20and%20ROI%20%28saliency%29%20encoding.%20Our%0Aapproach%20improves%20task%20performance%20for%20a%20given%20bit%20rate%20compared%20to%20traditional%0Atask%20agnostic%20encoding%20methods%2C%20paving%20the%20way%20for%20more%20efficient%20task-aware%0Avideo%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL-RC-DoT%253A%2520A%2520Block-level%2520RL%2520agent%2520for%2520Task-Aware%2520Video%2520Compression%26entry.906535625%3DUri%2520Gadot%2520and%2520Assaf%2520Shocher%2520and%2520Shie%2520Mannor%2520and%2520Gal%2520Chechik%2520and%2520Assaf%2520Hallak%26entry.1292438233%3D%2520%2520Video%2520encoders%2520optimize%2520compression%2520for%2520human%2520perception%2520by%2520minimizing%250Areconstruction%2520error%2520under%2520bit-rate%2520constraints.%2520In%2520many%2520modern%2520applications%250Asuch%2520as%2520autonomous%2520driving%252C%2520an%2520overwhelming%2520majority%2520of%2520videos%2520serve%2520as%2520input%250Afor%2520AI%2520systems%2520performing%2520tasks%2520like%2520object%2520recognition%2520or%2520segmentation%252C%2520rather%250Athan%2520being%2520watched%2520by%2520humans.%2520It%2520is%2520therefore%2520useful%2520to%2520optimize%2520the%2520encoder%250Afor%2520a%2520downstream%2520task%2520instead%2520of%2520for%2520perceptual%2520image%2520quality.%2520However%252C%2520a%2520major%250Achallenge%2520is%2520how%2520to%2520combine%2520such%2520downstream%2520optimization%2520with%2520existing%2520standard%250Avideo%2520encoders%252C%2520which%2520are%2520highly%2520efficient%2520and%2520popular.%2520Here%252C%2520we%2520address%2520this%250Achallenge%2520by%2520controlling%2520the%2520Quantization%2520Parameters%2520%2528QPs%2529%2520at%2520the%2520macro-block%250Alevel%2520to%2520optimize%2520the%2520downstream%2520task.%2520This%2520granular%2520control%2520allows%2520us%2520to%250Aprioritize%2520encoding%2520for%2520task-relevant%2520regions%2520within%2520each%2520frame.%2520We%2520formulate%250Athis%2520optimization%2520problem%2520as%2520a%2520Reinforcement%2520Learning%2520%2528RL%2529%2520task%252C%2520where%2520the%250Aagent%2520learns%2520to%2520balance%2520long-term%2520implications%2520of%2520choosing%2520QPs%2520on%2520both%2520task%250Aperformance%2520and%2520bit-rate%2520constraints.%2520Notably%252C%2520our%2520policy%2520does%2520not%2520require%2520the%250Adownstream%2520task%2520as%2520an%2520input%2520during%2520inference%252C%2520making%2520it%2520suitable%2520for%2520streaming%250Aapplications%2520and%2520edge%2520devices%2520such%2520as%2520vehicles.%2520We%2520demonstrate%2520significant%250Aimprovements%2520in%2520two%2520tasks%252C%2520car%2520detection%252C%2520and%2520ROI%2520%2528saliency%2529%2520encoding.%2520Our%250Aapproach%2520improves%2520task%2520performance%2520for%2520a%2520given%2520bit%2520rate%2520compared%2520to%2520traditional%250Atask%2520agnostic%2520encoding%2520methods%252C%2520paving%2520the%2520way%2520for%2520more%2520efficient%2520task-aware%250Avideo%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL-RC-DoT%3A%20A%20Block-level%20RL%20agent%20for%20Task-Aware%20Video%20Compression&entry.906535625=Uri%20Gadot%20and%20Assaf%20Shocher%20and%20Shie%20Mannor%20and%20Gal%20Chechik%20and%20Assaf%20Hallak&entry.1292438233=%20%20Video%20encoders%20optimize%20compression%20for%20human%20perception%20by%20minimizing%0Areconstruction%20error%20under%20bit-rate%20constraints.%20In%20many%20modern%20applications%0Asuch%20as%20autonomous%20driving%2C%20an%20overwhelming%20majority%20of%20videos%20serve%20as%20input%0Afor%20AI%20systems%20performing%20tasks%20like%20object%20recognition%20or%20segmentation%2C%20rather%0Athan%20being%20watched%20by%20humans.%20It%20is%20therefore%20useful%20to%20optimize%20the%20encoder%0Afor%20a%20downstream%20task%20instead%20of%20for%20perceptual%20image%20quality.%20However%2C%20a%20major%0Achallenge%20is%20how%20to%20combine%20such%20downstream%20optimization%20with%20existing%20standard%0Avideo%20encoders%2C%20which%20are%20highly%20efficient%20and%20popular.%20Here%2C%20we%20address%20this%0Achallenge%20by%20controlling%20the%20Quantization%20Parameters%20%28QPs%29%20at%20the%20macro-block%0Alevel%20to%20optimize%20the%20downstream%20task.%20This%20granular%20control%20allows%20us%20to%0Aprioritize%20encoding%20for%20task-relevant%20regions%20within%20each%20frame.%20We%20formulate%0Athis%20optimization%20problem%20as%20a%20Reinforcement%20Learning%20%28RL%29%20task%2C%20where%20the%0Aagent%20learns%20to%20balance%20long-term%20implications%20of%20choosing%20QPs%20on%20both%20task%0Aperformance%20and%20bit-rate%20constraints.%20Notably%2C%20our%20policy%20does%20not%20require%20the%0Adownstream%20task%20as%20an%20input%20during%20inference%2C%20making%20it%20suitable%20for%20streaming%0Aapplications%20and%20edge%20devices%20such%20as%20vehicles.%20We%20demonstrate%20significant%0Aimprovements%20in%20two%20tasks%2C%20car%20detection%2C%20and%20ROI%20%28saliency%29%20encoding.%20Our%0Aapproach%20improves%20task%20performance%20for%20a%20given%20bit%20rate%20compared%20to%20traditional%0Atask%20agnostic%20encoding%20methods%2C%20paving%20the%20way%20for%20more%20efficient%20task-aware%0Avideo%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12216v1&entry.124074799=Read"},
{"title": "Beyond Specialization: Assessing the Capabilities of MLLMs in Age and\n  Gender Estimation", "author": "Maksim Kuprashevich and Grigorii Alekseenko and Irina Tolstykh", "abstract": "  Multimodal Large Language Models (MLLMs) have recently gained immense\npopularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as\nopen-source ones such as LLaVA, are essentially general-purpose models and are\napplied to solve a wide variety of tasks, including those in computer vision.\nThese neural networks possess such strong general knowledge and reasoning\nabilities that they have proven capable of working even on tasks for which they\nwere not specifically trained. We compared the capabilities of the most\npowerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task\nof age and gender estimation with our state-of-the-art specialized model,\nMiVOLO. We also updated MiVOLO and provide details and new metrics in this\narticle. This comparison has yielded some interesting results and insights\nabout the strengths and weaknesses of the participating models. Furthermore, we\nattempted various ways to fine-tune the ShareGPT4V model for this specific\ntask, aiming to achieve state-of-the-art results in this particular challenge.\nAlthough such a model would not be practical in production, as it is incredibly\nexpensive compared to a specialized model like MiVOLO, it could be very useful\nin some tasks, like data annotation.\n", "link": "http://arxiv.org/abs/2403.02302v4", "date": "2025-01-21", "relevancy": 2.5729, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Specialization%3A%20Assessing%20the%20Capabilities%20of%20MLLMs%20in%20Age%20and%0A%20%20Gender%20Estimation&body=Title%3A%20Beyond%20Specialization%3A%20Assessing%20the%20Capabilities%20of%20MLLMs%20in%20Age%20and%0A%20%20Gender%20Estimation%0AAuthor%3A%20Maksim%20Kuprashevich%20and%20Grigorii%20Alekseenko%20and%20Irina%20Tolstykh%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20gained%20immense%0Apopularity.%20Powerful%20commercial%20models%20like%20ChatGPT-4V%20and%20Gemini%2C%20as%20well%20as%0Aopen-source%20ones%20such%20as%20LLaVA%2C%20are%20essentially%20general-purpose%20models%20and%20are%0Aapplied%20to%20solve%20a%20wide%20variety%20of%20tasks%2C%20including%20those%20in%20computer%20vision.%0AThese%20neural%20networks%20possess%20such%20strong%20general%20knowledge%20and%20reasoning%0Aabilities%20that%20they%20have%20proven%20capable%20of%20working%20even%20on%20tasks%20for%20which%20they%0Awere%20not%20specifically%20trained.%20We%20compared%20the%20capabilities%20of%20the%20most%0Apowerful%20MLLMs%20to%20date%3A%20ShareGPT4V%2C%20ChatGPT%2C%20LLaVA-Next%20in%20a%20specialized%20task%0Aof%20age%20and%20gender%20estimation%20with%20our%20state-of-the-art%20specialized%20model%2C%0AMiVOLO.%20We%20also%20updated%20MiVOLO%20and%20provide%20details%20and%20new%20metrics%20in%20this%0Aarticle.%20This%20comparison%20has%20yielded%20some%20interesting%20results%20and%20insights%0Aabout%20the%20strengths%20and%20weaknesses%20of%20the%20participating%20models.%20Furthermore%2C%20we%0Aattempted%20various%20ways%20to%20fine-tune%20the%20ShareGPT4V%20model%20for%20this%20specific%0Atask%2C%20aiming%20to%20achieve%20state-of-the-art%20results%20in%20this%20particular%20challenge.%0AAlthough%20such%20a%20model%20would%20not%20be%20practical%20in%20production%2C%20as%20it%20is%20incredibly%0Aexpensive%20compared%20to%20a%20specialized%20model%20like%20MiVOLO%2C%20it%20could%20be%20very%20useful%0Ain%20some%20tasks%2C%20like%20data%20annotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02302v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Specialization%253A%2520Assessing%2520the%2520Capabilities%2520of%2520MLLMs%2520in%2520Age%2520and%250A%2520%2520Gender%2520Estimation%26entry.906535625%3DMaksim%2520Kuprashevich%2520and%2520Grigorii%2520Alekseenko%2520and%2520Irina%2520Tolstykh%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520recently%2520gained%2520immense%250Apopularity.%2520Powerful%2520commercial%2520models%2520like%2520ChatGPT-4V%2520and%2520Gemini%252C%2520as%2520well%2520as%250Aopen-source%2520ones%2520such%2520as%2520LLaVA%252C%2520are%2520essentially%2520general-purpose%2520models%2520and%2520are%250Aapplied%2520to%2520solve%2520a%2520wide%2520variety%2520of%2520tasks%252C%2520including%2520those%2520in%2520computer%2520vision.%250AThese%2520neural%2520networks%2520possess%2520such%2520strong%2520general%2520knowledge%2520and%2520reasoning%250Aabilities%2520that%2520they%2520have%2520proven%2520capable%2520of%2520working%2520even%2520on%2520tasks%2520for%2520which%2520they%250Awere%2520not%2520specifically%2520trained.%2520We%2520compared%2520the%2520capabilities%2520of%2520the%2520most%250Apowerful%2520MLLMs%2520to%2520date%253A%2520ShareGPT4V%252C%2520ChatGPT%252C%2520LLaVA-Next%2520in%2520a%2520specialized%2520task%250Aof%2520age%2520and%2520gender%2520estimation%2520with%2520our%2520state-of-the-art%2520specialized%2520model%252C%250AMiVOLO.%2520We%2520also%2520updated%2520MiVOLO%2520and%2520provide%2520details%2520and%2520new%2520metrics%2520in%2520this%250Aarticle.%2520This%2520comparison%2520has%2520yielded%2520some%2520interesting%2520results%2520and%2520insights%250Aabout%2520the%2520strengths%2520and%2520weaknesses%2520of%2520the%2520participating%2520models.%2520Furthermore%252C%2520we%250Aattempted%2520various%2520ways%2520to%2520fine-tune%2520the%2520ShareGPT4V%2520model%2520for%2520this%2520specific%250Atask%252C%2520aiming%2520to%2520achieve%2520state-of-the-art%2520results%2520in%2520this%2520particular%2520challenge.%250AAlthough%2520such%2520a%2520model%2520would%2520not%2520be%2520practical%2520in%2520production%252C%2520as%2520it%2520is%2520incredibly%250Aexpensive%2520compared%2520to%2520a%2520specialized%2520model%2520like%2520MiVOLO%252C%2520it%2520could%2520be%2520very%2520useful%250Ain%2520some%2520tasks%252C%2520like%2520data%2520annotation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02302v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Specialization%3A%20Assessing%20the%20Capabilities%20of%20MLLMs%20in%20Age%20and%0A%20%20Gender%20Estimation&entry.906535625=Maksim%20Kuprashevich%20and%20Grigorii%20Alekseenko%20and%20Irina%20Tolstykh&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20gained%20immense%0Apopularity.%20Powerful%20commercial%20models%20like%20ChatGPT-4V%20and%20Gemini%2C%20as%20well%20as%0Aopen-source%20ones%20such%20as%20LLaVA%2C%20are%20essentially%20general-purpose%20models%20and%20are%0Aapplied%20to%20solve%20a%20wide%20variety%20of%20tasks%2C%20including%20those%20in%20computer%20vision.%0AThese%20neural%20networks%20possess%20such%20strong%20general%20knowledge%20and%20reasoning%0Aabilities%20that%20they%20have%20proven%20capable%20of%20working%20even%20on%20tasks%20for%20which%20they%0Awere%20not%20specifically%20trained.%20We%20compared%20the%20capabilities%20of%20the%20most%0Apowerful%20MLLMs%20to%20date%3A%20ShareGPT4V%2C%20ChatGPT%2C%20LLaVA-Next%20in%20a%20specialized%20task%0Aof%20age%20and%20gender%20estimation%20with%20our%20state-of-the-art%20specialized%20model%2C%0AMiVOLO.%20We%20also%20updated%20MiVOLO%20and%20provide%20details%20and%20new%20metrics%20in%20this%0Aarticle.%20This%20comparison%20has%20yielded%20some%20interesting%20results%20and%20insights%0Aabout%20the%20strengths%20and%20weaknesses%20of%20the%20participating%20models.%20Furthermore%2C%20we%0Aattempted%20various%20ways%20to%20fine-tune%20the%20ShareGPT4V%20model%20for%20this%20specific%0Atask%2C%20aiming%20to%20achieve%20state-of-the-art%20results%20in%20this%20particular%20challenge.%0AAlthough%20such%20a%20model%20would%20not%20be%20practical%20in%20production%2C%20as%20it%20is%20incredibly%0Aexpensive%20compared%20to%20a%20specialized%20model%20like%20MiVOLO%2C%20it%20could%20be%20very%20useful%0Ain%20some%20tasks%2C%20like%20data%20annotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02302v4&entry.124074799=Read"},
{"title": "Optimally-Weighted Maximum Mean Discrepancy Framework for Continual\n  Learning", "author": "KaiHui Huang and RunQing Wu and Fei Ye", "abstract": "  Continual learning has emerged as a pivotal area of research, primarily due\nto its advantageous characteristic that allows models to persistently acquire\nand retain information. However, catastrophic forgetting can severely impair\nmodel performance. In this study, we tackle the issue of network forgetting by\nintroducing a novel framework termed Optimally-Weighted Maximum Mean\nDiscrepancy (OWMMD), which imposes penalties on representation alterations via\na Multi-Level Feature Matching Mechanism (MLFMM). Furthermore, we propose an\nAdaptive Regularization Optimization (ARO) strategy to refine the adaptive\nweight vectors, which autonomously assess the significance of each feature\nlayer throughout the optimization process. We conduct a comprehensive series of\nexperiments, benchmarking our proposed method against several established\nbaselines. The empirical findings indicate that our approach achieves\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2501.12121v1", "date": "2025-01-21", "relevancy": 2.5524, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5114}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5112}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimally-Weighted%20Maximum%20Mean%20Discrepancy%20Framework%20for%20Continual%0A%20%20Learning&body=Title%3A%20Optimally-Weighted%20Maximum%20Mean%20Discrepancy%20Framework%20for%20Continual%0A%20%20Learning%0AAuthor%3A%20KaiHui%20Huang%20and%20RunQing%20Wu%20and%20Fei%20Ye%0AAbstract%3A%20%20%20Continual%20learning%20has%20emerged%20as%20a%20pivotal%20area%20of%20research%2C%20primarily%20due%0Ato%20its%20advantageous%20characteristic%20that%20allows%20models%20to%20persistently%20acquire%0Aand%20retain%20information.%20However%2C%20catastrophic%20forgetting%20can%20severely%20impair%0Amodel%20performance.%20In%20this%20study%2C%20we%20tackle%20the%20issue%20of%20network%20forgetting%20by%0Aintroducing%20a%20novel%20framework%20termed%20Optimally-Weighted%20Maximum%20Mean%0ADiscrepancy%20%28OWMMD%29%2C%20which%20imposes%20penalties%20on%20representation%20alterations%20via%0Aa%20Multi-Level%20Feature%20Matching%20Mechanism%20%28MLFMM%29.%20Furthermore%2C%20we%20propose%20an%0AAdaptive%20Regularization%20Optimization%20%28ARO%29%20strategy%20to%20refine%20the%20adaptive%0Aweight%20vectors%2C%20which%20autonomously%20assess%20the%20significance%20of%20each%20feature%0Alayer%20throughout%20the%20optimization%20process.%20We%20conduct%20a%20comprehensive%20series%20of%0Aexperiments%2C%20benchmarking%20our%20proposed%20method%20against%20several%20established%0Abaselines.%20The%20empirical%20findings%20indicate%20that%20our%20approach%20achieves%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimally-Weighted%2520Maximum%2520Mean%2520Discrepancy%2520Framework%2520for%2520Continual%250A%2520%2520Learning%26entry.906535625%3DKaiHui%2520Huang%2520and%2520RunQing%2520Wu%2520and%2520Fei%2520Ye%26entry.1292438233%3D%2520%2520Continual%2520learning%2520has%2520emerged%2520as%2520a%2520pivotal%2520area%2520of%2520research%252C%2520primarily%2520due%250Ato%2520its%2520advantageous%2520characteristic%2520that%2520allows%2520models%2520to%2520persistently%2520acquire%250Aand%2520retain%2520information.%2520However%252C%2520catastrophic%2520forgetting%2520can%2520severely%2520impair%250Amodel%2520performance.%2520In%2520this%2520study%252C%2520we%2520tackle%2520the%2520issue%2520of%2520network%2520forgetting%2520by%250Aintroducing%2520a%2520novel%2520framework%2520termed%2520Optimally-Weighted%2520Maximum%2520Mean%250ADiscrepancy%2520%2528OWMMD%2529%252C%2520which%2520imposes%2520penalties%2520on%2520representation%2520alterations%2520via%250Aa%2520Multi-Level%2520Feature%2520Matching%2520Mechanism%2520%2528MLFMM%2529.%2520Furthermore%252C%2520we%2520propose%2520an%250AAdaptive%2520Regularization%2520Optimization%2520%2528ARO%2529%2520strategy%2520to%2520refine%2520the%2520adaptive%250Aweight%2520vectors%252C%2520which%2520autonomously%2520assess%2520the%2520significance%2520of%2520each%2520feature%250Alayer%2520throughout%2520the%2520optimization%2520process.%2520We%2520conduct%2520a%2520comprehensive%2520series%2520of%250Aexperiments%252C%2520benchmarking%2520our%2520proposed%2520method%2520against%2520several%2520established%250Abaselines.%2520The%2520empirical%2520findings%2520indicate%2520that%2520our%2520approach%2520achieves%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimally-Weighted%20Maximum%20Mean%20Discrepancy%20Framework%20for%20Continual%0A%20%20Learning&entry.906535625=KaiHui%20Huang%20and%20RunQing%20Wu%20and%20Fei%20Ye&entry.1292438233=%20%20Continual%20learning%20has%20emerged%20as%20a%20pivotal%20area%20of%20research%2C%20primarily%20due%0Ato%20its%20advantageous%20characteristic%20that%20allows%20models%20to%20persistently%20acquire%0Aand%20retain%20information.%20However%2C%20catastrophic%20forgetting%20can%20severely%20impair%0Amodel%20performance.%20In%20this%20study%2C%20we%20tackle%20the%20issue%20of%20network%20forgetting%20by%0Aintroducing%20a%20novel%20framework%20termed%20Optimally-Weighted%20Maximum%20Mean%0ADiscrepancy%20%28OWMMD%29%2C%20which%20imposes%20penalties%20on%20representation%20alterations%20via%0Aa%20Multi-Level%20Feature%20Matching%20Mechanism%20%28MLFMM%29.%20Furthermore%2C%20we%20propose%20an%0AAdaptive%20Regularization%20Optimization%20%28ARO%29%20strategy%20to%20refine%20the%20adaptive%0Aweight%20vectors%2C%20which%20autonomously%20assess%20the%20significance%20of%20each%20feature%0Alayer%20throughout%20the%20optimization%20process.%20We%20conduct%20a%20comprehensive%20series%20of%0Aexperiments%2C%20benchmarking%20our%20proposed%20method%20against%20several%20established%0Abaselines.%20The%20empirical%20findings%20indicate%20that%20our%20approach%20achieves%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12121v1&entry.124074799=Read"},
{"title": "VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free\n  Denoising Diffusion Models", "author": "Chaohao Xie and Kai Han and Kwan-Yee K. Wong", "abstract": "  Recent video inpainting methods have achieved encouraging improvements by\nleveraging optical flow to guide pixel propagation from reference frames either\nin the image space or feature space. However, they would produce severe\nartifacts in the mask center when the masked area is too large and no pixel\ncorrespondences can be found for the center. Recently, diffusion models have\ndemonstrated impressive performance in generating diverse and high-quality\nimages, and have been exploited in a number of works for image inpainting.\nThese methods, however, cannot be applied directly to videos to produce\ntemporal-coherent inpainting results. In this paper, we propose a training-free\nframework, named VipDiff, for conditioning diffusion model on the reverse\ndiffusion process to produce temporal-coherent inpainting results without\nrequiring any training data or fine-tuning the pre-trained diffusion models.\nVipDiff takes optical flow as guidance to extract valid pixels from reference\nframes to serve as constraints in optimizing the randomly sampled Gaussian\nnoise, and uses the generated results for further pixel propagation and\nconditional generation. VipDiff also allows for generating diverse video\ninpainting results over different sampled noise. Experiments demonstrate that\nVipDiff can largely outperform state-of-the-art video inpainting methods in\nterms of both spatial-temporal coherence and fidelity.\n", "link": "http://arxiv.org/abs/2501.12267v1", "date": "2025-01-21", "relevancy": 2.5502, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6601}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6254}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VipDiff%3A%20Towards%20Coherent%20and%20Diverse%20Video%20Inpainting%20via%20Training-free%0A%20%20Denoising%20Diffusion%20Models&body=Title%3A%20VipDiff%3A%20Towards%20Coherent%20and%20Diverse%20Video%20Inpainting%20via%20Training-free%0A%20%20Denoising%20Diffusion%20Models%0AAuthor%3A%20Chaohao%20Xie%20and%20Kai%20Han%20and%20Kwan-Yee%20K.%20Wong%0AAbstract%3A%20%20%20Recent%20video%20inpainting%20methods%20have%20achieved%20encouraging%20improvements%20by%0Aleveraging%20optical%20flow%20to%20guide%20pixel%20propagation%20from%20reference%20frames%20either%0Ain%20the%20image%20space%20or%20feature%20space.%20However%2C%20they%20would%20produce%20severe%0Aartifacts%20in%20the%20mask%20center%20when%20the%20masked%20area%20is%20too%20large%20and%20no%20pixel%0Acorrespondences%20can%20be%20found%20for%20the%20center.%20Recently%2C%20diffusion%20models%20have%0Ademonstrated%20impressive%20performance%20in%20generating%20diverse%20and%20high-quality%0Aimages%2C%20and%20have%20been%20exploited%20in%20a%20number%20of%20works%20for%20image%20inpainting.%0AThese%20methods%2C%20however%2C%20cannot%20be%20applied%20directly%20to%20videos%20to%20produce%0Atemporal-coherent%20inpainting%20results.%20In%20this%20paper%2C%20we%20propose%20a%20training-free%0Aframework%2C%20named%20VipDiff%2C%20for%20conditioning%20diffusion%20model%20on%20the%20reverse%0Adiffusion%20process%20to%20produce%20temporal-coherent%20inpainting%20results%20without%0Arequiring%20any%20training%20data%20or%20fine-tuning%20the%20pre-trained%20diffusion%20models.%0AVipDiff%20takes%20optical%20flow%20as%20guidance%20to%20extract%20valid%20pixels%20from%20reference%0Aframes%20to%20serve%20as%20constraints%20in%20optimizing%20the%20randomly%20sampled%20Gaussian%0Anoise%2C%20and%20uses%20the%20generated%20results%20for%20further%20pixel%20propagation%20and%0Aconditional%20generation.%20VipDiff%20also%20allows%20for%20generating%20diverse%20video%0Ainpainting%20results%20over%20different%20sampled%20noise.%20Experiments%20demonstrate%20that%0AVipDiff%20can%20largely%20outperform%20state-of-the-art%20video%20inpainting%20methods%20in%0Aterms%20of%20both%20spatial-temporal%20coherence%20and%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVipDiff%253A%2520Towards%2520Coherent%2520and%2520Diverse%2520Video%2520Inpainting%2520via%2520Training-free%250A%2520%2520Denoising%2520Diffusion%2520Models%26entry.906535625%3DChaohao%2520Xie%2520and%2520Kai%2520Han%2520and%2520Kwan-Yee%2520K.%2520Wong%26entry.1292438233%3D%2520%2520Recent%2520video%2520inpainting%2520methods%2520have%2520achieved%2520encouraging%2520improvements%2520by%250Aleveraging%2520optical%2520flow%2520to%2520guide%2520pixel%2520propagation%2520from%2520reference%2520frames%2520either%250Ain%2520the%2520image%2520space%2520or%2520feature%2520space.%2520However%252C%2520they%2520would%2520produce%2520severe%250Aartifacts%2520in%2520the%2520mask%2520center%2520when%2520the%2520masked%2520area%2520is%2520too%2520large%2520and%2520no%2520pixel%250Acorrespondences%2520can%2520be%2520found%2520for%2520the%2520center.%2520Recently%252C%2520diffusion%2520models%2520have%250Ademonstrated%2520impressive%2520performance%2520in%2520generating%2520diverse%2520and%2520high-quality%250Aimages%252C%2520and%2520have%2520been%2520exploited%2520in%2520a%2520number%2520of%2520works%2520for%2520image%2520inpainting.%250AThese%2520methods%252C%2520however%252C%2520cannot%2520be%2520applied%2520directly%2520to%2520videos%2520to%2520produce%250Atemporal-coherent%2520inpainting%2520results.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520training-free%250Aframework%252C%2520named%2520VipDiff%252C%2520for%2520conditioning%2520diffusion%2520model%2520on%2520the%2520reverse%250Adiffusion%2520process%2520to%2520produce%2520temporal-coherent%2520inpainting%2520results%2520without%250Arequiring%2520any%2520training%2520data%2520or%2520fine-tuning%2520the%2520pre-trained%2520diffusion%2520models.%250AVipDiff%2520takes%2520optical%2520flow%2520as%2520guidance%2520to%2520extract%2520valid%2520pixels%2520from%2520reference%250Aframes%2520to%2520serve%2520as%2520constraints%2520in%2520optimizing%2520the%2520randomly%2520sampled%2520Gaussian%250Anoise%252C%2520and%2520uses%2520the%2520generated%2520results%2520for%2520further%2520pixel%2520propagation%2520and%250Aconditional%2520generation.%2520VipDiff%2520also%2520allows%2520for%2520generating%2520diverse%2520video%250Ainpainting%2520results%2520over%2520different%2520sampled%2520noise.%2520Experiments%2520demonstrate%2520that%250AVipDiff%2520can%2520largely%2520outperform%2520state-of-the-art%2520video%2520inpainting%2520methods%2520in%250Aterms%2520of%2520both%2520spatial-temporal%2520coherence%2520and%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VipDiff%3A%20Towards%20Coherent%20and%20Diverse%20Video%20Inpainting%20via%20Training-free%0A%20%20Denoising%20Diffusion%20Models&entry.906535625=Chaohao%20Xie%20and%20Kai%20Han%20and%20Kwan-Yee%20K.%20Wong&entry.1292438233=%20%20Recent%20video%20inpainting%20methods%20have%20achieved%20encouraging%20improvements%20by%0Aleveraging%20optical%20flow%20to%20guide%20pixel%20propagation%20from%20reference%20frames%20either%0Ain%20the%20image%20space%20or%20feature%20space.%20However%2C%20they%20would%20produce%20severe%0Aartifacts%20in%20the%20mask%20center%20when%20the%20masked%20area%20is%20too%20large%20and%20no%20pixel%0Acorrespondences%20can%20be%20found%20for%20the%20center.%20Recently%2C%20diffusion%20models%20have%0Ademonstrated%20impressive%20performance%20in%20generating%20diverse%20and%20high-quality%0Aimages%2C%20and%20have%20been%20exploited%20in%20a%20number%20of%20works%20for%20image%20inpainting.%0AThese%20methods%2C%20however%2C%20cannot%20be%20applied%20directly%20to%20videos%20to%20produce%0Atemporal-coherent%20inpainting%20results.%20In%20this%20paper%2C%20we%20propose%20a%20training-free%0Aframework%2C%20named%20VipDiff%2C%20for%20conditioning%20diffusion%20model%20on%20the%20reverse%0Adiffusion%20process%20to%20produce%20temporal-coherent%20inpainting%20results%20without%0Arequiring%20any%20training%20data%20or%20fine-tuning%20the%20pre-trained%20diffusion%20models.%0AVipDiff%20takes%20optical%20flow%20as%20guidance%20to%20extract%20valid%20pixels%20from%20reference%0Aframes%20to%20serve%20as%20constraints%20in%20optimizing%20the%20randomly%20sampled%20Gaussian%0Anoise%2C%20and%20uses%20the%20generated%20results%20for%20further%20pixel%20propagation%20and%0Aconditional%20generation.%20VipDiff%20also%20allows%20for%20generating%20diverse%20video%0Ainpainting%20results%20over%20different%20sampled%20noise.%20Experiments%20demonstrate%20that%0AVipDiff%20can%20largely%20outperform%20state-of-the-art%20video%20inpainting%20methods%20in%0Aterms%20of%20both%20spatial-temporal%20coherence%20and%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12267v1&entry.124074799=Read"},
{"title": "Taming Teacher Forcing for Masked Autoregressive Video Generation", "author": "Deyu Zhou and Quan Sun and Yuang Peng and Kun Yan and Runpei Dong and Duomin Wang and Zheng Ge and Nan Duan and Xiangyu Zhang and Lionel M. Ni and Heung-Yeung Shum", "abstract": "  We introduce MAGI, a hybrid video generation framework that combines masked\nmodeling for intra-frame generation with causal modeling for next-frame\ngeneration. Our key innovation, Complete Teacher Forcing (CTF), conditions\nmasked frames on complete observation frames rather than masked ones (namely\nMasked Teacher Forcing, MTF), enabling a smooth transition from token-level\n(patch-level) to frame-level autoregressive generation. CTF significantly\noutperforms MTF, achieving a +23% improvement in FVD scores on first-frame\nconditioned video prediction. To address issues like exposure bias, we employ\ntargeted training strategies, setting a new benchmark in autoregressive video\ngeneration. Experiments show that MAGI can generate long, coherent video\nsequences exceeding 100 frames, even when trained on as few as 16 frames,\nhighlighting its potential for scalable, high-quality video generation.\n", "link": "http://arxiv.org/abs/2501.12389v1", "date": "2025-01-21", "relevancy": 2.5183, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.676}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6013}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20Teacher%20Forcing%20for%20Masked%20Autoregressive%20Video%20Generation&body=Title%3A%20Taming%20Teacher%20Forcing%20for%20Masked%20Autoregressive%20Video%20Generation%0AAuthor%3A%20Deyu%20Zhou%20and%20Quan%20Sun%20and%20Yuang%20Peng%20and%20Kun%20Yan%20and%20Runpei%20Dong%20and%20Duomin%20Wang%20and%20Zheng%20Ge%20and%20Nan%20Duan%20and%20Xiangyu%20Zhang%20and%20Lionel%20M.%20Ni%20and%20Heung-Yeung%20Shum%0AAbstract%3A%20%20%20We%20introduce%20MAGI%2C%20a%20hybrid%20video%20generation%20framework%20that%20combines%20masked%0Amodeling%20for%20intra-frame%20generation%20with%20causal%20modeling%20for%20next-frame%0Ageneration.%20Our%20key%20innovation%2C%20Complete%20Teacher%20Forcing%20%28CTF%29%2C%20conditions%0Amasked%20frames%20on%20complete%20observation%20frames%20rather%20than%20masked%20ones%20%28namely%0AMasked%20Teacher%20Forcing%2C%20MTF%29%2C%20enabling%20a%20smooth%20transition%20from%20token-level%0A%28patch-level%29%20to%20frame-level%20autoregressive%20generation.%20CTF%20significantly%0Aoutperforms%20MTF%2C%20achieving%20a%20%2B23%25%20improvement%20in%20FVD%20scores%20on%20first-frame%0Aconditioned%20video%20prediction.%20To%20address%20issues%20like%20exposure%20bias%2C%20we%20employ%0Atargeted%20training%20strategies%2C%20setting%20a%20new%20benchmark%20in%20autoregressive%20video%0Ageneration.%20Experiments%20show%20that%20MAGI%20can%20generate%20long%2C%20coherent%20video%0Asequences%20exceeding%20100%20frames%2C%20even%20when%20trained%20on%20as%20few%20as%2016%20frames%2C%0Ahighlighting%20its%20potential%20for%20scalable%2C%20high-quality%20video%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520Teacher%2520Forcing%2520for%2520Masked%2520Autoregressive%2520Video%2520Generation%26entry.906535625%3DDeyu%2520Zhou%2520and%2520Quan%2520Sun%2520and%2520Yuang%2520Peng%2520and%2520Kun%2520Yan%2520and%2520Runpei%2520Dong%2520and%2520Duomin%2520Wang%2520and%2520Zheng%2520Ge%2520and%2520Nan%2520Duan%2520and%2520Xiangyu%2520Zhang%2520and%2520Lionel%2520M.%2520Ni%2520and%2520Heung-Yeung%2520Shum%26entry.1292438233%3D%2520%2520We%2520introduce%2520MAGI%252C%2520a%2520hybrid%2520video%2520generation%2520framework%2520that%2520combines%2520masked%250Amodeling%2520for%2520intra-frame%2520generation%2520with%2520causal%2520modeling%2520for%2520next-frame%250Ageneration.%2520Our%2520key%2520innovation%252C%2520Complete%2520Teacher%2520Forcing%2520%2528CTF%2529%252C%2520conditions%250Amasked%2520frames%2520on%2520complete%2520observation%2520frames%2520rather%2520than%2520masked%2520ones%2520%2528namely%250AMasked%2520Teacher%2520Forcing%252C%2520MTF%2529%252C%2520enabling%2520a%2520smooth%2520transition%2520from%2520token-level%250A%2528patch-level%2529%2520to%2520frame-level%2520autoregressive%2520generation.%2520CTF%2520significantly%250Aoutperforms%2520MTF%252C%2520achieving%2520a%2520%252B23%2525%2520improvement%2520in%2520FVD%2520scores%2520on%2520first-frame%250Aconditioned%2520video%2520prediction.%2520To%2520address%2520issues%2520like%2520exposure%2520bias%252C%2520we%2520employ%250Atargeted%2520training%2520strategies%252C%2520setting%2520a%2520new%2520benchmark%2520in%2520autoregressive%2520video%250Ageneration.%2520Experiments%2520show%2520that%2520MAGI%2520can%2520generate%2520long%252C%2520coherent%2520video%250Asequences%2520exceeding%2520100%2520frames%252C%2520even%2520when%2520trained%2520on%2520as%2520few%2520as%252016%2520frames%252C%250Ahighlighting%2520its%2520potential%2520for%2520scalable%252C%2520high-quality%2520video%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Teacher%20Forcing%20for%20Masked%20Autoregressive%20Video%20Generation&entry.906535625=Deyu%20Zhou%20and%20Quan%20Sun%20and%20Yuang%20Peng%20and%20Kun%20Yan%20and%20Runpei%20Dong%20and%20Duomin%20Wang%20and%20Zheng%20Ge%20and%20Nan%20Duan%20and%20Xiangyu%20Zhang%20and%20Lionel%20M.%20Ni%20and%20Heung-Yeung%20Shum&entry.1292438233=%20%20We%20introduce%20MAGI%2C%20a%20hybrid%20video%20generation%20framework%20that%20combines%20masked%0Amodeling%20for%20intra-frame%20generation%20with%20causal%20modeling%20for%20next-frame%0Ageneration.%20Our%20key%20innovation%2C%20Complete%20Teacher%20Forcing%20%28CTF%29%2C%20conditions%0Amasked%20frames%20on%20complete%20observation%20frames%20rather%20than%20masked%20ones%20%28namely%0AMasked%20Teacher%20Forcing%2C%20MTF%29%2C%20enabling%20a%20smooth%20transition%20from%20token-level%0A%28patch-level%29%20to%20frame-level%20autoregressive%20generation.%20CTF%20significantly%0Aoutperforms%20MTF%2C%20achieving%20a%20%2B23%25%20improvement%20in%20FVD%20scores%20on%20first-frame%0Aconditioned%20video%20prediction.%20To%20address%20issues%20like%20exposure%20bias%2C%20we%20employ%0Atargeted%20training%20strategies%2C%20setting%20a%20new%20benchmark%20in%20autoregressive%20video%0Ageneration.%20Experiments%20show%20that%20MAGI%20can%20generate%20long%2C%20coherent%20video%0Asequences%20exceeding%20100%20frames%2C%20even%20when%20trained%20on%20as%20few%20as%2016%20frames%2C%0Ahighlighting%20its%20potential%20for%20scalable%2C%20high-quality%20video%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12389v1&entry.124074799=Read"},
{"title": "Co-Paced Learning Strategy Based on Confidence for Flying Bird Object\n  Detection Model Training", "author": "Zi-Wei Sun and Ze-Xi Hua and Heng-Chao Li and Yan Li", "abstract": "  To mitigate the adverse effects of hard samples on the training of the Flying\nBird Object Detection (FBOD) model for surveillance videos, we propose a\nCo-Paced Learning Based on Confidence (CPL-BC) strategy and apply this strategy\nto the training process of the FBOD model. This strategy involves maintaining\ntwo models with identical structures but different initial parameter\nconfigurations, which collaborate with each other to select easy samples with\nprediction confidence exceeding a set threshold for training. As training\nprogresses, the strategy gradually lowers the threshold, allowing more samples\nto participate, enhancing the model's ability to recognize objects from easy to\nhard. Before applying the CPL-BC strategy to train the FBOD models, we\ninitially trained the two FBOD models to equip them with the capability to\nassess the difficulty level of flying bird object samples. Experimental results\non two different datasets of flying bird objects in surveillance videos\ndemonstrate that, compared to other model learning strategies, CPL-BC\nsignificantly improves detection accuracy, verifying the effectiveness and\nadvancement of this method.\n", "link": "http://arxiv.org/abs/2501.12071v1", "date": "2025-01-21", "relevancy": 2.5095, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.514}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5017}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-Paced%20Learning%20Strategy%20Based%20on%20Confidence%20for%20Flying%20Bird%20Object%0A%20%20Detection%20Model%20Training&body=Title%3A%20Co-Paced%20Learning%20Strategy%20Based%20on%20Confidence%20for%20Flying%20Bird%20Object%0A%20%20Detection%20Model%20Training%0AAuthor%3A%20Zi-Wei%20Sun%20and%20Ze-Xi%20Hua%20and%20Heng-Chao%20Li%20and%20Yan%20Li%0AAbstract%3A%20%20%20To%20mitigate%20the%20adverse%20effects%20of%20hard%20samples%20on%20the%20training%20of%20the%20Flying%0ABird%20Object%20Detection%20%28FBOD%29%20model%20for%20surveillance%20videos%2C%20we%20propose%20a%0ACo-Paced%20Learning%20Based%20on%20Confidence%20%28CPL-BC%29%20strategy%20and%20apply%20this%20strategy%0Ato%20the%20training%20process%20of%20the%20FBOD%20model.%20This%20strategy%20involves%20maintaining%0Atwo%20models%20with%20identical%20structures%20but%20different%20initial%20parameter%0Aconfigurations%2C%20which%20collaborate%20with%20each%20other%20to%20select%20easy%20samples%20with%0Aprediction%20confidence%20exceeding%20a%20set%20threshold%20for%20training.%20As%20training%0Aprogresses%2C%20the%20strategy%20gradually%20lowers%20the%20threshold%2C%20allowing%20more%20samples%0Ato%20participate%2C%20enhancing%20the%20model%27s%20ability%20to%20recognize%20objects%20from%20easy%20to%0Ahard.%20Before%20applying%20the%20CPL-BC%20strategy%20to%20train%20the%20FBOD%20models%2C%20we%0Ainitially%20trained%20the%20two%20FBOD%20models%20to%20equip%20them%20with%20the%20capability%20to%0Aassess%20the%20difficulty%20level%20of%20flying%20bird%20object%20samples.%20Experimental%20results%0Aon%20two%20different%20datasets%20of%20flying%20bird%20objects%20in%20surveillance%20videos%0Ademonstrate%20that%2C%20compared%20to%20other%20model%20learning%20strategies%2C%20CPL-BC%0Asignificantly%20improves%20detection%20accuracy%2C%20verifying%20the%20effectiveness%20and%0Aadvancement%20of%20this%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-Paced%2520Learning%2520Strategy%2520Based%2520on%2520Confidence%2520for%2520Flying%2520Bird%2520Object%250A%2520%2520Detection%2520Model%2520Training%26entry.906535625%3DZi-Wei%2520Sun%2520and%2520Ze-Xi%2520Hua%2520and%2520Heng-Chao%2520Li%2520and%2520Yan%2520Li%26entry.1292438233%3D%2520%2520To%2520mitigate%2520the%2520adverse%2520effects%2520of%2520hard%2520samples%2520on%2520the%2520training%2520of%2520the%2520Flying%250ABird%2520Object%2520Detection%2520%2528FBOD%2529%2520model%2520for%2520surveillance%2520videos%252C%2520we%2520propose%2520a%250ACo-Paced%2520Learning%2520Based%2520on%2520Confidence%2520%2528CPL-BC%2529%2520strategy%2520and%2520apply%2520this%2520strategy%250Ato%2520the%2520training%2520process%2520of%2520the%2520FBOD%2520model.%2520This%2520strategy%2520involves%2520maintaining%250Atwo%2520models%2520with%2520identical%2520structures%2520but%2520different%2520initial%2520parameter%250Aconfigurations%252C%2520which%2520collaborate%2520with%2520each%2520other%2520to%2520select%2520easy%2520samples%2520with%250Aprediction%2520confidence%2520exceeding%2520a%2520set%2520threshold%2520for%2520training.%2520As%2520training%250Aprogresses%252C%2520the%2520strategy%2520gradually%2520lowers%2520the%2520threshold%252C%2520allowing%2520more%2520samples%250Ato%2520participate%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520recognize%2520objects%2520from%2520easy%2520to%250Ahard.%2520Before%2520applying%2520the%2520CPL-BC%2520strategy%2520to%2520train%2520the%2520FBOD%2520models%252C%2520we%250Ainitially%2520trained%2520the%2520two%2520FBOD%2520models%2520to%2520equip%2520them%2520with%2520the%2520capability%2520to%250Aassess%2520the%2520difficulty%2520level%2520of%2520flying%2520bird%2520object%2520samples.%2520Experimental%2520results%250Aon%2520two%2520different%2520datasets%2520of%2520flying%2520bird%2520objects%2520in%2520surveillance%2520videos%250Ademonstrate%2520that%252C%2520compared%2520to%2520other%2520model%2520learning%2520strategies%252C%2520CPL-BC%250Asignificantly%2520improves%2520detection%2520accuracy%252C%2520verifying%2520the%2520effectiveness%2520and%250Aadvancement%2520of%2520this%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Paced%20Learning%20Strategy%20Based%20on%20Confidence%20for%20Flying%20Bird%20Object%0A%20%20Detection%20Model%20Training&entry.906535625=Zi-Wei%20Sun%20and%20Ze-Xi%20Hua%20and%20Heng-Chao%20Li%20and%20Yan%20Li&entry.1292438233=%20%20To%20mitigate%20the%20adverse%20effects%20of%20hard%20samples%20on%20the%20training%20of%20the%20Flying%0ABird%20Object%20Detection%20%28FBOD%29%20model%20for%20surveillance%20videos%2C%20we%20propose%20a%0ACo-Paced%20Learning%20Based%20on%20Confidence%20%28CPL-BC%29%20strategy%20and%20apply%20this%20strategy%0Ato%20the%20training%20process%20of%20the%20FBOD%20model.%20This%20strategy%20involves%20maintaining%0Atwo%20models%20with%20identical%20structures%20but%20different%20initial%20parameter%0Aconfigurations%2C%20which%20collaborate%20with%20each%20other%20to%20select%20easy%20samples%20with%0Aprediction%20confidence%20exceeding%20a%20set%20threshold%20for%20training.%20As%20training%0Aprogresses%2C%20the%20strategy%20gradually%20lowers%20the%20threshold%2C%20allowing%20more%20samples%0Ato%20participate%2C%20enhancing%20the%20model%27s%20ability%20to%20recognize%20objects%20from%20easy%20to%0Ahard.%20Before%20applying%20the%20CPL-BC%20strategy%20to%20train%20the%20FBOD%20models%2C%20we%0Ainitially%20trained%20the%20two%20FBOD%20models%20to%20equip%20them%20with%20the%20capability%20to%0Aassess%20the%20difficulty%20level%20of%20flying%20bird%20object%20samples.%20Experimental%20results%0Aon%20two%20different%20datasets%20of%20flying%20bird%20objects%20in%20surveillance%20videos%0Ademonstrate%20that%2C%20compared%20to%20other%20model%20learning%20strategies%2C%20CPL-BC%0Asignificantly%20improves%20detection%20accuracy%2C%20verifying%20the%20effectiveness%20and%0Aadvancement%20of%20this%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12071v1&entry.124074799=Read"},
{"title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding", "author": "Yilun Zhao and Lujing Xie and Haowei Zhang and Guo Gan and Yitao Long and Zhiyuan Hu and Tongyan Hu and Weiyuan Chen and Chuhan Li and Junyang Song and Zhijian Xu and Chengye Wang and Weifeng Pan and Ziyao Shangguan and Xiangru Tang and Zhenwen Liang and Yixin Liu and Chen Zhao and Arman Cohan", "abstract": "  We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark\nfor evaluating foundation models in video understanding. MMVU includes 3,000\nexpert-annotated questions spanning 27 subjects across four core disciplines:\nScience, Healthcare, Humanities & Social Sciences, and Engineering. Compared to\nprior benchmarks, MMVU features three key advancements. First, it challenges\nmodels to apply domain-specific knowledge and perform expert-level reasoning to\nanalyze specialized-domain videos, moving beyond the basic visual perception\ntypically assessed in current video benchmarks. Second, each example is\nannotated by human experts from scratch. We implement strict data quality\ncontrols to ensure the high quality of the dataset. Finally, each example is\nenriched with expert-annotated reasoning rationals and relevant domain\nknowledge, facilitating in-depth analysis. We conduct an extensive evaluation\nof 32 frontier multimodal foundation models on MMVU. The latest\nSystem-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest\nperformance among the tested models. However, they still fall short of matching\nhuman expertise. Through in-depth error analyses and case studies, we offer\nactionable insights for future advancements in expert-level,\nknowledge-intensive video understanding for specialized domains.\n", "link": "http://arxiv.org/abs/2501.12380v1", "date": "2025-01-21", "relevancy": 2.4995, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6362}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6362}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMVU%3A%20Measuring%20Expert-Level%20Multi-Discipline%20Video%20Understanding&body=Title%3A%20MMVU%3A%20Measuring%20Expert-Level%20Multi-Discipline%20Video%20Understanding%0AAuthor%3A%20Yilun%20Zhao%20and%20Lujing%20Xie%20and%20Haowei%20Zhang%20and%20Guo%20Gan%20and%20Yitao%20Long%20and%20Zhiyuan%20Hu%20and%20Tongyan%20Hu%20and%20Weiyuan%20Chen%20and%20Chuhan%20Li%20and%20Junyang%20Song%20and%20Zhijian%20Xu%20and%20Chengye%20Wang%20and%20Weifeng%20Pan%20and%20Ziyao%20Shangguan%20and%20Xiangru%20Tang%20and%20Zhenwen%20Liang%20and%20Yixin%20Liu%20and%20Chen%20Zhao%20and%20Arman%20Cohan%0AAbstract%3A%20%20%20We%20introduce%20MMVU%2C%20a%20comprehensive%20expert-level%2C%20multi-discipline%20benchmark%0Afor%20evaluating%20foundation%20models%20in%20video%20understanding.%20MMVU%20includes%203%2C000%0Aexpert-annotated%20questions%20spanning%2027%20subjects%20across%20four%20core%20disciplines%3A%0AScience%2C%20Healthcare%2C%20Humanities%20%26%20Social%20Sciences%2C%20and%20Engineering.%20Compared%20to%0Aprior%20benchmarks%2C%20MMVU%20features%20three%20key%20advancements.%20First%2C%20it%20challenges%0Amodels%20to%20apply%20domain-specific%20knowledge%20and%20perform%20expert-level%20reasoning%20to%0Aanalyze%20specialized-domain%20videos%2C%20moving%20beyond%20the%20basic%20visual%20perception%0Atypically%20assessed%20in%20current%20video%20benchmarks.%20Second%2C%20each%20example%20is%0Aannotated%20by%20human%20experts%20from%20scratch.%20We%20implement%20strict%20data%20quality%0Acontrols%20to%20ensure%20the%20high%20quality%20of%20the%20dataset.%20Finally%2C%20each%20example%20is%0Aenriched%20with%20expert-annotated%20reasoning%20rationals%20and%20relevant%20domain%0Aknowledge%2C%20facilitating%20in-depth%20analysis.%20We%20conduct%20an%20extensive%20evaluation%0Aof%2032%20frontier%20multimodal%20foundation%20models%20on%20MMVU.%20The%20latest%0ASystem-2-capable%20models%2C%20o1%20and%20Gemini%202.0%20Flash%20Thinking%2C%20achieve%20the%20highest%0Aperformance%20among%20the%20tested%20models.%20However%2C%20they%20still%20fall%20short%20of%20matching%0Ahuman%20expertise.%20Through%20in-depth%20error%20analyses%20and%20case%20studies%2C%20we%20offer%0Aactionable%20insights%20for%20future%20advancements%20in%20expert-level%2C%0Aknowledge-intensive%20video%20understanding%20for%20specialized%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMVU%253A%2520Measuring%2520Expert-Level%2520Multi-Discipline%2520Video%2520Understanding%26entry.906535625%3DYilun%2520Zhao%2520and%2520Lujing%2520Xie%2520and%2520Haowei%2520Zhang%2520and%2520Guo%2520Gan%2520and%2520Yitao%2520Long%2520and%2520Zhiyuan%2520Hu%2520and%2520Tongyan%2520Hu%2520and%2520Weiyuan%2520Chen%2520and%2520Chuhan%2520Li%2520and%2520Junyang%2520Song%2520and%2520Zhijian%2520Xu%2520and%2520Chengye%2520Wang%2520and%2520Weifeng%2520Pan%2520and%2520Ziyao%2520Shangguan%2520and%2520Xiangru%2520Tang%2520and%2520Zhenwen%2520Liang%2520and%2520Yixin%2520Liu%2520and%2520Chen%2520Zhao%2520and%2520Arman%2520Cohan%26entry.1292438233%3D%2520%2520We%2520introduce%2520MMVU%252C%2520a%2520comprehensive%2520expert-level%252C%2520multi-discipline%2520benchmark%250Afor%2520evaluating%2520foundation%2520models%2520in%2520video%2520understanding.%2520MMVU%2520includes%25203%252C000%250Aexpert-annotated%2520questions%2520spanning%252027%2520subjects%2520across%2520four%2520core%2520disciplines%253A%250AScience%252C%2520Healthcare%252C%2520Humanities%2520%2526%2520Social%2520Sciences%252C%2520and%2520Engineering.%2520Compared%2520to%250Aprior%2520benchmarks%252C%2520MMVU%2520features%2520three%2520key%2520advancements.%2520First%252C%2520it%2520challenges%250Amodels%2520to%2520apply%2520domain-specific%2520knowledge%2520and%2520perform%2520expert-level%2520reasoning%2520to%250Aanalyze%2520specialized-domain%2520videos%252C%2520moving%2520beyond%2520the%2520basic%2520visual%2520perception%250Atypically%2520assessed%2520in%2520current%2520video%2520benchmarks.%2520Second%252C%2520each%2520example%2520is%250Aannotated%2520by%2520human%2520experts%2520from%2520scratch.%2520We%2520implement%2520strict%2520data%2520quality%250Acontrols%2520to%2520ensure%2520the%2520high%2520quality%2520of%2520the%2520dataset.%2520Finally%252C%2520each%2520example%2520is%250Aenriched%2520with%2520expert-annotated%2520reasoning%2520rationals%2520and%2520relevant%2520domain%250Aknowledge%252C%2520facilitating%2520in-depth%2520analysis.%2520We%2520conduct%2520an%2520extensive%2520evaluation%250Aof%252032%2520frontier%2520multimodal%2520foundation%2520models%2520on%2520MMVU.%2520The%2520latest%250ASystem-2-capable%2520models%252C%2520o1%2520and%2520Gemini%25202.0%2520Flash%2520Thinking%252C%2520achieve%2520the%2520highest%250Aperformance%2520among%2520the%2520tested%2520models.%2520However%252C%2520they%2520still%2520fall%2520short%2520of%2520matching%250Ahuman%2520expertise.%2520Through%2520in-depth%2520error%2520analyses%2520and%2520case%2520studies%252C%2520we%2520offer%250Aactionable%2520insights%2520for%2520future%2520advancements%2520in%2520expert-level%252C%250Aknowledge-intensive%2520video%2520understanding%2520for%2520specialized%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMVU%3A%20Measuring%20Expert-Level%20Multi-Discipline%20Video%20Understanding&entry.906535625=Yilun%20Zhao%20and%20Lujing%20Xie%20and%20Haowei%20Zhang%20and%20Guo%20Gan%20and%20Yitao%20Long%20and%20Zhiyuan%20Hu%20and%20Tongyan%20Hu%20and%20Weiyuan%20Chen%20and%20Chuhan%20Li%20and%20Junyang%20Song%20and%20Zhijian%20Xu%20and%20Chengye%20Wang%20and%20Weifeng%20Pan%20and%20Ziyao%20Shangguan%20and%20Xiangru%20Tang%20and%20Zhenwen%20Liang%20and%20Yixin%20Liu%20and%20Chen%20Zhao%20and%20Arman%20Cohan&entry.1292438233=%20%20We%20introduce%20MMVU%2C%20a%20comprehensive%20expert-level%2C%20multi-discipline%20benchmark%0Afor%20evaluating%20foundation%20models%20in%20video%20understanding.%20MMVU%20includes%203%2C000%0Aexpert-annotated%20questions%20spanning%2027%20subjects%20across%20four%20core%20disciplines%3A%0AScience%2C%20Healthcare%2C%20Humanities%20%26%20Social%20Sciences%2C%20and%20Engineering.%20Compared%20to%0Aprior%20benchmarks%2C%20MMVU%20features%20three%20key%20advancements.%20First%2C%20it%20challenges%0Amodels%20to%20apply%20domain-specific%20knowledge%20and%20perform%20expert-level%20reasoning%20to%0Aanalyze%20specialized-domain%20videos%2C%20moving%20beyond%20the%20basic%20visual%20perception%0Atypically%20assessed%20in%20current%20video%20benchmarks.%20Second%2C%20each%20example%20is%0Aannotated%20by%20human%20experts%20from%20scratch.%20We%20implement%20strict%20data%20quality%0Acontrols%20to%20ensure%20the%20high%20quality%20of%20the%20dataset.%20Finally%2C%20each%20example%20is%0Aenriched%20with%20expert-annotated%20reasoning%20rationals%20and%20relevant%20domain%0Aknowledge%2C%20facilitating%20in-depth%20analysis.%20We%20conduct%20an%20extensive%20evaluation%0Aof%2032%20frontier%20multimodal%20foundation%20models%20on%20MMVU.%20The%20latest%0ASystem-2-capable%20models%2C%20o1%20and%20Gemini%202.0%20Flash%20Thinking%2C%20achieve%20the%20highest%0Aperformance%20among%20the%20tested%20models.%20However%2C%20they%20still%20fall%20short%20of%20matching%0Ahuman%20expertise.%20Through%20in-depth%20error%20analyses%20and%20case%20studies%2C%20we%20offer%0Aactionable%20insights%20for%20future%20advancements%20in%20expert-level%2C%0Aknowledge-intensive%20video%20understanding%20for%20specialized%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12380v1&entry.124074799=Read"},
{"title": "Meta-Sparsity: Learning Optimal Sparse Structures in Multi-task Networks\n  through Meta-learning", "author": "Richa Upadhyay and Ronald Phlypo and Rajkumar Saini and Marcus Liwicki", "abstract": "  This paper presents meta-sparsity, a framework for learning model sparsity,\nbasically learning the parameter that controls the degree of sparsity, that\nallows deep neural networks (DNNs) to inherently generate optimal sparse shared\nstructures in multi-task learning (MTL) setting. This proposed approach enables\nthe dynamic learning of sparsity patterns across a variety of tasks, unlike\ntraditional sparsity methods that rely heavily on manual hyperparameter tuning.\nInspired by Model Agnostic Meta-Learning (MAML), the emphasis is on learning\nshared and optimally sparse parameters in multi-task scenarios by implementing\na penalty-based, channel-wise structured sparsity during the meta-training\nphase. This method improves the model's efficacy by removing unnecessary\nparameters and enhances its ability to handle both seen and previously unseen\ntasks. The effectiveness of meta-sparsity is rigorously evaluated by extensive\nexperiments on two datasets, NYU-v2 and CelebAMask-HQ, covering a broad\nspectrum of tasks ranging from pixel-level to image-level predictions. The\nresults show that the proposed approach performs well across many tasks,\nindicating its potential as a versatile tool for creating efficient and\nadaptable sparse neural networks. This work, therefore, presents an approach\ntowards learning sparsity, contributing to the efforts in the field of sparse\nneural networks and suggesting new directions for research towards parsimonious\nmodels.\n", "link": "http://arxiv.org/abs/2501.12115v1", "date": "2025-01-21", "relevancy": 2.487, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5203}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Sparsity%3A%20Learning%20Optimal%20Sparse%20Structures%20in%20Multi-task%20Networks%0A%20%20through%20Meta-learning&body=Title%3A%20Meta-Sparsity%3A%20Learning%20Optimal%20Sparse%20Structures%20in%20Multi-task%20Networks%0A%20%20through%20Meta-learning%0AAuthor%3A%20Richa%20Upadhyay%20and%20Ronald%20Phlypo%20and%20Rajkumar%20Saini%20and%20Marcus%20Liwicki%0AAbstract%3A%20%20%20This%20paper%20presents%20meta-sparsity%2C%20a%20framework%20for%20learning%20model%20sparsity%2C%0Abasically%20learning%20the%20parameter%20that%20controls%20the%20degree%20of%20sparsity%2C%20that%0Aallows%20deep%20neural%20networks%20%28DNNs%29%20to%20inherently%20generate%20optimal%20sparse%20shared%0Astructures%20in%20multi-task%20learning%20%28MTL%29%20setting.%20This%20proposed%20approach%20enables%0Athe%20dynamic%20learning%20of%20sparsity%20patterns%20across%20a%20variety%20of%20tasks%2C%20unlike%0Atraditional%20sparsity%20methods%20that%20rely%20heavily%20on%20manual%20hyperparameter%20tuning.%0AInspired%20by%20Model%20Agnostic%20Meta-Learning%20%28MAML%29%2C%20the%20emphasis%20is%20on%20learning%0Ashared%20and%20optimally%20sparse%20parameters%20in%20multi-task%20scenarios%20by%20implementing%0Aa%20penalty-based%2C%20channel-wise%20structured%20sparsity%20during%20the%20meta-training%0Aphase.%20This%20method%20improves%20the%20model%27s%20efficacy%20by%20removing%20unnecessary%0Aparameters%20and%20enhances%20its%20ability%20to%20handle%20both%20seen%20and%20previously%20unseen%0Atasks.%20The%20effectiveness%20of%20meta-sparsity%20is%20rigorously%20evaluated%20by%20extensive%0Aexperiments%20on%20two%20datasets%2C%20NYU-v2%20and%20CelebAMask-HQ%2C%20covering%20a%20broad%0Aspectrum%20of%20tasks%20ranging%20from%20pixel-level%20to%20image-level%20predictions.%20The%0Aresults%20show%20that%20the%20proposed%20approach%20performs%20well%20across%20many%20tasks%2C%0Aindicating%20its%20potential%20as%20a%20versatile%20tool%20for%20creating%20efficient%20and%0Aadaptable%20sparse%20neural%20networks.%20This%20work%2C%20therefore%2C%20presents%20an%20approach%0Atowards%20learning%20sparsity%2C%20contributing%20to%20the%20efforts%20in%20the%20field%20of%20sparse%0Aneural%20networks%20and%20suggesting%20new%20directions%20for%20research%20towards%20parsimonious%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Sparsity%253A%2520Learning%2520Optimal%2520Sparse%2520Structures%2520in%2520Multi-task%2520Networks%250A%2520%2520through%2520Meta-learning%26entry.906535625%3DRicha%2520Upadhyay%2520and%2520Ronald%2520Phlypo%2520and%2520Rajkumar%2520Saini%2520and%2520Marcus%2520Liwicki%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520meta-sparsity%252C%2520a%2520framework%2520for%2520learning%2520model%2520sparsity%252C%250Abasically%2520learning%2520the%2520parameter%2520that%2520controls%2520the%2520degree%2520of%2520sparsity%252C%2520that%250Aallows%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520to%2520inherently%2520generate%2520optimal%2520sparse%2520shared%250Astructures%2520in%2520multi-task%2520learning%2520%2528MTL%2529%2520setting.%2520This%2520proposed%2520approach%2520enables%250Athe%2520dynamic%2520learning%2520of%2520sparsity%2520patterns%2520across%2520a%2520variety%2520of%2520tasks%252C%2520unlike%250Atraditional%2520sparsity%2520methods%2520that%2520rely%2520heavily%2520on%2520manual%2520hyperparameter%2520tuning.%250AInspired%2520by%2520Model%2520Agnostic%2520Meta-Learning%2520%2528MAML%2529%252C%2520the%2520emphasis%2520is%2520on%2520learning%250Ashared%2520and%2520optimally%2520sparse%2520parameters%2520in%2520multi-task%2520scenarios%2520by%2520implementing%250Aa%2520penalty-based%252C%2520channel-wise%2520structured%2520sparsity%2520during%2520the%2520meta-training%250Aphase.%2520This%2520method%2520improves%2520the%2520model%2527s%2520efficacy%2520by%2520removing%2520unnecessary%250Aparameters%2520and%2520enhances%2520its%2520ability%2520to%2520handle%2520both%2520seen%2520and%2520previously%2520unseen%250Atasks.%2520The%2520effectiveness%2520of%2520meta-sparsity%2520is%2520rigorously%2520evaluated%2520by%2520extensive%250Aexperiments%2520on%2520two%2520datasets%252C%2520NYU-v2%2520and%2520CelebAMask-HQ%252C%2520covering%2520a%2520broad%250Aspectrum%2520of%2520tasks%2520ranging%2520from%2520pixel-level%2520to%2520image-level%2520predictions.%2520The%250Aresults%2520show%2520that%2520the%2520proposed%2520approach%2520performs%2520well%2520across%2520many%2520tasks%252C%250Aindicating%2520its%2520potential%2520as%2520a%2520versatile%2520tool%2520for%2520creating%2520efficient%2520and%250Aadaptable%2520sparse%2520neural%2520networks.%2520This%2520work%252C%2520therefore%252C%2520presents%2520an%2520approach%250Atowards%2520learning%2520sparsity%252C%2520contributing%2520to%2520the%2520efforts%2520in%2520the%2520field%2520of%2520sparse%250Aneural%2520networks%2520and%2520suggesting%2520new%2520directions%2520for%2520research%2520towards%2520parsimonious%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Sparsity%3A%20Learning%20Optimal%20Sparse%20Structures%20in%20Multi-task%20Networks%0A%20%20through%20Meta-learning&entry.906535625=Richa%20Upadhyay%20and%20Ronald%20Phlypo%20and%20Rajkumar%20Saini%20and%20Marcus%20Liwicki&entry.1292438233=%20%20This%20paper%20presents%20meta-sparsity%2C%20a%20framework%20for%20learning%20model%20sparsity%2C%0Abasically%20learning%20the%20parameter%20that%20controls%20the%20degree%20of%20sparsity%2C%20that%0Aallows%20deep%20neural%20networks%20%28DNNs%29%20to%20inherently%20generate%20optimal%20sparse%20shared%0Astructures%20in%20multi-task%20learning%20%28MTL%29%20setting.%20This%20proposed%20approach%20enables%0Athe%20dynamic%20learning%20of%20sparsity%20patterns%20across%20a%20variety%20of%20tasks%2C%20unlike%0Atraditional%20sparsity%20methods%20that%20rely%20heavily%20on%20manual%20hyperparameter%20tuning.%0AInspired%20by%20Model%20Agnostic%20Meta-Learning%20%28MAML%29%2C%20the%20emphasis%20is%20on%20learning%0Ashared%20and%20optimally%20sparse%20parameters%20in%20multi-task%20scenarios%20by%20implementing%0Aa%20penalty-based%2C%20channel-wise%20structured%20sparsity%20during%20the%20meta-training%0Aphase.%20This%20method%20improves%20the%20model%27s%20efficacy%20by%20removing%20unnecessary%0Aparameters%20and%20enhances%20its%20ability%20to%20handle%20both%20seen%20and%20previously%20unseen%0Atasks.%20The%20effectiveness%20of%20meta-sparsity%20is%20rigorously%20evaluated%20by%20extensive%0Aexperiments%20on%20two%20datasets%2C%20NYU-v2%20and%20CelebAMask-HQ%2C%20covering%20a%20broad%0Aspectrum%20of%20tasks%20ranging%20from%20pixel-level%20to%20image-level%20predictions.%20The%0Aresults%20show%20that%20the%20proposed%20approach%20performs%20well%20across%20many%20tasks%2C%0Aindicating%20its%20potential%20as%20a%20versatile%20tool%20for%20creating%20efficient%20and%0Aadaptable%20sparse%20neural%20networks.%20This%20work%2C%20therefore%2C%20presents%20an%20approach%0Atowards%20learning%20sparsity%2C%20contributing%20to%20the%20efforts%20in%20the%20field%20of%20sparse%0Aneural%20networks%20and%20suggesting%20new%20directions%20for%20research%20towards%20parsimonious%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12115v1&entry.124074799=Read"},
{"title": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D\n  Assets Generation", "author": "Zibo Zhao and Zeqiang Lai and Qingxiang Lin and Yunfei Zhao and Haolin Liu and Shuhui Yang and Yifei Feng and Mingxin Yang and Sheng Zhang and Xianghui Yang and Huiwen Shi and Sicong Liu and Junta Wu and Yihang Lian and Fan Yang and Ruining Tang and Zebin He and Xinzhou Wang and Jian Liu and Xuhui Zuo and Zhuo Chen and Biwen Lei and Haohan Weng and Jing Xu and Yiling Zhu and Xinhai Liu and Lixin Xu and Changrong Hu and Tianyu Huang and Lifu Wang and Jihong Zhang and Meng Chen and Liang Dong and Yiwen Jia and Yulin Cai and Jiaao Yu and Yixuan Tang and Hao Zhang and Zheng Ye and Peng He and Runzhou Wu and Chao Zhang and Yonghao Tan and Jie Xiao and Yangyu Tao and Jianchen Zhu and Jinbao Xue and Kai Liu and Chongqing Zhao and Xinming Wu and Zhichao Hu and Lei Qin and Jianbing Peng and Zhan Li and Minghui Chen and Xipeng Zhang and Lin Niu and Paige Wang and Yingkai Wang and Haozhao Kuang and Zhongyi Fan and Xu Zheng and Weihao Zhuang and YingPing He and Tian Liu and Yong Yang and Di Wang and Yuhong Liu and Jie Jiang and Jingwei Huang and Chunchao Guo", "abstract": "  We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for\ngenerating high-resolution textured 3D assets. This system includes two\nfoundation components: a large-scale shape generation model -- Hunyuan3D-DiT,\nand a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape\ngenerative model, built on a scalable flow-based diffusion transformer, aims to\ncreate geometry that properly aligns with a given condition image, laying a\nsolid foundation for downstream applications. The texture synthesis model,\nbenefiting from strong geometric and diffusion priors, produces high-resolution\nand vibrant texture maps for either generated or hand-crafted meshes.\nFurthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production\nplatform that simplifies the re-creation process of 3D assets. It allows both\nprofessional and amateur users to manipulate or even animate their meshes\nefficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0\noutperforms previous state-of-the-art models, including the open-source models\nand closed-source models in geometry details, condition alignment, texture\nquality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps\nin the open-source 3D community for large-scale foundation generative models.\nThe code and pre-trained weights of our models are available at:\nhttps://github.com/Tencent/Hunyuan3D-2\n", "link": "http://arxiv.org/abs/2501.12202v1", "date": "2025-01-21", "relevancy": 2.473, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6193}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6193}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hunyuan3D%202.0%3A%20Scaling%20Diffusion%20Models%20for%20High%20Resolution%20Textured%203D%0A%20%20Assets%20Generation&body=Title%3A%20Hunyuan3D%202.0%3A%20Scaling%20Diffusion%20Models%20for%20High%20Resolution%20Textured%203D%0A%20%20Assets%20Generation%0AAuthor%3A%20Zibo%20Zhao%20and%20Zeqiang%20Lai%20and%20Qingxiang%20Lin%20and%20Yunfei%20Zhao%20and%20Haolin%20Liu%20and%20Shuhui%20Yang%20and%20Yifei%20Feng%20and%20Mingxin%20Yang%20and%20Sheng%20Zhang%20and%20Xianghui%20Yang%20and%20Huiwen%20Shi%20and%20Sicong%20Liu%20and%20Junta%20Wu%20and%20Yihang%20Lian%20and%20Fan%20Yang%20and%20Ruining%20Tang%20and%20Zebin%20He%20and%20Xinzhou%20Wang%20and%20Jian%20Liu%20and%20Xuhui%20Zuo%20and%20Zhuo%20Chen%20and%20Biwen%20Lei%20and%20Haohan%20Weng%20and%20Jing%20Xu%20and%20Yiling%20Zhu%20and%20Xinhai%20Liu%20and%20Lixin%20Xu%20and%20Changrong%20Hu%20and%20Tianyu%20Huang%20and%20Lifu%20Wang%20and%20Jihong%20Zhang%20and%20Meng%20Chen%20and%20Liang%20Dong%20and%20Yiwen%20Jia%20and%20Yulin%20Cai%20and%20Jiaao%20Yu%20and%20Yixuan%20Tang%20and%20Hao%20Zhang%20and%20Zheng%20Ye%20and%20Peng%20He%20and%20Runzhou%20Wu%20and%20Chao%20Zhang%20and%20Yonghao%20Tan%20and%20Jie%20Xiao%20and%20Yangyu%20Tao%20and%20Jianchen%20Zhu%20and%20Jinbao%20Xue%20and%20Kai%20Liu%20and%20Chongqing%20Zhao%20and%20Xinming%20Wu%20and%20Zhichao%20Hu%20and%20Lei%20Qin%20and%20Jianbing%20Peng%20and%20Zhan%20Li%20and%20Minghui%20Chen%20and%20Xipeng%20Zhang%20and%20Lin%20Niu%20and%20Paige%20Wang%20and%20Yingkai%20Wang%20and%20Haozhao%20Kuang%20and%20Zhongyi%20Fan%20and%20Xu%20Zheng%20and%20Weihao%20Zhuang%20and%20YingPing%20He%20and%20Tian%20Liu%20and%20Yong%20Yang%20and%20Di%20Wang%20and%20Yuhong%20Liu%20and%20Jie%20Jiang%20and%20Jingwei%20Huang%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20We%20present%20Hunyuan3D%202.0%2C%20an%20advanced%20large-scale%203D%20synthesis%20system%20for%0Agenerating%20high-resolution%20textured%203D%20assets.%20This%20system%20includes%20two%0Afoundation%20components%3A%20a%20large-scale%20shape%20generation%20model%20--%20Hunyuan3D-DiT%2C%0Aand%20a%20large-scale%20texture%20synthesis%20model%20--%20Hunyuan3D-Paint.%20The%20shape%0Agenerative%20model%2C%20built%20on%20a%20scalable%20flow-based%20diffusion%20transformer%2C%20aims%20to%0Acreate%20geometry%20that%20properly%20aligns%20with%20a%20given%20condition%20image%2C%20laying%20a%0Asolid%20foundation%20for%20downstream%20applications.%20The%20texture%20synthesis%20model%2C%0Abenefiting%20from%20strong%20geometric%20and%20diffusion%20priors%2C%20produces%20high-resolution%0Aand%20vibrant%20texture%20maps%20for%20either%20generated%20or%20hand-crafted%20meshes.%0AFurthermore%2C%20we%20build%20Hunyuan3D-Studio%20--%20a%20versatile%2C%20user-friendly%20production%0Aplatform%20that%20simplifies%20the%20re-creation%20process%20of%203D%20assets.%20It%20allows%20both%0Aprofessional%20and%20amateur%20users%20to%20manipulate%20or%20even%20animate%20their%20meshes%0Aefficiently.%20We%20systematically%20evaluate%20our%20models%2C%20showing%20that%20Hunyuan3D%202.0%0Aoutperforms%20previous%20state-of-the-art%20models%2C%20including%20the%20open-source%20models%0Aand%20closed-source%20models%20in%20geometry%20details%2C%20condition%20alignment%2C%20texture%0Aquality%2C%20and%20etc.%20Hunyuan3D%202.0%20is%20publicly%20released%20in%20order%20to%20fill%20the%20gaps%0Ain%20the%20open-source%203D%20community%20for%20large-scale%20foundation%20generative%20models.%0AThe%20code%20and%20pre-trained%20weights%20of%20our%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/Tencent/Hunyuan3D-2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunyuan3D%25202.0%253A%2520Scaling%2520Diffusion%2520Models%2520for%2520High%2520Resolution%2520Textured%25203D%250A%2520%2520Assets%2520Generation%26entry.906535625%3DZibo%2520Zhao%2520and%2520Zeqiang%2520Lai%2520and%2520Qingxiang%2520Lin%2520and%2520Yunfei%2520Zhao%2520and%2520Haolin%2520Liu%2520and%2520Shuhui%2520Yang%2520and%2520Yifei%2520Feng%2520and%2520Mingxin%2520Yang%2520and%2520Sheng%2520Zhang%2520and%2520Xianghui%2520Yang%2520and%2520Huiwen%2520Shi%2520and%2520Sicong%2520Liu%2520and%2520Junta%2520Wu%2520and%2520Yihang%2520Lian%2520and%2520Fan%2520Yang%2520and%2520Ruining%2520Tang%2520and%2520Zebin%2520He%2520and%2520Xinzhou%2520Wang%2520and%2520Jian%2520Liu%2520and%2520Xuhui%2520Zuo%2520and%2520Zhuo%2520Chen%2520and%2520Biwen%2520Lei%2520and%2520Haohan%2520Weng%2520and%2520Jing%2520Xu%2520and%2520Yiling%2520Zhu%2520and%2520Xinhai%2520Liu%2520and%2520Lixin%2520Xu%2520and%2520Changrong%2520Hu%2520and%2520Tianyu%2520Huang%2520and%2520Lifu%2520Wang%2520and%2520Jihong%2520Zhang%2520and%2520Meng%2520Chen%2520and%2520Liang%2520Dong%2520and%2520Yiwen%2520Jia%2520and%2520Yulin%2520Cai%2520and%2520Jiaao%2520Yu%2520and%2520Yixuan%2520Tang%2520and%2520Hao%2520Zhang%2520and%2520Zheng%2520Ye%2520and%2520Peng%2520He%2520and%2520Runzhou%2520Wu%2520and%2520Chao%2520Zhang%2520and%2520Yonghao%2520Tan%2520and%2520Jie%2520Xiao%2520and%2520Yangyu%2520Tao%2520and%2520Jianchen%2520Zhu%2520and%2520Jinbao%2520Xue%2520and%2520Kai%2520Liu%2520and%2520Chongqing%2520Zhao%2520and%2520Xinming%2520Wu%2520and%2520Zhichao%2520Hu%2520and%2520Lei%2520Qin%2520and%2520Jianbing%2520Peng%2520and%2520Zhan%2520Li%2520and%2520Minghui%2520Chen%2520and%2520Xipeng%2520Zhang%2520and%2520Lin%2520Niu%2520and%2520Paige%2520Wang%2520and%2520Yingkai%2520Wang%2520and%2520Haozhao%2520Kuang%2520and%2520Zhongyi%2520Fan%2520and%2520Xu%2520Zheng%2520and%2520Weihao%2520Zhuang%2520and%2520YingPing%2520He%2520and%2520Tian%2520Liu%2520and%2520Yong%2520Yang%2520and%2520Di%2520Wang%2520and%2520Yuhong%2520Liu%2520and%2520Jie%2520Jiang%2520and%2520Jingwei%2520Huang%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520We%2520present%2520Hunyuan3D%25202.0%252C%2520an%2520advanced%2520large-scale%25203D%2520synthesis%2520system%2520for%250Agenerating%2520high-resolution%2520textured%25203D%2520assets.%2520This%2520system%2520includes%2520two%250Afoundation%2520components%253A%2520a%2520large-scale%2520shape%2520generation%2520model%2520--%2520Hunyuan3D-DiT%252C%250Aand%2520a%2520large-scale%2520texture%2520synthesis%2520model%2520--%2520Hunyuan3D-Paint.%2520The%2520shape%250Agenerative%2520model%252C%2520built%2520on%2520a%2520scalable%2520flow-based%2520diffusion%2520transformer%252C%2520aims%2520to%250Acreate%2520geometry%2520that%2520properly%2520aligns%2520with%2520a%2520given%2520condition%2520image%252C%2520laying%2520a%250Asolid%2520foundation%2520for%2520downstream%2520applications.%2520The%2520texture%2520synthesis%2520model%252C%250Abenefiting%2520from%2520strong%2520geometric%2520and%2520diffusion%2520priors%252C%2520produces%2520high-resolution%250Aand%2520vibrant%2520texture%2520maps%2520for%2520either%2520generated%2520or%2520hand-crafted%2520meshes.%250AFurthermore%252C%2520we%2520build%2520Hunyuan3D-Studio%2520--%2520a%2520versatile%252C%2520user-friendly%2520production%250Aplatform%2520that%2520simplifies%2520the%2520re-creation%2520process%2520of%25203D%2520assets.%2520It%2520allows%2520both%250Aprofessional%2520and%2520amateur%2520users%2520to%2520manipulate%2520or%2520even%2520animate%2520their%2520meshes%250Aefficiently.%2520We%2520systematically%2520evaluate%2520our%2520models%252C%2520showing%2520that%2520Hunyuan3D%25202.0%250Aoutperforms%2520previous%2520state-of-the-art%2520models%252C%2520including%2520the%2520open-source%2520models%250Aand%2520closed-source%2520models%2520in%2520geometry%2520details%252C%2520condition%2520alignment%252C%2520texture%250Aquality%252C%2520and%2520etc.%2520Hunyuan3D%25202.0%2520is%2520publicly%2520released%2520in%2520order%2520to%2520fill%2520the%2520gaps%250Ain%2520the%2520open-source%25203D%2520community%2520for%2520large-scale%2520foundation%2520generative%2520models.%250AThe%2520code%2520and%2520pre-trained%2520weights%2520of%2520our%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/Tencent/Hunyuan3D-2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hunyuan3D%202.0%3A%20Scaling%20Diffusion%20Models%20for%20High%20Resolution%20Textured%203D%0A%20%20Assets%20Generation&entry.906535625=Zibo%20Zhao%20and%20Zeqiang%20Lai%20and%20Qingxiang%20Lin%20and%20Yunfei%20Zhao%20and%20Haolin%20Liu%20and%20Shuhui%20Yang%20and%20Yifei%20Feng%20and%20Mingxin%20Yang%20and%20Sheng%20Zhang%20and%20Xianghui%20Yang%20and%20Huiwen%20Shi%20and%20Sicong%20Liu%20and%20Junta%20Wu%20and%20Yihang%20Lian%20and%20Fan%20Yang%20and%20Ruining%20Tang%20and%20Zebin%20He%20and%20Xinzhou%20Wang%20and%20Jian%20Liu%20and%20Xuhui%20Zuo%20and%20Zhuo%20Chen%20and%20Biwen%20Lei%20and%20Haohan%20Weng%20and%20Jing%20Xu%20and%20Yiling%20Zhu%20and%20Xinhai%20Liu%20and%20Lixin%20Xu%20and%20Changrong%20Hu%20and%20Tianyu%20Huang%20and%20Lifu%20Wang%20and%20Jihong%20Zhang%20and%20Meng%20Chen%20and%20Liang%20Dong%20and%20Yiwen%20Jia%20and%20Yulin%20Cai%20and%20Jiaao%20Yu%20and%20Yixuan%20Tang%20and%20Hao%20Zhang%20and%20Zheng%20Ye%20and%20Peng%20He%20and%20Runzhou%20Wu%20and%20Chao%20Zhang%20and%20Yonghao%20Tan%20and%20Jie%20Xiao%20and%20Yangyu%20Tao%20and%20Jianchen%20Zhu%20and%20Jinbao%20Xue%20and%20Kai%20Liu%20and%20Chongqing%20Zhao%20and%20Xinming%20Wu%20and%20Zhichao%20Hu%20and%20Lei%20Qin%20and%20Jianbing%20Peng%20and%20Zhan%20Li%20and%20Minghui%20Chen%20and%20Xipeng%20Zhang%20and%20Lin%20Niu%20and%20Paige%20Wang%20and%20Yingkai%20Wang%20and%20Haozhao%20Kuang%20and%20Zhongyi%20Fan%20and%20Xu%20Zheng%20and%20Weihao%20Zhuang%20and%20YingPing%20He%20and%20Tian%20Liu%20and%20Yong%20Yang%20and%20Di%20Wang%20and%20Yuhong%20Liu%20and%20Jie%20Jiang%20and%20Jingwei%20Huang%20and%20Chunchao%20Guo&entry.1292438233=%20%20We%20present%20Hunyuan3D%202.0%2C%20an%20advanced%20large-scale%203D%20synthesis%20system%20for%0Agenerating%20high-resolution%20textured%203D%20assets.%20This%20system%20includes%20two%0Afoundation%20components%3A%20a%20large-scale%20shape%20generation%20model%20--%20Hunyuan3D-DiT%2C%0Aand%20a%20large-scale%20texture%20synthesis%20model%20--%20Hunyuan3D-Paint.%20The%20shape%0Agenerative%20model%2C%20built%20on%20a%20scalable%20flow-based%20diffusion%20transformer%2C%20aims%20to%0Acreate%20geometry%20that%20properly%20aligns%20with%20a%20given%20condition%20image%2C%20laying%20a%0Asolid%20foundation%20for%20downstream%20applications.%20The%20texture%20synthesis%20model%2C%0Abenefiting%20from%20strong%20geometric%20and%20diffusion%20priors%2C%20produces%20high-resolution%0Aand%20vibrant%20texture%20maps%20for%20either%20generated%20or%20hand-crafted%20meshes.%0AFurthermore%2C%20we%20build%20Hunyuan3D-Studio%20--%20a%20versatile%2C%20user-friendly%20production%0Aplatform%20that%20simplifies%20the%20re-creation%20process%20of%203D%20assets.%20It%20allows%20both%0Aprofessional%20and%20amateur%20users%20to%20manipulate%20or%20even%20animate%20their%20meshes%0Aefficiently.%20We%20systematically%20evaluate%20our%20models%2C%20showing%20that%20Hunyuan3D%202.0%0Aoutperforms%20previous%20state-of-the-art%20models%2C%20including%20the%20open-source%20models%0Aand%20closed-source%20models%20in%20geometry%20details%2C%20condition%20alignment%2C%20texture%0Aquality%2C%20and%20etc.%20Hunyuan3D%202.0%20is%20publicly%20released%20in%20order%20to%20fill%20the%20gaps%0Ain%20the%20open-source%203D%20community%20for%20large-scale%20foundation%20generative%20models.%0AThe%20code%20and%20pre-trained%20weights%20of%20our%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/Tencent/Hunyuan3D-2%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12202v1&entry.124074799=Read"},
{"title": "Physics of Skill Learning", "author": "Ziming Liu and Yizhou Liu and Eric J. Michaud and Jeff Gore and Max Tegmark", "abstract": "  We aim to understand physics of skill learning, i.e., how skills are learned\nin neural networks during training. We start by observing the Domino effect,\ni.e., skills are learned sequentially, and notably, some skills kick off\nlearning right after others complete learning, similar to the sequential fall\nof domino cards. To understand the Domino effect and relevant behaviors of\nskill learning, we take physicists' approach of abstraction and simplification.\nWe propose three models with varying complexities -- the Geometry model, the\nResource model, and the Domino model, trading between reality and simplicity.\nThe Domino effect can be reproduced in the Geometry model, whose resource\ninterpretation inspires the Resource model, which can be further simplified to\nthe Domino model. These models present different levels of abstraction and\nsimplification; each is useful to study some aspects of skill learning. The\nGeometry model provides interesting insights into neural scaling laws and\noptimizers; the Resource model sheds light on the learning dynamics of\ncompositional tasks; the Domino model reveals the benefits of modularity. These\nmodels are not only conceptually interesting -- e.g., we show how Chinchilla\nscaling laws can emerge from the Geometry model, but also are useful in\npractice by inspiring algorithmic development -- e.g., we show how simple\nalgorithmic changes, motivated by these toy models, can speed up the training\nof deep learning models.\n", "link": "http://arxiv.org/abs/2501.12391v1", "date": "2025-01-21", "relevancy": 2.4447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics%20of%20Skill%20Learning&body=Title%3A%20Physics%20of%20Skill%20Learning%0AAuthor%3A%20Ziming%20Liu%20and%20Yizhou%20Liu%20and%20Eric%20J.%20Michaud%20and%20Jeff%20Gore%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20We%20aim%20to%20understand%20physics%20of%20skill%20learning%2C%20i.e.%2C%20how%20skills%20are%20learned%0Ain%20neural%20networks%20during%20training.%20We%20start%20by%20observing%20the%20Domino%20effect%2C%0Ai.e.%2C%20skills%20are%20learned%20sequentially%2C%20and%20notably%2C%20some%20skills%20kick%20off%0Alearning%20right%20after%20others%20complete%20learning%2C%20similar%20to%20the%20sequential%20fall%0Aof%20domino%20cards.%20To%20understand%20the%20Domino%20effect%20and%20relevant%20behaviors%20of%0Askill%20learning%2C%20we%20take%20physicists%27%20approach%20of%20abstraction%20and%20simplification.%0AWe%20propose%20three%20models%20with%20varying%20complexities%20--%20the%20Geometry%20model%2C%20the%0AResource%20model%2C%20and%20the%20Domino%20model%2C%20trading%20between%20reality%20and%20simplicity.%0AThe%20Domino%20effect%20can%20be%20reproduced%20in%20the%20Geometry%20model%2C%20whose%20resource%0Ainterpretation%20inspires%20the%20Resource%20model%2C%20which%20can%20be%20further%20simplified%20to%0Athe%20Domino%20model.%20These%20models%20present%20different%20levels%20of%20abstraction%20and%0Asimplification%3B%20each%20is%20useful%20to%20study%20some%20aspects%20of%20skill%20learning.%20The%0AGeometry%20model%20provides%20interesting%20insights%20into%20neural%20scaling%20laws%20and%0Aoptimizers%3B%20the%20Resource%20model%20sheds%20light%20on%20the%20learning%20dynamics%20of%0Acompositional%20tasks%3B%20the%20Domino%20model%20reveals%20the%20benefits%20of%20modularity.%20These%0Amodels%20are%20not%20only%20conceptually%20interesting%20--%20e.g.%2C%20we%20show%20how%20Chinchilla%0Ascaling%20laws%20can%20emerge%20from%20the%20Geometry%20model%2C%20but%20also%20are%20useful%20in%0Apractice%20by%20inspiring%20algorithmic%20development%20--%20e.g.%2C%20we%20show%20how%20simple%0Aalgorithmic%20changes%2C%20motivated%20by%20these%20toy%20models%2C%20can%20speed%20up%20the%20training%0Aof%20deep%20learning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics%2520of%2520Skill%2520Learning%26entry.906535625%3DZiming%2520Liu%2520and%2520Yizhou%2520Liu%2520and%2520Eric%2520J.%2520Michaud%2520and%2520Jeff%2520Gore%2520and%2520Max%2520Tegmark%26entry.1292438233%3D%2520%2520We%2520aim%2520to%2520understand%2520physics%2520of%2520skill%2520learning%252C%2520i.e.%252C%2520how%2520skills%2520are%2520learned%250Ain%2520neural%2520networks%2520during%2520training.%2520We%2520start%2520by%2520observing%2520the%2520Domino%2520effect%252C%250Ai.e.%252C%2520skills%2520are%2520learned%2520sequentially%252C%2520and%2520notably%252C%2520some%2520skills%2520kick%2520off%250Alearning%2520right%2520after%2520others%2520complete%2520learning%252C%2520similar%2520to%2520the%2520sequential%2520fall%250Aof%2520domino%2520cards.%2520To%2520understand%2520the%2520Domino%2520effect%2520and%2520relevant%2520behaviors%2520of%250Askill%2520learning%252C%2520we%2520take%2520physicists%2527%2520approach%2520of%2520abstraction%2520and%2520simplification.%250AWe%2520propose%2520three%2520models%2520with%2520varying%2520complexities%2520--%2520the%2520Geometry%2520model%252C%2520the%250AResource%2520model%252C%2520and%2520the%2520Domino%2520model%252C%2520trading%2520between%2520reality%2520and%2520simplicity.%250AThe%2520Domino%2520effect%2520can%2520be%2520reproduced%2520in%2520the%2520Geometry%2520model%252C%2520whose%2520resource%250Ainterpretation%2520inspires%2520the%2520Resource%2520model%252C%2520which%2520can%2520be%2520further%2520simplified%2520to%250Athe%2520Domino%2520model.%2520These%2520models%2520present%2520different%2520levels%2520of%2520abstraction%2520and%250Asimplification%253B%2520each%2520is%2520useful%2520to%2520study%2520some%2520aspects%2520of%2520skill%2520learning.%2520The%250AGeometry%2520model%2520provides%2520interesting%2520insights%2520into%2520neural%2520scaling%2520laws%2520and%250Aoptimizers%253B%2520the%2520Resource%2520model%2520sheds%2520light%2520on%2520the%2520learning%2520dynamics%2520of%250Acompositional%2520tasks%253B%2520the%2520Domino%2520model%2520reveals%2520the%2520benefits%2520of%2520modularity.%2520These%250Amodels%2520are%2520not%2520only%2520conceptually%2520interesting%2520--%2520e.g.%252C%2520we%2520show%2520how%2520Chinchilla%250Ascaling%2520laws%2520can%2520emerge%2520from%2520the%2520Geometry%2520model%252C%2520but%2520also%2520are%2520useful%2520in%250Apractice%2520by%2520inspiring%2520algorithmic%2520development%2520--%2520e.g.%252C%2520we%2520show%2520how%2520simple%250Aalgorithmic%2520changes%252C%2520motivated%2520by%2520these%2520toy%2520models%252C%2520can%2520speed%2520up%2520the%2520training%250Aof%2520deep%2520learning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics%20of%20Skill%20Learning&entry.906535625=Ziming%20Liu%20and%20Yizhou%20Liu%20and%20Eric%20J.%20Michaud%20and%20Jeff%20Gore%20and%20Max%20Tegmark&entry.1292438233=%20%20We%20aim%20to%20understand%20physics%20of%20skill%20learning%2C%20i.e.%2C%20how%20skills%20are%20learned%0Ain%20neural%20networks%20during%20training.%20We%20start%20by%20observing%20the%20Domino%20effect%2C%0Ai.e.%2C%20skills%20are%20learned%20sequentially%2C%20and%20notably%2C%20some%20skills%20kick%20off%0Alearning%20right%20after%20others%20complete%20learning%2C%20similar%20to%20the%20sequential%20fall%0Aof%20domino%20cards.%20To%20understand%20the%20Domino%20effect%20and%20relevant%20behaviors%20of%0Askill%20learning%2C%20we%20take%20physicists%27%20approach%20of%20abstraction%20and%20simplification.%0AWe%20propose%20three%20models%20with%20varying%20complexities%20--%20the%20Geometry%20model%2C%20the%0AResource%20model%2C%20and%20the%20Domino%20model%2C%20trading%20between%20reality%20and%20simplicity.%0AThe%20Domino%20effect%20can%20be%20reproduced%20in%20the%20Geometry%20model%2C%20whose%20resource%0Ainterpretation%20inspires%20the%20Resource%20model%2C%20which%20can%20be%20further%20simplified%20to%0Athe%20Domino%20model.%20These%20models%20present%20different%20levels%20of%20abstraction%20and%0Asimplification%3B%20each%20is%20useful%20to%20study%20some%20aspects%20of%20skill%20learning.%20The%0AGeometry%20model%20provides%20interesting%20insights%20into%20neural%20scaling%20laws%20and%0Aoptimizers%3B%20the%20Resource%20model%20sheds%20light%20on%20the%20learning%20dynamics%20of%0Acompositional%20tasks%3B%20the%20Domino%20model%20reveals%20the%20benefits%20of%20modularity.%20These%0Amodels%20are%20not%20only%20conceptually%20interesting%20--%20e.g.%2C%20we%20show%20how%20Chinchilla%0Ascaling%20laws%20can%20emerge%20from%20the%20Geometry%20model%2C%20but%20also%20are%20useful%20in%0Apractice%20by%20inspiring%20algorithmic%20development%20--%20e.g.%2C%20we%20show%20how%20simple%0Aalgorithmic%20changes%2C%20motivated%20by%20these%20toy%20models%2C%20can%20speed%20up%20the%20training%0Aof%20deep%20learning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12391v1&entry.124074799=Read"},
{"title": "With Great Backbones Comes Great Adversarial Transferability", "author": "Erik Arakelyan and Karen Hambardzumyan and Davit Papikyan and Pasquale Minervini and Albert Gordo and Isabelle Augenstein and Aram H. Markosyan", "abstract": "  Advances in self-supervised learning (SSL) for machine vision have improved\nrepresentation robustness and model performance, giving rise to pre-trained\nbackbones like \\emph{ResNet} and \\emph{ViT} models tuned with SSL methods such\nas \\emph{SimCLR}. Due to the computational and data demands of pre-training,\nthe utilization of such backbones becomes a strenuous necessity. However,\nemploying these backbones may inherit vulnerabilities to adversarial attacks.\nWhile adversarial robustness has been studied under \\emph{white-box} and\n\\emph{black-box} settings, the robustness of models tuned on pre-trained\nbackbones remains largely unexplored. Additionally, the role of tuning\nmeta-information in mitigating exploitation risks is unclear. This work\nsystematically evaluates the adversarial robustness of such models across\n$20,000$ combinations of tuning meta-information, including fine-tuning\ntechniques, backbone families, datasets, and attack types. We propose using\nproxy models to transfer attacks, simulating varying levels of target knowledge\nby fine-tuning these proxies with diverse configurations. Our findings reveal\nthat proxy-based attacks approach the effectiveness of \\emph{white-box}\nmethods, even with minimal tuning knowledge. We also introduce a naive\n\"backbone attack,\" leveraging only the backbone to generate adversarial\nsamples, which outperforms \\emph{black-box} attacks and rivals \\emph{white-box}\nmethods, highlighting critical risks in model-sharing practices. Finally, our\nablations reveal how increasing tuning meta-information impacts attack\ntransferability, measuring each meta-information combination.\n", "link": "http://arxiv.org/abs/2501.12275v1", "date": "2025-01-21", "relevancy": 2.4379, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4877}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20With%20Great%20Backbones%20Comes%20Great%20Adversarial%20Transferability&body=Title%3A%20With%20Great%20Backbones%20Comes%20Great%20Adversarial%20Transferability%0AAuthor%3A%20Erik%20Arakelyan%20and%20Karen%20Hambardzumyan%20and%20Davit%20Papikyan%20and%20Pasquale%20Minervini%20and%20Albert%20Gordo%20and%20Isabelle%20Augenstein%20and%20Aram%20H.%20Markosyan%0AAbstract%3A%20%20%20Advances%20in%20self-supervised%20learning%20%28SSL%29%20for%20machine%20vision%20have%20improved%0Arepresentation%20robustness%20and%20model%20performance%2C%20giving%20rise%20to%20pre-trained%0Abackbones%20like%20%5Cemph%7BResNet%7D%20and%20%5Cemph%7BViT%7D%20models%20tuned%20with%20SSL%20methods%20such%0Aas%20%5Cemph%7BSimCLR%7D.%20Due%20to%20the%20computational%20and%20data%20demands%20of%20pre-training%2C%0Athe%20utilization%20of%20such%20backbones%20becomes%20a%20strenuous%20necessity.%20However%2C%0Aemploying%20these%20backbones%20may%20inherit%20vulnerabilities%20to%20adversarial%20attacks.%0AWhile%20adversarial%20robustness%20has%20been%20studied%20under%20%5Cemph%7Bwhite-box%7D%20and%0A%5Cemph%7Bblack-box%7D%20settings%2C%20the%20robustness%20of%20models%20tuned%20on%20pre-trained%0Abackbones%20remains%20largely%20unexplored.%20Additionally%2C%20the%20role%20of%20tuning%0Ameta-information%20in%20mitigating%20exploitation%20risks%20is%20unclear.%20This%20work%0Asystematically%20evaluates%20the%20adversarial%20robustness%20of%20such%20models%20across%0A%2420%2C000%24%20combinations%20of%20tuning%20meta-information%2C%20including%20fine-tuning%0Atechniques%2C%20backbone%20families%2C%20datasets%2C%20and%20attack%20types.%20We%20propose%20using%0Aproxy%20models%20to%20transfer%20attacks%2C%20simulating%20varying%20levels%20of%20target%20knowledge%0Aby%20fine-tuning%20these%20proxies%20with%20diverse%20configurations.%20Our%20findings%20reveal%0Athat%20proxy-based%20attacks%20approach%20the%20effectiveness%20of%20%5Cemph%7Bwhite-box%7D%0Amethods%2C%20even%20with%20minimal%20tuning%20knowledge.%20We%20also%20introduce%20a%20naive%0A%22backbone%20attack%2C%22%20leveraging%20only%20the%20backbone%20to%20generate%20adversarial%0Asamples%2C%20which%20outperforms%20%5Cemph%7Bblack-box%7D%20attacks%20and%20rivals%20%5Cemph%7Bwhite-box%7D%0Amethods%2C%20highlighting%20critical%20risks%20in%20model-sharing%20practices.%20Finally%2C%20our%0Aablations%20reveal%20how%20increasing%20tuning%20meta-information%20impacts%20attack%0Atransferability%2C%20measuring%20each%20meta-information%20combination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWith%2520Great%2520Backbones%2520Comes%2520Great%2520Adversarial%2520Transferability%26entry.906535625%3DErik%2520Arakelyan%2520and%2520Karen%2520Hambardzumyan%2520and%2520Davit%2520Papikyan%2520and%2520Pasquale%2520Minervini%2520and%2520Albert%2520Gordo%2520and%2520Isabelle%2520Augenstein%2520and%2520Aram%2520H.%2520Markosyan%26entry.1292438233%3D%2520%2520Advances%2520in%2520self-supervised%2520learning%2520%2528SSL%2529%2520for%2520machine%2520vision%2520have%2520improved%250Arepresentation%2520robustness%2520and%2520model%2520performance%252C%2520giving%2520rise%2520to%2520pre-trained%250Abackbones%2520like%2520%255Cemph%257BResNet%257D%2520and%2520%255Cemph%257BViT%257D%2520models%2520tuned%2520with%2520SSL%2520methods%2520such%250Aas%2520%255Cemph%257BSimCLR%257D.%2520Due%2520to%2520the%2520computational%2520and%2520data%2520demands%2520of%2520pre-training%252C%250Athe%2520utilization%2520of%2520such%2520backbones%2520becomes%2520a%2520strenuous%2520necessity.%2520However%252C%250Aemploying%2520these%2520backbones%2520may%2520inherit%2520vulnerabilities%2520to%2520adversarial%2520attacks.%250AWhile%2520adversarial%2520robustness%2520has%2520been%2520studied%2520under%2520%255Cemph%257Bwhite-box%257D%2520and%250A%255Cemph%257Bblack-box%257D%2520settings%252C%2520the%2520robustness%2520of%2520models%2520tuned%2520on%2520pre-trained%250Abackbones%2520remains%2520largely%2520unexplored.%2520Additionally%252C%2520the%2520role%2520of%2520tuning%250Ameta-information%2520in%2520mitigating%2520exploitation%2520risks%2520is%2520unclear.%2520This%2520work%250Asystematically%2520evaluates%2520the%2520adversarial%2520robustness%2520of%2520such%2520models%2520across%250A%252420%252C000%2524%2520combinations%2520of%2520tuning%2520meta-information%252C%2520including%2520fine-tuning%250Atechniques%252C%2520backbone%2520families%252C%2520datasets%252C%2520and%2520attack%2520types.%2520We%2520propose%2520using%250Aproxy%2520models%2520to%2520transfer%2520attacks%252C%2520simulating%2520varying%2520levels%2520of%2520target%2520knowledge%250Aby%2520fine-tuning%2520these%2520proxies%2520with%2520diverse%2520configurations.%2520Our%2520findings%2520reveal%250Athat%2520proxy-based%2520attacks%2520approach%2520the%2520effectiveness%2520of%2520%255Cemph%257Bwhite-box%257D%250Amethods%252C%2520even%2520with%2520minimal%2520tuning%2520knowledge.%2520We%2520also%2520introduce%2520a%2520naive%250A%2522backbone%2520attack%252C%2522%2520leveraging%2520only%2520the%2520backbone%2520to%2520generate%2520adversarial%250Asamples%252C%2520which%2520outperforms%2520%255Cemph%257Bblack-box%257D%2520attacks%2520and%2520rivals%2520%255Cemph%257Bwhite-box%257D%250Amethods%252C%2520highlighting%2520critical%2520risks%2520in%2520model-sharing%2520practices.%2520Finally%252C%2520our%250Aablations%2520reveal%2520how%2520increasing%2520tuning%2520meta-information%2520impacts%2520attack%250Atransferability%252C%2520measuring%2520each%2520meta-information%2520combination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=With%20Great%20Backbones%20Comes%20Great%20Adversarial%20Transferability&entry.906535625=Erik%20Arakelyan%20and%20Karen%20Hambardzumyan%20and%20Davit%20Papikyan%20and%20Pasquale%20Minervini%20and%20Albert%20Gordo%20and%20Isabelle%20Augenstein%20and%20Aram%20H.%20Markosyan&entry.1292438233=%20%20Advances%20in%20self-supervised%20learning%20%28SSL%29%20for%20machine%20vision%20have%20improved%0Arepresentation%20robustness%20and%20model%20performance%2C%20giving%20rise%20to%20pre-trained%0Abackbones%20like%20%5Cemph%7BResNet%7D%20and%20%5Cemph%7BViT%7D%20models%20tuned%20with%20SSL%20methods%20such%0Aas%20%5Cemph%7BSimCLR%7D.%20Due%20to%20the%20computational%20and%20data%20demands%20of%20pre-training%2C%0Athe%20utilization%20of%20such%20backbones%20becomes%20a%20strenuous%20necessity.%20However%2C%0Aemploying%20these%20backbones%20may%20inherit%20vulnerabilities%20to%20adversarial%20attacks.%0AWhile%20adversarial%20robustness%20has%20been%20studied%20under%20%5Cemph%7Bwhite-box%7D%20and%0A%5Cemph%7Bblack-box%7D%20settings%2C%20the%20robustness%20of%20models%20tuned%20on%20pre-trained%0Abackbones%20remains%20largely%20unexplored.%20Additionally%2C%20the%20role%20of%20tuning%0Ameta-information%20in%20mitigating%20exploitation%20risks%20is%20unclear.%20This%20work%0Asystematically%20evaluates%20the%20adversarial%20robustness%20of%20such%20models%20across%0A%2420%2C000%24%20combinations%20of%20tuning%20meta-information%2C%20including%20fine-tuning%0Atechniques%2C%20backbone%20families%2C%20datasets%2C%20and%20attack%20types.%20We%20propose%20using%0Aproxy%20models%20to%20transfer%20attacks%2C%20simulating%20varying%20levels%20of%20target%20knowledge%0Aby%20fine-tuning%20these%20proxies%20with%20diverse%20configurations.%20Our%20findings%20reveal%0Athat%20proxy-based%20attacks%20approach%20the%20effectiveness%20of%20%5Cemph%7Bwhite-box%7D%0Amethods%2C%20even%20with%20minimal%20tuning%20knowledge.%20We%20also%20introduce%20a%20naive%0A%22backbone%20attack%2C%22%20leveraging%20only%20the%20backbone%20to%20generate%20adversarial%0Asamples%2C%20which%20outperforms%20%5Cemph%7Bblack-box%7D%20attacks%20and%20rivals%20%5Cemph%7Bwhite-box%7D%0Amethods%2C%20highlighting%20critical%20risks%20in%20model-sharing%20practices.%20Finally%2C%20our%0Aablations%20reveal%20how%20increasing%20tuning%20meta-information%20impacts%20attack%0Atransferability%2C%20measuring%20each%20meta-information%20combination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12275v1&entry.124074799=Read"},
{"title": "TokenVerse: Versatile Multi-concept Personalization in Token Modulation\n  Space", "author": "Daniel Garibi and Shahar Yadin and Roni Paiss and Omer Tov and Shiran Zada and Ariel Ephrat and Tomer Michaeli and Inbar Mosseri and Tali Dekel", "abstract": "  We present TokenVerse -- a method for multi-concept personalization,\nleveraging a pre-trained text-to-image diffusion model. Our framework can\ndisentangle complex visual elements and attributes from as little as a single\nimage, while enabling seamless plug-and-play generation of combinations of\nconcepts extracted from multiple images. As opposed to existing works,\nTokenVerse can handle multiple images with multiple concepts each, and supports\na wide-range of concepts, including objects, accessories, materials, pose, and\nlighting. Our work exploits a DiT-based text-to-image model, in which the input\ntext affects the generation through both attention and modulation (shift and\nscale). We observe that the modulation space is semantic and enables localized\ncontrol over complex concepts. Building on this insight, we devise an\noptimization-based framework that takes as input an image and a text\ndescription, and finds for each word a distinct direction in the modulation\nspace. These directions can then be used to generate new images that combine\nthe learned concepts in a desired configuration. We demonstrate the\neffectiveness of TokenVerse in challenging personalization settings, and\nshowcase its advantages over existing methods. project's webpage in\nhttps://token-verse.github.io/\n", "link": "http://arxiv.org/abs/2501.12224v1", "date": "2025-01-21", "relevancy": 2.437, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6434}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6276}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenVerse%3A%20Versatile%20Multi-concept%20Personalization%20in%20Token%20Modulation%0A%20%20Space&body=Title%3A%20TokenVerse%3A%20Versatile%20Multi-concept%20Personalization%20in%20Token%20Modulation%0A%20%20Space%0AAuthor%3A%20Daniel%20Garibi%20and%20Shahar%20Yadin%20and%20Roni%20Paiss%20and%20Omer%20Tov%20and%20Shiran%20Zada%20and%20Ariel%20Ephrat%20and%20Tomer%20Michaeli%20and%20Inbar%20Mosseri%20and%20Tali%20Dekel%0AAbstract%3A%20%20%20We%20present%20TokenVerse%20--%20a%20method%20for%20multi-concept%20personalization%2C%0Aleveraging%20a%20pre-trained%20text-to-image%20diffusion%20model.%20Our%20framework%20can%0Adisentangle%20complex%20visual%20elements%20and%20attributes%20from%20as%20little%20as%20a%20single%0Aimage%2C%20while%20enabling%20seamless%20plug-and-play%20generation%20of%20combinations%20of%0Aconcepts%20extracted%20from%20multiple%20images.%20As%20opposed%20to%20existing%20works%2C%0ATokenVerse%20can%20handle%20multiple%20images%20with%20multiple%20concepts%20each%2C%20and%20supports%0Aa%20wide-range%20of%20concepts%2C%20including%20objects%2C%20accessories%2C%20materials%2C%20pose%2C%20and%0Alighting.%20Our%20work%20exploits%20a%20DiT-based%20text-to-image%20model%2C%20in%20which%20the%20input%0Atext%20affects%20the%20generation%20through%20both%20attention%20and%20modulation%20%28shift%20and%0Ascale%29.%20We%20observe%20that%20the%20modulation%20space%20is%20semantic%20and%20enables%20localized%0Acontrol%20over%20complex%20concepts.%20Building%20on%20this%20insight%2C%20we%20devise%20an%0Aoptimization-based%20framework%20that%20takes%20as%20input%20an%20image%20and%20a%20text%0Adescription%2C%20and%20finds%20for%20each%20word%20a%20distinct%20direction%20in%20the%20modulation%0Aspace.%20These%20directions%20can%20then%20be%20used%20to%20generate%20new%20images%20that%20combine%0Athe%20learned%20concepts%20in%20a%20desired%20configuration.%20We%20demonstrate%20the%0Aeffectiveness%20of%20TokenVerse%20in%20challenging%20personalization%20settings%2C%20and%0Ashowcase%20its%20advantages%20over%20existing%20methods.%20project%27s%20webpage%20in%0Ahttps%3A//token-verse.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenVerse%253A%2520Versatile%2520Multi-concept%2520Personalization%2520in%2520Token%2520Modulation%250A%2520%2520Space%26entry.906535625%3DDaniel%2520Garibi%2520and%2520Shahar%2520Yadin%2520and%2520Roni%2520Paiss%2520and%2520Omer%2520Tov%2520and%2520Shiran%2520Zada%2520and%2520Ariel%2520Ephrat%2520and%2520Tomer%2520Michaeli%2520and%2520Inbar%2520Mosseri%2520and%2520Tali%2520Dekel%26entry.1292438233%3D%2520%2520We%2520present%2520TokenVerse%2520--%2520a%2520method%2520for%2520multi-concept%2520personalization%252C%250Aleveraging%2520a%2520pre-trained%2520text-to-image%2520diffusion%2520model.%2520Our%2520framework%2520can%250Adisentangle%2520complex%2520visual%2520elements%2520and%2520attributes%2520from%2520as%2520little%2520as%2520a%2520single%250Aimage%252C%2520while%2520enabling%2520seamless%2520plug-and-play%2520generation%2520of%2520combinations%2520of%250Aconcepts%2520extracted%2520from%2520multiple%2520images.%2520As%2520opposed%2520to%2520existing%2520works%252C%250ATokenVerse%2520can%2520handle%2520multiple%2520images%2520with%2520multiple%2520concepts%2520each%252C%2520and%2520supports%250Aa%2520wide-range%2520of%2520concepts%252C%2520including%2520objects%252C%2520accessories%252C%2520materials%252C%2520pose%252C%2520and%250Alighting.%2520Our%2520work%2520exploits%2520a%2520DiT-based%2520text-to-image%2520model%252C%2520in%2520which%2520the%2520input%250Atext%2520affects%2520the%2520generation%2520through%2520both%2520attention%2520and%2520modulation%2520%2528shift%2520and%250Ascale%2529.%2520We%2520observe%2520that%2520the%2520modulation%2520space%2520is%2520semantic%2520and%2520enables%2520localized%250Acontrol%2520over%2520complex%2520concepts.%2520Building%2520on%2520this%2520insight%252C%2520we%2520devise%2520an%250Aoptimization-based%2520framework%2520that%2520takes%2520as%2520input%2520an%2520image%2520and%2520a%2520text%250Adescription%252C%2520and%2520finds%2520for%2520each%2520word%2520a%2520distinct%2520direction%2520in%2520the%2520modulation%250Aspace.%2520These%2520directions%2520can%2520then%2520be%2520used%2520to%2520generate%2520new%2520images%2520that%2520combine%250Athe%2520learned%2520concepts%2520in%2520a%2520desired%2520configuration.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520TokenVerse%2520in%2520challenging%2520personalization%2520settings%252C%2520and%250Ashowcase%2520its%2520advantages%2520over%2520existing%2520methods.%2520project%2527s%2520webpage%2520in%250Ahttps%253A//token-verse.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenVerse%3A%20Versatile%20Multi-concept%20Personalization%20in%20Token%20Modulation%0A%20%20Space&entry.906535625=Daniel%20Garibi%20and%20Shahar%20Yadin%20and%20Roni%20Paiss%20and%20Omer%20Tov%20and%20Shiran%20Zada%20and%20Ariel%20Ephrat%20and%20Tomer%20Michaeli%20and%20Inbar%20Mosseri%20and%20Tali%20Dekel&entry.1292438233=%20%20We%20present%20TokenVerse%20--%20a%20method%20for%20multi-concept%20personalization%2C%0Aleveraging%20a%20pre-trained%20text-to-image%20diffusion%20model.%20Our%20framework%20can%0Adisentangle%20complex%20visual%20elements%20and%20attributes%20from%20as%20little%20as%20a%20single%0Aimage%2C%20while%20enabling%20seamless%20plug-and-play%20generation%20of%20combinations%20of%0Aconcepts%20extracted%20from%20multiple%20images.%20As%20opposed%20to%20existing%20works%2C%0ATokenVerse%20can%20handle%20multiple%20images%20with%20multiple%20concepts%20each%2C%20and%20supports%0Aa%20wide-range%20of%20concepts%2C%20including%20objects%2C%20accessories%2C%20materials%2C%20pose%2C%20and%0Alighting.%20Our%20work%20exploits%20a%20DiT-based%20text-to-image%20model%2C%20in%20which%20the%20input%0Atext%20affects%20the%20generation%20through%20both%20attention%20and%20modulation%20%28shift%20and%0Ascale%29.%20We%20observe%20that%20the%20modulation%20space%20is%20semantic%20and%20enables%20localized%0Acontrol%20over%20complex%20concepts.%20Building%20on%20this%20insight%2C%20we%20devise%20an%0Aoptimization-based%20framework%20that%20takes%20as%20input%20an%20image%20and%20a%20text%0Adescription%2C%20and%20finds%20for%20each%20word%20a%20distinct%20direction%20in%20the%20modulation%0Aspace.%20These%20directions%20can%20then%20be%20used%20to%20generate%20new%20images%20that%20combine%0Athe%20learned%20concepts%20in%20a%20desired%20configuration.%20We%20demonstrate%20the%0Aeffectiveness%20of%20TokenVerse%20in%20challenging%20personalization%20settings%2C%20and%0Ashowcase%20its%20advantages%20over%20existing%20methods.%20project%27s%20webpage%20in%0Ahttps%3A//token-verse.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12224v1&entry.124074799=Read"},
{"title": "SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP", "author": "Yusuke Hirota and Min-Hung Chen and Chien-Yi Wang and Yuta Nakashima and Yu-Chiang Frank Wang and Ryo Hachiuma", "abstract": "  Large-scale vision-language models, such as CLIP, are known to contain\nsocietal bias regarding protected attributes (e.g., gender, age). This paper\naims to address the problems of societal bias in CLIP. Although previous\nstudies have proposed to debias societal bias through adversarial learning or\ntest-time projecting, our comprehensive study of these works identifies two\ncritical limitations: 1) loss of attribute information when it is explicitly\ndisclosed in the input and 2) use of the attribute annotations during debiasing\nprocess. To mitigate societal bias in CLIP and overcome these limitations\nsimultaneously, we introduce a simple-yet-effective debiasing method called\nSANER (societal attribute neutralizer) that eliminates attribute information\nfrom CLIP text features only of attribute-neutral descriptions. Experimental\nresults show that SANER, which does not require attribute annotations and\npreserves original information for attribute-specific descriptions,\ndemonstrates superior debiasing ability than the existing methods.\nAdditionally, we observe that SANER does not require retraining CLIP from\nscratch with the original dataset. Moreover, the debiased model can be directly\napplied to the text-to-image generation model by simply replacing the text\nencoder.\n", "link": "http://arxiv.org/abs/2408.10202v2", "date": "2025-01-21", "relevancy": 2.3938, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SANER%3A%20Annotation-free%20Societal%20Attribute%20Neutralizer%20for%20Debiasing%20CLIP&body=Title%3A%20SANER%3A%20Annotation-free%20Societal%20Attribute%20Neutralizer%20for%20Debiasing%20CLIP%0AAuthor%3A%20Yusuke%20Hirota%20and%20Min-Hung%20Chen%20and%20Chien-Yi%20Wang%20and%20Yuta%20Nakashima%20and%20Yu-Chiang%20Frank%20Wang%20and%20Ryo%20Hachiuma%0AAbstract%3A%20%20%20Large-scale%20vision-language%20models%2C%20such%20as%20CLIP%2C%20are%20known%20to%20contain%0Asocietal%20bias%20regarding%20protected%20attributes%20%28e.g.%2C%20gender%2C%20age%29.%20This%20paper%0Aaims%20to%20address%20the%20problems%20of%20societal%20bias%20in%20CLIP.%20Although%20previous%0Astudies%20have%20proposed%20to%20debias%20societal%20bias%20through%20adversarial%20learning%20or%0Atest-time%20projecting%2C%20our%20comprehensive%20study%20of%20these%20works%20identifies%20two%0Acritical%20limitations%3A%201%29%20loss%20of%20attribute%20information%20when%20it%20is%20explicitly%0Adisclosed%20in%20the%20input%20and%202%29%20use%20of%20the%20attribute%20annotations%20during%20debiasing%0Aprocess.%20To%20mitigate%20societal%20bias%20in%20CLIP%20and%20overcome%20these%20limitations%0Asimultaneously%2C%20we%20introduce%20a%20simple-yet-effective%20debiasing%20method%20called%0ASANER%20%28societal%20attribute%20neutralizer%29%20that%20eliminates%20attribute%20information%0Afrom%20CLIP%20text%20features%20only%20of%20attribute-neutral%20descriptions.%20Experimental%0Aresults%20show%20that%20SANER%2C%20which%20does%20not%20require%20attribute%20annotations%20and%0Apreserves%20original%20information%20for%20attribute-specific%20descriptions%2C%0Ademonstrates%20superior%20debiasing%20ability%20than%20the%20existing%20methods.%0AAdditionally%2C%20we%20observe%20that%20SANER%20does%20not%20require%20retraining%20CLIP%20from%0Ascratch%20with%20the%20original%20dataset.%20Moreover%2C%20the%20debiased%20model%20can%20be%20directly%0Aapplied%20to%20the%20text-to-image%20generation%20model%20by%20simply%20replacing%20the%20text%0Aencoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSANER%253A%2520Annotation-free%2520Societal%2520Attribute%2520Neutralizer%2520for%2520Debiasing%2520CLIP%26entry.906535625%3DYusuke%2520Hirota%2520and%2520Min-Hung%2520Chen%2520and%2520Chien-Yi%2520Wang%2520and%2520Yuta%2520Nakashima%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Ryo%2520Hachiuma%26entry.1292438233%3D%2520%2520Large-scale%2520vision-language%2520models%252C%2520such%2520as%2520CLIP%252C%2520are%2520known%2520to%2520contain%250Asocietal%2520bias%2520regarding%2520protected%2520attributes%2520%2528e.g.%252C%2520gender%252C%2520age%2529.%2520This%2520paper%250Aaims%2520to%2520address%2520the%2520problems%2520of%2520societal%2520bias%2520in%2520CLIP.%2520Although%2520previous%250Astudies%2520have%2520proposed%2520to%2520debias%2520societal%2520bias%2520through%2520adversarial%2520learning%2520or%250Atest-time%2520projecting%252C%2520our%2520comprehensive%2520study%2520of%2520these%2520works%2520identifies%2520two%250Acritical%2520limitations%253A%25201%2529%2520loss%2520of%2520attribute%2520information%2520when%2520it%2520is%2520explicitly%250Adisclosed%2520in%2520the%2520input%2520and%25202%2529%2520use%2520of%2520the%2520attribute%2520annotations%2520during%2520debiasing%250Aprocess.%2520To%2520mitigate%2520societal%2520bias%2520in%2520CLIP%2520and%2520overcome%2520these%2520limitations%250Asimultaneously%252C%2520we%2520introduce%2520a%2520simple-yet-effective%2520debiasing%2520method%2520called%250ASANER%2520%2528societal%2520attribute%2520neutralizer%2529%2520that%2520eliminates%2520attribute%2520information%250Afrom%2520CLIP%2520text%2520features%2520only%2520of%2520attribute-neutral%2520descriptions.%2520Experimental%250Aresults%2520show%2520that%2520SANER%252C%2520which%2520does%2520not%2520require%2520attribute%2520annotations%2520and%250Apreserves%2520original%2520information%2520for%2520attribute-specific%2520descriptions%252C%250Ademonstrates%2520superior%2520debiasing%2520ability%2520than%2520the%2520existing%2520methods.%250AAdditionally%252C%2520we%2520observe%2520that%2520SANER%2520does%2520not%2520require%2520retraining%2520CLIP%2520from%250Ascratch%2520with%2520the%2520original%2520dataset.%2520Moreover%252C%2520the%2520debiased%2520model%2520can%2520be%2520directly%250Aapplied%2520to%2520the%2520text-to-image%2520generation%2520model%2520by%2520simply%2520replacing%2520the%2520text%250Aencoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SANER%3A%20Annotation-free%20Societal%20Attribute%20Neutralizer%20for%20Debiasing%20CLIP&entry.906535625=Yusuke%20Hirota%20and%20Min-Hung%20Chen%20and%20Chien-Yi%20Wang%20and%20Yuta%20Nakashima%20and%20Yu-Chiang%20Frank%20Wang%20and%20Ryo%20Hachiuma&entry.1292438233=%20%20Large-scale%20vision-language%20models%2C%20such%20as%20CLIP%2C%20are%20known%20to%20contain%0Asocietal%20bias%20regarding%20protected%20attributes%20%28e.g.%2C%20gender%2C%20age%29.%20This%20paper%0Aaims%20to%20address%20the%20problems%20of%20societal%20bias%20in%20CLIP.%20Although%20previous%0Astudies%20have%20proposed%20to%20debias%20societal%20bias%20through%20adversarial%20learning%20or%0Atest-time%20projecting%2C%20our%20comprehensive%20study%20of%20these%20works%20identifies%20two%0Acritical%20limitations%3A%201%29%20loss%20of%20attribute%20information%20when%20it%20is%20explicitly%0Adisclosed%20in%20the%20input%20and%202%29%20use%20of%20the%20attribute%20annotations%20during%20debiasing%0Aprocess.%20To%20mitigate%20societal%20bias%20in%20CLIP%20and%20overcome%20these%20limitations%0Asimultaneously%2C%20we%20introduce%20a%20simple-yet-effective%20debiasing%20method%20called%0ASANER%20%28societal%20attribute%20neutralizer%29%20that%20eliminates%20attribute%20information%0Afrom%20CLIP%20text%20features%20only%20of%20attribute-neutral%20descriptions.%20Experimental%0Aresults%20show%20that%20SANER%2C%20which%20does%20not%20require%20attribute%20annotations%20and%0Apreserves%20original%20information%20for%20attribute-specific%20descriptions%2C%0Ademonstrates%20superior%20debiasing%20ability%20than%20the%20existing%20methods.%0AAdditionally%2C%20we%20observe%20that%20SANER%20does%20not%20require%20retraining%20CLIP%20from%0Ascratch%20with%20the%20original%20dataset.%20Moreover%2C%20the%20debiased%20model%20can%20be%20directly%0Aapplied%20to%20the%20text-to-image%20generation%20model%20by%20simply%20replacing%20the%20text%0Aencoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10202v2&entry.124074799=Read"},
{"title": "CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning", "author": "Yuanheng Fang and Guoqing Chao and Wenqiang Lei and Shaobo Li and Dianhui Chu", "abstract": "  Large Language Models (LLMs) have recently achieved impressive results in\ncomplex reasoning tasks through Chain of Thought (CoT) prompting. However, most\nexisting CoT methods rely on using the same prompts, whether manually designed\nor automatically generated, to handle the entire dataset. This\none-size-fits-all approach may fail to meet the specific needs arising from the\ndiversities within a single dataset. To solve this problem, we propose the\nClustered Distance-Weighted Chain of Thought (CDW-CoT) method, which\ndynamically constructs prompts tailored to the characteristics of each data\ninstance by integrating clustering and prompt optimization techniques. Our\nmethod employs clustering algorithms to categorize the dataset into distinct\ngroups, from which a candidate pool of prompts is selected to reflect the\ninherent diversity within the dataset. For each cluster, CDW-CoT trains the\noptimal prompt probability distribution tailored to their specific\ncharacteristics. Finally, it dynamically constructs a unique prompt probability\ndistribution for each test instance, based on its proximity to cluster centers,\nfrom which prompts are selected for reasoning. CDW-CoT consistently outperforms\ntraditional CoT methods across six datasets, including commonsense, symbolic,\nand mathematical reasoning tasks. Specifically, when compared to manual CoT,\nCDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and\n15.72% on LLaMA3 (8B).\n", "link": "http://arxiv.org/abs/2501.12226v1", "date": "2025-01-21", "relevancy": 2.3646, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CDW-CoT%3A%20Clustered%20Distance-Weighted%20Chain-of-Thoughts%20Reasoning&body=Title%3A%20CDW-CoT%3A%20Clustered%20Distance-Weighted%20Chain-of-Thoughts%20Reasoning%0AAuthor%3A%20Yuanheng%20Fang%20and%20Guoqing%20Chao%20and%20Wenqiang%20Lei%20and%20Shaobo%20Li%20and%20Dianhui%20Chu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20achieved%20impressive%20results%20in%0Acomplex%20reasoning%20tasks%20through%20Chain%20of%20Thought%20%28CoT%29%20prompting.%20However%2C%20most%0Aexisting%20CoT%20methods%20rely%20on%20using%20the%20same%20prompts%2C%20whether%20manually%20designed%0Aor%20automatically%20generated%2C%20to%20handle%20the%20entire%20dataset.%20This%0Aone-size-fits-all%20approach%20may%20fail%20to%20meet%20the%20specific%20needs%20arising%20from%20the%0Adiversities%20within%20a%20single%20dataset.%20To%20solve%20this%20problem%2C%20we%20propose%20the%0AClustered%20Distance-Weighted%20Chain%20of%20Thought%20%28CDW-CoT%29%20method%2C%20which%0Adynamically%20constructs%20prompts%20tailored%20to%20the%20characteristics%20of%20each%20data%0Ainstance%20by%20integrating%20clustering%20and%20prompt%20optimization%20techniques.%20Our%0Amethod%20employs%20clustering%20algorithms%20to%20categorize%20the%20dataset%20into%20distinct%0Agroups%2C%20from%20which%20a%20candidate%20pool%20of%20prompts%20is%20selected%20to%20reflect%20the%0Ainherent%20diversity%20within%20the%20dataset.%20For%20each%20cluster%2C%20CDW-CoT%20trains%20the%0Aoptimal%20prompt%20probability%20distribution%20tailored%20to%20their%20specific%0Acharacteristics.%20Finally%2C%20it%20dynamically%20constructs%20a%20unique%20prompt%20probability%0Adistribution%20for%20each%20test%20instance%2C%20based%20on%20its%20proximity%20to%20cluster%20centers%2C%0Afrom%20which%20prompts%20are%20selected%20for%20reasoning.%20CDW-CoT%20consistently%20outperforms%0Atraditional%20CoT%20methods%20across%20six%20datasets%2C%20including%20commonsense%2C%20symbolic%2C%0Aand%20mathematical%20reasoning%20tasks.%20Specifically%2C%20when%20compared%20to%20manual%20CoT%2C%0ACDW-CoT%20achieves%20an%20average%20accuracy%20improvement%20of%2025.34%25%20on%20LLaMA2%20%2813B%29%20and%0A15.72%25%20on%20LLaMA3%20%288B%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCDW-CoT%253A%2520Clustered%2520Distance-Weighted%2520Chain-of-Thoughts%2520Reasoning%26entry.906535625%3DYuanheng%2520Fang%2520and%2520Guoqing%2520Chao%2520and%2520Wenqiang%2520Lei%2520and%2520Shaobo%2520Li%2520and%2520Dianhui%2520Chu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520achieved%2520impressive%2520results%2520in%250Acomplex%2520reasoning%2520tasks%2520through%2520Chain%2520of%2520Thought%2520%2528CoT%2529%2520prompting.%2520However%252C%2520most%250Aexisting%2520CoT%2520methods%2520rely%2520on%2520using%2520the%2520same%2520prompts%252C%2520whether%2520manually%2520designed%250Aor%2520automatically%2520generated%252C%2520to%2520handle%2520the%2520entire%2520dataset.%2520This%250Aone-size-fits-all%2520approach%2520may%2520fail%2520to%2520meet%2520the%2520specific%2520needs%2520arising%2520from%2520the%250Adiversities%2520within%2520a%2520single%2520dataset.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520the%250AClustered%2520Distance-Weighted%2520Chain%2520of%2520Thought%2520%2528CDW-CoT%2529%2520method%252C%2520which%250Adynamically%2520constructs%2520prompts%2520tailored%2520to%2520the%2520characteristics%2520of%2520each%2520data%250Ainstance%2520by%2520integrating%2520clustering%2520and%2520prompt%2520optimization%2520techniques.%2520Our%250Amethod%2520employs%2520clustering%2520algorithms%2520to%2520categorize%2520the%2520dataset%2520into%2520distinct%250Agroups%252C%2520from%2520which%2520a%2520candidate%2520pool%2520of%2520prompts%2520is%2520selected%2520to%2520reflect%2520the%250Ainherent%2520diversity%2520within%2520the%2520dataset.%2520For%2520each%2520cluster%252C%2520CDW-CoT%2520trains%2520the%250Aoptimal%2520prompt%2520probability%2520distribution%2520tailored%2520to%2520their%2520specific%250Acharacteristics.%2520Finally%252C%2520it%2520dynamically%2520constructs%2520a%2520unique%2520prompt%2520probability%250Adistribution%2520for%2520each%2520test%2520instance%252C%2520based%2520on%2520its%2520proximity%2520to%2520cluster%2520centers%252C%250Afrom%2520which%2520prompts%2520are%2520selected%2520for%2520reasoning.%2520CDW-CoT%2520consistently%2520outperforms%250Atraditional%2520CoT%2520methods%2520across%2520six%2520datasets%252C%2520including%2520commonsense%252C%2520symbolic%252C%250Aand%2520mathematical%2520reasoning%2520tasks.%2520Specifically%252C%2520when%2520compared%2520to%2520manual%2520CoT%252C%250ACDW-CoT%2520achieves%2520an%2520average%2520accuracy%2520improvement%2520of%252025.34%2525%2520on%2520LLaMA2%2520%252813B%2529%2520and%250A15.72%2525%2520on%2520LLaMA3%2520%25288B%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDW-CoT%3A%20Clustered%20Distance-Weighted%20Chain-of-Thoughts%20Reasoning&entry.906535625=Yuanheng%20Fang%20and%20Guoqing%20Chao%20and%20Wenqiang%20Lei%20and%20Shaobo%20Li%20and%20Dianhui%20Chu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20achieved%20impressive%20results%20in%0Acomplex%20reasoning%20tasks%20through%20Chain%20of%20Thought%20%28CoT%29%20prompting.%20However%2C%20most%0Aexisting%20CoT%20methods%20rely%20on%20using%20the%20same%20prompts%2C%20whether%20manually%20designed%0Aor%20automatically%20generated%2C%20to%20handle%20the%20entire%20dataset.%20This%0Aone-size-fits-all%20approach%20may%20fail%20to%20meet%20the%20specific%20needs%20arising%20from%20the%0Adiversities%20within%20a%20single%20dataset.%20To%20solve%20this%20problem%2C%20we%20propose%20the%0AClustered%20Distance-Weighted%20Chain%20of%20Thought%20%28CDW-CoT%29%20method%2C%20which%0Adynamically%20constructs%20prompts%20tailored%20to%20the%20characteristics%20of%20each%20data%0Ainstance%20by%20integrating%20clustering%20and%20prompt%20optimization%20techniques.%20Our%0Amethod%20employs%20clustering%20algorithms%20to%20categorize%20the%20dataset%20into%20distinct%0Agroups%2C%20from%20which%20a%20candidate%20pool%20of%20prompts%20is%20selected%20to%20reflect%20the%0Ainherent%20diversity%20within%20the%20dataset.%20For%20each%20cluster%2C%20CDW-CoT%20trains%20the%0Aoptimal%20prompt%20probability%20distribution%20tailored%20to%20their%20specific%0Acharacteristics.%20Finally%2C%20it%20dynamically%20constructs%20a%20unique%20prompt%20probability%0Adistribution%20for%20each%20test%20instance%2C%20based%20on%20its%20proximity%20to%20cluster%20centers%2C%0Afrom%20which%20prompts%20are%20selected%20for%20reasoning.%20CDW-CoT%20consistently%20outperforms%0Atraditional%20CoT%20methods%20across%20six%20datasets%2C%20including%20commonsense%2C%20symbolic%2C%0Aand%20mathematical%20reasoning%20tasks.%20Specifically%2C%20when%20compared%20to%20manual%20CoT%2C%0ACDW-CoT%20achieves%20an%20average%20accuracy%20improvement%20of%2025.34%25%20on%20LLaMA2%20%2813B%29%20and%0A15.72%25%20on%20LLaMA3%20%288B%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12226v1&entry.124074799=Read"},
{"title": "Sublinear Variational Optimization of Gaussian Mixture Models with\n  Millions to Billions of Parameters", "author": "Sebastian Salwig and Till Kahlke and Florian Hirschberger and Dennis Forster and J\u00f6rg L\u00fccke", "abstract": "  Gaussian Mixture Models (GMMs) range among the most frequently used machine\nlearning models. However, training large, general GMMs becomes computationally\nprohibitive for datasets with many data points $N$ of high-dimensionality $D$.\nFor GMMs with arbitrary covariances, we here derive a highly efficient\nvariational approximation, which is integrated with mixtures of factor\nanalyzers (MFAs). For GMMs with $C$ components, our proposed algorithm\nsignificantly reduces runtime complexity per iteration from\n$\\mathcal{O}(NCD^2)$ to a complexity scaling linearly with $D$ and remaining\nconstant w.r.t. $C$. Numerical validation of this theoretical complexity\nreduction then shows the following: the distance evaluations required for the\nentire GMM optimization process scale sublinearly with $NC$. On large-scale\nbenchmarks, this sublinearity results in speed-ups of an order-of-magnitude\ncompared to the state-of-the-art. As a proof of concept, we train GMMs with\nover 10 billion parameters on about 100 million images, and observe training\ntimes of approximately nine hours on a single state-of-the-art CPU.\n", "link": "http://arxiv.org/abs/2501.12299v1", "date": "2025-01-21", "relevancy": 2.3644, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4737}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4737}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sublinear%20Variational%20Optimization%20of%20Gaussian%20Mixture%20Models%20with%0A%20%20Millions%20to%20Billions%20of%20Parameters&body=Title%3A%20Sublinear%20Variational%20Optimization%20of%20Gaussian%20Mixture%20Models%20with%0A%20%20Millions%20to%20Billions%20of%20Parameters%0AAuthor%3A%20Sebastian%20Salwig%20and%20Till%20Kahlke%20and%20Florian%20Hirschberger%20and%20Dennis%20Forster%20and%20J%C3%B6rg%20L%C3%BCcke%0AAbstract%3A%20%20%20Gaussian%20Mixture%20Models%20%28GMMs%29%20range%20among%20the%20most%20frequently%20used%20machine%0Alearning%20models.%20However%2C%20training%20large%2C%20general%20GMMs%20becomes%20computationally%0Aprohibitive%20for%20datasets%20with%20many%20data%20points%20%24N%24%20of%20high-dimensionality%20%24D%24.%0AFor%20GMMs%20with%20arbitrary%20covariances%2C%20we%20here%20derive%20a%20highly%20efficient%0Avariational%20approximation%2C%20which%20is%20integrated%20with%20mixtures%20of%20factor%0Aanalyzers%20%28MFAs%29.%20For%20GMMs%20with%20%24C%24%20components%2C%20our%20proposed%20algorithm%0Asignificantly%20reduces%20runtime%20complexity%20per%20iteration%20from%0A%24%5Cmathcal%7BO%7D%28NCD%5E2%29%24%20to%20a%20complexity%20scaling%20linearly%20with%20%24D%24%20and%20remaining%0Aconstant%20w.r.t.%20%24C%24.%20Numerical%20validation%20of%20this%20theoretical%20complexity%0Areduction%20then%20shows%20the%20following%3A%20the%20distance%20evaluations%20required%20for%20the%0Aentire%20GMM%20optimization%20process%20scale%20sublinearly%20with%20%24NC%24.%20On%20large-scale%0Abenchmarks%2C%20this%20sublinearity%20results%20in%20speed-ups%20of%20an%20order-of-magnitude%0Acompared%20to%20the%20state-of-the-art.%20As%20a%20proof%20of%20concept%2C%20we%20train%20GMMs%20with%0Aover%2010%20billion%20parameters%20on%20about%20100%20million%20images%2C%20and%20observe%20training%0Atimes%20of%20approximately%20nine%20hours%20on%20a%20single%20state-of-the-art%20CPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSublinear%2520Variational%2520Optimization%2520of%2520Gaussian%2520Mixture%2520Models%2520with%250A%2520%2520Millions%2520to%2520Billions%2520of%2520Parameters%26entry.906535625%3DSebastian%2520Salwig%2520and%2520Till%2520Kahlke%2520and%2520Florian%2520Hirschberger%2520and%2520Dennis%2520Forster%2520and%2520J%25C3%25B6rg%2520L%25C3%25BCcke%26entry.1292438233%3D%2520%2520Gaussian%2520Mixture%2520Models%2520%2528GMMs%2529%2520range%2520among%2520the%2520most%2520frequently%2520used%2520machine%250Alearning%2520models.%2520However%252C%2520training%2520large%252C%2520general%2520GMMs%2520becomes%2520computationally%250Aprohibitive%2520for%2520datasets%2520with%2520many%2520data%2520points%2520%2524N%2524%2520of%2520high-dimensionality%2520%2524D%2524.%250AFor%2520GMMs%2520with%2520arbitrary%2520covariances%252C%2520we%2520here%2520derive%2520a%2520highly%2520efficient%250Avariational%2520approximation%252C%2520which%2520is%2520integrated%2520with%2520mixtures%2520of%2520factor%250Aanalyzers%2520%2528MFAs%2529.%2520For%2520GMMs%2520with%2520%2524C%2524%2520components%252C%2520our%2520proposed%2520algorithm%250Asignificantly%2520reduces%2520runtime%2520complexity%2520per%2520iteration%2520from%250A%2524%255Cmathcal%257BO%257D%2528NCD%255E2%2529%2524%2520to%2520a%2520complexity%2520scaling%2520linearly%2520with%2520%2524D%2524%2520and%2520remaining%250Aconstant%2520w.r.t.%2520%2524C%2524.%2520Numerical%2520validation%2520of%2520this%2520theoretical%2520complexity%250Areduction%2520then%2520shows%2520the%2520following%253A%2520the%2520distance%2520evaluations%2520required%2520for%2520the%250Aentire%2520GMM%2520optimization%2520process%2520scale%2520sublinearly%2520with%2520%2524NC%2524.%2520On%2520large-scale%250Abenchmarks%252C%2520this%2520sublinearity%2520results%2520in%2520speed-ups%2520of%2520an%2520order-of-magnitude%250Acompared%2520to%2520the%2520state-of-the-art.%2520As%2520a%2520proof%2520of%2520concept%252C%2520we%2520train%2520GMMs%2520with%250Aover%252010%2520billion%2520parameters%2520on%2520about%2520100%2520million%2520images%252C%2520and%2520observe%2520training%250Atimes%2520of%2520approximately%2520nine%2520hours%2520on%2520a%2520single%2520state-of-the-art%2520CPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sublinear%20Variational%20Optimization%20of%20Gaussian%20Mixture%20Models%20with%0A%20%20Millions%20to%20Billions%20of%20Parameters&entry.906535625=Sebastian%20Salwig%20and%20Till%20Kahlke%20and%20Florian%20Hirschberger%20and%20Dennis%20Forster%20and%20J%C3%B6rg%20L%C3%BCcke&entry.1292438233=%20%20Gaussian%20Mixture%20Models%20%28GMMs%29%20range%20among%20the%20most%20frequently%20used%20machine%0Alearning%20models.%20However%2C%20training%20large%2C%20general%20GMMs%20becomes%20computationally%0Aprohibitive%20for%20datasets%20with%20many%20data%20points%20%24N%24%20of%20high-dimensionality%20%24D%24.%0AFor%20GMMs%20with%20arbitrary%20covariances%2C%20we%20here%20derive%20a%20highly%20efficient%0Avariational%20approximation%2C%20which%20is%20integrated%20with%20mixtures%20of%20factor%0Aanalyzers%20%28MFAs%29.%20For%20GMMs%20with%20%24C%24%20components%2C%20our%20proposed%20algorithm%0Asignificantly%20reduces%20runtime%20complexity%20per%20iteration%20from%0A%24%5Cmathcal%7BO%7D%28NCD%5E2%29%24%20to%20a%20complexity%20scaling%20linearly%20with%20%24D%24%20and%20remaining%0Aconstant%20w.r.t.%20%24C%24.%20Numerical%20validation%20of%20this%20theoretical%20complexity%0Areduction%20then%20shows%20the%20following%3A%20the%20distance%20evaluations%20required%20for%20the%0Aentire%20GMM%20optimization%20process%20scale%20sublinearly%20with%20%24NC%24.%20On%20large-scale%0Abenchmarks%2C%20this%20sublinearity%20results%20in%20speed-ups%20of%20an%20order-of-magnitude%0Acompared%20to%20the%20state-of-the-art.%20As%20a%20proof%20of%20concept%2C%20we%20train%20GMMs%20with%0Aover%2010%20billion%20parameters%20on%20about%20100%20million%20images%2C%20and%20observe%20training%0Atimes%20of%20approximately%20nine%20hours%20on%20a%20single%20state-of-the-art%20CPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12299v1&entry.124074799=Read"},
{"title": "Towards Affordance-Aware Articulation Synthesis for Rigged Objects", "author": "Yu-Chu Yu and Chieh Hubert Lin and Hsin-Ying Lee and Chaoyang Wang and Yu-Chiang Frank Wang and Ming-Hsuan Yang", "abstract": "  Rigged objects are commonly used in artist pipelines, as they can flexibly\nadapt to different scenes and postures. However, articulating the rigs into\nrealistic affordance-aware postures (e.g., following the context, respecting\nthe physics and the personalities of the object) remains time-consuming and\nheavily relies on human labor from experienced artists. In this paper, we\ntackle the novel problem and design A3Syn. With a given context, such as the\nenvironment mesh and a text prompt of the desired posture, A3Syn synthesizes\narticulation parameters for arbitrary and open-domain rigged objects obtained\nfrom the Internet. The task is incredibly challenging due to the lack of\ntraining data, and we do not make any topological assumptions about the\nopen-domain rigs. We propose using 2D inpainting diffusion model and several\ncontrol techniques to synthesize in-context affordance information. Then, we\ndevelop an efficient bone correspondence alignment using a combination of\ndifferentiable rendering and semantic correspondence. A3Syn has stable\nconvergence, completes in minutes, and synthesizes plausible affordance on\ndifferent combinations of in-the-wild object rigs and scenes.\n", "link": "http://arxiv.org/abs/2501.12393v1", "date": "2025-01-21", "relevancy": 2.3422, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6554}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5357}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Affordance-Aware%20Articulation%20Synthesis%20for%20Rigged%20Objects&body=Title%3A%20Towards%20Affordance-Aware%20Articulation%20Synthesis%20for%20Rigged%20Objects%0AAuthor%3A%20Yu-Chu%20Yu%20and%20Chieh%20Hubert%20Lin%20and%20Hsin-Ying%20Lee%20and%20Chaoyang%20Wang%20and%20Yu-Chiang%20Frank%20Wang%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Rigged%20objects%20are%20commonly%20used%20in%20artist%20pipelines%2C%20as%20they%20can%20flexibly%0Aadapt%20to%20different%20scenes%20and%20postures.%20However%2C%20articulating%20the%20rigs%20into%0Arealistic%20affordance-aware%20postures%20%28e.g.%2C%20following%20the%20context%2C%20respecting%0Athe%20physics%20and%20the%20personalities%20of%20the%20object%29%20remains%20time-consuming%20and%0Aheavily%20relies%20on%20human%20labor%20from%20experienced%20artists.%20In%20this%20paper%2C%20we%0Atackle%20the%20novel%20problem%20and%20design%20A3Syn.%20With%20a%20given%20context%2C%20such%20as%20the%0Aenvironment%20mesh%20and%20a%20text%20prompt%20of%20the%20desired%20posture%2C%20A3Syn%20synthesizes%0Aarticulation%20parameters%20for%20arbitrary%20and%20open-domain%20rigged%20objects%20obtained%0Afrom%20the%20Internet.%20The%20task%20is%20incredibly%20challenging%20due%20to%20the%20lack%20of%0Atraining%20data%2C%20and%20we%20do%20not%20make%20any%20topological%20assumptions%20about%20the%0Aopen-domain%20rigs.%20We%20propose%20using%202D%20inpainting%20diffusion%20model%20and%20several%0Acontrol%20techniques%20to%20synthesize%20in-context%20affordance%20information.%20Then%2C%20we%0Adevelop%20an%20efficient%20bone%20correspondence%20alignment%20using%20a%20combination%20of%0Adifferentiable%20rendering%20and%20semantic%20correspondence.%20A3Syn%20has%20stable%0Aconvergence%2C%20completes%20in%20minutes%2C%20and%20synthesizes%20plausible%20affordance%20on%0Adifferent%20combinations%20of%20in-the-wild%20object%20rigs%20and%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Affordance-Aware%2520Articulation%2520Synthesis%2520for%2520Rigged%2520Objects%26entry.906535625%3DYu-Chu%2520Yu%2520and%2520Chieh%2520Hubert%2520Lin%2520and%2520Hsin-Ying%2520Lee%2520and%2520Chaoyang%2520Wang%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Rigged%2520objects%2520are%2520commonly%2520used%2520in%2520artist%2520pipelines%252C%2520as%2520they%2520can%2520flexibly%250Aadapt%2520to%2520different%2520scenes%2520and%2520postures.%2520However%252C%2520articulating%2520the%2520rigs%2520into%250Arealistic%2520affordance-aware%2520postures%2520%2528e.g.%252C%2520following%2520the%2520context%252C%2520respecting%250Athe%2520physics%2520and%2520the%2520personalities%2520of%2520the%2520object%2529%2520remains%2520time-consuming%2520and%250Aheavily%2520relies%2520on%2520human%2520labor%2520from%2520experienced%2520artists.%2520In%2520this%2520paper%252C%2520we%250Atackle%2520the%2520novel%2520problem%2520and%2520design%2520A3Syn.%2520With%2520a%2520given%2520context%252C%2520such%2520as%2520the%250Aenvironment%2520mesh%2520and%2520a%2520text%2520prompt%2520of%2520the%2520desired%2520posture%252C%2520A3Syn%2520synthesizes%250Aarticulation%2520parameters%2520for%2520arbitrary%2520and%2520open-domain%2520rigged%2520objects%2520obtained%250Afrom%2520the%2520Internet.%2520The%2520task%2520is%2520incredibly%2520challenging%2520due%2520to%2520the%2520lack%2520of%250Atraining%2520data%252C%2520and%2520we%2520do%2520not%2520make%2520any%2520topological%2520assumptions%2520about%2520the%250Aopen-domain%2520rigs.%2520We%2520propose%2520using%25202D%2520inpainting%2520diffusion%2520model%2520and%2520several%250Acontrol%2520techniques%2520to%2520synthesize%2520in-context%2520affordance%2520information.%2520Then%252C%2520we%250Adevelop%2520an%2520efficient%2520bone%2520correspondence%2520alignment%2520using%2520a%2520combination%2520of%250Adifferentiable%2520rendering%2520and%2520semantic%2520correspondence.%2520A3Syn%2520has%2520stable%250Aconvergence%252C%2520completes%2520in%2520minutes%252C%2520and%2520synthesizes%2520plausible%2520affordance%2520on%250Adifferent%2520combinations%2520of%2520in-the-wild%2520object%2520rigs%2520and%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Affordance-Aware%20Articulation%20Synthesis%20for%20Rigged%20Objects&entry.906535625=Yu-Chu%20Yu%20and%20Chieh%20Hubert%20Lin%20and%20Hsin-Ying%20Lee%20and%20Chaoyang%20Wang%20and%20Yu-Chiang%20Frank%20Wang%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Rigged%20objects%20are%20commonly%20used%20in%20artist%20pipelines%2C%20as%20they%20can%20flexibly%0Aadapt%20to%20different%20scenes%20and%20postures.%20However%2C%20articulating%20the%20rigs%20into%0Arealistic%20affordance-aware%20postures%20%28e.g.%2C%20following%20the%20context%2C%20respecting%0Athe%20physics%20and%20the%20personalities%20of%20the%20object%29%20remains%20time-consuming%20and%0Aheavily%20relies%20on%20human%20labor%20from%20experienced%20artists.%20In%20this%20paper%2C%20we%0Atackle%20the%20novel%20problem%20and%20design%20A3Syn.%20With%20a%20given%20context%2C%20such%20as%20the%0Aenvironment%20mesh%20and%20a%20text%20prompt%20of%20the%20desired%20posture%2C%20A3Syn%20synthesizes%0Aarticulation%20parameters%20for%20arbitrary%20and%20open-domain%20rigged%20objects%20obtained%0Afrom%20the%20Internet.%20The%20task%20is%20incredibly%20challenging%20due%20to%20the%20lack%20of%0Atraining%20data%2C%20and%20we%20do%20not%20make%20any%20topological%20assumptions%20about%20the%0Aopen-domain%20rigs.%20We%20propose%20using%202D%20inpainting%20diffusion%20model%20and%20several%0Acontrol%20techniques%20to%20synthesize%20in-context%20affordance%20information.%20Then%2C%20we%0Adevelop%20an%20efficient%20bone%20correspondence%20alignment%20using%20a%20combination%20of%0Adifferentiable%20rendering%20and%20semantic%20correspondence.%20A3Syn%20has%20stable%0Aconvergence%2C%20completes%20in%20minutes%2C%20and%20synthesizes%20plausible%20affordance%20on%0Adifferent%20combinations%20of%20in-the-wild%20object%20rigs%20and%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12393v1&entry.124074799=Read"},
{"title": "RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with\n  Retrieval-Augmented Learning", "author": "Jiacheng Zuo and Haibo Hu and Zikang Zhou and Yufei Cui and Ziquan Liu and Jianping Wang and Nan Guan and Jin Wang and Chun Jason Xue", "abstract": "  In the pursuit of robust autonomous driving systems, models trained on\nreal-world datasets often struggle to adapt to new environments, particularly\nwhen confronted with corner cases such as extreme weather conditions.\nCollecting these corner cases in the real world is non-trivial, which\nnecessitates the use of simulators for validation. However,the high\ncomputational cost and the domain gap in data distribution have hindered the\nseamless transition between real and simulated driving scenarios. To tackle\nthis challenge, we propose Retrieval-Augmented Learning for Autonomous Driving\n(RALAD), a novel framework designed to bridge the real-to-sim gap at a low\ncost. RALAD features three primary designs, including (1) domain adaptation via\nan enhanced Optimal Transport (OT) method that accounts for both individual and\ngrouped image distances, (2) a simple and unified framework that can be applied\nto various models, and (3) efficient fine-tuning techniques that freeze the\ncomputationally expensive layers while maintaining robustness. Experimental\nresults demonstrate that RALAD compensates for the performance degradation in\nsimulated environments while maintaining accuracy in real-world scenarios\nacross three different models. Taking Cross View as an example, the mIOU and\nmAP metrics in real-world scenarios remain stable before and after RALAD\nfine-tuning, while in simulated environments,the mIOU and mAP metrics are\nimproved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of\nour approach is reduced by approximately 88.1%. Our code is available at\nhttps://github.com/JiachengZuo/RALAD.git.\n", "link": "http://arxiv.org/abs/2501.12296v1", "date": "2025-01-21", "relevancy": 2.2953, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5773}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5746}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RALAD%3A%20Bridging%20the%20Real-to-Sim%20Domain%20Gap%20in%20Autonomous%20Driving%20with%0A%20%20Retrieval-Augmented%20Learning&body=Title%3A%20RALAD%3A%20Bridging%20the%20Real-to-Sim%20Domain%20Gap%20in%20Autonomous%20Driving%20with%0A%20%20Retrieval-Augmented%20Learning%0AAuthor%3A%20Jiacheng%20Zuo%20and%20Haibo%20Hu%20and%20Zikang%20Zhou%20and%20Yufei%20Cui%20and%20Ziquan%20Liu%20and%20Jianping%20Wang%20and%20Nan%20Guan%20and%20Jin%20Wang%20and%20Chun%20Jason%20Xue%0AAbstract%3A%20%20%20In%20the%20pursuit%20of%20robust%20autonomous%20driving%20systems%2C%20models%20trained%20on%0Areal-world%20datasets%20often%20struggle%20to%20adapt%20to%20new%20environments%2C%20particularly%0Awhen%20confronted%20with%20corner%20cases%20such%20as%20extreme%20weather%20conditions.%0ACollecting%20these%20corner%20cases%20in%20the%20real%20world%20is%20non-trivial%2C%20which%0Anecessitates%20the%20use%20of%20simulators%20for%20validation.%20However%2Cthe%20high%0Acomputational%20cost%20and%20the%20domain%20gap%20in%20data%20distribution%20have%20hindered%20the%0Aseamless%20transition%20between%20real%20and%20simulated%20driving%20scenarios.%20To%20tackle%0Athis%20challenge%2C%20we%20propose%20Retrieval-Augmented%20Learning%20for%20Autonomous%20Driving%0A%28RALAD%29%2C%20a%20novel%20framework%20designed%20to%20bridge%20the%20real-to-sim%20gap%20at%20a%20low%0Acost.%20RALAD%20features%20three%20primary%20designs%2C%20including%20%281%29%20domain%20adaptation%20via%0Aan%20enhanced%20Optimal%20Transport%20%28OT%29%20method%20that%20accounts%20for%20both%20individual%20and%0Agrouped%20image%20distances%2C%20%282%29%20a%20simple%20and%20unified%20framework%20that%20can%20be%20applied%0Ato%20various%20models%2C%20and%20%283%29%20efficient%20fine-tuning%20techniques%20that%20freeze%20the%0Acomputationally%20expensive%20layers%20while%20maintaining%20robustness.%20Experimental%0Aresults%20demonstrate%20that%20RALAD%20compensates%20for%20the%20performance%20degradation%20in%0Asimulated%20environments%20while%20maintaining%20accuracy%20in%20real-world%20scenarios%0Aacross%20three%20different%20models.%20Taking%20Cross%20View%20as%20an%20example%2C%20the%20mIOU%20and%0AmAP%20metrics%20in%20real-world%20scenarios%20remain%20stable%20before%20and%20after%20RALAD%0Afine-tuning%2C%20while%20in%20simulated%20environments%2Cthe%20mIOU%20and%20mAP%20metrics%20are%0Aimproved%20by%2010.30%25%20and%2012.29%25%2C%20respectively.%20Moreover%2C%20the%20re-training%20cost%20of%0Aour%20approach%20is%20reduced%20by%20approximately%2088.1%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/JiachengZuo/RALAD.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRALAD%253A%2520Bridging%2520the%2520Real-to-Sim%2520Domain%2520Gap%2520in%2520Autonomous%2520Driving%2520with%250A%2520%2520Retrieval-Augmented%2520Learning%26entry.906535625%3DJiacheng%2520Zuo%2520and%2520Haibo%2520Hu%2520and%2520Zikang%2520Zhou%2520and%2520Yufei%2520Cui%2520and%2520Ziquan%2520Liu%2520and%2520Jianping%2520Wang%2520and%2520Nan%2520Guan%2520and%2520Jin%2520Wang%2520and%2520Chun%2520Jason%2520Xue%26entry.1292438233%3D%2520%2520In%2520the%2520pursuit%2520of%2520robust%2520autonomous%2520driving%2520systems%252C%2520models%2520trained%2520on%250Areal-world%2520datasets%2520often%2520struggle%2520to%2520adapt%2520to%2520new%2520environments%252C%2520particularly%250Awhen%2520confronted%2520with%2520corner%2520cases%2520such%2520as%2520extreme%2520weather%2520conditions.%250ACollecting%2520these%2520corner%2520cases%2520in%2520the%2520real%2520world%2520is%2520non-trivial%252C%2520which%250Anecessitates%2520the%2520use%2520of%2520simulators%2520for%2520validation.%2520However%252Cthe%2520high%250Acomputational%2520cost%2520and%2520the%2520domain%2520gap%2520in%2520data%2520distribution%2520have%2520hindered%2520the%250Aseamless%2520transition%2520between%2520real%2520and%2520simulated%2520driving%2520scenarios.%2520To%2520tackle%250Athis%2520challenge%252C%2520we%2520propose%2520Retrieval-Augmented%2520Learning%2520for%2520Autonomous%2520Driving%250A%2528RALAD%2529%252C%2520a%2520novel%2520framework%2520designed%2520to%2520bridge%2520the%2520real-to-sim%2520gap%2520at%2520a%2520low%250Acost.%2520RALAD%2520features%2520three%2520primary%2520designs%252C%2520including%2520%25281%2529%2520domain%2520adaptation%2520via%250Aan%2520enhanced%2520Optimal%2520Transport%2520%2528OT%2529%2520method%2520that%2520accounts%2520for%2520both%2520individual%2520and%250Agrouped%2520image%2520distances%252C%2520%25282%2529%2520a%2520simple%2520and%2520unified%2520framework%2520that%2520can%2520be%2520applied%250Ato%2520various%2520models%252C%2520and%2520%25283%2529%2520efficient%2520fine-tuning%2520techniques%2520that%2520freeze%2520the%250Acomputationally%2520expensive%2520layers%2520while%2520maintaining%2520robustness.%2520Experimental%250Aresults%2520demonstrate%2520that%2520RALAD%2520compensates%2520for%2520the%2520performance%2520degradation%2520in%250Asimulated%2520environments%2520while%2520maintaining%2520accuracy%2520in%2520real-world%2520scenarios%250Aacross%2520three%2520different%2520models.%2520Taking%2520Cross%2520View%2520as%2520an%2520example%252C%2520the%2520mIOU%2520and%250AmAP%2520metrics%2520in%2520real-world%2520scenarios%2520remain%2520stable%2520before%2520and%2520after%2520RALAD%250Afine-tuning%252C%2520while%2520in%2520simulated%2520environments%252Cthe%2520mIOU%2520and%2520mAP%2520metrics%2520are%250Aimproved%2520by%252010.30%2525%2520and%252012.29%2525%252C%2520respectively.%2520Moreover%252C%2520the%2520re-training%2520cost%2520of%250Aour%2520approach%2520is%2520reduced%2520by%2520approximately%252088.1%2525.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/JiachengZuo/RALAD.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RALAD%3A%20Bridging%20the%20Real-to-Sim%20Domain%20Gap%20in%20Autonomous%20Driving%20with%0A%20%20Retrieval-Augmented%20Learning&entry.906535625=Jiacheng%20Zuo%20and%20Haibo%20Hu%20and%20Zikang%20Zhou%20and%20Yufei%20Cui%20and%20Ziquan%20Liu%20and%20Jianping%20Wang%20and%20Nan%20Guan%20and%20Jin%20Wang%20and%20Chun%20Jason%20Xue&entry.1292438233=%20%20In%20the%20pursuit%20of%20robust%20autonomous%20driving%20systems%2C%20models%20trained%20on%0Areal-world%20datasets%20often%20struggle%20to%20adapt%20to%20new%20environments%2C%20particularly%0Awhen%20confronted%20with%20corner%20cases%20such%20as%20extreme%20weather%20conditions.%0ACollecting%20these%20corner%20cases%20in%20the%20real%20world%20is%20non-trivial%2C%20which%0Anecessitates%20the%20use%20of%20simulators%20for%20validation.%20However%2Cthe%20high%0Acomputational%20cost%20and%20the%20domain%20gap%20in%20data%20distribution%20have%20hindered%20the%0Aseamless%20transition%20between%20real%20and%20simulated%20driving%20scenarios.%20To%20tackle%0Athis%20challenge%2C%20we%20propose%20Retrieval-Augmented%20Learning%20for%20Autonomous%20Driving%0A%28RALAD%29%2C%20a%20novel%20framework%20designed%20to%20bridge%20the%20real-to-sim%20gap%20at%20a%20low%0Acost.%20RALAD%20features%20three%20primary%20designs%2C%20including%20%281%29%20domain%20adaptation%20via%0Aan%20enhanced%20Optimal%20Transport%20%28OT%29%20method%20that%20accounts%20for%20both%20individual%20and%0Agrouped%20image%20distances%2C%20%282%29%20a%20simple%20and%20unified%20framework%20that%20can%20be%20applied%0Ato%20various%20models%2C%20and%20%283%29%20efficient%20fine-tuning%20techniques%20that%20freeze%20the%0Acomputationally%20expensive%20layers%20while%20maintaining%20robustness.%20Experimental%0Aresults%20demonstrate%20that%20RALAD%20compensates%20for%20the%20performance%20degradation%20in%0Asimulated%20environments%20while%20maintaining%20accuracy%20in%20real-world%20scenarios%0Aacross%20three%20different%20models.%20Taking%20Cross%20View%20as%20an%20example%2C%20the%20mIOU%20and%0AmAP%20metrics%20in%20real-world%20scenarios%20remain%20stable%20before%20and%20after%20RALAD%0Afine-tuning%2C%20while%20in%20simulated%20environments%2Cthe%20mIOU%20and%20mAP%20metrics%20are%0Aimproved%20by%2010.30%25%20and%2012.29%25%2C%20respectively.%20Moreover%2C%20the%20re-training%20cost%20of%0Aour%20approach%20is%20reduced%20by%20approximately%2088.1%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/JiachengZuo/RALAD.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12296v1&entry.124074799=Read"},
{"title": "Implementation of an Asymmetric Adjusted Activation Function for Class\n  Imbalance Credit Scoring", "author": "Xia Li and Hanghang Zheng and Kunpeng Tao and Mao Mao", "abstract": "  Credit scoring is a systematic approach to evaluate a borrower's probability\nof default (PD) on a bank loan. The data associated with such scenarios are\ncharacteristically imbalanced, complicating binary classification owing to the\noften-underestimated cost of misclassification during the classifier's learning\nprocess. Considering the high imbalance ratio (IR) of these datasets, we\nintroduce an innovative yet straightforward optimized activation function by\nincorporating an IR-dependent asymmetric adjusted factor embedded Sigmoid\nactivation function (ASIG). The embedding of ASIG makes the sensitive margin of\nthe Sigmoid function auto-adjustable, depending on the imbalance nature of the\ndatasets distributed, thereby giving the activation function an asymmetric\ncharacteristic that prevents the underrepresentation of the minority class\n(positive samples) during the classifier's learning process. The experimental\nresults show that the ASIG-embedded-classifier outperforms traditional\nclassifiers on datasets across wide-ranging IRs in the downstream\ncredit-scoring task. The algorithm also shows robustness and stability, even\nwhen the IR is ultra-high. Therefore, the algorithm provides a competitive\nalternative in the financial industry, especially in credit scoring, possessing\nthe ability to effectively process highly imbalanced distribution data.\n", "link": "http://arxiv.org/abs/2501.12285v1", "date": "2025-01-21", "relevancy": 2.2885, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4896}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4527}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implementation%20of%20an%20Asymmetric%20Adjusted%20Activation%20Function%20for%20Class%0A%20%20Imbalance%20Credit%20Scoring&body=Title%3A%20Implementation%20of%20an%20Asymmetric%20Adjusted%20Activation%20Function%20for%20Class%0A%20%20Imbalance%20Credit%20Scoring%0AAuthor%3A%20Xia%20Li%20and%20Hanghang%20Zheng%20and%20Kunpeng%20Tao%20and%20Mao%20Mao%0AAbstract%3A%20%20%20Credit%20scoring%20is%20a%20systematic%20approach%20to%20evaluate%20a%20borrower%27s%20probability%0Aof%20default%20%28PD%29%20on%20a%20bank%20loan.%20The%20data%20associated%20with%20such%20scenarios%20are%0Acharacteristically%20imbalanced%2C%20complicating%20binary%20classification%20owing%20to%20the%0Aoften-underestimated%20cost%20of%20misclassification%20during%20the%20classifier%27s%20learning%0Aprocess.%20Considering%20the%20high%20imbalance%20ratio%20%28IR%29%20of%20these%20datasets%2C%20we%0Aintroduce%20an%20innovative%20yet%20straightforward%20optimized%20activation%20function%20by%0Aincorporating%20an%20IR-dependent%20asymmetric%20adjusted%20factor%20embedded%20Sigmoid%0Aactivation%20function%20%28ASIG%29.%20The%20embedding%20of%20ASIG%20makes%20the%20sensitive%20margin%20of%0Athe%20Sigmoid%20function%20auto-adjustable%2C%20depending%20on%20the%20imbalance%20nature%20of%20the%0Adatasets%20distributed%2C%20thereby%20giving%20the%20activation%20function%20an%20asymmetric%0Acharacteristic%20that%20prevents%20the%20underrepresentation%20of%20the%20minority%20class%0A%28positive%20samples%29%20during%20the%20classifier%27s%20learning%20process.%20The%20experimental%0Aresults%20show%20that%20the%20ASIG-embedded-classifier%20outperforms%20traditional%0Aclassifiers%20on%20datasets%20across%20wide-ranging%20IRs%20in%20the%20downstream%0Acredit-scoring%20task.%20The%20algorithm%20also%20shows%20robustness%20and%20stability%2C%20even%0Awhen%20the%20IR%20is%20ultra-high.%20Therefore%2C%20the%20algorithm%20provides%20a%20competitive%0Aalternative%20in%20the%20financial%20industry%2C%20especially%20in%20credit%20scoring%2C%20possessing%0Athe%20ability%20to%20effectively%20process%20highly%20imbalanced%20distribution%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplementation%2520of%2520an%2520Asymmetric%2520Adjusted%2520Activation%2520Function%2520for%2520Class%250A%2520%2520Imbalance%2520Credit%2520Scoring%26entry.906535625%3DXia%2520Li%2520and%2520Hanghang%2520Zheng%2520and%2520Kunpeng%2520Tao%2520and%2520Mao%2520Mao%26entry.1292438233%3D%2520%2520Credit%2520scoring%2520is%2520a%2520systematic%2520approach%2520to%2520evaluate%2520a%2520borrower%2527s%2520probability%250Aof%2520default%2520%2528PD%2529%2520on%2520a%2520bank%2520loan.%2520The%2520data%2520associated%2520with%2520such%2520scenarios%2520are%250Acharacteristically%2520imbalanced%252C%2520complicating%2520binary%2520classification%2520owing%2520to%2520the%250Aoften-underestimated%2520cost%2520of%2520misclassification%2520during%2520the%2520classifier%2527s%2520learning%250Aprocess.%2520Considering%2520the%2520high%2520imbalance%2520ratio%2520%2528IR%2529%2520of%2520these%2520datasets%252C%2520we%250Aintroduce%2520an%2520innovative%2520yet%2520straightforward%2520optimized%2520activation%2520function%2520by%250Aincorporating%2520an%2520IR-dependent%2520asymmetric%2520adjusted%2520factor%2520embedded%2520Sigmoid%250Aactivation%2520function%2520%2528ASIG%2529.%2520The%2520embedding%2520of%2520ASIG%2520makes%2520the%2520sensitive%2520margin%2520of%250Athe%2520Sigmoid%2520function%2520auto-adjustable%252C%2520depending%2520on%2520the%2520imbalance%2520nature%2520of%2520the%250Adatasets%2520distributed%252C%2520thereby%2520giving%2520the%2520activation%2520function%2520an%2520asymmetric%250Acharacteristic%2520that%2520prevents%2520the%2520underrepresentation%2520of%2520the%2520minority%2520class%250A%2528positive%2520samples%2529%2520during%2520the%2520classifier%2527s%2520learning%2520process.%2520The%2520experimental%250Aresults%2520show%2520that%2520the%2520ASIG-embedded-classifier%2520outperforms%2520traditional%250Aclassifiers%2520on%2520datasets%2520across%2520wide-ranging%2520IRs%2520in%2520the%2520downstream%250Acredit-scoring%2520task.%2520The%2520algorithm%2520also%2520shows%2520robustness%2520and%2520stability%252C%2520even%250Awhen%2520the%2520IR%2520is%2520ultra-high.%2520Therefore%252C%2520the%2520algorithm%2520provides%2520a%2520competitive%250Aalternative%2520in%2520the%2520financial%2520industry%252C%2520especially%2520in%2520credit%2520scoring%252C%2520possessing%250Athe%2520ability%2520to%2520effectively%2520process%2520highly%2520imbalanced%2520distribution%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implementation%20of%20an%20Asymmetric%20Adjusted%20Activation%20Function%20for%20Class%0A%20%20Imbalance%20Credit%20Scoring&entry.906535625=Xia%20Li%20and%20Hanghang%20Zheng%20and%20Kunpeng%20Tao%20and%20Mao%20Mao&entry.1292438233=%20%20Credit%20scoring%20is%20a%20systematic%20approach%20to%20evaluate%20a%20borrower%27s%20probability%0Aof%20default%20%28PD%29%20on%20a%20bank%20loan.%20The%20data%20associated%20with%20such%20scenarios%20are%0Acharacteristically%20imbalanced%2C%20complicating%20binary%20classification%20owing%20to%20the%0Aoften-underestimated%20cost%20of%20misclassification%20during%20the%20classifier%27s%20learning%0Aprocess.%20Considering%20the%20high%20imbalance%20ratio%20%28IR%29%20of%20these%20datasets%2C%20we%0Aintroduce%20an%20innovative%20yet%20straightforward%20optimized%20activation%20function%20by%0Aincorporating%20an%20IR-dependent%20asymmetric%20adjusted%20factor%20embedded%20Sigmoid%0Aactivation%20function%20%28ASIG%29.%20The%20embedding%20of%20ASIG%20makes%20the%20sensitive%20margin%20of%0Athe%20Sigmoid%20function%20auto-adjustable%2C%20depending%20on%20the%20imbalance%20nature%20of%20the%0Adatasets%20distributed%2C%20thereby%20giving%20the%20activation%20function%20an%20asymmetric%0Acharacteristic%20that%20prevents%20the%20underrepresentation%20of%20the%20minority%20class%0A%28positive%20samples%29%20during%20the%20classifier%27s%20learning%20process.%20The%20experimental%0Aresults%20show%20that%20the%20ASIG-embedded-classifier%20outperforms%20traditional%0Aclassifiers%20on%20datasets%20across%20wide-ranging%20IRs%20in%20the%20downstream%0Acredit-scoring%20task.%20The%20algorithm%20also%20shows%20robustness%20and%20stability%2C%20even%0Awhen%20the%20IR%20is%20ultra-high.%20Therefore%2C%20the%20algorithm%20provides%20a%20competitive%0Aalternative%20in%20the%20financial%20industry%2C%20especially%20in%20credit%20scoring%2C%20possessing%0Athe%20ability%20to%20effectively%20process%20highly%20imbalanced%20distribution%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12285v1&entry.124074799=Read"},
{"title": "High-dimensional multimodal uncertainty estimation by manifold\n  alignment:Application to 3D right ventricular strain computations", "author": "Maxime Di Folco and Gabriel Bernardino and Patrick Clarysse and Nicolas Duchateau", "abstract": "  Confidence in the results is a key ingredient to improve the adoption of\nmachine learning methods by clinicians. Uncertainties on the results have been\nconsidered in the literature, but mostly those originating from the learning\nand processing methods. Uncertainty on the data is hardly challenged, as a\nsingle sample is often considered representative enough of each subject\nincluded in the analysis. In this paper, we propose a representation learning\nstrategy to estimate local uncertainties on a physiological descriptor (here,\nmyocardial deformation) previously obtained from medical images by different\ndefinitions or computations. We first use manifold alignment to match the\nlatent representations associated to different high-dimensional input\ndescriptors. Then, we formulate plausible distributions of latent\nuncertainties, and finally exploit them to reconstruct uncertainties on the\ninput high-dimensional descriptors. We demonstrate its relevance for the\nquantification of myocardial deformation (strain) from 3D echocardiographic\nimage sequences of the right ventricle, for which a lack of consensus exists in\nits definition and which directional component to use. We used a database of\n100 control subjects with right ventricle overload, for which different types\nof strain are available at each point of the right ventricle endocardial\nsurface mesh. Our approach quantifies local uncertainties on myocardial\ndeformation from different descriptors defining this physiological concept.\nSuch uncertainties cannot be directly estimated by local statistics on such\ndescriptors, potentially of heterogeneous types. Beyond this controlled\nillustrative application, our methodology has the potential to be generalized\nto many other population analyses considering heterogeneous high-dimensional\ndescriptors.\n", "link": "http://arxiv.org/abs/2501.12178v1", "date": "2025-01-21", "relevancy": 2.2667, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6035}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5655}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-dimensional%20multimodal%20uncertainty%20estimation%20by%20manifold%0A%20%20alignment%3AApplication%20to%203D%20right%20ventricular%20strain%20computations&body=Title%3A%20High-dimensional%20multimodal%20uncertainty%20estimation%20by%20manifold%0A%20%20alignment%3AApplication%20to%203D%20right%20ventricular%20strain%20computations%0AAuthor%3A%20Maxime%20Di%20Folco%20and%20Gabriel%20Bernardino%20and%20Patrick%20Clarysse%20and%20Nicolas%20Duchateau%0AAbstract%3A%20%20%20Confidence%20in%20the%20results%20is%20a%20key%20ingredient%20to%20improve%20the%20adoption%20of%0Amachine%20learning%20methods%20by%20clinicians.%20Uncertainties%20on%20the%20results%20have%20been%0Aconsidered%20in%20the%20literature%2C%20but%20mostly%20those%20originating%20from%20the%20learning%0Aand%20processing%20methods.%20Uncertainty%20on%20the%20data%20is%20hardly%20challenged%2C%20as%20a%0Asingle%20sample%20is%20often%20considered%20representative%20enough%20of%20each%20subject%0Aincluded%20in%20the%20analysis.%20In%20this%20paper%2C%20we%20propose%20a%20representation%20learning%0Astrategy%20to%20estimate%20local%20uncertainties%20on%20a%20physiological%20descriptor%20%28here%2C%0Amyocardial%20deformation%29%20previously%20obtained%20from%20medical%20images%20by%20different%0Adefinitions%20or%20computations.%20We%20first%20use%20manifold%20alignment%20to%20match%20the%0Alatent%20representations%20associated%20to%20different%20high-dimensional%20input%0Adescriptors.%20Then%2C%20we%20formulate%20plausible%20distributions%20of%20latent%0Auncertainties%2C%20and%20finally%20exploit%20them%20to%20reconstruct%20uncertainties%20on%20the%0Ainput%20high-dimensional%20descriptors.%20We%20demonstrate%20its%20relevance%20for%20the%0Aquantification%20of%20myocardial%20deformation%20%28strain%29%20from%203D%20echocardiographic%0Aimage%20sequences%20of%20the%20right%20ventricle%2C%20for%20which%20a%20lack%20of%20consensus%20exists%20in%0Aits%20definition%20and%20which%20directional%20component%20to%20use.%20We%20used%20a%20database%20of%0A100%20control%20subjects%20with%20right%20ventricle%20overload%2C%20for%20which%20different%20types%0Aof%20strain%20are%20available%20at%20each%20point%20of%20the%20right%20ventricle%20endocardial%0Asurface%20mesh.%20Our%20approach%20quantifies%20local%20uncertainties%20on%20myocardial%0Adeformation%20from%20different%20descriptors%20defining%20this%20physiological%20concept.%0ASuch%20uncertainties%20cannot%20be%20directly%20estimated%20by%20local%20statistics%20on%20such%0Adescriptors%2C%20potentially%20of%20heterogeneous%20types.%20Beyond%20this%20controlled%0Aillustrative%20application%2C%20our%20methodology%20has%20the%20potential%20to%20be%20generalized%0Ato%20many%20other%20population%20analyses%20considering%20heterogeneous%20high-dimensional%0Adescriptors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-dimensional%2520multimodal%2520uncertainty%2520estimation%2520by%2520manifold%250A%2520%2520alignment%253AApplication%2520to%25203D%2520right%2520ventricular%2520strain%2520computations%26entry.906535625%3DMaxime%2520Di%2520Folco%2520and%2520Gabriel%2520Bernardino%2520and%2520Patrick%2520Clarysse%2520and%2520Nicolas%2520Duchateau%26entry.1292438233%3D%2520%2520Confidence%2520in%2520the%2520results%2520is%2520a%2520key%2520ingredient%2520to%2520improve%2520the%2520adoption%2520of%250Amachine%2520learning%2520methods%2520by%2520clinicians.%2520Uncertainties%2520on%2520the%2520results%2520have%2520been%250Aconsidered%2520in%2520the%2520literature%252C%2520but%2520mostly%2520those%2520originating%2520from%2520the%2520learning%250Aand%2520processing%2520methods.%2520Uncertainty%2520on%2520the%2520data%2520is%2520hardly%2520challenged%252C%2520as%2520a%250Asingle%2520sample%2520is%2520often%2520considered%2520representative%2520enough%2520of%2520each%2520subject%250Aincluded%2520in%2520the%2520analysis.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520representation%2520learning%250Astrategy%2520to%2520estimate%2520local%2520uncertainties%2520on%2520a%2520physiological%2520descriptor%2520%2528here%252C%250Amyocardial%2520deformation%2529%2520previously%2520obtained%2520from%2520medical%2520images%2520by%2520different%250Adefinitions%2520or%2520computations.%2520We%2520first%2520use%2520manifold%2520alignment%2520to%2520match%2520the%250Alatent%2520representations%2520associated%2520to%2520different%2520high-dimensional%2520input%250Adescriptors.%2520Then%252C%2520we%2520formulate%2520plausible%2520distributions%2520of%2520latent%250Auncertainties%252C%2520and%2520finally%2520exploit%2520them%2520to%2520reconstruct%2520uncertainties%2520on%2520the%250Ainput%2520high-dimensional%2520descriptors.%2520We%2520demonstrate%2520its%2520relevance%2520for%2520the%250Aquantification%2520of%2520myocardial%2520deformation%2520%2528strain%2529%2520from%25203D%2520echocardiographic%250Aimage%2520sequences%2520of%2520the%2520right%2520ventricle%252C%2520for%2520which%2520a%2520lack%2520of%2520consensus%2520exists%2520in%250Aits%2520definition%2520and%2520which%2520directional%2520component%2520to%2520use.%2520We%2520used%2520a%2520database%2520of%250A100%2520control%2520subjects%2520with%2520right%2520ventricle%2520overload%252C%2520for%2520which%2520different%2520types%250Aof%2520strain%2520are%2520available%2520at%2520each%2520point%2520of%2520the%2520right%2520ventricle%2520endocardial%250Asurface%2520mesh.%2520Our%2520approach%2520quantifies%2520local%2520uncertainties%2520on%2520myocardial%250Adeformation%2520from%2520different%2520descriptors%2520defining%2520this%2520physiological%2520concept.%250ASuch%2520uncertainties%2520cannot%2520be%2520directly%2520estimated%2520by%2520local%2520statistics%2520on%2520such%250Adescriptors%252C%2520potentially%2520of%2520heterogeneous%2520types.%2520Beyond%2520this%2520controlled%250Aillustrative%2520application%252C%2520our%2520methodology%2520has%2520the%2520potential%2520to%2520be%2520generalized%250Ato%2520many%2520other%2520population%2520analyses%2520considering%2520heterogeneous%2520high-dimensional%250Adescriptors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-dimensional%20multimodal%20uncertainty%20estimation%20by%20manifold%0A%20%20alignment%3AApplication%20to%203D%20right%20ventricular%20strain%20computations&entry.906535625=Maxime%20Di%20Folco%20and%20Gabriel%20Bernardino%20and%20Patrick%20Clarysse%20and%20Nicolas%20Duchateau&entry.1292438233=%20%20Confidence%20in%20the%20results%20is%20a%20key%20ingredient%20to%20improve%20the%20adoption%20of%0Amachine%20learning%20methods%20by%20clinicians.%20Uncertainties%20on%20the%20results%20have%20been%0Aconsidered%20in%20the%20literature%2C%20but%20mostly%20those%20originating%20from%20the%20learning%0Aand%20processing%20methods.%20Uncertainty%20on%20the%20data%20is%20hardly%20challenged%2C%20as%20a%0Asingle%20sample%20is%20often%20considered%20representative%20enough%20of%20each%20subject%0Aincluded%20in%20the%20analysis.%20In%20this%20paper%2C%20we%20propose%20a%20representation%20learning%0Astrategy%20to%20estimate%20local%20uncertainties%20on%20a%20physiological%20descriptor%20%28here%2C%0Amyocardial%20deformation%29%20previously%20obtained%20from%20medical%20images%20by%20different%0Adefinitions%20or%20computations.%20We%20first%20use%20manifold%20alignment%20to%20match%20the%0Alatent%20representations%20associated%20to%20different%20high-dimensional%20input%0Adescriptors.%20Then%2C%20we%20formulate%20plausible%20distributions%20of%20latent%0Auncertainties%2C%20and%20finally%20exploit%20them%20to%20reconstruct%20uncertainties%20on%20the%0Ainput%20high-dimensional%20descriptors.%20We%20demonstrate%20its%20relevance%20for%20the%0Aquantification%20of%20myocardial%20deformation%20%28strain%29%20from%203D%20echocardiographic%0Aimage%20sequences%20of%20the%20right%20ventricle%2C%20for%20which%20a%20lack%20of%20consensus%20exists%20in%0Aits%20definition%20and%20which%20directional%20component%20to%20use.%20We%20used%20a%20database%20of%0A100%20control%20subjects%20with%20right%20ventricle%20overload%2C%20for%20which%20different%20types%0Aof%20strain%20are%20available%20at%20each%20point%20of%20the%20right%20ventricle%20endocardial%0Asurface%20mesh.%20Our%20approach%20quantifies%20local%20uncertainties%20on%20myocardial%0Adeformation%20from%20different%20descriptors%20defining%20this%20physiological%20concept.%0ASuch%20uncertainties%20cannot%20be%20directly%20estimated%20by%20local%20statistics%20on%20such%0Adescriptors%2C%20potentially%20of%20heterogeneous%20types.%20Beyond%20this%20controlled%0Aillustrative%20application%2C%20our%20methodology%20has%20the%20potential%20to%20be%20generalized%0Ato%20many%20other%20population%20analyses%20considering%20heterogeneous%20high-dimensional%0Adescriptors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12178v1&entry.124074799=Read"},
{"title": "Multi-Agent Feedback Motion Planning using Probably Approximately\n  Correct Nonlinear Model Predictive Control", "author": "Mark Gonzales and Adam Polevoy and Marin Kobilarov and Joseph Moore", "abstract": "  For many tasks, multi-robot teams often provide greater efficiency,\nrobustness, and resiliency. However, multi-robot collaboration in real-world\nscenarios poses a number of major challenges, especially when dynamic robots\nmust balance competing objectives like formation control and obstacle avoidance\nin the presence of stochastic dynamics and sensor uncertainty. In this paper,\nwe propose a distributed, multi-agent receding-horizon feedback motion planning\napproach using Probably Approximately Correct Nonlinear Model Predictive\nControl (PAC-NMPC) that is able to reason about both model and measurement\nuncertainty to achieve robust multi-agent formation control while navigating\ncluttered obstacle fields and avoiding inter-robot collisions. Our approach\nrelies not only on the underlying PAC-NMPC algorithm but also on a terminal\ncost-function derived from gyroscopic obstacle avoidance. Through numerical\nsimulation, we show that our distributed approach performs on par with a\ncentralized formulation, that it offers improved performance in the case of\nsignificant measurement noise, and that it can scale to more complex dynamical\nsystems.\n", "link": "http://arxiv.org/abs/2501.12234v1", "date": "2025-01-21", "relevancy": 2.2518, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6231}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5659}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Feedback%20Motion%20Planning%20using%20Probably%20Approximately%0A%20%20Correct%20Nonlinear%20Model%20Predictive%20Control&body=Title%3A%20Multi-Agent%20Feedback%20Motion%20Planning%20using%20Probably%20Approximately%0A%20%20Correct%20Nonlinear%20Model%20Predictive%20Control%0AAuthor%3A%20Mark%20Gonzales%20and%20Adam%20Polevoy%20and%20Marin%20Kobilarov%20and%20Joseph%20Moore%0AAbstract%3A%20%20%20For%20many%20tasks%2C%20multi-robot%20teams%20often%20provide%20greater%20efficiency%2C%0Arobustness%2C%20and%20resiliency.%20However%2C%20multi-robot%20collaboration%20in%20real-world%0Ascenarios%20poses%20a%20number%20of%20major%20challenges%2C%20especially%20when%20dynamic%20robots%0Amust%20balance%20competing%20objectives%20like%20formation%20control%20and%20obstacle%20avoidance%0Ain%20the%20presence%20of%20stochastic%20dynamics%20and%20sensor%20uncertainty.%20In%20this%20paper%2C%0Awe%20propose%20a%20distributed%2C%20multi-agent%20receding-horizon%20feedback%20motion%20planning%0Aapproach%20using%20Probably%20Approximately%20Correct%20Nonlinear%20Model%20Predictive%0AControl%20%28PAC-NMPC%29%20that%20is%20able%20to%20reason%20about%20both%20model%20and%20measurement%0Auncertainty%20to%20achieve%20robust%20multi-agent%20formation%20control%20while%20navigating%0Acluttered%20obstacle%20fields%20and%20avoiding%20inter-robot%20collisions.%20Our%20approach%0Arelies%20not%20only%20on%20the%20underlying%20PAC-NMPC%20algorithm%20but%20also%20on%20a%20terminal%0Acost-function%20derived%20from%20gyroscopic%20obstacle%20avoidance.%20Through%20numerical%0Asimulation%2C%20we%20show%20that%20our%20distributed%20approach%20performs%20on%20par%20with%20a%0Acentralized%20formulation%2C%20that%20it%20offers%20improved%20performance%20in%20the%20case%20of%0Asignificant%20measurement%20noise%2C%20and%20that%20it%20can%20scale%20to%20more%20complex%20dynamical%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Feedback%2520Motion%2520Planning%2520using%2520Probably%2520Approximately%250A%2520%2520Correct%2520Nonlinear%2520Model%2520Predictive%2520Control%26entry.906535625%3DMark%2520Gonzales%2520and%2520Adam%2520Polevoy%2520and%2520Marin%2520Kobilarov%2520and%2520Joseph%2520Moore%26entry.1292438233%3D%2520%2520For%2520many%2520tasks%252C%2520multi-robot%2520teams%2520often%2520provide%2520greater%2520efficiency%252C%250Arobustness%252C%2520and%2520resiliency.%2520However%252C%2520multi-robot%2520collaboration%2520in%2520real-world%250Ascenarios%2520poses%2520a%2520number%2520of%2520major%2520challenges%252C%2520especially%2520when%2520dynamic%2520robots%250Amust%2520balance%2520competing%2520objectives%2520like%2520formation%2520control%2520and%2520obstacle%2520avoidance%250Ain%2520the%2520presence%2520of%2520stochastic%2520dynamics%2520and%2520sensor%2520uncertainty.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520distributed%252C%2520multi-agent%2520receding-horizon%2520feedback%2520motion%2520planning%250Aapproach%2520using%2520Probably%2520Approximately%2520Correct%2520Nonlinear%2520Model%2520Predictive%250AControl%2520%2528PAC-NMPC%2529%2520that%2520is%2520able%2520to%2520reason%2520about%2520both%2520model%2520and%2520measurement%250Auncertainty%2520to%2520achieve%2520robust%2520multi-agent%2520formation%2520control%2520while%2520navigating%250Acluttered%2520obstacle%2520fields%2520and%2520avoiding%2520inter-robot%2520collisions.%2520Our%2520approach%250Arelies%2520not%2520only%2520on%2520the%2520underlying%2520PAC-NMPC%2520algorithm%2520but%2520also%2520on%2520a%2520terminal%250Acost-function%2520derived%2520from%2520gyroscopic%2520obstacle%2520avoidance.%2520Through%2520numerical%250Asimulation%252C%2520we%2520show%2520that%2520our%2520distributed%2520approach%2520performs%2520on%2520par%2520with%2520a%250Acentralized%2520formulation%252C%2520that%2520it%2520offers%2520improved%2520performance%2520in%2520the%2520case%2520of%250Asignificant%2520measurement%2520noise%252C%2520and%2520that%2520it%2520can%2520scale%2520to%2520more%2520complex%2520dynamical%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Feedback%20Motion%20Planning%20using%20Probably%20Approximately%0A%20%20Correct%20Nonlinear%20Model%20Predictive%20Control&entry.906535625=Mark%20Gonzales%20and%20Adam%20Polevoy%20and%20Marin%20Kobilarov%20and%20Joseph%20Moore&entry.1292438233=%20%20For%20many%20tasks%2C%20multi-robot%20teams%20often%20provide%20greater%20efficiency%2C%0Arobustness%2C%20and%20resiliency.%20However%2C%20multi-robot%20collaboration%20in%20real-world%0Ascenarios%20poses%20a%20number%20of%20major%20challenges%2C%20especially%20when%20dynamic%20robots%0Amust%20balance%20competing%20objectives%20like%20formation%20control%20and%20obstacle%20avoidance%0Ain%20the%20presence%20of%20stochastic%20dynamics%20and%20sensor%20uncertainty.%20In%20this%20paper%2C%0Awe%20propose%20a%20distributed%2C%20multi-agent%20receding-horizon%20feedback%20motion%20planning%0Aapproach%20using%20Probably%20Approximately%20Correct%20Nonlinear%20Model%20Predictive%0AControl%20%28PAC-NMPC%29%20that%20is%20able%20to%20reason%20about%20both%20model%20and%20measurement%0Auncertainty%20to%20achieve%20robust%20multi-agent%20formation%20control%20while%20navigating%0Acluttered%20obstacle%20fields%20and%20avoiding%20inter-robot%20collisions.%20Our%20approach%0Arelies%20not%20only%20on%20the%20underlying%20PAC-NMPC%20algorithm%20but%20also%20on%20a%20terminal%0Acost-function%20derived%20from%20gyroscopic%20obstacle%20avoidance.%20Through%20numerical%0Asimulation%2C%20we%20show%20that%20our%20distributed%20approach%20performs%20on%20par%20with%20a%0Acentralized%20formulation%2C%20that%20it%20offers%20improved%20performance%20in%20the%20case%20of%0Asignificant%20measurement%20noise%2C%20and%20that%20it%20can%20scale%20to%20more%20complex%20dynamical%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12234v1&entry.124074799=Read"},
{"title": "DLEN: Dual Branch of Transformer for Low-Light Image Enhancement in Dual\n  Domains", "author": "Junyu Xia and Jiesong Bai and Yihang Dong", "abstract": "  Low-light image enhancement (LLE) aims to improve the visual quality of\nimages captured in poorly lit conditions, which often suffer from low\nbrightness, low contrast, noise, and color distortions. These issues hinder the\nperformance of computer vision tasks such as object detection, facial\nrecognition, and autonomous driving.Traditional enhancement techniques, such as\nmulti-scale fusion and histogram equalization, fail to preserve fine details\nand often struggle with maintaining the natural appearance of enhanced images\nunder complex lighting conditions. Although the Retinex theory provides a\nfoundation for image decomposition, it often amplifies noise, leading to\nsuboptimal image quality. In this paper, we propose the Dual Light Enhance\nNetwork (DLEN), a novel architecture that incorporates two distinct attention\nmechanisms, considering both spatial and frequency domains. Our model\nintroduces a learnable wavelet transform module in the illumination estimation\nphase, preserving high- and low-frequency components to enhance edge and\ntexture details. Additionally, we design a dual-branch structure that leverages\nthe power of the Transformer architecture to enhance both the illumination and\nstructural components of the image.Through extensive experiments, our model\noutperforms state-of-the-art methods on standard benchmarks.Code is available\nhere: https://github.com/LaLaLoXX/DLEN\n", "link": "http://arxiv.org/abs/2501.12235v1", "date": "2025-01-21", "relevancy": 2.2497, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DLEN%3A%20Dual%20Branch%20of%20Transformer%20for%20Low-Light%20Image%20Enhancement%20in%20Dual%0A%20%20Domains&body=Title%3A%20DLEN%3A%20Dual%20Branch%20of%20Transformer%20for%20Low-Light%20Image%20Enhancement%20in%20Dual%0A%20%20Domains%0AAuthor%3A%20Junyu%20Xia%20and%20Jiesong%20Bai%20and%20Yihang%20Dong%0AAbstract%3A%20%20%20Low-light%20image%20enhancement%20%28LLE%29%20aims%20to%20improve%20the%20visual%20quality%20of%0Aimages%20captured%20in%20poorly%20lit%20conditions%2C%20which%20often%20suffer%20from%20low%0Abrightness%2C%20low%20contrast%2C%20noise%2C%20and%20color%20distortions.%20These%20issues%20hinder%20the%0Aperformance%20of%20computer%20vision%20tasks%20such%20as%20object%20detection%2C%20facial%0Arecognition%2C%20and%20autonomous%20driving.Traditional%20enhancement%20techniques%2C%20such%20as%0Amulti-scale%20fusion%20and%20histogram%20equalization%2C%20fail%20to%20preserve%20fine%20details%0Aand%20often%20struggle%20with%20maintaining%20the%20natural%20appearance%20of%20enhanced%20images%0Aunder%20complex%20lighting%20conditions.%20Although%20the%20Retinex%20theory%20provides%20a%0Afoundation%20for%20image%20decomposition%2C%20it%20often%20amplifies%20noise%2C%20leading%20to%0Asuboptimal%20image%20quality.%20In%20this%20paper%2C%20we%20propose%20the%20Dual%20Light%20Enhance%0ANetwork%20%28DLEN%29%2C%20a%20novel%20architecture%20that%20incorporates%20two%20distinct%20attention%0Amechanisms%2C%20considering%20both%20spatial%20and%20frequency%20domains.%20Our%20model%0Aintroduces%20a%20learnable%20wavelet%20transform%20module%20in%20the%20illumination%20estimation%0Aphase%2C%20preserving%20high-%20and%20low-frequency%20components%20to%20enhance%20edge%20and%0Atexture%20details.%20Additionally%2C%20we%20design%20a%20dual-branch%20structure%20that%20leverages%0Athe%20power%20of%20the%20Transformer%20architecture%20to%20enhance%20both%20the%20illumination%20and%0Astructural%20components%20of%20the%20image.Through%20extensive%20experiments%2C%20our%20model%0Aoutperforms%20state-of-the-art%20methods%20on%20standard%20benchmarks.Code%20is%20available%0Ahere%3A%20https%3A//github.com/LaLaLoXX/DLEN%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDLEN%253A%2520Dual%2520Branch%2520of%2520Transformer%2520for%2520Low-Light%2520Image%2520Enhancement%2520in%2520Dual%250A%2520%2520Domains%26entry.906535625%3DJunyu%2520Xia%2520and%2520Jiesong%2520Bai%2520and%2520Yihang%2520Dong%26entry.1292438233%3D%2520%2520Low-light%2520image%2520enhancement%2520%2528LLE%2529%2520aims%2520to%2520improve%2520the%2520visual%2520quality%2520of%250Aimages%2520captured%2520in%2520poorly%2520lit%2520conditions%252C%2520which%2520often%2520suffer%2520from%2520low%250Abrightness%252C%2520low%2520contrast%252C%2520noise%252C%2520and%2520color%2520distortions.%2520These%2520issues%2520hinder%2520the%250Aperformance%2520of%2520computer%2520vision%2520tasks%2520such%2520as%2520object%2520detection%252C%2520facial%250Arecognition%252C%2520and%2520autonomous%2520driving.Traditional%2520enhancement%2520techniques%252C%2520such%2520as%250Amulti-scale%2520fusion%2520and%2520histogram%2520equalization%252C%2520fail%2520to%2520preserve%2520fine%2520details%250Aand%2520often%2520struggle%2520with%2520maintaining%2520the%2520natural%2520appearance%2520of%2520enhanced%2520images%250Aunder%2520complex%2520lighting%2520conditions.%2520Although%2520the%2520Retinex%2520theory%2520provides%2520a%250Afoundation%2520for%2520image%2520decomposition%252C%2520it%2520often%2520amplifies%2520noise%252C%2520leading%2520to%250Asuboptimal%2520image%2520quality.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Dual%2520Light%2520Enhance%250ANetwork%2520%2528DLEN%2529%252C%2520a%2520novel%2520architecture%2520that%2520incorporates%2520two%2520distinct%2520attention%250Amechanisms%252C%2520considering%2520both%2520spatial%2520and%2520frequency%2520domains.%2520Our%2520model%250Aintroduces%2520a%2520learnable%2520wavelet%2520transform%2520module%2520in%2520the%2520illumination%2520estimation%250Aphase%252C%2520preserving%2520high-%2520and%2520low-frequency%2520components%2520to%2520enhance%2520edge%2520and%250Atexture%2520details.%2520Additionally%252C%2520we%2520design%2520a%2520dual-branch%2520structure%2520that%2520leverages%250Athe%2520power%2520of%2520the%2520Transformer%2520architecture%2520to%2520enhance%2520both%2520the%2520illumination%2520and%250Astructural%2520components%2520of%2520the%2520image.Through%2520extensive%2520experiments%252C%2520our%2520model%250Aoutperforms%2520state-of-the-art%2520methods%2520on%2520standard%2520benchmarks.Code%2520is%2520available%250Ahere%253A%2520https%253A//github.com/LaLaLoXX/DLEN%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DLEN%3A%20Dual%20Branch%20of%20Transformer%20for%20Low-Light%20Image%20Enhancement%20in%20Dual%0A%20%20Domains&entry.906535625=Junyu%20Xia%20and%20Jiesong%20Bai%20and%20Yihang%20Dong&entry.1292438233=%20%20Low-light%20image%20enhancement%20%28LLE%29%20aims%20to%20improve%20the%20visual%20quality%20of%0Aimages%20captured%20in%20poorly%20lit%20conditions%2C%20which%20often%20suffer%20from%20low%0Abrightness%2C%20low%20contrast%2C%20noise%2C%20and%20color%20distortions.%20These%20issues%20hinder%20the%0Aperformance%20of%20computer%20vision%20tasks%20such%20as%20object%20detection%2C%20facial%0Arecognition%2C%20and%20autonomous%20driving.Traditional%20enhancement%20techniques%2C%20such%20as%0Amulti-scale%20fusion%20and%20histogram%20equalization%2C%20fail%20to%20preserve%20fine%20details%0Aand%20often%20struggle%20with%20maintaining%20the%20natural%20appearance%20of%20enhanced%20images%0Aunder%20complex%20lighting%20conditions.%20Although%20the%20Retinex%20theory%20provides%20a%0Afoundation%20for%20image%20decomposition%2C%20it%20often%20amplifies%20noise%2C%20leading%20to%0Asuboptimal%20image%20quality.%20In%20this%20paper%2C%20we%20propose%20the%20Dual%20Light%20Enhance%0ANetwork%20%28DLEN%29%2C%20a%20novel%20architecture%20that%20incorporates%20two%20distinct%20attention%0Amechanisms%2C%20considering%20both%20spatial%20and%20frequency%20domains.%20Our%20model%0Aintroduces%20a%20learnable%20wavelet%20transform%20module%20in%20the%20illumination%20estimation%0Aphase%2C%20preserving%20high-%20and%20low-frequency%20components%20to%20enhance%20edge%20and%0Atexture%20details.%20Additionally%2C%20we%20design%20a%20dual-branch%20structure%20that%20leverages%0Athe%20power%20of%20the%20Transformer%20architecture%20to%20enhance%20both%20the%20illumination%20and%0Astructural%20components%20of%20the%20image.Through%20extensive%20experiments%2C%20our%20model%0Aoutperforms%20state-of-the-art%20methods%20on%20standard%20benchmarks.Code%20is%20available%0Ahere%3A%20https%3A//github.com/LaLaLoXX/DLEN%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12235v1&entry.124074799=Read"},
{"title": "InsTALL: Context-aware Instructional Task Assistance with Multi-modal\n  Large Language Models", "author": "Pha Nguyen and Sailik Sengupta and Girik Malik and Arshit Gupta and Bonan Min", "abstract": "  The improved competence of generative models can help building multi-modal\nvirtual assistants that leverage modalities beyond language. By observing\nhumans performing multi-step tasks, one can build assistants that have\nsituational awareness of actions and tasks being performed, enabling them to\ncater assistance based on this understanding. In this paper, we develop a\nContext-aware Instructional Task Assistant with Multi-modal Large Language\nModels (InsTALL) that leverages an online visual stream (e.g. a user's screen\nshare or video recording) and responds in real-time to user queries related to\nthe task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal\nmodel on task videos and paired textual data, and 2) automatically extracts\ntask graph from video data and leverages it at training and inference time. We\nshow InsTALL achieves state-of-the-art performance across proposed sub-tasks\nconsidered for multimodal activity understanding -- task recognition (TR),\naction recognition (AR), next action prediction (AP), and plan prediction (PP)\n-- and outperforms existing baselines on two novel sub-tasks related to\nautomatic error identification.\n", "link": "http://arxiv.org/abs/2501.12231v1", "date": "2025-01-21", "relevancy": 2.2429, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5795}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InsTALL%3A%20Context-aware%20Instructional%20Task%20Assistance%20with%20Multi-modal%0A%20%20Large%20Language%20Models&body=Title%3A%20InsTALL%3A%20Context-aware%20Instructional%20Task%20Assistance%20with%20Multi-modal%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Pha%20Nguyen%20and%20Sailik%20Sengupta%20and%20Girik%20Malik%20and%20Arshit%20Gupta%20and%20Bonan%20Min%0AAbstract%3A%20%20%20The%20improved%20competence%20of%20generative%20models%20can%20help%20building%20multi-modal%0Avirtual%20assistants%20that%20leverage%20modalities%20beyond%20language.%20By%20observing%0Ahumans%20performing%20multi-step%20tasks%2C%20one%20can%20build%20assistants%20that%20have%0Asituational%20awareness%20of%20actions%20and%20tasks%20being%20performed%2C%20enabling%20them%20to%0Acater%20assistance%20based%20on%20this%20understanding.%20In%20this%20paper%2C%20we%20develop%20a%0AContext-aware%20Instructional%20Task%20Assistant%20with%20Multi-modal%20Large%20Language%0AModels%20%28InsTALL%29%20that%20leverages%20an%20online%20visual%20stream%20%28e.g.%20a%20user%27s%20screen%0Ashare%20or%20video%20recording%29%20and%20responds%20in%20real-time%20to%20user%20queries%20related%20to%0Athe%20task%20at%20hand.%20To%20enable%20useful%20assistance%2C%20InsTALL%201%29%20trains%20a%20multi-modal%0Amodel%20on%20task%20videos%20and%20paired%20textual%20data%2C%20and%202%29%20automatically%20extracts%0Atask%20graph%20from%20video%20data%20and%20leverages%20it%20at%20training%20and%20inference%20time.%20We%0Ashow%20InsTALL%20achieves%20state-of-the-art%20performance%20across%20proposed%20sub-tasks%0Aconsidered%20for%20multimodal%20activity%20understanding%20--%20task%20recognition%20%28TR%29%2C%0Aaction%20recognition%20%28AR%29%2C%20next%20action%20prediction%20%28AP%29%2C%20and%20plan%20prediction%20%28PP%29%0A--%20and%20outperforms%20existing%20baselines%20on%20two%20novel%20sub-tasks%20related%20to%0Aautomatic%20error%20identification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsTALL%253A%2520Context-aware%2520Instructional%2520Task%2520Assistance%2520with%2520Multi-modal%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DPha%2520Nguyen%2520and%2520Sailik%2520Sengupta%2520and%2520Girik%2520Malik%2520and%2520Arshit%2520Gupta%2520and%2520Bonan%2520Min%26entry.1292438233%3D%2520%2520The%2520improved%2520competence%2520of%2520generative%2520models%2520can%2520help%2520building%2520multi-modal%250Avirtual%2520assistants%2520that%2520leverage%2520modalities%2520beyond%2520language.%2520By%2520observing%250Ahumans%2520performing%2520multi-step%2520tasks%252C%2520one%2520can%2520build%2520assistants%2520that%2520have%250Asituational%2520awareness%2520of%2520actions%2520and%2520tasks%2520being%2520performed%252C%2520enabling%2520them%2520to%250Acater%2520assistance%2520based%2520on%2520this%2520understanding.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%250AContext-aware%2520Instructional%2520Task%2520Assistant%2520with%2520Multi-modal%2520Large%2520Language%250AModels%2520%2528InsTALL%2529%2520that%2520leverages%2520an%2520online%2520visual%2520stream%2520%2528e.g.%2520a%2520user%2527s%2520screen%250Ashare%2520or%2520video%2520recording%2529%2520and%2520responds%2520in%2520real-time%2520to%2520user%2520queries%2520related%2520to%250Athe%2520task%2520at%2520hand.%2520To%2520enable%2520useful%2520assistance%252C%2520InsTALL%25201%2529%2520trains%2520a%2520multi-modal%250Amodel%2520on%2520task%2520videos%2520and%2520paired%2520textual%2520data%252C%2520and%25202%2529%2520automatically%2520extracts%250Atask%2520graph%2520from%2520video%2520data%2520and%2520leverages%2520it%2520at%2520training%2520and%2520inference%2520time.%2520We%250Ashow%2520InsTALL%2520achieves%2520state-of-the-art%2520performance%2520across%2520proposed%2520sub-tasks%250Aconsidered%2520for%2520multimodal%2520activity%2520understanding%2520--%2520task%2520recognition%2520%2528TR%2529%252C%250Aaction%2520recognition%2520%2528AR%2529%252C%2520next%2520action%2520prediction%2520%2528AP%2529%252C%2520and%2520plan%2520prediction%2520%2528PP%2529%250A--%2520and%2520outperforms%2520existing%2520baselines%2520on%2520two%2520novel%2520sub-tasks%2520related%2520to%250Aautomatic%2520error%2520identification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InsTALL%3A%20Context-aware%20Instructional%20Task%20Assistance%20with%20Multi-modal%0A%20%20Large%20Language%20Models&entry.906535625=Pha%20Nguyen%20and%20Sailik%20Sengupta%20and%20Girik%20Malik%20and%20Arshit%20Gupta%20and%20Bonan%20Min&entry.1292438233=%20%20The%20improved%20competence%20of%20generative%20models%20can%20help%20building%20multi-modal%0Avirtual%20assistants%20that%20leverage%20modalities%20beyond%20language.%20By%20observing%0Ahumans%20performing%20multi-step%20tasks%2C%20one%20can%20build%20assistants%20that%20have%0Asituational%20awareness%20of%20actions%20and%20tasks%20being%20performed%2C%20enabling%20them%20to%0Acater%20assistance%20based%20on%20this%20understanding.%20In%20this%20paper%2C%20we%20develop%20a%0AContext-aware%20Instructional%20Task%20Assistant%20with%20Multi-modal%20Large%20Language%0AModels%20%28InsTALL%29%20that%20leverages%20an%20online%20visual%20stream%20%28e.g.%20a%20user%27s%20screen%0Ashare%20or%20video%20recording%29%20and%20responds%20in%20real-time%20to%20user%20queries%20related%20to%0Athe%20task%20at%20hand.%20To%20enable%20useful%20assistance%2C%20InsTALL%201%29%20trains%20a%20multi-modal%0Amodel%20on%20task%20videos%20and%20paired%20textual%20data%2C%20and%202%29%20automatically%20extracts%0Atask%20graph%20from%20video%20data%20and%20leverages%20it%20at%20training%20and%20inference%20time.%20We%0Ashow%20InsTALL%20achieves%20state-of-the-art%20performance%20across%20proposed%20sub-tasks%0Aconsidered%20for%20multimodal%20activity%20understanding%20--%20task%20recognition%20%28TR%29%2C%0Aaction%20recognition%20%28AR%29%2C%20next%20action%20prediction%20%28AP%29%2C%20and%20plan%20prediction%20%28PP%29%0A--%20and%20outperforms%20existing%20baselines%20on%20two%20novel%20sub-tasks%20related%20to%0Aautomatic%20error%20identification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12231v1&entry.124074799=Read"},
{"title": "Learning segmentation from point trajectories", "author": "Laurynas Karazija and Iro Laina and Christian Rupprecht and Andrea Vedaldi", "abstract": "  We consider the problem of segmenting objects in videos based on their motion\nand no other forms of supervision. Prior work has often approached this problem\nby using the principle of common fate, namely the fact that the motion of\npoints that belong to the same object is strongly correlated. However, most\nauthors have only considered instantaneous motion from optical flow. In this\nwork, we present a way to train a segmentation network using long-term point\ntrajectories as a supervisory signal to complement optical flow. The key\ndifficulty is that long-term motion, unlike instantaneous motion, is difficult\nto model -- any parametric approximation is unlikely to capture complex motion\npatterns over long periods of time. We instead draw inspiration from subspace\nclustering approaches, proposing a loss function that seeks to group the\ntrajectories into low-rank matrices where the motion of object points can be\napproximately explained as a linear combination of other point tracks. Our\nmethod outperforms the prior art on motion-based segmentation, which shows the\nutility of long-term motion and the effectiveness of our formulation.\n", "link": "http://arxiv.org/abs/2501.12392v1", "date": "2025-01-21", "relevancy": 2.2364, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5741}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5543}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20segmentation%20from%20point%20trajectories&body=Title%3A%20Learning%20segmentation%20from%20point%20trajectories%0AAuthor%3A%20Laurynas%20Karazija%20and%20Iro%20Laina%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20segmenting%20objects%20in%20videos%20based%20on%20their%20motion%0Aand%20no%20other%20forms%20of%20supervision.%20Prior%20work%20has%20often%20approached%20this%20problem%0Aby%20using%20the%20principle%20of%20common%20fate%2C%20namely%20the%20fact%20that%20the%20motion%20of%0Apoints%20that%20belong%20to%20the%20same%20object%20is%20strongly%20correlated.%20However%2C%20most%0Aauthors%20have%20only%20considered%20instantaneous%20motion%20from%20optical%20flow.%20In%20this%0Awork%2C%20we%20present%20a%20way%20to%20train%20a%20segmentation%20network%20using%20long-term%20point%0Atrajectories%20as%20a%20supervisory%20signal%20to%20complement%20optical%20flow.%20The%20key%0Adifficulty%20is%20that%20long-term%20motion%2C%20unlike%20instantaneous%20motion%2C%20is%20difficult%0Ato%20model%20--%20any%20parametric%20approximation%20is%20unlikely%20to%20capture%20complex%20motion%0Apatterns%20over%20long%20periods%20of%20time.%20We%20instead%20draw%20inspiration%20from%20subspace%0Aclustering%20approaches%2C%20proposing%20a%20loss%20function%20that%20seeks%20to%20group%20the%0Atrajectories%20into%20low-rank%20matrices%20where%20the%20motion%20of%20object%20points%20can%20be%0Aapproximately%20explained%20as%20a%20linear%20combination%20of%20other%20point%20tracks.%20Our%0Amethod%20outperforms%20the%20prior%20art%20on%20motion-based%20segmentation%2C%20which%20shows%20the%0Autility%20of%20long-term%20motion%20and%20the%20effectiveness%20of%20our%20formulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520segmentation%2520from%2520point%2520trajectories%26entry.906535625%3DLaurynas%2520Karazija%2520and%2520Iro%2520Laina%2520and%2520Christian%2520Rupprecht%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520segmenting%2520objects%2520in%2520videos%2520based%2520on%2520their%2520motion%250Aand%2520no%2520other%2520forms%2520of%2520supervision.%2520Prior%2520work%2520has%2520often%2520approached%2520this%2520problem%250Aby%2520using%2520the%2520principle%2520of%2520common%2520fate%252C%2520namely%2520the%2520fact%2520that%2520the%2520motion%2520of%250Apoints%2520that%2520belong%2520to%2520the%2520same%2520object%2520is%2520strongly%2520correlated.%2520However%252C%2520most%250Aauthors%2520have%2520only%2520considered%2520instantaneous%2520motion%2520from%2520optical%2520flow.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520way%2520to%2520train%2520a%2520segmentation%2520network%2520using%2520long-term%2520point%250Atrajectories%2520as%2520a%2520supervisory%2520signal%2520to%2520complement%2520optical%2520flow.%2520The%2520key%250Adifficulty%2520is%2520that%2520long-term%2520motion%252C%2520unlike%2520instantaneous%2520motion%252C%2520is%2520difficult%250Ato%2520model%2520--%2520any%2520parametric%2520approximation%2520is%2520unlikely%2520to%2520capture%2520complex%2520motion%250Apatterns%2520over%2520long%2520periods%2520of%2520time.%2520We%2520instead%2520draw%2520inspiration%2520from%2520subspace%250Aclustering%2520approaches%252C%2520proposing%2520a%2520loss%2520function%2520that%2520seeks%2520to%2520group%2520the%250Atrajectories%2520into%2520low-rank%2520matrices%2520where%2520the%2520motion%2520of%2520object%2520points%2520can%2520be%250Aapproximately%2520explained%2520as%2520a%2520linear%2520combination%2520of%2520other%2520point%2520tracks.%2520Our%250Amethod%2520outperforms%2520the%2520prior%2520art%2520on%2520motion-based%2520segmentation%252C%2520which%2520shows%2520the%250Autility%2520of%2520long-term%2520motion%2520and%2520the%2520effectiveness%2520of%2520our%2520formulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20segmentation%20from%20point%20trajectories&entry.906535625=Laurynas%20Karazija%20and%20Iro%20Laina%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20segmenting%20objects%20in%20videos%20based%20on%20their%20motion%0Aand%20no%20other%20forms%20of%20supervision.%20Prior%20work%20has%20often%20approached%20this%20problem%0Aby%20using%20the%20principle%20of%20common%20fate%2C%20namely%20the%20fact%20that%20the%20motion%20of%0Apoints%20that%20belong%20to%20the%20same%20object%20is%20strongly%20correlated.%20However%2C%20most%0Aauthors%20have%20only%20considered%20instantaneous%20motion%20from%20optical%20flow.%20In%20this%0Awork%2C%20we%20present%20a%20way%20to%20train%20a%20segmentation%20network%20using%20long-term%20point%0Atrajectories%20as%20a%20supervisory%20signal%20to%20complement%20optical%20flow.%20The%20key%0Adifficulty%20is%20that%20long-term%20motion%2C%20unlike%20instantaneous%20motion%2C%20is%20difficult%0Ato%20model%20--%20any%20parametric%20approximation%20is%20unlikely%20to%20capture%20complex%20motion%0Apatterns%20over%20long%20periods%20of%20time.%20We%20instead%20draw%20inspiration%20from%20subspace%0Aclustering%20approaches%2C%20proposing%20a%20loss%20function%20that%20seeks%20to%20group%20the%0Atrajectories%20into%20low-rank%20matrices%20where%20the%20motion%20of%20object%20points%20can%20be%0Aapproximately%20explained%20as%20a%20linear%20combination%20of%20other%20point%20tracks.%20Our%0Amethod%20outperforms%20the%20prior%20art%20on%20motion-based%20segmentation%2C%20which%20shows%20the%0Autility%20of%20long-term%20motion%20and%20the%20effectiveness%20of%20our%20formulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12392v1&entry.124074799=Read"},
{"title": "Memory Storyboard: Leveraging Temporal Segmentation for Streaming\n  Self-Supervised Learning from Egocentric Videos", "author": "Yanlai Yang and Mengye Ren", "abstract": "  Self-supervised learning holds the promise to learn good representations from\nreal-world continuous uncurated data streams. However, most existing works in\nvisual self-supervised learning focus on static images or artificial data\nstreams. Towards exploring a more realistic learning substrate, we investigate\nstreaming self-supervised learning from long-form real-world egocentric video\nstreams. Inspired by the event segmentation mechanism in human perception and\nmemory, we propose \"Memory Storyboard\" that groups recent past frames into\ntemporal segments for more effective summarization of the past visual streams\nfor memory replay. To accommodate efficient temporal segmentation, we propose a\ntwo-tier memory hierarchy: the recent past is stored in a short-term memory,\nand the storyboard temporal segments are then transferred to a long-term\nmemory. Experiments on real-world egocentric video datasets including SAYCam\nand KrishnaCam show that contrastive learning objectives on top of storyboard\nframes result in semantically meaningful representations which outperform those\nproduced by state-of-the-art unsupervised continual learning methods.\n", "link": "http://arxiv.org/abs/2501.12254v1", "date": "2025-01-21", "relevancy": 2.2316, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5676}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5606}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20Storyboard%3A%20Leveraging%20Temporal%20Segmentation%20for%20Streaming%0A%20%20Self-Supervised%20Learning%20from%20Egocentric%20Videos&body=Title%3A%20Memory%20Storyboard%3A%20Leveraging%20Temporal%20Segmentation%20for%20Streaming%0A%20%20Self-Supervised%20Learning%20from%20Egocentric%20Videos%0AAuthor%3A%20Yanlai%20Yang%20and%20Mengye%20Ren%0AAbstract%3A%20%20%20Self-supervised%20learning%20holds%20the%20promise%20to%20learn%20good%20representations%20from%0Areal-world%20continuous%20uncurated%20data%20streams.%20However%2C%20most%20existing%20works%20in%0Avisual%20self-supervised%20learning%20focus%20on%20static%20images%20or%20artificial%20data%0Astreams.%20Towards%20exploring%20a%20more%20realistic%20learning%20substrate%2C%20we%20investigate%0Astreaming%20self-supervised%20learning%20from%20long-form%20real-world%20egocentric%20video%0Astreams.%20Inspired%20by%20the%20event%20segmentation%20mechanism%20in%20human%20perception%20and%0Amemory%2C%20we%20propose%20%22Memory%20Storyboard%22%20that%20groups%20recent%20past%20frames%20into%0Atemporal%20segments%20for%20more%20effective%20summarization%20of%20the%20past%20visual%20streams%0Afor%20memory%20replay.%20To%20accommodate%20efficient%20temporal%20segmentation%2C%20we%20propose%20a%0Atwo-tier%20memory%20hierarchy%3A%20the%20recent%20past%20is%20stored%20in%20a%20short-term%20memory%2C%0Aand%20the%20storyboard%20temporal%20segments%20are%20then%20transferred%20to%20a%20long-term%0Amemory.%20Experiments%20on%20real-world%20egocentric%20video%20datasets%20including%20SAYCam%0Aand%20KrishnaCam%20show%20that%20contrastive%20learning%20objectives%20on%20top%20of%20storyboard%0Aframes%20result%20in%20semantically%20meaningful%20representations%20which%20outperform%20those%0Aproduced%20by%20state-of-the-art%20unsupervised%20continual%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520Storyboard%253A%2520Leveraging%2520Temporal%2520Segmentation%2520for%2520Streaming%250A%2520%2520Self-Supervised%2520Learning%2520from%2520Egocentric%2520Videos%26entry.906535625%3DYanlai%2520Yang%2520and%2520Mengye%2520Ren%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520holds%2520the%2520promise%2520to%2520learn%2520good%2520representations%2520from%250Areal-world%2520continuous%2520uncurated%2520data%2520streams.%2520However%252C%2520most%2520existing%2520works%2520in%250Avisual%2520self-supervised%2520learning%2520focus%2520on%2520static%2520images%2520or%2520artificial%2520data%250Astreams.%2520Towards%2520exploring%2520a%2520more%2520realistic%2520learning%2520substrate%252C%2520we%2520investigate%250Astreaming%2520self-supervised%2520learning%2520from%2520long-form%2520real-world%2520egocentric%2520video%250Astreams.%2520Inspired%2520by%2520the%2520event%2520segmentation%2520mechanism%2520in%2520human%2520perception%2520and%250Amemory%252C%2520we%2520propose%2520%2522Memory%2520Storyboard%2522%2520that%2520groups%2520recent%2520past%2520frames%2520into%250Atemporal%2520segments%2520for%2520more%2520effective%2520summarization%2520of%2520the%2520past%2520visual%2520streams%250Afor%2520memory%2520replay.%2520To%2520accommodate%2520efficient%2520temporal%2520segmentation%252C%2520we%2520propose%2520a%250Atwo-tier%2520memory%2520hierarchy%253A%2520the%2520recent%2520past%2520is%2520stored%2520in%2520a%2520short-term%2520memory%252C%250Aand%2520the%2520storyboard%2520temporal%2520segments%2520are%2520then%2520transferred%2520to%2520a%2520long-term%250Amemory.%2520Experiments%2520on%2520real-world%2520egocentric%2520video%2520datasets%2520including%2520SAYCam%250Aand%2520KrishnaCam%2520show%2520that%2520contrastive%2520learning%2520objectives%2520on%2520top%2520of%2520storyboard%250Aframes%2520result%2520in%2520semantically%2520meaningful%2520representations%2520which%2520outperform%2520those%250Aproduced%2520by%2520state-of-the-art%2520unsupervised%2520continual%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20Storyboard%3A%20Leveraging%20Temporal%20Segmentation%20for%20Streaming%0A%20%20Self-Supervised%20Learning%20from%20Egocentric%20Videos&entry.906535625=Yanlai%20Yang%20and%20Mengye%20Ren&entry.1292438233=%20%20Self-supervised%20learning%20holds%20the%20promise%20to%20learn%20good%20representations%20from%0Areal-world%20continuous%20uncurated%20data%20streams.%20However%2C%20most%20existing%20works%20in%0Avisual%20self-supervised%20learning%20focus%20on%20static%20images%20or%20artificial%20data%0Astreams.%20Towards%20exploring%20a%20more%20realistic%20learning%20substrate%2C%20we%20investigate%0Astreaming%20self-supervised%20learning%20from%20long-form%20real-world%20egocentric%20video%0Astreams.%20Inspired%20by%20the%20event%20segmentation%20mechanism%20in%20human%20perception%20and%0Amemory%2C%20we%20propose%20%22Memory%20Storyboard%22%20that%20groups%20recent%20past%20frames%20into%0Atemporal%20segments%20for%20more%20effective%20summarization%20of%20the%20past%20visual%20streams%0Afor%20memory%20replay.%20To%20accommodate%20efficient%20temporal%20segmentation%2C%20we%20propose%20a%0Atwo-tier%20memory%20hierarchy%3A%20the%20recent%20past%20is%20stored%20in%20a%20short-term%20memory%2C%0Aand%20the%20storyboard%20temporal%20segments%20are%20then%20transferred%20to%20a%20long-term%0Amemory.%20Experiments%20on%20real-world%20egocentric%20video%20datasets%20including%20SAYCam%0Aand%20KrishnaCam%20show%20that%20contrastive%20learning%20objectives%20on%20top%20of%20storyboard%0Aframes%20result%20in%20semantically%20meaningful%20representations%20which%20outperform%20those%0Aproduced%20by%20state-of-the-art%20unsupervised%20continual%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12254v1&entry.124074799=Read"},
{"title": "TAB: Transformer Attention Bottlenecks enable User Intervention and\n  Debugging in Vision-Language Models", "author": "Pooyan Rahmanzadehgervi and Hung Huy Nguyen and Rosanne Liu and Long Mai and Anh Totti Nguyen", "abstract": "  Multi-head self-attention (MHSA) is a key component of Transformers, a widely\npopular architecture in both language and vision. Multiple heads intuitively\nenable different parallel processes over the same input. Yet, they also obscure\nthe attribution of each input patch to the output of a model. We propose a\nnovel 1-head Transformer Attention Bottleneck (TAB) layer, inserted after the\ntraditional MHSA architecture, to serve as an attention bottleneck for\ninterpretability and intervention. Unlike standard self-attention, TAB\nconstrains the total attention over all patches to $\\in [0, 1]$. That is, when\nthe total attention is 0, no visual information is propagated further into the\nnetwork and the vision-language model (VLM) would default to a generic,\nimage-independent response. To demonstrate the advantages of TAB, we train VLMs\nwith TAB to perform image difference captioning. Over three datasets, our\nmodels perform similarly to baseline VLMs in captioning but the bottleneck is\nsuperior in localizing changes and in identifying when no changes occur. TAB is\nthe first architecture to enable users to intervene by editing attention, which\noften produces expected outputs by VLMs.\n", "link": "http://arxiv.org/abs/2412.18675v3", "date": "2025-01-21", "relevancy": 2.2275, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5898}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.54}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAB%3A%20Transformer%20Attention%20Bottlenecks%20enable%20User%20Intervention%20and%0A%20%20Debugging%20in%20Vision-Language%20Models&body=Title%3A%20TAB%3A%20Transformer%20Attention%20Bottlenecks%20enable%20User%20Intervention%20and%0A%20%20Debugging%20in%20Vision-Language%20Models%0AAuthor%3A%20Pooyan%20Rahmanzadehgervi%20and%20Hung%20Huy%20Nguyen%20and%20Rosanne%20Liu%20and%20Long%20Mai%20and%20Anh%20Totti%20Nguyen%0AAbstract%3A%20%20%20Multi-head%20self-attention%20%28MHSA%29%20is%20a%20key%20component%20of%20Transformers%2C%20a%20widely%0Apopular%20architecture%20in%20both%20language%20and%20vision.%20Multiple%20heads%20intuitively%0Aenable%20different%20parallel%20processes%20over%20the%20same%20input.%20Yet%2C%20they%20also%20obscure%0Athe%20attribution%20of%20each%20input%20patch%20to%20the%20output%20of%20a%20model.%20We%20propose%20a%0Anovel%201-head%20Transformer%20Attention%20Bottleneck%20%28TAB%29%20layer%2C%20inserted%20after%20the%0Atraditional%20MHSA%20architecture%2C%20to%20serve%20as%20an%20attention%20bottleneck%20for%0Ainterpretability%20and%20intervention.%20Unlike%20standard%20self-attention%2C%20TAB%0Aconstrains%20the%20total%20attention%20over%20all%20patches%20to%20%24%5Cin%20%5B0%2C%201%5D%24.%20That%20is%2C%20when%0Athe%20total%20attention%20is%200%2C%20no%20visual%20information%20is%20propagated%20further%20into%20the%0Anetwork%20and%20the%20vision-language%20model%20%28VLM%29%20would%20default%20to%20a%20generic%2C%0Aimage-independent%20response.%20To%20demonstrate%20the%20advantages%20of%20TAB%2C%20we%20train%20VLMs%0Awith%20TAB%20to%20perform%20image%20difference%20captioning.%20Over%20three%20datasets%2C%20our%0Amodels%20perform%20similarly%20to%20baseline%20VLMs%20in%20captioning%20but%20the%20bottleneck%20is%0Asuperior%20in%20localizing%20changes%20and%20in%20identifying%20when%20no%20changes%20occur.%20TAB%20is%0Athe%20first%20architecture%20to%20enable%20users%20to%20intervene%20by%20editing%20attention%2C%20which%0Aoften%20produces%20expected%20outputs%20by%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18675v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAB%253A%2520Transformer%2520Attention%2520Bottlenecks%2520enable%2520User%2520Intervention%2520and%250A%2520%2520Debugging%2520in%2520Vision-Language%2520Models%26entry.906535625%3DPooyan%2520Rahmanzadehgervi%2520and%2520Hung%2520Huy%2520Nguyen%2520and%2520Rosanne%2520Liu%2520and%2520Long%2520Mai%2520and%2520Anh%2520Totti%2520Nguyen%26entry.1292438233%3D%2520%2520Multi-head%2520self-attention%2520%2528MHSA%2529%2520is%2520a%2520key%2520component%2520of%2520Transformers%252C%2520a%2520widely%250Apopular%2520architecture%2520in%2520both%2520language%2520and%2520vision.%2520Multiple%2520heads%2520intuitively%250Aenable%2520different%2520parallel%2520processes%2520over%2520the%2520same%2520input.%2520Yet%252C%2520they%2520also%2520obscure%250Athe%2520attribution%2520of%2520each%2520input%2520patch%2520to%2520the%2520output%2520of%2520a%2520model.%2520We%2520propose%2520a%250Anovel%25201-head%2520Transformer%2520Attention%2520Bottleneck%2520%2528TAB%2529%2520layer%252C%2520inserted%2520after%2520the%250Atraditional%2520MHSA%2520architecture%252C%2520to%2520serve%2520as%2520an%2520attention%2520bottleneck%2520for%250Ainterpretability%2520and%2520intervention.%2520Unlike%2520standard%2520self-attention%252C%2520TAB%250Aconstrains%2520the%2520total%2520attention%2520over%2520all%2520patches%2520to%2520%2524%255Cin%2520%255B0%252C%25201%255D%2524.%2520That%2520is%252C%2520when%250Athe%2520total%2520attention%2520is%25200%252C%2520no%2520visual%2520information%2520is%2520propagated%2520further%2520into%2520the%250Anetwork%2520and%2520the%2520vision-language%2520model%2520%2528VLM%2529%2520would%2520default%2520to%2520a%2520generic%252C%250Aimage-independent%2520response.%2520To%2520demonstrate%2520the%2520advantages%2520of%2520TAB%252C%2520we%2520train%2520VLMs%250Awith%2520TAB%2520to%2520perform%2520image%2520difference%2520captioning.%2520Over%2520three%2520datasets%252C%2520our%250Amodels%2520perform%2520similarly%2520to%2520baseline%2520VLMs%2520in%2520captioning%2520but%2520the%2520bottleneck%2520is%250Asuperior%2520in%2520localizing%2520changes%2520and%2520in%2520identifying%2520when%2520no%2520changes%2520occur.%2520TAB%2520is%250Athe%2520first%2520architecture%2520to%2520enable%2520users%2520to%2520intervene%2520by%2520editing%2520attention%252C%2520which%250Aoften%2520produces%2520expected%2520outputs%2520by%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18675v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAB%3A%20Transformer%20Attention%20Bottlenecks%20enable%20User%20Intervention%20and%0A%20%20Debugging%20in%20Vision-Language%20Models&entry.906535625=Pooyan%20Rahmanzadehgervi%20and%20Hung%20Huy%20Nguyen%20and%20Rosanne%20Liu%20and%20Long%20Mai%20and%20Anh%20Totti%20Nguyen&entry.1292438233=%20%20Multi-head%20self-attention%20%28MHSA%29%20is%20a%20key%20component%20of%20Transformers%2C%20a%20widely%0Apopular%20architecture%20in%20both%20language%20and%20vision.%20Multiple%20heads%20intuitively%0Aenable%20different%20parallel%20processes%20over%20the%20same%20input.%20Yet%2C%20they%20also%20obscure%0Athe%20attribution%20of%20each%20input%20patch%20to%20the%20output%20of%20a%20model.%20We%20propose%20a%0Anovel%201-head%20Transformer%20Attention%20Bottleneck%20%28TAB%29%20layer%2C%20inserted%20after%20the%0Atraditional%20MHSA%20architecture%2C%20to%20serve%20as%20an%20attention%20bottleneck%20for%0Ainterpretability%20and%20intervention.%20Unlike%20standard%20self-attention%2C%20TAB%0Aconstrains%20the%20total%20attention%20over%20all%20patches%20to%20%24%5Cin%20%5B0%2C%201%5D%24.%20That%20is%2C%20when%0Athe%20total%20attention%20is%200%2C%20no%20visual%20information%20is%20propagated%20further%20into%20the%0Anetwork%20and%20the%20vision-language%20model%20%28VLM%29%20would%20default%20to%20a%20generic%2C%0Aimage-independent%20response.%20To%20demonstrate%20the%20advantages%20of%20TAB%2C%20we%20train%20VLMs%0Awith%20TAB%20to%20perform%20image%20difference%20captioning.%20Over%20three%20datasets%2C%20our%0Amodels%20perform%20similarly%20to%20baseline%20VLMs%20in%20captioning%20but%20the%20bottleneck%20is%0Asuperior%20in%20localizing%20changes%20and%20in%20identifying%20when%20no%20changes%20occur.%20TAB%20is%0Athe%20first%20architecture%20to%20enable%20users%20to%20intervene%20by%20editing%20attention%2C%20which%0Aoften%20produces%20expected%20outputs%20by%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18675v3&entry.124074799=Read"},
{"title": "CBVLM: Training-free Explainable Concept-based Large Vision Language\n  Models for Medical Image Classification", "author": "Cristiano Patr\u00edcio and Isabel Rio-Torto and Jaime S. Cardoso and Lu\u00eds F. Teixeira and Jo\u00e3o C. Neves", "abstract": "  The main challenges limiting the adoption of deep learning-based solutions in\nmedical workflows are the availability of annotated data and the lack of\ninterpretability of such systems. Concept Bottleneck Models (CBMs) tackle the\nlatter by constraining the final disease prediction on a set of predefined and\nhuman-interpretable concepts. However, the increased interpretability achieved\nthrough these concept-based explanations implies a higher annotation burden.\nMoreover, if a new concept needs to be added, the whole system needs to be\nretrained. Inspired by the remarkable performance shown by Large\nVision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet\neffective, methodology, CBVLM, which tackles both of the aforementioned\nchallenges. First, for each concept, we prompt the LVLM to answer if the\nconcept is present in the input image. Then, we ask the LVLM to classify the\nimage based on the previous concept predictions. Moreover, in both stages, we\nincorporate a retrieval module responsible for selecting the best examples for\nin-context learning. By grounding the final diagnosis on the predicted\nconcepts, we ensure explainability, and by leveraging the few-shot capabilities\nof LVLMs, we drastically lower the annotation cost. We validate our approach\nwith extensive experiments across four medical datasets and twelve LVLMs (both\ngeneric and medical) and show that CBVLM consistently outperforms CBMs and\ntask-specific supervised methods without requiring any training and using just\na few annotated examples. More information on our project page:\nhttps://cristianopatricio.github.io/CBVLM/.\n", "link": "http://arxiv.org/abs/2501.12266v1", "date": "2025-01-21", "relevancy": 2.2253, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5627}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5627}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CBVLM%3A%20Training-free%20Explainable%20Concept-based%20Large%20Vision%20Language%0A%20%20Models%20for%20Medical%20Image%20Classification&body=Title%3A%20CBVLM%3A%20Training-free%20Explainable%20Concept-based%20Large%20Vision%20Language%0A%20%20Models%20for%20Medical%20Image%20Classification%0AAuthor%3A%20Cristiano%20Patr%C3%ADcio%20and%20Isabel%20Rio-Torto%20and%20Jaime%20S.%20Cardoso%20and%20Lu%C3%ADs%20F.%20Teixeira%20and%20Jo%C3%A3o%20C.%20Neves%0AAbstract%3A%20%20%20The%20main%20challenges%20limiting%20the%20adoption%20of%20deep%20learning-based%20solutions%20in%0Amedical%20workflows%20are%20the%20availability%20of%20annotated%20data%20and%20the%20lack%20of%0Ainterpretability%20of%20such%20systems.%20Concept%20Bottleneck%20Models%20%28CBMs%29%20tackle%20the%0Alatter%20by%20constraining%20the%20final%20disease%20prediction%20on%20a%20set%20of%20predefined%20and%0Ahuman-interpretable%20concepts.%20However%2C%20the%20increased%20interpretability%20achieved%0Athrough%20these%20concept-based%20explanations%20implies%20a%20higher%20annotation%20burden.%0AMoreover%2C%20if%20a%20new%20concept%20needs%20to%20be%20added%2C%20the%20whole%20system%20needs%20to%20be%0Aretrained.%20Inspired%20by%20the%20remarkable%20performance%20shown%20by%20Large%0AVision-Language%20Models%20%28LVLMs%29%20in%20few-shot%20settings%2C%20we%20propose%20a%20simple%2C%20yet%0Aeffective%2C%20methodology%2C%20CBVLM%2C%20which%20tackles%20both%20of%20the%20aforementioned%0Achallenges.%20First%2C%20for%20each%20concept%2C%20we%20prompt%20the%20LVLM%20to%20answer%20if%20the%0Aconcept%20is%20present%20in%20the%20input%20image.%20Then%2C%20we%20ask%20the%20LVLM%20to%20classify%20the%0Aimage%20based%20on%20the%20previous%20concept%20predictions.%20Moreover%2C%20in%20both%20stages%2C%20we%0Aincorporate%20a%20retrieval%20module%20responsible%20for%20selecting%20the%20best%20examples%20for%0Ain-context%20learning.%20By%20grounding%20the%20final%20diagnosis%20on%20the%20predicted%0Aconcepts%2C%20we%20ensure%20explainability%2C%20and%20by%20leveraging%20the%20few-shot%20capabilities%0Aof%20LVLMs%2C%20we%20drastically%20lower%20the%20annotation%20cost.%20We%20validate%20our%20approach%0Awith%20extensive%20experiments%20across%20four%20medical%20datasets%20and%20twelve%20LVLMs%20%28both%0Ageneric%20and%20medical%29%20and%20show%20that%20CBVLM%20consistently%20outperforms%20CBMs%20and%0Atask-specific%20supervised%20methods%20without%20requiring%20any%20training%20and%20using%20just%0Aa%20few%20annotated%20examples.%20More%20information%20on%20our%20project%20page%3A%0Ahttps%3A//cristianopatricio.github.io/CBVLM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCBVLM%253A%2520Training-free%2520Explainable%2520Concept-based%2520Large%2520Vision%2520Language%250A%2520%2520Models%2520for%2520Medical%2520Image%2520Classification%26entry.906535625%3DCristiano%2520Patr%25C3%25ADcio%2520and%2520Isabel%2520Rio-Torto%2520and%2520Jaime%2520S.%2520Cardoso%2520and%2520Lu%25C3%25ADs%2520F.%2520Teixeira%2520and%2520Jo%25C3%25A3o%2520C.%2520Neves%26entry.1292438233%3D%2520%2520The%2520main%2520challenges%2520limiting%2520the%2520adoption%2520of%2520deep%2520learning-based%2520solutions%2520in%250Amedical%2520workflows%2520are%2520the%2520availability%2520of%2520annotated%2520data%2520and%2520the%2520lack%2520of%250Ainterpretability%2520of%2520such%2520systems.%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520tackle%2520the%250Alatter%2520by%2520constraining%2520the%2520final%2520disease%2520prediction%2520on%2520a%2520set%2520of%2520predefined%2520and%250Ahuman-interpretable%2520concepts.%2520However%252C%2520the%2520increased%2520interpretability%2520achieved%250Athrough%2520these%2520concept-based%2520explanations%2520implies%2520a%2520higher%2520annotation%2520burden.%250AMoreover%252C%2520if%2520a%2520new%2520concept%2520needs%2520to%2520be%2520added%252C%2520the%2520whole%2520system%2520needs%2520to%2520be%250Aretrained.%2520Inspired%2520by%2520the%2520remarkable%2520performance%2520shown%2520by%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%2520in%2520few-shot%2520settings%252C%2520we%2520propose%2520a%2520simple%252C%2520yet%250Aeffective%252C%2520methodology%252C%2520CBVLM%252C%2520which%2520tackles%2520both%2520of%2520the%2520aforementioned%250Achallenges.%2520First%252C%2520for%2520each%2520concept%252C%2520we%2520prompt%2520the%2520LVLM%2520to%2520answer%2520if%2520the%250Aconcept%2520is%2520present%2520in%2520the%2520input%2520image.%2520Then%252C%2520we%2520ask%2520the%2520LVLM%2520to%2520classify%2520the%250Aimage%2520based%2520on%2520the%2520previous%2520concept%2520predictions.%2520Moreover%252C%2520in%2520both%2520stages%252C%2520we%250Aincorporate%2520a%2520retrieval%2520module%2520responsible%2520for%2520selecting%2520the%2520best%2520examples%2520for%250Ain-context%2520learning.%2520By%2520grounding%2520the%2520final%2520diagnosis%2520on%2520the%2520predicted%250Aconcepts%252C%2520we%2520ensure%2520explainability%252C%2520and%2520by%2520leveraging%2520the%2520few-shot%2520capabilities%250Aof%2520LVLMs%252C%2520we%2520drastically%2520lower%2520the%2520annotation%2520cost.%2520We%2520validate%2520our%2520approach%250Awith%2520extensive%2520experiments%2520across%2520four%2520medical%2520datasets%2520and%2520twelve%2520LVLMs%2520%2528both%250Ageneric%2520and%2520medical%2529%2520and%2520show%2520that%2520CBVLM%2520consistently%2520outperforms%2520CBMs%2520and%250Atask-specific%2520supervised%2520methods%2520without%2520requiring%2520any%2520training%2520and%2520using%2520just%250Aa%2520few%2520annotated%2520examples.%2520More%2520information%2520on%2520our%2520project%2520page%253A%250Ahttps%253A//cristianopatricio.github.io/CBVLM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CBVLM%3A%20Training-free%20Explainable%20Concept-based%20Large%20Vision%20Language%0A%20%20Models%20for%20Medical%20Image%20Classification&entry.906535625=Cristiano%20Patr%C3%ADcio%20and%20Isabel%20Rio-Torto%20and%20Jaime%20S.%20Cardoso%20and%20Lu%C3%ADs%20F.%20Teixeira%20and%20Jo%C3%A3o%20C.%20Neves&entry.1292438233=%20%20The%20main%20challenges%20limiting%20the%20adoption%20of%20deep%20learning-based%20solutions%20in%0Amedical%20workflows%20are%20the%20availability%20of%20annotated%20data%20and%20the%20lack%20of%0Ainterpretability%20of%20such%20systems.%20Concept%20Bottleneck%20Models%20%28CBMs%29%20tackle%20the%0Alatter%20by%20constraining%20the%20final%20disease%20prediction%20on%20a%20set%20of%20predefined%20and%0Ahuman-interpretable%20concepts.%20However%2C%20the%20increased%20interpretability%20achieved%0Athrough%20these%20concept-based%20explanations%20implies%20a%20higher%20annotation%20burden.%0AMoreover%2C%20if%20a%20new%20concept%20needs%20to%20be%20added%2C%20the%20whole%20system%20needs%20to%20be%0Aretrained.%20Inspired%20by%20the%20remarkable%20performance%20shown%20by%20Large%0AVision-Language%20Models%20%28LVLMs%29%20in%20few-shot%20settings%2C%20we%20propose%20a%20simple%2C%20yet%0Aeffective%2C%20methodology%2C%20CBVLM%2C%20which%20tackles%20both%20of%20the%20aforementioned%0Achallenges.%20First%2C%20for%20each%20concept%2C%20we%20prompt%20the%20LVLM%20to%20answer%20if%20the%0Aconcept%20is%20present%20in%20the%20input%20image.%20Then%2C%20we%20ask%20the%20LVLM%20to%20classify%20the%0Aimage%20based%20on%20the%20previous%20concept%20predictions.%20Moreover%2C%20in%20both%20stages%2C%20we%0Aincorporate%20a%20retrieval%20module%20responsible%20for%20selecting%20the%20best%20examples%20for%0Ain-context%20learning.%20By%20grounding%20the%20final%20diagnosis%20on%20the%20predicted%0Aconcepts%2C%20we%20ensure%20explainability%2C%20and%20by%20leveraging%20the%20few-shot%20capabilities%0Aof%20LVLMs%2C%20we%20drastically%20lower%20the%20annotation%20cost.%20We%20validate%20our%20approach%0Awith%20extensive%20experiments%20across%20four%20medical%20datasets%20and%20twelve%20LVLMs%20%28both%0Ageneric%20and%20medical%29%20and%20show%20that%20CBVLM%20consistently%20outperforms%20CBMs%20and%0Atask-specific%20supervised%20methods%20without%20requiring%20any%20training%20and%20using%20just%0Aa%20few%20annotated%20examples.%20More%20information%20on%20our%20project%20page%3A%0Ahttps%3A//cristianopatricio.github.io/CBVLM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12266v1&entry.124074799=Read"},
{"title": "PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision", "author": "Arnav M. Das and Chi Ian Tang and Fahim Kawsar and Mohammad Malekzadeh", "abstract": "  Sensing human motions through Inertial Measurement Units (IMUs) embedded in\npersonal devices has enabled significant applications in health and wellness.\nLabeled IMU data is scarce, however, unlabeled or weakly labeled IMU data can\nbe used to model human motions. For video or text modalities, the \"pretrain and\nadapt\" approach utilizes large volumes of unlabeled or weakly labeled data to\nbuild a strong feature extractor, followed by adaptation to specific tasks\nusing limited labeled data. However, pretraining methods are poorly understood\nfor IMU data, and pipelines are rarely evaluated on out-of-domain tasks. We\npropose PRIMUS: a method for PRetraining IMU encoderS that uses a novel\npretraining objective that is empirically validated based on downstream\nperformance on both in-domain and out-of-domain datasets. The PRIMUS objective\neffectively enhances downstream performance by combining self-supervision,\nmultimodal, and nearest-neighbor supervision. With fewer than 500 labeled\nsamples per class, PRIMUS improves test accuracy by up to 15%, compared to\nstate-of-the-art baselines. To benefit the broader community, we have\nopen-sourced our code at github.com/nokia-bell-labs/pretrained-imu-encoders.\n", "link": "http://arxiv.org/abs/2411.15127v2", "date": "2025-01-21", "relevancy": 2.2195, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5636}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5507}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRIMUS%3A%20Pretraining%20IMU%20Encoders%20with%20Multimodal%20Self-Supervision&body=Title%3A%20PRIMUS%3A%20Pretraining%20IMU%20Encoders%20with%20Multimodal%20Self-Supervision%0AAuthor%3A%20Arnav%20M.%20Das%20and%20Chi%20Ian%20Tang%20and%20Fahim%20Kawsar%20and%20Mohammad%20Malekzadeh%0AAbstract%3A%20%20%20Sensing%20human%20motions%20through%20Inertial%20Measurement%20Units%20%28IMUs%29%20embedded%20in%0Apersonal%20devices%20has%20enabled%20significant%20applications%20in%20health%20and%20wellness.%0ALabeled%20IMU%20data%20is%20scarce%2C%20however%2C%20unlabeled%20or%20weakly%20labeled%20IMU%20data%20can%0Abe%20used%20to%20model%20human%20motions.%20For%20video%20or%20text%20modalities%2C%20the%20%22pretrain%20and%0Aadapt%22%20approach%20utilizes%20large%20volumes%20of%20unlabeled%20or%20weakly%20labeled%20data%20to%0Abuild%20a%20strong%20feature%20extractor%2C%20followed%20by%20adaptation%20to%20specific%20tasks%0Ausing%20limited%20labeled%20data.%20However%2C%20pretraining%20methods%20are%20poorly%20understood%0Afor%20IMU%20data%2C%20and%20pipelines%20are%20rarely%20evaluated%20on%20out-of-domain%20tasks.%20We%0Apropose%20PRIMUS%3A%20a%20method%20for%20PRetraining%20IMU%20encoderS%20that%20uses%20a%20novel%0Apretraining%20objective%20that%20is%20empirically%20validated%20based%20on%20downstream%0Aperformance%20on%20both%20in-domain%20and%20out-of-domain%20datasets.%20The%20PRIMUS%20objective%0Aeffectively%20enhances%20downstream%20performance%20by%20combining%20self-supervision%2C%0Amultimodal%2C%20and%20nearest-neighbor%20supervision.%20With%20fewer%20than%20500%20labeled%0Asamples%20per%20class%2C%20PRIMUS%20improves%20test%20accuracy%20by%20up%20to%2015%25%2C%20compared%20to%0Astate-of-the-art%20baselines.%20To%20benefit%20the%20broader%20community%2C%20we%20have%0Aopen-sourced%20our%20code%20at%20github.com/nokia-bell-labs/pretrained-imu-encoders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15127v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRIMUS%253A%2520Pretraining%2520IMU%2520Encoders%2520with%2520Multimodal%2520Self-Supervision%26entry.906535625%3DArnav%2520M.%2520Das%2520and%2520Chi%2520Ian%2520Tang%2520and%2520Fahim%2520Kawsar%2520and%2520Mohammad%2520Malekzadeh%26entry.1292438233%3D%2520%2520Sensing%2520human%2520motions%2520through%2520Inertial%2520Measurement%2520Units%2520%2528IMUs%2529%2520embedded%2520in%250Apersonal%2520devices%2520has%2520enabled%2520significant%2520applications%2520in%2520health%2520and%2520wellness.%250ALabeled%2520IMU%2520data%2520is%2520scarce%252C%2520however%252C%2520unlabeled%2520or%2520weakly%2520labeled%2520IMU%2520data%2520can%250Abe%2520used%2520to%2520model%2520human%2520motions.%2520For%2520video%2520or%2520text%2520modalities%252C%2520the%2520%2522pretrain%2520and%250Aadapt%2522%2520approach%2520utilizes%2520large%2520volumes%2520of%2520unlabeled%2520or%2520weakly%2520labeled%2520data%2520to%250Abuild%2520a%2520strong%2520feature%2520extractor%252C%2520followed%2520by%2520adaptation%2520to%2520specific%2520tasks%250Ausing%2520limited%2520labeled%2520data.%2520However%252C%2520pretraining%2520methods%2520are%2520poorly%2520understood%250Afor%2520IMU%2520data%252C%2520and%2520pipelines%2520are%2520rarely%2520evaluated%2520on%2520out-of-domain%2520tasks.%2520We%250Apropose%2520PRIMUS%253A%2520a%2520method%2520for%2520PRetraining%2520IMU%2520encoderS%2520that%2520uses%2520a%2520novel%250Apretraining%2520objective%2520that%2520is%2520empirically%2520validated%2520based%2520on%2520downstream%250Aperformance%2520on%2520both%2520in-domain%2520and%2520out-of-domain%2520datasets.%2520The%2520PRIMUS%2520objective%250Aeffectively%2520enhances%2520downstream%2520performance%2520by%2520combining%2520self-supervision%252C%250Amultimodal%252C%2520and%2520nearest-neighbor%2520supervision.%2520With%2520fewer%2520than%2520500%2520labeled%250Asamples%2520per%2520class%252C%2520PRIMUS%2520improves%2520test%2520accuracy%2520by%2520up%2520to%252015%2525%252C%2520compared%2520to%250Astate-of-the-art%2520baselines.%2520To%2520benefit%2520the%2520broader%2520community%252C%2520we%2520have%250Aopen-sourced%2520our%2520code%2520at%2520github.com/nokia-bell-labs/pretrained-imu-encoders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15127v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIMUS%3A%20Pretraining%20IMU%20Encoders%20with%20Multimodal%20Self-Supervision&entry.906535625=Arnav%20M.%20Das%20and%20Chi%20Ian%20Tang%20and%20Fahim%20Kawsar%20and%20Mohammad%20Malekzadeh&entry.1292438233=%20%20Sensing%20human%20motions%20through%20Inertial%20Measurement%20Units%20%28IMUs%29%20embedded%20in%0Apersonal%20devices%20has%20enabled%20significant%20applications%20in%20health%20and%20wellness.%0ALabeled%20IMU%20data%20is%20scarce%2C%20however%2C%20unlabeled%20or%20weakly%20labeled%20IMU%20data%20can%0Abe%20used%20to%20model%20human%20motions.%20For%20video%20or%20text%20modalities%2C%20the%20%22pretrain%20and%0Aadapt%22%20approach%20utilizes%20large%20volumes%20of%20unlabeled%20or%20weakly%20labeled%20data%20to%0Abuild%20a%20strong%20feature%20extractor%2C%20followed%20by%20adaptation%20to%20specific%20tasks%0Ausing%20limited%20labeled%20data.%20However%2C%20pretraining%20methods%20are%20poorly%20understood%0Afor%20IMU%20data%2C%20and%20pipelines%20are%20rarely%20evaluated%20on%20out-of-domain%20tasks.%20We%0Apropose%20PRIMUS%3A%20a%20method%20for%20PRetraining%20IMU%20encoderS%20that%20uses%20a%20novel%0Apretraining%20objective%20that%20is%20empirically%20validated%20based%20on%20downstream%0Aperformance%20on%20both%20in-domain%20and%20out-of-domain%20datasets.%20The%20PRIMUS%20objective%0Aeffectively%20enhances%20downstream%20performance%20by%20combining%20self-supervision%2C%0Amultimodal%2C%20and%20nearest-neighbor%20supervision.%20With%20fewer%20than%20500%20labeled%0Asamples%20per%20class%2C%20PRIMUS%20improves%20test%20accuracy%20by%20up%20to%2015%25%2C%20compared%20to%0Astate-of-the-art%20baselines.%20To%20benefit%20the%20broader%20community%2C%20we%20have%0Aopen-sourced%20our%20code%20at%20github.com/nokia-bell-labs/pretrained-imu-encoders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15127v2&entry.124074799=Read"},
{"title": "VARGPT: Unified Understanding and Generation in a Visual Autoregressive\n  Multimodal Large Language Model", "author": "Xianwei Zhuang and Yuxin Xie and Yufan Deng and Liming Liang and Jinghan Ru and Yuguo Yin and Yuexian Zou", "abstract": "  We present VARGPT, a novel multimodal large language model (MLLM) that\nunifies visual understanding and generation within a single autoregressive\nframework. VARGPT employs a next-token prediction paradigm for visual\nunderstanding and a next-scale prediction paradigm for visual autoregressive\ngeneration. VARGPT innovatively extends the LLaVA architecture, achieving\nefficient scale-wise autoregressive visual generation within MLLMs while\nseamlessly accommodating mixed-modal input and output within a single model\nframework. Our VARGPT undergoes a three-stage unified training process on\nspecially curated datasets, comprising a pre-training phase and two mixed\nvisual instruction-tuning phases. The unified training strategy are designed to\nachieve alignment between visual and textual features, enhance instruction\nfollowing for both understanding and generation, and improve visual generation\nquality, respectively. Despite its LLAVA-based architecture for multimodel\nunderstanding, VARGPT significantly outperforms LLaVA-1.5 across various\nvision-centric benchmarks, such as visual question-answering and reasoning\ntasks. Notably, VARGPT naturally supports capabilities in autoregressive visual\ngeneration and instruction-to-image synthesis, showcasing its versatility in\nboth visual understanding and generation tasks. Project page is at:\n\\url{https://vargpt-1.github.io/}\n", "link": "http://arxiv.org/abs/2501.12327v1", "date": "2025-01-21", "relevancy": 2.218, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5578}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5537}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VARGPT%3A%20Unified%20Understanding%20and%20Generation%20in%20a%20Visual%20Autoregressive%0A%20%20Multimodal%20Large%20Language%20Model&body=Title%3A%20VARGPT%3A%20Unified%20Understanding%20and%20Generation%20in%20a%20Visual%20Autoregressive%0A%20%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Xianwei%20Zhuang%20and%20Yuxin%20Xie%20and%20Yufan%20Deng%20and%20Liming%20Liang%20and%20Jinghan%20Ru%20and%20Yuguo%20Yin%20and%20Yuexian%20Zou%0AAbstract%3A%20%20%20We%20present%20VARGPT%2C%20a%20novel%20multimodal%20large%20language%20model%20%28MLLM%29%20that%0Aunifies%20visual%20understanding%20and%20generation%20within%20a%20single%20autoregressive%0Aframework.%20VARGPT%20employs%20a%20next-token%20prediction%20paradigm%20for%20visual%0Aunderstanding%20and%20a%20next-scale%20prediction%20paradigm%20for%20visual%20autoregressive%0Ageneration.%20VARGPT%20innovatively%20extends%20the%20LLaVA%20architecture%2C%20achieving%0Aefficient%20scale-wise%20autoregressive%20visual%20generation%20within%20MLLMs%20while%0Aseamlessly%20accommodating%20mixed-modal%20input%20and%20output%20within%20a%20single%20model%0Aframework.%20Our%20VARGPT%20undergoes%20a%20three-stage%20unified%20training%20process%20on%0Aspecially%20curated%20datasets%2C%20comprising%20a%20pre-training%20phase%20and%20two%20mixed%0Avisual%20instruction-tuning%20phases.%20The%20unified%20training%20strategy%20are%20designed%20to%0Aachieve%20alignment%20between%20visual%20and%20textual%20features%2C%20enhance%20instruction%0Afollowing%20for%20both%20understanding%20and%20generation%2C%20and%20improve%20visual%20generation%0Aquality%2C%20respectively.%20Despite%20its%20LLAVA-based%20architecture%20for%20multimodel%0Aunderstanding%2C%20VARGPT%20significantly%20outperforms%20LLaVA-1.5%20across%20various%0Avision-centric%20benchmarks%2C%20such%20as%20visual%20question-answering%20and%20reasoning%0Atasks.%20Notably%2C%20VARGPT%20naturally%20supports%20capabilities%20in%20autoregressive%20visual%0Ageneration%20and%20instruction-to-image%20synthesis%2C%20showcasing%20its%20versatility%20in%0Aboth%20visual%20understanding%20and%20generation%20tasks.%20Project%20page%20is%20at%3A%0A%5Curl%7Bhttps%3A//vargpt-1.github.io/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVARGPT%253A%2520Unified%2520Understanding%2520and%2520Generation%2520in%2520a%2520Visual%2520Autoregressive%250A%2520%2520Multimodal%2520Large%2520Language%2520Model%26entry.906535625%3DXianwei%2520Zhuang%2520and%2520Yuxin%2520Xie%2520and%2520Yufan%2520Deng%2520and%2520Liming%2520Liang%2520and%2520Jinghan%2520Ru%2520and%2520Yuguo%2520Yin%2520and%2520Yuexian%2520Zou%26entry.1292438233%3D%2520%2520We%2520present%2520VARGPT%252C%2520a%2520novel%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520that%250Aunifies%2520visual%2520understanding%2520and%2520generation%2520within%2520a%2520single%2520autoregressive%250Aframework.%2520VARGPT%2520employs%2520a%2520next-token%2520prediction%2520paradigm%2520for%2520visual%250Aunderstanding%2520and%2520a%2520next-scale%2520prediction%2520paradigm%2520for%2520visual%2520autoregressive%250Ageneration.%2520VARGPT%2520innovatively%2520extends%2520the%2520LLaVA%2520architecture%252C%2520achieving%250Aefficient%2520scale-wise%2520autoregressive%2520visual%2520generation%2520within%2520MLLMs%2520while%250Aseamlessly%2520accommodating%2520mixed-modal%2520input%2520and%2520output%2520within%2520a%2520single%2520model%250Aframework.%2520Our%2520VARGPT%2520undergoes%2520a%2520three-stage%2520unified%2520training%2520process%2520on%250Aspecially%2520curated%2520datasets%252C%2520comprising%2520a%2520pre-training%2520phase%2520and%2520two%2520mixed%250Avisual%2520instruction-tuning%2520phases.%2520The%2520unified%2520training%2520strategy%2520are%2520designed%2520to%250Aachieve%2520alignment%2520between%2520visual%2520and%2520textual%2520features%252C%2520enhance%2520instruction%250Afollowing%2520for%2520both%2520understanding%2520and%2520generation%252C%2520and%2520improve%2520visual%2520generation%250Aquality%252C%2520respectively.%2520Despite%2520its%2520LLAVA-based%2520architecture%2520for%2520multimodel%250Aunderstanding%252C%2520VARGPT%2520significantly%2520outperforms%2520LLaVA-1.5%2520across%2520various%250Avision-centric%2520benchmarks%252C%2520such%2520as%2520visual%2520question-answering%2520and%2520reasoning%250Atasks.%2520Notably%252C%2520VARGPT%2520naturally%2520supports%2520capabilities%2520in%2520autoregressive%2520visual%250Ageneration%2520and%2520instruction-to-image%2520synthesis%252C%2520showcasing%2520its%2520versatility%2520in%250Aboth%2520visual%2520understanding%2520and%2520generation%2520tasks.%2520Project%2520page%2520is%2520at%253A%250A%255Curl%257Bhttps%253A//vargpt-1.github.io/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VARGPT%3A%20Unified%20Understanding%20and%20Generation%20in%20a%20Visual%20Autoregressive%0A%20%20Multimodal%20Large%20Language%20Model&entry.906535625=Xianwei%20Zhuang%20and%20Yuxin%20Xie%20and%20Yufan%20Deng%20and%20Liming%20Liang%20and%20Jinghan%20Ru%20and%20Yuguo%20Yin%20and%20Yuexian%20Zou&entry.1292438233=%20%20We%20present%20VARGPT%2C%20a%20novel%20multimodal%20large%20language%20model%20%28MLLM%29%20that%0Aunifies%20visual%20understanding%20and%20generation%20within%20a%20single%20autoregressive%0Aframework.%20VARGPT%20employs%20a%20next-token%20prediction%20paradigm%20for%20visual%0Aunderstanding%20and%20a%20next-scale%20prediction%20paradigm%20for%20visual%20autoregressive%0Ageneration.%20VARGPT%20innovatively%20extends%20the%20LLaVA%20architecture%2C%20achieving%0Aefficient%20scale-wise%20autoregressive%20visual%20generation%20within%20MLLMs%20while%0Aseamlessly%20accommodating%20mixed-modal%20input%20and%20output%20within%20a%20single%20model%0Aframework.%20Our%20VARGPT%20undergoes%20a%20three-stage%20unified%20training%20process%20on%0Aspecially%20curated%20datasets%2C%20comprising%20a%20pre-training%20phase%20and%20two%20mixed%0Avisual%20instruction-tuning%20phases.%20The%20unified%20training%20strategy%20are%20designed%20to%0Aachieve%20alignment%20between%20visual%20and%20textual%20features%2C%20enhance%20instruction%0Afollowing%20for%20both%20understanding%20and%20generation%2C%20and%20improve%20visual%20generation%0Aquality%2C%20respectively.%20Despite%20its%20LLAVA-based%20architecture%20for%20multimodel%0Aunderstanding%2C%20VARGPT%20significantly%20outperforms%20LLaVA-1.5%20across%20various%0Avision-centric%20benchmarks%2C%20such%20as%20visual%20question-answering%20and%20reasoning%0Atasks.%20Notably%2C%20VARGPT%20naturally%20supports%20capabilities%20in%20autoregressive%20visual%0Ageneration%20and%20instruction-to-image%20synthesis%2C%20showcasing%20its%20versatility%20in%0Aboth%20visual%20understanding%20and%20generation%20tasks.%20Project%20page%20is%20at%3A%0A%5Curl%7Bhttps%3A//vargpt-1.github.io/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12327v1&entry.124074799=Read"},
{"title": "LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent\n  Diffusion Models", "author": "Seyedmorteza Sadat and Jakob Buhmann and Derek Bradley and Otmar Hilliges and Romann M. Weber", "abstract": "  Advances in latent diffusion models (LDMs) have revolutionized\nhigh-resolution image generation, but the design space of the autoencoder that\nis central to these systems remains underexplored. In this paper, we introduce\nLiteVAE, a new autoencoder design for LDMs, which leverages the 2D discrete\nwavelet transform to enhance scalability and computational efficiency over\nstandard variational autoencoders (VAEs) with no sacrifice in output quality.\nWe investigate the training methodologies and the decoder architecture of\nLiteVAE and propose several enhancements that improve the training dynamics and\nreconstruction quality. Our base LiteVAE model matches the quality of the\nestablished VAEs in current LDMs with a six-fold reduction in encoder\nparameters, leading to faster training and lower GPU memory requirements, while\nour larger model outperforms VAEs of comparable complexity across all evaluated\nmetrics (rFID, LPIPS, PSNR, and SSIM).\n", "link": "http://arxiv.org/abs/2405.14477v2", "date": "2025-01-21", "relevancy": 2.2105, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6376}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.536}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiteVAE%3A%20Lightweight%20and%20Efficient%20Variational%20Autoencoders%20for%20Latent%0A%20%20Diffusion%20Models&body=Title%3A%20LiteVAE%3A%20Lightweight%20and%20Efficient%20Variational%20Autoencoders%20for%20Latent%0A%20%20Diffusion%20Models%0AAuthor%3A%20Seyedmorteza%20Sadat%20and%20Jakob%20Buhmann%20and%20Derek%20Bradley%20and%20Otmar%20Hilliges%20and%20Romann%20M.%20Weber%0AAbstract%3A%20%20%20Advances%20in%20latent%20diffusion%20models%20%28LDMs%29%20have%20revolutionized%0Ahigh-resolution%20image%20generation%2C%20but%20the%20design%20space%20of%20the%20autoencoder%20that%0Ais%20central%20to%20these%20systems%20remains%20underexplored.%20In%20this%20paper%2C%20we%20introduce%0ALiteVAE%2C%20a%20new%20autoencoder%20design%20for%20LDMs%2C%20which%20leverages%20the%202D%20discrete%0Awavelet%20transform%20to%20enhance%20scalability%20and%20computational%20efficiency%20over%0Astandard%20variational%20autoencoders%20%28VAEs%29%20with%20no%20sacrifice%20in%20output%20quality.%0AWe%20investigate%20the%20training%20methodologies%20and%20the%20decoder%20architecture%20of%0ALiteVAE%20and%20propose%20several%20enhancements%20that%20improve%20the%20training%20dynamics%20and%0Areconstruction%20quality.%20Our%20base%20LiteVAE%20model%20matches%20the%20quality%20of%20the%0Aestablished%20VAEs%20in%20current%20LDMs%20with%20a%20six-fold%20reduction%20in%20encoder%0Aparameters%2C%20leading%20to%20faster%20training%20and%20lower%20GPU%20memory%20requirements%2C%20while%0Aour%20larger%20model%20outperforms%20VAEs%20of%20comparable%20complexity%20across%20all%20evaluated%0Ametrics%20%28rFID%2C%20LPIPS%2C%20PSNR%2C%20and%20SSIM%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiteVAE%253A%2520Lightweight%2520and%2520Efficient%2520Variational%2520Autoencoders%2520for%2520Latent%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DSeyedmorteza%2520Sadat%2520and%2520Jakob%2520Buhmann%2520and%2520Derek%2520Bradley%2520and%2520Otmar%2520Hilliges%2520and%2520Romann%2520M.%2520Weber%26entry.1292438233%3D%2520%2520Advances%2520in%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%2520have%2520revolutionized%250Ahigh-resolution%2520image%2520generation%252C%2520but%2520the%2520design%2520space%2520of%2520the%2520autoencoder%2520that%250Ais%2520central%2520to%2520these%2520systems%2520remains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ALiteVAE%252C%2520a%2520new%2520autoencoder%2520design%2520for%2520LDMs%252C%2520which%2520leverages%2520the%25202D%2520discrete%250Awavelet%2520transform%2520to%2520enhance%2520scalability%2520and%2520computational%2520efficiency%2520over%250Astandard%2520variational%2520autoencoders%2520%2528VAEs%2529%2520with%2520no%2520sacrifice%2520in%2520output%2520quality.%250AWe%2520investigate%2520the%2520training%2520methodologies%2520and%2520the%2520decoder%2520architecture%2520of%250ALiteVAE%2520and%2520propose%2520several%2520enhancements%2520that%2520improve%2520the%2520training%2520dynamics%2520and%250Areconstruction%2520quality.%2520Our%2520base%2520LiteVAE%2520model%2520matches%2520the%2520quality%2520of%2520the%250Aestablished%2520VAEs%2520in%2520current%2520LDMs%2520with%2520a%2520six-fold%2520reduction%2520in%2520encoder%250Aparameters%252C%2520leading%2520to%2520faster%2520training%2520and%2520lower%2520GPU%2520memory%2520requirements%252C%2520while%250Aour%2520larger%2520model%2520outperforms%2520VAEs%2520of%2520comparable%2520complexity%2520across%2520all%2520evaluated%250Ametrics%2520%2528rFID%252C%2520LPIPS%252C%2520PSNR%252C%2520and%2520SSIM%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiteVAE%3A%20Lightweight%20and%20Efficient%20Variational%20Autoencoders%20for%20Latent%0A%20%20Diffusion%20Models&entry.906535625=Seyedmorteza%20Sadat%20and%20Jakob%20Buhmann%20and%20Derek%20Bradley%20and%20Otmar%20Hilliges%20and%20Romann%20M.%20Weber&entry.1292438233=%20%20Advances%20in%20latent%20diffusion%20models%20%28LDMs%29%20have%20revolutionized%0Ahigh-resolution%20image%20generation%2C%20but%20the%20design%20space%20of%20the%20autoencoder%20that%0Ais%20central%20to%20these%20systems%20remains%20underexplored.%20In%20this%20paper%2C%20we%20introduce%0ALiteVAE%2C%20a%20new%20autoencoder%20design%20for%20LDMs%2C%20which%20leverages%20the%202D%20discrete%0Awavelet%20transform%20to%20enhance%20scalability%20and%20computational%20efficiency%20over%0Astandard%20variational%20autoencoders%20%28VAEs%29%20with%20no%20sacrifice%20in%20output%20quality.%0AWe%20investigate%20the%20training%20methodologies%20and%20the%20decoder%20architecture%20of%0ALiteVAE%20and%20propose%20several%20enhancements%20that%20improve%20the%20training%20dynamics%20and%0Areconstruction%20quality.%20Our%20base%20LiteVAE%20model%20matches%20the%20quality%20of%20the%0Aestablished%20VAEs%20in%20current%20LDMs%20with%20a%20six-fold%20reduction%20in%20encoder%0Aparameters%2C%20leading%20to%20faster%20training%20and%20lower%20GPU%20memory%20requirements%2C%20while%0Aour%20larger%20model%20outperforms%20VAEs%20of%20comparable%20complexity%20across%20all%20evaluated%0Ametrics%20%28rFID%2C%20LPIPS%2C%20PSNR%2C%20and%20SSIM%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14477v2&entry.124074799=Read"},
{"title": "Untrained Perceptual Loss for image denoising of line-like structures in\n  MR images", "author": "Elisabeth Pfaehler and Daniel Pflugfelder and Hanno Scharr", "abstract": "  In the acquisition of Magnetic Resonance (MR) images shorter scan times lead\nto higher image noise. Therefore, automatic image denoising using deep learning\nmethods is of high interest. MR images containing line-like structures such as\nroots or vessels yield special characteristics as they display connected\nstructures and yield sparse information. For this kind of data, it is important\nto consider voxel neighborhoods when training a denoising network. In this\npaper, we translate the Perceptual Loss to 3D data by comparing feature maps of\nuntrained networks in the loss function as done previously for 2D data. We\ntested the performance of untrained Perceptual Loss (uPL) on 3D image denoising\nof MR images displaying brain vessels (MR angiograms - MRA) and images of plant\nroots in soil. We investigate the impact of various uPL characteristics such as\nweight initialization, network depth, kernel size, and pooling operations on\nthe results. We tested the performance of the uPL loss on four Rician noise\nlevels using evaluation metrics such as the Structural Similarity Index Metric\n(SSIM). We observe, that our uPL outperforms conventional loss functions such\nas the L1 loss or a loss based on the Structural Similarity Index Metric\n(SSIM). The uPL network's initialization is not important, while network depth\nand pooling operations impact denoising performance. E.g. for both datasets a\nnetwork with five convolutional layers led to the best performance while a\nnetwork with more layers led to a performance drop. We also find that small uPL\nnetworks led to better or comparable results than using large networks such as\nVGG. We observe superior performance of our loss for both datasets, all noise\nlevels, and three network architectures. In conclusion, for images containing\nline-like structures, uPL is an alternative to other loss functions for 3D\nimage denoising.\n", "link": "http://arxiv.org/abs/2411.05884v2", "date": "2025-01-21", "relevancy": 2.2052, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5756}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5518}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Untrained%20Perceptual%20Loss%20for%20image%20denoising%20of%20line-like%20structures%20in%0A%20%20MR%20images&body=Title%3A%20Untrained%20Perceptual%20Loss%20for%20image%20denoising%20of%20line-like%20structures%20in%0A%20%20MR%20images%0AAuthor%3A%20Elisabeth%20Pfaehler%20and%20Daniel%20Pflugfelder%20and%20Hanno%20Scharr%0AAbstract%3A%20%20%20In%20the%20acquisition%20of%20Magnetic%20Resonance%20%28MR%29%20images%20shorter%20scan%20times%20lead%0Ato%20higher%20image%20noise.%20Therefore%2C%20automatic%20image%20denoising%20using%20deep%20learning%0Amethods%20is%20of%20high%20interest.%20MR%20images%20containing%20line-like%20structures%20such%20as%0Aroots%20or%20vessels%20yield%20special%20characteristics%20as%20they%20display%20connected%0Astructures%20and%20yield%20sparse%20information.%20For%20this%20kind%20of%20data%2C%20it%20is%20important%0Ato%20consider%20voxel%20neighborhoods%20when%20training%20a%20denoising%20network.%20In%20this%0Apaper%2C%20we%20translate%20the%20Perceptual%20Loss%20to%203D%20data%20by%20comparing%20feature%20maps%20of%0Auntrained%20networks%20in%20the%20loss%20function%20as%20done%20previously%20for%202D%20data.%20We%0Atested%20the%20performance%20of%20untrained%20Perceptual%20Loss%20%28uPL%29%20on%203D%20image%20denoising%0Aof%20MR%20images%20displaying%20brain%20vessels%20%28MR%20angiograms%20-%20MRA%29%20and%20images%20of%20plant%0Aroots%20in%20soil.%20We%20investigate%20the%20impact%20of%20various%20uPL%20characteristics%20such%20as%0Aweight%20initialization%2C%20network%20depth%2C%20kernel%20size%2C%20and%20pooling%20operations%20on%0Athe%20results.%20We%20tested%20the%20performance%20of%20the%20uPL%20loss%20on%20four%20Rician%20noise%0Alevels%20using%20evaluation%20metrics%20such%20as%20the%20Structural%20Similarity%20Index%20Metric%0A%28SSIM%29.%20We%20observe%2C%20that%20our%20uPL%20outperforms%20conventional%20loss%20functions%20such%0Aas%20the%20L1%20loss%20or%20a%20loss%20based%20on%20the%20Structural%20Similarity%20Index%20Metric%0A%28SSIM%29.%20The%20uPL%20network%27s%20initialization%20is%20not%20important%2C%20while%20network%20depth%0Aand%20pooling%20operations%20impact%20denoising%20performance.%20E.g.%20for%20both%20datasets%20a%0Anetwork%20with%20five%20convolutional%20layers%20led%20to%20the%20best%20performance%20while%20a%0Anetwork%20with%20more%20layers%20led%20to%20a%20performance%20drop.%20We%20also%20find%20that%20small%20uPL%0Anetworks%20led%20to%20better%20or%20comparable%20results%20than%20using%20large%20networks%20such%20as%0AVGG.%20We%20observe%20superior%20performance%20of%20our%20loss%20for%20both%20datasets%2C%20all%20noise%0Alevels%2C%20and%20three%20network%20architectures.%20In%20conclusion%2C%20for%20images%20containing%0Aline-like%20structures%2C%20uPL%20is%20an%20alternative%20to%20other%20loss%20functions%20for%203D%0Aimage%20denoising.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUntrained%2520Perceptual%2520Loss%2520for%2520image%2520denoising%2520of%2520line-like%2520structures%2520in%250A%2520%2520MR%2520images%26entry.906535625%3DElisabeth%2520Pfaehler%2520and%2520Daniel%2520Pflugfelder%2520and%2520Hanno%2520Scharr%26entry.1292438233%3D%2520%2520In%2520the%2520acquisition%2520of%2520Magnetic%2520Resonance%2520%2528MR%2529%2520images%2520shorter%2520scan%2520times%2520lead%250Ato%2520higher%2520image%2520noise.%2520Therefore%252C%2520automatic%2520image%2520denoising%2520using%2520deep%2520learning%250Amethods%2520is%2520of%2520high%2520interest.%2520MR%2520images%2520containing%2520line-like%2520structures%2520such%2520as%250Aroots%2520or%2520vessels%2520yield%2520special%2520characteristics%2520as%2520they%2520display%2520connected%250Astructures%2520and%2520yield%2520sparse%2520information.%2520For%2520this%2520kind%2520of%2520data%252C%2520it%2520is%2520important%250Ato%2520consider%2520voxel%2520neighborhoods%2520when%2520training%2520a%2520denoising%2520network.%2520In%2520this%250Apaper%252C%2520we%2520translate%2520the%2520Perceptual%2520Loss%2520to%25203D%2520data%2520by%2520comparing%2520feature%2520maps%2520of%250Auntrained%2520networks%2520in%2520the%2520loss%2520function%2520as%2520done%2520previously%2520for%25202D%2520data.%2520We%250Atested%2520the%2520performance%2520of%2520untrained%2520Perceptual%2520Loss%2520%2528uPL%2529%2520on%25203D%2520image%2520denoising%250Aof%2520MR%2520images%2520displaying%2520brain%2520vessels%2520%2528MR%2520angiograms%2520-%2520MRA%2529%2520and%2520images%2520of%2520plant%250Aroots%2520in%2520soil.%2520We%2520investigate%2520the%2520impact%2520of%2520various%2520uPL%2520characteristics%2520such%2520as%250Aweight%2520initialization%252C%2520network%2520depth%252C%2520kernel%2520size%252C%2520and%2520pooling%2520operations%2520on%250Athe%2520results.%2520We%2520tested%2520the%2520performance%2520of%2520the%2520uPL%2520loss%2520on%2520four%2520Rician%2520noise%250Alevels%2520using%2520evaluation%2520metrics%2520such%2520as%2520the%2520Structural%2520Similarity%2520Index%2520Metric%250A%2528SSIM%2529.%2520We%2520observe%252C%2520that%2520our%2520uPL%2520outperforms%2520conventional%2520loss%2520functions%2520such%250Aas%2520the%2520L1%2520loss%2520or%2520a%2520loss%2520based%2520on%2520the%2520Structural%2520Similarity%2520Index%2520Metric%250A%2528SSIM%2529.%2520The%2520uPL%2520network%2527s%2520initialization%2520is%2520not%2520important%252C%2520while%2520network%2520depth%250Aand%2520pooling%2520operations%2520impact%2520denoising%2520performance.%2520E.g.%2520for%2520both%2520datasets%2520a%250Anetwork%2520with%2520five%2520convolutional%2520layers%2520led%2520to%2520the%2520best%2520performance%2520while%2520a%250Anetwork%2520with%2520more%2520layers%2520led%2520to%2520a%2520performance%2520drop.%2520We%2520also%2520find%2520that%2520small%2520uPL%250Anetworks%2520led%2520to%2520better%2520or%2520comparable%2520results%2520than%2520using%2520large%2520networks%2520such%2520as%250AVGG.%2520We%2520observe%2520superior%2520performance%2520of%2520our%2520loss%2520for%2520both%2520datasets%252C%2520all%2520noise%250Alevels%252C%2520and%2520three%2520network%2520architectures.%2520In%2520conclusion%252C%2520for%2520images%2520containing%250Aline-like%2520structures%252C%2520uPL%2520is%2520an%2520alternative%2520to%2520other%2520loss%2520functions%2520for%25203D%250Aimage%2520denoising.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Untrained%20Perceptual%20Loss%20for%20image%20denoising%20of%20line-like%20structures%20in%0A%20%20MR%20images&entry.906535625=Elisabeth%20Pfaehler%20and%20Daniel%20Pflugfelder%20and%20Hanno%20Scharr&entry.1292438233=%20%20In%20the%20acquisition%20of%20Magnetic%20Resonance%20%28MR%29%20images%20shorter%20scan%20times%20lead%0Ato%20higher%20image%20noise.%20Therefore%2C%20automatic%20image%20denoising%20using%20deep%20learning%0Amethods%20is%20of%20high%20interest.%20MR%20images%20containing%20line-like%20structures%20such%20as%0Aroots%20or%20vessels%20yield%20special%20characteristics%20as%20they%20display%20connected%0Astructures%20and%20yield%20sparse%20information.%20For%20this%20kind%20of%20data%2C%20it%20is%20important%0Ato%20consider%20voxel%20neighborhoods%20when%20training%20a%20denoising%20network.%20In%20this%0Apaper%2C%20we%20translate%20the%20Perceptual%20Loss%20to%203D%20data%20by%20comparing%20feature%20maps%20of%0Auntrained%20networks%20in%20the%20loss%20function%20as%20done%20previously%20for%202D%20data.%20We%0Atested%20the%20performance%20of%20untrained%20Perceptual%20Loss%20%28uPL%29%20on%203D%20image%20denoising%0Aof%20MR%20images%20displaying%20brain%20vessels%20%28MR%20angiograms%20-%20MRA%29%20and%20images%20of%20plant%0Aroots%20in%20soil.%20We%20investigate%20the%20impact%20of%20various%20uPL%20characteristics%20such%20as%0Aweight%20initialization%2C%20network%20depth%2C%20kernel%20size%2C%20and%20pooling%20operations%20on%0Athe%20results.%20We%20tested%20the%20performance%20of%20the%20uPL%20loss%20on%20four%20Rician%20noise%0Alevels%20using%20evaluation%20metrics%20such%20as%20the%20Structural%20Similarity%20Index%20Metric%0A%28SSIM%29.%20We%20observe%2C%20that%20our%20uPL%20outperforms%20conventional%20loss%20functions%20such%0Aas%20the%20L1%20loss%20or%20a%20loss%20based%20on%20the%20Structural%20Similarity%20Index%20Metric%0A%28SSIM%29.%20The%20uPL%20network%27s%20initialization%20is%20not%20important%2C%20while%20network%20depth%0Aand%20pooling%20operations%20impact%20denoising%20performance.%20E.g.%20for%20both%20datasets%20a%0Anetwork%20with%20five%20convolutional%20layers%20led%20to%20the%20best%20performance%20while%20a%0Anetwork%20with%20more%20layers%20led%20to%20a%20performance%20drop.%20We%20also%20find%20that%20small%20uPL%0Anetworks%20led%20to%20better%20or%20comparable%20results%20than%20using%20large%20networks%20such%20as%0AVGG.%20We%20observe%20superior%20performance%20of%20our%20loss%20for%20both%20datasets%2C%20all%20noise%0Alevels%2C%20and%20three%20network%20architectures.%20In%20conclusion%2C%20for%20images%20containing%0Aline-like%20structures%2C%20uPL%20is%20an%20alternative%20to%20other%20loss%20functions%20for%203D%0Aimage%20denoising.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05884v2&entry.124074799=Read"},
{"title": "Generative Topological Networks", "author": "Alona Levy-Jurgenson and Zohar Yakhini", "abstract": "  Generative methods have recently seen significant improvements by generating\nin a lower-dimensional latent representation of the data. However, many of the\ngenerative methods applied in the latent space remain complex and difficult to\ntrain. Further, it is not entirely clear why transitioning to a\nlower-dimensional latent space can improve generative quality. In this work, we\nintroduce a new and simple generative method grounded in topology theory --\nGenerative Topological Networks (GTNs) -- which also provides insights into why\nlower-dimensional latent-space representations might be better-suited for data\ngeneration. GTNs are simple to train -- they employ a standard supervised\nlearning approach and do not suffer from common generative pitfalls such as\nmode collapse, posterior collapse or the need to pose constraints on the neural\nnetwork architecture. We demonstrate the use of GTNs on several datasets,\nincluding MNIST, CelebA, CIFAR-10 and the Hands and Palm Images dataset by\ntraining GTNs on a lower-dimensional latent representation of the data. We show\nthat GTNs can improve upon VAEs and that they are quick to converge, generating\nrealistic samples in early epochs. Further, we use the topological\nconsiderations behind the development of GTNs to offer insights into why\ngenerative models may benefit from operating on a lower-dimensional latent\nspace, highlighting the important link between the intrinsic dimension of the\ndata and the dimension in which the data is generated. Particularly, we\ndemonstrate that generating in high dimensional ambient spaces may be a\ncontributing factor to out-of-distribution samples generated by diffusion\nmodels. We also highlight other topological properties that are important to\nconsider when using and designing generative models. Our code is available at:\nhttps://github.com/alonalj/GTN\n", "link": "http://arxiv.org/abs/2406.15152v3", "date": "2025-01-21", "relevancy": 2.1946, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5575}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5436}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Topological%20Networks&body=Title%3A%20Generative%20Topological%20Networks%0AAuthor%3A%20Alona%20Levy-Jurgenson%20and%20Zohar%20Yakhini%0AAbstract%3A%20%20%20Generative%20methods%20have%20recently%20seen%20significant%20improvements%20by%20generating%0Ain%20a%20lower-dimensional%20latent%20representation%20of%20the%20data.%20However%2C%20many%20of%20the%0Agenerative%20methods%20applied%20in%20the%20latent%20space%20remain%20complex%20and%20difficult%20to%0Atrain.%20Further%2C%20it%20is%20not%20entirely%20clear%20why%20transitioning%20to%20a%0Alower-dimensional%20latent%20space%20can%20improve%20generative%20quality.%20In%20this%20work%2C%20we%0Aintroduce%20a%20new%20and%20simple%20generative%20method%20grounded%20in%20topology%20theory%20--%0AGenerative%20Topological%20Networks%20%28GTNs%29%20--%20which%20also%20provides%20insights%20into%20why%0Alower-dimensional%20latent-space%20representations%20might%20be%20better-suited%20for%20data%0Ageneration.%20GTNs%20are%20simple%20to%20train%20--%20they%20employ%20a%20standard%20supervised%0Alearning%20approach%20and%20do%20not%20suffer%20from%20common%20generative%20pitfalls%20such%20as%0Amode%20collapse%2C%20posterior%20collapse%20or%20the%20need%20to%20pose%20constraints%20on%20the%20neural%0Anetwork%20architecture.%20We%20demonstrate%20the%20use%20of%20GTNs%20on%20several%20datasets%2C%0Aincluding%20MNIST%2C%20CelebA%2C%20CIFAR-10%20and%20the%20Hands%20and%20Palm%20Images%20dataset%20by%0Atraining%20GTNs%20on%20a%20lower-dimensional%20latent%20representation%20of%20the%20data.%20We%20show%0Athat%20GTNs%20can%20improve%20upon%20VAEs%20and%20that%20they%20are%20quick%20to%20converge%2C%20generating%0Arealistic%20samples%20in%20early%20epochs.%20Further%2C%20we%20use%20the%20topological%0Aconsiderations%20behind%20the%20development%20of%20GTNs%20to%20offer%20insights%20into%20why%0Agenerative%20models%20may%20benefit%20from%20operating%20on%20a%20lower-dimensional%20latent%0Aspace%2C%20highlighting%20the%20important%20link%20between%20the%20intrinsic%20dimension%20of%20the%0Adata%20and%20the%20dimension%20in%20which%20the%20data%20is%20generated.%20Particularly%2C%20we%0Ademonstrate%20that%20generating%20in%20high%20dimensional%20ambient%20spaces%20may%20be%20a%0Acontributing%20factor%20to%20out-of-distribution%20samples%20generated%20by%20diffusion%0Amodels.%20We%20also%20highlight%20other%20topological%20properties%20that%20are%20important%20to%0Aconsider%20when%20using%20and%20designing%20generative%20models.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/alonalj/GTN%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15152v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Topological%2520Networks%26entry.906535625%3DAlona%2520Levy-Jurgenson%2520and%2520Zohar%2520Yakhini%26entry.1292438233%3D%2520%2520Generative%2520methods%2520have%2520recently%2520seen%2520significant%2520improvements%2520by%2520generating%250Ain%2520a%2520lower-dimensional%2520latent%2520representation%2520of%2520the%2520data.%2520However%252C%2520many%2520of%2520the%250Agenerative%2520methods%2520applied%2520in%2520the%2520latent%2520space%2520remain%2520complex%2520and%2520difficult%2520to%250Atrain.%2520Further%252C%2520it%2520is%2520not%2520entirely%2520clear%2520why%2520transitioning%2520to%2520a%250Alower-dimensional%2520latent%2520space%2520can%2520improve%2520generative%2520quality.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520new%2520and%2520simple%2520generative%2520method%2520grounded%2520in%2520topology%2520theory%2520--%250AGenerative%2520Topological%2520Networks%2520%2528GTNs%2529%2520--%2520which%2520also%2520provides%2520insights%2520into%2520why%250Alower-dimensional%2520latent-space%2520representations%2520might%2520be%2520better-suited%2520for%2520data%250Ageneration.%2520GTNs%2520are%2520simple%2520to%2520train%2520--%2520they%2520employ%2520a%2520standard%2520supervised%250Alearning%2520approach%2520and%2520do%2520not%2520suffer%2520from%2520common%2520generative%2520pitfalls%2520such%2520as%250Amode%2520collapse%252C%2520posterior%2520collapse%2520or%2520the%2520need%2520to%2520pose%2520constraints%2520on%2520the%2520neural%250Anetwork%2520architecture.%2520We%2520demonstrate%2520the%2520use%2520of%2520GTNs%2520on%2520several%2520datasets%252C%250Aincluding%2520MNIST%252C%2520CelebA%252C%2520CIFAR-10%2520and%2520the%2520Hands%2520and%2520Palm%2520Images%2520dataset%2520by%250Atraining%2520GTNs%2520on%2520a%2520lower-dimensional%2520latent%2520representation%2520of%2520the%2520data.%2520We%2520show%250Athat%2520GTNs%2520can%2520improve%2520upon%2520VAEs%2520and%2520that%2520they%2520are%2520quick%2520to%2520converge%252C%2520generating%250Arealistic%2520samples%2520in%2520early%2520epochs.%2520Further%252C%2520we%2520use%2520the%2520topological%250Aconsiderations%2520behind%2520the%2520development%2520of%2520GTNs%2520to%2520offer%2520insights%2520into%2520why%250Agenerative%2520models%2520may%2520benefit%2520from%2520operating%2520on%2520a%2520lower-dimensional%2520latent%250Aspace%252C%2520highlighting%2520the%2520important%2520link%2520between%2520the%2520intrinsic%2520dimension%2520of%2520the%250Adata%2520and%2520the%2520dimension%2520in%2520which%2520the%2520data%2520is%2520generated.%2520Particularly%252C%2520we%250Ademonstrate%2520that%2520generating%2520in%2520high%2520dimensional%2520ambient%2520spaces%2520may%2520be%2520a%250Acontributing%2520factor%2520to%2520out-of-distribution%2520samples%2520generated%2520by%2520diffusion%250Amodels.%2520We%2520also%2520highlight%2520other%2520topological%2520properties%2520that%2520are%2520important%2520to%250Aconsider%2520when%2520using%2520and%2520designing%2520generative%2520models.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/alonalj/GTN%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15152v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Topological%20Networks&entry.906535625=Alona%20Levy-Jurgenson%20and%20Zohar%20Yakhini&entry.1292438233=%20%20Generative%20methods%20have%20recently%20seen%20significant%20improvements%20by%20generating%0Ain%20a%20lower-dimensional%20latent%20representation%20of%20the%20data.%20However%2C%20many%20of%20the%0Agenerative%20methods%20applied%20in%20the%20latent%20space%20remain%20complex%20and%20difficult%20to%0Atrain.%20Further%2C%20it%20is%20not%20entirely%20clear%20why%20transitioning%20to%20a%0Alower-dimensional%20latent%20space%20can%20improve%20generative%20quality.%20In%20this%20work%2C%20we%0Aintroduce%20a%20new%20and%20simple%20generative%20method%20grounded%20in%20topology%20theory%20--%0AGenerative%20Topological%20Networks%20%28GTNs%29%20--%20which%20also%20provides%20insights%20into%20why%0Alower-dimensional%20latent-space%20representations%20might%20be%20better-suited%20for%20data%0Ageneration.%20GTNs%20are%20simple%20to%20train%20--%20they%20employ%20a%20standard%20supervised%0Alearning%20approach%20and%20do%20not%20suffer%20from%20common%20generative%20pitfalls%20such%20as%0Amode%20collapse%2C%20posterior%20collapse%20or%20the%20need%20to%20pose%20constraints%20on%20the%20neural%0Anetwork%20architecture.%20We%20demonstrate%20the%20use%20of%20GTNs%20on%20several%20datasets%2C%0Aincluding%20MNIST%2C%20CelebA%2C%20CIFAR-10%20and%20the%20Hands%20and%20Palm%20Images%20dataset%20by%0Atraining%20GTNs%20on%20a%20lower-dimensional%20latent%20representation%20of%20the%20data.%20We%20show%0Athat%20GTNs%20can%20improve%20upon%20VAEs%20and%20that%20they%20are%20quick%20to%20converge%2C%20generating%0Arealistic%20samples%20in%20early%20epochs.%20Further%2C%20we%20use%20the%20topological%0Aconsiderations%20behind%20the%20development%20of%20GTNs%20to%20offer%20insights%20into%20why%0Agenerative%20models%20may%20benefit%20from%20operating%20on%20a%20lower-dimensional%20latent%0Aspace%2C%20highlighting%20the%20important%20link%20between%20the%20intrinsic%20dimension%20of%20the%0Adata%20and%20the%20dimension%20in%20which%20the%20data%20is%20generated.%20Particularly%2C%20we%0Ademonstrate%20that%20generating%20in%20high%20dimensional%20ambient%20spaces%20may%20be%20a%0Acontributing%20factor%20to%20out-of-distribution%20samples%20generated%20by%20diffusion%0Amodels.%20We%20also%20highlight%20other%20topological%20properties%20that%20are%20important%20to%0Aconsider%20when%20using%20and%20designing%20generative%20models.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/alonalj/GTN%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15152v3&entry.124074799=Read"},
{"title": "Parallel Sequence Modeling via Generalized Spatial Propagation Network", "author": "Hongjun Wang and Wonmin Byeon and Jiarui Xu and Jinwei Gu and Ka Chun Cheung and Xiaolong Wang and Kai Han and Jan Kautz and Sifei Liu", "abstract": "  We present the Generalized Spatial Propagation Network (GSPN), a new\nattention mechanism optimized for vision tasks that inherently captures 2D\nspatial structures. Existing attention models, including transformers, linear\nattention, and state-space models like Mamba, process multi-dimensional data as\n1D sequences, compromising spatial coherence and efficiency. GSPN overcomes\nthese limitations by directly operating on spatially coherent image data and\nforming dense pairwise connections through a line-scan approach. Central to\nGSPN is the Stability-Context Condition, which ensures stable, context-aware\npropagation across 2D sequences and reduces the effective sequence length to\n$\\sqrt{N}$ for a square map with N elements, significantly enhancing\ncomputational efficiency. With learnable, input-dependent weights and no\nreliance on positional embeddings, GSPN achieves superior spatial fidelity and\nstate-of-the-art performance in vision tasks, including ImageNet\nclassification, class-guided image generation, and text-to-image generation.\nNotably, GSPN accelerates SD-XL with softmax-attention by over $84\\times$ when\ngenerating 16K images.\n", "link": "http://arxiv.org/abs/2501.12381v1", "date": "2025-01-21", "relevancy": 2.1923, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5555}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.549}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallel%20Sequence%20Modeling%20via%20Generalized%20Spatial%20Propagation%20Network&body=Title%3A%20Parallel%20Sequence%20Modeling%20via%20Generalized%20Spatial%20Propagation%20Network%0AAuthor%3A%20Hongjun%20Wang%20and%20Wonmin%20Byeon%20and%20Jiarui%20Xu%20and%20Jinwei%20Gu%20and%20Ka%20Chun%20Cheung%20and%20Xiaolong%20Wang%20and%20Kai%20Han%20and%20Jan%20Kautz%20and%20Sifei%20Liu%0AAbstract%3A%20%20%20We%20present%20the%20Generalized%20Spatial%20Propagation%20Network%20%28GSPN%29%2C%20a%20new%0Aattention%20mechanism%20optimized%20for%20vision%20tasks%20that%20inherently%20captures%202D%0Aspatial%20structures.%20Existing%20attention%20models%2C%20including%20transformers%2C%20linear%0Aattention%2C%20and%20state-space%20models%20like%20Mamba%2C%20process%20multi-dimensional%20data%20as%0A1D%20sequences%2C%20compromising%20spatial%20coherence%20and%20efficiency.%20GSPN%20overcomes%0Athese%20limitations%20by%20directly%20operating%20on%20spatially%20coherent%20image%20data%20and%0Aforming%20dense%20pairwise%20connections%20through%20a%20line-scan%20approach.%20Central%20to%0AGSPN%20is%20the%20Stability-Context%20Condition%2C%20which%20ensures%20stable%2C%20context-aware%0Apropagation%20across%202D%20sequences%20and%20reduces%20the%20effective%20sequence%20length%20to%0A%24%5Csqrt%7BN%7D%24%20for%20a%20square%20map%20with%20N%20elements%2C%20significantly%20enhancing%0Acomputational%20efficiency.%20With%20learnable%2C%20input-dependent%20weights%20and%20no%0Areliance%20on%20positional%20embeddings%2C%20GSPN%20achieves%20superior%20spatial%20fidelity%20and%0Astate-of-the-art%20performance%20in%20vision%20tasks%2C%20including%20ImageNet%0Aclassification%2C%20class-guided%20image%20generation%2C%20and%20text-to-image%20generation.%0ANotably%2C%20GSPN%20accelerates%20SD-XL%20with%20softmax-attention%20by%20over%20%2484%5Ctimes%24%20when%0Agenerating%2016K%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallel%2520Sequence%2520Modeling%2520via%2520Generalized%2520Spatial%2520Propagation%2520Network%26entry.906535625%3DHongjun%2520Wang%2520and%2520Wonmin%2520Byeon%2520and%2520Jiarui%2520Xu%2520and%2520Jinwei%2520Gu%2520and%2520Ka%2520Chun%2520Cheung%2520and%2520Xiaolong%2520Wang%2520and%2520Kai%2520Han%2520and%2520Jan%2520Kautz%2520and%2520Sifei%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520Generalized%2520Spatial%2520Propagation%2520Network%2520%2528GSPN%2529%252C%2520a%2520new%250Aattention%2520mechanism%2520optimized%2520for%2520vision%2520tasks%2520that%2520inherently%2520captures%25202D%250Aspatial%2520structures.%2520Existing%2520attention%2520models%252C%2520including%2520transformers%252C%2520linear%250Aattention%252C%2520and%2520state-space%2520models%2520like%2520Mamba%252C%2520process%2520multi-dimensional%2520data%2520as%250A1D%2520sequences%252C%2520compromising%2520spatial%2520coherence%2520and%2520efficiency.%2520GSPN%2520overcomes%250Athese%2520limitations%2520by%2520directly%2520operating%2520on%2520spatially%2520coherent%2520image%2520data%2520and%250Aforming%2520dense%2520pairwise%2520connections%2520through%2520a%2520line-scan%2520approach.%2520Central%2520to%250AGSPN%2520is%2520the%2520Stability-Context%2520Condition%252C%2520which%2520ensures%2520stable%252C%2520context-aware%250Apropagation%2520across%25202D%2520sequences%2520and%2520reduces%2520the%2520effective%2520sequence%2520length%2520to%250A%2524%255Csqrt%257BN%257D%2524%2520for%2520a%2520square%2520map%2520with%2520N%2520elements%252C%2520significantly%2520enhancing%250Acomputational%2520efficiency.%2520With%2520learnable%252C%2520input-dependent%2520weights%2520and%2520no%250Areliance%2520on%2520positional%2520embeddings%252C%2520GSPN%2520achieves%2520superior%2520spatial%2520fidelity%2520and%250Astate-of-the-art%2520performance%2520in%2520vision%2520tasks%252C%2520including%2520ImageNet%250Aclassification%252C%2520class-guided%2520image%2520generation%252C%2520and%2520text-to-image%2520generation.%250ANotably%252C%2520GSPN%2520accelerates%2520SD-XL%2520with%2520softmax-attention%2520by%2520over%2520%252484%255Ctimes%2524%2520when%250Agenerating%252016K%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallel%20Sequence%20Modeling%20via%20Generalized%20Spatial%20Propagation%20Network&entry.906535625=Hongjun%20Wang%20and%20Wonmin%20Byeon%20and%20Jiarui%20Xu%20and%20Jinwei%20Gu%20and%20Ka%20Chun%20Cheung%20and%20Xiaolong%20Wang%20and%20Kai%20Han%20and%20Jan%20Kautz%20and%20Sifei%20Liu&entry.1292438233=%20%20We%20present%20the%20Generalized%20Spatial%20Propagation%20Network%20%28GSPN%29%2C%20a%20new%0Aattention%20mechanism%20optimized%20for%20vision%20tasks%20that%20inherently%20captures%202D%0Aspatial%20structures.%20Existing%20attention%20models%2C%20including%20transformers%2C%20linear%0Aattention%2C%20and%20state-space%20models%20like%20Mamba%2C%20process%20multi-dimensional%20data%20as%0A1D%20sequences%2C%20compromising%20spatial%20coherence%20and%20efficiency.%20GSPN%20overcomes%0Athese%20limitations%20by%20directly%20operating%20on%20spatially%20coherent%20image%20data%20and%0Aforming%20dense%20pairwise%20connections%20through%20a%20line-scan%20approach.%20Central%20to%0AGSPN%20is%20the%20Stability-Context%20Condition%2C%20which%20ensures%20stable%2C%20context-aware%0Apropagation%20across%202D%20sequences%20and%20reduces%20the%20effective%20sequence%20length%20to%0A%24%5Csqrt%7BN%7D%24%20for%20a%20square%20map%20with%20N%20elements%2C%20significantly%20enhancing%0Acomputational%20efficiency.%20With%20learnable%2C%20input-dependent%20weights%20and%20no%0Areliance%20on%20positional%20embeddings%2C%20GSPN%20achieves%20superior%20spatial%20fidelity%20and%0Astate-of-the-art%20performance%20in%20vision%20tasks%2C%20including%20ImageNet%0Aclassification%2C%20class-guided%20image%20generation%2C%20and%20text-to-image%20generation.%0ANotably%2C%20GSPN%20accelerates%20SD-XL%20with%20softmax-attention%20by%20over%20%2484%5Ctimes%24%20when%0Agenerating%2016K%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12381v1&entry.124074799=Read"},
{"title": "mmCooper: A Multi-agent Multi-stage Communication-efficient and\n  Collaboration-robust Cooperative Perception Framework", "author": "Bingyi Liu and Jian Teng and Hongfei Xue and Enshu Wang and Chuanhui Zhu and Pu Wang and Libing Wu", "abstract": "  Collaborative perception significantly enhances individual vehicle perception\nperformance through the exchange of sensory information among agents. However,\nreal-world deployment faces challenges due to bandwidth constraints and\ninevitable calibration errors during information exchange. To address these\nissues, we propose mmCooper, a novel multi-agent, multi-stage,\ncommunication-efficient, and collaboration-robust cooperative perception\nframework. Our framework leverages a multi-stage collaboration strategy that\ndynamically and adaptively balances intermediate- and late-stage information to\nshare among agents, enhancing perceptual performance while maintaining\ncommunication efficiency. To support robust collaboration despite potential\nmisalignments and calibration errors, our framework captures multi-scale\ncontextual information for robust fusion in the intermediate stage and\ncalibrates the received detection results to improve accuracy in the late\nstage. We validate the effectiveness of mmCooper through extensive experiments\non real-world and simulated datasets. The results demonstrate the superiority\nof our proposed framework and the effectiveness of each component.\n", "link": "http://arxiv.org/abs/2501.12263v1", "date": "2025-01-21", "relevancy": 2.189, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5793}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5685}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mmCooper%3A%20A%20Multi-agent%20Multi-stage%20Communication-efficient%20and%0A%20%20Collaboration-robust%20Cooperative%20Perception%20Framework&body=Title%3A%20mmCooper%3A%20A%20Multi-agent%20Multi-stage%20Communication-efficient%20and%0A%20%20Collaboration-robust%20Cooperative%20Perception%20Framework%0AAuthor%3A%20Bingyi%20Liu%20and%20Jian%20Teng%20and%20Hongfei%20Xue%20and%20Enshu%20Wang%20and%20Chuanhui%20Zhu%20and%20Pu%20Wang%20and%20Libing%20Wu%0AAbstract%3A%20%20%20Collaborative%20perception%20significantly%20enhances%20individual%20vehicle%20perception%0Aperformance%20through%20the%20exchange%20of%20sensory%20information%20among%20agents.%20However%2C%0Areal-world%20deployment%20faces%20challenges%20due%20to%20bandwidth%20constraints%20and%0Ainevitable%20calibration%20errors%20during%20information%20exchange.%20To%20address%20these%0Aissues%2C%20we%20propose%20mmCooper%2C%20a%20novel%20multi-agent%2C%20multi-stage%2C%0Acommunication-efficient%2C%20and%20collaboration-robust%20cooperative%20perception%0Aframework.%20Our%20framework%20leverages%20a%20multi-stage%20collaboration%20strategy%20that%0Adynamically%20and%20adaptively%20balances%20intermediate-%20and%20late-stage%20information%20to%0Ashare%20among%20agents%2C%20enhancing%20perceptual%20performance%20while%20maintaining%0Acommunication%20efficiency.%20To%20support%20robust%20collaboration%20despite%20potential%0Amisalignments%20and%20calibration%20errors%2C%20our%20framework%20captures%20multi-scale%0Acontextual%20information%20for%20robust%20fusion%20in%20the%20intermediate%20stage%20and%0Acalibrates%20the%20received%20detection%20results%20to%20improve%20accuracy%20in%20the%20late%0Astage.%20We%20validate%20the%20effectiveness%20of%20mmCooper%20through%20extensive%20experiments%0Aon%20real-world%20and%20simulated%20datasets.%20The%20results%20demonstrate%20the%20superiority%0Aof%20our%20proposed%20framework%20and%20the%20effectiveness%20of%20each%20component.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmmCooper%253A%2520A%2520Multi-agent%2520Multi-stage%2520Communication-efficient%2520and%250A%2520%2520Collaboration-robust%2520Cooperative%2520Perception%2520Framework%26entry.906535625%3DBingyi%2520Liu%2520and%2520Jian%2520Teng%2520and%2520Hongfei%2520Xue%2520and%2520Enshu%2520Wang%2520and%2520Chuanhui%2520Zhu%2520and%2520Pu%2520Wang%2520and%2520Libing%2520Wu%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520significantly%2520enhances%2520individual%2520vehicle%2520perception%250Aperformance%2520through%2520the%2520exchange%2520of%2520sensory%2520information%2520among%2520agents.%2520However%252C%250Areal-world%2520deployment%2520faces%2520challenges%2520due%2520to%2520bandwidth%2520constraints%2520and%250Ainevitable%2520calibration%2520errors%2520during%2520information%2520exchange.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520mmCooper%252C%2520a%2520novel%2520multi-agent%252C%2520multi-stage%252C%250Acommunication-efficient%252C%2520and%2520collaboration-robust%2520cooperative%2520perception%250Aframework.%2520Our%2520framework%2520leverages%2520a%2520multi-stage%2520collaboration%2520strategy%2520that%250Adynamically%2520and%2520adaptively%2520balances%2520intermediate-%2520and%2520late-stage%2520information%2520to%250Ashare%2520among%2520agents%252C%2520enhancing%2520perceptual%2520performance%2520while%2520maintaining%250Acommunication%2520efficiency.%2520To%2520support%2520robust%2520collaboration%2520despite%2520potential%250Amisalignments%2520and%2520calibration%2520errors%252C%2520our%2520framework%2520captures%2520multi-scale%250Acontextual%2520information%2520for%2520robust%2520fusion%2520in%2520the%2520intermediate%2520stage%2520and%250Acalibrates%2520the%2520received%2520detection%2520results%2520to%2520improve%2520accuracy%2520in%2520the%2520late%250Astage.%2520We%2520validate%2520the%2520effectiveness%2520of%2520mmCooper%2520through%2520extensive%2520experiments%250Aon%2520real-world%2520and%2520simulated%2520datasets.%2520The%2520results%2520demonstrate%2520the%2520superiority%250Aof%2520our%2520proposed%2520framework%2520and%2520the%2520effectiveness%2520of%2520each%2520component.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mmCooper%3A%20A%20Multi-agent%20Multi-stage%20Communication-efficient%20and%0A%20%20Collaboration-robust%20Cooperative%20Perception%20Framework&entry.906535625=Bingyi%20Liu%20and%20Jian%20Teng%20and%20Hongfei%20Xue%20and%20Enshu%20Wang%20and%20Chuanhui%20Zhu%20and%20Pu%20Wang%20and%20Libing%20Wu&entry.1292438233=%20%20Collaborative%20perception%20significantly%20enhances%20individual%20vehicle%20perception%0Aperformance%20through%20the%20exchange%20of%20sensory%20information%20among%20agents.%20However%2C%0Areal-world%20deployment%20faces%20challenges%20due%20to%20bandwidth%20constraints%20and%0Ainevitable%20calibration%20errors%20during%20information%20exchange.%20To%20address%20these%0Aissues%2C%20we%20propose%20mmCooper%2C%20a%20novel%20multi-agent%2C%20multi-stage%2C%0Acommunication-efficient%2C%20and%20collaboration-robust%20cooperative%20perception%0Aframework.%20Our%20framework%20leverages%20a%20multi-stage%20collaboration%20strategy%20that%0Adynamically%20and%20adaptively%20balances%20intermediate-%20and%20late-stage%20information%20to%0Ashare%20among%20agents%2C%20enhancing%20perceptual%20performance%20while%20maintaining%0Acommunication%20efficiency.%20To%20support%20robust%20collaboration%20despite%20potential%0Amisalignments%20and%20calibration%20errors%2C%20our%20framework%20captures%20multi-scale%0Acontextual%20information%20for%20robust%20fusion%20in%20the%20intermediate%20stage%20and%0Acalibrates%20the%20received%20detection%20results%20to%20improve%20accuracy%20in%20the%20late%0Astage.%20We%20validate%20the%20effectiveness%20of%20mmCooper%20through%20extensive%20experiments%0Aon%20real-world%20and%20simulated%20datasets.%20The%20results%20demonstrate%20the%20superiority%0Aof%20our%20proposed%20framework%20and%20the%20effectiveness%20of%20each%20component.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12263v1&entry.124074799=Read"},
{"title": "Heterogeneous Federated Learning System for Sparse Healthcare\n  Time-Series Prediction", "author": "Jia-Hao Syu and Jerry Chun-Wei Lin", "abstract": "  In this paper, we propose a heterogeneous federated learning (HFL) system for\nsparse time series prediction in healthcare, which is a decentralized federated\nlearning algorithm with heterogeneous transfers. We design dense and sparse\nfeature tensors to deal with the sparsity of data sources. Heterogeneous\nfederated learning is developed to share asynchronous parts of networks and\nselect appropriate models for knowledge transfer. Experimental results show\nthat the proposed HFL achieves the lowest prediction error among all benchmark\nsystems on eight out of ten prediction tasks, with MSE reduction of 94.8%,\n48.3%, and 52.1% compared to the benchmark systems. These results demonstrate\nthe effectiveness of HFL in transferring knowledge from heterogeneous domains,\nespecially in the smaller target domain. Ablation studies then demonstrate the\neffectiveness of the designed mechanisms for heterogeneous domain selection and\nswitching in predicting healthcare time series with privacy, model security,\nand heterogeneous knowledge transfer.\n", "link": "http://arxiv.org/abs/2501.12125v1", "date": "2025-01-21", "relevancy": 2.1871, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4418}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4367}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Federated%20Learning%20System%20for%20Sparse%20Healthcare%0A%20%20Time-Series%20Prediction&body=Title%3A%20Heterogeneous%20Federated%20Learning%20System%20for%20Sparse%20Healthcare%0A%20%20Time-Series%20Prediction%0AAuthor%3A%20Jia-Hao%20Syu%20and%20Jerry%20Chun-Wei%20Lin%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20heterogeneous%20federated%20learning%20%28HFL%29%20system%20for%0Asparse%20time%20series%20prediction%20in%20healthcare%2C%20which%20is%20a%20decentralized%20federated%0Alearning%20algorithm%20with%20heterogeneous%20transfers.%20We%20design%20dense%20and%20sparse%0Afeature%20tensors%20to%20deal%20with%20the%20sparsity%20of%20data%20sources.%20Heterogeneous%0Afederated%20learning%20is%20developed%20to%20share%20asynchronous%20parts%20of%20networks%20and%0Aselect%20appropriate%20models%20for%20knowledge%20transfer.%20Experimental%20results%20show%0Athat%20the%20proposed%20HFL%20achieves%20the%20lowest%20prediction%20error%20among%20all%20benchmark%0Asystems%20on%20eight%20out%20of%20ten%20prediction%20tasks%2C%20with%20MSE%20reduction%20of%2094.8%25%2C%0A48.3%25%2C%20and%2052.1%25%20compared%20to%20the%20benchmark%20systems.%20These%20results%20demonstrate%0Athe%20effectiveness%20of%20HFL%20in%20transferring%20knowledge%20from%20heterogeneous%20domains%2C%0Aespecially%20in%20the%20smaller%20target%20domain.%20Ablation%20studies%20then%20demonstrate%20the%0Aeffectiveness%20of%20the%20designed%20mechanisms%20for%20heterogeneous%20domain%20selection%20and%0Aswitching%20in%20predicting%20healthcare%20time%20series%20with%20privacy%2C%20model%20security%2C%0Aand%20heterogeneous%20knowledge%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneous%2520Federated%2520Learning%2520System%2520for%2520Sparse%2520Healthcare%250A%2520%2520Time-Series%2520Prediction%26entry.906535625%3DJia-Hao%2520Syu%2520and%2520Jerry%2520Chun-Wei%2520Lin%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520heterogeneous%2520federated%2520learning%2520%2528HFL%2529%2520system%2520for%250Asparse%2520time%2520series%2520prediction%2520in%2520healthcare%252C%2520which%2520is%2520a%2520decentralized%2520federated%250Alearning%2520algorithm%2520with%2520heterogeneous%2520transfers.%2520We%2520design%2520dense%2520and%2520sparse%250Afeature%2520tensors%2520to%2520deal%2520with%2520the%2520sparsity%2520of%2520data%2520sources.%2520Heterogeneous%250Afederated%2520learning%2520is%2520developed%2520to%2520share%2520asynchronous%2520parts%2520of%2520networks%2520and%250Aselect%2520appropriate%2520models%2520for%2520knowledge%2520transfer.%2520Experimental%2520results%2520show%250Athat%2520the%2520proposed%2520HFL%2520achieves%2520the%2520lowest%2520prediction%2520error%2520among%2520all%2520benchmark%250Asystems%2520on%2520eight%2520out%2520of%2520ten%2520prediction%2520tasks%252C%2520with%2520MSE%2520reduction%2520of%252094.8%2525%252C%250A48.3%2525%252C%2520and%252052.1%2525%2520compared%2520to%2520the%2520benchmark%2520systems.%2520These%2520results%2520demonstrate%250Athe%2520effectiveness%2520of%2520HFL%2520in%2520transferring%2520knowledge%2520from%2520heterogeneous%2520domains%252C%250Aespecially%2520in%2520the%2520smaller%2520target%2520domain.%2520Ablation%2520studies%2520then%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520designed%2520mechanisms%2520for%2520heterogeneous%2520domain%2520selection%2520and%250Aswitching%2520in%2520predicting%2520healthcare%2520time%2520series%2520with%2520privacy%252C%2520model%2520security%252C%250Aand%2520heterogeneous%2520knowledge%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Federated%20Learning%20System%20for%20Sparse%20Healthcare%0A%20%20Time-Series%20Prediction&entry.906535625=Jia-Hao%20Syu%20and%20Jerry%20Chun-Wei%20Lin&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20heterogeneous%20federated%20learning%20%28HFL%29%20system%20for%0Asparse%20time%20series%20prediction%20in%20healthcare%2C%20which%20is%20a%20decentralized%20federated%0Alearning%20algorithm%20with%20heterogeneous%20transfers.%20We%20design%20dense%20and%20sparse%0Afeature%20tensors%20to%20deal%20with%20the%20sparsity%20of%20data%20sources.%20Heterogeneous%0Afederated%20learning%20is%20developed%20to%20share%20asynchronous%20parts%20of%20networks%20and%0Aselect%20appropriate%20models%20for%20knowledge%20transfer.%20Experimental%20results%20show%0Athat%20the%20proposed%20HFL%20achieves%20the%20lowest%20prediction%20error%20among%20all%20benchmark%0Asystems%20on%20eight%20out%20of%20ten%20prediction%20tasks%2C%20with%20MSE%20reduction%20of%2094.8%25%2C%0A48.3%25%2C%20and%2052.1%25%20compared%20to%20the%20benchmark%20systems.%20These%20results%20demonstrate%0Athe%20effectiveness%20of%20HFL%20in%20transferring%20knowledge%20from%20heterogeneous%20domains%2C%0Aespecially%20in%20the%20smaller%20target%20domain.%20Ablation%20studies%20then%20demonstrate%20the%0Aeffectiveness%20of%20the%20designed%20mechanisms%20for%20heterogeneous%20domain%20selection%20and%0Aswitching%20in%20predicting%20healthcare%20time%20series%20with%20privacy%2C%20model%20security%2C%0Aand%20heterogeneous%20knowledge%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12125v1&entry.124074799=Read"},
{"title": "Exploring Temporally-Aware Features for Point Tracking", "author": "In\u00e8s Hyeonsu Kim and Seokju Cho and Jiahui Huang and Jung Yi and Joon-Young Lee and Seungryong Kim", "abstract": "  Point tracking in videos is a fundamental task with applications in robotics,\nvideo editing, and more. While many vision tasks benefit from pre-trained\nfeature backbones to improve generalizability, point tracking has primarily\nrelied on simpler backbones trained from scratch on synthetic data, which may\nlimit robustness in real-world scenarios. Additionally, point tracking requires\ntemporal awareness to ensure coherence across frames, but using\ntemporally-aware features is still underexplored. Most current methods often\nemploy a two-stage process: an initial coarse prediction followed by a\nrefinement stage to inject temporal information and correct errors from the\ncoarse stage. These approach, however, is computationally expensive and\npotentially redundant if the feature backbone itself captures sufficient\ntemporal information.\n  In this work, we introduce Chrono, a feature backbone specifically designed\nfor point tracking with built-in temporal awareness. Leveraging pre-trained\nrepresentations from self-supervised learner DINOv2 and enhanced with a\ntemporal adapter, Chrono effectively captures long-term temporal context,\nenabling precise prediction even without the refinement stage. Experimental\nresults demonstrate that Chrono achieves state-of-the-art performance in a\nrefiner-free setting on the TAP-Vid-DAVIS and TAP-Vid-Kinetics datasets, among\ncommon feature backbones used in point tracking as well as DINOv2, with\nexceptional efficiency. Project page: https://cvlab-kaist.github.io/Chrono/\n", "link": "http://arxiv.org/abs/2501.12218v1", "date": "2025-01-21", "relevancy": 2.1848, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5599}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5557}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Temporally-Aware%20Features%20for%20Point%20Tracking&body=Title%3A%20Exploring%20Temporally-Aware%20Features%20for%20Point%20Tracking%0AAuthor%3A%20In%C3%A8s%20Hyeonsu%20Kim%20and%20Seokju%20Cho%20and%20Jiahui%20Huang%20and%20Jung%20Yi%20and%20Joon-Young%20Lee%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20Point%20tracking%20in%20videos%20is%20a%20fundamental%20task%20with%20applications%20in%20robotics%2C%0Avideo%20editing%2C%20and%20more.%20While%20many%20vision%20tasks%20benefit%20from%20pre-trained%0Afeature%20backbones%20to%20improve%20generalizability%2C%20point%20tracking%20has%20primarily%0Arelied%20on%20simpler%20backbones%20trained%20from%20scratch%20on%20synthetic%20data%2C%20which%20may%0Alimit%20robustness%20in%20real-world%20scenarios.%20Additionally%2C%20point%20tracking%20requires%0Atemporal%20awareness%20to%20ensure%20coherence%20across%20frames%2C%20but%20using%0Atemporally-aware%20features%20is%20still%20underexplored.%20Most%20current%20methods%20often%0Aemploy%20a%20two-stage%20process%3A%20an%20initial%20coarse%20prediction%20followed%20by%20a%0Arefinement%20stage%20to%20inject%20temporal%20information%20and%20correct%20errors%20from%20the%0Acoarse%20stage.%20These%20approach%2C%20however%2C%20is%20computationally%20expensive%20and%0Apotentially%20redundant%20if%20the%20feature%20backbone%20itself%20captures%20sufficient%0Atemporal%20information.%0A%20%20In%20this%20work%2C%20we%20introduce%20Chrono%2C%20a%20feature%20backbone%20specifically%20designed%0Afor%20point%20tracking%20with%20built-in%20temporal%20awareness.%20Leveraging%20pre-trained%0Arepresentations%20from%20self-supervised%20learner%20DINOv2%20and%20enhanced%20with%20a%0Atemporal%20adapter%2C%20Chrono%20effectively%20captures%20long-term%20temporal%20context%2C%0Aenabling%20precise%20prediction%20even%20without%20the%20refinement%20stage.%20Experimental%0Aresults%20demonstrate%20that%20Chrono%20achieves%20state-of-the-art%20performance%20in%20a%0Arefiner-free%20setting%20on%20the%20TAP-Vid-DAVIS%20and%20TAP-Vid-Kinetics%20datasets%2C%20among%0Acommon%20feature%20backbones%20used%20in%20point%20tracking%20as%20well%20as%20DINOv2%2C%20with%0Aexceptional%20efficiency.%20Project%20page%3A%20https%3A//cvlab-kaist.github.io/Chrono/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Temporally-Aware%2520Features%2520for%2520Point%2520Tracking%26entry.906535625%3DIn%25C3%25A8s%2520Hyeonsu%2520Kim%2520and%2520Seokju%2520Cho%2520and%2520Jiahui%2520Huang%2520and%2520Jung%2520Yi%2520and%2520Joon-Young%2520Lee%2520and%2520Seungryong%2520Kim%26entry.1292438233%3D%2520%2520Point%2520tracking%2520in%2520videos%2520is%2520a%2520fundamental%2520task%2520with%2520applications%2520in%2520robotics%252C%250Avideo%2520editing%252C%2520and%2520more.%2520While%2520many%2520vision%2520tasks%2520benefit%2520from%2520pre-trained%250Afeature%2520backbones%2520to%2520improve%2520generalizability%252C%2520point%2520tracking%2520has%2520primarily%250Arelied%2520on%2520simpler%2520backbones%2520trained%2520from%2520scratch%2520on%2520synthetic%2520data%252C%2520which%2520may%250Alimit%2520robustness%2520in%2520real-world%2520scenarios.%2520Additionally%252C%2520point%2520tracking%2520requires%250Atemporal%2520awareness%2520to%2520ensure%2520coherence%2520across%2520frames%252C%2520but%2520using%250Atemporally-aware%2520features%2520is%2520still%2520underexplored.%2520Most%2520current%2520methods%2520often%250Aemploy%2520a%2520two-stage%2520process%253A%2520an%2520initial%2520coarse%2520prediction%2520followed%2520by%2520a%250Arefinement%2520stage%2520to%2520inject%2520temporal%2520information%2520and%2520correct%2520errors%2520from%2520the%250Acoarse%2520stage.%2520These%2520approach%252C%2520however%252C%2520is%2520computationally%2520expensive%2520and%250Apotentially%2520redundant%2520if%2520the%2520feature%2520backbone%2520itself%2520captures%2520sufficient%250Atemporal%2520information.%250A%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520Chrono%252C%2520a%2520feature%2520backbone%2520specifically%2520designed%250Afor%2520point%2520tracking%2520with%2520built-in%2520temporal%2520awareness.%2520Leveraging%2520pre-trained%250Arepresentations%2520from%2520self-supervised%2520learner%2520DINOv2%2520and%2520enhanced%2520with%2520a%250Atemporal%2520adapter%252C%2520Chrono%2520effectively%2520captures%2520long-term%2520temporal%2520context%252C%250Aenabling%2520precise%2520prediction%2520even%2520without%2520the%2520refinement%2520stage.%2520Experimental%250Aresults%2520demonstrate%2520that%2520Chrono%2520achieves%2520state-of-the-art%2520performance%2520in%2520a%250Arefiner-free%2520setting%2520on%2520the%2520TAP-Vid-DAVIS%2520and%2520TAP-Vid-Kinetics%2520datasets%252C%2520among%250Acommon%2520feature%2520backbones%2520used%2520in%2520point%2520tracking%2520as%2520well%2520as%2520DINOv2%252C%2520with%250Aexceptional%2520efficiency.%2520Project%2520page%253A%2520https%253A//cvlab-kaist.github.io/Chrono/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Temporally-Aware%20Features%20for%20Point%20Tracking&entry.906535625=In%C3%A8s%20Hyeonsu%20Kim%20and%20Seokju%20Cho%20and%20Jiahui%20Huang%20and%20Jung%20Yi%20and%20Joon-Young%20Lee%20and%20Seungryong%20Kim&entry.1292438233=%20%20Point%20tracking%20in%20videos%20is%20a%20fundamental%20task%20with%20applications%20in%20robotics%2C%0Avideo%20editing%2C%20and%20more.%20While%20many%20vision%20tasks%20benefit%20from%20pre-trained%0Afeature%20backbones%20to%20improve%20generalizability%2C%20point%20tracking%20has%20primarily%0Arelied%20on%20simpler%20backbones%20trained%20from%20scratch%20on%20synthetic%20data%2C%20which%20may%0Alimit%20robustness%20in%20real-world%20scenarios.%20Additionally%2C%20point%20tracking%20requires%0Atemporal%20awareness%20to%20ensure%20coherence%20across%20frames%2C%20but%20using%0Atemporally-aware%20features%20is%20still%20underexplored.%20Most%20current%20methods%20often%0Aemploy%20a%20two-stage%20process%3A%20an%20initial%20coarse%20prediction%20followed%20by%20a%0Arefinement%20stage%20to%20inject%20temporal%20information%20and%20correct%20errors%20from%20the%0Acoarse%20stage.%20These%20approach%2C%20however%2C%20is%20computationally%20expensive%20and%0Apotentially%20redundant%20if%20the%20feature%20backbone%20itself%20captures%20sufficient%0Atemporal%20information.%0A%20%20In%20this%20work%2C%20we%20introduce%20Chrono%2C%20a%20feature%20backbone%20specifically%20designed%0Afor%20point%20tracking%20with%20built-in%20temporal%20awareness.%20Leveraging%20pre-trained%0Arepresentations%20from%20self-supervised%20learner%20DINOv2%20and%20enhanced%20with%20a%0Atemporal%20adapter%2C%20Chrono%20effectively%20captures%20long-term%20temporal%20context%2C%0Aenabling%20precise%20prediction%20even%20without%20the%20refinement%20stage.%20Experimental%0Aresults%20demonstrate%20that%20Chrono%20achieves%20state-of-the-art%20performance%20in%20a%0Arefiner-free%20setting%20on%20the%20TAP-Vid-DAVIS%20and%20TAP-Vid-Kinetics%20datasets%2C%20among%0Acommon%20feature%20backbones%20used%20in%20point%20tracking%20as%20well%20as%20DINOv2%2C%20with%0Aexceptional%20efficiency.%20Project%20page%3A%20https%3A//cvlab-kaist.github.io/Chrono/%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12218v1&entry.124074799=Read"},
{"title": "Towards Accurate Unified Anomaly Segmentation", "author": "Wenxin Ma and Qingsong Yao and Xiang Zhang and Zhelong Huang and Zihang Jiang and S. Kevin Zhou", "abstract": "  Unsupervised anomaly detection (UAD) from images strives to model normal data\ndistributions, creating discriminative representations to distinguish and\nprecisely localize anomalies. Despite recent advancements in the efficient and\nunified one-for-all scheme, challenges persist in accurately segmenting\nanomalies for further monitoring. Moreover, this problem is obscured by the\nwidely-used AUROC metric under imbalanced UAD settings. This motivates us to\nemphasize the significance of precise segmentation of anomaly pixels using pAP\nand DSC as metrics. To address the unsolved segmentation task, we introduce the\nUnified Anomaly Segmentation (UniAS). UniAS presents a multi-level hybrid\npipeline that progressively enhances normal information from coarse to fine,\nincorporating a novel multi-granularity gated CNN (MGG-CNN) into Transformer\nlayers to explicitly aggregate local details from different granularities.\nUniAS achieves state-of-the-art anomaly segmentation performance, attaining\n65.12/59.33 and 40.06/32.50 in pAP/DSC on the MVTec-AD and VisA datasets,\nrespectively, surpassing previous methods significantly. The codes are shared\nat https://github.com/Mwxinnn/UniAS.\n", "link": "http://arxiv.org/abs/2501.12295v1", "date": "2025-01-21", "relevancy": 2.182, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5652}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5562}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Accurate%20Unified%20Anomaly%20Segmentation&body=Title%3A%20Towards%20Accurate%20Unified%20Anomaly%20Segmentation%0AAuthor%3A%20Wenxin%20Ma%20and%20Qingsong%20Yao%20and%20Xiang%20Zhang%20and%20Zhelong%20Huang%20and%20Zihang%20Jiang%20and%20S.%20Kevin%20Zhou%0AAbstract%3A%20%20%20Unsupervised%20anomaly%20detection%20%28UAD%29%20from%20images%20strives%20to%20model%20normal%20data%0Adistributions%2C%20creating%20discriminative%20representations%20to%20distinguish%20and%0Aprecisely%20localize%20anomalies.%20Despite%20recent%20advancements%20in%20the%20efficient%20and%0Aunified%20one-for-all%20scheme%2C%20challenges%20persist%20in%20accurately%20segmenting%0Aanomalies%20for%20further%20monitoring.%20Moreover%2C%20this%20problem%20is%20obscured%20by%20the%0Awidely-used%20AUROC%20metric%20under%20imbalanced%20UAD%20settings.%20This%20motivates%20us%20to%0Aemphasize%20the%20significance%20of%20precise%20segmentation%20of%20anomaly%20pixels%20using%20pAP%0Aand%20DSC%20as%20metrics.%20To%20address%20the%20unsolved%20segmentation%20task%2C%20we%20introduce%20the%0AUnified%20Anomaly%20Segmentation%20%28UniAS%29.%20UniAS%20presents%20a%20multi-level%20hybrid%0Apipeline%20that%20progressively%20enhances%20normal%20information%20from%20coarse%20to%20fine%2C%0Aincorporating%20a%20novel%20multi-granularity%20gated%20CNN%20%28MGG-CNN%29%20into%20Transformer%0Alayers%20to%20explicitly%20aggregate%20local%20details%20from%20different%20granularities.%0AUniAS%20achieves%20state-of-the-art%20anomaly%20segmentation%20performance%2C%20attaining%0A65.12/59.33%20and%2040.06/32.50%20in%20pAP/DSC%20on%20the%20MVTec-AD%20and%20VisA%20datasets%2C%0Arespectively%2C%20surpassing%20previous%20methods%20significantly.%20The%20codes%20are%20shared%0Aat%20https%3A//github.com/Mwxinnn/UniAS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Accurate%2520Unified%2520Anomaly%2520Segmentation%26entry.906535625%3DWenxin%2520Ma%2520and%2520Qingsong%2520Yao%2520and%2520Xiang%2520Zhang%2520and%2520Zhelong%2520Huang%2520and%2520Zihang%2520Jiang%2520and%2520S.%2520Kevin%2520Zhou%26entry.1292438233%3D%2520%2520Unsupervised%2520anomaly%2520detection%2520%2528UAD%2529%2520from%2520images%2520strives%2520to%2520model%2520normal%2520data%250Adistributions%252C%2520creating%2520discriminative%2520representations%2520to%2520distinguish%2520and%250Aprecisely%2520localize%2520anomalies.%2520Despite%2520recent%2520advancements%2520in%2520the%2520efficient%2520and%250Aunified%2520one-for-all%2520scheme%252C%2520challenges%2520persist%2520in%2520accurately%2520segmenting%250Aanomalies%2520for%2520further%2520monitoring.%2520Moreover%252C%2520this%2520problem%2520is%2520obscured%2520by%2520the%250Awidely-used%2520AUROC%2520metric%2520under%2520imbalanced%2520UAD%2520settings.%2520This%2520motivates%2520us%2520to%250Aemphasize%2520the%2520significance%2520of%2520precise%2520segmentation%2520of%2520anomaly%2520pixels%2520using%2520pAP%250Aand%2520DSC%2520as%2520metrics.%2520To%2520address%2520the%2520unsolved%2520segmentation%2520task%252C%2520we%2520introduce%2520the%250AUnified%2520Anomaly%2520Segmentation%2520%2528UniAS%2529.%2520UniAS%2520presents%2520a%2520multi-level%2520hybrid%250Apipeline%2520that%2520progressively%2520enhances%2520normal%2520information%2520from%2520coarse%2520to%2520fine%252C%250Aincorporating%2520a%2520novel%2520multi-granularity%2520gated%2520CNN%2520%2528MGG-CNN%2529%2520into%2520Transformer%250Alayers%2520to%2520explicitly%2520aggregate%2520local%2520details%2520from%2520different%2520granularities.%250AUniAS%2520achieves%2520state-of-the-art%2520anomaly%2520segmentation%2520performance%252C%2520attaining%250A65.12/59.33%2520and%252040.06/32.50%2520in%2520pAP/DSC%2520on%2520the%2520MVTec-AD%2520and%2520VisA%2520datasets%252C%250Arespectively%252C%2520surpassing%2520previous%2520methods%2520significantly.%2520The%2520codes%2520are%2520shared%250Aat%2520https%253A//github.com/Mwxinnn/UniAS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Accurate%20Unified%20Anomaly%20Segmentation&entry.906535625=Wenxin%20Ma%20and%20Qingsong%20Yao%20and%20Xiang%20Zhang%20and%20Zhelong%20Huang%20and%20Zihang%20Jiang%20and%20S.%20Kevin%20Zhou&entry.1292438233=%20%20Unsupervised%20anomaly%20detection%20%28UAD%29%20from%20images%20strives%20to%20model%20normal%20data%0Adistributions%2C%20creating%20discriminative%20representations%20to%20distinguish%20and%0Aprecisely%20localize%20anomalies.%20Despite%20recent%20advancements%20in%20the%20efficient%20and%0Aunified%20one-for-all%20scheme%2C%20challenges%20persist%20in%20accurately%20segmenting%0Aanomalies%20for%20further%20monitoring.%20Moreover%2C%20this%20problem%20is%20obscured%20by%20the%0Awidely-used%20AUROC%20metric%20under%20imbalanced%20UAD%20settings.%20This%20motivates%20us%20to%0Aemphasize%20the%20significance%20of%20precise%20segmentation%20of%20anomaly%20pixels%20using%20pAP%0Aand%20DSC%20as%20metrics.%20To%20address%20the%20unsolved%20segmentation%20task%2C%20we%20introduce%20the%0AUnified%20Anomaly%20Segmentation%20%28UniAS%29.%20UniAS%20presents%20a%20multi-level%20hybrid%0Apipeline%20that%20progressively%20enhances%20normal%20information%20from%20coarse%20to%20fine%2C%0Aincorporating%20a%20novel%20multi-granularity%20gated%20CNN%20%28MGG-CNN%29%20into%20Transformer%0Alayers%20to%20explicitly%20aggregate%20local%20details%20from%20different%20granularities.%0AUniAS%20achieves%20state-of-the-art%20anomaly%20segmentation%20performance%2C%20attaining%0A65.12/59.33%20and%2040.06/32.50%20in%20pAP/DSC%20on%20the%20MVTec-AD%20and%20VisA%20datasets%2C%0Arespectively%2C%20surpassing%20previous%20methods%20significantly.%20The%20codes%20are%20shared%0Aat%20https%3A//github.com/Mwxinnn/UniAS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12295v1&entry.124074799=Read"},
{"title": "Are Traditional Deep Learning Model Approaches as Effective as a\n  Retinal-Specific Foundation Model for Ocular and Systemic Disease Detection?", "author": "Samantha Min Er Yew and Xiaofeng Lei and Jocelyn Hui Lin Goh and Yibing Chen and Sahana Srinivasan and Miao-li Chee and Krithi Pushpanathan and Ke Zou and Qingshan Hou and Zhi Da Soh and Cancan Xue and Marco Chak Yan Yu and Charumathi Sabanayagam and E Shyong Tai and Xueling Sim and Yaxing Wang and Jost B. Jonas and Vinay Nangia and Gabriel Dawei Yang and Emma Anran Ran and Carol Yim-Lui Cheung and Yangqin Feng and Jun Zhou and Rick Siow Mong Goh and Yukun Zhou and Pearse A. Keane and Yong Liu and Ching-Yu Cheng and Yih-Chung Tham", "abstract": "  Background: RETFound, a self-supervised, retina-specific foundation model\n(FM), showed potential in downstream applications. However, its comparative\nperformance with traditional deep learning (DL) models remains incompletely\nunderstood. This study aimed to evaluate RETFound against three\nImageNet-pretrained supervised DL models (ResNet50, ViT-base, SwinV2) in\ndetecting ocular and systemic diseases.\n  Methods: We fine-tuned/trained RETFound and three DL models on full datasets,\n50%, 20%, and fixed sample sizes (400, 200, 100 images, with half comprising\ndisease cases; for each DR severity class, 100 and 50 cases were used.\nFine-tuned models were tested internally using the SEED (53,090 images) and\nAPTOS-2019 (3,672 images) datasets and externally validated on population-based\n(BES, CIEMS, SP2, UKBB) and open-source datasets (ODIR-5k, PAPILA, GAMMA,\nIDRiD, MESSIDOR-2). Model performance was compared using area under the\nreceiver operating characteristic curve (AUC) and Z-tests with Bonferroni\ncorrection (P<0.05/3).\n  Interpretation: Traditional DL models are mostly comparable to RETFound for\nocular disease detection with large datasets. However, RETFound is superior in\nsystemic disease detection with smaller datasets. These findings offer valuable\ninsights into the respective merits and limitation of traditional models and\nFMs.\n", "link": "http://arxiv.org/abs/2501.12016v1", "date": "2025-01-21", "relevancy": 2.1787, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Traditional%20Deep%20Learning%20Model%20Approaches%20as%20Effective%20as%20a%0A%20%20Retinal-Specific%20Foundation%20Model%20for%20Ocular%20and%20Systemic%20Disease%20Detection%3F&body=Title%3A%20Are%20Traditional%20Deep%20Learning%20Model%20Approaches%20as%20Effective%20as%20a%0A%20%20Retinal-Specific%20Foundation%20Model%20for%20Ocular%20and%20Systemic%20Disease%20Detection%3F%0AAuthor%3A%20Samantha%20Min%20Er%20Yew%20and%20Xiaofeng%20Lei%20and%20Jocelyn%20Hui%20Lin%20Goh%20and%20Yibing%20Chen%20and%20Sahana%20Srinivasan%20and%20Miao-li%20Chee%20and%20Krithi%20Pushpanathan%20and%20Ke%20Zou%20and%20Qingshan%20Hou%20and%20Zhi%20Da%20Soh%20and%20Cancan%20Xue%20and%20Marco%20Chak%20Yan%20Yu%20and%20Charumathi%20Sabanayagam%20and%20E%20Shyong%20Tai%20and%20Xueling%20Sim%20and%20Yaxing%20Wang%20and%20Jost%20B.%20Jonas%20and%20Vinay%20Nangia%20and%20Gabriel%20Dawei%20Yang%20and%20Emma%20Anran%20Ran%20and%20Carol%20Yim-Lui%20Cheung%20and%20Yangqin%20Feng%20and%20Jun%20Zhou%20and%20Rick%20Siow%20Mong%20Goh%20and%20Yukun%20Zhou%20and%20Pearse%20A.%20Keane%20and%20Yong%20Liu%20and%20Ching-Yu%20Cheng%20and%20Yih-Chung%20Tham%0AAbstract%3A%20%20%20Background%3A%20RETFound%2C%20a%20self-supervised%2C%20retina-specific%20foundation%20model%0A%28FM%29%2C%20showed%20potential%20in%20downstream%20applications.%20However%2C%20its%20comparative%0Aperformance%20with%20traditional%20deep%20learning%20%28DL%29%20models%20remains%20incompletely%0Aunderstood.%20This%20study%20aimed%20to%20evaluate%20RETFound%20against%20three%0AImageNet-pretrained%20supervised%20DL%20models%20%28ResNet50%2C%20ViT-base%2C%20SwinV2%29%20in%0Adetecting%20ocular%20and%20systemic%20diseases.%0A%20%20Methods%3A%20We%20fine-tuned/trained%20RETFound%20and%20three%20DL%20models%20on%20full%20datasets%2C%0A50%25%2C%2020%25%2C%20and%20fixed%20sample%20sizes%20%28400%2C%20200%2C%20100%20images%2C%20with%20half%20comprising%0Adisease%20cases%3B%20for%20each%20DR%20severity%20class%2C%20100%20and%2050%20cases%20were%20used.%0AFine-tuned%20models%20were%20tested%20internally%20using%20the%20SEED%20%2853%2C090%20images%29%20and%0AAPTOS-2019%20%283%2C672%20images%29%20datasets%20and%20externally%20validated%20on%20population-based%0A%28BES%2C%20CIEMS%2C%20SP2%2C%20UKBB%29%20and%20open-source%20datasets%20%28ODIR-5k%2C%20PAPILA%2C%20GAMMA%2C%0AIDRiD%2C%20MESSIDOR-2%29.%20Model%20performance%20was%20compared%20using%20area%20under%20the%0Areceiver%20operating%20characteristic%20curve%20%28AUC%29%20and%20Z-tests%20with%20Bonferroni%0Acorrection%20%28P%3C0.05/3%29.%0A%20%20Interpretation%3A%20Traditional%20DL%20models%20are%20mostly%20comparable%20to%20RETFound%20for%0Aocular%20disease%20detection%20with%20large%20datasets.%20However%2C%20RETFound%20is%20superior%20in%0Asystemic%20disease%20detection%20with%20smaller%20datasets.%20These%20findings%20offer%20valuable%0Ainsights%20into%20the%20respective%20merits%20and%20limitation%20of%20traditional%20models%20and%0AFMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Traditional%2520Deep%2520Learning%2520Model%2520Approaches%2520as%2520Effective%2520as%2520a%250A%2520%2520Retinal-Specific%2520Foundation%2520Model%2520for%2520Ocular%2520and%2520Systemic%2520Disease%2520Detection%253F%26entry.906535625%3DSamantha%2520Min%2520Er%2520Yew%2520and%2520Xiaofeng%2520Lei%2520and%2520Jocelyn%2520Hui%2520Lin%2520Goh%2520and%2520Yibing%2520Chen%2520and%2520Sahana%2520Srinivasan%2520and%2520Miao-li%2520Chee%2520and%2520Krithi%2520Pushpanathan%2520and%2520Ke%2520Zou%2520and%2520Qingshan%2520Hou%2520and%2520Zhi%2520Da%2520Soh%2520and%2520Cancan%2520Xue%2520and%2520Marco%2520Chak%2520Yan%2520Yu%2520and%2520Charumathi%2520Sabanayagam%2520and%2520E%2520Shyong%2520Tai%2520and%2520Xueling%2520Sim%2520and%2520Yaxing%2520Wang%2520and%2520Jost%2520B.%2520Jonas%2520and%2520Vinay%2520Nangia%2520and%2520Gabriel%2520Dawei%2520Yang%2520and%2520Emma%2520Anran%2520Ran%2520and%2520Carol%2520Yim-Lui%2520Cheung%2520and%2520Yangqin%2520Feng%2520and%2520Jun%2520Zhou%2520and%2520Rick%2520Siow%2520Mong%2520Goh%2520and%2520Yukun%2520Zhou%2520and%2520Pearse%2520A.%2520Keane%2520and%2520Yong%2520Liu%2520and%2520Ching-Yu%2520Cheng%2520and%2520Yih-Chung%2520Tham%26entry.1292438233%3D%2520%2520Background%253A%2520RETFound%252C%2520a%2520self-supervised%252C%2520retina-specific%2520foundation%2520model%250A%2528FM%2529%252C%2520showed%2520potential%2520in%2520downstream%2520applications.%2520However%252C%2520its%2520comparative%250Aperformance%2520with%2520traditional%2520deep%2520learning%2520%2528DL%2529%2520models%2520remains%2520incompletely%250Aunderstood.%2520This%2520study%2520aimed%2520to%2520evaluate%2520RETFound%2520against%2520three%250AImageNet-pretrained%2520supervised%2520DL%2520models%2520%2528ResNet50%252C%2520ViT-base%252C%2520SwinV2%2529%2520in%250Adetecting%2520ocular%2520and%2520systemic%2520diseases.%250A%2520%2520Methods%253A%2520We%2520fine-tuned/trained%2520RETFound%2520and%2520three%2520DL%2520models%2520on%2520full%2520datasets%252C%250A50%2525%252C%252020%2525%252C%2520and%2520fixed%2520sample%2520sizes%2520%2528400%252C%2520200%252C%2520100%2520images%252C%2520with%2520half%2520comprising%250Adisease%2520cases%253B%2520for%2520each%2520DR%2520severity%2520class%252C%2520100%2520and%252050%2520cases%2520were%2520used.%250AFine-tuned%2520models%2520were%2520tested%2520internally%2520using%2520the%2520SEED%2520%252853%252C090%2520images%2529%2520and%250AAPTOS-2019%2520%25283%252C672%2520images%2529%2520datasets%2520and%2520externally%2520validated%2520on%2520population-based%250A%2528BES%252C%2520CIEMS%252C%2520SP2%252C%2520UKBB%2529%2520and%2520open-source%2520datasets%2520%2528ODIR-5k%252C%2520PAPILA%252C%2520GAMMA%252C%250AIDRiD%252C%2520MESSIDOR-2%2529.%2520Model%2520performance%2520was%2520compared%2520using%2520area%2520under%2520the%250Areceiver%2520operating%2520characteristic%2520curve%2520%2528AUC%2529%2520and%2520Z-tests%2520with%2520Bonferroni%250Acorrection%2520%2528P%253C0.05/3%2529.%250A%2520%2520Interpretation%253A%2520Traditional%2520DL%2520models%2520are%2520mostly%2520comparable%2520to%2520RETFound%2520for%250Aocular%2520disease%2520detection%2520with%2520large%2520datasets.%2520However%252C%2520RETFound%2520is%2520superior%2520in%250Asystemic%2520disease%2520detection%2520with%2520smaller%2520datasets.%2520These%2520findings%2520offer%2520valuable%250Ainsights%2520into%2520the%2520respective%2520merits%2520and%2520limitation%2520of%2520traditional%2520models%2520and%250AFMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Traditional%20Deep%20Learning%20Model%20Approaches%20as%20Effective%20as%20a%0A%20%20Retinal-Specific%20Foundation%20Model%20for%20Ocular%20and%20Systemic%20Disease%20Detection%3F&entry.906535625=Samantha%20Min%20Er%20Yew%20and%20Xiaofeng%20Lei%20and%20Jocelyn%20Hui%20Lin%20Goh%20and%20Yibing%20Chen%20and%20Sahana%20Srinivasan%20and%20Miao-li%20Chee%20and%20Krithi%20Pushpanathan%20and%20Ke%20Zou%20and%20Qingshan%20Hou%20and%20Zhi%20Da%20Soh%20and%20Cancan%20Xue%20and%20Marco%20Chak%20Yan%20Yu%20and%20Charumathi%20Sabanayagam%20and%20E%20Shyong%20Tai%20and%20Xueling%20Sim%20and%20Yaxing%20Wang%20and%20Jost%20B.%20Jonas%20and%20Vinay%20Nangia%20and%20Gabriel%20Dawei%20Yang%20and%20Emma%20Anran%20Ran%20and%20Carol%20Yim-Lui%20Cheung%20and%20Yangqin%20Feng%20and%20Jun%20Zhou%20and%20Rick%20Siow%20Mong%20Goh%20and%20Yukun%20Zhou%20and%20Pearse%20A.%20Keane%20and%20Yong%20Liu%20and%20Ching-Yu%20Cheng%20and%20Yih-Chung%20Tham&entry.1292438233=%20%20Background%3A%20RETFound%2C%20a%20self-supervised%2C%20retina-specific%20foundation%20model%0A%28FM%29%2C%20showed%20potential%20in%20downstream%20applications.%20However%2C%20its%20comparative%0Aperformance%20with%20traditional%20deep%20learning%20%28DL%29%20models%20remains%20incompletely%0Aunderstood.%20This%20study%20aimed%20to%20evaluate%20RETFound%20against%20three%0AImageNet-pretrained%20supervised%20DL%20models%20%28ResNet50%2C%20ViT-base%2C%20SwinV2%29%20in%0Adetecting%20ocular%20and%20systemic%20diseases.%0A%20%20Methods%3A%20We%20fine-tuned/trained%20RETFound%20and%20three%20DL%20models%20on%20full%20datasets%2C%0A50%25%2C%2020%25%2C%20and%20fixed%20sample%20sizes%20%28400%2C%20200%2C%20100%20images%2C%20with%20half%20comprising%0Adisease%20cases%3B%20for%20each%20DR%20severity%20class%2C%20100%20and%2050%20cases%20were%20used.%0AFine-tuned%20models%20were%20tested%20internally%20using%20the%20SEED%20%2853%2C090%20images%29%20and%0AAPTOS-2019%20%283%2C672%20images%29%20datasets%20and%20externally%20validated%20on%20population-based%0A%28BES%2C%20CIEMS%2C%20SP2%2C%20UKBB%29%20and%20open-source%20datasets%20%28ODIR-5k%2C%20PAPILA%2C%20GAMMA%2C%0AIDRiD%2C%20MESSIDOR-2%29.%20Model%20performance%20was%20compared%20using%20area%20under%20the%0Areceiver%20operating%20characteristic%20curve%20%28AUC%29%20and%20Z-tests%20with%20Bonferroni%0Acorrection%20%28P%3C0.05/3%29.%0A%20%20Interpretation%3A%20Traditional%20DL%20models%20are%20mostly%20comparable%20to%20RETFound%20for%0Aocular%20disease%20detection%20with%20large%20datasets.%20However%2C%20RETFound%20is%20superior%20in%0Asystemic%20disease%20detection%20with%20smaller%20datasets.%20These%20findings%20offer%20valuable%0Ainsights%20into%20the%20respective%20merits%20and%20limitation%20of%20traditional%20models%20and%0AFMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12016v1&entry.124074799=Read"},
{"title": "Cinepro: Robust Training of Foundation Models for Cancer Detection in\n  Prostate Ultrasound Cineloops", "author": "Mohamed Harmanani and Amoon Jamzad and Minh Nguyen Nhat To and Paul F. R. Wilson and Zhuoxin Guo and Fahimeh Fooladgar and Samira Sojoudi and Mahdi Gilany and Silvia Chang and Peter Black and Michael Leveridge and Robert Siemens and Purang Abolmaesumi and Parvin Mousavi", "abstract": "  Prostate cancer (PCa) detection using deep learning (DL) models has shown\npotential for enhancing real-time guidance during biopsies. However, prostate\nultrasound images lack pixel-level cancer annotations, introducing label noise.\nCurrent approaches often focus on limited regions of interest (ROIs),\ndisregarding anatomical context necessary for accurate diagnosis. Foundation\nmodels can overcome this limitation by analyzing entire images to capture\nglobal spatial relationships; however, they still encounter challenges stemming\nfrom the weak labels associated with coarse pathology annotations in ultrasound\ndata. We introduce Cinepro, a novel framework that strengthens foundation\nmodels' ability to localize PCa in ultrasound cineloops. Cinepro adapts robust\ntraining by integrating the proportion of cancer tissue reported by pathology\nin a biopsy core into its loss function to address label noise, providing a\nmore nuanced supervision. Additionally, it leverages temporal data across\nmultiple frames to apply robust augmentations, enhancing the model's ability to\nlearn stable cancer-related features. Cinepro demonstrates superior performance\non a multi-center prostate ultrasound dataset, achieving an AUROC of 77.1% and\na balanced accuracy of 83.8%, surpassing current benchmarks. These findings\nunderscore Cinepro's promise in advancing foundation models for weakly labeled\nultrasound data.\n", "link": "http://arxiv.org/abs/2501.12331v1", "date": "2025-01-21", "relevancy": 2.1756, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5482}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cinepro%3A%20Robust%20Training%20of%20Foundation%20Models%20for%20Cancer%20Detection%20in%0A%20%20Prostate%20Ultrasound%20Cineloops&body=Title%3A%20Cinepro%3A%20Robust%20Training%20of%20Foundation%20Models%20for%20Cancer%20Detection%20in%0A%20%20Prostate%20Ultrasound%20Cineloops%0AAuthor%3A%20Mohamed%20Harmanani%20and%20Amoon%20Jamzad%20and%20Minh%20Nguyen%20Nhat%20To%20and%20Paul%20F.%20R.%20Wilson%20and%20Zhuoxin%20Guo%20and%20Fahimeh%20Fooladgar%20and%20Samira%20Sojoudi%20and%20Mahdi%20Gilany%20and%20Silvia%20Chang%20and%20Peter%20Black%20and%20Michael%20Leveridge%20and%20Robert%20Siemens%20and%20Purang%20Abolmaesumi%20and%20Parvin%20Mousavi%0AAbstract%3A%20%20%20Prostate%20cancer%20%28PCa%29%20detection%20using%20deep%20learning%20%28DL%29%20models%20has%20shown%0Apotential%20for%20enhancing%20real-time%20guidance%20during%20biopsies.%20However%2C%20prostate%0Aultrasound%20images%20lack%20pixel-level%20cancer%20annotations%2C%20introducing%20label%20noise.%0ACurrent%20approaches%20often%20focus%20on%20limited%20regions%20of%20interest%20%28ROIs%29%2C%0Adisregarding%20anatomical%20context%20necessary%20for%20accurate%20diagnosis.%20Foundation%0Amodels%20can%20overcome%20this%20limitation%20by%20analyzing%20entire%20images%20to%20capture%0Aglobal%20spatial%20relationships%3B%20however%2C%20they%20still%20encounter%20challenges%20stemming%0Afrom%20the%20weak%20labels%20associated%20with%20coarse%20pathology%20annotations%20in%20ultrasound%0Adata.%20We%20introduce%20Cinepro%2C%20a%20novel%20framework%20that%20strengthens%20foundation%0Amodels%27%20ability%20to%20localize%20PCa%20in%20ultrasound%20cineloops.%20Cinepro%20adapts%20robust%0Atraining%20by%20integrating%20the%20proportion%20of%20cancer%20tissue%20reported%20by%20pathology%0Ain%20a%20biopsy%20core%20into%20its%20loss%20function%20to%20address%20label%20noise%2C%20providing%20a%0Amore%20nuanced%20supervision.%20Additionally%2C%20it%20leverages%20temporal%20data%20across%0Amultiple%20frames%20to%20apply%20robust%20augmentations%2C%20enhancing%20the%20model%27s%20ability%20to%0Alearn%20stable%20cancer-related%20features.%20Cinepro%20demonstrates%20superior%20performance%0Aon%20a%20multi-center%20prostate%20ultrasound%20dataset%2C%20achieving%20an%20AUROC%20of%2077.1%25%20and%0Aa%20balanced%20accuracy%20of%2083.8%25%2C%20surpassing%20current%20benchmarks.%20These%20findings%0Aunderscore%20Cinepro%27s%20promise%20in%20advancing%20foundation%20models%20for%20weakly%20labeled%0Aultrasound%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCinepro%253A%2520Robust%2520Training%2520of%2520Foundation%2520Models%2520for%2520Cancer%2520Detection%2520in%250A%2520%2520Prostate%2520Ultrasound%2520Cineloops%26entry.906535625%3DMohamed%2520Harmanani%2520and%2520Amoon%2520Jamzad%2520and%2520Minh%2520Nguyen%2520Nhat%2520To%2520and%2520Paul%2520F.%2520R.%2520Wilson%2520and%2520Zhuoxin%2520Guo%2520and%2520Fahimeh%2520Fooladgar%2520and%2520Samira%2520Sojoudi%2520and%2520Mahdi%2520Gilany%2520and%2520Silvia%2520Chang%2520and%2520Peter%2520Black%2520and%2520Michael%2520Leveridge%2520and%2520Robert%2520Siemens%2520and%2520Purang%2520Abolmaesumi%2520and%2520Parvin%2520Mousavi%26entry.1292438233%3D%2520%2520Prostate%2520cancer%2520%2528PCa%2529%2520detection%2520using%2520deep%2520learning%2520%2528DL%2529%2520models%2520has%2520shown%250Apotential%2520for%2520enhancing%2520real-time%2520guidance%2520during%2520biopsies.%2520However%252C%2520prostate%250Aultrasound%2520images%2520lack%2520pixel-level%2520cancer%2520annotations%252C%2520introducing%2520label%2520noise.%250ACurrent%2520approaches%2520often%2520focus%2520on%2520limited%2520regions%2520of%2520interest%2520%2528ROIs%2529%252C%250Adisregarding%2520anatomical%2520context%2520necessary%2520for%2520accurate%2520diagnosis.%2520Foundation%250Amodels%2520can%2520overcome%2520this%2520limitation%2520by%2520analyzing%2520entire%2520images%2520to%2520capture%250Aglobal%2520spatial%2520relationships%253B%2520however%252C%2520they%2520still%2520encounter%2520challenges%2520stemming%250Afrom%2520the%2520weak%2520labels%2520associated%2520with%2520coarse%2520pathology%2520annotations%2520in%2520ultrasound%250Adata.%2520We%2520introduce%2520Cinepro%252C%2520a%2520novel%2520framework%2520that%2520strengthens%2520foundation%250Amodels%2527%2520ability%2520to%2520localize%2520PCa%2520in%2520ultrasound%2520cineloops.%2520Cinepro%2520adapts%2520robust%250Atraining%2520by%2520integrating%2520the%2520proportion%2520of%2520cancer%2520tissue%2520reported%2520by%2520pathology%250Ain%2520a%2520biopsy%2520core%2520into%2520its%2520loss%2520function%2520to%2520address%2520label%2520noise%252C%2520providing%2520a%250Amore%2520nuanced%2520supervision.%2520Additionally%252C%2520it%2520leverages%2520temporal%2520data%2520across%250Amultiple%2520frames%2520to%2520apply%2520robust%2520augmentations%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%250Alearn%2520stable%2520cancer-related%2520features.%2520Cinepro%2520demonstrates%2520superior%2520performance%250Aon%2520a%2520multi-center%2520prostate%2520ultrasound%2520dataset%252C%2520achieving%2520an%2520AUROC%2520of%252077.1%2525%2520and%250Aa%2520balanced%2520accuracy%2520of%252083.8%2525%252C%2520surpassing%2520current%2520benchmarks.%2520These%2520findings%250Aunderscore%2520Cinepro%2527s%2520promise%2520in%2520advancing%2520foundation%2520models%2520for%2520weakly%2520labeled%250Aultrasound%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cinepro%3A%20Robust%20Training%20of%20Foundation%20Models%20for%20Cancer%20Detection%20in%0A%20%20Prostate%20Ultrasound%20Cineloops&entry.906535625=Mohamed%20Harmanani%20and%20Amoon%20Jamzad%20and%20Minh%20Nguyen%20Nhat%20To%20and%20Paul%20F.%20R.%20Wilson%20and%20Zhuoxin%20Guo%20and%20Fahimeh%20Fooladgar%20and%20Samira%20Sojoudi%20and%20Mahdi%20Gilany%20and%20Silvia%20Chang%20and%20Peter%20Black%20and%20Michael%20Leveridge%20and%20Robert%20Siemens%20and%20Purang%20Abolmaesumi%20and%20Parvin%20Mousavi&entry.1292438233=%20%20Prostate%20cancer%20%28PCa%29%20detection%20using%20deep%20learning%20%28DL%29%20models%20has%20shown%0Apotential%20for%20enhancing%20real-time%20guidance%20during%20biopsies.%20However%2C%20prostate%0Aultrasound%20images%20lack%20pixel-level%20cancer%20annotations%2C%20introducing%20label%20noise.%0ACurrent%20approaches%20often%20focus%20on%20limited%20regions%20of%20interest%20%28ROIs%29%2C%0Adisregarding%20anatomical%20context%20necessary%20for%20accurate%20diagnosis.%20Foundation%0Amodels%20can%20overcome%20this%20limitation%20by%20analyzing%20entire%20images%20to%20capture%0Aglobal%20spatial%20relationships%3B%20however%2C%20they%20still%20encounter%20challenges%20stemming%0Afrom%20the%20weak%20labels%20associated%20with%20coarse%20pathology%20annotations%20in%20ultrasound%0Adata.%20We%20introduce%20Cinepro%2C%20a%20novel%20framework%20that%20strengthens%20foundation%0Amodels%27%20ability%20to%20localize%20PCa%20in%20ultrasound%20cineloops.%20Cinepro%20adapts%20robust%0Atraining%20by%20integrating%20the%20proportion%20of%20cancer%20tissue%20reported%20by%20pathology%0Ain%20a%20biopsy%20core%20into%20its%20loss%20function%20to%20address%20label%20noise%2C%20providing%20a%0Amore%20nuanced%20supervision.%20Additionally%2C%20it%20leverages%20temporal%20data%20across%0Amultiple%20frames%20to%20apply%20robust%20augmentations%2C%20enhancing%20the%20model%27s%20ability%20to%0Alearn%20stable%20cancer-related%20features.%20Cinepro%20demonstrates%20superior%20performance%0Aon%20a%20multi-center%20prostate%20ultrasound%20dataset%2C%20achieving%20an%20AUROC%20of%2077.1%25%20and%0Aa%20balanced%20accuracy%20of%2083.8%25%2C%20surpassing%20current%20benchmarks.%20These%20findings%0Aunderscore%20Cinepro%27s%20promise%20in%20advancing%20foundation%20models%20for%20weakly%20labeled%0Aultrasound%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12331v1&entry.124074799=Read"},
{"title": "Large Language Model-Brained GUI Agents: A Survey", "author": "Chaoyun Zhang and Shilin He and Jiaxu Qian and Bowen Li and Liqun Li and Si Qin and Yu Kang and Minghua Ma and Guyue Liu and Qingwei Lin and Saravan Rajmohan and Dongmei Zhang and Qi Zhang", "abstract": "  GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.\n", "link": "http://arxiv.org/abs/2411.18279v7", "date": "2025-01-21", "relevancy": 2.1753, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5735}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model-Brained%20GUI%20Agents%3A%20A%20Survey&body=Title%3A%20Large%20Language%20Model-Brained%20GUI%20Agents%3A%20A%20Survey%0AAuthor%3A%20Chaoyun%20Zhang%20and%20Shilin%20He%20and%20Jiaxu%20Qian%20and%20Bowen%20Li%20and%20Liqun%20Li%20and%20Si%20Qin%20and%20Yu%20Kang%20and%20Minghua%20Ma%20and%20Guyue%20Liu%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20GUIs%20have%20long%20been%20central%20to%20human-computer%20interaction%2C%20providing%20an%0Aintuitive%20and%20visually-driven%20way%20to%20access%20and%20interact%20with%20digital%20systems.%0AThe%20advent%20of%20LLMs%2C%20particularly%20multimodal%20models%2C%20has%20ushered%20in%20a%20new%20era%20of%0AGUI%20automation.%20They%20have%20demonstrated%20exceptional%20capabilities%20in%20natural%0Alanguage%20understanding%2C%20code%20generation%2C%20and%20visual%20processing.%20This%20has%20paved%0Athe%20way%20for%20a%20new%20generation%20of%20LLM-brained%20GUI%20agents%20capable%20of%20interpreting%0Acomplex%20GUI%20elements%20and%20autonomously%20executing%20actions%20based%20on%20natural%0Alanguage%20instructions.%20These%20agents%20represent%20a%20paradigm%20shift%2C%20enabling%20users%0Ato%20perform%20intricate%2C%20multi-step%20tasks%20through%20simple%20conversational%20commands.%0ATheir%20applications%20span%20across%20web%20navigation%2C%20mobile%20app%20interactions%2C%20and%0Adesktop%20automation%2C%20offering%20a%20transformative%20user%20experience%20that%0Arevolutionizes%20how%20individuals%20interact%20with%20software.%20This%20emerging%20field%20is%0Arapidly%20advancing%2C%20with%20significant%20progress%20in%20both%20research%20and%20industry.%0A%20%20To%20provide%20a%20structured%20understanding%20of%20this%20trend%2C%20this%20paper%20presents%20a%0Acomprehensive%20survey%20of%20LLM-brained%20GUI%20agents%2C%20exploring%20their%20historical%0Aevolution%2C%20core%20components%2C%20and%20advanced%20techniques.%20We%20address%20research%0Aquestions%20such%20as%20existing%20GUI%20agent%20frameworks%2C%20the%20collection%20and%20utilization%0Aof%20data%20for%20training%20specialized%20GUI%20agents%2C%20the%20development%20of%20large%20action%0Amodels%20tailored%20for%20GUI%20tasks%2C%20and%20the%20evaluation%20metrics%20and%20benchmarks%0Anecessary%20to%20assess%20their%20effectiveness.%20Additionally%2C%20we%20examine%20emerging%0Aapplications%20powered%20by%20these%20agents.%20Through%20a%20detailed%20analysis%2C%20this%20survey%0Aidentifies%20key%20research%20gaps%20and%20outlines%20a%20roadmap%20for%20future%20advancements%20in%0Athe%20field.%20By%20consolidating%20foundational%20knowledge%20and%20state-of-the-art%0Adevelopments%2C%20this%20work%20aims%20to%20guide%20both%20researchers%20and%20practitioners%20in%0Aovercoming%20challenges%20and%20unlocking%20the%20full%20potential%20of%20LLM-brained%20GUI%0Aagents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18279v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model-Brained%2520GUI%2520Agents%253A%2520A%2520Survey%26entry.906535625%3DChaoyun%2520Zhang%2520and%2520Shilin%2520He%2520and%2520Jiaxu%2520Qian%2520and%2520Bowen%2520Li%2520and%2520Liqun%2520Li%2520and%2520Si%2520Qin%2520and%2520Yu%2520Kang%2520and%2520Minghua%2520Ma%2520and%2520Guyue%2520Liu%2520and%2520Qingwei%2520Lin%2520and%2520Saravan%2520Rajmohan%2520and%2520Dongmei%2520Zhang%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520GUIs%2520have%2520long%2520been%2520central%2520to%2520human-computer%2520interaction%252C%2520providing%2520an%250Aintuitive%2520and%2520visually-driven%2520way%2520to%2520access%2520and%2520interact%2520with%2520digital%2520systems.%250AThe%2520advent%2520of%2520LLMs%252C%2520particularly%2520multimodal%2520models%252C%2520has%2520ushered%2520in%2520a%2520new%2520era%2520of%250AGUI%2520automation.%2520They%2520have%2520demonstrated%2520exceptional%2520capabilities%2520in%2520natural%250Alanguage%2520understanding%252C%2520code%2520generation%252C%2520and%2520visual%2520processing.%2520This%2520has%2520paved%250Athe%2520way%2520for%2520a%2520new%2520generation%2520of%2520LLM-brained%2520GUI%2520agents%2520capable%2520of%2520interpreting%250Acomplex%2520GUI%2520elements%2520and%2520autonomously%2520executing%2520actions%2520based%2520on%2520natural%250Alanguage%2520instructions.%2520These%2520agents%2520represent%2520a%2520paradigm%2520shift%252C%2520enabling%2520users%250Ato%2520perform%2520intricate%252C%2520multi-step%2520tasks%2520through%2520simple%2520conversational%2520commands.%250ATheir%2520applications%2520span%2520across%2520web%2520navigation%252C%2520mobile%2520app%2520interactions%252C%2520and%250Adesktop%2520automation%252C%2520offering%2520a%2520transformative%2520user%2520experience%2520that%250Arevolutionizes%2520how%2520individuals%2520interact%2520with%2520software.%2520This%2520emerging%2520field%2520is%250Arapidly%2520advancing%252C%2520with%2520significant%2520progress%2520in%2520both%2520research%2520and%2520industry.%250A%2520%2520To%2520provide%2520a%2520structured%2520understanding%2520of%2520this%2520trend%252C%2520this%2520paper%2520presents%2520a%250Acomprehensive%2520survey%2520of%2520LLM-brained%2520GUI%2520agents%252C%2520exploring%2520their%2520historical%250Aevolution%252C%2520core%2520components%252C%2520and%2520advanced%2520techniques.%2520We%2520address%2520research%250Aquestions%2520such%2520as%2520existing%2520GUI%2520agent%2520frameworks%252C%2520the%2520collection%2520and%2520utilization%250Aof%2520data%2520for%2520training%2520specialized%2520GUI%2520agents%252C%2520the%2520development%2520of%2520large%2520action%250Amodels%2520tailored%2520for%2520GUI%2520tasks%252C%2520and%2520the%2520evaluation%2520metrics%2520and%2520benchmarks%250Anecessary%2520to%2520assess%2520their%2520effectiveness.%2520Additionally%252C%2520we%2520examine%2520emerging%250Aapplications%2520powered%2520by%2520these%2520agents.%2520Through%2520a%2520detailed%2520analysis%252C%2520this%2520survey%250Aidentifies%2520key%2520research%2520gaps%2520and%2520outlines%2520a%2520roadmap%2520for%2520future%2520advancements%2520in%250Athe%2520field.%2520By%2520consolidating%2520foundational%2520knowledge%2520and%2520state-of-the-art%250Adevelopments%252C%2520this%2520work%2520aims%2520to%2520guide%2520both%2520researchers%2520and%2520practitioners%2520in%250Aovercoming%2520challenges%2520and%2520unlocking%2520the%2520full%2520potential%2520of%2520LLM-brained%2520GUI%250Aagents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18279v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model-Brained%20GUI%20Agents%3A%20A%20Survey&entry.906535625=Chaoyun%20Zhang%20and%20Shilin%20He%20and%20Jiaxu%20Qian%20and%20Bowen%20Li%20and%20Liqun%20Li%20and%20Si%20Qin%20and%20Yu%20Kang%20and%20Minghua%20Ma%20and%20Guyue%20Liu%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang&entry.1292438233=%20%20GUIs%20have%20long%20been%20central%20to%20human-computer%20interaction%2C%20providing%20an%0Aintuitive%20and%20visually-driven%20way%20to%20access%20and%20interact%20with%20digital%20systems.%0AThe%20advent%20of%20LLMs%2C%20particularly%20multimodal%20models%2C%20has%20ushered%20in%20a%20new%20era%20of%0AGUI%20automation.%20They%20have%20demonstrated%20exceptional%20capabilities%20in%20natural%0Alanguage%20understanding%2C%20code%20generation%2C%20and%20visual%20processing.%20This%20has%20paved%0Athe%20way%20for%20a%20new%20generation%20of%20LLM-brained%20GUI%20agents%20capable%20of%20interpreting%0Acomplex%20GUI%20elements%20and%20autonomously%20executing%20actions%20based%20on%20natural%0Alanguage%20instructions.%20These%20agents%20represent%20a%20paradigm%20shift%2C%20enabling%20users%0Ato%20perform%20intricate%2C%20multi-step%20tasks%20through%20simple%20conversational%20commands.%0ATheir%20applications%20span%20across%20web%20navigation%2C%20mobile%20app%20interactions%2C%20and%0Adesktop%20automation%2C%20offering%20a%20transformative%20user%20experience%20that%0Arevolutionizes%20how%20individuals%20interact%20with%20software.%20This%20emerging%20field%20is%0Arapidly%20advancing%2C%20with%20significant%20progress%20in%20both%20research%20and%20industry.%0A%20%20To%20provide%20a%20structured%20understanding%20of%20this%20trend%2C%20this%20paper%20presents%20a%0Acomprehensive%20survey%20of%20LLM-brained%20GUI%20agents%2C%20exploring%20their%20historical%0Aevolution%2C%20core%20components%2C%20and%20advanced%20techniques.%20We%20address%20research%0Aquestions%20such%20as%20existing%20GUI%20agent%20frameworks%2C%20the%20collection%20and%20utilization%0Aof%20data%20for%20training%20specialized%20GUI%20agents%2C%20the%20development%20of%20large%20action%0Amodels%20tailored%20for%20GUI%20tasks%2C%20and%20the%20evaluation%20metrics%20and%20benchmarks%0Anecessary%20to%20assess%20their%20effectiveness.%20Additionally%2C%20we%20examine%20emerging%0Aapplications%20powered%20by%20these%20agents.%20Through%20a%20detailed%20analysis%2C%20this%20survey%0Aidentifies%20key%20research%20gaps%20and%20outlines%20a%20roadmap%20for%20future%20advancements%20in%0Athe%20field.%20By%20consolidating%20foundational%20knowledge%20and%20state-of-the-art%0Adevelopments%2C%20this%20work%20aims%20to%20guide%20both%20researchers%20and%20practitioners%20in%0Aovercoming%20challenges%20and%20unlocking%20the%20full%20potential%20of%20LLM-brained%20GUI%0Aagents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18279v7&entry.124074799=Read"},
{"title": "Multi-Scale Texture Loss for CT denoising with GANs", "author": "Francesco Di Feola and Lorenzo Tronchin and Valerio Guarrasi and Paolo Soda", "abstract": "  Generative Adversarial Networks (GANs) have proved as a powerful framework\nfor denoising applications in medical imaging. However, GAN-based denoising\nalgorithms still suffer from limitations in capturing complex relationships\nwithin the images. In this regard, the loss function plays a crucial role in\nguiding the image generation process, encompassing how much a synthetic image\ndiffers from a real image. To grasp highly complex and non-linear textural\nrelationships in the training process, this work presents a novel approach to\ncapture and embed multi-scale texture information into the loss function. Our\nmethod introduces a differentiable multi-scale texture representation of the\nimages dynamically aggregated by a self-attention layer, thus exploiting\nend-to-end gradient-based optimization. We validate our approach by carrying\nout extensive experiments in the context of low-dose CT denoising, a\nchallenging application that aims to enhance the quality of noisy CT scans. We\nutilize three publicly available datasets, including one simulated and two real\ndatasets. The results are promising as compared to other well-established loss\nfunctions, being also consistent across three different GAN architectures. The\ncode is available at:\nhttps://github.com/TrainLaboratory/MultiScaleTextureLoss-MSTLF\n", "link": "http://arxiv.org/abs/2403.16640v2", "date": "2025-01-21", "relevancy": 2.1715, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5517}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5506}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Texture%20Loss%20for%20CT%20denoising%20with%20GANs&body=Title%3A%20Multi-Scale%20Texture%20Loss%20for%20CT%20denoising%20with%20GANs%0AAuthor%3A%20Francesco%20Di%20Feola%20and%20Lorenzo%20Tronchin%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20proved%20as%20a%20powerful%20framework%0Afor%20denoising%20applications%20in%20medical%20imaging.%20However%2C%20GAN-based%20denoising%0Aalgorithms%20still%20suffer%20from%20limitations%20in%20capturing%20complex%20relationships%0Awithin%20the%20images.%20In%20this%20regard%2C%20the%20loss%20function%20plays%20a%20crucial%20role%20in%0Aguiding%20the%20image%20generation%20process%2C%20encompassing%20how%20much%20a%20synthetic%20image%0Adiffers%20from%20a%20real%20image.%20To%20grasp%20highly%20complex%20and%20non-linear%20textural%0Arelationships%20in%20the%20training%20process%2C%20this%20work%20presents%20a%20novel%20approach%20to%0Acapture%20and%20embed%20multi-scale%20texture%20information%20into%20the%20loss%20function.%20Our%0Amethod%20introduces%20a%20differentiable%20multi-scale%20texture%20representation%20of%20the%0Aimages%20dynamically%20aggregated%20by%20a%20self-attention%20layer%2C%20thus%20exploiting%0Aend-to-end%20gradient-based%20optimization.%20We%20validate%20our%20approach%20by%20carrying%0Aout%20extensive%20experiments%20in%20the%20context%20of%20low-dose%20CT%20denoising%2C%20a%0Achallenging%20application%20that%20aims%20to%20enhance%20the%20quality%20of%20noisy%20CT%20scans.%20We%0Autilize%20three%20publicly%20available%20datasets%2C%20including%20one%20simulated%20and%20two%20real%0Adatasets.%20The%20results%20are%20promising%20as%20compared%20to%20other%20well-established%20loss%0Afunctions%2C%20being%20also%20consistent%20across%20three%20different%20GAN%20architectures.%20The%0Acode%20is%20available%20at%3A%0Ahttps%3A//github.com/TrainLaboratory/MultiScaleTextureLoss-MSTLF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16640v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520Texture%2520Loss%2520for%2520CT%2520denoising%2520with%2520GANs%26entry.906535625%3DFrancesco%2520Di%2520Feola%2520and%2520Lorenzo%2520Tronchin%2520and%2520Valerio%2520Guarrasi%2520and%2520Paolo%2520Soda%26entry.1292438233%3D%2520%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520have%2520proved%2520as%2520a%2520powerful%2520framework%250Afor%2520denoising%2520applications%2520in%2520medical%2520imaging.%2520However%252C%2520GAN-based%2520denoising%250Aalgorithms%2520still%2520suffer%2520from%2520limitations%2520in%2520capturing%2520complex%2520relationships%250Awithin%2520the%2520images.%2520In%2520this%2520regard%252C%2520the%2520loss%2520function%2520plays%2520a%2520crucial%2520role%2520in%250Aguiding%2520the%2520image%2520generation%2520process%252C%2520encompassing%2520how%2520much%2520a%2520synthetic%2520image%250Adiffers%2520from%2520a%2520real%2520image.%2520To%2520grasp%2520highly%2520complex%2520and%2520non-linear%2520textural%250Arelationships%2520in%2520the%2520training%2520process%252C%2520this%2520work%2520presents%2520a%2520novel%2520approach%2520to%250Acapture%2520and%2520embed%2520multi-scale%2520texture%2520information%2520into%2520the%2520loss%2520function.%2520Our%250Amethod%2520introduces%2520a%2520differentiable%2520multi-scale%2520texture%2520representation%2520of%2520the%250Aimages%2520dynamically%2520aggregated%2520by%2520a%2520self-attention%2520layer%252C%2520thus%2520exploiting%250Aend-to-end%2520gradient-based%2520optimization.%2520We%2520validate%2520our%2520approach%2520by%2520carrying%250Aout%2520extensive%2520experiments%2520in%2520the%2520context%2520of%2520low-dose%2520CT%2520denoising%252C%2520a%250Achallenging%2520application%2520that%2520aims%2520to%2520enhance%2520the%2520quality%2520of%2520noisy%2520CT%2520scans.%2520We%250Autilize%2520three%2520publicly%2520available%2520datasets%252C%2520including%2520one%2520simulated%2520and%2520two%2520real%250Adatasets.%2520The%2520results%2520are%2520promising%2520as%2520compared%2520to%2520other%2520well-established%2520loss%250Afunctions%252C%2520being%2520also%2520consistent%2520across%2520three%2520different%2520GAN%2520architectures.%2520The%250Acode%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/TrainLaboratory/MultiScaleTextureLoss-MSTLF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16640v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Texture%20Loss%20for%20CT%20denoising%20with%20GANs&entry.906535625=Francesco%20Di%20Feola%20and%20Lorenzo%20Tronchin%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20proved%20as%20a%20powerful%20framework%0Afor%20denoising%20applications%20in%20medical%20imaging.%20However%2C%20GAN-based%20denoising%0Aalgorithms%20still%20suffer%20from%20limitations%20in%20capturing%20complex%20relationships%0Awithin%20the%20images.%20In%20this%20regard%2C%20the%20loss%20function%20plays%20a%20crucial%20role%20in%0Aguiding%20the%20image%20generation%20process%2C%20encompassing%20how%20much%20a%20synthetic%20image%0Adiffers%20from%20a%20real%20image.%20To%20grasp%20highly%20complex%20and%20non-linear%20textural%0Arelationships%20in%20the%20training%20process%2C%20this%20work%20presents%20a%20novel%20approach%20to%0Acapture%20and%20embed%20multi-scale%20texture%20information%20into%20the%20loss%20function.%20Our%0Amethod%20introduces%20a%20differentiable%20multi-scale%20texture%20representation%20of%20the%0Aimages%20dynamically%20aggregated%20by%20a%20self-attention%20layer%2C%20thus%20exploiting%0Aend-to-end%20gradient-based%20optimization.%20We%20validate%20our%20approach%20by%20carrying%0Aout%20extensive%20experiments%20in%20the%20context%20of%20low-dose%20CT%20denoising%2C%20a%0Achallenging%20application%20that%20aims%20to%20enhance%20the%20quality%20of%20noisy%20CT%20scans.%20We%0Autilize%20three%20publicly%20available%20datasets%2C%20including%20one%20simulated%20and%20two%20real%0Adatasets.%20The%20results%20are%20promising%20as%20compared%20to%20other%20well-established%20loss%0Afunctions%2C%20being%20also%20consistent%20across%20three%20different%20GAN%20architectures.%20The%0Acode%20is%20available%20at%3A%0Ahttps%3A//github.com/TrainLaboratory/MultiScaleTextureLoss-MSTLF%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16640v2&entry.124074799=Read"},
{"title": "CoDTS: Enhancing Sparsely Supervised Collaborative Perception with a\n  Dual Teacher-Student Framework", "author": "Yushan Han and Hui Zhang and Honglei Zhang and Jing Wang and Yidong Li", "abstract": "  Current collaborative perception methods often rely on fully annotated\ndatasets, which can be expensive to obtain in practical situations. To reduce\nannotation costs, some works adopt sparsely supervised learning techniques and\ngenerate pseudo labels for the missing instances. However, these methods fail\nto achieve an optimal confidence threshold that harmonizes the quality and\nquantity of pseudo labels. To address this issue, we propose an end-to-end\nCollaborative perception Dual Teacher-Student framework (CoDTS), which employs\nadaptive complementary learning to produce both high-quality and high-quantity\npseudo labels. Specifically, the Main Foreground Mining (MFM) module generates\nhigh-quality pseudo labels based on the prediction of the static teacher.\nSubsequently, the Supplement Foreground Mining (SFM) module ensures a balance\nbetween the quality and quantity of pseudo labels by adaptively identifying\nmissing instances based on the prediction of the dynamic teacher. Additionally,\nthe Neighbor Anchor Sampling (NAS) module is incorporated to enhance the\nrepresentation of pseudo labels. To promote the adaptive complementary\nlearning, we implement a staged training strategy that trains the student and\ndynamic teacher in a mutually beneficial manner. Extensive experiments\ndemonstrate that the CoDTS effectively ensures an optimal balance of pseudo\nlabels in both quality and quantity, establishing a new state-of-the-art in\nsparsely supervised collaborative perception.\n", "link": "http://arxiv.org/abs/2412.08344v3", "date": "2025-01-21", "relevancy": 2.1598, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5435}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoDTS%3A%20Enhancing%20Sparsely%20Supervised%20Collaborative%20Perception%20with%20a%0A%20%20Dual%20Teacher-Student%20Framework&body=Title%3A%20CoDTS%3A%20Enhancing%20Sparsely%20Supervised%20Collaborative%20Perception%20with%20a%0A%20%20Dual%20Teacher-Student%20Framework%0AAuthor%3A%20Yushan%20Han%20and%20Hui%20Zhang%20and%20Honglei%20Zhang%20and%20Jing%20Wang%20and%20Yidong%20Li%0AAbstract%3A%20%20%20Current%20collaborative%20perception%20methods%20often%20rely%20on%20fully%20annotated%0Adatasets%2C%20which%20can%20be%20expensive%20to%20obtain%20in%20practical%20situations.%20To%20reduce%0Aannotation%20costs%2C%20some%20works%20adopt%20sparsely%20supervised%20learning%20techniques%20and%0Agenerate%20pseudo%20labels%20for%20the%20missing%20instances.%20However%2C%20these%20methods%20fail%0Ato%20achieve%20an%20optimal%20confidence%20threshold%20that%20harmonizes%20the%20quality%20and%0Aquantity%20of%20pseudo%20labels.%20To%20address%20this%20issue%2C%20we%20propose%20an%20end-to-end%0ACollaborative%20perception%20Dual%20Teacher-Student%20framework%20%28CoDTS%29%2C%20which%20employs%0Aadaptive%20complementary%20learning%20to%20produce%20both%20high-quality%20and%20high-quantity%0Apseudo%20labels.%20Specifically%2C%20the%20Main%20Foreground%20Mining%20%28MFM%29%20module%20generates%0Ahigh-quality%20pseudo%20labels%20based%20on%20the%20prediction%20of%20the%20static%20teacher.%0ASubsequently%2C%20the%20Supplement%20Foreground%20Mining%20%28SFM%29%20module%20ensures%20a%20balance%0Abetween%20the%20quality%20and%20quantity%20of%20pseudo%20labels%20by%20adaptively%20identifying%0Amissing%20instances%20based%20on%20the%20prediction%20of%20the%20dynamic%20teacher.%20Additionally%2C%0Athe%20Neighbor%20Anchor%20Sampling%20%28NAS%29%20module%20is%20incorporated%20to%20enhance%20the%0Arepresentation%20of%20pseudo%20labels.%20To%20promote%20the%20adaptive%20complementary%0Alearning%2C%20we%20implement%20a%20staged%20training%20strategy%20that%20trains%20the%20student%20and%0Adynamic%20teacher%20in%20a%20mutually%20beneficial%20manner.%20Extensive%20experiments%0Ademonstrate%20that%20the%20CoDTS%20effectively%20ensures%20an%20optimal%20balance%20of%20pseudo%0Alabels%20in%20both%20quality%20and%20quantity%2C%20establishing%20a%20new%20state-of-the-art%20in%0Asparsely%20supervised%20collaborative%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08344v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoDTS%253A%2520Enhancing%2520Sparsely%2520Supervised%2520Collaborative%2520Perception%2520with%2520a%250A%2520%2520Dual%2520Teacher-Student%2520Framework%26entry.906535625%3DYushan%2520Han%2520and%2520Hui%2520Zhang%2520and%2520Honglei%2520Zhang%2520and%2520Jing%2520Wang%2520and%2520Yidong%2520Li%26entry.1292438233%3D%2520%2520Current%2520collaborative%2520perception%2520methods%2520often%2520rely%2520on%2520fully%2520annotated%250Adatasets%252C%2520which%2520can%2520be%2520expensive%2520to%2520obtain%2520in%2520practical%2520situations.%2520To%2520reduce%250Aannotation%2520costs%252C%2520some%2520works%2520adopt%2520sparsely%2520supervised%2520learning%2520techniques%2520and%250Agenerate%2520pseudo%2520labels%2520for%2520the%2520missing%2520instances.%2520However%252C%2520these%2520methods%2520fail%250Ato%2520achieve%2520an%2520optimal%2520confidence%2520threshold%2520that%2520harmonizes%2520the%2520quality%2520and%250Aquantity%2520of%2520pseudo%2520labels.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520an%2520end-to-end%250ACollaborative%2520perception%2520Dual%2520Teacher-Student%2520framework%2520%2528CoDTS%2529%252C%2520which%2520employs%250Aadaptive%2520complementary%2520learning%2520to%2520produce%2520both%2520high-quality%2520and%2520high-quantity%250Apseudo%2520labels.%2520Specifically%252C%2520the%2520Main%2520Foreground%2520Mining%2520%2528MFM%2529%2520module%2520generates%250Ahigh-quality%2520pseudo%2520labels%2520based%2520on%2520the%2520prediction%2520of%2520the%2520static%2520teacher.%250ASubsequently%252C%2520the%2520Supplement%2520Foreground%2520Mining%2520%2528SFM%2529%2520module%2520ensures%2520a%2520balance%250Abetween%2520the%2520quality%2520and%2520quantity%2520of%2520pseudo%2520labels%2520by%2520adaptively%2520identifying%250Amissing%2520instances%2520based%2520on%2520the%2520prediction%2520of%2520the%2520dynamic%2520teacher.%2520Additionally%252C%250Athe%2520Neighbor%2520Anchor%2520Sampling%2520%2528NAS%2529%2520module%2520is%2520incorporated%2520to%2520enhance%2520the%250Arepresentation%2520of%2520pseudo%2520labels.%2520To%2520promote%2520the%2520adaptive%2520complementary%250Alearning%252C%2520we%2520implement%2520a%2520staged%2520training%2520strategy%2520that%2520trains%2520the%2520student%2520and%250Adynamic%2520teacher%2520in%2520a%2520mutually%2520beneficial%2520manner.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520the%2520CoDTS%2520effectively%2520ensures%2520an%2520optimal%2520balance%2520of%2520pseudo%250Alabels%2520in%2520both%2520quality%2520and%2520quantity%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520in%250Asparsely%2520supervised%2520collaborative%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08344v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoDTS%3A%20Enhancing%20Sparsely%20Supervised%20Collaborative%20Perception%20with%20a%0A%20%20Dual%20Teacher-Student%20Framework&entry.906535625=Yushan%20Han%20and%20Hui%20Zhang%20and%20Honglei%20Zhang%20and%20Jing%20Wang%20and%20Yidong%20Li&entry.1292438233=%20%20Current%20collaborative%20perception%20methods%20often%20rely%20on%20fully%20annotated%0Adatasets%2C%20which%20can%20be%20expensive%20to%20obtain%20in%20practical%20situations.%20To%20reduce%0Aannotation%20costs%2C%20some%20works%20adopt%20sparsely%20supervised%20learning%20techniques%20and%0Agenerate%20pseudo%20labels%20for%20the%20missing%20instances.%20However%2C%20these%20methods%20fail%0Ato%20achieve%20an%20optimal%20confidence%20threshold%20that%20harmonizes%20the%20quality%20and%0Aquantity%20of%20pseudo%20labels.%20To%20address%20this%20issue%2C%20we%20propose%20an%20end-to-end%0ACollaborative%20perception%20Dual%20Teacher-Student%20framework%20%28CoDTS%29%2C%20which%20employs%0Aadaptive%20complementary%20learning%20to%20produce%20both%20high-quality%20and%20high-quantity%0Apseudo%20labels.%20Specifically%2C%20the%20Main%20Foreground%20Mining%20%28MFM%29%20module%20generates%0Ahigh-quality%20pseudo%20labels%20based%20on%20the%20prediction%20of%20the%20static%20teacher.%0ASubsequently%2C%20the%20Supplement%20Foreground%20Mining%20%28SFM%29%20module%20ensures%20a%20balance%0Abetween%20the%20quality%20and%20quantity%20of%20pseudo%20labels%20by%20adaptively%20identifying%0Amissing%20instances%20based%20on%20the%20prediction%20of%20the%20dynamic%20teacher.%20Additionally%2C%0Athe%20Neighbor%20Anchor%20Sampling%20%28NAS%29%20module%20is%20incorporated%20to%20enhance%20the%0Arepresentation%20of%20pseudo%20labels.%20To%20promote%20the%20adaptive%20complementary%0Alearning%2C%20we%20implement%20a%20staged%20training%20strategy%20that%20trains%20the%20student%20and%0Adynamic%20teacher%20in%20a%20mutually%20beneficial%20manner.%20Extensive%20experiments%0Ademonstrate%20that%20the%20CoDTS%20effectively%20ensures%20an%20optimal%20balance%20of%20pseudo%0Alabels%20in%20both%20quality%20and%20quantity%2C%20establishing%20a%20new%20state-of-the-art%20in%0Asparsely%20supervised%20collaborative%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08344v3&entry.124074799=Read"},
{"title": "Video Deblurring by Sharpness Prior Detection and Edge Information", "author": "Yang Tian and Fabio Brau and Giulio Rossolini and Giorgio Buttazzo and Hao Meng", "abstract": "  Video deblurring is essential task for autonomous driving, facial\nrecognition, and security surveillance. Traditional methods directly estimate\nmotion blur kernels, often introducing artifacts and leading to poor results.\nRecent approaches utilize the detection of sharp frames within video sequences\nto enhance deblurring. However, existing datasets rely on fixed number of sharp\nframes, which may be too restrictive for some applications and may introduce a\nbias during model training. To address these limitations and enhance domain\nadaptability, this work first introduces GoPro Random Sharp (GoProRS), a new\ndataset where the the frequency of sharp frames within the sequence is\ncustomizable, allowing more diverse training and testing scenarios.\nFurthermore, it presents a novel video deblurring model, called SPEINet, that\nintegrates sharp frame features into blurry frame reconstruction through an\nattention-based encoder-decoder architecture, a lightweight yet robust sharp\nframe detection and an edge extraction phase. Extensive experimental results\ndemonstrate that SPEINet outperforms state-of-the-art methods across multiple\ndatasets, achieving an average of +3.2% PSNR improvement over recent\ntechniques. Given such promising results, we believe that both the proposed\nmodel and dataset pave the way for future advancements in video deblurring\nbased on the detection of sharp frames.\n", "link": "http://arxiv.org/abs/2501.12246v1", "date": "2025-01-21", "relevancy": 2.1515, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5632}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5272}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Deblurring%20by%20Sharpness%20Prior%20Detection%20and%20Edge%20Information&body=Title%3A%20Video%20Deblurring%20by%20Sharpness%20Prior%20Detection%20and%20Edge%20Information%0AAuthor%3A%20Yang%20Tian%20and%20Fabio%20Brau%20and%20Giulio%20Rossolini%20and%20Giorgio%20Buttazzo%20and%20Hao%20Meng%0AAbstract%3A%20%20%20Video%20deblurring%20is%20essential%20task%20for%20autonomous%20driving%2C%20facial%0Arecognition%2C%20and%20security%20surveillance.%20Traditional%20methods%20directly%20estimate%0Amotion%20blur%20kernels%2C%20often%20introducing%20artifacts%20and%20leading%20to%20poor%20results.%0ARecent%20approaches%20utilize%20the%20detection%20of%20sharp%20frames%20within%20video%20sequences%0Ato%20enhance%20deblurring.%20However%2C%20existing%20datasets%20rely%20on%20fixed%20number%20of%20sharp%0Aframes%2C%20which%20may%20be%20too%20restrictive%20for%20some%20applications%20and%20may%20introduce%20a%0Abias%20during%20model%20training.%20To%20address%20these%20limitations%20and%20enhance%20domain%0Aadaptability%2C%20this%20work%20first%20introduces%20GoPro%20Random%20Sharp%20%28GoProRS%29%2C%20a%20new%0Adataset%20where%20the%20the%20frequency%20of%20sharp%20frames%20within%20the%20sequence%20is%0Acustomizable%2C%20allowing%20more%20diverse%20training%20and%20testing%20scenarios.%0AFurthermore%2C%20it%20presents%20a%20novel%20video%20deblurring%20model%2C%20called%20SPEINet%2C%20that%0Aintegrates%20sharp%20frame%20features%20into%20blurry%20frame%20reconstruction%20through%20an%0Aattention-based%20encoder-decoder%20architecture%2C%20a%20lightweight%20yet%20robust%20sharp%0Aframe%20detection%20and%20an%20edge%20extraction%20phase.%20Extensive%20experimental%20results%0Ademonstrate%20that%20SPEINet%20outperforms%20state-of-the-art%20methods%20across%20multiple%0Adatasets%2C%20achieving%20an%20average%20of%20%2B3.2%25%20PSNR%20improvement%20over%20recent%0Atechniques.%20Given%20such%20promising%20results%2C%20we%20believe%20that%20both%20the%20proposed%0Amodel%20and%20dataset%20pave%20the%20way%20for%20future%20advancements%20in%20video%20deblurring%0Abased%20on%20the%20detection%20of%20sharp%20frames.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Deblurring%2520by%2520Sharpness%2520Prior%2520Detection%2520and%2520Edge%2520Information%26entry.906535625%3DYang%2520Tian%2520and%2520Fabio%2520Brau%2520and%2520Giulio%2520Rossolini%2520and%2520Giorgio%2520Buttazzo%2520and%2520Hao%2520Meng%26entry.1292438233%3D%2520%2520Video%2520deblurring%2520is%2520essential%2520task%2520for%2520autonomous%2520driving%252C%2520facial%250Arecognition%252C%2520and%2520security%2520surveillance.%2520Traditional%2520methods%2520directly%2520estimate%250Amotion%2520blur%2520kernels%252C%2520often%2520introducing%2520artifacts%2520and%2520leading%2520to%2520poor%2520results.%250ARecent%2520approaches%2520utilize%2520the%2520detection%2520of%2520sharp%2520frames%2520within%2520video%2520sequences%250Ato%2520enhance%2520deblurring.%2520However%252C%2520existing%2520datasets%2520rely%2520on%2520fixed%2520number%2520of%2520sharp%250Aframes%252C%2520which%2520may%2520be%2520too%2520restrictive%2520for%2520some%2520applications%2520and%2520may%2520introduce%2520a%250Abias%2520during%2520model%2520training.%2520To%2520address%2520these%2520limitations%2520and%2520enhance%2520domain%250Aadaptability%252C%2520this%2520work%2520first%2520introduces%2520GoPro%2520Random%2520Sharp%2520%2528GoProRS%2529%252C%2520a%2520new%250Adataset%2520where%2520the%2520the%2520frequency%2520of%2520sharp%2520frames%2520within%2520the%2520sequence%2520is%250Acustomizable%252C%2520allowing%2520more%2520diverse%2520training%2520and%2520testing%2520scenarios.%250AFurthermore%252C%2520it%2520presents%2520a%2520novel%2520video%2520deblurring%2520model%252C%2520called%2520SPEINet%252C%2520that%250Aintegrates%2520sharp%2520frame%2520features%2520into%2520blurry%2520frame%2520reconstruction%2520through%2520an%250Aattention-based%2520encoder-decoder%2520architecture%252C%2520a%2520lightweight%2520yet%2520robust%2520sharp%250Aframe%2520detection%2520and%2520an%2520edge%2520extraction%2520phase.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520SPEINet%2520outperforms%2520state-of-the-art%2520methods%2520across%2520multiple%250Adatasets%252C%2520achieving%2520an%2520average%2520of%2520%252B3.2%2525%2520PSNR%2520improvement%2520over%2520recent%250Atechniques.%2520Given%2520such%2520promising%2520results%252C%2520we%2520believe%2520that%2520both%2520the%2520proposed%250Amodel%2520and%2520dataset%2520pave%2520the%2520way%2520for%2520future%2520advancements%2520in%2520video%2520deblurring%250Abased%2520on%2520the%2520detection%2520of%2520sharp%2520frames.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Deblurring%20by%20Sharpness%20Prior%20Detection%20and%20Edge%20Information&entry.906535625=Yang%20Tian%20and%20Fabio%20Brau%20and%20Giulio%20Rossolini%20and%20Giorgio%20Buttazzo%20and%20Hao%20Meng&entry.1292438233=%20%20Video%20deblurring%20is%20essential%20task%20for%20autonomous%20driving%2C%20facial%0Arecognition%2C%20and%20security%20surveillance.%20Traditional%20methods%20directly%20estimate%0Amotion%20blur%20kernels%2C%20often%20introducing%20artifacts%20and%20leading%20to%20poor%20results.%0ARecent%20approaches%20utilize%20the%20detection%20of%20sharp%20frames%20within%20video%20sequences%0Ato%20enhance%20deblurring.%20However%2C%20existing%20datasets%20rely%20on%20fixed%20number%20of%20sharp%0Aframes%2C%20which%20may%20be%20too%20restrictive%20for%20some%20applications%20and%20may%20introduce%20a%0Abias%20during%20model%20training.%20To%20address%20these%20limitations%20and%20enhance%20domain%0Aadaptability%2C%20this%20work%20first%20introduces%20GoPro%20Random%20Sharp%20%28GoProRS%29%2C%20a%20new%0Adataset%20where%20the%20the%20frequency%20of%20sharp%20frames%20within%20the%20sequence%20is%0Acustomizable%2C%20allowing%20more%20diverse%20training%20and%20testing%20scenarios.%0AFurthermore%2C%20it%20presents%20a%20novel%20video%20deblurring%20model%2C%20called%20SPEINet%2C%20that%0Aintegrates%20sharp%20frame%20features%20into%20blurry%20frame%20reconstruction%20through%20an%0Aattention-based%20encoder-decoder%20architecture%2C%20a%20lightweight%20yet%20robust%20sharp%0Aframe%20detection%20and%20an%20edge%20extraction%20phase.%20Extensive%20experimental%20results%0Ademonstrate%20that%20SPEINet%20outperforms%20state-of-the-art%20methods%20across%20multiple%0Adatasets%2C%20achieving%20an%20average%20of%20%2B3.2%25%20PSNR%20improvement%20over%20recent%0Atechniques.%20Given%20such%20promising%20results%2C%20we%20believe%20that%20both%20the%20proposed%0Amodel%20and%20dataset%20pave%20the%20way%20for%20future%20advancements%20in%20video%20deblurring%0Abased%20on%20the%20detection%20of%20sharp%20frames.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12246v1&entry.124074799=Read"},
{"title": "Automatic Labelling with Open-source LLMs using Dynamic Label Schema\n  Integration", "author": "Thomas Walshe and Sae Young Moon and Chunyang Xiao and Yawwani Gunawardana and Fran Silavong", "abstract": "  Acquiring labelled training data remains a costly task in real world machine\nlearning projects to meet quantity and quality requirements. Recently Large\nLanguage Models (LLMs), notably GPT-4, have shown great promises in labelling\ndata with high accuracy. However, privacy and cost concerns prevent the\nubiquitous use of GPT-4. In this work, we explore effectively leveraging\nopen-source models for automatic labelling. We identify integrating label\nschema as a promising technology but found that naively using the label\ndescription for classification leads to poor performance on high cardinality\ntasks. To address this, we propose Retrieval Augmented Classification (RAC) for\nwhich LLM performs inferences for one label at a time using corresponding label\nschema; we start with the most related label and iterates until a label is\nchosen by the LLM. We show that our method, which dynamically integrates label\ndescription, leads to performance improvements in labelling tasks. We further\nshow that by focusing only on the most promising labels, RAC can trade off\nbetween label quality and coverage - a property we leverage to automatically\nlabel our internal datasets.\n", "link": "http://arxiv.org/abs/2501.12332v1", "date": "2025-01-21", "relevancy": 2.1468, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5514}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Labelling%20with%20Open-source%20LLMs%20using%20Dynamic%20Label%20Schema%0A%20%20Integration&body=Title%3A%20Automatic%20Labelling%20with%20Open-source%20LLMs%20using%20Dynamic%20Label%20Schema%0A%20%20Integration%0AAuthor%3A%20Thomas%20Walshe%20and%20Sae%20Young%20Moon%20and%20Chunyang%20Xiao%20and%20Yawwani%20Gunawardana%20and%20Fran%20Silavong%0AAbstract%3A%20%20%20Acquiring%20labelled%20training%20data%20remains%20a%20costly%20task%20in%20real%20world%20machine%0Alearning%20projects%20to%20meet%20quantity%20and%20quality%20requirements.%20Recently%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20notably%20GPT-4%2C%20have%20shown%20great%20promises%20in%20labelling%0Adata%20with%20high%20accuracy.%20However%2C%20privacy%20and%20cost%20concerns%20prevent%20the%0Aubiquitous%20use%20of%20GPT-4.%20In%20this%20work%2C%20we%20explore%20effectively%20leveraging%0Aopen-source%20models%20for%20automatic%20labelling.%20We%20identify%20integrating%20label%0Aschema%20as%20a%20promising%20technology%20but%20found%20that%20naively%20using%20the%20label%0Adescription%20for%20classification%20leads%20to%20poor%20performance%20on%20high%20cardinality%0Atasks.%20To%20address%20this%2C%20we%20propose%20Retrieval%20Augmented%20Classification%20%28RAC%29%20for%0Awhich%20LLM%20performs%20inferences%20for%20one%20label%20at%20a%20time%20using%20corresponding%20label%0Aschema%3B%20we%20start%20with%20the%20most%20related%20label%20and%20iterates%20until%20a%20label%20is%0Achosen%20by%20the%20LLM.%20We%20show%20that%20our%20method%2C%20which%20dynamically%20integrates%20label%0Adescription%2C%20leads%20to%20performance%20improvements%20in%20labelling%20tasks.%20We%20further%0Ashow%20that%20by%20focusing%20only%20on%20the%20most%20promising%20labels%2C%20RAC%20can%20trade%20off%0Abetween%20label%20quality%20and%20coverage%20-%20a%20property%20we%20leverage%20to%20automatically%0Alabel%20our%20internal%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Labelling%2520with%2520Open-source%2520LLMs%2520using%2520Dynamic%2520Label%2520Schema%250A%2520%2520Integration%26entry.906535625%3DThomas%2520Walshe%2520and%2520Sae%2520Young%2520Moon%2520and%2520Chunyang%2520Xiao%2520and%2520Yawwani%2520Gunawardana%2520and%2520Fran%2520Silavong%26entry.1292438233%3D%2520%2520Acquiring%2520labelled%2520training%2520data%2520remains%2520a%2520costly%2520task%2520in%2520real%2520world%2520machine%250Alearning%2520projects%2520to%2520meet%2520quantity%2520and%2520quality%2520requirements.%2520Recently%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520notably%2520GPT-4%252C%2520have%2520shown%2520great%2520promises%2520in%2520labelling%250Adata%2520with%2520high%2520accuracy.%2520However%252C%2520privacy%2520and%2520cost%2520concerns%2520prevent%2520the%250Aubiquitous%2520use%2520of%2520GPT-4.%2520In%2520this%2520work%252C%2520we%2520explore%2520effectively%2520leveraging%250Aopen-source%2520models%2520for%2520automatic%2520labelling.%2520We%2520identify%2520integrating%2520label%250Aschema%2520as%2520a%2520promising%2520technology%2520but%2520found%2520that%2520naively%2520using%2520the%2520label%250Adescription%2520for%2520classification%2520leads%2520to%2520poor%2520performance%2520on%2520high%2520cardinality%250Atasks.%2520To%2520address%2520this%252C%2520we%2520propose%2520Retrieval%2520Augmented%2520Classification%2520%2528RAC%2529%2520for%250Awhich%2520LLM%2520performs%2520inferences%2520for%2520one%2520label%2520at%2520a%2520time%2520using%2520corresponding%2520label%250Aschema%253B%2520we%2520start%2520with%2520the%2520most%2520related%2520label%2520and%2520iterates%2520until%2520a%2520label%2520is%250Achosen%2520by%2520the%2520LLM.%2520We%2520show%2520that%2520our%2520method%252C%2520which%2520dynamically%2520integrates%2520label%250Adescription%252C%2520leads%2520to%2520performance%2520improvements%2520in%2520labelling%2520tasks.%2520We%2520further%250Ashow%2520that%2520by%2520focusing%2520only%2520on%2520the%2520most%2520promising%2520labels%252C%2520RAC%2520can%2520trade%2520off%250Abetween%2520label%2520quality%2520and%2520coverage%2520-%2520a%2520property%2520we%2520leverage%2520to%2520automatically%250Alabel%2520our%2520internal%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Labelling%20with%20Open-source%20LLMs%20using%20Dynamic%20Label%20Schema%0A%20%20Integration&entry.906535625=Thomas%20Walshe%20and%20Sae%20Young%20Moon%20and%20Chunyang%20Xiao%20and%20Yawwani%20Gunawardana%20and%20Fran%20Silavong&entry.1292438233=%20%20Acquiring%20labelled%20training%20data%20remains%20a%20costly%20task%20in%20real%20world%20machine%0Alearning%20projects%20to%20meet%20quantity%20and%20quality%20requirements.%20Recently%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20notably%20GPT-4%2C%20have%20shown%20great%20promises%20in%20labelling%0Adata%20with%20high%20accuracy.%20However%2C%20privacy%20and%20cost%20concerns%20prevent%20the%0Aubiquitous%20use%20of%20GPT-4.%20In%20this%20work%2C%20we%20explore%20effectively%20leveraging%0Aopen-source%20models%20for%20automatic%20labelling.%20We%20identify%20integrating%20label%0Aschema%20as%20a%20promising%20technology%20but%20found%20that%20naively%20using%20the%20label%0Adescription%20for%20classification%20leads%20to%20poor%20performance%20on%20high%20cardinality%0Atasks.%20To%20address%20this%2C%20we%20propose%20Retrieval%20Augmented%20Classification%20%28RAC%29%20for%0Awhich%20LLM%20performs%20inferences%20for%20one%20label%20at%20a%20time%20using%20corresponding%20label%0Aschema%3B%20we%20start%20with%20the%20most%20related%20label%20and%20iterates%20until%20a%20label%20is%0Achosen%20by%20the%20LLM.%20We%20show%20that%20our%20method%2C%20which%20dynamically%20integrates%20label%0Adescription%2C%20leads%20to%20performance%20improvements%20in%20labelling%20tasks.%20We%20further%0Ashow%20that%20by%20focusing%20only%20on%20the%20most%20promising%20labels%2C%20RAC%20can%20trade%20off%0Abetween%20label%20quality%20and%20coverage%20-%20a%20property%20we%20leverage%20to%20automatically%0Alabel%20our%20internal%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12332v1&entry.124074799=Read"},
{"title": "FViT: A Focal Vision Transformer with Gabor Filter", "author": "Yulong Shi and Mingwei Sun and Yongshuai Wang and Zengqiang Chen", "abstract": "  Vision transformers have achieved encouraging progress in various computer\nvision tasks. A common belief is that this is attributed to the capability of\nself-attention in modeling the global dependencies among feature tokens.\nHowever, self-attention still faces several challenges in dense prediction\ntasks, including high computational complexity and absence of desirable\ninductive bias. To alleviate these issues, the potential advantages of\ncombining vision transformers with Gabor filters are revisited, and a learnable\nGabor filter (LGF) using convolution is proposed. The LGF does not rely on\nself-attention, and it is used to simulate the response of fundamental cells in\nthe biological visual system to the input images. This encourages vision\ntransformers to focus on discriminative feature representations of targets\nacross different scales and orientations. In addition, a Bionic Focal Vision\n(BFV) block is designed based on the LGF. This block draws inspiration from\nneuroscience and introduces a Dual-Path Feed Forward Network (DPFFN) to emulate\nthe parallel and cascaded information processing scheme of the biological\nvisual cortex. Furthermore, a unified and efficient family of pyramid backbone\nnetworks called Focal Vision Transformers (FViTs) is developed by stacking BFV\nblocks. Experimental results indicate that FViTs demonstrate superior\nperformance in various vision tasks. In terms of computational efficiency and\nscalability, FViTs show significant advantages compared with other\ncounterparts.\n", "link": "http://arxiv.org/abs/2402.11303v3", "date": "2025-01-21", "relevancy": 2.1377, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5689}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5314}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FViT%3A%20A%20Focal%20Vision%20Transformer%20with%20Gabor%20Filter&body=Title%3A%20FViT%3A%20A%20Focal%20Vision%20Transformer%20with%20Gabor%20Filter%0AAuthor%3A%20Yulong%20Shi%20and%20Mingwei%20Sun%20and%20Yongshuai%20Wang%20and%20Zengqiang%20Chen%0AAbstract%3A%20%20%20Vision%20transformers%20have%20achieved%20encouraging%20progress%20in%20various%20computer%0Avision%20tasks.%20A%20common%20belief%20is%20that%20this%20is%20attributed%20to%20the%20capability%20of%0Aself-attention%20in%20modeling%20the%20global%20dependencies%20among%20feature%20tokens.%0AHowever%2C%20self-attention%20still%20faces%20several%20challenges%20in%20dense%20prediction%0Atasks%2C%20including%20high%20computational%20complexity%20and%20absence%20of%20desirable%0Ainductive%20bias.%20To%20alleviate%20these%20issues%2C%20the%20potential%20advantages%20of%0Acombining%20vision%20transformers%20with%20Gabor%20filters%20are%20revisited%2C%20and%20a%20learnable%0AGabor%20filter%20%28LGF%29%20using%20convolution%20is%20proposed.%20The%20LGF%20does%20not%20rely%20on%0Aself-attention%2C%20and%20it%20is%20used%20to%20simulate%20the%20response%20of%20fundamental%20cells%20in%0Athe%20biological%20visual%20system%20to%20the%20input%20images.%20This%20encourages%20vision%0Atransformers%20to%20focus%20on%20discriminative%20feature%20representations%20of%20targets%0Aacross%20different%20scales%20and%20orientations.%20In%20addition%2C%20a%20Bionic%20Focal%20Vision%0A%28BFV%29%20block%20is%20designed%20based%20on%20the%20LGF.%20This%20block%20draws%20inspiration%20from%0Aneuroscience%20and%20introduces%20a%20Dual-Path%20Feed%20Forward%20Network%20%28DPFFN%29%20to%20emulate%0Athe%20parallel%20and%20cascaded%20information%20processing%20scheme%20of%20the%20biological%0Avisual%20cortex.%20Furthermore%2C%20a%20unified%20and%20efficient%20family%20of%20pyramid%20backbone%0Anetworks%20called%20Focal%20Vision%20Transformers%20%28FViTs%29%20is%20developed%20by%20stacking%20BFV%0Ablocks.%20Experimental%20results%20indicate%20that%20FViTs%20demonstrate%20superior%0Aperformance%20in%20various%20vision%20tasks.%20In%20terms%20of%20computational%20efficiency%20and%0Ascalability%2C%20FViTs%20show%20significant%20advantages%20compared%20with%20other%0Acounterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11303v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFViT%253A%2520A%2520Focal%2520Vision%2520Transformer%2520with%2520Gabor%2520Filter%26entry.906535625%3DYulong%2520Shi%2520and%2520Mingwei%2520Sun%2520and%2520Yongshuai%2520Wang%2520and%2520Zengqiang%2520Chen%26entry.1292438233%3D%2520%2520Vision%2520transformers%2520have%2520achieved%2520encouraging%2520progress%2520in%2520various%2520computer%250Avision%2520tasks.%2520A%2520common%2520belief%2520is%2520that%2520this%2520is%2520attributed%2520to%2520the%2520capability%2520of%250Aself-attention%2520in%2520modeling%2520the%2520global%2520dependencies%2520among%2520feature%2520tokens.%250AHowever%252C%2520self-attention%2520still%2520faces%2520several%2520challenges%2520in%2520dense%2520prediction%250Atasks%252C%2520including%2520high%2520computational%2520complexity%2520and%2520absence%2520of%2520desirable%250Ainductive%2520bias.%2520To%2520alleviate%2520these%2520issues%252C%2520the%2520potential%2520advantages%2520of%250Acombining%2520vision%2520transformers%2520with%2520Gabor%2520filters%2520are%2520revisited%252C%2520and%2520a%2520learnable%250AGabor%2520filter%2520%2528LGF%2529%2520using%2520convolution%2520is%2520proposed.%2520The%2520LGF%2520does%2520not%2520rely%2520on%250Aself-attention%252C%2520and%2520it%2520is%2520used%2520to%2520simulate%2520the%2520response%2520of%2520fundamental%2520cells%2520in%250Athe%2520biological%2520visual%2520system%2520to%2520the%2520input%2520images.%2520This%2520encourages%2520vision%250Atransformers%2520to%2520focus%2520on%2520discriminative%2520feature%2520representations%2520of%2520targets%250Aacross%2520different%2520scales%2520and%2520orientations.%2520In%2520addition%252C%2520a%2520Bionic%2520Focal%2520Vision%250A%2528BFV%2529%2520block%2520is%2520designed%2520based%2520on%2520the%2520LGF.%2520This%2520block%2520draws%2520inspiration%2520from%250Aneuroscience%2520and%2520introduces%2520a%2520Dual-Path%2520Feed%2520Forward%2520Network%2520%2528DPFFN%2529%2520to%2520emulate%250Athe%2520parallel%2520and%2520cascaded%2520information%2520processing%2520scheme%2520of%2520the%2520biological%250Avisual%2520cortex.%2520Furthermore%252C%2520a%2520unified%2520and%2520efficient%2520family%2520of%2520pyramid%2520backbone%250Anetworks%2520called%2520Focal%2520Vision%2520Transformers%2520%2528FViTs%2529%2520is%2520developed%2520by%2520stacking%2520BFV%250Ablocks.%2520Experimental%2520results%2520indicate%2520that%2520FViTs%2520demonstrate%2520superior%250Aperformance%2520in%2520various%2520vision%2520tasks.%2520In%2520terms%2520of%2520computational%2520efficiency%2520and%250Ascalability%252C%2520FViTs%2520show%2520significant%2520advantages%2520compared%2520with%2520other%250Acounterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11303v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FViT%3A%20A%20Focal%20Vision%20Transformer%20with%20Gabor%20Filter&entry.906535625=Yulong%20Shi%20and%20Mingwei%20Sun%20and%20Yongshuai%20Wang%20and%20Zengqiang%20Chen&entry.1292438233=%20%20Vision%20transformers%20have%20achieved%20encouraging%20progress%20in%20various%20computer%0Avision%20tasks.%20A%20common%20belief%20is%20that%20this%20is%20attributed%20to%20the%20capability%20of%0Aself-attention%20in%20modeling%20the%20global%20dependencies%20among%20feature%20tokens.%0AHowever%2C%20self-attention%20still%20faces%20several%20challenges%20in%20dense%20prediction%0Atasks%2C%20including%20high%20computational%20complexity%20and%20absence%20of%20desirable%0Ainductive%20bias.%20To%20alleviate%20these%20issues%2C%20the%20potential%20advantages%20of%0Acombining%20vision%20transformers%20with%20Gabor%20filters%20are%20revisited%2C%20and%20a%20learnable%0AGabor%20filter%20%28LGF%29%20using%20convolution%20is%20proposed.%20The%20LGF%20does%20not%20rely%20on%0Aself-attention%2C%20and%20it%20is%20used%20to%20simulate%20the%20response%20of%20fundamental%20cells%20in%0Athe%20biological%20visual%20system%20to%20the%20input%20images.%20This%20encourages%20vision%0Atransformers%20to%20focus%20on%20discriminative%20feature%20representations%20of%20targets%0Aacross%20different%20scales%20and%20orientations.%20In%20addition%2C%20a%20Bionic%20Focal%20Vision%0A%28BFV%29%20block%20is%20designed%20based%20on%20the%20LGF.%20This%20block%20draws%20inspiration%20from%0Aneuroscience%20and%20introduces%20a%20Dual-Path%20Feed%20Forward%20Network%20%28DPFFN%29%20to%20emulate%0Athe%20parallel%20and%20cascaded%20information%20processing%20scheme%20of%20the%20biological%0Avisual%20cortex.%20Furthermore%2C%20a%20unified%20and%20efficient%20family%20of%20pyramid%20backbone%0Anetworks%20called%20Focal%20Vision%20Transformers%20%28FViTs%29%20is%20developed%20by%20stacking%20BFV%0Ablocks.%20Experimental%20results%20indicate%20that%20FViTs%20demonstrate%20superior%0Aperformance%20in%20various%20vision%20tasks.%20In%20terms%20of%20computational%20efficiency%20and%0Ascalability%2C%20FViTs%20show%20significant%20advantages%20compared%20with%20other%0Acounterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11303v3&entry.124074799=Read"},
{"title": "Zero-shot Bias Correction: Efficient MR Image Inhomogeneity Reduction\n  Without Any Data", "author": "Hongxu Yang and Edina Timko and Brice Fernandez", "abstract": "  In recent years, deep neural networks for image inhomogeneity reduction have\nshown promising results. However, current methods with (un)supervised solutions\nrequire preparing a training dataset, which is expensive and laborious for data\ncollection. In this work, we demonstrate a novel zero-shot deep neural\nnetworks, which requires no data for pre-training and dedicated assumption of\nthe bias field. The designed light-weight CNN enables an efficient zero-shot\nadaptation for bias-corrupted image correction. Our method provides a novel\nsolution to mitigate the biased corrupted image as iterative homogeneity\nrefinement, which therefore ensures the considered issue can be solved easier\nwith stable convergence of zero-shot optimization. Extensive comparison on\ndifferent datasets show that the proposed method performs better than current\ndata-free N4 methods in both efficiency and accuracy.\n", "link": "http://arxiv.org/abs/2501.12244v1", "date": "2025-01-21", "relevancy": 2.1267, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5496}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5192}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Bias%20Correction%3A%20Efficient%20MR%20Image%20Inhomogeneity%20Reduction%0A%20%20Without%20Any%20Data&body=Title%3A%20Zero-shot%20Bias%20Correction%3A%20Efficient%20MR%20Image%20Inhomogeneity%20Reduction%0A%20%20Without%20Any%20Data%0AAuthor%3A%20Hongxu%20Yang%20and%20Edina%20Timko%20and%20Brice%20Fernandez%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20neural%20networks%20for%20image%20inhomogeneity%20reduction%20have%0Ashown%20promising%20results.%20However%2C%20current%20methods%20with%20%28un%29supervised%20solutions%0Arequire%20preparing%20a%20training%20dataset%2C%20which%20is%20expensive%20and%20laborious%20for%20data%0Acollection.%20In%20this%20work%2C%20we%20demonstrate%20a%20novel%20zero-shot%20deep%20neural%0Anetworks%2C%20which%20requires%20no%20data%20for%20pre-training%20and%20dedicated%20assumption%20of%0Athe%20bias%20field.%20The%20designed%20light-weight%20CNN%20enables%20an%20efficient%20zero-shot%0Aadaptation%20for%20bias-corrupted%20image%20correction.%20Our%20method%20provides%20a%20novel%0Asolution%20to%20mitigate%20the%20biased%20corrupted%20image%20as%20iterative%20homogeneity%0Arefinement%2C%20which%20therefore%20ensures%20the%20considered%20issue%20can%20be%20solved%20easier%0Awith%20stable%20convergence%20of%20zero-shot%20optimization.%20Extensive%20comparison%20on%0Adifferent%20datasets%20show%20that%20the%20proposed%20method%20performs%20better%20than%20current%0Adata-free%20N4%20methods%20in%20both%20efficiency%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Bias%2520Correction%253A%2520Efficient%2520MR%2520Image%2520Inhomogeneity%2520Reduction%250A%2520%2520Without%2520Any%2520Data%26entry.906535625%3DHongxu%2520Yang%2520and%2520Edina%2520Timko%2520and%2520Brice%2520Fernandez%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520deep%2520neural%2520networks%2520for%2520image%2520inhomogeneity%2520reduction%2520have%250Ashown%2520promising%2520results.%2520However%252C%2520current%2520methods%2520with%2520%2528un%2529supervised%2520solutions%250Arequire%2520preparing%2520a%2520training%2520dataset%252C%2520which%2520is%2520expensive%2520and%2520laborious%2520for%2520data%250Acollection.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520a%2520novel%2520zero-shot%2520deep%2520neural%250Anetworks%252C%2520which%2520requires%2520no%2520data%2520for%2520pre-training%2520and%2520dedicated%2520assumption%2520of%250Athe%2520bias%2520field.%2520The%2520designed%2520light-weight%2520CNN%2520enables%2520an%2520efficient%2520zero-shot%250Aadaptation%2520for%2520bias-corrupted%2520image%2520correction.%2520Our%2520method%2520provides%2520a%2520novel%250Asolution%2520to%2520mitigate%2520the%2520biased%2520corrupted%2520image%2520as%2520iterative%2520homogeneity%250Arefinement%252C%2520which%2520therefore%2520ensures%2520the%2520considered%2520issue%2520can%2520be%2520solved%2520easier%250Awith%2520stable%2520convergence%2520of%2520zero-shot%2520optimization.%2520Extensive%2520comparison%2520on%250Adifferent%2520datasets%2520show%2520that%2520the%2520proposed%2520method%2520performs%2520better%2520than%2520current%250Adata-free%2520N4%2520methods%2520in%2520both%2520efficiency%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Bias%20Correction%3A%20Efficient%20MR%20Image%20Inhomogeneity%20Reduction%0A%20%20Without%20Any%20Data&entry.906535625=Hongxu%20Yang%20and%20Edina%20Timko%20and%20Brice%20Fernandez&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20neural%20networks%20for%20image%20inhomogeneity%20reduction%20have%0Ashown%20promising%20results.%20However%2C%20current%20methods%20with%20%28un%29supervised%20solutions%0Arequire%20preparing%20a%20training%20dataset%2C%20which%20is%20expensive%20and%20laborious%20for%20data%0Acollection.%20In%20this%20work%2C%20we%20demonstrate%20a%20novel%20zero-shot%20deep%20neural%0Anetworks%2C%20which%20requires%20no%20data%20for%20pre-training%20and%20dedicated%20assumption%20of%0Athe%20bias%20field.%20The%20designed%20light-weight%20CNN%20enables%20an%20efficient%20zero-shot%0Aadaptation%20for%20bias-corrupted%20image%20correction.%20Our%20method%20provides%20a%20novel%0Asolution%20to%20mitigate%20the%20biased%20corrupted%20image%20as%20iterative%20homogeneity%0Arefinement%2C%20which%20therefore%20ensures%20the%20considered%20issue%20can%20be%20solved%20easier%0Awith%20stable%20convergence%20of%20zero-shot%20optimization.%20Extensive%20comparison%20on%0Adifferent%20datasets%20show%20that%20the%20proposed%20method%20performs%20better%20than%20current%0Adata-free%20N4%20methods%20in%20both%20efficiency%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12244v1&entry.124074799=Read"},
{"title": "Benchmarking Image Perturbations for Testing Automated Driving\n  Assistance Systems", "author": "Stefano Carlo Lambertenghi and Hannes Leonhard and Andrea Stocco", "abstract": "  Advanced Driver Assistance Systems (ADAS) based on deep neural networks\n(DNNs) are widely used in autonomous vehicles for critical perception tasks\nsuch as object detection, semantic segmentation, and lane recognition. However,\nthese systems are highly sensitive to input variations, such as noise and\nchanges in lighting, which can compromise their effectiveness and potentially\nlead to safety-critical failures.\n  This study offers a comprehensive empirical evaluation of image\nperturbations, techniques commonly used to assess the robustness of DNNs, to\nvalidate and improve the robustness and generalization of ADAS perception\nsystems. We first conducted a systematic review of the literature, identifying\n38 categories of perturbations. Next, we evaluated their effectiveness in\nrevealing failures in two different ADAS, both at the component and at the\nsystem level. Finally, we explored the use of perturbation-based data\naugmentation and continuous learning strategies to improve ADAS adaptation to\nnew operational design domains. Our results demonstrate that all categories of\nimage perturbations successfully expose robustness issues in ADAS and that the\nuse of dataset augmentation and continuous learning significantly improves ADAS\nperformance in novel, unseen environments.\n", "link": "http://arxiv.org/abs/2501.12269v1", "date": "2025-01-21", "relevancy": 2.1176, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5554}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5148}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Image%20Perturbations%20for%20Testing%20Automated%20Driving%0A%20%20Assistance%20Systems&body=Title%3A%20Benchmarking%20Image%20Perturbations%20for%20Testing%20Automated%20Driving%0A%20%20Assistance%20Systems%0AAuthor%3A%20Stefano%20Carlo%20Lambertenghi%20and%20Hannes%20Leonhard%20and%20Andrea%20Stocco%0AAbstract%3A%20%20%20Advanced%20Driver%20Assistance%20Systems%20%28ADAS%29%20based%20on%20deep%20neural%20networks%0A%28DNNs%29%20are%20widely%20used%20in%20autonomous%20vehicles%20for%20critical%20perception%20tasks%0Asuch%20as%20object%20detection%2C%20semantic%20segmentation%2C%20and%20lane%20recognition.%20However%2C%0Athese%20systems%20are%20highly%20sensitive%20to%20input%20variations%2C%20such%20as%20noise%20and%0Achanges%20in%20lighting%2C%20which%20can%20compromise%20their%20effectiveness%20and%20potentially%0Alead%20to%20safety-critical%20failures.%0A%20%20This%20study%20offers%20a%20comprehensive%20empirical%20evaluation%20of%20image%0Aperturbations%2C%20techniques%20commonly%20used%20to%20assess%20the%20robustness%20of%20DNNs%2C%20to%0Avalidate%20and%20improve%20the%20robustness%20and%20generalization%20of%20ADAS%20perception%0Asystems.%20We%20first%20conducted%20a%20systematic%20review%20of%20the%20literature%2C%20identifying%0A38%20categories%20of%20perturbations.%20Next%2C%20we%20evaluated%20their%20effectiveness%20in%0Arevealing%20failures%20in%20two%20different%20ADAS%2C%20both%20at%20the%20component%20and%20at%20the%0Asystem%20level.%20Finally%2C%20we%20explored%20the%20use%20of%20perturbation-based%20data%0Aaugmentation%20and%20continuous%20learning%20strategies%20to%20improve%20ADAS%20adaptation%20to%0Anew%20operational%20design%20domains.%20Our%20results%20demonstrate%20that%20all%20categories%20of%0Aimage%20perturbations%20successfully%20expose%20robustness%20issues%20in%20ADAS%20and%20that%20the%0Ause%20of%20dataset%20augmentation%20and%20continuous%20learning%20significantly%20improves%20ADAS%0Aperformance%20in%20novel%2C%20unseen%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Image%2520Perturbations%2520for%2520Testing%2520Automated%2520Driving%250A%2520%2520Assistance%2520Systems%26entry.906535625%3DStefano%2520Carlo%2520Lambertenghi%2520and%2520Hannes%2520Leonhard%2520and%2520Andrea%2520Stocco%26entry.1292438233%3D%2520%2520Advanced%2520Driver%2520Assistance%2520Systems%2520%2528ADAS%2529%2520based%2520on%2520deep%2520neural%2520networks%250A%2528DNNs%2529%2520are%2520widely%2520used%2520in%2520autonomous%2520vehicles%2520for%2520critical%2520perception%2520tasks%250Asuch%2520as%2520object%2520detection%252C%2520semantic%2520segmentation%252C%2520and%2520lane%2520recognition.%2520However%252C%250Athese%2520systems%2520are%2520highly%2520sensitive%2520to%2520input%2520variations%252C%2520such%2520as%2520noise%2520and%250Achanges%2520in%2520lighting%252C%2520which%2520can%2520compromise%2520their%2520effectiveness%2520and%2520potentially%250Alead%2520to%2520safety-critical%2520failures.%250A%2520%2520This%2520study%2520offers%2520a%2520comprehensive%2520empirical%2520evaluation%2520of%2520image%250Aperturbations%252C%2520techniques%2520commonly%2520used%2520to%2520assess%2520the%2520robustness%2520of%2520DNNs%252C%2520to%250Avalidate%2520and%2520improve%2520the%2520robustness%2520and%2520generalization%2520of%2520ADAS%2520perception%250Asystems.%2520We%2520first%2520conducted%2520a%2520systematic%2520review%2520of%2520the%2520literature%252C%2520identifying%250A38%2520categories%2520of%2520perturbations.%2520Next%252C%2520we%2520evaluated%2520their%2520effectiveness%2520in%250Arevealing%2520failures%2520in%2520two%2520different%2520ADAS%252C%2520both%2520at%2520the%2520component%2520and%2520at%2520the%250Asystem%2520level.%2520Finally%252C%2520we%2520explored%2520the%2520use%2520of%2520perturbation-based%2520data%250Aaugmentation%2520and%2520continuous%2520learning%2520strategies%2520to%2520improve%2520ADAS%2520adaptation%2520to%250Anew%2520operational%2520design%2520domains.%2520Our%2520results%2520demonstrate%2520that%2520all%2520categories%2520of%250Aimage%2520perturbations%2520successfully%2520expose%2520robustness%2520issues%2520in%2520ADAS%2520and%2520that%2520the%250Ause%2520of%2520dataset%2520augmentation%2520and%2520continuous%2520learning%2520significantly%2520improves%2520ADAS%250Aperformance%2520in%2520novel%252C%2520unseen%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Image%20Perturbations%20for%20Testing%20Automated%20Driving%0A%20%20Assistance%20Systems&entry.906535625=Stefano%20Carlo%20Lambertenghi%20and%20Hannes%20Leonhard%20and%20Andrea%20Stocco&entry.1292438233=%20%20Advanced%20Driver%20Assistance%20Systems%20%28ADAS%29%20based%20on%20deep%20neural%20networks%0A%28DNNs%29%20are%20widely%20used%20in%20autonomous%20vehicles%20for%20critical%20perception%20tasks%0Asuch%20as%20object%20detection%2C%20semantic%20segmentation%2C%20and%20lane%20recognition.%20However%2C%0Athese%20systems%20are%20highly%20sensitive%20to%20input%20variations%2C%20such%20as%20noise%20and%0Achanges%20in%20lighting%2C%20which%20can%20compromise%20their%20effectiveness%20and%20potentially%0Alead%20to%20safety-critical%20failures.%0A%20%20This%20study%20offers%20a%20comprehensive%20empirical%20evaluation%20of%20image%0Aperturbations%2C%20techniques%20commonly%20used%20to%20assess%20the%20robustness%20of%20DNNs%2C%20to%0Avalidate%20and%20improve%20the%20robustness%20and%20generalization%20of%20ADAS%20perception%0Asystems.%20We%20first%20conducted%20a%20systematic%20review%20of%20the%20literature%2C%20identifying%0A38%20categories%20of%20perturbations.%20Next%2C%20we%20evaluated%20their%20effectiveness%20in%0Arevealing%20failures%20in%20two%20different%20ADAS%2C%20both%20at%20the%20component%20and%20at%20the%0Asystem%20level.%20Finally%2C%20we%20explored%20the%20use%20of%20perturbation-based%20data%0Aaugmentation%20and%20continuous%20learning%20strategies%20to%20improve%20ADAS%20adaptation%20to%0Anew%20operational%20design%20domains.%20Our%20results%20demonstrate%20that%20all%20categories%20of%0Aimage%20perturbations%20successfully%20expose%20robustness%20issues%20in%20ADAS%20and%20that%20the%0Ause%20of%20dataset%20augmentation%20and%20continuous%20learning%20significantly%20improves%20ADAS%0Aperformance%20in%20novel%2C%20unseen%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12269v1&entry.124074799=Read"},
{"title": "HIVEX: A High-Impact Environment Suite for Multi-Agent Research\n  (extended version)", "author": "Philipp Dominic Siedler", "abstract": "  Games have been vital test beds for the rapid development of Agent-based\nresearch. Remarkable progress has been achieved in the past, but it is unclear\nif the findings equip for real-world problems. While pressure grows, some of\nthe most critical ecological challenges can find mitigation and prevention\nsolutions through technology and its applications. Most real-world domains\ninclude multi-agent scenarios and require machine-machine and human-machine\ncollaboration. Open-source environments have not advanced and are often toy\nscenarios, too abstract or not suitable for multi-agent research. By mimicking\nreal-world problems and increasing the complexity of environments, we hope to\nadvance state-of-the-art multi-agent research and inspire researchers to work\non immediate real-world problems. Here, we present HIVEX, an environment suite\nto benchmark multi-agent research focusing on ecological challenges. HIVEX\nincludes the following environments: Wind Farm Control, Wildfire Resource\nManagement, Drone-Based Reforestation, Ocean Plastic Collection, and Aerial\nWildfire Suppression. We provide environments, training examples, and baselines\nfor the main and sub-tasks. All trained models resulting from the experiments\nof this work are hosted on Hugging Face. We also provide a leaderboard on\nHugging Face and encourage the community to submit models trained on our\nenvironment suite.\n", "link": "http://arxiv.org/abs/2501.04180v2", "date": "2025-01-21", "relevancy": 2.1039, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5638}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HIVEX%3A%20A%20High-Impact%20Environment%20Suite%20for%20Multi-Agent%20Research%0A%20%20%28extended%20version%29&body=Title%3A%20HIVEX%3A%20A%20High-Impact%20Environment%20Suite%20for%20Multi-Agent%20Research%0A%20%20%28extended%20version%29%0AAuthor%3A%20Philipp%20Dominic%20Siedler%0AAbstract%3A%20%20%20Games%20have%20been%20vital%20test%20beds%20for%20the%20rapid%20development%20of%20Agent-based%0Aresearch.%20Remarkable%20progress%20has%20been%20achieved%20in%20the%20past%2C%20but%20it%20is%20unclear%0Aif%20the%20findings%20equip%20for%20real-world%20problems.%20While%20pressure%20grows%2C%20some%20of%0Athe%20most%20critical%20ecological%20challenges%20can%20find%20mitigation%20and%20prevention%0Asolutions%20through%20technology%20and%20its%20applications.%20Most%20real-world%20domains%0Ainclude%20multi-agent%20scenarios%20and%20require%20machine-machine%20and%20human-machine%0Acollaboration.%20Open-source%20environments%20have%20not%20advanced%20and%20are%20often%20toy%0Ascenarios%2C%20too%20abstract%20or%20not%20suitable%20for%20multi-agent%20research.%20By%20mimicking%0Areal-world%20problems%20and%20increasing%20the%20complexity%20of%20environments%2C%20we%20hope%20to%0Aadvance%20state-of-the-art%20multi-agent%20research%20and%20inspire%20researchers%20to%20work%0Aon%20immediate%20real-world%20problems.%20Here%2C%20we%20present%20HIVEX%2C%20an%20environment%20suite%0Ato%20benchmark%20multi-agent%20research%20focusing%20on%20ecological%20challenges.%20HIVEX%0Aincludes%20the%20following%20environments%3A%20Wind%20Farm%20Control%2C%20Wildfire%20Resource%0AManagement%2C%20Drone-Based%20Reforestation%2C%20Ocean%20Plastic%20Collection%2C%20and%20Aerial%0AWildfire%20Suppression.%20We%20provide%20environments%2C%20training%20examples%2C%20and%20baselines%0Afor%20the%20main%20and%20sub-tasks.%20All%20trained%20models%20resulting%20from%20the%20experiments%0Aof%20this%20work%20are%20hosted%20on%20Hugging%20Face.%20We%20also%20provide%20a%20leaderboard%20on%0AHugging%20Face%20and%20encourage%20the%20community%20to%20submit%20models%20trained%20on%20our%0Aenvironment%20suite.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04180v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHIVEX%253A%2520A%2520High-Impact%2520Environment%2520Suite%2520for%2520Multi-Agent%2520Research%250A%2520%2520%2528extended%2520version%2529%26entry.906535625%3DPhilipp%2520Dominic%2520Siedler%26entry.1292438233%3D%2520%2520Games%2520have%2520been%2520vital%2520test%2520beds%2520for%2520the%2520rapid%2520development%2520of%2520Agent-based%250Aresearch.%2520Remarkable%2520progress%2520has%2520been%2520achieved%2520in%2520the%2520past%252C%2520but%2520it%2520is%2520unclear%250Aif%2520the%2520findings%2520equip%2520for%2520real-world%2520problems.%2520While%2520pressure%2520grows%252C%2520some%2520of%250Athe%2520most%2520critical%2520ecological%2520challenges%2520can%2520find%2520mitigation%2520and%2520prevention%250Asolutions%2520through%2520technology%2520and%2520its%2520applications.%2520Most%2520real-world%2520domains%250Ainclude%2520multi-agent%2520scenarios%2520and%2520require%2520machine-machine%2520and%2520human-machine%250Acollaboration.%2520Open-source%2520environments%2520have%2520not%2520advanced%2520and%2520are%2520often%2520toy%250Ascenarios%252C%2520too%2520abstract%2520or%2520not%2520suitable%2520for%2520multi-agent%2520research.%2520By%2520mimicking%250Areal-world%2520problems%2520and%2520increasing%2520the%2520complexity%2520of%2520environments%252C%2520we%2520hope%2520to%250Aadvance%2520state-of-the-art%2520multi-agent%2520research%2520and%2520inspire%2520researchers%2520to%2520work%250Aon%2520immediate%2520real-world%2520problems.%2520Here%252C%2520we%2520present%2520HIVEX%252C%2520an%2520environment%2520suite%250Ato%2520benchmark%2520multi-agent%2520research%2520focusing%2520on%2520ecological%2520challenges.%2520HIVEX%250Aincludes%2520the%2520following%2520environments%253A%2520Wind%2520Farm%2520Control%252C%2520Wildfire%2520Resource%250AManagement%252C%2520Drone-Based%2520Reforestation%252C%2520Ocean%2520Plastic%2520Collection%252C%2520and%2520Aerial%250AWildfire%2520Suppression.%2520We%2520provide%2520environments%252C%2520training%2520examples%252C%2520and%2520baselines%250Afor%2520the%2520main%2520and%2520sub-tasks.%2520All%2520trained%2520models%2520resulting%2520from%2520the%2520experiments%250Aof%2520this%2520work%2520are%2520hosted%2520on%2520Hugging%2520Face.%2520We%2520also%2520provide%2520a%2520leaderboard%2520on%250AHugging%2520Face%2520and%2520encourage%2520the%2520community%2520to%2520submit%2520models%2520trained%2520on%2520our%250Aenvironment%2520suite.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04180v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HIVEX%3A%20A%20High-Impact%20Environment%20Suite%20for%20Multi-Agent%20Research%0A%20%20%28extended%20version%29&entry.906535625=Philipp%20Dominic%20Siedler&entry.1292438233=%20%20Games%20have%20been%20vital%20test%20beds%20for%20the%20rapid%20development%20of%20Agent-based%0Aresearch.%20Remarkable%20progress%20has%20been%20achieved%20in%20the%20past%2C%20but%20it%20is%20unclear%0Aif%20the%20findings%20equip%20for%20real-world%20problems.%20While%20pressure%20grows%2C%20some%20of%0Athe%20most%20critical%20ecological%20challenges%20can%20find%20mitigation%20and%20prevention%0Asolutions%20through%20technology%20and%20its%20applications.%20Most%20real-world%20domains%0Ainclude%20multi-agent%20scenarios%20and%20require%20machine-machine%20and%20human-machine%0Acollaboration.%20Open-source%20environments%20have%20not%20advanced%20and%20are%20often%20toy%0Ascenarios%2C%20too%20abstract%20or%20not%20suitable%20for%20multi-agent%20research.%20By%20mimicking%0Areal-world%20problems%20and%20increasing%20the%20complexity%20of%20environments%2C%20we%20hope%20to%0Aadvance%20state-of-the-art%20multi-agent%20research%20and%20inspire%20researchers%20to%20work%0Aon%20immediate%20real-world%20problems.%20Here%2C%20we%20present%20HIVEX%2C%20an%20environment%20suite%0Ato%20benchmark%20multi-agent%20research%20focusing%20on%20ecological%20challenges.%20HIVEX%0Aincludes%20the%20following%20environments%3A%20Wind%20Farm%20Control%2C%20Wildfire%20Resource%0AManagement%2C%20Drone-Based%20Reforestation%2C%20Ocean%20Plastic%20Collection%2C%20and%20Aerial%0AWildfire%20Suppression.%20We%20provide%20environments%2C%20training%20examples%2C%20and%20baselines%0Afor%20the%20main%20and%20sub-tasks.%20All%20trained%20models%20resulting%20from%20the%20experiments%0Aof%20this%20work%20are%20hosted%20on%20Hugging%20Face.%20We%20also%20provide%20a%20leaderboard%20on%0AHugging%20Face%20and%20encourage%20the%20community%20to%20submit%20models%20trained%20on%20our%0Aenvironment%20suite.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04180v2&entry.124074799=Read"},
{"title": "Efficient PINNs: Multi-Head Unimodular Regularization of the Solutions\n  Space", "author": "Pedro Taranc\u00f3n-\u00c1lvarez and Pablo Tejerina-P\u00e9rez and Raul Jimenez and Pavlos Protopapas", "abstract": "  We present a machine learning framework to facilitate the solution of\nnonlinear multiscale differential equations and, especially, inverse problems\nusing Physics-Informed Neural Networks (PINNs). This framework is based on what\nis called multihead (MH) training, which involves training the network to learn\na general space of all solutions for a given set of equations with certain\nvariability, rather than learning a specific solution of the system. This setup\nis used with a second novel technique that we call Unimodular Regularization\n(UR) of the latent space of solutions. We show that the multihead approach,\ncombined with the regularization, significantly improves the efficiency of\nPINNs by facilitating the transfer learning process thereby enabling the\nfinding of solutions for nonlinear, coupled, and multiscale differential\nequations.\n", "link": "http://arxiv.org/abs/2501.12116v1", "date": "2025-01-21", "relevancy": 2.1028, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5283}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5253}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20PINNs%3A%20Multi-Head%20Unimodular%20Regularization%20of%20the%20Solutions%0A%20%20Space&body=Title%3A%20Efficient%20PINNs%3A%20Multi-Head%20Unimodular%20Regularization%20of%20the%20Solutions%0A%20%20Space%0AAuthor%3A%20Pedro%20Taranc%C3%B3n-%C3%81lvarez%20and%20Pablo%20Tejerina-P%C3%A9rez%20and%20Raul%20Jimenez%20and%20Pavlos%20Protopapas%0AAbstract%3A%20%20%20We%20present%20a%20machine%20learning%20framework%20to%20facilitate%20the%20solution%20of%0Anonlinear%20multiscale%20differential%20equations%20and%2C%20especially%2C%20inverse%20problems%0Ausing%20Physics-Informed%20Neural%20Networks%20%28PINNs%29.%20This%20framework%20is%20based%20on%20what%0Ais%20called%20multihead%20%28MH%29%20training%2C%20which%20involves%20training%20the%20network%20to%20learn%0Aa%20general%20space%20of%20all%20solutions%20for%20a%20given%20set%20of%20equations%20with%20certain%0Avariability%2C%20rather%20than%20learning%20a%20specific%20solution%20of%20the%20system.%20This%20setup%0Ais%20used%20with%20a%20second%20novel%20technique%20that%20we%20call%20Unimodular%20Regularization%0A%28UR%29%20of%20the%20latent%20space%20of%20solutions.%20We%20show%20that%20the%20multihead%20approach%2C%0Acombined%20with%20the%20regularization%2C%20significantly%20improves%20the%20efficiency%20of%0APINNs%20by%20facilitating%20the%20transfer%20learning%20process%20thereby%20enabling%20the%0Afinding%20of%20solutions%20for%20nonlinear%2C%20coupled%2C%20and%20multiscale%20differential%0Aequations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520PINNs%253A%2520Multi-Head%2520Unimodular%2520Regularization%2520of%2520the%2520Solutions%250A%2520%2520Space%26entry.906535625%3DPedro%2520Taranc%25C3%25B3n-%25C3%2581lvarez%2520and%2520Pablo%2520Tejerina-P%25C3%25A9rez%2520and%2520Raul%2520Jimenez%2520and%2520Pavlos%2520Protopapas%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520machine%2520learning%2520framework%2520to%2520facilitate%2520the%2520solution%2520of%250Anonlinear%2520multiscale%2520differential%2520equations%2520and%252C%2520especially%252C%2520inverse%2520problems%250Ausing%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529.%2520This%2520framework%2520is%2520based%2520on%2520what%250Ais%2520called%2520multihead%2520%2528MH%2529%2520training%252C%2520which%2520involves%2520training%2520the%2520network%2520to%2520learn%250Aa%2520general%2520space%2520of%2520all%2520solutions%2520for%2520a%2520given%2520set%2520of%2520equations%2520with%2520certain%250Avariability%252C%2520rather%2520than%2520learning%2520a%2520specific%2520solution%2520of%2520the%2520system.%2520This%2520setup%250Ais%2520used%2520with%2520a%2520second%2520novel%2520technique%2520that%2520we%2520call%2520Unimodular%2520Regularization%250A%2528UR%2529%2520of%2520the%2520latent%2520space%2520of%2520solutions.%2520We%2520show%2520that%2520the%2520multihead%2520approach%252C%250Acombined%2520with%2520the%2520regularization%252C%2520significantly%2520improves%2520the%2520efficiency%2520of%250APINNs%2520by%2520facilitating%2520the%2520transfer%2520learning%2520process%2520thereby%2520enabling%2520the%250Afinding%2520of%2520solutions%2520for%2520nonlinear%252C%2520coupled%252C%2520and%2520multiscale%2520differential%250Aequations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20PINNs%3A%20Multi-Head%20Unimodular%20Regularization%20of%20the%20Solutions%0A%20%20Space&entry.906535625=Pedro%20Taranc%C3%B3n-%C3%81lvarez%20and%20Pablo%20Tejerina-P%C3%A9rez%20and%20Raul%20Jimenez%20and%20Pavlos%20Protopapas&entry.1292438233=%20%20We%20present%20a%20machine%20learning%20framework%20to%20facilitate%20the%20solution%20of%0Anonlinear%20multiscale%20differential%20equations%20and%2C%20especially%2C%20inverse%20problems%0Ausing%20Physics-Informed%20Neural%20Networks%20%28PINNs%29.%20This%20framework%20is%20based%20on%20what%0Ais%20called%20multihead%20%28MH%29%20training%2C%20which%20involves%20training%20the%20network%20to%20learn%0Aa%20general%20space%20of%20all%20solutions%20for%20a%20given%20set%20of%20equations%20with%20certain%0Avariability%2C%20rather%20than%20learning%20a%20specific%20solution%20of%20the%20system.%20This%20setup%0Ais%20used%20with%20a%20second%20novel%20technique%20that%20we%20call%20Unimodular%20Regularization%0A%28UR%29%20of%20the%20latent%20space%20of%20solutions.%20We%20show%20that%20the%20multihead%20approach%2C%0Acombined%20with%20the%20regularization%2C%20significantly%20improves%20the%20efficiency%20of%0APINNs%20by%20facilitating%20the%20transfer%20learning%20process%20thereby%20enabling%20the%0Afinding%20of%20solutions%20for%20nonlinear%2C%20coupled%2C%20and%20multiscale%20differential%0Aequations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12116v1&entry.124074799=Read"},
{"title": "Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of\n  Agents", "author": "Marco Pleines and Matthias Pallasch and Frank Zimmer and Mike Preuss", "abstract": "  Memory Gym presents a suite of 2D partially observable environments, namely\nMortar Mayhem, Mystery Path, and Searing Spotlights, designed to benchmark\nmemory capabilities in decision-making agents. These environments, originally\nwith finite tasks, are expanded into innovative, endless formats, mirroring the\nescalating challenges of cumulative memory games such as \"I packed my bag\".\nThis progression in task design shifts the focus from merely assessing sample\nefficiency to also probing the levels of memory effectiveness in dynamic,\nprolonged scenarios. To address the gap in available memory-based Deep\nReinforcement Learning baselines, we introduce an implementation within the\nopen-source CleanRL library that integrates Transformer-XL (TrXL) with Proximal\nPolicy Optimization. This approach utilizes TrXL as a form of episodic memory,\nemploying a sliding window technique. Our comparative study between the Gated\nRecurrent Unit (GRU) and TrXL reveals varied performances across our finite and\nendless tasks. TrXL, on the finite environments, demonstrates superior\neffectiveness over GRU, but only when utilizing an auxiliary loss to\nreconstruct observations. Notably, GRU makes a remarkable resurgence in all\nendless tasks, consistently outperforming TrXL by significant margins. Website\nand Source Code: https://marcometer.github.io/jmlr_2024.github.io/\n", "link": "http://arxiv.org/abs/2309.17207v6", "date": "2025-01-21", "relevancy": 2.0966, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5613}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20Gym%3A%20Towards%20Endless%20Tasks%20to%20Benchmark%20Memory%20Capabilities%20of%0A%20%20Agents&body=Title%3A%20Memory%20Gym%3A%20Towards%20Endless%20Tasks%20to%20Benchmark%20Memory%20Capabilities%20of%0A%20%20Agents%0AAuthor%3A%20Marco%20Pleines%20and%20Matthias%20Pallasch%20and%20Frank%20Zimmer%20and%20Mike%20Preuss%0AAbstract%3A%20%20%20Memory%20Gym%20presents%20a%20suite%20of%202D%20partially%20observable%20environments%2C%20namely%0AMortar%20Mayhem%2C%20Mystery%20Path%2C%20and%20Searing%20Spotlights%2C%20designed%20to%20benchmark%0Amemory%20capabilities%20in%20decision-making%20agents.%20These%20environments%2C%20originally%0Awith%20finite%20tasks%2C%20are%20expanded%20into%20innovative%2C%20endless%20formats%2C%20mirroring%20the%0Aescalating%20challenges%20of%20cumulative%20memory%20games%20such%20as%20%22I%20packed%20my%20bag%22.%0AThis%20progression%20in%20task%20design%20shifts%20the%20focus%20from%20merely%20assessing%20sample%0Aefficiency%20to%20also%20probing%20the%20levels%20of%20memory%20effectiveness%20in%20dynamic%2C%0Aprolonged%20scenarios.%20To%20address%20the%20gap%20in%20available%20memory-based%20Deep%0AReinforcement%20Learning%20baselines%2C%20we%20introduce%20an%20implementation%20within%20the%0Aopen-source%20CleanRL%20library%20that%20integrates%20Transformer-XL%20%28TrXL%29%20with%20Proximal%0APolicy%20Optimization.%20This%20approach%20utilizes%20TrXL%20as%20a%20form%20of%20episodic%20memory%2C%0Aemploying%20a%20sliding%20window%20technique.%20Our%20comparative%20study%20between%20the%20Gated%0ARecurrent%20Unit%20%28GRU%29%20and%20TrXL%20reveals%20varied%20performances%20across%20our%20finite%20and%0Aendless%20tasks.%20TrXL%2C%20on%20the%20finite%20environments%2C%20demonstrates%20superior%0Aeffectiveness%20over%20GRU%2C%20but%20only%20when%20utilizing%20an%20auxiliary%20loss%20to%0Areconstruct%20observations.%20Notably%2C%20GRU%20makes%20a%20remarkable%20resurgence%20in%20all%0Aendless%20tasks%2C%20consistently%20outperforming%20TrXL%20by%20significant%20margins.%20Website%0Aand%20Source%20Code%3A%20https%3A//marcometer.github.io/jmlr_2024.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17207v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520Gym%253A%2520Towards%2520Endless%2520Tasks%2520to%2520Benchmark%2520Memory%2520Capabilities%2520of%250A%2520%2520Agents%26entry.906535625%3DMarco%2520Pleines%2520and%2520Matthias%2520Pallasch%2520and%2520Frank%2520Zimmer%2520and%2520Mike%2520Preuss%26entry.1292438233%3D%2520%2520Memory%2520Gym%2520presents%2520a%2520suite%2520of%25202D%2520partially%2520observable%2520environments%252C%2520namely%250AMortar%2520Mayhem%252C%2520Mystery%2520Path%252C%2520and%2520Searing%2520Spotlights%252C%2520designed%2520to%2520benchmark%250Amemory%2520capabilities%2520in%2520decision-making%2520agents.%2520These%2520environments%252C%2520originally%250Awith%2520finite%2520tasks%252C%2520are%2520expanded%2520into%2520innovative%252C%2520endless%2520formats%252C%2520mirroring%2520the%250Aescalating%2520challenges%2520of%2520cumulative%2520memory%2520games%2520such%2520as%2520%2522I%2520packed%2520my%2520bag%2522.%250AThis%2520progression%2520in%2520task%2520design%2520shifts%2520the%2520focus%2520from%2520merely%2520assessing%2520sample%250Aefficiency%2520to%2520also%2520probing%2520the%2520levels%2520of%2520memory%2520effectiveness%2520in%2520dynamic%252C%250Aprolonged%2520scenarios.%2520To%2520address%2520the%2520gap%2520in%2520available%2520memory-based%2520Deep%250AReinforcement%2520Learning%2520baselines%252C%2520we%2520introduce%2520an%2520implementation%2520within%2520the%250Aopen-source%2520CleanRL%2520library%2520that%2520integrates%2520Transformer-XL%2520%2528TrXL%2529%2520with%2520Proximal%250APolicy%2520Optimization.%2520This%2520approach%2520utilizes%2520TrXL%2520as%2520a%2520form%2520of%2520episodic%2520memory%252C%250Aemploying%2520a%2520sliding%2520window%2520technique.%2520Our%2520comparative%2520study%2520between%2520the%2520Gated%250ARecurrent%2520Unit%2520%2528GRU%2529%2520and%2520TrXL%2520reveals%2520varied%2520performances%2520across%2520our%2520finite%2520and%250Aendless%2520tasks.%2520TrXL%252C%2520on%2520the%2520finite%2520environments%252C%2520demonstrates%2520superior%250Aeffectiveness%2520over%2520GRU%252C%2520but%2520only%2520when%2520utilizing%2520an%2520auxiliary%2520loss%2520to%250Areconstruct%2520observations.%2520Notably%252C%2520GRU%2520makes%2520a%2520remarkable%2520resurgence%2520in%2520all%250Aendless%2520tasks%252C%2520consistently%2520outperforming%2520TrXL%2520by%2520significant%2520margins.%2520Website%250Aand%2520Source%2520Code%253A%2520https%253A//marcometer.github.io/jmlr_2024.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.17207v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20Gym%3A%20Towards%20Endless%20Tasks%20to%20Benchmark%20Memory%20Capabilities%20of%0A%20%20Agents&entry.906535625=Marco%20Pleines%20and%20Matthias%20Pallasch%20and%20Frank%20Zimmer%20and%20Mike%20Preuss&entry.1292438233=%20%20Memory%20Gym%20presents%20a%20suite%20of%202D%20partially%20observable%20environments%2C%20namely%0AMortar%20Mayhem%2C%20Mystery%20Path%2C%20and%20Searing%20Spotlights%2C%20designed%20to%20benchmark%0Amemory%20capabilities%20in%20decision-making%20agents.%20These%20environments%2C%20originally%0Awith%20finite%20tasks%2C%20are%20expanded%20into%20innovative%2C%20endless%20formats%2C%20mirroring%20the%0Aescalating%20challenges%20of%20cumulative%20memory%20games%20such%20as%20%22I%20packed%20my%20bag%22.%0AThis%20progression%20in%20task%20design%20shifts%20the%20focus%20from%20merely%20assessing%20sample%0Aefficiency%20to%20also%20probing%20the%20levels%20of%20memory%20effectiveness%20in%20dynamic%2C%0Aprolonged%20scenarios.%20To%20address%20the%20gap%20in%20available%20memory-based%20Deep%0AReinforcement%20Learning%20baselines%2C%20we%20introduce%20an%20implementation%20within%20the%0Aopen-source%20CleanRL%20library%20that%20integrates%20Transformer-XL%20%28TrXL%29%20with%20Proximal%0APolicy%20Optimization.%20This%20approach%20utilizes%20TrXL%20as%20a%20form%20of%20episodic%20memory%2C%0Aemploying%20a%20sliding%20window%20technique.%20Our%20comparative%20study%20between%20the%20Gated%0ARecurrent%20Unit%20%28GRU%29%20and%20TrXL%20reveals%20varied%20performances%20across%20our%20finite%20and%0Aendless%20tasks.%20TrXL%2C%20on%20the%20finite%20environments%2C%20demonstrates%20superior%0Aeffectiveness%20over%20GRU%2C%20but%20only%20when%20utilizing%20an%20auxiliary%20loss%20to%0Areconstruct%20observations.%20Notably%2C%20GRU%20makes%20a%20remarkable%20resurgence%20in%20all%0Aendless%20tasks%2C%20consistently%20outperforming%20TrXL%20by%20significant%20margins.%20Website%0Aand%20Source%20Code%3A%20https%3A//marcometer.github.io/jmlr_2024.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17207v6&entry.124074799=Read"},
{"title": "MoGERNN: An Inductive Traffic Predictor for Unobserved Locations in\n  Dynamic Sensing Networks", "author": "Qishen Zhou and Yifan Zhang and Michail A. Makridis and Anastasios Kouvelas and Yibing Wang and Simon Hu", "abstract": "  Given a partially observed road network, how can we predict the traffic state\nof unobserved locations? While deep learning approaches show exceptional\nperformance in traffic prediction, most assume sensors at all locations of\ninterest, which is impractical due to financial constraints. Furthermore, these\nmethods typically require costly retraining when sensor configurations change.\nWe propose MoGERNN, an inductive spatio-temporal graph representation model, to\naddress these challenges. Inspired by the Mixture of Experts approach in Large\nLanguage Models, we introduce a Mixture of Graph Expert (MoGE) block to model\ncomplex spatial dependencies through multiple graph message aggregators and a\nsparse gating network. This block estimates initial states for unobserved\nlocations, which are then processed by a GRU-based Encoder-Decoder that\nintegrates a graph message aggregator to capture spatio-temporal dependencies\nand predict future states. Experiments on two real-world datasets show MoGERNN\nconsistently outperforms baseline methods for both observed and unobserved\nlocations. MoGERNN can accurately predict congestion evolution even in areas\nwithout sensors, offering valuable information for traffic management.\nMoreover, MoGERNN is adaptable to dynamic sensing networks, maintaining\ncompetitive performance even compared to its retrained counterpart. Tests with\ndifferent numbers of available sensors confirm its consistent superiority, and\nablation studies validate the effectiveness of its key modules.\n", "link": "http://arxiv.org/abs/2501.12281v1", "date": "2025-01-21", "relevancy": 2.0922, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5473}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5299}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoGERNN%3A%20An%20Inductive%20Traffic%20Predictor%20for%20Unobserved%20Locations%20in%0A%20%20Dynamic%20Sensing%20Networks&body=Title%3A%20MoGERNN%3A%20An%20Inductive%20Traffic%20Predictor%20for%20Unobserved%20Locations%20in%0A%20%20Dynamic%20Sensing%20Networks%0AAuthor%3A%20Qishen%20Zhou%20and%20Yifan%20Zhang%20and%20Michail%20A.%20Makridis%20and%20Anastasios%20Kouvelas%20and%20Yibing%20Wang%20and%20Simon%20Hu%0AAbstract%3A%20%20%20Given%20a%20partially%20observed%20road%20network%2C%20how%20can%20we%20predict%20the%20traffic%20state%0Aof%20unobserved%20locations%3F%20While%20deep%20learning%20approaches%20show%20exceptional%0Aperformance%20in%20traffic%20prediction%2C%20most%20assume%20sensors%20at%20all%20locations%20of%0Ainterest%2C%20which%20is%20impractical%20due%20to%20financial%20constraints.%20Furthermore%2C%20these%0Amethods%20typically%20require%20costly%20retraining%20when%20sensor%20configurations%20change.%0AWe%20propose%20MoGERNN%2C%20an%20inductive%20spatio-temporal%20graph%20representation%20model%2C%20to%0Aaddress%20these%20challenges.%20Inspired%20by%20the%20Mixture%20of%20Experts%20approach%20in%20Large%0ALanguage%20Models%2C%20we%20introduce%20a%20Mixture%20of%20Graph%20Expert%20%28MoGE%29%20block%20to%20model%0Acomplex%20spatial%20dependencies%20through%20multiple%20graph%20message%20aggregators%20and%20a%0Asparse%20gating%20network.%20This%20block%20estimates%20initial%20states%20for%20unobserved%0Alocations%2C%20which%20are%20then%20processed%20by%20a%20GRU-based%20Encoder-Decoder%20that%0Aintegrates%20a%20graph%20message%20aggregator%20to%20capture%20spatio-temporal%20dependencies%0Aand%20predict%20future%20states.%20Experiments%20on%20two%20real-world%20datasets%20show%20MoGERNN%0Aconsistently%20outperforms%20baseline%20methods%20for%20both%20observed%20and%20unobserved%0Alocations.%20MoGERNN%20can%20accurately%20predict%20congestion%20evolution%20even%20in%20areas%0Awithout%20sensors%2C%20offering%20valuable%20information%20for%20traffic%20management.%0AMoreover%2C%20MoGERNN%20is%20adaptable%20to%20dynamic%20sensing%20networks%2C%20maintaining%0Acompetitive%20performance%20even%20compared%20to%20its%20retrained%20counterpart.%20Tests%20with%0Adifferent%20numbers%20of%20available%20sensors%20confirm%20its%20consistent%20superiority%2C%20and%0Aablation%20studies%20validate%20the%20effectiveness%20of%20its%20key%20modules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoGERNN%253A%2520An%2520Inductive%2520Traffic%2520Predictor%2520for%2520Unobserved%2520Locations%2520in%250A%2520%2520Dynamic%2520Sensing%2520Networks%26entry.906535625%3DQishen%2520Zhou%2520and%2520Yifan%2520Zhang%2520and%2520Michail%2520A.%2520Makridis%2520and%2520Anastasios%2520Kouvelas%2520and%2520Yibing%2520Wang%2520and%2520Simon%2520Hu%26entry.1292438233%3D%2520%2520Given%2520a%2520partially%2520observed%2520road%2520network%252C%2520how%2520can%2520we%2520predict%2520the%2520traffic%2520state%250Aof%2520unobserved%2520locations%253F%2520While%2520deep%2520learning%2520approaches%2520show%2520exceptional%250Aperformance%2520in%2520traffic%2520prediction%252C%2520most%2520assume%2520sensors%2520at%2520all%2520locations%2520of%250Ainterest%252C%2520which%2520is%2520impractical%2520due%2520to%2520financial%2520constraints.%2520Furthermore%252C%2520these%250Amethods%2520typically%2520require%2520costly%2520retraining%2520when%2520sensor%2520configurations%2520change.%250AWe%2520propose%2520MoGERNN%252C%2520an%2520inductive%2520spatio-temporal%2520graph%2520representation%2520model%252C%2520to%250Aaddress%2520these%2520challenges.%2520Inspired%2520by%2520the%2520Mixture%2520of%2520Experts%2520approach%2520in%2520Large%250ALanguage%2520Models%252C%2520we%2520introduce%2520a%2520Mixture%2520of%2520Graph%2520Expert%2520%2528MoGE%2529%2520block%2520to%2520model%250Acomplex%2520spatial%2520dependencies%2520through%2520multiple%2520graph%2520message%2520aggregators%2520and%2520a%250Asparse%2520gating%2520network.%2520This%2520block%2520estimates%2520initial%2520states%2520for%2520unobserved%250Alocations%252C%2520which%2520are%2520then%2520processed%2520by%2520a%2520GRU-based%2520Encoder-Decoder%2520that%250Aintegrates%2520a%2520graph%2520message%2520aggregator%2520to%2520capture%2520spatio-temporal%2520dependencies%250Aand%2520predict%2520future%2520states.%2520Experiments%2520on%2520two%2520real-world%2520datasets%2520show%2520MoGERNN%250Aconsistently%2520outperforms%2520baseline%2520methods%2520for%2520both%2520observed%2520and%2520unobserved%250Alocations.%2520MoGERNN%2520can%2520accurately%2520predict%2520congestion%2520evolution%2520even%2520in%2520areas%250Awithout%2520sensors%252C%2520offering%2520valuable%2520information%2520for%2520traffic%2520management.%250AMoreover%252C%2520MoGERNN%2520is%2520adaptable%2520to%2520dynamic%2520sensing%2520networks%252C%2520maintaining%250Acompetitive%2520performance%2520even%2520compared%2520to%2520its%2520retrained%2520counterpart.%2520Tests%2520with%250Adifferent%2520numbers%2520of%2520available%2520sensors%2520confirm%2520its%2520consistent%2520superiority%252C%2520and%250Aablation%2520studies%2520validate%2520the%2520effectiveness%2520of%2520its%2520key%2520modules.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoGERNN%3A%20An%20Inductive%20Traffic%20Predictor%20for%20Unobserved%20Locations%20in%0A%20%20Dynamic%20Sensing%20Networks&entry.906535625=Qishen%20Zhou%20and%20Yifan%20Zhang%20and%20Michail%20A.%20Makridis%20and%20Anastasios%20Kouvelas%20and%20Yibing%20Wang%20and%20Simon%20Hu&entry.1292438233=%20%20Given%20a%20partially%20observed%20road%20network%2C%20how%20can%20we%20predict%20the%20traffic%20state%0Aof%20unobserved%20locations%3F%20While%20deep%20learning%20approaches%20show%20exceptional%0Aperformance%20in%20traffic%20prediction%2C%20most%20assume%20sensors%20at%20all%20locations%20of%0Ainterest%2C%20which%20is%20impractical%20due%20to%20financial%20constraints.%20Furthermore%2C%20these%0Amethods%20typically%20require%20costly%20retraining%20when%20sensor%20configurations%20change.%0AWe%20propose%20MoGERNN%2C%20an%20inductive%20spatio-temporal%20graph%20representation%20model%2C%20to%0Aaddress%20these%20challenges.%20Inspired%20by%20the%20Mixture%20of%20Experts%20approach%20in%20Large%0ALanguage%20Models%2C%20we%20introduce%20a%20Mixture%20of%20Graph%20Expert%20%28MoGE%29%20block%20to%20model%0Acomplex%20spatial%20dependencies%20through%20multiple%20graph%20message%20aggregators%20and%20a%0Asparse%20gating%20network.%20This%20block%20estimates%20initial%20states%20for%20unobserved%0Alocations%2C%20which%20are%20then%20processed%20by%20a%20GRU-based%20Encoder-Decoder%20that%0Aintegrates%20a%20graph%20message%20aggregator%20to%20capture%20spatio-temporal%20dependencies%0Aand%20predict%20future%20states.%20Experiments%20on%20two%20real-world%20datasets%20show%20MoGERNN%0Aconsistently%20outperforms%20baseline%20methods%20for%20both%20observed%20and%20unobserved%0Alocations.%20MoGERNN%20can%20accurately%20predict%20congestion%20evolution%20even%20in%20areas%0Awithout%20sensors%2C%20offering%20valuable%20information%20for%20traffic%20management.%0AMoreover%2C%20MoGERNN%20is%20adaptable%20to%20dynamic%20sensing%20networks%2C%20maintaining%0Acompetitive%20performance%20even%20compared%20to%20its%20retrained%20counterpart.%20Tests%20with%0Adifferent%20numbers%20of%20available%20sensors%20confirm%20its%20consistent%20superiority%2C%20and%0Aablation%20studies%20validate%20the%20effectiveness%20of%20its%20key%20modules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12281v1&entry.124074799=Read"},
{"title": "Efficient Algorithm for Sparse Fourier Transform of Generalized q-ary\n  Functions", "author": "Darin Tsui and Kunal Talreja and Amirali Aghazadeh", "abstract": "  Computing the Fourier transform of a $q$-ary function\n$f:\\mathbb{Z}_{q}^n\\rightarrow \\mathbb{R}$, which maps $q$-ary sequences to\nreal numbers, is an important problem in mathematics with wide-ranging\napplications in biology, signal processing, and machine learning. Previous\nstudies have shown that, under the sparsity assumption, the Fourier transform\ncan be computed efficiently using fast and sample-efficient algorithms.\nHowever, in many practical settings, the function is defined over a more\ngeneral space -- the space of generalized $q$-ary sequences $\\mathbb{Z}_{q_1}\n\\times \\mathbb{Z}_{q_2} \\times \\cdots \\times \\mathbb{Z}_{q_n}$ -- where each\n$\\mathbb{Z}_{q_i}$ corresponds to integers modulo $q_i$. A naive approach\ninvolves setting $q=\\max_i{q_i}$ and treating the function as $q$-ary, which\nresults in heavy computational overheads. Herein, we develop GFast, an\nalgorithm that computes the $S$-sparse Fourier transform of $f$ with a sample\ncomplexity of $O(Sn)$, computational complexity of $O(Sn \\log N)$, and a\nfailure probability that approaches zero as $N=\\prod_{i=1}^n q_i \\rightarrow\n\\infty$ with $S = N^\\delta$ for some $0 \\leq \\delta < 1$. In the presence of\nnoise, we further demonstrate that a robust version of GFast computes the\ntransform with a sample complexity of $O(Sn^2)$ and computational complexity of\n$O(Sn^2 \\log N)$ under the same high probability guarantees. Using large-scale\nsynthetic experiments, we demonstrate that GFast computes the sparse Fourier\ntransform of generalized $q$-ary functions using $16\\times$ fewer samples and\nrunning $8\\times$ faster than existing algorithms. In real-world protein\nfitness datasets, GFast explains the predictive interactions of a neural\nnetwork with $>25\\%$ smaller normalized mean-squared error compared to existing\nalgorithms.\n", "link": "http://arxiv.org/abs/2501.12365v1", "date": "2025-01-21", "relevancy": 2.0882, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4248}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4233}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Algorithm%20for%20Sparse%20Fourier%20Transform%20of%20Generalized%20q-ary%0A%20%20Functions&body=Title%3A%20Efficient%20Algorithm%20for%20Sparse%20Fourier%20Transform%20of%20Generalized%20q-ary%0A%20%20Functions%0AAuthor%3A%20Darin%20Tsui%20and%20Kunal%20Talreja%20and%20Amirali%20Aghazadeh%0AAbstract%3A%20%20%20Computing%20the%20Fourier%20transform%20of%20a%20%24q%24-ary%20function%0A%24f%3A%5Cmathbb%7BZ%7D_%7Bq%7D%5En%5Crightarrow%20%5Cmathbb%7BR%7D%24%2C%20which%20maps%20%24q%24-ary%20sequences%20to%0Areal%20numbers%2C%20is%20an%20important%20problem%20in%20mathematics%20with%20wide-ranging%0Aapplications%20in%20biology%2C%20signal%20processing%2C%20and%20machine%20learning.%20Previous%0Astudies%20have%20shown%20that%2C%20under%20the%20sparsity%20assumption%2C%20the%20Fourier%20transform%0Acan%20be%20computed%20efficiently%20using%20fast%20and%20sample-efficient%20algorithms.%0AHowever%2C%20in%20many%20practical%20settings%2C%20the%20function%20is%20defined%20over%20a%20more%0Ageneral%20space%20--%20the%20space%20of%20generalized%20%24q%24-ary%20sequences%20%24%5Cmathbb%7BZ%7D_%7Bq_1%7D%0A%5Ctimes%20%5Cmathbb%7BZ%7D_%7Bq_2%7D%20%5Ctimes%20%5Ccdots%20%5Ctimes%20%5Cmathbb%7BZ%7D_%7Bq_n%7D%24%20--%20where%20each%0A%24%5Cmathbb%7BZ%7D_%7Bq_i%7D%24%20corresponds%20to%20integers%20modulo%20%24q_i%24.%20A%20naive%20approach%0Ainvolves%20setting%20%24q%3D%5Cmax_i%7Bq_i%7D%24%20and%20treating%20the%20function%20as%20%24q%24-ary%2C%20which%0Aresults%20in%20heavy%20computational%20overheads.%20Herein%2C%20we%20develop%20GFast%2C%20an%0Aalgorithm%20that%20computes%20the%20%24S%24-sparse%20Fourier%20transform%20of%20%24f%24%20with%20a%20sample%0Acomplexity%20of%20%24O%28Sn%29%24%2C%20computational%20complexity%20of%20%24O%28Sn%20%5Clog%20N%29%24%2C%20and%20a%0Afailure%20probability%20that%20approaches%20zero%20as%20%24N%3D%5Cprod_%7Bi%3D1%7D%5En%20q_i%20%5Crightarrow%0A%5Cinfty%24%20with%20%24S%20%3D%20N%5E%5Cdelta%24%20for%20some%20%240%20%5Cleq%20%5Cdelta%20%3C%201%24.%20In%20the%20presence%20of%0Anoise%2C%20we%20further%20demonstrate%20that%20a%20robust%20version%20of%20GFast%20computes%20the%0Atransform%20with%20a%20sample%20complexity%20of%20%24O%28Sn%5E2%29%24%20and%20computational%20complexity%20of%0A%24O%28Sn%5E2%20%5Clog%20N%29%24%20under%20the%20same%20high%20probability%20guarantees.%20Using%20large-scale%0Asynthetic%20experiments%2C%20we%20demonstrate%20that%20GFast%20computes%20the%20sparse%20Fourier%0Atransform%20of%20generalized%20%24q%24-ary%20functions%20using%20%2416%5Ctimes%24%20fewer%20samples%20and%0Arunning%20%248%5Ctimes%24%20faster%20than%20existing%20algorithms.%20In%20real-world%20protein%0Afitness%20datasets%2C%20GFast%20explains%20the%20predictive%20interactions%20of%20a%20neural%0Anetwork%20with%20%24%3E25%5C%25%24%20smaller%20normalized%20mean-squared%20error%20compared%20to%20existing%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Algorithm%2520for%2520Sparse%2520Fourier%2520Transform%2520of%2520Generalized%2520q-ary%250A%2520%2520Functions%26entry.906535625%3DDarin%2520Tsui%2520and%2520Kunal%2520Talreja%2520and%2520Amirali%2520Aghazadeh%26entry.1292438233%3D%2520%2520Computing%2520the%2520Fourier%2520transform%2520of%2520a%2520%2524q%2524-ary%2520function%250A%2524f%253A%255Cmathbb%257BZ%257D_%257Bq%257D%255En%255Crightarrow%2520%255Cmathbb%257BR%257D%2524%252C%2520which%2520maps%2520%2524q%2524-ary%2520sequences%2520to%250Areal%2520numbers%252C%2520is%2520an%2520important%2520problem%2520in%2520mathematics%2520with%2520wide-ranging%250Aapplications%2520in%2520biology%252C%2520signal%2520processing%252C%2520and%2520machine%2520learning.%2520Previous%250Astudies%2520have%2520shown%2520that%252C%2520under%2520the%2520sparsity%2520assumption%252C%2520the%2520Fourier%2520transform%250Acan%2520be%2520computed%2520efficiently%2520using%2520fast%2520and%2520sample-efficient%2520algorithms.%250AHowever%252C%2520in%2520many%2520practical%2520settings%252C%2520the%2520function%2520is%2520defined%2520over%2520a%2520more%250Ageneral%2520space%2520--%2520the%2520space%2520of%2520generalized%2520%2524q%2524-ary%2520sequences%2520%2524%255Cmathbb%257BZ%257D_%257Bq_1%257D%250A%255Ctimes%2520%255Cmathbb%257BZ%257D_%257Bq_2%257D%2520%255Ctimes%2520%255Ccdots%2520%255Ctimes%2520%255Cmathbb%257BZ%257D_%257Bq_n%257D%2524%2520--%2520where%2520each%250A%2524%255Cmathbb%257BZ%257D_%257Bq_i%257D%2524%2520corresponds%2520to%2520integers%2520modulo%2520%2524q_i%2524.%2520A%2520naive%2520approach%250Ainvolves%2520setting%2520%2524q%253D%255Cmax_i%257Bq_i%257D%2524%2520and%2520treating%2520the%2520function%2520as%2520%2524q%2524-ary%252C%2520which%250Aresults%2520in%2520heavy%2520computational%2520overheads.%2520Herein%252C%2520we%2520develop%2520GFast%252C%2520an%250Aalgorithm%2520that%2520computes%2520the%2520%2524S%2524-sparse%2520Fourier%2520transform%2520of%2520%2524f%2524%2520with%2520a%2520sample%250Acomplexity%2520of%2520%2524O%2528Sn%2529%2524%252C%2520computational%2520complexity%2520of%2520%2524O%2528Sn%2520%255Clog%2520N%2529%2524%252C%2520and%2520a%250Afailure%2520probability%2520that%2520approaches%2520zero%2520as%2520%2524N%253D%255Cprod_%257Bi%253D1%257D%255En%2520q_i%2520%255Crightarrow%250A%255Cinfty%2524%2520with%2520%2524S%2520%253D%2520N%255E%255Cdelta%2524%2520for%2520some%2520%25240%2520%255Cleq%2520%255Cdelta%2520%253C%25201%2524.%2520In%2520the%2520presence%2520of%250Anoise%252C%2520we%2520further%2520demonstrate%2520that%2520a%2520robust%2520version%2520of%2520GFast%2520computes%2520the%250Atransform%2520with%2520a%2520sample%2520complexity%2520of%2520%2524O%2528Sn%255E2%2529%2524%2520and%2520computational%2520complexity%2520of%250A%2524O%2528Sn%255E2%2520%255Clog%2520N%2529%2524%2520under%2520the%2520same%2520high%2520probability%2520guarantees.%2520Using%2520large-scale%250Asynthetic%2520experiments%252C%2520we%2520demonstrate%2520that%2520GFast%2520computes%2520the%2520sparse%2520Fourier%250Atransform%2520of%2520generalized%2520%2524q%2524-ary%2520functions%2520using%2520%252416%255Ctimes%2524%2520fewer%2520samples%2520and%250Arunning%2520%25248%255Ctimes%2524%2520faster%2520than%2520existing%2520algorithms.%2520In%2520real-world%2520protein%250Afitness%2520datasets%252C%2520GFast%2520explains%2520the%2520predictive%2520interactions%2520of%2520a%2520neural%250Anetwork%2520with%2520%2524%253E25%255C%2525%2524%2520smaller%2520normalized%2520mean-squared%2520error%2520compared%2520to%2520existing%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Algorithm%20for%20Sparse%20Fourier%20Transform%20of%20Generalized%20q-ary%0A%20%20Functions&entry.906535625=Darin%20Tsui%20and%20Kunal%20Talreja%20and%20Amirali%20Aghazadeh&entry.1292438233=%20%20Computing%20the%20Fourier%20transform%20of%20a%20%24q%24-ary%20function%0A%24f%3A%5Cmathbb%7BZ%7D_%7Bq%7D%5En%5Crightarrow%20%5Cmathbb%7BR%7D%24%2C%20which%20maps%20%24q%24-ary%20sequences%20to%0Areal%20numbers%2C%20is%20an%20important%20problem%20in%20mathematics%20with%20wide-ranging%0Aapplications%20in%20biology%2C%20signal%20processing%2C%20and%20machine%20learning.%20Previous%0Astudies%20have%20shown%20that%2C%20under%20the%20sparsity%20assumption%2C%20the%20Fourier%20transform%0Acan%20be%20computed%20efficiently%20using%20fast%20and%20sample-efficient%20algorithms.%0AHowever%2C%20in%20many%20practical%20settings%2C%20the%20function%20is%20defined%20over%20a%20more%0Ageneral%20space%20--%20the%20space%20of%20generalized%20%24q%24-ary%20sequences%20%24%5Cmathbb%7BZ%7D_%7Bq_1%7D%0A%5Ctimes%20%5Cmathbb%7BZ%7D_%7Bq_2%7D%20%5Ctimes%20%5Ccdots%20%5Ctimes%20%5Cmathbb%7BZ%7D_%7Bq_n%7D%24%20--%20where%20each%0A%24%5Cmathbb%7BZ%7D_%7Bq_i%7D%24%20corresponds%20to%20integers%20modulo%20%24q_i%24.%20A%20naive%20approach%0Ainvolves%20setting%20%24q%3D%5Cmax_i%7Bq_i%7D%24%20and%20treating%20the%20function%20as%20%24q%24-ary%2C%20which%0Aresults%20in%20heavy%20computational%20overheads.%20Herein%2C%20we%20develop%20GFast%2C%20an%0Aalgorithm%20that%20computes%20the%20%24S%24-sparse%20Fourier%20transform%20of%20%24f%24%20with%20a%20sample%0Acomplexity%20of%20%24O%28Sn%29%24%2C%20computational%20complexity%20of%20%24O%28Sn%20%5Clog%20N%29%24%2C%20and%20a%0Afailure%20probability%20that%20approaches%20zero%20as%20%24N%3D%5Cprod_%7Bi%3D1%7D%5En%20q_i%20%5Crightarrow%0A%5Cinfty%24%20with%20%24S%20%3D%20N%5E%5Cdelta%24%20for%20some%20%240%20%5Cleq%20%5Cdelta%20%3C%201%24.%20In%20the%20presence%20of%0Anoise%2C%20we%20further%20demonstrate%20that%20a%20robust%20version%20of%20GFast%20computes%20the%0Atransform%20with%20a%20sample%20complexity%20of%20%24O%28Sn%5E2%29%24%20and%20computational%20complexity%20of%0A%24O%28Sn%5E2%20%5Clog%20N%29%24%20under%20the%20same%20high%20probability%20guarantees.%20Using%20large-scale%0Asynthetic%20experiments%2C%20we%20demonstrate%20that%20GFast%20computes%20the%20sparse%20Fourier%0Atransform%20of%20generalized%20%24q%24-ary%20functions%20using%20%2416%5Ctimes%24%20fewer%20samples%20and%0Arunning%20%248%5Ctimes%24%20faster%20than%20existing%20algorithms.%20In%20real-world%20protein%0Afitness%20datasets%2C%20GFast%20explains%20the%20predictive%20interactions%20of%20a%20neural%0Anetwork%20with%20%24%3E25%5C%25%24%20smaller%20normalized%20mean-squared%20error%20compared%20to%20existing%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12365v1&entry.124074799=Read"},
{"title": "Learning to generate feasible graphs using graph grammars", "author": "Stefan Mautner and Rolf Backofen and Fabrizio Costa", "abstract": "  Generative methods for graphs need to be sufficiently flexible to model\ncomplex dependencies between sets of nodes. At the same time, the generated\ngraphs need to satisfy domain-dependent feasibility conditions, that is, they\nshould not violate certain constraints that would make their interpretation\nimpossible within the given application domain (e.g. a molecular graph where an\natom has a very large number of chemical bounds). Crucially, constraints can\ninvolve not only local but also long-range dependencies: for example, the\nmaximal length of a cycle can be bounded.\n  Currently, a large class of generative approaches for graphs, such as methods\nbased on artificial neural networks, is based on message passing schemes. These\napproaches suffer from information 'dilution' issues that severely limit the\nmaximal range of the dependencies that can be modeled. To address this problem,\nwe propose a generative approach based on the notion of graph grammars. The key\nnovel idea is to introduce a domain-dependent coarsening procedure to provide\nshort-cuts for long-range dependencies.\n  We show the effectiveness of our proposal in two domains: 1) small drugs and\n2) RNA secondary structures. In the first case, we compare the quality of the\ngenerated molecular graphs via the Molecular Sets (MOSES) benchmark suite,\nwhich evaluates the distance between generated and real molecules, their\nlipophilicity, synthesizability, and drug-likeness. In the second case, we show\nthat the approach can generate very large graphs (with hundreds of nodes) that\nare accepted as valid examples for a desired RNA family by the \"Infernal\"\ncovariance model, a state-of-the-art RNA classifier.\n  Our implementation is available on github:\ngithub.com/fabriziocosta/GraphLearn\n", "link": "http://arxiv.org/abs/2501.06003v2", "date": "2025-01-21", "relevancy": 2.0862, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.523}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5218}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20generate%20feasible%20graphs%20using%20graph%20grammars&body=Title%3A%20Learning%20to%20generate%20feasible%20graphs%20using%20graph%20grammars%0AAuthor%3A%20Stefan%20Mautner%20and%20Rolf%20Backofen%20and%20Fabrizio%20Costa%0AAbstract%3A%20%20%20Generative%20methods%20for%20graphs%20need%20to%20be%20sufficiently%20flexible%20to%20model%0Acomplex%20dependencies%20between%20sets%20of%20nodes.%20At%20the%20same%20time%2C%20the%20generated%0Agraphs%20need%20to%20satisfy%20domain-dependent%20feasibility%20conditions%2C%20that%20is%2C%20they%0Ashould%20not%20violate%20certain%20constraints%20that%20would%20make%20their%20interpretation%0Aimpossible%20within%20the%20given%20application%20domain%20%28e.g.%20a%20molecular%20graph%20where%20an%0Aatom%20has%20a%20very%20large%20number%20of%20chemical%20bounds%29.%20Crucially%2C%20constraints%20can%0Ainvolve%20not%20only%20local%20but%20also%20long-range%20dependencies%3A%20for%20example%2C%20the%0Amaximal%20length%20of%20a%20cycle%20can%20be%20bounded.%0A%20%20Currently%2C%20a%20large%20class%20of%20generative%20approaches%20for%20graphs%2C%20such%20as%20methods%0Abased%20on%20artificial%20neural%20networks%2C%20is%20based%20on%20message%20passing%20schemes.%20These%0Aapproaches%20suffer%20from%20information%20%27dilution%27%20issues%20that%20severely%20limit%20the%0Amaximal%20range%20of%20the%20dependencies%20that%20can%20be%20modeled.%20To%20address%20this%20problem%2C%0Awe%20propose%20a%20generative%20approach%20based%20on%20the%20notion%20of%20graph%20grammars.%20The%20key%0Anovel%20idea%20is%20to%20introduce%20a%20domain-dependent%20coarsening%20procedure%20to%20provide%0Ashort-cuts%20for%20long-range%20dependencies.%0A%20%20We%20show%20the%20effectiveness%20of%20our%20proposal%20in%20two%20domains%3A%201%29%20small%20drugs%20and%0A2%29%20RNA%20secondary%20structures.%20In%20the%20first%20case%2C%20we%20compare%20the%20quality%20of%20the%0Agenerated%20molecular%20graphs%20via%20the%20Molecular%20Sets%20%28MOSES%29%20benchmark%20suite%2C%0Awhich%20evaluates%20the%20distance%20between%20generated%20and%20real%20molecules%2C%20their%0Alipophilicity%2C%20synthesizability%2C%20and%20drug-likeness.%20In%20the%20second%20case%2C%20we%20show%0Athat%20the%20approach%20can%20generate%20very%20large%20graphs%20%28with%20hundreds%20of%20nodes%29%20that%0Aare%20accepted%20as%20valid%20examples%20for%20a%20desired%20RNA%20family%20by%20the%20%22Infernal%22%0Acovariance%20model%2C%20a%20state-of-the-art%20RNA%20classifier.%0A%20%20Our%20implementation%20is%20available%20on%20github%3A%0Agithub.com/fabriziocosta/GraphLearn%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06003v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520generate%2520feasible%2520graphs%2520using%2520graph%2520grammars%26entry.906535625%3DStefan%2520Mautner%2520and%2520Rolf%2520Backofen%2520and%2520Fabrizio%2520Costa%26entry.1292438233%3D%2520%2520Generative%2520methods%2520for%2520graphs%2520need%2520to%2520be%2520sufficiently%2520flexible%2520to%2520model%250Acomplex%2520dependencies%2520between%2520sets%2520of%2520nodes.%2520At%2520the%2520same%2520time%252C%2520the%2520generated%250Agraphs%2520need%2520to%2520satisfy%2520domain-dependent%2520feasibility%2520conditions%252C%2520that%2520is%252C%2520they%250Ashould%2520not%2520violate%2520certain%2520constraints%2520that%2520would%2520make%2520their%2520interpretation%250Aimpossible%2520within%2520the%2520given%2520application%2520domain%2520%2528e.g.%2520a%2520molecular%2520graph%2520where%2520an%250Aatom%2520has%2520a%2520very%2520large%2520number%2520of%2520chemical%2520bounds%2529.%2520Crucially%252C%2520constraints%2520can%250Ainvolve%2520not%2520only%2520local%2520but%2520also%2520long-range%2520dependencies%253A%2520for%2520example%252C%2520the%250Amaximal%2520length%2520of%2520a%2520cycle%2520can%2520be%2520bounded.%250A%2520%2520Currently%252C%2520a%2520large%2520class%2520of%2520generative%2520approaches%2520for%2520graphs%252C%2520such%2520as%2520methods%250Abased%2520on%2520artificial%2520neural%2520networks%252C%2520is%2520based%2520on%2520message%2520passing%2520schemes.%2520These%250Aapproaches%2520suffer%2520from%2520information%2520%2527dilution%2527%2520issues%2520that%2520severely%2520limit%2520the%250Amaximal%2520range%2520of%2520the%2520dependencies%2520that%2520can%2520be%2520modeled.%2520To%2520address%2520this%2520problem%252C%250Awe%2520propose%2520a%2520generative%2520approach%2520based%2520on%2520the%2520notion%2520of%2520graph%2520grammars.%2520The%2520key%250Anovel%2520idea%2520is%2520to%2520introduce%2520a%2520domain-dependent%2520coarsening%2520procedure%2520to%2520provide%250Ashort-cuts%2520for%2520long-range%2520dependencies.%250A%2520%2520We%2520show%2520the%2520effectiveness%2520of%2520our%2520proposal%2520in%2520two%2520domains%253A%25201%2529%2520small%2520drugs%2520and%250A2%2529%2520RNA%2520secondary%2520structures.%2520In%2520the%2520first%2520case%252C%2520we%2520compare%2520the%2520quality%2520of%2520the%250Agenerated%2520molecular%2520graphs%2520via%2520the%2520Molecular%2520Sets%2520%2528MOSES%2529%2520benchmark%2520suite%252C%250Awhich%2520evaluates%2520the%2520distance%2520between%2520generated%2520and%2520real%2520molecules%252C%2520their%250Alipophilicity%252C%2520synthesizability%252C%2520and%2520drug-likeness.%2520In%2520the%2520second%2520case%252C%2520we%2520show%250Athat%2520the%2520approach%2520can%2520generate%2520very%2520large%2520graphs%2520%2528with%2520hundreds%2520of%2520nodes%2529%2520that%250Aare%2520accepted%2520as%2520valid%2520examples%2520for%2520a%2520desired%2520RNA%2520family%2520by%2520the%2520%2522Infernal%2522%250Acovariance%2520model%252C%2520a%2520state-of-the-art%2520RNA%2520classifier.%250A%2520%2520Our%2520implementation%2520is%2520available%2520on%2520github%253A%250Agithub.com/fabriziocosta/GraphLearn%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06003v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20generate%20feasible%20graphs%20using%20graph%20grammars&entry.906535625=Stefan%20Mautner%20and%20Rolf%20Backofen%20and%20Fabrizio%20Costa&entry.1292438233=%20%20Generative%20methods%20for%20graphs%20need%20to%20be%20sufficiently%20flexible%20to%20model%0Acomplex%20dependencies%20between%20sets%20of%20nodes.%20At%20the%20same%20time%2C%20the%20generated%0Agraphs%20need%20to%20satisfy%20domain-dependent%20feasibility%20conditions%2C%20that%20is%2C%20they%0Ashould%20not%20violate%20certain%20constraints%20that%20would%20make%20their%20interpretation%0Aimpossible%20within%20the%20given%20application%20domain%20%28e.g.%20a%20molecular%20graph%20where%20an%0Aatom%20has%20a%20very%20large%20number%20of%20chemical%20bounds%29.%20Crucially%2C%20constraints%20can%0Ainvolve%20not%20only%20local%20but%20also%20long-range%20dependencies%3A%20for%20example%2C%20the%0Amaximal%20length%20of%20a%20cycle%20can%20be%20bounded.%0A%20%20Currently%2C%20a%20large%20class%20of%20generative%20approaches%20for%20graphs%2C%20such%20as%20methods%0Abased%20on%20artificial%20neural%20networks%2C%20is%20based%20on%20message%20passing%20schemes.%20These%0Aapproaches%20suffer%20from%20information%20%27dilution%27%20issues%20that%20severely%20limit%20the%0Amaximal%20range%20of%20the%20dependencies%20that%20can%20be%20modeled.%20To%20address%20this%20problem%2C%0Awe%20propose%20a%20generative%20approach%20based%20on%20the%20notion%20of%20graph%20grammars.%20The%20key%0Anovel%20idea%20is%20to%20introduce%20a%20domain-dependent%20coarsening%20procedure%20to%20provide%0Ashort-cuts%20for%20long-range%20dependencies.%0A%20%20We%20show%20the%20effectiveness%20of%20our%20proposal%20in%20two%20domains%3A%201%29%20small%20drugs%20and%0A2%29%20RNA%20secondary%20structures.%20In%20the%20first%20case%2C%20we%20compare%20the%20quality%20of%20the%0Agenerated%20molecular%20graphs%20via%20the%20Molecular%20Sets%20%28MOSES%29%20benchmark%20suite%2C%0Awhich%20evaluates%20the%20distance%20between%20generated%20and%20real%20molecules%2C%20their%0Alipophilicity%2C%20synthesizability%2C%20and%20drug-likeness.%20In%20the%20second%20case%2C%20we%20show%0Athat%20the%20approach%20can%20generate%20very%20large%20graphs%20%28with%20hundreds%20of%20nodes%29%20that%0Aare%20accepted%20as%20valid%20examples%20for%20a%20desired%20RNA%20family%20by%20the%20%22Infernal%22%0Acovariance%20model%2C%20a%20state-of-the-art%20RNA%20classifier.%0A%20%20Our%20implementation%20is%20available%20on%20github%3A%0Agithub.com/fabriziocosta/GraphLearn%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06003v2&entry.124074799=Read"},
{"title": "Comparative Analysis of Pre-trained Deep Learning Models and DINOv2 for\n  Cushing's Syndrome Diagnosis in Facial Analysis", "author": "Hongjun Liu and Changwei Song and Jiaqi Qiang and Jianqiang Li and Hui Pan and Lin Lu and Xiao Long and Qing Zhao and Jiuzuo Huang and Shi Chen", "abstract": "  Cushing's syndrome is a condition caused by excessive glucocorticoid\nsecretion from the adrenal cortex, often manifesting with moon facies and\nplethora, making facial data crucial for diagnosis. Previous studies have used\npre-trained convolutional neural networks (CNNs) for diagnosing Cushing's\nsyndrome using frontal facial images. However, CNNs are better at capturing\nlocal features, while Cushing's syndrome often presents with global facial\nfeatures. Transformer-based models like ViT and SWIN, which utilize\nself-attention mechanisms, can better capture long-range dependencies and\nglobal features. Recently, DINOv2, a foundation model based on visual\nTransformers, has gained interest. This study compares the performance of\nvarious pre-trained models, including CNNs, Transformer-based models, and\nDINOv2, in diagnosing Cushing's syndrome. We also analyze gender bias and the\nimpact of freezing mechanisms on DINOv2. Our results show that\nTransformer-based models and DINOv2 outperformed CNNs, with ViT achieving the\nhighest F1 score of 85.74%. Both the pre-trained model and DINOv2 had higher\naccuracy for female samples. DINOv2 also showed improved performance when\nfreezing parameters. In conclusion, Transformer-based models and DINOv2 are\neffective for Cushing's syndrome classification.\n", "link": "http://arxiv.org/abs/2501.12023v1", "date": "2025-01-21", "relevancy": 2.084, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5243}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5243}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20of%20Pre-trained%20Deep%20Learning%20Models%20and%20DINOv2%20for%0A%20%20Cushing%27s%20Syndrome%20Diagnosis%20in%20Facial%20Analysis&body=Title%3A%20Comparative%20Analysis%20of%20Pre-trained%20Deep%20Learning%20Models%20and%20DINOv2%20for%0A%20%20Cushing%27s%20Syndrome%20Diagnosis%20in%20Facial%20Analysis%0AAuthor%3A%20Hongjun%20Liu%20and%20Changwei%20Song%20and%20Jiaqi%20Qiang%20and%20Jianqiang%20Li%20and%20Hui%20Pan%20and%20Lin%20Lu%20and%20Xiao%20Long%20and%20Qing%20Zhao%20and%20Jiuzuo%20Huang%20and%20Shi%20Chen%0AAbstract%3A%20%20%20Cushing%27s%20syndrome%20is%20a%20condition%20caused%20by%20excessive%20glucocorticoid%0Asecretion%20from%20the%20adrenal%20cortex%2C%20often%20manifesting%20with%20moon%20facies%20and%0Aplethora%2C%20making%20facial%20data%20crucial%20for%20diagnosis.%20Previous%20studies%20have%20used%0Apre-trained%20convolutional%20neural%20networks%20%28CNNs%29%20for%20diagnosing%20Cushing%27s%0Asyndrome%20using%20frontal%20facial%20images.%20However%2C%20CNNs%20are%20better%20at%20capturing%0Alocal%20features%2C%20while%20Cushing%27s%20syndrome%20often%20presents%20with%20global%20facial%0Afeatures.%20Transformer-based%20models%20like%20ViT%20and%20SWIN%2C%20which%20utilize%0Aself-attention%20mechanisms%2C%20can%20better%20capture%20long-range%20dependencies%20and%0Aglobal%20features.%20Recently%2C%20DINOv2%2C%20a%20foundation%20model%20based%20on%20visual%0ATransformers%2C%20has%20gained%20interest.%20This%20study%20compares%20the%20performance%20of%0Avarious%20pre-trained%20models%2C%20including%20CNNs%2C%20Transformer-based%20models%2C%20and%0ADINOv2%2C%20in%20diagnosing%20Cushing%27s%20syndrome.%20We%20also%20analyze%20gender%20bias%20and%20the%0Aimpact%20of%20freezing%20mechanisms%20on%20DINOv2.%20Our%20results%20show%20that%0ATransformer-based%20models%20and%20DINOv2%20outperformed%20CNNs%2C%20with%20ViT%20achieving%20the%0Ahighest%20F1%20score%20of%2085.74%25.%20Both%20the%20pre-trained%20model%20and%20DINOv2%20had%20higher%0Aaccuracy%20for%20female%20samples.%20DINOv2%20also%20showed%20improved%20performance%20when%0Afreezing%20parameters.%20In%20conclusion%2C%20Transformer-based%20models%20and%20DINOv2%20are%0Aeffective%20for%20Cushing%27s%20syndrome%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520of%2520Pre-trained%2520Deep%2520Learning%2520Models%2520and%2520DINOv2%2520for%250A%2520%2520Cushing%2527s%2520Syndrome%2520Diagnosis%2520in%2520Facial%2520Analysis%26entry.906535625%3DHongjun%2520Liu%2520and%2520Changwei%2520Song%2520and%2520Jiaqi%2520Qiang%2520and%2520Jianqiang%2520Li%2520and%2520Hui%2520Pan%2520and%2520Lin%2520Lu%2520and%2520Xiao%2520Long%2520and%2520Qing%2520Zhao%2520and%2520Jiuzuo%2520Huang%2520and%2520Shi%2520Chen%26entry.1292438233%3D%2520%2520Cushing%2527s%2520syndrome%2520is%2520a%2520condition%2520caused%2520by%2520excessive%2520glucocorticoid%250Asecretion%2520from%2520the%2520adrenal%2520cortex%252C%2520often%2520manifesting%2520with%2520moon%2520facies%2520and%250Aplethora%252C%2520making%2520facial%2520data%2520crucial%2520for%2520diagnosis.%2520Previous%2520studies%2520have%2520used%250Apre-trained%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520for%2520diagnosing%2520Cushing%2527s%250Asyndrome%2520using%2520frontal%2520facial%2520images.%2520However%252C%2520CNNs%2520are%2520better%2520at%2520capturing%250Alocal%2520features%252C%2520while%2520Cushing%2527s%2520syndrome%2520often%2520presents%2520with%2520global%2520facial%250Afeatures.%2520Transformer-based%2520models%2520like%2520ViT%2520and%2520SWIN%252C%2520which%2520utilize%250Aself-attention%2520mechanisms%252C%2520can%2520better%2520capture%2520long-range%2520dependencies%2520and%250Aglobal%2520features.%2520Recently%252C%2520DINOv2%252C%2520a%2520foundation%2520model%2520based%2520on%2520visual%250ATransformers%252C%2520has%2520gained%2520interest.%2520This%2520study%2520compares%2520the%2520performance%2520of%250Avarious%2520pre-trained%2520models%252C%2520including%2520CNNs%252C%2520Transformer-based%2520models%252C%2520and%250ADINOv2%252C%2520in%2520diagnosing%2520Cushing%2527s%2520syndrome.%2520We%2520also%2520analyze%2520gender%2520bias%2520and%2520the%250Aimpact%2520of%2520freezing%2520mechanisms%2520on%2520DINOv2.%2520Our%2520results%2520show%2520that%250ATransformer-based%2520models%2520and%2520DINOv2%2520outperformed%2520CNNs%252C%2520with%2520ViT%2520achieving%2520the%250Ahighest%2520F1%2520score%2520of%252085.74%2525.%2520Both%2520the%2520pre-trained%2520model%2520and%2520DINOv2%2520had%2520higher%250Aaccuracy%2520for%2520female%2520samples.%2520DINOv2%2520also%2520showed%2520improved%2520performance%2520when%250Afreezing%2520parameters.%2520In%2520conclusion%252C%2520Transformer-based%2520models%2520and%2520DINOv2%2520are%250Aeffective%2520for%2520Cushing%2527s%2520syndrome%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20of%20Pre-trained%20Deep%20Learning%20Models%20and%20DINOv2%20for%0A%20%20Cushing%27s%20Syndrome%20Diagnosis%20in%20Facial%20Analysis&entry.906535625=Hongjun%20Liu%20and%20Changwei%20Song%20and%20Jiaqi%20Qiang%20and%20Jianqiang%20Li%20and%20Hui%20Pan%20and%20Lin%20Lu%20and%20Xiao%20Long%20and%20Qing%20Zhao%20and%20Jiuzuo%20Huang%20and%20Shi%20Chen&entry.1292438233=%20%20Cushing%27s%20syndrome%20is%20a%20condition%20caused%20by%20excessive%20glucocorticoid%0Asecretion%20from%20the%20adrenal%20cortex%2C%20often%20manifesting%20with%20moon%20facies%20and%0Aplethora%2C%20making%20facial%20data%20crucial%20for%20diagnosis.%20Previous%20studies%20have%20used%0Apre-trained%20convolutional%20neural%20networks%20%28CNNs%29%20for%20diagnosing%20Cushing%27s%0Asyndrome%20using%20frontal%20facial%20images.%20However%2C%20CNNs%20are%20better%20at%20capturing%0Alocal%20features%2C%20while%20Cushing%27s%20syndrome%20often%20presents%20with%20global%20facial%0Afeatures.%20Transformer-based%20models%20like%20ViT%20and%20SWIN%2C%20which%20utilize%0Aself-attention%20mechanisms%2C%20can%20better%20capture%20long-range%20dependencies%20and%0Aglobal%20features.%20Recently%2C%20DINOv2%2C%20a%20foundation%20model%20based%20on%20visual%0ATransformers%2C%20has%20gained%20interest.%20This%20study%20compares%20the%20performance%20of%0Avarious%20pre-trained%20models%2C%20including%20CNNs%2C%20Transformer-based%20models%2C%20and%0ADINOv2%2C%20in%20diagnosing%20Cushing%27s%20syndrome.%20We%20also%20analyze%20gender%20bias%20and%20the%0Aimpact%20of%20freezing%20mechanisms%20on%20DINOv2.%20Our%20results%20show%20that%0ATransformer-based%20models%20and%20DINOv2%20outperformed%20CNNs%2C%20with%20ViT%20achieving%20the%0Ahighest%20F1%20score%20of%2085.74%25.%20Both%20the%20pre-trained%20model%20and%20DINOv2%20had%20higher%0Aaccuracy%20for%20female%20samples.%20DINOv2%20also%20showed%20improved%20performance%20when%0Afreezing%20parameters.%20In%20conclusion%2C%20Transformer-based%20models%20and%20DINOv2%20are%0Aeffective%20for%20Cushing%27s%20syndrome%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12023v1&entry.124074799=Read"},
{"title": "Vision-Language Models for Automated Chest X-ray Interpretation:\n  Leveraging ViT and GPT-2", "author": "Md. Rakibul Islam and Md. Zahid Hossain and Mustofa Ahmed and Most. Sharmin Sultana Samu", "abstract": "  Radiology plays a pivotal role in modern medicine due to its non-invasive\ndiagnostic capabilities. However, the manual generation of unstructured medical\nreports is time consuming and prone to errors. It creates a significant\nbottleneck in clinical workflows. Despite advancements in AI-generated\nradiology reports, challenges remain in achieving detailed and accurate report\ngeneration. In this study we have evaluated different combinations of\nmultimodal models that integrate Computer Vision and Natural Language\nProcessing to generate comprehensive radiology reports. We employed a\npretrained Vision Transformer (ViT-B16) and a SWIN Transformer as the image\nencoders. The BART and GPT-2 models serve as the textual decoders. We used\nChest X-ray images and reports from the IU-Xray dataset to evaluate the\nusability of the SWIN Transformer-BART, SWIN Transformer-GPT-2, ViT-B16-BART\nand ViT-B16-GPT-2 models for report generation. We aimed at finding the best\ncombination among the models. The SWIN-BART model performs as the\nbest-performing model among the four models achieving remarkable results in\nalmost all the evaluation metrics like ROUGE, BLEU and BERTScore.\n", "link": "http://arxiv.org/abs/2501.12356v1", "date": "2025-01-21", "relevancy": 2.0763, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Models%20for%20Automated%20Chest%20X-ray%20Interpretation%3A%0A%20%20Leveraging%20ViT%20and%20GPT-2&body=Title%3A%20Vision-Language%20Models%20for%20Automated%20Chest%20X-ray%20Interpretation%3A%0A%20%20Leveraging%20ViT%20and%20GPT-2%0AAuthor%3A%20Md.%20Rakibul%20Islam%20and%20Md.%20Zahid%20Hossain%20and%20Mustofa%20Ahmed%20and%20Most.%20Sharmin%20Sultana%20Samu%0AAbstract%3A%20%20%20Radiology%20plays%20a%20pivotal%20role%20in%20modern%20medicine%20due%20to%20its%20non-invasive%0Adiagnostic%20capabilities.%20However%2C%20the%20manual%20generation%20of%20unstructured%20medical%0Areports%20is%20time%20consuming%20and%20prone%20to%20errors.%20It%20creates%20a%20significant%0Abottleneck%20in%20clinical%20workflows.%20Despite%20advancements%20in%20AI-generated%0Aradiology%20reports%2C%20challenges%20remain%20in%20achieving%20detailed%20and%20accurate%20report%0Ageneration.%20In%20this%20study%20we%20have%20evaluated%20different%20combinations%20of%0Amultimodal%20models%20that%20integrate%20Computer%20Vision%20and%20Natural%20Language%0AProcessing%20to%20generate%20comprehensive%20radiology%20reports.%20We%20employed%20a%0Apretrained%20Vision%20Transformer%20%28ViT-B16%29%20and%20a%20SWIN%20Transformer%20as%20the%20image%0Aencoders.%20The%20BART%20and%20GPT-2%20models%20serve%20as%20the%20textual%20decoders.%20We%20used%0AChest%20X-ray%20images%20and%20reports%20from%20the%20IU-Xray%20dataset%20to%20evaluate%20the%0Ausability%20of%20the%20SWIN%20Transformer-BART%2C%20SWIN%20Transformer-GPT-2%2C%20ViT-B16-BART%0Aand%20ViT-B16-GPT-2%20models%20for%20report%20generation.%20We%20aimed%20at%20finding%20the%20best%0Acombination%20among%20the%20models.%20The%20SWIN-BART%20model%20performs%20as%20the%0Abest-performing%20model%20among%20the%20four%20models%20achieving%20remarkable%20results%20in%0Aalmost%20all%20the%20evaluation%20metrics%20like%20ROUGE%2C%20BLEU%20and%20BERTScore.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Models%2520for%2520Automated%2520Chest%2520X-ray%2520Interpretation%253A%250A%2520%2520Leveraging%2520ViT%2520and%2520GPT-2%26entry.906535625%3DMd.%2520Rakibul%2520Islam%2520and%2520Md.%2520Zahid%2520Hossain%2520and%2520Mustofa%2520Ahmed%2520and%2520Most.%2520Sharmin%2520Sultana%2520Samu%26entry.1292438233%3D%2520%2520Radiology%2520plays%2520a%2520pivotal%2520role%2520in%2520modern%2520medicine%2520due%2520to%2520its%2520non-invasive%250Adiagnostic%2520capabilities.%2520However%252C%2520the%2520manual%2520generation%2520of%2520unstructured%2520medical%250Areports%2520is%2520time%2520consuming%2520and%2520prone%2520to%2520errors.%2520It%2520creates%2520a%2520significant%250Abottleneck%2520in%2520clinical%2520workflows.%2520Despite%2520advancements%2520in%2520AI-generated%250Aradiology%2520reports%252C%2520challenges%2520remain%2520in%2520achieving%2520detailed%2520and%2520accurate%2520report%250Ageneration.%2520In%2520this%2520study%2520we%2520have%2520evaluated%2520different%2520combinations%2520of%250Amultimodal%2520models%2520that%2520integrate%2520Computer%2520Vision%2520and%2520Natural%2520Language%250AProcessing%2520to%2520generate%2520comprehensive%2520radiology%2520reports.%2520We%2520employed%2520a%250Apretrained%2520Vision%2520Transformer%2520%2528ViT-B16%2529%2520and%2520a%2520SWIN%2520Transformer%2520as%2520the%2520image%250Aencoders.%2520The%2520BART%2520and%2520GPT-2%2520models%2520serve%2520as%2520the%2520textual%2520decoders.%2520We%2520used%250AChest%2520X-ray%2520images%2520and%2520reports%2520from%2520the%2520IU-Xray%2520dataset%2520to%2520evaluate%2520the%250Ausability%2520of%2520the%2520SWIN%2520Transformer-BART%252C%2520SWIN%2520Transformer-GPT-2%252C%2520ViT-B16-BART%250Aand%2520ViT-B16-GPT-2%2520models%2520for%2520report%2520generation.%2520We%2520aimed%2520at%2520finding%2520the%2520best%250Acombination%2520among%2520the%2520models.%2520The%2520SWIN-BART%2520model%2520performs%2520as%2520the%250Abest-performing%2520model%2520among%2520the%2520four%2520models%2520achieving%2520remarkable%2520results%2520in%250Aalmost%2520all%2520the%2520evaluation%2520metrics%2520like%2520ROUGE%252C%2520BLEU%2520and%2520BERTScore.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Models%20for%20Automated%20Chest%20X-ray%20Interpretation%3A%0A%20%20Leveraging%20ViT%20and%20GPT-2&entry.906535625=Md.%20Rakibul%20Islam%20and%20Md.%20Zahid%20Hossain%20and%20Mustofa%20Ahmed%20and%20Most.%20Sharmin%20Sultana%20Samu&entry.1292438233=%20%20Radiology%20plays%20a%20pivotal%20role%20in%20modern%20medicine%20due%20to%20its%20non-invasive%0Adiagnostic%20capabilities.%20However%2C%20the%20manual%20generation%20of%20unstructured%20medical%0Areports%20is%20time%20consuming%20and%20prone%20to%20errors.%20It%20creates%20a%20significant%0Abottleneck%20in%20clinical%20workflows.%20Despite%20advancements%20in%20AI-generated%0Aradiology%20reports%2C%20challenges%20remain%20in%20achieving%20detailed%20and%20accurate%20report%0Ageneration.%20In%20this%20study%20we%20have%20evaluated%20different%20combinations%20of%0Amultimodal%20models%20that%20integrate%20Computer%20Vision%20and%20Natural%20Language%0AProcessing%20to%20generate%20comprehensive%20radiology%20reports.%20We%20employed%20a%0Apretrained%20Vision%20Transformer%20%28ViT-B16%29%20and%20a%20SWIN%20Transformer%20as%20the%20image%0Aencoders.%20The%20BART%20and%20GPT-2%20models%20serve%20as%20the%20textual%20decoders.%20We%20used%0AChest%20X-ray%20images%20and%20reports%20from%20the%20IU-Xray%20dataset%20to%20evaluate%20the%0Ausability%20of%20the%20SWIN%20Transformer-BART%2C%20SWIN%20Transformer-GPT-2%2C%20ViT-B16-BART%0Aand%20ViT-B16-GPT-2%20models%20for%20report%20generation.%20We%20aimed%20at%20finding%20the%20best%0Acombination%20among%20the%20models.%20The%20SWIN-BART%20model%20performs%20as%20the%0Abest-performing%20model%20among%20the%20four%20models%20achieving%20remarkable%20results%20in%0Aalmost%20all%20the%20evaluation%20metrics%20like%20ROUGE%2C%20BLEU%20and%20BERTScore.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12356v1&entry.124074799=Read"},
{"title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents", "author": "Yujia Qin and Yining Ye and Junjie Fang and Haoming Wang and Shihao Liang and Shizuo Tian and Junda Zhang and Jiahao Li and Yunxin Li and Shijue Huang and Wanjun Zhong and Kuanye Li and Jiale Yang and Yu Miao and Woyu Lin and Longxiang Liu and Xu Jiang and Qianli Ma and Jingyu Li and Xiaojun Xiao and Kai Cai and Chuang Li and Yaowei Zheng and Chaolin Jin and Chen Li and Xiao Zhou and Minchao Wang and Haoli Chen and Zhaojian Li and Haihua Yang and Haifeng Liu and Feng Lin and Tao Peng and Xin Liu and Guang Shi", "abstract": "  This paper introduces UI-TARS, a native GUI agent model that solely perceives\nthe screenshots as input and performs human-like interactions (e.g., keyboard\nand mouse operations). Unlike prevailing agent frameworks that depend on\nheavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts\nand workflows, UI-TARS is an end-to-end model that outperforms these\nsophisticated frameworks. Experiments demonstrate its superior performance:\nUI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating\nperception, grounding, and GUI task execution. Notably, in the OSWorld\nbenchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15\nsteps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld,\nUI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several\nkey innovations: (1) Enhanced Perception: leveraging a large-scale dataset of\nGUI screenshots for context-aware understanding of UI elements and precise\ncaptioning; (2) Unified Action Modeling, which standardizes actions into a\nunified space across platforms and achieves precise grounding and interaction\nthrough large-scale action traces; (3) System-2 Reasoning, which incorporates\ndeliberate reasoning into multi-step decision making, involving multiple\nreasoning patterns such as task decomposition, reflection thinking, milestone\nrecognition, etc. (4) Iterative Training with Reflective Online Traces, which\naddresses the data bottleneck by automatically collecting, filtering, and\nreflectively refining new interaction traces on hundreds of virtual machines.\nThrough iterative training and reflection tuning, UI-TARS continuously learns\nfrom its mistakes and adapts to unforeseen situations with minimal human\nintervention. We also analyze the evolution path of GUI agents to guide the\nfurther development of this domain.\n", "link": "http://arxiv.org/abs/2501.12326v1", "date": "2025-01-21", "relevancy": 2.0664, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5333}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5316}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UI-TARS%3A%20Pioneering%20Automated%20GUI%20Interaction%20with%20Native%20Agents&body=Title%3A%20UI-TARS%3A%20Pioneering%20Automated%20GUI%20Interaction%20with%20Native%20Agents%0AAuthor%3A%20Yujia%20Qin%20and%20Yining%20Ye%20and%20Junjie%20Fang%20and%20Haoming%20Wang%20and%20Shihao%20Liang%20and%20Shizuo%20Tian%20and%20Junda%20Zhang%20and%20Jiahao%20Li%20and%20Yunxin%20Li%20and%20Shijue%20Huang%20and%20Wanjun%20Zhong%20and%20Kuanye%20Li%20and%20Jiale%20Yang%20and%20Yu%20Miao%20and%20Woyu%20Lin%20and%20Longxiang%20Liu%20and%20Xu%20Jiang%20and%20Qianli%20Ma%20and%20Jingyu%20Li%20and%20Xiaojun%20Xiao%20and%20Kai%20Cai%20and%20Chuang%20Li%20and%20Yaowei%20Zheng%20and%20Chaolin%20Jin%20and%20Chen%20Li%20and%20Xiao%20Zhou%20and%20Minchao%20Wang%20and%20Haoli%20Chen%20and%20Zhaojian%20Li%20and%20Haihua%20Yang%20and%20Haifeng%20Liu%20and%20Feng%20Lin%20and%20Tao%20Peng%20and%20Xin%20Liu%20and%20Guang%20Shi%0AAbstract%3A%20%20%20This%20paper%20introduces%20UI-TARS%2C%20a%20native%20GUI%20agent%20model%20that%20solely%20perceives%0Athe%20screenshots%20as%20input%20and%20performs%20human-like%20interactions%20%28e.g.%2C%20keyboard%0Aand%20mouse%20operations%29.%20Unlike%20prevailing%20agent%20frameworks%20that%20depend%20on%0Aheavily%20wrapped%20commercial%20models%20%28e.g.%2C%20GPT-4o%29%20with%20expert-crafted%20prompts%0Aand%20workflows%2C%20UI-TARS%20is%20an%20end-to-end%20model%20that%20outperforms%20these%0Asophisticated%20frameworks.%20Experiments%20demonstrate%20its%20superior%20performance%3A%0AUI-TARS%20achieves%20SOTA%20performance%20in%2010%2B%20GUI%20agent%20benchmarks%20evaluating%0Aperception%2C%20grounding%2C%20and%20GUI%20task%20execution.%20Notably%2C%20in%20the%20OSWorld%0Abenchmark%2C%20UI-TARS%20achieves%20scores%20of%2024.6%20with%2050%20steps%20and%2022.7%20with%2015%0Asteps%2C%20outperforming%20Claude%20%2822.0%20and%2014.9%20respectively%29.%20In%20AndroidWorld%2C%0AUI-TARS%20achieves%2046.6%2C%20surpassing%20GPT-4o%20%2834.5%29.%20UI-TARS%20incorporates%20several%0Akey%20innovations%3A%20%281%29%20Enhanced%20Perception%3A%20leveraging%20a%20large-scale%20dataset%20of%0AGUI%20screenshots%20for%20context-aware%20understanding%20of%20UI%20elements%20and%20precise%0Acaptioning%3B%20%282%29%20Unified%20Action%20Modeling%2C%20which%20standardizes%20actions%20into%20a%0Aunified%20space%20across%20platforms%20and%20achieves%20precise%20grounding%20and%20interaction%0Athrough%20large-scale%20action%20traces%3B%20%283%29%20System-2%20Reasoning%2C%20which%20incorporates%0Adeliberate%20reasoning%20into%20multi-step%20decision%20making%2C%20involving%20multiple%0Areasoning%20patterns%20such%20as%20task%20decomposition%2C%20reflection%20thinking%2C%20milestone%0Arecognition%2C%20etc.%20%284%29%20Iterative%20Training%20with%20Reflective%20Online%20Traces%2C%20which%0Aaddresses%20the%20data%20bottleneck%20by%20automatically%20collecting%2C%20filtering%2C%20and%0Areflectively%20refining%20new%20interaction%20traces%20on%20hundreds%20of%20virtual%20machines.%0AThrough%20iterative%20training%20and%20reflection%20tuning%2C%20UI-TARS%20continuously%20learns%0Afrom%20its%20mistakes%20and%20adapts%20to%20unforeseen%20situations%20with%20minimal%20human%0Aintervention.%20We%20also%20analyze%20the%20evolution%20path%20of%20GUI%20agents%20to%20guide%20the%0Afurther%20development%20of%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUI-TARS%253A%2520Pioneering%2520Automated%2520GUI%2520Interaction%2520with%2520Native%2520Agents%26entry.906535625%3DYujia%2520Qin%2520and%2520Yining%2520Ye%2520and%2520Junjie%2520Fang%2520and%2520Haoming%2520Wang%2520and%2520Shihao%2520Liang%2520and%2520Shizuo%2520Tian%2520and%2520Junda%2520Zhang%2520and%2520Jiahao%2520Li%2520and%2520Yunxin%2520Li%2520and%2520Shijue%2520Huang%2520and%2520Wanjun%2520Zhong%2520and%2520Kuanye%2520Li%2520and%2520Jiale%2520Yang%2520and%2520Yu%2520Miao%2520and%2520Woyu%2520Lin%2520and%2520Longxiang%2520Liu%2520and%2520Xu%2520Jiang%2520and%2520Qianli%2520Ma%2520and%2520Jingyu%2520Li%2520and%2520Xiaojun%2520Xiao%2520and%2520Kai%2520Cai%2520and%2520Chuang%2520Li%2520and%2520Yaowei%2520Zheng%2520and%2520Chaolin%2520Jin%2520and%2520Chen%2520Li%2520and%2520Xiao%2520Zhou%2520and%2520Minchao%2520Wang%2520and%2520Haoli%2520Chen%2520and%2520Zhaojian%2520Li%2520and%2520Haihua%2520Yang%2520and%2520Haifeng%2520Liu%2520and%2520Feng%2520Lin%2520and%2520Tao%2520Peng%2520and%2520Xin%2520Liu%2520and%2520Guang%2520Shi%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520UI-TARS%252C%2520a%2520native%2520GUI%2520agent%2520model%2520that%2520solely%2520perceives%250Athe%2520screenshots%2520as%2520input%2520and%2520performs%2520human-like%2520interactions%2520%2528e.g.%252C%2520keyboard%250Aand%2520mouse%2520operations%2529.%2520Unlike%2520prevailing%2520agent%2520frameworks%2520that%2520depend%2520on%250Aheavily%2520wrapped%2520commercial%2520models%2520%2528e.g.%252C%2520GPT-4o%2529%2520with%2520expert-crafted%2520prompts%250Aand%2520workflows%252C%2520UI-TARS%2520is%2520an%2520end-to-end%2520model%2520that%2520outperforms%2520these%250Asophisticated%2520frameworks.%2520Experiments%2520demonstrate%2520its%2520superior%2520performance%253A%250AUI-TARS%2520achieves%2520SOTA%2520performance%2520in%252010%252B%2520GUI%2520agent%2520benchmarks%2520evaluating%250Aperception%252C%2520grounding%252C%2520and%2520GUI%2520task%2520execution.%2520Notably%252C%2520in%2520the%2520OSWorld%250Abenchmark%252C%2520UI-TARS%2520achieves%2520scores%2520of%252024.6%2520with%252050%2520steps%2520and%252022.7%2520with%252015%250Asteps%252C%2520outperforming%2520Claude%2520%252822.0%2520and%252014.9%2520respectively%2529.%2520In%2520AndroidWorld%252C%250AUI-TARS%2520achieves%252046.6%252C%2520surpassing%2520GPT-4o%2520%252834.5%2529.%2520UI-TARS%2520incorporates%2520several%250Akey%2520innovations%253A%2520%25281%2529%2520Enhanced%2520Perception%253A%2520leveraging%2520a%2520large-scale%2520dataset%2520of%250AGUI%2520screenshots%2520for%2520context-aware%2520understanding%2520of%2520UI%2520elements%2520and%2520precise%250Acaptioning%253B%2520%25282%2529%2520Unified%2520Action%2520Modeling%252C%2520which%2520standardizes%2520actions%2520into%2520a%250Aunified%2520space%2520across%2520platforms%2520and%2520achieves%2520precise%2520grounding%2520and%2520interaction%250Athrough%2520large-scale%2520action%2520traces%253B%2520%25283%2529%2520System-2%2520Reasoning%252C%2520which%2520incorporates%250Adeliberate%2520reasoning%2520into%2520multi-step%2520decision%2520making%252C%2520involving%2520multiple%250Areasoning%2520patterns%2520such%2520as%2520task%2520decomposition%252C%2520reflection%2520thinking%252C%2520milestone%250Arecognition%252C%2520etc.%2520%25284%2529%2520Iterative%2520Training%2520with%2520Reflective%2520Online%2520Traces%252C%2520which%250Aaddresses%2520the%2520data%2520bottleneck%2520by%2520automatically%2520collecting%252C%2520filtering%252C%2520and%250Areflectively%2520refining%2520new%2520interaction%2520traces%2520on%2520hundreds%2520of%2520virtual%2520machines.%250AThrough%2520iterative%2520training%2520and%2520reflection%2520tuning%252C%2520UI-TARS%2520continuously%2520learns%250Afrom%2520its%2520mistakes%2520and%2520adapts%2520to%2520unforeseen%2520situations%2520with%2520minimal%2520human%250Aintervention.%2520We%2520also%2520analyze%2520the%2520evolution%2520path%2520of%2520GUI%2520agents%2520to%2520guide%2520the%250Afurther%2520development%2520of%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UI-TARS%3A%20Pioneering%20Automated%20GUI%20Interaction%20with%20Native%20Agents&entry.906535625=Yujia%20Qin%20and%20Yining%20Ye%20and%20Junjie%20Fang%20and%20Haoming%20Wang%20and%20Shihao%20Liang%20and%20Shizuo%20Tian%20and%20Junda%20Zhang%20and%20Jiahao%20Li%20and%20Yunxin%20Li%20and%20Shijue%20Huang%20and%20Wanjun%20Zhong%20and%20Kuanye%20Li%20and%20Jiale%20Yang%20and%20Yu%20Miao%20and%20Woyu%20Lin%20and%20Longxiang%20Liu%20and%20Xu%20Jiang%20and%20Qianli%20Ma%20and%20Jingyu%20Li%20and%20Xiaojun%20Xiao%20and%20Kai%20Cai%20and%20Chuang%20Li%20and%20Yaowei%20Zheng%20and%20Chaolin%20Jin%20and%20Chen%20Li%20and%20Xiao%20Zhou%20and%20Minchao%20Wang%20and%20Haoli%20Chen%20and%20Zhaojian%20Li%20and%20Haihua%20Yang%20and%20Haifeng%20Liu%20and%20Feng%20Lin%20and%20Tao%20Peng%20and%20Xin%20Liu%20and%20Guang%20Shi&entry.1292438233=%20%20This%20paper%20introduces%20UI-TARS%2C%20a%20native%20GUI%20agent%20model%20that%20solely%20perceives%0Athe%20screenshots%20as%20input%20and%20performs%20human-like%20interactions%20%28e.g.%2C%20keyboard%0Aand%20mouse%20operations%29.%20Unlike%20prevailing%20agent%20frameworks%20that%20depend%20on%0Aheavily%20wrapped%20commercial%20models%20%28e.g.%2C%20GPT-4o%29%20with%20expert-crafted%20prompts%0Aand%20workflows%2C%20UI-TARS%20is%20an%20end-to-end%20model%20that%20outperforms%20these%0Asophisticated%20frameworks.%20Experiments%20demonstrate%20its%20superior%20performance%3A%0AUI-TARS%20achieves%20SOTA%20performance%20in%2010%2B%20GUI%20agent%20benchmarks%20evaluating%0Aperception%2C%20grounding%2C%20and%20GUI%20task%20execution.%20Notably%2C%20in%20the%20OSWorld%0Abenchmark%2C%20UI-TARS%20achieves%20scores%20of%2024.6%20with%2050%20steps%20and%2022.7%20with%2015%0Asteps%2C%20outperforming%20Claude%20%2822.0%20and%2014.9%20respectively%29.%20In%20AndroidWorld%2C%0AUI-TARS%20achieves%2046.6%2C%20surpassing%20GPT-4o%20%2834.5%29.%20UI-TARS%20incorporates%20several%0Akey%20innovations%3A%20%281%29%20Enhanced%20Perception%3A%20leveraging%20a%20large-scale%20dataset%20of%0AGUI%20screenshots%20for%20context-aware%20understanding%20of%20UI%20elements%20and%20precise%0Acaptioning%3B%20%282%29%20Unified%20Action%20Modeling%2C%20which%20standardizes%20actions%20into%20a%0Aunified%20space%20across%20platforms%20and%20achieves%20precise%20grounding%20and%20interaction%0Athrough%20large-scale%20action%20traces%3B%20%283%29%20System-2%20Reasoning%2C%20which%20incorporates%0Adeliberate%20reasoning%20into%20multi-step%20decision%20making%2C%20involving%20multiple%0Areasoning%20patterns%20such%20as%20task%20decomposition%2C%20reflection%20thinking%2C%20milestone%0Arecognition%2C%20etc.%20%284%29%20Iterative%20Training%20with%20Reflective%20Online%20Traces%2C%20which%0Aaddresses%20the%20data%20bottleneck%20by%20automatically%20collecting%2C%20filtering%2C%20and%0Areflectively%20refining%20new%20interaction%20traces%20on%20hundreds%20of%20virtual%20machines.%0AThrough%20iterative%20training%20and%20reflection%20tuning%2C%20UI-TARS%20continuously%20learns%0Afrom%20its%20mistakes%20and%20adapts%20to%20unforeseen%20situations%20with%20minimal%20human%0Aintervention.%20We%20also%20analyze%20the%20evolution%20path%20of%20GUI%20agents%20to%20guide%20the%0Afurther%20development%20of%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12326v1&entry.124074799=Read"},
{"title": "Treatment-aware Diffusion Probabilistic Model for Longitudinal MRI\n  Generation and Diffuse Glioma Growth Prediction", "author": "Qinghui Liu and Elies Fuster-Garcia and Ivar Thokle Hovden and Bradley J MacIntosh and Edvard Gr\u00f8dem and Petter Brandal and Carles Lopez-Mateu and Donatas Sederevicius and Karoline Skogen and Till Schellhorn and Atle Bj\u00f8rnerud and Kyrre Eeg Emblem", "abstract": "  Diffuse gliomas are malignant brain tumors that grow widespread through the\nbrain. The complex interactions between neoplastic cells and normal tissue, as\nwell as the treatment-induced changes often encountered, make glioma tumor\ngrowth modeling challenging. In this paper, we present a novel end-to-end\nnetwork capable of future predictions of tumor masks and multi-parametric\nmagnetic resonance images (MRI) of how the tumor will look at any future time\npoints for different treatment plans. Our approach is based on cutting-edge\ndiffusion probabilistic models and deep-segmentation neural networks. We\nincluded sequential multi-parametric MRI and treatment information as\nconditioning inputs to guide the generative diffusion process as well as a\njoint segmentation process. This allows for tumor growth estimates and\nrealistic MRI generation at any given treatment and time point. We trained the\nmodel using real-world postoperative longitudinal MRI data with glioma tumor\ngrowth trajectories represented as tumor segmentation maps over time. The model\ndemonstrates promising performance across various tasks, including generating\nhigh-quality multi-parametric MRI with tumor masks, performing time-series\ntumor segmentations, and providing uncertainty estimates. Combined with the\ntreatment-aware generated MRI, the tumor growth predictions with uncertainty\nestimates can provide useful information for clinical decision-making.\n", "link": "http://arxiv.org/abs/2309.05406v4", "date": "2025-01-21", "relevancy": 2.0581, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.559}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5056}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Treatment-aware%20Diffusion%20Probabilistic%20Model%20for%20Longitudinal%20MRI%0A%20%20Generation%20and%20Diffuse%20Glioma%20Growth%20Prediction&body=Title%3A%20Treatment-aware%20Diffusion%20Probabilistic%20Model%20for%20Longitudinal%20MRI%0A%20%20Generation%20and%20Diffuse%20Glioma%20Growth%20Prediction%0AAuthor%3A%20Qinghui%20Liu%20and%20Elies%20Fuster-Garcia%20and%20Ivar%20Thokle%20Hovden%20and%20Bradley%20J%20MacIntosh%20and%20Edvard%20Gr%C3%B8dem%20and%20Petter%20Brandal%20and%20Carles%20Lopez-Mateu%20and%20Donatas%20Sederevicius%20and%20Karoline%20Skogen%20and%20Till%20Schellhorn%20and%20Atle%20Bj%C3%B8rnerud%20and%20Kyrre%20Eeg%20Emblem%0AAbstract%3A%20%20%20Diffuse%20gliomas%20are%20malignant%20brain%20tumors%20that%20grow%20widespread%20through%20the%0Abrain.%20The%20complex%20interactions%20between%20neoplastic%20cells%20and%20normal%20tissue%2C%20as%0Awell%20as%20the%20treatment-induced%20changes%20often%20encountered%2C%20make%20glioma%20tumor%0Agrowth%20modeling%20challenging.%20In%20this%20paper%2C%20we%20present%20a%20novel%20end-to-end%0Anetwork%20capable%20of%20future%20predictions%20of%20tumor%20masks%20and%20multi-parametric%0Amagnetic%20resonance%20images%20%28MRI%29%20of%20how%20the%20tumor%20will%20look%20at%20any%20future%20time%0Apoints%20for%20different%20treatment%20plans.%20Our%20approach%20is%20based%20on%20cutting-edge%0Adiffusion%20probabilistic%20models%20and%20deep-segmentation%20neural%20networks.%20We%0Aincluded%20sequential%20multi-parametric%20MRI%20and%20treatment%20information%20as%0Aconditioning%20inputs%20to%20guide%20the%20generative%20diffusion%20process%20as%20well%20as%20a%0Ajoint%20segmentation%20process.%20This%20allows%20for%20tumor%20growth%20estimates%20and%0Arealistic%20MRI%20generation%20at%20any%20given%20treatment%20and%20time%20point.%20We%20trained%20the%0Amodel%20using%20real-world%20postoperative%20longitudinal%20MRI%20data%20with%20glioma%20tumor%0Agrowth%20trajectories%20represented%20as%20tumor%20segmentation%20maps%20over%20time.%20The%20model%0Ademonstrates%20promising%20performance%20across%20various%20tasks%2C%20including%20generating%0Ahigh-quality%20multi-parametric%20MRI%20with%20tumor%20masks%2C%20performing%20time-series%0Atumor%20segmentations%2C%20and%20providing%20uncertainty%20estimates.%20Combined%20with%20the%0Atreatment-aware%20generated%20MRI%2C%20the%20tumor%20growth%20predictions%20with%20uncertainty%0Aestimates%20can%20provide%20useful%20information%20for%20clinical%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05406v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTreatment-aware%2520Diffusion%2520Probabilistic%2520Model%2520for%2520Longitudinal%2520MRI%250A%2520%2520Generation%2520and%2520Diffuse%2520Glioma%2520Growth%2520Prediction%26entry.906535625%3DQinghui%2520Liu%2520and%2520Elies%2520Fuster-Garcia%2520and%2520Ivar%2520Thokle%2520Hovden%2520and%2520Bradley%2520J%2520MacIntosh%2520and%2520Edvard%2520Gr%25C3%25B8dem%2520and%2520Petter%2520Brandal%2520and%2520Carles%2520Lopez-Mateu%2520and%2520Donatas%2520Sederevicius%2520and%2520Karoline%2520Skogen%2520and%2520Till%2520Schellhorn%2520and%2520Atle%2520Bj%25C3%25B8rnerud%2520and%2520Kyrre%2520Eeg%2520Emblem%26entry.1292438233%3D%2520%2520Diffuse%2520gliomas%2520are%2520malignant%2520brain%2520tumors%2520that%2520grow%2520widespread%2520through%2520the%250Abrain.%2520The%2520complex%2520interactions%2520between%2520neoplastic%2520cells%2520and%2520normal%2520tissue%252C%2520as%250Awell%2520as%2520the%2520treatment-induced%2520changes%2520often%2520encountered%252C%2520make%2520glioma%2520tumor%250Agrowth%2520modeling%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520end-to-end%250Anetwork%2520capable%2520of%2520future%2520predictions%2520of%2520tumor%2520masks%2520and%2520multi-parametric%250Amagnetic%2520resonance%2520images%2520%2528MRI%2529%2520of%2520how%2520the%2520tumor%2520will%2520look%2520at%2520any%2520future%2520time%250Apoints%2520for%2520different%2520treatment%2520plans.%2520Our%2520approach%2520is%2520based%2520on%2520cutting-edge%250Adiffusion%2520probabilistic%2520models%2520and%2520deep-segmentation%2520neural%2520networks.%2520We%250Aincluded%2520sequential%2520multi-parametric%2520MRI%2520and%2520treatment%2520information%2520as%250Aconditioning%2520inputs%2520to%2520guide%2520the%2520generative%2520diffusion%2520process%2520as%2520well%2520as%2520a%250Ajoint%2520segmentation%2520process.%2520This%2520allows%2520for%2520tumor%2520growth%2520estimates%2520and%250Arealistic%2520MRI%2520generation%2520at%2520any%2520given%2520treatment%2520and%2520time%2520point.%2520We%2520trained%2520the%250Amodel%2520using%2520real-world%2520postoperative%2520longitudinal%2520MRI%2520data%2520with%2520glioma%2520tumor%250Agrowth%2520trajectories%2520represented%2520as%2520tumor%2520segmentation%2520maps%2520over%2520time.%2520The%2520model%250Ademonstrates%2520promising%2520performance%2520across%2520various%2520tasks%252C%2520including%2520generating%250Ahigh-quality%2520multi-parametric%2520MRI%2520with%2520tumor%2520masks%252C%2520performing%2520time-series%250Atumor%2520segmentations%252C%2520and%2520providing%2520uncertainty%2520estimates.%2520Combined%2520with%2520the%250Atreatment-aware%2520generated%2520MRI%252C%2520the%2520tumor%2520growth%2520predictions%2520with%2520uncertainty%250Aestimates%2520can%2520provide%2520useful%2520information%2520for%2520clinical%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.05406v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Treatment-aware%20Diffusion%20Probabilistic%20Model%20for%20Longitudinal%20MRI%0A%20%20Generation%20and%20Diffuse%20Glioma%20Growth%20Prediction&entry.906535625=Qinghui%20Liu%20and%20Elies%20Fuster-Garcia%20and%20Ivar%20Thokle%20Hovden%20and%20Bradley%20J%20MacIntosh%20and%20Edvard%20Gr%C3%B8dem%20and%20Petter%20Brandal%20and%20Carles%20Lopez-Mateu%20and%20Donatas%20Sederevicius%20and%20Karoline%20Skogen%20and%20Till%20Schellhorn%20and%20Atle%20Bj%C3%B8rnerud%20and%20Kyrre%20Eeg%20Emblem&entry.1292438233=%20%20Diffuse%20gliomas%20are%20malignant%20brain%20tumors%20that%20grow%20widespread%20through%20the%0Abrain.%20The%20complex%20interactions%20between%20neoplastic%20cells%20and%20normal%20tissue%2C%20as%0Awell%20as%20the%20treatment-induced%20changes%20often%20encountered%2C%20make%20glioma%20tumor%0Agrowth%20modeling%20challenging.%20In%20this%20paper%2C%20we%20present%20a%20novel%20end-to-end%0Anetwork%20capable%20of%20future%20predictions%20of%20tumor%20masks%20and%20multi-parametric%0Amagnetic%20resonance%20images%20%28MRI%29%20of%20how%20the%20tumor%20will%20look%20at%20any%20future%20time%0Apoints%20for%20different%20treatment%20plans.%20Our%20approach%20is%20based%20on%20cutting-edge%0Adiffusion%20probabilistic%20models%20and%20deep-segmentation%20neural%20networks.%20We%0Aincluded%20sequential%20multi-parametric%20MRI%20and%20treatment%20information%20as%0Aconditioning%20inputs%20to%20guide%20the%20generative%20diffusion%20process%20as%20well%20as%20a%0Ajoint%20segmentation%20process.%20This%20allows%20for%20tumor%20growth%20estimates%20and%0Arealistic%20MRI%20generation%20at%20any%20given%20treatment%20and%20time%20point.%20We%20trained%20the%0Amodel%20using%20real-world%20postoperative%20longitudinal%20MRI%20data%20with%20glioma%20tumor%0Agrowth%20trajectories%20represented%20as%20tumor%20segmentation%20maps%20over%20time.%20The%20model%0Ademonstrates%20promising%20performance%20across%20various%20tasks%2C%20including%20generating%0Ahigh-quality%20multi-parametric%20MRI%20with%20tumor%20masks%2C%20performing%20time-series%0Atumor%20segmentations%2C%20and%20providing%20uncertainty%20estimates.%20Combined%20with%20the%0Atreatment-aware%20generated%20MRI%2C%20the%20tumor%20growth%20predictions%20with%20uncertainty%0Aestimates%20can%20provide%20useful%20information%20for%20clinical%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05406v4&entry.124074799=Read"},
{"title": "The Gap Between Principle and Practice of Lossy Image Coding", "author": "Haotian Zhang and Dong Liu", "abstract": "  Lossy image coding is the art of computing that is principally bounded by the\nimage's rate-distortion function. This bound, though never accurately\ncharacterized, has been approached practically via deep learning technologies\nin recent years. Indeed, learned image coding schemes allow direct optimization\nof the joint rate-distortion cost, thereby outperforming the handcrafted image\ncoding schemes by a large margin. Still, it is observed that there is room for\nfurther improvement in the rate-distortion performance of learned image coding.\nIn this article, we identify the gap between the ideal rate-distortion function\nforecasted by Shannon's information theory and the empirical rate-distortion\nfunction achieved by the state-of-the-art learned image coding schemes,\nrevealing that the gap is incurred by five different effects: modeling effect,\napproximation effect, amortization effect, digitization effect, and asymptotic\neffect. We design simulations and experiments to quantitively evaluate the last\nthree effects, which demonstrates the high potential of future lossy image\ncoding technologies.\n", "link": "http://arxiv.org/abs/2501.12330v1", "date": "2025-01-21", "relevancy": 2.0575, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5187}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5126}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Gap%20Between%20Principle%20and%20Practice%20of%20Lossy%20Image%20Coding&body=Title%3A%20The%20Gap%20Between%20Principle%20and%20Practice%20of%20Lossy%20Image%20Coding%0AAuthor%3A%20Haotian%20Zhang%20and%20Dong%20Liu%0AAbstract%3A%20%20%20Lossy%20image%20coding%20is%20the%20art%20of%20computing%20that%20is%20principally%20bounded%20by%20the%0Aimage%27s%20rate-distortion%20function.%20This%20bound%2C%20though%20never%20accurately%0Acharacterized%2C%20has%20been%20approached%20practically%20via%20deep%20learning%20technologies%0Ain%20recent%20years.%20Indeed%2C%20learned%20image%20coding%20schemes%20allow%20direct%20optimization%0Aof%20the%20joint%20rate-distortion%20cost%2C%20thereby%20outperforming%20the%20handcrafted%20image%0Acoding%20schemes%20by%20a%20large%20margin.%20Still%2C%20it%20is%20observed%20that%20there%20is%20room%20for%0Afurther%20improvement%20in%20the%20rate-distortion%20performance%20of%20learned%20image%20coding.%0AIn%20this%20article%2C%20we%20identify%20the%20gap%20between%20the%20ideal%20rate-distortion%20function%0Aforecasted%20by%20Shannon%27s%20information%20theory%20and%20the%20empirical%20rate-distortion%0Afunction%20achieved%20by%20the%20state-of-the-art%20learned%20image%20coding%20schemes%2C%0Arevealing%20that%20the%20gap%20is%20incurred%20by%20five%20different%20effects%3A%20modeling%20effect%2C%0Aapproximation%20effect%2C%20amortization%20effect%2C%20digitization%20effect%2C%20and%20asymptotic%0Aeffect.%20We%20design%20simulations%20and%20experiments%20to%20quantitively%20evaluate%20the%20last%0Athree%20effects%2C%20which%20demonstrates%20the%20high%20potential%20of%20future%20lossy%20image%0Acoding%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Gap%2520Between%2520Principle%2520and%2520Practice%2520of%2520Lossy%2520Image%2520Coding%26entry.906535625%3DHaotian%2520Zhang%2520and%2520Dong%2520Liu%26entry.1292438233%3D%2520%2520Lossy%2520image%2520coding%2520is%2520the%2520art%2520of%2520computing%2520that%2520is%2520principally%2520bounded%2520by%2520the%250Aimage%2527s%2520rate-distortion%2520function.%2520This%2520bound%252C%2520though%2520never%2520accurately%250Acharacterized%252C%2520has%2520been%2520approached%2520practically%2520via%2520deep%2520learning%2520technologies%250Ain%2520recent%2520years.%2520Indeed%252C%2520learned%2520image%2520coding%2520schemes%2520allow%2520direct%2520optimization%250Aof%2520the%2520joint%2520rate-distortion%2520cost%252C%2520thereby%2520outperforming%2520the%2520handcrafted%2520image%250Acoding%2520schemes%2520by%2520a%2520large%2520margin.%2520Still%252C%2520it%2520is%2520observed%2520that%2520there%2520is%2520room%2520for%250Afurther%2520improvement%2520in%2520the%2520rate-distortion%2520performance%2520of%2520learned%2520image%2520coding.%250AIn%2520this%2520article%252C%2520we%2520identify%2520the%2520gap%2520between%2520the%2520ideal%2520rate-distortion%2520function%250Aforecasted%2520by%2520Shannon%2527s%2520information%2520theory%2520and%2520the%2520empirical%2520rate-distortion%250Afunction%2520achieved%2520by%2520the%2520state-of-the-art%2520learned%2520image%2520coding%2520schemes%252C%250Arevealing%2520that%2520the%2520gap%2520is%2520incurred%2520by%2520five%2520different%2520effects%253A%2520modeling%2520effect%252C%250Aapproximation%2520effect%252C%2520amortization%2520effect%252C%2520digitization%2520effect%252C%2520and%2520asymptotic%250Aeffect.%2520We%2520design%2520simulations%2520and%2520experiments%2520to%2520quantitively%2520evaluate%2520the%2520last%250Athree%2520effects%252C%2520which%2520demonstrates%2520the%2520high%2520potential%2520of%2520future%2520lossy%2520image%250Acoding%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Gap%20Between%20Principle%20and%20Practice%20of%20Lossy%20Image%20Coding&entry.906535625=Haotian%20Zhang%20and%20Dong%20Liu&entry.1292438233=%20%20Lossy%20image%20coding%20is%20the%20art%20of%20computing%20that%20is%20principally%20bounded%20by%20the%0Aimage%27s%20rate-distortion%20function.%20This%20bound%2C%20though%20never%20accurately%0Acharacterized%2C%20has%20been%20approached%20practically%20via%20deep%20learning%20technologies%0Ain%20recent%20years.%20Indeed%2C%20learned%20image%20coding%20schemes%20allow%20direct%20optimization%0Aof%20the%20joint%20rate-distortion%20cost%2C%20thereby%20outperforming%20the%20handcrafted%20image%0Acoding%20schemes%20by%20a%20large%20margin.%20Still%2C%20it%20is%20observed%20that%20there%20is%20room%20for%0Afurther%20improvement%20in%20the%20rate-distortion%20performance%20of%20learned%20image%20coding.%0AIn%20this%20article%2C%20we%20identify%20the%20gap%20between%20the%20ideal%20rate-distortion%20function%0Aforecasted%20by%20Shannon%27s%20information%20theory%20and%20the%20empirical%20rate-distortion%0Afunction%20achieved%20by%20the%20state-of-the-art%20learned%20image%20coding%20schemes%2C%0Arevealing%20that%20the%20gap%20is%20incurred%20by%20five%20different%20effects%3A%20modeling%20effect%2C%0Aapproximation%20effect%2C%20amortization%20effect%2C%20digitization%20effect%2C%20and%20asymptotic%0Aeffect.%20We%20design%20simulations%20and%20experiments%20to%20quantitively%20evaluate%20the%20last%0Athree%20effects%2C%20which%20demonstrates%20the%20high%20potential%20of%20future%20lossy%20image%0Acoding%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12330v1&entry.124074799=Read"},
{"title": "Harnessing Generative Pre-Trained Transformer for Datacenter Packet\n  Trace Generation", "author": "Chen Griner", "abstract": "  Today, the rapid growth of applications reliant on datacenters calls for new\nadvancements to meet the increasing traffic and computational demands. Traffic\ntraces from datacenters are essential for further development and optimization\nof future datacenters. However, traces are rarely released to the public.\nResearchers often use simplified mathematical models that lack the depth needed\nto recreate intricate traffic patterns and, thus, miss optimization\nopportunities found in realistic traffic. In this preliminary work, we\nintroduce DTG-GPT, a packet-level Datacenter Traffic Generator (DTG), based on\nthe generative pre-trained transformer (GPT) architecture used by many\nstate-of-the-art large language models. We train our model on a small set of\navailable traffic traces from different domains and offer a simple methodology\nto evaluate the fidelity of the generated traces to their original\ncounterparts. We show that DTG-GPT can synthesize novel traces that mimic the\nspatiotemporal patterns found in real traffic traces. We further demonstrate\nthat DTG-GPT can generate traces for networks of different scales while\nmaintaining fidelity. Our findings indicate the potential that, in the future,\nsimilar models to DTG-GPT will allow datacenter operators to release traffic\ninformation to the research community via trained GPT models.\n", "link": "http://arxiv.org/abs/2501.12033v1", "date": "2025-01-21", "relevancy": 2.0489, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.561}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5054}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Generative%20Pre-Trained%20Transformer%20for%20Datacenter%20Packet%0A%20%20Trace%20Generation&body=Title%3A%20Harnessing%20Generative%20Pre-Trained%20Transformer%20for%20Datacenter%20Packet%0A%20%20Trace%20Generation%0AAuthor%3A%20Chen%20Griner%0AAbstract%3A%20%20%20Today%2C%20the%20rapid%20growth%20of%20applications%20reliant%20on%20datacenters%20calls%20for%20new%0Aadvancements%20to%20meet%20the%20increasing%20traffic%20and%20computational%20demands.%20Traffic%0Atraces%20from%20datacenters%20are%20essential%20for%20further%20development%20and%20optimization%0Aof%20future%20datacenters.%20However%2C%20traces%20are%20rarely%20released%20to%20the%20public.%0AResearchers%20often%20use%20simplified%20mathematical%20models%20that%20lack%20the%20depth%20needed%0Ato%20recreate%20intricate%20traffic%20patterns%20and%2C%20thus%2C%20miss%20optimization%0Aopportunities%20found%20in%20realistic%20traffic.%20In%20this%20preliminary%20work%2C%20we%0Aintroduce%20DTG-GPT%2C%20a%20packet-level%20Datacenter%20Traffic%20Generator%20%28DTG%29%2C%20based%20on%0Athe%20generative%20pre-trained%20transformer%20%28GPT%29%20architecture%20used%20by%20many%0Astate-of-the-art%20large%20language%20models.%20We%20train%20our%20model%20on%20a%20small%20set%20of%0Aavailable%20traffic%20traces%20from%20different%20domains%20and%20offer%20a%20simple%20methodology%0Ato%20evaluate%20the%20fidelity%20of%20the%20generated%20traces%20to%20their%20original%0Acounterparts.%20We%20show%20that%20DTG-GPT%20can%20synthesize%20novel%20traces%20that%20mimic%20the%0Aspatiotemporal%20patterns%20found%20in%20real%20traffic%20traces.%20We%20further%20demonstrate%0Athat%20DTG-GPT%20can%20generate%20traces%20for%20networks%20of%20different%20scales%20while%0Amaintaining%20fidelity.%20Our%20findings%20indicate%20the%20potential%20that%2C%20in%20the%20future%2C%0Asimilar%20models%20to%20DTG-GPT%20will%20allow%20datacenter%20operators%20to%20release%20traffic%0Ainformation%20to%20the%20research%20community%20via%20trained%20GPT%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Generative%2520Pre-Trained%2520Transformer%2520for%2520Datacenter%2520Packet%250A%2520%2520Trace%2520Generation%26entry.906535625%3DChen%2520Griner%26entry.1292438233%3D%2520%2520Today%252C%2520the%2520rapid%2520growth%2520of%2520applications%2520reliant%2520on%2520datacenters%2520calls%2520for%2520new%250Aadvancements%2520to%2520meet%2520the%2520increasing%2520traffic%2520and%2520computational%2520demands.%2520Traffic%250Atraces%2520from%2520datacenters%2520are%2520essential%2520for%2520further%2520development%2520and%2520optimization%250Aof%2520future%2520datacenters.%2520However%252C%2520traces%2520are%2520rarely%2520released%2520to%2520the%2520public.%250AResearchers%2520often%2520use%2520simplified%2520mathematical%2520models%2520that%2520lack%2520the%2520depth%2520needed%250Ato%2520recreate%2520intricate%2520traffic%2520patterns%2520and%252C%2520thus%252C%2520miss%2520optimization%250Aopportunities%2520found%2520in%2520realistic%2520traffic.%2520In%2520this%2520preliminary%2520work%252C%2520we%250Aintroduce%2520DTG-GPT%252C%2520a%2520packet-level%2520Datacenter%2520Traffic%2520Generator%2520%2528DTG%2529%252C%2520based%2520on%250Athe%2520generative%2520pre-trained%2520transformer%2520%2528GPT%2529%2520architecture%2520used%2520by%2520many%250Astate-of-the-art%2520large%2520language%2520models.%2520We%2520train%2520our%2520model%2520on%2520a%2520small%2520set%2520of%250Aavailable%2520traffic%2520traces%2520from%2520different%2520domains%2520and%2520offer%2520a%2520simple%2520methodology%250Ato%2520evaluate%2520the%2520fidelity%2520of%2520the%2520generated%2520traces%2520to%2520their%2520original%250Acounterparts.%2520We%2520show%2520that%2520DTG-GPT%2520can%2520synthesize%2520novel%2520traces%2520that%2520mimic%2520the%250Aspatiotemporal%2520patterns%2520found%2520in%2520real%2520traffic%2520traces.%2520We%2520further%2520demonstrate%250Athat%2520DTG-GPT%2520can%2520generate%2520traces%2520for%2520networks%2520of%2520different%2520scales%2520while%250Amaintaining%2520fidelity.%2520Our%2520findings%2520indicate%2520the%2520potential%2520that%252C%2520in%2520the%2520future%252C%250Asimilar%2520models%2520to%2520DTG-GPT%2520will%2520allow%2520datacenter%2520operators%2520to%2520release%2520traffic%250Ainformation%2520to%2520the%2520research%2520community%2520via%2520trained%2520GPT%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Generative%20Pre-Trained%20Transformer%20for%20Datacenter%20Packet%0A%20%20Trace%20Generation&entry.906535625=Chen%20Griner&entry.1292438233=%20%20Today%2C%20the%20rapid%20growth%20of%20applications%20reliant%20on%20datacenters%20calls%20for%20new%0Aadvancements%20to%20meet%20the%20increasing%20traffic%20and%20computational%20demands.%20Traffic%0Atraces%20from%20datacenters%20are%20essential%20for%20further%20development%20and%20optimization%0Aof%20future%20datacenters.%20However%2C%20traces%20are%20rarely%20released%20to%20the%20public.%0AResearchers%20often%20use%20simplified%20mathematical%20models%20that%20lack%20the%20depth%20needed%0Ato%20recreate%20intricate%20traffic%20patterns%20and%2C%20thus%2C%20miss%20optimization%0Aopportunities%20found%20in%20realistic%20traffic.%20In%20this%20preliminary%20work%2C%20we%0Aintroduce%20DTG-GPT%2C%20a%20packet-level%20Datacenter%20Traffic%20Generator%20%28DTG%29%2C%20based%20on%0Athe%20generative%20pre-trained%20transformer%20%28GPT%29%20architecture%20used%20by%20many%0Astate-of-the-art%20large%20language%20models.%20We%20train%20our%20model%20on%20a%20small%20set%20of%0Aavailable%20traffic%20traces%20from%20different%20domains%20and%20offer%20a%20simple%20methodology%0Ato%20evaluate%20the%20fidelity%20of%20the%20generated%20traces%20to%20their%20original%0Acounterparts.%20We%20show%20that%20DTG-GPT%20can%20synthesize%20novel%20traces%20that%20mimic%20the%0Aspatiotemporal%20patterns%20found%20in%20real%20traffic%20traces.%20We%20further%20demonstrate%0Athat%20DTG-GPT%20can%20generate%20traces%20for%20networks%20of%20different%20scales%20while%0Amaintaining%20fidelity.%20Our%20findings%20indicate%20the%20potential%20that%2C%20in%20the%20future%2C%0Asimilar%20models%20to%20DTG-GPT%20will%20allow%20datacenter%20operators%20to%20release%20traffic%0Ainformation%20to%20the%20research%20community%20via%20trained%20GPT%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12033v1&entry.124074799=Read"},
{"title": "Robust Federated Learning Over the Air: Combating Heavy-Tailed Noise\n  with Median Anchored Clipping", "author": "Jiaxing Li and Zihan Chen and Kai Fong Ernest Chong and Bikramjit Das and Tony Q. S. Quek and Howard H. Yang", "abstract": "  Leveraging over-the-air computations for model aggregation is an effective\napproach to cope with the communication bottleneck in federated edge learning.\nBy exploiting the superposition properties of multi-access channels, this\napproach facilitates an integrated design of communication and computation,\nthereby enhancing system privacy while reducing implementation costs. However,\nthe inherent electromagnetic interference in radio channels often exhibits\nheavy-tailed distributions, giving rise to exceptionally strong noise in\nglobally aggregated gradients that can significantly deteriorate the training\nperformance. To address this issue, we propose a novel gradient clipping\nmethod, termed Median Anchored Clipping (MAC), to combat the detrimental\neffects of heavy-tailed noise. We also derive analytical expressions for the\nconvergence rate of model training with analog over-the-air federated learning\nunder MAC, which quantitatively demonstrates the effect of MAC on training\nperformance. Extensive experimental results show that the proposed MAC\nalgorithm effectively mitigates the impact of heavy-tailed noise, hence\nsubstantially enhancing system robustness.\n", "link": "http://arxiv.org/abs/2409.15100v4", "date": "2025-01-21", "relevancy": 2.0371, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5189}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5051}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Federated%20Learning%20Over%20the%20Air%3A%20Combating%20Heavy-Tailed%20Noise%0A%20%20with%20Median%20Anchored%20Clipping&body=Title%3A%20Robust%20Federated%20Learning%20Over%20the%20Air%3A%20Combating%20Heavy-Tailed%20Noise%0A%20%20with%20Median%20Anchored%20Clipping%0AAuthor%3A%20Jiaxing%20Li%20and%20Zihan%20Chen%20and%20Kai%20Fong%20Ernest%20Chong%20and%20Bikramjit%20Das%20and%20Tony%20Q.%20S.%20Quek%20and%20Howard%20H.%20Yang%0AAbstract%3A%20%20%20Leveraging%20over-the-air%20computations%20for%20model%20aggregation%20is%20an%20effective%0Aapproach%20to%20cope%20with%20the%20communication%20bottleneck%20in%20federated%20edge%20learning.%0ABy%20exploiting%20the%20superposition%20properties%20of%20multi-access%20channels%2C%20this%0Aapproach%20facilitates%20an%20integrated%20design%20of%20communication%20and%20computation%2C%0Athereby%20enhancing%20system%20privacy%20while%20reducing%20implementation%20costs.%20However%2C%0Athe%20inherent%20electromagnetic%20interference%20in%20radio%20channels%20often%20exhibits%0Aheavy-tailed%20distributions%2C%20giving%20rise%20to%20exceptionally%20strong%20noise%20in%0Aglobally%20aggregated%20gradients%20that%20can%20significantly%20deteriorate%20the%20training%0Aperformance.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20gradient%20clipping%0Amethod%2C%20termed%20Median%20Anchored%20Clipping%20%28MAC%29%2C%20to%20combat%20the%20detrimental%0Aeffects%20of%20heavy-tailed%20noise.%20We%20also%20derive%20analytical%20expressions%20for%20the%0Aconvergence%20rate%20of%20model%20training%20with%20analog%20over-the-air%20federated%20learning%0Aunder%20MAC%2C%20which%20quantitatively%20demonstrates%20the%20effect%20of%20MAC%20on%20training%0Aperformance.%20Extensive%20experimental%20results%20show%20that%20the%20proposed%20MAC%0Aalgorithm%20effectively%20mitigates%20the%20impact%20of%20heavy-tailed%20noise%2C%20hence%0Asubstantially%20enhancing%20system%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15100v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Federated%2520Learning%2520Over%2520the%2520Air%253A%2520Combating%2520Heavy-Tailed%2520Noise%250A%2520%2520with%2520Median%2520Anchored%2520Clipping%26entry.906535625%3DJiaxing%2520Li%2520and%2520Zihan%2520Chen%2520and%2520Kai%2520Fong%2520Ernest%2520Chong%2520and%2520Bikramjit%2520Das%2520and%2520Tony%2520Q.%2520S.%2520Quek%2520and%2520Howard%2520H.%2520Yang%26entry.1292438233%3D%2520%2520Leveraging%2520over-the-air%2520computations%2520for%2520model%2520aggregation%2520is%2520an%2520effective%250Aapproach%2520to%2520cope%2520with%2520the%2520communication%2520bottleneck%2520in%2520federated%2520edge%2520learning.%250ABy%2520exploiting%2520the%2520superposition%2520properties%2520of%2520multi-access%2520channels%252C%2520this%250Aapproach%2520facilitates%2520an%2520integrated%2520design%2520of%2520communication%2520and%2520computation%252C%250Athereby%2520enhancing%2520system%2520privacy%2520while%2520reducing%2520implementation%2520costs.%2520However%252C%250Athe%2520inherent%2520electromagnetic%2520interference%2520in%2520radio%2520channels%2520often%2520exhibits%250Aheavy-tailed%2520distributions%252C%2520giving%2520rise%2520to%2520exceptionally%2520strong%2520noise%2520in%250Aglobally%2520aggregated%2520gradients%2520that%2520can%2520significantly%2520deteriorate%2520the%2520training%250Aperformance.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520gradient%2520clipping%250Amethod%252C%2520termed%2520Median%2520Anchored%2520Clipping%2520%2528MAC%2529%252C%2520to%2520combat%2520the%2520detrimental%250Aeffects%2520of%2520heavy-tailed%2520noise.%2520We%2520also%2520derive%2520analytical%2520expressions%2520for%2520the%250Aconvergence%2520rate%2520of%2520model%2520training%2520with%2520analog%2520over-the-air%2520federated%2520learning%250Aunder%2520MAC%252C%2520which%2520quantitatively%2520demonstrates%2520the%2520effect%2520of%2520MAC%2520on%2520training%250Aperformance.%2520Extensive%2520experimental%2520results%2520show%2520that%2520the%2520proposed%2520MAC%250Aalgorithm%2520effectively%2520mitigates%2520the%2520impact%2520of%2520heavy-tailed%2520noise%252C%2520hence%250Asubstantially%2520enhancing%2520system%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15100v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Federated%20Learning%20Over%20the%20Air%3A%20Combating%20Heavy-Tailed%20Noise%0A%20%20with%20Median%20Anchored%20Clipping&entry.906535625=Jiaxing%20Li%20and%20Zihan%20Chen%20and%20Kai%20Fong%20Ernest%20Chong%20and%20Bikramjit%20Das%20and%20Tony%20Q.%20S.%20Quek%20and%20Howard%20H.%20Yang&entry.1292438233=%20%20Leveraging%20over-the-air%20computations%20for%20model%20aggregation%20is%20an%20effective%0Aapproach%20to%20cope%20with%20the%20communication%20bottleneck%20in%20federated%20edge%20learning.%0ABy%20exploiting%20the%20superposition%20properties%20of%20multi-access%20channels%2C%20this%0Aapproach%20facilitates%20an%20integrated%20design%20of%20communication%20and%20computation%2C%0Athereby%20enhancing%20system%20privacy%20while%20reducing%20implementation%20costs.%20However%2C%0Athe%20inherent%20electromagnetic%20interference%20in%20radio%20channels%20often%20exhibits%0Aheavy-tailed%20distributions%2C%20giving%20rise%20to%20exceptionally%20strong%20noise%20in%0Aglobally%20aggregated%20gradients%20that%20can%20significantly%20deteriorate%20the%20training%0Aperformance.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20gradient%20clipping%0Amethod%2C%20termed%20Median%20Anchored%20Clipping%20%28MAC%29%2C%20to%20combat%20the%20detrimental%0Aeffects%20of%20heavy-tailed%20noise.%20We%20also%20derive%20analytical%20expressions%20for%20the%0Aconvergence%20rate%20of%20model%20training%20with%20analog%20over-the-air%20federated%20learning%0Aunder%20MAC%2C%20which%20quantitatively%20demonstrates%20the%20effect%20of%20MAC%20on%20training%0Aperformance.%20Extensive%20experimental%20results%20show%20that%20the%20proposed%20MAC%0Aalgorithm%20effectively%20mitigates%20the%20impact%20of%20heavy-tailed%20noise%2C%20hence%0Asubstantially%20enhancing%20system%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15100v4&entry.124074799=Read"},
{"title": "Positional encoding is not the same as context: A study on positional\n  encoding for sequential recommendation", "author": "Alejo Lopez-Avila and Jinhua Du and Abbas Shimary and Ze Li", "abstract": "  The rapid growth of streaming media and e-commerce has driven advancements in\nrecommendation systems, particularly Sequential Recommendation Systems (SRS).\nThese systems employ users' interaction histories to predict future\npreferences. While recent research has focused on architectural innovations\nlike transformer blocks and feature extraction, positional encodings, crucial\nfor capturing temporal patterns, have received less attention. These encodings\nare often conflated with contextual, such as the temporal footprint, which\nprevious works tend to treat as interchangeable with positional information.\nThis paper highlights the critical distinction between temporal footprint and\npositional encodings, demonstrating that the latter offers unique relational\ncues between items, which the temporal footprint alone cannot provide. Through\nextensive experimentation on eight Amazon datasets and subsets, we assess the\nimpact of various encodings on performance metrics and training stability. We\nintroduce new positional encodings and investigate integration strategies that\nimprove both metrics and stability, surpassing state-of-the-art results at the\ntime of this work's initial preprint. Importantly, we demonstrate that\nselecting the appropriate encoding is not only key to better performance but\nalso essential for building robust, reliable SRS models.\n", "link": "http://arxiv.org/abs/2405.10436v2", "date": "2025-01-21", "relevancy": 2.0354, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5162}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Positional%20encoding%20is%20not%20the%20same%20as%20context%3A%20A%20study%20on%20positional%0A%20%20encoding%20for%20sequential%20recommendation&body=Title%3A%20Positional%20encoding%20is%20not%20the%20same%20as%20context%3A%20A%20study%20on%20positional%0A%20%20encoding%20for%20sequential%20recommendation%0AAuthor%3A%20Alejo%20Lopez-Avila%20and%20Jinhua%20Du%20and%20Abbas%20Shimary%20and%20Ze%20Li%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20streaming%20media%20and%20e-commerce%20has%20driven%20advancements%20in%0Arecommendation%20systems%2C%20particularly%20Sequential%20Recommendation%20Systems%20%28SRS%29.%0AThese%20systems%20employ%20users%27%20interaction%20histories%20to%20predict%20future%0Apreferences.%20While%20recent%20research%20has%20focused%20on%20architectural%20innovations%0Alike%20transformer%20blocks%20and%20feature%20extraction%2C%20positional%20encodings%2C%20crucial%0Afor%20capturing%20temporal%20patterns%2C%20have%20received%20less%20attention.%20These%20encodings%0Aare%20often%20conflated%20with%20contextual%2C%20such%20as%20the%20temporal%20footprint%2C%20which%0Aprevious%20works%20tend%20to%20treat%20as%20interchangeable%20with%20positional%20information.%0AThis%20paper%20highlights%20the%20critical%20distinction%20between%20temporal%20footprint%20and%0Apositional%20encodings%2C%20demonstrating%20that%20the%20latter%20offers%20unique%20relational%0Acues%20between%20items%2C%20which%20the%20temporal%20footprint%20alone%20cannot%20provide.%20Through%0Aextensive%20experimentation%20on%20eight%20Amazon%20datasets%20and%20subsets%2C%20we%20assess%20the%0Aimpact%20of%20various%20encodings%20on%20performance%20metrics%20and%20training%20stability.%20We%0Aintroduce%20new%20positional%20encodings%20and%20investigate%20integration%20strategies%20that%0Aimprove%20both%20metrics%20and%20stability%2C%20surpassing%20state-of-the-art%20results%20at%20the%0Atime%20of%20this%20work%27s%20initial%20preprint.%20Importantly%2C%20we%20demonstrate%20that%0Aselecting%20the%20appropriate%20encoding%20is%20not%20only%20key%20to%20better%20performance%20but%0Aalso%20essential%20for%20building%20robust%2C%20reliable%20SRS%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10436v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPositional%2520encoding%2520is%2520not%2520the%2520same%2520as%2520context%253A%2520A%2520study%2520on%2520positional%250A%2520%2520encoding%2520for%2520sequential%2520recommendation%26entry.906535625%3DAlejo%2520Lopez-Avila%2520and%2520Jinhua%2520Du%2520and%2520Abbas%2520Shimary%2520and%2520Ze%2520Li%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520streaming%2520media%2520and%2520e-commerce%2520has%2520driven%2520advancements%2520in%250Arecommendation%2520systems%252C%2520particularly%2520Sequential%2520Recommendation%2520Systems%2520%2528SRS%2529.%250AThese%2520systems%2520employ%2520users%2527%2520interaction%2520histories%2520to%2520predict%2520future%250Apreferences.%2520While%2520recent%2520research%2520has%2520focused%2520on%2520architectural%2520innovations%250Alike%2520transformer%2520blocks%2520and%2520feature%2520extraction%252C%2520positional%2520encodings%252C%2520crucial%250Afor%2520capturing%2520temporal%2520patterns%252C%2520have%2520received%2520less%2520attention.%2520These%2520encodings%250Aare%2520often%2520conflated%2520with%2520contextual%252C%2520such%2520as%2520the%2520temporal%2520footprint%252C%2520which%250Aprevious%2520works%2520tend%2520to%2520treat%2520as%2520interchangeable%2520with%2520positional%2520information.%250AThis%2520paper%2520highlights%2520the%2520critical%2520distinction%2520between%2520temporal%2520footprint%2520and%250Apositional%2520encodings%252C%2520demonstrating%2520that%2520the%2520latter%2520offers%2520unique%2520relational%250Acues%2520between%2520items%252C%2520which%2520the%2520temporal%2520footprint%2520alone%2520cannot%2520provide.%2520Through%250Aextensive%2520experimentation%2520on%2520eight%2520Amazon%2520datasets%2520and%2520subsets%252C%2520we%2520assess%2520the%250Aimpact%2520of%2520various%2520encodings%2520on%2520performance%2520metrics%2520and%2520training%2520stability.%2520We%250Aintroduce%2520new%2520positional%2520encodings%2520and%2520investigate%2520integration%2520strategies%2520that%250Aimprove%2520both%2520metrics%2520and%2520stability%252C%2520surpassing%2520state-of-the-art%2520results%2520at%2520the%250Atime%2520of%2520this%2520work%2527s%2520initial%2520preprint.%2520Importantly%252C%2520we%2520demonstrate%2520that%250Aselecting%2520the%2520appropriate%2520encoding%2520is%2520not%2520only%2520key%2520to%2520better%2520performance%2520but%250Aalso%2520essential%2520for%2520building%2520robust%252C%2520reliable%2520SRS%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10436v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Positional%20encoding%20is%20not%20the%20same%20as%20context%3A%20A%20study%20on%20positional%0A%20%20encoding%20for%20sequential%20recommendation&entry.906535625=Alejo%20Lopez-Avila%20and%20Jinhua%20Du%20and%20Abbas%20Shimary%20and%20Ze%20Li&entry.1292438233=%20%20The%20rapid%20growth%20of%20streaming%20media%20and%20e-commerce%20has%20driven%20advancements%20in%0Arecommendation%20systems%2C%20particularly%20Sequential%20Recommendation%20Systems%20%28SRS%29.%0AThese%20systems%20employ%20users%27%20interaction%20histories%20to%20predict%20future%0Apreferences.%20While%20recent%20research%20has%20focused%20on%20architectural%20innovations%0Alike%20transformer%20blocks%20and%20feature%20extraction%2C%20positional%20encodings%2C%20crucial%0Afor%20capturing%20temporal%20patterns%2C%20have%20received%20less%20attention.%20These%20encodings%0Aare%20often%20conflated%20with%20contextual%2C%20such%20as%20the%20temporal%20footprint%2C%20which%0Aprevious%20works%20tend%20to%20treat%20as%20interchangeable%20with%20positional%20information.%0AThis%20paper%20highlights%20the%20critical%20distinction%20between%20temporal%20footprint%20and%0Apositional%20encodings%2C%20demonstrating%20that%20the%20latter%20offers%20unique%20relational%0Acues%20between%20items%2C%20which%20the%20temporal%20footprint%20alone%20cannot%20provide.%20Through%0Aextensive%20experimentation%20on%20eight%20Amazon%20datasets%20and%20subsets%2C%20we%20assess%20the%0Aimpact%20of%20various%20encodings%20on%20performance%20metrics%20and%20training%20stability.%20We%0Aintroduce%20new%20positional%20encodings%20and%20investigate%20integration%20strategies%20that%0Aimprove%20both%20metrics%20and%20stability%2C%20surpassing%20state-of-the-art%20results%20at%20the%0Atime%20of%20this%20work%27s%20initial%20preprint.%20Importantly%2C%20we%20demonstrate%20that%0Aselecting%20the%20appropriate%20encoding%20is%20not%20only%20key%20to%20better%20performance%20but%0Aalso%20essential%20for%20building%20robust%2C%20reliable%20SRS%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10436v2&entry.124074799=Read"},
{"title": "FOCUS: First Order Concentrated Updating Scheme", "author": "Yizhou Liu and Ziming Liu and Jeff Gore", "abstract": "  Large language models (LLMs) demonstrate remarkable performance, and\nimproving their pre-training process appears to be key to enhancing their\ncapabilities further. Based on the documented success of Adam, learning rate\ndecay, and weight decay, we hypothesize that the pre-training loss landscape\nfeatures a narrowing valley structure. Through experiments with synthetic loss\nfunctions, we discover that when gradient query noise is high relative to the\nvalley's sharpness, Adam's performance falls behind that of Signum because Adam\nreduces the effective step size too drastically. This observation led us to\ndevelop FOCUS, an optimizer that enhances Signum by incorporating attraction\ntoward moving averaged parameters, allowing it to handle noise better while\nmaintaining larger step sizes. In training GPT-2, FOCUS proves to be more\nstable than Signum and faster than Adam. These results suggest that gradient\nnoise may be an underappreciated limiting factor in LLM training, and FOCUS\noffers promising solutions.\n", "link": "http://arxiv.org/abs/2501.12243v1", "date": "2025-01-21", "relevancy": 2.0257, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5211}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5002}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOCUS%3A%20First%20Order%20Concentrated%20Updating%20Scheme&body=Title%3A%20FOCUS%3A%20First%20Order%20Concentrated%20Updating%20Scheme%0AAuthor%3A%20Yizhou%20Liu%20and%20Ziming%20Liu%20and%20Jeff%20Gore%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20remarkable%20performance%2C%20and%0Aimproving%20their%20pre-training%20process%20appears%20to%20be%20key%20to%20enhancing%20their%0Acapabilities%20further.%20Based%20on%20the%20documented%20success%20of%20Adam%2C%20learning%20rate%0Adecay%2C%20and%20weight%20decay%2C%20we%20hypothesize%20that%20the%20pre-training%20loss%20landscape%0Afeatures%20a%20narrowing%20valley%20structure.%20Through%20experiments%20with%20synthetic%20loss%0Afunctions%2C%20we%20discover%20that%20when%20gradient%20query%20noise%20is%20high%20relative%20to%20the%0Avalley%27s%20sharpness%2C%20Adam%27s%20performance%20falls%20behind%20that%20of%20Signum%20because%20Adam%0Areduces%20the%20effective%20step%20size%20too%20drastically.%20This%20observation%20led%20us%20to%0Adevelop%20FOCUS%2C%20an%20optimizer%20that%20enhances%20Signum%20by%20incorporating%20attraction%0Atoward%20moving%20averaged%20parameters%2C%20allowing%20it%20to%20handle%20noise%20better%20while%0Amaintaining%20larger%20step%20sizes.%20In%20training%20GPT-2%2C%20FOCUS%20proves%20to%20be%20more%0Astable%20than%20Signum%20and%20faster%20than%20Adam.%20These%20results%20suggest%20that%20gradient%0Anoise%20may%20be%20an%20underappreciated%20limiting%20factor%20in%20LLM%20training%2C%20and%20FOCUS%0Aoffers%20promising%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOCUS%253A%2520First%2520Order%2520Concentrated%2520Updating%2520Scheme%26entry.906535625%3DYizhou%2520Liu%2520and%2520Ziming%2520Liu%2520and%2520Jeff%2520Gore%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520remarkable%2520performance%252C%2520and%250Aimproving%2520their%2520pre-training%2520process%2520appears%2520to%2520be%2520key%2520to%2520enhancing%2520their%250Acapabilities%2520further.%2520Based%2520on%2520the%2520documented%2520success%2520of%2520Adam%252C%2520learning%2520rate%250Adecay%252C%2520and%2520weight%2520decay%252C%2520we%2520hypothesize%2520that%2520the%2520pre-training%2520loss%2520landscape%250Afeatures%2520a%2520narrowing%2520valley%2520structure.%2520Through%2520experiments%2520with%2520synthetic%2520loss%250Afunctions%252C%2520we%2520discover%2520that%2520when%2520gradient%2520query%2520noise%2520is%2520high%2520relative%2520to%2520the%250Avalley%2527s%2520sharpness%252C%2520Adam%2527s%2520performance%2520falls%2520behind%2520that%2520of%2520Signum%2520because%2520Adam%250Areduces%2520the%2520effective%2520step%2520size%2520too%2520drastically.%2520This%2520observation%2520led%2520us%2520to%250Adevelop%2520FOCUS%252C%2520an%2520optimizer%2520that%2520enhances%2520Signum%2520by%2520incorporating%2520attraction%250Atoward%2520moving%2520averaged%2520parameters%252C%2520allowing%2520it%2520to%2520handle%2520noise%2520better%2520while%250Amaintaining%2520larger%2520step%2520sizes.%2520In%2520training%2520GPT-2%252C%2520FOCUS%2520proves%2520to%2520be%2520more%250Astable%2520than%2520Signum%2520and%2520faster%2520than%2520Adam.%2520These%2520results%2520suggest%2520that%2520gradient%250Anoise%2520may%2520be%2520an%2520underappreciated%2520limiting%2520factor%2520in%2520LLM%2520training%252C%2520and%2520FOCUS%250Aoffers%2520promising%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOCUS%3A%20First%20Order%20Concentrated%20Updating%20Scheme&entry.906535625=Yizhou%20Liu%20and%20Ziming%20Liu%20and%20Jeff%20Gore&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20remarkable%20performance%2C%20and%0Aimproving%20their%20pre-training%20process%20appears%20to%20be%20key%20to%20enhancing%20their%0Acapabilities%20further.%20Based%20on%20the%20documented%20success%20of%20Adam%2C%20learning%20rate%0Adecay%2C%20and%20weight%20decay%2C%20we%20hypothesize%20that%20the%20pre-training%20loss%20landscape%0Afeatures%20a%20narrowing%20valley%20structure.%20Through%20experiments%20with%20synthetic%20loss%0Afunctions%2C%20we%20discover%20that%20when%20gradient%20query%20noise%20is%20high%20relative%20to%20the%0Avalley%27s%20sharpness%2C%20Adam%27s%20performance%20falls%20behind%20that%20of%20Signum%20because%20Adam%0Areduces%20the%20effective%20step%20size%20too%20drastically.%20This%20observation%20led%20us%20to%0Adevelop%20FOCUS%2C%20an%20optimizer%20that%20enhances%20Signum%20by%20incorporating%20attraction%0Atoward%20moving%20averaged%20parameters%2C%20allowing%20it%20to%20handle%20noise%20better%20while%0Amaintaining%20larger%20step%20sizes.%20In%20training%20GPT-2%2C%20FOCUS%20proves%20to%20be%20more%0Astable%20than%20Signum%20and%20faster%20than%20Adam.%20These%20results%20suggest%20that%20gradient%0Anoise%20may%20be%20an%20underappreciated%20limiting%20factor%20in%20LLM%20training%2C%20and%20FOCUS%0Aoffers%20promising%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12243v1&entry.124074799=Read"},
{"title": "Communication-Efficient and Privacy-Adaptable Mechanism for Federated\n  Learning", "author": "Chih Wei Ling and Youqi Wu and Jiande Sun and Cheuk Ting Li and Linqi Song and Weitao Xu", "abstract": "  Training machine learning models on decentralized private data via federated\nlearning (FL) poses two key challenges: communication efficiency and privacy\nprotection. In this work, we address these challenges within the trusted\naggregator model by introducing a novel approach called the\nCommunication-Efficient and Privacy-Adaptable Mechanism (CEPAM), achieving both\nobjectives simultaneously. In particular, CEPAM leverages the rejection-sampled\nuniversal quantizer (RSUQ), a construction of randomized vector quantizer whose\nresulting distortion is equivalent to a prescribed noise, such as Gaussian or\nLaplace noise, enabling joint differential privacy and compression. Moreover,\nwe analyze the trade-offs among user privacy, global utility, and transmission\nrate of CEPAM by defining appropriate metrics for FL with differential privacy\nand compression. Our CEPAM provides the additional benefit of privacy\nadaptability, allowing clients and the server to customize privacy protection\nbased on required accuracy and protection. We assess CEPAM's utility\nperformance using MNIST dataset, demonstrating that CEPAM surpasses baseline\nmodels in terms of learning accuracy.\n", "link": "http://arxiv.org/abs/2501.12046v1", "date": "2025-01-21", "relevancy": 2.0221, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5162}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5085}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20and%20Privacy-Adaptable%20Mechanism%20for%20Federated%0A%20%20Learning&body=Title%3A%20Communication-Efficient%20and%20Privacy-Adaptable%20Mechanism%20for%20Federated%0A%20%20Learning%0AAuthor%3A%20Chih%20Wei%20Ling%20and%20Youqi%20Wu%20and%20Jiande%20Sun%20and%20Cheuk%20Ting%20Li%20and%20Linqi%20Song%20and%20Weitao%20Xu%0AAbstract%3A%20%20%20Training%20machine%20learning%20models%20on%20decentralized%20private%20data%20via%20federated%0Alearning%20%28FL%29%20poses%20two%20key%20challenges%3A%20communication%20efficiency%20and%20privacy%0Aprotection.%20In%20this%20work%2C%20we%20address%20these%20challenges%20within%20the%20trusted%0Aaggregator%20model%20by%20introducing%20a%20novel%20approach%20called%20the%0ACommunication-Efficient%20and%20Privacy-Adaptable%20Mechanism%20%28CEPAM%29%2C%20achieving%20both%0Aobjectives%20simultaneously.%20In%20particular%2C%20CEPAM%20leverages%20the%20rejection-sampled%0Auniversal%20quantizer%20%28RSUQ%29%2C%20a%20construction%20of%20randomized%20vector%20quantizer%20whose%0Aresulting%20distortion%20is%20equivalent%20to%20a%20prescribed%20noise%2C%20such%20as%20Gaussian%20or%0ALaplace%20noise%2C%20enabling%20joint%20differential%20privacy%20and%20compression.%20Moreover%2C%0Awe%20analyze%20the%20trade-offs%20among%20user%20privacy%2C%20global%20utility%2C%20and%20transmission%0Arate%20of%20CEPAM%20by%20defining%20appropriate%20metrics%20for%20FL%20with%20differential%20privacy%0Aand%20compression.%20Our%20CEPAM%20provides%20the%20additional%20benefit%20of%20privacy%0Aadaptability%2C%20allowing%20clients%20and%20the%20server%20to%20customize%20privacy%20protection%0Abased%20on%20required%20accuracy%20and%20protection.%20We%20assess%20CEPAM%27s%20utility%0Aperformance%20using%20MNIST%20dataset%2C%20demonstrating%20that%20CEPAM%20surpasses%20baseline%0Amodels%20in%20terms%20of%20learning%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520and%2520Privacy-Adaptable%2520Mechanism%2520for%2520Federated%250A%2520%2520Learning%26entry.906535625%3DChih%2520Wei%2520Ling%2520and%2520Youqi%2520Wu%2520and%2520Jiande%2520Sun%2520and%2520Cheuk%2520Ting%2520Li%2520and%2520Linqi%2520Song%2520and%2520Weitao%2520Xu%26entry.1292438233%3D%2520%2520Training%2520machine%2520learning%2520models%2520on%2520decentralized%2520private%2520data%2520via%2520federated%250Alearning%2520%2528FL%2529%2520poses%2520two%2520key%2520challenges%253A%2520communication%2520efficiency%2520and%2520privacy%250Aprotection.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520challenges%2520within%2520the%2520trusted%250Aaggregator%2520model%2520by%2520introducing%2520a%2520novel%2520approach%2520called%2520the%250ACommunication-Efficient%2520and%2520Privacy-Adaptable%2520Mechanism%2520%2528CEPAM%2529%252C%2520achieving%2520both%250Aobjectives%2520simultaneously.%2520In%2520particular%252C%2520CEPAM%2520leverages%2520the%2520rejection-sampled%250Auniversal%2520quantizer%2520%2528RSUQ%2529%252C%2520a%2520construction%2520of%2520randomized%2520vector%2520quantizer%2520whose%250Aresulting%2520distortion%2520is%2520equivalent%2520to%2520a%2520prescribed%2520noise%252C%2520such%2520as%2520Gaussian%2520or%250ALaplace%2520noise%252C%2520enabling%2520joint%2520differential%2520privacy%2520and%2520compression.%2520Moreover%252C%250Awe%2520analyze%2520the%2520trade-offs%2520among%2520user%2520privacy%252C%2520global%2520utility%252C%2520and%2520transmission%250Arate%2520of%2520CEPAM%2520by%2520defining%2520appropriate%2520metrics%2520for%2520FL%2520with%2520differential%2520privacy%250Aand%2520compression.%2520Our%2520CEPAM%2520provides%2520the%2520additional%2520benefit%2520of%2520privacy%250Aadaptability%252C%2520allowing%2520clients%2520and%2520the%2520server%2520to%2520customize%2520privacy%2520protection%250Abased%2520on%2520required%2520accuracy%2520and%2520protection.%2520We%2520assess%2520CEPAM%2527s%2520utility%250Aperformance%2520using%2520MNIST%2520dataset%252C%2520demonstrating%2520that%2520CEPAM%2520surpasses%2520baseline%250Amodels%2520in%2520terms%2520of%2520learning%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20and%20Privacy-Adaptable%20Mechanism%20for%20Federated%0A%20%20Learning&entry.906535625=Chih%20Wei%20Ling%20and%20Youqi%20Wu%20and%20Jiande%20Sun%20and%20Cheuk%20Ting%20Li%20and%20Linqi%20Song%20and%20Weitao%20Xu&entry.1292438233=%20%20Training%20machine%20learning%20models%20on%20decentralized%20private%20data%20via%20federated%0Alearning%20%28FL%29%20poses%20two%20key%20challenges%3A%20communication%20efficiency%20and%20privacy%0Aprotection.%20In%20this%20work%2C%20we%20address%20these%20challenges%20within%20the%20trusted%0Aaggregator%20model%20by%20introducing%20a%20novel%20approach%20called%20the%0ACommunication-Efficient%20and%20Privacy-Adaptable%20Mechanism%20%28CEPAM%29%2C%20achieving%20both%0Aobjectives%20simultaneously.%20In%20particular%2C%20CEPAM%20leverages%20the%20rejection-sampled%0Auniversal%20quantizer%20%28RSUQ%29%2C%20a%20construction%20of%20randomized%20vector%20quantizer%20whose%0Aresulting%20distortion%20is%20equivalent%20to%20a%20prescribed%20noise%2C%20such%20as%20Gaussian%20or%0ALaplace%20noise%2C%20enabling%20joint%20differential%20privacy%20and%20compression.%20Moreover%2C%0Awe%20analyze%20the%20trade-offs%20among%20user%20privacy%2C%20global%20utility%2C%20and%20transmission%0Arate%20of%20CEPAM%20by%20defining%20appropriate%20metrics%20for%20FL%20with%20differential%20privacy%0Aand%20compression.%20Our%20CEPAM%20provides%20the%20additional%20benefit%20of%20privacy%0Aadaptability%2C%20allowing%20clients%20and%20the%20server%20to%20customize%20privacy%20protection%0Abased%20on%20required%20accuracy%20and%20protection.%20We%20assess%20CEPAM%27s%20utility%0Aperformance%20using%20MNIST%20dataset%2C%20demonstrating%20that%20CEPAM%20surpasses%20baseline%0Amodels%20in%20terms%20of%20learning%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12046v1&entry.124074799=Read"},
{"title": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL", "author": "Yeounoh Chung and Gaurav T. Kakkar and Yu Gan and Brenton Milne and Fatma Ozcan", "abstract": "  Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve a strong\nperformance with 67.41\\% on BIRD benchmark (dev) without finetuning and\nexpensive self-consistency based techniques.\n", "link": "http://arxiv.org/abs/2501.12372v1", "date": "2025-01-21", "relevancy": 2.0183, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Long%20Context%20All%20You%20Need%3F%20Leveraging%20LLM%27s%20Extended%20Context%20for%0A%20%20NL2SQL&body=Title%3A%20Is%20Long%20Context%20All%20You%20Need%3F%20Leveraging%20LLM%27s%20Extended%20Context%20for%0A%20%20NL2SQL%0AAuthor%3A%20Yeounoh%20Chung%20and%20Gaurav%20T.%20Kakkar%20and%20Yu%20Gan%20and%20Brenton%20Milne%20and%20Fatma%20Ozcan%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20across%0Aa%20range%20of%20natural%20language%20processing%20tasks.%20In%20particular%2C%20improvements%20in%0Areasoning%20abilities%20and%20the%20expansion%20of%20context%20windows%20have%20opened%20new%0Aavenues%20for%20leveraging%20these%20powerful%20models.%20NL2SQL%20is%20challenging%20in%20that%20the%0Anatural%20language%20question%20is%20inherently%20ambiguous%2C%20while%20the%20SQL%20generation%0Arequires%20a%20precise%20understanding%20of%20complex%20data%20schema%20and%20semantics.%20One%0Aapproach%20to%20this%20semantic%20ambiguous%20problem%20is%20to%20provide%20more%20and%20sufficient%0Acontextual%20information.%0A%20%20In%20this%20work%2C%20we%20explore%20the%20performance%20and%20the%20latency%20trade-offs%20of%20the%0Aextended%20context%20window%20%28a.k.a.%2C%20long%20context%29%20offered%20by%20Google%27s%0Astate-of-the-art%20LLM%20%28%5Ctextit%7Bgemini-1.5-pro%7D%29.%20We%20study%20the%20impact%20of%20various%0Acontextual%20information%2C%20including%20column%20example%20values%2C%20question%20and%20SQL%20query%0Apairs%2C%20user-provided%20hints%2C%20SQL%20documentation%2C%20and%20schema.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20study%20how%20the%20extended%20context%20window%20and%0Aextra%20contextual%20information%20can%20help%20NL2SQL%20generation%20with%20respect%20to%20both%0Aaccuracy%20and%20latency%20cost.%20We%20show%20that%20long%20context%20LLMs%20are%20robust%20and%20do%20not%0Aget%20lost%20in%20the%20extended%20contextual%20information.%20Additionally%2C%20our%20long-context%0ANL2SQL%20pipeline%20based%20on%20Google%27s%20%5Ctextit%7Bgemini-pro-1.5%7D%20achieve%20a%20strong%0Aperformance%20with%2067.41%5C%25%20on%20BIRD%20benchmark%20%28dev%29%20without%20finetuning%20and%0Aexpensive%20self-consistency%20based%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Long%2520Context%2520All%2520You%2520Need%253F%2520Leveraging%2520LLM%2527s%2520Extended%2520Context%2520for%250A%2520%2520NL2SQL%26entry.906535625%3DYeounoh%2520Chung%2520and%2520Gaurav%2520T.%2520Kakkar%2520and%2520Yu%2520Gan%2520and%2520Brenton%2520Milne%2520and%2520Fatma%2520Ozcan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520across%250Aa%2520range%2520of%2520natural%2520language%2520processing%2520tasks.%2520In%2520particular%252C%2520improvements%2520in%250Areasoning%2520abilities%2520and%2520the%2520expansion%2520of%2520context%2520windows%2520have%2520opened%2520new%250Aavenues%2520for%2520leveraging%2520these%2520powerful%2520models.%2520NL2SQL%2520is%2520challenging%2520in%2520that%2520the%250Anatural%2520language%2520question%2520is%2520inherently%2520ambiguous%252C%2520while%2520the%2520SQL%2520generation%250Arequires%2520a%2520precise%2520understanding%2520of%2520complex%2520data%2520schema%2520and%2520semantics.%2520One%250Aapproach%2520to%2520this%2520semantic%2520ambiguous%2520problem%2520is%2520to%2520provide%2520more%2520and%2520sufficient%250Acontextual%2520information.%250A%2520%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520performance%2520and%2520the%2520latency%2520trade-offs%2520of%2520the%250Aextended%2520context%2520window%2520%2528a.k.a.%252C%2520long%2520context%2529%2520offered%2520by%2520Google%2527s%250Astate-of-the-art%2520LLM%2520%2528%255Ctextit%257Bgemini-1.5-pro%257D%2529.%2520We%2520study%2520the%2520impact%2520of%2520various%250Acontextual%2520information%252C%2520including%2520column%2520example%2520values%252C%2520question%2520and%2520SQL%2520query%250Apairs%252C%2520user-provided%2520hints%252C%2520SQL%2520documentation%252C%2520and%2520schema.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520study%2520how%2520the%2520extended%2520context%2520window%2520and%250Aextra%2520contextual%2520information%2520can%2520help%2520NL2SQL%2520generation%2520with%2520respect%2520to%2520both%250Aaccuracy%2520and%2520latency%2520cost.%2520We%2520show%2520that%2520long%2520context%2520LLMs%2520are%2520robust%2520and%2520do%2520not%250Aget%2520lost%2520in%2520the%2520extended%2520contextual%2520information.%2520Additionally%252C%2520our%2520long-context%250ANL2SQL%2520pipeline%2520based%2520on%2520Google%2527s%2520%255Ctextit%257Bgemini-pro-1.5%257D%2520achieve%2520a%2520strong%250Aperformance%2520with%252067.41%255C%2525%2520on%2520BIRD%2520benchmark%2520%2528dev%2529%2520without%2520finetuning%2520and%250Aexpensive%2520self-consistency%2520based%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Long%20Context%20All%20You%20Need%3F%20Leveraging%20LLM%27s%20Extended%20Context%20for%0A%20%20NL2SQL&entry.906535625=Yeounoh%20Chung%20and%20Gaurav%20T.%20Kakkar%20and%20Yu%20Gan%20and%20Brenton%20Milne%20and%20Fatma%20Ozcan&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20across%0Aa%20range%20of%20natural%20language%20processing%20tasks.%20In%20particular%2C%20improvements%20in%0Areasoning%20abilities%20and%20the%20expansion%20of%20context%20windows%20have%20opened%20new%0Aavenues%20for%20leveraging%20these%20powerful%20models.%20NL2SQL%20is%20challenging%20in%20that%20the%0Anatural%20language%20question%20is%20inherently%20ambiguous%2C%20while%20the%20SQL%20generation%0Arequires%20a%20precise%20understanding%20of%20complex%20data%20schema%20and%20semantics.%20One%0Aapproach%20to%20this%20semantic%20ambiguous%20problem%20is%20to%20provide%20more%20and%20sufficient%0Acontextual%20information.%0A%20%20In%20this%20work%2C%20we%20explore%20the%20performance%20and%20the%20latency%20trade-offs%20of%20the%0Aextended%20context%20window%20%28a.k.a.%2C%20long%20context%29%20offered%20by%20Google%27s%0Astate-of-the-art%20LLM%20%28%5Ctextit%7Bgemini-1.5-pro%7D%29.%20We%20study%20the%20impact%20of%20various%0Acontextual%20information%2C%20including%20column%20example%20values%2C%20question%20and%20SQL%20query%0Apairs%2C%20user-provided%20hints%2C%20SQL%20documentation%2C%20and%20schema.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20study%20how%20the%20extended%20context%20window%20and%0Aextra%20contextual%20information%20can%20help%20NL2SQL%20generation%20with%20respect%20to%20both%0Aaccuracy%20and%20latency%20cost.%20We%20show%20that%20long%20context%20LLMs%20are%20robust%20and%20do%20not%0Aget%20lost%20in%20the%20extended%20contextual%20information.%20Additionally%2C%20our%20long-context%0ANL2SQL%20pipeline%20based%20on%20Google%27s%20%5Ctextit%7Bgemini-pro-1.5%7D%20achieve%20a%20strong%0Aperformance%20with%2067.41%5C%25%20on%20BIRD%20benchmark%20%28dev%29%20without%20finetuning%20and%0Aexpensive%20self-consistency%20based%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12372v1&entry.124074799=Read"},
{"title": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens", "author": "Zhepeng Cen and Yao Liu and Siliang Zeng and Pratik Chaudhari and Huzefa Rangwala and George Karypis and Rasool Fakoor", "abstract": "  Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks.\n", "link": "http://arxiv.org/abs/2410.14655v2", "date": "2025-01-21", "relevancy": 2.0074, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5203}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4945}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Training-Inference%20Gap%20in%20LLMs%20by%20Leveraging%20Self-Generated%0A%20%20Tokens&body=Title%3A%20Bridging%20the%20Training-Inference%20Gap%20in%20LLMs%20by%20Leveraging%20Self-Generated%0A%20%20Tokens%0AAuthor%3A%20Zhepeng%20Cen%20and%20Yao%20Liu%20and%20Siliang%20Zeng%20and%20Pratik%20Chaudhari%20and%20Huzefa%20Rangwala%20and%20George%20Karypis%20and%20Rasool%20Fakoor%0AAbstract%3A%20%20%20Language%20models%20are%20often%20trained%20to%20maximize%20the%20likelihood%20of%20the%20next%0Atoken%20given%20past%20tokens%20in%20the%20training%20dataset.%20However%2C%20during%20inference%0Atime%2C%20they%20are%20utilized%20differently%2C%20generating%20text%20sequentially%20and%0Aauto-regressively%20by%20using%20previously%20generated%20tokens%20as%20input%20to%20predict%20the%0Anext%20one.%20Marginal%20differences%20in%20predictions%20at%20each%20step%20can%20cascade%20over%0Asuccessive%20steps%2C%20resulting%20in%20different%20distributions%20from%20what%20the%20models%0Awere%20trained%20for%20and%20potentially%20leading%20to%20unpredictable%20behavior.%20This%20paper%0Aproposes%20two%20simple%20approaches%20based%20on%20model%20own%20generation%20to%20address%20this%0Adiscrepancy%20between%20the%20training%20and%20inference%20time.%20Our%20first%20approach%20is%0ABatch-Scheduled%20Sampling%2C%20where%2C%20during%20training%2C%20we%20stochastically%20choose%0Abetween%20the%20ground-truth%20token%20from%20the%20dataset%20and%20the%20model%27s%20own%20generated%0Atoken%20as%20input%20to%20predict%20the%20next%20token.%20This%20is%20done%20in%20an%20offline%20manner%2C%0Amodifying%20the%20context%20window%20by%20interleaving%20ground-truth%20tokens%20with%20those%0Agenerated%20by%20the%20model.%20Our%20second%20approach%20is%20Reference-Answer-based%0ACorrection%2C%20where%20we%20explicitly%20incorporate%20a%20self-correction%20capability%20into%0Athe%20model%20during%20training.%20This%20enables%20the%20model%20to%20effectively%20self-correct%0Athe%20gaps%20between%20the%20generated%20sequences%20and%20the%20ground%20truth%20data%20without%0Arelying%20on%20an%20external%20oracle%20model.%20By%20incorporating%20our%20proposed%20strategies%0Aduring%20training%2C%20we%20have%20observed%20an%20overall%20improvement%20in%20performance%0Acompared%20to%20baseline%20methods%2C%20as%20demonstrated%20by%20our%20extensive%20experiments%0Ausing%20summarization%2C%20general%20question-answering%2C%20and%20math%20question-answering%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520the%2520Training-Inference%2520Gap%2520in%2520LLMs%2520by%2520Leveraging%2520Self-Generated%250A%2520%2520Tokens%26entry.906535625%3DZhepeng%2520Cen%2520and%2520Yao%2520Liu%2520and%2520Siliang%2520Zeng%2520and%2520Pratik%2520Chaudhari%2520and%2520Huzefa%2520Rangwala%2520and%2520George%2520Karypis%2520and%2520Rasool%2520Fakoor%26entry.1292438233%3D%2520%2520Language%2520models%2520are%2520often%2520trained%2520to%2520maximize%2520the%2520likelihood%2520of%2520the%2520next%250Atoken%2520given%2520past%2520tokens%2520in%2520the%2520training%2520dataset.%2520However%252C%2520during%2520inference%250Atime%252C%2520they%2520are%2520utilized%2520differently%252C%2520generating%2520text%2520sequentially%2520and%250Aauto-regressively%2520by%2520using%2520previously%2520generated%2520tokens%2520as%2520input%2520to%2520predict%2520the%250Anext%2520one.%2520Marginal%2520differences%2520in%2520predictions%2520at%2520each%2520step%2520can%2520cascade%2520over%250Asuccessive%2520steps%252C%2520resulting%2520in%2520different%2520distributions%2520from%2520what%2520the%2520models%250Awere%2520trained%2520for%2520and%2520potentially%2520leading%2520to%2520unpredictable%2520behavior.%2520This%2520paper%250Aproposes%2520two%2520simple%2520approaches%2520based%2520on%2520model%2520own%2520generation%2520to%2520address%2520this%250Adiscrepancy%2520between%2520the%2520training%2520and%2520inference%2520time.%2520Our%2520first%2520approach%2520is%250ABatch-Scheduled%2520Sampling%252C%2520where%252C%2520during%2520training%252C%2520we%2520stochastically%2520choose%250Abetween%2520the%2520ground-truth%2520token%2520from%2520the%2520dataset%2520and%2520the%2520model%2527s%2520own%2520generated%250Atoken%2520as%2520input%2520to%2520predict%2520the%2520next%2520token.%2520This%2520is%2520done%2520in%2520an%2520offline%2520manner%252C%250Amodifying%2520the%2520context%2520window%2520by%2520interleaving%2520ground-truth%2520tokens%2520with%2520those%250Agenerated%2520by%2520the%2520model.%2520Our%2520second%2520approach%2520is%2520Reference-Answer-based%250ACorrection%252C%2520where%2520we%2520explicitly%2520incorporate%2520a%2520self-correction%2520capability%2520into%250Athe%2520model%2520during%2520training.%2520This%2520enables%2520the%2520model%2520to%2520effectively%2520self-correct%250Athe%2520gaps%2520between%2520the%2520generated%2520sequences%2520and%2520the%2520ground%2520truth%2520data%2520without%250Arelying%2520on%2520an%2520external%2520oracle%2520model.%2520By%2520incorporating%2520our%2520proposed%2520strategies%250Aduring%2520training%252C%2520we%2520have%2520observed%2520an%2520overall%2520improvement%2520in%2520performance%250Acompared%2520to%2520baseline%2520methods%252C%2520as%2520demonstrated%2520by%2520our%2520extensive%2520experiments%250Ausing%2520summarization%252C%2520general%2520question-answering%252C%2520and%2520math%2520question-answering%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Training-Inference%20Gap%20in%20LLMs%20by%20Leveraging%20Self-Generated%0A%20%20Tokens&entry.906535625=Zhepeng%20Cen%20and%20Yao%20Liu%20and%20Siliang%20Zeng%20and%20Pratik%20Chaudhari%20and%20Huzefa%20Rangwala%20and%20George%20Karypis%20and%20Rasool%20Fakoor&entry.1292438233=%20%20Language%20models%20are%20often%20trained%20to%20maximize%20the%20likelihood%20of%20the%20next%0Atoken%20given%20past%20tokens%20in%20the%20training%20dataset.%20However%2C%20during%20inference%0Atime%2C%20they%20are%20utilized%20differently%2C%20generating%20text%20sequentially%20and%0Aauto-regressively%20by%20using%20previously%20generated%20tokens%20as%20input%20to%20predict%20the%0Anext%20one.%20Marginal%20differences%20in%20predictions%20at%20each%20step%20can%20cascade%20over%0Asuccessive%20steps%2C%20resulting%20in%20different%20distributions%20from%20what%20the%20models%0Awere%20trained%20for%20and%20potentially%20leading%20to%20unpredictable%20behavior.%20This%20paper%0Aproposes%20two%20simple%20approaches%20based%20on%20model%20own%20generation%20to%20address%20this%0Adiscrepancy%20between%20the%20training%20and%20inference%20time.%20Our%20first%20approach%20is%0ABatch-Scheduled%20Sampling%2C%20where%2C%20during%20training%2C%20we%20stochastically%20choose%0Abetween%20the%20ground-truth%20token%20from%20the%20dataset%20and%20the%20model%27s%20own%20generated%0Atoken%20as%20input%20to%20predict%20the%20next%20token.%20This%20is%20done%20in%20an%20offline%20manner%2C%0Amodifying%20the%20context%20window%20by%20interleaving%20ground-truth%20tokens%20with%20those%0Agenerated%20by%20the%20model.%20Our%20second%20approach%20is%20Reference-Answer-based%0ACorrection%2C%20where%20we%20explicitly%20incorporate%20a%20self-correction%20capability%20into%0Athe%20model%20during%20training.%20This%20enables%20the%20model%20to%20effectively%20self-correct%0Athe%20gaps%20between%20the%20generated%20sequences%20and%20the%20ground%20truth%20data%20without%0Arelying%20on%20an%20external%20oracle%20model.%20By%20incorporating%20our%20proposed%20strategies%0Aduring%20training%2C%20we%20have%20observed%20an%20overall%20improvement%20in%20performance%0Acompared%20to%20baseline%20methods%2C%20as%20demonstrated%20by%20our%20extensive%20experiments%0Ausing%20summarization%2C%20general%20question-answering%2C%20and%20math%20question-answering%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14655v2&entry.124074799=Read"},
{"title": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes", "author": "Stefan Lenz and Arsenij Ustjanzew and Marco Jeray and Torsten Panholzer", "abstract": "  Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP.\n", "link": "http://arxiv.org/abs/2501.12106v1", "date": "2025-01-21", "relevancy": 2.0051, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5243}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4967}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20open%20source%20large%20language%20models%20be%20used%20for%20tumor%20documentation%20in%0A%20%20Germany%3F%20--%20An%20evaluation%20on%20urological%20doctors%27%20notes&body=Title%3A%20Can%20open%20source%20large%20language%20models%20be%20used%20for%20tumor%20documentation%20in%0A%20%20Germany%3F%20--%20An%20evaluation%20on%20urological%20doctors%27%20notes%0AAuthor%3A%20Stefan%20Lenz%20and%20Arsenij%20Ustjanzew%20and%20Marco%20Jeray%20and%20Torsten%20Panholzer%0AAbstract%3A%20%20%20Tumor%20documentation%20in%20Germany%20is%20largely%20done%20manually%2C%20requiring%20reading%0Apatient%20records%20and%20entering%20data%20into%20structured%20databases.%20Large%20language%0Amodels%20%28LLMs%29%20could%20potentially%20enhance%20this%20process%20by%20improving%20efficiency%0Aand%20reliability.%20This%20evaluation%20tests%20eleven%20different%20open%20source%20LLMs%20with%0Asizes%20ranging%20from%201-70%20billion%20model%20parameters%20on%20three%20basic%20tasks%20of%20the%0Atumor%20documentation%20process%3A%20identifying%20tumor%20diagnoses%2C%20assigning%20ICD-10%0Acodes%2C%20and%20extracting%20the%20date%20of%20first%20diagnosis.%20For%20evaluating%20the%20LLMs%20on%0Athese%20tasks%2C%20a%20dataset%20of%20annotated%20text%20snippets%20based%20on%20anonymized%20doctors%27%0Anotes%20from%20urology%20was%20prepared.%20Different%20prompting%20strategies%20were%20used%20to%0Ainvestigate%20the%20effect%20of%20the%20number%20of%20examples%20in%20few-shot%20prompting%20and%20to%0Aexplore%20the%20capabilities%20of%20the%20LLMs%20in%20general.%20The%20models%20Llama%203.1%208B%2C%0AMistral%207B%2C%20and%20Mistral%20NeMo%2012%20B%20performed%20comparably%20well%20in%20the%20tasks.%0AModels%20with%20less%20extensive%20training%20data%20or%20having%20fewer%20than%207%20billion%0Aparameters%20showed%20notably%20lower%20performance%2C%20while%20larger%20models%20did%20not%0Adisplay%20performance%20gains.%20Examples%20from%20a%20different%20medical%20domain%20than%0Aurology%20could%20also%20improve%20the%20outcome%20in%20few-shot%20prompting%2C%20which%0Ademonstrates%20the%20ability%20of%20LLMs%20to%20handle%20tasks%20needed%20for%20tumor%0Adocumentation.%20Open%20source%20LLMs%20show%20a%20strong%20potential%20for%20automating%20tumor%0Adocumentation.%20Models%20from%207-12%20billion%20parameters%20could%20offer%20an%20optimal%0Abalance%20between%20performance%20and%20resource%20efficiency.%20With%20tailored%20fine-tuning%0Aand%20well-designed%20prompting%2C%20these%20models%20might%20become%20important%20tools%20for%0Aclinical%20documentation%20in%20the%20future.%20The%20code%20for%20the%20evaluation%20is%20available%0Afrom%20https%3A//github.com/stefan-m-lenz/UroLlmEval.%20We%20also%20release%20the%20dataset%0Aas%20a%20new%20valuable%20resource%20that%20addresses%20the%20shortage%20of%20authentic%20and%20easily%0Aaccessible%20benchmarks%20in%20German-language%20medical%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520open%2520source%2520large%2520language%2520models%2520be%2520used%2520for%2520tumor%2520documentation%2520in%250A%2520%2520Germany%253F%2520--%2520An%2520evaluation%2520on%2520urological%2520doctors%2527%2520notes%26entry.906535625%3DStefan%2520Lenz%2520and%2520Arsenij%2520Ustjanzew%2520and%2520Marco%2520Jeray%2520and%2520Torsten%2520Panholzer%26entry.1292438233%3D%2520%2520Tumor%2520documentation%2520in%2520Germany%2520is%2520largely%2520done%2520manually%252C%2520requiring%2520reading%250Apatient%2520records%2520and%2520entering%2520data%2520into%2520structured%2520databases.%2520Large%2520language%250Amodels%2520%2528LLMs%2529%2520could%2520potentially%2520enhance%2520this%2520process%2520by%2520improving%2520efficiency%250Aand%2520reliability.%2520This%2520evaluation%2520tests%2520eleven%2520different%2520open%2520source%2520LLMs%2520with%250Asizes%2520ranging%2520from%25201-70%2520billion%2520model%2520parameters%2520on%2520three%2520basic%2520tasks%2520of%2520the%250Atumor%2520documentation%2520process%253A%2520identifying%2520tumor%2520diagnoses%252C%2520assigning%2520ICD-10%250Acodes%252C%2520and%2520extracting%2520the%2520date%2520of%2520first%2520diagnosis.%2520For%2520evaluating%2520the%2520LLMs%2520on%250Athese%2520tasks%252C%2520a%2520dataset%2520of%2520annotated%2520text%2520snippets%2520based%2520on%2520anonymized%2520doctors%2527%250Anotes%2520from%2520urology%2520was%2520prepared.%2520Different%2520prompting%2520strategies%2520were%2520used%2520to%250Ainvestigate%2520the%2520effect%2520of%2520the%2520number%2520of%2520examples%2520in%2520few-shot%2520prompting%2520and%2520to%250Aexplore%2520the%2520capabilities%2520of%2520the%2520LLMs%2520in%2520general.%2520The%2520models%2520Llama%25203.1%25208B%252C%250AMistral%25207B%252C%2520and%2520Mistral%2520NeMo%252012%2520B%2520performed%2520comparably%2520well%2520in%2520the%2520tasks.%250AModels%2520with%2520less%2520extensive%2520training%2520data%2520or%2520having%2520fewer%2520than%25207%2520billion%250Aparameters%2520showed%2520notably%2520lower%2520performance%252C%2520while%2520larger%2520models%2520did%2520not%250Adisplay%2520performance%2520gains.%2520Examples%2520from%2520a%2520different%2520medical%2520domain%2520than%250Aurology%2520could%2520also%2520improve%2520the%2520outcome%2520in%2520few-shot%2520prompting%252C%2520which%250Ademonstrates%2520the%2520ability%2520of%2520LLMs%2520to%2520handle%2520tasks%2520needed%2520for%2520tumor%250Adocumentation.%2520Open%2520source%2520LLMs%2520show%2520a%2520strong%2520potential%2520for%2520automating%2520tumor%250Adocumentation.%2520Models%2520from%25207-12%2520billion%2520parameters%2520could%2520offer%2520an%2520optimal%250Abalance%2520between%2520performance%2520and%2520resource%2520efficiency.%2520With%2520tailored%2520fine-tuning%250Aand%2520well-designed%2520prompting%252C%2520these%2520models%2520might%2520become%2520important%2520tools%2520for%250Aclinical%2520documentation%2520in%2520the%2520future.%2520The%2520code%2520for%2520the%2520evaluation%2520is%2520available%250Afrom%2520https%253A//github.com/stefan-m-lenz/UroLlmEval.%2520We%2520also%2520release%2520the%2520dataset%250Aas%2520a%2520new%2520valuable%2520resource%2520that%2520addresses%2520the%2520shortage%2520of%2520authentic%2520and%2520easily%250Aaccessible%2520benchmarks%2520in%2520German-language%2520medical%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20open%20source%20large%20language%20models%20be%20used%20for%20tumor%20documentation%20in%0A%20%20Germany%3F%20--%20An%20evaluation%20on%20urological%20doctors%27%20notes&entry.906535625=Stefan%20Lenz%20and%20Arsenij%20Ustjanzew%20and%20Marco%20Jeray%20and%20Torsten%20Panholzer&entry.1292438233=%20%20Tumor%20documentation%20in%20Germany%20is%20largely%20done%20manually%2C%20requiring%20reading%0Apatient%20records%20and%20entering%20data%20into%20structured%20databases.%20Large%20language%0Amodels%20%28LLMs%29%20could%20potentially%20enhance%20this%20process%20by%20improving%20efficiency%0Aand%20reliability.%20This%20evaluation%20tests%20eleven%20different%20open%20source%20LLMs%20with%0Asizes%20ranging%20from%201-70%20billion%20model%20parameters%20on%20three%20basic%20tasks%20of%20the%0Atumor%20documentation%20process%3A%20identifying%20tumor%20diagnoses%2C%20assigning%20ICD-10%0Acodes%2C%20and%20extracting%20the%20date%20of%20first%20diagnosis.%20For%20evaluating%20the%20LLMs%20on%0Athese%20tasks%2C%20a%20dataset%20of%20annotated%20text%20snippets%20based%20on%20anonymized%20doctors%27%0Anotes%20from%20urology%20was%20prepared.%20Different%20prompting%20strategies%20were%20used%20to%0Ainvestigate%20the%20effect%20of%20the%20number%20of%20examples%20in%20few-shot%20prompting%20and%20to%0Aexplore%20the%20capabilities%20of%20the%20LLMs%20in%20general.%20The%20models%20Llama%203.1%208B%2C%0AMistral%207B%2C%20and%20Mistral%20NeMo%2012%20B%20performed%20comparably%20well%20in%20the%20tasks.%0AModels%20with%20less%20extensive%20training%20data%20or%20having%20fewer%20than%207%20billion%0Aparameters%20showed%20notably%20lower%20performance%2C%20while%20larger%20models%20did%20not%0Adisplay%20performance%20gains.%20Examples%20from%20a%20different%20medical%20domain%20than%0Aurology%20could%20also%20improve%20the%20outcome%20in%20few-shot%20prompting%2C%20which%0Ademonstrates%20the%20ability%20of%20LLMs%20to%20handle%20tasks%20needed%20for%20tumor%0Adocumentation.%20Open%20source%20LLMs%20show%20a%20strong%20potential%20for%20automating%20tumor%0Adocumentation.%20Models%20from%207-12%20billion%20parameters%20could%20offer%20an%20optimal%0Abalance%20between%20performance%20and%20resource%20efficiency.%20With%20tailored%20fine-tuning%0Aand%20well-designed%20prompting%2C%20these%20models%20might%20become%20important%20tools%20for%0Aclinical%20documentation%20in%20the%20future.%20The%20code%20for%20the%20evaluation%20is%20available%0Afrom%20https%3A//github.com/stefan-m-lenz/UroLlmEval.%20We%20also%20release%20the%20dataset%0Aas%20a%20new%20valuable%20resource%20that%20addresses%20the%20shortage%20of%20authentic%20and%20easily%0Aaccessible%20benchmarks%20in%20German-language%20medical%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12106v1&entry.124074799=Read"},
{"title": "COmoving Computer Acceleration (COCA): $N$-body simulations in an\n  emulated frame of reference", "author": "Deaglan J. Bartlett and Marco Chiarenza and Ludvig Doeser and Florent Leclercq", "abstract": "  $N$-body simulations are computationally expensive, so machine-learning\n(ML)-based emulation techniques have emerged as a way to increase their speed.\nAlthough fast, surrogate models have limited trustworthiness due to potentially\nsubstantial emulation errors that current approaches cannot correct for. To\nalleviate this problem, we introduce COmoving Computer Acceleration (COCA), a\nhybrid framework interfacing ML with an $N$-body simulator. The correct\nphysical equations of motion are solved in an emulated frame of reference, so\nthat any emulation error is corrected by design. This approach corresponds to\nsolving for the perturbation of particle trajectories around the machine-learnt\nsolution, which is computationally cheaper than obtaining the full solution,\nyet is guaranteed to converge to the truth as one increases the number of force\nevaluations. Although applicable to any ML algorithm and $N$-body simulator,\nthis approach is assessed in the particular case of particle-mesh cosmological\nsimulations in a frame of reference predicted by a convolutional neural\nnetwork, where the time dependence is encoded as an additional input parameter\nto the network. COCA efficiently reduces emulation errors in particle\ntrajectories, requiring far fewer force evaluations than running the\ncorresponding simulation without ML. We obtain accurate final density and\nvelocity fields for a reduced computational budget. We demonstrate that this\nmethod shows robustness when applied to examples outside the range of the\ntraining data. When compared to the direct emulation of the Lagrangian\ndisplacement field using the same training resources, COCA's ability to correct\nemulation errors results in more accurate predictions. COCA makes $N$-body\nsimulations cheaper by skipping unnecessary force evaluations, while still\nsolving the correct equations of motion and correcting for emulation errors\nmade by ML.\n", "link": "http://arxiv.org/abs/2409.02154v2", "date": "2025-01-21", "relevancy": 2.0047, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5434}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5035}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COmoving%20Computer%20Acceleration%20%28COCA%29%3A%20%24N%24-body%20simulations%20in%20an%0A%20%20emulated%20frame%20of%20reference&body=Title%3A%20COmoving%20Computer%20Acceleration%20%28COCA%29%3A%20%24N%24-body%20simulations%20in%20an%0A%20%20emulated%20frame%20of%20reference%0AAuthor%3A%20Deaglan%20J.%20Bartlett%20and%20Marco%20Chiarenza%20and%20Ludvig%20Doeser%20and%20Florent%20Leclercq%0AAbstract%3A%20%20%20%24N%24-body%20simulations%20are%20computationally%20expensive%2C%20so%20machine-learning%0A%28ML%29-based%20emulation%20techniques%20have%20emerged%20as%20a%20way%20to%20increase%20their%20speed.%0AAlthough%20fast%2C%20surrogate%20models%20have%20limited%20trustworthiness%20due%20to%20potentially%0Asubstantial%20emulation%20errors%20that%20current%20approaches%20cannot%20correct%20for.%20To%0Aalleviate%20this%20problem%2C%20we%20introduce%20COmoving%20Computer%20Acceleration%20%28COCA%29%2C%20a%0Ahybrid%20framework%20interfacing%20ML%20with%20an%20%24N%24-body%20simulator.%20The%20correct%0Aphysical%20equations%20of%20motion%20are%20solved%20in%20an%20emulated%20frame%20of%20reference%2C%20so%0Athat%20any%20emulation%20error%20is%20corrected%20by%20design.%20This%20approach%20corresponds%20to%0Asolving%20for%20the%20perturbation%20of%20particle%20trajectories%20around%20the%20machine-learnt%0Asolution%2C%20which%20is%20computationally%20cheaper%20than%20obtaining%20the%20full%20solution%2C%0Ayet%20is%20guaranteed%20to%20converge%20to%20the%20truth%20as%20one%20increases%20the%20number%20of%20force%0Aevaluations.%20Although%20applicable%20to%20any%20ML%20algorithm%20and%20%24N%24-body%20simulator%2C%0Athis%20approach%20is%20assessed%20in%20the%20particular%20case%20of%20particle-mesh%20cosmological%0Asimulations%20in%20a%20frame%20of%20reference%20predicted%20by%20a%20convolutional%20neural%0Anetwork%2C%20where%20the%20time%20dependence%20is%20encoded%20as%20an%20additional%20input%20parameter%0Ato%20the%20network.%20COCA%20efficiently%20reduces%20emulation%20errors%20in%20particle%0Atrajectories%2C%20requiring%20far%20fewer%20force%20evaluations%20than%20running%20the%0Acorresponding%20simulation%20without%20ML.%20We%20obtain%20accurate%20final%20density%20and%0Avelocity%20fields%20for%20a%20reduced%20computational%20budget.%20We%20demonstrate%20that%20this%0Amethod%20shows%20robustness%20when%20applied%20to%20examples%20outside%20the%20range%20of%20the%0Atraining%20data.%20When%20compared%20to%20the%20direct%20emulation%20of%20the%20Lagrangian%0Adisplacement%20field%20using%20the%20same%20training%20resources%2C%20COCA%27s%20ability%20to%20correct%0Aemulation%20errors%20results%20in%20more%20accurate%20predictions.%20COCA%20makes%20%24N%24-body%0Asimulations%20cheaper%20by%20skipping%20unnecessary%20force%20evaluations%2C%20while%20still%0Asolving%20the%20correct%20equations%20of%20motion%20and%20correcting%20for%20emulation%20errors%0Amade%20by%20ML.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02154v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOmoving%2520Computer%2520Acceleration%2520%2528COCA%2529%253A%2520%2524N%2524-body%2520simulations%2520in%2520an%250A%2520%2520emulated%2520frame%2520of%2520reference%26entry.906535625%3DDeaglan%2520J.%2520Bartlett%2520and%2520Marco%2520Chiarenza%2520and%2520Ludvig%2520Doeser%2520and%2520Florent%2520Leclercq%26entry.1292438233%3D%2520%2520%2524N%2524-body%2520simulations%2520are%2520computationally%2520expensive%252C%2520so%2520machine-learning%250A%2528ML%2529-based%2520emulation%2520techniques%2520have%2520emerged%2520as%2520a%2520way%2520to%2520increase%2520their%2520speed.%250AAlthough%2520fast%252C%2520surrogate%2520models%2520have%2520limited%2520trustworthiness%2520due%2520to%2520potentially%250Asubstantial%2520emulation%2520errors%2520that%2520current%2520approaches%2520cannot%2520correct%2520for.%2520To%250Aalleviate%2520this%2520problem%252C%2520we%2520introduce%2520COmoving%2520Computer%2520Acceleration%2520%2528COCA%2529%252C%2520a%250Ahybrid%2520framework%2520interfacing%2520ML%2520with%2520an%2520%2524N%2524-body%2520simulator.%2520The%2520correct%250Aphysical%2520equations%2520of%2520motion%2520are%2520solved%2520in%2520an%2520emulated%2520frame%2520of%2520reference%252C%2520so%250Athat%2520any%2520emulation%2520error%2520is%2520corrected%2520by%2520design.%2520This%2520approach%2520corresponds%2520to%250Asolving%2520for%2520the%2520perturbation%2520of%2520particle%2520trajectories%2520around%2520the%2520machine-learnt%250Asolution%252C%2520which%2520is%2520computationally%2520cheaper%2520than%2520obtaining%2520the%2520full%2520solution%252C%250Ayet%2520is%2520guaranteed%2520to%2520converge%2520to%2520the%2520truth%2520as%2520one%2520increases%2520the%2520number%2520of%2520force%250Aevaluations.%2520Although%2520applicable%2520to%2520any%2520ML%2520algorithm%2520and%2520%2524N%2524-body%2520simulator%252C%250Athis%2520approach%2520is%2520assessed%2520in%2520the%2520particular%2520case%2520of%2520particle-mesh%2520cosmological%250Asimulations%2520in%2520a%2520frame%2520of%2520reference%2520predicted%2520by%2520a%2520convolutional%2520neural%250Anetwork%252C%2520where%2520the%2520time%2520dependence%2520is%2520encoded%2520as%2520an%2520additional%2520input%2520parameter%250Ato%2520the%2520network.%2520COCA%2520efficiently%2520reduces%2520emulation%2520errors%2520in%2520particle%250Atrajectories%252C%2520requiring%2520far%2520fewer%2520force%2520evaluations%2520than%2520running%2520the%250Acorresponding%2520simulation%2520without%2520ML.%2520We%2520obtain%2520accurate%2520final%2520density%2520and%250Avelocity%2520fields%2520for%2520a%2520reduced%2520computational%2520budget.%2520We%2520demonstrate%2520that%2520this%250Amethod%2520shows%2520robustness%2520when%2520applied%2520to%2520examples%2520outside%2520the%2520range%2520of%2520the%250Atraining%2520data.%2520When%2520compared%2520to%2520the%2520direct%2520emulation%2520of%2520the%2520Lagrangian%250Adisplacement%2520field%2520using%2520the%2520same%2520training%2520resources%252C%2520COCA%2527s%2520ability%2520to%2520correct%250Aemulation%2520errors%2520results%2520in%2520more%2520accurate%2520predictions.%2520COCA%2520makes%2520%2524N%2524-body%250Asimulations%2520cheaper%2520by%2520skipping%2520unnecessary%2520force%2520evaluations%252C%2520while%2520still%250Asolving%2520the%2520correct%2520equations%2520of%2520motion%2520and%2520correcting%2520for%2520emulation%2520errors%250Amade%2520by%2520ML.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02154v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COmoving%20Computer%20Acceleration%20%28COCA%29%3A%20%24N%24-body%20simulations%20in%20an%0A%20%20emulated%20frame%20of%20reference&entry.906535625=Deaglan%20J.%20Bartlett%20and%20Marco%20Chiarenza%20and%20Ludvig%20Doeser%20and%20Florent%20Leclercq&entry.1292438233=%20%20%24N%24-body%20simulations%20are%20computationally%20expensive%2C%20so%20machine-learning%0A%28ML%29-based%20emulation%20techniques%20have%20emerged%20as%20a%20way%20to%20increase%20their%20speed.%0AAlthough%20fast%2C%20surrogate%20models%20have%20limited%20trustworthiness%20due%20to%20potentially%0Asubstantial%20emulation%20errors%20that%20current%20approaches%20cannot%20correct%20for.%20To%0Aalleviate%20this%20problem%2C%20we%20introduce%20COmoving%20Computer%20Acceleration%20%28COCA%29%2C%20a%0Ahybrid%20framework%20interfacing%20ML%20with%20an%20%24N%24-body%20simulator.%20The%20correct%0Aphysical%20equations%20of%20motion%20are%20solved%20in%20an%20emulated%20frame%20of%20reference%2C%20so%0Athat%20any%20emulation%20error%20is%20corrected%20by%20design.%20This%20approach%20corresponds%20to%0Asolving%20for%20the%20perturbation%20of%20particle%20trajectories%20around%20the%20machine-learnt%0Asolution%2C%20which%20is%20computationally%20cheaper%20than%20obtaining%20the%20full%20solution%2C%0Ayet%20is%20guaranteed%20to%20converge%20to%20the%20truth%20as%20one%20increases%20the%20number%20of%20force%0Aevaluations.%20Although%20applicable%20to%20any%20ML%20algorithm%20and%20%24N%24-body%20simulator%2C%0Athis%20approach%20is%20assessed%20in%20the%20particular%20case%20of%20particle-mesh%20cosmological%0Asimulations%20in%20a%20frame%20of%20reference%20predicted%20by%20a%20convolutional%20neural%0Anetwork%2C%20where%20the%20time%20dependence%20is%20encoded%20as%20an%20additional%20input%20parameter%0Ato%20the%20network.%20COCA%20efficiently%20reduces%20emulation%20errors%20in%20particle%0Atrajectories%2C%20requiring%20far%20fewer%20force%20evaluations%20than%20running%20the%0Acorresponding%20simulation%20without%20ML.%20We%20obtain%20accurate%20final%20density%20and%0Avelocity%20fields%20for%20a%20reduced%20computational%20budget.%20We%20demonstrate%20that%20this%0Amethod%20shows%20robustness%20when%20applied%20to%20examples%20outside%20the%20range%20of%20the%0Atraining%20data.%20When%20compared%20to%20the%20direct%20emulation%20of%20the%20Lagrangian%0Adisplacement%20field%20using%20the%20same%20training%20resources%2C%20COCA%27s%20ability%20to%20correct%0Aemulation%20errors%20results%20in%20more%20accurate%20predictions.%20COCA%20makes%20%24N%24-body%0Asimulations%20cheaper%20by%20skipping%20unnecessary%20force%20evaluations%2C%20while%20still%0Asolving%20the%20correct%20equations%20of%20motion%20and%20correcting%20for%20emulation%20errors%0Amade%20by%20ML.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02154v2&entry.124074799=Read"},
{"title": "Uncertainty Quantification With Noise Injection in Neural Networks: A\n  Bayesian Perspective", "author": "Xueqiong Yuan and Jipeng Li and Ercan Engin Kuruoglu", "abstract": "  Model uncertainty quantification involves measuring and evaluating the\nuncertainty linked to a model's predictions, helping assess their reliability\nand confidence. Noise injection is a technique used to enhance the robustness\nof neural networks by introducing randomness. In this paper, we establish a\nconnection between noise injection and uncertainty quantification from a\nBayesian standpoint. We theoretically demonstrate that injecting noise into the\nweights of a neural network is equivalent to Bayesian inference on a deep\nGaussian process. Consequently, we introduce a Monte Carlo Noise Injection\n(MCNI) method, which involves injecting noise into the parameters during\ntraining and performing multiple forward propagations during inference to\nestimate the uncertainty of the prediction. Through simulation and experiments\non regression and classification tasks, our method demonstrates superior\nperformance compared to the baseline model.\n", "link": "http://arxiv.org/abs/2501.12314v1", "date": "2025-01-21", "relevancy": 2.0002, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4963}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Quantification%20With%20Noise%20Injection%20in%20Neural%20Networks%3A%20A%0A%20%20Bayesian%20Perspective&body=Title%3A%20Uncertainty%20Quantification%20With%20Noise%20Injection%20in%20Neural%20Networks%3A%20A%0A%20%20Bayesian%20Perspective%0AAuthor%3A%20Xueqiong%20Yuan%20and%20Jipeng%20Li%20and%20Ercan%20Engin%20Kuruoglu%0AAbstract%3A%20%20%20Model%20uncertainty%20quantification%20involves%20measuring%20and%20evaluating%20the%0Auncertainty%20linked%20to%20a%20model%27s%20predictions%2C%20helping%20assess%20their%20reliability%0Aand%20confidence.%20Noise%20injection%20is%20a%20technique%20used%20to%20enhance%20the%20robustness%0Aof%20neural%20networks%20by%20introducing%20randomness.%20In%20this%20paper%2C%20we%20establish%20a%0Aconnection%20between%20noise%20injection%20and%20uncertainty%20quantification%20from%20a%0ABayesian%20standpoint.%20We%20theoretically%20demonstrate%20that%20injecting%20noise%20into%20the%0Aweights%20of%20a%20neural%20network%20is%20equivalent%20to%20Bayesian%20inference%20on%20a%20deep%0AGaussian%20process.%20Consequently%2C%20we%20introduce%20a%20Monte%20Carlo%20Noise%20Injection%0A%28MCNI%29%20method%2C%20which%20involves%20injecting%20noise%20into%20the%20parameters%20during%0Atraining%20and%20performing%20multiple%20forward%20propagations%20during%20inference%20to%0Aestimate%20the%20uncertainty%20of%20the%20prediction.%20Through%20simulation%20and%20experiments%0Aon%20regression%20and%20classification%20tasks%2C%20our%20method%20demonstrates%20superior%0Aperformance%20compared%20to%20the%20baseline%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Quantification%2520With%2520Noise%2520Injection%2520in%2520Neural%2520Networks%253A%2520A%250A%2520%2520Bayesian%2520Perspective%26entry.906535625%3DXueqiong%2520Yuan%2520and%2520Jipeng%2520Li%2520and%2520Ercan%2520Engin%2520Kuruoglu%26entry.1292438233%3D%2520%2520Model%2520uncertainty%2520quantification%2520involves%2520measuring%2520and%2520evaluating%2520the%250Auncertainty%2520linked%2520to%2520a%2520model%2527s%2520predictions%252C%2520helping%2520assess%2520their%2520reliability%250Aand%2520confidence.%2520Noise%2520injection%2520is%2520a%2520technique%2520used%2520to%2520enhance%2520the%2520robustness%250Aof%2520neural%2520networks%2520by%2520introducing%2520randomness.%2520In%2520this%2520paper%252C%2520we%2520establish%2520a%250Aconnection%2520between%2520noise%2520injection%2520and%2520uncertainty%2520quantification%2520from%2520a%250ABayesian%2520standpoint.%2520We%2520theoretically%2520demonstrate%2520that%2520injecting%2520noise%2520into%2520the%250Aweights%2520of%2520a%2520neural%2520network%2520is%2520equivalent%2520to%2520Bayesian%2520inference%2520on%2520a%2520deep%250AGaussian%2520process.%2520Consequently%252C%2520we%2520introduce%2520a%2520Monte%2520Carlo%2520Noise%2520Injection%250A%2528MCNI%2529%2520method%252C%2520which%2520involves%2520injecting%2520noise%2520into%2520the%2520parameters%2520during%250Atraining%2520and%2520performing%2520multiple%2520forward%2520propagations%2520during%2520inference%2520to%250Aestimate%2520the%2520uncertainty%2520of%2520the%2520prediction.%2520Through%2520simulation%2520and%2520experiments%250Aon%2520regression%2520and%2520classification%2520tasks%252C%2520our%2520method%2520demonstrates%2520superior%250Aperformance%2520compared%2520to%2520the%2520baseline%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20With%20Noise%20Injection%20in%20Neural%20Networks%3A%20A%0A%20%20Bayesian%20Perspective&entry.906535625=Xueqiong%20Yuan%20and%20Jipeng%20Li%20and%20Ercan%20Engin%20Kuruoglu&entry.1292438233=%20%20Model%20uncertainty%20quantification%20involves%20measuring%20and%20evaluating%20the%0Auncertainty%20linked%20to%20a%20model%27s%20predictions%2C%20helping%20assess%20their%20reliability%0Aand%20confidence.%20Noise%20injection%20is%20a%20technique%20used%20to%20enhance%20the%20robustness%0Aof%20neural%20networks%20by%20introducing%20randomness.%20In%20this%20paper%2C%20we%20establish%20a%0Aconnection%20between%20noise%20injection%20and%20uncertainty%20quantification%20from%20a%0ABayesian%20standpoint.%20We%20theoretically%20demonstrate%20that%20injecting%20noise%20into%20the%0Aweights%20of%20a%20neural%20network%20is%20equivalent%20to%20Bayesian%20inference%20on%20a%20deep%0AGaussian%20process.%20Consequently%2C%20we%20introduce%20a%20Monte%20Carlo%20Noise%20Injection%0A%28MCNI%29%20method%2C%20which%20involves%20injecting%20noise%20into%20the%20parameters%20during%0Atraining%20and%20performing%20multiple%20forward%20propagations%20during%20inference%20to%0Aestimate%20the%20uncertainty%20of%20the%20prediction.%20Through%20simulation%20and%20experiments%0Aon%20regression%20and%20classification%20tasks%2C%20our%20method%20demonstrates%20superior%0Aperformance%20compared%20to%20the%20baseline%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12314v1&entry.124074799=Read"},
{"title": "Foreign object segmentation in chest x-rays through anatomy-guided shape\n  insertion", "author": "Constantin Seibold and Hamza Kalisch and Lukas Heine and Simon Rei\u00df and Jens Kleesiek", "abstract": "  In this paper, we tackle the challenge of instance segmentation for foreign\nobjects in chest radiographs, commonly seen in postoperative follow-ups with\nstents, pacemakers, or ingested objects in children. The diversity of foreign\nobjects complicates dense annotation, as shown in insufficient existing\ndatasets. To address this, we propose the simple generation of synthetic data\nthrough (1) insertion of arbitrary shapes (lines, polygons, ellipses) with\nvarying contrasts and opacities, and (2) cut-paste augmentations from a small\nset of semi-automatically extracted labels. These insertions are guided by\nanatomy labels to ensure realistic placements, such as stents appearing only in\nrelevant vessels. Our approach enables networks to segment complex structures\nwith minimal manually labeled data. Notably, it achieves performance comparable\nto fully supervised models while using 93\\% fewer manual annotations.\n", "link": "http://arxiv.org/abs/2501.12022v1", "date": "2025-01-21", "relevancy": 1.9992, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5071}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4978}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foreign%20object%20segmentation%20in%20chest%20x-rays%20through%20anatomy-guided%20shape%0A%20%20insertion&body=Title%3A%20Foreign%20object%20segmentation%20in%20chest%20x-rays%20through%20anatomy-guided%20shape%0A%20%20insertion%0AAuthor%3A%20Constantin%20Seibold%20and%20Hamza%20Kalisch%20and%20Lukas%20Heine%20and%20Simon%20Rei%C3%9F%20and%20Jens%20Kleesiek%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20tackle%20the%20challenge%20of%20instance%20segmentation%20for%20foreign%0Aobjects%20in%20chest%20radiographs%2C%20commonly%20seen%20in%20postoperative%20follow-ups%20with%0Astents%2C%20pacemakers%2C%20or%20ingested%20objects%20in%20children.%20The%20diversity%20of%20foreign%0Aobjects%20complicates%20dense%20annotation%2C%20as%20shown%20in%20insufficient%20existing%0Adatasets.%20To%20address%20this%2C%20we%20propose%20the%20simple%20generation%20of%20synthetic%20data%0Athrough%20%281%29%20insertion%20of%20arbitrary%20shapes%20%28lines%2C%20polygons%2C%20ellipses%29%20with%0Avarying%20contrasts%20and%20opacities%2C%20and%20%282%29%20cut-paste%20augmentations%20from%20a%20small%0Aset%20of%20semi-automatically%20extracted%20labels.%20These%20insertions%20are%20guided%20by%0Aanatomy%20labels%20to%20ensure%20realistic%20placements%2C%20such%20as%20stents%20appearing%20only%20in%0Arelevant%20vessels.%20Our%20approach%20enables%20networks%20to%20segment%20complex%20structures%0Awith%20minimal%20manually%20labeled%20data.%20Notably%2C%20it%20achieves%20performance%20comparable%0Ato%20fully%20supervised%20models%20while%20using%2093%5C%25%20fewer%20manual%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForeign%2520object%2520segmentation%2520in%2520chest%2520x-rays%2520through%2520anatomy-guided%2520shape%250A%2520%2520insertion%26entry.906535625%3DConstantin%2520Seibold%2520and%2520Hamza%2520Kalisch%2520and%2520Lukas%2520Heine%2520and%2520Simon%2520Rei%25C3%259F%2520and%2520Jens%2520Kleesiek%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520tackle%2520the%2520challenge%2520of%2520instance%2520segmentation%2520for%2520foreign%250Aobjects%2520in%2520chest%2520radiographs%252C%2520commonly%2520seen%2520in%2520postoperative%2520follow-ups%2520with%250Astents%252C%2520pacemakers%252C%2520or%2520ingested%2520objects%2520in%2520children.%2520The%2520diversity%2520of%2520foreign%250Aobjects%2520complicates%2520dense%2520annotation%252C%2520as%2520shown%2520in%2520insufficient%2520existing%250Adatasets.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520simple%2520generation%2520of%2520synthetic%2520data%250Athrough%2520%25281%2529%2520insertion%2520of%2520arbitrary%2520shapes%2520%2528lines%252C%2520polygons%252C%2520ellipses%2529%2520with%250Avarying%2520contrasts%2520and%2520opacities%252C%2520and%2520%25282%2529%2520cut-paste%2520augmentations%2520from%2520a%2520small%250Aset%2520of%2520semi-automatically%2520extracted%2520labels.%2520These%2520insertions%2520are%2520guided%2520by%250Aanatomy%2520labels%2520to%2520ensure%2520realistic%2520placements%252C%2520such%2520as%2520stents%2520appearing%2520only%2520in%250Arelevant%2520vessels.%2520Our%2520approach%2520enables%2520networks%2520to%2520segment%2520complex%2520structures%250Awith%2520minimal%2520manually%2520labeled%2520data.%2520Notably%252C%2520it%2520achieves%2520performance%2520comparable%250Ato%2520fully%2520supervised%2520models%2520while%2520using%252093%255C%2525%2520fewer%2520manual%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foreign%20object%20segmentation%20in%20chest%20x-rays%20through%20anatomy-guided%20shape%0A%20%20insertion&entry.906535625=Constantin%20Seibold%20and%20Hamza%20Kalisch%20and%20Lukas%20Heine%20and%20Simon%20Rei%C3%9F%20and%20Jens%20Kleesiek&entry.1292438233=%20%20In%20this%20paper%2C%20we%20tackle%20the%20challenge%20of%20instance%20segmentation%20for%20foreign%0Aobjects%20in%20chest%20radiographs%2C%20commonly%20seen%20in%20postoperative%20follow-ups%20with%0Astents%2C%20pacemakers%2C%20or%20ingested%20objects%20in%20children.%20The%20diversity%20of%20foreign%0Aobjects%20complicates%20dense%20annotation%2C%20as%20shown%20in%20insufficient%20existing%0Adatasets.%20To%20address%20this%2C%20we%20propose%20the%20simple%20generation%20of%20synthetic%20data%0Athrough%20%281%29%20insertion%20of%20arbitrary%20shapes%20%28lines%2C%20polygons%2C%20ellipses%29%20with%0Avarying%20contrasts%20and%20opacities%2C%20and%20%282%29%20cut-paste%20augmentations%20from%20a%20small%0Aset%20of%20semi-automatically%20extracted%20labels.%20These%20insertions%20are%20guided%20by%0Aanatomy%20labels%20to%20ensure%20realistic%20placements%2C%20such%20as%20stents%20appearing%20only%20in%0Arelevant%20vessels.%20Our%20approach%20enables%20networks%20to%20segment%20complex%20structures%0Awith%20minimal%20manually%20labeled%20data.%20Notably%2C%20it%20achieves%20performance%20comparable%0Ato%20fully%20supervised%20models%20while%20using%2093%5C%25%20fewer%20manual%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12022v1&entry.124074799=Read"},
{"title": "Towards autonomous photogrammetric forest inventory using a lightweight\n  under-canopy robotic drone", "author": "V\u00e4in\u00f6 Karjalainen and Niko Koivum\u00e4ki and Teemu Hakala and Jesse Muhojoki and Eric Hyypp\u00e4 and Anand George and Juha Suomalainen and Eija Honkavaara", "abstract": "  Drones are increasingly used in forestry to capture high-resolution remote\nsensing data. While operations above the forest canopy are already highly\nautomated, flying inside forests remains challenging, primarily relying on\nmanual piloting. Inside dense forests, reliance on the Global Navigation\nSatellite System (GNSS) for localization is not feasible. Additionally, the\ndrone must autonomously adjust its flight path to avoid collisions. Recently,\nadvancements in robotics have enabled autonomous drone flights in GNSS-denied\nobstacle-rich areas. In this article, a step towards autonomous forest data\ncollection is taken by building a prototype of a robotic under-canopy drone\nutilizing state-of-the-art open-source methods and validating its performance\nfor data collection inside forests. The autonomous flight capability was\nevaluated through multiple test flights in two boreal forest test sites. The\ntree parameter estimation capability was studied by conducting diameter at\nbreast height (DBH) estimation using onboard stereo camera data and\nphotogrammetric methods. The prototype conducted flights in selected\nchallenging forest environments, and the experiments showed excellent\nperformance in forest reconstruction with a miniaturized stereoscopic\nphotogrammetric system. The stem detection algorithm managed to identify 79.31\n% of the stems. The DBH estimation had a root mean square error (RMSE) of 3.33\ncm (12.79 %) and a bias of 1.01 cm (3.87 %) across all trees. For trees with a\nDBH less than 30 cm, the RMSE was 1.16 cm (5.74 %), and the bias was 0.13 cm\n(0.64 %). When considering the overall performance in terms of DBH accuracy,\nautonomy, and forest complexity, the proposed approach was superior compared to\nmethods proposed in the scientific literature. Results provided valuable\ninsights into autonomous forest reconstruction using drones, and several\nfurther development topics were proposed.\n", "link": "http://arxiv.org/abs/2501.12073v1", "date": "2025-01-21", "relevancy": 1.9834, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5154}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4956}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20autonomous%20photogrammetric%20forest%20inventory%20using%20a%20lightweight%0A%20%20under-canopy%20robotic%20drone&body=Title%3A%20Towards%20autonomous%20photogrammetric%20forest%20inventory%20using%20a%20lightweight%0A%20%20under-canopy%20robotic%20drone%0AAuthor%3A%20V%C3%A4in%C3%B6%20Karjalainen%20and%20Niko%20Koivum%C3%A4ki%20and%20Teemu%20Hakala%20and%20Jesse%20Muhojoki%20and%20Eric%20Hyypp%C3%A4%20and%20Anand%20George%20and%20Juha%20Suomalainen%20and%20Eija%20Honkavaara%0AAbstract%3A%20%20%20Drones%20are%20increasingly%20used%20in%20forestry%20to%20capture%20high-resolution%20remote%0Asensing%20data.%20While%20operations%20above%20the%20forest%20canopy%20are%20already%20highly%0Aautomated%2C%20flying%20inside%20forests%20remains%20challenging%2C%20primarily%20relying%20on%0Amanual%20piloting.%20Inside%20dense%20forests%2C%20reliance%20on%20the%20Global%20Navigation%0ASatellite%20System%20%28GNSS%29%20for%20localization%20is%20not%20feasible.%20Additionally%2C%20the%0Adrone%20must%20autonomously%20adjust%20its%20flight%20path%20to%20avoid%20collisions.%20Recently%2C%0Aadvancements%20in%20robotics%20have%20enabled%20autonomous%20drone%20flights%20in%20GNSS-denied%0Aobstacle-rich%20areas.%20In%20this%20article%2C%20a%20step%20towards%20autonomous%20forest%20data%0Acollection%20is%20taken%20by%20building%20a%20prototype%20of%20a%20robotic%20under-canopy%20drone%0Autilizing%20state-of-the-art%20open-source%20methods%20and%20validating%20its%20performance%0Afor%20data%20collection%20inside%20forests.%20The%20autonomous%20flight%20capability%20was%0Aevaluated%20through%20multiple%20test%20flights%20in%20two%20boreal%20forest%20test%20sites.%20The%0Atree%20parameter%20estimation%20capability%20was%20studied%20by%20conducting%20diameter%20at%0Abreast%20height%20%28DBH%29%20estimation%20using%20onboard%20stereo%20camera%20data%20and%0Aphotogrammetric%20methods.%20The%20prototype%20conducted%20flights%20in%20selected%0Achallenging%20forest%20environments%2C%20and%20the%20experiments%20showed%20excellent%0Aperformance%20in%20forest%20reconstruction%20with%20a%20miniaturized%20stereoscopic%0Aphotogrammetric%20system.%20The%20stem%20detection%20algorithm%20managed%20to%20identify%2079.31%0A%25%20of%20the%20stems.%20The%20DBH%20estimation%20had%20a%20root%20mean%20square%20error%20%28RMSE%29%20of%203.33%0Acm%20%2812.79%20%25%29%20and%20a%20bias%20of%201.01%20cm%20%283.87%20%25%29%20across%20all%20trees.%20For%20trees%20with%20a%0ADBH%20less%20than%2030%20cm%2C%20the%20RMSE%20was%201.16%20cm%20%285.74%20%25%29%2C%20and%20the%20bias%20was%200.13%20cm%0A%280.64%20%25%29.%20When%20considering%20the%20overall%20performance%20in%20terms%20of%20DBH%20accuracy%2C%0Aautonomy%2C%20and%20forest%20complexity%2C%20the%20proposed%20approach%20was%20superior%20compared%20to%0Amethods%20proposed%20in%20the%20scientific%20literature.%20Results%20provided%20valuable%0Ainsights%20into%20autonomous%20forest%20reconstruction%20using%20drones%2C%20and%20several%0Afurther%20development%20topics%20were%20proposed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520autonomous%2520photogrammetric%2520forest%2520inventory%2520using%2520a%2520lightweight%250A%2520%2520under-canopy%2520robotic%2520drone%26entry.906535625%3DV%25C3%25A4in%25C3%25B6%2520Karjalainen%2520and%2520Niko%2520Koivum%25C3%25A4ki%2520and%2520Teemu%2520Hakala%2520and%2520Jesse%2520Muhojoki%2520and%2520Eric%2520Hyypp%25C3%25A4%2520and%2520Anand%2520George%2520and%2520Juha%2520Suomalainen%2520and%2520Eija%2520Honkavaara%26entry.1292438233%3D%2520%2520Drones%2520are%2520increasingly%2520used%2520in%2520forestry%2520to%2520capture%2520high-resolution%2520remote%250Asensing%2520data.%2520While%2520operations%2520above%2520the%2520forest%2520canopy%2520are%2520already%2520highly%250Aautomated%252C%2520flying%2520inside%2520forests%2520remains%2520challenging%252C%2520primarily%2520relying%2520on%250Amanual%2520piloting.%2520Inside%2520dense%2520forests%252C%2520reliance%2520on%2520the%2520Global%2520Navigation%250ASatellite%2520System%2520%2528GNSS%2529%2520for%2520localization%2520is%2520not%2520feasible.%2520Additionally%252C%2520the%250Adrone%2520must%2520autonomously%2520adjust%2520its%2520flight%2520path%2520to%2520avoid%2520collisions.%2520Recently%252C%250Aadvancements%2520in%2520robotics%2520have%2520enabled%2520autonomous%2520drone%2520flights%2520in%2520GNSS-denied%250Aobstacle-rich%2520areas.%2520In%2520this%2520article%252C%2520a%2520step%2520towards%2520autonomous%2520forest%2520data%250Acollection%2520is%2520taken%2520by%2520building%2520a%2520prototype%2520of%2520a%2520robotic%2520under-canopy%2520drone%250Autilizing%2520state-of-the-art%2520open-source%2520methods%2520and%2520validating%2520its%2520performance%250Afor%2520data%2520collection%2520inside%2520forests.%2520The%2520autonomous%2520flight%2520capability%2520was%250Aevaluated%2520through%2520multiple%2520test%2520flights%2520in%2520two%2520boreal%2520forest%2520test%2520sites.%2520The%250Atree%2520parameter%2520estimation%2520capability%2520was%2520studied%2520by%2520conducting%2520diameter%2520at%250Abreast%2520height%2520%2528DBH%2529%2520estimation%2520using%2520onboard%2520stereo%2520camera%2520data%2520and%250Aphotogrammetric%2520methods.%2520The%2520prototype%2520conducted%2520flights%2520in%2520selected%250Achallenging%2520forest%2520environments%252C%2520and%2520the%2520experiments%2520showed%2520excellent%250Aperformance%2520in%2520forest%2520reconstruction%2520with%2520a%2520miniaturized%2520stereoscopic%250Aphotogrammetric%2520system.%2520The%2520stem%2520detection%2520algorithm%2520managed%2520to%2520identify%252079.31%250A%2525%2520of%2520the%2520stems.%2520The%2520DBH%2520estimation%2520had%2520a%2520root%2520mean%2520square%2520error%2520%2528RMSE%2529%2520of%25203.33%250Acm%2520%252812.79%2520%2525%2529%2520and%2520a%2520bias%2520of%25201.01%2520cm%2520%25283.87%2520%2525%2529%2520across%2520all%2520trees.%2520For%2520trees%2520with%2520a%250ADBH%2520less%2520than%252030%2520cm%252C%2520the%2520RMSE%2520was%25201.16%2520cm%2520%25285.74%2520%2525%2529%252C%2520and%2520the%2520bias%2520was%25200.13%2520cm%250A%25280.64%2520%2525%2529.%2520When%2520considering%2520the%2520overall%2520performance%2520in%2520terms%2520of%2520DBH%2520accuracy%252C%250Aautonomy%252C%2520and%2520forest%2520complexity%252C%2520the%2520proposed%2520approach%2520was%2520superior%2520compared%2520to%250Amethods%2520proposed%2520in%2520the%2520scientific%2520literature.%2520Results%2520provided%2520valuable%250Ainsights%2520into%2520autonomous%2520forest%2520reconstruction%2520using%2520drones%252C%2520and%2520several%250Afurther%2520development%2520topics%2520were%2520proposed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20autonomous%20photogrammetric%20forest%20inventory%20using%20a%20lightweight%0A%20%20under-canopy%20robotic%20drone&entry.906535625=V%C3%A4in%C3%B6%20Karjalainen%20and%20Niko%20Koivum%C3%A4ki%20and%20Teemu%20Hakala%20and%20Jesse%20Muhojoki%20and%20Eric%20Hyypp%C3%A4%20and%20Anand%20George%20and%20Juha%20Suomalainen%20and%20Eija%20Honkavaara&entry.1292438233=%20%20Drones%20are%20increasingly%20used%20in%20forestry%20to%20capture%20high-resolution%20remote%0Asensing%20data.%20While%20operations%20above%20the%20forest%20canopy%20are%20already%20highly%0Aautomated%2C%20flying%20inside%20forests%20remains%20challenging%2C%20primarily%20relying%20on%0Amanual%20piloting.%20Inside%20dense%20forests%2C%20reliance%20on%20the%20Global%20Navigation%0ASatellite%20System%20%28GNSS%29%20for%20localization%20is%20not%20feasible.%20Additionally%2C%20the%0Adrone%20must%20autonomously%20adjust%20its%20flight%20path%20to%20avoid%20collisions.%20Recently%2C%0Aadvancements%20in%20robotics%20have%20enabled%20autonomous%20drone%20flights%20in%20GNSS-denied%0Aobstacle-rich%20areas.%20In%20this%20article%2C%20a%20step%20towards%20autonomous%20forest%20data%0Acollection%20is%20taken%20by%20building%20a%20prototype%20of%20a%20robotic%20under-canopy%20drone%0Autilizing%20state-of-the-art%20open-source%20methods%20and%20validating%20its%20performance%0Afor%20data%20collection%20inside%20forests.%20The%20autonomous%20flight%20capability%20was%0Aevaluated%20through%20multiple%20test%20flights%20in%20two%20boreal%20forest%20test%20sites.%20The%0Atree%20parameter%20estimation%20capability%20was%20studied%20by%20conducting%20diameter%20at%0Abreast%20height%20%28DBH%29%20estimation%20using%20onboard%20stereo%20camera%20data%20and%0Aphotogrammetric%20methods.%20The%20prototype%20conducted%20flights%20in%20selected%0Achallenging%20forest%20environments%2C%20and%20the%20experiments%20showed%20excellent%0Aperformance%20in%20forest%20reconstruction%20with%20a%20miniaturized%20stereoscopic%0Aphotogrammetric%20system.%20The%20stem%20detection%20algorithm%20managed%20to%20identify%2079.31%0A%25%20of%20the%20stems.%20The%20DBH%20estimation%20had%20a%20root%20mean%20square%20error%20%28RMSE%29%20of%203.33%0Acm%20%2812.79%20%25%29%20and%20a%20bias%20of%201.01%20cm%20%283.87%20%25%29%20across%20all%20trees.%20For%20trees%20with%20a%0ADBH%20less%20than%2030%20cm%2C%20the%20RMSE%20was%201.16%20cm%20%285.74%20%25%29%2C%20and%20the%20bias%20was%200.13%20cm%0A%280.64%20%25%29.%20When%20considering%20the%20overall%20performance%20in%20terms%20of%20DBH%20accuracy%2C%0Aautonomy%2C%20and%20forest%20complexity%2C%20the%20proposed%20approach%20was%20superior%20compared%20to%0Amethods%20proposed%20in%20the%20scientific%20literature.%20Results%20provided%20valuable%0Ainsights%20into%20autonomous%20forest%20reconstruction%20using%20drones%2C%20and%20several%0Afurther%20development%20topics%20were%20proposed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12073v1&entry.124074799=Read"},
{"title": "Test-time regression: a unifying framework for designing sequence models\n  with associative memory", "author": "Ke Alexander Wang and Jiaxin Shi and Emily B. Fox", "abstract": "  Sequences provide a remarkably general way to represent and process\ninformation. This powerful abstraction has placed sequence modeling at the\ncenter of modern deep learning applications, inspiring numerous architectures\nfrom transformers to recurrent networks. While this fragmented development has\nyielded powerful models, it has left us without a unified framework to\nunderstand their fundamental similarities and explain their effectiveness. We\npresent a unifying framework motivated by an empirical observation: effective\nsequence models must be able to perform associative recall. Our key insight is\nthat memorizing input tokens through an associative memory is equivalent to\nperforming regression at test-time. This regression-memory correspondence\nprovides a framework for deriving sequence models that can perform associative\nrecall, offering a systematic lens to understand seemingly ad-hoc architectural\nchoices. We show numerous recent architectures -- including linear attention\nmodels, their gated variants, state-space models, online learners, and softmax\nattention -- emerge naturally as specific approaches to test-time regression.\nEach architecture corresponds to three design choices: the relative importance\nof each association, the regressor function class, and the optimization\nalgorithm. This connection leads to new understanding: we provide theoretical\njustification for QKNorm in softmax attention, and we motivate higher-order\ngeneralizations of softmax attention. Beyond unification, our work unlocks\ndecades of rich statistical tools that can guide future development of more\npowerful yet principled sequence models.\n", "link": "http://arxiv.org/abs/2501.12352v1", "date": "2025-01-21", "relevancy": 1.9829, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4993}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-time%20regression%3A%20a%20unifying%20framework%20for%20designing%20sequence%20models%0A%20%20with%20associative%20memory&body=Title%3A%20Test-time%20regression%3A%20a%20unifying%20framework%20for%20designing%20sequence%20models%0A%20%20with%20associative%20memory%0AAuthor%3A%20Ke%20Alexander%20Wang%20and%20Jiaxin%20Shi%20and%20Emily%20B.%20Fox%0AAbstract%3A%20%20%20Sequences%20provide%20a%20remarkably%20general%20way%20to%20represent%20and%20process%0Ainformation.%20This%20powerful%20abstraction%20has%20placed%20sequence%20modeling%20at%20the%0Acenter%20of%20modern%20deep%20learning%20applications%2C%20inspiring%20numerous%20architectures%0Afrom%20transformers%20to%20recurrent%20networks.%20While%20this%20fragmented%20development%20has%0Ayielded%20powerful%20models%2C%20it%20has%20left%20us%20without%20a%20unified%20framework%20to%0Aunderstand%20their%20fundamental%20similarities%20and%20explain%20their%20effectiveness.%20We%0Apresent%20a%20unifying%20framework%20motivated%20by%20an%20empirical%20observation%3A%20effective%0Asequence%20models%20must%20be%20able%20to%20perform%20associative%20recall.%20Our%20key%20insight%20is%0Athat%20memorizing%20input%20tokens%20through%20an%20associative%20memory%20is%20equivalent%20to%0Aperforming%20regression%20at%20test-time.%20This%20regression-memory%20correspondence%0Aprovides%20a%20framework%20for%20deriving%20sequence%20models%20that%20can%20perform%20associative%0Arecall%2C%20offering%20a%20systematic%20lens%20to%20understand%20seemingly%20ad-hoc%20architectural%0Achoices.%20We%20show%20numerous%20recent%20architectures%20--%20including%20linear%20attention%0Amodels%2C%20their%20gated%20variants%2C%20state-space%20models%2C%20online%20learners%2C%20and%20softmax%0Aattention%20--%20emerge%20naturally%20as%20specific%20approaches%20to%20test-time%20regression.%0AEach%20architecture%20corresponds%20to%20three%20design%20choices%3A%20the%20relative%20importance%0Aof%20each%20association%2C%20the%20regressor%20function%20class%2C%20and%20the%20optimization%0Aalgorithm.%20This%20connection%20leads%20to%20new%20understanding%3A%20we%20provide%20theoretical%0Ajustification%20for%20QKNorm%20in%20softmax%20attention%2C%20and%20we%20motivate%20higher-order%0Ageneralizations%20of%20softmax%20attention.%20Beyond%20unification%2C%20our%20work%20unlocks%0Adecades%20of%20rich%20statistical%20tools%20that%20can%20guide%20future%20development%20of%20more%0Apowerful%20yet%20principled%20sequence%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-time%2520regression%253A%2520a%2520unifying%2520framework%2520for%2520designing%2520sequence%2520models%250A%2520%2520with%2520associative%2520memory%26entry.906535625%3DKe%2520Alexander%2520Wang%2520and%2520Jiaxin%2520Shi%2520and%2520Emily%2520B.%2520Fox%26entry.1292438233%3D%2520%2520Sequences%2520provide%2520a%2520remarkably%2520general%2520way%2520to%2520represent%2520and%2520process%250Ainformation.%2520This%2520powerful%2520abstraction%2520has%2520placed%2520sequence%2520modeling%2520at%2520the%250Acenter%2520of%2520modern%2520deep%2520learning%2520applications%252C%2520inspiring%2520numerous%2520architectures%250Afrom%2520transformers%2520to%2520recurrent%2520networks.%2520While%2520this%2520fragmented%2520development%2520has%250Ayielded%2520powerful%2520models%252C%2520it%2520has%2520left%2520us%2520without%2520a%2520unified%2520framework%2520to%250Aunderstand%2520their%2520fundamental%2520similarities%2520and%2520explain%2520their%2520effectiveness.%2520We%250Apresent%2520a%2520unifying%2520framework%2520motivated%2520by%2520an%2520empirical%2520observation%253A%2520effective%250Asequence%2520models%2520must%2520be%2520able%2520to%2520perform%2520associative%2520recall.%2520Our%2520key%2520insight%2520is%250Athat%2520memorizing%2520input%2520tokens%2520through%2520an%2520associative%2520memory%2520is%2520equivalent%2520to%250Aperforming%2520regression%2520at%2520test-time.%2520This%2520regression-memory%2520correspondence%250Aprovides%2520a%2520framework%2520for%2520deriving%2520sequence%2520models%2520that%2520can%2520perform%2520associative%250Arecall%252C%2520offering%2520a%2520systematic%2520lens%2520to%2520understand%2520seemingly%2520ad-hoc%2520architectural%250Achoices.%2520We%2520show%2520numerous%2520recent%2520architectures%2520--%2520including%2520linear%2520attention%250Amodels%252C%2520their%2520gated%2520variants%252C%2520state-space%2520models%252C%2520online%2520learners%252C%2520and%2520softmax%250Aattention%2520--%2520emerge%2520naturally%2520as%2520specific%2520approaches%2520to%2520test-time%2520regression.%250AEach%2520architecture%2520corresponds%2520to%2520three%2520design%2520choices%253A%2520the%2520relative%2520importance%250Aof%2520each%2520association%252C%2520the%2520regressor%2520function%2520class%252C%2520and%2520the%2520optimization%250Aalgorithm.%2520This%2520connection%2520leads%2520to%2520new%2520understanding%253A%2520we%2520provide%2520theoretical%250Ajustification%2520for%2520QKNorm%2520in%2520softmax%2520attention%252C%2520and%2520we%2520motivate%2520higher-order%250Ageneralizations%2520of%2520softmax%2520attention.%2520Beyond%2520unification%252C%2520our%2520work%2520unlocks%250Adecades%2520of%2520rich%2520statistical%2520tools%2520that%2520can%2520guide%2520future%2520development%2520of%2520more%250Apowerful%2520yet%2520principled%2520sequence%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-time%20regression%3A%20a%20unifying%20framework%20for%20designing%20sequence%20models%0A%20%20with%20associative%20memory&entry.906535625=Ke%20Alexander%20Wang%20and%20Jiaxin%20Shi%20and%20Emily%20B.%20Fox&entry.1292438233=%20%20Sequences%20provide%20a%20remarkably%20general%20way%20to%20represent%20and%20process%0Ainformation.%20This%20powerful%20abstraction%20has%20placed%20sequence%20modeling%20at%20the%0Acenter%20of%20modern%20deep%20learning%20applications%2C%20inspiring%20numerous%20architectures%0Afrom%20transformers%20to%20recurrent%20networks.%20While%20this%20fragmented%20development%20has%0Ayielded%20powerful%20models%2C%20it%20has%20left%20us%20without%20a%20unified%20framework%20to%0Aunderstand%20their%20fundamental%20similarities%20and%20explain%20their%20effectiveness.%20We%0Apresent%20a%20unifying%20framework%20motivated%20by%20an%20empirical%20observation%3A%20effective%0Asequence%20models%20must%20be%20able%20to%20perform%20associative%20recall.%20Our%20key%20insight%20is%0Athat%20memorizing%20input%20tokens%20through%20an%20associative%20memory%20is%20equivalent%20to%0Aperforming%20regression%20at%20test-time.%20This%20regression-memory%20correspondence%0Aprovides%20a%20framework%20for%20deriving%20sequence%20models%20that%20can%20perform%20associative%0Arecall%2C%20offering%20a%20systematic%20lens%20to%20understand%20seemingly%20ad-hoc%20architectural%0Achoices.%20We%20show%20numerous%20recent%20architectures%20--%20including%20linear%20attention%0Amodels%2C%20their%20gated%20variants%2C%20state-space%20models%2C%20online%20learners%2C%20and%20softmax%0Aattention%20--%20emerge%20naturally%20as%20specific%20approaches%20to%20test-time%20regression.%0AEach%20architecture%20corresponds%20to%20three%20design%20choices%3A%20the%20relative%20importance%0Aof%20each%20association%2C%20the%20regressor%20function%20class%2C%20and%20the%20optimization%0Aalgorithm.%20This%20connection%20leads%20to%20new%20understanding%3A%20we%20provide%20theoretical%0Ajustification%20for%20QKNorm%20in%20softmax%20attention%2C%20and%20we%20motivate%20higher-order%0Ageneralizations%20of%20softmax%20attention.%20Beyond%20unification%2C%20our%20work%20unlocks%0Adecades%20of%20rich%20statistical%20tools%20that%20can%20guide%20future%20development%20of%20more%0Apowerful%20yet%20principled%20sequence%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12352v1&entry.124074799=Read"},
{"title": "Adaptive Class Learning to Screen Diabetic Disorders in Fundus Images of\n  Eye", "author": "Shramana Dey and Pallabi Dutta and Riddhasree Bhattacharyya and Surochita Pal and Sushmita Mitra and Rajiv Raman", "abstract": "  The prevalence of ocular illnesses is growing globally, presenting a\nsubstantial public health challenge. Early detection and timely intervention\nare crucial for averting visual impairment and enhancing patient prognosis.\nThis research introduces a new framework called Class Extension with Limited\nData (CELD) to train a classifier to categorize retinal fundus images. The\nclassifier is initially trained to identify relevant features concerning\nHealthy and Diabetic Retinopathy (DR) classes and later fine-tuned to adapt to\nthe task of classifying the input images into three classes: Healthy, DR, and\nGlaucoma. This strategy allows the model to gradually enhance its\nclassification capabilities, which is beneficial in situations where there are\nonly a limited number of labeled datasets available. Perturbation methods are\nalso used to identify the input image characteristics responsible for\ninfluencing the models decision-making process. We achieve an overall accuracy\nof 91% on publicly available datasets.\n", "link": "http://arxiv.org/abs/2501.12048v1", "date": "2025-01-21", "relevancy": 1.977, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5072}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4853}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Class%20Learning%20to%20Screen%20Diabetic%20Disorders%20in%20Fundus%20Images%20of%0A%20%20Eye&body=Title%3A%20Adaptive%20Class%20Learning%20to%20Screen%20Diabetic%20Disorders%20in%20Fundus%20Images%20of%0A%20%20Eye%0AAuthor%3A%20Shramana%20Dey%20and%20Pallabi%20Dutta%20and%20Riddhasree%20Bhattacharyya%20and%20Surochita%20Pal%20and%20Sushmita%20Mitra%20and%20Rajiv%20Raman%0AAbstract%3A%20%20%20The%20prevalence%20of%20ocular%20illnesses%20is%20growing%20globally%2C%20presenting%20a%0Asubstantial%20public%20health%20challenge.%20Early%20detection%20and%20timely%20intervention%0Aare%20crucial%20for%20averting%20visual%20impairment%20and%20enhancing%20patient%20prognosis.%0AThis%20research%20introduces%20a%20new%20framework%20called%20Class%20Extension%20with%20Limited%0AData%20%28CELD%29%20to%20train%20a%20classifier%20to%20categorize%20retinal%20fundus%20images.%20The%0Aclassifier%20is%20initially%20trained%20to%20identify%20relevant%20features%20concerning%0AHealthy%20and%20Diabetic%20Retinopathy%20%28DR%29%20classes%20and%20later%20fine-tuned%20to%20adapt%20to%0Athe%20task%20of%20classifying%20the%20input%20images%20into%20three%20classes%3A%20Healthy%2C%20DR%2C%20and%0AGlaucoma.%20This%20strategy%20allows%20the%20model%20to%20gradually%20enhance%20its%0Aclassification%20capabilities%2C%20which%20is%20beneficial%20in%20situations%20where%20there%20are%0Aonly%20a%20limited%20number%20of%20labeled%20datasets%20available.%20Perturbation%20methods%20are%0Aalso%20used%20to%20identify%20the%20input%20image%20characteristics%20responsible%20for%0Ainfluencing%20the%20models%20decision-making%20process.%20We%20achieve%20an%20overall%20accuracy%0Aof%2091%25%20on%20publicly%20available%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Class%2520Learning%2520to%2520Screen%2520Diabetic%2520Disorders%2520in%2520Fundus%2520Images%2520of%250A%2520%2520Eye%26entry.906535625%3DShramana%2520Dey%2520and%2520Pallabi%2520Dutta%2520and%2520Riddhasree%2520Bhattacharyya%2520and%2520Surochita%2520Pal%2520and%2520Sushmita%2520Mitra%2520and%2520Rajiv%2520Raman%26entry.1292438233%3D%2520%2520The%2520prevalence%2520of%2520ocular%2520illnesses%2520is%2520growing%2520globally%252C%2520presenting%2520a%250Asubstantial%2520public%2520health%2520challenge.%2520Early%2520detection%2520and%2520timely%2520intervention%250Aare%2520crucial%2520for%2520averting%2520visual%2520impairment%2520and%2520enhancing%2520patient%2520prognosis.%250AThis%2520research%2520introduces%2520a%2520new%2520framework%2520called%2520Class%2520Extension%2520with%2520Limited%250AData%2520%2528CELD%2529%2520to%2520train%2520a%2520classifier%2520to%2520categorize%2520retinal%2520fundus%2520images.%2520The%250Aclassifier%2520is%2520initially%2520trained%2520to%2520identify%2520relevant%2520features%2520concerning%250AHealthy%2520and%2520Diabetic%2520Retinopathy%2520%2528DR%2529%2520classes%2520and%2520later%2520fine-tuned%2520to%2520adapt%2520to%250Athe%2520task%2520of%2520classifying%2520the%2520input%2520images%2520into%2520three%2520classes%253A%2520Healthy%252C%2520DR%252C%2520and%250AGlaucoma.%2520This%2520strategy%2520allows%2520the%2520model%2520to%2520gradually%2520enhance%2520its%250Aclassification%2520capabilities%252C%2520which%2520is%2520beneficial%2520in%2520situations%2520where%2520there%2520are%250Aonly%2520a%2520limited%2520number%2520of%2520labeled%2520datasets%2520available.%2520Perturbation%2520methods%2520are%250Aalso%2520used%2520to%2520identify%2520the%2520input%2520image%2520characteristics%2520responsible%2520for%250Ainfluencing%2520the%2520models%2520decision-making%2520process.%2520We%2520achieve%2520an%2520overall%2520accuracy%250Aof%252091%2525%2520on%2520publicly%2520available%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Class%20Learning%20to%20Screen%20Diabetic%20Disorders%20in%20Fundus%20Images%20of%0A%20%20Eye&entry.906535625=Shramana%20Dey%20and%20Pallabi%20Dutta%20and%20Riddhasree%20Bhattacharyya%20and%20Surochita%20Pal%20and%20Sushmita%20Mitra%20and%20Rajiv%20Raman&entry.1292438233=%20%20The%20prevalence%20of%20ocular%20illnesses%20is%20growing%20globally%2C%20presenting%20a%0Asubstantial%20public%20health%20challenge.%20Early%20detection%20and%20timely%20intervention%0Aare%20crucial%20for%20averting%20visual%20impairment%20and%20enhancing%20patient%20prognosis.%0AThis%20research%20introduces%20a%20new%20framework%20called%20Class%20Extension%20with%20Limited%0AData%20%28CELD%29%20to%20train%20a%20classifier%20to%20categorize%20retinal%20fundus%20images.%20The%0Aclassifier%20is%20initially%20trained%20to%20identify%20relevant%20features%20concerning%0AHealthy%20and%20Diabetic%20Retinopathy%20%28DR%29%20classes%20and%20later%20fine-tuned%20to%20adapt%20to%0Athe%20task%20of%20classifying%20the%20input%20images%20into%20three%20classes%3A%20Healthy%2C%20DR%2C%20and%0AGlaucoma.%20This%20strategy%20allows%20the%20model%20to%20gradually%20enhance%20its%0Aclassification%20capabilities%2C%20which%20is%20beneficial%20in%20situations%20where%20there%20are%0Aonly%20a%20limited%20number%20of%20labeled%20datasets%20available.%20Perturbation%20methods%20are%0Aalso%20used%20to%20identify%20the%20input%20image%20characteristics%20responsible%20for%0Ainfluencing%20the%20models%20decision-making%20process.%20We%20achieve%20an%20overall%20accuracy%0Aof%2091%25%20on%20publicly%20available%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12048v1&entry.124074799=Read"},
{"title": "LLM-Assisted Knowledge Graph Completion for Curriculum and Domain\n  Modelling in Personalized Higher Education Recommendations", "author": "Hasan Abu-Rasheed and Constance Jumbo and Rashed Al Amin and Christian Weber and Veit Wiese and Roman Obermaisser and Madjid Fathi", "abstract": "  While learning personalization offers great potential for learners, modern\npractices in higher education require a deeper consideration of domain models\nand learning contexts, to develop effective personalization algorithms. This\npaper introduces an innovative approach to higher education curriculum\nmodelling that utilizes large language models (LLMs) for knowledge graph (KG)\ncompletion, with the goal of creating personalized learning-path\nrecommendations. Our research focuses on modelling university subjects and\nlinking their topics to corresponding domain models, enabling the integration\nof learning modules from different faculties and institutions in the student's\nlearning path. Central to our approach is a collaborative process, where LLMs\nassist human experts in extracting high-quality, fine-grained topics from\nlecture materials. We develop a domain, curriculum, and user models for\nuniversity modules and stakeholders. We implement this model to create the KG\nfrom two study modules: Embedded Systems and Development of Embedded Systems\nUsing FPGA. The resulting KG structures the curriculum and links it to the\ndomain models. We evaluate our approach through qualitative expert feedback and\nquantitative graph quality metrics. Domain experts validated the relevance and\naccuracy of the model, while the graph quality metrics measured the structural\nproperties of our KG. Our results show that the LLM-assisted graph completion\napproach enhances the ability to connect related courses across disciplines to\npersonalize the learning experience. Expert feedback also showed high\nacceptance of the proposed collaborative approach for concept extraction and\nclassification.\n", "link": "http://arxiv.org/abs/2501.12300v1", "date": "2025-01-21", "relevancy": 1.9735, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4803}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Assisted%20Knowledge%20Graph%20Completion%20for%20Curriculum%20and%20Domain%0A%20%20Modelling%20in%20Personalized%20Higher%20Education%20Recommendations&body=Title%3A%20LLM-Assisted%20Knowledge%20Graph%20Completion%20for%20Curriculum%20and%20Domain%0A%20%20Modelling%20in%20Personalized%20Higher%20Education%20Recommendations%0AAuthor%3A%20Hasan%20Abu-Rasheed%20and%20Constance%20Jumbo%20and%20Rashed%20Al%20Amin%20and%20Christian%20Weber%20and%20Veit%20Wiese%20and%20Roman%20Obermaisser%20and%20Madjid%20Fathi%0AAbstract%3A%20%20%20While%20learning%20personalization%20offers%20great%20potential%20for%20learners%2C%20modern%0Apractices%20in%20higher%20education%20require%20a%20deeper%20consideration%20of%20domain%20models%0Aand%20learning%20contexts%2C%20to%20develop%20effective%20personalization%20algorithms.%20This%0Apaper%20introduces%20an%20innovative%20approach%20to%20higher%20education%20curriculum%0Amodelling%20that%20utilizes%20large%20language%20models%20%28LLMs%29%20for%20knowledge%20graph%20%28KG%29%0Acompletion%2C%20with%20the%20goal%20of%20creating%20personalized%20learning-path%0Arecommendations.%20Our%20research%20focuses%20on%20modelling%20university%20subjects%20and%0Alinking%20their%20topics%20to%20corresponding%20domain%20models%2C%20enabling%20the%20integration%0Aof%20learning%20modules%20from%20different%20faculties%20and%20institutions%20in%20the%20student%27s%0Alearning%20path.%20Central%20to%20our%20approach%20is%20a%20collaborative%20process%2C%20where%20LLMs%0Aassist%20human%20experts%20in%20extracting%20high-quality%2C%20fine-grained%20topics%20from%0Alecture%20materials.%20We%20develop%20a%20domain%2C%20curriculum%2C%20and%20user%20models%20for%0Auniversity%20modules%20and%20stakeholders.%20We%20implement%20this%20model%20to%20create%20the%20KG%0Afrom%20two%20study%20modules%3A%20Embedded%20Systems%20and%20Development%20of%20Embedded%20Systems%0AUsing%20FPGA.%20The%20resulting%20KG%20structures%20the%20curriculum%20and%20links%20it%20to%20the%0Adomain%20models.%20We%20evaluate%20our%20approach%20through%20qualitative%20expert%20feedback%20and%0Aquantitative%20graph%20quality%20metrics.%20Domain%20experts%20validated%20the%20relevance%20and%0Aaccuracy%20of%20the%20model%2C%20while%20the%20graph%20quality%20metrics%20measured%20the%20structural%0Aproperties%20of%20our%20KG.%20Our%20results%20show%20that%20the%20LLM-assisted%20graph%20completion%0Aapproach%20enhances%20the%20ability%20to%20connect%20related%20courses%20across%20disciplines%20to%0Apersonalize%20the%20learning%20experience.%20Expert%20feedback%20also%20showed%20high%0Aacceptance%20of%20the%20proposed%20collaborative%20approach%20for%20concept%20extraction%20and%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Assisted%2520Knowledge%2520Graph%2520Completion%2520for%2520Curriculum%2520and%2520Domain%250A%2520%2520Modelling%2520in%2520Personalized%2520Higher%2520Education%2520Recommendations%26entry.906535625%3DHasan%2520Abu-Rasheed%2520and%2520Constance%2520Jumbo%2520and%2520Rashed%2520Al%2520Amin%2520and%2520Christian%2520Weber%2520and%2520Veit%2520Wiese%2520and%2520Roman%2520Obermaisser%2520and%2520Madjid%2520Fathi%26entry.1292438233%3D%2520%2520While%2520learning%2520personalization%2520offers%2520great%2520potential%2520for%2520learners%252C%2520modern%250Apractices%2520in%2520higher%2520education%2520require%2520a%2520deeper%2520consideration%2520of%2520domain%2520models%250Aand%2520learning%2520contexts%252C%2520to%2520develop%2520effective%2520personalization%2520algorithms.%2520This%250Apaper%2520introduces%2520an%2520innovative%2520approach%2520to%2520higher%2520education%2520curriculum%250Amodelling%2520that%2520utilizes%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520knowledge%2520graph%2520%2528KG%2529%250Acompletion%252C%2520with%2520the%2520goal%2520of%2520creating%2520personalized%2520learning-path%250Arecommendations.%2520Our%2520research%2520focuses%2520on%2520modelling%2520university%2520subjects%2520and%250Alinking%2520their%2520topics%2520to%2520corresponding%2520domain%2520models%252C%2520enabling%2520the%2520integration%250Aof%2520learning%2520modules%2520from%2520different%2520faculties%2520and%2520institutions%2520in%2520the%2520student%2527s%250Alearning%2520path.%2520Central%2520to%2520our%2520approach%2520is%2520a%2520collaborative%2520process%252C%2520where%2520LLMs%250Aassist%2520human%2520experts%2520in%2520extracting%2520high-quality%252C%2520fine-grained%2520topics%2520from%250Alecture%2520materials.%2520We%2520develop%2520a%2520domain%252C%2520curriculum%252C%2520and%2520user%2520models%2520for%250Auniversity%2520modules%2520and%2520stakeholders.%2520We%2520implement%2520this%2520model%2520to%2520create%2520the%2520KG%250Afrom%2520two%2520study%2520modules%253A%2520Embedded%2520Systems%2520and%2520Development%2520of%2520Embedded%2520Systems%250AUsing%2520FPGA.%2520The%2520resulting%2520KG%2520structures%2520the%2520curriculum%2520and%2520links%2520it%2520to%2520the%250Adomain%2520models.%2520We%2520evaluate%2520our%2520approach%2520through%2520qualitative%2520expert%2520feedback%2520and%250Aquantitative%2520graph%2520quality%2520metrics.%2520Domain%2520experts%2520validated%2520the%2520relevance%2520and%250Aaccuracy%2520of%2520the%2520model%252C%2520while%2520the%2520graph%2520quality%2520metrics%2520measured%2520the%2520structural%250Aproperties%2520of%2520our%2520KG.%2520Our%2520results%2520show%2520that%2520the%2520LLM-assisted%2520graph%2520completion%250Aapproach%2520enhances%2520the%2520ability%2520to%2520connect%2520related%2520courses%2520across%2520disciplines%2520to%250Apersonalize%2520the%2520learning%2520experience.%2520Expert%2520feedback%2520also%2520showed%2520high%250Aacceptance%2520of%2520the%2520proposed%2520collaborative%2520approach%2520for%2520concept%2520extraction%2520and%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Assisted%20Knowledge%20Graph%20Completion%20for%20Curriculum%20and%20Domain%0A%20%20Modelling%20in%20Personalized%20Higher%20Education%20Recommendations&entry.906535625=Hasan%20Abu-Rasheed%20and%20Constance%20Jumbo%20and%20Rashed%20Al%20Amin%20and%20Christian%20Weber%20and%20Veit%20Wiese%20and%20Roman%20Obermaisser%20and%20Madjid%20Fathi&entry.1292438233=%20%20While%20learning%20personalization%20offers%20great%20potential%20for%20learners%2C%20modern%0Apractices%20in%20higher%20education%20require%20a%20deeper%20consideration%20of%20domain%20models%0Aand%20learning%20contexts%2C%20to%20develop%20effective%20personalization%20algorithms.%20This%0Apaper%20introduces%20an%20innovative%20approach%20to%20higher%20education%20curriculum%0Amodelling%20that%20utilizes%20large%20language%20models%20%28LLMs%29%20for%20knowledge%20graph%20%28KG%29%0Acompletion%2C%20with%20the%20goal%20of%20creating%20personalized%20learning-path%0Arecommendations.%20Our%20research%20focuses%20on%20modelling%20university%20subjects%20and%0Alinking%20their%20topics%20to%20corresponding%20domain%20models%2C%20enabling%20the%20integration%0Aof%20learning%20modules%20from%20different%20faculties%20and%20institutions%20in%20the%20student%27s%0Alearning%20path.%20Central%20to%20our%20approach%20is%20a%20collaborative%20process%2C%20where%20LLMs%0Aassist%20human%20experts%20in%20extracting%20high-quality%2C%20fine-grained%20topics%20from%0Alecture%20materials.%20We%20develop%20a%20domain%2C%20curriculum%2C%20and%20user%20models%20for%0Auniversity%20modules%20and%20stakeholders.%20We%20implement%20this%20model%20to%20create%20the%20KG%0Afrom%20two%20study%20modules%3A%20Embedded%20Systems%20and%20Development%20of%20Embedded%20Systems%0AUsing%20FPGA.%20The%20resulting%20KG%20structures%20the%20curriculum%20and%20links%20it%20to%20the%0Adomain%20models.%20We%20evaluate%20our%20approach%20through%20qualitative%20expert%20feedback%20and%0Aquantitative%20graph%20quality%20metrics.%20Domain%20experts%20validated%20the%20relevance%20and%0Aaccuracy%20of%20the%20model%2C%20while%20the%20graph%20quality%20metrics%20measured%20the%20structural%0Aproperties%20of%20our%20KG.%20Our%20results%20show%20that%20the%20LLM-assisted%20graph%20completion%0Aapproach%20enhances%20the%20ability%20to%20connect%20related%20courses%20across%20disciplines%20to%0Apersonalize%20the%20learning%20experience.%20Expert%20feedback%20also%20showed%20high%0Aacceptance%20of%20the%20proposed%20collaborative%20approach%20for%20concept%20extraction%20and%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12300v1&entry.124074799=Read"},
{"title": "Investigating Market Strength Prediction with CNNs on Candlestick Chart\n  Images", "author": "Thanh Nam Duong and Trung Kien Hoang and Quoc Khanh Duong and Quoc Dat Dinh and Duc Hoan Le and Huy Tuan Nguyen and Xuan Bach Nguyen and Quy Ban Tran", "abstract": "  This paper investigates predicting market strength solely from candlestick\nchart images to assist investment decisions. The core research problem is\ndeveloping an effective computer vision-based model using raw candlestick\nvisuals without time-series data. We specifically analyze the impact of\nincorporating candlestick patterns that were detected by YOLOv8. The study\nimplements two approaches: pure CNN on chart images and a Decomposer\narchitecture detecting patterns. Experiments utilize diverse financial datasets\nspanning stocks, cryptocurrencies, and forex assets. Key findings demonstrate\ncandlestick patterns do not improve model performance over only image data in\nour research. The significance is illuminating limitations in candlestick image\nsignals. Performance peaked at approximately 0.7 accuracy, below more complex\ntime-series models. Outcomes reveal challenges in distilling sufficient\npredictive power from visual shapes alone, motivating the incorporation of\nother data modalities. This research clarifies how purely image-based models\ncan inform trading while confirming patterns add little value over raw charts.\nOur content is endeavored to be delineated into distinct sections, each\nautonomously furnishing a unique contribution while maintaining cohesive\nlinkage. Note that, the examples discussed herein are not limited to the scope,\napplicability, or knowledge outlined in the paper.\n", "link": "http://arxiv.org/abs/2501.12239v1", "date": "2025-01-21", "relevancy": 1.718, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Market%20Strength%20Prediction%20with%20CNNs%20on%20Candlestick%20Chart%0A%20%20Images&body=Title%3A%20Investigating%20Market%20Strength%20Prediction%20with%20CNNs%20on%20Candlestick%20Chart%0A%20%20Images%0AAuthor%3A%20Thanh%20Nam%20Duong%20and%20Trung%20Kien%20Hoang%20and%20Quoc%20Khanh%20Duong%20and%20Quoc%20Dat%20Dinh%20and%20Duc%20Hoan%20Le%20and%20Huy%20Tuan%20Nguyen%20and%20Xuan%20Bach%20Nguyen%20and%20Quy%20Ban%20Tran%0AAbstract%3A%20%20%20This%20paper%20investigates%20predicting%20market%20strength%20solely%20from%20candlestick%0Achart%20images%20to%20assist%20investment%20decisions.%20The%20core%20research%20problem%20is%0Adeveloping%20an%20effective%20computer%20vision-based%20model%20using%20raw%20candlestick%0Avisuals%20without%20time-series%20data.%20We%20specifically%20analyze%20the%20impact%20of%0Aincorporating%20candlestick%20patterns%20that%20were%20detected%20by%20YOLOv8.%20The%20study%0Aimplements%20two%20approaches%3A%20pure%20CNN%20on%20chart%20images%20and%20a%20Decomposer%0Aarchitecture%20detecting%20patterns.%20Experiments%20utilize%20diverse%20financial%20datasets%0Aspanning%20stocks%2C%20cryptocurrencies%2C%20and%20forex%20assets.%20Key%20findings%20demonstrate%0Acandlestick%20patterns%20do%20not%20improve%20model%20performance%20over%20only%20image%20data%20in%0Aour%20research.%20The%20significance%20is%20illuminating%20limitations%20in%20candlestick%20image%0Asignals.%20Performance%20peaked%20at%20approximately%200.7%20accuracy%2C%20below%20more%20complex%0Atime-series%20models.%20Outcomes%20reveal%20challenges%20in%20distilling%20sufficient%0Apredictive%20power%20from%20visual%20shapes%20alone%2C%20motivating%20the%20incorporation%20of%0Aother%20data%20modalities.%20This%20research%20clarifies%20how%20purely%20image-based%20models%0Acan%20inform%20trading%20while%20confirming%20patterns%20add%20little%20value%20over%20raw%20charts.%0AOur%20content%20is%20endeavored%20to%20be%20delineated%20into%20distinct%20sections%2C%20each%0Aautonomously%20furnishing%20a%20unique%20contribution%20while%20maintaining%20cohesive%0Alinkage.%20Note%20that%2C%20the%20examples%20discussed%20herein%20are%20not%20limited%20to%20the%20scope%2C%0Aapplicability%2C%20or%20knowledge%20outlined%20in%20the%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Market%2520Strength%2520Prediction%2520with%2520CNNs%2520on%2520Candlestick%2520Chart%250A%2520%2520Images%26entry.906535625%3DThanh%2520Nam%2520Duong%2520and%2520Trung%2520Kien%2520Hoang%2520and%2520Quoc%2520Khanh%2520Duong%2520and%2520Quoc%2520Dat%2520Dinh%2520and%2520Duc%2520Hoan%2520Le%2520and%2520Huy%2520Tuan%2520Nguyen%2520and%2520Xuan%2520Bach%2520Nguyen%2520and%2520Quy%2520Ban%2520Tran%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520predicting%2520market%2520strength%2520solely%2520from%2520candlestick%250Achart%2520images%2520to%2520assist%2520investment%2520decisions.%2520The%2520core%2520research%2520problem%2520is%250Adeveloping%2520an%2520effective%2520computer%2520vision-based%2520model%2520using%2520raw%2520candlestick%250Avisuals%2520without%2520time-series%2520data.%2520We%2520specifically%2520analyze%2520the%2520impact%2520of%250Aincorporating%2520candlestick%2520patterns%2520that%2520were%2520detected%2520by%2520YOLOv8.%2520The%2520study%250Aimplements%2520two%2520approaches%253A%2520pure%2520CNN%2520on%2520chart%2520images%2520and%2520a%2520Decomposer%250Aarchitecture%2520detecting%2520patterns.%2520Experiments%2520utilize%2520diverse%2520financial%2520datasets%250Aspanning%2520stocks%252C%2520cryptocurrencies%252C%2520and%2520forex%2520assets.%2520Key%2520findings%2520demonstrate%250Acandlestick%2520patterns%2520do%2520not%2520improve%2520model%2520performance%2520over%2520only%2520image%2520data%2520in%250Aour%2520research.%2520The%2520significance%2520is%2520illuminating%2520limitations%2520in%2520candlestick%2520image%250Asignals.%2520Performance%2520peaked%2520at%2520approximately%25200.7%2520accuracy%252C%2520below%2520more%2520complex%250Atime-series%2520models.%2520Outcomes%2520reveal%2520challenges%2520in%2520distilling%2520sufficient%250Apredictive%2520power%2520from%2520visual%2520shapes%2520alone%252C%2520motivating%2520the%2520incorporation%2520of%250Aother%2520data%2520modalities.%2520This%2520research%2520clarifies%2520how%2520purely%2520image-based%2520models%250Acan%2520inform%2520trading%2520while%2520confirming%2520patterns%2520add%2520little%2520value%2520over%2520raw%2520charts.%250AOur%2520content%2520is%2520endeavored%2520to%2520be%2520delineated%2520into%2520distinct%2520sections%252C%2520each%250Aautonomously%2520furnishing%2520a%2520unique%2520contribution%2520while%2520maintaining%2520cohesive%250Alinkage.%2520Note%2520that%252C%2520the%2520examples%2520discussed%2520herein%2520are%2520not%2520limited%2520to%2520the%2520scope%252C%250Aapplicability%252C%2520or%2520knowledge%2520outlined%2520in%2520the%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Market%20Strength%20Prediction%20with%20CNNs%20on%20Candlestick%20Chart%0A%20%20Images&entry.906535625=Thanh%20Nam%20Duong%20and%20Trung%20Kien%20Hoang%20and%20Quoc%20Khanh%20Duong%20and%20Quoc%20Dat%20Dinh%20and%20Duc%20Hoan%20Le%20and%20Huy%20Tuan%20Nguyen%20and%20Xuan%20Bach%20Nguyen%20and%20Quy%20Ban%20Tran&entry.1292438233=%20%20This%20paper%20investigates%20predicting%20market%20strength%20solely%20from%20candlestick%0Achart%20images%20to%20assist%20investment%20decisions.%20The%20core%20research%20problem%20is%0Adeveloping%20an%20effective%20computer%20vision-based%20model%20using%20raw%20candlestick%0Avisuals%20without%20time-series%20data.%20We%20specifically%20analyze%20the%20impact%20of%0Aincorporating%20candlestick%20patterns%20that%20were%20detected%20by%20YOLOv8.%20The%20study%0Aimplements%20two%20approaches%3A%20pure%20CNN%20on%20chart%20images%20and%20a%20Decomposer%0Aarchitecture%20detecting%20patterns.%20Experiments%20utilize%20diverse%20financial%20datasets%0Aspanning%20stocks%2C%20cryptocurrencies%2C%20and%20forex%20assets.%20Key%20findings%20demonstrate%0Acandlestick%20patterns%20do%20not%20improve%20model%20performance%20over%20only%20image%20data%20in%0Aour%20research.%20The%20significance%20is%20illuminating%20limitations%20in%20candlestick%20image%0Asignals.%20Performance%20peaked%20at%20approximately%200.7%20accuracy%2C%20below%20more%20complex%0Atime-series%20models.%20Outcomes%20reveal%20challenges%20in%20distilling%20sufficient%0Apredictive%20power%20from%20visual%20shapes%20alone%2C%20motivating%20the%20incorporation%20of%0Aother%20data%20modalities.%20This%20research%20clarifies%20how%20purely%20image-based%20models%0Acan%20inform%20trading%20while%20confirming%20patterns%20add%20little%20value%20over%20raw%20charts.%0AOur%20content%20is%20endeavored%20to%20be%20delineated%20into%20distinct%20sections%2C%20each%0Aautonomously%20furnishing%20a%20unique%20contribution%20while%20maintaining%20cohesive%0Alinkage.%20Note%20that%2C%20the%20examples%20discussed%20herein%20are%20not%20limited%20to%20the%20scope%2C%0Aapplicability%2C%20or%20knowledge%20outlined%20in%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12239v1&entry.124074799=Read"},
{"title": "ORCAst: Operational High-Resolution Current Forecasts", "author": "Pierre Garcia and In\u00e8s Larroche and Am\u00e9lie Pesnec and Hannah Bull and Th\u00e9o Archambault and Evangelos Moschos and Alexandre Stegner and Anastase Charantonis and Dominique B\u00e9r\u00e9ziat", "abstract": "  We present ORCAst, a multi-stage, multi-arm network for Operational\nhigh-Resolution Current forecAsts over one week. Producing real-time nowcasts\nand forecasts of ocean surface currents is a challenging problem due to\nindirect or incomplete information from satellite remote sensing data. Entirely\ntrained on real satellite data and in situ measurements from drifters, our\nmodel learns to forecast global ocean surface currents using various sources of\nground truth observations in a multi-stage learning procedure. Our multi-arm\nencoder-decoder model architecture allows us to first predict sea surface\nheight and geostrophic currents from larger quantities of nadir and SWOT\naltimetry data, before learning to predict ocean surface currents from much\nmore sparse in situ measurements from drifters. Training our model on specific\nregions improves performance. Our model achieves stronger nowcast and forecast\nperformance in predicting ocean surface currents than various state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2501.12054v1", "date": "2025-01-21", "relevancy": 1.3443, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.457}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4449}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ORCAst%3A%20Operational%20High-Resolution%20Current%20Forecasts&body=Title%3A%20ORCAst%3A%20Operational%20High-Resolution%20Current%20Forecasts%0AAuthor%3A%20Pierre%20Garcia%20and%20In%C3%A8s%20Larroche%20and%20Am%C3%A9lie%20Pesnec%20and%20Hannah%20Bull%20and%20Th%C3%A9o%20Archambault%20and%20Evangelos%20Moschos%20and%20Alexandre%20Stegner%20and%20Anastase%20Charantonis%20and%20Dominique%20B%C3%A9r%C3%A9ziat%0AAbstract%3A%20%20%20We%20present%20ORCAst%2C%20a%20multi-stage%2C%20multi-arm%20network%20for%20Operational%0Ahigh-Resolution%20Current%20forecAsts%20over%20one%20week.%20Producing%20real-time%20nowcasts%0Aand%20forecasts%20of%20ocean%20surface%20currents%20is%20a%20challenging%20problem%20due%20to%0Aindirect%20or%20incomplete%20information%20from%20satellite%20remote%20sensing%20data.%20Entirely%0Atrained%20on%20real%20satellite%20data%20and%20in%20situ%20measurements%20from%20drifters%2C%20our%0Amodel%20learns%20to%20forecast%20global%20ocean%20surface%20currents%20using%20various%20sources%20of%0Aground%20truth%20observations%20in%20a%20multi-stage%20learning%20procedure.%20Our%20multi-arm%0Aencoder-decoder%20model%20architecture%20allows%20us%20to%20first%20predict%20sea%20surface%0Aheight%20and%20geostrophic%20currents%20from%20larger%20quantities%20of%20nadir%20and%20SWOT%0Aaltimetry%20data%2C%20before%20learning%20to%20predict%20ocean%20surface%20currents%20from%20much%0Amore%20sparse%20in%20situ%20measurements%20from%20drifters.%20Training%20our%20model%20on%20specific%0Aregions%20improves%20performance.%20Our%20model%20achieves%20stronger%20nowcast%20and%20forecast%0Aperformance%20in%20predicting%20ocean%20surface%20currents%20than%20various%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DORCAst%253A%2520Operational%2520High-Resolution%2520Current%2520Forecasts%26entry.906535625%3DPierre%2520Garcia%2520and%2520In%25C3%25A8s%2520Larroche%2520and%2520Am%25C3%25A9lie%2520Pesnec%2520and%2520Hannah%2520Bull%2520and%2520Th%25C3%25A9o%2520Archambault%2520and%2520Evangelos%2520Moschos%2520and%2520Alexandre%2520Stegner%2520and%2520Anastase%2520Charantonis%2520and%2520Dominique%2520B%25C3%25A9r%25C3%25A9ziat%26entry.1292438233%3D%2520%2520We%2520present%2520ORCAst%252C%2520a%2520multi-stage%252C%2520multi-arm%2520network%2520for%2520Operational%250Ahigh-Resolution%2520Current%2520forecAsts%2520over%2520one%2520week.%2520Producing%2520real-time%2520nowcasts%250Aand%2520forecasts%2520of%2520ocean%2520surface%2520currents%2520is%2520a%2520challenging%2520problem%2520due%2520to%250Aindirect%2520or%2520incomplete%2520information%2520from%2520satellite%2520remote%2520sensing%2520data.%2520Entirely%250Atrained%2520on%2520real%2520satellite%2520data%2520and%2520in%2520situ%2520measurements%2520from%2520drifters%252C%2520our%250Amodel%2520learns%2520to%2520forecast%2520global%2520ocean%2520surface%2520currents%2520using%2520various%2520sources%2520of%250Aground%2520truth%2520observations%2520in%2520a%2520multi-stage%2520learning%2520procedure.%2520Our%2520multi-arm%250Aencoder-decoder%2520model%2520architecture%2520allows%2520us%2520to%2520first%2520predict%2520sea%2520surface%250Aheight%2520and%2520geostrophic%2520currents%2520from%2520larger%2520quantities%2520of%2520nadir%2520and%2520SWOT%250Aaltimetry%2520data%252C%2520before%2520learning%2520to%2520predict%2520ocean%2520surface%2520currents%2520from%2520much%250Amore%2520sparse%2520in%2520situ%2520measurements%2520from%2520drifters.%2520Training%2520our%2520model%2520on%2520specific%250Aregions%2520improves%2520performance.%2520Our%2520model%2520achieves%2520stronger%2520nowcast%2520and%2520forecast%250Aperformance%2520in%2520predicting%2520ocean%2520surface%2520currents%2520than%2520various%2520state-of-the-art%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ORCAst%3A%20Operational%20High-Resolution%20Current%20Forecasts&entry.906535625=Pierre%20Garcia%20and%20In%C3%A8s%20Larroche%20and%20Am%C3%A9lie%20Pesnec%20and%20Hannah%20Bull%20and%20Th%C3%A9o%20Archambault%20and%20Evangelos%20Moschos%20and%20Alexandre%20Stegner%20and%20Anastase%20Charantonis%20and%20Dominique%20B%C3%A9r%C3%A9ziat&entry.1292438233=%20%20We%20present%20ORCAst%2C%20a%20multi-stage%2C%20multi-arm%20network%20for%20Operational%0Ahigh-Resolution%20Current%20forecAsts%20over%20one%20week.%20Producing%20real-time%20nowcasts%0Aand%20forecasts%20of%20ocean%20surface%20currents%20is%20a%20challenging%20problem%20due%20to%0Aindirect%20or%20incomplete%20information%20from%20satellite%20remote%20sensing%20data.%20Entirely%0Atrained%20on%20real%20satellite%20data%20and%20in%20situ%20measurements%20from%20drifters%2C%20our%0Amodel%20learns%20to%20forecast%20global%20ocean%20surface%20currents%20using%20various%20sources%20of%0Aground%20truth%20observations%20in%20a%20multi-stage%20learning%20procedure.%20Our%20multi-arm%0Aencoder-decoder%20model%20architecture%20allows%20us%20to%20first%20predict%20sea%20surface%0Aheight%20and%20geostrophic%20currents%20from%20larger%20quantities%20of%20nadir%20and%20SWOT%0Aaltimetry%20data%2C%20before%20learning%20to%20predict%20ocean%20surface%20currents%20from%20much%0Amore%20sparse%20in%20situ%20measurements%20from%20drifters.%20Training%20our%20model%20on%20specific%0Aregions%20improves%20performance.%20Our%20model%20achieves%20stronger%20nowcast%20and%20forecast%0Aperformance%20in%20predicting%20ocean%20surface%20currents%20than%20various%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12054v1&entry.124074799=Read"},
{"title": "Experience-replay Innovative Dynamics", "author": "Tuo Zhang and Leonardo Stella and Julian Barreiro Gomez", "abstract": "  Despite its groundbreaking success, multi-agent reinforcement learning (MARL)\nstill suffers from instability and nonstationarity. Replicator dynamics, the\nmost well-known model from evolutionary game theory (EGT), provide a\ntheoretical framework for the convergence of the trajectories to Nash\nequilibria and, as a result, have been used to ensure formal guarantees for\nMARL algorithms in stable game settings. However, they exhibit the opposite\nbehavior in other settings, which poses the problem of finding alternatives to\nensure convergence. In contrast, innovative dynamics, such as the Brown-von\nNeumann-Nash (BNN) or Smith, result in periodic trajectories with the potential\nto approximate Nash equilibria. Yet, no MARL algorithms based on these dynamics\nhave been proposed. In response to this challenge, we develop a novel\nexperience replay-based MARL algorithm that incorporates revision protocols as\ntunable hyperparameters. We demonstrate, by appropriately adjusting the\nrevision protocols, that the behavior of our algorithm mirrors the trajectories\nresulting from these dynamics. Importantly, our contribution provides a\nframework capable of extending the theoretical guarantees of MARL algorithms\nbeyond replicator dynamics. Finally, we corroborate our theoretical findings\nwith empirical results.\n", "link": "http://arxiv.org/abs/2501.12199v1", "date": "2025-01-21", "relevancy": 1.4969, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5195}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4941}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Experience-replay%20Innovative%20Dynamics&body=Title%3A%20Experience-replay%20Innovative%20Dynamics%0AAuthor%3A%20Tuo%20Zhang%20and%20Leonardo%20Stella%20and%20Julian%20Barreiro%20Gomez%0AAbstract%3A%20%20%20Despite%20its%20groundbreaking%20success%2C%20multi-agent%20reinforcement%20learning%20%28MARL%29%0Astill%20suffers%20from%20instability%20and%20nonstationarity.%20Replicator%20dynamics%2C%20the%0Amost%20well-known%20model%20from%20evolutionary%20game%20theory%20%28EGT%29%2C%20provide%20a%0Atheoretical%20framework%20for%20the%20convergence%20of%20the%20trajectories%20to%20Nash%0Aequilibria%20and%2C%20as%20a%20result%2C%20have%20been%20used%20to%20ensure%20formal%20guarantees%20for%0AMARL%20algorithms%20in%20stable%20game%20settings.%20However%2C%20they%20exhibit%20the%20opposite%0Abehavior%20in%20other%20settings%2C%20which%20poses%20the%20problem%20of%20finding%20alternatives%20to%0Aensure%20convergence.%20In%20contrast%2C%20innovative%20dynamics%2C%20such%20as%20the%20Brown-von%0ANeumann-Nash%20%28BNN%29%20or%20Smith%2C%20result%20in%20periodic%20trajectories%20with%20the%20potential%0Ato%20approximate%20Nash%20equilibria.%20Yet%2C%20no%20MARL%20algorithms%20based%20on%20these%20dynamics%0Ahave%20been%20proposed.%20In%20response%20to%20this%20challenge%2C%20we%20develop%20a%20novel%0Aexperience%20replay-based%20MARL%20algorithm%20that%20incorporates%20revision%20protocols%20as%0Atunable%20hyperparameters.%20We%20demonstrate%2C%20by%20appropriately%20adjusting%20the%0Arevision%20protocols%2C%20that%20the%20behavior%20of%20our%20algorithm%20mirrors%20the%20trajectories%0Aresulting%20from%20these%20dynamics.%20Importantly%2C%20our%20contribution%20provides%20a%0Aframework%20capable%20of%20extending%20the%20theoretical%20guarantees%20of%20MARL%20algorithms%0Abeyond%20replicator%20dynamics.%20Finally%2C%20we%20corroborate%20our%20theoretical%20findings%0Awith%20empirical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExperience-replay%2520Innovative%2520Dynamics%26entry.906535625%3DTuo%2520Zhang%2520and%2520Leonardo%2520Stella%2520and%2520Julian%2520Barreiro%2520Gomez%26entry.1292438233%3D%2520%2520Despite%2520its%2520groundbreaking%2520success%252C%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%250Astill%2520suffers%2520from%2520instability%2520and%2520nonstationarity.%2520Replicator%2520dynamics%252C%2520the%250Amost%2520well-known%2520model%2520from%2520evolutionary%2520game%2520theory%2520%2528EGT%2529%252C%2520provide%2520a%250Atheoretical%2520framework%2520for%2520the%2520convergence%2520of%2520the%2520trajectories%2520to%2520Nash%250Aequilibria%2520and%252C%2520as%2520a%2520result%252C%2520have%2520been%2520used%2520to%2520ensure%2520formal%2520guarantees%2520for%250AMARL%2520algorithms%2520in%2520stable%2520game%2520settings.%2520However%252C%2520they%2520exhibit%2520the%2520opposite%250Abehavior%2520in%2520other%2520settings%252C%2520which%2520poses%2520the%2520problem%2520of%2520finding%2520alternatives%2520to%250Aensure%2520convergence.%2520In%2520contrast%252C%2520innovative%2520dynamics%252C%2520such%2520as%2520the%2520Brown-von%250ANeumann-Nash%2520%2528BNN%2529%2520or%2520Smith%252C%2520result%2520in%2520periodic%2520trajectories%2520with%2520the%2520potential%250Ato%2520approximate%2520Nash%2520equilibria.%2520Yet%252C%2520no%2520MARL%2520algorithms%2520based%2520on%2520these%2520dynamics%250Ahave%2520been%2520proposed.%2520In%2520response%2520to%2520this%2520challenge%252C%2520we%2520develop%2520a%2520novel%250Aexperience%2520replay-based%2520MARL%2520algorithm%2520that%2520incorporates%2520revision%2520protocols%2520as%250Atunable%2520hyperparameters.%2520We%2520demonstrate%252C%2520by%2520appropriately%2520adjusting%2520the%250Arevision%2520protocols%252C%2520that%2520the%2520behavior%2520of%2520our%2520algorithm%2520mirrors%2520the%2520trajectories%250Aresulting%2520from%2520these%2520dynamics.%2520Importantly%252C%2520our%2520contribution%2520provides%2520a%250Aframework%2520capable%2520of%2520extending%2520the%2520theoretical%2520guarantees%2520of%2520MARL%2520algorithms%250Abeyond%2520replicator%2520dynamics.%2520Finally%252C%2520we%2520corroborate%2520our%2520theoretical%2520findings%250Awith%2520empirical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Experience-replay%20Innovative%20Dynamics&entry.906535625=Tuo%20Zhang%20and%20Leonardo%20Stella%20and%20Julian%20Barreiro%20Gomez&entry.1292438233=%20%20Despite%20its%20groundbreaking%20success%2C%20multi-agent%20reinforcement%20learning%20%28MARL%29%0Astill%20suffers%20from%20instability%20and%20nonstationarity.%20Replicator%20dynamics%2C%20the%0Amost%20well-known%20model%20from%20evolutionary%20game%20theory%20%28EGT%29%2C%20provide%20a%0Atheoretical%20framework%20for%20the%20convergence%20of%20the%20trajectories%20to%20Nash%0Aequilibria%20and%2C%20as%20a%20result%2C%20have%20been%20used%20to%20ensure%20formal%20guarantees%20for%0AMARL%20algorithms%20in%20stable%20game%20settings.%20However%2C%20they%20exhibit%20the%20opposite%0Abehavior%20in%20other%20settings%2C%20which%20poses%20the%20problem%20of%20finding%20alternatives%20to%0Aensure%20convergence.%20In%20contrast%2C%20innovative%20dynamics%2C%20such%20as%20the%20Brown-von%0ANeumann-Nash%20%28BNN%29%20or%20Smith%2C%20result%20in%20periodic%20trajectories%20with%20the%20potential%0Ato%20approximate%20Nash%20equilibria.%20Yet%2C%20no%20MARL%20algorithms%20based%20on%20these%20dynamics%0Ahave%20been%20proposed.%20In%20response%20to%20this%20challenge%2C%20we%20develop%20a%20novel%0Aexperience%20replay-based%20MARL%20algorithm%20that%20incorporates%20revision%20protocols%20as%0Atunable%20hyperparameters.%20We%20demonstrate%2C%20by%20appropriately%20adjusting%20the%0Arevision%20protocols%2C%20that%20the%20behavior%20of%20our%20algorithm%20mirrors%20the%20trajectories%0Aresulting%20from%20these%20dynamics.%20Importantly%2C%20our%20contribution%20provides%20a%0Aframework%20capable%20of%20extending%20the%20theoretical%20guarantees%20of%20MARL%20algorithms%0Abeyond%20replicator%20dynamics.%20Finally%2C%20we%20corroborate%20our%20theoretical%20findings%0Awith%20empirical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12199v1&entry.124074799=Read"},
{"title": "FedCLEAN: byzantine defense by CLustering Errors of Activation maps in\n  Non-IID federated learning environments", "author": "Mehdi Ben Ghali and Reda Bellafqira and Gouenou Coatrieux", "abstract": "  Federated Learning (FL) enables clients to collaboratively train a global\nmodel using their local datasets while reinforcing data privacy. However, FL is\nsusceptible to poisoning attacks. Existing defense mechanisms assume that\nclients' data are independent and identically distributed (IID), making them\nineffective in real-world applications where data are non-IID. This paper\npresents FedCLEAN, the first defense capable of filtering attackers' model\nupdates in a non-IID FL environment. The originality of FedCLEAN is twofold.\nFirst, it relies on a client confidence score derived from the reconstruction\nerrors of each client's model activation maps for a given trigger set, with\nreconstruction errors obtained by means of a Conditional Variational\nAutoencoder trained according to a novel server-side strategy. Second, we\npropose an ad-hoc trust propagation algorithm based on client scores, which\nallows building a cluster of benign clients while flagging potential attackers.\nExperimental results on the datasets MNIST and FashionMNIST demonstrate the\nrobustness of FedCLEAN against Byzantine attackers in non-IID scenarios and a\nclose-to-zero benign client misclassification rate, even in the absence of an\nattack.\n", "link": "http://arxiv.org/abs/2501.12123v1", "date": "2025-01-21", "relevancy": 1.934, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4878}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4848}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedCLEAN%3A%20byzantine%20defense%20by%20CLustering%20Errors%20of%20Activation%20maps%20in%0A%20%20Non-IID%20federated%20learning%20environments&body=Title%3A%20FedCLEAN%3A%20byzantine%20defense%20by%20CLustering%20Errors%20of%20Activation%20maps%20in%0A%20%20Non-IID%20federated%20learning%20environments%0AAuthor%3A%20Mehdi%20Ben%20Ghali%20and%20Reda%20Bellafqira%20and%20Gouenou%20Coatrieux%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20clients%20to%20collaboratively%20train%20a%20global%0Amodel%20using%20their%20local%20datasets%20while%20reinforcing%20data%20privacy.%20However%2C%20FL%20is%0Asusceptible%20to%20poisoning%20attacks.%20Existing%20defense%20mechanisms%20assume%20that%0Aclients%27%20data%20are%20independent%20and%20identically%20distributed%20%28IID%29%2C%20making%20them%0Aineffective%20in%20real-world%20applications%20where%20data%20are%20non-IID.%20This%20paper%0Apresents%20FedCLEAN%2C%20the%20first%20defense%20capable%20of%20filtering%20attackers%27%20model%0Aupdates%20in%20a%20non-IID%20FL%20environment.%20The%20originality%20of%20FedCLEAN%20is%20twofold.%0AFirst%2C%20it%20relies%20on%20a%20client%20confidence%20score%20derived%20from%20the%20reconstruction%0Aerrors%20of%20each%20client%27s%20model%20activation%20maps%20for%20a%20given%20trigger%20set%2C%20with%0Areconstruction%20errors%20obtained%20by%20means%20of%20a%20Conditional%20Variational%0AAutoencoder%20trained%20according%20to%20a%20novel%20server-side%20strategy.%20Second%2C%20we%0Apropose%20an%20ad-hoc%20trust%20propagation%20algorithm%20based%20on%20client%20scores%2C%20which%0Aallows%20building%20a%20cluster%20of%20benign%20clients%20while%20flagging%20potential%20attackers.%0AExperimental%20results%20on%20the%20datasets%20MNIST%20and%20FashionMNIST%20demonstrate%20the%0Arobustness%20of%20FedCLEAN%20against%20Byzantine%20attackers%20in%20non-IID%20scenarios%20and%20a%0Aclose-to-zero%20benign%20client%20misclassification%20rate%2C%20even%20in%20the%20absence%20of%20an%0Aattack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedCLEAN%253A%2520byzantine%2520defense%2520by%2520CLustering%2520Errors%2520of%2520Activation%2520maps%2520in%250A%2520%2520Non-IID%2520federated%2520learning%2520environments%26entry.906535625%3DMehdi%2520Ben%2520Ghali%2520and%2520Reda%2520Bellafqira%2520and%2520Gouenou%2520Coatrieux%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520clients%2520to%2520collaboratively%2520train%2520a%2520global%250Amodel%2520using%2520their%2520local%2520datasets%2520while%2520reinforcing%2520data%2520privacy.%2520However%252C%2520FL%2520is%250Asusceptible%2520to%2520poisoning%2520attacks.%2520Existing%2520defense%2520mechanisms%2520assume%2520that%250Aclients%2527%2520data%2520are%2520independent%2520and%2520identically%2520distributed%2520%2528IID%2529%252C%2520making%2520them%250Aineffective%2520in%2520real-world%2520applications%2520where%2520data%2520are%2520non-IID.%2520This%2520paper%250Apresents%2520FedCLEAN%252C%2520the%2520first%2520defense%2520capable%2520of%2520filtering%2520attackers%2527%2520model%250Aupdates%2520in%2520a%2520non-IID%2520FL%2520environment.%2520The%2520originality%2520of%2520FedCLEAN%2520is%2520twofold.%250AFirst%252C%2520it%2520relies%2520on%2520a%2520client%2520confidence%2520score%2520derived%2520from%2520the%2520reconstruction%250Aerrors%2520of%2520each%2520client%2527s%2520model%2520activation%2520maps%2520for%2520a%2520given%2520trigger%2520set%252C%2520with%250Areconstruction%2520errors%2520obtained%2520by%2520means%2520of%2520a%2520Conditional%2520Variational%250AAutoencoder%2520trained%2520according%2520to%2520a%2520novel%2520server-side%2520strategy.%2520Second%252C%2520we%250Apropose%2520an%2520ad-hoc%2520trust%2520propagation%2520algorithm%2520based%2520on%2520client%2520scores%252C%2520which%250Aallows%2520building%2520a%2520cluster%2520of%2520benign%2520clients%2520while%2520flagging%2520potential%2520attackers.%250AExperimental%2520results%2520on%2520the%2520datasets%2520MNIST%2520and%2520FashionMNIST%2520demonstrate%2520the%250Arobustness%2520of%2520FedCLEAN%2520against%2520Byzantine%2520attackers%2520in%2520non-IID%2520scenarios%2520and%2520a%250Aclose-to-zero%2520benign%2520client%2520misclassification%2520rate%252C%2520even%2520in%2520the%2520absence%2520of%2520an%250Aattack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedCLEAN%3A%20byzantine%20defense%20by%20CLustering%20Errors%20of%20Activation%20maps%20in%0A%20%20Non-IID%20federated%20learning%20environments&entry.906535625=Mehdi%20Ben%20Ghali%20and%20Reda%20Bellafqira%20and%20Gouenou%20Coatrieux&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20clients%20to%20collaboratively%20train%20a%20global%0Amodel%20using%20their%20local%20datasets%20while%20reinforcing%20data%20privacy.%20However%2C%20FL%20is%0Asusceptible%20to%20poisoning%20attacks.%20Existing%20defense%20mechanisms%20assume%20that%0Aclients%27%20data%20are%20independent%20and%20identically%20distributed%20%28IID%29%2C%20making%20them%0Aineffective%20in%20real-world%20applications%20where%20data%20are%20non-IID.%20This%20paper%0Apresents%20FedCLEAN%2C%20the%20first%20defense%20capable%20of%20filtering%20attackers%27%20model%0Aupdates%20in%20a%20non-IID%20FL%20environment.%20The%20originality%20of%20FedCLEAN%20is%20twofold.%0AFirst%2C%20it%20relies%20on%20a%20client%20confidence%20score%20derived%20from%20the%20reconstruction%0Aerrors%20of%20each%20client%27s%20model%20activation%20maps%20for%20a%20given%20trigger%20set%2C%20with%0Areconstruction%20errors%20obtained%20by%20means%20of%20a%20Conditional%20Variational%0AAutoencoder%20trained%20according%20to%20a%20novel%20server-side%20strategy.%20Second%2C%20we%0Apropose%20an%20ad-hoc%20trust%20propagation%20algorithm%20based%20on%20client%20scores%2C%20which%0Aallows%20building%20a%20cluster%20of%20benign%20clients%20while%20flagging%20potential%20attackers.%0AExperimental%20results%20on%20the%20datasets%20MNIST%20and%20FashionMNIST%20demonstrate%20the%0Arobustness%20of%20FedCLEAN%20against%20Byzantine%20attackers%20in%20non-IID%20scenarios%20and%20a%0Aclose-to-zero%20benign%20client%20misclassification%20rate%2C%20even%20in%20the%20absence%20of%20an%0Aattack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12123v1&entry.124074799=Read"},
{"title": "A recent evaluation on the performance of LLMs on radiation oncology\n  physics using questions of randomly shuffled options", "author": "Peilong Wang and Jason Holmes and Zhengliang Liu and Dequan Chen and Tianming Liu and Jiajian Shen and Wei Liu", "abstract": "  Purpose: We present an updated study evaluating the performance of large\nlanguage models (LLMs) in answering radiation oncology physics questions,\nfocusing on the recently released models.\n  Methods: A set of 100 multiple-choice radiation oncology physics questions,\npreviously created by a well-experienced physicist, was used for this study.\nThe answer options of the questions were randomly shuffled to create \"new\" exam\nsets. Five LLMs -- OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,\nand Claude 3.5 Sonnet -- with the versions released before September 30, 2024,\nwere queried using these new exam sets. To evaluate their deductive reasoning\nability, the correct answer options in the questions were replaced with \"None\nof the above.\" Then, the explain-first and step-by-step instruction prompts\nwere used to test if this strategy improved their reasoning ability. The\nperformance of the LLMs was compared with the answers from medical physicists.\n  Results: All models demonstrated expert-level performance on these questions,\nwith o1-preview even surpassing medical physicists with a majority vote. When\nreplacing the correct answer options with 'None of the above', all models\nexhibited a considerable decline in performance, suggesting room for\nimprovement. The explain-first and step-by-step instruction prompts helped\nenhance the reasoning ability of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and\nClaude 3.5 Sonnet models.\n  Conclusion: These recently released LLMs demonstrated expert-level\nperformance in answering radiation oncology physics questions, exhibiting great\npotential to assist in radiation oncology physics education and training.\n", "link": "http://arxiv.org/abs/2412.10622v3", "date": "2025-01-21", "relevancy": 1.9615, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4972}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20recent%20evaluation%20on%20the%20performance%20of%20LLMs%20on%20radiation%20oncology%0A%20%20physics%20using%20questions%20of%20randomly%20shuffled%20options&body=Title%3A%20A%20recent%20evaluation%20on%20the%20performance%20of%20LLMs%20on%20radiation%20oncology%0A%20%20physics%20using%20questions%20of%20randomly%20shuffled%20options%0AAuthor%3A%20Peilong%20Wang%20and%20Jason%20Holmes%20and%20Zhengliang%20Liu%20and%20Dequan%20Chen%20and%20Tianming%20Liu%20and%20Jiajian%20Shen%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Purpose%3A%20We%20present%20an%20updated%20study%20evaluating%20the%20performance%20of%20large%0Alanguage%20models%20%28LLMs%29%20in%20answering%20radiation%20oncology%20physics%20questions%2C%0Afocusing%20on%20the%20recently%20released%20models.%0A%20%20Methods%3A%20A%20set%20of%20100%20multiple-choice%20radiation%20oncology%20physics%20questions%2C%0Apreviously%20created%20by%20a%20well-experienced%20physicist%2C%20was%20used%20for%20this%20study.%0AThe%20answer%20options%20of%20the%20questions%20were%20randomly%20shuffled%20to%20create%20%22new%22%20exam%0Asets.%20Five%20LLMs%20--%20OpenAI%20o1-preview%2C%20GPT-4o%2C%20LLaMA%203.1%20%28405B%29%2C%20Gemini%201.5%20Pro%2C%0Aand%20Claude%203.5%20Sonnet%20--%20with%20the%20versions%20released%20before%20September%2030%2C%202024%2C%0Awere%20queried%20using%20these%20new%20exam%20sets.%20To%20evaluate%20their%20deductive%20reasoning%0Aability%2C%20the%20correct%20answer%20options%20in%20the%20questions%20were%20replaced%20with%20%22None%0Aof%20the%20above.%22%20Then%2C%20the%20explain-first%20and%20step-by-step%20instruction%20prompts%0Awere%20used%20to%20test%20if%20this%20strategy%20improved%20their%20reasoning%20ability.%20The%0Aperformance%20of%20the%20LLMs%20was%20compared%20with%20the%20answers%20from%20medical%20physicists.%0A%20%20Results%3A%20All%20models%20demonstrated%20expert-level%20performance%20on%20these%20questions%2C%0Awith%20o1-preview%20even%20surpassing%20medical%20physicists%20with%20a%20majority%20vote.%20When%0Areplacing%20the%20correct%20answer%20options%20with%20%27None%20of%20the%20above%27%2C%20all%20models%0Aexhibited%20a%20considerable%20decline%20in%20performance%2C%20suggesting%20room%20for%0Aimprovement.%20The%20explain-first%20and%20step-by-step%20instruction%20prompts%20helped%0Aenhance%20the%20reasoning%20ability%20of%20the%20LLaMA%203.1%20%28405B%29%2C%20Gemini%201.5%20Pro%2C%20and%0AClaude%203.5%20Sonnet%20models.%0A%20%20Conclusion%3A%20These%20recently%20released%20LLMs%20demonstrated%20expert-level%0Aperformance%20in%20answering%20radiation%20oncology%20physics%20questions%2C%20exhibiting%20great%0Apotential%20to%20assist%20in%20radiation%20oncology%20physics%20education%20and%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10622v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520recent%2520evaluation%2520on%2520the%2520performance%2520of%2520LLMs%2520on%2520radiation%2520oncology%250A%2520%2520physics%2520using%2520questions%2520of%2520randomly%2520shuffled%2520options%26entry.906535625%3DPeilong%2520Wang%2520and%2520Jason%2520Holmes%2520and%2520Zhengliang%2520Liu%2520and%2520Dequan%2520Chen%2520and%2520Tianming%2520Liu%2520and%2520Jiajian%2520Shen%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520Purpose%253A%2520We%2520present%2520an%2520updated%2520study%2520evaluating%2520the%2520performance%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520in%2520answering%2520radiation%2520oncology%2520physics%2520questions%252C%250Afocusing%2520on%2520the%2520recently%2520released%2520models.%250A%2520%2520Methods%253A%2520A%2520set%2520of%2520100%2520multiple-choice%2520radiation%2520oncology%2520physics%2520questions%252C%250Apreviously%2520created%2520by%2520a%2520well-experienced%2520physicist%252C%2520was%2520used%2520for%2520this%2520study.%250AThe%2520answer%2520options%2520of%2520the%2520questions%2520were%2520randomly%2520shuffled%2520to%2520create%2520%2522new%2522%2520exam%250Asets.%2520Five%2520LLMs%2520--%2520OpenAI%2520o1-preview%252C%2520GPT-4o%252C%2520LLaMA%25203.1%2520%2528405B%2529%252C%2520Gemini%25201.5%2520Pro%252C%250Aand%2520Claude%25203.5%2520Sonnet%2520--%2520with%2520the%2520versions%2520released%2520before%2520September%252030%252C%25202024%252C%250Awere%2520queried%2520using%2520these%2520new%2520exam%2520sets.%2520To%2520evaluate%2520their%2520deductive%2520reasoning%250Aability%252C%2520the%2520correct%2520answer%2520options%2520in%2520the%2520questions%2520were%2520replaced%2520with%2520%2522None%250Aof%2520the%2520above.%2522%2520Then%252C%2520the%2520explain-first%2520and%2520step-by-step%2520instruction%2520prompts%250Awere%2520used%2520to%2520test%2520if%2520this%2520strategy%2520improved%2520their%2520reasoning%2520ability.%2520The%250Aperformance%2520of%2520the%2520LLMs%2520was%2520compared%2520with%2520the%2520answers%2520from%2520medical%2520physicists.%250A%2520%2520Results%253A%2520All%2520models%2520demonstrated%2520expert-level%2520performance%2520on%2520these%2520questions%252C%250Awith%2520o1-preview%2520even%2520surpassing%2520medical%2520physicists%2520with%2520a%2520majority%2520vote.%2520When%250Areplacing%2520the%2520correct%2520answer%2520options%2520with%2520%2527None%2520of%2520the%2520above%2527%252C%2520all%2520models%250Aexhibited%2520a%2520considerable%2520decline%2520in%2520performance%252C%2520suggesting%2520room%2520for%250Aimprovement.%2520The%2520explain-first%2520and%2520step-by-step%2520instruction%2520prompts%2520helped%250Aenhance%2520the%2520reasoning%2520ability%2520of%2520the%2520LLaMA%25203.1%2520%2528405B%2529%252C%2520Gemini%25201.5%2520Pro%252C%2520and%250AClaude%25203.5%2520Sonnet%2520models.%250A%2520%2520Conclusion%253A%2520These%2520recently%2520released%2520LLMs%2520demonstrated%2520expert-level%250Aperformance%2520in%2520answering%2520radiation%2520oncology%2520physics%2520questions%252C%2520exhibiting%2520great%250Apotential%2520to%2520assist%2520in%2520radiation%2520oncology%2520physics%2520education%2520and%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10622v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20recent%20evaluation%20on%20the%20performance%20of%20LLMs%20on%20radiation%20oncology%0A%20%20physics%20using%20questions%20of%20randomly%20shuffled%20options&entry.906535625=Peilong%20Wang%20and%20Jason%20Holmes%20and%20Zhengliang%20Liu%20and%20Dequan%20Chen%20and%20Tianming%20Liu%20and%20Jiajian%20Shen%20and%20Wei%20Liu&entry.1292438233=%20%20Purpose%3A%20We%20present%20an%20updated%20study%20evaluating%20the%20performance%20of%20large%0Alanguage%20models%20%28LLMs%29%20in%20answering%20radiation%20oncology%20physics%20questions%2C%0Afocusing%20on%20the%20recently%20released%20models.%0A%20%20Methods%3A%20A%20set%20of%20100%20multiple-choice%20radiation%20oncology%20physics%20questions%2C%0Apreviously%20created%20by%20a%20well-experienced%20physicist%2C%20was%20used%20for%20this%20study.%0AThe%20answer%20options%20of%20the%20questions%20were%20randomly%20shuffled%20to%20create%20%22new%22%20exam%0Asets.%20Five%20LLMs%20--%20OpenAI%20o1-preview%2C%20GPT-4o%2C%20LLaMA%203.1%20%28405B%29%2C%20Gemini%201.5%20Pro%2C%0Aand%20Claude%203.5%20Sonnet%20--%20with%20the%20versions%20released%20before%20September%2030%2C%202024%2C%0Awere%20queried%20using%20these%20new%20exam%20sets.%20To%20evaluate%20their%20deductive%20reasoning%0Aability%2C%20the%20correct%20answer%20options%20in%20the%20questions%20were%20replaced%20with%20%22None%0Aof%20the%20above.%22%20Then%2C%20the%20explain-first%20and%20step-by-step%20instruction%20prompts%0Awere%20used%20to%20test%20if%20this%20strategy%20improved%20their%20reasoning%20ability.%20The%0Aperformance%20of%20the%20LLMs%20was%20compared%20with%20the%20answers%20from%20medical%20physicists.%0A%20%20Results%3A%20All%20models%20demonstrated%20expert-level%20performance%20on%20these%20questions%2C%0Awith%20o1-preview%20even%20surpassing%20medical%20physicists%20with%20a%20majority%20vote.%20When%0Areplacing%20the%20correct%20answer%20options%20with%20%27None%20of%20the%20above%27%2C%20all%20models%0Aexhibited%20a%20considerable%20decline%20in%20performance%2C%20suggesting%20room%20for%0Aimprovement.%20The%20explain-first%20and%20step-by-step%20instruction%20prompts%20helped%0Aenhance%20the%20reasoning%20ability%20of%20the%20LLaMA%203.1%20%28405B%29%2C%20Gemini%201.5%20Pro%2C%20and%0AClaude%203.5%20Sonnet%20models.%0A%20%20Conclusion%3A%20These%20recently%20released%20LLMs%20demonstrated%20expert-level%0Aperformance%20in%20answering%20radiation%20oncology%20physics%20questions%2C%20exhibiting%20great%0Apotential%20to%20assist%20in%20radiation%20oncology%20physics%20education%20and%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10622v3&entry.124074799=Read"},
{"title": "ENTIRE: Learning-based Volume Rendering Time Prediction", "author": "Zikai Yin and Hamid Gadirov and Jiri Kosinka and Steffen Frey", "abstract": "  We present ENTIRE, a novel approach for volume rendering time prediction.\nTime-dependent volume data from simulations or experiments typically comprise\ncomplex deforming structures across hundreds or thousands of time steps, which\nin addition to the camera configuration has a significant impact on rendering\nperformance. We first extract a feature vector from a volume that captures its\nstructure that is relevant for rendering time performance. Then we combine this\nfeature vector with further relevant parameters (e.g. camera setup), and with\nthis perform the final prediction. Our experiments conducted on various\ndatasets demonstrate that our model is capable of efficiently achieving high\nprediction accuracy with fast response rates. We showcase ENTIRE's capability\nof enabling dynamic parameter adaptation for stable frame rates and load\nbalancing in two case studies.\n", "link": "http://arxiv.org/abs/2501.12119v1", "date": "2025-01-21", "relevancy": 1.5622, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5231}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5181}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENTIRE%3A%20Learning-based%20Volume%20Rendering%20Time%20Prediction&body=Title%3A%20ENTIRE%3A%20Learning-based%20Volume%20Rendering%20Time%20Prediction%0AAuthor%3A%20Zikai%20Yin%20and%20Hamid%20Gadirov%20and%20Jiri%20Kosinka%20and%20Steffen%20Frey%0AAbstract%3A%20%20%20We%20present%20ENTIRE%2C%20a%20novel%20approach%20for%20volume%20rendering%20time%20prediction.%0ATime-dependent%20volume%20data%20from%20simulations%20or%20experiments%20typically%20comprise%0Acomplex%20deforming%20structures%20across%20hundreds%20or%20thousands%20of%20time%20steps%2C%20which%0Ain%20addition%20to%20the%20camera%20configuration%20has%20a%20significant%20impact%20on%20rendering%0Aperformance.%20We%20first%20extract%20a%20feature%20vector%20from%20a%20volume%20that%20captures%20its%0Astructure%20that%20is%20relevant%20for%20rendering%20time%20performance.%20Then%20we%20combine%20this%0Afeature%20vector%20with%20further%20relevant%20parameters%20%28e.g.%20camera%20setup%29%2C%20and%20with%0Athis%20perform%20the%20final%20prediction.%20Our%20experiments%20conducted%20on%20various%0Adatasets%20demonstrate%20that%20our%20model%20is%20capable%20of%20efficiently%20achieving%20high%0Aprediction%20accuracy%20with%20fast%20response%20rates.%20We%20showcase%20ENTIRE%27s%20capability%0Aof%20enabling%20dynamic%20parameter%20adaptation%20for%20stable%20frame%20rates%20and%20load%0Abalancing%20in%20two%20case%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENTIRE%253A%2520Learning-based%2520Volume%2520Rendering%2520Time%2520Prediction%26entry.906535625%3DZikai%2520Yin%2520and%2520Hamid%2520Gadirov%2520and%2520Jiri%2520Kosinka%2520and%2520Steffen%2520Frey%26entry.1292438233%3D%2520%2520We%2520present%2520ENTIRE%252C%2520a%2520novel%2520approach%2520for%2520volume%2520rendering%2520time%2520prediction.%250ATime-dependent%2520volume%2520data%2520from%2520simulations%2520or%2520experiments%2520typically%2520comprise%250Acomplex%2520deforming%2520structures%2520across%2520hundreds%2520or%2520thousands%2520of%2520time%2520steps%252C%2520which%250Ain%2520addition%2520to%2520the%2520camera%2520configuration%2520has%2520a%2520significant%2520impact%2520on%2520rendering%250Aperformance.%2520We%2520first%2520extract%2520a%2520feature%2520vector%2520from%2520a%2520volume%2520that%2520captures%2520its%250Astructure%2520that%2520is%2520relevant%2520for%2520rendering%2520time%2520performance.%2520Then%2520we%2520combine%2520this%250Afeature%2520vector%2520with%2520further%2520relevant%2520parameters%2520%2528e.g.%2520camera%2520setup%2529%252C%2520and%2520with%250Athis%2520perform%2520the%2520final%2520prediction.%2520Our%2520experiments%2520conducted%2520on%2520various%250Adatasets%2520demonstrate%2520that%2520our%2520model%2520is%2520capable%2520of%2520efficiently%2520achieving%2520high%250Aprediction%2520accuracy%2520with%2520fast%2520response%2520rates.%2520We%2520showcase%2520ENTIRE%2527s%2520capability%250Aof%2520enabling%2520dynamic%2520parameter%2520adaptation%2520for%2520stable%2520frame%2520rates%2520and%2520load%250Abalancing%2520in%2520two%2520case%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENTIRE%3A%20Learning-based%20Volume%20Rendering%20Time%20Prediction&entry.906535625=Zikai%20Yin%20and%20Hamid%20Gadirov%20and%20Jiri%20Kosinka%20and%20Steffen%20Frey&entry.1292438233=%20%20We%20present%20ENTIRE%2C%20a%20novel%20approach%20for%20volume%20rendering%20time%20prediction.%0ATime-dependent%20volume%20data%20from%20simulations%20or%20experiments%20typically%20comprise%0Acomplex%20deforming%20structures%20across%20hundreds%20or%20thousands%20of%20time%20steps%2C%20which%0Ain%20addition%20to%20the%20camera%20configuration%20has%20a%20significant%20impact%20on%20rendering%0Aperformance.%20We%20first%20extract%20a%20feature%20vector%20from%20a%20volume%20that%20captures%20its%0Astructure%20that%20is%20relevant%20for%20rendering%20time%20performance.%20Then%20we%20combine%20this%0Afeature%20vector%20with%20further%20relevant%20parameters%20%28e.g.%20camera%20setup%29%2C%20and%20with%0Athis%20perform%20the%20final%20prediction.%20Our%20experiments%20conducted%20on%20various%0Adatasets%20demonstrate%20that%20our%20model%20is%20capable%20of%20efficiently%20achieving%20high%0Aprediction%20accuracy%20with%20fast%20response%20rates.%20We%20showcase%20ENTIRE%27s%20capability%0Aof%20enabling%20dynamic%20parameter%20adaptation%20for%20stable%20frame%20rates%20and%20load%0Abalancing%20in%20two%20case%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12119v1&entry.124074799=Read"},
{"title": "FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with\n  XLM-Roberta Sentence Embeddings and Deep Neural Regression", "author": "Phuoc Duong Huy Chu", "abstract": "  This paper presents results of our system for CoMeDi Shared Task, focusing on\nSubtask 2: Disagreement Ranking. Our system leverages sentence embeddings\ngenerated by the paraphrase-xlm-r-multilingual-v1 model, combined with a deep\nneural regression model incorporating batch normalization and dropout for\nimproved generalization. By predicting the mean of pairwise judgment\ndifferences between annotators, our method explicitly targets disagreement\nranking, diverging from traditional \"gold label\" aggregation approaches. We\noptimized our system with a customized architecture and training procedure,\nachieving competitive performance in Spearman correlation against mean\ndisagreement labels. Our results highlight the importance of robust embeddings,\neffective model architecture, and careful handling of judgment differences for\nranking disagreement in multilingual contexts. These findings provide insights\ninto the use of contextualized representations for ordinal judgment tasks and\nopen avenues for further refinement of disagreement prediction models.\n", "link": "http://arxiv.org/abs/2501.12336v1", "date": "2025-01-21", "relevancy": 1.9297, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5183}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FuocChuVIP123%20at%20CoMeDi%20Shared%20Task%3A%20Disagreement%20Ranking%20with%0A%20%20XLM-Roberta%20Sentence%20Embeddings%20and%20Deep%20Neural%20Regression&body=Title%3A%20FuocChuVIP123%20at%20CoMeDi%20Shared%20Task%3A%20Disagreement%20Ranking%20with%0A%20%20XLM-Roberta%20Sentence%20Embeddings%20and%20Deep%20Neural%20Regression%0AAuthor%3A%20Phuoc%20Duong%20Huy%20Chu%0AAbstract%3A%20%20%20This%20paper%20presents%20results%20of%20our%20system%20for%20CoMeDi%20Shared%20Task%2C%20focusing%20on%0ASubtask%202%3A%20Disagreement%20Ranking.%20Our%20system%20leverages%20sentence%20embeddings%0Agenerated%20by%20the%20paraphrase-xlm-r-multilingual-v1%20model%2C%20combined%20with%20a%20deep%0Aneural%20regression%20model%20incorporating%20batch%20normalization%20and%20dropout%20for%0Aimproved%20generalization.%20By%20predicting%20the%20mean%20of%20pairwise%20judgment%0Adifferences%20between%20annotators%2C%20our%20method%20explicitly%20targets%20disagreement%0Aranking%2C%20diverging%20from%20traditional%20%22gold%20label%22%20aggregation%20approaches.%20We%0Aoptimized%20our%20system%20with%20a%20customized%20architecture%20and%20training%20procedure%2C%0Aachieving%20competitive%20performance%20in%20Spearman%20correlation%20against%20mean%0Adisagreement%20labels.%20Our%20results%20highlight%20the%20importance%20of%20robust%20embeddings%2C%0Aeffective%20model%20architecture%2C%20and%20careful%20handling%20of%20judgment%20differences%20for%0Aranking%20disagreement%20in%20multilingual%20contexts.%20These%20findings%20provide%20insights%0Ainto%20the%20use%20of%20contextualized%20representations%20for%20ordinal%20judgment%20tasks%20and%0Aopen%20avenues%20for%20further%20refinement%20of%20disagreement%20prediction%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuocChuVIP123%2520at%2520CoMeDi%2520Shared%2520Task%253A%2520Disagreement%2520Ranking%2520with%250A%2520%2520XLM-Roberta%2520Sentence%2520Embeddings%2520and%2520Deep%2520Neural%2520Regression%26entry.906535625%3DPhuoc%2520Duong%2520Huy%2520Chu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520results%2520of%2520our%2520system%2520for%2520CoMeDi%2520Shared%2520Task%252C%2520focusing%2520on%250ASubtask%25202%253A%2520Disagreement%2520Ranking.%2520Our%2520system%2520leverages%2520sentence%2520embeddings%250Agenerated%2520by%2520the%2520paraphrase-xlm-r-multilingual-v1%2520model%252C%2520combined%2520with%2520a%2520deep%250Aneural%2520regression%2520model%2520incorporating%2520batch%2520normalization%2520and%2520dropout%2520for%250Aimproved%2520generalization.%2520By%2520predicting%2520the%2520mean%2520of%2520pairwise%2520judgment%250Adifferences%2520between%2520annotators%252C%2520our%2520method%2520explicitly%2520targets%2520disagreement%250Aranking%252C%2520diverging%2520from%2520traditional%2520%2522gold%2520label%2522%2520aggregation%2520approaches.%2520We%250Aoptimized%2520our%2520system%2520with%2520a%2520customized%2520architecture%2520and%2520training%2520procedure%252C%250Aachieving%2520competitive%2520performance%2520in%2520Spearman%2520correlation%2520against%2520mean%250Adisagreement%2520labels.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520robust%2520embeddings%252C%250Aeffective%2520model%2520architecture%252C%2520and%2520careful%2520handling%2520of%2520judgment%2520differences%2520for%250Aranking%2520disagreement%2520in%2520multilingual%2520contexts.%2520These%2520findings%2520provide%2520insights%250Ainto%2520the%2520use%2520of%2520contextualized%2520representations%2520for%2520ordinal%2520judgment%2520tasks%2520and%250Aopen%2520avenues%2520for%2520further%2520refinement%2520of%2520disagreement%2520prediction%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FuocChuVIP123%20at%20CoMeDi%20Shared%20Task%3A%20Disagreement%20Ranking%20with%0A%20%20XLM-Roberta%20Sentence%20Embeddings%20and%20Deep%20Neural%20Regression&entry.906535625=Phuoc%20Duong%20Huy%20Chu&entry.1292438233=%20%20This%20paper%20presents%20results%20of%20our%20system%20for%20CoMeDi%20Shared%20Task%2C%20focusing%20on%0ASubtask%202%3A%20Disagreement%20Ranking.%20Our%20system%20leverages%20sentence%20embeddings%0Agenerated%20by%20the%20paraphrase-xlm-r-multilingual-v1%20model%2C%20combined%20with%20a%20deep%0Aneural%20regression%20model%20incorporating%20batch%20normalization%20and%20dropout%20for%0Aimproved%20generalization.%20By%20predicting%20the%20mean%20of%20pairwise%20judgment%0Adifferences%20between%20annotators%2C%20our%20method%20explicitly%20targets%20disagreement%0Aranking%2C%20diverging%20from%20traditional%20%22gold%20label%22%20aggregation%20approaches.%20We%0Aoptimized%20our%20system%20with%20a%20customized%20architecture%20and%20training%20procedure%2C%0Aachieving%20competitive%20performance%20in%20Spearman%20correlation%20against%20mean%0Adisagreement%20labels.%20Our%20results%20highlight%20the%20importance%20of%20robust%20embeddings%2C%0Aeffective%20model%20architecture%2C%20and%20careful%20handling%20of%20judgment%20differences%20for%0Aranking%20disagreement%20in%20multilingual%20contexts.%20These%20findings%20provide%20insights%0Ainto%20the%20use%20of%20contextualized%20representations%20for%20ordinal%20judgment%20tasks%20and%0Aopen%20avenues%20for%20further%20refinement%20of%20disagreement%20prediction%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12336v1&entry.124074799=Read"},
{"title": "Regressor-Guided Image Editing Regulates Emotional Response to Reduce\n  Online Engagement", "author": "Christoph Gebhardt and Robin Willardt and Seyedmorteza Sadat and Chih-Wei Ning and Andreas Brombach and Jie Song and Otmar Hilliges and Christian Holz", "abstract": "  Emotions are known to mediate the relationship between users' content\nconsumption and their online engagement, with heightened emotional intensity\nleading to increased engagement. Building on this insight, we propose three\nregressor-guided image editing approaches aimed at diminishing the emotional\nimpact of images. These include (i) a parameter optimization approach based on\nglobal image transformations known to influence emotions, (ii) an optimization\napproach targeting the style latent space of a generative adversarial network,\nand (iii) a diffusion-based approach employing classifier guidance and\nclassifier-free guidance. Our findings demonstrate that approaches can\neffectively alter the emotional properties of images while maintaining high\nvisual quality. Optimization-based methods primarily adjust low-level\nproperties like color hues and brightness, whereas the diffusion-based approach\nintroduces semantic changes, such as altering appearance or facial expressions.\nNotably, results from a behavioral study reveal that only the diffusion-based\napproach successfully elicits changes in viewers' emotional responses while\npreserving high perceived image quality. In future work, we will investigate\nthe impact of these image adaptations on internet user behavior.\n", "link": "http://arxiv.org/abs/2501.12289v1", "date": "2025-01-21", "relevancy": 1.5867, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5613}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5216}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regressor-Guided%20Image%20Editing%20Regulates%20Emotional%20Response%20to%20Reduce%0A%20%20Online%20Engagement&body=Title%3A%20Regressor-Guided%20Image%20Editing%20Regulates%20Emotional%20Response%20to%20Reduce%0A%20%20Online%20Engagement%0AAuthor%3A%20Christoph%20Gebhardt%20and%20Robin%20Willardt%20and%20Seyedmorteza%20Sadat%20and%20Chih-Wei%20Ning%20and%20Andreas%20Brombach%20and%20Jie%20Song%20and%20Otmar%20Hilliges%20and%20Christian%20Holz%0AAbstract%3A%20%20%20Emotions%20are%20known%20to%20mediate%20the%20relationship%20between%20users%27%20content%0Aconsumption%20and%20their%20online%20engagement%2C%20with%20heightened%20emotional%20intensity%0Aleading%20to%20increased%20engagement.%20Building%20on%20this%20insight%2C%20we%20propose%20three%0Aregressor-guided%20image%20editing%20approaches%20aimed%20at%20diminishing%20the%20emotional%0Aimpact%20of%20images.%20These%20include%20%28i%29%20a%20parameter%20optimization%20approach%20based%20on%0Aglobal%20image%20transformations%20known%20to%20influence%20emotions%2C%20%28ii%29%20an%20optimization%0Aapproach%20targeting%20the%20style%20latent%20space%20of%20a%20generative%20adversarial%20network%2C%0Aand%20%28iii%29%20a%20diffusion-based%20approach%20employing%20classifier%20guidance%20and%0Aclassifier-free%20guidance.%20Our%20findings%20demonstrate%20that%20approaches%20can%0Aeffectively%20alter%20the%20emotional%20properties%20of%20images%20while%20maintaining%20high%0Avisual%20quality.%20Optimization-based%20methods%20primarily%20adjust%20low-level%0Aproperties%20like%20color%20hues%20and%20brightness%2C%20whereas%20the%20diffusion-based%20approach%0Aintroduces%20semantic%20changes%2C%20such%20as%20altering%20appearance%20or%20facial%20expressions.%0ANotably%2C%20results%20from%20a%20behavioral%20study%20reveal%20that%20only%20the%20diffusion-based%0Aapproach%20successfully%20elicits%20changes%20in%20viewers%27%20emotional%20responses%20while%0Apreserving%20high%20perceived%20image%20quality.%20In%20future%20work%2C%20we%20will%20investigate%0Athe%20impact%20of%20these%20image%20adaptations%20on%20internet%20user%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegressor-Guided%2520Image%2520Editing%2520Regulates%2520Emotional%2520Response%2520to%2520Reduce%250A%2520%2520Online%2520Engagement%26entry.906535625%3DChristoph%2520Gebhardt%2520and%2520Robin%2520Willardt%2520and%2520Seyedmorteza%2520Sadat%2520and%2520Chih-Wei%2520Ning%2520and%2520Andreas%2520Brombach%2520and%2520Jie%2520Song%2520and%2520Otmar%2520Hilliges%2520and%2520Christian%2520Holz%26entry.1292438233%3D%2520%2520Emotions%2520are%2520known%2520to%2520mediate%2520the%2520relationship%2520between%2520users%2527%2520content%250Aconsumption%2520and%2520their%2520online%2520engagement%252C%2520with%2520heightened%2520emotional%2520intensity%250Aleading%2520to%2520increased%2520engagement.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520three%250Aregressor-guided%2520image%2520editing%2520approaches%2520aimed%2520at%2520diminishing%2520the%2520emotional%250Aimpact%2520of%2520images.%2520These%2520include%2520%2528i%2529%2520a%2520parameter%2520optimization%2520approach%2520based%2520on%250Aglobal%2520image%2520transformations%2520known%2520to%2520influence%2520emotions%252C%2520%2528ii%2529%2520an%2520optimization%250Aapproach%2520targeting%2520the%2520style%2520latent%2520space%2520of%2520a%2520generative%2520adversarial%2520network%252C%250Aand%2520%2528iii%2529%2520a%2520diffusion-based%2520approach%2520employing%2520classifier%2520guidance%2520and%250Aclassifier-free%2520guidance.%2520Our%2520findings%2520demonstrate%2520that%2520approaches%2520can%250Aeffectively%2520alter%2520the%2520emotional%2520properties%2520of%2520images%2520while%2520maintaining%2520high%250Avisual%2520quality.%2520Optimization-based%2520methods%2520primarily%2520adjust%2520low-level%250Aproperties%2520like%2520color%2520hues%2520and%2520brightness%252C%2520whereas%2520the%2520diffusion-based%2520approach%250Aintroduces%2520semantic%2520changes%252C%2520such%2520as%2520altering%2520appearance%2520or%2520facial%2520expressions.%250ANotably%252C%2520results%2520from%2520a%2520behavioral%2520study%2520reveal%2520that%2520only%2520the%2520diffusion-based%250Aapproach%2520successfully%2520elicits%2520changes%2520in%2520viewers%2527%2520emotional%2520responses%2520while%250Apreserving%2520high%2520perceived%2520image%2520quality.%2520In%2520future%2520work%252C%2520we%2520will%2520investigate%250Athe%2520impact%2520of%2520these%2520image%2520adaptations%2520on%2520internet%2520user%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regressor-Guided%20Image%20Editing%20Regulates%20Emotional%20Response%20to%20Reduce%0A%20%20Online%20Engagement&entry.906535625=Christoph%20Gebhardt%20and%20Robin%20Willardt%20and%20Seyedmorteza%20Sadat%20and%20Chih-Wei%20Ning%20and%20Andreas%20Brombach%20and%20Jie%20Song%20and%20Otmar%20Hilliges%20and%20Christian%20Holz&entry.1292438233=%20%20Emotions%20are%20known%20to%20mediate%20the%20relationship%20between%20users%27%20content%0Aconsumption%20and%20their%20online%20engagement%2C%20with%20heightened%20emotional%20intensity%0Aleading%20to%20increased%20engagement.%20Building%20on%20this%20insight%2C%20we%20propose%20three%0Aregressor-guided%20image%20editing%20approaches%20aimed%20at%20diminishing%20the%20emotional%0Aimpact%20of%20images.%20These%20include%20%28i%29%20a%20parameter%20optimization%20approach%20based%20on%0Aglobal%20image%20transformations%20known%20to%20influence%20emotions%2C%20%28ii%29%20an%20optimization%0Aapproach%20targeting%20the%20style%20latent%20space%20of%20a%20generative%20adversarial%20network%2C%0Aand%20%28iii%29%20a%20diffusion-based%20approach%20employing%20classifier%20guidance%20and%0Aclassifier-free%20guidance.%20Our%20findings%20demonstrate%20that%20approaches%20can%0Aeffectively%20alter%20the%20emotional%20properties%20of%20images%20while%20maintaining%20high%0Avisual%20quality.%20Optimization-based%20methods%20primarily%20adjust%20low-level%0Aproperties%20like%20color%20hues%20and%20brightness%2C%20whereas%20the%20diffusion-based%20approach%0Aintroduces%20semantic%20changes%2C%20such%20as%20altering%20appearance%20or%20facial%20expressions.%0ANotably%2C%20results%20from%20a%20behavioral%20study%20reveal%20that%20only%20the%20diffusion-based%0Aapproach%20successfully%20elicits%20changes%20in%20viewers%27%20emotional%20responses%20while%0Apreserving%20high%20perceived%20image%20quality.%20In%20future%20work%2C%20we%20will%20investigate%0Athe%20impact%20of%20these%20image%20adaptations%20on%20internet%20user%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12289v1&entry.124074799=Read"},
{"title": "Heterogeneous Federated Learning Systems for Time-Series Power\n  Consumption Prediction with Multi-Head Embedding Mechanism", "author": "Jia-Hao Syu and Jerry Chun-Wei Lin and Gautam Srivastava and Unil Yun", "abstract": "  Time-series prediction is increasingly popular in a variety of applications,\nsuch as smart factories and smart transportation. Researchers have used various\ntechniques to predict power consumption, but existing models lack discussion of\ncollaborative learning and privacy issues among multiple clients. To address\nthese issues, we propose Multi-Head Heterogeneous Federated Learning (MHHFL)\nsystems that consist of multiple head networks, which independently act as\ncarriers for federated learning. In the federated period, each head network is\nembedded into 2-dimensional vectors and shared with the centralized source\npool. MHHFL then selects appropriate source networks and blends the head\nnetworks as knowledge transfer in federated learning. The experimental results\nshow that the proposed MHHFL systems significantly outperform the benchmark and\nstate-of-the-art systems and reduce the prediction error by 24.9% to 94.1%. The\nablation studies demonstrate the effectiveness of the proposed mechanisms in\nthe MHHFL (head network embedding and selection mechanisms), which\nsignificantly outperforms traditional federated average and random transfer.\n", "link": "http://arxiv.org/abs/2501.12136v1", "date": "2025-01-21", "relevancy": 1.4628, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.488}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Federated%20Learning%20Systems%20for%20Time-Series%20Power%0A%20%20Consumption%20Prediction%20with%20Multi-Head%20Embedding%20Mechanism&body=Title%3A%20Heterogeneous%20Federated%20Learning%20Systems%20for%20Time-Series%20Power%0A%20%20Consumption%20Prediction%20with%20Multi-Head%20Embedding%20Mechanism%0AAuthor%3A%20Jia-Hao%20Syu%20and%20Jerry%20Chun-Wei%20Lin%20and%20Gautam%20Srivastava%20and%20Unil%20Yun%0AAbstract%3A%20%20%20Time-series%20prediction%20is%20increasingly%20popular%20in%20a%20variety%20of%20applications%2C%0Asuch%20as%20smart%20factories%20and%20smart%20transportation.%20Researchers%20have%20used%20various%0Atechniques%20to%20predict%20power%20consumption%2C%20but%20existing%20models%20lack%20discussion%20of%0Acollaborative%20learning%20and%20privacy%20issues%20among%20multiple%20clients.%20To%20address%0Athese%20issues%2C%20we%20propose%20Multi-Head%20Heterogeneous%20Federated%20Learning%20%28MHHFL%29%0Asystems%20that%20consist%20of%20multiple%20head%20networks%2C%20which%20independently%20act%20as%0Acarriers%20for%20federated%20learning.%20In%20the%20federated%20period%2C%20each%20head%20network%20is%0Aembedded%20into%202-dimensional%20vectors%20and%20shared%20with%20the%20centralized%20source%0Apool.%20MHHFL%20then%20selects%20appropriate%20source%20networks%20and%20blends%20the%20head%0Anetworks%20as%20knowledge%20transfer%20in%20federated%20learning.%20The%20experimental%20results%0Ashow%20that%20the%20proposed%20MHHFL%20systems%20significantly%20outperform%20the%20benchmark%20and%0Astate-of-the-art%20systems%20and%20reduce%20the%20prediction%20error%20by%2024.9%25%20to%2094.1%25.%20The%0Aablation%20studies%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20mechanisms%20in%0Athe%20MHHFL%20%28head%20network%20embedding%20and%20selection%20mechanisms%29%2C%20which%0Asignificantly%20outperforms%20traditional%20federated%20average%20and%20random%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneous%2520Federated%2520Learning%2520Systems%2520for%2520Time-Series%2520Power%250A%2520%2520Consumption%2520Prediction%2520with%2520Multi-Head%2520Embedding%2520Mechanism%26entry.906535625%3DJia-Hao%2520Syu%2520and%2520Jerry%2520Chun-Wei%2520Lin%2520and%2520Gautam%2520Srivastava%2520and%2520Unil%2520Yun%26entry.1292438233%3D%2520%2520Time-series%2520prediction%2520is%2520increasingly%2520popular%2520in%2520a%2520variety%2520of%2520applications%252C%250Asuch%2520as%2520smart%2520factories%2520and%2520smart%2520transportation.%2520Researchers%2520have%2520used%2520various%250Atechniques%2520to%2520predict%2520power%2520consumption%252C%2520but%2520existing%2520models%2520lack%2520discussion%2520of%250Acollaborative%2520learning%2520and%2520privacy%2520issues%2520among%2520multiple%2520clients.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520Multi-Head%2520Heterogeneous%2520Federated%2520Learning%2520%2528MHHFL%2529%250Asystems%2520that%2520consist%2520of%2520multiple%2520head%2520networks%252C%2520which%2520independently%2520act%2520as%250Acarriers%2520for%2520federated%2520learning.%2520In%2520the%2520federated%2520period%252C%2520each%2520head%2520network%2520is%250Aembedded%2520into%25202-dimensional%2520vectors%2520and%2520shared%2520with%2520the%2520centralized%2520source%250Apool.%2520MHHFL%2520then%2520selects%2520appropriate%2520source%2520networks%2520and%2520blends%2520the%2520head%250Anetworks%2520as%2520knowledge%2520transfer%2520in%2520federated%2520learning.%2520The%2520experimental%2520results%250Ashow%2520that%2520the%2520proposed%2520MHHFL%2520systems%2520significantly%2520outperform%2520the%2520benchmark%2520and%250Astate-of-the-art%2520systems%2520and%2520reduce%2520the%2520prediction%2520error%2520by%252024.9%2525%2520to%252094.1%2525.%2520The%250Aablation%2520studies%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520mechanisms%2520in%250Athe%2520MHHFL%2520%2528head%2520network%2520embedding%2520and%2520selection%2520mechanisms%2529%252C%2520which%250Asignificantly%2520outperforms%2520traditional%2520federated%2520average%2520and%2520random%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Federated%20Learning%20Systems%20for%20Time-Series%20Power%0A%20%20Consumption%20Prediction%20with%20Multi-Head%20Embedding%20Mechanism&entry.906535625=Jia-Hao%20Syu%20and%20Jerry%20Chun-Wei%20Lin%20and%20Gautam%20Srivastava%20and%20Unil%20Yun&entry.1292438233=%20%20Time-series%20prediction%20is%20increasingly%20popular%20in%20a%20variety%20of%20applications%2C%0Asuch%20as%20smart%20factories%20and%20smart%20transportation.%20Researchers%20have%20used%20various%0Atechniques%20to%20predict%20power%20consumption%2C%20but%20existing%20models%20lack%20discussion%20of%0Acollaborative%20learning%20and%20privacy%20issues%20among%20multiple%20clients.%20To%20address%0Athese%20issues%2C%20we%20propose%20Multi-Head%20Heterogeneous%20Federated%20Learning%20%28MHHFL%29%0Asystems%20that%20consist%20of%20multiple%20head%20networks%2C%20which%20independently%20act%20as%0Acarriers%20for%20federated%20learning.%20In%20the%20federated%20period%2C%20each%20head%20network%20is%0Aembedded%20into%202-dimensional%20vectors%20and%20shared%20with%20the%20centralized%20source%0Apool.%20MHHFL%20then%20selects%20appropriate%20source%20networks%20and%20blends%20the%20head%0Anetworks%20as%20knowledge%20transfer%20in%20federated%20learning.%20The%20experimental%20results%0Ashow%20that%20the%20proposed%20MHHFL%20systems%20significantly%20outperform%20the%20benchmark%20and%0Astate-of-the-art%20systems%20and%20reduce%20the%20prediction%20error%20by%2024.9%25%20to%2094.1%25.%20The%0Aablation%20studies%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20mechanisms%20in%0Athe%20MHHFL%20%28head%20network%20embedding%20and%20selection%20mechanisms%29%2C%20which%0Asignificantly%20outperforms%20traditional%20federated%20average%20and%20random%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12136v1&entry.124074799=Read"},
{"title": "Parameterised Quantum Circuits for Novel Representation Learning in\n  Speech Emotion Recognition", "author": "Thejan Rajapakshe and Rajib Rana and Farina Riaz and Sara Khalifa and Bj\u00f6rn W. Schuller", "abstract": "  Speech Emotion Recognition (SER) is a complex and challenging task in\nhuman-computer interaction due to the intricate dependencies of features and\nthe overlapping nature of emotional expressions conveyed through speech.\nAlthough traditional deep learning methods have shown effectiveness, they often\nstruggle to capture subtle emotional variations and overlapping states. This\npaper introduces a hybrid classical-quantum framework that integrates\nParameterised Quantum Circuits (PQCs) with conventional Convolutional Neural\nNetwork (CNN) architectures. By leveraging quantum properties such as\nsuperposition and entanglement, the proposed model enhances feature\nrepresentation and captures complex dependencies more effectively than\nclassical methods. Experimental evaluations conducted on benchmark datasets,\nincluding IEMOCAP, RECOLA, and MSP-Improv, demonstrate that the hybrid model\nachieves higher accuracy in both binary and multi-class emotion classification\nwhile significantly reducing the number of trainable parameters. While a few\nexisting studies have explored the feasibility of using Quantum Circuits to\nreduce model complexity, none have successfully shown how they can enhance\naccuracy. This study is the first to demonstrate that Quantum Circuits has the\npotential to improve the accuracy of SER. The findings highlight the promise of\nQML to transform SER, suggesting a promising direction for future research and\npractical applications in emotion-aware systems.\n", "link": "http://arxiv.org/abs/2501.12050v1", "date": "2025-01-21", "relevancy": 1.9163, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameterised%20Quantum%20Circuits%20for%20Novel%20Representation%20Learning%20in%0A%20%20Speech%20Emotion%20Recognition&body=Title%3A%20Parameterised%20Quantum%20Circuits%20for%20Novel%20Representation%20Learning%20in%0A%20%20Speech%20Emotion%20Recognition%0AAuthor%3A%20Thejan%20Rajapakshe%20and%20Rajib%20Rana%20and%20Farina%20Riaz%20and%20Sara%20Khalifa%20and%20Bj%C3%B6rn%20W.%20Schuller%0AAbstract%3A%20%20%20Speech%20Emotion%20Recognition%20%28SER%29%20is%20a%20complex%20and%20challenging%20task%20in%0Ahuman-computer%20interaction%20due%20to%20the%20intricate%20dependencies%20of%20features%20and%0Athe%20overlapping%20nature%20of%20emotional%20expressions%20conveyed%20through%20speech.%0AAlthough%20traditional%20deep%20learning%20methods%20have%20shown%20effectiveness%2C%20they%20often%0Astruggle%20to%20capture%20subtle%20emotional%20variations%20and%20overlapping%20states.%20This%0Apaper%20introduces%20a%20hybrid%20classical-quantum%20framework%20that%20integrates%0AParameterised%20Quantum%20Circuits%20%28PQCs%29%20with%20conventional%20Convolutional%20Neural%0ANetwork%20%28CNN%29%20architectures.%20By%20leveraging%20quantum%20properties%20such%20as%0Asuperposition%20and%20entanglement%2C%20the%20proposed%20model%20enhances%20feature%0Arepresentation%20and%20captures%20complex%20dependencies%20more%20effectively%20than%0Aclassical%20methods.%20Experimental%20evaluations%20conducted%20on%20benchmark%20datasets%2C%0Aincluding%20IEMOCAP%2C%20RECOLA%2C%20and%20MSP-Improv%2C%20demonstrate%20that%20the%20hybrid%20model%0Aachieves%20higher%20accuracy%20in%20both%20binary%20and%20multi-class%20emotion%20classification%0Awhile%20significantly%20reducing%20the%20number%20of%20trainable%20parameters.%20While%20a%20few%0Aexisting%20studies%20have%20explored%20the%20feasibility%20of%20using%20Quantum%20Circuits%20to%0Areduce%20model%20complexity%2C%20none%20have%20successfully%20shown%20how%20they%20can%20enhance%0Aaccuracy.%20This%20study%20is%20the%20first%20to%20demonstrate%20that%20Quantum%20Circuits%20has%20the%0Apotential%20to%20improve%20the%20accuracy%20of%20SER.%20The%20findings%20highlight%20the%20promise%20of%0AQML%20to%20transform%20SER%2C%20suggesting%20a%20promising%20direction%20for%20future%20research%20and%0Apractical%20applications%20in%20emotion-aware%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameterised%2520Quantum%2520Circuits%2520for%2520Novel%2520Representation%2520Learning%2520in%250A%2520%2520Speech%2520Emotion%2520Recognition%26entry.906535625%3DThejan%2520Rajapakshe%2520and%2520Rajib%2520Rana%2520and%2520Farina%2520Riaz%2520and%2520Sara%2520Khalifa%2520and%2520Bj%25C3%25B6rn%2520W.%2520Schuller%26entry.1292438233%3D%2520%2520Speech%2520Emotion%2520Recognition%2520%2528SER%2529%2520is%2520a%2520complex%2520and%2520challenging%2520task%2520in%250Ahuman-computer%2520interaction%2520due%2520to%2520the%2520intricate%2520dependencies%2520of%2520features%2520and%250Athe%2520overlapping%2520nature%2520of%2520emotional%2520expressions%2520conveyed%2520through%2520speech.%250AAlthough%2520traditional%2520deep%2520learning%2520methods%2520have%2520shown%2520effectiveness%252C%2520they%2520often%250Astruggle%2520to%2520capture%2520subtle%2520emotional%2520variations%2520and%2520overlapping%2520states.%2520This%250Apaper%2520introduces%2520a%2520hybrid%2520classical-quantum%2520framework%2520that%2520integrates%250AParameterised%2520Quantum%2520Circuits%2520%2528PQCs%2529%2520with%2520conventional%2520Convolutional%2520Neural%250ANetwork%2520%2528CNN%2529%2520architectures.%2520By%2520leveraging%2520quantum%2520properties%2520such%2520as%250Asuperposition%2520and%2520entanglement%252C%2520the%2520proposed%2520model%2520enhances%2520feature%250Arepresentation%2520and%2520captures%2520complex%2520dependencies%2520more%2520effectively%2520than%250Aclassical%2520methods.%2520Experimental%2520evaluations%2520conducted%2520on%2520benchmark%2520datasets%252C%250Aincluding%2520IEMOCAP%252C%2520RECOLA%252C%2520and%2520MSP-Improv%252C%2520demonstrate%2520that%2520the%2520hybrid%2520model%250Aachieves%2520higher%2520accuracy%2520in%2520both%2520binary%2520and%2520multi-class%2520emotion%2520classification%250Awhile%2520significantly%2520reducing%2520the%2520number%2520of%2520trainable%2520parameters.%2520While%2520a%2520few%250Aexisting%2520studies%2520have%2520explored%2520the%2520feasibility%2520of%2520using%2520Quantum%2520Circuits%2520to%250Areduce%2520model%2520complexity%252C%2520none%2520have%2520successfully%2520shown%2520how%2520they%2520can%2520enhance%250Aaccuracy.%2520This%2520study%2520is%2520the%2520first%2520to%2520demonstrate%2520that%2520Quantum%2520Circuits%2520has%2520the%250Apotential%2520to%2520improve%2520the%2520accuracy%2520of%2520SER.%2520The%2520findings%2520highlight%2520the%2520promise%2520of%250AQML%2520to%2520transform%2520SER%252C%2520suggesting%2520a%2520promising%2520direction%2520for%2520future%2520research%2520and%250Apractical%2520applications%2520in%2520emotion-aware%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameterised%20Quantum%20Circuits%20for%20Novel%20Representation%20Learning%20in%0A%20%20Speech%20Emotion%20Recognition&entry.906535625=Thejan%20Rajapakshe%20and%20Rajib%20Rana%20and%20Farina%20Riaz%20and%20Sara%20Khalifa%20and%20Bj%C3%B6rn%20W.%20Schuller&entry.1292438233=%20%20Speech%20Emotion%20Recognition%20%28SER%29%20is%20a%20complex%20and%20challenging%20task%20in%0Ahuman-computer%20interaction%20due%20to%20the%20intricate%20dependencies%20of%20features%20and%0Athe%20overlapping%20nature%20of%20emotional%20expressions%20conveyed%20through%20speech.%0AAlthough%20traditional%20deep%20learning%20methods%20have%20shown%20effectiveness%2C%20they%20often%0Astruggle%20to%20capture%20subtle%20emotional%20variations%20and%20overlapping%20states.%20This%0Apaper%20introduces%20a%20hybrid%20classical-quantum%20framework%20that%20integrates%0AParameterised%20Quantum%20Circuits%20%28PQCs%29%20with%20conventional%20Convolutional%20Neural%0ANetwork%20%28CNN%29%20architectures.%20By%20leveraging%20quantum%20properties%20such%20as%0Asuperposition%20and%20entanglement%2C%20the%20proposed%20model%20enhances%20feature%0Arepresentation%20and%20captures%20complex%20dependencies%20more%20effectively%20than%0Aclassical%20methods.%20Experimental%20evaluations%20conducted%20on%20benchmark%20datasets%2C%0Aincluding%20IEMOCAP%2C%20RECOLA%2C%20and%20MSP-Improv%2C%20demonstrate%20that%20the%20hybrid%20model%0Aachieves%20higher%20accuracy%20in%20both%20binary%20and%20multi-class%20emotion%20classification%0Awhile%20significantly%20reducing%20the%20number%20of%20trainable%20parameters.%20While%20a%20few%0Aexisting%20studies%20have%20explored%20the%20feasibility%20of%20using%20Quantum%20Circuits%20to%0Areduce%20model%20complexity%2C%20none%20have%20successfully%20shown%20how%20they%20can%20enhance%0Aaccuracy.%20This%20study%20is%20the%20first%20to%20demonstrate%20that%20Quantum%20Circuits%20has%20the%0Apotential%20to%20improve%20the%20accuracy%20of%20SER.%20The%20findings%20highlight%20the%20promise%20of%0AQML%20to%20transform%20SER%2C%20suggesting%20a%20promising%20direction%20for%20future%20research%20and%0Apractical%20applications%20in%20emotion-aware%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12050v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


