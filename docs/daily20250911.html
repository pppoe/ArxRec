<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250910.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SAFT: Shape and Appearance of Fabrics from Template via Differentiable\n  Physical Simulations from Monocular Video", "author": "David Stotko and Reinhard Klein", "abstract": "  The reconstruction of three-dimensional dynamic scenes is a well-established\nyet challenging task within the domain of computer vision. In this paper, we\npropose a novel approach that combines the domains of 3D geometry\nreconstruction and appearance estimation for physically based rendering and\npresent a system that is able to perform both tasks for fabrics, utilizing only\na single monocular RGB video sequence as input. In order to obtain realistic\nand high-quality deformations and renderings, a physical simulation of the\ncloth geometry and differentiable rendering are employed. In this paper, we\nintroduce two novel regularization terms for the 3D reconstruction task that\nimprove the plausibility of the reconstruction by addressing the depth\nambiguity problem in monocular video. In comparison with the most recent\nmethods in the field, we have reduced the error in the 3D reconstruction by a\nfactor of 2.64 while requiring a medium runtime of 30 min per scene.\nFurthermore, the optimized motion achieves sufficient quality to perform an\nappearance estimation of the deforming object, recovering sharp details from\nthis single monocular RGB video.\n", "link": "http://arxiv.org/abs/2509.08828v1", "date": "2025-09-10", "relevancy": 3.1836, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6677}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6399}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAFT%3A%20Shape%20and%20Appearance%20of%20Fabrics%20from%20Template%20via%20Differentiable%0A%20%20Physical%20Simulations%20from%20Monocular%20Video&body=Title%3A%20SAFT%3A%20Shape%20and%20Appearance%20of%20Fabrics%20from%20Template%20via%20Differentiable%0A%20%20Physical%20Simulations%20from%20Monocular%20Video%0AAuthor%3A%20David%20Stotko%20and%20Reinhard%20Klein%0AAbstract%3A%20%20%20The%20reconstruction%20of%20three-dimensional%20dynamic%20scenes%20is%20a%20well-established%0Ayet%20challenging%20task%20within%20the%20domain%20of%20computer%20vision.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20approach%20that%20combines%20the%20domains%20of%203D%20geometry%0Areconstruction%20and%20appearance%20estimation%20for%20physically%20based%20rendering%20and%0Apresent%20a%20system%20that%20is%20able%20to%20perform%20both%20tasks%20for%20fabrics%2C%20utilizing%20only%0Aa%20single%20monocular%20RGB%20video%20sequence%20as%20input.%20In%20order%20to%20obtain%20realistic%0Aand%20high-quality%20deformations%20and%20renderings%2C%20a%20physical%20simulation%20of%20the%0Acloth%20geometry%20and%20differentiable%20rendering%20are%20employed.%20In%20this%20paper%2C%20we%0Aintroduce%20two%20novel%20regularization%20terms%20for%20the%203D%20reconstruction%20task%20that%0Aimprove%20the%20plausibility%20of%20the%20reconstruction%20by%20addressing%20the%20depth%0Aambiguity%20problem%20in%20monocular%20video.%20In%20comparison%20with%20the%20most%20recent%0Amethods%20in%20the%20field%2C%20we%20have%20reduced%20the%20error%20in%20the%203D%20reconstruction%20by%20a%0Afactor%20of%202.64%20while%20requiring%20a%20medium%20runtime%20of%2030%20min%20per%20scene.%0AFurthermore%2C%20the%20optimized%20motion%20achieves%20sufficient%20quality%20to%20perform%20an%0Aappearance%20estimation%20of%20the%20deforming%20object%2C%20recovering%20sharp%20details%20from%0Athis%20single%20monocular%20RGB%20video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAFT%253A%2520Shape%2520and%2520Appearance%2520of%2520Fabrics%2520from%2520Template%2520via%2520Differentiable%250A%2520%2520Physical%2520Simulations%2520from%2520Monocular%2520Video%26entry.906535625%3DDavid%2520Stotko%2520and%2520Reinhard%2520Klein%26entry.1292438233%3D%2520%2520The%2520reconstruction%2520of%2520three-dimensional%2520dynamic%2520scenes%2520is%2520a%2520well-established%250Ayet%2520challenging%2520task%2520within%2520the%2520domain%2520of%2520computer%2520vision.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520that%2520combines%2520the%2520domains%2520of%25203D%2520geometry%250Areconstruction%2520and%2520appearance%2520estimation%2520for%2520physically%2520based%2520rendering%2520and%250Apresent%2520a%2520system%2520that%2520is%2520able%2520to%2520perform%2520both%2520tasks%2520for%2520fabrics%252C%2520utilizing%2520only%250Aa%2520single%2520monocular%2520RGB%2520video%2520sequence%2520as%2520input.%2520In%2520order%2520to%2520obtain%2520realistic%250Aand%2520high-quality%2520deformations%2520and%2520renderings%252C%2520a%2520physical%2520simulation%2520of%2520the%250Acloth%2520geometry%2520and%2520differentiable%2520rendering%2520are%2520employed.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520two%2520novel%2520regularization%2520terms%2520for%2520the%25203D%2520reconstruction%2520task%2520that%250Aimprove%2520the%2520plausibility%2520of%2520the%2520reconstruction%2520by%2520addressing%2520the%2520depth%250Aambiguity%2520problem%2520in%2520monocular%2520video.%2520In%2520comparison%2520with%2520the%2520most%2520recent%250Amethods%2520in%2520the%2520field%252C%2520we%2520have%2520reduced%2520the%2520error%2520in%2520the%25203D%2520reconstruction%2520by%2520a%250Afactor%2520of%25202.64%2520while%2520requiring%2520a%2520medium%2520runtime%2520of%252030%2520min%2520per%2520scene.%250AFurthermore%252C%2520the%2520optimized%2520motion%2520achieves%2520sufficient%2520quality%2520to%2520perform%2520an%250Aappearance%2520estimation%2520of%2520the%2520deforming%2520object%252C%2520recovering%2520sharp%2520details%2520from%250Athis%2520single%2520monocular%2520RGB%2520video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAFT%3A%20Shape%20and%20Appearance%20of%20Fabrics%20from%20Template%20via%20Differentiable%0A%20%20Physical%20Simulations%20from%20Monocular%20Video&entry.906535625=David%20Stotko%20and%20Reinhard%20Klein&entry.1292438233=%20%20The%20reconstruction%20of%20three-dimensional%20dynamic%20scenes%20is%20a%20well-established%0Ayet%20challenging%20task%20within%20the%20domain%20of%20computer%20vision.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20approach%20that%20combines%20the%20domains%20of%203D%20geometry%0Areconstruction%20and%20appearance%20estimation%20for%20physically%20based%20rendering%20and%0Apresent%20a%20system%20that%20is%20able%20to%20perform%20both%20tasks%20for%20fabrics%2C%20utilizing%20only%0Aa%20single%20monocular%20RGB%20video%20sequence%20as%20input.%20In%20order%20to%20obtain%20realistic%0Aand%20high-quality%20deformations%20and%20renderings%2C%20a%20physical%20simulation%20of%20the%0Acloth%20geometry%20and%20differentiable%20rendering%20are%20employed.%20In%20this%20paper%2C%20we%0Aintroduce%20two%20novel%20regularization%20terms%20for%20the%203D%20reconstruction%20task%20that%0Aimprove%20the%20plausibility%20of%20the%20reconstruction%20by%20addressing%20the%20depth%0Aambiguity%20problem%20in%20monocular%20video.%20In%20comparison%20with%20the%20most%20recent%0Amethods%20in%20the%20field%2C%20we%20have%20reduced%20the%20error%20in%20the%203D%20reconstruction%20by%20a%0Afactor%20of%202.64%20while%20requiring%20a%20medium%20runtime%20of%2030%20min%20per%20scene.%0AFurthermore%2C%20the%20optimized%20motion%20achieves%20sufficient%20quality%20to%20perform%20an%0Aappearance%20estimation%20of%20the%20deforming%20object%2C%20recovering%20sharp%20details%20from%0Athis%20single%20monocular%20RGB%20video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08828v1&entry.124074799=Read"},
{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "author": "Jenna Kang and Maria Silva and Patsorn Sangkloy and Kenneth Chen and Niall Williams and Qi Sun", "abstract": "  Recent advances in probabilistic generative models have extended capabilities\nfrom static image synthesis to text-driven video generation. However, the\ninherent randomness of their generation process can lead to unpredictable\nartifacts, such as impossible physics and temporal inconsistency. Progress in\naddressing these challenges requires systematic benchmarks, yet existing\ndatasets primarily focus on generative images due to the unique spatio-temporal\ncomplexities of videos. To bridge this gap, we introduce GeneVA, a large-scale\nartifact dataset with rich human annotations that focuses on spatio-temporal\nartifacts in videos generated from natural text prompts. We hope GeneVA can\nenable and assist critical applications, such as benchmarking model performance\nand improving generative video quality.\n", "link": "http://arxiv.org/abs/2509.08818v1", "date": "2025-09-10", "relevancy": 3.0319, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6273}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6096}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeneVA%3A%20A%20Dataset%20of%20Human%20Annotations%20for%20Generative%20Text%20to%20Video%0A%20%20Artifacts&body=Title%3A%20GeneVA%3A%20A%20Dataset%20of%20Human%20Annotations%20for%20Generative%20Text%20to%20Video%0A%20%20Artifacts%0AAuthor%3A%20Jenna%20Kang%20and%20Maria%20Silva%20and%20Patsorn%20Sangkloy%20and%20Kenneth%20Chen%20and%20Niall%20Williams%20and%20Qi%20Sun%0AAbstract%3A%20%20%20Recent%20advances%20in%20probabilistic%20generative%20models%20have%20extended%20capabilities%0Afrom%20static%20image%20synthesis%20to%20text-driven%20video%20generation.%20However%2C%20the%0Ainherent%20randomness%20of%20their%20generation%20process%20can%20lead%20to%20unpredictable%0Aartifacts%2C%20such%20as%20impossible%20physics%20and%20temporal%20inconsistency.%20Progress%20in%0Aaddressing%20these%20challenges%20requires%20systematic%20benchmarks%2C%20yet%20existing%0Adatasets%20primarily%20focus%20on%20generative%20images%20due%20to%20the%20unique%20spatio-temporal%0Acomplexities%20of%20videos.%20To%20bridge%20this%20gap%2C%20we%20introduce%20GeneVA%2C%20a%20large-scale%0Aartifact%20dataset%20with%20rich%20human%20annotations%20that%20focuses%20on%20spatio-temporal%0Aartifacts%20in%20videos%20generated%20from%20natural%20text%20prompts.%20We%20hope%20GeneVA%20can%0Aenable%20and%20assist%20critical%20applications%2C%20such%20as%20benchmarking%20model%20performance%0Aand%20improving%20generative%20video%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneVA%253A%2520A%2520Dataset%2520of%2520Human%2520Annotations%2520for%2520Generative%2520Text%2520to%2520Video%250A%2520%2520Artifacts%26entry.906535625%3DJenna%2520Kang%2520and%2520Maria%2520Silva%2520and%2520Patsorn%2520Sangkloy%2520and%2520Kenneth%2520Chen%2520and%2520Niall%2520Williams%2520and%2520Qi%2520Sun%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520probabilistic%2520generative%2520models%2520have%2520extended%2520capabilities%250Afrom%2520static%2520image%2520synthesis%2520to%2520text-driven%2520video%2520generation.%2520However%252C%2520the%250Ainherent%2520randomness%2520of%2520their%2520generation%2520process%2520can%2520lead%2520to%2520unpredictable%250Aartifacts%252C%2520such%2520as%2520impossible%2520physics%2520and%2520temporal%2520inconsistency.%2520Progress%2520in%250Aaddressing%2520these%2520challenges%2520requires%2520systematic%2520benchmarks%252C%2520yet%2520existing%250Adatasets%2520primarily%2520focus%2520on%2520generative%2520images%2520due%2520to%2520the%2520unique%2520spatio-temporal%250Acomplexities%2520of%2520videos.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520GeneVA%252C%2520a%2520large-scale%250Aartifact%2520dataset%2520with%2520rich%2520human%2520annotations%2520that%2520focuses%2520on%2520spatio-temporal%250Aartifacts%2520in%2520videos%2520generated%2520from%2520natural%2520text%2520prompts.%2520We%2520hope%2520GeneVA%2520can%250Aenable%2520and%2520assist%2520critical%2520applications%252C%2520such%2520as%2520benchmarking%2520model%2520performance%250Aand%2520improving%2520generative%2520video%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeneVA%3A%20A%20Dataset%20of%20Human%20Annotations%20for%20Generative%20Text%20to%20Video%0A%20%20Artifacts&entry.906535625=Jenna%20Kang%20and%20Maria%20Silva%20and%20Patsorn%20Sangkloy%20and%20Kenneth%20Chen%20and%20Niall%20Williams%20and%20Qi%20Sun&entry.1292438233=%20%20Recent%20advances%20in%20probabilistic%20generative%20models%20have%20extended%20capabilities%0Afrom%20static%20image%20synthesis%20to%20text-driven%20video%20generation.%20However%2C%20the%0Ainherent%20randomness%20of%20their%20generation%20process%20can%20lead%20to%20unpredictable%0Aartifacts%2C%20such%20as%20impossible%20physics%20and%20temporal%20inconsistency.%20Progress%20in%0Aaddressing%20these%20challenges%20requires%20systematic%20benchmarks%2C%20yet%20existing%0Adatasets%20primarily%20focus%20on%20generative%20images%20due%20to%20the%20unique%20spatio-temporal%0Acomplexities%20of%20videos.%20To%20bridge%20this%20gap%2C%20we%20introduce%20GeneVA%2C%20a%20large-scale%0Aartifact%20dataset%20with%20rich%20human%20annotations%20that%20focuses%20on%20spatio-temporal%0Aartifacts%20in%20videos%20generated%20from%20natural%20text%20prompts.%20We%20hope%20GeneVA%20can%0Aenable%20and%20assist%20critical%20applications%2C%20such%20as%20benchmarking%20model%20performance%0Aand%20improving%20generative%20video%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08818v1&entry.124074799=Read"},
{"title": "CLAPS: A CLIP-Unified Auto-Prompt Segmentation for Multi-Modal Retinal\n  Imaging", "author": "Zhihao Zhao and Yinzheng Zhao and Junjie Yang and Xiangtong Yao and Quanmin Liang and Shahrooz Faghihroohi and Kai Huang and Nassir Navab and M. Ali Nasseri", "abstract": "  Recent advancements in foundation models, such as the Segment Anything Model\n(SAM), have significantly impacted medical image segmentation, especially in\nretinal imaging, where precise segmentation is vital for diagnosis. Despite\nthis progress, current methods face critical challenges: 1) modality ambiguity\nin textual disease descriptions, 2) a continued reliance on manual prompting\nfor SAM-based workflows, and 3) a lack of a unified framework, with most\nmethods being modality- and task-specific. To overcome these hurdles, we\npropose CLIP-unified Auto-Prompt Segmentation (\\CLAPS), a novel method for\nunified segmentation across diverse tasks and modalities in retinal imaging.\nOur approach begins by pre-training a CLIP-based image encoder on a large,\nmulti-modal retinal dataset to handle data scarcity and distribution imbalance.\nWe then leverage GroundingDINO to automatically generate spatial bounding box\nprompts by detecting local lesions. To unify tasks and resolve ambiguity, we\nuse text prompts enhanced with a unique \"modality signature\" for each imaging\nmodality. Ultimately, these automated textual and spatial prompts guide SAM to\nexecute precise segmentation, creating a fully automated and unified pipeline.\nExtensive experiments on 12 diverse datasets across 11 critical segmentation\ncategories show that CLAPS achieves performance on par with specialized expert\nmodels while surpassing existing benchmarks across most metrics, demonstrating\nits broad generalizability as a foundation model.\n", "link": "http://arxiv.org/abs/2509.08618v1", "date": "2025-09-10", "relevancy": 2.9713, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLAPS%3A%20A%20CLIP-Unified%20Auto-Prompt%20Segmentation%20for%20Multi-Modal%20Retinal%0A%20%20Imaging&body=Title%3A%20CLAPS%3A%20A%20CLIP-Unified%20Auto-Prompt%20Segmentation%20for%20Multi-Modal%20Retinal%0A%20%20Imaging%0AAuthor%3A%20Zhihao%20Zhao%20and%20Yinzheng%20Zhao%20and%20Junjie%20Yang%20and%20Xiangtong%20Yao%20and%20Quanmin%20Liang%20and%20Shahrooz%20Faghihroohi%20and%20Kai%20Huang%20and%20Nassir%20Navab%20and%20M.%20Ali%20Nasseri%0AAbstract%3A%20%20%20Recent%20advancements%20in%20foundation%20models%2C%20such%20as%20the%20Segment%20Anything%20Model%0A%28SAM%29%2C%20have%20significantly%20impacted%20medical%20image%20segmentation%2C%20especially%20in%0Aretinal%20imaging%2C%20where%20precise%20segmentation%20is%20vital%20for%20diagnosis.%20Despite%0Athis%20progress%2C%20current%20methods%20face%20critical%20challenges%3A%201%29%20modality%20ambiguity%0Ain%20textual%20disease%20descriptions%2C%202%29%20a%20continued%20reliance%20on%20manual%20prompting%0Afor%20SAM-based%20workflows%2C%20and%203%29%20a%20lack%20of%20a%20unified%20framework%2C%20with%20most%0Amethods%20being%20modality-%20and%20task-specific.%20To%20overcome%20these%20hurdles%2C%20we%0Apropose%20CLIP-unified%20Auto-Prompt%20Segmentation%20%28%5CCLAPS%29%2C%20a%20novel%20method%20for%0Aunified%20segmentation%20across%20diverse%20tasks%20and%20modalities%20in%20retinal%20imaging.%0AOur%20approach%20begins%20by%20pre-training%20a%20CLIP-based%20image%20encoder%20on%20a%20large%2C%0Amulti-modal%20retinal%20dataset%20to%20handle%20data%20scarcity%20and%20distribution%20imbalance.%0AWe%20then%20leverage%20GroundingDINO%20to%20automatically%20generate%20spatial%20bounding%20box%0Aprompts%20by%20detecting%20local%20lesions.%20To%20unify%20tasks%20and%20resolve%20ambiguity%2C%20we%0Ause%20text%20prompts%20enhanced%20with%20a%20unique%20%22modality%20signature%22%20for%20each%20imaging%0Amodality.%20Ultimately%2C%20these%20automated%20textual%20and%20spatial%20prompts%20guide%20SAM%20to%0Aexecute%20precise%20segmentation%2C%20creating%20a%20fully%20automated%20and%20unified%20pipeline.%0AExtensive%20experiments%20on%2012%20diverse%20datasets%20across%2011%20critical%20segmentation%0Acategories%20show%20that%20CLAPS%20achieves%20performance%20on%20par%20with%20specialized%20expert%0Amodels%20while%20surpassing%20existing%20benchmarks%20across%20most%20metrics%2C%20demonstrating%0Aits%20broad%20generalizability%20as%20a%20foundation%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLAPS%253A%2520A%2520CLIP-Unified%2520Auto-Prompt%2520Segmentation%2520for%2520Multi-Modal%2520Retinal%250A%2520%2520Imaging%26entry.906535625%3DZhihao%2520Zhao%2520and%2520Yinzheng%2520Zhao%2520and%2520Junjie%2520Yang%2520and%2520Xiangtong%2520Yao%2520and%2520Quanmin%2520Liang%2520and%2520Shahrooz%2520Faghihroohi%2520and%2520Kai%2520Huang%2520and%2520Nassir%2520Navab%2520and%2520M.%2520Ali%2520Nasseri%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520foundation%2520models%252C%2520such%2520as%2520the%2520Segment%2520Anything%2520Model%250A%2528SAM%2529%252C%2520have%2520significantly%2520impacted%2520medical%2520image%2520segmentation%252C%2520especially%2520in%250Aretinal%2520imaging%252C%2520where%2520precise%2520segmentation%2520is%2520vital%2520for%2520diagnosis.%2520Despite%250Athis%2520progress%252C%2520current%2520methods%2520face%2520critical%2520challenges%253A%25201%2529%2520modality%2520ambiguity%250Ain%2520textual%2520disease%2520descriptions%252C%25202%2529%2520a%2520continued%2520reliance%2520on%2520manual%2520prompting%250Afor%2520SAM-based%2520workflows%252C%2520and%25203%2529%2520a%2520lack%2520of%2520a%2520unified%2520framework%252C%2520with%2520most%250Amethods%2520being%2520modality-%2520and%2520task-specific.%2520To%2520overcome%2520these%2520hurdles%252C%2520we%250Apropose%2520CLIP-unified%2520Auto-Prompt%2520Segmentation%2520%2528%255CCLAPS%2529%252C%2520a%2520novel%2520method%2520for%250Aunified%2520segmentation%2520across%2520diverse%2520tasks%2520and%2520modalities%2520in%2520retinal%2520imaging.%250AOur%2520approach%2520begins%2520by%2520pre-training%2520a%2520CLIP-based%2520image%2520encoder%2520on%2520a%2520large%252C%250Amulti-modal%2520retinal%2520dataset%2520to%2520handle%2520data%2520scarcity%2520and%2520distribution%2520imbalance.%250AWe%2520then%2520leverage%2520GroundingDINO%2520to%2520automatically%2520generate%2520spatial%2520bounding%2520box%250Aprompts%2520by%2520detecting%2520local%2520lesions.%2520To%2520unify%2520tasks%2520and%2520resolve%2520ambiguity%252C%2520we%250Ause%2520text%2520prompts%2520enhanced%2520with%2520a%2520unique%2520%2522modality%2520signature%2522%2520for%2520each%2520imaging%250Amodality.%2520Ultimately%252C%2520these%2520automated%2520textual%2520and%2520spatial%2520prompts%2520guide%2520SAM%2520to%250Aexecute%2520precise%2520segmentation%252C%2520creating%2520a%2520fully%2520automated%2520and%2520unified%2520pipeline.%250AExtensive%2520experiments%2520on%252012%2520diverse%2520datasets%2520across%252011%2520critical%2520segmentation%250Acategories%2520show%2520that%2520CLAPS%2520achieves%2520performance%2520on%2520par%2520with%2520specialized%2520expert%250Amodels%2520while%2520surpassing%2520existing%2520benchmarks%2520across%2520most%2520metrics%252C%2520demonstrating%250Aits%2520broad%2520generalizability%2520as%2520a%2520foundation%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLAPS%3A%20A%20CLIP-Unified%20Auto-Prompt%20Segmentation%20for%20Multi-Modal%20Retinal%0A%20%20Imaging&entry.906535625=Zhihao%20Zhao%20and%20Yinzheng%20Zhao%20and%20Junjie%20Yang%20and%20Xiangtong%20Yao%20and%20Quanmin%20Liang%20and%20Shahrooz%20Faghihroohi%20and%20Kai%20Huang%20and%20Nassir%20Navab%20and%20M.%20Ali%20Nasseri&entry.1292438233=%20%20Recent%20advancements%20in%20foundation%20models%2C%20such%20as%20the%20Segment%20Anything%20Model%0A%28SAM%29%2C%20have%20significantly%20impacted%20medical%20image%20segmentation%2C%20especially%20in%0Aretinal%20imaging%2C%20where%20precise%20segmentation%20is%20vital%20for%20diagnosis.%20Despite%0Athis%20progress%2C%20current%20methods%20face%20critical%20challenges%3A%201%29%20modality%20ambiguity%0Ain%20textual%20disease%20descriptions%2C%202%29%20a%20continued%20reliance%20on%20manual%20prompting%0Afor%20SAM-based%20workflows%2C%20and%203%29%20a%20lack%20of%20a%20unified%20framework%2C%20with%20most%0Amethods%20being%20modality-%20and%20task-specific.%20To%20overcome%20these%20hurdles%2C%20we%0Apropose%20CLIP-unified%20Auto-Prompt%20Segmentation%20%28%5CCLAPS%29%2C%20a%20novel%20method%20for%0Aunified%20segmentation%20across%20diverse%20tasks%20and%20modalities%20in%20retinal%20imaging.%0AOur%20approach%20begins%20by%20pre-training%20a%20CLIP-based%20image%20encoder%20on%20a%20large%2C%0Amulti-modal%20retinal%20dataset%20to%20handle%20data%20scarcity%20and%20distribution%20imbalance.%0AWe%20then%20leverage%20GroundingDINO%20to%20automatically%20generate%20spatial%20bounding%20box%0Aprompts%20by%20detecting%20local%20lesions.%20To%20unify%20tasks%20and%20resolve%20ambiguity%2C%20we%0Ause%20text%20prompts%20enhanced%20with%20a%20unique%20%22modality%20signature%22%20for%20each%20imaging%0Amodality.%20Ultimately%2C%20these%20automated%20textual%20and%20spatial%20prompts%20guide%20SAM%20to%0Aexecute%20precise%20segmentation%2C%20creating%20a%20fully%20automated%20and%20unified%20pipeline.%0AExtensive%20experiments%20on%2012%20diverse%20datasets%20across%2011%20critical%20segmentation%0Acategories%20show%20that%20CLAPS%20achieves%20performance%20on%20par%20with%20specialized%20expert%0Amodels%20while%20surpassing%20existing%20benchmarks%20across%20most%20metrics%2C%20demonstrating%0Aits%20broad%20generalizability%20as%20a%20foundation%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08618v1&entry.124074799=Read"},
{"title": "BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated\n  Cross-Modal Fusion", "author": "Sike Xiang and Shuang Chen and Amir Atapour-Abarghouei", "abstract": "  As multimodal large language models (MLLMs) advance, their large-scale\narchitectures pose challenges for deployment in resource-constrained\nenvironments. In the age of large models, where energy efficiency,\ncomputational scalability and environmental sustainability are paramount, the\ndevelopment of lightweight and high-performance models is critical for\nreal-world applications. As such, we propose a lightweight MLLM framework for\nend-to-end visual question answering. Our proposed approach centres on\nBreezeCLIP, a compact yet powerful vision-language encoder optimised for\nefficient multimodal understanding. With only 1.2 billion parameters overall,\nour model significantly reduces computational cost while achieving performance\ncomparable to standard-size MLLMs. Experiments conducted on multiple datasets\nfurther validate its effectiveness in balancing accuracy and efficiency. The\nmodular and extensible design enables generalisation to broader multimodal\ntasks. The proposed lightweight vision-language framework is denoted as BcQLM\n(BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising\npath toward deployable MLLMs under practical hardware constraints. The source\ncode is available at https://github.com/thico0224/BcQLM.\n", "link": "http://arxiv.org/abs/2509.08715v1", "date": "2025-09-10", "relevancy": 2.9443, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BcQLM%3A%20Efficient%20Vision-Language%20Understanding%20with%20Distilled%20Q-Gated%0A%20%20Cross-Modal%20Fusion&body=Title%3A%20BcQLM%3A%20Efficient%20Vision-Language%20Understanding%20with%20Distilled%20Q-Gated%0A%20%20Cross-Modal%20Fusion%0AAuthor%3A%20Sike%20Xiang%20and%20Shuang%20Chen%20and%20Amir%20Atapour-Abarghouei%0AAbstract%3A%20%20%20As%20multimodal%20large%20language%20models%20%28MLLMs%29%20advance%2C%20their%20large-scale%0Aarchitectures%20pose%20challenges%20for%20deployment%20in%20resource-constrained%0Aenvironments.%20In%20the%20age%20of%20large%20models%2C%20where%20energy%20efficiency%2C%0Acomputational%20scalability%20and%20environmental%20sustainability%20are%20paramount%2C%20the%0Adevelopment%20of%20lightweight%20and%20high-performance%20models%20is%20critical%20for%0Areal-world%20applications.%20As%20such%2C%20we%20propose%20a%20lightweight%20MLLM%20framework%20for%0Aend-to-end%20visual%20question%20answering.%20Our%20proposed%20approach%20centres%20on%0ABreezeCLIP%2C%20a%20compact%20yet%20powerful%20vision-language%20encoder%20optimised%20for%0Aefficient%20multimodal%20understanding.%20With%20only%201.2%20billion%20parameters%20overall%2C%0Aour%20model%20significantly%20reduces%20computational%20cost%20while%20achieving%20performance%0Acomparable%20to%20standard-size%20MLLMs.%20Experiments%20conducted%20on%20multiple%20datasets%0Afurther%20validate%20its%20effectiveness%20in%20balancing%20accuracy%20and%20efficiency.%20The%0Amodular%20and%20extensible%20design%20enables%20generalisation%20to%20broader%20multimodal%0Atasks.%20The%20proposed%20lightweight%20vision-language%20framework%20is%20denoted%20as%20BcQLM%0A%28BreezeCLIP-enhanced%20Q-Gated%20Multimodal%20Language%20Model%29.%20It%20offers%20a%20promising%0Apath%20toward%20deployable%20MLLMs%20under%20practical%20hardware%20constraints.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/thico0224/BcQLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBcQLM%253A%2520Efficient%2520Vision-Language%2520Understanding%2520with%2520Distilled%2520Q-Gated%250A%2520%2520Cross-Modal%2520Fusion%26entry.906535625%3DSike%2520Xiang%2520and%2520Shuang%2520Chen%2520and%2520Amir%2520Atapour-Abarghouei%26entry.1292438233%3D%2520%2520As%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520advance%252C%2520their%2520large-scale%250Aarchitectures%2520pose%2520challenges%2520for%2520deployment%2520in%2520resource-constrained%250Aenvironments.%2520In%2520the%2520age%2520of%2520large%2520models%252C%2520where%2520energy%2520efficiency%252C%250Acomputational%2520scalability%2520and%2520environmental%2520sustainability%2520are%2520paramount%252C%2520the%250Adevelopment%2520of%2520lightweight%2520and%2520high-performance%2520models%2520is%2520critical%2520for%250Areal-world%2520applications.%2520As%2520such%252C%2520we%2520propose%2520a%2520lightweight%2520MLLM%2520framework%2520for%250Aend-to-end%2520visual%2520question%2520answering.%2520Our%2520proposed%2520approach%2520centres%2520on%250ABreezeCLIP%252C%2520a%2520compact%2520yet%2520powerful%2520vision-language%2520encoder%2520optimised%2520for%250Aefficient%2520multimodal%2520understanding.%2520With%2520only%25201.2%2520billion%2520parameters%2520overall%252C%250Aour%2520model%2520significantly%2520reduces%2520computational%2520cost%2520while%2520achieving%2520performance%250Acomparable%2520to%2520standard-size%2520MLLMs.%2520Experiments%2520conducted%2520on%2520multiple%2520datasets%250Afurther%2520validate%2520its%2520effectiveness%2520in%2520balancing%2520accuracy%2520and%2520efficiency.%2520The%250Amodular%2520and%2520extensible%2520design%2520enables%2520generalisation%2520to%2520broader%2520multimodal%250Atasks.%2520The%2520proposed%2520lightweight%2520vision-language%2520framework%2520is%2520denoted%2520as%2520BcQLM%250A%2528BreezeCLIP-enhanced%2520Q-Gated%2520Multimodal%2520Language%2520Model%2529.%2520It%2520offers%2520a%2520promising%250Apath%2520toward%2520deployable%2520MLLMs%2520under%2520practical%2520hardware%2520constraints.%2520The%2520source%250Acode%2520is%2520available%2520at%2520https%253A//github.com/thico0224/BcQLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BcQLM%3A%20Efficient%20Vision-Language%20Understanding%20with%20Distilled%20Q-Gated%0A%20%20Cross-Modal%20Fusion&entry.906535625=Sike%20Xiang%20and%20Shuang%20Chen%20and%20Amir%20Atapour-Abarghouei&entry.1292438233=%20%20As%20multimodal%20large%20language%20models%20%28MLLMs%29%20advance%2C%20their%20large-scale%0Aarchitectures%20pose%20challenges%20for%20deployment%20in%20resource-constrained%0Aenvironments.%20In%20the%20age%20of%20large%20models%2C%20where%20energy%20efficiency%2C%0Acomputational%20scalability%20and%20environmental%20sustainability%20are%20paramount%2C%20the%0Adevelopment%20of%20lightweight%20and%20high-performance%20models%20is%20critical%20for%0Areal-world%20applications.%20As%20such%2C%20we%20propose%20a%20lightweight%20MLLM%20framework%20for%0Aend-to-end%20visual%20question%20answering.%20Our%20proposed%20approach%20centres%20on%0ABreezeCLIP%2C%20a%20compact%20yet%20powerful%20vision-language%20encoder%20optimised%20for%0Aefficient%20multimodal%20understanding.%20With%20only%201.2%20billion%20parameters%20overall%2C%0Aour%20model%20significantly%20reduces%20computational%20cost%20while%20achieving%20performance%0Acomparable%20to%20standard-size%20MLLMs.%20Experiments%20conducted%20on%20multiple%20datasets%0Afurther%20validate%20its%20effectiveness%20in%20balancing%20accuracy%20and%20efficiency.%20The%0Amodular%20and%20extensible%20design%20enables%20generalisation%20to%20broader%20multimodal%0Atasks.%20The%20proposed%20lightweight%20vision-language%20framework%20is%20denoted%20as%20BcQLM%0A%28BreezeCLIP-enhanced%20Q-Gated%20Multimodal%20Language%20Model%29.%20It%20offers%20a%20promising%0Apath%20toward%20deployable%20MLLMs%20under%20practical%20hardware%20constraints.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/thico0224/BcQLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08715v1&entry.124074799=Read"},
{"title": "Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud\n  Representation Learning", "author": "Zhongyu Chen and Rong Zhao and Xie Han and Xindong Guo and Song Wang and Zherui Qiao", "abstract": "  Existing point cloud representation learning methods primarily rely on\ndata-driven strategies to extract geometric information from large amounts of\nscattered data. However, most methods focus solely on the spatial distribution\nfeatures of point clouds while overlooking the relationship between local\ninformation and the whole structure, which limits the accuracy of point cloud\nrepresentation. Local information reflect the fine-grained variations of an\nobject, while the whole structure is determined by the interaction and\ncombination of these local features, collectively defining the object's shape.\nIn real-world, objects undergo deformation under external forces, and this\ndeformation gradually affects the whole structure through the propagation of\nforces from local regions, thereby altering the object's geometric features.\nTherefore, the appropriate introduction of physics-driven mechanism can\neffectively compensate for the limitations of data-driven methods in structural\nmodeling and significantly enhance the generalization and interpretability of\npoint cloud representations in downstream tasks such as understanding and\nrecognition. Inspired by this, we incorporate a physics-driven mechanism into\nthe data-driven method to learn fine-grained features in point clouds and model\nthe structural relationship between local regions and the whole shape.\nSpecifically, we design a dual-task encoder-decoder framework that combines the\ngeometric modeling capability of data-driven implicit fields with\nphysics-driven elastic deformation. Through the integration of physics-based\nloss functions, the framework is guided to predict localized deformation and\nexplicitly capture the correspondence between local structural changes and\nwhole shape variations. Experimental results show that our method outperforms\nexisting approaches in object classification and segmentation, demonstrating\nits effectiveness.\n", "link": "http://arxiv.org/abs/2505.13812v2", "date": "2025-09-10", "relevancy": 2.8949, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5973}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5965}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Driven%20Local-Whole%20Elastic%20Deformation%20Modeling%20for%20Point%20Cloud%0A%20%20Representation%20Learning&body=Title%3A%20Physics-Driven%20Local-Whole%20Elastic%20Deformation%20Modeling%20for%20Point%20Cloud%0A%20%20Representation%20Learning%0AAuthor%3A%20Zhongyu%20Chen%20and%20Rong%20Zhao%20and%20Xie%20Han%20and%20Xindong%20Guo%20and%20Song%20Wang%20and%20Zherui%20Qiao%0AAbstract%3A%20%20%20Existing%20point%20cloud%20representation%20learning%20methods%20primarily%20rely%20on%0Adata-driven%20strategies%20to%20extract%20geometric%20information%20from%20large%20amounts%20of%0Ascattered%20data.%20However%2C%20most%20methods%20focus%20solely%20on%20the%20spatial%20distribution%0Afeatures%20of%20point%20clouds%20while%20overlooking%20the%20relationship%20between%20local%0Ainformation%20and%20the%20whole%20structure%2C%20which%20limits%20the%20accuracy%20of%20point%20cloud%0Arepresentation.%20Local%20information%20reflect%20the%20fine-grained%20variations%20of%20an%0Aobject%2C%20while%20the%20whole%20structure%20is%20determined%20by%20the%20interaction%20and%0Acombination%20of%20these%20local%20features%2C%20collectively%20defining%20the%20object%27s%20shape.%0AIn%20real-world%2C%20objects%20undergo%20deformation%20under%20external%20forces%2C%20and%20this%0Adeformation%20gradually%20affects%20the%20whole%20structure%20through%20the%20propagation%20of%0Aforces%20from%20local%20regions%2C%20thereby%20altering%20the%20object%27s%20geometric%20features.%0ATherefore%2C%20the%20appropriate%20introduction%20of%20physics-driven%20mechanism%20can%0Aeffectively%20compensate%20for%20the%20limitations%20of%20data-driven%20methods%20in%20structural%0Amodeling%20and%20significantly%20enhance%20the%20generalization%20and%20interpretability%20of%0Apoint%20cloud%20representations%20in%20downstream%20tasks%20such%20as%20understanding%20and%0Arecognition.%20Inspired%20by%20this%2C%20we%20incorporate%20a%20physics-driven%20mechanism%20into%0Athe%20data-driven%20method%20to%20learn%20fine-grained%20features%20in%20point%20clouds%20and%20model%0Athe%20structural%20relationship%20between%20local%20regions%20and%20the%20whole%20shape.%0ASpecifically%2C%20we%20design%20a%20dual-task%20encoder-decoder%20framework%20that%20combines%20the%0Ageometric%20modeling%20capability%20of%20data-driven%20implicit%20fields%20with%0Aphysics-driven%20elastic%20deformation.%20Through%20the%20integration%20of%20physics-based%0Aloss%20functions%2C%20the%20framework%20is%20guided%20to%20predict%20localized%20deformation%20and%0Aexplicitly%20capture%20the%20correspondence%20between%20local%20structural%20changes%20and%0Awhole%20shape%20variations.%20Experimental%20results%20show%20that%20our%20method%20outperforms%0Aexisting%20approaches%20in%20object%20classification%20and%20segmentation%2C%20demonstrating%0Aits%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13812v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Driven%2520Local-Whole%2520Elastic%2520Deformation%2520Modeling%2520for%2520Point%2520Cloud%250A%2520%2520Representation%2520Learning%26entry.906535625%3DZhongyu%2520Chen%2520and%2520Rong%2520Zhao%2520and%2520Xie%2520Han%2520and%2520Xindong%2520Guo%2520and%2520Song%2520Wang%2520and%2520Zherui%2520Qiao%26entry.1292438233%3D%2520%2520Existing%2520point%2520cloud%2520representation%2520learning%2520methods%2520primarily%2520rely%2520on%250Adata-driven%2520strategies%2520to%2520extract%2520geometric%2520information%2520from%2520large%2520amounts%2520of%250Ascattered%2520data.%2520However%252C%2520most%2520methods%2520focus%2520solely%2520on%2520the%2520spatial%2520distribution%250Afeatures%2520of%2520point%2520clouds%2520while%2520overlooking%2520the%2520relationship%2520between%2520local%250Ainformation%2520and%2520the%2520whole%2520structure%252C%2520which%2520limits%2520the%2520accuracy%2520of%2520point%2520cloud%250Arepresentation.%2520Local%2520information%2520reflect%2520the%2520fine-grained%2520variations%2520of%2520an%250Aobject%252C%2520while%2520the%2520whole%2520structure%2520is%2520determined%2520by%2520the%2520interaction%2520and%250Acombination%2520of%2520these%2520local%2520features%252C%2520collectively%2520defining%2520the%2520object%2527s%2520shape.%250AIn%2520real-world%252C%2520objects%2520undergo%2520deformation%2520under%2520external%2520forces%252C%2520and%2520this%250Adeformation%2520gradually%2520affects%2520the%2520whole%2520structure%2520through%2520the%2520propagation%2520of%250Aforces%2520from%2520local%2520regions%252C%2520thereby%2520altering%2520the%2520object%2527s%2520geometric%2520features.%250ATherefore%252C%2520the%2520appropriate%2520introduction%2520of%2520physics-driven%2520mechanism%2520can%250Aeffectively%2520compensate%2520for%2520the%2520limitations%2520of%2520data-driven%2520methods%2520in%2520structural%250Amodeling%2520and%2520significantly%2520enhance%2520the%2520generalization%2520and%2520interpretability%2520of%250Apoint%2520cloud%2520representations%2520in%2520downstream%2520tasks%2520such%2520as%2520understanding%2520and%250Arecognition.%2520Inspired%2520by%2520this%252C%2520we%2520incorporate%2520a%2520physics-driven%2520mechanism%2520into%250Athe%2520data-driven%2520method%2520to%2520learn%2520fine-grained%2520features%2520in%2520point%2520clouds%2520and%2520model%250Athe%2520structural%2520relationship%2520between%2520local%2520regions%2520and%2520the%2520whole%2520shape.%250ASpecifically%252C%2520we%2520design%2520a%2520dual-task%2520encoder-decoder%2520framework%2520that%2520combines%2520the%250Ageometric%2520modeling%2520capability%2520of%2520data-driven%2520implicit%2520fields%2520with%250Aphysics-driven%2520elastic%2520deformation.%2520Through%2520the%2520integration%2520of%2520physics-based%250Aloss%2520functions%252C%2520the%2520framework%2520is%2520guided%2520to%2520predict%2520localized%2520deformation%2520and%250Aexplicitly%2520capture%2520the%2520correspondence%2520between%2520local%2520structural%2520changes%2520and%250Awhole%2520shape%2520variations.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520outperforms%250Aexisting%2520approaches%2520in%2520object%2520classification%2520and%2520segmentation%252C%2520demonstrating%250Aits%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13812v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Driven%20Local-Whole%20Elastic%20Deformation%20Modeling%20for%20Point%20Cloud%0A%20%20Representation%20Learning&entry.906535625=Zhongyu%20Chen%20and%20Rong%20Zhao%20and%20Xie%20Han%20and%20Xindong%20Guo%20and%20Song%20Wang%20and%20Zherui%20Qiao&entry.1292438233=%20%20Existing%20point%20cloud%20representation%20learning%20methods%20primarily%20rely%20on%0Adata-driven%20strategies%20to%20extract%20geometric%20information%20from%20large%20amounts%20of%0Ascattered%20data.%20However%2C%20most%20methods%20focus%20solely%20on%20the%20spatial%20distribution%0Afeatures%20of%20point%20clouds%20while%20overlooking%20the%20relationship%20between%20local%0Ainformation%20and%20the%20whole%20structure%2C%20which%20limits%20the%20accuracy%20of%20point%20cloud%0Arepresentation.%20Local%20information%20reflect%20the%20fine-grained%20variations%20of%20an%0Aobject%2C%20while%20the%20whole%20structure%20is%20determined%20by%20the%20interaction%20and%0Acombination%20of%20these%20local%20features%2C%20collectively%20defining%20the%20object%27s%20shape.%0AIn%20real-world%2C%20objects%20undergo%20deformation%20under%20external%20forces%2C%20and%20this%0Adeformation%20gradually%20affects%20the%20whole%20structure%20through%20the%20propagation%20of%0Aforces%20from%20local%20regions%2C%20thereby%20altering%20the%20object%27s%20geometric%20features.%0ATherefore%2C%20the%20appropriate%20introduction%20of%20physics-driven%20mechanism%20can%0Aeffectively%20compensate%20for%20the%20limitations%20of%20data-driven%20methods%20in%20structural%0Amodeling%20and%20significantly%20enhance%20the%20generalization%20and%20interpretability%20of%0Apoint%20cloud%20representations%20in%20downstream%20tasks%20such%20as%20understanding%20and%0Arecognition.%20Inspired%20by%20this%2C%20we%20incorporate%20a%20physics-driven%20mechanism%20into%0Athe%20data-driven%20method%20to%20learn%20fine-grained%20features%20in%20point%20clouds%20and%20model%0Athe%20structural%20relationship%20between%20local%20regions%20and%20the%20whole%20shape.%0ASpecifically%2C%20we%20design%20a%20dual-task%20encoder-decoder%20framework%20that%20combines%20the%0Ageometric%20modeling%20capability%20of%20data-driven%20implicit%20fields%20with%0Aphysics-driven%20elastic%20deformation.%20Through%20the%20integration%20of%20physics-based%0Aloss%20functions%2C%20the%20framework%20is%20guided%20to%20predict%20localized%20deformation%20and%0Aexplicitly%20capture%20the%20correspondence%20between%20local%20structural%20changes%20and%0Awhole%20shape%20variations.%20Experimental%20results%20show%20that%20our%20method%20outperforms%0Aexisting%20approaches%20in%20object%20classification%20and%20segmentation%2C%20demonstrating%0Aits%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13812v2&entry.124074799=Read"},
{"title": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and\n  LLM Spatial Reasoning", "author": "Chenghao Liu and Zhimu Zhou and Jiachen Zhang and Minghao Zhang and Songfang Huang and Huiling Duan", "abstract": "  Vision-and-Language Navigation (VLN) requires an agent to interpret natural\nlanguage instructions and navigate complex environments. Current approaches\noften adopt a \"black-box\" paradigm, where a single Large Language Model (LLM)\nmakes end-to-end decisions. However, it is plagued by critical vulnerabilities,\nincluding poor spatial reasoning, weak cross-modal grounding, and memory\noverload in long-horizon tasks. To systematically address these issues, we\npropose Memory Spatial Navigation(MSNav), a framework that fuses three modules\ninto a synergistic architecture, which transforms fragile inference into a\nrobust, integrated intelligence. MSNav integrates three modules: Memory Module,\na dynamic map memory module that tackles memory overload through selective node\npruning, enhancing long-range exploration; Spatial Module, a module for spatial\nreasoning and object relationship inference that improves endpoint recognition;\nand Decision Module, a module using LLM-based path planning to execute robust\nactions. Powering Spatial Module, we also introduce an Instruction-Object-Space\n(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),\nwhich outperforms leading commercial LLMs in object list extraction, achieving\nhigher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the\nRoom-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art\nperformance with significant improvements in Success Rate (SR) and Success\nweighted by Path Length (SPL).\n", "link": "http://arxiv.org/abs/2508.16654v3", "date": "2025-09-10", "relevancy": 2.8771, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5866}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSNav%3A%20Zero-Shot%20Vision-and-Language%20Navigation%20with%20Dynamic%20Memory%20and%0A%20%20LLM%20Spatial%20Reasoning&body=Title%3A%20MSNav%3A%20Zero-Shot%20Vision-and-Language%20Navigation%20with%20Dynamic%20Memory%20and%0A%20%20LLM%20Spatial%20Reasoning%0AAuthor%3A%20Chenghao%20Liu%20and%20Zhimu%20Zhou%20and%20Jiachen%20Zhang%20and%20Minghao%20Zhang%20and%20Songfang%20Huang%20and%20Huiling%20Duan%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20%28VLN%29%20requires%20an%20agent%20to%20interpret%20natural%0Alanguage%20instructions%20and%20navigate%20complex%20environments.%20Current%20approaches%0Aoften%20adopt%20a%20%22black-box%22%20paradigm%2C%20where%20a%20single%20Large%20Language%20Model%20%28LLM%29%0Amakes%20end-to-end%20decisions.%20However%2C%20it%20is%20plagued%20by%20critical%20vulnerabilities%2C%0Aincluding%20poor%20spatial%20reasoning%2C%20weak%20cross-modal%20grounding%2C%20and%20memory%0Aoverload%20in%20long-horizon%20tasks.%20To%20systematically%20address%20these%20issues%2C%20we%0Apropose%20Memory%20Spatial%20Navigation%28MSNav%29%2C%20a%20framework%20that%20fuses%20three%20modules%0Ainto%20a%20synergistic%20architecture%2C%20which%20transforms%20fragile%20inference%20into%20a%0Arobust%2C%20integrated%20intelligence.%20MSNav%20integrates%20three%20modules%3A%20Memory%20Module%2C%0Aa%20dynamic%20map%20memory%20module%20that%20tackles%20memory%20overload%20through%20selective%20node%0Apruning%2C%20enhancing%20long-range%20exploration%3B%20Spatial%20Module%2C%20a%20module%20for%20spatial%0Areasoning%20and%20object%20relationship%20inference%20that%20improves%20endpoint%20recognition%3B%0Aand%20Decision%20Module%2C%20a%20module%20using%20LLM-based%20path%20planning%20to%20execute%20robust%0Aactions.%20Powering%20Spatial%20Module%2C%20we%20also%20introduce%20an%20Instruction-Object-Space%0A%28I-O-S%29%20dataset%20and%20fine-tune%20the%20Qwen3-4B%20model%20into%20Qwen-Spatial%20%28Qwen-Sp%29%2C%0Awhich%20outperforms%20leading%20commercial%20LLMs%20in%20object%20list%20extraction%2C%20achieving%0Ahigher%20F1%20and%20NDCG%20scores%20on%20the%20I-O-S%20test%20set.%20Extensive%20experiments%20on%20the%0ARoom-to-Room%20%28R2R%29%20and%20REVERIE%20datasets%20demonstrate%20MSNav%27s%20state-of-the-art%0Aperformance%20with%20significant%20improvements%20in%20Success%20Rate%20%28SR%29%20and%20Success%0Aweighted%20by%20Path%20Length%20%28SPL%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16654v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSNav%253A%2520Zero-Shot%2520Vision-and-Language%2520Navigation%2520with%2520Dynamic%2520Memory%2520and%250A%2520%2520LLM%2520Spatial%2520Reasoning%26entry.906535625%3DChenghao%2520Liu%2520and%2520Zhimu%2520Zhou%2520and%2520Jiachen%2520Zhang%2520and%2520Minghao%2520Zhang%2520and%2520Songfang%2520Huang%2520and%2520Huiling%2520Duan%26entry.1292438233%3D%2520%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520requires%2520an%2520agent%2520to%2520interpret%2520natural%250Alanguage%2520instructions%2520and%2520navigate%2520complex%2520environments.%2520Current%2520approaches%250Aoften%2520adopt%2520a%2520%2522black-box%2522%2520paradigm%252C%2520where%2520a%2520single%2520Large%2520Language%2520Model%2520%2528LLM%2529%250Amakes%2520end-to-end%2520decisions.%2520However%252C%2520it%2520is%2520plagued%2520by%2520critical%2520vulnerabilities%252C%250Aincluding%2520poor%2520spatial%2520reasoning%252C%2520weak%2520cross-modal%2520grounding%252C%2520and%2520memory%250Aoverload%2520in%2520long-horizon%2520tasks.%2520To%2520systematically%2520address%2520these%2520issues%252C%2520we%250Apropose%2520Memory%2520Spatial%2520Navigation%2528MSNav%2529%252C%2520a%2520framework%2520that%2520fuses%2520three%2520modules%250Ainto%2520a%2520synergistic%2520architecture%252C%2520which%2520transforms%2520fragile%2520inference%2520into%2520a%250Arobust%252C%2520integrated%2520intelligence.%2520MSNav%2520integrates%2520three%2520modules%253A%2520Memory%2520Module%252C%250Aa%2520dynamic%2520map%2520memory%2520module%2520that%2520tackles%2520memory%2520overload%2520through%2520selective%2520node%250Apruning%252C%2520enhancing%2520long-range%2520exploration%253B%2520Spatial%2520Module%252C%2520a%2520module%2520for%2520spatial%250Areasoning%2520and%2520object%2520relationship%2520inference%2520that%2520improves%2520endpoint%2520recognition%253B%250Aand%2520Decision%2520Module%252C%2520a%2520module%2520using%2520LLM-based%2520path%2520planning%2520to%2520execute%2520robust%250Aactions.%2520Powering%2520Spatial%2520Module%252C%2520we%2520also%2520introduce%2520an%2520Instruction-Object-Space%250A%2528I-O-S%2529%2520dataset%2520and%2520fine-tune%2520the%2520Qwen3-4B%2520model%2520into%2520Qwen-Spatial%2520%2528Qwen-Sp%2529%252C%250Awhich%2520outperforms%2520leading%2520commercial%2520LLMs%2520in%2520object%2520list%2520extraction%252C%2520achieving%250Ahigher%2520F1%2520and%2520NDCG%2520scores%2520on%2520the%2520I-O-S%2520test%2520set.%2520Extensive%2520experiments%2520on%2520the%250ARoom-to-Room%2520%2528R2R%2529%2520and%2520REVERIE%2520datasets%2520demonstrate%2520MSNav%2527s%2520state-of-the-art%250Aperformance%2520with%2520significant%2520improvements%2520in%2520Success%2520Rate%2520%2528SR%2529%2520and%2520Success%250Aweighted%2520by%2520Path%2520Length%2520%2528SPL%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16654v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSNav%3A%20Zero-Shot%20Vision-and-Language%20Navigation%20with%20Dynamic%20Memory%20and%0A%20%20LLM%20Spatial%20Reasoning&entry.906535625=Chenghao%20Liu%20and%20Zhimu%20Zhou%20and%20Jiachen%20Zhang%20and%20Minghao%20Zhang%20and%20Songfang%20Huang%20and%20Huiling%20Duan&entry.1292438233=%20%20Vision-and-Language%20Navigation%20%28VLN%29%20requires%20an%20agent%20to%20interpret%20natural%0Alanguage%20instructions%20and%20navigate%20complex%20environments.%20Current%20approaches%0Aoften%20adopt%20a%20%22black-box%22%20paradigm%2C%20where%20a%20single%20Large%20Language%20Model%20%28LLM%29%0Amakes%20end-to-end%20decisions.%20However%2C%20it%20is%20plagued%20by%20critical%20vulnerabilities%2C%0Aincluding%20poor%20spatial%20reasoning%2C%20weak%20cross-modal%20grounding%2C%20and%20memory%0Aoverload%20in%20long-horizon%20tasks.%20To%20systematically%20address%20these%20issues%2C%20we%0Apropose%20Memory%20Spatial%20Navigation%28MSNav%29%2C%20a%20framework%20that%20fuses%20three%20modules%0Ainto%20a%20synergistic%20architecture%2C%20which%20transforms%20fragile%20inference%20into%20a%0Arobust%2C%20integrated%20intelligence.%20MSNav%20integrates%20three%20modules%3A%20Memory%20Module%2C%0Aa%20dynamic%20map%20memory%20module%20that%20tackles%20memory%20overload%20through%20selective%20node%0Apruning%2C%20enhancing%20long-range%20exploration%3B%20Spatial%20Module%2C%20a%20module%20for%20spatial%0Areasoning%20and%20object%20relationship%20inference%20that%20improves%20endpoint%20recognition%3B%0Aand%20Decision%20Module%2C%20a%20module%20using%20LLM-based%20path%20planning%20to%20execute%20robust%0Aactions.%20Powering%20Spatial%20Module%2C%20we%20also%20introduce%20an%20Instruction-Object-Space%0A%28I-O-S%29%20dataset%20and%20fine-tune%20the%20Qwen3-4B%20model%20into%20Qwen-Spatial%20%28Qwen-Sp%29%2C%0Awhich%20outperforms%20leading%20commercial%20LLMs%20in%20object%20list%20extraction%2C%20achieving%0Ahigher%20F1%20and%20NDCG%20scores%20on%20the%20I-O-S%20test%20set.%20Extensive%20experiments%20on%20the%0ARoom-to-Room%20%28R2R%29%20and%20REVERIE%20datasets%20demonstrate%20MSNav%27s%20state-of-the-art%0Aperformance%20with%20significant%20improvements%20in%20Success%20Rate%20%28SR%29%20and%20Success%0Aweighted%20by%20Path%20Length%20%28SPL%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16654v3&entry.124074799=Read"},
{"title": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in\n  Large Video Models", "author": "Garry Yang and Zizhe Chen and Man Hon Wong and Haoyu Lei and Yongqiang Chen and Zhenguo Li and Kaiwen Zhou and James Cheng", "abstract": "  Large Video Models (LVMs) build on the semantic capabilities of Large\nLanguage Models (LLMs) and vision modules by integrating temporal information\nto better understand dynamic video content. Despite their progress, LVMs are\nprone to hallucinations-producing inaccurate or irrelevant descriptions.\nCurrent benchmarks for video hallucination depend heavily on manual\ncategorization of video content, neglecting the perception-based processes\nthrough which humans naturally interpret videos. We introduce MESH, a benchmark\ndesigned to evaluate hallucinations in LVMs systematically. MESH uses a\nQuestion-Answering framework with binary and multi-choice formats incorporating\ntarget and trap instances. It follows a bottom-up approach, evaluating basic\nobjects, coarse-to-fine subject features, and subject-action pairs, aligning\nwith human video understanding. We demonstrate that MESH offers an effective\nand comprehensive approach for identifying hallucinations in videos. Our\nevaluations show that while LVMs excel at recognizing basic objects and\nfeatures, their susceptibility to hallucinations increases markedly when\nhandling fine details or aligning multiple actions involving various subjects\nin longer videos.\n", "link": "http://arxiv.org/abs/2509.08538v1", "date": "2025-09-10", "relevancy": 2.8335, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5613}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MESH%20--%20Understanding%20Videos%20Like%20Human%3A%20Measuring%20Hallucinations%20in%0A%20%20Large%20Video%20Models&body=Title%3A%20MESH%20--%20Understanding%20Videos%20Like%20Human%3A%20Measuring%20Hallucinations%20in%0A%20%20Large%20Video%20Models%0AAuthor%3A%20Garry%20Yang%20and%20Zizhe%20Chen%20and%20Man%20Hon%20Wong%20and%20Haoyu%20Lei%20and%20Yongqiang%20Chen%20and%20Zhenguo%20Li%20and%20Kaiwen%20Zhou%20and%20James%20Cheng%0AAbstract%3A%20%20%20Large%20Video%20Models%20%28LVMs%29%20build%20on%20the%20semantic%20capabilities%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20and%20vision%20modules%20by%20integrating%20temporal%20information%0Ato%20better%20understand%20dynamic%20video%20content.%20Despite%20their%20progress%2C%20LVMs%20are%0Aprone%20to%20hallucinations-producing%20inaccurate%20or%20irrelevant%20descriptions.%0ACurrent%20benchmarks%20for%20video%20hallucination%20depend%20heavily%20on%20manual%0Acategorization%20of%20video%20content%2C%20neglecting%20the%20perception-based%20processes%0Athrough%20which%20humans%20naturally%20interpret%20videos.%20We%20introduce%20MESH%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20hallucinations%20in%20LVMs%20systematically.%20MESH%20uses%20a%0AQuestion-Answering%20framework%20with%20binary%20and%20multi-choice%20formats%20incorporating%0Atarget%20and%20trap%20instances.%20It%20follows%20a%20bottom-up%20approach%2C%20evaluating%20basic%0Aobjects%2C%20coarse-to-fine%20subject%20features%2C%20and%20subject-action%20pairs%2C%20aligning%0Awith%20human%20video%20understanding.%20We%20demonstrate%20that%20MESH%20offers%20an%20effective%0Aand%20comprehensive%20approach%20for%20identifying%20hallucinations%20in%20videos.%20Our%0Aevaluations%20show%20that%20while%20LVMs%20excel%20at%20recognizing%20basic%20objects%20and%0Afeatures%2C%20their%20susceptibility%20to%20hallucinations%20increases%20markedly%20when%0Ahandling%20fine%20details%20or%20aligning%20multiple%20actions%20involving%20various%20subjects%0Ain%20longer%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMESH%2520--%2520Understanding%2520Videos%2520Like%2520Human%253A%2520Measuring%2520Hallucinations%2520in%250A%2520%2520Large%2520Video%2520Models%26entry.906535625%3DGarry%2520Yang%2520and%2520Zizhe%2520Chen%2520and%2520Man%2520Hon%2520Wong%2520and%2520Haoyu%2520Lei%2520and%2520Yongqiang%2520Chen%2520and%2520Zhenguo%2520Li%2520and%2520Kaiwen%2520Zhou%2520and%2520James%2520Cheng%26entry.1292438233%3D%2520%2520Large%2520Video%2520Models%2520%2528LVMs%2529%2520build%2520on%2520the%2520semantic%2520capabilities%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520and%2520vision%2520modules%2520by%2520integrating%2520temporal%2520information%250Ato%2520better%2520understand%2520dynamic%2520video%2520content.%2520Despite%2520their%2520progress%252C%2520LVMs%2520are%250Aprone%2520to%2520hallucinations-producing%2520inaccurate%2520or%2520irrelevant%2520descriptions.%250ACurrent%2520benchmarks%2520for%2520video%2520hallucination%2520depend%2520heavily%2520on%2520manual%250Acategorization%2520of%2520video%2520content%252C%2520neglecting%2520the%2520perception-based%2520processes%250Athrough%2520which%2520humans%2520naturally%2520interpret%2520videos.%2520We%2520introduce%2520MESH%252C%2520a%2520benchmark%250Adesigned%2520to%2520evaluate%2520hallucinations%2520in%2520LVMs%2520systematically.%2520MESH%2520uses%2520a%250AQuestion-Answering%2520framework%2520with%2520binary%2520and%2520multi-choice%2520formats%2520incorporating%250Atarget%2520and%2520trap%2520instances.%2520It%2520follows%2520a%2520bottom-up%2520approach%252C%2520evaluating%2520basic%250Aobjects%252C%2520coarse-to-fine%2520subject%2520features%252C%2520and%2520subject-action%2520pairs%252C%2520aligning%250Awith%2520human%2520video%2520understanding.%2520We%2520demonstrate%2520that%2520MESH%2520offers%2520an%2520effective%250Aand%2520comprehensive%2520approach%2520for%2520identifying%2520hallucinations%2520in%2520videos.%2520Our%250Aevaluations%2520show%2520that%2520while%2520LVMs%2520excel%2520at%2520recognizing%2520basic%2520objects%2520and%250Afeatures%252C%2520their%2520susceptibility%2520to%2520hallucinations%2520increases%2520markedly%2520when%250Ahandling%2520fine%2520details%2520or%2520aligning%2520multiple%2520actions%2520involving%2520various%2520subjects%250Ain%2520longer%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MESH%20--%20Understanding%20Videos%20Like%20Human%3A%20Measuring%20Hallucinations%20in%0A%20%20Large%20Video%20Models&entry.906535625=Garry%20Yang%20and%20Zizhe%20Chen%20and%20Man%20Hon%20Wong%20and%20Haoyu%20Lei%20and%20Yongqiang%20Chen%20and%20Zhenguo%20Li%20and%20Kaiwen%20Zhou%20and%20James%20Cheng&entry.1292438233=%20%20Large%20Video%20Models%20%28LVMs%29%20build%20on%20the%20semantic%20capabilities%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20and%20vision%20modules%20by%20integrating%20temporal%20information%0Ato%20better%20understand%20dynamic%20video%20content.%20Despite%20their%20progress%2C%20LVMs%20are%0Aprone%20to%20hallucinations-producing%20inaccurate%20or%20irrelevant%20descriptions.%0ACurrent%20benchmarks%20for%20video%20hallucination%20depend%20heavily%20on%20manual%0Acategorization%20of%20video%20content%2C%20neglecting%20the%20perception-based%20processes%0Athrough%20which%20humans%20naturally%20interpret%20videos.%20We%20introduce%20MESH%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20hallucinations%20in%20LVMs%20systematically.%20MESH%20uses%20a%0AQuestion-Answering%20framework%20with%20binary%20and%20multi-choice%20formats%20incorporating%0Atarget%20and%20trap%20instances.%20It%20follows%20a%20bottom-up%20approach%2C%20evaluating%20basic%0Aobjects%2C%20coarse-to-fine%20subject%20features%2C%20and%20subject-action%20pairs%2C%20aligning%0Awith%20human%20video%20understanding.%20We%20demonstrate%20that%20MESH%20offers%20an%20effective%0Aand%20comprehensive%20approach%20for%20identifying%20hallucinations%20in%20videos.%20Our%0Aevaluations%20show%20that%20while%20LVMs%20excel%20at%20recognizing%20basic%20objects%20and%0Afeatures%2C%20their%20susceptibility%20to%20hallucinations%20increases%20markedly%20when%0Ahandling%20fine%20details%20or%20aligning%20multiple%20actions%20involving%20various%20subjects%0Ain%20longer%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08538v1&entry.124074799=Read"},
{"title": "P3-SAM: Native 3D Part Segmentation", "author": "Changfeng Ma and Yang Li and Xinhao Yan and Jiachen Xu and Yunhan Yang and Chunshi Wang and Zibo Zhao and Yanwen Guo and Zhuo Chen and Chunchao Guo", "abstract": "  Segmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P3-SAM, designed to fully automate the segmentation\nof any 3D objects into components. Inspired by SAM, P3-SAM consists of a\nfeature extractor, multiple segmentation heads, and an IoU predictor, enabling\ninteractive segmentation for users. We also propose an algorithm to\nautomatically select and merge masks predicted by our model for part instance\nsegmentation. Our model is trained on a newly built dataset containing nearly\n3.7 million models with reasonable segmentation labels. Comparisons show that\nour method achieves precise segmentation results and strong robustness on any\ncomplex objects, attaining state-of-the-art performance. Our code will be\nreleased soon.\n", "link": "http://arxiv.org/abs/2509.06784v3", "date": "2025-09-10", "relevancy": 2.8154, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5637}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5637}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P3-SAM%3A%20Native%203D%20Part%20Segmentation&body=Title%3A%20P3-SAM%3A%20Native%203D%20Part%20Segmentation%0AAuthor%3A%20Changfeng%20Ma%20and%20Yang%20Li%20and%20Xinhao%20Yan%20and%20Jiachen%20Xu%20and%20Yunhan%20Yang%20and%20Chunshi%20Wang%20and%20Zibo%20Zhao%20and%20Yanwen%20Guo%20and%20Zhuo%20Chen%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20Segmenting%203D%20assets%20into%20their%20constituent%20parts%20is%20crucial%20for%20enhancing%203D%0Aunderstanding%2C%20facilitating%20model%20reuse%2C%20and%20supporting%20various%20applications%0Asuch%20as%20part%20generation.%20However%2C%20current%20methods%20face%20limitations%20such%20as%20poor%0Arobustness%20when%20dealing%20with%20complex%20objects%20and%20cannot%20fully%20automate%20the%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20a%20native%203D%20point-promptable%20part%0Asegmentation%20model%20termed%20P3-SAM%2C%20designed%20to%20fully%20automate%20the%20segmentation%0Aof%20any%203D%20objects%20into%20components.%20Inspired%20by%20SAM%2C%20P3-SAM%20consists%20of%20a%0Afeature%20extractor%2C%20multiple%20segmentation%20heads%2C%20and%20an%20IoU%20predictor%2C%20enabling%0Ainteractive%20segmentation%20for%20users.%20We%20also%20propose%20an%20algorithm%20to%0Aautomatically%20select%20and%20merge%20masks%20predicted%20by%20our%20model%20for%20part%20instance%0Asegmentation.%20Our%20model%20is%20trained%20on%20a%20newly%20built%20dataset%20containing%20nearly%0A3.7%20million%20models%20with%20reasonable%20segmentation%20labels.%20Comparisons%20show%20that%0Aour%20method%20achieves%20precise%20segmentation%20results%20and%20strong%20robustness%20on%20any%0Acomplex%20objects%2C%20attaining%20state-of-the-art%20performance.%20Our%20code%20will%20be%0Areleased%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06784v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP3-SAM%253A%2520Native%25203D%2520Part%2520Segmentation%26entry.906535625%3DChangfeng%2520Ma%2520and%2520Yang%2520Li%2520and%2520Xinhao%2520Yan%2520and%2520Jiachen%2520Xu%2520and%2520Yunhan%2520Yang%2520and%2520Chunshi%2520Wang%2520and%2520Zibo%2520Zhao%2520and%2520Yanwen%2520Guo%2520and%2520Zhuo%2520Chen%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520Segmenting%25203D%2520assets%2520into%2520their%2520constituent%2520parts%2520is%2520crucial%2520for%2520enhancing%25203D%250Aunderstanding%252C%2520facilitating%2520model%2520reuse%252C%2520and%2520supporting%2520various%2520applications%250Asuch%2520as%2520part%2520generation.%2520However%252C%2520current%2520methods%2520face%2520limitations%2520such%2520as%2520poor%250Arobustness%2520when%2520dealing%2520with%2520complex%2520objects%2520and%2520cannot%2520fully%2520automate%2520the%250Aprocess.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520native%25203D%2520point-promptable%2520part%250Asegmentation%2520model%2520termed%2520P3-SAM%252C%2520designed%2520to%2520fully%2520automate%2520the%2520segmentation%250Aof%2520any%25203D%2520objects%2520into%2520components.%2520Inspired%2520by%2520SAM%252C%2520P3-SAM%2520consists%2520of%2520a%250Afeature%2520extractor%252C%2520multiple%2520segmentation%2520heads%252C%2520and%2520an%2520IoU%2520predictor%252C%2520enabling%250Ainteractive%2520segmentation%2520for%2520users.%2520We%2520also%2520propose%2520an%2520algorithm%2520to%250Aautomatically%2520select%2520and%2520merge%2520masks%2520predicted%2520by%2520our%2520model%2520for%2520part%2520instance%250Asegmentation.%2520Our%2520model%2520is%2520trained%2520on%2520a%2520newly%2520built%2520dataset%2520containing%2520nearly%250A3.7%2520million%2520models%2520with%2520reasonable%2520segmentation%2520labels.%2520Comparisons%2520show%2520that%250Aour%2520method%2520achieves%2520precise%2520segmentation%2520results%2520and%2520strong%2520robustness%2520on%2520any%250Acomplex%2520objects%252C%2520attaining%2520state-of-the-art%2520performance.%2520Our%2520code%2520will%2520be%250Areleased%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06784v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P3-SAM%3A%20Native%203D%20Part%20Segmentation&entry.906535625=Changfeng%20Ma%20and%20Yang%20Li%20and%20Xinhao%20Yan%20and%20Jiachen%20Xu%20and%20Yunhan%20Yang%20and%20Chunshi%20Wang%20and%20Zibo%20Zhao%20and%20Yanwen%20Guo%20and%20Zhuo%20Chen%20and%20Chunchao%20Guo&entry.1292438233=%20%20Segmenting%203D%20assets%20into%20their%20constituent%20parts%20is%20crucial%20for%20enhancing%203D%0Aunderstanding%2C%20facilitating%20model%20reuse%2C%20and%20supporting%20various%20applications%0Asuch%20as%20part%20generation.%20However%2C%20current%20methods%20face%20limitations%20such%20as%20poor%0Arobustness%20when%20dealing%20with%20complex%20objects%20and%20cannot%20fully%20automate%20the%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20a%20native%203D%20point-promptable%20part%0Asegmentation%20model%20termed%20P3-SAM%2C%20designed%20to%20fully%20automate%20the%20segmentation%0Aof%20any%203D%20objects%20into%20components.%20Inspired%20by%20SAM%2C%20P3-SAM%20consists%20of%20a%0Afeature%20extractor%2C%20multiple%20segmentation%20heads%2C%20and%20an%20IoU%20predictor%2C%20enabling%0Ainteractive%20segmentation%20for%20users.%20We%20also%20propose%20an%20algorithm%20to%0Aautomatically%20select%20and%20merge%20masks%20predicted%20by%20our%20model%20for%20part%20instance%0Asegmentation.%20Our%20model%20is%20trained%20on%20a%20newly%20built%20dataset%20containing%20nearly%0A3.7%20million%20models%20with%20reasonable%20segmentation%20labels.%20Comparisons%20show%20that%0Aour%20method%20achieves%20precise%20segmentation%20results%20and%20strong%20robustness%20on%20any%0Acomplex%20objects%2C%20attaining%20state-of-the-art%20performance.%20Our%20code%20will%20be%0Areleased%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06784v3&entry.124074799=Read"},
{"title": "AdsQA: Towards Advertisement Video Understanding", "author": "Xinwei Long and Kai Tian and Peng Xu and Guoli Jia and Jingxuan Li and Sa Yang and Yihua Shao and Kaiyan Zhang and Che Jiang and Hao Xu and Yang Liu and Jiaheng Ma and Bowen Zhou", "abstract": "  Large language models (LLMs) have taken a great step towards AGI. Meanwhile,\nan increasing number of domain-specific problems such as math and programming\nboost these general-purpose models to continuously evolve via learning deeper\nexpertise. Now is thus the time further to extend the diversity of specialized\napplications for knowledgeable LLMs, though collecting high quality data with\nunexpected and informative tasks is challenging. In this paper, we propose to\nuse advertisement (ad) videos as a challenging test-bed to probe the ability of\nLLMs in perceiving beyond the objective physical content of common visual\ndomain. Our motivation is to take full advantage of the clue-rich and\ninformation-dense ad videos' traits, e.g., marketing logic, persuasive\nstrategies, and audience engagement. Our contribution is three-fold: (1) To our\nknowledge, this is the first attempt to use ad videos with well-designed tasks\nto evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark\nderived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing\n5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that\nreflects on questions, and generates answers via reward-driven optimization.\n(3) We benchmark 14 top-tier LLMs on AdsQA, and our \\texttt{ReAd-R}~achieves\nthe state-of-the-art outperforming strong competitors equipped with long-chain\nreasoning capabilities by a clear margin.\n", "link": "http://arxiv.org/abs/2509.08621v1", "date": "2025-09-10", "relevancy": 2.7941, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5704}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdsQA%3A%20Towards%20Advertisement%20Video%20Understanding&body=Title%3A%20AdsQA%3A%20Towards%20Advertisement%20Video%20Understanding%0AAuthor%3A%20Xinwei%20Long%20and%20Kai%20Tian%20and%20Peng%20Xu%20and%20Guoli%20Jia%20and%20Jingxuan%20Li%20and%20Sa%20Yang%20and%20Yihua%20Shao%20and%20Kaiyan%20Zhang%20and%20Che%20Jiang%20and%20Hao%20Xu%20and%20Yang%20Liu%20and%20Jiaheng%20Ma%20and%20Bowen%20Zhou%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20taken%20a%20great%20step%20towards%20AGI.%20Meanwhile%2C%0Aan%20increasing%20number%20of%20domain-specific%20problems%20such%20as%20math%20and%20programming%0Aboost%20these%20general-purpose%20models%20to%20continuously%20evolve%20via%20learning%20deeper%0Aexpertise.%20Now%20is%20thus%20the%20time%20further%20to%20extend%20the%20diversity%20of%20specialized%0Aapplications%20for%20knowledgeable%20LLMs%2C%20though%20collecting%20high%20quality%20data%20with%0Aunexpected%20and%20informative%20tasks%20is%20challenging.%20In%20this%20paper%2C%20we%20propose%20to%0Ause%20advertisement%20%28ad%29%20videos%20as%20a%20challenging%20test-bed%20to%20probe%20the%20ability%20of%0ALLMs%20in%20perceiving%20beyond%20the%20objective%20physical%20content%20of%20common%20visual%0Adomain.%20Our%20motivation%20is%20to%20take%20full%20advantage%20of%20the%20clue-rich%20and%0Ainformation-dense%20ad%20videos%27%20traits%2C%20e.g.%2C%20marketing%20logic%2C%20persuasive%0Astrategies%2C%20and%20audience%20engagement.%20Our%20contribution%20is%20three-fold%3A%20%281%29%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20attempt%20to%20use%20ad%20videos%20with%20well-designed%20tasks%0Ato%20evaluate%20LLMs.%20We%20contribute%20AdsQA%2C%20a%20challenging%20ad%20Video%20QA%20benchmark%0Aderived%20from%201%2C544%20ad%20videos%20with%2010%2C962%20clips%2C%20totaling%2022.7%20hours%2C%20providing%0A5%20challenging%20tasks.%20%282%29%20We%20propose%20ReAd-R%2C%20a%20Deepseek-R1%20styled%20RL%20model%20that%0Areflects%20on%20questions%2C%20and%20generates%20answers%20via%20reward-driven%20optimization.%0A%283%29%20We%20benchmark%2014%20top-tier%20LLMs%20on%20AdsQA%2C%20and%20our%20%5Ctexttt%7BReAd-R%7D~achieves%0Athe%20state-of-the-art%20outperforming%20strong%20competitors%20equipped%20with%20long-chain%0Areasoning%20capabilities%20by%20a%20clear%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdsQA%253A%2520Towards%2520Advertisement%2520Video%2520Understanding%26entry.906535625%3DXinwei%2520Long%2520and%2520Kai%2520Tian%2520and%2520Peng%2520Xu%2520and%2520Guoli%2520Jia%2520and%2520Jingxuan%2520Li%2520and%2520Sa%2520Yang%2520and%2520Yihua%2520Shao%2520and%2520Kaiyan%2520Zhang%2520and%2520Che%2520Jiang%2520and%2520Hao%2520Xu%2520and%2520Yang%2520Liu%2520and%2520Jiaheng%2520Ma%2520and%2520Bowen%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520taken%2520a%2520great%2520step%2520towards%2520AGI.%2520Meanwhile%252C%250Aan%2520increasing%2520number%2520of%2520domain-specific%2520problems%2520such%2520as%2520math%2520and%2520programming%250Aboost%2520these%2520general-purpose%2520models%2520to%2520continuously%2520evolve%2520via%2520learning%2520deeper%250Aexpertise.%2520Now%2520is%2520thus%2520the%2520time%2520further%2520to%2520extend%2520the%2520diversity%2520of%2520specialized%250Aapplications%2520for%2520knowledgeable%2520LLMs%252C%2520though%2520collecting%2520high%2520quality%2520data%2520with%250Aunexpected%2520and%2520informative%2520tasks%2520is%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%250Ause%2520advertisement%2520%2528ad%2529%2520videos%2520as%2520a%2520challenging%2520test-bed%2520to%2520probe%2520the%2520ability%2520of%250ALLMs%2520in%2520perceiving%2520beyond%2520the%2520objective%2520physical%2520content%2520of%2520common%2520visual%250Adomain.%2520Our%2520motivation%2520is%2520to%2520take%2520full%2520advantage%2520of%2520the%2520clue-rich%2520and%250Ainformation-dense%2520ad%2520videos%2527%2520traits%252C%2520e.g.%252C%2520marketing%2520logic%252C%2520persuasive%250Astrategies%252C%2520and%2520audience%2520engagement.%2520Our%2520contribution%2520is%2520three-fold%253A%2520%25281%2529%2520To%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520attempt%2520to%2520use%2520ad%2520videos%2520with%2520well-designed%2520tasks%250Ato%2520evaluate%2520LLMs.%2520We%2520contribute%2520AdsQA%252C%2520a%2520challenging%2520ad%2520Video%2520QA%2520benchmark%250Aderived%2520from%25201%252C544%2520ad%2520videos%2520with%252010%252C962%2520clips%252C%2520totaling%252022.7%2520hours%252C%2520providing%250A5%2520challenging%2520tasks.%2520%25282%2529%2520We%2520propose%2520ReAd-R%252C%2520a%2520Deepseek-R1%2520styled%2520RL%2520model%2520that%250Areflects%2520on%2520questions%252C%2520and%2520generates%2520answers%2520via%2520reward-driven%2520optimization.%250A%25283%2529%2520We%2520benchmark%252014%2520top-tier%2520LLMs%2520on%2520AdsQA%252C%2520and%2520our%2520%255Ctexttt%257BReAd-R%257D~achieves%250Athe%2520state-of-the-art%2520outperforming%2520strong%2520competitors%2520equipped%2520with%2520long-chain%250Areasoning%2520capabilities%2520by%2520a%2520clear%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdsQA%3A%20Towards%20Advertisement%20Video%20Understanding&entry.906535625=Xinwei%20Long%20and%20Kai%20Tian%20and%20Peng%20Xu%20and%20Guoli%20Jia%20and%20Jingxuan%20Li%20and%20Sa%20Yang%20and%20Yihua%20Shao%20and%20Kaiyan%20Zhang%20and%20Che%20Jiang%20and%20Hao%20Xu%20and%20Yang%20Liu%20and%20Jiaheng%20Ma%20and%20Bowen%20Zhou&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20taken%20a%20great%20step%20towards%20AGI.%20Meanwhile%2C%0Aan%20increasing%20number%20of%20domain-specific%20problems%20such%20as%20math%20and%20programming%0Aboost%20these%20general-purpose%20models%20to%20continuously%20evolve%20via%20learning%20deeper%0Aexpertise.%20Now%20is%20thus%20the%20time%20further%20to%20extend%20the%20diversity%20of%20specialized%0Aapplications%20for%20knowledgeable%20LLMs%2C%20though%20collecting%20high%20quality%20data%20with%0Aunexpected%20and%20informative%20tasks%20is%20challenging.%20In%20this%20paper%2C%20we%20propose%20to%0Ause%20advertisement%20%28ad%29%20videos%20as%20a%20challenging%20test-bed%20to%20probe%20the%20ability%20of%0ALLMs%20in%20perceiving%20beyond%20the%20objective%20physical%20content%20of%20common%20visual%0Adomain.%20Our%20motivation%20is%20to%20take%20full%20advantage%20of%20the%20clue-rich%20and%0Ainformation-dense%20ad%20videos%27%20traits%2C%20e.g.%2C%20marketing%20logic%2C%20persuasive%0Astrategies%2C%20and%20audience%20engagement.%20Our%20contribution%20is%20three-fold%3A%20%281%29%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20attempt%20to%20use%20ad%20videos%20with%20well-designed%20tasks%0Ato%20evaluate%20LLMs.%20We%20contribute%20AdsQA%2C%20a%20challenging%20ad%20Video%20QA%20benchmark%0Aderived%20from%201%2C544%20ad%20videos%20with%2010%2C962%20clips%2C%20totaling%2022.7%20hours%2C%20providing%0A5%20challenging%20tasks.%20%282%29%20We%20propose%20ReAd-R%2C%20a%20Deepseek-R1%20styled%20RL%20model%20that%0Areflects%20on%20questions%2C%20and%20generates%20answers%20via%20reward-driven%20optimization.%0A%283%29%20We%20benchmark%2014%20top-tier%20LLMs%20on%20AdsQA%2C%20and%20our%20%5Ctexttt%7BReAd-R%7D~achieves%0Athe%20state-of-the-art%20outperforming%20strong%20competitors%20equipped%20with%20long-chain%0Areasoning%20capabilities%20by%20a%20clear%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08621v1&entry.124074799=Read"},
{"title": "UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image\n  Diagnosis Augmentation", "author": "Zhihao Zhao and Yinzheng Zhao and Junjie Yang and Xiangtong Yao and Quanmin Liang and Daniel Zapp and Kai Huang and Nassir Navab and M. Ali Nasseri", "abstract": "  Significant advancements in AI-driven multimodal medical image diagnosis have\nled to substantial improvements in ophthalmic disease identification in recent\nyears. However, acquiring paired multimodal ophthalmic images remains\nprohibitively expensive. While fundus photography is simple and cost-effective,\nthe limited availability of OCT data and inherent modality imbalance hinder\nfurther progress. Conventional approaches that rely solely on fundus or textual\nfeatures often fail to capture fine-grained spatial information, as each\nimaging modality provides distinct cues about lesion predilection sites. In\nthis study, we propose a novel unpaired multimodal framework \\UOPSL that\nutilizes extensive OCT-derived spatial priors to dynamically identify\npredilection sites, enhancing fundus image-based disease recognition. Our\napproach bridges unpaired fundus and OCTs via extended disease text\ndescriptions. Initially, we employ contrastive learning on a large corpus of\nunpaired OCT and fundus images while simultaneously learning the predilection\nsites matrix in the OCT latent space. Through extensive optimization, this\nmatrix captures lesion localization patterns within the OCT feature space.\nDuring the fine-tuning or inference phase of the downstream classification task\nbased solely on fundus images, where paired OCT data is unavailable, we\neliminate OCT input and utilize the predilection sites matrix to assist in\nfundus image classification learning. Extensive experiments conducted on 9\ndiverse datasets across 28 critical categories demonstrate that our framework\noutperforms existing benchmarks.\n", "link": "http://arxiv.org/abs/2509.08624v1", "date": "2025-09-10", "relevancy": 2.7906, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5693}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5583}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UOPSL%3A%20Unpaired%20OCT%20Predilection%20Sites%20Learning%20for%20Fundus%20Image%0A%20%20Diagnosis%20Augmentation&body=Title%3A%20UOPSL%3A%20Unpaired%20OCT%20Predilection%20Sites%20Learning%20for%20Fundus%20Image%0A%20%20Diagnosis%20Augmentation%0AAuthor%3A%20Zhihao%20Zhao%20and%20Yinzheng%20Zhao%20and%20Junjie%20Yang%20and%20Xiangtong%20Yao%20and%20Quanmin%20Liang%20and%20Daniel%20Zapp%20and%20Kai%20Huang%20and%20Nassir%20Navab%20and%20M.%20Ali%20Nasseri%0AAbstract%3A%20%20%20Significant%20advancements%20in%20AI-driven%20multimodal%20medical%20image%20diagnosis%20have%0Aled%20to%20substantial%20improvements%20in%20ophthalmic%20disease%20identification%20in%20recent%0Ayears.%20However%2C%20acquiring%20paired%20multimodal%20ophthalmic%20images%20remains%0Aprohibitively%20expensive.%20While%20fundus%20photography%20is%20simple%20and%20cost-effective%2C%0Athe%20limited%20availability%20of%20OCT%20data%20and%20inherent%20modality%20imbalance%20hinder%0Afurther%20progress.%20Conventional%20approaches%20that%20rely%20solely%20on%20fundus%20or%20textual%0Afeatures%20often%20fail%20to%20capture%20fine-grained%20spatial%20information%2C%20as%20each%0Aimaging%20modality%20provides%20distinct%20cues%20about%20lesion%20predilection%20sites.%20In%0Athis%20study%2C%20we%20propose%20a%20novel%20unpaired%20multimodal%20framework%20%5CUOPSL%20that%0Autilizes%20extensive%20OCT-derived%20spatial%20priors%20to%20dynamically%20identify%0Apredilection%20sites%2C%20enhancing%20fundus%20image-based%20disease%20recognition.%20Our%0Aapproach%20bridges%20unpaired%20fundus%20and%20OCTs%20via%20extended%20disease%20text%0Adescriptions.%20Initially%2C%20we%20employ%20contrastive%20learning%20on%20a%20large%20corpus%20of%0Aunpaired%20OCT%20and%20fundus%20images%20while%20simultaneously%20learning%20the%20predilection%0Asites%20matrix%20in%20the%20OCT%20latent%20space.%20Through%20extensive%20optimization%2C%20this%0Amatrix%20captures%20lesion%20localization%20patterns%20within%20the%20OCT%20feature%20space.%0ADuring%20the%20fine-tuning%20or%20inference%20phase%20of%20the%20downstream%20classification%20task%0Abased%20solely%20on%20fundus%20images%2C%20where%20paired%20OCT%20data%20is%20unavailable%2C%20we%0Aeliminate%20OCT%20input%20and%20utilize%20the%20predilection%20sites%20matrix%20to%20assist%20in%0Afundus%20image%20classification%20learning.%20Extensive%20experiments%20conducted%20on%209%0Adiverse%20datasets%20across%2028%20critical%20categories%20demonstrate%20that%20our%20framework%0Aoutperforms%20existing%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUOPSL%253A%2520Unpaired%2520OCT%2520Predilection%2520Sites%2520Learning%2520for%2520Fundus%2520Image%250A%2520%2520Diagnosis%2520Augmentation%26entry.906535625%3DZhihao%2520Zhao%2520and%2520Yinzheng%2520Zhao%2520and%2520Junjie%2520Yang%2520and%2520Xiangtong%2520Yao%2520and%2520Quanmin%2520Liang%2520and%2520Daniel%2520Zapp%2520and%2520Kai%2520Huang%2520and%2520Nassir%2520Navab%2520and%2520M.%2520Ali%2520Nasseri%26entry.1292438233%3D%2520%2520Significant%2520advancements%2520in%2520AI-driven%2520multimodal%2520medical%2520image%2520diagnosis%2520have%250Aled%2520to%2520substantial%2520improvements%2520in%2520ophthalmic%2520disease%2520identification%2520in%2520recent%250Ayears.%2520However%252C%2520acquiring%2520paired%2520multimodal%2520ophthalmic%2520images%2520remains%250Aprohibitively%2520expensive.%2520While%2520fundus%2520photography%2520is%2520simple%2520and%2520cost-effective%252C%250Athe%2520limited%2520availability%2520of%2520OCT%2520data%2520and%2520inherent%2520modality%2520imbalance%2520hinder%250Afurther%2520progress.%2520Conventional%2520approaches%2520that%2520rely%2520solely%2520on%2520fundus%2520or%2520textual%250Afeatures%2520often%2520fail%2520to%2520capture%2520fine-grained%2520spatial%2520information%252C%2520as%2520each%250Aimaging%2520modality%2520provides%2520distinct%2520cues%2520about%2520lesion%2520predilection%2520sites.%2520In%250Athis%2520study%252C%2520we%2520propose%2520a%2520novel%2520unpaired%2520multimodal%2520framework%2520%255CUOPSL%2520that%250Autilizes%2520extensive%2520OCT-derived%2520spatial%2520priors%2520to%2520dynamically%2520identify%250Apredilection%2520sites%252C%2520enhancing%2520fundus%2520image-based%2520disease%2520recognition.%2520Our%250Aapproach%2520bridges%2520unpaired%2520fundus%2520and%2520OCTs%2520via%2520extended%2520disease%2520text%250Adescriptions.%2520Initially%252C%2520we%2520employ%2520contrastive%2520learning%2520on%2520a%2520large%2520corpus%2520of%250Aunpaired%2520OCT%2520and%2520fundus%2520images%2520while%2520simultaneously%2520learning%2520the%2520predilection%250Asites%2520matrix%2520in%2520the%2520OCT%2520latent%2520space.%2520Through%2520extensive%2520optimization%252C%2520this%250Amatrix%2520captures%2520lesion%2520localization%2520patterns%2520within%2520the%2520OCT%2520feature%2520space.%250ADuring%2520the%2520fine-tuning%2520or%2520inference%2520phase%2520of%2520the%2520downstream%2520classification%2520task%250Abased%2520solely%2520on%2520fundus%2520images%252C%2520where%2520paired%2520OCT%2520data%2520is%2520unavailable%252C%2520we%250Aeliminate%2520OCT%2520input%2520and%2520utilize%2520the%2520predilection%2520sites%2520matrix%2520to%2520assist%2520in%250Afundus%2520image%2520classification%2520learning.%2520Extensive%2520experiments%2520conducted%2520on%25209%250Adiverse%2520datasets%2520across%252028%2520critical%2520categories%2520demonstrate%2520that%2520our%2520framework%250Aoutperforms%2520existing%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UOPSL%3A%20Unpaired%20OCT%20Predilection%20Sites%20Learning%20for%20Fundus%20Image%0A%20%20Diagnosis%20Augmentation&entry.906535625=Zhihao%20Zhao%20and%20Yinzheng%20Zhao%20and%20Junjie%20Yang%20and%20Xiangtong%20Yao%20and%20Quanmin%20Liang%20and%20Daniel%20Zapp%20and%20Kai%20Huang%20and%20Nassir%20Navab%20and%20M.%20Ali%20Nasseri&entry.1292438233=%20%20Significant%20advancements%20in%20AI-driven%20multimodal%20medical%20image%20diagnosis%20have%0Aled%20to%20substantial%20improvements%20in%20ophthalmic%20disease%20identification%20in%20recent%0Ayears.%20However%2C%20acquiring%20paired%20multimodal%20ophthalmic%20images%20remains%0Aprohibitively%20expensive.%20While%20fundus%20photography%20is%20simple%20and%20cost-effective%2C%0Athe%20limited%20availability%20of%20OCT%20data%20and%20inherent%20modality%20imbalance%20hinder%0Afurther%20progress.%20Conventional%20approaches%20that%20rely%20solely%20on%20fundus%20or%20textual%0Afeatures%20often%20fail%20to%20capture%20fine-grained%20spatial%20information%2C%20as%20each%0Aimaging%20modality%20provides%20distinct%20cues%20about%20lesion%20predilection%20sites.%20In%0Athis%20study%2C%20we%20propose%20a%20novel%20unpaired%20multimodal%20framework%20%5CUOPSL%20that%0Autilizes%20extensive%20OCT-derived%20spatial%20priors%20to%20dynamically%20identify%0Apredilection%20sites%2C%20enhancing%20fundus%20image-based%20disease%20recognition.%20Our%0Aapproach%20bridges%20unpaired%20fundus%20and%20OCTs%20via%20extended%20disease%20text%0Adescriptions.%20Initially%2C%20we%20employ%20contrastive%20learning%20on%20a%20large%20corpus%20of%0Aunpaired%20OCT%20and%20fundus%20images%20while%20simultaneously%20learning%20the%20predilection%0Asites%20matrix%20in%20the%20OCT%20latent%20space.%20Through%20extensive%20optimization%2C%20this%0Amatrix%20captures%20lesion%20localization%20patterns%20within%20the%20OCT%20feature%20space.%0ADuring%20the%20fine-tuning%20or%20inference%20phase%20of%20the%20downstream%20classification%20task%0Abased%20solely%20on%20fundus%20images%2C%20where%20paired%20OCT%20data%20is%20unavailable%2C%20we%0Aeliminate%20OCT%20input%20and%20utilize%20the%20predilection%20sites%20matrix%20to%20assist%20in%0Afundus%20image%20classification%20learning.%20Extensive%20experiments%20conducted%20on%209%0Adiverse%20datasets%20across%2028%20critical%20categories%20demonstrate%20that%20our%20framework%0Aoutperforms%20existing%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08624v1&entry.124074799=Read"},
{"title": "Adapting Vision-Language Models for Neutrino Event Classification in\n  High-Energy Physics", "author": "Dikshant Sagar and Kaiwen Yu and Alejandro Yankelevich and Jianming Bian and Pierre Baldi", "abstract": "  Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics.\n", "link": "http://arxiv.org/abs/2509.08461v1", "date": "2025-09-10", "relevancy": 2.7481, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5659}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5659}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Vision-Language%20Models%20for%20Neutrino%20Event%20Classification%20in%0A%20%20High-Energy%20Physics&body=Title%3A%20Adapting%20Vision-Language%20Models%20for%20Neutrino%20Event%20Classification%20in%0A%20%20High-Energy%20Physics%0AAuthor%3A%20Dikshant%20Sagar%20and%20Kaiwen%20Yu%20and%20Alejandro%20Yankelevich%20and%20Jianming%20Bian%20and%20Pierre%20Baldi%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20their%0Aremarkable%20capacity%20to%20process%20and%20reason%20over%20structured%20and%20unstructured%20data%0Amodalities%20beyond%20natural%20language.%20In%20this%20work%2C%20we%20explore%20the%20applications%0Aof%20Vision%20Language%20Models%20%28VLMs%29%2C%20specifically%20a%20fine-tuned%20variant%20of%20LLaMa%0A3.2%2C%20to%20the%20task%20of%20identifying%20neutrino%20interactions%20in%20pixelated%20detector%0Adata%20from%20high-energy%20physics%20%28HEP%29%20experiments.%20We%20benchmark%20this%20model%0Aagainst%20a%20state-of-the-art%20convolutional%20neural%20network%20%28CNN%29%20architecture%2C%0Asimilar%20to%20those%20used%20in%20the%20NOvA%20and%20DUNE%20experiments%2C%20which%20have%20achieved%0Ahigh%20efficiency%20and%20purity%20in%20classifying%20electron%20and%20muon%20neutrino%20events.%0AOur%20evaluation%20considers%20both%20the%20classification%20performance%20and%0Ainterpretability%20of%20the%20model%20predictions.%20We%20find%20that%20VLMs%20can%20outperform%0ACNNs%2C%20while%20also%20providing%20greater%20flexibility%20in%20integrating%20auxiliary%20textual%0Aor%20semantic%20information%20and%20offering%20more%20interpretable%2C%20reasoning-based%0Apredictions.%20This%20work%20highlights%20the%20potential%20of%20VLMs%20as%20a%20general-purpose%0Abackbone%20for%20physics%20event%20classification%2C%20due%20to%20their%20high%20performance%2C%0Ainterpretability%2C%20and%20generalizability%2C%20which%20opens%20new%20avenues%20for%20integrating%0Amultimodal%20reasoning%20in%20experimental%20neutrino%20physics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Vision-Language%2520Models%2520for%2520Neutrino%2520Event%2520Classification%2520in%250A%2520%2520High-Energy%2520Physics%26entry.906535625%3DDikshant%2520Sagar%2520and%2520Kaiwen%2520Yu%2520and%2520Alejandro%2520Yankelevich%2520and%2520Jianming%2520Bian%2520and%2520Pierre%2520Baldi%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520their%250Aremarkable%2520capacity%2520to%2520process%2520and%2520reason%2520over%2520structured%2520and%2520unstructured%2520data%250Amodalities%2520beyond%2520natural%2520language.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520applications%250Aof%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520specifically%2520a%2520fine-tuned%2520variant%2520of%2520LLaMa%250A3.2%252C%2520to%2520the%2520task%2520of%2520identifying%2520neutrino%2520interactions%2520in%2520pixelated%2520detector%250Adata%2520from%2520high-energy%2520physics%2520%2528HEP%2529%2520experiments.%2520We%2520benchmark%2520this%2520model%250Aagainst%2520a%2520state-of-the-art%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520architecture%252C%250Asimilar%2520to%2520those%2520used%2520in%2520the%2520NOvA%2520and%2520DUNE%2520experiments%252C%2520which%2520have%2520achieved%250Ahigh%2520efficiency%2520and%2520purity%2520in%2520classifying%2520electron%2520and%2520muon%2520neutrino%2520events.%250AOur%2520evaluation%2520considers%2520both%2520the%2520classification%2520performance%2520and%250Ainterpretability%2520of%2520the%2520model%2520predictions.%2520We%2520find%2520that%2520VLMs%2520can%2520outperform%250ACNNs%252C%2520while%2520also%2520providing%2520greater%2520flexibility%2520in%2520integrating%2520auxiliary%2520textual%250Aor%2520semantic%2520information%2520and%2520offering%2520more%2520interpretable%252C%2520reasoning-based%250Apredictions.%2520This%2520work%2520highlights%2520the%2520potential%2520of%2520VLMs%2520as%2520a%2520general-purpose%250Abackbone%2520for%2520physics%2520event%2520classification%252C%2520due%2520to%2520their%2520high%2520performance%252C%250Ainterpretability%252C%2520and%2520generalizability%252C%2520which%2520opens%2520new%2520avenues%2520for%2520integrating%250Amultimodal%2520reasoning%2520in%2520experimental%2520neutrino%2520physics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Vision-Language%20Models%20for%20Neutrino%20Event%20Classification%20in%0A%20%20High-Energy%20Physics&entry.906535625=Dikshant%20Sagar%20and%20Kaiwen%20Yu%20and%20Alejandro%20Yankelevich%20and%20Jianming%20Bian%20and%20Pierre%20Baldi&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20their%0Aremarkable%20capacity%20to%20process%20and%20reason%20over%20structured%20and%20unstructured%20data%0Amodalities%20beyond%20natural%20language.%20In%20this%20work%2C%20we%20explore%20the%20applications%0Aof%20Vision%20Language%20Models%20%28VLMs%29%2C%20specifically%20a%20fine-tuned%20variant%20of%20LLaMa%0A3.2%2C%20to%20the%20task%20of%20identifying%20neutrino%20interactions%20in%20pixelated%20detector%0Adata%20from%20high-energy%20physics%20%28HEP%29%20experiments.%20We%20benchmark%20this%20model%0Aagainst%20a%20state-of-the-art%20convolutional%20neural%20network%20%28CNN%29%20architecture%2C%0Asimilar%20to%20those%20used%20in%20the%20NOvA%20and%20DUNE%20experiments%2C%20which%20have%20achieved%0Ahigh%20efficiency%20and%20purity%20in%20classifying%20electron%20and%20muon%20neutrino%20events.%0AOur%20evaluation%20considers%20both%20the%20classification%20performance%20and%0Ainterpretability%20of%20the%20model%20predictions.%20We%20find%20that%20VLMs%20can%20outperform%0ACNNs%2C%20while%20also%20providing%20greater%20flexibility%20in%20integrating%20auxiliary%20textual%0Aor%20semantic%20information%20and%20offering%20more%20interpretable%2C%20reasoning-based%0Apredictions.%20This%20work%20highlights%20the%20potential%20of%20VLMs%20as%20a%20general-purpose%0Abackbone%20for%20physics%20event%20classification%2C%20due%20to%20their%20high%20performance%2C%0Ainterpretability%2C%20and%20generalizability%2C%20which%20opens%20new%20avenues%20for%20integrating%0Amultimodal%20reasoning%20in%20experimental%20neutrino%20physics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08461v1&entry.124074799=Read"},
{"title": "Deep Unrolling of Sparsity-Induced RDO for 3D Point Cloud Attribute\n  Coding", "author": "Tam Thuc Do and Philip A. Chou and Gene Cheung", "abstract": "  Given encoded 3D point cloud geometry available at the decoder, we study the\nproblem of lossy attribute compression in a multi-resolution B-spline\nprojection framework. A target continuous 3D attribute function is first\nprojected onto a sequence of nested subspaces $\\mathcal{F}^{(p)}_{l_0}\n\\subseteq \\cdots \\subseteq \\mathcal{F}^{(p)}_{L}$, where\n$\\mathcal{F}^{(p)}_{l}$ is a family of functions spanned by a B-spline basis\nfunction of order $p$ at a chosen scale and its integer shifts. The projected\nlow-pass coefficients $F_l^*$ are computed by variable-complexity unrolling of\na rate-distortion (RD) optimization algorithm into a feed-forward network,\nwhere the rate term is the sparsity-promoting $\\ell_1$-norm. Thus, the\nprojection operation is end-to-end differentiable. For a chosen coarse-to-fine\npredictor, the coefficients are then adjusted to account for the prediction\nfrom a lower-resolution to a higher-resolution, which is also optimized in a\ndata-driven manner.\n", "link": "http://arxiv.org/abs/2509.08685v1", "date": "2025-09-10", "relevancy": 2.7206, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5574}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5502}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Unrolling%20of%20Sparsity-Induced%20RDO%20for%203D%20Point%20Cloud%20Attribute%0A%20%20Coding&body=Title%3A%20Deep%20Unrolling%20of%20Sparsity-Induced%20RDO%20for%203D%20Point%20Cloud%20Attribute%0A%20%20Coding%0AAuthor%3A%20Tam%20Thuc%20Do%20and%20Philip%20A.%20Chou%20and%20Gene%20Cheung%0AAbstract%3A%20%20%20Given%20encoded%203D%20point%20cloud%20geometry%20available%20at%20the%20decoder%2C%20we%20study%20the%0Aproblem%20of%20lossy%20attribute%20compression%20in%20a%20multi-resolution%20B-spline%0Aprojection%20framework.%20A%20target%20continuous%203D%20attribute%20function%20is%20first%0Aprojected%20onto%20a%20sequence%20of%20nested%20subspaces%20%24%5Cmathcal%7BF%7D%5E%7B%28p%29%7D_%7Bl_0%7D%0A%5Csubseteq%20%5Ccdots%20%5Csubseteq%20%5Cmathcal%7BF%7D%5E%7B%28p%29%7D_%7BL%7D%24%2C%20where%0A%24%5Cmathcal%7BF%7D%5E%7B%28p%29%7D_%7Bl%7D%24%20is%20a%20family%20of%20functions%20spanned%20by%20a%20B-spline%20basis%0Afunction%20of%20order%20%24p%24%20at%20a%20chosen%20scale%20and%20its%20integer%20shifts.%20The%20projected%0Alow-pass%20coefficients%20%24F_l%5E%2A%24%20are%20computed%20by%20variable-complexity%20unrolling%20of%0Aa%20rate-distortion%20%28RD%29%20optimization%20algorithm%20into%20a%20feed-forward%20network%2C%0Awhere%20the%20rate%20term%20is%20the%20sparsity-promoting%20%24%5Cell_1%24-norm.%20Thus%2C%20the%0Aprojection%20operation%20is%20end-to-end%20differentiable.%20For%20a%20chosen%20coarse-to-fine%0Apredictor%2C%20the%20coefficients%20are%20then%20adjusted%20to%20account%20for%20the%20prediction%0Afrom%20a%20lower-resolution%20to%20a%20higher-resolution%2C%20which%20is%20also%20optimized%20in%20a%0Adata-driven%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Unrolling%2520of%2520Sparsity-Induced%2520RDO%2520for%25203D%2520Point%2520Cloud%2520Attribute%250A%2520%2520Coding%26entry.906535625%3DTam%2520Thuc%2520Do%2520and%2520Philip%2520A.%2520Chou%2520and%2520Gene%2520Cheung%26entry.1292438233%3D%2520%2520Given%2520encoded%25203D%2520point%2520cloud%2520geometry%2520available%2520at%2520the%2520decoder%252C%2520we%2520study%2520the%250Aproblem%2520of%2520lossy%2520attribute%2520compression%2520in%2520a%2520multi-resolution%2520B-spline%250Aprojection%2520framework.%2520A%2520target%2520continuous%25203D%2520attribute%2520function%2520is%2520first%250Aprojected%2520onto%2520a%2520sequence%2520of%2520nested%2520subspaces%2520%2524%255Cmathcal%257BF%257D%255E%257B%2528p%2529%257D_%257Bl_0%257D%250A%255Csubseteq%2520%255Ccdots%2520%255Csubseteq%2520%255Cmathcal%257BF%257D%255E%257B%2528p%2529%257D_%257BL%257D%2524%252C%2520where%250A%2524%255Cmathcal%257BF%257D%255E%257B%2528p%2529%257D_%257Bl%257D%2524%2520is%2520a%2520family%2520of%2520functions%2520spanned%2520by%2520a%2520B-spline%2520basis%250Afunction%2520of%2520order%2520%2524p%2524%2520at%2520a%2520chosen%2520scale%2520and%2520its%2520integer%2520shifts.%2520The%2520projected%250Alow-pass%2520coefficients%2520%2524F_l%255E%252A%2524%2520are%2520computed%2520by%2520variable-complexity%2520unrolling%2520of%250Aa%2520rate-distortion%2520%2528RD%2529%2520optimization%2520algorithm%2520into%2520a%2520feed-forward%2520network%252C%250Awhere%2520the%2520rate%2520term%2520is%2520the%2520sparsity-promoting%2520%2524%255Cell_1%2524-norm.%2520Thus%252C%2520the%250Aprojection%2520operation%2520is%2520end-to-end%2520differentiable.%2520For%2520a%2520chosen%2520coarse-to-fine%250Apredictor%252C%2520the%2520coefficients%2520are%2520then%2520adjusted%2520to%2520account%2520for%2520the%2520prediction%250Afrom%2520a%2520lower-resolution%2520to%2520a%2520higher-resolution%252C%2520which%2520is%2520also%2520optimized%2520in%2520a%250Adata-driven%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Unrolling%20of%20Sparsity-Induced%20RDO%20for%203D%20Point%20Cloud%20Attribute%0A%20%20Coding&entry.906535625=Tam%20Thuc%20Do%20and%20Philip%20A.%20Chou%20and%20Gene%20Cheung&entry.1292438233=%20%20Given%20encoded%203D%20point%20cloud%20geometry%20available%20at%20the%20decoder%2C%20we%20study%20the%0Aproblem%20of%20lossy%20attribute%20compression%20in%20a%20multi-resolution%20B-spline%0Aprojection%20framework.%20A%20target%20continuous%203D%20attribute%20function%20is%20first%0Aprojected%20onto%20a%20sequence%20of%20nested%20subspaces%20%24%5Cmathcal%7BF%7D%5E%7B%28p%29%7D_%7Bl_0%7D%0A%5Csubseteq%20%5Ccdots%20%5Csubseteq%20%5Cmathcal%7BF%7D%5E%7B%28p%29%7D_%7BL%7D%24%2C%20where%0A%24%5Cmathcal%7BF%7D%5E%7B%28p%29%7D_%7Bl%7D%24%20is%20a%20family%20of%20functions%20spanned%20by%20a%20B-spline%20basis%0Afunction%20of%20order%20%24p%24%20at%20a%20chosen%20scale%20and%20its%20integer%20shifts.%20The%20projected%0Alow-pass%20coefficients%20%24F_l%5E%2A%24%20are%20computed%20by%20variable-complexity%20unrolling%20of%0Aa%20rate-distortion%20%28RD%29%20optimization%20algorithm%20into%20a%20feed-forward%20network%2C%0Awhere%20the%20rate%20term%20is%20the%20sparsity-promoting%20%24%5Cell_1%24-norm.%20Thus%2C%20the%0Aprojection%20operation%20is%20end-to-end%20differentiable.%20For%20a%20chosen%20coarse-to-fine%0Apredictor%2C%20the%20coefficients%20are%20then%20adjusted%20to%20account%20for%20the%20prediction%0Afrom%20a%20lower-resolution%20to%20a%20higher-resolution%2C%20which%20is%20also%20optimized%20in%20a%0Adata-driven%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08685v1&entry.124074799=Read"},
{"title": "Maximally Useful and Minimally Redundant: The Key to Self Supervised\n  Learning for Imbalanced Data", "author": "Yash Kumar Sharma and Vineet Nair and Wilson Naik", "abstract": "  The robustness of contrastive self-supervised learning (CSSL) for imbalanced\ndatasets is largely unexplored. CSSL usually makes use of \\emph{multi-view}\nassumptions to learn discriminatory features via similar and dissimilar data\nsamples. CSSL works well on balanced datasets, but does not generalize well for\nimbalanced datasets. In a very recent paper, as part of future work, Yann LeCun\npointed out that the self-supervised multiview framework can be extended to\ncases involving \\emph{more than two views}. Taking a cue from this insight we\npropose a theoretical justification based on the concept of \\emph{mutual\ninformation} to support the \\emph{more than two views} objective and apply it\nto the problem of dataset imbalance in self-supervised learning. The proposed\nmethod helps extract representative characteristics of the tail classes by\nsegregating between \\emph{intra} and \\emph{inter} discriminatory\ncharacteristics. We introduce a loss function that helps us to learn better\nrepresentations by filtering out extreme features. Experimental evaluation on a\nvariety of self-supervised frameworks (both contrastive and non-contrastive)\nalso prove that the \\emph{more than two view} objective works well for\nimbalanced datasets. We achieve a new state-of-the-art accuracy in\nself-supervised imbalanced dataset classification (2\\% improvement in\nCifar10-LT using Resnet-18, 5\\% improvement in Cifar100-LT using Resnet-18, 3\\%\nimprovement in Imagenet-LT (1k) using Resnet-50).\n", "link": "http://arxiv.org/abs/2509.08469v1", "date": "2025-09-10", "relevancy": 2.7112, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5487}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5439}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Maximally%20Useful%20and%20Minimally%20Redundant%3A%20The%20Key%20to%20Self%20Supervised%0A%20%20Learning%20for%20Imbalanced%20Data&body=Title%3A%20Maximally%20Useful%20and%20Minimally%20Redundant%3A%20The%20Key%20to%20Self%20Supervised%0A%20%20Learning%20for%20Imbalanced%20Data%0AAuthor%3A%20Yash%20Kumar%20Sharma%20and%20Vineet%20Nair%20and%20Wilson%20Naik%0AAbstract%3A%20%20%20The%20robustness%20of%20contrastive%20self-supervised%20learning%20%28CSSL%29%20for%20imbalanced%0Adatasets%20is%20largely%20unexplored.%20CSSL%20usually%20makes%20use%20of%20%5Cemph%7Bmulti-view%7D%0Aassumptions%20to%20learn%20discriminatory%20features%20via%20similar%20and%20dissimilar%20data%0Asamples.%20CSSL%20works%20well%20on%20balanced%20datasets%2C%20but%20does%20not%20generalize%20well%20for%0Aimbalanced%20datasets.%20In%20a%20very%20recent%20paper%2C%20as%20part%20of%20future%20work%2C%20Yann%20LeCun%0Apointed%20out%20that%20the%20self-supervised%20multiview%20framework%20can%20be%20extended%20to%0Acases%20involving%20%5Cemph%7Bmore%20than%20two%20views%7D.%20Taking%20a%20cue%20from%20this%20insight%20we%0Apropose%20a%20theoretical%20justification%20based%20on%20the%20concept%20of%20%5Cemph%7Bmutual%0Ainformation%7D%20to%20support%20the%20%5Cemph%7Bmore%20than%20two%20views%7D%20objective%20and%20apply%20it%0Ato%20the%20problem%20of%20dataset%20imbalance%20in%20self-supervised%20learning.%20The%20proposed%0Amethod%20helps%20extract%20representative%20characteristics%20of%20the%20tail%20classes%20by%0Asegregating%20between%20%5Cemph%7Bintra%7D%20and%20%5Cemph%7Binter%7D%20discriminatory%0Acharacteristics.%20We%20introduce%20a%20loss%20function%20that%20helps%20us%20to%20learn%20better%0Arepresentations%20by%20filtering%20out%20extreme%20features.%20Experimental%20evaluation%20on%20a%0Avariety%20of%20self-supervised%20frameworks%20%28both%20contrastive%20and%20non-contrastive%29%0Aalso%20prove%20that%20the%20%5Cemph%7Bmore%20than%20two%20view%7D%20objective%20works%20well%20for%0Aimbalanced%20datasets.%20We%20achieve%20a%20new%20state-of-the-art%20accuracy%20in%0Aself-supervised%20imbalanced%20dataset%20classification%20%282%5C%25%20improvement%20in%0ACifar10-LT%20using%20Resnet-18%2C%205%5C%25%20improvement%20in%20Cifar100-LT%20using%20Resnet-18%2C%203%5C%25%0Aimprovement%20in%20Imagenet-LT%20%281k%29%20using%20Resnet-50%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaximally%2520Useful%2520and%2520Minimally%2520Redundant%253A%2520The%2520Key%2520to%2520Self%2520Supervised%250A%2520%2520Learning%2520for%2520Imbalanced%2520Data%26entry.906535625%3DYash%2520Kumar%2520Sharma%2520and%2520Vineet%2520Nair%2520and%2520Wilson%2520Naik%26entry.1292438233%3D%2520%2520The%2520robustness%2520of%2520contrastive%2520self-supervised%2520learning%2520%2528CSSL%2529%2520for%2520imbalanced%250Adatasets%2520is%2520largely%2520unexplored.%2520CSSL%2520usually%2520makes%2520use%2520of%2520%255Cemph%257Bmulti-view%257D%250Aassumptions%2520to%2520learn%2520discriminatory%2520features%2520via%2520similar%2520and%2520dissimilar%2520data%250Asamples.%2520CSSL%2520works%2520well%2520on%2520balanced%2520datasets%252C%2520but%2520does%2520not%2520generalize%2520well%2520for%250Aimbalanced%2520datasets.%2520In%2520a%2520very%2520recent%2520paper%252C%2520as%2520part%2520of%2520future%2520work%252C%2520Yann%2520LeCun%250Apointed%2520out%2520that%2520the%2520self-supervised%2520multiview%2520framework%2520can%2520be%2520extended%2520to%250Acases%2520involving%2520%255Cemph%257Bmore%2520than%2520two%2520views%257D.%2520Taking%2520a%2520cue%2520from%2520this%2520insight%2520we%250Apropose%2520a%2520theoretical%2520justification%2520based%2520on%2520the%2520concept%2520of%2520%255Cemph%257Bmutual%250Ainformation%257D%2520to%2520support%2520the%2520%255Cemph%257Bmore%2520than%2520two%2520views%257D%2520objective%2520and%2520apply%2520it%250Ato%2520the%2520problem%2520of%2520dataset%2520imbalance%2520in%2520self-supervised%2520learning.%2520The%2520proposed%250Amethod%2520helps%2520extract%2520representative%2520characteristics%2520of%2520the%2520tail%2520classes%2520by%250Asegregating%2520between%2520%255Cemph%257Bintra%257D%2520and%2520%255Cemph%257Binter%257D%2520discriminatory%250Acharacteristics.%2520We%2520introduce%2520a%2520loss%2520function%2520that%2520helps%2520us%2520to%2520learn%2520better%250Arepresentations%2520by%2520filtering%2520out%2520extreme%2520features.%2520Experimental%2520evaluation%2520on%2520a%250Avariety%2520of%2520self-supervised%2520frameworks%2520%2528both%2520contrastive%2520and%2520non-contrastive%2529%250Aalso%2520prove%2520that%2520the%2520%255Cemph%257Bmore%2520than%2520two%2520view%257D%2520objective%2520works%2520well%2520for%250Aimbalanced%2520datasets.%2520We%2520achieve%2520a%2520new%2520state-of-the-art%2520accuracy%2520in%250Aself-supervised%2520imbalanced%2520dataset%2520classification%2520%25282%255C%2525%2520improvement%2520in%250ACifar10-LT%2520using%2520Resnet-18%252C%25205%255C%2525%2520improvement%2520in%2520Cifar100-LT%2520using%2520Resnet-18%252C%25203%255C%2525%250Aimprovement%2520in%2520Imagenet-LT%2520%25281k%2529%2520using%2520Resnet-50%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Maximally%20Useful%20and%20Minimally%20Redundant%3A%20The%20Key%20to%20Self%20Supervised%0A%20%20Learning%20for%20Imbalanced%20Data&entry.906535625=Yash%20Kumar%20Sharma%20and%20Vineet%20Nair%20and%20Wilson%20Naik&entry.1292438233=%20%20The%20robustness%20of%20contrastive%20self-supervised%20learning%20%28CSSL%29%20for%20imbalanced%0Adatasets%20is%20largely%20unexplored.%20CSSL%20usually%20makes%20use%20of%20%5Cemph%7Bmulti-view%7D%0Aassumptions%20to%20learn%20discriminatory%20features%20via%20similar%20and%20dissimilar%20data%0Asamples.%20CSSL%20works%20well%20on%20balanced%20datasets%2C%20but%20does%20not%20generalize%20well%20for%0Aimbalanced%20datasets.%20In%20a%20very%20recent%20paper%2C%20as%20part%20of%20future%20work%2C%20Yann%20LeCun%0Apointed%20out%20that%20the%20self-supervised%20multiview%20framework%20can%20be%20extended%20to%0Acases%20involving%20%5Cemph%7Bmore%20than%20two%20views%7D.%20Taking%20a%20cue%20from%20this%20insight%20we%0Apropose%20a%20theoretical%20justification%20based%20on%20the%20concept%20of%20%5Cemph%7Bmutual%0Ainformation%7D%20to%20support%20the%20%5Cemph%7Bmore%20than%20two%20views%7D%20objective%20and%20apply%20it%0Ato%20the%20problem%20of%20dataset%20imbalance%20in%20self-supervised%20learning.%20The%20proposed%0Amethod%20helps%20extract%20representative%20characteristics%20of%20the%20tail%20classes%20by%0Asegregating%20between%20%5Cemph%7Bintra%7D%20and%20%5Cemph%7Binter%7D%20discriminatory%0Acharacteristics.%20We%20introduce%20a%20loss%20function%20that%20helps%20us%20to%20learn%20better%0Arepresentations%20by%20filtering%20out%20extreme%20features.%20Experimental%20evaluation%20on%20a%0Avariety%20of%20self-supervised%20frameworks%20%28both%20contrastive%20and%20non-contrastive%29%0Aalso%20prove%20that%20the%20%5Cemph%7Bmore%20than%20two%20view%7D%20objective%20works%20well%20for%0Aimbalanced%20datasets.%20We%20achieve%20a%20new%20state-of-the-art%20accuracy%20in%0Aself-supervised%20imbalanced%20dataset%20classification%20%282%5C%25%20improvement%20in%0ACifar10-LT%20using%20Resnet-18%2C%205%5C%25%20improvement%20in%20Cifar100-LT%20using%20Resnet-18%2C%203%5C%25%0Aimprovement%20in%20Imagenet-LT%20%281k%29%20using%20Resnet-50%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08469v1&entry.124074799=Read"},
{"title": "CamC2V: Context-aware Controllable Video Generation", "author": "Luis Denninger and Sina Mokhtarzadeh Azar and Juergen Gall", "abstract": "  Recently, image-to-video (I2V) diffusion models have demonstrated impressive\nscene understanding and generative quality, incorporating image conditions to\nguide generation. However, these models primarily animate static images without\nextending beyond their provided context. Introducing additional constraints,\nsuch as camera trajectories, can enhance diversity but often degrade visual\nquality, limiting their applicability for tasks requiring faithful scene\nrepresentation. We propose CamC2V, a context-to-video (C2V) model that\nintegrates multiple image conditions as context with 3D constraints alongside\ncamera control to enrich both global semantics and fine-grained visual details.\nThis enables more coherent and context-aware video generation. Moreover, we\nmotivate the necessity of temporal awareness for an effective context\nrepresentation. Our comprehensive study on the RealEstate10K dataset\ndemonstrates improvements in visual quality and camera controllability. We will\npublish our code upon acceptance.\n", "link": "http://arxiv.org/abs/2504.06022v2", "date": "2025-09-10", "relevancy": 2.6555, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.7274}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6797}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CamC2V%3A%20Context-aware%20Controllable%20Video%20Generation&body=Title%3A%20CamC2V%3A%20Context-aware%20Controllable%20Video%20Generation%0AAuthor%3A%20Luis%20Denninger%20and%20Sina%20Mokhtarzadeh%20Azar%20and%20Juergen%20Gall%0AAbstract%3A%20%20%20Recently%2C%20image-to-video%20%28I2V%29%20diffusion%20models%20have%20demonstrated%20impressive%0Ascene%20understanding%20and%20generative%20quality%2C%20incorporating%20image%20conditions%20to%0Aguide%20generation.%20However%2C%20these%20models%20primarily%20animate%20static%20images%20without%0Aextending%20beyond%20their%20provided%20context.%20Introducing%20additional%20constraints%2C%0Asuch%20as%20camera%20trajectories%2C%20can%20enhance%20diversity%20but%20often%20degrade%20visual%0Aquality%2C%20limiting%20their%20applicability%20for%20tasks%20requiring%20faithful%20scene%0Arepresentation.%20We%20propose%20CamC2V%2C%20a%20context-to-video%20%28C2V%29%20model%20that%0Aintegrates%20multiple%20image%20conditions%20as%20context%20with%203D%20constraints%20alongside%0Acamera%20control%20to%20enrich%20both%20global%20semantics%20and%20fine-grained%20visual%20details.%0AThis%20enables%20more%20coherent%20and%20context-aware%20video%20generation.%20Moreover%2C%20we%0Amotivate%20the%20necessity%20of%20temporal%20awareness%20for%20an%20effective%20context%0Arepresentation.%20Our%20comprehensive%20study%20on%20the%20RealEstate10K%20dataset%0Ademonstrates%20improvements%20in%20visual%20quality%20and%20camera%20controllability.%20We%20will%0Apublish%20our%20code%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06022v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamC2V%253A%2520Context-aware%2520Controllable%2520Video%2520Generation%26entry.906535625%3DLuis%2520Denninger%2520and%2520Sina%2520Mokhtarzadeh%2520Azar%2520and%2520Juergen%2520Gall%26entry.1292438233%3D%2520%2520Recently%252C%2520image-to-video%2520%2528I2V%2529%2520diffusion%2520models%2520have%2520demonstrated%2520impressive%250Ascene%2520understanding%2520and%2520generative%2520quality%252C%2520incorporating%2520image%2520conditions%2520to%250Aguide%2520generation.%2520However%252C%2520these%2520models%2520primarily%2520animate%2520static%2520images%2520without%250Aextending%2520beyond%2520their%2520provided%2520context.%2520Introducing%2520additional%2520constraints%252C%250Asuch%2520as%2520camera%2520trajectories%252C%2520can%2520enhance%2520diversity%2520but%2520often%2520degrade%2520visual%250Aquality%252C%2520limiting%2520their%2520applicability%2520for%2520tasks%2520requiring%2520faithful%2520scene%250Arepresentation.%2520We%2520propose%2520CamC2V%252C%2520a%2520context-to-video%2520%2528C2V%2529%2520model%2520that%250Aintegrates%2520multiple%2520image%2520conditions%2520as%2520context%2520with%25203D%2520constraints%2520alongside%250Acamera%2520control%2520to%2520enrich%2520both%2520global%2520semantics%2520and%2520fine-grained%2520visual%2520details.%250AThis%2520enables%2520more%2520coherent%2520and%2520context-aware%2520video%2520generation.%2520Moreover%252C%2520we%250Amotivate%2520the%2520necessity%2520of%2520temporal%2520awareness%2520for%2520an%2520effective%2520context%250Arepresentation.%2520Our%2520comprehensive%2520study%2520on%2520the%2520RealEstate10K%2520dataset%250Ademonstrates%2520improvements%2520in%2520visual%2520quality%2520and%2520camera%2520controllability.%2520We%2520will%250Apublish%2520our%2520code%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06022v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CamC2V%3A%20Context-aware%20Controllable%20Video%20Generation&entry.906535625=Luis%20Denninger%20and%20Sina%20Mokhtarzadeh%20Azar%20and%20Juergen%20Gall&entry.1292438233=%20%20Recently%2C%20image-to-video%20%28I2V%29%20diffusion%20models%20have%20demonstrated%20impressive%0Ascene%20understanding%20and%20generative%20quality%2C%20incorporating%20image%20conditions%20to%0Aguide%20generation.%20However%2C%20these%20models%20primarily%20animate%20static%20images%20without%0Aextending%20beyond%20their%20provided%20context.%20Introducing%20additional%20constraints%2C%0Asuch%20as%20camera%20trajectories%2C%20can%20enhance%20diversity%20but%20often%20degrade%20visual%0Aquality%2C%20limiting%20their%20applicability%20for%20tasks%20requiring%20faithful%20scene%0Arepresentation.%20We%20propose%20CamC2V%2C%20a%20context-to-video%20%28C2V%29%20model%20that%0Aintegrates%20multiple%20image%20conditions%20as%20context%20with%203D%20constraints%20alongside%0Acamera%20control%20to%20enrich%20both%20global%20semantics%20and%20fine-grained%20visual%20details.%0AThis%20enables%20more%20coherent%20and%20context-aware%20video%20generation.%20Moreover%2C%20we%0Amotivate%20the%20necessity%20of%20temporal%20awareness%20for%20an%20effective%20context%0Arepresentation.%20Our%20comprehensive%20study%20on%20the%20RealEstate10K%20dataset%0Ademonstrates%20improvements%20in%20visual%20quality%20and%20camera%20controllability.%20We%20will%0Apublish%20our%20code%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06022v2&entry.124074799=Read"},
{"title": "ViewSparsifier: Killing Redundancy in Multi-View Plant Phenotyping", "author": "Robin-Nico Kampa and Fabian Deuser and Konrad Habel and Norbert Oswald", "abstract": "  Plant phenotyping involves analyzing observable characteristics of plants to\nbetter understand their growth, health, and development. In the context of deep\nlearning, this analysis is often approached through single-view classification\nor regression models. However, these methods often fail to capture all\ninformation required for accurate estimation of target phenotypic traits, which\ncan adversely affect plant health assessment and harvest readiness prediction.\nTo address this, the Growth Modelling (GroMo) Grand Challenge at ACM Multimedia\n2025 provides a multi-view dataset featuring multiple plants and two tasks:\nPlant Age Prediction and Leaf Count Estimation. Each plant is photographed from\nmultiple heights and angles, leading to significant overlap and redundancy in\nthe captured information. To learn view-invariant embeddings, we incorporate 24\nviews, referred to as the selection vector, in a random selection. Our\nViewSparsifier approach won both tasks. For further improvement and as a\ndirection for future research, we also experimented with randomized view\nselection across all five height levels (120 views total), referred to as\nselection matrices.\n", "link": "http://arxiv.org/abs/2509.08550v1", "date": "2025-09-10", "relevancy": 2.6535, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViewSparsifier%3A%20Killing%20Redundancy%20in%20Multi-View%20Plant%20Phenotyping&body=Title%3A%20ViewSparsifier%3A%20Killing%20Redundancy%20in%20Multi-View%20Plant%20Phenotyping%0AAuthor%3A%20Robin-Nico%20Kampa%20and%20Fabian%20Deuser%20and%20Konrad%20Habel%20and%20Norbert%20Oswald%0AAbstract%3A%20%20%20Plant%20phenotyping%20involves%20analyzing%20observable%20characteristics%20of%20plants%20to%0Abetter%20understand%20their%20growth%2C%20health%2C%20and%20development.%20In%20the%20context%20of%20deep%0Alearning%2C%20this%20analysis%20is%20often%20approached%20through%20single-view%20classification%0Aor%20regression%20models.%20However%2C%20these%20methods%20often%20fail%20to%20capture%20all%0Ainformation%20required%20for%20accurate%20estimation%20of%20target%20phenotypic%20traits%2C%20which%0Acan%20adversely%20affect%20plant%20health%20assessment%20and%20harvest%20readiness%20prediction.%0ATo%20address%20this%2C%20the%20Growth%20Modelling%20%28GroMo%29%20Grand%20Challenge%20at%20ACM%20Multimedia%0A2025%20provides%20a%20multi-view%20dataset%20featuring%20multiple%20plants%20and%20two%20tasks%3A%0APlant%20Age%20Prediction%20and%20Leaf%20Count%20Estimation.%20Each%20plant%20is%20photographed%20from%0Amultiple%20heights%20and%20angles%2C%20leading%20to%20significant%20overlap%20and%20redundancy%20in%0Athe%20captured%20information.%20To%20learn%20view-invariant%20embeddings%2C%20we%20incorporate%2024%0Aviews%2C%20referred%20to%20as%20the%20selection%20vector%2C%20in%20a%20random%20selection.%20Our%0AViewSparsifier%20approach%20won%20both%20tasks.%20For%20further%20improvement%20and%20as%20a%0Adirection%20for%20future%20research%2C%20we%20also%20experimented%20with%20randomized%20view%0Aselection%20across%20all%20five%20height%20levels%20%28120%20views%20total%29%2C%20referred%20to%20as%0Aselection%20matrices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewSparsifier%253A%2520Killing%2520Redundancy%2520in%2520Multi-View%2520Plant%2520Phenotyping%26entry.906535625%3DRobin-Nico%2520Kampa%2520and%2520Fabian%2520Deuser%2520and%2520Konrad%2520Habel%2520and%2520Norbert%2520Oswald%26entry.1292438233%3D%2520%2520Plant%2520phenotyping%2520involves%2520analyzing%2520observable%2520characteristics%2520of%2520plants%2520to%250Abetter%2520understand%2520their%2520growth%252C%2520health%252C%2520and%2520development.%2520In%2520the%2520context%2520of%2520deep%250Alearning%252C%2520this%2520analysis%2520is%2520often%2520approached%2520through%2520single-view%2520classification%250Aor%2520regression%2520models.%2520However%252C%2520these%2520methods%2520often%2520fail%2520to%2520capture%2520all%250Ainformation%2520required%2520for%2520accurate%2520estimation%2520of%2520target%2520phenotypic%2520traits%252C%2520which%250Acan%2520adversely%2520affect%2520plant%2520health%2520assessment%2520and%2520harvest%2520readiness%2520prediction.%250ATo%2520address%2520this%252C%2520the%2520Growth%2520Modelling%2520%2528GroMo%2529%2520Grand%2520Challenge%2520at%2520ACM%2520Multimedia%250A2025%2520provides%2520a%2520multi-view%2520dataset%2520featuring%2520multiple%2520plants%2520and%2520two%2520tasks%253A%250APlant%2520Age%2520Prediction%2520and%2520Leaf%2520Count%2520Estimation.%2520Each%2520plant%2520is%2520photographed%2520from%250Amultiple%2520heights%2520and%2520angles%252C%2520leading%2520to%2520significant%2520overlap%2520and%2520redundancy%2520in%250Athe%2520captured%2520information.%2520To%2520learn%2520view-invariant%2520embeddings%252C%2520we%2520incorporate%252024%250Aviews%252C%2520referred%2520to%2520as%2520the%2520selection%2520vector%252C%2520in%2520a%2520random%2520selection.%2520Our%250AViewSparsifier%2520approach%2520won%2520both%2520tasks.%2520For%2520further%2520improvement%2520and%2520as%2520a%250Adirection%2520for%2520future%2520research%252C%2520we%2520also%2520experimented%2520with%2520randomized%2520view%250Aselection%2520across%2520all%2520five%2520height%2520levels%2520%2528120%2520views%2520total%2529%252C%2520referred%2520to%2520as%250Aselection%2520matrices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViewSparsifier%3A%20Killing%20Redundancy%20in%20Multi-View%20Plant%20Phenotyping&entry.906535625=Robin-Nico%20Kampa%20and%20Fabian%20Deuser%20and%20Konrad%20Habel%20and%20Norbert%20Oswald&entry.1292438233=%20%20Plant%20phenotyping%20involves%20analyzing%20observable%20characteristics%20of%20plants%20to%0Abetter%20understand%20their%20growth%2C%20health%2C%20and%20development.%20In%20the%20context%20of%20deep%0Alearning%2C%20this%20analysis%20is%20often%20approached%20through%20single-view%20classification%0Aor%20regression%20models.%20However%2C%20these%20methods%20often%20fail%20to%20capture%20all%0Ainformation%20required%20for%20accurate%20estimation%20of%20target%20phenotypic%20traits%2C%20which%0Acan%20adversely%20affect%20plant%20health%20assessment%20and%20harvest%20readiness%20prediction.%0ATo%20address%20this%2C%20the%20Growth%20Modelling%20%28GroMo%29%20Grand%20Challenge%20at%20ACM%20Multimedia%0A2025%20provides%20a%20multi-view%20dataset%20featuring%20multiple%20plants%20and%20two%20tasks%3A%0APlant%20Age%20Prediction%20and%20Leaf%20Count%20Estimation.%20Each%20plant%20is%20photographed%20from%0Amultiple%20heights%20and%20angles%2C%20leading%20to%20significant%20overlap%20and%20redundancy%20in%0Athe%20captured%20information.%20To%20learn%20view-invariant%20embeddings%2C%20we%20incorporate%2024%0Aviews%2C%20referred%20to%20as%20the%20selection%20vector%2C%20in%20a%20random%20selection.%20Our%0AViewSparsifier%20approach%20won%20both%20tasks.%20For%20further%20improvement%20and%20as%20a%0Adirection%20for%20future%20research%2C%20we%20also%20experimented%20with%20randomized%20view%0Aselection%20across%20all%20five%20height%20levels%20%28120%20views%20total%29%2C%20referred%20to%20as%0Aselection%20matrices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08550v1&entry.124074799=Read"},
{"title": "CURE: Controlled Unlearning for Robust Embeddings -- Mitigating\n  Conceptual Shortcuts in Pre-Trained Language Models", "author": "Aysenur Kocak and Shuo Yang and Bardh Prenkaj and Gjergji Kasneci", "abstract": "  Pre-trained language models have achieved remarkable success across diverse\napplications but remain susceptible to spurious, concept-driven correlations\nthat impair robustness and fairness. In this work, we introduce CURE, a novel\nand lightweight framework that systematically disentangles and suppresses\nconceptual shortcuts while preserving essential content information. Our method\nfirst extracts concept-irrelevant representations via a dedicated content\nextractor reinforced by a reversal network, ensuring minimal loss of\ntask-relevant information. A subsequent controllable debiasing module employs\ncontrastive learning to finely adjust the influence of residual conceptual\ncues, enabling the model to either diminish harmful biases or harness\nbeneficial correlations as appropriate for the target task. Evaluated on the\nIMDB and Yelp datasets using three pre-trained architectures, CURE achieves an\nabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,\nwhile introducing minimal computational overhead. Our approach establishes a\nflexible, unsupervised blueprint for combating conceptual biases, paving the\nway for more reliable and fair language understanding systems.\n", "link": "http://arxiv.org/abs/2509.05230v2", "date": "2025-09-10", "relevancy": 2.622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CURE%3A%20Controlled%20Unlearning%20for%20Robust%20Embeddings%20--%20Mitigating%0A%20%20Conceptual%20Shortcuts%20in%20Pre-Trained%20Language%20Models&body=Title%3A%20CURE%3A%20Controlled%20Unlearning%20for%20Robust%20Embeddings%20--%20Mitigating%0A%20%20Conceptual%20Shortcuts%20in%20Pre-Trained%20Language%20Models%0AAuthor%3A%20Aysenur%20Kocak%20and%20Shuo%20Yang%20and%20Bardh%20Prenkaj%20and%20Gjergji%20Kasneci%0AAbstract%3A%20%20%20Pre-trained%20language%20models%20have%20achieved%20remarkable%20success%20across%20diverse%0Aapplications%20but%20remain%20susceptible%20to%20spurious%2C%20concept-driven%20correlations%0Athat%20impair%20robustness%20and%20fairness.%20In%20this%20work%2C%20we%20introduce%20CURE%2C%20a%20novel%0Aand%20lightweight%20framework%20that%20systematically%20disentangles%20and%20suppresses%0Aconceptual%20shortcuts%20while%20preserving%20essential%20content%20information.%20Our%20method%0Afirst%20extracts%20concept-irrelevant%20representations%20via%20a%20dedicated%20content%0Aextractor%20reinforced%20by%20a%20reversal%20network%2C%20ensuring%20minimal%20loss%20of%0Atask-relevant%20information.%20A%20subsequent%20controllable%20debiasing%20module%20employs%0Acontrastive%20learning%20to%20finely%20adjust%20the%20influence%20of%20residual%20conceptual%0Acues%2C%20enabling%20the%20model%20to%20either%20diminish%20harmful%20biases%20or%20harness%0Abeneficial%20correlations%20as%20appropriate%20for%20the%20target%20task.%20Evaluated%20on%20the%0AIMDB%20and%20Yelp%20datasets%20using%20three%20pre-trained%20architectures%2C%20CURE%20achieves%20an%0Aabsolute%20improvement%20of%20%2B10%20points%20in%20F1%20score%20on%20IMDB%20and%20%2B2%20points%20on%20Yelp%2C%0Awhile%20introducing%20minimal%20computational%20overhead.%20Our%20approach%20establishes%20a%0Aflexible%2C%20unsupervised%20blueprint%20for%20combating%20conceptual%20biases%2C%20paving%20the%0Away%20for%20more%20reliable%20and%20fair%20language%20understanding%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05230v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCURE%253A%2520Controlled%2520Unlearning%2520for%2520Robust%2520Embeddings%2520--%2520Mitigating%250A%2520%2520Conceptual%2520Shortcuts%2520in%2520Pre-Trained%2520Language%2520Models%26entry.906535625%3DAysenur%2520Kocak%2520and%2520Shuo%2520Yang%2520and%2520Bardh%2520Prenkaj%2520and%2520Gjergji%2520Kasneci%26entry.1292438233%3D%2520%2520Pre-trained%2520language%2520models%2520have%2520achieved%2520remarkable%2520success%2520across%2520diverse%250Aapplications%2520but%2520remain%2520susceptible%2520to%2520spurious%252C%2520concept-driven%2520correlations%250Athat%2520impair%2520robustness%2520and%2520fairness.%2520In%2520this%2520work%252C%2520we%2520introduce%2520CURE%252C%2520a%2520novel%250Aand%2520lightweight%2520framework%2520that%2520systematically%2520disentangles%2520and%2520suppresses%250Aconceptual%2520shortcuts%2520while%2520preserving%2520essential%2520content%2520information.%2520Our%2520method%250Afirst%2520extracts%2520concept-irrelevant%2520representations%2520via%2520a%2520dedicated%2520content%250Aextractor%2520reinforced%2520by%2520a%2520reversal%2520network%252C%2520ensuring%2520minimal%2520loss%2520of%250Atask-relevant%2520information.%2520A%2520subsequent%2520controllable%2520debiasing%2520module%2520employs%250Acontrastive%2520learning%2520to%2520finely%2520adjust%2520the%2520influence%2520of%2520residual%2520conceptual%250Acues%252C%2520enabling%2520the%2520model%2520to%2520either%2520diminish%2520harmful%2520biases%2520or%2520harness%250Abeneficial%2520correlations%2520as%2520appropriate%2520for%2520the%2520target%2520task.%2520Evaluated%2520on%2520the%250AIMDB%2520and%2520Yelp%2520datasets%2520using%2520three%2520pre-trained%2520architectures%252C%2520CURE%2520achieves%2520an%250Aabsolute%2520improvement%2520of%2520%252B10%2520points%2520in%2520F1%2520score%2520on%2520IMDB%2520and%2520%252B2%2520points%2520on%2520Yelp%252C%250Awhile%2520introducing%2520minimal%2520computational%2520overhead.%2520Our%2520approach%2520establishes%2520a%250Aflexible%252C%2520unsupervised%2520blueprint%2520for%2520combating%2520conceptual%2520biases%252C%2520paving%2520the%250Away%2520for%2520more%2520reliable%2520and%2520fair%2520language%2520understanding%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05230v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CURE%3A%20Controlled%20Unlearning%20for%20Robust%20Embeddings%20--%20Mitigating%0A%20%20Conceptual%20Shortcuts%20in%20Pre-Trained%20Language%20Models&entry.906535625=Aysenur%20Kocak%20and%20Shuo%20Yang%20and%20Bardh%20Prenkaj%20and%20Gjergji%20Kasneci&entry.1292438233=%20%20Pre-trained%20language%20models%20have%20achieved%20remarkable%20success%20across%20diverse%0Aapplications%20but%20remain%20susceptible%20to%20spurious%2C%20concept-driven%20correlations%0Athat%20impair%20robustness%20and%20fairness.%20In%20this%20work%2C%20we%20introduce%20CURE%2C%20a%20novel%0Aand%20lightweight%20framework%20that%20systematically%20disentangles%20and%20suppresses%0Aconceptual%20shortcuts%20while%20preserving%20essential%20content%20information.%20Our%20method%0Afirst%20extracts%20concept-irrelevant%20representations%20via%20a%20dedicated%20content%0Aextractor%20reinforced%20by%20a%20reversal%20network%2C%20ensuring%20minimal%20loss%20of%0Atask-relevant%20information.%20A%20subsequent%20controllable%20debiasing%20module%20employs%0Acontrastive%20learning%20to%20finely%20adjust%20the%20influence%20of%20residual%20conceptual%0Acues%2C%20enabling%20the%20model%20to%20either%20diminish%20harmful%20biases%20or%20harness%0Abeneficial%20correlations%20as%20appropriate%20for%20the%20target%20task.%20Evaluated%20on%20the%0AIMDB%20and%20Yelp%20datasets%20using%20three%20pre-trained%20architectures%2C%20CURE%20achieves%20an%0Aabsolute%20improvement%20of%20%2B10%20points%20in%20F1%20score%20on%20IMDB%20and%20%2B2%20points%20on%20Yelp%2C%0Awhile%20introducing%20minimal%20computational%20overhead.%20Our%20approach%20establishes%20a%0Aflexible%2C%20unsupervised%20blueprint%20for%20combating%20conceptual%20biases%2C%20paving%20the%0Away%20for%20more%20reliable%20and%20fair%20language%20understanding%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05230v2&entry.124074799=Read"},
{"title": "Improving Greenland Bed Topography Mapping with Uncertainty-Aware Graph\n  Learning on Sparse Radar Data", "author": "Bayu Adhi Tama and Homayra Alam and Mostafa Cham and Omar Faruque and Jianwu Wang and Vandana Janeja", "abstract": "  Accurate maps of Greenland's subglacial bed are essential for sea-level\nprojections, but radar observations are sparse and uneven. We introduce\nGraphTopoNet, a graph-learning framework that fuses heterogeneous supervision\nand explicitly models uncertainty via Monte Carlo dropout. Spatial graphs built\nfrom surface observables (elevation, velocity, mass balance) are augmented with\ngradient features and polynomial trends to capture both local variability and\nbroad structure. To handle data gaps, we employ a hybrid loss that combines\nconfidence-weighted radar supervision with dynamically balanced regularization.\nApplied to three Greenland subregions, GraphTopoNet outperforms interpolation,\nconvolutional, and graph-based baselines, reducing error by up to 60 percent\nwhile preserving fine-scale glacial features. The resulting bed maps improve\nreliability for operational modeling, supporting agencies engaged in climate\nforecasting and policy. More broadly, GraphTopoNet shows how graph machine\nlearning can convert sparse, uncertain geophysical observations into actionable\nknowledge at continental scale.\n", "link": "http://arxiv.org/abs/2509.08571v1", "date": "2025-09-10", "relevancy": 2.6036, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.567}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5039}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Greenland%20Bed%20Topography%20Mapping%20with%20Uncertainty-Aware%20Graph%0A%20%20Learning%20on%20Sparse%20Radar%20Data&body=Title%3A%20Improving%20Greenland%20Bed%20Topography%20Mapping%20with%20Uncertainty-Aware%20Graph%0A%20%20Learning%20on%20Sparse%20Radar%20Data%0AAuthor%3A%20Bayu%20Adhi%20Tama%20and%20Homayra%20Alam%20and%20Mostafa%20Cham%20and%20Omar%20Faruque%20and%20Jianwu%20Wang%20and%20Vandana%20Janeja%0AAbstract%3A%20%20%20Accurate%20maps%20of%20Greenland%27s%20subglacial%20bed%20are%20essential%20for%20sea-level%0Aprojections%2C%20but%20radar%20observations%20are%20sparse%20and%20uneven.%20We%20introduce%0AGraphTopoNet%2C%20a%20graph-learning%20framework%20that%20fuses%20heterogeneous%20supervision%0Aand%20explicitly%20models%20uncertainty%20via%20Monte%20Carlo%20dropout.%20Spatial%20graphs%20built%0Afrom%20surface%20observables%20%28elevation%2C%20velocity%2C%20mass%20balance%29%20are%20augmented%20with%0Agradient%20features%20and%20polynomial%20trends%20to%20capture%20both%20local%20variability%20and%0Abroad%20structure.%20To%20handle%20data%20gaps%2C%20we%20employ%20a%20hybrid%20loss%20that%20combines%0Aconfidence-weighted%20radar%20supervision%20with%20dynamically%20balanced%20regularization.%0AApplied%20to%20three%20Greenland%20subregions%2C%20GraphTopoNet%20outperforms%20interpolation%2C%0Aconvolutional%2C%20and%20graph-based%20baselines%2C%20reducing%20error%20by%20up%20to%2060%20percent%0Awhile%20preserving%20fine-scale%20glacial%20features.%20The%20resulting%20bed%20maps%20improve%0Areliability%20for%20operational%20modeling%2C%20supporting%20agencies%20engaged%20in%20climate%0Aforecasting%20and%20policy.%20More%20broadly%2C%20GraphTopoNet%20shows%20how%20graph%20machine%0Alearning%20can%20convert%20sparse%2C%20uncertain%20geophysical%20observations%20into%20actionable%0Aknowledge%20at%20continental%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Greenland%2520Bed%2520Topography%2520Mapping%2520with%2520Uncertainty-Aware%2520Graph%250A%2520%2520Learning%2520on%2520Sparse%2520Radar%2520Data%26entry.906535625%3DBayu%2520Adhi%2520Tama%2520and%2520Homayra%2520Alam%2520and%2520Mostafa%2520Cham%2520and%2520Omar%2520Faruque%2520and%2520Jianwu%2520Wang%2520and%2520Vandana%2520Janeja%26entry.1292438233%3D%2520%2520Accurate%2520maps%2520of%2520Greenland%2527s%2520subglacial%2520bed%2520are%2520essential%2520for%2520sea-level%250Aprojections%252C%2520but%2520radar%2520observations%2520are%2520sparse%2520and%2520uneven.%2520We%2520introduce%250AGraphTopoNet%252C%2520a%2520graph-learning%2520framework%2520that%2520fuses%2520heterogeneous%2520supervision%250Aand%2520explicitly%2520models%2520uncertainty%2520via%2520Monte%2520Carlo%2520dropout.%2520Spatial%2520graphs%2520built%250Afrom%2520surface%2520observables%2520%2528elevation%252C%2520velocity%252C%2520mass%2520balance%2529%2520are%2520augmented%2520with%250Agradient%2520features%2520and%2520polynomial%2520trends%2520to%2520capture%2520both%2520local%2520variability%2520and%250Abroad%2520structure.%2520To%2520handle%2520data%2520gaps%252C%2520we%2520employ%2520a%2520hybrid%2520loss%2520that%2520combines%250Aconfidence-weighted%2520radar%2520supervision%2520with%2520dynamically%2520balanced%2520regularization.%250AApplied%2520to%2520three%2520Greenland%2520subregions%252C%2520GraphTopoNet%2520outperforms%2520interpolation%252C%250Aconvolutional%252C%2520and%2520graph-based%2520baselines%252C%2520reducing%2520error%2520by%2520up%2520to%252060%2520percent%250Awhile%2520preserving%2520fine-scale%2520glacial%2520features.%2520The%2520resulting%2520bed%2520maps%2520improve%250Areliability%2520for%2520operational%2520modeling%252C%2520supporting%2520agencies%2520engaged%2520in%2520climate%250Aforecasting%2520and%2520policy.%2520More%2520broadly%252C%2520GraphTopoNet%2520shows%2520how%2520graph%2520machine%250Alearning%2520can%2520convert%2520sparse%252C%2520uncertain%2520geophysical%2520observations%2520into%2520actionable%250Aknowledge%2520at%2520continental%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Greenland%20Bed%20Topography%20Mapping%20with%20Uncertainty-Aware%20Graph%0A%20%20Learning%20on%20Sparse%20Radar%20Data&entry.906535625=Bayu%20Adhi%20Tama%20and%20Homayra%20Alam%20and%20Mostafa%20Cham%20and%20Omar%20Faruque%20and%20Jianwu%20Wang%20and%20Vandana%20Janeja&entry.1292438233=%20%20Accurate%20maps%20of%20Greenland%27s%20subglacial%20bed%20are%20essential%20for%20sea-level%0Aprojections%2C%20but%20radar%20observations%20are%20sparse%20and%20uneven.%20We%20introduce%0AGraphTopoNet%2C%20a%20graph-learning%20framework%20that%20fuses%20heterogeneous%20supervision%0Aand%20explicitly%20models%20uncertainty%20via%20Monte%20Carlo%20dropout.%20Spatial%20graphs%20built%0Afrom%20surface%20observables%20%28elevation%2C%20velocity%2C%20mass%20balance%29%20are%20augmented%20with%0Agradient%20features%20and%20polynomial%20trends%20to%20capture%20both%20local%20variability%20and%0Abroad%20structure.%20To%20handle%20data%20gaps%2C%20we%20employ%20a%20hybrid%20loss%20that%20combines%0Aconfidence-weighted%20radar%20supervision%20with%20dynamically%20balanced%20regularization.%0AApplied%20to%20three%20Greenland%20subregions%2C%20GraphTopoNet%20outperforms%20interpolation%2C%0Aconvolutional%2C%20and%20graph-based%20baselines%2C%20reducing%20error%20by%20up%20to%2060%20percent%0Awhile%20preserving%20fine-scale%20glacial%20features.%20The%20resulting%20bed%20maps%20improve%0Areliability%20for%20operational%20modeling%2C%20supporting%20agencies%20engaged%20in%20climate%0Aforecasting%20and%20policy.%20More%20broadly%2C%20GraphTopoNet%20shows%20how%20graph%20machine%0Alearning%20can%20convert%20sparse%2C%20uncertain%20geophysical%20observations%20into%20actionable%0Aknowledge%20at%20continental%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08571v1&entry.124074799=Read"},
{"title": "Narrative-Guided Reinforcement Learning: A Platform for Studying\n  Language Model Influence on Decision Making", "author": "Anup Tuladhar and Araz Minhas and Adam Kirton and Eli Kinney-Lang", "abstract": "  We present a preliminary experimental platform that explores how narrative\nelements might shape AI decision-making by combining reinforcement learning\n(RL) with language model reasoning. While AI systems can now both make\ndecisions and engage in narrative reasoning, these capabilities have mostly\nbeen studied separately. Our platform attempts to bridge this gap using a\ndual-system architecture to examine how narrative frameworks could influence\nreward-based learning. The system comprises a reinforcement learning policy\nthat suggests actions based on past experience, and a language model that\nprocesses these suggestions through different narrative frameworks to guide\ndecisions. This setup enables initial experimentation with narrative elements\nwhile maintaining consistent environment and reward structures. We implement\nthis architecture in a configurable gridworld environment, where agents receive\nboth policy suggestions and information about their surroundings. The\nplatform's modular design facilitates controlled testing of environmental\ncomplexity, narrative parameters, and the interaction between reinforcement\nlearning and narrative-based decisions. Our logging system captures basic\ndecision metrics, from RL policy values to language model reasoning to action\nselection patterns. While preliminary, this implementation provides a\nfoundation for studying how different narrative frameworks might affect\nreward-based decisions and exploring potential interactions between\noptimization-based learning and symbolic reasoning in AI systems.\n", "link": "http://arxiv.org/abs/2509.08785v1", "date": "2025-09-10", "relevancy": 2.5872, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Narrative-Guided%20Reinforcement%20Learning%3A%20A%20Platform%20for%20Studying%0A%20%20Language%20Model%20Influence%20on%20Decision%20Making&body=Title%3A%20Narrative-Guided%20Reinforcement%20Learning%3A%20A%20Platform%20for%20Studying%0A%20%20Language%20Model%20Influence%20on%20Decision%20Making%0AAuthor%3A%20Anup%20Tuladhar%20and%20Araz%20Minhas%20and%20Adam%20Kirton%20and%20Eli%20Kinney-Lang%0AAbstract%3A%20%20%20We%20present%20a%20preliminary%20experimental%20platform%20that%20explores%20how%20narrative%0Aelements%20might%20shape%20AI%20decision-making%20by%20combining%20reinforcement%20learning%0A%28RL%29%20with%20language%20model%20reasoning.%20While%20AI%20systems%20can%20now%20both%20make%0Adecisions%20and%20engage%20in%20narrative%20reasoning%2C%20these%20capabilities%20have%20mostly%0Abeen%20studied%20separately.%20Our%20platform%20attempts%20to%20bridge%20this%20gap%20using%20a%0Adual-system%20architecture%20to%20examine%20how%20narrative%20frameworks%20could%20influence%0Areward-based%20learning.%20The%20system%20comprises%20a%20reinforcement%20learning%20policy%0Athat%20suggests%20actions%20based%20on%20past%20experience%2C%20and%20a%20language%20model%20that%0Aprocesses%20these%20suggestions%20through%20different%20narrative%20frameworks%20to%20guide%0Adecisions.%20This%20setup%20enables%20initial%20experimentation%20with%20narrative%20elements%0Awhile%20maintaining%20consistent%20environment%20and%20reward%20structures.%20We%20implement%0Athis%20architecture%20in%20a%20configurable%20gridworld%20environment%2C%20where%20agents%20receive%0Aboth%20policy%20suggestions%20and%20information%20about%20their%20surroundings.%20The%0Aplatform%27s%20modular%20design%20facilitates%20controlled%20testing%20of%20environmental%0Acomplexity%2C%20narrative%20parameters%2C%20and%20the%20interaction%20between%20reinforcement%0Alearning%20and%20narrative-based%20decisions.%20Our%20logging%20system%20captures%20basic%0Adecision%20metrics%2C%20from%20RL%20policy%20values%20to%20language%20model%20reasoning%20to%20action%0Aselection%20patterns.%20While%20preliminary%2C%20this%20implementation%20provides%20a%0Afoundation%20for%20studying%20how%20different%20narrative%20frameworks%20might%20affect%0Areward-based%20decisions%20and%20exploring%20potential%20interactions%20between%0Aoptimization-based%20learning%20and%20symbolic%20reasoning%20in%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNarrative-Guided%2520Reinforcement%2520Learning%253A%2520A%2520Platform%2520for%2520Studying%250A%2520%2520Language%2520Model%2520Influence%2520on%2520Decision%2520Making%26entry.906535625%3DAnup%2520Tuladhar%2520and%2520Araz%2520Minhas%2520and%2520Adam%2520Kirton%2520and%2520Eli%2520Kinney-Lang%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520preliminary%2520experimental%2520platform%2520that%2520explores%2520how%2520narrative%250Aelements%2520might%2520shape%2520AI%2520decision-making%2520by%2520combining%2520reinforcement%2520learning%250A%2528RL%2529%2520with%2520language%2520model%2520reasoning.%2520While%2520AI%2520systems%2520can%2520now%2520both%2520make%250Adecisions%2520and%2520engage%2520in%2520narrative%2520reasoning%252C%2520these%2520capabilities%2520have%2520mostly%250Abeen%2520studied%2520separately.%2520Our%2520platform%2520attempts%2520to%2520bridge%2520this%2520gap%2520using%2520a%250Adual-system%2520architecture%2520to%2520examine%2520how%2520narrative%2520frameworks%2520could%2520influence%250Areward-based%2520learning.%2520The%2520system%2520comprises%2520a%2520reinforcement%2520learning%2520policy%250Athat%2520suggests%2520actions%2520based%2520on%2520past%2520experience%252C%2520and%2520a%2520language%2520model%2520that%250Aprocesses%2520these%2520suggestions%2520through%2520different%2520narrative%2520frameworks%2520to%2520guide%250Adecisions.%2520This%2520setup%2520enables%2520initial%2520experimentation%2520with%2520narrative%2520elements%250Awhile%2520maintaining%2520consistent%2520environment%2520and%2520reward%2520structures.%2520We%2520implement%250Athis%2520architecture%2520in%2520a%2520configurable%2520gridworld%2520environment%252C%2520where%2520agents%2520receive%250Aboth%2520policy%2520suggestions%2520and%2520information%2520about%2520their%2520surroundings.%2520The%250Aplatform%2527s%2520modular%2520design%2520facilitates%2520controlled%2520testing%2520of%2520environmental%250Acomplexity%252C%2520narrative%2520parameters%252C%2520and%2520the%2520interaction%2520between%2520reinforcement%250Alearning%2520and%2520narrative-based%2520decisions.%2520Our%2520logging%2520system%2520captures%2520basic%250Adecision%2520metrics%252C%2520from%2520RL%2520policy%2520values%2520to%2520language%2520model%2520reasoning%2520to%2520action%250Aselection%2520patterns.%2520While%2520preliminary%252C%2520this%2520implementation%2520provides%2520a%250Afoundation%2520for%2520studying%2520how%2520different%2520narrative%2520frameworks%2520might%2520affect%250Areward-based%2520decisions%2520and%2520exploring%2520potential%2520interactions%2520between%250Aoptimization-based%2520learning%2520and%2520symbolic%2520reasoning%2520in%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Narrative-Guided%20Reinforcement%20Learning%3A%20A%20Platform%20for%20Studying%0A%20%20Language%20Model%20Influence%20on%20Decision%20Making&entry.906535625=Anup%20Tuladhar%20and%20Araz%20Minhas%20and%20Adam%20Kirton%20and%20Eli%20Kinney-Lang&entry.1292438233=%20%20We%20present%20a%20preliminary%20experimental%20platform%20that%20explores%20how%20narrative%0Aelements%20might%20shape%20AI%20decision-making%20by%20combining%20reinforcement%20learning%0A%28RL%29%20with%20language%20model%20reasoning.%20While%20AI%20systems%20can%20now%20both%20make%0Adecisions%20and%20engage%20in%20narrative%20reasoning%2C%20these%20capabilities%20have%20mostly%0Abeen%20studied%20separately.%20Our%20platform%20attempts%20to%20bridge%20this%20gap%20using%20a%0Adual-system%20architecture%20to%20examine%20how%20narrative%20frameworks%20could%20influence%0Areward-based%20learning.%20The%20system%20comprises%20a%20reinforcement%20learning%20policy%0Athat%20suggests%20actions%20based%20on%20past%20experience%2C%20and%20a%20language%20model%20that%0Aprocesses%20these%20suggestions%20through%20different%20narrative%20frameworks%20to%20guide%0Adecisions.%20This%20setup%20enables%20initial%20experimentation%20with%20narrative%20elements%0Awhile%20maintaining%20consistent%20environment%20and%20reward%20structures.%20We%20implement%0Athis%20architecture%20in%20a%20configurable%20gridworld%20environment%2C%20where%20agents%20receive%0Aboth%20policy%20suggestions%20and%20information%20about%20their%20surroundings.%20The%0Aplatform%27s%20modular%20design%20facilitates%20controlled%20testing%20of%20environmental%0Acomplexity%2C%20narrative%20parameters%2C%20and%20the%20interaction%20between%20reinforcement%0Alearning%20and%20narrative-based%20decisions.%20Our%20logging%20system%20captures%20basic%0Adecision%20metrics%2C%20from%20RL%20policy%20values%20to%20language%20model%20reasoning%20to%20action%0Aselection%20patterns.%20While%20preliminary%2C%20this%20implementation%20provides%20a%0Afoundation%20for%20studying%20how%20different%20narrative%20frameworks%20might%20affect%0Areward-based%20decisions%20and%20exploring%20potential%20interactions%20between%0Aoptimization-based%20learning%20and%20symbolic%20reasoning%20in%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08785v1&entry.124074799=Read"},
{"title": "From Channel Bias to Feature Redundancy: Uncovering the \"Less is More\"\n  Principle in Few-Shot Learning", "author": "Ji Zhang and Xu Luo and Lianli Gao and Difan Zou and Hengtao Shen and Jingkuan Song", "abstract": "  Deep neural networks often fail to adapt representations to novel tasks under\ndistribution shifts, especially when only a few examples are available. This\npaper identifies a core obstacle behind this failure: channel bias, where\nnetworks develop a rigid emphasis on feature dimensions that were\ndiscriminative for the source task, but this emphasis is misaligned and fails\nto adapt to the distinct needs of a novel task. This bias leads to a striking\nand detrimental consequence: feature redundancy. We demonstrate that for\nfew-shot tasks, classification accuracy is significantly improved by using as\nfew as 1-5% of the most discriminative feature dimensions, revealing that the\nvast majority are actively harmful. Our theoretical analysis confirms that this\nredundancy originates from confounding feature dimensions-those with high\nintra-class variance but low inter-class separability-which are especially\nproblematic in low-data regimes. This \"less is more\" phenomenon is a defining\ncharacteristic of the few-shot setting, diminishing as more samples become\navailable. To address this, we propose a simple yet effective soft-masking\nmethod, Augmented Feature Importance Adjustment (AFIA), which estimates feature\nimportance from augmented data to mitigate the issue. By establishing the\ncohesive link from channel bias to its consequence of extreme feature\nredundancy, this work provides a foundational principle for few-shot\nrepresentation transfer and a practical method for developing more robust\nfew-shot learning algorithms.\n", "link": "http://arxiv.org/abs/2310.03843v2", "date": "2025-09-10", "relevancy": 2.578, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5364}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5099}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Channel%20Bias%20to%20Feature%20Redundancy%3A%20Uncovering%20the%20%22Less%20is%20More%22%0A%20%20Principle%20in%20Few-Shot%20Learning&body=Title%3A%20From%20Channel%20Bias%20to%20Feature%20Redundancy%3A%20Uncovering%20the%20%22Less%20is%20More%22%0A%20%20Principle%20in%20Few-Shot%20Learning%0AAuthor%3A%20Ji%20Zhang%20and%20Xu%20Luo%20and%20Lianli%20Gao%20and%20Difan%20Zou%20and%20Hengtao%20Shen%20and%20Jingkuan%20Song%0AAbstract%3A%20%20%20Deep%20neural%20networks%20often%20fail%20to%20adapt%20representations%20to%20novel%20tasks%20under%0Adistribution%20shifts%2C%20especially%20when%20only%20a%20few%20examples%20are%20available.%20This%0Apaper%20identifies%20a%20core%20obstacle%20behind%20this%20failure%3A%20channel%20bias%2C%20where%0Anetworks%20develop%20a%20rigid%20emphasis%20on%20feature%20dimensions%20that%20were%0Adiscriminative%20for%20the%20source%20task%2C%20but%20this%20emphasis%20is%20misaligned%20and%20fails%0Ato%20adapt%20to%20the%20distinct%20needs%20of%20a%20novel%20task.%20This%20bias%20leads%20to%20a%20striking%0Aand%20detrimental%20consequence%3A%20feature%20redundancy.%20We%20demonstrate%20that%20for%0Afew-shot%20tasks%2C%20classification%20accuracy%20is%20significantly%20improved%20by%20using%20as%0Afew%20as%201-5%25%20of%20the%20most%20discriminative%20feature%20dimensions%2C%20revealing%20that%20the%0Avast%20majority%20are%20actively%20harmful.%20Our%20theoretical%20analysis%20confirms%20that%20this%0Aredundancy%20originates%20from%20confounding%20feature%20dimensions-those%20with%20high%0Aintra-class%20variance%20but%20low%20inter-class%20separability-which%20are%20especially%0Aproblematic%20in%20low-data%20regimes.%20This%20%22less%20is%20more%22%20phenomenon%20is%20a%20defining%0Acharacteristic%20of%20the%20few-shot%20setting%2C%20diminishing%20as%20more%20samples%20become%0Aavailable.%20To%20address%20this%2C%20we%20propose%20a%20simple%20yet%20effective%20soft-masking%0Amethod%2C%20Augmented%20Feature%20Importance%20Adjustment%20%28AFIA%29%2C%20which%20estimates%20feature%0Aimportance%20from%20augmented%20data%20to%20mitigate%20the%20issue.%20By%20establishing%20the%0Acohesive%20link%20from%20channel%20bias%20to%20its%20consequence%20of%20extreme%20feature%0Aredundancy%2C%20this%20work%20provides%20a%20foundational%20principle%20for%20few-shot%0Arepresentation%20transfer%20and%20a%20practical%20method%20for%20developing%20more%20robust%0Afew-shot%20learning%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03843v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Channel%2520Bias%2520to%2520Feature%2520Redundancy%253A%2520Uncovering%2520the%2520%2522Less%2520is%2520More%2522%250A%2520%2520Principle%2520in%2520Few-Shot%2520Learning%26entry.906535625%3DJi%2520Zhang%2520and%2520Xu%2520Luo%2520and%2520Lianli%2520Gao%2520and%2520Difan%2520Zou%2520and%2520Hengtao%2520Shen%2520and%2520Jingkuan%2520Song%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520often%2520fail%2520to%2520adapt%2520representations%2520to%2520novel%2520tasks%2520under%250Adistribution%2520shifts%252C%2520especially%2520when%2520only%2520a%2520few%2520examples%2520are%2520available.%2520This%250Apaper%2520identifies%2520a%2520core%2520obstacle%2520behind%2520this%2520failure%253A%2520channel%2520bias%252C%2520where%250Anetworks%2520develop%2520a%2520rigid%2520emphasis%2520on%2520feature%2520dimensions%2520that%2520were%250Adiscriminative%2520for%2520the%2520source%2520task%252C%2520but%2520this%2520emphasis%2520is%2520misaligned%2520and%2520fails%250Ato%2520adapt%2520to%2520the%2520distinct%2520needs%2520of%2520a%2520novel%2520task.%2520This%2520bias%2520leads%2520to%2520a%2520striking%250Aand%2520detrimental%2520consequence%253A%2520feature%2520redundancy.%2520We%2520demonstrate%2520that%2520for%250Afew-shot%2520tasks%252C%2520classification%2520accuracy%2520is%2520significantly%2520improved%2520by%2520using%2520as%250Afew%2520as%25201-5%2525%2520of%2520the%2520most%2520discriminative%2520feature%2520dimensions%252C%2520revealing%2520that%2520the%250Avast%2520majority%2520are%2520actively%2520harmful.%2520Our%2520theoretical%2520analysis%2520confirms%2520that%2520this%250Aredundancy%2520originates%2520from%2520confounding%2520feature%2520dimensions-those%2520with%2520high%250Aintra-class%2520variance%2520but%2520low%2520inter-class%2520separability-which%2520are%2520especially%250Aproblematic%2520in%2520low-data%2520regimes.%2520This%2520%2522less%2520is%2520more%2522%2520phenomenon%2520is%2520a%2520defining%250Acharacteristic%2520of%2520the%2520few-shot%2520setting%252C%2520diminishing%2520as%2520more%2520samples%2520become%250Aavailable.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520soft-masking%250Amethod%252C%2520Augmented%2520Feature%2520Importance%2520Adjustment%2520%2528AFIA%2529%252C%2520which%2520estimates%2520feature%250Aimportance%2520from%2520augmented%2520data%2520to%2520mitigate%2520the%2520issue.%2520By%2520establishing%2520the%250Acohesive%2520link%2520from%2520channel%2520bias%2520to%2520its%2520consequence%2520of%2520extreme%2520feature%250Aredundancy%252C%2520this%2520work%2520provides%2520a%2520foundational%2520principle%2520for%2520few-shot%250Arepresentation%2520transfer%2520and%2520a%2520practical%2520method%2520for%2520developing%2520more%2520robust%250Afew-shot%2520learning%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03843v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Channel%20Bias%20to%20Feature%20Redundancy%3A%20Uncovering%20the%20%22Less%20is%20More%22%0A%20%20Principle%20in%20Few-Shot%20Learning&entry.906535625=Ji%20Zhang%20and%20Xu%20Luo%20and%20Lianli%20Gao%20and%20Difan%20Zou%20and%20Hengtao%20Shen%20and%20Jingkuan%20Song&entry.1292438233=%20%20Deep%20neural%20networks%20often%20fail%20to%20adapt%20representations%20to%20novel%20tasks%20under%0Adistribution%20shifts%2C%20especially%20when%20only%20a%20few%20examples%20are%20available.%20This%0Apaper%20identifies%20a%20core%20obstacle%20behind%20this%20failure%3A%20channel%20bias%2C%20where%0Anetworks%20develop%20a%20rigid%20emphasis%20on%20feature%20dimensions%20that%20were%0Adiscriminative%20for%20the%20source%20task%2C%20but%20this%20emphasis%20is%20misaligned%20and%20fails%0Ato%20adapt%20to%20the%20distinct%20needs%20of%20a%20novel%20task.%20This%20bias%20leads%20to%20a%20striking%0Aand%20detrimental%20consequence%3A%20feature%20redundancy.%20We%20demonstrate%20that%20for%0Afew-shot%20tasks%2C%20classification%20accuracy%20is%20significantly%20improved%20by%20using%20as%0Afew%20as%201-5%25%20of%20the%20most%20discriminative%20feature%20dimensions%2C%20revealing%20that%20the%0Avast%20majority%20are%20actively%20harmful.%20Our%20theoretical%20analysis%20confirms%20that%20this%0Aredundancy%20originates%20from%20confounding%20feature%20dimensions-those%20with%20high%0Aintra-class%20variance%20but%20low%20inter-class%20separability-which%20are%20especially%0Aproblematic%20in%20low-data%20regimes.%20This%20%22less%20is%20more%22%20phenomenon%20is%20a%20defining%0Acharacteristic%20of%20the%20few-shot%20setting%2C%20diminishing%20as%20more%20samples%20become%0Aavailable.%20To%20address%20this%2C%20we%20propose%20a%20simple%20yet%20effective%20soft-masking%0Amethod%2C%20Augmented%20Feature%20Importance%20Adjustment%20%28AFIA%29%2C%20which%20estimates%20feature%0Aimportance%20from%20augmented%20data%20to%20mitigate%20the%20issue.%20By%20establishing%20the%0Acohesive%20link%20from%20channel%20bias%20to%20its%20consequence%20of%20extreme%20feature%0Aredundancy%2C%20this%20work%20provides%20a%20foundational%20principle%20for%20few-shot%0Arepresentation%20transfer%20and%20a%20practical%20method%20for%20developing%20more%20robust%0Afew-shot%20learning%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03843v2&entry.124074799=Read"},
{"title": "To See a World in a Spark of Neuron: Disentangling Multi-task\n  Interference for Training-free Model Merging", "author": "Zitao Fang and Guodong DU and Shuyang Yu and Yifei Guo and Yiwei Zhang and Yiyao Cao and Jing Li and Ho-Kin Tang and Sim Kuan Goh", "abstract": "  Fine-tuning pre-trained models on targeted datasets enhances task-specific\nperformance but often comes at the expense of generalization. Model merging\ntechniques, which integrate multiple fine-tuned models into a single multi-task\nmodel through task arithmetic, offer a promising solution. However, task\ninterference remains a fundamental challenge, leading to performance\ndegradation and suboptimal merged models. Existing approaches largely\noverlooked the fundamental roles of neurons, their connectivity, and\nactivation, resulting in a merging process and a merged model that does not\nconsider how neurons relay and process information. In this work, we present\nthe first study that relies on neuronal mechanisms for model merging.\nSpecifically, we decomposed task-specific representations into two\ncomplementary neuronal subspaces that regulate input sensitivity and task\nadaptability. Leveraging this decomposition, we introduced NeuroMerging, a\nnovel merging framework developed to mitigate task interference within neuronal\nsubspaces, enabling training-free model fusion across diverse tasks. Through\nextensive experiments, we demonstrated that NeuroMerging achieved superior\nperformance compared to existing methods on multi-task benchmarks across both\nnatural language and vision domains. Our findings highlighted the importance of\naligning neuronal mechanisms in model merging, offering new insights into\nmitigating task interference and improving knowledge fusion. Our project is\navailable at https://ZzzitaoFang.github.io/projects/NeuroMerging/.\n", "link": "http://arxiv.org/abs/2503.05320v4", "date": "2025-09-10", "relevancy": 2.573, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20See%20a%20World%20in%20a%20Spark%20of%20Neuron%3A%20Disentangling%20Multi-task%0A%20%20Interference%20for%20Training-free%20Model%20Merging&body=Title%3A%20To%20See%20a%20World%20in%20a%20Spark%20of%20Neuron%3A%20Disentangling%20Multi-task%0A%20%20Interference%20for%20Training-free%20Model%20Merging%0AAuthor%3A%20Zitao%20Fang%20and%20Guodong%20DU%20and%20Shuyang%20Yu%20and%20Yifei%20Guo%20and%20Yiwei%20Zhang%20and%20Yiyao%20Cao%20and%20Jing%20Li%20and%20Ho-Kin%20Tang%20and%20Sim%20Kuan%20Goh%0AAbstract%3A%20%20%20Fine-tuning%20pre-trained%20models%20on%20targeted%20datasets%20enhances%20task-specific%0Aperformance%20but%20often%20comes%20at%20the%20expense%20of%20generalization.%20Model%20merging%0Atechniques%2C%20which%20integrate%20multiple%20fine-tuned%20models%20into%20a%20single%20multi-task%0Amodel%20through%20task%20arithmetic%2C%20offer%20a%20promising%20solution.%20However%2C%20task%0Ainterference%20remains%20a%20fundamental%20challenge%2C%20leading%20to%20performance%0Adegradation%20and%20suboptimal%20merged%20models.%20Existing%20approaches%20largely%0Aoverlooked%20the%20fundamental%20roles%20of%20neurons%2C%20their%20connectivity%2C%20and%0Aactivation%2C%20resulting%20in%20a%20merging%20process%20and%20a%20merged%20model%20that%20does%20not%0Aconsider%20how%20neurons%20relay%20and%20process%20information.%20In%20this%20work%2C%20we%20present%0Athe%20first%20study%20that%20relies%20on%20neuronal%20mechanisms%20for%20model%20merging.%0ASpecifically%2C%20we%20decomposed%20task-specific%20representations%20into%20two%0Acomplementary%20neuronal%20subspaces%20that%20regulate%20input%20sensitivity%20and%20task%0Aadaptability.%20Leveraging%20this%20decomposition%2C%20we%20introduced%20NeuroMerging%2C%20a%0Anovel%20merging%20framework%20developed%20to%20mitigate%20task%20interference%20within%20neuronal%0Asubspaces%2C%20enabling%20training-free%20model%20fusion%20across%20diverse%20tasks.%20Through%0Aextensive%20experiments%2C%20we%20demonstrated%20that%20NeuroMerging%20achieved%20superior%0Aperformance%20compared%20to%20existing%20methods%20on%20multi-task%20benchmarks%20across%20both%0Anatural%20language%20and%20vision%20domains.%20Our%20findings%20highlighted%20the%20importance%20of%0Aaligning%20neuronal%20mechanisms%20in%20model%20merging%2C%20offering%20new%20insights%20into%0Amitigating%20task%20interference%20and%20improving%20knowledge%20fusion.%20Our%20project%20is%0Aavailable%20at%20https%3A//ZzzitaoFang.github.io/projects/NeuroMerging/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05320v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520See%2520a%2520World%2520in%2520a%2520Spark%2520of%2520Neuron%253A%2520Disentangling%2520Multi-task%250A%2520%2520Interference%2520for%2520Training-free%2520Model%2520Merging%26entry.906535625%3DZitao%2520Fang%2520and%2520Guodong%2520DU%2520and%2520Shuyang%2520Yu%2520and%2520Yifei%2520Guo%2520and%2520Yiwei%2520Zhang%2520and%2520Yiyao%2520Cao%2520and%2520Jing%2520Li%2520and%2520Ho-Kin%2520Tang%2520and%2520Sim%2520Kuan%2520Goh%26entry.1292438233%3D%2520%2520Fine-tuning%2520pre-trained%2520models%2520on%2520targeted%2520datasets%2520enhances%2520task-specific%250Aperformance%2520but%2520often%2520comes%2520at%2520the%2520expense%2520of%2520generalization.%2520Model%2520merging%250Atechniques%252C%2520which%2520integrate%2520multiple%2520fine-tuned%2520models%2520into%2520a%2520single%2520multi-task%250Amodel%2520through%2520task%2520arithmetic%252C%2520offer%2520a%2520promising%2520solution.%2520However%252C%2520task%250Ainterference%2520remains%2520a%2520fundamental%2520challenge%252C%2520leading%2520to%2520performance%250Adegradation%2520and%2520suboptimal%2520merged%2520models.%2520Existing%2520approaches%2520largely%250Aoverlooked%2520the%2520fundamental%2520roles%2520of%2520neurons%252C%2520their%2520connectivity%252C%2520and%250Aactivation%252C%2520resulting%2520in%2520a%2520merging%2520process%2520and%2520a%2520merged%2520model%2520that%2520does%2520not%250Aconsider%2520how%2520neurons%2520relay%2520and%2520process%2520information.%2520In%2520this%2520work%252C%2520we%2520present%250Athe%2520first%2520study%2520that%2520relies%2520on%2520neuronal%2520mechanisms%2520for%2520model%2520merging.%250ASpecifically%252C%2520we%2520decomposed%2520task-specific%2520representations%2520into%2520two%250Acomplementary%2520neuronal%2520subspaces%2520that%2520regulate%2520input%2520sensitivity%2520and%2520task%250Aadaptability.%2520Leveraging%2520this%2520decomposition%252C%2520we%2520introduced%2520NeuroMerging%252C%2520a%250Anovel%2520merging%2520framework%2520developed%2520to%2520mitigate%2520task%2520interference%2520within%2520neuronal%250Asubspaces%252C%2520enabling%2520training-free%2520model%2520fusion%2520across%2520diverse%2520tasks.%2520Through%250Aextensive%2520experiments%252C%2520we%2520demonstrated%2520that%2520NeuroMerging%2520achieved%2520superior%250Aperformance%2520compared%2520to%2520existing%2520methods%2520on%2520multi-task%2520benchmarks%2520across%2520both%250Anatural%2520language%2520and%2520vision%2520domains.%2520Our%2520findings%2520highlighted%2520the%2520importance%2520of%250Aaligning%2520neuronal%2520mechanisms%2520in%2520model%2520merging%252C%2520offering%2520new%2520insights%2520into%250Amitigating%2520task%2520interference%2520and%2520improving%2520knowledge%2520fusion.%2520Our%2520project%2520is%250Aavailable%2520at%2520https%253A//ZzzitaoFang.github.io/projects/NeuroMerging/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05320v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20See%20a%20World%20in%20a%20Spark%20of%20Neuron%3A%20Disentangling%20Multi-task%0A%20%20Interference%20for%20Training-free%20Model%20Merging&entry.906535625=Zitao%20Fang%20and%20Guodong%20DU%20and%20Shuyang%20Yu%20and%20Yifei%20Guo%20and%20Yiwei%20Zhang%20and%20Yiyao%20Cao%20and%20Jing%20Li%20and%20Ho-Kin%20Tang%20and%20Sim%20Kuan%20Goh&entry.1292438233=%20%20Fine-tuning%20pre-trained%20models%20on%20targeted%20datasets%20enhances%20task-specific%0Aperformance%20but%20often%20comes%20at%20the%20expense%20of%20generalization.%20Model%20merging%0Atechniques%2C%20which%20integrate%20multiple%20fine-tuned%20models%20into%20a%20single%20multi-task%0Amodel%20through%20task%20arithmetic%2C%20offer%20a%20promising%20solution.%20However%2C%20task%0Ainterference%20remains%20a%20fundamental%20challenge%2C%20leading%20to%20performance%0Adegradation%20and%20suboptimal%20merged%20models.%20Existing%20approaches%20largely%0Aoverlooked%20the%20fundamental%20roles%20of%20neurons%2C%20their%20connectivity%2C%20and%0Aactivation%2C%20resulting%20in%20a%20merging%20process%20and%20a%20merged%20model%20that%20does%20not%0Aconsider%20how%20neurons%20relay%20and%20process%20information.%20In%20this%20work%2C%20we%20present%0Athe%20first%20study%20that%20relies%20on%20neuronal%20mechanisms%20for%20model%20merging.%0ASpecifically%2C%20we%20decomposed%20task-specific%20representations%20into%20two%0Acomplementary%20neuronal%20subspaces%20that%20regulate%20input%20sensitivity%20and%20task%0Aadaptability.%20Leveraging%20this%20decomposition%2C%20we%20introduced%20NeuroMerging%2C%20a%0Anovel%20merging%20framework%20developed%20to%20mitigate%20task%20interference%20within%20neuronal%0Asubspaces%2C%20enabling%20training-free%20model%20fusion%20across%20diverse%20tasks.%20Through%0Aextensive%20experiments%2C%20we%20demonstrated%20that%20NeuroMerging%20achieved%20superior%0Aperformance%20compared%20to%20existing%20methods%20on%20multi-task%20benchmarks%20across%20both%0Anatural%20language%20and%20vision%20domains.%20Our%20findings%20highlighted%20the%20importance%20of%0Aaligning%20neuronal%20mechanisms%20in%20model%20merging%2C%20offering%20new%20insights%20into%0Amitigating%20task%20interference%20and%20improving%20knowledge%20fusion.%20Our%20project%20is%0Aavailable%20at%20https%3A//ZzzitaoFang.github.io/projects/NeuroMerging/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05320v4&entry.124074799=Read"},
{"title": "Randomly Sampled Language Reasoning Problems Elucidate Limitations of\n  In-Context Learning", "author": "Kavi Gupta and Kate Sanders and Armando Solar-Lezama", "abstract": "  While LLMs have revolutionized the field of machine learning due to their\nhigh performance on a strikingly wide range of problems, they are also known to\nhallucinate false answers and underperform on less canonical versions of the\nsame tasks. There are several emerging theories of LLM performance, among them\nthat LLMs lack world modeling ability, that they have an undesirable bias\ntowards an autoregressive prior, and that they struggle on more novel problems.\nThe existing literature on LLM input novelty has focused on tasks of relatively\nhigh complexity, studying perturbations of canonical but complex problems. In\nthis paper, we attempt to minimize complexity in order to isolate novelty as a\nfactor in LLM underperformance and investigate the power of\nin-context-learning. To this end, we consider an extremely simple domain: next\ntoken prediction on simple language tasks. The twist is that these language\ntasks are wholly unseen, as they are randomly drawn from a large,\nparsimoniously defined set of languages arising from simple grammar rules. This\nexperimental setup allows us to evaluate ICL independently of models'\nparametric knowledge. We find that LLMs uniformly underperform n-gram models on\nthis task, both when used as next token predictors and in chain-of-thought.\n", "link": "http://arxiv.org/abs/2501.02825v6", "date": "2025-09-10", "relevancy": 2.563, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Randomly%20Sampled%20Language%20Reasoning%20Problems%20Elucidate%20Limitations%20of%0A%20%20In-Context%20Learning&body=Title%3A%20Randomly%20Sampled%20Language%20Reasoning%20Problems%20Elucidate%20Limitations%20of%0A%20%20In-Context%20Learning%0AAuthor%3A%20Kavi%20Gupta%20and%20Kate%20Sanders%20and%20Armando%20Solar-Lezama%0AAbstract%3A%20%20%20While%20LLMs%20have%20revolutionized%20the%20field%20of%20machine%20learning%20due%20to%20their%0Ahigh%20performance%20on%20a%20strikingly%20wide%20range%20of%20problems%2C%20they%20are%20also%20known%20to%0Ahallucinate%20false%20answers%20and%20underperform%20on%20less%20canonical%20versions%20of%20the%0Asame%20tasks.%20There%20are%20several%20emerging%20theories%20of%20LLM%20performance%2C%20among%20them%0Athat%20LLMs%20lack%20world%20modeling%20ability%2C%20that%20they%20have%20an%20undesirable%20bias%0Atowards%20an%20autoregressive%20prior%2C%20and%20that%20they%20struggle%20on%20more%20novel%20problems.%0AThe%20existing%20literature%20on%20LLM%20input%20novelty%20has%20focused%20on%20tasks%20of%20relatively%0Ahigh%20complexity%2C%20studying%20perturbations%20of%20canonical%20but%20complex%20problems.%20In%0Athis%20paper%2C%20we%20attempt%20to%20minimize%20complexity%20in%20order%20to%20isolate%20novelty%20as%20a%0Afactor%20in%20LLM%20underperformance%20and%20investigate%20the%20power%20of%0Ain-context-learning.%20To%20this%20end%2C%20we%20consider%20an%20extremely%20simple%20domain%3A%20next%0Atoken%20prediction%20on%20simple%20language%20tasks.%20The%20twist%20is%20that%20these%20language%0Atasks%20are%20wholly%20unseen%2C%20as%20they%20are%20randomly%20drawn%20from%20a%20large%2C%0Aparsimoniously%20defined%20set%20of%20languages%20arising%20from%20simple%20grammar%20rules.%20This%0Aexperimental%20setup%20allows%20us%20to%20evaluate%20ICL%20independently%20of%20models%27%0Aparametric%20knowledge.%20We%20find%20that%20LLMs%20uniformly%20underperform%20n-gram%20models%20on%0Athis%20task%2C%20both%20when%20used%20as%20next%20token%20predictors%20and%20in%20chain-of-thought.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02825v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandomly%2520Sampled%2520Language%2520Reasoning%2520Problems%2520Elucidate%2520Limitations%2520of%250A%2520%2520In-Context%2520Learning%26entry.906535625%3DKavi%2520Gupta%2520and%2520Kate%2520Sanders%2520and%2520Armando%2520Solar-Lezama%26entry.1292438233%3D%2520%2520While%2520LLMs%2520have%2520revolutionized%2520the%2520field%2520of%2520machine%2520learning%2520due%2520to%2520their%250Ahigh%2520performance%2520on%2520a%2520strikingly%2520wide%2520range%2520of%2520problems%252C%2520they%2520are%2520also%2520known%2520to%250Ahallucinate%2520false%2520answers%2520and%2520underperform%2520on%2520less%2520canonical%2520versions%2520of%2520the%250Asame%2520tasks.%2520There%2520are%2520several%2520emerging%2520theories%2520of%2520LLM%2520performance%252C%2520among%2520them%250Athat%2520LLMs%2520lack%2520world%2520modeling%2520ability%252C%2520that%2520they%2520have%2520an%2520undesirable%2520bias%250Atowards%2520an%2520autoregressive%2520prior%252C%2520and%2520that%2520they%2520struggle%2520on%2520more%2520novel%2520problems.%250AThe%2520existing%2520literature%2520on%2520LLM%2520input%2520novelty%2520has%2520focused%2520on%2520tasks%2520of%2520relatively%250Ahigh%2520complexity%252C%2520studying%2520perturbations%2520of%2520canonical%2520but%2520complex%2520problems.%2520In%250Athis%2520paper%252C%2520we%2520attempt%2520to%2520minimize%2520complexity%2520in%2520order%2520to%2520isolate%2520novelty%2520as%2520a%250Afactor%2520in%2520LLM%2520underperformance%2520and%2520investigate%2520the%2520power%2520of%250Ain-context-learning.%2520To%2520this%2520end%252C%2520we%2520consider%2520an%2520extremely%2520simple%2520domain%253A%2520next%250Atoken%2520prediction%2520on%2520simple%2520language%2520tasks.%2520The%2520twist%2520is%2520that%2520these%2520language%250Atasks%2520are%2520wholly%2520unseen%252C%2520as%2520they%2520are%2520randomly%2520drawn%2520from%2520a%2520large%252C%250Aparsimoniously%2520defined%2520set%2520of%2520languages%2520arising%2520from%2520simple%2520grammar%2520rules.%2520This%250Aexperimental%2520setup%2520allows%2520us%2520to%2520evaluate%2520ICL%2520independently%2520of%2520models%2527%250Aparametric%2520knowledge.%2520We%2520find%2520that%2520LLMs%2520uniformly%2520underperform%2520n-gram%2520models%2520on%250Athis%2520task%252C%2520both%2520when%2520used%2520as%2520next%2520token%2520predictors%2520and%2520in%2520chain-of-thought.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02825v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Randomly%20Sampled%20Language%20Reasoning%20Problems%20Elucidate%20Limitations%20of%0A%20%20In-Context%20Learning&entry.906535625=Kavi%20Gupta%20and%20Kate%20Sanders%20and%20Armando%20Solar-Lezama&entry.1292438233=%20%20While%20LLMs%20have%20revolutionized%20the%20field%20of%20machine%20learning%20due%20to%20their%0Ahigh%20performance%20on%20a%20strikingly%20wide%20range%20of%20problems%2C%20they%20are%20also%20known%20to%0Ahallucinate%20false%20answers%20and%20underperform%20on%20less%20canonical%20versions%20of%20the%0Asame%20tasks.%20There%20are%20several%20emerging%20theories%20of%20LLM%20performance%2C%20among%20them%0Athat%20LLMs%20lack%20world%20modeling%20ability%2C%20that%20they%20have%20an%20undesirable%20bias%0Atowards%20an%20autoregressive%20prior%2C%20and%20that%20they%20struggle%20on%20more%20novel%20problems.%0AThe%20existing%20literature%20on%20LLM%20input%20novelty%20has%20focused%20on%20tasks%20of%20relatively%0Ahigh%20complexity%2C%20studying%20perturbations%20of%20canonical%20but%20complex%20problems.%20In%0Athis%20paper%2C%20we%20attempt%20to%20minimize%20complexity%20in%20order%20to%20isolate%20novelty%20as%20a%0Afactor%20in%20LLM%20underperformance%20and%20investigate%20the%20power%20of%0Ain-context-learning.%20To%20this%20end%2C%20we%20consider%20an%20extremely%20simple%20domain%3A%20next%0Atoken%20prediction%20on%20simple%20language%20tasks.%20The%20twist%20is%20that%20these%20language%0Atasks%20are%20wholly%20unseen%2C%20as%20they%20are%20randomly%20drawn%20from%20a%20large%2C%0Aparsimoniously%20defined%20set%20of%20languages%20arising%20from%20simple%20grammar%20rules.%20This%0Aexperimental%20setup%20allows%20us%20to%20evaluate%20ICL%20independently%20of%20models%27%0Aparametric%20knowledge.%20We%20find%20that%20LLMs%20uniformly%20underperform%20n-gram%20models%20on%0Athis%20task%2C%20both%20when%20used%20as%20next%20token%20predictors%20and%20in%20chain-of-thought.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02825v6&entry.124074799=Read"},
{"title": "ArgoTweak: Towards Self-Updating HD Maps through Structured Priors", "author": "Lena Wild and Rafael Valencia and Patric Jensfelt", "abstract": "  Reliable integration of prior information is crucial for self-verifying and\nself-updating HD maps. However, no public dataset includes the required triplet\nof prior maps, current maps, and sensor data. As a result, existing methods\nmust rely on synthetic priors, which create inconsistencies and lead to a\nsignificant sim2real gap. To address this, we introduce ArgoTweak, the first\ndataset to complete the triplet with realistic map priors. At its core,\nArgoTweak employs a bijective mapping framework, breaking down large-scale\nmodifications into fine-grained atomic changes at the map element level, thus\nensuring interpretability. This paradigm shift enables accurate change\ndetection and integration while preserving unchanged elements with high\nfidelity. Experiments show that training models on ArgoTweak significantly\nreduces the sim2real gap compared to synthetic priors. Extensive ablations\nfurther highlight the impact of structured priors and detailed change\nannotations. By establishing a benchmark for explainable, prior-aided HD\nmapping, ArgoTweak advances scalable, self-improving mapping solutions. The\ndataset, baselines, map modification toolbox, and further resources are\navailable at https://kth-rpl.github.io/ArgoTweak/.\n", "link": "http://arxiv.org/abs/2509.08764v1", "date": "2025-09-10", "relevancy": 2.5575, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5244}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5088}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArgoTweak%3A%20Towards%20Self-Updating%20HD%20Maps%20through%20Structured%20Priors&body=Title%3A%20ArgoTweak%3A%20Towards%20Self-Updating%20HD%20Maps%20through%20Structured%20Priors%0AAuthor%3A%20Lena%20Wild%20and%20Rafael%20Valencia%20and%20Patric%20Jensfelt%0AAbstract%3A%20%20%20Reliable%20integration%20of%20prior%20information%20is%20crucial%20for%20self-verifying%20and%0Aself-updating%20HD%20maps.%20However%2C%20no%20public%20dataset%20includes%20the%20required%20triplet%0Aof%20prior%20maps%2C%20current%20maps%2C%20and%20sensor%20data.%20As%20a%20result%2C%20existing%20methods%0Amust%20rely%20on%20synthetic%20priors%2C%20which%20create%20inconsistencies%20and%20lead%20to%20a%0Asignificant%20sim2real%20gap.%20To%20address%20this%2C%20we%20introduce%20ArgoTweak%2C%20the%20first%0Adataset%20to%20complete%20the%20triplet%20with%20realistic%20map%20priors.%20At%20its%20core%2C%0AArgoTweak%20employs%20a%20bijective%20mapping%20framework%2C%20breaking%20down%20large-scale%0Amodifications%20into%20fine-grained%20atomic%20changes%20at%20the%20map%20element%20level%2C%20thus%0Aensuring%20interpretability.%20This%20paradigm%20shift%20enables%20accurate%20change%0Adetection%20and%20integration%20while%20preserving%20unchanged%20elements%20with%20high%0Afidelity.%20Experiments%20show%20that%20training%20models%20on%20ArgoTweak%20significantly%0Areduces%20the%20sim2real%20gap%20compared%20to%20synthetic%20priors.%20Extensive%20ablations%0Afurther%20highlight%20the%20impact%20of%20structured%20priors%20and%20detailed%20change%0Aannotations.%20By%20establishing%20a%20benchmark%20for%20explainable%2C%20prior-aided%20HD%0Amapping%2C%20ArgoTweak%20advances%20scalable%2C%20self-improving%20mapping%20solutions.%20The%0Adataset%2C%20baselines%2C%20map%20modification%20toolbox%2C%20and%20further%20resources%20are%0Aavailable%20at%20https%3A//kth-rpl.github.io/ArgoTweak/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArgoTweak%253A%2520Towards%2520Self-Updating%2520HD%2520Maps%2520through%2520Structured%2520Priors%26entry.906535625%3DLena%2520Wild%2520and%2520Rafael%2520Valencia%2520and%2520Patric%2520Jensfelt%26entry.1292438233%3D%2520%2520Reliable%2520integration%2520of%2520prior%2520information%2520is%2520crucial%2520for%2520self-verifying%2520and%250Aself-updating%2520HD%2520maps.%2520However%252C%2520no%2520public%2520dataset%2520includes%2520the%2520required%2520triplet%250Aof%2520prior%2520maps%252C%2520current%2520maps%252C%2520and%2520sensor%2520data.%2520As%2520a%2520result%252C%2520existing%2520methods%250Amust%2520rely%2520on%2520synthetic%2520priors%252C%2520which%2520create%2520inconsistencies%2520and%2520lead%2520to%2520a%250Asignificant%2520sim2real%2520gap.%2520To%2520address%2520this%252C%2520we%2520introduce%2520ArgoTweak%252C%2520the%2520first%250Adataset%2520to%2520complete%2520the%2520triplet%2520with%2520realistic%2520map%2520priors.%2520At%2520its%2520core%252C%250AArgoTweak%2520employs%2520a%2520bijective%2520mapping%2520framework%252C%2520breaking%2520down%2520large-scale%250Amodifications%2520into%2520fine-grained%2520atomic%2520changes%2520at%2520the%2520map%2520element%2520level%252C%2520thus%250Aensuring%2520interpretability.%2520This%2520paradigm%2520shift%2520enables%2520accurate%2520change%250Adetection%2520and%2520integration%2520while%2520preserving%2520unchanged%2520elements%2520with%2520high%250Afidelity.%2520Experiments%2520show%2520that%2520training%2520models%2520on%2520ArgoTweak%2520significantly%250Areduces%2520the%2520sim2real%2520gap%2520compared%2520to%2520synthetic%2520priors.%2520Extensive%2520ablations%250Afurther%2520highlight%2520the%2520impact%2520of%2520structured%2520priors%2520and%2520detailed%2520change%250Aannotations.%2520By%2520establishing%2520a%2520benchmark%2520for%2520explainable%252C%2520prior-aided%2520HD%250Amapping%252C%2520ArgoTweak%2520advances%2520scalable%252C%2520self-improving%2520mapping%2520solutions.%2520The%250Adataset%252C%2520baselines%252C%2520map%2520modification%2520toolbox%252C%2520and%2520further%2520resources%2520are%250Aavailable%2520at%2520https%253A//kth-rpl.github.io/ArgoTweak/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArgoTweak%3A%20Towards%20Self-Updating%20HD%20Maps%20through%20Structured%20Priors&entry.906535625=Lena%20Wild%20and%20Rafael%20Valencia%20and%20Patric%20Jensfelt&entry.1292438233=%20%20Reliable%20integration%20of%20prior%20information%20is%20crucial%20for%20self-verifying%20and%0Aself-updating%20HD%20maps.%20However%2C%20no%20public%20dataset%20includes%20the%20required%20triplet%0Aof%20prior%20maps%2C%20current%20maps%2C%20and%20sensor%20data.%20As%20a%20result%2C%20existing%20methods%0Amust%20rely%20on%20synthetic%20priors%2C%20which%20create%20inconsistencies%20and%20lead%20to%20a%0Asignificant%20sim2real%20gap.%20To%20address%20this%2C%20we%20introduce%20ArgoTweak%2C%20the%20first%0Adataset%20to%20complete%20the%20triplet%20with%20realistic%20map%20priors.%20At%20its%20core%2C%0AArgoTweak%20employs%20a%20bijective%20mapping%20framework%2C%20breaking%20down%20large-scale%0Amodifications%20into%20fine-grained%20atomic%20changes%20at%20the%20map%20element%20level%2C%20thus%0Aensuring%20interpretability.%20This%20paradigm%20shift%20enables%20accurate%20change%0Adetection%20and%20integration%20while%20preserving%20unchanged%20elements%20with%20high%0Afidelity.%20Experiments%20show%20that%20training%20models%20on%20ArgoTweak%20significantly%0Areduces%20the%20sim2real%20gap%20compared%20to%20synthetic%20priors.%20Extensive%20ablations%0Afurther%20highlight%20the%20impact%20of%20structured%20priors%20and%20detailed%20change%0Aannotations.%20By%20establishing%20a%20benchmark%20for%20explainable%2C%20prior-aided%20HD%0Amapping%2C%20ArgoTweak%20advances%20scalable%2C%20self-improving%20mapping%20solutions.%20The%0Adataset%2C%20baselines%2C%20map%20modification%20toolbox%2C%20and%20further%20resources%20are%0Aavailable%20at%20https%3A//kth-rpl.github.io/ArgoTweak/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08764v1&entry.124074799=Read"},
{"title": "Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching", "author": "Matthieu Vilain and R\u00e9mi Giraud and Yannick Berthoumieu and Guillaume Bourmaud", "abstract": "  Dense image matching aims to find a correspondent for every pixel of a source\nimage in a partially overlapping target image. State-of-the-art methods\ntypically rely on a coarse-to-fine mechanism where a single correspondent\nhypothesis is produced per source location at each scale. In challenging cases\n-- such as at depth discontinuities or when the target image is a strong\nzoom-in of the source image -- the correspondents of neighboring source\nlocations are often widely spread and predicting a single correspondent\nhypothesis per source location at each scale may lead to erroneous matches. In\nthis paper, we investigate the idea of predicting multiple correspondent\nhypotheses per source location at each scale instead. We consider a beam search\nstrategy to propagat multiple hypotheses at each scale and propose integrating\nthese multiple hypotheses into cross-attention layers, resulting in a novel\ndense matching architecture called BEAMER. BEAMER learns to preserve and\npropagate multiple hypotheses across scales, making it significantly more\nrobust than state-of-the-art methods, especially at depth discontinuities or\nwhen the target image is a strong zoom-in of the source image.\n", "link": "http://arxiv.org/abs/2509.08805v1", "date": "2025-09-10", "relevancy": 2.5336, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5232}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Handling%20Multiple%20Hypotheses%20in%20Coarse-to-Fine%20Dense%20Image%20Matching&body=Title%3A%20Handling%20Multiple%20Hypotheses%20in%20Coarse-to-Fine%20Dense%20Image%20Matching%0AAuthor%3A%20Matthieu%20Vilain%20and%20R%C3%A9mi%20Giraud%20and%20Yannick%20Berthoumieu%20and%20Guillaume%20Bourmaud%0AAbstract%3A%20%20%20Dense%20image%20matching%20aims%20to%20find%20a%20correspondent%20for%20every%20pixel%20of%20a%20source%0Aimage%20in%20a%20partially%20overlapping%20target%20image.%20State-of-the-art%20methods%0Atypically%20rely%20on%20a%20coarse-to-fine%20mechanism%20where%20a%20single%20correspondent%0Ahypothesis%20is%20produced%20per%20source%20location%20at%20each%20scale.%20In%20challenging%20cases%0A--%20such%20as%20at%20depth%20discontinuities%20or%20when%20the%20target%20image%20is%20a%20strong%0Azoom-in%20of%20the%20source%20image%20--%20the%20correspondents%20of%20neighboring%20source%0Alocations%20are%20often%20widely%20spread%20and%20predicting%20a%20single%20correspondent%0Ahypothesis%20per%20source%20location%20at%20each%20scale%20may%20lead%20to%20erroneous%20matches.%20In%0Athis%20paper%2C%20we%20investigate%20the%20idea%20of%20predicting%20multiple%20correspondent%0Ahypotheses%20per%20source%20location%20at%20each%20scale%20instead.%20We%20consider%20a%20beam%20search%0Astrategy%20to%20propagat%20multiple%20hypotheses%20at%20each%20scale%20and%20propose%20integrating%0Athese%20multiple%20hypotheses%20into%20cross-attention%20layers%2C%20resulting%20in%20a%20novel%0Adense%20matching%20architecture%20called%20BEAMER.%20BEAMER%20learns%20to%20preserve%20and%0Apropagate%20multiple%20hypotheses%20across%20scales%2C%20making%20it%20significantly%20more%0Arobust%20than%20state-of-the-art%20methods%2C%20especially%20at%20depth%20discontinuities%20or%0Awhen%20the%20target%20image%20is%20a%20strong%20zoom-in%20of%20the%20source%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandling%2520Multiple%2520Hypotheses%2520in%2520Coarse-to-Fine%2520Dense%2520Image%2520Matching%26entry.906535625%3DMatthieu%2520Vilain%2520and%2520R%25C3%25A9mi%2520Giraud%2520and%2520Yannick%2520Berthoumieu%2520and%2520Guillaume%2520Bourmaud%26entry.1292438233%3D%2520%2520Dense%2520image%2520matching%2520aims%2520to%2520find%2520a%2520correspondent%2520for%2520every%2520pixel%2520of%2520a%2520source%250Aimage%2520in%2520a%2520partially%2520overlapping%2520target%2520image.%2520State-of-the-art%2520methods%250Atypically%2520rely%2520on%2520a%2520coarse-to-fine%2520mechanism%2520where%2520a%2520single%2520correspondent%250Ahypothesis%2520is%2520produced%2520per%2520source%2520location%2520at%2520each%2520scale.%2520In%2520challenging%2520cases%250A--%2520such%2520as%2520at%2520depth%2520discontinuities%2520or%2520when%2520the%2520target%2520image%2520is%2520a%2520strong%250Azoom-in%2520of%2520the%2520source%2520image%2520--%2520the%2520correspondents%2520of%2520neighboring%2520source%250Alocations%2520are%2520often%2520widely%2520spread%2520and%2520predicting%2520a%2520single%2520correspondent%250Ahypothesis%2520per%2520source%2520location%2520at%2520each%2520scale%2520may%2520lead%2520to%2520erroneous%2520matches.%2520In%250Athis%2520paper%252C%2520we%2520investigate%2520the%2520idea%2520of%2520predicting%2520multiple%2520correspondent%250Ahypotheses%2520per%2520source%2520location%2520at%2520each%2520scale%2520instead.%2520We%2520consider%2520a%2520beam%2520search%250Astrategy%2520to%2520propagat%2520multiple%2520hypotheses%2520at%2520each%2520scale%2520and%2520propose%2520integrating%250Athese%2520multiple%2520hypotheses%2520into%2520cross-attention%2520layers%252C%2520resulting%2520in%2520a%2520novel%250Adense%2520matching%2520architecture%2520called%2520BEAMER.%2520BEAMER%2520learns%2520to%2520preserve%2520and%250Apropagate%2520multiple%2520hypotheses%2520across%2520scales%252C%2520making%2520it%2520significantly%2520more%250Arobust%2520than%2520state-of-the-art%2520methods%252C%2520especially%2520at%2520depth%2520discontinuities%2520or%250Awhen%2520the%2520target%2520image%2520is%2520a%2520strong%2520zoom-in%2520of%2520the%2520source%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Handling%20Multiple%20Hypotheses%20in%20Coarse-to-Fine%20Dense%20Image%20Matching&entry.906535625=Matthieu%20Vilain%20and%20R%C3%A9mi%20Giraud%20and%20Yannick%20Berthoumieu%20and%20Guillaume%20Bourmaud&entry.1292438233=%20%20Dense%20image%20matching%20aims%20to%20find%20a%20correspondent%20for%20every%20pixel%20of%20a%20source%0Aimage%20in%20a%20partially%20overlapping%20target%20image.%20State-of-the-art%20methods%0Atypically%20rely%20on%20a%20coarse-to-fine%20mechanism%20where%20a%20single%20correspondent%0Ahypothesis%20is%20produced%20per%20source%20location%20at%20each%20scale.%20In%20challenging%20cases%0A--%20such%20as%20at%20depth%20discontinuities%20or%20when%20the%20target%20image%20is%20a%20strong%0Azoom-in%20of%20the%20source%20image%20--%20the%20correspondents%20of%20neighboring%20source%0Alocations%20are%20often%20widely%20spread%20and%20predicting%20a%20single%20correspondent%0Ahypothesis%20per%20source%20location%20at%20each%20scale%20may%20lead%20to%20erroneous%20matches.%20In%0Athis%20paper%2C%20we%20investigate%20the%20idea%20of%20predicting%20multiple%20correspondent%0Ahypotheses%20per%20source%20location%20at%20each%20scale%20instead.%20We%20consider%20a%20beam%20search%0Astrategy%20to%20propagat%20multiple%20hypotheses%20at%20each%20scale%20and%20propose%20integrating%0Athese%20multiple%20hypotheses%20into%20cross-attention%20layers%2C%20resulting%20in%20a%20novel%0Adense%20matching%20architecture%20called%20BEAMER.%20BEAMER%20learns%20to%20preserve%20and%0Apropagate%20multiple%20hypotheses%20across%20scales%2C%20making%20it%20significantly%20more%0Arobust%20than%20state-of-the-art%20methods%2C%20especially%20at%20depth%20discontinuities%20or%0Awhen%20the%20target%20image%20is%20a%20strong%20zoom-in%20of%20the%20source%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08805v1&entry.124074799=Read"},
{"title": "Gaussian Process Regression -- Neural Network Hybrid with Optimized\n  Redundant Coordinates", "author": "Sergei Manzhos and Manabu Ihara", "abstract": "  Recently, a Gaussian Process Regression - neural network (GPRNN) hybrid\nmachine learning method was proposed, which is based on additive-kernel GPR in\nredundant coordinates constructed by rules [J. Phys. Chem. A 127 (2023) 7823].\nThe method combined the expressive power of an NN with the robustness of linear\nregression, in particular, with respect to overfitting when the number of\nneurons is increased beyond optimal. We introduce opt-GPRNN, in which the\nredundant coordinates of GPRNN are optimized with a Monte Carlo algorithm and\nshow that when combined with optimization of redundant coordinates, GPRNN\nattains the lowest test set error with much fewer terms / neurons and retains\nthe advantage of avoiding overfitting when the number of neurons is increased\nbeyond optimal value. The method, opt-GPRNN possesses an expressive power\ncloser to that of a multilayer NN and could obviate the need for deep NNs in\nsome applications. With optimized redundant coordinates, a dimensionality\nreduction regime is also possible. Examples of application to machine learning\nan interatomic potential and materials informatics are given.\n", "link": "http://arxiv.org/abs/2509.08457v1", "date": "2025-09-10", "relevancy": 2.5273, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5199}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5043}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Process%20Regression%20--%20Neural%20Network%20Hybrid%20with%20Optimized%0A%20%20Redundant%20Coordinates&body=Title%3A%20Gaussian%20Process%20Regression%20--%20Neural%20Network%20Hybrid%20with%20Optimized%0A%20%20Redundant%20Coordinates%0AAuthor%3A%20Sergei%20Manzhos%20and%20Manabu%20Ihara%0AAbstract%3A%20%20%20Recently%2C%20a%20Gaussian%20Process%20Regression%20-%20neural%20network%20%28GPRNN%29%20hybrid%0Amachine%20learning%20method%20was%20proposed%2C%20which%20is%20based%20on%20additive-kernel%20GPR%20in%0Aredundant%20coordinates%20constructed%20by%20rules%20%5BJ.%20Phys.%20Chem.%20A%20127%20%282023%29%207823%5D.%0AThe%20method%20combined%20the%20expressive%20power%20of%20an%20NN%20with%20the%20robustness%20of%20linear%0Aregression%2C%20in%20particular%2C%20with%20respect%20to%20overfitting%20when%20the%20number%20of%0Aneurons%20is%20increased%20beyond%20optimal.%20We%20introduce%20opt-GPRNN%2C%20in%20which%20the%0Aredundant%20coordinates%20of%20GPRNN%20are%20optimized%20with%20a%20Monte%20Carlo%20algorithm%20and%0Ashow%20that%20when%20combined%20with%20optimization%20of%20redundant%20coordinates%2C%20GPRNN%0Aattains%20the%20lowest%20test%20set%20error%20with%20much%20fewer%20terms%20/%20neurons%20and%20retains%0Athe%20advantage%20of%20avoiding%20overfitting%20when%20the%20number%20of%20neurons%20is%20increased%0Abeyond%20optimal%20value.%20The%20method%2C%20opt-GPRNN%20possesses%20an%20expressive%20power%0Acloser%20to%20that%20of%20a%20multilayer%20NN%20and%20could%20obviate%20the%20need%20for%20deep%20NNs%20in%0Asome%20applications.%20With%20optimized%20redundant%20coordinates%2C%20a%20dimensionality%0Areduction%20regime%20is%20also%20possible.%20Examples%20of%20application%20to%20machine%20learning%0Aan%20interatomic%20potential%20and%20materials%20informatics%20are%20given.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Process%2520Regression%2520--%2520Neural%2520Network%2520Hybrid%2520with%2520Optimized%250A%2520%2520Redundant%2520Coordinates%26entry.906535625%3DSergei%2520Manzhos%2520and%2520Manabu%2520Ihara%26entry.1292438233%3D%2520%2520Recently%252C%2520a%2520Gaussian%2520Process%2520Regression%2520-%2520neural%2520network%2520%2528GPRNN%2529%2520hybrid%250Amachine%2520learning%2520method%2520was%2520proposed%252C%2520which%2520is%2520based%2520on%2520additive-kernel%2520GPR%2520in%250Aredundant%2520coordinates%2520constructed%2520by%2520rules%2520%255BJ.%2520Phys.%2520Chem.%2520A%2520127%2520%25282023%2529%25207823%255D.%250AThe%2520method%2520combined%2520the%2520expressive%2520power%2520of%2520an%2520NN%2520with%2520the%2520robustness%2520of%2520linear%250Aregression%252C%2520in%2520particular%252C%2520with%2520respect%2520to%2520overfitting%2520when%2520the%2520number%2520of%250Aneurons%2520is%2520increased%2520beyond%2520optimal.%2520We%2520introduce%2520opt-GPRNN%252C%2520in%2520which%2520the%250Aredundant%2520coordinates%2520of%2520GPRNN%2520are%2520optimized%2520with%2520a%2520Monte%2520Carlo%2520algorithm%2520and%250Ashow%2520that%2520when%2520combined%2520with%2520optimization%2520of%2520redundant%2520coordinates%252C%2520GPRNN%250Aattains%2520the%2520lowest%2520test%2520set%2520error%2520with%2520much%2520fewer%2520terms%2520/%2520neurons%2520and%2520retains%250Athe%2520advantage%2520of%2520avoiding%2520overfitting%2520when%2520the%2520number%2520of%2520neurons%2520is%2520increased%250Abeyond%2520optimal%2520value.%2520The%2520method%252C%2520opt-GPRNN%2520possesses%2520an%2520expressive%2520power%250Acloser%2520to%2520that%2520of%2520a%2520multilayer%2520NN%2520and%2520could%2520obviate%2520the%2520need%2520for%2520deep%2520NNs%2520in%250Asome%2520applications.%2520With%2520optimized%2520redundant%2520coordinates%252C%2520a%2520dimensionality%250Areduction%2520regime%2520is%2520also%2520possible.%2520Examples%2520of%2520application%2520to%2520machine%2520learning%250Aan%2520interatomic%2520potential%2520and%2520materials%2520informatics%2520are%2520given.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Process%20Regression%20--%20Neural%20Network%20Hybrid%20with%20Optimized%0A%20%20Redundant%20Coordinates&entry.906535625=Sergei%20Manzhos%20and%20Manabu%20Ihara&entry.1292438233=%20%20Recently%2C%20a%20Gaussian%20Process%20Regression%20-%20neural%20network%20%28GPRNN%29%20hybrid%0Amachine%20learning%20method%20was%20proposed%2C%20which%20is%20based%20on%20additive-kernel%20GPR%20in%0Aredundant%20coordinates%20constructed%20by%20rules%20%5BJ.%20Phys.%20Chem.%20A%20127%20%282023%29%207823%5D.%0AThe%20method%20combined%20the%20expressive%20power%20of%20an%20NN%20with%20the%20robustness%20of%20linear%0Aregression%2C%20in%20particular%2C%20with%20respect%20to%20overfitting%20when%20the%20number%20of%0Aneurons%20is%20increased%20beyond%20optimal.%20We%20introduce%20opt-GPRNN%2C%20in%20which%20the%0Aredundant%20coordinates%20of%20GPRNN%20are%20optimized%20with%20a%20Monte%20Carlo%20algorithm%20and%0Ashow%20that%20when%20combined%20with%20optimization%20of%20redundant%20coordinates%2C%20GPRNN%0Aattains%20the%20lowest%20test%20set%20error%20with%20much%20fewer%20terms%20/%20neurons%20and%20retains%0Athe%20advantage%20of%20avoiding%20overfitting%20when%20the%20number%20of%20neurons%20is%20increased%0Abeyond%20optimal%20value.%20The%20method%2C%20opt-GPRNN%20possesses%20an%20expressive%20power%0Acloser%20to%20that%20of%20a%20multilayer%20NN%20and%20could%20obviate%20the%20need%20for%20deep%20NNs%20in%0Asome%20applications.%20With%20optimized%20redundant%20coordinates%2C%20a%20dimensionality%0Areduction%20regime%20is%20also%20possible.%20Examples%20of%20application%20to%20machine%20learning%0Aan%20interatomic%20potential%20and%20materials%20informatics%20are%20given.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08457v1&entry.124074799=Read"},
{"title": "Subjective Behaviors and Preferences in LLM: Language of Browsing", "author": "Sai Sundaresan and Harshita Chopra and Atanu R. Sinha and Koustava Goswami and Nagasai Saketh Naidu and Raghav Karan and N Anushka", "abstract": "  A Large Language Model (LLM) offers versatility across domains and tasks,\npurportedly benefiting users with a wide variety of behaviors and preferences.\nWe question this perception about an LLM when users have inherently subjective\nbehaviors and preferences, as seen in their ubiquitous and idiosyncratic\nbrowsing of websites or apps. The sequential behavior logs of pages, thus\ngenerated, form something akin to each user's self-constructed \"language\",\nalbeit without the structure and grammar imbued in natural languages. We ask:\n(i) Can a small LM represent the \"language of browsing\" better than a large LM?\n(ii) Can an LM with a single set of parameters (or, single LM) adequately\ncapture myriad users' heterogeneous, subjective behaviors and preferences?\n(iii) Can a single LM with high average performance, yield low variance in\nperformance to make alignment good at user level? We introduce clusterwise LM\ntraining, HeTLM (Heterogeneity aware Training of Language Model), appropriate\nfor subjective behaviors. We find that (i) a small LM trained using a\npage-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM\nwith heterogeneous cluster specific set of parameters outperforms a single LM\nof the same family, controlling for the number of parameters; and (iii) a\nhigher mean and a lower variance in generation ensues, implying improved\nalignment.\n", "link": "http://arxiv.org/abs/2508.15474v2", "date": "2025-09-10", "relevancy": 2.5157, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subjective%20Behaviors%20and%20Preferences%20in%20LLM%3A%20Language%20of%20Browsing&body=Title%3A%20Subjective%20Behaviors%20and%20Preferences%20in%20LLM%3A%20Language%20of%20Browsing%0AAuthor%3A%20Sai%20Sundaresan%20and%20Harshita%20Chopra%20and%20Atanu%20R.%20Sinha%20and%20Koustava%20Goswami%20and%20Nagasai%20Saketh%20Naidu%20and%20Raghav%20Karan%20and%20N%20Anushka%0AAbstract%3A%20%20%20A%20Large%20Language%20Model%20%28LLM%29%20offers%20versatility%20across%20domains%20and%20tasks%2C%0Apurportedly%20benefiting%20users%20with%20a%20wide%20variety%20of%20behaviors%20and%20preferences.%0AWe%20question%20this%20perception%20about%20an%20LLM%20when%20users%20have%20inherently%20subjective%0Abehaviors%20and%20preferences%2C%20as%20seen%20in%20their%20ubiquitous%20and%20idiosyncratic%0Abrowsing%20of%20websites%20or%20apps.%20The%20sequential%20behavior%20logs%20of%20pages%2C%20thus%0Agenerated%2C%20form%20something%20akin%20to%20each%20user%27s%20self-constructed%20%22language%22%2C%0Aalbeit%20without%20the%20structure%20and%20grammar%20imbued%20in%20natural%20languages.%20We%20ask%3A%0A%28i%29%20Can%20a%20small%20LM%20represent%20the%20%22language%20of%20browsing%22%20better%20than%20a%20large%20LM%3F%0A%28ii%29%20Can%20an%20LM%20with%20a%20single%20set%20of%20parameters%20%28or%2C%20single%20LM%29%20adequately%0Acapture%20myriad%20users%27%20heterogeneous%2C%20subjective%20behaviors%20and%20preferences%3F%0A%28iii%29%20Can%20a%20single%20LM%20with%20high%20average%20performance%2C%20yield%20low%20variance%20in%0Aperformance%20to%20make%20alignment%20good%20at%20user%20level%3F%20We%20introduce%20clusterwise%20LM%0Atraining%2C%20HeTLM%20%28Heterogeneity%20aware%20Training%20of%20Language%20Model%29%2C%20appropriate%0Afor%20subjective%20behaviors.%20We%20find%20that%20%28i%29%20a%20small%20LM%20trained%20using%20a%0Apage-level%20tokenizer%20outperforms%20large%20pretrained%20or%20finetuned%20LMs%3B%20%28ii%29%20HeTLM%0Awith%20heterogeneous%20cluster%20specific%20set%20of%20parameters%20outperforms%20a%20single%20LM%0Aof%20the%20same%20family%2C%20controlling%20for%20the%20number%20of%20parameters%3B%20and%20%28iii%29%20a%0Ahigher%20mean%20and%20a%20lower%20variance%20in%20generation%20ensues%2C%20implying%20improved%0Aalignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15474v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubjective%2520Behaviors%2520and%2520Preferences%2520in%2520LLM%253A%2520Language%2520of%2520Browsing%26entry.906535625%3DSai%2520Sundaresan%2520and%2520Harshita%2520Chopra%2520and%2520Atanu%2520R.%2520Sinha%2520and%2520Koustava%2520Goswami%2520and%2520Nagasai%2520Saketh%2520Naidu%2520and%2520Raghav%2520Karan%2520and%2520N%2520Anushka%26entry.1292438233%3D%2520%2520A%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520offers%2520versatility%2520across%2520domains%2520and%2520tasks%252C%250Apurportedly%2520benefiting%2520users%2520with%2520a%2520wide%2520variety%2520of%2520behaviors%2520and%2520preferences.%250AWe%2520question%2520this%2520perception%2520about%2520an%2520LLM%2520when%2520users%2520have%2520inherently%2520subjective%250Abehaviors%2520and%2520preferences%252C%2520as%2520seen%2520in%2520their%2520ubiquitous%2520and%2520idiosyncratic%250Abrowsing%2520of%2520websites%2520or%2520apps.%2520The%2520sequential%2520behavior%2520logs%2520of%2520pages%252C%2520thus%250Agenerated%252C%2520form%2520something%2520akin%2520to%2520each%2520user%2527s%2520self-constructed%2520%2522language%2522%252C%250Aalbeit%2520without%2520the%2520structure%2520and%2520grammar%2520imbued%2520in%2520natural%2520languages.%2520We%2520ask%253A%250A%2528i%2529%2520Can%2520a%2520small%2520LM%2520represent%2520the%2520%2522language%2520of%2520browsing%2522%2520better%2520than%2520a%2520large%2520LM%253F%250A%2528ii%2529%2520Can%2520an%2520LM%2520with%2520a%2520single%2520set%2520of%2520parameters%2520%2528or%252C%2520single%2520LM%2529%2520adequately%250Acapture%2520myriad%2520users%2527%2520heterogeneous%252C%2520subjective%2520behaviors%2520and%2520preferences%253F%250A%2528iii%2529%2520Can%2520a%2520single%2520LM%2520with%2520high%2520average%2520performance%252C%2520yield%2520low%2520variance%2520in%250Aperformance%2520to%2520make%2520alignment%2520good%2520at%2520user%2520level%253F%2520We%2520introduce%2520clusterwise%2520LM%250Atraining%252C%2520HeTLM%2520%2528Heterogeneity%2520aware%2520Training%2520of%2520Language%2520Model%2529%252C%2520appropriate%250Afor%2520subjective%2520behaviors.%2520We%2520find%2520that%2520%2528i%2529%2520a%2520small%2520LM%2520trained%2520using%2520a%250Apage-level%2520tokenizer%2520outperforms%2520large%2520pretrained%2520or%2520finetuned%2520LMs%253B%2520%2528ii%2529%2520HeTLM%250Awith%2520heterogeneous%2520cluster%2520specific%2520set%2520of%2520parameters%2520outperforms%2520a%2520single%2520LM%250Aof%2520the%2520same%2520family%252C%2520controlling%2520for%2520the%2520number%2520of%2520parameters%253B%2520and%2520%2528iii%2529%2520a%250Ahigher%2520mean%2520and%2520a%2520lower%2520variance%2520in%2520generation%2520ensues%252C%2520implying%2520improved%250Aalignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15474v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subjective%20Behaviors%20and%20Preferences%20in%20LLM%3A%20Language%20of%20Browsing&entry.906535625=Sai%20Sundaresan%20and%20Harshita%20Chopra%20and%20Atanu%20R.%20Sinha%20and%20Koustava%20Goswami%20and%20Nagasai%20Saketh%20Naidu%20and%20Raghav%20Karan%20and%20N%20Anushka&entry.1292438233=%20%20A%20Large%20Language%20Model%20%28LLM%29%20offers%20versatility%20across%20domains%20and%20tasks%2C%0Apurportedly%20benefiting%20users%20with%20a%20wide%20variety%20of%20behaviors%20and%20preferences.%0AWe%20question%20this%20perception%20about%20an%20LLM%20when%20users%20have%20inherently%20subjective%0Abehaviors%20and%20preferences%2C%20as%20seen%20in%20their%20ubiquitous%20and%20idiosyncratic%0Abrowsing%20of%20websites%20or%20apps.%20The%20sequential%20behavior%20logs%20of%20pages%2C%20thus%0Agenerated%2C%20form%20something%20akin%20to%20each%20user%27s%20self-constructed%20%22language%22%2C%0Aalbeit%20without%20the%20structure%20and%20grammar%20imbued%20in%20natural%20languages.%20We%20ask%3A%0A%28i%29%20Can%20a%20small%20LM%20represent%20the%20%22language%20of%20browsing%22%20better%20than%20a%20large%20LM%3F%0A%28ii%29%20Can%20an%20LM%20with%20a%20single%20set%20of%20parameters%20%28or%2C%20single%20LM%29%20adequately%0Acapture%20myriad%20users%27%20heterogeneous%2C%20subjective%20behaviors%20and%20preferences%3F%0A%28iii%29%20Can%20a%20single%20LM%20with%20high%20average%20performance%2C%20yield%20low%20variance%20in%0Aperformance%20to%20make%20alignment%20good%20at%20user%20level%3F%20We%20introduce%20clusterwise%20LM%0Atraining%2C%20HeTLM%20%28Heterogeneity%20aware%20Training%20of%20Language%20Model%29%2C%20appropriate%0Afor%20subjective%20behaviors.%20We%20find%20that%20%28i%29%20a%20small%20LM%20trained%20using%20a%0Apage-level%20tokenizer%20outperforms%20large%20pretrained%20or%20finetuned%20LMs%3B%20%28ii%29%20HeTLM%0Awith%20heterogeneous%20cluster%20specific%20set%20of%20parameters%20outperforms%20a%20single%20LM%0Aof%20the%20same%20family%2C%20controlling%20for%20the%20number%20of%20parameters%3B%20and%20%28iii%29%20a%0Ahigher%20mean%20and%20a%20lower%20variance%20in%20generation%20ensues%2C%20implying%20improved%0Aalignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15474v2&entry.124074799=Read"},
{"title": "Calib3R: A 3D Foundation Model for Multi-Camera to Robot Calibration and\n  3D Metric-Scaled Scene Reconstruction", "author": "Davide Allegro and Matteo Terreran and Stefano Ghidoni", "abstract": "  Robots often rely on RGB images for tasks like manipulation and navigation.\nHowever, reliable interaction typically requires a 3D scene representation that\nis metric-scaled and aligned with the robot reference frame. This depends on\naccurate camera-to-robot calibration and dense 3D reconstruction, tasks usually\ntreated separately, despite both relying on geometric correspondences from RGB\ndata. Traditional calibration needs patterns, while RGB-based reconstruction\nyields geometry with an unknown scale in an arbitrary frame. Multi-camera\nsetups add further complexity, as data must be expressed in a shared reference\nframe. We present Calib3R, a patternless method that jointly performs\ncamera-to-robot calibration and metric-scaled 3D reconstruction via unified\noptimization. Calib3R handles single- and multi-camera setups on robot arms or\nmobile robots. It builds on the 3D foundation model MASt3R to extract pointmaps\nfrom RGB images, which are combined with robot poses to reconstruct a scaled 3D\nscene aligned with the robot. Experiments on diverse datasets show that Calib3R\nachieves accurate calibration with less than 10 images, outperforming\ntarget-less and marker-based methods.\n", "link": "http://arxiv.org/abs/2509.08813v1", "date": "2025-09-10", "relevancy": 2.5156, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6451}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6257}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calib3R%3A%20A%203D%20Foundation%20Model%20for%20Multi-Camera%20to%20Robot%20Calibration%20and%0A%20%203D%20Metric-Scaled%20Scene%20Reconstruction&body=Title%3A%20Calib3R%3A%20A%203D%20Foundation%20Model%20for%20Multi-Camera%20to%20Robot%20Calibration%20and%0A%20%203D%20Metric-Scaled%20Scene%20Reconstruction%0AAuthor%3A%20Davide%20Allegro%20and%20Matteo%20Terreran%20and%20Stefano%20Ghidoni%0AAbstract%3A%20%20%20Robots%20often%20rely%20on%20RGB%20images%20for%20tasks%20like%20manipulation%20and%20navigation.%0AHowever%2C%20reliable%20interaction%20typically%20requires%20a%203D%20scene%20representation%20that%0Ais%20metric-scaled%20and%20aligned%20with%20the%20robot%20reference%20frame.%20This%20depends%20on%0Aaccurate%20camera-to-robot%20calibration%20and%20dense%203D%20reconstruction%2C%20tasks%20usually%0Atreated%20separately%2C%20despite%20both%20relying%20on%20geometric%20correspondences%20from%20RGB%0Adata.%20Traditional%20calibration%20needs%20patterns%2C%20while%20RGB-based%20reconstruction%0Ayields%20geometry%20with%20an%20unknown%20scale%20in%20an%20arbitrary%20frame.%20Multi-camera%0Asetups%20add%20further%20complexity%2C%20as%20data%20must%20be%20expressed%20in%20a%20shared%20reference%0Aframe.%20We%20present%20Calib3R%2C%20a%20patternless%20method%20that%20jointly%20performs%0Acamera-to-robot%20calibration%20and%20metric-scaled%203D%20reconstruction%20via%20unified%0Aoptimization.%20Calib3R%20handles%20single-%20and%20multi-camera%20setups%20on%20robot%20arms%20or%0Amobile%20robots.%20It%20builds%20on%20the%203D%20foundation%20model%20MASt3R%20to%20extract%20pointmaps%0Afrom%20RGB%20images%2C%20which%20are%20combined%20with%20robot%20poses%20to%20reconstruct%20a%20scaled%203D%0Ascene%20aligned%20with%20the%20robot.%20Experiments%20on%20diverse%20datasets%20show%20that%20Calib3R%0Aachieves%20accurate%20calibration%20with%20less%20than%2010%20images%2C%20outperforming%0Atarget-less%20and%20marker-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalib3R%253A%2520A%25203D%2520Foundation%2520Model%2520for%2520Multi-Camera%2520to%2520Robot%2520Calibration%2520and%250A%2520%25203D%2520Metric-Scaled%2520Scene%2520Reconstruction%26entry.906535625%3DDavide%2520Allegro%2520and%2520Matteo%2520Terreran%2520and%2520Stefano%2520Ghidoni%26entry.1292438233%3D%2520%2520Robots%2520often%2520rely%2520on%2520RGB%2520images%2520for%2520tasks%2520like%2520manipulation%2520and%2520navigation.%250AHowever%252C%2520reliable%2520interaction%2520typically%2520requires%2520a%25203D%2520scene%2520representation%2520that%250Ais%2520metric-scaled%2520and%2520aligned%2520with%2520the%2520robot%2520reference%2520frame.%2520This%2520depends%2520on%250Aaccurate%2520camera-to-robot%2520calibration%2520and%2520dense%25203D%2520reconstruction%252C%2520tasks%2520usually%250Atreated%2520separately%252C%2520despite%2520both%2520relying%2520on%2520geometric%2520correspondences%2520from%2520RGB%250Adata.%2520Traditional%2520calibration%2520needs%2520patterns%252C%2520while%2520RGB-based%2520reconstruction%250Ayields%2520geometry%2520with%2520an%2520unknown%2520scale%2520in%2520an%2520arbitrary%2520frame.%2520Multi-camera%250Asetups%2520add%2520further%2520complexity%252C%2520as%2520data%2520must%2520be%2520expressed%2520in%2520a%2520shared%2520reference%250Aframe.%2520We%2520present%2520Calib3R%252C%2520a%2520patternless%2520method%2520that%2520jointly%2520performs%250Acamera-to-robot%2520calibration%2520and%2520metric-scaled%25203D%2520reconstruction%2520via%2520unified%250Aoptimization.%2520Calib3R%2520handles%2520single-%2520and%2520multi-camera%2520setups%2520on%2520robot%2520arms%2520or%250Amobile%2520robots.%2520It%2520builds%2520on%2520the%25203D%2520foundation%2520model%2520MASt3R%2520to%2520extract%2520pointmaps%250Afrom%2520RGB%2520images%252C%2520which%2520are%2520combined%2520with%2520robot%2520poses%2520to%2520reconstruct%2520a%2520scaled%25203D%250Ascene%2520aligned%2520with%2520the%2520robot.%2520Experiments%2520on%2520diverse%2520datasets%2520show%2520that%2520Calib3R%250Aachieves%2520accurate%2520calibration%2520with%2520less%2520than%252010%2520images%252C%2520outperforming%250Atarget-less%2520and%2520marker-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calib3R%3A%20A%203D%20Foundation%20Model%20for%20Multi-Camera%20to%20Robot%20Calibration%20and%0A%20%203D%20Metric-Scaled%20Scene%20Reconstruction&entry.906535625=Davide%20Allegro%20and%20Matteo%20Terreran%20and%20Stefano%20Ghidoni&entry.1292438233=%20%20Robots%20often%20rely%20on%20RGB%20images%20for%20tasks%20like%20manipulation%20and%20navigation.%0AHowever%2C%20reliable%20interaction%20typically%20requires%20a%203D%20scene%20representation%20that%0Ais%20metric-scaled%20and%20aligned%20with%20the%20robot%20reference%20frame.%20This%20depends%20on%0Aaccurate%20camera-to-robot%20calibration%20and%20dense%203D%20reconstruction%2C%20tasks%20usually%0Atreated%20separately%2C%20despite%20both%20relying%20on%20geometric%20correspondences%20from%20RGB%0Adata.%20Traditional%20calibration%20needs%20patterns%2C%20while%20RGB-based%20reconstruction%0Ayields%20geometry%20with%20an%20unknown%20scale%20in%20an%20arbitrary%20frame.%20Multi-camera%0Asetups%20add%20further%20complexity%2C%20as%20data%20must%20be%20expressed%20in%20a%20shared%20reference%0Aframe.%20We%20present%20Calib3R%2C%20a%20patternless%20method%20that%20jointly%20performs%0Acamera-to-robot%20calibration%20and%20metric-scaled%203D%20reconstruction%20via%20unified%0Aoptimization.%20Calib3R%20handles%20single-%20and%20multi-camera%20setups%20on%20robot%20arms%20or%0Amobile%20robots.%20It%20builds%20on%20the%203D%20foundation%20model%20MASt3R%20to%20extract%20pointmaps%0Afrom%20RGB%20images%2C%20which%20are%20combined%20with%20robot%20poses%20to%20reconstruct%20a%20scaled%203D%0Ascene%20aligned%20with%20the%20robot.%20Experiments%20on%20diverse%20datasets%20show%20that%20Calib3R%0Aachieves%20accurate%20calibration%20with%20less%20than%2010%20images%2C%20outperforming%0Atarget-less%20and%20marker-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08813v1&entry.124074799=Read"},
{"title": "QR-VC: Leveraging Quantization Residuals for Linear Disentanglement in\n  Zero-Shot Voice Conversion", "author": "Youngjun Sim and Jinsung Yoon and Wooyeol Jeong and Young-Joo Suh", "abstract": "  Zero-shot voice conversion is a technique that alters the speaker identity of\nan input speech to match a target speaker using only a single reference\nutterance, without requiring additional training. Recent approaches extensively\nutilize self-supervised learning features with K-means quantization to extract\nhigh-quality content representations while removing speaker identity. However,\nthis quantization process also eliminates fine-grained phonetic and prosodic\nvariations, degrading intelligibility and prosody preservation. While prior\nworks have primarily focused on quantized representations, quantization\nresiduals remain underutilized and deserve further exploration. In this paper,\nwe introduce a novel approach that fully utilizes quantization residuals by\nleveraging temporal properties of speech components. This facilitates the\ndisentanglement of speaker identity and the recovery of phonetic and prosodic\ndetails lost during quantization. By applying only K-means quantization and\nlinear projections, our method achieves simple yet effective disentanglement,\nwithout requiring complex architectures or explicit supervision. This allows\nfor high-fidelity voice conversion trained solely with reconstruction losses.\nExperiments show that the proposed model outperforms existing methods across\nboth subjective and objective metrics. It achieves superior intelligibility and\nspeaker similarity, along with improved prosody preservation, highlighting the\nimpact of our Linear Disentangler module.\n", "link": "http://arxiv.org/abs/2411.16147v2", "date": "2025-09-10", "relevancy": 2.487, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5042}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QR-VC%3A%20Leveraging%20Quantization%20Residuals%20for%20Linear%20Disentanglement%20in%0A%20%20Zero-Shot%20Voice%20Conversion&body=Title%3A%20QR-VC%3A%20Leveraging%20Quantization%20Residuals%20for%20Linear%20Disentanglement%20in%0A%20%20Zero-Shot%20Voice%20Conversion%0AAuthor%3A%20Youngjun%20Sim%20and%20Jinsung%20Yoon%20and%20Wooyeol%20Jeong%20and%20Young-Joo%20Suh%0AAbstract%3A%20%20%20Zero-shot%20voice%20conversion%20is%20a%20technique%20that%20alters%20the%20speaker%20identity%20of%0Aan%20input%20speech%20to%20match%20a%20target%20speaker%20using%20only%20a%20single%20reference%0Autterance%2C%20without%20requiring%20additional%20training.%20Recent%20approaches%20extensively%0Autilize%20self-supervised%20learning%20features%20with%20K-means%20quantization%20to%20extract%0Ahigh-quality%20content%20representations%20while%20removing%20speaker%20identity.%20However%2C%0Athis%20quantization%20process%20also%20eliminates%20fine-grained%20phonetic%20and%20prosodic%0Avariations%2C%20degrading%20intelligibility%20and%20prosody%20preservation.%20While%20prior%0Aworks%20have%20primarily%20focused%20on%20quantized%20representations%2C%20quantization%0Aresiduals%20remain%20underutilized%20and%20deserve%20further%20exploration.%20In%20this%20paper%2C%0Awe%20introduce%20a%20novel%20approach%20that%20fully%20utilizes%20quantization%20residuals%20by%0Aleveraging%20temporal%20properties%20of%20speech%20components.%20This%20facilitates%20the%0Adisentanglement%20of%20speaker%20identity%20and%20the%20recovery%20of%20phonetic%20and%20prosodic%0Adetails%20lost%20during%20quantization.%20By%20applying%20only%20K-means%20quantization%20and%0Alinear%20projections%2C%20our%20method%20achieves%20simple%20yet%20effective%20disentanglement%2C%0Awithout%20requiring%20complex%20architectures%20or%20explicit%20supervision.%20This%20allows%0Afor%20high-fidelity%20voice%20conversion%20trained%20solely%20with%20reconstruction%20losses.%0AExperiments%20show%20that%20the%20proposed%20model%20outperforms%20existing%20methods%20across%0Aboth%20subjective%20and%20objective%20metrics.%20It%20achieves%20superior%20intelligibility%20and%0Aspeaker%20similarity%2C%20along%20with%20improved%20prosody%20preservation%2C%20highlighting%20the%0Aimpact%20of%20our%20Linear%20Disentangler%20module.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16147v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQR-VC%253A%2520Leveraging%2520Quantization%2520Residuals%2520for%2520Linear%2520Disentanglement%2520in%250A%2520%2520Zero-Shot%2520Voice%2520Conversion%26entry.906535625%3DYoungjun%2520Sim%2520and%2520Jinsung%2520Yoon%2520and%2520Wooyeol%2520Jeong%2520and%2520Young-Joo%2520Suh%26entry.1292438233%3D%2520%2520Zero-shot%2520voice%2520conversion%2520is%2520a%2520technique%2520that%2520alters%2520the%2520speaker%2520identity%2520of%250Aan%2520input%2520speech%2520to%2520match%2520a%2520target%2520speaker%2520using%2520only%2520a%2520single%2520reference%250Autterance%252C%2520without%2520requiring%2520additional%2520training.%2520Recent%2520approaches%2520extensively%250Autilize%2520self-supervised%2520learning%2520features%2520with%2520K-means%2520quantization%2520to%2520extract%250Ahigh-quality%2520content%2520representations%2520while%2520removing%2520speaker%2520identity.%2520However%252C%250Athis%2520quantization%2520process%2520also%2520eliminates%2520fine-grained%2520phonetic%2520and%2520prosodic%250Avariations%252C%2520degrading%2520intelligibility%2520and%2520prosody%2520preservation.%2520While%2520prior%250Aworks%2520have%2520primarily%2520focused%2520on%2520quantized%2520representations%252C%2520quantization%250Aresiduals%2520remain%2520underutilized%2520and%2520deserve%2520further%2520exploration.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520a%2520novel%2520approach%2520that%2520fully%2520utilizes%2520quantization%2520residuals%2520by%250Aleveraging%2520temporal%2520properties%2520of%2520speech%2520components.%2520This%2520facilitates%2520the%250Adisentanglement%2520of%2520speaker%2520identity%2520and%2520the%2520recovery%2520of%2520phonetic%2520and%2520prosodic%250Adetails%2520lost%2520during%2520quantization.%2520By%2520applying%2520only%2520K-means%2520quantization%2520and%250Alinear%2520projections%252C%2520our%2520method%2520achieves%2520simple%2520yet%2520effective%2520disentanglement%252C%250Awithout%2520requiring%2520complex%2520architectures%2520or%2520explicit%2520supervision.%2520This%2520allows%250Afor%2520high-fidelity%2520voice%2520conversion%2520trained%2520solely%2520with%2520reconstruction%2520losses.%250AExperiments%2520show%2520that%2520the%2520proposed%2520model%2520outperforms%2520existing%2520methods%2520across%250Aboth%2520subjective%2520and%2520objective%2520metrics.%2520It%2520achieves%2520superior%2520intelligibility%2520and%250Aspeaker%2520similarity%252C%2520along%2520with%2520improved%2520prosody%2520preservation%252C%2520highlighting%2520the%250Aimpact%2520of%2520our%2520Linear%2520Disentangler%2520module.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16147v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QR-VC%3A%20Leveraging%20Quantization%20Residuals%20for%20Linear%20Disentanglement%20in%0A%20%20Zero-Shot%20Voice%20Conversion&entry.906535625=Youngjun%20Sim%20and%20Jinsung%20Yoon%20and%20Wooyeol%20Jeong%20and%20Young-Joo%20Suh&entry.1292438233=%20%20Zero-shot%20voice%20conversion%20is%20a%20technique%20that%20alters%20the%20speaker%20identity%20of%0Aan%20input%20speech%20to%20match%20a%20target%20speaker%20using%20only%20a%20single%20reference%0Autterance%2C%20without%20requiring%20additional%20training.%20Recent%20approaches%20extensively%0Autilize%20self-supervised%20learning%20features%20with%20K-means%20quantization%20to%20extract%0Ahigh-quality%20content%20representations%20while%20removing%20speaker%20identity.%20However%2C%0Athis%20quantization%20process%20also%20eliminates%20fine-grained%20phonetic%20and%20prosodic%0Avariations%2C%20degrading%20intelligibility%20and%20prosody%20preservation.%20While%20prior%0Aworks%20have%20primarily%20focused%20on%20quantized%20representations%2C%20quantization%0Aresiduals%20remain%20underutilized%20and%20deserve%20further%20exploration.%20In%20this%20paper%2C%0Awe%20introduce%20a%20novel%20approach%20that%20fully%20utilizes%20quantization%20residuals%20by%0Aleveraging%20temporal%20properties%20of%20speech%20components.%20This%20facilitates%20the%0Adisentanglement%20of%20speaker%20identity%20and%20the%20recovery%20of%20phonetic%20and%20prosodic%0Adetails%20lost%20during%20quantization.%20By%20applying%20only%20K-means%20quantization%20and%0Alinear%20projections%2C%20our%20method%20achieves%20simple%20yet%20effective%20disentanglement%2C%0Awithout%20requiring%20complex%20architectures%20or%20explicit%20supervision.%20This%20allows%0Afor%20high-fidelity%20voice%20conversion%20trained%20solely%20with%20reconstruction%20losses.%0AExperiments%20show%20that%20the%20proposed%20model%20outperforms%20existing%20methods%20across%0Aboth%20subjective%20and%20objective%20metrics.%20It%20achieves%20superior%20intelligibility%20and%0Aspeaker%20similarity%2C%20along%20with%20improved%20prosody%20preservation%2C%20highlighting%20the%0Aimpact%20of%20our%20Linear%20Disentangler%20module.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16147v2&entry.124074799=Read"},
{"title": "GenFlow: Interactive Modular System for Image Generation", "author": "Duc-Hung Nguyen and Huu-Phuc Huynh and Minh-Triet Tran and Trung-Nghia Le", "abstract": "  Generative art unlocks boundless creative possibilities, yet its full\npotential remains untapped due to the technical expertise required for advanced\narchitectural concepts and computational workflows. To bridge this gap, we\npresent GenFlow, a novel modular framework that empowers users of all skill\nlevels to generate images with precision and ease. Featuring a node-based\neditor for seamless customization and an intelligent assistant powered by\nnatural language processing, GenFlow transforms the complexity of workflow\ncreation into an intuitive and accessible experience. By automating deployment\nprocesses and minimizing technical barriers, our framework makes cutting-edge\ngenerative art tools available to everyone. A user study demonstrated GenFlow's\nability to optimize workflows, reduce task completion times, and enhance user\nunderstanding through its intuitive interface and adaptive features. These\nresults position GenFlow as a groundbreaking solution that redefines\naccessibility and efficiency in the realm of generative art.\n", "link": "http://arxiv.org/abs/2506.21369v2", "date": "2025-09-10", "relevancy": 2.4632, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6294}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6116}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenFlow%3A%20Interactive%20Modular%20System%20for%20Image%20Generation&body=Title%3A%20GenFlow%3A%20Interactive%20Modular%20System%20for%20Image%20Generation%0AAuthor%3A%20Duc-Hung%20Nguyen%20and%20Huu-Phuc%20Huynh%20and%20Minh-Triet%20Tran%20and%20Trung-Nghia%20Le%0AAbstract%3A%20%20%20Generative%20art%20unlocks%20boundless%20creative%20possibilities%2C%20yet%20its%20full%0Apotential%20remains%20untapped%20due%20to%20the%20technical%20expertise%20required%20for%20advanced%0Aarchitectural%20concepts%20and%20computational%20workflows.%20To%20bridge%20this%20gap%2C%20we%0Apresent%20GenFlow%2C%20a%20novel%20modular%20framework%20that%20empowers%20users%20of%20all%20skill%0Alevels%20to%20generate%20images%20with%20precision%20and%20ease.%20Featuring%20a%20node-based%0Aeditor%20for%20seamless%20customization%20and%20an%20intelligent%20assistant%20powered%20by%0Anatural%20language%20processing%2C%20GenFlow%20transforms%20the%20complexity%20of%20workflow%0Acreation%20into%20an%20intuitive%20and%20accessible%20experience.%20By%20automating%20deployment%0Aprocesses%20and%20minimizing%20technical%20barriers%2C%20our%20framework%20makes%20cutting-edge%0Agenerative%20art%20tools%20available%20to%20everyone.%20A%20user%20study%20demonstrated%20GenFlow%27s%0Aability%20to%20optimize%20workflows%2C%20reduce%20task%20completion%20times%2C%20and%20enhance%20user%0Aunderstanding%20through%20its%20intuitive%20interface%20and%20adaptive%20features.%20These%0Aresults%20position%20GenFlow%20as%20a%20groundbreaking%20solution%20that%20redefines%0Aaccessibility%20and%20efficiency%20in%20the%20realm%20of%20generative%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21369v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenFlow%253A%2520Interactive%2520Modular%2520System%2520for%2520Image%2520Generation%26entry.906535625%3DDuc-Hung%2520Nguyen%2520and%2520Huu-Phuc%2520Huynh%2520and%2520Minh-Triet%2520Tran%2520and%2520Trung-Nghia%2520Le%26entry.1292438233%3D%2520%2520Generative%2520art%2520unlocks%2520boundless%2520creative%2520possibilities%252C%2520yet%2520its%2520full%250Apotential%2520remains%2520untapped%2520due%2520to%2520the%2520technical%2520expertise%2520required%2520for%2520advanced%250Aarchitectural%2520concepts%2520and%2520computational%2520workflows.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Apresent%2520GenFlow%252C%2520a%2520novel%2520modular%2520framework%2520that%2520empowers%2520users%2520of%2520all%2520skill%250Alevels%2520to%2520generate%2520images%2520with%2520precision%2520and%2520ease.%2520Featuring%2520a%2520node-based%250Aeditor%2520for%2520seamless%2520customization%2520and%2520an%2520intelligent%2520assistant%2520powered%2520by%250Anatural%2520language%2520processing%252C%2520GenFlow%2520transforms%2520the%2520complexity%2520of%2520workflow%250Acreation%2520into%2520an%2520intuitive%2520and%2520accessible%2520experience.%2520By%2520automating%2520deployment%250Aprocesses%2520and%2520minimizing%2520technical%2520barriers%252C%2520our%2520framework%2520makes%2520cutting-edge%250Agenerative%2520art%2520tools%2520available%2520to%2520everyone.%2520A%2520user%2520study%2520demonstrated%2520GenFlow%2527s%250Aability%2520to%2520optimize%2520workflows%252C%2520reduce%2520task%2520completion%2520times%252C%2520and%2520enhance%2520user%250Aunderstanding%2520through%2520its%2520intuitive%2520interface%2520and%2520adaptive%2520features.%2520These%250Aresults%2520position%2520GenFlow%2520as%2520a%2520groundbreaking%2520solution%2520that%2520redefines%250Aaccessibility%2520and%2520efficiency%2520in%2520the%2520realm%2520of%2520generative%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21369v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenFlow%3A%20Interactive%20Modular%20System%20for%20Image%20Generation&entry.906535625=Duc-Hung%20Nguyen%20and%20Huu-Phuc%20Huynh%20and%20Minh-Triet%20Tran%20and%20Trung-Nghia%20Le&entry.1292438233=%20%20Generative%20art%20unlocks%20boundless%20creative%20possibilities%2C%20yet%20its%20full%0Apotential%20remains%20untapped%20due%20to%20the%20technical%20expertise%20required%20for%20advanced%0Aarchitectural%20concepts%20and%20computational%20workflows.%20To%20bridge%20this%20gap%2C%20we%0Apresent%20GenFlow%2C%20a%20novel%20modular%20framework%20that%20empowers%20users%20of%20all%20skill%0Alevels%20to%20generate%20images%20with%20precision%20and%20ease.%20Featuring%20a%20node-based%0Aeditor%20for%20seamless%20customization%20and%20an%20intelligent%20assistant%20powered%20by%0Anatural%20language%20processing%2C%20GenFlow%20transforms%20the%20complexity%20of%20workflow%0Acreation%20into%20an%20intuitive%20and%20accessible%20experience.%20By%20automating%20deployment%0Aprocesses%20and%20minimizing%20technical%20barriers%2C%20our%20framework%20makes%20cutting-edge%0Agenerative%20art%20tools%20available%20to%20everyone.%20A%20user%20study%20demonstrated%20GenFlow%27s%0Aability%20to%20optimize%20workflows%2C%20reduce%20task%20completion%20times%2C%20and%20enhance%20user%0Aunderstanding%20through%20its%20intuitive%20interface%20and%20adaptive%20features.%20These%0Aresults%20position%20GenFlow%20as%20a%20groundbreaking%20solution%20that%20redefines%0Aaccessibility%20and%20efficiency%20in%20the%20realm%20of%20generative%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21369v2&entry.124074799=Read"},
{"title": "Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper\n  for Speech Emotion Recognition", "author": "Yujian Ma and Jinqiu Sang and Ruizhe Li", "abstract": "  Large pre-trained speech models such as Whisper offer strong generalization\nbut pose significant challenges for resource-efficient adaptation. Low-Rank\nAdaptation (LoRA) has become a popular parameter-efficient fine-tuning method,\nyet its underlying mechanisms in speech tasks remain poorly understood. In this\nwork, we conduct the first systematic mechanistic interpretability study of\nLoRA within the Whisper encoder for speech emotion recognition (SER). Using a\nsuite of analytical tools, including layer contribution probing, logit-lens\ninspection, and representational similarity via singular value decomposition\n(SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a\ndelayed specialization process that preserves general features in early layers\nbefore consolidating task-specific information, and a forward alignment,\nbackward differentiation dynamic between LoRA's matrices. Our findings clarify\nhow LoRA reshapes encoder hierarchies, providing both empirical insights and a\ndeeper mechanistic understanding for designing efficient and interpretable\nadaptation strategies in large speech models.\n", "link": "http://arxiv.org/abs/2509.08454v1", "date": "2025-09-10", "relevancy": 2.4569, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Behind%20the%20Scenes%3A%20Mechanistic%20Interpretability%20of%20LoRA-adapted%20Whisper%0A%20%20for%20Speech%20Emotion%20Recognition&body=Title%3A%20Behind%20the%20Scenes%3A%20Mechanistic%20Interpretability%20of%20LoRA-adapted%20Whisper%0A%20%20for%20Speech%20Emotion%20Recognition%0AAuthor%3A%20Yujian%20Ma%20and%20Jinqiu%20Sang%20and%20Ruizhe%20Li%0AAbstract%3A%20%20%20Large%20pre-trained%20speech%20models%20such%20as%20Whisper%20offer%20strong%20generalization%0Abut%20pose%20significant%20challenges%20for%20resource-efficient%20adaptation.%20Low-Rank%0AAdaptation%20%28LoRA%29%20has%20become%20a%20popular%20parameter-efficient%20fine-tuning%20method%2C%0Ayet%20its%20underlying%20mechanisms%20in%20speech%20tasks%20remain%20poorly%20understood.%20In%20this%0Awork%2C%20we%20conduct%20the%20first%20systematic%20mechanistic%20interpretability%20study%20of%0ALoRA%20within%20the%20Whisper%20encoder%20for%20speech%20emotion%20recognition%20%28SER%29.%20Using%20a%0Asuite%20of%20analytical%20tools%2C%20including%20layer%20contribution%20probing%2C%20logit-lens%0Ainspection%2C%20and%20representational%20similarity%20via%20singular%20value%20decomposition%0A%28SVD%29%20and%20centered%20kernel%20alignment%20%28CKA%29%2C%20we%20reveal%20two%20key%20mechanisms%3A%20a%0Adelayed%20specialization%20process%20that%20preserves%20general%20features%20in%20early%20layers%0Abefore%20consolidating%20task-specific%20information%2C%20and%20a%20forward%20alignment%2C%0Abackward%20differentiation%20dynamic%20between%20LoRA%27s%20matrices.%20Our%20findings%20clarify%0Ahow%20LoRA%20reshapes%20encoder%20hierarchies%2C%20providing%20both%20empirical%20insights%20and%20a%0Adeeper%20mechanistic%20understanding%20for%20designing%20efficient%20and%20interpretable%0Aadaptation%20strategies%20in%20large%20speech%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehind%2520the%2520Scenes%253A%2520Mechanistic%2520Interpretability%2520of%2520LoRA-adapted%2520Whisper%250A%2520%2520for%2520Speech%2520Emotion%2520Recognition%26entry.906535625%3DYujian%2520Ma%2520and%2520Jinqiu%2520Sang%2520and%2520Ruizhe%2520Li%26entry.1292438233%3D%2520%2520Large%2520pre-trained%2520speech%2520models%2520such%2520as%2520Whisper%2520offer%2520strong%2520generalization%250Abut%2520pose%2520significant%2520challenges%2520for%2520resource-efficient%2520adaptation.%2520Low-Rank%250AAdaptation%2520%2528LoRA%2529%2520has%2520become%2520a%2520popular%2520parameter-efficient%2520fine-tuning%2520method%252C%250Ayet%2520its%2520underlying%2520mechanisms%2520in%2520speech%2520tasks%2520remain%2520poorly%2520understood.%2520In%2520this%250Awork%252C%2520we%2520conduct%2520the%2520first%2520systematic%2520mechanistic%2520interpretability%2520study%2520of%250ALoRA%2520within%2520the%2520Whisper%2520encoder%2520for%2520speech%2520emotion%2520recognition%2520%2528SER%2529.%2520Using%2520a%250Asuite%2520of%2520analytical%2520tools%252C%2520including%2520layer%2520contribution%2520probing%252C%2520logit-lens%250Ainspection%252C%2520and%2520representational%2520similarity%2520via%2520singular%2520value%2520decomposition%250A%2528SVD%2529%2520and%2520centered%2520kernel%2520alignment%2520%2528CKA%2529%252C%2520we%2520reveal%2520two%2520key%2520mechanisms%253A%2520a%250Adelayed%2520specialization%2520process%2520that%2520preserves%2520general%2520features%2520in%2520early%2520layers%250Abefore%2520consolidating%2520task-specific%2520information%252C%2520and%2520a%2520forward%2520alignment%252C%250Abackward%2520differentiation%2520dynamic%2520between%2520LoRA%2527s%2520matrices.%2520Our%2520findings%2520clarify%250Ahow%2520LoRA%2520reshapes%2520encoder%2520hierarchies%252C%2520providing%2520both%2520empirical%2520insights%2520and%2520a%250Adeeper%2520mechanistic%2520understanding%2520for%2520designing%2520efficient%2520and%2520interpretable%250Aadaptation%2520strategies%2520in%2520large%2520speech%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Behind%20the%20Scenes%3A%20Mechanistic%20Interpretability%20of%20LoRA-adapted%20Whisper%0A%20%20for%20Speech%20Emotion%20Recognition&entry.906535625=Yujian%20Ma%20and%20Jinqiu%20Sang%20and%20Ruizhe%20Li&entry.1292438233=%20%20Large%20pre-trained%20speech%20models%20such%20as%20Whisper%20offer%20strong%20generalization%0Abut%20pose%20significant%20challenges%20for%20resource-efficient%20adaptation.%20Low-Rank%0AAdaptation%20%28LoRA%29%20has%20become%20a%20popular%20parameter-efficient%20fine-tuning%20method%2C%0Ayet%20its%20underlying%20mechanisms%20in%20speech%20tasks%20remain%20poorly%20understood.%20In%20this%0Awork%2C%20we%20conduct%20the%20first%20systematic%20mechanistic%20interpretability%20study%20of%0ALoRA%20within%20the%20Whisper%20encoder%20for%20speech%20emotion%20recognition%20%28SER%29.%20Using%20a%0Asuite%20of%20analytical%20tools%2C%20including%20layer%20contribution%20probing%2C%20logit-lens%0Ainspection%2C%20and%20representational%20similarity%20via%20singular%20value%20decomposition%0A%28SVD%29%20and%20centered%20kernel%20alignment%20%28CKA%29%2C%20we%20reveal%20two%20key%20mechanisms%3A%20a%0Adelayed%20specialization%20process%20that%20preserves%20general%20features%20in%20early%20layers%0Abefore%20consolidating%20task-specific%20information%2C%20and%20a%20forward%20alignment%2C%0Abackward%20differentiation%20dynamic%20between%20LoRA%27s%20matrices.%20Our%20findings%20clarify%0Ahow%20LoRA%20reshapes%20encoder%20hierarchies%2C%20providing%20both%20empirical%20insights%20and%20a%0Adeeper%20mechanistic%20understanding%20for%20designing%20efficient%20and%20interpretable%0Aadaptation%20strategies%20in%20large%20speech%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08454v1&entry.124074799=Read"},
{"title": "Skeleton-based sign language recognition using a dual-stream\n  spatio-temporal dynamic graph convolutional network", "author": "Liangjin Liu and Haoyang Zheng and Pei Zhou", "abstract": "  Isolated Sign Language Recognition (ISLR) is challenged by gestures that are\nmorphologically similar yet semantically distinct, a problem rooted in the\ncomplex interplay between hand shape and motion trajectory. Existing methods,\noften relying on a single reference frame, struggle to resolve this geometric\nambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a\ndual-reference, dual-stream architecture that decouples and models gesture\nmorphology and trajectory in separate, complementary coordinate systems. Our\napproach utilizes a wrist-centric frame for view-invariant shape analysis and a\nfacial-centric frame for context-aware trajectory modeling. These streams are\nprocessed by specialized networks-a topology-aware graph convolution for shape\nand a Finsler geometry-based encoder for trajectory-and are integrated via a\ngeometry-driven optimal transport fusion mechanism. DSLNet sets a new\nstate-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the\nchallenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with\nsignificantly fewer parameters than competing models.\n", "link": "http://arxiv.org/abs/2509.08661v1", "date": "2025-09-10", "relevancy": 2.4459, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4928}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skeleton-based%20sign%20language%20recognition%20using%20a%20dual-stream%0A%20%20spatio-temporal%20dynamic%20graph%20convolutional%20network&body=Title%3A%20Skeleton-based%20sign%20language%20recognition%20using%20a%20dual-stream%0A%20%20spatio-temporal%20dynamic%20graph%20convolutional%20network%0AAuthor%3A%20Liangjin%20Liu%20and%20Haoyang%20Zheng%20and%20Pei%20Zhou%0AAbstract%3A%20%20%20Isolated%20Sign%20Language%20Recognition%20%28ISLR%29%20is%20challenged%20by%20gestures%20that%20are%0Amorphologically%20similar%20yet%20semantically%20distinct%2C%20a%20problem%20rooted%20in%20the%0Acomplex%20interplay%20between%20hand%20shape%20and%20motion%20trajectory.%20Existing%20methods%2C%0Aoften%20relying%20on%20a%20single%20reference%20frame%2C%20struggle%20to%20resolve%20this%20geometric%0Aambiguity.%20This%20paper%20introduces%20Dual-SignLanguageNet%20%28DSLNet%29%2C%20a%0Adual-reference%2C%20dual-stream%20architecture%20that%20decouples%20and%20models%20gesture%0Amorphology%20and%20trajectory%20in%20separate%2C%20complementary%20coordinate%20systems.%20Our%0Aapproach%20utilizes%20a%20wrist-centric%20frame%20for%20view-invariant%20shape%20analysis%20and%20a%0Afacial-centric%20frame%20for%20context-aware%20trajectory%20modeling.%20These%20streams%20are%0Aprocessed%20by%20specialized%20networks-a%20topology-aware%20graph%20convolution%20for%20shape%0Aand%20a%20Finsler%20geometry-based%20encoder%20for%20trajectory-and%20are%20integrated%20via%20a%0Ageometry-driven%20optimal%20transport%20fusion%20mechanism.%20DSLNet%20sets%20a%20new%0Astate-of-the-art%2C%20achieving%2093.70%25%2C%2089.97%25%20and%2099.79%25%20accuracy%20on%20the%0Achallenging%20WLASL-100%2C%20WLASL-300%20and%20LSA64%20datasets%2C%20respectively%2C%20with%0Asignificantly%20fewer%20parameters%20than%20competing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkeleton-based%2520sign%2520language%2520recognition%2520using%2520a%2520dual-stream%250A%2520%2520spatio-temporal%2520dynamic%2520graph%2520convolutional%2520network%26entry.906535625%3DLiangjin%2520Liu%2520and%2520Haoyang%2520Zheng%2520and%2520Pei%2520Zhou%26entry.1292438233%3D%2520%2520Isolated%2520Sign%2520Language%2520Recognition%2520%2528ISLR%2529%2520is%2520challenged%2520by%2520gestures%2520that%2520are%250Amorphologically%2520similar%2520yet%2520semantically%2520distinct%252C%2520a%2520problem%2520rooted%2520in%2520the%250Acomplex%2520interplay%2520between%2520hand%2520shape%2520and%2520motion%2520trajectory.%2520Existing%2520methods%252C%250Aoften%2520relying%2520on%2520a%2520single%2520reference%2520frame%252C%2520struggle%2520to%2520resolve%2520this%2520geometric%250Aambiguity.%2520This%2520paper%2520introduces%2520Dual-SignLanguageNet%2520%2528DSLNet%2529%252C%2520a%250Adual-reference%252C%2520dual-stream%2520architecture%2520that%2520decouples%2520and%2520models%2520gesture%250Amorphology%2520and%2520trajectory%2520in%2520separate%252C%2520complementary%2520coordinate%2520systems.%2520Our%250Aapproach%2520utilizes%2520a%2520wrist-centric%2520frame%2520for%2520view-invariant%2520shape%2520analysis%2520and%2520a%250Afacial-centric%2520frame%2520for%2520context-aware%2520trajectory%2520modeling.%2520These%2520streams%2520are%250Aprocessed%2520by%2520specialized%2520networks-a%2520topology-aware%2520graph%2520convolution%2520for%2520shape%250Aand%2520a%2520Finsler%2520geometry-based%2520encoder%2520for%2520trajectory-and%2520are%2520integrated%2520via%2520a%250Ageometry-driven%2520optimal%2520transport%2520fusion%2520mechanism.%2520DSLNet%2520sets%2520a%2520new%250Astate-of-the-art%252C%2520achieving%252093.70%2525%252C%252089.97%2525%2520and%252099.79%2525%2520accuracy%2520on%2520the%250Achallenging%2520WLASL-100%252C%2520WLASL-300%2520and%2520LSA64%2520datasets%252C%2520respectively%252C%2520with%250Asignificantly%2520fewer%2520parameters%2520than%2520competing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skeleton-based%20sign%20language%20recognition%20using%20a%20dual-stream%0A%20%20spatio-temporal%20dynamic%20graph%20convolutional%20network&entry.906535625=Liangjin%20Liu%20and%20Haoyang%20Zheng%20and%20Pei%20Zhou&entry.1292438233=%20%20Isolated%20Sign%20Language%20Recognition%20%28ISLR%29%20is%20challenged%20by%20gestures%20that%20are%0Amorphologically%20similar%20yet%20semantically%20distinct%2C%20a%20problem%20rooted%20in%20the%0Acomplex%20interplay%20between%20hand%20shape%20and%20motion%20trajectory.%20Existing%20methods%2C%0Aoften%20relying%20on%20a%20single%20reference%20frame%2C%20struggle%20to%20resolve%20this%20geometric%0Aambiguity.%20This%20paper%20introduces%20Dual-SignLanguageNet%20%28DSLNet%29%2C%20a%0Adual-reference%2C%20dual-stream%20architecture%20that%20decouples%20and%20models%20gesture%0Amorphology%20and%20trajectory%20in%20separate%2C%20complementary%20coordinate%20systems.%20Our%0Aapproach%20utilizes%20a%20wrist-centric%20frame%20for%20view-invariant%20shape%20analysis%20and%20a%0Afacial-centric%20frame%20for%20context-aware%20trajectory%20modeling.%20These%20streams%20are%0Aprocessed%20by%20specialized%20networks-a%20topology-aware%20graph%20convolution%20for%20shape%0Aand%20a%20Finsler%20geometry-based%20encoder%20for%20trajectory-and%20are%20integrated%20via%20a%0Ageometry-driven%20optimal%20transport%20fusion%20mechanism.%20DSLNet%20sets%20a%20new%0Astate-of-the-art%2C%20achieving%2093.70%25%2C%2089.97%25%20and%2099.79%25%20accuracy%20on%20the%0Achallenging%20WLASL-100%2C%20WLASL-300%20and%20LSA64%20datasets%2C%20respectively%2C%20with%0Asignificantly%20fewer%20parameters%20than%20competing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08661v1&entry.124074799=Read"},
{"title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation", "author": "Yang Zhou and Shiyu Zhao and Yuxiao Chen and Zhenting Wang and Can Jin and Dimitris N. Metaxas", "abstract": "  Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.\n", "link": "http://arxiv.org/abs/2503.13794v5", "date": "2025-09-10", "relevancy": 2.4391, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6104}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LED%3A%20LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%20without%20Human%20Curated%0A%20%20Data%20Generation&body=Title%3A%20LED%3A%20LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%20without%20Human%20Curated%0A%20%20Data%20Generation%0AAuthor%3A%20Yang%20Zhou%20and%20Shiyu%20Zhao%20and%20Yuxiao%20Chen%20and%20Zhenting%20Wang%20and%20Can%20Jin%20and%20Dimitris%20N.%20Metaxas%0AAbstract%3A%20%20%20Large%20foundation%20models%20trained%20on%20large-scale%20vision-language%20data%20can%20boost%0AOpen-Vocabulary%20Object%20Detection%20%28OVD%29%20via%20synthetic%20training%20data%2C%20yet%20the%0Ahand-crafted%20pipelines%20often%20introduce%20bias%20and%20overfit%20to%20specific%20prompts.%20We%0Asidestep%20this%20issue%20by%20directly%20fusing%20hidden%20states%20from%20Large%20Language%20Models%0A%28LLMs%29%20into%20detectors-an%20avenue%20surprisingly%20under-explored.%20This%20paper%0Apresents%20a%20systematic%20method%20to%20enhance%20visual%20grounding%20by%20utilizing%20decoder%0Alayers%20of%20the%20LLM%20of%20an%20MLLM.%20We%20introduce%20a%20zero-initialized%20cross-attention%0Aadapter%20to%20enable%20efficient%20knowledge%20fusion%20from%20LLMs%20to%20object%20detectors%2C%20a%0Anew%20approach%20called%20LED%20%28LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%29.%20We%0Afind%20that%20intermediate%20LLM%20layers%20already%20encode%20rich%20spatial%20semantics%3B%0Aadapting%20only%20the%20early%20layers%20yields%20most%20of%20the%20gain.%20With%20Swin-T%20as%20the%0Avision%20encoder%2C%20Qwen2-0.5B%20%2B%20LED%20lifts%20GroundingDINO%20by%203.82%20%25%20on%20OmniLabel%20at%0Ajust%208.7%20%25%20extra%20GFLOPs%2C%20and%20a%20larger%20vision%20backbone%20pushes%20the%20improvement%20to%0A6.22%20%25.%20Extensive%20ablations%20on%20adapter%20variants%2C%20LLM%20scales%20and%20fusion%20depths%0Afurther%20corroborate%20our%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.13794v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLED%253A%2520LLM%2520Enhanced%2520Open-Vocabulary%2520Object%2520Detection%2520without%2520Human%2520Curated%250A%2520%2520Data%2520Generation%26entry.906535625%3DYang%2520Zhou%2520and%2520Shiyu%2520Zhao%2520and%2520Yuxiao%2520Chen%2520and%2520Zhenting%2520Wang%2520and%2520Can%2520Jin%2520and%2520Dimitris%2520N.%2520Metaxas%26entry.1292438233%3D%2520%2520Large%2520foundation%2520models%2520trained%2520on%2520large-scale%2520vision-language%2520data%2520can%2520boost%250AOpen-Vocabulary%2520Object%2520Detection%2520%2528OVD%2529%2520via%2520synthetic%2520training%2520data%252C%2520yet%2520the%250Ahand-crafted%2520pipelines%2520often%2520introduce%2520bias%2520and%2520overfit%2520to%2520specific%2520prompts.%2520We%250Asidestep%2520this%2520issue%2520by%2520directly%2520fusing%2520hidden%2520states%2520from%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520into%2520detectors-an%2520avenue%2520surprisingly%2520under-explored.%2520This%2520paper%250Apresents%2520a%2520systematic%2520method%2520to%2520enhance%2520visual%2520grounding%2520by%2520utilizing%2520decoder%250Alayers%2520of%2520the%2520LLM%2520of%2520an%2520MLLM.%2520We%2520introduce%2520a%2520zero-initialized%2520cross-attention%250Aadapter%2520to%2520enable%2520efficient%2520knowledge%2520fusion%2520from%2520LLMs%2520to%2520object%2520detectors%252C%2520a%250Anew%2520approach%2520called%2520LED%2520%2528LLM%2520Enhanced%2520Open-Vocabulary%2520Object%2520Detection%2529.%2520We%250Afind%2520that%2520intermediate%2520LLM%2520layers%2520already%2520encode%2520rich%2520spatial%2520semantics%253B%250Aadapting%2520only%2520the%2520early%2520layers%2520yields%2520most%2520of%2520the%2520gain.%2520With%2520Swin-T%2520as%2520the%250Avision%2520encoder%252C%2520Qwen2-0.5B%2520%252B%2520LED%2520lifts%2520GroundingDINO%2520by%25203.82%2520%2525%2520on%2520OmniLabel%2520at%250Ajust%25208.7%2520%2525%2520extra%2520GFLOPs%252C%2520and%2520a%2520larger%2520vision%2520backbone%2520pushes%2520the%2520improvement%2520to%250A6.22%2520%2525.%2520Extensive%2520ablations%2520on%2520adapter%2520variants%252C%2520LLM%2520scales%2520and%2520fusion%2520depths%250Afurther%2520corroborate%2520our%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13794v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LED%3A%20LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%20without%20Human%20Curated%0A%20%20Data%20Generation&entry.906535625=Yang%20Zhou%20and%20Shiyu%20Zhao%20and%20Yuxiao%20Chen%20and%20Zhenting%20Wang%20and%20Can%20Jin%20and%20Dimitris%20N.%20Metaxas&entry.1292438233=%20%20Large%20foundation%20models%20trained%20on%20large-scale%20vision-language%20data%20can%20boost%0AOpen-Vocabulary%20Object%20Detection%20%28OVD%29%20via%20synthetic%20training%20data%2C%20yet%20the%0Ahand-crafted%20pipelines%20often%20introduce%20bias%20and%20overfit%20to%20specific%20prompts.%20We%0Asidestep%20this%20issue%20by%20directly%20fusing%20hidden%20states%20from%20Large%20Language%20Models%0A%28LLMs%29%20into%20detectors-an%20avenue%20surprisingly%20under-explored.%20This%20paper%0Apresents%20a%20systematic%20method%20to%20enhance%20visual%20grounding%20by%20utilizing%20decoder%0Alayers%20of%20the%20LLM%20of%20an%20MLLM.%20We%20introduce%20a%20zero-initialized%20cross-attention%0Aadapter%20to%20enable%20efficient%20knowledge%20fusion%20from%20LLMs%20to%20object%20detectors%2C%20a%0Anew%20approach%20called%20LED%20%28LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%29.%20We%0Afind%20that%20intermediate%20LLM%20layers%20already%20encode%20rich%20spatial%20semantics%3B%0Aadapting%20only%20the%20early%20layers%20yields%20most%20of%20the%20gain.%20With%20Swin-T%20as%20the%0Avision%20encoder%2C%20Qwen2-0.5B%20%2B%20LED%20lifts%20GroundingDINO%20by%203.82%20%25%20on%20OmniLabel%20at%0Ajust%208.7%20%25%20extra%20GFLOPs%2C%20and%20a%20larger%20vision%20backbone%20pushes%20the%20improvement%20to%0A6.22%20%25.%20Extensive%20ablations%20on%20adapter%20variants%2C%20LLM%20scales%20and%20fusion%20depths%0Afurther%20corroborate%20our%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.13794v5&entry.124074799=Read"},
{"title": "HOFT: Householder Orthogonal Fine-tuning", "author": "Alejandro Moreno Arcas and Albert Sanchis and Jorge Civera and Alfons Juan", "abstract": "  Adaptation of foundation models using low-rank methods is a widespread\napproach. Another way to adapt these models is to employ orthogonal fine-tuning\nmethods, which are less time and memory efficient despite their good\ngeneralization properties. In this work, we propose Householder Orthogonal\nFine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to\nalleviate time and space complexity. Moreover, some theoretical properties of\nthe orthogonal fine-tuning paradigm are explored. From this exploration, Scaled\nHouseholder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are\nevaluated in downstream tasks, namely commonsense reasoning, machine\ntranslation, subject-driven generation and mathematical reasoning. Compared\nwith state-of-the-art adaptation methods, HOFT and SHOFT show comparable or\nbetter results.\n", "link": "http://arxiv.org/abs/2505.16531v2", "date": "2025-09-10", "relevancy": 2.4385, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5136}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4774}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOFT%3A%20Householder%20Orthogonal%20Fine-tuning&body=Title%3A%20HOFT%3A%20Householder%20Orthogonal%20Fine-tuning%0AAuthor%3A%20Alejandro%20Moreno%20Arcas%20and%20Albert%20Sanchis%20and%20Jorge%20Civera%20and%20Alfons%20Juan%0AAbstract%3A%20%20%20Adaptation%20of%20foundation%20models%20using%20low-rank%20methods%20is%20a%20widespread%0Aapproach.%20Another%20way%20to%20adapt%20these%20models%20is%20to%20employ%20orthogonal%20fine-tuning%0Amethods%2C%20which%20are%20less%20time%20and%20memory%20efficient%20despite%20their%20good%0Ageneralization%20properties.%20In%20this%20work%2C%20we%20propose%20Householder%20Orthogonal%0AFine-tuning%20%28HOFT%29%2C%20a%20novel%20orthogonal%20fine-tuning%20method%20that%20aims%20to%0Aalleviate%20time%20and%20space%20complexity.%20Moreover%2C%20some%20theoretical%20properties%20of%0Athe%20orthogonal%20fine-tuning%20paradigm%20are%20explored.%20From%20this%20exploration%2C%20Scaled%0AHouseholder%20Orthogonal%20Fine-tuning%20%28SHOFT%29%20is%20proposed.%20Both%20HOFT%20and%20SHOFT%20are%0Aevaluated%20in%20downstream%20tasks%2C%20namely%20commonsense%20reasoning%2C%20machine%0Atranslation%2C%20subject-driven%20generation%20and%20mathematical%20reasoning.%20Compared%0Awith%20state-of-the-art%20adaptation%20methods%2C%20HOFT%20and%20SHOFT%20show%20comparable%20or%0Abetter%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16531v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOFT%253A%2520Householder%2520Orthogonal%2520Fine-tuning%26entry.906535625%3DAlejandro%2520Moreno%2520Arcas%2520and%2520Albert%2520Sanchis%2520and%2520Jorge%2520Civera%2520and%2520Alfons%2520Juan%26entry.1292438233%3D%2520%2520Adaptation%2520of%2520foundation%2520models%2520using%2520low-rank%2520methods%2520is%2520a%2520widespread%250Aapproach.%2520Another%2520way%2520to%2520adapt%2520these%2520models%2520is%2520to%2520employ%2520orthogonal%2520fine-tuning%250Amethods%252C%2520which%2520are%2520less%2520time%2520and%2520memory%2520efficient%2520despite%2520their%2520good%250Ageneralization%2520properties.%2520In%2520this%2520work%252C%2520we%2520propose%2520Householder%2520Orthogonal%250AFine-tuning%2520%2528HOFT%2529%252C%2520a%2520novel%2520orthogonal%2520fine-tuning%2520method%2520that%2520aims%2520to%250Aalleviate%2520time%2520and%2520space%2520complexity.%2520Moreover%252C%2520some%2520theoretical%2520properties%2520of%250Athe%2520orthogonal%2520fine-tuning%2520paradigm%2520are%2520explored.%2520From%2520this%2520exploration%252C%2520Scaled%250AHouseholder%2520Orthogonal%2520Fine-tuning%2520%2528SHOFT%2529%2520is%2520proposed.%2520Both%2520HOFT%2520and%2520SHOFT%2520are%250Aevaluated%2520in%2520downstream%2520tasks%252C%2520namely%2520commonsense%2520reasoning%252C%2520machine%250Atranslation%252C%2520subject-driven%2520generation%2520and%2520mathematical%2520reasoning.%2520Compared%250Awith%2520state-of-the-art%2520adaptation%2520methods%252C%2520HOFT%2520and%2520SHOFT%2520show%2520comparable%2520or%250Abetter%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16531v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOFT%3A%20Householder%20Orthogonal%20Fine-tuning&entry.906535625=Alejandro%20Moreno%20Arcas%20and%20Albert%20Sanchis%20and%20Jorge%20Civera%20and%20Alfons%20Juan&entry.1292438233=%20%20Adaptation%20of%20foundation%20models%20using%20low-rank%20methods%20is%20a%20widespread%0Aapproach.%20Another%20way%20to%20adapt%20these%20models%20is%20to%20employ%20orthogonal%20fine-tuning%0Amethods%2C%20which%20are%20less%20time%20and%20memory%20efficient%20despite%20their%20good%0Ageneralization%20properties.%20In%20this%20work%2C%20we%20propose%20Householder%20Orthogonal%0AFine-tuning%20%28HOFT%29%2C%20a%20novel%20orthogonal%20fine-tuning%20method%20that%20aims%20to%0Aalleviate%20time%20and%20space%20complexity.%20Moreover%2C%20some%20theoretical%20properties%20of%0Athe%20orthogonal%20fine-tuning%20paradigm%20are%20explored.%20From%20this%20exploration%2C%20Scaled%0AHouseholder%20Orthogonal%20Fine-tuning%20%28SHOFT%29%20is%20proposed.%20Both%20HOFT%20and%20SHOFT%20are%0Aevaluated%20in%20downstream%20tasks%2C%20namely%20commonsense%20reasoning%2C%20machine%0Atranslation%2C%20subject-driven%20generation%20and%20mathematical%20reasoning.%20Compared%0Awith%20state-of-the-art%20adaptation%20methods%2C%20HOFT%20and%20SHOFT%20show%20comparable%20or%0Abetter%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16531v2&entry.124074799=Read"},
{"title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot\n  Navigation", "author": "Michael J. Munje and Chen Tang and Shuijing Liu and Zichao Hu and Yifeng Zhu and Jiaxun Cui and Garrett Warnell and Joydeep Biswas and Peter Stone", "abstract": "  Robot navigation in dynamic, human-centered environments requires\nsocially-compliant decisions grounded in robust scene understanding. Recent\nVision-Language Models (VLMs) exhibit promising capabilities such as object\nrecognition, common-sense reasoning, and contextual understanding-capabilities\nthat align with the nuanced requirements of social robot navigation. However,\nit remains unclear whether VLMs can accurately understand complex social\nnavigation scenes (e.g., inferring the spatial-temporal relations among agents\nand human intentions), which is essential for safe and socially compliant robot\nnavigation. While some recent works have explored the use of VLMs in social\nrobot navigation, no existing work systematically evaluates their ability to\nmeet these necessary conditions. In this paper, we introduce the Social\nNavigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question\nAnswering (VQA) dataset and benchmark designed to evaluate VLMs for scene\nunderstanding in real-world social robot navigation scenarios. SocialNav-SUB\nprovides a unified framework for evaluating VLMs against human and rule-based\nbaselines across VQA tasks requiring spatial, spatiotemporal, and social\nreasoning in social robot navigation. Through experiments with state-of-the-art\nVLMs, we find that while the best-performing VLM achieves an encouraging\nprobability of agreeing with human answers, it still underperforms simpler\nrule-based approach and human consensus baselines, indicating critical gaps in\nsocial scene understanding of current VLMs. Our benchmark sets the stage for\nfurther research on foundation models for social robot navigation, offering a\nframework to explore how VLMs can be tailored to meet real-world social robot\nnavigation needs. An overview of this paper along with the code and data can be\nfound at https://larg.github.io/socialnav-sub .\n", "link": "http://arxiv.org/abs/2509.08757v1", "date": "2025-09-10", "relevancy": 2.4109, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6095}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SocialNav-SUB%3A%20Benchmarking%20VLMs%20for%20Scene%20Understanding%20in%20Social%20Robot%0A%20%20Navigation&body=Title%3A%20SocialNav-SUB%3A%20Benchmarking%20VLMs%20for%20Scene%20Understanding%20in%20Social%20Robot%0A%20%20Navigation%0AAuthor%3A%20Michael%20J.%20Munje%20and%20Chen%20Tang%20and%20Shuijing%20Liu%20and%20Zichao%20Hu%20and%20Yifeng%20Zhu%20and%20Jiaxun%20Cui%20and%20Garrett%20Warnell%20and%20Joydeep%20Biswas%20and%20Peter%20Stone%0AAbstract%3A%20%20%20Robot%20navigation%20in%20dynamic%2C%20human-centered%20environments%20requires%0Asocially-compliant%20decisions%20grounded%20in%20robust%20scene%20understanding.%20Recent%0AVision-Language%20Models%20%28VLMs%29%20exhibit%20promising%20capabilities%20such%20as%20object%0Arecognition%2C%20common-sense%20reasoning%2C%20and%20contextual%20understanding-capabilities%0Athat%20align%20with%20the%20nuanced%20requirements%20of%20social%20robot%20navigation.%20However%2C%0Ait%20remains%20unclear%20whether%20VLMs%20can%20accurately%20understand%20complex%20social%0Anavigation%20scenes%20%28e.g.%2C%20inferring%20the%20spatial-temporal%20relations%20among%20agents%0Aand%20human%20intentions%29%2C%20which%20is%20essential%20for%20safe%20and%20socially%20compliant%20robot%0Anavigation.%20While%20some%20recent%20works%20have%20explored%20the%20use%20of%20VLMs%20in%20social%0Arobot%20navigation%2C%20no%20existing%20work%20systematically%20evaluates%20their%20ability%20to%0Ameet%20these%20necessary%20conditions.%20In%20this%20paper%2C%20we%20introduce%20the%20Social%0ANavigation%20Scene%20Understanding%20Benchmark%20%28SocialNav-SUB%29%2C%20a%20Visual%20Question%0AAnswering%20%28VQA%29%20dataset%20and%20benchmark%20designed%20to%20evaluate%20VLMs%20for%20scene%0Aunderstanding%20in%20real-world%20social%20robot%20navigation%20scenarios.%20SocialNav-SUB%0Aprovides%20a%20unified%20framework%20for%20evaluating%20VLMs%20against%20human%20and%20rule-based%0Abaselines%20across%20VQA%20tasks%20requiring%20spatial%2C%20spatiotemporal%2C%20and%20social%0Areasoning%20in%20social%20robot%20navigation.%20Through%20experiments%20with%20state-of-the-art%0AVLMs%2C%20we%20find%20that%20while%20the%20best-performing%20VLM%20achieves%20an%20encouraging%0Aprobability%20of%20agreeing%20with%20human%20answers%2C%20it%20still%20underperforms%20simpler%0Arule-based%20approach%20and%20human%20consensus%20baselines%2C%20indicating%20critical%20gaps%20in%0Asocial%20scene%20understanding%20of%20current%20VLMs.%20Our%20benchmark%20sets%20the%20stage%20for%0Afurther%20research%20on%20foundation%20models%20for%20social%20robot%20navigation%2C%20offering%20a%0Aframework%20to%20explore%20how%20VLMs%20can%20be%20tailored%20to%20meet%20real-world%20social%20robot%0Anavigation%20needs.%20An%20overview%20of%20this%20paper%20along%20with%20the%20code%20and%20data%20can%20be%0Afound%20at%20https%3A//larg.github.io/socialnav-sub%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocialNav-SUB%253A%2520Benchmarking%2520VLMs%2520for%2520Scene%2520Understanding%2520in%2520Social%2520Robot%250A%2520%2520Navigation%26entry.906535625%3DMichael%2520J.%2520Munje%2520and%2520Chen%2520Tang%2520and%2520Shuijing%2520Liu%2520and%2520Zichao%2520Hu%2520and%2520Yifeng%2520Zhu%2520and%2520Jiaxun%2520Cui%2520and%2520Garrett%2520Warnell%2520and%2520Joydeep%2520Biswas%2520and%2520Peter%2520Stone%26entry.1292438233%3D%2520%2520Robot%2520navigation%2520in%2520dynamic%252C%2520human-centered%2520environments%2520requires%250Asocially-compliant%2520decisions%2520grounded%2520in%2520robust%2520scene%2520understanding.%2520Recent%250AVision-Language%2520Models%2520%2528VLMs%2529%2520exhibit%2520promising%2520capabilities%2520such%2520as%2520object%250Arecognition%252C%2520common-sense%2520reasoning%252C%2520and%2520contextual%2520understanding-capabilities%250Athat%2520align%2520with%2520the%2520nuanced%2520requirements%2520of%2520social%2520robot%2520navigation.%2520However%252C%250Ait%2520remains%2520unclear%2520whether%2520VLMs%2520can%2520accurately%2520understand%2520complex%2520social%250Anavigation%2520scenes%2520%2528e.g.%252C%2520inferring%2520the%2520spatial-temporal%2520relations%2520among%2520agents%250Aand%2520human%2520intentions%2529%252C%2520which%2520is%2520essential%2520for%2520safe%2520and%2520socially%2520compliant%2520robot%250Anavigation.%2520While%2520some%2520recent%2520works%2520have%2520explored%2520the%2520use%2520of%2520VLMs%2520in%2520social%250Arobot%2520navigation%252C%2520no%2520existing%2520work%2520systematically%2520evaluates%2520their%2520ability%2520to%250Ameet%2520these%2520necessary%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Social%250ANavigation%2520Scene%2520Understanding%2520Benchmark%2520%2528SocialNav-SUB%2529%252C%2520a%2520Visual%2520Question%250AAnswering%2520%2528VQA%2529%2520dataset%2520and%2520benchmark%2520designed%2520to%2520evaluate%2520VLMs%2520for%2520scene%250Aunderstanding%2520in%2520real-world%2520social%2520robot%2520navigation%2520scenarios.%2520SocialNav-SUB%250Aprovides%2520a%2520unified%2520framework%2520for%2520evaluating%2520VLMs%2520against%2520human%2520and%2520rule-based%250Abaselines%2520across%2520VQA%2520tasks%2520requiring%2520spatial%252C%2520spatiotemporal%252C%2520and%2520social%250Areasoning%2520in%2520social%2520robot%2520navigation.%2520Through%2520experiments%2520with%2520state-of-the-art%250AVLMs%252C%2520we%2520find%2520that%2520while%2520the%2520best-performing%2520VLM%2520achieves%2520an%2520encouraging%250Aprobability%2520of%2520agreeing%2520with%2520human%2520answers%252C%2520it%2520still%2520underperforms%2520simpler%250Arule-based%2520approach%2520and%2520human%2520consensus%2520baselines%252C%2520indicating%2520critical%2520gaps%2520in%250Asocial%2520scene%2520understanding%2520of%2520current%2520VLMs.%2520Our%2520benchmark%2520sets%2520the%2520stage%2520for%250Afurther%2520research%2520on%2520foundation%2520models%2520for%2520social%2520robot%2520navigation%252C%2520offering%2520a%250Aframework%2520to%2520explore%2520how%2520VLMs%2520can%2520be%2520tailored%2520to%2520meet%2520real-world%2520social%2520robot%250Anavigation%2520needs.%2520An%2520overview%2520of%2520this%2520paper%2520along%2520with%2520the%2520code%2520and%2520data%2520can%2520be%250Afound%2520at%2520https%253A//larg.github.io/socialnav-sub%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SocialNav-SUB%3A%20Benchmarking%20VLMs%20for%20Scene%20Understanding%20in%20Social%20Robot%0A%20%20Navigation&entry.906535625=Michael%20J.%20Munje%20and%20Chen%20Tang%20and%20Shuijing%20Liu%20and%20Zichao%20Hu%20and%20Yifeng%20Zhu%20and%20Jiaxun%20Cui%20and%20Garrett%20Warnell%20and%20Joydeep%20Biswas%20and%20Peter%20Stone&entry.1292438233=%20%20Robot%20navigation%20in%20dynamic%2C%20human-centered%20environments%20requires%0Asocially-compliant%20decisions%20grounded%20in%20robust%20scene%20understanding.%20Recent%0AVision-Language%20Models%20%28VLMs%29%20exhibit%20promising%20capabilities%20such%20as%20object%0Arecognition%2C%20common-sense%20reasoning%2C%20and%20contextual%20understanding-capabilities%0Athat%20align%20with%20the%20nuanced%20requirements%20of%20social%20robot%20navigation.%20However%2C%0Ait%20remains%20unclear%20whether%20VLMs%20can%20accurately%20understand%20complex%20social%0Anavigation%20scenes%20%28e.g.%2C%20inferring%20the%20spatial-temporal%20relations%20among%20agents%0Aand%20human%20intentions%29%2C%20which%20is%20essential%20for%20safe%20and%20socially%20compliant%20robot%0Anavigation.%20While%20some%20recent%20works%20have%20explored%20the%20use%20of%20VLMs%20in%20social%0Arobot%20navigation%2C%20no%20existing%20work%20systematically%20evaluates%20their%20ability%20to%0Ameet%20these%20necessary%20conditions.%20In%20this%20paper%2C%20we%20introduce%20the%20Social%0ANavigation%20Scene%20Understanding%20Benchmark%20%28SocialNav-SUB%29%2C%20a%20Visual%20Question%0AAnswering%20%28VQA%29%20dataset%20and%20benchmark%20designed%20to%20evaluate%20VLMs%20for%20scene%0Aunderstanding%20in%20real-world%20social%20robot%20navigation%20scenarios.%20SocialNav-SUB%0Aprovides%20a%20unified%20framework%20for%20evaluating%20VLMs%20against%20human%20and%20rule-based%0Abaselines%20across%20VQA%20tasks%20requiring%20spatial%2C%20spatiotemporal%2C%20and%20social%0Areasoning%20in%20social%20robot%20navigation.%20Through%20experiments%20with%20state-of-the-art%0AVLMs%2C%20we%20find%20that%20while%20the%20best-performing%20VLM%20achieves%20an%20encouraging%0Aprobability%20of%20agreeing%20with%20human%20answers%2C%20it%20still%20underperforms%20simpler%0Arule-based%20approach%20and%20human%20consensus%20baselines%2C%20indicating%20critical%20gaps%20in%0Asocial%20scene%20understanding%20of%20current%20VLMs.%20Our%20benchmark%20sets%20the%20stage%20for%0Afurther%20research%20on%20foundation%20models%20for%20social%20robot%20navigation%2C%20offering%20a%0Aframework%20to%20explore%20how%20VLMs%20can%20be%20tailored%20to%20meet%20real-world%20social%20robot%0Anavigation%20needs.%20An%20overview%20of%20this%20paper%20along%20with%20the%20code%20and%20data%20can%20be%0Afound%20at%20https%3A//larg.github.io/socialnav-sub%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08757v1&entry.124074799=Read"},
{"title": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal\n  Conditioning", "author": "Liyang Chen and Tianxiang Ma and Jiawei Liu and Bingchuan Li and Zhuowei Chen and Lijie Liu and Xu He and Gen Li and Qian He and Zhiyong Wu", "abstract": "  Human-Centric Video Generation (HCVG) methods seek to synthesize human videos\nfrom multimodal inputs, including text, image, and audio. Existing methods\nstruggle to effectively coordinate these heterogeneous modalities due to two\nchallenges: the scarcity of training data with paired triplet conditions and\nthe difficulty of collaborating the sub-tasks of subject preservation and\naudio-visual sync with multimodal inputs. In this work, we present HuMo, a\nunified HCVG framework for collaborative multimodal control. For the first\nchallenge, we construct a high-quality dataset with diverse and paired text,\nreference images, and audio. For the second challenge, we propose a two-stage\nprogressive multimodal training paradigm with task-specific strategies. For the\nsubject preservation task, to maintain the prompt following and visual\ngeneration abilities of the foundation model, we adopt the minimal-invasive\nimage injection strategy. For the audio-visual sync task, besides the commonly\nadopted audio cross-attention layer, we propose a focus-by-predicting strategy\nthat implicitly guides the model to associate audio with facial regions. For\njoint learning of controllabilities across multimodal inputs, building on\npreviously acquired capabilities, we progressively incorporate the audio-visual\nsync task. During inference, for flexible and fine-grained multimodal control,\nwe design a time-adaptive Classifier-Free Guidance strategy that dynamically\nadjusts guidance weights across denoising steps. Extensive experimental results\ndemonstrate that HuMo surpasses specialized state-of-the-art methods in\nsub-tasks, establishing a unified framework for collaborative\nmultimodal-conditioned HCVG. Project Page:\nhttps://phantom-video.github.io/HuMo.\n", "link": "http://arxiv.org/abs/2509.08519v1", "date": "2025-09-10", "relevancy": 2.4055, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6149}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5942}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HuMo%3A%20Human-Centric%20Video%20Generation%20via%20Collaborative%20Multi-Modal%0A%20%20Conditioning&body=Title%3A%20HuMo%3A%20Human-Centric%20Video%20Generation%20via%20Collaborative%20Multi-Modal%0A%20%20Conditioning%0AAuthor%3A%20Liyang%20Chen%20and%20Tianxiang%20Ma%20and%20Jiawei%20Liu%20and%20Bingchuan%20Li%20and%20Zhuowei%20Chen%20and%20Lijie%20Liu%20and%20Xu%20He%20and%20Gen%20Li%20and%20Qian%20He%20and%20Zhiyong%20Wu%0AAbstract%3A%20%20%20Human-Centric%20Video%20Generation%20%28HCVG%29%20methods%20seek%20to%20synthesize%20human%20videos%0Afrom%20multimodal%20inputs%2C%20including%20text%2C%20image%2C%20and%20audio.%20Existing%20methods%0Astruggle%20to%20effectively%20coordinate%20these%20heterogeneous%20modalities%20due%20to%20two%0Achallenges%3A%20the%20scarcity%20of%20training%20data%20with%20paired%20triplet%20conditions%20and%0Athe%20difficulty%20of%20collaborating%20the%20sub-tasks%20of%20subject%20preservation%20and%0Aaudio-visual%20sync%20with%20multimodal%20inputs.%20In%20this%20work%2C%20we%20present%20HuMo%2C%20a%0Aunified%20HCVG%20framework%20for%20collaborative%20multimodal%20control.%20For%20the%20first%0Achallenge%2C%20we%20construct%20a%20high-quality%20dataset%20with%20diverse%20and%20paired%20text%2C%0Areference%20images%2C%20and%20audio.%20For%20the%20second%20challenge%2C%20we%20propose%20a%20two-stage%0Aprogressive%20multimodal%20training%20paradigm%20with%20task-specific%20strategies.%20For%20the%0Asubject%20preservation%20task%2C%20to%20maintain%20the%20prompt%20following%20and%20visual%0Ageneration%20abilities%20of%20the%20foundation%20model%2C%20we%20adopt%20the%20minimal-invasive%0Aimage%20injection%20strategy.%20For%20the%20audio-visual%20sync%20task%2C%20besides%20the%20commonly%0Aadopted%20audio%20cross-attention%20layer%2C%20we%20propose%20a%20focus-by-predicting%20strategy%0Athat%20implicitly%20guides%20the%20model%20to%20associate%20audio%20with%20facial%20regions.%20For%0Ajoint%20learning%20of%20controllabilities%20across%20multimodal%20inputs%2C%20building%20on%0Apreviously%20acquired%20capabilities%2C%20we%20progressively%20incorporate%20the%20audio-visual%0Async%20task.%20During%20inference%2C%20for%20flexible%20and%20fine-grained%20multimodal%20control%2C%0Awe%20design%20a%20time-adaptive%20Classifier-Free%20Guidance%20strategy%20that%20dynamically%0Aadjusts%20guidance%20weights%20across%20denoising%20steps.%20Extensive%20experimental%20results%0Ademonstrate%20that%20HuMo%20surpasses%20specialized%20state-of-the-art%20methods%20in%0Asub-tasks%2C%20establishing%20a%20unified%20framework%20for%20collaborative%0Amultimodal-conditioned%20HCVG.%20Project%20Page%3A%0Ahttps%3A//phantom-video.github.io/HuMo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuMo%253A%2520Human-Centric%2520Video%2520Generation%2520via%2520Collaborative%2520Multi-Modal%250A%2520%2520Conditioning%26entry.906535625%3DLiyang%2520Chen%2520and%2520Tianxiang%2520Ma%2520and%2520Jiawei%2520Liu%2520and%2520Bingchuan%2520Li%2520and%2520Zhuowei%2520Chen%2520and%2520Lijie%2520Liu%2520and%2520Xu%2520He%2520and%2520Gen%2520Li%2520and%2520Qian%2520He%2520and%2520Zhiyong%2520Wu%26entry.1292438233%3D%2520%2520Human-Centric%2520Video%2520Generation%2520%2528HCVG%2529%2520methods%2520seek%2520to%2520synthesize%2520human%2520videos%250Afrom%2520multimodal%2520inputs%252C%2520including%2520text%252C%2520image%252C%2520and%2520audio.%2520Existing%2520methods%250Astruggle%2520to%2520effectively%2520coordinate%2520these%2520heterogeneous%2520modalities%2520due%2520to%2520two%250Achallenges%253A%2520the%2520scarcity%2520of%2520training%2520data%2520with%2520paired%2520triplet%2520conditions%2520and%250Athe%2520difficulty%2520of%2520collaborating%2520the%2520sub-tasks%2520of%2520subject%2520preservation%2520and%250Aaudio-visual%2520sync%2520with%2520multimodal%2520inputs.%2520In%2520this%2520work%252C%2520we%2520present%2520HuMo%252C%2520a%250Aunified%2520HCVG%2520framework%2520for%2520collaborative%2520multimodal%2520control.%2520For%2520the%2520first%250Achallenge%252C%2520we%2520construct%2520a%2520high-quality%2520dataset%2520with%2520diverse%2520and%2520paired%2520text%252C%250Areference%2520images%252C%2520and%2520audio.%2520For%2520the%2520second%2520challenge%252C%2520we%2520propose%2520a%2520two-stage%250Aprogressive%2520multimodal%2520training%2520paradigm%2520with%2520task-specific%2520strategies.%2520For%2520the%250Asubject%2520preservation%2520task%252C%2520to%2520maintain%2520the%2520prompt%2520following%2520and%2520visual%250Ageneration%2520abilities%2520of%2520the%2520foundation%2520model%252C%2520we%2520adopt%2520the%2520minimal-invasive%250Aimage%2520injection%2520strategy.%2520For%2520the%2520audio-visual%2520sync%2520task%252C%2520besides%2520the%2520commonly%250Aadopted%2520audio%2520cross-attention%2520layer%252C%2520we%2520propose%2520a%2520focus-by-predicting%2520strategy%250Athat%2520implicitly%2520guides%2520the%2520model%2520to%2520associate%2520audio%2520with%2520facial%2520regions.%2520For%250Ajoint%2520learning%2520of%2520controllabilities%2520across%2520multimodal%2520inputs%252C%2520building%2520on%250Apreviously%2520acquired%2520capabilities%252C%2520we%2520progressively%2520incorporate%2520the%2520audio-visual%250Async%2520task.%2520During%2520inference%252C%2520for%2520flexible%2520and%2520fine-grained%2520multimodal%2520control%252C%250Awe%2520design%2520a%2520time-adaptive%2520Classifier-Free%2520Guidance%2520strategy%2520that%2520dynamically%250Aadjusts%2520guidance%2520weights%2520across%2520denoising%2520steps.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520HuMo%2520surpasses%2520specialized%2520state-of-the-art%2520methods%2520in%250Asub-tasks%252C%2520establishing%2520a%2520unified%2520framework%2520for%2520collaborative%250Amultimodal-conditioned%2520HCVG.%2520Project%2520Page%253A%250Ahttps%253A//phantom-video.github.io/HuMo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HuMo%3A%20Human-Centric%20Video%20Generation%20via%20Collaborative%20Multi-Modal%0A%20%20Conditioning&entry.906535625=Liyang%20Chen%20and%20Tianxiang%20Ma%20and%20Jiawei%20Liu%20and%20Bingchuan%20Li%20and%20Zhuowei%20Chen%20and%20Lijie%20Liu%20and%20Xu%20He%20and%20Gen%20Li%20and%20Qian%20He%20and%20Zhiyong%20Wu&entry.1292438233=%20%20Human-Centric%20Video%20Generation%20%28HCVG%29%20methods%20seek%20to%20synthesize%20human%20videos%0Afrom%20multimodal%20inputs%2C%20including%20text%2C%20image%2C%20and%20audio.%20Existing%20methods%0Astruggle%20to%20effectively%20coordinate%20these%20heterogeneous%20modalities%20due%20to%20two%0Achallenges%3A%20the%20scarcity%20of%20training%20data%20with%20paired%20triplet%20conditions%20and%0Athe%20difficulty%20of%20collaborating%20the%20sub-tasks%20of%20subject%20preservation%20and%0Aaudio-visual%20sync%20with%20multimodal%20inputs.%20In%20this%20work%2C%20we%20present%20HuMo%2C%20a%0Aunified%20HCVG%20framework%20for%20collaborative%20multimodal%20control.%20For%20the%20first%0Achallenge%2C%20we%20construct%20a%20high-quality%20dataset%20with%20diverse%20and%20paired%20text%2C%0Areference%20images%2C%20and%20audio.%20For%20the%20second%20challenge%2C%20we%20propose%20a%20two-stage%0Aprogressive%20multimodal%20training%20paradigm%20with%20task-specific%20strategies.%20For%20the%0Asubject%20preservation%20task%2C%20to%20maintain%20the%20prompt%20following%20and%20visual%0Ageneration%20abilities%20of%20the%20foundation%20model%2C%20we%20adopt%20the%20minimal-invasive%0Aimage%20injection%20strategy.%20For%20the%20audio-visual%20sync%20task%2C%20besides%20the%20commonly%0Aadopted%20audio%20cross-attention%20layer%2C%20we%20propose%20a%20focus-by-predicting%20strategy%0Athat%20implicitly%20guides%20the%20model%20to%20associate%20audio%20with%20facial%20regions.%20For%0Ajoint%20learning%20of%20controllabilities%20across%20multimodal%20inputs%2C%20building%20on%0Apreviously%20acquired%20capabilities%2C%20we%20progressively%20incorporate%20the%20audio-visual%0Async%20task.%20During%20inference%2C%20for%20flexible%20and%20fine-grained%20multimodal%20control%2C%0Awe%20design%20a%20time-adaptive%20Classifier-Free%20Guidance%20strategy%20that%20dynamically%0Aadjusts%20guidance%20weights%20across%20denoising%20steps.%20Extensive%20experimental%20results%0Ademonstrate%20that%20HuMo%20surpasses%20specialized%20state-of-the-art%20methods%20in%0Asub-tasks%2C%20establishing%20a%20unified%20framework%20for%20collaborative%0Amultimodal-conditioned%20HCVG.%20Project%20Page%3A%0Ahttps%3A//phantom-video.github.io/HuMo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08519v1&entry.124074799=Read"},
{"title": "TANGO: Traversability-Aware Navigation with Local Metric Control for\n  Topological Goals", "author": "Stefan Podgorski and Sourav Garg and Mehdi Hosseinzadeh and Lachlan Mares and Feras Dayoub and Ian Reid", "abstract": "  Visual navigation in robotics traditionally relies on globally-consistent 3D\nmaps or learned controllers, which can be computationally expensive and\ndifficult to generalize across diverse environments. In this work, we present a\nnovel RGB-only, object-level topometric navigation pipeline that enables\nzero-shot, long-horizon robot navigation without requiring 3D maps or\npre-trained controllers. Our approach integrates global topological path\nplanning with local metric trajectory control, allowing the robot to navigate\ntowards object-level sub-goals while avoiding obstacles. We address key\nlimitations of previous methods by continuously predicting local trajectory\nusing monocular depth and traversability estimation, and incorporating an\nauto-switching mechanism that falls back to a baseline controller when\nnecessary. The system operates using foundational models, ensuring open-set\napplicability without the need for domain-specific fine-tuning. We demonstrate\nthe effectiveness of our method in both simulated environments and real-world\ntests, highlighting its robustness and deployability. Our approach outperforms\nexisting state-of-the-art methods, offering a more adaptable and effective\nsolution for visual navigation in open-set environments. The source code is\nmade publicly available: https://github.com/podgorki/TANGO.\n", "link": "http://arxiv.org/abs/2509.08699v1", "date": "2025-09-10", "relevancy": 2.3909, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6047}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5966}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TANGO%3A%20Traversability-Aware%20Navigation%20with%20Local%20Metric%20Control%20for%0A%20%20Topological%20Goals&body=Title%3A%20TANGO%3A%20Traversability-Aware%20Navigation%20with%20Local%20Metric%20Control%20for%0A%20%20Topological%20Goals%0AAuthor%3A%20Stefan%20Podgorski%20and%20Sourav%20Garg%20and%20Mehdi%20Hosseinzadeh%20and%20Lachlan%20Mares%20and%20Feras%20Dayoub%20and%20Ian%20Reid%0AAbstract%3A%20%20%20Visual%20navigation%20in%20robotics%20traditionally%20relies%20on%20globally-consistent%203D%0Amaps%20or%20learned%20controllers%2C%20which%20can%20be%20computationally%20expensive%20and%0Adifficult%20to%20generalize%20across%20diverse%20environments.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20RGB-only%2C%20object-level%20topometric%20navigation%20pipeline%20that%20enables%0Azero-shot%2C%20long-horizon%20robot%20navigation%20without%20requiring%203D%20maps%20or%0Apre-trained%20controllers.%20Our%20approach%20integrates%20global%20topological%20path%0Aplanning%20with%20local%20metric%20trajectory%20control%2C%20allowing%20the%20robot%20to%20navigate%0Atowards%20object-level%20sub-goals%20while%20avoiding%20obstacles.%20We%20address%20key%0Alimitations%20of%20previous%20methods%20by%20continuously%20predicting%20local%20trajectory%0Ausing%20monocular%20depth%20and%20traversability%20estimation%2C%20and%20incorporating%20an%0Aauto-switching%20mechanism%20that%20falls%20back%20to%20a%20baseline%20controller%20when%0Anecessary.%20The%20system%20operates%20using%20foundational%20models%2C%20ensuring%20open-set%0Aapplicability%20without%20the%20need%20for%20domain-specific%20fine-tuning.%20We%20demonstrate%0Athe%20effectiveness%20of%20our%20method%20in%20both%20simulated%20environments%20and%20real-world%0Atests%2C%20highlighting%20its%20robustness%20and%20deployability.%20Our%20approach%20outperforms%0Aexisting%20state-of-the-art%20methods%2C%20offering%20a%20more%20adaptable%20and%20effective%0Asolution%20for%20visual%20navigation%20in%20open-set%20environments.%20The%20source%20code%20is%0Amade%20publicly%20available%3A%20https%3A//github.com/podgorki/TANGO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTANGO%253A%2520Traversability-Aware%2520Navigation%2520with%2520Local%2520Metric%2520Control%2520for%250A%2520%2520Topological%2520Goals%26entry.906535625%3DStefan%2520Podgorski%2520and%2520Sourav%2520Garg%2520and%2520Mehdi%2520Hosseinzadeh%2520and%2520Lachlan%2520Mares%2520and%2520Feras%2520Dayoub%2520and%2520Ian%2520Reid%26entry.1292438233%3D%2520%2520Visual%2520navigation%2520in%2520robotics%2520traditionally%2520relies%2520on%2520globally-consistent%25203D%250Amaps%2520or%2520learned%2520controllers%252C%2520which%2520can%2520be%2520computationally%2520expensive%2520and%250Adifficult%2520to%2520generalize%2520across%2520diverse%2520environments.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Anovel%2520RGB-only%252C%2520object-level%2520topometric%2520navigation%2520pipeline%2520that%2520enables%250Azero-shot%252C%2520long-horizon%2520robot%2520navigation%2520without%2520requiring%25203D%2520maps%2520or%250Apre-trained%2520controllers.%2520Our%2520approach%2520integrates%2520global%2520topological%2520path%250Aplanning%2520with%2520local%2520metric%2520trajectory%2520control%252C%2520allowing%2520the%2520robot%2520to%2520navigate%250Atowards%2520object-level%2520sub-goals%2520while%2520avoiding%2520obstacles.%2520We%2520address%2520key%250Alimitations%2520of%2520previous%2520methods%2520by%2520continuously%2520predicting%2520local%2520trajectory%250Ausing%2520monocular%2520depth%2520and%2520traversability%2520estimation%252C%2520and%2520incorporating%2520an%250Aauto-switching%2520mechanism%2520that%2520falls%2520back%2520to%2520a%2520baseline%2520controller%2520when%250Anecessary.%2520The%2520system%2520operates%2520using%2520foundational%2520models%252C%2520ensuring%2520open-set%250Aapplicability%2520without%2520the%2520need%2520for%2520domain-specific%2520fine-tuning.%2520We%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520method%2520in%2520both%2520simulated%2520environments%2520and%2520real-world%250Atests%252C%2520highlighting%2520its%2520robustness%2520and%2520deployability.%2520Our%2520approach%2520outperforms%250Aexisting%2520state-of-the-art%2520methods%252C%2520offering%2520a%2520more%2520adaptable%2520and%2520effective%250Asolution%2520for%2520visual%2520navigation%2520in%2520open-set%2520environments.%2520The%2520source%2520code%2520is%250Amade%2520publicly%2520available%253A%2520https%253A//github.com/podgorki/TANGO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TANGO%3A%20Traversability-Aware%20Navigation%20with%20Local%20Metric%20Control%20for%0A%20%20Topological%20Goals&entry.906535625=Stefan%20Podgorski%20and%20Sourav%20Garg%20and%20Mehdi%20Hosseinzadeh%20and%20Lachlan%20Mares%20and%20Feras%20Dayoub%20and%20Ian%20Reid&entry.1292438233=%20%20Visual%20navigation%20in%20robotics%20traditionally%20relies%20on%20globally-consistent%203D%0Amaps%20or%20learned%20controllers%2C%20which%20can%20be%20computationally%20expensive%20and%0Adifficult%20to%20generalize%20across%20diverse%20environments.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20RGB-only%2C%20object-level%20topometric%20navigation%20pipeline%20that%20enables%0Azero-shot%2C%20long-horizon%20robot%20navigation%20without%20requiring%203D%20maps%20or%0Apre-trained%20controllers.%20Our%20approach%20integrates%20global%20topological%20path%0Aplanning%20with%20local%20metric%20trajectory%20control%2C%20allowing%20the%20robot%20to%20navigate%0Atowards%20object-level%20sub-goals%20while%20avoiding%20obstacles.%20We%20address%20key%0Alimitations%20of%20previous%20methods%20by%20continuously%20predicting%20local%20trajectory%0Ausing%20monocular%20depth%20and%20traversability%20estimation%2C%20and%20incorporating%20an%0Aauto-switching%20mechanism%20that%20falls%20back%20to%20a%20baseline%20controller%20when%0Anecessary.%20The%20system%20operates%20using%20foundational%20models%2C%20ensuring%20open-set%0Aapplicability%20without%20the%20need%20for%20domain-specific%20fine-tuning.%20We%20demonstrate%0Athe%20effectiveness%20of%20our%20method%20in%20both%20simulated%20environments%20and%20real-world%0Atests%2C%20highlighting%20its%20robustness%20and%20deployability.%20Our%20approach%20outperforms%0Aexisting%20state-of-the-art%20methods%2C%20offering%20a%20more%20adaptable%20and%20effective%0Asolution%20for%20visual%20navigation%20in%20open-set%20environments.%20The%20source%20code%20is%0Amade%20publicly%20available%3A%20https%3A//github.com/podgorki/TANGO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08699v1&entry.124074799=Read"},
{"title": "T-araVLN: Translator for Agricultural Robotic Agents on\n  Vision-and-Language Navigation", "author": "Xiaobei Zhao and Xingqi Lyu and Xiang Li", "abstract": "  Agricultural robotic agents have been becoming powerful helpers in a wide\nrange of agricultural tasks, nevertheless, still heavily rely on manual\noperation or untransportable railway for movement. The AgriVLN method and the\nA2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the\nagricultural domain, enabling agents navigate to the target position following\nthe natural language instructions. AgriVLN effectively understands the simple\ninstructions, however, often misunderstands the complicated instructions. To\nbridge this gap, we propose the method of Translator for Agricultural Robotic\nAgents on Vision-and-Language Navigation (T-araVLN), in which the Instruction\nTranslator module translates the original instruction to be both refined and\nprecise. Being evaluated on the A2A benchmark, our T-araVLN effectively\nimproves Success Rate from 0.47 to 0.63 and reduces Navigation Error from 2.91m\nto 2.28m, demonstrating the state-of-the-art performance in the agricultural\ndomain. Code: https://github.com/AlexTraveling/T-araVLN.\n", "link": "http://arxiv.org/abs/2509.06644v3", "date": "2025-09-10", "relevancy": 2.3831, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4768}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4768}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-araVLN%3A%20Translator%20for%20Agricultural%20Robotic%20Agents%20on%0A%20%20Vision-and-Language%20Navigation&body=Title%3A%20T-araVLN%3A%20Translator%20for%20Agricultural%20Robotic%20Agents%20on%0A%20%20Vision-and-Language%20Navigation%0AAuthor%3A%20Xiaobei%20Zhao%20and%20Xingqi%20Lyu%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Agricultural%20robotic%20agents%20have%20been%20becoming%20powerful%20helpers%20in%20a%20wide%0Arange%20of%20agricultural%20tasks%2C%20nevertheless%2C%20still%20heavily%20rely%20on%20manual%0Aoperation%20or%20untransportable%20railway%20for%20movement.%20The%20AgriVLN%20method%20and%20the%0AA2A%20benchmark%20pioneeringly%20extend%20Vision-and-Language%20Navigation%20%28VLN%29%20to%20the%0Aagricultural%20domain%2C%20enabling%20agents%20navigate%20to%20the%20target%20position%20following%0Athe%20natural%20language%20instructions.%20AgriVLN%20effectively%20understands%20the%20simple%0Ainstructions%2C%20however%2C%20often%20misunderstands%20the%20complicated%20instructions.%20To%0Abridge%20this%20gap%2C%20we%20propose%20the%20method%20of%20Translator%20for%20Agricultural%20Robotic%0AAgents%20on%20Vision-and-Language%20Navigation%20%28T-araVLN%29%2C%20in%20which%20the%20Instruction%0ATranslator%20module%20translates%20the%20original%20instruction%20to%20be%20both%20refined%20and%0Aprecise.%20Being%20evaluated%20on%20the%20A2A%20benchmark%2C%20our%20T-araVLN%20effectively%0Aimproves%20Success%20Rate%20from%200.47%20to%200.63%20and%20reduces%20Navigation%20Error%20from%202.91m%0Ato%202.28m%2C%20demonstrating%20the%20state-of-the-art%20performance%20in%20the%20agricultural%0Adomain.%20Code%3A%20https%3A//github.com/AlexTraveling/T-araVLN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06644v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-araVLN%253A%2520Translator%2520for%2520Agricultural%2520Robotic%2520Agents%2520on%250A%2520%2520Vision-and-Language%2520Navigation%26entry.906535625%3DXiaobei%2520Zhao%2520and%2520Xingqi%2520Lyu%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Agricultural%2520robotic%2520agents%2520have%2520been%2520becoming%2520powerful%2520helpers%2520in%2520a%2520wide%250Arange%2520of%2520agricultural%2520tasks%252C%2520nevertheless%252C%2520still%2520heavily%2520rely%2520on%2520manual%250Aoperation%2520or%2520untransportable%2520railway%2520for%2520movement.%2520The%2520AgriVLN%2520method%2520and%2520the%250AA2A%2520benchmark%2520pioneeringly%2520extend%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520to%2520the%250Aagricultural%2520domain%252C%2520enabling%2520agents%2520navigate%2520to%2520the%2520target%2520position%2520following%250Athe%2520natural%2520language%2520instructions.%2520AgriVLN%2520effectively%2520understands%2520the%2520simple%250Ainstructions%252C%2520however%252C%2520often%2520misunderstands%2520the%2520complicated%2520instructions.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520propose%2520the%2520method%2520of%2520Translator%2520for%2520Agricultural%2520Robotic%250AAgents%2520on%2520Vision-and-Language%2520Navigation%2520%2528T-araVLN%2529%252C%2520in%2520which%2520the%2520Instruction%250ATranslator%2520module%2520translates%2520the%2520original%2520instruction%2520to%2520be%2520both%2520refined%2520and%250Aprecise.%2520Being%2520evaluated%2520on%2520the%2520A2A%2520benchmark%252C%2520our%2520T-araVLN%2520effectively%250Aimproves%2520Success%2520Rate%2520from%25200.47%2520to%25200.63%2520and%2520reduces%2520Navigation%2520Error%2520from%25202.91m%250Ato%25202.28m%252C%2520demonstrating%2520the%2520state-of-the-art%2520performance%2520in%2520the%2520agricultural%250Adomain.%2520Code%253A%2520https%253A//github.com/AlexTraveling/T-araVLN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06644v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-araVLN%3A%20Translator%20for%20Agricultural%20Robotic%20Agents%20on%0A%20%20Vision-and-Language%20Navigation&entry.906535625=Xiaobei%20Zhao%20and%20Xingqi%20Lyu%20and%20Xiang%20Li&entry.1292438233=%20%20Agricultural%20robotic%20agents%20have%20been%20becoming%20powerful%20helpers%20in%20a%20wide%0Arange%20of%20agricultural%20tasks%2C%20nevertheless%2C%20still%20heavily%20rely%20on%20manual%0Aoperation%20or%20untransportable%20railway%20for%20movement.%20The%20AgriVLN%20method%20and%20the%0AA2A%20benchmark%20pioneeringly%20extend%20Vision-and-Language%20Navigation%20%28VLN%29%20to%20the%0Aagricultural%20domain%2C%20enabling%20agents%20navigate%20to%20the%20target%20position%20following%0Athe%20natural%20language%20instructions.%20AgriVLN%20effectively%20understands%20the%20simple%0Ainstructions%2C%20however%2C%20often%20misunderstands%20the%20complicated%20instructions.%20To%0Abridge%20this%20gap%2C%20we%20propose%20the%20method%20of%20Translator%20for%20Agricultural%20Robotic%0AAgents%20on%20Vision-and-Language%20Navigation%20%28T-araVLN%29%2C%20in%20which%20the%20Instruction%0ATranslator%20module%20translates%20the%20original%20instruction%20to%20be%20both%20refined%20and%0Aprecise.%20Being%20evaluated%20on%20the%20A2A%20benchmark%2C%20our%20T-araVLN%20effectively%0Aimproves%20Success%20Rate%20from%200.47%20to%200.63%20and%20reduces%20Navigation%20Error%20from%202.91m%0Ato%202.28m%2C%20demonstrating%20the%20state-of-the-art%20performance%20in%20the%20agricultural%0Adomain.%20Code%3A%20https%3A//github.com/AlexTraveling/T-araVLN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06644v3&entry.124074799=Read"},
{"title": "Memorization in Large Language Models in Medicine: Prevalence,\n  Characteristics, and Implications", "author": "Anran Li and Lingfei Qian and Mengmeng Du and Yu Yin and Yan Hu and Zihao Sun and Yihang Fu and Erica Stutz and Xuguang Ai and Qianqian Xie and Rui Zhu and Jimin Huang and Yifan Yang and Siru Liu and Yih-Chung Tham and Lucila Ohno-Machado and Hyunghoon Cho and Zhiyong Lu and Hua Xu and Qingyu Chen", "abstract": "  Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information.\n", "link": "http://arxiv.org/abs/2509.08604v1", "date": "2025-09-10", "relevancy": 2.3687, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memorization%20in%20Large%20Language%20Models%20in%20Medicine%3A%20Prevalence%2C%0A%20%20Characteristics%2C%20and%20Implications&body=Title%3A%20Memorization%20in%20Large%20Language%20Models%20in%20Medicine%3A%20Prevalence%2C%0A%20%20Characteristics%2C%20and%20Implications%0AAuthor%3A%20Anran%20Li%20and%20Lingfei%20Qian%20and%20Mengmeng%20Du%20and%20Yu%20Yin%20and%20Yan%20Hu%20and%20Zihao%20Sun%20and%20Yihang%20Fu%20and%20Erica%20Stutz%20and%20Xuguang%20Ai%20and%20Qianqian%20Xie%20and%20Rui%20Zhu%20and%20Jimin%20Huang%20and%20Yifan%20Yang%20and%20Siru%20Liu%20and%20Yih-Chung%20Tham%20and%20Lucila%20Ohno-Machado%20and%20Hyunghoon%20Cho%20and%20Zhiyong%20Lu%20and%20Hua%20Xu%20and%20Qingyu%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%20potential%20in%0Amedicine.%20To%20date%2C%20LLMs%20have%20been%20widely%20applied%20to%20tasks%20such%20as%20diagnostic%0Aassistance%2C%20medical%20question%20answering%2C%20and%20clinical%20information%20synthesis.%0AHowever%2C%20a%20key%20open%20question%20remains%3A%20to%20what%20extent%20do%20LLMs%20memorize%20medical%0Atraining%20data.%20In%20this%20study%2C%20we%20present%20the%20first%20comprehensive%20evaluation%20of%0Amemorization%20of%20LLMs%20in%20medicine%2C%20assessing%20its%20prevalence%20%28how%20frequently%20it%0Aoccurs%29%2C%20characteristics%20%28what%20is%20memorized%29%2C%20volume%20%28how%20much%20content%20is%0Amemorized%29%2C%20and%20potential%20downstream%20impacts%20%28how%20memorization%20may%20affect%0Amedical%20applications%29.%20We%20systematically%20analyze%20common%20adaptation%20scenarios%3A%0A%281%29%20continued%20pretraining%20on%20medical%20corpora%2C%20%282%29%20fine-tuning%20on%20standard%0Amedical%20benchmarks%2C%20and%20%283%29%20fine-tuning%20on%20real-world%20clinical%20data%2C%20including%0Aover%2013%2C000%20unique%20inpatient%20records%20from%20Yale%20New%20Haven%20Health%20System.%20The%0Aresults%20demonstrate%20that%20memorization%20is%20prevalent%20across%20all%20adaptation%0Ascenarios%20and%20significantly%20higher%20than%20reported%20in%20the%20general%20domain.%0AMemorization%20affects%20both%20the%20development%20and%20adoption%20of%20LLMs%20in%20medicine%20and%0Acan%20be%20categorized%20into%20three%20types%3A%20beneficial%20%28e.g.%2C%20accurate%20recall%20of%0Aclinical%20guidelines%20and%20biomedical%20references%29%2C%20uninformative%20%28e.g.%2C%20repeated%0Adisclaimers%20or%20templated%20medical%20document%20language%29%2C%20and%20harmful%20%28e.g.%2C%0Aregeneration%20of%20dataset-specific%20or%20sensitive%20clinical%20content%29.%20Based%20on%20these%0Afindings%2C%20we%20offer%20practical%20recommendations%20to%20facilitate%20beneficial%0Amemorization%20that%20enhances%20domain-specific%20reasoning%20and%20factual%20accuracy%2C%0Aminimize%20uninformative%20memorization%20to%20promote%20deeper%20learning%20beyond%0Asurface-level%20patterns%2C%20and%20mitigate%20harmful%20memorization%20to%20prevent%20the%0Aleakage%20of%20sensitive%20or%20identifiable%20patient%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemorization%2520in%2520Large%2520Language%2520Models%2520in%2520Medicine%253A%2520Prevalence%252C%250A%2520%2520Characteristics%252C%2520and%2520Implications%26entry.906535625%3DAnran%2520Li%2520and%2520Lingfei%2520Qian%2520and%2520Mengmeng%2520Du%2520and%2520Yu%2520Yin%2520and%2520Yan%2520Hu%2520and%2520Zihao%2520Sun%2520and%2520Yihang%2520Fu%2520and%2520Erica%2520Stutz%2520and%2520Xuguang%2520Ai%2520and%2520Qianqian%2520Xie%2520and%2520Rui%2520Zhu%2520and%2520Jimin%2520Huang%2520and%2520Yifan%2520Yang%2520and%2520Siru%2520Liu%2520and%2520Yih-Chung%2520Tham%2520and%2520Lucila%2520Ohno-Machado%2520and%2520Hyunghoon%2520Cho%2520and%2520Zhiyong%2520Lu%2520and%2520Hua%2520Xu%2520and%2520Qingyu%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520significant%2520potential%2520in%250Amedicine.%2520To%2520date%252C%2520LLMs%2520have%2520been%2520widely%2520applied%2520to%2520tasks%2520such%2520as%2520diagnostic%250Aassistance%252C%2520medical%2520question%2520answering%252C%2520and%2520clinical%2520information%2520synthesis.%250AHowever%252C%2520a%2520key%2520open%2520question%2520remains%253A%2520to%2520what%2520extent%2520do%2520LLMs%2520memorize%2520medical%250Atraining%2520data.%2520In%2520this%2520study%252C%2520we%2520present%2520the%2520first%2520comprehensive%2520evaluation%2520of%250Amemorization%2520of%2520LLMs%2520in%2520medicine%252C%2520assessing%2520its%2520prevalence%2520%2528how%2520frequently%2520it%250Aoccurs%2529%252C%2520characteristics%2520%2528what%2520is%2520memorized%2529%252C%2520volume%2520%2528how%2520much%2520content%2520is%250Amemorized%2529%252C%2520and%2520potential%2520downstream%2520impacts%2520%2528how%2520memorization%2520may%2520affect%250Amedical%2520applications%2529.%2520We%2520systematically%2520analyze%2520common%2520adaptation%2520scenarios%253A%250A%25281%2529%2520continued%2520pretraining%2520on%2520medical%2520corpora%252C%2520%25282%2529%2520fine-tuning%2520on%2520standard%250Amedical%2520benchmarks%252C%2520and%2520%25283%2529%2520fine-tuning%2520on%2520real-world%2520clinical%2520data%252C%2520including%250Aover%252013%252C000%2520unique%2520inpatient%2520records%2520from%2520Yale%2520New%2520Haven%2520Health%2520System.%2520The%250Aresults%2520demonstrate%2520that%2520memorization%2520is%2520prevalent%2520across%2520all%2520adaptation%250Ascenarios%2520and%2520significantly%2520higher%2520than%2520reported%2520in%2520the%2520general%2520domain.%250AMemorization%2520affects%2520both%2520the%2520development%2520and%2520adoption%2520of%2520LLMs%2520in%2520medicine%2520and%250Acan%2520be%2520categorized%2520into%2520three%2520types%253A%2520beneficial%2520%2528e.g.%252C%2520accurate%2520recall%2520of%250Aclinical%2520guidelines%2520and%2520biomedical%2520references%2529%252C%2520uninformative%2520%2528e.g.%252C%2520repeated%250Adisclaimers%2520or%2520templated%2520medical%2520document%2520language%2529%252C%2520and%2520harmful%2520%2528e.g.%252C%250Aregeneration%2520of%2520dataset-specific%2520or%2520sensitive%2520clinical%2520content%2529.%2520Based%2520on%2520these%250Afindings%252C%2520we%2520offer%2520practical%2520recommendations%2520to%2520facilitate%2520beneficial%250Amemorization%2520that%2520enhances%2520domain-specific%2520reasoning%2520and%2520factual%2520accuracy%252C%250Aminimize%2520uninformative%2520memorization%2520to%2520promote%2520deeper%2520learning%2520beyond%250Asurface-level%2520patterns%252C%2520and%2520mitigate%2520harmful%2520memorization%2520to%2520prevent%2520the%250Aleakage%2520of%2520sensitive%2520or%2520identifiable%2520patient%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorization%20in%20Large%20Language%20Models%20in%20Medicine%3A%20Prevalence%2C%0A%20%20Characteristics%2C%20and%20Implications&entry.906535625=Anran%20Li%20and%20Lingfei%20Qian%20and%20Mengmeng%20Du%20and%20Yu%20Yin%20and%20Yan%20Hu%20and%20Zihao%20Sun%20and%20Yihang%20Fu%20and%20Erica%20Stutz%20and%20Xuguang%20Ai%20and%20Qianqian%20Xie%20and%20Rui%20Zhu%20and%20Jimin%20Huang%20and%20Yifan%20Yang%20and%20Siru%20Liu%20and%20Yih-Chung%20Tham%20and%20Lucila%20Ohno-Machado%20and%20Hyunghoon%20Cho%20and%20Zhiyong%20Lu%20and%20Hua%20Xu%20and%20Qingyu%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%20potential%20in%0Amedicine.%20To%20date%2C%20LLMs%20have%20been%20widely%20applied%20to%20tasks%20such%20as%20diagnostic%0Aassistance%2C%20medical%20question%20answering%2C%20and%20clinical%20information%20synthesis.%0AHowever%2C%20a%20key%20open%20question%20remains%3A%20to%20what%20extent%20do%20LLMs%20memorize%20medical%0Atraining%20data.%20In%20this%20study%2C%20we%20present%20the%20first%20comprehensive%20evaluation%20of%0Amemorization%20of%20LLMs%20in%20medicine%2C%20assessing%20its%20prevalence%20%28how%20frequently%20it%0Aoccurs%29%2C%20characteristics%20%28what%20is%20memorized%29%2C%20volume%20%28how%20much%20content%20is%0Amemorized%29%2C%20and%20potential%20downstream%20impacts%20%28how%20memorization%20may%20affect%0Amedical%20applications%29.%20We%20systematically%20analyze%20common%20adaptation%20scenarios%3A%0A%281%29%20continued%20pretraining%20on%20medical%20corpora%2C%20%282%29%20fine-tuning%20on%20standard%0Amedical%20benchmarks%2C%20and%20%283%29%20fine-tuning%20on%20real-world%20clinical%20data%2C%20including%0Aover%2013%2C000%20unique%20inpatient%20records%20from%20Yale%20New%20Haven%20Health%20System.%20The%0Aresults%20demonstrate%20that%20memorization%20is%20prevalent%20across%20all%20adaptation%0Ascenarios%20and%20significantly%20higher%20than%20reported%20in%20the%20general%20domain.%0AMemorization%20affects%20both%20the%20development%20and%20adoption%20of%20LLMs%20in%20medicine%20and%0Acan%20be%20categorized%20into%20three%20types%3A%20beneficial%20%28e.g.%2C%20accurate%20recall%20of%0Aclinical%20guidelines%20and%20biomedical%20references%29%2C%20uninformative%20%28e.g.%2C%20repeated%0Adisclaimers%20or%20templated%20medical%20document%20language%29%2C%20and%20harmful%20%28e.g.%2C%0Aregeneration%20of%20dataset-specific%20or%20sensitive%20clinical%20content%29.%20Based%20on%20these%0Afindings%2C%20we%20offer%20practical%20recommendations%20to%20facilitate%20beneficial%0Amemorization%20that%20enhances%20domain-specific%20reasoning%20and%20factual%20accuracy%2C%0Aminimize%20uninformative%20memorization%20to%20promote%20deeper%20learning%20beyond%0Asurface-level%20patterns%2C%20and%20mitigate%20harmful%20memorization%20to%20prevent%20the%0Aleakage%20of%20sensitive%20or%20identifiable%20patient%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08604v1&entry.124074799=Read"},
{"title": "Vision-Language Semantic Aggregation Leveraging Foundation Model for\n  Generalizable Medical Image Segmentation", "author": "Wenjun Yu and Yinchen Zhou and Jia-Xuan Jiang and Shubin Zeng and Yuee Li and Zhong Wang", "abstract": "  Multimodal models have achieved remarkable success in natural image\nsegmentation, yet they often underperform when applied to the medical domain.\nThrough extensive study, we attribute this performance gap to the challenges of\nmultimodal fusion, primarily the significant semantic gap between abstract\ntextual prompts and fine-grained medical visual features, as well as the\nresulting feature dispersion. To address these issues, we revisit the problem\nfrom the perspective of semantic aggregation. Specifically, we propose an\nExpectation-Maximization (EM) Aggregation mechanism and a Text-Guided Pixel\nDecoder. The former mitigates feature dispersion by dynamically clustering\nfeatures into compact semantic centers to enhance cross-modal correspondence.\nThe latter is designed to bridge the semantic gap by leveraging\ndomain-invariant textual knowledge to effectively guide deep visual\nrepresentations. The synergy between these two mechanisms significantly\nimproves the model's generalization ability. Extensive experiments on public\ncardiac and fundus datasets demonstrate that our method consistently\noutperforms existing SOTA approaches across multiple domain generalization\nbenchmarks.\n", "link": "http://arxiv.org/abs/2509.08570v1", "date": "2025-09-10", "relevancy": 2.3603, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5967}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5967}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Semantic%20Aggregation%20Leveraging%20Foundation%20Model%20for%0A%20%20Generalizable%20Medical%20Image%20Segmentation&body=Title%3A%20Vision-Language%20Semantic%20Aggregation%20Leveraging%20Foundation%20Model%20for%0A%20%20Generalizable%20Medical%20Image%20Segmentation%0AAuthor%3A%20Wenjun%20Yu%20and%20Yinchen%20Zhou%20and%20Jia-Xuan%20Jiang%20and%20Shubin%20Zeng%20and%20Yuee%20Li%20and%20Zhong%20Wang%0AAbstract%3A%20%20%20Multimodal%20models%20have%20achieved%20remarkable%20success%20in%20natural%20image%0Asegmentation%2C%20yet%20they%20often%20underperform%20when%20applied%20to%20the%20medical%20domain.%0AThrough%20extensive%20study%2C%20we%20attribute%20this%20performance%20gap%20to%20the%20challenges%20of%0Amultimodal%20fusion%2C%20primarily%20the%20significant%20semantic%20gap%20between%20abstract%0Atextual%20prompts%20and%20fine-grained%20medical%20visual%20features%2C%20as%20well%20as%20the%0Aresulting%20feature%20dispersion.%20To%20address%20these%20issues%2C%20we%20revisit%20the%20problem%0Afrom%20the%20perspective%20of%20semantic%20aggregation.%20Specifically%2C%20we%20propose%20an%0AExpectation-Maximization%20%28EM%29%20Aggregation%20mechanism%20and%20a%20Text-Guided%20Pixel%0ADecoder.%20The%20former%20mitigates%20feature%20dispersion%20by%20dynamically%20clustering%0Afeatures%20into%20compact%20semantic%20centers%20to%20enhance%20cross-modal%20correspondence.%0AThe%20latter%20is%20designed%20to%20bridge%20the%20semantic%20gap%20by%20leveraging%0Adomain-invariant%20textual%20knowledge%20to%20effectively%20guide%20deep%20visual%0Arepresentations.%20The%20synergy%20between%20these%20two%20mechanisms%20significantly%0Aimproves%20the%20model%27s%20generalization%20ability.%20Extensive%20experiments%20on%20public%0Acardiac%20and%20fundus%20datasets%20demonstrate%20that%20our%20method%20consistently%0Aoutperforms%20existing%20SOTA%20approaches%20across%20multiple%20domain%20generalization%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Semantic%2520Aggregation%2520Leveraging%2520Foundation%2520Model%2520for%250A%2520%2520Generalizable%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DWenjun%2520Yu%2520and%2520Yinchen%2520Zhou%2520and%2520Jia-Xuan%2520Jiang%2520and%2520Shubin%2520Zeng%2520and%2520Yuee%2520Li%2520and%2520Zhong%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520models%2520have%2520achieved%2520remarkable%2520success%2520in%2520natural%2520image%250Asegmentation%252C%2520yet%2520they%2520often%2520underperform%2520when%2520applied%2520to%2520the%2520medical%2520domain.%250AThrough%2520extensive%2520study%252C%2520we%2520attribute%2520this%2520performance%2520gap%2520to%2520the%2520challenges%2520of%250Amultimodal%2520fusion%252C%2520primarily%2520the%2520significant%2520semantic%2520gap%2520between%2520abstract%250Atextual%2520prompts%2520and%2520fine-grained%2520medical%2520visual%2520features%252C%2520as%2520well%2520as%2520the%250Aresulting%2520feature%2520dispersion.%2520To%2520address%2520these%2520issues%252C%2520we%2520revisit%2520the%2520problem%250Afrom%2520the%2520perspective%2520of%2520semantic%2520aggregation.%2520Specifically%252C%2520we%2520propose%2520an%250AExpectation-Maximization%2520%2528EM%2529%2520Aggregation%2520mechanism%2520and%2520a%2520Text-Guided%2520Pixel%250ADecoder.%2520The%2520former%2520mitigates%2520feature%2520dispersion%2520by%2520dynamically%2520clustering%250Afeatures%2520into%2520compact%2520semantic%2520centers%2520to%2520enhance%2520cross-modal%2520correspondence.%250AThe%2520latter%2520is%2520designed%2520to%2520bridge%2520the%2520semantic%2520gap%2520by%2520leveraging%250Adomain-invariant%2520textual%2520knowledge%2520to%2520effectively%2520guide%2520deep%2520visual%250Arepresentations.%2520The%2520synergy%2520between%2520these%2520two%2520mechanisms%2520significantly%250Aimproves%2520the%2520model%2527s%2520generalization%2520ability.%2520Extensive%2520experiments%2520on%2520public%250Acardiac%2520and%2520fundus%2520datasets%2520demonstrate%2520that%2520our%2520method%2520consistently%250Aoutperforms%2520existing%2520SOTA%2520approaches%2520across%2520multiple%2520domain%2520generalization%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Semantic%20Aggregation%20Leveraging%20Foundation%20Model%20for%0A%20%20Generalizable%20Medical%20Image%20Segmentation&entry.906535625=Wenjun%20Yu%20and%20Yinchen%20Zhou%20and%20Jia-Xuan%20Jiang%20and%20Shubin%20Zeng%20and%20Yuee%20Li%20and%20Zhong%20Wang&entry.1292438233=%20%20Multimodal%20models%20have%20achieved%20remarkable%20success%20in%20natural%20image%0Asegmentation%2C%20yet%20they%20often%20underperform%20when%20applied%20to%20the%20medical%20domain.%0AThrough%20extensive%20study%2C%20we%20attribute%20this%20performance%20gap%20to%20the%20challenges%20of%0Amultimodal%20fusion%2C%20primarily%20the%20significant%20semantic%20gap%20between%20abstract%0Atextual%20prompts%20and%20fine-grained%20medical%20visual%20features%2C%20as%20well%20as%20the%0Aresulting%20feature%20dispersion.%20To%20address%20these%20issues%2C%20we%20revisit%20the%20problem%0Afrom%20the%20perspective%20of%20semantic%20aggregation.%20Specifically%2C%20we%20propose%20an%0AExpectation-Maximization%20%28EM%29%20Aggregation%20mechanism%20and%20a%20Text-Guided%20Pixel%0ADecoder.%20The%20former%20mitigates%20feature%20dispersion%20by%20dynamically%20clustering%0Afeatures%20into%20compact%20semantic%20centers%20to%20enhance%20cross-modal%20correspondence.%0AThe%20latter%20is%20designed%20to%20bridge%20the%20semantic%20gap%20by%20leveraging%0Adomain-invariant%20textual%20knowledge%20to%20effectively%20guide%20deep%20visual%0Arepresentations.%20The%20synergy%20between%20these%20two%20mechanisms%20significantly%0Aimproves%20the%20model%27s%20generalization%20ability.%20Extensive%20experiments%20on%20public%0Acardiac%20and%20fundus%20datasets%20demonstrate%20that%20our%20method%20consistently%0Aoutperforms%20existing%20SOTA%20approaches%20across%20multiple%20domain%20generalization%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08570v1&entry.124074799=Read"},
{"title": "CLAP: Clustering to Localize Across n Possibilities, A Simple, Robust\n  Geometric Approach in the Presence of Symmetries", "author": "Gabriel I. Fernandez and Ruochen Hou and Alex Xu and Colin Togashi and Dennis W. Hong", "abstract": "  In this paper, we present our localization method called CLAP, Clustering to\nLocalize Across $n$ Possibilities, which helped us win the RoboCup 2024\nadult-sized autonomous humanoid soccer competition. Competition rules limited\nour sensor suite to stereo vision and an inertial sensor, similar to humans. In\naddition, our robot had to deal with varying lighting conditions, dynamic\nfeature occlusions, noise from high-impact stepping, and mistaken features from\nbystanders and neighboring fields. Therefore, we needed an accurate, and most\nimportantly robust localization algorithm that would be the foundation for our\npath-planning and game-strategy algorithms. CLAP achieves these requirements by\nclustering estimated states of our robot from pairs of field features to\nlocalize its global position and orientation. Correct state estimates naturally\ncluster together, while incorrect estimates spread apart, making CLAP resilient\nto noise and incorrect inputs. CLAP is paired with a particle filter and an\nextended Kalman filter to improve consistency and smoothness. Tests of CLAP\nwith other landmark-based localization methods showed similar accuracy.\nHowever, tests with increased false positive feature detection showed that CLAP\noutperformed other methods in terms of robustness with very little divergence\nand velocity jumps. Our localization performed well in competition, allowing\nour robot to shoot faraway goals and narrowly defend our goal.\n", "link": "http://arxiv.org/abs/2509.08495v1", "date": "2025-09-10", "relevancy": 2.3573, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6421}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5726}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLAP%3A%20Clustering%20to%20Localize%20Across%20n%20Possibilities%2C%20A%20Simple%2C%20Robust%0A%20%20Geometric%20Approach%20in%20the%20Presence%20of%20Symmetries&body=Title%3A%20CLAP%3A%20Clustering%20to%20Localize%20Across%20n%20Possibilities%2C%20A%20Simple%2C%20Robust%0A%20%20Geometric%20Approach%20in%20the%20Presence%20of%20Symmetries%0AAuthor%3A%20Gabriel%20I.%20Fernandez%20and%20Ruochen%20Hou%20and%20Alex%20Xu%20and%20Colin%20Togashi%20and%20Dennis%20W.%20Hong%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20our%20localization%20method%20called%20CLAP%2C%20Clustering%20to%0ALocalize%20Across%20%24n%24%20Possibilities%2C%20which%20helped%20us%20win%20the%20RoboCup%202024%0Aadult-sized%20autonomous%20humanoid%20soccer%20competition.%20Competition%20rules%20limited%0Aour%20sensor%20suite%20to%20stereo%20vision%20and%20an%20inertial%20sensor%2C%20similar%20to%20humans.%20In%0Aaddition%2C%20our%20robot%20had%20to%20deal%20with%20varying%20lighting%20conditions%2C%20dynamic%0Afeature%20occlusions%2C%20noise%20from%20high-impact%20stepping%2C%20and%20mistaken%20features%20from%0Abystanders%20and%20neighboring%20fields.%20Therefore%2C%20we%20needed%20an%20accurate%2C%20and%20most%0Aimportantly%20robust%20localization%20algorithm%20that%20would%20be%20the%20foundation%20for%20our%0Apath-planning%20and%20game-strategy%20algorithms.%20CLAP%20achieves%20these%20requirements%20by%0Aclustering%20estimated%20states%20of%20our%20robot%20from%20pairs%20of%20field%20features%20to%0Alocalize%20its%20global%20position%20and%20orientation.%20Correct%20state%20estimates%20naturally%0Acluster%20together%2C%20while%20incorrect%20estimates%20spread%20apart%2C%20making%20CLAP%20resilient%0Ato%20noise%20and%20incorrect%20inputs.%20CLAP%20is%20paired%20with%20a%20particle%20filter%20and%20an%0Aextended%20Kalman%20filter%20to%20improve%20consistency%20and%20smoothness.%20Tests%20of%20CLAP%0Awith%20other%20landmark-based%20localization%20methods%20showed%20similar%20accuracy.%0AHowever%2C%20tests%20with%20increased%20false%20positive%20feature%20detection%20showed%20that%20CLAP%0Aoutperformed%20other%20methods%20in%20terms%20of%20robustness%20with%20very%20little%20divergence%0Aand%20velocity%20jumps.%20Our%20localization%20performed%20well%20in%20competition%2C%20allowing%0Aour%20robot%20to%20shoot%20faraway%20goals%20and%20narrowly%20defend%20our%20goal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLAP%253A%2520Clustering%2520to%2520Localize%2520Across%2520n%2520Possibilities%252C%2520A%2520Simple%252C%2520Robust%250A%2520%2520Geometric%2520Approach%2520in%2520the%2520Presence%2520of%2520Symmetries%26entry.906535625%3DGabriel%2520I.%2520Fernandez%2520and%2520Ruochen%2520Hou%2520and%2520Alex%2520Xu%2520and%2520Colin%2520Togashi%2520and%2520Dennis%2520W.%2520Hong%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520our%2520localization%2520method%2520called%2520CLAP%252C%2520Clustering%2520to%250ALocalize%2520Across%2520%2524n%2524%2520Possibilities%252C%2520which%2520helped%2520us%2520win%2520the%2520RoboCup%25202024%250Aadult-sized%2520autonomous%2520humanoid%2520soccer%2520competition.%2520Competition%2520rules%2520limited%250Aour%2520sensor%2520suite%2520to%2520stereo%2520vision%2520and%2520an%2520inertial%2520sensor%252C%2520similar%2520to%2520humans.%2520In%250Aaddition%252C%2520our%2520robot%2520had%2520to%2520deal%2520with%2520varying%2520lighting%2520conditions%252C%2520dynamic%250Afeature%2520occlusions%252C%2520noise%2520from%2520high-impact%2520stepping%252C%2520and%2520mistaken%2520features%2520from%250Abystanders%2520and%2520neighboring%2520fields.%2520Therefore%252C%2520we%2520needed%2520an%2520accurate%252C%2520and%2520most%250Aimportantly%2520robust%2520localization%2520algorithm%2520that%2520would%2520be%2520the%2520foundation%2520for%2520our%250Apath-planning%2520and%2520game-strategy%2520algorithms.%2520CLAP%2520achieves%2520these%2520requirements%2520by%250Aclustering%2520estimated%2520states%2520of%2520our%2520robot%2520from%2520pairs%2520of%2520field%2520features%2520to%250Alocalize%2520its%2520global%2520position%2520and%2520orientation.%2520Correct%2520state%2520estimates%2520naturally%250Acluster%2520together%252C%2520while%2520incorrect%2520estimates%2520spread%2520apart%252C%2520making%2520CLAP%2520resilient%250Ato%2520noise%2520and%2520incorrect%2520inputs.%2520CLAP%2520is%2520paired%2520with%2520a%2520particle%2520filter%2520and%2520an%250Aextended%2520Kalman%2520filter%2520to%2520improve%2520consistency%2520and%2520smoothness.%2520Tests%2520of%2520CLAP%250Awith%2520other%2520landmark-based%2520localization%2520methods%2520showed%2520similar%2520accuracy.%250AHowever%252C%2520tests%2520with%2520increased%2520false%2520positive%2520feature%2520detection%2520showed%2520that%2520CLAP%250Aoutperformed%2520other%2520methods%2520in%2520terms%2520of%2520robustness%2520with%2520very%2520little%2520divergence%250Aand%2520velocity%2520jumps.%2520Our%2520localization%2520performed%2520well%2520in%2520competition%252C%2520allowing%250Aour%2520robot%2520to%2520shoot%2520faraway%2520goals%2520and%2520narrowly%2520defend%2520our%2520goal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLAP%3A%20Clustering%20to%20Localize%20Across%20n%20Possibilities%2C%20A%20Simple%2C%20Robust%0A%20%20Geometric%20Approach%20in%20the%20Presence%20of%20Symmetries&entry.906535625=Gabriel%20I.%20Fernandez%20and%20Ruochen%20Hou%20and%20Alex%20Xu%20and%20Colin%20Togashi%20and%20Dennis%20W.%20Hong&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20our%20localization%20method%20called%20CLAP%2C%20Clustering%20to%0ALocalize%20Across%20%24n%24%20Possibilities%2C%20which%20helped%20us%20win%20the%20RoboCup%202024%0Aadult-sized%20autonomous%20humanoid%20soccer%20competition.%20Competition%20rules%20limited%0Aour%20sensor%20suite%20to%20stereo%20vision%20and%20an%20inertial%20sensor%2C%20similar%20to%20humans.%20In%0Aaddition%2C%20our%20robot%20had%20to%20deal%20with%20varying%20lighting%20conditions%2C%20dynamic%0Afeature%20occlusions%2C%20noise%20from%20high-impact%20stepping%2C%20and%20mistaken%20features%20from%0Abystanders%20and%20neighboring%20fields.%20Therefore%2C%20we%20needed%20an%20accurate%2C%20and%20most%0Aimportantly%20robust%20localization%20algorithm%20that%20would%20be%20the%20foundation%20for%20our%0Apath-planning%20and%20game-strategy%20algorithms.%20CLAP%20achieves%20these%20requirements%20by%0Aclustering%20estimated%20states%20of%20our%20robot%20from%20pairs%20of%20field%20features%20to%0Alocalize%20its%20global%20position%20and%20orientation.%20Correct%20state%20estimates%20naturally%0Acluster%20together%2C%20while%20incorrect%20estimates%20spread%20apart%2C%20making%20CLAP%20resilient%0Ato%20noise%20and%20incorrect%20inputs.%20CLAP%20is%20paired%20with%20a%20particle%20filter%20and%20an%0Aextended%20Kalman%20filter%20to%20improve%20consistency%20and%20smoothness.%20Tests%20of%20CLAP%0Awith%20other%20landmark-based%20localization%20methods%20showed%20similar%20accuracy.%0AHowever%2C%20tests%20with%20increased%20false%20positive%20feature%20detection%20showed%20that%20CLAP%0Aoutperformed%20other%20methods%20in%20terms%20of%20robustness%20with%20very%20little%20divergence%0Aand%20velocity%20jumps.%20Our%20localization%20performed%20well%20in%20competition%2C%20allowing%0Aour%20robot%20to%20shoot%20faraway%20goals%20and%20narrowly%20defend%20our%20goal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08495v1&entry.124074799=Read"},
{"title": "Real Time Semantic Segmentation of High Resolution Automotive LiDAR\n  Scans", "author": "Hannes Reichert and Benjamin Serfling and Elijah Sch\u00fcssler and Kerim Turacan and Konrad Doll and Bernhard Sick", "abstract": "  In recent studies, numerous previous works emphasize the importance of\nsemantic segmentation of LiDAR data as a critical component to the development\nof driver-assistance systems and autonomous vehicles. However, many\nstate-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors\nand struggle with real-time constraints. This study introduces a novel semantic\nsegmentation framework tailored for modern high-resolution LiDAR sensors that\naddresses both accuracy and real-time processing demands. We propose a novel\nLiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban\ntraffic scenes. Furthermore, we propose a semantic segmentation method\nutilizing surface normals as strong input features. Our approach is bridging\nthe gap between cutting-edge research and practical automotive applications.\nAdditionaly, we provide a Robot Operating System (ROS2) implementation that we\noperate on our research vehicle. Our dataset and code are publicly available:\nhttps://github.com/kav-institute/SemanticLiDAR.\n", "link": "http://arxiv.org/abs/2504.21602v2", "date": "2025-09-10", "relevancy": 2.338, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6024}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5823}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real%20Time%20Semantic%20Segmentation%20of%20High%20Resolution%20Automotive%20LiDAR%0A%20%20Scans&body=Title%3A%20Real%20Time%20Semantic%20Segmentation%20of%20High%20Resolution%20Automotive%20LiDAR%0A%20%20Scans%0AAuthor%3A%20Hannes%20Reichert%20and%20Benjamin%20Serfling%20and%20Elijah%20Sch%C3%BCssler%20and%20Kerim%20Turacan%20and%20Konrad%20Doll%20and%20Bernhard%20Sick%0AAbstract%3A%20%20%20In%20recent%20studies%2C%20numerous%20previous%20works%20emphasize%20the%20importance%20of%0Asemantic%20segmentation%20of%20LiDAR%20data%20as%20a%20critical%20component%20to%20the%20development%0Aof%20driver-assistance%20systems%20and%20autonomous%20vehicles.%20However%2C%20many%0Astate-of-the-art%20methods%20are%20tested%20on%20outdated%2C%20lower-resolution%20LiDAR%20sensors%0Aand%20struggle%20with%20real-time%20constraints.%20This%20study%20introduces%20a%20novel%20semantic%0Asegmentation%20framework%20tailored%20for%20modern%20high-resolution%20LiDAR%20sensors%20that%0Aaddresses%20both%20accuracy%20and%20real-time%20processing%20demands.%20We%20propose%20a%20novel%0ALiDAR%20dataset%20collected%20by%20a%20cutting-edge%20automotive%20128%20layer%20LiDAR%20in%20urban%0Atraffic%20scenes.%20Furthermore%2C%20we%20propose%20a%20semantic%20segmentation%20method%0Autilizing%20surface%20normals%20as%20strong%20input%20features.%20Our%20approach%20is%20bridging%0Athe%20gap%20between%20cutting-edge%20research%20and%20practical%20automotive%20applications.%0AAdditionaly%2C%20we%20provide%20a%20Robot%20Operating%20System%20%28ROS2%29%20implementation%20that%20we%0Aoperate%20on%20our%20research%20vehicle.%20Our%20dataset%20and%20code%20are%20publicly%20available%3A%0Ahttps%3A//github.com/kav-institute/SemanticLiDAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal%2520Time%2520Semantic%2520Segmentation%2520of%2520High%2520Resolution%2520Automotive%2520LiDAR%250A%2520%2520Scans%26entry.906535625%3DHannes%2520Reichert%2520and%2520Benjamin%2520Serfling%2520and%2520Elijah%2520Sch%25C3%25BCssler%2520and%2520Kerim%2520Turacan%2520and%2520Konrad%2520Doll%2520and%2520Bernhard%2520Sick%26entry.1292438233%3D%2520%2520In%2520recent%2520studies%252C%2520numerous%2520previous%2520works%2520emphasize%2520the%2520importance%2520of%250Asemantic%2520segmentation%2520of%2520LiDAR%2520data%2520as%2520a%2520critical%2520component%2520to%2520the%2520development%250Aof%2520driver-assistance%2520systems%2520and%2520autonomous%2520vehicles.%2520However%252C%2520many%250Astate-of-the-art%2520methods%2520are%2520tested%2520on%2520outdated%252C%2520lower-resolution%2520LiDAR%2520sensors%250Aand%2520struggle%2520with%2520real-time%2520constraints.%2520This%2520study%2520introduces%2520a%2520novel%2520semantic%250Asegmentation%2520framework%2520tailored%2520for%2520modern%2520high-resolution%2520LiDAR%2520sensors%2520that%250Aaddresses%2520both%2520accuracy%2520and%2520real-time%2520processing%2520demands.%2520We%2520propose%2520a%2520novel%250ALiDAR%2520dataset%2520collected%2520by%2520a%2520cutting-edge%2520automotive%2520128%2520layer%2520LiDAR%2520in%2520urban%250Atraffic%2520scenes.%2520Furthermore%252C%2520we%2520propose%2520a%2520semantic%2520segmentation%2520method%250Autilizing%2520surface%2520normals%2520as%2520strong%2520input%2520features.%2520Our%2520approach%2520is%2520bridging%250Athe%2520gap%2520between%2520cutting-edge%2520research%2520and%2520practical%2520automotive%2520applications.%250AAdditionaly%252C%2520we%2520provide%2520a%2520Robot%2520Operating%2520System%2520%2528ROS2%2529%2520implementation%2520that%2520we%250Aoperate%2520on%2520our%2520research%2520vehicle.%2520Our%2520dataset%2520and%2520code%2520are%2520publicly%2520available%253A%250Ahttps%253A//github.com/kav-institute/SemanticLiDAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real%20Time%20Semantic%20Segmentation%20of%20High%20Resolution%20Automotive%20LiDAR%0A%20%20Scans&entry.906535625=Hannes%20Reichert%20and%20Benjamin%20Serfling%20and%20Elijah%20Sch%C3%BCssler%20and%20Kerim%20Turacan%20and%20Konrad%20Doll%20and%20Bernhard%20Sick&entry.1292438233=%20%20In%20recent%20studies%2C%20numerous%20previous%20works%20emphasize%20the%20importance%20of%0Asemantic%20segmentation%20of%20LiDAR%20data%20as%20a%20critical%20component%20to%20the%20development%0Aof%20driver-assistance%20systems%20and%20autonomous%20vehicles.%20However%2C%20many%0Astate-of-the-art%20methods%20are%20tested%20on%20outdated%2C%20lower-resolution%20LiDAR%20sensors%0Aand%20struggle%20with%20real-time%20constraints.%20This%20study%20introduces%20a%20novel%20semantic%0Asegmentation%20framework%20tailored%20for%20modern%20high-resolution%20LiDAR%20sensors%20that%0Aaddresses%20both%20accuracy%20and%20real-time%20processing%20demands.%20We%20propose%20a%20novel%0ALiDAR%20dataset%20collected%20by%20a%20cutting-edge%20automotive%20128%20layer%20LiDAR%20in%20urban%0Atraffic%20scenes.%20Furthermore%2C%20we%20propose%20a%20semantic%20segmentation%20method%0Autilizing%20surface%20normals%20as%20strong%20input%20features.%20Our%20approach%20is%20bridging%0Athe%20gap%20between%20cutting-edge%20research%20and%20practical%20automotive%20applications.%0AAdditionaly%2C%20we%20provide%20a%20Robot%20Operating%20System%20%28ROS2%29%20implementation%20that%20we%0Aoperate%20on%20our%20research%20vehicle.%20Our%20dataset%20and%20code%20are%20publicly%20available%3A%0Ahttps%3A//github.com/kav-institute/SemanticLiDAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21602v2&entry.124074799=Read"},
{"title": "A Structured Review of Underwater Object Detection Challenges and\n  Solutions: From Traditional to Large Vision Language Models", "author": "Edwine Nabahirwa and Wei Song and Minghua Zhang and Yi Fang and Zhou Ni", "abstract": "  Underwater object detection (UOD) is vital to diverse marine applications,\nincluding oceanographic research, underwater robotics, and marine conservation.\nHowever, UOD faces numerous challenges that compromise its performance. Over\nthe years, various methods have been proposed to address these issues, but they\noften fail to fully capture the complexities of underwater environments. This\nreview systematically categorizes UOD challenges into five key areas: Image\nquality degradation, target-related issues, data-related challenges,\ncomputational and processing constraints, and limitations in detection\nmethodologies. To address these challenges, we analyze the progression from\ntraditional image processing and object detection techniques to modern\napproaches. Additionally, we explore the potential of large vision-language\nmodels (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated\nin other domains. We also present case studies, including synthetic dataset\ngeneration using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review\nidentifies three key insights: (i) Current UOD methods are insufficient to\nfully address challenges like image degradation and small object detection in\ndynamic underwater environments. (ii) Synthetic data generation using LVLMs\nshows potential for augmenting datasets but requires further refinement to\nensure realism and applicability. (iii) LVLMs hold significant promise for UOD,\nbut their real-time application remains under-explored, requiring further\nresearch on optimization techniques.\n", "link": "http://arxiv.org/abs/2509.08490v1", "date": "2025-09-10", "relevancy": 2.3363, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5916}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5916}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Structured%20Review%20of%20Underwater%20Object%20Detection%20Challenges%20and%0A%20%20Solutions%3A%20From%20Traditional%20to%20Large%20Vision%20Language%20Models&body=Title%3A%20A%20Structured%20Review%20of%20Underwater%20Object%20Detection%20Challenges%20and%0A%20%20Solutions%3A%20From%20Traditional%20to%20Large%20Vision%20Language%20Models%0AAuthor%3A%20Edwine%20Nabahirwa%20and%20Wei%20Song%20and%20Minghua%20Zhang%20and%20Yi%20Fang%20and%20Zhou%20Ni%0AAbstract%3A%20%20%20Underwater%20object%20detection%20%28UOD%29%20is%20vital%20to%20diverse%20marine%20applications%2C%0Aincluding%20oceanographic%20research%2C%20underwater%20robotics%2C%20and%20marine%20conservation.%0AHowever%2C%20UOD%20faces%20numerous%20challenges%20that%20compromise%20its%20performance.%20Over%0Athe%20years%2C%20various%20methods%20have%20been%20proposed%20to%20address%20these%20issues%2C%20but%20they%0Aoften%20fail%20to%20fully%20capture%20the%20complexities%20of%20underwater%20environments.%20This%0Areview%20systematically%20categorizes%20UOD%20challenges%20into%20five%20key%20areas%3A%20Image%0Aquality%20degradation%2C%20target-related%20issues%2C%20data-related%20challenges%2C%0Acomputational%20and%20processing%20constraints%2C%20and%20limitations%20in%20detection%0Amethodologies.%20To%20address%20these%20challenges%2C%20we%20analyze%20the%20progression%20from%0Atraditional%20image%20processing%20and%20object%20detection%20techniques%20to%20modern%0Aapproaches.%20Additionally%2C%20we%20explore%20the%20potential%20of%20large%20vision-language%0Amodels%20%28LVLMs%29%20in%20UOD%2C%20leveraging%20their%20multi-modal%20capabilities%20demonstrated%0Ain%20other%20domains.%20We%20also%20present%20case%20studies%2C%20including%20synthetic%20dataset%0Ageneration%20using%20DALL-E%203%20and%20fine-tuning%20Florence-2%20LVLM%20for%20UOD.%20This%20review%0Aidentifies%20three%20key%20insights%3A%20%28i%29%20Current%20UOD%20methods%20are%20insufficient%20to%0Afully%20address%20challenges%20like%20image%20degradation%20and%20small%20object%20detection%20in%0Adynamic%20underwater%20environments.%20%28ii%29%20Synthetic%20data%20generation%20using%20LVLMs%0Ashows%20potential%20for%20augmenting%20datasets%20but%20requires%20further%20refinement%20to%0Aensure%20realism%20and%20applicability.%20%28iii%29%20LVLMs%20hold%20significant%20promise%20for%20UOD%2C%0Abut%20their%20real-time%20application%20remains%20under-explored%2C%20requiring%20further%0Aresearch%20on%20optimization%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Structured%2520Review%2520of%2520Underwater%2520Object%2520Detection%2520Challenges%2520and%250A%2520%2520Solutions%253A%2520From%2520Traditional%2520to%2520Large%2520Vision%2520Language%2520Models%26entry.906535625%3DEdwine%2520Nabahirwa%2520and%2520Wei%2520Song%2520and%2520Minghua%2520Zhang%2520and%2520Yi%2520Fang%2520and%2520Zhou%2520Ni%26entry.1292438233%3D%2520%2520Underwater%2520object%2520detection%2520%2528UOD%2529%2520is%2520vital%2520to%2520diverse%2520marine%2520applications%252C%250Aincluding%2520oceanographic%2520research%252C%2520underwater%2520robotics%252C%2520and%2520marine%2520conservation.%250AHowever%252C%2520UOD%2520faces%2520numerous%2520challenges%2520that%2520compromise%2520its%2520performance.%2520Over%250Athe%2520years%252C%2520various%2520methods%2520have%2520been%2520proposed%2520to%2520address%2520these%2520issues%252C%2520but%2520they%250Aoften%2520fail%2520to%2520fully%2520capture%2520the%2520complexities%2520of%2520underwater%2520environments.%2520This%250Areview%2520systematically%2520categorizes%2520UOD%2520challenges%2520into%2520five%2520key%2520areas%253A%2520Image%250Aquality%2520degradation%252C%2520target-related%2520issues%252C%2520data-related%2520challenges%252C%250Acomputational%2520and%2520processing%2520constraints%252C%2520and%2520limitations%2520in%2520detection%250Amethodologies.%2520To%2520address%2520these%2520challenges%252C%2520we%2520analyze%2520the%2520progression%2520from%250Atraditional%2520image%2520processing%2520and%2520object%2520detection%2520techniques%2520to%2520modern%250Aapproaches.%2520Additionally%252C%2520we%2520explore%2520the%2520potential%2520of%2520large%2520vision-language%250Amodels%2520%2528LVLMs%2529%2520in%2520UOD%252C%2520leveraging%2520their%2520multi-modal%2520capabilities%2520demonstrated%250Ain%2520other%2520domains.%2520We%2520also%2520present%2520case%2520studies%252C%2520including%2520synthetic%2520dataset%250Ageneration%2520using%2520DALL-E%25203%2520and%2520fine-tuning%2520Florence-2%2520LVLM%2520for%2520UOD.%2520This%2520review%250Aidentifies%2520three%2520key%2520insights%253A%2520%2528i%2529%2520Current%2520UOD%2520methods%2520are%2520insufficient%2520to%250Afully%2520address%2520challenges%2520like%2520image%2520degradation%2520and%2520small%2520object%2520detection%2520in%250Adynamic%2520underwater%2520environments.%2520%2528ii%2529%2520Synthetic%2520data%2520generation%2520using%2520LVLMs%250Ashows%2520potential%2520for%2520augmenting%2520datasets%2520but%2520requires%2520further%2520refinement%2520to%250Aensure%2520realism%2520and%2520applicability.%2520%2528iii%2529%2520LVLMs%2520hold%2520significant%2520promise%2520for%2520UOD%252C%250Abut%2520their%2520real-time%2520application%2520remains%2520under-explored%252C%2520requiring%2520further%250Aresearch%2520on%2520optimization%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Structured%20Review%20of%20Underwater%20Object%20Detection%20Challenges%20and%0A%20%20Solutions%3A%20From%20Traditional%20to%20Large%20Vision%20Language%20Models&entry.906535625=Edwine%20Nabahirwa%20and%20Wei%20Song%20and%20Minghua%20Zhang%20and%20Yi%20Fang%20and%20Zhou%20Ni&entry.1292438233=%20%20Underwater%20object%20detection%20%28UOD%29%20is%20vital%20to%20diverse%20marine%20applications%2C%0Aincluding%20oceanographic%20research%2C%20underwater%20robotics%2C%20and%20marine%20conservation.%0AHowever%2C%20UOD%20faces%20numerous%20challenges%20that%20compromise%20its%20performance.%20Over%0Athe%20years%2C%20various%20methods%20have%20been%20proposed%20to%20address%20these%20issues%2C%20but%20they%0Aoften%20fail%20to%20fully%20capture%20the%20complexities%20of%20underwater%20environments.%20This%0Areview%20systematically%20categorizes%20UOD%20challenges%20into%20five%20key%20areas%3A%20Image%0Aquality%20degradation%2C%20target-related%20issues%2C%20data-related%20challenges%2C%0Acomputational%20and%20processing%20constraints%2C%20and%20limitations%20in%20detection%0Amethodologies.%20To%20address%20these%20challenges%2C%20we%20analyze%20the%20progression%20from%0Atraditional%20image%20processing%20and%20object%20detection%20techniques%20to%20modern%0Aapproaches.%20Additionally%2C%20we%20explore%20the%20potential%20of%20large%20vision-language%0Amodels%20%28LVLMs%29%20in%20UOD%2C%20leveraging%20their%20multi-modal%20capabilities%20demonstrated%0Ain%20other%20domains.%20We%20also%20present%20case%20studies%2C%20including%20synthetic%20dataset%0Ageneration%20using%20DALL-E%203%20and%20fine-tuning%20Florence-2%20LVLM%20for%20UOD.%20This%20review%0Aidentifies%20three%20key%20insights%3A%20%28i%29%20Current%20UOD%20methods%20are%20insufficient%20to%0Afully%20address%20challenges%20like%20image%20degradation%20and%20small%20object%20detection%20in%0Adynamic%20underwater%20environments.%20%28ii%29%20Synthetic%20data%20generation%20using%20LVLMs%0Ashows%20potential%20for%20augmenting%20datasets%20but%20requires%20further%20refinement%20to%0Aensure%20realism%20and%20applicability.%20%28iii%29%20LVLMs%20hold%20significant%20promise%20for%20UOD%2C%0Abut%20their%20real-time%20application%20remains%20under-explored%2C%20requiring%20further%0Aresearch%20on%20optimization%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08490v1&entry.124074799=Read"},
{"title": "VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making", "author": "Mohamed Salim Aissi and Clemence Grislain and Mohamed Chetouani and Olivier Sigaud and Laure Soulier and Nicolas Thome", "abstract": "  While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.\n", "link": "http://arxiv.org/abs/2503.15108v3", "date": "2025-09-10", "relevancy": 2.3297, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5881}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIPER%3A%20Visual%20Perception%20and%20Explainable%20Reasoning%20for%20Sequential%0A%20%20Decision-Making&body=Title%3A%20VIPER%3A%20Visual%20Perception%20and%20Explainable%20Reasoning%20for%20Sequential%0A%20%20Decision-Making%0AAuthor%3A%20Mohamed%20Salim%20Aissi%20and%20Clemence%20Grislain%20and%20Mohamed%20Chetouani%20and%20Olivier%20Sigaud%20and%20Laure%20Soulier%20and%20Nicolas%20Thome%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20reasoning%20on%20text%20and%0AVision-Language%20Models%20%28VLMs%29%20are%20highly%20effective%20for%20visual%20perception%2C%0Aapplying%20those%20models%20for%20visual%20instruction-based%20planning%20remains%20a%20widely%0Aopen%20problem.%20In%20this%20paper%2C%20we%20introduce%20VIPER%2C%20a%20novel%20framework%20for%0Amultimodal%20instruction-based%20planning%20that%20integrates%20VLM-based%20perception%20with%0ALLM-based%20reasoning.%20Our%20approach%20uses%20a%20modular%20pipeline%20where%20a%20frozen%20VLM%0Agenerates%20textual%20descriptions%20of%20image%20observations%2C%20which%20are%20then%20processed%0Aby%20an%20LLM%20policy%20to%20predict%20actions%20based%20on%20the%20task%20goal.%20We%20fine-tune%20the%0Areasoning%20module%20using%20behavioral%20cloning%20and%20reinforcement%20learning%2C%20improving%0Aour%20agent%27s%20decision-making%20capabilities.%20Experiments%20on%20the%20ALFWorld%20benchmark%0Ashow%20that%20VIPER%20significantly%20outperforms%20state-of-the-art%20visual%0Ainstruction-based%20planners%20while%20narrowing%20the%20gap%20with%20purely%20text-based%0Aoracles.%20By%20leveraging%20text%20as%20an%20intermediate%20representation%2C%20VIPER%20also%0Aenhances%20explainability%2C%20paving%20the%20way%20for%20a%20fine-grained%20analysis%20of%0Aperception%20and%20reasoning%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.15108v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIPER%253A%2520Visual%2520Perception%2520and%2520Explainable%2520Reasoning%2520for%2520Sequential%250A%2520%2520Decision-Making%26entry.906535625%3DMohamed%2520Salim%2520Aissi%2520and%2520Clemence%2520Grislain%2520and%2520Mohamed%2520Chetouani%2520and%2520Olivier%2520Sigaud%2520and%2520Laure%2520Soulier%2520and%2520Nicolas%2520Thome%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520at%2520reasoning%2520on%2520text%2520and%250AVision-Language%2520Models%2520%2528VLMs%2529%2520are%2520highly%2520effective%2520for%2520visual%2520perception%252C%250Aapplying%2520those%2520models%2520for%2520visual%2520instruction-based%2520planning%2520remains%2520a%2520widely%250Aopen%2520problem.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VIPER%252C%2520a%2520novel%2520framework%2520for%250Amultimodal%2520instruction-based%2520planning%2520that%2520integrates%2520VLM-based%2520perception%2520with%250ALLM-based%2520reasoning.%2520Our%2520approach%2520uses%2520a%2520modular%2520pipeline%2520where%2520a%2520frozen%2520VLM%250Agenerates%2520textual%2520descriptions%2520of%2520image%2520observations%252C%2520which%2520are%2520then%2520processed%250Aby%2520an%2520LLM%2520policy%2520to%2520predict%2520actions%2520based%2520on%2520the%2520task%2520goal.%2520We%2520fine-tune%2520the%250Areasoning%2520module%2520using%2520behavioral%2520cloning%2520and%2520reinforcement%2520learning%252C%2520improving%250Aour%2520agent%2527s%2520decision-making%2520capabilities.%2520Experiments%2520on%2520the%2520ALFWorld%2520benchmark%250Ashow%2520that%2520VIPER%2520significantly%2520outperforms%2520state-of-the-art%2520visual%250Ainstruction-based%2520planners%2520while%2520narrowing%2520the%2520gap%2520with%2520purely%2520text-based%250Aoracles.%2520By%2520leveraging%2520text%2520as%2520an%2520intermediate%2520representation%252C%2520VIPER%2520also%250Aenhances%2520explainability%252C%2520paving%2520the%2520way%2520for%2520a%2520fine-grained%2520analysis%2520of%250Aperception%2520and%2520reasoning%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15108v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIPER%3A%20Visual%20Perception%20and%20Explainable%20Reasoning%20for%20Sequential%0A%20%20Decision-Making&entry.906535625=Mohamed%20Salim%20Aissi%20and%20Clemence%20Grislain%20and%20Mohamed%20Chetouani%20and%20Olivier%20Sigaud%20and%20Laure%20Soulier%20and%20Nicolas%20Thome&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20reasoning%20on%20text%20and%0AVision-Language%20Models%20%28VLMs%29%20are%20highly%20effective%20for%20visual%20perception%2C%0Aapplying%20those%20models%20for%20visual%20instruction-based%20planning%20remains%20a%20widely%0Aopen%20problem.%20In%20this%20paper%2C%20we%20introduce%20VIPER%2C%20a%20novel%20framework%20for%0Amultimodal%20instruction-based%20planning%20that%20integrates%20VLM-based%20perception%20with%0ALLM-based%20reasoning.%20Our%20approach%20uses%20a%20modular%20pipeline%20where%20a%20frozen%20VLM%0Agenerates%20textual%20descriptions%20of%20image%20observations%2C%20which%20are%20then%20processed%0Aby%20an%20LLM%20policy%20to%20predict%20actions%20based%20on%20the%20task%20goal.%20We%20fine-tune%20the%0Areasoning%20module%20using%20behavioral%20cloning%20and%20reinforcement%20learning%2C%20improving%0Aour%20agent%27s%20decision-making%20capabilities.%20Experiments%20on%20the%20ALFWorld%20benchmark%0Ashow%20that%20VIPER%20significantly%20outperforms%20state-of-the-art%20visual%0Ainstruction-based%20planners%20while%20narrowing%20the%20gap%20with%20purely%20text-based%0Aoracles.%20By%20leveraging%20text%20as%20an%20intermediate%20representation%2C%20VIPER%20also%0Aenhances%20explainability%2C%20paving%20the%20way%20for%20a%20fine-grained%20analysis%20of%0Aperception%20and%20reasoning%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.15108v3&entry.124074799=Read"},
{"title": "EfficientIML: Efficient High-Resolution Image Manipulation Localization", "author": "Jinhan Li and Haoyang He and Lei Xie and Jiangning Zhang", "abstract": "  With imaging devices delivering ever-higher resolutions and the emerging\ndiffusion-based forgery methods, current detectors trained only on traditional\ndatasets (with splicing, copy-moving and object removal forgeries) lack\nexposure to this new manipulation type. To address this, we propose a novel\nhigh-resolution SIF dataset of 1200+ diffusion-generated manipulations with\nsemantically extracted masks. However, this also imposes a challenge on\nexisting methods, as they face significant computational resource constraints\ndue to their prohibitive computational complexities. Therefore, we propose a\nnovel EfficientIML model with a lightweight, three-stage EfficientRWKV\nbackbone. EfficientRWKV's hybrid state-space and attention network captures\nglobal context and local details in parallel, while a multi-scale supervision\nstrategy enforces consistency across hierarchical predictions. Extensive\nevaluations on our dataset and standard benchmarks demonstrate that our\napproach outperforms ViT-based and other SOTA lightweight baselines in\nlocalization performance, FLOPs and inference speed, underscoring its\nsuitability for real-time forensic applications.\n", "link": "http://arxiv.org/abs/2509.08583v1", "date": "2025-09-10", "relevancy": 2.3258, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5994}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5841}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EfficientIML%3A%20Efficient%20High-Resolution%20Image%20Manipulation%20Localization&body=Title%3A%20EfficientIML%3A%20Efficient%20High-Resolution%20Image%20Manipulation%20Localization%0AAuthor%3A%20Jinhan%20Li%20and%20Haoyang%20He%20and%20Lei%20Xie%20and%20Jiangning%20Zhang%0AAbstract%3A%20%20%20With%20imaging%20devices%20delivering%20ever-higher%20resolutions%20and%20the%20emerging%0Adiffusion-based%20forgery%20methods%2C%20current%20detectors%20trained%20only%20on%20traditional%0Adatasets%20%28with%20splicing%2C%20copy-moving%20and%20object%20removal%20forgeries%29%20lack%0Aexposure%20to%20this%20new%20manipulation%20type.%20To%20address%20this%2C%20we%20propose%20a%20novel%0Ahigh-resolution%20SIF%20dataset%20of%201200%2B%20diffusion-generated%20manipulations%20with%0Asemantically%20extracted%20masks.%20However%2C%20this%20also%20imposes%20a%20challenge%20on%0Aexisting%20methods%2C%20as%20they%20face%20significant%20computational%20resource%20constraints%0Adue%20to%20their%20prohibitive%20computational%20complexities.%20Therefore%2C%20we%20propose%20a%0Anovel%20EfficientIML%20model%20with%20a%20lightweight%2C%20three-stage%20EfficientRWKV%0Abackbone.%20EfficientRWKV%27s%20hybrid%20state-space%20and%20attention%20network%20captures%0Aglobal%20context%20and%20local%20details%20in%20parallel%2C%20while%20a%20multi-scale%20supervision%0Astrategy%20enforces%20consistency%20across%20hierarchical%20predictions.%20Extensive%0Aevaluations%20on%20our%20dataset%20and%20standard%20benchmarks%20demonstrate%20that%20our%0Aapproach%20outperforms%20ViT-based%20and%20other%20SOTA%20lightweight%20baselines%20in%0Alocalization%20performance%2C%20FLOPs%20and%20inference%20speed%2C%20underscoring%20its%0Asuitability%20for%20real-time%20forensic%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficientIML%253A%2520Efficient%2520High-Resolution%2520Image%2520Manipulation%2520Localization%26entry.906535625%3DJinhan%2520Li%2520and%2520Haoyang%2520He%2520and%2520Lei%2520Xie%2520and%2520Jiangning%2520Zhang%26entry.1292438233%3D%2520%2520With%2520imaging%2520devices%2520delivering%2520ever-higher%2520resolutions%2520and%2520the%2520emerging%250Adiffusion-based%2520forgery%2520methods%252C%2520current%2520detectors%2520trained%2520only%2520on%2520traditional%250Adatasets%2520%2528with%2520splicing%252C%2520copy-moving%2520and%2520object%2520removal%2520forgeries%2529%2520lack%250Aexposure%2520to%2520this%2520new%2520manipulation%2520type.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%250Ahigh-resolution%2520SIF%2520dataset%2520of%25201200%252B%2520diffusion-generated%2520manipulations%2520with%250Asemantically%2520extracted%2520masks.%2520However%252C%2520this%2520also%2520imposes%2520a%2520challenge%2520on%250Aexisting%2520methods%252C%2520as%2520they%2520face%2520significant%2520computational%2520resource%2520constraints%250Adue%2520to%2520their%2520prohibitive%2520computational%2520complexities.%2520Therefore%252C%2520we%2520propose%2520a%250Anovel%2520EfficientIML%2520model%2520with%2520a%2520lightweight%252C%2520three-stage%2520EfficientRWKV%250Abackbone.%2520EfficientRWKV%2527s%2520hybrid%2520state-space%2520and%2520attention%2520network%2520captures%250Aglobal%2520context%2520and%2520local%2520details%2520in%2520parallel%252C%2520while%2520a%2520multi-scale%2520supervision%250Astrategy%2520enforces%2520consistency%2520across%2520hierarchical%2520predictions.%2520Extensive%250Aevaluations%2520on%2520our%2520dataset%2520and%2520standard%2520benchmarks%2520demonstrate%2520that%2520our%250Aapproach%2520outperforms%2520ViT-based%2520and%2520other%2520SOTA%2520lightweight%2520baselines%2520in%250Alocalization%2520performance%252C%2520FLOPs%2520and%2520inference%2520speed%252C%2520underscoring%2520its%250Asuitability%2520for%2520real-time%2520forensic%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EfficientIML%3A%20Efficient%20High-Resolution%20Image%20Manipulation%20Localization&entry.906535625=Jinhan%20Li%20and%20Haoyang%20He%20and%20Lei%20Xie%20and%20Jiangning%20Zhang&entry.1292438233=%20%20With%20imaging%20devices%20delivering%20ever-higher%20resolutions%20and%20the%20emerging%0Adiffusion-based%20forgery%20methods%2C%20current%20detectors%20trained%20only%20on%20traditional%0Adatasets%20%28with%20splicing%2C%20copy-moving%20and%20object%20removal%20forgeries%29%20lack%0Aexposure%20to%20this%20new%20manipulation%20type.%20To%20address%20this%2C%20we%20propose%20a%20novel%0Ahigh-resolution%20SIF%20dataset%20of%201200%2B%20diffusion-generated%20manipulations%20with%0Asemantically%20extracted%20masks.%20However%2C%20this%20also%20imposes%20a%20challenge%20on%0Aexisting%20methods%2C%20as%20they%20face%20significant%20computational%20resource%20constraints%0Adue%20to%20their%20prohibitive%20computational%20complexities.%20Therefore%2C%20we%20propose%20a%0Anovel%20EfficientIML%20model%20with%20a%20lightweight%2C%20three-stage%20EfficientRWKV%0Abackbone.%20EfficientRWKV%27s%20hybrid%20state-space%20and%20attention%20network%20captures%0Aglobal%20context%20and%20local%20details%20in%20parallel%2C%20while%20a%20multi-scale%20supervision%0Astrategy%20enforces%20consistency%20across%20hierarchical%20predictions.%20Extensive%0Aevaluations%20on%20our%20dataset%20and%20standard%20benchmarks%20demonstrate%20that%20our%0Aapproach%20outperforms%20ViT-based%20and%20other%20SOTA%20lightweight%20baselines%20in%0Alocalization%20performance%2C%20FLOPs%20and%20inference%20speed%2C%20underscoring%20its%0Asuitability%20for%20real-time%20forensic%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08583v1&entry.124074799=Read"},
{"title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning", "author": "Zhiheng Xi and Jixuan Huang and Chenyang Liao and Baodai Huang and Honglin Guo and Jiaqi Liu and Rui Zheng and Junjie Ye and Jiazheng Zhang and Wenxiang Chen and Wei He and Yiwen Ding and Guanyu Li and Zehui Chen and Zhengyin Du and Xuesong Yao and Yufei Xu and Jiecao Chen and Tao Gui and Zuxuan Wu and Qi Zhang and Xuanjing Huang and Yu-Gang Jiang", "abstract": "  Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.\n", "link": "http://arxiv.org/abs/2509.08755v1", "date": "2025-09-10", "relevancy": 2.3144, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.6012}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5877}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentGym-RL%3A%20Training%20LLM%20Agents%20for%20Long-Horizon%20Decision%20Making%0A%20%20through%20Multi-Turn%20Reinforcement%20Learning&body=Title%3A%20AgentGym-RL%3A%20Training%20LLM%20Agents%20for%20Long-Horizon%20Decision%20Making%0A%20%20through%20Multi-Turn%20Reinforcement%20Learning%0AAuthor%3A%20Zhiheng%20Xi%20and%20Jixuan%20Huang%20and%20Chenyang%20Liao%20and%20Baodai%20Huang%20and%20Honglin%20Guo%20and%20Jiaqi%20Liu%20and%20Rui%20Zheng%20and%20Junjie%20Ye%20and%20Jiazheng%20Zhang%20and%20Wenxiang%20Chen%20and%20Wei%20He%20and%20Yiwen%20Ding%20and%20Guanyu%20Li%20and%20Zehui%20Chen%20and%20Zhengyin%20Du%20and%20Xuesong%20Yao%20and%20Yufei%20Xu%20and%20Jiecao%20Chen%20and%20Tao%20Gui%20and%20Zuxuan%20Wu%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Developing%20autonomous%20LLM%20agents%20capable%20of%20making%20a%20series%20of%20intelligent%0Adecisions%20to%20solve%20complex%2C%20real-world%20tasks%20is%20a%20fast-evolving%20frontier.%20Like%0Ahuman%20cognitive%20development%2C%20agents%20are%20expected%20to%20acquire%20knowledge%20and%0Askills%20through%20exploration%20and%20interaction%20with%20the%20environment.%20Despite%0Aadvances%2C%20the%20community%20still%20lacks%20a%20unified%2C%20interactive%20reinforcement%0Alearning%20%28RL%29%20framework%20that%20can%20effectively%20train%20such%20agents%20from%20scratch%20--%0Awithout%20relying%20on%20supervised%20fine-tuning%20%28SFT%29%20--%20across%20diverse%20and%20realistic%0Aenvironments.%20To%20bridge%20this%20gap%2C%20we%20introduce%20AgentGym-RL%2C%20a%20new%20framework%20to%0Atrain%20LLM%20agents%20for%20multi-turn%20interactive%20decision-making%20through%20RL.%20The%0Aframework%20features%20a%20modular%20and%20decoupled%20architecture%2C%20ensuring%20high%0Aflexibility%20and%20extensibility.%20It%20encompasses%20a%20wide%20variety%20of%20real-world%0Ascenarios%2C%20and%20supports%20mainstream%20RL%20algorithms.%20Furthermore%2C%20we%20propose%0AScalingInter-RL%2C%20a%20training%20approach%20designed%20for%20exploration-exploitation%0Abalance%20and%20stable%20RL%20optimization.%20In%20early%20stages%2C%20it%20emphasizes%20exploitation%0Aby%20restricting%20the%20number%20of%20interactions%2C%20and%20gradually%20shifts%20towards%0Aexploration%20with%20larger%20horizons%20to%20encourage%20diverse%20problem-solving%0Astrategies.%20In%20this%20way%2C%20the%20agent%20develops%20more%20diverse%20behaviors%20and%20is%20less%0Aprone%20to%20collapse%20under%20long%20horizons.%20We%20perform%20extensive%20experiments%20to%0Avalidate%20the%20stability%20and%20effectiveness%20of%20both%20the%20AgentGym-RL%20framework%20and%0Athe%20ScalingInter-RL%20approach.%20Our%20agents%20match%20or%20surpass%20commercial%20models%20on%0A27%20tasks%20across%20diverse%20environments.%20We%20offer%20key%20insights%20and%20will%0Aopen-source%20the%20complete%20AgentGym-RL%20framework%20--%20including%20code%20and%20datasets%0A--%20to%20empower%20the%20research%20community%20in%20developing%20the%20next%20generation%20of%0Aintelligent%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentGym-RL%253A%2520Training%2520LLM%2520Agents%2520for%2520Long-Horizon%2520Decision%2520Making%250A%2520%2520through%2520Multi-Turn%2520Reinforcement%2520Learning%26entry.906535625%3DZhiheng%2520Xi%2520and%2520Jixuan%2520Huang%2520and%2520Chenyang%2520Liao%2520and%2520Baodai%2520Huang%2520and%2520Honglin%2520Guo%2520and%2520Jiaqi%2520Liu%2520and%2520Rui%2520Zheng%2520and%2520Junjie%2520Ye%2520and%2520Jiazheng%2520Zhang%2520and%2520Wenxiang%2520Chen%2520and%2520Wei%2520He%2520and%2520Yiwen%2520Ding%2520and%2520Guanyu%2520Li%2520and%2520Zehui%2520Chen%2520and%2520Zhengyin%2520Du%2520and%2520Xuesong%2520Yao%2520and%2520Yufei%2520Xu%2520and%2520Jiecao%2520Chen%2520and%2520Tao%2520Gui%2520and%2520Zuxuan%2520Wu%2520and%2520Qi%2520Zhang%2520and%2520Xuanjing%2520Huang%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Developing%2520autonomous%2520LLM%2520agents%2520capable%2520of%2520making%2520a%2520series%2520of%2520intelligent%250Adecisions%2520to%2520solve%2520complex%252C%2520real-world%2520tasks%2520is%2520a%2520fast-evolving%2520frontier.%2520Like%250Ahuman%2520cognitive%2520development%252C%2520agents%2520are%2520expected%2520to%2520acquire%2520knowledge%2520and%250Askills%2520through%2520exploration%2520and%2520interaction%2520with%2520the%2520environment.%2520Despite%250Aadvances%252C%2520the%2520community%2520still%2520lacks%2520a%2520unified%252C%2520interactive%2520reinforcement%250Alearning%2520%2528RL%2529%2520framework%2520that%2520can%2520effectively%2520train%2520such%2520agents%2520from%2520scratch%2520--%250Awithout%2520relying%2520on%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520--%2520across%2520diverse%2520and%2520realistic%250Aenvironments.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520AgentGym-RL%252C%2520a%2520new%2520framework%2520to%250Atrain%2520LLM%2520agents%2520for%2520multi-turn%2520interactive%2520decision-making%2520through%2520RL.%2520The%250Aframework%2520features%2520a%2520modular%2520and%2520decoupled%2520architecture%252C%2520ensuring%2520high%250Aflexibility%2520and%2520extensibility.%2520It%2520encompasses%2520a%2520wide%2520variety%2520of%2520real-world%250Ascenarios%252C%2520and%2520supports%2520mainstream%2520RL%2520algorithms.%2520Furthermore%252C%2520we%2520propose%250AScalingInter-RL%252C%2520a%2520training%2520approach%2520designed%2520for%2520exploration-exploitation%250Abalance%2520and%2520stable%2520RL%2520optimization.%2520In%2520early%2520stages%252C%2520it%2520emphasizes%2520exploitation%250Aby%2520restricting%2520the%2520number%2520of%2520interactions%252C%2520and%2520gradually%2520shifts%2520towards%250Aexploration%2520with%2520larger%2520horizons%2520to%2520encourage%2520diverse%2520problem-solving%250Astrategies.%2520In%2520this%2520way%252C%2520the%2520agent%2520develops%2520more%2520diverse%2520behaviors%2520and%2520is%2520less%250Aprone%2520to%2520collapse%2520under%2520long%2520horizons.%2520We%2520perform%2520extensive%2520experiments%2520to%250Avalidate%2520the%2520stability%2520and%2520effectiveness%2520of%2520both%2520the%2520AgentGym-RL%2520framework%2520and%250Athe%2520ScalingInter-RL%2520approach.%2520Our%2520agents%2520match%2520or%2520surpass%2520commercial%2520models%2520on%250A27%2520tasks%2520across%2520diverse%2520environments.%2520We%2520offer%2520key%2520insights%2520and%2520will%250Aopen-source%2520the%2520complete%2520AgentGym-RL%2520framework%2520--%2520including%2520code%2520and%2520datasets%250A--%2520to%2520empower%2520the%2520research%2520community%2520in%2520developing%2520the%2520next%2520generation%2520of%250Aintelligent%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentGym-RL%3A%20Training%20LLM%20Agents%20for%20Long-Horizon%20Decision%20Making%0A%20%20through%20Multi-Turn%20Reinforcement%20Learning&entry.906535625=Zhiheng%20Xi%20and%20Jixuan%20Huang%20and%20Chenyang%20Liao%20and%20Baodai%20Huang%20and%20Honglin%20Guo%20and%20Jiaqi%20Liu%20and%20Rui%20Zheng%20and%20Junjie%20Ye%20and%20Jiazheng%20Zhang%20and%20Wenxiang%20Chen%20and%20Wei%20He%20and%20Yiwen%20Ding%20and%20Guanyu%20Li%20and%20Zehui%20Chen%20and%20Zhengyin%20Du%20and%20Xuesong%20Yao%20and%20Yufei%20Xu%20and%20Jiecao%20Chen%20and%20Tao%20Gui%20and%20Zuxuan%20Wu%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Developing%20autonomous%20LLM%20agents%20capable%20of%20making%20a%20series%20of%20intelligent%0Adecisions%20to%20solve%20complex%2C%20real-world%20tasks%20is%20a%20fast-evolving%20frontier.%20Like%0Ahuman%20cognitive%20development%2C%20agents%20are%20expected%20to%20acquire%20knowledge%20and%0Askills%20through%20exploration%20and%20interaction%20with%20the%20environment.%20Despite%0Aadvances%2C%20the%20community%20still%20lacks%20a%20unified%2C%20interactive%20reinforcement%0Alearning%20%28RL%29%20framework%20that%20can%20effectively%20train%20such%20agents%20from%20scratch%20--%0Awithout%20relying%20on%20supervised%20fine-tuning%20%28SFT%29%20--%20across%20diverse%20and%20realistic%0Aenvironments.%20To%20bridge%20this%20gap%2C%20we%20introduce%20AgentGym-RL%2C%20a%20new%20framework%20to%0Atrain%20LLM%20agents%20for%20multi-turn%20interactive%20decision-making%20through%20RL.%20The%0Aframework%20features%20a%20modular%20and%20decoupled%20architecture%2C%20ensuring%20high%0Aflexibility%20and%20extensibility.%20It%20encompasses%20a%20wide%20variety%20of%20real-world%0Ascenarios%2C%20and%20supports%20mainstream%20RL%20algorithms.%20Furthermore%2C%20we%20propose%0AScalingInter-RL%2C%20a%20training%20approach%20designed%20for%20exploration-exploitation%0Abalance%20and%20stable%20RL%20optimization.%20In%20early%20stages%2C%20it%20emphasizes%20exploitation%0Aby%20restricting%20the%20number%20of%20interactions%2C%20and%20gradually%20shifts%20towards%0Aexploration%20with%20larger%20horizons%20to%20encourage%20diverse%20problem-solving%0Astrategies.%20In%20this%20way%2C%20the%20agent%20develops%20more%20diverse%20behaviors%20and%20is%20less%0Aprone%20to%20collapse%20under%20long%20horizons.%20We%20perform%20extensive%20experiments%20to%0Avalidate%20the%20stability%20and%20effectiveness%20of%20both%20the%20AgentGym-RL%20framework%20and%0Athe%20ScalingInter-RL%20approach.%20Our%20agents%20match%20or%20surpass%20commercial%20models%20on%0A27%20tasks%20across%20diverse%20environments.%20We%20offer%20key%20insights%20and%20will%0Aopen-source%20the%20complete%20AgentGym-RL%20framework%20--%20including%20code%20and%20datasets%0A--%20to%20empower%20the%20research%20community%20in%20developing%20the%20next%20generation%20of%0Aintelligent%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08755v1&entry.124074799=Read"},
{"title": "Explainability of CNN Based Classification Models for Acoustic Signal", "author": "Zubair Faruqui and Mackenzie S. McIntire and Rahul Dubey and Jay McEntee", "abstract": "  Explainable Artificial Intelligence (XAI) has emerged as a critical tool for\ninterpreting the predictions of complex deep learning models. While XAI has\nbeen increasingly applied in various domains within acoustics, its use in\nbioacoustics, which involves analyzing audio signals from living organisms,\nremains relatively underexplored. In this paper, we investigate the\nvocalizations of a bird species with strong geographic variation throughout its\nrange in North America. Audio recordings were converted into spectrogram images\nand used to train a deep Convolutional Neural Network (CNN) for classification,\nachieving an accuracy of 94.8\\%. To interpret the model's predictions, we\napplied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT,\nGrad-CAM) XAI techniques. These techniques produced different but complementary\nexplanations, and when their explanations were considered together, they\nprovided more complete and interpretable insights into the model's\ndecision-making. This work highlights the importance of using a combination of\nXAI techniques to improve trust and interoperability, not only in broader\nacoustics signal analysis but also argues for broader applicability in\ndifferent domain specific tasks.\n", "link": "http://arxiv.org/abs/2509.08717v1", "date": "2025-09-10", "relevancy": 2.3105, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainability%20of%20CNN%20Based%20Classification%20Models%20for%20Acoustic%20Signal&body=Title%3A%20Explainability%20of%20CNN%20Based%20Classification%20Models%20for%20Acoustic%20Signal%0AAuthor%3A%20Zubair%20Faruqui%20and%20Mackenzie%20S.%20McIntire%20and%20Rahul%20Dubey%20and%20Jay%20McEntee%0AAbstract%3A%20%20%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20has%20emerged%20as%20a%20critical%20tool%20for%0Ainterpreting%20the%20predictions%20of%20complex%20deep%20learning%20models.%20While%20XAI%20has%0Abeen%20increasingly%20applied%20in%20various%20domains%20within%20acoustics%2C%20its%20use%20in%0Abioacoustics%2C%20which%20involves%20analyzing%20audio%20signals%20from%20living%20organisms%2C%0Aremains%20relatively%20underexplored.%20In%20this%20paper%2C%20we%20investigate%20the%0Avocalizations%20of%20a%20bird%20species%20with%20strong%20geographic%20variation%20throughout%20its%0Arange%20in%20North%20America.%20Audio%20recordings%20were%20converted%20into%20spectrogram%20images%0Aand%20used%20to%20train%20a%20deep%20Convolutional%20Neural%20Network%20%28CNN%29%20for%20classification%2C%0Aachieving%20an%20accuracy%20of%2094.8%5C%25.%20To%20interpret%20the%20model%27s%20predictions%2C%20we%0Aapplied%20both%20model-agnostic%20%28LIME%2C%20SHAP%29%20and%20model-specific%20%28DeepLIFT%2C%0AGrad-CAM%29%20XAI%20techniques.%20These%20techniques%20produced%20different%20but%20complementary%0Aexplanations%2C%20and%20when%20their%20explanations%20were%20considered%20together%2C%20they%0Aprovided%20more%20complete%20and%20interpretable%20insights%20into%20the%20model%27s%0Adecision-making.%20This%20work%20highlights%20the%20importance%20of%20using%20a%20combination%20of%0AXAI%20techniques%20to%20improve%20trust%20and%20interoperability%2C%20not%20only%20in%20broader%0Aacoustics%20signal%20analysis%20but%20also%20argues%20for%20broader%20applicability%20in%0Adifferent%20domain%20specific%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainability%2520of%2520CNN%2520Based%2520Classification%2520Models%2520for%2520Acoustic%2520Signal%26entry.906535625%3DZubair%2520Faruqui%2520and%2520Mackenzie%2520S.%2520McIntire%2520and%2520Rahul%2520Dubey%2520and%2520Jay%2520McEntee%26entry.1292438233%3D%2520%2520Explainable%2520Artificial%2520Intelligence%2520%2528XAI%2529%2520has%2520emerged%2520as%2520a%2520critical%2520tool%2520for%250Ainterpreting%2520the%2520predictions%2520of%2520complex%2520deep%2520learning%2520models.%2520While%2520XAI%2520has%250Abeen%2520increasingly%2520applied%2520in%2520various%2520domains%2520within%2520acoustics%252C%2520its%2520use%2520in%250Abioacoustics%252C%2520which%2520involves%2520analyzing%2520audio%2520signals%2520from%2520living%2520organisms%252C%250Aremains%2520relatively%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250Avocalizations%2520of%2520a%2520bird%2520species%2520with%2520strong%2520geographic%2520variation%2520throughout%2520its%250Arange%2520in%2520North%2520America.%2520Audio%2520recordings%2520were%2520converted%2520into%2520spectrogram%2520images%250Aand%2520used%2520to%2520train%2520a%2520deep%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520for%2520classification%252C%250Aachieving%2520an%2520accuracy%2520of%252094.8%255C%2525.%2520To%2520interpret%2520the%2520model%2527s%2520predictions%252C%2520we%250Aapplied%2520both%2520model-agnostic%2520%2528LIME%252C%2520SHAP%2529%2520and%2520model-specific%2520%2528DeepLIFT%252C%250AGrad-CAM%2529%2520XAI%2520techniques.%2520These%2520techniques%2520produced%2520different%2520but%2520complementary%250Aexplanations%252C%2520and%2520when%2520their%2520explanations%2520were%2520considered%2520together%252C%2520they%250Aprovided%2520more%2520complete%2520and%2520interpretable%2520insights%2520into%2520the%2520model%2527s%250Adecision-making.%2520This%2520work%2520highlights%2520the%2520importance%2520of%2520using%2520a%2520combination%2520of%250AXAI%2520techniques%2520to%2520improve%2520trust%2520and%2520interoperability%252C%2520not%2520only%2520in%2520broader%250Aacoustics%2520signal%2520analysis%2520but%2520also%2520argues%2520for%2520broader%2520applicability%2520in%250Adifferent%2520domain%2520specific%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainability%20of%20CNN%20Based%20Classification%20Models%20for%20Acoustic%20Signal&entry.906535625=Zubair%20Faruqui%20and%20Mackenzie%20S.%20McIntire%20and%20Rahul%20Dubey%20and%20Jay%20McEntee&entry.1292438233=%20%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20has%20emerged%20as%20a%20critical%20tool%20for%0Ainterpreting%20the%20predictions%20of%20complex%20deep%20learning%20models.%20While%20XAI%20has%0Abeen%20increasingly%20applied%20in%20various%20domains%20within%20acoustics%2C%20its%20use%20in%0Abioacoustics%2C%20which%20involves%20analyzing%20audio%20signals%20from%20living%20organisms%2C%0Aremains%20relatively%20underexplored.%20In%20this%20paper%2C%20we%20investigate%20the%0Avocalizations%20of%20a%20bird%20species%20with%20strong%20geographic%20variation%20throughout%20its%0Arange%20in%20North%20America.%20Audio%20recordings%20were%20converted%20into%20spectrogram%20images%0Aand%20used%20to%20train%20a%20deep%20Convolutional%20Neural%20Network%20%28CNN%29%20for%20classification%2C%0Aachieving%20an%20accuracy%20of%2094.8%5C%25.%20To%20interpret%20the%20model%27s%20predictions%2C%20we%0Aapplied%20both%20model-agnostic%20%28LIME%2C%20SHAP%29%20and%20model-specific%20%28DeepLIFT%2C%0AGrad-CAM%29%20XAI%20techniques.%20These%20techniques%20produced%20different%20but%20complementary%0Aexplanations%2C%20and%20when%20their%20explanations%20were%20considered%20together%2C%20they%0Aprovided%20more%20complete%20and%20interpretable%20insights%20into%20the%20model%27s%0Adecision-making.%20This%20work%20highlights%20the%20importance%20of%20using%20a%20combination%20of%0AXAI%20techniques%20to%20improve%20trust%20and%20interoperability%2C%20not%20only%20in%20broader%0Aacoustics%20signal%20analysis%20but%20also%20argues%20for%20broader%20applicability%20in%0Adifferent%20domain%20specific%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08717v1&entry.124074799=Read"},
{"title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation", "author": "Johannes Jakubik and Felix Yang and Benedikt Blumenstiel and Erik Scheurer and Rocco Sedona and Stefano Maurogiovanni and Jente Bosmans and Nikolaos Dionelis and Valerio Marsocci and Niklas Kopp and Rahul Ramachandran and Paolo Fraccaro and Thomas Brunschwiler and Gabriele Cavallaro and Juan Bernabe-Moreno and Nicolas Long\u00e9p\u00e9", "abstract": "  We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code are open-sourced under a permissive license.\n", "link": "http://arxiv.org/abs/2504.11171v4", "date": "2025-09-10", "relevancy": 2.3079, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.589}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5717}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TerraMind%3A%20Large-Scale%20Generative%20Multimodality%20for%20Earth%20Observation&body=Title%3A%20TerraMind%3A%20Large-Scale%20Generative%20Multimodality%20for%20Earth%20Observation%0AAuthor%3A%20Johannes%20Jakubik%20and%20Felix%20Yang%20and%20Benedikt%20Blumenstiel%20and%20Erik%20Scheurer%20and%20Rocco%20Sedona%20and%20Stefano%20Maurogiovanni%20and%20Jente%20Bosmans%20and%20Nikolaos%20Dionelis%20and%20Valerio%20Marsocci%20and%20Niklas%20Kopp%20and%20Rahul%20Ramachandran%20and%20Paolo%20Fraccaro%20and%20Thomas%20Brunschwiler%20and%20Gabriele%20Cavallaro%20and%20Juan%20Bernabe-Moreno%20and%20Nicolas%20Long%C3%A9p%C3%A9%0AAbstract%3A%20%20%20We%20present%20TerraMind%2C%20the%20first%20any-to-any%20generative%2C%20multimodal%20foundation%0Amodel%20for%20Earth%20observation%20%28EO%29.%20Unlike%20other%20multimodal%20models%2C%20TerraMind%20is%0Apretrained%20on%20dual-scale%20representations%20combining%20both%20token-level%20and%0Apixel-level%20data%20across%20modalities.%20On%20a%20token%20level%2C%20TerraMind%20encodes%0Ahigh-level%20contextual%20information%20to%20learn%20cross-modal%20relationships%2C%20while%20on%0Aa%20pixel%20level%2C%20TerraMind%20leverages%20fine-grained%20representations%20to%20capture%0Acritical%20spatial%20nuances.%20We%20pretrained%20TerraMind%20on%20nine%20geospatial%20modalities%0Aof%20a%20global%2C%20large-scale%20dataset.%20In%20this%20paper%2C%20we%20demonstrate%20that%20%28i%29%0ATerraMind%27s%20dual-scale%20early%20fusion%20approach%20unlocks%20a%20range%20of%20zero-shot%20and%0Afew-shot%20applications%20for%20Earth%20observation%2C%20%28ii%29%20TerraMind%20introduces%0A%22Thinking-in-Modalities%22%20%28TiM%29%20--%20the%20capability%20of%20generating%20additional%0Aartificial%20data%20during%20finetuning%20and%20inference%20to%20improve%20the%20model%20output%20--%0Aand%20%28iii%29%20TerraMind%20achieves%20beyond%20state-of-the-art%20performance%20in%0Acommunity-standard%20benchmarks%20for%20EO%20like%20PANGAEA.%20The%20pretraining%20dataset%2C%20the%0Amodel%20weights%2C%20and%20our%20code%20are%20open-sourced%20under%20a%20permissive%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11171v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTerraMind%253A%2520Large-Scale%2520Generative%2520Multimodality%2520for%2520Earth%2520Observation%26entry.906535625%3DJohannes%2520Jakubik%2520and%2520Felix%2520Yang%2520and%2520Benedikt%2520Blumenstiel%2520and%2520Erik%2520Scheurer%2520and%2520Rocco%2520Sedona%2520and%2520Stefano%2520Maurogiovanni%2520and%2520Jente%2520Bosmans%2520and%2520Nikolaos%2520Dionelis%2520and%2520Valerio%2520Marsocci%2520and%2520Niklas%2520Kopp%2520and%2520Rahul%2520Ramachandran%2520and%2520Paolo%2520Fraccaro%2520and%2520Thomas%2520Brunschwiler%2520and%2520Gabriele%2520Cavallaro%2520and%2520Juan%2520Bernabe-Moreno%2520and%2520Nicolas%2520Long%25C3%25A9p%25C3%25A9%26entry.1292438233%3D%2520%2520We%2520present%2520TerraMind%252C%2520the%2520first%2520any-to-any%2520generative%252C%2520multimodal%2520foundation%250Amodel%2520for%2520Earth%2520observation%2520%2528EO%2529.%2520Unlike%2520other%2520multimodal%2520models%252C%2520TerraMind%2520is%250Apretrained%2520on%2520dual-scale%2520representations%2520combining%2520both%2520token-level%2520and%250Apixel-level%2520data%2520across%2520modalities.%2520On%2520a%2520token%2520level%252C%2520TerraMind%2520encodes%250Ahigh-level%2520contextual%2520information%2520to%2520learn%2520cross-modal%2520relationships%252C%2520while%2520on%250Aa%2520pixel%2520level%252C%2520TerraMind%2520leverages%2520fine-grained%2520representations%2520to%2520capture%250Acritical%2520spatial%2520nuances.%2520We%2520pretrained%2520TerraMind%2520on%2520nine%2520geospatial%2520modalities%250Aof%2520a%2520global%252C%2520large-scale%2520dataset.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%2520%2528i%2529%250ATerraMind%2527s%2520dual-scale%2520early%2520fusion%2520approach%2520unlocks%2520a%2520range%2520of%2520zero-shot%2520and%250Afew-shot%2520applications%2520for%2520Earth%2520observation%252C%2520%2528ii%2529%2520TerraMind%2520introduces%250A%2522Thinking-in-Modalities%2522%2520%2528TiM%2529%2520--%2520the%2520capability%2520of%2520generating%2520additional%250Aartificial%2520data%2520during%2520finetuning%2520and%2520inference%2520to%2520improve%2520the%2520model%2520output%2520--%250Aand%2520%2528iii%2529%2520TerraMind%2520achieves%2520beyond%2520state-of-the-art%2520performance%2520in%250Acommunity-standard%2520benchmarks%2520for%2520EO%2520like%2520PANGAEA.%2520The%2520pretraining%2520dataset%252C%2520the%250Amodel%2520weights%252C%2520and%2520our%2520code%2520are%2520open-sourced%2520under%2520a%2520permissive%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11171v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TerraMind%3A%20Large-Scale%20Generative%20Multimodality%20for%20Earth%20Observation&entry.906535625=Johannes%20Jakubik%20and%20Felix%20Yang%20and%20Benedikt%20Blumenstiel%20and%20Erik%20Scheurer%20and%20Rocco%20Sedona%20and%20Stefano%20Maurogiovanni%20and%20Jente%20Bosmans%20and%20Nikolaos%20Dionelis%20and%20Valerio%20Marsocci%20and%20Niklas%20Kopp%20and%20Rahul%20Ramachandran%20and%20Paolo%20Fraccaro%20and%20Thomas%20Brunschwiler%20and%20Gabriele%20Cavallaro%20and%20Juan%20Bernabe-Moreno%20and%20Nicolas%20Long%C3%A9p%C3%A9&entry.1292438233=%20%20We%20present%20TerraMind%2C%20the%20first%20any-to-any%20generative%2C%20multimodal%20foundation%0Amodel%20for%20Earth%20observation%20%28EO%29.%20Unlike%20other%20multimodal%20models%2C%20TerraMind%20is%0Apretrained%20on%20dual-scale%20representations%20combining%20both%20token-level%20and%0Apixel-level%20data%20across%20modalities.%20On%20a%20token%20level%2C%20TerraMind%20encodes%0Ahigh-level%20contextual%20information%20to%20learn%20cross-modal%20relationships%2C%20while%20on%0Aa%20pixel%20level%2C%20TerraMind%20leverages%20fine-grained%20representations%20to%20capture%0Acritical%20spatial%20nuances.%20We%20pretrained%20TerraMind%20on%20nine%20geospatial%20modalities%0Aof%20a%20global%2C%20large-scale%20dataset.%20In%20this%20paper%2C%20we%20demonstrate%20that%20%28i%29%0ATerraMind%27s%20dual-scale%20early%20fusion%20approach%20unlocks%20a%20range%20of%20zero-shot%20and%0Afew-shot%20applications%20for%20Earth%20observation%2C%20%28ii%29%20TerraMind%20introduces%0A%22Thinking-in-Modalities%22%20%28TiM%29%20--%20the%20capability%20of%20generating%20additional%0Aartificial%20data%20during%20finetuning%20and%20inference%20to%20improve%20the%20model%20output%20--%0Aand%20%28iii%29%20TerraMind%20achieves%20beyond%20state-of-the-art%20performance%20in%0Acommunity-standard%20benchmarks%20for%20EO%20like%20PANGAEA.%20The%20pretraining%20dataset%2C%20the%0Amodel%20weights%2C%20and%20our%20code%20are%20open-sourced%20under%20a%20permissive%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11171v4&entry.124074799=Read"},
{"title": "Have Large Vision-Language Models Mastered Art History?", "author": "Ombretta Strafforello and Derya Soydaner and Michiel Willems and Anne-Sofie Maerten and Stefanie De Winter", "abstract": "  The emergence of large Vision-Language Models (VLMs) has established new\nbaselines in image classification across multiple domains. We examine whether\ntheir multimodal reasoning can also address a challenge mastered by human\nexperts. Specifically, we test whether VLMs can classify the style, author and\ncreation date of paintings, a domain traditionally mastered by art historians.\nArtworks pose a unique challenge compared to natural images due to their\ninherently complex and diverse structures, characterized by variable\ncompositions and styles. This requires a contextual and stylistic\ninterpretation rather than straightforward object recognition. Art historians\nhave long studied the unique aspects of artworks, with style prediction being a\ncrucial component of their discipline. This paper investigates whether large\nVLMs, which integrate visual and textual data, can effectively reason about the\nhistorical and stylistic attributes of paintings. We present the first study of\nits kind, conducting an in-depth analysis of three VLMs, namely CLIP, LLaVA,\nand GPT-4o, evaluating their zero-shot classification of art style, author and\ntime period. Using two image benchmarks of artworks, we assess the models'\nability to interpret style, evaluate their sensitivity to prompts, and examine\nfailure cases. Additionally, we focus on how these models compare to human art\nhistorical expertise by analyzing misclassifications, providing insights into\ntheir reasoning and classification patterns.\n", "link": "http://arxiv.org/abs/2409.03521v2", "date": "2025-09-10", "relevancy": 2.3067, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5818}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5818}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Have%20Large%20Vision-Language%20Models%20Mastered%20Art%20History%3F&body=Title%3A%20Have%20Large%20Vision-Language%20Models%20Mastered%20Art%20History%3F%0AAuthor%3A%20Ombretta%20Strafforello%20and%20Derya%20Soydaner%20and%20Michiel%20Willems%20and%20Anne-Sofie%20Maerten%20and%20Stefanie%20De%20Winter%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20Vision-Language%20Models%20%28VLMs%29%20has%20established%20new%0Abaselines%20in%20image%20classification%20across%20multiple%20domains.%20We%20examine%20whether%0Atheir%20multimodal%20reasoning%20can%20also%20address%20a%20challenge%20mastered%20by%20human%0Aexperts.%20Specifically%2C%20we%20test%20whether%20VLMs%20can%20classify%20the%20style%2C%20author%20and%0Acreation%20date%20of%20paintings%2C%20a%20domain%20traditionally%20mastered%20by%20art%20historians.%0AArtworks%20pose%20a%20unique%20challenge%20compared%20to%20natural%20images%20due%20to%20their%0Ainherently%20complex%20and%20diverse%20structures%2C%20characterized%20by%20variable%0Acompositions%20and%20styles.%20This%20requires%20a%20contextual%20and%20stylistic%0Ainterpretation%20rather%20than%20straightforward%20object%20recognition.%20Art%20historians%0Ahave%20long%20studied%20the%20unique%20aspects%20of%20artworks%2C%20with%20style%20prediction%20being%20a%0Acrucial%20component%20of%20their%20discipline.%20This%20paper%20investigates%20whether%20large%0AVLMs%2C%20which%20integrate%20visual%20and%20textual%20data%2C%20can%20effectively%20reason%20about%20the%0Ahistorical%20and%20stylistic%20attributes%20of%20paintings.%20We%20present%20the%20first%20study%20of%0Aits%20kind%2C%20conducting%20an%20in-depth%20analysis%20of%20three%20VLMs%2C%20namely%20CLIP%2C%20LLaVA%2C%0Aand%20GPT-4o%2C%20evaluating%20their%20zero-shot%20classification%20of%20art%20style%2C%20author%20and%0Atime%20period.%20Using%20two%20image%20benchmarks%20of%20artworks%2C%20we%20assess%20the%20models%27%0Aability%20to%20interpret%20style%2C%20evaluate%20their%20sensitivity%20to%20prompts%2C%20and%20examine%0Afailure%20cases.%20Additionally%2C%20we%20focus%20on%20how%20these%20models%20compare%20to%20human%20art%0Ahistorical%20expertise%20by%20analyzing%20misclassifications%2C%20providing%20insights%20into%0Atheir%20reasoning%20and%20classification%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03521v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHave%2520Large%2520Vision-Language%2520Models%2520Mastered%2520Art%2520History%253F%26entry.906535625%3DOmbretta%2520Strafforello%2520and%2520Derya%2520Soydaner%2520and%2520Michiel%2520Willems%2520and%2520Anne-Sofie%2520Maerten%2520and%2520Stefanie%2520De%2520Winter%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520has%2520established%2520new%250Abaselines%2520in%2520image%2520classification%2520across%2520multiple%2520domains.%2520We%2520examine%2520whether%250Atheir%2520multimodal%2520reasoning%2520can%2520also%2520address%2520a%2520challenge%2520mastered%2520by%2520human%250Aexperts.%2520Specifically%252C%2520we%2520test%2520whether%2520VLMs%2520can%2520classify%2520the%2520style%252C%2520author%2520and%250Acreation%2520date%2520of%2520paintings%252C%2520a%2520domain%2520traditionally%2520mastered%2520by%2520art%2520historians.%250AArtworks%2520pose%2520a%2520unique%2520challenge%2520compared%2520to%2520natural%2520images%2520due%2520to%2520their%250Ainherently%2520complex%2520and%2520diverse%2520structures%252C%2520characterized%2520by%2520variable%250Acompositions%2520and%2520styles.%2520This%2520requires%2520a%2520contextual%2520and%2520stylistic%250Ainterpretation%2520rather%2520than%2520straightforward%2520object%2520recognition.%2520Art%2520historians%250Ahave%2520long%2520studied%2520the%2520unique%2520aspects%2520of%2520artworks%252C%2520with%2520style%2520prediction%2520being%2520a%250Acrucial%2520component%2520of%2520their%2520discipline.%2520This%2520paper%2520investigates%2520whether%2520large%250AVLMs%252C%2520which%2520integrate%2520visual%2520and%2520textual%2520data%252C%2520can%2520effectively%2520reason%2520about%2520the%250Ahistorical%2520and%2520stylistic%2520attributes%2520of%2520paintings.%2520We%2520present%2520the%2520first%2520study%2520of%250Aits%2520kind%252C%2520conducting%2520an%2520in-depth%2520analysis%2520of%2520three%2520VLMs%252C%2520namely%2520CLIP%252C%2520LLaVA%252C%250Aand%2520GPT-4o%252C%2520evaluating%2520their%2520zero-shot%2520classification%2520of%2520art%2520style%252C%2520author%2520and%250Atime%2520period.%2520Using%2520two%2520image%2520benchmarks%2520of%2520artworks%252C%2520we%2520assess%2520the%2520models%2527%250Aability%2520to%2520interpret%2520style%252C%2520evaluate%2520their%2520sensitivity%2520to%2520prompts%252C%2520and%2520examine%250Afailure%2520cases.%2520Additionally%252C%2520we%2520focus%2520on%2520how%2520these%2520models%2520compare%2520to%2520human%2520art%250Ahistorical%2520expertise%2520by%2520analyzing%2520misclassifications%252C%2520providing%2520insights%2520into%250Atheir%2520reasoning%2520and%2520classification%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03521v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Have%20Large%20Vision-Language%20Models%20Mastered%20Art%20History%3F&entry.906535625=Ombretta%20Strafforello%20and%20Derya%20Soydaner%20and%20Michiel%20Willems%20and%20Anne-Sofie%20Maerten%20and%20Stefanie%20De%20Winter&entry.1292438233=%20%20The%20emergence%20of%20large%20Vision-Language%20Models%20%28VLMs%29%20has%20established%20new%0Abaselines%20in%20image%20classification%20across%20multiple%20domains.%20We%20examine%20whether%0Atheir%20multimodal%20reasoning%20can%20also%20address%20a%20challenge%20mastered%20by%20human%0Aexperts.%20Specifically%2C%20we%20test%20whether%20VLMs%20can%20classify%20the%20style%2C%20author%20and%0Acreation%20date%20of%20paintings%2C%20a%20domain%20traditionally%20mastered%20by%20art%20historians.%0AArtworks%20pose%20a%20unique%20challenge%20compared%20to%20natural%20images%20due%20to%20their%0Ainherently%20complex%20and%20diverse%20structures%2C%20characterized%20by%20variable%0Acompositions%20and%20styles.%20This%20requires%20a%20contextual%20and%20stylistic%0Ainterpretation%20rather%20than%20straightforward%20object%20recognition.%20Art%20historians%0Ahave%20long%20studied%20the%20unique%20aspects%20of%20artworks%2C%20with%20style%20prediction%20being%20a%0Acrucial%20component%20of%20their%20discipline.%20This%20paper%20investigates%20whether%20large%0AVLMs%2C%20which%20integrate%20visual%20and%20textual%20data%2C%20can%20effectively%20reason%20about%20the%0Ahistorical%20and%20stylistic%20attributes%20of%20paintings.%20We%20present%20the%20first%20study%20of%0Aits%20kind%2C%20conducting%20an%20in-depth%20analysis%20of%20three%20VLMs%2C%20namely%20CLIP%2C%20LLaVA%2C%0Aand%20GPT-4o%2C%20evaluating%20their%20zero-shot%20classification%20of%20art%20style%2C%20author%20and%0Atime%20period.%20Using%20two%20image%20benchmarks%20of%20artworks%2C%20we%20assess%20the%20models%27%0Aability%20to%20interpret%20style%2C%20evaluate%20their%20sensitivity%20to%20prompts%2C%20and%20examine%0Afailure%20cases.%20Additionally%2C%20we%20focus%20on%20how%20these%20models%20compare%20to%20human%20art%0Ahistorical%20expertise%20by%20analyzing%20misclassifications%2C%20providing%20insights%20into%0Atheir%20reasoning%20and%20classification%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03521v2&entry.124074799=Read"},
{"title": "Parallel, Asymptotically Optimal Algorithms for Moving Target Traveling\n  Salesman Problems", "author": "Anoop Bhat and Geordan Gutow and Bhaskar Vundurthy and Zhongqiang Ren and Sivakumar Rathinam and Howie Choset", "abstract": "  The Moving Target Traveling Salesman Problem (MT-TSP) seeks an agent\ntrajectory that intercepts several moving targets, within a particular time\nwindow for each target. In the presence of generic nonlinear target\ntrajectories or kinematic constraints on the agent, no prior algorithm\nguarantees convergence to an optimal MT-TSP solution. Therefore, we introduce\nthe Iterated Random Generalized (IRG) TSP framework. The key idea behind IRG is\nto alternate between randomly sampling a set of agent configuration-time\npoints, corresponding to interceptions of targets, and finding a sequence of\ninterception points by solving a generalized TSP (GTSP). This alternation\nenables asymptotic convergence to the optimum. We introduce two parallel\nalgorithms within the IRG framework. The first algorithm, IRG-PGLNS, solves\nGTSPs using PGLNS, our parallelized extension of the state-of-the-art solver\nGLNS. The second algorithm, Parallel Communicating GTSPs (PCG), solves GTSPs\ncorresponding to several sets of points simultaneously. We present numerical\nresults for three variants of the MT-TSP: one where intercepting a target only\nrequires coming within a particular distance, another where the agent is a\nvariable-speed Dubins car, and a third where the agent is a redundant robot\narm. We show that IRG-PGLNS and PCG both converge faster than a baseline based\non prior work.\n", "link": "http://arxiv.org/abs/2509.08743v1", "date": "2025-09-10", "relevancy": 2.3014, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4942}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.446}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallel%2C%20Asymptotically%20Optimal%20Algorithms%20for%20Moving%20Target%20Traveling%0A%20%20Salesman%20Problems&body=Title%3A%20Parallel%2C%20Asymptotically%20Optimal%20Algorithms%20for%20Moving%20Target%20Traveling%0A%20%20Salesman%20Problems%0AAuthor%3A%20Anoop%20Bhat%20and%20Geordan%20Gutow%20and%20Bhaskar%20Vundurthy%20and%20Zhongqiang%20Ren%20and%20Sivakumar%20Rathinam%20and%20Howie%20Choset%0AAbstract%3A%20%20%20The%20Moving%20Target%20Traveling%20Salesman%20Problem%20%28MT-TSP%29%20seeks%20an%20agent%0Atrajectory%20that%20intercepts%20several%20moving%20targets%2C%20within%20a%20particular%20time%0Awindow%20for%20each%20target.%20In%20the%20presence%20of%20generic%20nonlinear%20target%0Atrajectories%20or%20kinematic%20constraints%20on%20the%20agent%2C%20no%20prior%20algorithm%0Aguarantees%20convergence%20to%20an%20optimal%20MT-TSP%20solution.%20Therefore%2C%20we%20introduce%0Athe%20Iterated%20Random%20Generalized%20%28IRG%29%20TSP%20framework.%20The%20key%20idea%20behind%20IRG%20is%0Ato%20alternate%20between%20randomly%20sampling%20a%20set%20of%20agent%20configuration-time%0Apoints%2C%20corresponding%20to%20interceptions%20of%20targets%2C%20and%20finding%20a%20sequence%20of%0Ainterception%20points%20by%20solving%20a%20generalized%20TSP%20%28GTSP%29.%20This%20alternation%0Aenables%20asymptotic%20convergence%20to%20the%20optimum.%20We%20introduce%20two%20parallel%0Aalgorithms%20within%20the%20IRG%20framework.%20The%20first%20algorithm%2C%20IRG-PGLNS%2C%20solves%0AGTSPs%20using%20PGLNS%2C%20our%20parallelized%20extension%20of%20the%20state-of-the-art%20solver%0AGLNS.%20The%20second%20algorithm%2C%20Parallel%20Communicating%20GTSPs%20%28PCG%29%2C%20solves%20GTSPs%0Acorresponding%20to%20several%20sets%20of%20points%20simultaneously.%20We%20present%20numerical%0Aresults%20for%20three%20variants%20of%20the%20MT-TSP%3A%20one%20where%20intercepting%20a%20target%20only%0Arequires%20coming%20within%20a%20particular%20distance%2C%20another%20where%20the%20agent%20is%20a%0Avariable-speed%20Dubins%20car%2C%20and%20a%20third%20where%20the%20agent%20is%20a%20redundant%20robot%0Aarm.%20We%20show%20that%20IRG-PGLNS%20and%20PCG%20both%20converge%20faster%20than%20a%20baseline%20based%0Aon%20prior%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallel%252C%2520Asymptotically%2520Optimal%2520Algorithms%2520for%2520Moving%2520Target%2520Traveling%250A%2520%2520Salesman%2520Problems%26entry.906535625%3DAnoop%2520Bhat%2520and%2520Geordan%2520Gutow%2520and%2520Bhaskar%2520Vundurthy%2520and%2520Zhongqiang%2520Ren%2520and%2520Sivakumar%2520Rathinam%2520and%2520Howie%2520Choset%26entry.1292438233%3D%2520%2520The%2520Moving%2520Target%2520Traveling%2520Salesman%2520Problem%2520%2528MT-TSP%2529%2520seeks%2520an%2520agent%250Atrajectory%2520that%2520intercepts%2520several%2520moving%2520targets%252C%2520within%2520a%2520particular%2520time%250Awindow%2520for%2520each%2520target.%2520In%2520the%2520presence%2520of%2520generic%2520nonlinear%2520target%250Atrajectories%2520or%2520kinematic%2520constraints%2520on%2520the%2520agent%252C%2520no%2520prior%2520algorithm%250Aguarantees%2520convergence%2520to%2520an%2520optimal%2520MT-TSP%2520solution.%2520Therefore%252C%2520we%2520introduce%250Athe%2520Iterated%2520Random%2520Generalized%2520%2528IRG%2529%2520TSP%2520framework.%2520The%2520key%2520idea%2520behind%2520IRG%2520is%250Ato%2520alternate%2520between%2520randomly%2520sampling%2520a%2520set%2520of%2520agent%2520configuration-time%250Apoints%252C%2520corresponding%2520to%2520interceptions%2520of%2520targets%252C%2520and%2520finding%2520a%2520sequence%2520of%250Ainterception%2520points%2520by%2520solving%2520a%2520generalized%2520TSP%2520%2528GTSP%2529.%2520This%2520alternation%250Aenables%2520asymptotic%2520convergence%2520to%2520the%2520optimum.%2520We%2520introduce%2520two%2520parallel%250Aalgorithms%2520within%2520the%2520IRG%2520framework.%2520The%2520first%2520algorithm%252C%2520IRG-PGLNS%252C%2520solves%250AGTSPs%2520using%2520PGLNS%252C%2520our%2520parallelized%2520extension%2520of%2520the%2520state-of-the-art%2520solver%250AGLNS.%2520The%2520second%2520algorithm%252C%2520Parallel%2520Communicating%2520GTSPs%2520%2528PCG%2529%252C%2520solves%2520GTSPs%250Acorresponding%2520to%2520several%2520sets%2520of%2520points%2520simultaneously.%2520We%2520present%2520numerical%250Aresults%2520for%2520three%2520variants%2520of%2520the%2520MT-TSP%253A%2520one%2520where%2520intercepting%2520a%2520target%2520only%250Arequires%2520coming%2520within%2520a%2520particular%2520distance%252C%2520another%2520where%2520the%2520agent%2520is%2520a%250Avariable-speed%2520Dubins%2520car%252C%2520and%2520a%2520third%2520where%2520the%2520agent%2520is%2520a%2520redundant%2520robot%250Aarm.%2520We%2520show%2520that%2520IRG-PGLNS%2520and%2520PCG%2520both%2520converge%2520faster%2520than%2520a%2520baseline%2520based%250Aon%2520prior%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallel%2C%20Asymptotically%20Optimal%20Algorithms%20for%20Moving%20Target%20Traveling%0A%20%20Salesman%20Problems&entry.906535625=Anoop%20Bhat%20and%20Geordan%20Gutow%20and%20Bhaskar%20Vundurthy%20and%20Zhongqiang%20Ren%20and%20Sivakumar%20Rathinam%20and%20Howie%20Choset&entry.1292438233=%20%20The%20Moving%20Target%20Traveling%20Salesman%20Problem%20%28MT-TSP%29%20seeks%20an%20agent%0Atrajectory%20that%20intercepts%20several%20moving%20targets%2C%20within%20a%20particular%20time%0Awindow%20for%20each%20target.%20In%20the%20presence%20of%20generic%20nonlinear%20target%0Atrajectories%20or%20kinematic%20constraints%20on%20the%20agent%2C%20no%20prior%20algorithm%0Aguarantees%20convergence%20to%20an%20optimal%20MT-TSP%20solution.%20Therefore%2C%20we%20introduce%0Athe%20Iterated%20Random%20Generalized%20%28IRG%29%20TSP%20framework.%20The%20key%20idea%20behind%20IRG%20is%0Ato%20alternate%20between%20randomly%20sampling%20a%20set%20of%20agent%20configuration-time%0Apoints%2C%20corresponding%20to%20interceptions%20of%20targets%2C%20and%20finding%20a%20sequence%20of%0Ainterception%20points%20by%20solving%20a%20generalized%20TSP%20%28GTSP%29.%20This%20alternation%0Aenables%20asymptotic%20convergence%20to%20the%20optimum.%20We%20introduce%20two%20parallel%0Aalgorithms%20within%20the%20IRG%20framework.%20The%20first%20algorithm%2C%20IRG-PGLNS%2C%20solves%0AGTSPs%20using%20PGLNS%2C%20our%20parallelized%20extension%20of%20the%20state-of-the-art%20solver%0AGLNS.%20The%20second%20algorithm%2C%20Parallel%20Communicating%20GTSPs%20%28PCG%29%2C%20solves%20GTSPs%0Acorresponding%20to%20several%20sets%20of%20points%20simultaneously.%20We%20present%20numerical%0Aresults%20for%20three%20variants%20of%20the%20MT-TSP%3A%20one%20where%20intercepting%20a%20target%20only%0Arequires%20coming%20within%20a%20particular%20distance%2C%20another%20where%20the%20agent%20is%20a%0Avariable-speed%20Dubins%20car%2C%20and%20a%20third%20where%20the%20agent%20is%20a%20redundant%20robot%0Aarm.%20We%20show%20that%20IRG-PGLNS%20and%20PCG%20both%20converge%20faster%20than%20a%20baseline%20based%0Aon%20prior%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08743v1&entry.124074799=Read"},
{"title": "A Survey of World Models for Autonomous Driving", "author": "Tuo Feng and Wenguan Wang and Yi Yang", "abstract": "  Recent breakthroughs in autonomous driving have been propelled by advances in\nrobust world modeling, fundamentally transforming how vehicles interpret\ndynamic scenes and execute safe decision-making. World models have emerged as a\nlinchpin technology, offering high-fidelity representations of the driving\nenvironment that integrate multi-sensor data, semantic cues, and temporal\ndynamics. This paper systematically reviews recent advances in world models for\nautonomous driving, proposing a three-tiered taxonomy: (i) Generation of Future\nPhysical World, covering Image-, BEV-, OG-, and PC-based generation methods\nthat enhance scene evolution modeling through diffusion models and 4D occupancy\nforecasting; (ii) Behavior Planning for Intelligent Agents, combining\nrule-driven and learning-based paradigms with cost map optimization and\nreinforcement learning for trajectory generation in complex traffic conditions;\n(ii) Interaction between Prediction and Planning, achieving multi-agent\ncollaborative decision-making through latent space diffusion and\nmemory-augmented architectures. The study further analyzes training paradigms,\nincluding self-supervised learning, multimodal pretraining, and generative data\naugmentation, while evaluating world models' performance in scene understanding\nand motion prediction tasks. Future research must address key challenges in\nself-supervised representation learning, multimodal fusion, and advanced\nsimulation to advance the practical deployment of world models in complex urban\nenvironments. Overall, the comprehensive analysis provides a technical roadmap\nfor harnessing the transformative potential of world models in advancing safe\nand reliable autonomous driving solutions.\n", "link": "http://arxiv.org/abs/2501.11260v4", "date": "2025-09-10", "relevancy": 2.3012, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20World%20Models%20for%20Autonomous%20Driving&body=Title%3A%20A%20Survey%20of%20World%20Models%20for%20Autonomous%20Driving%0AAuthor%3A%20Tuo%20Feng%20and%20Wenguan%20Wang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20autonomous%20driving%20have%20been%20propelled%20by%20advances%20in%0Arobust%20world%20modeling%2C%20fundamentally%20transforming%20how%20vehicles%20interpret%0Adynamic%20scenes%20and%20execute%20safe%20decision-making.%20World%20models%20have%20emerged%20as%20a%0Alinchpin%20technology%2C%20offering%20high-fidelity%20representations%20of%20the%20driving%0Aenvironment%20that%20integrate%20multi-sensor%20data%2C%20semantic%20cues%2C%20and%20temporal%0Adynamics.%20This%20paper%20systematically%20reviews%20recent%20advances%20in%20world%20models%20for%0Aautonomous%20driving%2C%20proposing%20a%20three-tiered%20taxonomy%3A%20%28i%29%20Generation%20of%20Future%0APhysical%20World%2C%20covering%20Image-%2C%20BEV-%2C%20OG-%2C%20and%20PC-based%20generation%20methods%0Athat%20enhance%20scene%20evolution%20modeling%20through%20diffusion%20models%20and%204D%20occupancy%0Aforecasting%3B%20%28ii%29%20Behavior%20Planning%20for%20Intelligent%20Agents%2C%20combining%0Arule-driven%20and%20learning-based%20paradigms%20with%20cost%20map%20optimization%20and%0Areinforcement%20learning%20for%20trajectory%20generation%20in%20complex%20traffic%20conditions%3B%0A%28ii%29%20Interaction%20between%20Prediction%20and%20Planning%2C%20achieving%20multi-agent%0Acollaborative%20decision-making%20through%20latent%20space%20diffusion%20and%0Amemory-augmented%20architectures.%20The%20study%20further%20analyzes%20training%20paradigms%2C%0Aincluding%20self-supervised%20learning%2C%20multimodal%20pretraining%2C%20and%20generative%20data%0Aaugmentation%2C%20while%20evaluating%20world%20models%27%20performance%20in%20scene%20understanding%0Aand%20motion%20prediction%20tasks.%20Future%20research%20must%20address%20key%20challenges%20in%0Aself-supervised%20representation%20learning%2C%20multimodal%20fusion%2C%20and%20advanced%0Asimulation%20to%20advance%20the%20practical%20deployment%20of%20world%20models%20in%20complex%20urban%0Aenvironments.%20Overall%2C%20the%20comprehensive%20analysis%20provides%20a%20technical%20roadmap%0Afor%20harnessing%20the%20transformative%20potential%20of%20world%20models%20in%20advancing%20safe%0Aand%20reliable%20autonomous%20driving%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11260v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520World%2520Models%2520for%2520Autonomous%2520Driving%26entry.906535625%3DTuo%2520Feng%2520and%2520Wenguan%2520Wang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520breakthroughs%2520in%2520autonomous%2520driving%2520have%2520been%2520propelled%2520by%2520advances%2520in%250Arobust%2520world%2520modeling%252C%2520fundamentally%2520transforming%2520how%2520vehicles%2520interpret%250Adynamic%2520scenes%2520and%2520execute%2520safe%2520decision-making.%2520World%2520models%2520have%2520emerged%2520as%2520a%250Alinchpin%2520technology%252C%2520offering%2520high-fidelity%2520representations%2520of%2520the%2520driving%250Aenvironment%2520that%2520integrate%2520multi-sensor%2520data%252C%2520semantic%2520cues%252C%2520and%2520temporal%250Adynamics.%2520This%2520paper%2520systematically%2520reviews%2520recent%2520advances%2520in%2520world%2520models%2520for%250Aautonomous%2520driving%252C%2520proposing%2520a%2520three-tiered%2520taxonomy%253A%2520%2528i%2529%2520Generation%2520of%2520Future%250APhysical%2520World%252C%2520covering%2520Image-%252C%2520BEV-%252C%2520OG-%252C%2520and%2520PC-based%2520generation%2520methods%250Athat%2520enhance%2520scene%2520evolution%2520modeling%2520through%2520diffusion%2520models%2520and%25204D%2520occupancy%250Aforecasting%253B%2520%2528ii%2529%2520Behavior%2520Planning%2520for%2520Intelligent%2520Agents%252C%2520combining%250Arule-driven%2520and%2520learning-based%2520paradigms%2520with%2520cost%2520map%2520optimization%2520and%250Areinforcement%2520learning%2520for%2520trajectory%2520generation%2520in%2520complex%2520traffic%2520conditions%253B%250A%2528ii%2529%2520Interaction%2520between%2520Prediction%2520and%2520Planning%252C%2520achieving%2520multi-agent%250Acollaborative%2520decision-making%2520through%2520latent%2520space%2520diffusion%2520and%250Amemory-augmented%2520architectures.%2520The%2520study%2520further%2520analyzes%2520training%2520paradigms%252C%250Aincluding%2520self-supervised%2520learning%252C%2520multimodal%2520pretraining%252C%2520and%2520generative%2520data%250Aaugmentation%252C%2520while%2520evaluating%2520world%2520models%2527%2520performance%2520in%2520scene%2520understanding%250Aand%2520motion%2520prediction%2520tasks.%2520Future%2520research%2520must%2520address%2520key%2520challenges%2520in%250Aself-supervised%2520representation%2520learning%252C%2520multimodal%2520fusion%252C%2520and%2520advanced%250Asimulation%2520to%2520advance%2520the%2520practical%2520deployment%2520of%2520world%2520models%2520in%2520complex%2520urban%250Aenvironments.%2520Overall%252C%2520the%2520comprehensive%2520analysis%2520provides%2520a%2520technical%2520roadmap%250Afor%2520harnessing%2520the%2520transformative%2520potential%2520of%2520world%2520models%2520in%2520advancing%2520safe%250Aand%2520reliable%2520autonomous%2520driving%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11260v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20World%20Models%20for%20Autonomous%20Driving&entry.906535625=Tuo%20Feng%20and%20Wenguan%20Wang%20and%20Yi%20Yang&entry.1292438233=%20%20Recent%20breakthroughs%20in%20autonomous%20driving%20have%20been%20propelled%20by%20advances%20in%0Arobust%20world%20modeling%2C%20fundamentally%20transforming%20how%20vehicles%20interpret%0Adynamic%20scenes%20and%20execute%20safe%20decision-making.%20World%20models%20have%20emerged%20as%20a%0Alinchpin%20technology%2C%20offering%20high-fidelity%20representations%20of%20the%20driving%0Aenvironment%20that%20integrate%20multi-sensor%20data%2C%20semantic%20cues%2C%20and%20temporal%0Adynamics.%20This%20paper%20systematically%20reviews%20recent%20advances%20in%20world%20models%20for%0Aautonomous%20driving%2C%20proposing%20a%20three-tiered%20taxonomy%3A%20%28i%29%20Generation%20of%20Future%0APhysical%20World%2C%20covering%20Image-%2C%20BEV-%2C%20OG-%2C%20and%20PC-based%20generation%20methods%0Athat%20enhance%20scene%20evolution%20modeling%20through%20diffusion%20models%20and%204D%20occupancy%0Aforecasting%3B%20%28ii%29%20Behavior%20Planning%20for%20Intelligent%20Agents%2C%20combining%0Arule-driven%20and%20learning-based%20paradigms%20with%20cost%20map%20optimization%20and%0Areinforcement%20learning%20for%20trajectory%20generation%20in%20complex%20traffic%20conditions%3B%0A%28ii%29%20Interaction%20between%20Prediction%20and%20Planning%2C%20achieving%20multi-agent%0Acollaborative%20decision-making%20through%20latent%20space%20diffusion%20and%0Amemory-augmented%20architectures.%20The%20study%20further%20analyzes%20training%20paradigms%2C%0Aincluding%20self-supervised%20learning%2C%20multimodal%20pretraining%2C%20and%20generative%20data%0Aaugmentation%2C%20while%20evaluating%20world%20models%27%20performance%20in%20scene%20understanding%0Aand%20motion%20prediction%20tasks.%20Future%20research%20must%20address%20key%20challenges%20in%0Aself-supervised%20representation%20learning%2C%20multimodal%20fusion%2C%20and%20advanced%0Asimulation%20to%20advance%20the%20practical%20deployment%20of%20world%20models%20in%20complex%20urban%0Aenvironments.%20Overall%2C%20the%20comprehensive%20analysis%20provides%20a%20technical%20roadmap%0Afor%20harnessing%20the%20transformative%20potential%20of%20world%20models%20in%20advancing%20safe%0Aand%20reliable%20autonomous%20driving%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11260v4&entry.124074799=Read"},
{"title": "Dexterous Manipulation through Imitation Learning: A Survey", "author": "Shan An and Ziyu Meng and Chao Tang and Yuning Zhou and Tengyu Liu and Fangqiang Ding and Shufang Zhang and Yao Mu and Ran Song and Wei Zhang and Zeng-Guang Hou and Hong Zhang", "abstract": "  Dexterous manipulation, which refers to the ability of a robotic hand or\nmulti-fingered end-effector to skillfully control, reorient, and manipulate\nobjects through precise, coordinated finger movements and adaptive force\nmodulation, enables complex interactions similar to human hand dexterity. With\nrecent advances in robotics and machine learning, there is a growing demand for\nthese systems to operate in complex and unstructured environments. Traditional\nmodel-based approaches struggle to generalize across tasks and object\nvariations due to the high dimensionality and complex contact dynamics of\ndexterous manipulation. Although model-free methods such as reinforcement\nlearning (RL) show promise, they require extensive training, large-scale\ninteraction data, and carefully designed rewards for stability and\neffectiveness. Imitation learning (IL) offers an alternative by allowing robots\nto acquire dexterous manipulation skills directly from expert demonstrations,\ncapturing fine-grained coordination and contact dynamics while bypassing the\nneed for explicit modeling and large-scale trial-and-error. This survey\nprovides an overview of dexterous manipulation methods based on imitation\nlearning, details recent advances, and addresses key challenges in the field.\nAdditionally, it explores potential research directions to enhance IL-driven\ndexterous manipulation. Our goal is to offer researchers and practitioners a\ncomprehensive introduction to this rapidly evolving domain.\n", "link": "http://arxiv.org/abs/2504.03515v4", "date": "2025-09-10", "relevancy": 2.2765, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6004}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5893}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dexterous%20Manipulation%20through%20Imitation%20Learning%3A%20A%20Survey&body=Title%3A%20Dexterous%20Manipulation%20through%20Imitation%20Learning%3A%20A%20Survey%0AAuthor%3A%20Shan%20An%20and%20Ziyu%20Meng%20and%20Chao%20Tang%20and%20Yuning%20Zhou%20and%20Tengyu%20Liu%20and%20Fangqiang%20Ding%20and%20Shufang%20Zhang%20and%20Yao%20Mu%20and%20Ran%20Song%20and%20Wei%20Zhang%20and%20Zeng-Guang%20Hou%20and%20Hong%20Zhang%0AAbstract%3A%20%20%20Dexterous%20manipulation%2C%20which%20refers%20to%20the%20ability%20of%20a%20robotic%20hand%20or%0Amulti-fingered%20end-effector%20to%20skillfully%20control%2C%20reorient%2C%20and%20manipulate%0Aobjects%20through%20precise%2C%20coordinated%20finger%20movements%20and%20adaptive%20force%0Amodulation%2C%20enables%20complex%20interactions%20similar%20to%20human%20hand%20dexterity.%20With%0Arecent%20advances%20in%20robotics%20and%20machine%20learning%2C%20there%20is%20a%20growing%20demand%20for%0Athese%20systems%20to%20operate%20in%20complex%20and%20unstructured%20environments.%20Traditional%0Amodel-based%20approaches%20struggle%20to%20generalize%20across%20tasks%20and%20object%0Avariations%20due%20to%20the%20high%20dimensionality%20and%20complex%20contact%20dynamics%20of%0Adexterous%20manipulation.%20Although%20model-free%20methods%20such%20as%20reinforcement%0Alearning%20%28RL%29%20show%20promise%2C%20they%20require%20extensive%20training%2C%20large-scale%0Ainteraction%20data%2C%20and%20carefully%20designed%20rewards%20for%20stability%20and%0Aeffectiveness.%20Imitation%20learning%20%28IL%29%20offers%20an%20alternative%20by%20allowing%20robots%0Ato%20acquire%20dexterous%20manipulation%20skills%20directly%20from%20expert%20demonstrations%2C%0Acapturing%20fine-grained%20coordination%20and%20contact%20dynamics%20while%20bypassing%20the%0Aneed%20for%20explicit%20modeling%20and%20large-scale%20trial-and-error.%20This%20survey%0Aprovides%20an%20overview%20of%20dexterous%20manipulation%20methods%20based%20on%20imitation%0Alearning%2C%20details%20recent%20advances%2C%20and%20addresses%20key%20challenges%20in%20the%20field.%0AAdditionally%2C%20it%20explores%20potential%20research%20directions%20to%20enhance%20IL-driven%0Adexterous%20manipulation.%20Our%20goal%20is%20to%20offer%20researchers%20and%20practitioners%20a%0Acomprehensive%20introduction%20to%20this%20rapidly%20evolving%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03515v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexterous%2520Manipulation%2520through%2520Imitation%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DShan%2520An%2520and%2520Ziyu%2520Meng%2520and%2520Chao%2520Tang%2520and%2520Yuning%2520Zhou%2520and%2520Tengyu%2520Liu%2520and%2520Fangqiang%2520Ding%2520and%2520Shufang%2520Zhang%2520and%2520Yao%2520Mu%2520and%2520Ran%2520Song%2520and%2520Wei%2520Zhang%2520and%2520Zeng-Guang%2520Hou%2520and%2520Hong%2520Zhang%26entry.1292438233%3D%2520%2520Dexterous%2520manipulation%252C%2520which%2520refers%2520to%2520the%2520ability%2520of%2520a%2520robotic%2520hand%2520or%250Amulti-fingered%2520end-effector%2520to%2520skillfully%2520control%252C%2520reorient%252C%2520and%2520manipulate%250Aobjects%2520through%2520precise%252C%2520coordinated%2520finger%2520movements%2520and%2520adaptive%2520force%250Amodulation%252C%2520enables%2520complex%2520interactions%2520similar%2520to%2520human%2520hand%2520dexterity.%2520With%250Arecent%2520advances%2520in%2520robotics%2520and%2520machine%2520learning%252C%2520there%2520is%2520a%2520growing%2520demand%2520for%250Athese%2520systems%2520to%2520operate%2520in%2520complex%2520and%2520unstructured%2520environments.%2520Traditional%250Amodel-based%2520approaches%2520struggle%2520to%2520generalize%2520across%2520tasks%2520and%2520object%250Avariations%2520due%2520to%2520the%2520high%2520dimensionality%2520and%2520complex%2520contact%2520dynamics%2520of%250Adexterous%2520manipulation.%2520Although%2520model-free%2520methods%2520such%2520as%2520reinforcement%250Alearning%2520%2528RL%2529%2520show%2520promise%252C%2520they%2520require%2520extensive%2520training%252C%2520large-scale%250Ainteraction%2520data%252C%2520and%2520carefully%2520designed%2520rewards%2520for%2520stability%2520and%250Aeffectiveness.%2520Imitation%2520learning%2520%2528IL%2529%2520offers%2520an%2520alternative%2520by%2520allowing%2520robots%250Ato%2520acquire%2520dexterous%2520manipulation%2520skills%2520directly%2520from%2520expert%2520demonstrations%252C%250Acapturing%2520fine-grained%2520coordination%2520and%2520contact%2520dynamics%2520while%2520bypassing%2520the%250Aneed%2520for%2520explicit%2520modeling%2520and%2520large-scale%2520trial-and-error.%2520This%2520survey%250Aprovides%2520an%2520overview%2520of%2520dexterous%2520manipulation%2520methods%2520based%2520on%2520imitation%250Alearning%252C%2520details%2520recent%2520advances%252C%2520and%2520addresses%2520key%2520challenges%2520in%2520the%2520field.%250AAdditionally%252C%2520it%2520explores%2520potential%2520research%2520directions%2520to%2520enhance%2520IL-driven%250Adexterous%2520manipulation.%2520Our%2520goal%2520is%2520to%2520offer%2520researchers%2520and%2520practitioners%2520a%250Acomprehensive%2520introduction%2520to%2520this%2520rapidly%2520evolving%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03515v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dexterous%20Manipulation%20through%20Imitation%20Learning%3A%20A%20Survey&entry.906535625=Shan%20An%20and%20Ziyu%20Meng%20and%20Chao%20Tang%20and%20Yuning%20Zhou%20and%20Tengyu%20Liu%20and%20Fangqiang%20Ding%20and%20Shufang%20Zhang%20and%20Yao%20Mu%20and%20Ran%20Song%20and%20Wei%20Zhang%20and%20Zeng-Guang%20Hou%20and%20Hong%20Zhang&entry.1292438233=%20%20Dexterous%20manipulation%2C%20which%20refers%20to%20the%20ability%20of%20a%20robotic%20hand%20or%0Amulti-fingered%20end-effector%20to%20skillfully%20control%2C%20reorient%2C%20and%20manipulate%0Aobjects%20through%20precise%2C%20coordinated%20finger%20movements%20and%20adaptive%20force%0Amodulation%2C%20enables%20complex%20interactions%20similar%20to%20human%20hand%20dexterity.%20With%0Arecent%20advances%20in%20robotics%20and%20machine%20learning%2C%20there%20is%20a%20growing%20demand%20for%0Athese%20systems%20to%20operate%20in%20complex%20and%20unstructured%20environments.%20Traditional%0Amodel-based%20approaches%20struggle%20to%20generalize%20across%20tasks%20and%20object%0Avariations%20due%20to%20the%20high%20dimensionality%20and%20complex%20contact%20dynamics%20of%0Adexterous%20manipulation.%20Although%20model-free%20methods%20such%20as%20reinforcement%0Alearning%20%28RL%29%20show%20promise%2C%20they%20require%20extensive%20training%2C%20large-scale%0Ainteraction%20data%2C%20and%20carefully%20designed%20rewards%20for%20stability%20and%0Aeffectiveness.%20Imitation%20learning%20%28IL%29%20offers%20an%20alternative%20by%20allowing%20robots%0Ato%20acquire%20dexterous%20manipulation%20skills%20directly%20from%20expert%20demonstrations%2C%0Acapturing%20fine-grained%20coordination%20and%20contact%20dynamics%20while%20bypassing%20the%0Aneed%20for%20explicit%20modeling%20and%20large-scale%20trial-and-error.%20This%20survey%0Aprovides%20an%20overview%20of%20dexterous%20manipulation%20methods%20based%20on%20imitation%0Alearning%2C%20details%20recent%20advances%2C%20and%20addresses%20key%20challenges%20in%20the%20field.%0AAdditionally%2C%20it%20explores%20potential%20research%20directions%20to%20enhance%20IL-driven%0Adexterous%20manipulation.%20Our%20goal%20is%20to%20offer%20researchers%20and%20practitioners%20a%0Acomprehensive%20introduction%20to%20this%20rapidly%20evolving%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03515v4&entry.124074799=Read"},
{"title": "RewardDance: Reward Scaling in Visual Generation", "author": "Jie Wu and Yu Gao and Zilyu Ye and Ming Li and Liang Li and Hanzhong Guo and Jie Liu and Zeyue Xue and Xiaoxia Hou and Wei Liu and Yan Zeng and Weilin Huang", "abstract": "  Reward Models (RMs) are critical for improving generation models via\nReinforcement Learning (RL), yet the RM scaling paradigm in visual generation\nremains largely unexplored. It primarily due to fundamental limitations in\nexisting approaches: CLIP-based RMs suffer from architectural and input\nmodality constraints, while prevalent Bradley-Terry losses are fundamentally\nmisaligned with the next-token prediction mechanism of Vision-Language Models\n(VLMs), hindering effective scaling. More critically, the RLHF optimization\nprocess is plagued by Reward Hacking issue, where models exploit flaws in the\nreward signal without improving true quality. To address these challenges, we\nintroduce RewardDance, a scalable reward modeling framework that overcomes\nthese barriers through a novel generative reward paradigm. By reformulating the\nreward score as the model's probability of predicting a \"yes\" token, indicating\nthat the generated image outperforms a reference image according to specific\ncriteria, RewardDance intrinsically aligns reward objectives with VLM\narchitectures. This alignment unlocks scaling across two dimensions: (1) Model\nScaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context\nScaling: Integration of task-specific instructions, reference examples, and\nchain-of-thought (CoT) reasoning. Extensive experiments demonstrate that\nRewardDance significantly surpasses state-of-the-art methods in text-to-image,\ntext-to-video, and image-to-video generation. Crucially, we resolve the\npersistent challenge of \"reward hacking\": Our large-scale RMs exhibit and\nmaintain high reward variance during RL fine-tuning, proving their resistance\nto hacking and ability to produce diverse, high-quality outputs. It greatly\nrelieves the mode collapse problem that plagues smaller models.\n", "link": "http://arxiv.org/abs/2509.08826v1", "date": "2025-09-10", "relevancy": 2.2689, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6031}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5657}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RewardDance%3A%20Reward%20Scaling%20in%20Visual%20Generation&body=Title%3A%20RewardDance%3A%20Reward%20Scaling%20in%20Visual%20Generation%0AAuthor%3A%20Jie%20Wu%20and%20Yu%20Gao%20and%20Zilyu%20Ye%20and%20Ming%20Li%20and%20Liang%20Li%20and%20Hanzhong%20Guo%20and%20Jie%20Liu%20and%20Zeyue%20Xue%20and%20Xiaoxia%20Hou%20and%20Wei%20Liu%20and%20Yan%20Zeng%20and%20Weilin%20Huang%0AAbstract%3A%20%20%20Reward%20Models%20%28RMs%29%20are%20critical%20for%20improving%20generation%20models%20via%0AReinforcement%20Learning%20%28RL%29%2C%20yet%20the%20RM%20scaling%20paradigm%20in%20visual%20generation%0Aremains%20largely%20unexplored.%20It%20primarily%20due%20to%20fundamental%20limitations%20in%0Aexisting%20approaches%3A%20CLIP-based%20RMs%20suffer%20from%20architectural%20and%20input%0Amodality%20constraints%2C%20while%20prevalent%20Bradley-Terry%20losses%20are%20fundamentally%0Amisaligned%20with%20the%20next-token%20prediction%20mechanism%20of%20Vision-Language%20Models%0A%28VLMs%29%2C%20hindering%20effective%20scaling.%20More%20critically%2C%20the%20RLHF%20optimization%0Aprocess%20is%20plagued%20by%20Reward%20Hacking%20issue%2C%20where%20models%20exploit%20flaws%20in%20the%0Areward%20signal%20without%20improving%20true%20quality.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20RewardDance%2C%20a%20scalable%20reward%20modeling%20framework%20that%20overcomes%0Athese%20barriers%20through%20a%20novel%20generative%20reward%20paradigm.%20By%20reformulating%20the%0Areward%20score%20as%20the%20model%27s%20probability%20of%20predicting%20a%20%22yes%22%20token%2C%20indicating%0Athat%20the%20generated%20image%20outperforms%20a%20reference%20image%20according%20to%20specific%0Acriteria%2C%20RewardDance%20intrinsically%20aligns%20reward%20objectives%20with%20VLM%0Aarchitectures.%20This%20alignment%20unlocks%20scaling%20across%20two%20dimensions%3A%20%281%29%20Model%0AScaling%3A%20Systematic%20scaling%20of%20RMs%20up%20to%2026%20billion%20parameters%3B%20%282%29%20Context%0AScaling%3A%20Integration%20of%20task-specific%20instructions%2C%20reference%20examples%2C%20and%0Achain-of-thought%20%28CoT%29%20reasoning.%20Extensive%20experiments%20demonstrate%20that%0ARewardDance%20significantly%20surpasses%20state-of-the-art%20methods%20in%20text-to-image%2C%0Atext-to-video%2C%20and%20image-to-video%20generation.%20Crucially%2C%20we%20resolve%20the%0Apersistent%20challenge%20of%20%22reward%20hacking%22%3A%20Our%20large-scale%20RMs%20exhibit%20and%0Amaintain%20high%20reward%20variance%20during%20RL%20fine-tuning%2C%20proving%20their%20resistance%0Ato%20hacking%20and%20ability%20to%20produce%20diverse%2C%20high-quality%20outputs.%20It%20greatly%0Arelieves%20the%20mode%20collapse%20problem%20that%20plagues%20smaller%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRewardDance%253A%2520Reward%2520Scaling%2520in%2520Visual%2520Generation%26entry.906535625%3DJie%2520Wu%2520and%2520Yu%2520Gao%2520and%2520Zilyu%2520Ye%2520and%2520Ming%2520Li%2520and%2520Liang%2520Li%2520and%2520Hanzhong%2520Guo%2520and%2520Jie%2520Liu%2520and%2520Zeyue%2520Xue%2520and%2520Xiaoxia%2520Hou%2520and%2520Wei%2520Liu%2520and%2520Yan%2520Zeng%2520and%2520Weilin%2520Huang%26entry.1292438233%3D%2520%2520Reward%2520Models%2520%2528RMs%2529%2520are%2520critical%2520for%2520improving%2520generation%2520models%2520via%250AReinforcement%2520Learning%2520%2528RL%2529%252C%2520yet%2520the%2520RM%2520scaling%2520paradigm%2520in%2520visual%2520generation%250Aremains%2520largely%2520unexplored.%2520It%2520primarily%2520due%2520to%2520fundamental%2520limitations%2520in%250Aexisting%2520approaches%253A%2520CLIP-based%2520RMs%2520suffer%2520from%2520architectural%2520and%2520input%250Amodality%2520constraints%252C%2520while%2520prevalent%2520Bradley-Terry%2520losses%2520are%2520fundamentally%250Amisaligned%2520with%2520the%2520next-token%2520prediction%2520mechanism%2520of%2520Vision-Language%2520Models%250A%2528VLMs%2529%252C%2520hindering%2520effective%2520scaling.%2520More%2520critically%252C%2520the%2520RLHF%2520optimization%250Aprocess%2520is%2520plagued%2520by%2520Reward%2520Hacking%2520issue%252C%2520where%2520models%2520exploit%2520flaws%2520in%2520the%250Areward%2520signal%2520without%2520improving%2520true%2520quality.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520RewardDance%252C%2520a%2520scalable%2520reward%2520modeling%2520framework%2520that%2520overcomes%250Athese%2520barriers%2520through%2520a%2520novel%2520generative%2520reward%2520paradigm.%2520By%2520reformulating%2520the%250Areward%2520score%2520as%2520the%2520model%2527s%2520probability%2520of%2520predicting%2520a%2520%2522yes%2522%2520token%252C%2520indicating%250Athat%2520the%2520generated%2520image%2520outperforms%2520a%2520reference%2520image%2520according%2520to%2520specific%250Acriteria%252C%2520RewardDance%2520intrinsically%2520aligns%2520reward%2520objectives%2520with%2520VLM%250Aarchitectures.%2520This%2520alignment%2520unlocks%2520scaling%2520across%2520two%2520dimensions%253A%2520%25281%2529%2520Model%250AScaling%253A%2520Systematic%2520scaling%2520of%2520RMs%2520up%2520to%252026%2520billion%2520parameters%253B%2520%25282%2529%2520Context%250AScaling%253A%2520Integration%2520of%2520task-specific%2520instructions%252C%2520reference%2520examples%252C%2520and%250Achain-of-thought%2520%2528CoT%2529%2520reasoning.%2520Extensive%2520experiments%2520demonstrate%2520that%250ARewardDance%2520significantly%2520surpasses%2520state-of-the-art%2520methods%2520in%2520text-to-image%252C%250Atext-to-video%252C%2520and%2520image-to-video%2520generation.%2520Crucially%252C%2520we%2520resolve%2520the%250Apersistent%2520challenge%2520of%2520%2522reward%2520hacking%2522%253A%2520Our%2520large-scale%2520RMs%2520exhibit%2520and%250Amaintain%2520high%2520reward%2520variance%2520during%2520RL%2520fine-tuning%252C%2520proving%2520their%2520resistance%250Ato%2520hacking%2520and%2520ability%2520to%2520produce%2520diverse%252C%2520high-quality%2520outputs.%2520It%2520greatly%250Arelieves%2520the%2520mode%2520collapse%2520problem%2520that%2520plagues%2520smaller%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RewardDance%3A%20Reward%20Scaling%20in%20Visual%20Generation&entry.906535625=Jie%20Wu%20and%20Yu%20Gao%20and%20Zilyu%20Ye%20and%20Ming%20Li%20and%20Liang%20Li%20and%20Hanzhong%20Guo%20and%20Jie%20Liu%20and%20Zeyue%20Xue%20and%20Xiaoxia%20Hou%20and%20Wei%20Liu%20and%20Yan%20Zeng%20and%20Weilin%20Huang&entry.1292438233=%20%20Reward%20Models%20%28RMs%29%20are%20critical%20for%20improving%20generation%20models%20via%0AReinforcement%20Learning%20%28RL%29%2C%20yet%20the%20RM%20scaling%20paradigm%20in%20visual%20generation%0Aremains%20largely%20unexplored.%20It%20primarily%20due%20to%20fundamental%20limitations%20in%0Aexisting%20approaches%3A%20CLIP-based%20RMs%20suffer%20from%20architectural%20and%20input%0Amodality%20constraints%2C%20while%20prevalent%20Bradley-Terry%20losses%20are%20fundamentally%0Amisaligned%20with%20the%20next-token%20prediction%20mechanism%20of%20Vision-Language%20Models%0A%28VLMs%29%2C%20hindering%20effective%20scaling.%20More%20critically%2C%20the%20RLHF%20optimization%0Aprocess%20is%20plagued%20by%20Reward%20Hacking%20issue%2C%20where%20models%20exploit%20flaws%20in%20the%0Areward%20signal%20without%20improving%20true%20quality.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20RewardDance%2C%20a%20scalable%20reward%20modeling%20framework%20that%20overcomes%0Athese%20barriers%20through%20a%20novel%20generative%20reward%20paradigm.%20By%20reformulating%20the%0Areward%20score%20as%20the%20model%27s%20probability%20of%20predicting%20a%20%22yes%22%20token%2C%20indicating%0Athat%20the%20generated%20image%20outperforms%20a%20reference%20image%20according%20to%20specific%0Acriteria%2C%20RewardDance%20intrinsically%20aligns%20reward%20objectives%20with%20VLM%0Aarchitectures.%20This%20alignment%20unlocks%20scaling%20across%20two%20dimensions%3A%20%281%29%20Model%0AScaling%3A%20Systematic%20scaling%20of%20RMs%20up%20to%2026%20billion%20parameters%3B%20%282%29%20Context%0AScaling%3A%20Integration%20of%20task-specific%20instructions%2C%20reference%20examples%2C%20and%0Achain-of-thought%20%28CoT%29%20reasoning.%20Extensive%20experiments%20demonstrate%20that%0ARewardDance%20significantly%20surpasses%20state-of-the-art%20methods%20in%20text-to-image%2C%0Atext-to-video%2C%20and%20image-to-video%20generation.%20Crucially%2C%20we%20resolve%20the%0Apersistent%20challenge%20of%20%22reward%20hacking%22%3A%20Our%20large-scale%20RMs%20exhibit%20and%0Amaintain%20high%20reward%20variance%20during%20RL%20fine-tuning%2C%20proving%20their%20resistance%0Ato%20hacking%20and%20ability%20to%20produce%20diverse%2C%20high-quality%20outputs.%20It%20greatly%0Arelieves%20the%20mode%20collapse%20problem%20that%20plagues%20smaller%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08826v1&entry.124074799=Read"},
{"title": "CNN-ViT Hybrid for Pneumonia Detection: Theory and Empiric on Limited\n  Data without Pretraining", "author": "Prashant Singh Basnet and Roshan Chitrakar", "abstract": "  This research explored the hybridization of CNN and ViT within a training\ndataset of limited size, and introduced a distinct class imbalance. The\ntraining was made from scratch with a mere focus on theoretically and\nexperimentally exploring the architectural strengths of the proposed hybrid\nmodel. Experiments were conducted across varied data fractions with balanced\nand imbalanced training datasets. Comparatively, the hybrid model,\ncomplementing the strengths of CNN and ViT, achieved the highest recall of\n0.9443 (50% data fraction in balanced) and consistency in F1 score around 0.85,\nsuggesting reliability in diagnosis. Additionally, the model was successful in\noutperforming CNN and ViT in imbalanced datasets. Despite its complex\narchitecture, it required comparable training time to the transformers in all\ndata fractions.\n", "link": "http://arxiv.org/abs/2509.08586v1", "date": "2025-09-10", "relevancy": 2.2631, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4619}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4489}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNN-ViT%20Hybrid%20for%20Pneumonia%20Detection%3A%20Theory%20and%20Empiric%20on%20Limited%0A%20%20Data%20without%20Pretraining&body=Title%3A%20CNN-ViT%20Hybrid%20for%20Pneumonia%20Detection%3A%20Theory%20and%20Empiric%20on%20Limited%0A%20%20Data%20without%20Pretraining%0AAuthor%3A%20Prashant%20Singh%20Basnet%20and%20Roshan%20Chitrakar%0AAbstract%3A%20%20%20This%20research%20explored%20the%20hybridization%20of%20CNN%20and%20ViT%20within%20a%20training%0Adataset%20of%20limited%20size%2C%20and%20introduced%20a%20distinct%20class%20imbalance.%20The%0Atraining%20was%20made%20from%20scratch%20with%20a%20mere%20focus%20on%20theoretically%20and%0Aexperimentally%20exploring%20the%20architectural%20strengths%20of%20the%20proposed%20hybrid%0Amodel.%20Experiments%20were%20conducted%20across%20varied%20data%20fractions%20with%20balanced%0Aand%20imbalanced%20training%20datasets.%20Comparatively%2C%20the%20hybrid%20model%2C%0Acomplementing%20the%20strengths%20of%20CNN%20and%20ViT%2C%20achieved%20the%20highest%20recall%20of%0A0.9443%20%2850%25%20data%20fraction%20in%20balanced%29%20and%20consistency%20in%20F1%20score%20around%200.85%2C%0Asuggesting%20reliability%20in%20diagnosis.%20Additionally%2C%20the%20model%20was%20successful%20in%0Aoutperforming%20CNN%20and%20ViT%20in%20imbalanced%20datasets.%20Despite%20its%20complex%0Aarchitecture%2C%20it%20required%20comparable%20training%20time%20to%20the%20transformers%20in%20all%0Adata%20fractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNN-ViT%2520Hybrid%2520for%2520Pneumonia%2520Detection%253A%2520Theory%2520and%2520Empiric%2520on%2520Limited%250A%2520%2520Data%2520without%2520Pretraining%26entry.906535625%3DPrashant%2520Singh%2520Basnet%2520and%2520Roshan%2520Chitrakar%26entry.1292438233%3D%2520%2520This%2520research%2520explored%2520the%2520hybridization%2520of%2520CNN%2520and%2520ViT%2520within%2520a%2520training%250Adataset%2520of%2520limited%2520size%252C%2520and%2520introduced%2520a%2520distinct%2520class%2520imbalance.%2520The%250Atraining%2520was%2520made%2520from%2520scratch%2520with%2520a%2520mere%2520focus%2520on%2520theoretically%2520and%250Aexperimentally%2520exploring%2520the%2520architectural%2520strengths%2520of%2520the%2520proposed%2520hybrid%250Amodel.%2520Experiments%2520were%2520conducted%2520across%2520varied%2520data%2520fractions%2520with%2520balanced%250Aand%2520imbalanced%2520training%2520datasets.%2520Comparatively%252C%2520the%2520hybrid%2520model%252C%250Acomplementing%2520the%2520strengths%2520of%2520CNN%2520and%2520ViT%252C%2520achieved%2520the%2520highest%2520recall%2520of%250A0.9443%2520%252850%2525%2520data%2520fraction%2520in%2520balanced%2529%2520and%2520consistency%2520in%2520F1%2520score%2520around%25200.85%252C%250Asuggesting%2520reliability%2520in%2520diagnosis.%2520Additionally%252C%2520the%2520model%2520was%2520successful%2520in%250Aoutperforming%2520CNN%2520and%2520ViT%2520in%2520imbalanced%2520datasets.%2520Despite%2520its%2520complex%250Aarchitecture%252C%2520it%2520required%2520comparable%2520training%2520time%2520to%2520the%2520transformers%2520in%2520all%250Adata%2520fractions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNN-ViT%20Hybrid%20for%20Pneumonia%20Detection%3A%20Theory%20and%20Empiric%20on%20Limited%0A%20%20Data%20without%20Pretraining&entry.906535625=Prashant%20Singh%20Basnet%20and%20Roshan%20Chitrakar&entry.1292438233=%20%20This%20research%20explored%20the%20hybridization%20of%20CNN%20and%20ViT%20within%20a%20training%0Adataset%20of%20limited%20size%2C%20and%20introduced%20a%20distinct%20class%20imbalance.%20The%0Atraining%20was%20made%20from%20scratch%20with%20a%20mere%20focus%20on%20theoretically%20and%0Aexperimentally%20exploring%20the%20architectural%20strengths%20of%20the%20proposed%20hybrid%0Amodel.%20Experiments%20were%20conducted%20across%20varied%20data%20fractions%20with%20balanced%0Aand%20imbalanced%20training%20datasets.%20Comparatively%2C%20the%20hybrid%20model%2C%0Acomplementing%20the%20strengths%20of%20CNN%20and%20ViT%2C%20achieved%20the%20highest%20recall%20of%0A0.9443%20%2850%25%20data%20fraction%20in%20balanced%29%20and%20consistency%20in%20F1%20score%20around%200.85%2C%0Asuggesting%20reliability%20in%20diagnosis.%20Additionally%2C%20the%20model%20was%20successful%20in%0Aoutperforming%20CNN%20and%20ViT%20in%20imbalanced%20datasets.%20Despite%20its%20complex%0Aarchitecture%2C%20it%20required%20comparable%20training%20time%20to%20the%20transformers%20in%20all%0Adata%20fractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08586v1&entry.124074799=Read"},
{"title": "LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain\n  Translation", "author": "Xuqin Wang and Tao Wu and Yanfeng Zhang and Lu Liu and Dong Wang and Mingwei Sun and Yongliang Wang and Niclas Zeller and Daniel Cremers", "abstract": "  Diffusion models excel at generating high-quality outputs but face challenges\nin data-scarce domains, where exhaustive retraining or costly paired data are\noften required. To address these limitations, we propose Latent Aligned\nDiffusion Bridges (LADB), a semi-supervised framework for sample-to-sample\ntranslation that effectively bridges domain gaps using partially paired data.\nBy aligning source and target distributions within a shared latent space, LADB\nseamlessly integrates pretrained source-domain diffusion models with a\ntarget-domain Latent Aligned Diffusion Model (LADM), trained on partially\npaired latent representations. This approach enables deterministic domain\nmapping without the need for full supervision. Compared to unpaired methods,\nwhich often lack controllability, and fully paired approaches that require\nlarge, domain-specific datasets, LADB strikes a balance between fidelity and\ndiversity by leveraging a mixture of paired and unpaired latent-target\ncouplings. Our experimental results demonstrate superior performance in\ndepth-to-image translation under partial supervision. Furthermore, we extend\nLADB to handle multi-source translation (from depth maps and segmentation\nmasks) and multi-target translation in a class-conditioned style transfer task,\nshowcasing its versatility in handling diverse and heterogeneous use cases.\nUltimately, we present LADB as a scalable and versatile solution for real-world\ndomain translation, particularly in scenarios where data annotation is costly\nor incomplete.\n", "link": "http://arxiv.org/abs/2509.08628v1", "date": "2025-09-10", "relevancy": 2.2597, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5858}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5698}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LADB%3A%20Latent%20Aligned%20Diffusion%20Bridges%20for%20Semi-Supervised%20Domain%0A%20%20Translation&body=Title%3A%20LADB%3A%20Latent%20Aligned%20Diffusion%20Bridges%20for%20Semi-Supervised%20Domain%0A%20%20Translation%0AAuthor%3A%20Xuqin%20Wang%20and%20Tao%20Wu%20and%20Yanfeng%20Zhang%20and%20Lu%20Liu%20and%20Dong%20Wang%20and%20Mingwei%20Sun%20and%20Yongliang%20Wang%20and%20Niclas%20Zeller%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Diffusion%20models%20excel%20at%20generating%20high-quality%20outputs%20but%20face%20challenges%0Ain%20data-scarce%20domains%2C%20where%20exhaustive%20retraining%20or%20costly%20paired%20data%20are%0Aoften%20required.%20To%20address%20these%20limitations%2C%20we%20propose%20Latent%20Aligned%0ADiffusion%20Bridges%20%28LADB%29%2C%20a%20semi-supervised%20framework%20for%20sample-to-sample%0Atranslation%20that%20effectively%20bridges%20domain%20gaps%20using%20partially%20paired%20data.%0ABy%20aligning%20source%20and%20target%20distributions%20within%20a%20shared%20latent%20space%2C%20LADB%0Aseamlessly%20integrates%20pretrained%20source-domain%20diffusion%20models%20with%20a%0Atarget-domain%20Latent%20Aligned%20Diffusion%20Model%20%28LADM%29%2C%20trained%20on%20partially%0Apaired%20latent%20representations.%20This%20approach%20enables%20deterministic%20domain%0Amapping%20without%20the%20need%20for%20full%20supervision.%20Compared%20to%20unpaired%20methods%2C%0Awhich%20often%20lack%20controllability%2C%20and%20fully%20paired%20approaches%20that%20require%0Alarge%2C%20domain-specific%20datasets%2C%20LADB%20strikes%20a%20balance%20between%20fidelity%20and%0Adiversity%20by%20leveraging%20a%20mixture%20of%20paired%20and%20unpaired%20latent-target%0Acouplings.%20Our%20experimental%20results%20demonstrate%20superior%20performance%20in%0Adepth-to-image%20translation%20under%20partial%20supervision.%20Furthermore%2C%20we%20extend%0ALADB%20to%20handle%20multi-source%20translation%20%28from%20depth%20maps%20and%20segmentation%0Amasks%29%20and%20multi-target%20translation%20in%20a%20class-conditioned%20style%20transfer%20task%2C%0Ashowcasing%20its%20versatility%20in%20handling%20diverse%20and%20heterogeneous%20use%20cases.%0AUltimately%2C%20we%20present%20LADB%20as%20a%20scalable%20and%20versatile%20solution%20for%20real-world%0Adomain%20translation%2C%20particularly%20in%20scenarios%20where%20data%20annotation%20is%20costly%0Aor%20incomplete.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLADB%253A%2520Latent%2520Aligned%2520Diffusion%2520Bridges%2520for%2520Semi-Supervised%2520Domain%250A%2520%2520Translation%26entry.906535625%3DXuqin%2520Wang%2520and%2520Tao%2520Wu%2520and%2520Yanfeng%2520Zhang%2520and%2520Lu%2520Liu%2520and%2520Dong%2520Wang%2520and%2520Mingwei%2520Sun%2520and%2520Yongliang%2520Wang%2520and%2520Niclas%2520Zeller%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520excel%2520at%2520generating%2520high-quality%2520outputs%2520but%2520face%2520challenges%250Ain%2520data-scarce%2520domains%252C%2520where%2520exhaustive%2520retraining%2520or%2520costly%2520paired%2520data%2520are%250Aoften%2520required.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Latent%2520Aligned%250ADiffusion%2520Bridges%2520%2528LADB%2529%252C%2520a%2520semi-supervised%2520framework%2520for%2520sample-to-sample%250Atranslation%2520that%2520effectively%2520bridges%2520domain%2520gaps%2520using%2520partially%2520paired%2520data.%250ABy%2520aligning%2520source%2520and%2520target%2520distributions%2520within%2520a%2520shared%2520latent%2520space%252C%2520LADB%250Aseamlessly%2520integrates%2520pretrained%2520source-domain%2520diffusion%2520models%2520with%2520a%250Atarget-domain%2520Latent%2520Aligned%2520Diffusion%2520Model%2520%2528LADM%2529%252C%2520trained%2520on%2520partially%250Apaired%2520latent%2520representations.%2520This%2520approach%2520enables%2520deterministic%2520domain%250Amapping%2520without%2520the%2520need%2520for%2520full%2520supervision.%2520Compared%2520to%2520unpaired%2520methods%252C%250Awhich%2520often%2520lack%2520controllability%252C%2520and%2520fully%2520paired%2520approaches%2520that%2520require%250Alarge%252C%2520domain-specific%2520datasets%252C%2520LADB%2520strikes%2520a%2520balance%2520between%2520fidelity%2520and%250Adiversity%2520by%2520leveraging%2520a%2520mixture%2520of%2520paired%2520and%2520unpaired%2520latent-target%250Acouplings.%2520Our%2520experimental%2520results%2520demonstrate%2520superior%2520performance%2520in%250Adepth-to-image%2520translation%2520under%2520partial%2520supervision.%2520Furthermore%252C%2520we%2520extend%250ALADB%2520to%2520handle%2520multi-source%2520translation%2520%2528from%2520depth%2520maps%2520and%2520segmentation%250Amasks%2529%2520and%2520multi-target%2520translation%2520in%2520a%2520class-conditioned%2520style%2520transfer%2520task%252C%250Ashowcasing%2520its%2520versatility%2520in%2520handling%2520diverse%2520and%2520heterogeneous%2520use%2520cases.%250AUltimately%252C%2520we%2520present%2520LADB%2520as%2520a%2520scalable%2520and%2520versatile%2520solution%2520for%2520real-world%250Adomain%2520translation%252C%2520particularly%2520in%2520scenarios%2520where%2520data%2520annotation%2520is%2520costly%250Aor%2520incomplete.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LADB%3A%20Latent%20Aligned%20Diffusion%20Bridges%20for%20Semi-Supervised%20Domain%0A%20%20Translation&entry.906535625=Xuqin%20Wang%20and%20Tao%20Wu%20and%20Yanfeng%20Zhang%20and%20Lu%20Liu%20and%20Dong%20Wang%20and%20Mingwei%20Sun%20and%20Yongliang%20Wang%20and%20Niclas%20Zeller%20and%20Daniel%20Cremers&entry.1292438233=%20%20Diffusion%20models%20excel%20at%20generating%20high-quality%20outputs%20but%20face%20challenges%0Ain%20data-scarce%20domains%2C%20where%20exhaustive%20retraining%20or%20costly%20paired%20data%20are%0Aoften%20required.%20To%20address%20these%20limitations%2C%20we%20propose%20Latent%20Aligned%0ADiffusion%20Bridges%20%28LADB%29%2C%20a%20semi-supervised%20framework%20for%20sample-to-sample%0Atranslation%20that%20effectively%20bridges%20domain%20gaps%20using%20partially%20paired%20data.%0ABy%20aligning%20source%20and%20target%20distributions%20within%20a%20shared%20latent%20space%2C%20LADB%0Aseamlessly%20integrates%20pretrained%20source-domain%20diffusion%20models%20with%20a%0Atarget-domain%20Latent%20Aligned%20Diffusion%20Model%20%28LADM%29%2C%20trained%20on%20partially%0Apaired%20latent%20representations.%20This%20approach%20enables%20deterministic%20domain%0Amapping%20without%20the%20need%20for%20full%20supervision.%20Compared%20to%20unpaired%20methods%2C%0Awhich%20often%20lack%20controllability%2C%20and%20fully%20paired%20approaches%20that%20require%0Alarge%2C%20domain-specific%20datasets%2C%20LADB%20strikes%20a%20balance%20between%20fidelity%20and%0Adiversity%20by%20leveraging%20a%20mixture%20of%20paired%20and%20unpaired%20latent-target%0Acouplings.%20Our%20experimental%20results%20demonstrate%20superior%20performance%20in%0Adepth-to-image%20translation%20under%20partial%20supervision.%20Furthermore%2C%20we%20extend%0ALADB%20to%20handle%20multi-source%20translation%20%28from%20depth%20maps%20and%20segmentation%0Amasks%29%20and%20multi-target%20translation%20in%20a%20class-conditioned%20style%20transfer%20task%2C%0Ashowcasing%20its%20versatility%20in%20handling%20diverse%20and%20heterogeneous%20use%20cases.%0AUltimately%2C%20we%20present%20LADB%20as%20a%20scalable%20and%20versatile%20solution%20for%20real-world%0Adomain%20translation%2C%20particularly%20in%20scenarios%20where%20data%20annotation%20is%20costly%0Aor%20incomplete.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08628v1&entry.124074799=Read"},
{"title": "Chirality in Action: Time-Aware Video Representation Learning by Latent\n  Straightening", "author": "Piyush Bagad and Andrew Zisserman", "abstract": "  Our objective is to develop compact video representations that are sensitive\nto visual change over time. To measure such time-sensitivity, we introduce a\nnew task: chiral action recognition, where one needs to distinguish between a\npair of temporally opposite actions, such as \"opening vs. closing a door\",\n\"approaching vs. moving away from something\", \"folding vs. unfolding paper\",\netc. Such actions (i) occur frequently in everyday life, (ii) require\nunderstanding of simple visual change over time (in object state, size, spatial\nposition, count . . . ), and (iii) are known to be poorly represented by many\nvideo embeddings. Our goal is to build time aware video representations which\noffer linear separability between these chiral pairs. To that end, we propose a\nself-supervised adaptation recipe to inject time-sensitivity into a sequence of\nfrozen image features. Our model is based on an auto-encoder with a latent\nspace with inductive bias inspired by perceptual straightening. We show that\nthis results in a compact but time-sensitive video representation for the\nproposed task across three datasets: Something-Something, EPIC-Kitchens, and\nCharade. Our method (i) outperforms much larger video models pre-trained on\nlarge-scale video datasets, and (ii) leads to an improvement in classification\nperformance on standard benchmarks when combined with these existing models.\n", "link": "http://arxiv.org/abs/2509.08502v1", "date": "2025-09-10", "relevancy": 2.2586, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5632}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chirality%20in%20Action%3A%20Time-Aware%20Video%20Representation%20Learning%20by%20Latent%0A%20%20Straightening&body=Title%3A%20Chirality%20in%20Action%3A%20Time-Aware%20Video%20Representation%20Learning%20by%20Latent%0A%20%20Straightening%0AAuthor%3A%20Piyush%20Bagad%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20Our%20objective%20is%20to%20develop%20compact%20video%20representations%20that%20are%20sensitive%0Ato%20visual%20change%20over%20time.%20To%20measure%20such%20time-sensitivity%2C%20we%20introduce%20a%0Anew%20task%3A%20chiral%20action%20recognition%2C%20where%20one%20needs%20to%20distinguish%20between%20a%0Apair%20of%20temporally%20opposite%20actions%2C%20such%20as%20%22opening%20vs.%20closing%20a%20door%22%2C%0A%22approaching%20vs.%20moving%20away%20from%20something%22%2C%20%22folding%20vs.%20unfolding%20paper%22%2C%0Aetc.%20Such%20actions%20%28i%29%20occur%20frequently%20in%20everyday%20life%2C%20%28ii%29%20require%0Aunderstanding%20of%20simple%20visual%20change%20over%20time%20%28in%20object%20state%2C%20size%2C%20spatial%0Aposition%2C%20count%20.%20.%20.%20%29%2C%20and%20%28iii%29%20are%20known%20to%20be%20poorly%20represented%20by%20many%0Avideo%20embeddings.%20Our%20goal%20is%20to%20build%20time%20aware%20video%20representations%20which%0Aoffer%20linear%20separability%20between%20these%20chiral%20pairs.%20To%20that%20end%2C%20we%20propose%20a%0Aself-supervised%20adaptation%20recipe%20to%20inject%20time-sensitivity%20into%20a%20sequence%20of%0Afrozen%20image%20features.%20Our%20model%20is%20based%20on%20an%20auto-encoder%20with%20a%20latent%0Aspace%20with%20inductive%20bias%20inspired%20by%20perceptual%20straightening.%20We%20show%20that%0Athis%20results%20in%20a%20compact%20but%20time-sensitive%20video%20representation%20for%20the%0Aproposed%20task%20across%20three%20datasets%3A%20Something-Something%2C%20EPIC-Kitchens%2C%20and%0ACharade.%20Our%20method%20%28i%29%20outperforms%20much%20larger%20video%20models%20pre-trained%20on%0Alarge-scale%20video%20datasets%2C%20and%20%28ii%29%20leads%20to%20an%20improvement%20in%20classification%0Aperformance%20on%20standard%20benchmarks%20when%20combined%20with%20these%20existing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChirality%2520in%2520Action%253A%2520Time-Aware%2520Video%2520Representation%2520Learning%2520by%2520Latent%250A%2520%2520Straightening%26entry.906535625%3DPiyush%2520Bagad%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520Our%2520objective%2520is%2520to%2520develop%2520compact%2520video%2520representations%2520that%2520are%2520sensitive%250Ato%2520visual%2520change%2520over%2520time.%2520To%2520measure%2520such%2520time-sensitivity%252C%2520we%2520introduce%2520a%250Anew%2520task%253A%2520chiral%2520action%2520recognition%252C%2520where%2520one%2520needs%2520to%2520distinguish%2520between%2520a%250Apair%2520of%2520temporally%2520opposite%2520actions%252C%2520such%2520as%2520%2522opening%2520vs.%2520closing%2520a%2520door%2522%252C%250A%2522approaching%2520vs.%2520moving%2520away%2520from%2520something%2522%252C%2520%2522folding%2520vs.%2520unfolding%2520paper%2522%252C%250Aetc.%2520Such%2520actions%2520%2528i%2529%2520occur%2520frequently%2520in%2520everyday%2520life%252C%2520%2528ii%2529%2520require%250Aunderstanding%2520of%2520simple%2520visual%2520change%2520over%2520time%2520%2528in%2520object%2520state%252C%2520size%252C%2520spatial%250Aposition%252C%2520count%2520.%2520.%2520.%2520%2529%252C%2520and%2520%2528iii%2529%2520are%2520known%2520to%2520be%2520poorly%2520represented%2520by%2520many%250Avideo%2520embeddings.%2520Our%2520goal%2520is%2520to%2520build%2520time%2520aware%2520video%2520representations%2520which%250Aoffer%2520linear%2520separability%2520between%2520these%2520chiral%2520pairs.%2520To%2520that%2520end%252C%2520we%2520propose%2520a%250Aself-supervised%2520adaptation%2520recipe%2520to%2520inject%2520time-sensitivity%2520into%2520a%2520sequence%2520of%250Afrozen%2520image%2520features.%2520Our%2520model%2520is%2520based%2520on%2520an%2520auto-encoder%2520with%2520a%2520latent%250Aspace%2520with%2520inductive%2520bias%2520inspired%2520by%2520perceptual%2520straightening.%2520We%2520show%2520that%250Athis%2520results%2520in%2520a%2520compact%2520but%2520time-sensitive%2520video%2520representation%2520for%2520the%250Aproposed%2520task%2520across%2520three%2520datasets%253A%2520Something-Something%252C%2520EPIC-Kitchens%252C%2520and%250ACharade.%2520Our%2520method%2520%2528i%2529%2520outperforms%2520much%2520larger%2520video%2520models%2520pre-trained%2520on%250Alarge-scale%2520video%2520datasets%252C%2520and%2520%2528ii%2529%2520leads%2520to%2520an%2520improvement%2520in%2520classification%250Aperformance%2520on%2520standard%2520benchmarks%2520when%2520combined%2520with%2520these%2520existing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chirality%20in%20Action%3A%20Time-Aware%20Video%20Representation%20Learning%20by%20Latent%0A%20%20Straightening&entry.906535625=Piyush%20Bagad%20and%20Andrew%20Zisserman&entry.1292438233=%20%20Our%20objective%20is%20to%20develop%20compact%20video%20representations%20that%20are%20sensitive%0Ato%20visual%20change%20over%20time.%20To%20measure%20such%20time-sensitivity%2C%20we%20introduce%20a%0Anew%20task%3A%20chiral%20action%20recognition%2C%20where%20one%20needs%20to%20distinguish%20between%20a%0Apair%20of%20temporally%20opposite%20actions%2C%20such%20as%20%22opening%20vs.%20closing%20a%20door%22%2C%0A%22approaching%20vs.%20moving%20away%20from%20something%22%2C%20%22folding%20vs.%20unfolding%20paper%22%2C%0Aetc.%20Such%20actions%20%28i%29%20occur%20frequently%20in%20everyday%20life%2C%20%28ii%29%20require%0Aunderstanding%20of%20simple%20visual%20change%20over%20time%20%28in%20object%20state%2C%20size%2C%20spatial%0Aposition%2C%20count%20.%20.%20.%20%29%2C%20and%20%28iii%29%20are%20known%20to%20be%20poorly%20represented%20by%20many%0Avideo%20embeddings.%20Our%20goal%20is%20to%20build%20time%20aware%20video%20representations%20which%0Aoffer%20linear%20separability%20between%20these%20chiral%20pairs.%20To%20that%20end%2C%20we%20propose%20a%0Aself-supervised%20adaptation%20recipe%20to%20inject%20time-sensitivity%20into%20a%20sequence%20of%0Afrozen%20image%20features.%20Our%20model%20is%20based%20on%20an%20auto-encoder%20with%20a%20latent%0Aspace%20with%20inductive%20bias%20inspired%20by%20perceptual%20straightening.%20We%20show%20that%0Athis%20results%20in%20a%20compact%20but%20time-sensitive%20video%20representation%20for%20the%0Aproposed%20task%20across%20three%20datasets%3A%20Something-Something%2C%20EPIC-Kitchens%2C%20and%0ACharade.%20Our%20method%20%28i%29%20outperforms%20much%20larger%20video%20models%20pre-trained%20on%0Alarge-scale%20video%20datasets%2C%20and%20%28ii%29%20leads%20to%20an%20improvement%20in%20classification%0Aperformance%20on%20standard%20benchmarks%20when%20combined%20with%20these%20existing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08502v1&entry.124074799=Read"},
{"title": "Computational Imaging for Enhanced Computer Vision", "author": "Humera Shaikh and Kaur Jashanpreet", "abstract": "  This paper presents a comprehensive survey of computational imaging (CI)\ntechniques and their transformative impact on computer vision (CV)\napplications. Conventional imaging methods often fail to deliver high-fidelity\nvisual data in challenging conditions, such as low light, motion blur, or high\ndynamic range scenes, thereby limiting the performance of state-of-the-art CV\nsystems. Computational imaging techniques, including light field imaging, high\ndynamic range (HDR) imaging, deblurring, high-speed imaging, and glare\nmitigation, address these limitations by enhancing image acquisition and\nreconstruction processes. This survey systematically explores the synergies\nbetween CI techniques and core CV tasks, including object detection, depth\nestimation, optical flow, face recognition, and keypoint detection. By\nanalyzing the relationships between CI methods and their practical\ncontributions to CV applications, this work highlights emerging opportunities,\nchallenges, and future research directions. We emphasize the potential for\ntask-specific, adaptive imaging pipelines that improve robustness, accuracy,\nand efficiency in real-world scenarios, such as autonomous navigation,\nsurveillance, augmented reality, and robotics.\n", "link": "http://arxiv.org/abs/2509.08712v1", "date": "2025-09-10", "relevancy": 2.2461, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5697}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5599}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computational%20Imaging%20for%20Enhanced%20Computer%20Vision&body=Title%3A%20Computational%20Imaging%20for%20Enhanced%20Computer%20Vision%0AAuthor%3A%20Humera%20Shaikh%20and%20Kaur%20Jashanpreet%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20survey%20of%20computational%20imaging%20%28CI%29%0Atechniques%20and%20their%20transformative%20impact%20on%20computer%20vision%20%28CV%29%0Aapplications.%20Conventional%20imaging%20methods%20often%20fail%20to%20deliver%20high-fidelity%0Avisual%20data%20in%20challenging%20conditions%2C%20such%20as%20low%20light%2C%20motion%20blur%2C%20or%20high%0Adynamic%20range%20scenes%2C%20thereby%20limiting%20the%20performance%20of%20state-of-the-art%20CV%0Asystems.%20Computational%20imaging%20techniques%2C%20including%20light%20field%20imaging%2C%20high%0Adynamic%20range%20%28HDR%29%20imaging%2C%20deblurring%2C%20high-speed%20imaging%2C%20and%20glare%0Amitigation%2C%20address%20these%20limitations%20by%20enhancing%20image%20acquisition%20and%0Areconstruction%20processes.%20This%20survey%20systematically%20explores%20the%20synergies%0Abetween%20CI%20techniques%20and%20core%20CV%20tasks%2C%20including%20object%20detection%2C%20depth%0Aestimation%2C%20optical%20flow%2C%20face%20recognition%2C%20and%20keypoint%20detection.%20By%0Aanalyzing%20the%20relationships%20between%20CI%20methods%20and%20their%20practical%0Acontributions%20to%20CV%20applications%2C%20this%20work%20highlights%20emerging%20opportunities%2C%0Achallenges%2C%20and%20future%20research%20directions.%20We%20emphasize%20the%20potential%20for%0Atask-specific%2C%20adaptive%20imaging%20pipelines%20that%20improve%20robustness%2C%20accuracy%2C%0Aand%20efficiency%20in%20real-world%20scenarios%2C%20such%20as%20autonomous%20navigation%2C%0Asurveillance%2C%20augmented%20reality%2C%20and%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputational%2520Imaging%2520for%2520Enhanced%2520Computer%2520Vision%26entry.906535625%3DHumera%2520Shaikh%2520and%2520Kaur%2520Jashanpreet%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520survey%2520of%2520computational%2520imaging%2520%2528CI%2529%250Atechniques%2520and%2520their%2520transformative%2520impact%2520on%2520computer%2520vision%2520%2528CV%2529%250Aapplications.%2520Conventional%2520imaging%2520methods%2520often%2520fail%2520to%2520deliver%2520high-fidelity%250Avisual%2520data%2520in%2520challenging%2520conditions%252C%2520such%2520as%2520low%2520light%252C%2520motion%2520blur%252C%2520or%2520high%250Adynamic%2520range%2520scenes%252C%2520thereby%2520limiting%2520the%2520performance%2520of%2520state-of-the-art%2520CV%250Asystems.%2520Computational%2520imaging%2520techniques%252C%2520including%2520light%2520field%2520imaging%252C%2520high%250Adynamic%2520range%2520%2528HDR%2529%2520imaging%252C%2520deblurring%252C%2520high-speed%2520imaging%252C%2520and%2520glare%250Amitigation%252C%2520address%2520these%2520limitations%2520by%2520enhancing%2520image%2520acquisition%2520and%250Areconstruction%2520processes.%2520This%2520survey%2520systematically%2520explores%2520the%2520synergies%250Abetween%2520CI%2520techniques%2520and%2520core%2520CV%2520tasks%252C%2520including%2520object%2520detection%252C%2520depth%250Aestimation%252C%2520optical%2520flow%252C%2520face%2520recognition%252C%2520and%2520keypoint%2520detection.%2520By%250Aanalyzing%2520the%2520relationships%2520between%2520CI%2520methods%2520and%2520their%2520practical%250Acontributions%2520to%2520CV%2520applications%252C%2520this%2520work%2520highlights%2520emerging%2520opportunities%252C%250Achallenges%252C%2520and%2520future%2520research%2520directions.%2520We%2520emphasize%2520the%2520potential%2520for%250Atask-specific%252C%2520adaptive%2520imaging%2520pipelines%2520that%2520improve%2520robustness%252C%2520accuracy%252C%250Aand%2520efficiency%2520in%2520real-world%2520scenarios%252C%2520such%2520as%2520autonomous%2520navigation%252C%250Asurveillance%252C%2520augmented%2520reality%252C%2520and%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computational%20Imaging%20for%20Enhanced%20Computer%20Vision&entry.906535625=Humera%20Shaikh%20and%20Kaur%20Jashanpreet&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20survey%20of%20computational%20imaging%20%28CI%29%0Atechniques%20and%20their%20transformative%20impact%20on%20computer%20vision%20%28CV%29%0Aapplications.%20Conventional%20imaging%20methods%20often%20fail%20to%20deliver%20high-fidelity%0Avisual%20data%20in%20challenging%20conditions%2C%20such%20as%20low%20light%2C%20motion%20blur%2C%20or%20high%0Adynamic%20range%20scenes%2C%20thereby%20limiting%20the%20performance%20of%20state-of-the-art%20CV%0Asystems.%20Computational%20imaging%20techniques%2C%20including%20light%20field%20imaging%2C%20high%0Adynamic%20range%20%28HDR%29%20imaging%2C%20deblurring%2C%20high-speed%20imaging%2C%20and%20glare%0Amitigation%2C%20address%20these%20limitations%20by%20enhancing%20image%20acquisition%20and%0Areconstruction%20processes.%20This%20survey%20systematically%20explores%20the%20synergies%0Abetween%20CI%20techniques%20and%20core%20CV%20tasks%2C%20including%20object%20detection%2C%20depth%0Aestimation%2C%20optical%20flow%2C%20face%20recognition%2C%20and%20keypoint%20detection.%20By%0Aanalyzing%20the%20relationships%20between%20CI%20methods%20and%20their%20practical%0Acontributions%20to%20CV%20applications%2C%20this%20work%20highlights%20emerging%20opportunities%2C%0Achallenges%2C%20and%20future%20research%20directions.%20We%20emphasize%20the%20potential%20for%0Atask-specific%2C%20adaptive%20imaging%20pipelines%20that%20improve%20robustness%2C%20accuracy%2C%0Aand%20efficiency%20in%20real-world%20scenarios%2C%20such%20as%20autonomous%20navigation%2C%0Asurveillance%2C%20augmented%20reality%2C%20and%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08712v1&entry.124074799=Read"},
{"title": "Tokenizing Loops of Antibodies", "author": "Ada Fang and Robert G. Alberstein and Simon Kelow and Fr\u00e9d\u00e9ric A. Dreyer", "abstract": "  The complementarity-determining regions of antibodies are loop structures\nthat are key to their interactions with antigens, and of high importance to the\ndesign of novel biologics. Since the 1980s, categorizing the diversity of CDR\nstructures into canonical clusters has enabled the identification of key\nstructural motifs of antibodies. However, existing approaches have limited\ncoverage and cannot be readily incorporated into protein foundation models.\nHere we introduce ImmunoGlobulin LOOp Tokenizer, Igloo, a multimodal antibody\nloop tokenizer that encodes backbone dihedral angles and sequence. Igloo is\ntrained using a contrastive learning objective to map loops with similar\nbackbone dihedral angles closer together in latent space. Igloo can efficiently\nretrieve the closest matching loop structures from a structural antibody\ndatabase, outperforming existing methods on identifying similar H3 loops by\n5.9\\%. Igloo assigns tokens to all loops, addressing the limited coverage issue\nof canonical clusters, while retaining the ability to recover canonical loop\nconformations. To demonstrate the versatility of Igloo tokens, we show that\nthey can be incorporated into protein language models with IglooLM and\nIglooALM. On predicting binding affinity of heavy chain variants, IglooLM\noutperforms the base protein language model on 8 out of 10 antibody-antigen\ntargets. Additionally, it is on par with existing state-of-the-art\nsequence-based and multimodal protein language models, performing comparably to\nmodels with $7\\times$ more parameters. IglooALM samples antibody loops which\nare diverse in sequence and more consistent in structure than state-of-the-art\nantibody inverse folding models. Igloo demonstrates the benefit of introducing\nmultimodal tokens for antibody loops for encoding the diverse landscape of\nantibody loops, improving protein foundation models, and for antibody CDR\ndesign.\n", "link": "http://arxiv.org/abs/2509.08707v1", "date": "2025-09-10", "relevancy": 2.2403, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokenizing%20Loops%20of%20Antibodies&body=Title%3A%20Tokenizing%20Loops%20of%20Antibodies%0AAuthor%3A%20Ada%20Fang%20and%20Robert%20G.%20Alberstein%20and%20Simon%20Kelow%20and%20Fr%C3%A9d%C3%A9ric%20A.%20Dreyer%0AAbstract%3A%20%20%20The%20complementarity-determining%20regions%20of%20antibodies%20are%20loop%20structures%0Athat%20are%20key%20to%20their%20interactions%20with%20antigens%2C%20and%20of%20high%20importance%20to%20the%0Adesign%20of%20novel%20biologics.%20Since%20the%201980s%2C%20categorizing%20the%20diversity%20of%20CDR%0Astructures%20into%20canonical%20clusters%20has%20enabled%20the%20identification%20of%20key%0Astructural%20motifs%20of%20antibodies.%20However%2C%20existing%20approaches%20have%20limited%0Acoverage%20and%20cannot%20be%20readily%20incorporated%20into%20protein%20foundation%20models.%0AHere%20we%20introduce%20ImmunoGlobulin%20LOOp%20Tokenizer%2C%20Igloo%2C%20a%20multimodal%20antibody%0Aloop%20tokenizer%20that%20encodes%20backbone%20dihedral%20angles%20and%20sequence.%20Igloo%20is%0Atrained%20using%20a%20contrastive%20learning%20objective%20to%20map%20loops%20with%20similar%0Abackbone%20dihedral%20angles%20closer%20together%20in%20latent%20space.%20Igloo%20can%20efficiently%0Aretrieve%20the%20closest%20matching%20loop%20structures%20from%20a%20structural%20antibody%0Adatabase%2C%20outperforming%20existing%20methods%20on%20identifying%20similar%20H3%20loops%20by%0A5.9%5C%25.%20Igloo%20assigns%20tokens%20to%20all%20loops%2C%20addressing%20the%20limited%20coverage%20issue%0Aof%20canonical%20clusters%2C%20while%20retaining%20the%20ability%20to%20recover%20canonical%20loop%0Aconformations.%20To%20demonstrate%20the%20versatility%20of%20Igloo%20tokens%2C%20we%20show%20that%0Athey%20can%20be%20incorporated%20into%20protein%20language%20models%20with%20IglooLM%20and%0AIglooALM.%20On%20predicting%20binding%20affinity%20of%20heavy%20chain%20variants%2C%20IglooLM%0Aoutperforms%20the%20base%20protein%20language%20model%20on%208%20out%20of%2010%20antibody-antigen%0Atargets.%20Additionally%2C%20it%20is%20on%20par%20with%20existing%20state-of-the-art%0Asequence-based%20and%20multimodal%20protein%20language%20models%2C%20performing%20comparably%20to%0Amodels%20with%20%247%5Ctimes%24%20more%20parameters.%20IglooALM%20samples%20antibody%20loops%20which%0Aare%20diverse%20in%20sequence%20and%20more%20consistent%20in%20structure%20than%20state-of-the-art%0Aantibody%20inverse%20folding%20models.%20Igloo%20demonstrates%20the%20benefit%20of%20introducing%0Amultimodal%20tokens%20for%20antibody%20loops%20for%20encoding%20the%20diverse%20landscape%20of%0Aantibody%20loops%2C%20improving%20protein%20foundation%20models%2C%20and%20for%20antibody%20CDR%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenizing%2520Loops%2520of%2520Antibodies%26entry.906535625%3DAda%2520Fang%2520and%2520Robert%2520G.%2520Alberstein%2520and%2520Simon%2520Kelow%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520A.%2520Dreyer%26entry.1292438233%3D%2520%2520The%2520complementarity-determining%2520regions%2520of%2520antibodies%2520are%2520loop%2520structures%250Athat%2520are%2520key%2520to%2520their%2520interactions%2520with%2520antigens%252C%2520and%2520of%2520high%2520importance%2520to%2520the%250Adesign%2520of%2520novel%2520biologics.%2520Since%2520the%25201980s%252C%2520categorizing%2520the%2520diversity%2520of%2520CDR%250Astructures%2520into%2520canonical%2520clusters%2520has%2520enabled%2520the%2520identification%2520of%2520key%250Astructural%2520motifs%2520of%2520antibodies.%2520However%252C%2520existing%2520approaches%2520have%2520limited%250Acoverage%2520and%2520cannot%2520be%2520readily%2520incorporated%2520into%2520protein%2520foundation%2520models.%250AHere%2520we%2520introduce%2520ImmunoGlobulin%2520LOOp%2520Tokenizer%252C%2520Igloo%252C%2520a%2520multimodal%2520antibody%250Aloop%2520tokenizer%2520that%2520encodes%2520backbone%2520dihedral%2520angles%2520and%2520sequence.%2520Igloo%2520is%250Atrained%2520using%2520a%2520contrastive%2520learning%2520objective%2520to%2520map%2520loops%2520with%2520similar%250Abackbone%2520dihedral%2520angles%2520closer%2520together%2520in%2520latent%2520space.%2520Igloo%2520can%2520efficiently%250Aretrieve%2520the%2520closest%2520matching%2520loop%2520structures%2520from%2520a%2520structural%2520antibody%250Adatabase%252C%2520outperforming%2520existing%2520methods%2520on%2520identifying%2520similar%2520H3%2520loops%2520by%250A5.9%255C%2525.%2520Igloo%2520assigns%2520tokens%2520to%2520all%2520loops%252C%2520addressing%2520the%2520limited%2520coverage%2520issue%250Aof%2520canonical%2520clusters%252C%2520while%2520retaining%2520the%2520ability%2520to%2520recover%2520canonical%2520loop%250Aconformations.%2520To%2520demonstrate%2520the%2520versatility%2520of%2520Igloo%2520tokens%252C%2520we%2520show%2520that%250Athey%2520can%2520be%2520incorporated%2520into%2520protein%2520language%2520models%2520with%2520IglooLM%2520and%250AIglooALM.%2520On%2520predicting%2520binding%2520affinity%2520of%2520heavy%2520chain%2520variants%252C%2520IglooLM%250Aoutperforms%2520the%2520base%2520protein%2520language%2520model%2520on%25208%2520out%2520of%252010%2520antibody-antigen%250Atargets.%2520Additionally%252C%2520it%2520is%2520on%2520par%2520with%2520existing%2520state-of-the-art%250Asequence-based%2520and%2520multimodal%2520protein%2520language%2520models%252C%2520performing%2520comparably%2520to%250Amodels%2520with%2520%25247%255Ctimes%2524%2520more%2520parameters.%2520IglooALM%2520samples%2520antibody%2520loops%2520which%250Aare%2520diverse%2520in%2520sequence%2520and%2520more%2520consistent%2520in%2520structure%2520than%2520state-of-the-art%250Aantibody%2520inverse%2520folding%2520models.%2520Igloo%2520demonstrates%2520the%2520benefit%2520of%2520introducing%250Amultimodal%2520tokens%2520for%2520antibody%2520loops%2520for%2520encoding%2520the%2520diverse%2520landscape%2520of%250Aantibody%2520loops%252C%2520improving%2520protein%2520foundation%2520models%252C%2520and%2520for%2520antibody%2520CDR%250Adesign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokenizing%20Loops%20of%20Antibodies&entry.906535625=Ada%20Fang%20and%20Robert%20G.%20Alberstein%20and%20Simon%20Kelow%20and%20Fr%C3%A9d%C3%A9ric%20A.%20Dreyer&entry.1292438233=%20%20The%20complementarity-determining%20regions%20of%20antibodies%20are%20loop%20structures%0Athat%20are%20key%20to%20their%20interactions%20with%20antigens%2C%20and%20of%20high%20importance%20to%20the%0Adesign%20of%20novel%20biologics.%20Since%20the%201980s%2C%20categorizing%20the%20diversity%20of%20CDR%0Astructures%20into%20canonical%20clusters%20has%20enabled%20the%20identification%20of%20key%0Astructural%20motifs%20of%20antibodies.%20However%2C%20existing%20approaches%20have%20limited%0Acoverage%20and%20cannot%20be%20readily%20incorporated%20into%20protein%20foundation%20models.%0AHere%20we%20introduce%20ImmunoGlobulin%20LOOp%20Tokenizer%2C%20Igloo%2C%20a%20multimodal%20antibody%0Aloop%20tokenizer%20that%20encodes%20backbone%20dihedral%20angles%20and%20sequence.%20Igloo%20is%0Atrained%20using%20a%20contrastive%20learning%20objective%20to%20map%20loops%20with%20similar%0Abackbone%20dihedral%20angles%20closer%20together%20in%20latent%20space.%20Igloo%20can%20efficiently%0Aretrieve%20the%20closest%20matching%20loop%20structures%20from%20a%20structural%20antibody%0Adatabase%2C%20outperforming%20existing%20methods%20on%20identifying%20similar%20H3%20loops%20by%0A5.9%5C%25.%20Igloo%20assigns%20tokens%20to%20all%20loops%2C%20addressing%20the%20limited%20coverage%20issue%0Aof%20canonical%20clusters%2C%20while%20retaining%20the%20ability%20to%20recover%20canonical%20loop%0Aconformations.%20To%20demonstrate%20the%20versatility%20of%20Igloo%20tokens%2C%20we%20show%20that%0Athey%20can%20be%20incorporated%20into%20protein%20language%20models%20with%20IglooLM%20and%0AIglooALM.%20On%20predicting%20binding%20affinity%20of%20heavy%20chain%20variants%2C%20IglooLM%0Aoutperforms%20the%20base%20protein%20language%20model%20on%208%20out%20of%2010%20antibody-antigen%0Atargets.%20Additionally%2C%20it%20is%20on%20par%20with%20existing%20state-of-the-art%0Asequence-based%20and%20multimodal%20protein%20language%20models%2C%20performing%20comparably%20to%0Amodels%20with%20%247%5Ctimes%24%20more%20parameters.%20IglooALM%20samples%20antibody%20loops%20which%0Aare%20diverse%20in%20sequence%20and%20more%20consistent%20in%20structure%20than%20state-of-the-art%0Aantibody%20inverse%20folding%20models.%20Igloo%20demonstrates%20the%20benefit%20of%20introducing%0Amultimodal%20tokens%20for%20antibody%20loops%20for%20encoding%20the%20diverse%20landscape%20of%0Aantibody%20loops%2C%20improving%20protein%20foundation%20models%2C%20and%20for%20antibody%20CDR%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08707v1&entry.124074799=Read"},
{"title": "MoVoC: Morphology-Aware Subword Construction for Geez Script Languages", "author": "Hailay Kidu Teklehaymanot and Dren Fazlija and Wolfgang Nejdl", "abstract": "  Subword-based tokenization methods often fail to preserve morphological\nboundaries, a limitation especially pronounced in low-resource, morphologically\ncomplex languages such as those written in the Geez script. To address this, we\npresent MoVoC (Morpheme-aware Subword Vocabulary Construction) and train\nMoVoC-Tok, a tokenizer that integrates supervised morphological analysis into\nthe subword vocabulary. This hybrid segmentation approach combines\nmorpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological\nintegrity while maintaining lexical meaning. To tackle resource scarcity, we\ncurate and release manually annotated morpheme data for four Geez script\nlanguages and a morpheme-aware vocabulary for two of them. While the proposed\ntokenization method does not lead to significant gains in automatic translation\nquality, we observe consistent improvements in intrinsic metrics, MorphoScore,\nand Boundary Precision, highlighting the value of morphology-aware segmentation\nin enhancing linguistic fidelity and token efficiency. Our morpheme-annotated\ndatasets and tokenizer will be publicly available to support further research\nin low-resource, morphologically rich languages. Our code and data are\navailable on GitHub: https://github.com/hailaykidu/MoVoC\n", "link": "http://arxiv.org/abs/2509.08812v1", "date": "2025-09-10", "relevancy": 2.2261, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4573}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4415}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoVoC%3A%20Morphology-Aware%20Subword%20Construction%20for%20Geez%20Script%20Languages&body=Title%3A%20MoVoC%3A%20Morphology-Aware%20Subword%20Construction%20for%20Geez%20Script%20Languages%0AAuthor%3A%20Hailay%20Kidu%20Teklehaymanot%20and%20Dren%20Fazlija%20and%20Wolfgang%20Nejdl%0AAbstract%3A%20%20%20Subword-based%20tokenization%20methods%20often%20fail%20to%20preserve%20morphological%0Aboundaries%2C%20a%20limitation%20especially%20pronounced%20in%20low-resource%2C%20morphologically%0Acomplex%20languages%20such%20as%20those%20written%20in%20the%20Geez%20script.%20To%20address%20this%2C%20we%0Apresent%20MoVoC%20%28Morpheme-aware%20Subword%20Vocabulary%20Construction%29%20and%20train%0AMoVoC-Tok%2C%20a%20tokenizer%20that%20integrates%20supervised%20morphological%20analysis%20into%0Athe%20subword%20vocabulary.%20This%20hybrid%20segmentation%20approach%20combines%0Amorpheme-based%20and%20Byte%20Pair%20Encoding%20%28BPE%29%20tokens%20to%20preserve%20morphological%0Aintegrity%20while%20maintaining%20lexical%20meaning.%20To%20tackle%20resource%20scarcity%2C%20we%0Acurate%20and%20release%20manually%20annotated%20morpheme%20data%20for%20four%20Geez%20script%0Alanguages%20and%20a%20morpheme-aware%20vocabulary%20for%20two%20of%20them.%20While%20the%20proposed%0Atokenization%20method%20does%20not%20lead%20to%20significant%20gains%20in%20automatic%20translation%0Aquality%2C%20we%20observe%20consistent%20improvements%20in%20intrinsic%20metrics%2C%20MorphoScore%2C%0Aand%20Boundary%20Precision%2C%20highlighting%20the%20value%20of%20morphology-aware%20segmentation%0Ain%20enhancing%20linguistic%20fidelity%20and%20token%20efficiency.%20Our%20morpheme-annotated%0Adatasets%20and%20tokenizer%20will%20be%20publicly%20available%20to%20support%20further%20research%0Ain%20low-resource%2C%20morphologically%20rich%20languages.%20Our%20code%20and%20data%20are%0Aavailable%20on%20GitHub%3A%20https%3A//github.com/hailaykidu/MoVoC%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoVoC%253A%2520Morphology-Aware%2520Subword%2520Construction%2520for%2520Geez%2520Script%2520Languages%26entry.906535625%3DHailay%2520Kidu%2520Teklehaymanot%2520and%2520Dren%2520Fazlija%2520and%2520Wolfgang%2520Nejdl%26entry.1292438233%3D%2520%2520Subword-based%2520tokenization%2520methods%2520often%2520fail%2520to%2520preserve%2520morphological%250Aboundaries%252C%2520a%2520limitation%2520especially%2520pronounced%2520in%2520low-resource%252C%2520morphologically%250Acomplex%2520languages%2520such%2520as%2520those%2520written%2520in%2520the%2520Geez%2520script.%2520To%2520address%2520this%252C%2520we%250Apresent%2520MoVoC%2520%2528Morpheme-aware%2520Subword%2520Vocabulary%2520Construction%2529%2520and%2520train%250AMoVoC-Tok%252C%2520a%2520tokenizer%2520that%2520integrates%2520supervised%2520morphological%2520analysis%2520into%250Athe%2520subword%2520vocabulary.%2520This%2520hybrid%2520segmentation%2520approach%2520combines%250Amorpheme-based%2520and%2520Byte%2520Pair%2520Encoding%2520%2528BPE%2529%2520tokens%2520to%2520preserve%2520morphological%250Aintegrity%2520while%2520maintaining%2520lexical%2520meaning.%2520To%2520tackle%2520resource%2520scarcity%252C%2520we%250Acurate%2520and%2520release%2520manually%2520annotated%2520morpheme%2520data%2520for%2520four%2520Geez%2520script%250Alanguages%2520and%2520a%2520morpheme-aware%2520vocabulary%2520for%2520two%2520of%2520them.%2520While%2520the%2520proposed%250Atokenization%2520method%2520does%2520not%2520lead%2520to%2520significant%2520gains%2520in%2520automatic%2520translation%250Aquality%252C%2520we%2520observe%2520consistent%2520improvements%2520in%2520intrinsic%2520metrics%252C%2520MorphoScore%252C%250Aand%2520Boundary%2520Precision%252C%2520highlighting%2520the%2520value%2520of%2520morphology-aware%2520segmentation%250Ain%2520enhancing%2520linguistic%2520fidelity%2520and%2520token%2520efficiency.%2520Our%2520morpheme-annotated%250Adatasets%2520and%2520tokenizer%2520will%2520be%2520publicly%2520available%2520to%2520support%2520further%2520research%250Ain%2520low-resource%252C%2520morphologically%2520rich%2520languages.%2520Our%2520code%2520and%2520data%2520are%250Aavailable%2520on%2520GitHub%253A%2520https%253A//github.com/hailaykidu/MoVoC%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoVoC%3A%20Morphology-Aware%20Subword%20Construction%20for%20Geez%20Script%20Languages&entry.906535625=Hailay%20Kidu%20Teklehaymanot%20and%20Dren%20Fazlija%20and%20Wolfgang%20Nejdl&entry.1292438233=%20%20Subword-based%20tokenization%20methods%20often%20fail%20to%20preserve%20morphological%0Aboundaries%2C%20a%20limitation%20especially%20pronounced%20in%20low-resource%2C%20morphologically%0Acomplex%20languages%20such%20as%20those%20written%20in%20the%20Geez%20script.%20To%20address%20this%2C%20we%0Apresent%20MoVoC%20%28Morpheme-aware%20Subword%20Vocabulary%20Construction%29%20and%20train%0AMoVoC-Tok%2C%20a%20tokenizer%20that%20integrates%20supervised%20morphological%20analysis%20into%0Athe%20subword%20vocabulary.%20This%20hybrid%20segmentation%20approach%20combines%0Amorpheme-based%20and%20Byte%20Pair%20Encoding%20%28BPE%29%20tokens%20to%20preserve%20morphological%0Aintegrity%20while%20maintaining%20lexical%20meaning.%20To%20tackle%20resource%20scarcity%2C%20we%0Acurate%20and%20release%20manually%20annotated%20morpheme%20data%20for%20four%20Geez%20script%0Alanguages%20and%20a%20morpheme-aware%20vocabulary%20for%20two%20of%20them.%20While%20the%20proposed%0Atokenization%20method%20does%20not%20lead%20to%20significant%20gains%20in%20automatic%20translation%0Aquality%2C%20we%20observe%20consistent%20improvements%20in%20intrinsic%20metrics%2C%20MorphoScore%2C%0Aand%20Boundary%20Precision%2C%20highlighting%20the%20value%20of%20morphology-aware%20segmentation%0Ain%20enhancing%20linguistic%20fidelity%20and%20token%20efficiency.%20Our%20morpheme-annotated%0Adatasets%20and%20tokenizer%20will%20be%20publicly%20available%20to%20support%20further%20research%0Ain%20low-resource%2C%20morphologically%20rich%20languages.%20Our%20code%20and%20data%20are%0Aavailable%20on%20GitHub%3A%20https%3A//github.com/hailaykidu/MoVoC%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08812v1&entry.124074799=Read"},
{"title": "Decentralized Stochastic Nonconvex Optimization under the Relaxed\n  Smoothness", "author": "Luo Luo and Xue Cui and Tingkai Jia and Cheng Chen", "abstract": "  This paper studies decentralized optimization problem\n$f(\\mathbf{x})=\\frac{1}{m}\\sum_{i=1}^m f_i(\\mathbf{x})$, where each local\nfunction has the form of $f_i(\\mathbf{x}) = {\\mathbb\nE}\\left[F(\\mathbf{x};{\\xi}_i)\\right]$ which is $(L_0,L_1)$-smooth but possibly\nnonconvex and the random variable ${\\xi}_i$ follows distribution ${\\mathcal\nD}_i$. We propose a novel algorithm called decentralized normalized stochastic\ngradient descent (DNSGD), which can achieve the $\\epsilon$-stationary point on\neach local agent. We present a new framework for analyzing decentralized\nfirst-order methods in the relaxed smooth setting, based on the Lyapunov\nfunction related to the product of the gradient norm and the consensus error.\nThe analysis shows upper bounds on sample complexity of ${\\mathcal\nO}(m^{-1}(L_f\\sigma^2\\Delta_f\\epsilon^{-4} + \\sigma^2\\epsilon^{-2} +\nL_f^{-2}L_1^3\\sigma^2\\Delta_f\\epsilon^{-1} + L_f^{-2}L_1^2\\sigma^2))$ per agent\nand communication complexity of $\\tilde{\\mathcal O}((L_f\\epsilon^{-2} +\nL_1\\epsilon^{-1})\\gamma^{-1/2}\\Delta_f)$, where $L_f=L_0 +L_1\\zeta$, $\\sigma^2$\nis the variance of the stochastic gradient, $\\Delta_f$ is the initial optimal\nfunction value gap, $\\gamma$ is the spectral gap of the network, and $\\zeta$ is\nthe degree of the gradient dissimilarity. In the special case of $L_1=0$, the\nabove results (nearly) match the lower bounds on decentralized nonconvex\noptimization in the standard smooth setting. We also conduct numerical\nexperiments to show the empirical superiority of our method.\n", "link": "http://arxiv.org/abs/2509.08726v1", "date": "2025-09-10", "relevancy": 2.2224, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4472}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.446}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Stochastic%20Nonconvex%20Optimization%20under%20the%20Relaxed%0A%20%20Smoothness&body=Title%3A%20Decentralized%20Stochastic%20Nonconvex%20Optimization%20under%20the%20Relaxed%0A%20%20Smoothness%0AAuthor%3A%20Luo%20Luo%20and%20Xue%20Cui%20and%20Tingkai%20Jia%20and%20Cheng%20Chen%0AAbstract%3A%20%20%20This%20paper%20studies%20decentralized%20optimization%20problem%0A%24f%28%5Cmathbf%7Bx%7D%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%20f_i%28%5Cmathbf%7Bx%7D%29%24%2C%20where%20each%20local%0Afunction%20has%20the%20form%20of%20%24f_i%28%5Cmathbf%7Bx%7D%29%20%3D%20%7B%5Cmathbb%0AE%7D%5Cleft%5BF%28%5Cmathbf%7Bx%7D%3B%7B%5Cxi%7D_i%29%5Cright%5D%24%20which%20is%20%24%28L_0%2CL_1%29%24-smooth%20but%20possibly%0Anonconvex%20and%20the%20random%20variable%20%24%7B%5Cxi%7D_i%24%20follows%20distribution%20%24%7B%5Cmathcal%0AD%7D_i%24.%20We%20propose%20a%20novel%20algorithm%20called%20decentralized%20normalized%20stochastic%0Agradient%20descent%20%28DNSGD%29%2C%20which%20can%20achieve%20the%20%24%5Cepsilon%24-stationary%20point%20on%0Aeach%20local%20agent.%20We%20present%20a%20new%20framework%20for%20analyzing%20decentralized%0Afirst-order%20methods%20in%20the%20relaxed%20smooth%20setting%2C%20based%20on%20the%20Lyapunov%0Afunction%20related%20to%20the%20product%20of%20the%20gradient%20norm%20and%20the%20consensus%20error.%0AThe%20analysis%20shows%20upper%20bounds%20on%20sample%20complexity%20of%20%24%7B%5Cmathcal%0AO%7D%28m%5E%7B-1%7D%28L_f%5Csigma%5E2%5CDelta_f%5Cepsilon%5E%7B-4%7D%20%2B%20%5Csigma%5E2%5Cepsilon%5E%7B-2%7D%20%2B%0AL_f%5E%7B-2%7DL_1%5E3%5Csigma%5E2%5CDelta_f%5Cepsilon%5E%7B-1%7D%20%2B%20L_f%5E%7B-2%7DL_1%5E2%5Csigma%5E2%29%29%24%20per%20agent%0Aand%20communication%20complexity%20of%20%24%5Ctilde%7B%5Cmathcal%20O%7D%28%28L_f%5Cepsilon%5E%7B-2%7D%20%2B%0AL_1%5Cepsilon%5E%7B-1%7D%29%5Cgamma%5E%7B-1/2%7D%5CDelta_f%29%24%2C%20where%20%24L_f%3DL_0%20%2BL_1%5Czeta%24%2C%20%24%5Csigma%5E2%24%0Ais%20the%20variance%20of%20the%20stochastic%20gradient%2C%20%24%5CDelta_f%24%20is%20the%20initial%20optimal%0Afunction%20value%20gap%2C%20%24%5Cgamma%24%20is%20the%20spectral%20gap%20of%20the%20network%2C%20and%20%24%5Czeta%24%20is%0Athe%20degree%20of%20the%20gradient%20dissimilarity.%20In%20the%20special%20case%20of%20%24L_1%3D0%24%2C%20the%0Aabove%20results%20%28nearly%29%20match%20the%20lower%20bounds%20on%20decentralized%20nonconvex%0Aoptimization%20in%20the%20standard%20smooth%20setting.%20We%20also%20conduct%20numerical%0Aexperiments%20to%20show%20the%20empirical%20superiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Stochastic%2520Nonconvex%2520Optimization%2520under%2520the%2520Relaxed%250A%2520%2520Smoothness%26entry.906535625%3DLuo%2520Luo%2520and%2520Xue%2520Cui%2520and%2520Tingkai%2520Jia%2520and%2520Cheng%2520Chen%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520decentralized%2520optimization%2520problem%250A%2524f%2528%255Cmathbf%257Bx%257D%2529%253D%255Cfrac%257B1%257D%257Bm%257D%255Csum_%257Bi%253D1%257D%255Em%2520f_i%2528%255Cmathbf%257Bx%257D%2529%2524%252C%2520where%2520each%2520local%250Afunction%2520has%2520the%2520form%2520of%2520%2524f_i%2528%255Cmathbf%257Bx%257D%2529%2520%253D%2520%257B%255Cmathbb%250AE%257D%255Cleft%255BF%2528%255Cmathbf%257Bx%257D%253B%257B%255Cxi%257D_i%2529%255Cright%255D%2524%2520which%2520is%2520%2524%2528L_0%252CL_1%2529%2524-smooth%2520but%2520possibly%250Anonconvex%2520and%2520the%2520random%2520variable%2520%2524%257B%255Cxi%257D_i%2524%2520follows%2520distribution%2520%2524%257B%255Cmathcal%250AD%257D_i%2524.%2520We%2520propose%2520a%2520novel%2520algorithm%2520called%2520decentralized%2520normalized%2520stochastic%250Agradient%2520descent%2520%2528DNSGD%2529%252C%2520which%2520can%2520achieve%2520the%2520%2524%255Cepsilon%2524-stationary%2520point%2520on%250Aeach%2520local%2520agent.%2520We%2520present%2520a%2520new%2520framework%2520for%2520analyzing%2520decentralized%250Afirst-order%2520methods%2520in%2520the%2520relaxed%2520smooth%2520setting%252C%2520based%2520on%2520the%2520Lyapunov%250Afunction%2520related%2520to%2520the%2520product%2520of%2520the%2520gradient%2520norm%2520and%2520the%2520consensus%2520error.%250AThe%2520analysis%2520shows%2520upper%2520bounds%2520on%2520sample%2520complexity%2520of%2520%2524%257B%255Cmathcal%250AO%257D%2528m%255E%257B-1%257D%2528L_f%255Csigma%255E2%255CDelta_f%255Cepsilon%255E%257B-4%257D%2520%252B%2520%255Csigma%255E2%255Cepsilon%255E%257B-2%257D%2520%252B%250AL_f%255E%257B-2%257DL_1%255E3%255Csigma%255E2%255CDelta_f%255Cepsilon%255E%257B-1%257D%2520%252B%2520L_f%255E%257B-2%257DL_1%255E2%255Csigma%255E2%2529%2529%2524%2520per%2520agent%250Aand%2520communication%2520complexity%2520of%2520%2524%255Ctilde%257B%255Cmathcal%2520O%257D%2528%2528L_f%255Cepsilon%255E%257B-2%257D%2520%252B%250AL_1%255Cepsilon%255E%257B-1%257D%2529%255Cgamma%255E%257B-1/2%257D%255CDelta_f%2529%2524%252C%2520where%2520%2524L_f%253DL_0%2520%252BL_1%255Czeta%2524%252C%2520%2524%255Csigma%255E2%2524%250Ais%2520the%2520variance%2520of%2520the%2520stochastic%2520gradient%252C%2520%2524%255CDelta_f%2524%2520is%2520the%2520initial%2520optimal%250Afunction%2520value%2520gap%252C%2520%2524%255Cgamma%2524%2520is%2520the%2520spectral%2520gap%2520of%2520the%2520network%252C%2520and%2520%2524%255Czeta%2524%2520is%250Athe%2520degree%2520of%2520the%2520gradient%2520dissimilarity.%2520In%2520the%2520special%2520case%2520of%2520%2524L_1%253D0%2524%252C%2520the%250Aabove%2520results%2520%2528nearly%2529%2520match%2520the%2520lower%2520bounds%2520on%2520decentralized%2520nonconvex%250Aoptimization%2520in%2520the%2520standard%2520smooth%2520setting.%2520We%2520also%2520conduct%2520numerical%250Aexperiments%2520to%2520show%2520the%2520empirical%2520superiority%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Stochastic%20Nonconvex%20Optimization%20under%20the%20Relaxed%0A%20%20Smoothness&entry.906535625=Luo%20Luo%20and%20Xue%20Cui%20and%20Tingkai%20Jia%20and%20Cheng%20Chen&entry.1292438233=%20%20This%20paper%20studies%20decentralized%20optimization%20problem%0A%24f%28%5Cmathbf%7Bx%7D%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%20f_i%28%5Cmathbf%7Bx%7D%29%24%2C%20where%20each%20local%0Afunction%20has%20the%20form%20of%20%24f_i%28%5Cmathbf%7Bx%7D%29%20%3D%20%7B%5Cmathbb%0AE%7D%5Cleft%5BF%28%5Cmathbf%7Bx%7D%3B%7B%5Cxi%7D_i%29%5Cright%5D%24%20which%20is%20%24%28L_0%2CL_1%29%24-smooth%20but%20possibly%0Anonconvex%20and%20the%20random%20variable%20%24%7B%5Cxi%7D_i%24%20follows%20distribution%20%24%7B%5Cmathcal%0AD%7D_i%24.%20We%20propose%20a%20novel%20algorithm%20called%20decentralized%20normalized%20stochastic%0Agradient%20descent%20%28DNSGD%29%2C%20which%20can%20achieve%20the%20%24%5Cepsilon%24-stationary%20point%20on%0Aeach%20local%20agent.%20We%20present%20a%20new%20framework%20for%20analyzing%20decentralized%0Afirst-order%20methods%20in%20the%20relaxed%20smooth%20setting%2C%20based%20on%20the%20Lyapunov%0Afunction%20related%20to%20the%20product%20of%20the%20gradient%20norm%20and%20the%20consensus%20error.%0AThe%20analysis%20shows%20upper%20bounds%20on%20sample%20complexity%20of%20%24%7B%5Cmathcal%0AO%7D%28m%5E%7B-1%7D%28L_f%5Csigma%5E2%5CDelta_f%5Cepsilon%5E%7B-4%7D%20%2B%20%5Csigma%5E2%5Cepsilon%5E%7B-2%7D%20%2B%0AL_f%5E%7B-2%7DL_1%5E3%5Csigma%5E2%5CDelta_f%5Cepsilon%5E%7B-1%7D%20%2B%20L_f%5E%7B-2%7DL_1%5E2%5Csigma%5E2%29%29%24%20per%20agent%0Aand%20communication%20complexity%20of%20%24%5Ctilde%7B%5Cmathcal%20O%7D%28%28L_f%5Cepsilon%5E%7B-2%7D%20%2B%0AL_1%5Cepsilon%5E%7B-1%7D%29%5Cgamma%5E%7B-1/2%7D%5CDelta_f%29%24%2C%20where%20%24L_f%3DL_0%20%2BL_1%5Czeta%24%2C%20%24%5Csigma%5E2%24%0Ais%20the%20variance%20of%20the%20stochastic%20gradient%2C%20%24%5CDelta_f%24%20is%20the%20initial%20optimal%0Afunction%20value%20gap%2C%20%24%5Cgamma%24%20is%20the%20spectral%20gap%20of%20the%20network%2C%20and%20%24%5Czeta%24%20is%0Athe%20degree%20of%20the%20gradient%20dissimilarity.%20In%20the%20special%20case%20of%20%24L_1%3D0%24%2C%20the%0Aabove%20results%20%28nearly%29%20match%20the%20lower%20bounds%20on%20decentralized%20nonconvex%0Aoptimization%20in%20the%20standard%20smooth%20setting.%20We%20also%20conduct%20numerical%0Aexperiments%20to%20show%20the%20empirical%20superiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08726v1&entry.124074799=Read"},
{"title": "MasconCube: Fast and Accurate Gravity Modeling with an Explicit\n  Representation", "author": "Pietro Fanti and Dario Izzo", "abstract": "  The geodesy of irregularly shaped small bodies presents fundamental\nchallenges for gravitational field modeling, particularly as deep space\nexploration missions increasingly target asteroids and comets. Traditional\napproaches suffer from critical limitations: spherical harmonics diverge within\nthe Brillouin sphere where spacecraft typically operate, polyhedral models\nassume unrealistic homogeneous density distributions, and existing machine\nlearning methods like GeodesyNets and Physics-Informed Neural Networks\n(PINN-GM) require extensive computational resources and training time. This\nwork introduces MasconCubes, a novel self-supervised learning approach that\nformulates gravity inversion as a direct optimization problem over a regular 3D\ngrid of point masses (mascons). Unlike implicit neural representations,\nMasconCubes explicitly model mass distributions while leveraging known asteroid\nshape information to constrain the solution space. Comprehensive evaluation on\ndiverse asteroid models including Bennu, Eros, Itokawa, and synthetic\nplanetesimals demonstrates that MasconCubes achieve superior performance across\nmultiple metrics. Most notably, MasconCubes demonstrate computational\nefficiency advantages with training times approximately 40 times faster than\nGeodesyNets while maintaining physical interpretability through explicit mass\ndistributions. These results establish MasconCubes as a promising approach for\nmission-critical gravitational modeling applications requiring high accuracy,\ncomputational efficiency, and physical insight into internal mass distributions\nof irregular celestial bodies.\n", "link": "http://arxiv.org/abs/2509.08607v1", "date": "2025-09-10", "relevancy": 2.2189, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5662}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5581}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MasconCube%3A%20Fast%20and%20Accurate%20Gravity%20Modeling%20with%20an%20Explicit%0A%20%20Representation&body=Title%3A%20MasconCube%3A%20Fast%20and%20Accurate%20Gravity%20Modeling%20with%20an%20Explicit%0A%20%20Representation%0AAuthor%3A%20Pietro%20Fanti%20and%20Dario%20Izzo%0AAbstract%3A%20%20%20The%20geodesy%20of%20irregularly%20shaped%20small%20bodies%20presents%20fundamental%0Achallenges%20for%20gravitational%20field%20modeling%2C%20particularly%20as%20deep%20space%0Aexploration%20missions%20increasingly%20target%20asteroids%20and%20comets.%20Traditional%0Aapproaches%20suffer%20from%20critical%20limitations%3A%20spherical%20harmonics%20diverge%20within%0Athe%20Brillouin%20sphere%20where%20spacecraft%20typically%20operate%2C%20polyhedral%20models%0Aassume%20unrealistic%20homogeneous%20density%20distributions%2C%20and%20existing%20machine%0Alearning%20methods%20like%20GeodesyNets%20and%20Physics-Informed%20Neural%20Networks%0A%28PINN-GM%29%20require%20extensive%20computational%20resources%20and%20training%20time.%20This%0Awork%20introduces%20MasconCubes%2C%20a%20novel%20self-supervised%20learning%20approach%20that%0Aformulates%20gravity%20inversion%20as%20a%20direct%20optimization%20problem%20over%20a%20regular%203D%0Agrid%20of%20point%20masses%20%28mascons%29.%20Unlike%20implicit%20neural%20representations%2C%0AMasconCubes%20explicitly%20model%20mass%20distributions%20while%20leveraging%20known%20asteroid%0Ashape%20information%20to%20constrain%20the%20solution%20space.%20Comprehensive%20evaluation%20on%0Adiverse%20asteroid%20models%20including%20Bennu%2C%20Eros%2C%20Itokawa%2C%20and%20synthetic%0Aplanetesimals%20demonstrates%20that%20MasconCubes%20achieve%20superior%20performance%20across%0Amultiple%20metrics.%20Most%20notably%2C%20MasconCubes%20demonstrate%20computational%0Aefficiency%20advantages%20with%20training%20times%20approximately%2040%20times%20faster%20than%0AGeodesyNets%20while%20maintaining%20physical%20interpretability%20through%20explicit%20mass%0Adistributions.%20These%20results%20establish%20MasconCubes%20as%20a%20promising%20approach%20for%0Amission-critical%20gravitational%20modeling%20applications%20requiring%20high%20accuracy%2C%0Acomputational%20efficiency%2C%20and%20physical%20insight%20into%20internal%20mass%20distributions%0Aof%20irregular%20celestial%20bodies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasconCube%253A%2520Fast%2520and%2520Accurate%2520Gravity%2520Modeling%2520with%2520an%2520Explicit%250A%2520%2520Representation%26entry.906535625%3DPietro%2520Fanti%2520and%2520Dario%2520Izzo%26entry.1292438233%3D%2520%2520The%2520geodesy%2520of%2520irregularly%2520shaped%2520small%2520bodies%2520presents%2520fundamental%250Achallenges%2520for%2520gravitational%2520field%2520modeling%252C%2520particularly%2520as%2520deep%2520space%250Aexploration%2520missions%2520increasingly%2520target%2520asteroids%2520and%2520comets.%2520Traditional%250Aapproaches%2520suffer%2520from%2520critical%2520limitations%253A%2520spherical%2520harmonics%2520diverge%2520within%250Athe%2520Brillouin%2520sphere%2520where%2520spacecraft%2520typically%2520operate%252C%2520polyhedral%2520models%250Aassume%2520unrealistic%2520homogeneous%2520density%2520distributions%252C%2520and%2520existing%2520machine%250Alearning%2520methods%2520like%2520GeodesyNets%2520and%2520Physics-Informed%2520Neural%2520Networks%250A%2528PINN-GM%2529%2520require%2520extensive%2520computational%2520resources%2520and%2520training%2520time.%2520This%250Awork%2520introduces%2520MasconCubes%252C%2520a%2520novel%2520self-supervised%2520learning%2520approach%2520that%250Aformulates%2520gravity%2520inversion%2520as%2520a%2520direct%2520optimization%2520problem%2520over%2520a%2520regular%25203D%250Agrid%2520of%2520point%2520masses%2520%2528mascons%2529.%2520Unlike%2520implicit%2520neural%2520representations%252C%250AMasconCubes%2520explicitly%2520model%2520mass%2520distributions%2520while%2520leveraging%2520known%2520asteroid%250Ashape%2520information%2520to%2520constrain%2520the%2520solution%2520space.%2520Comprehensive%2520evaluation%2520on%250Adiverse%2520asteroid%2520models%2520including%2520Bennu%252C%2520Eros%252C%2520Itokawa%252C%2520and%2520synthetic%250Aplanetesimals%2520demonstrates%2520that%2520MasconCubes%2520achieve%2520superior%2520performance%2520across%250Amultiple%2520metrics.%2520Most%2520notably%252C%2520MasconCubes%2520demonstrate%2520computational%250Aefficiency%2520advantages%2520with%2520training%2520times%2520approximately%252040%2520times%2520faster%2520than%250AGeodesyNets%2520while%2520maintaining%2520physical%2520interpretability%2520through%2520explicit%2520mass%250Adistributions.%2520These%2520results%2520establish%2520MasconCubes%2520as%2520a%2520promising%2520approach%2520for%250Amission-critical%2520gravitational%2520modeling%2520applications%2520requiring%2520high%2520accuracy%252C%250Acomputational%2520efficiency%252C%2520and%2520physical%2520insight%2520into%2520internal%2520mass%2520distributions%250Aof%2520irregular%2520celestial%2520bodies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MasconCube%3A%20Fast%20and%20Accurate%20Gravity%20Modeling%20with%20an%20Explicit%0A%20%20Representation&entry.906535625=Pietro%20Fanti%20and%20Dario%20Izzo&entry.1292438233=%20%20The%20geodesy%20of%20irregularly%20shaped%20small%20bodies%20presents%20fundamental%0Achallenges%20for%20gravitational%20field%20modeling%2C%20particularly%20as%20deep%20space%0Aexploration%20missions%20increasingly%20target%20asteroids%20and%20comets.%20Traditional%0Aapproaches%20suffer%20from%20critical%20limitations%3A%20spherical%20harmonics%20diverge%20within%0Athe%20Brillouin%20sphere%20where%20spacecraft%20typically%20operate%2C%20polyhedral%20models%0Aassume%20unrealistic%20homogeneous%20density%20distributions%2C%20and%20existing%20machine%0Alearning%20methods%20like%20GeodesyNets%20and%20Physics-Informed%20Neural%20Networks%0A%28PINN-GM%29%20require%20extensive%20computational%20resources%20and%20training%20time.%20This%0Awork%20introduces%20MasconCubes%2C%20a%20novel%20self-supervised%20learning%20approach%20that%0Aformulates%20gravity%20inversion%20as%20a%20direct%20optimization%20problem%20over%20a%20regular%203D%0Agrid%20of%20point%20masses%20%28mascons%29.%20Unlike%20implicit%20neural%20representations%2C%0AMasconCubes%20explicitly%20model%20mass%20distributions%20while%20leveraging%20known%20asteroid%0Ashape%20information%20to%20constrain%20the%20solution%20space.%20Comprehensive%20evaluation%20on%0Adiverse%20asteroid%20models%20including%20Bennu%2C%20Eros%2C%20Itokawa%2C%20and%20synthetic%0Aplanetesimals%20demonstrates%20that%20MasconCubes%20achieve%20superior%20performance%20across%0Amultiple%20metrics.%20Most%20notably%2C%20MasconCubes%20demonstrate%20computational%0Aefficiency%20advantages%20with%20training%20times%20approximately%2040%20times%20faster%20than%0AGeodesyNets%20while%20maintaining%20physical%20interpretability%20through%20explicit%20mass%0Adistributions.%20These%20results%20establish%20MasconCubes%20as%20a%20promising%20approach%20for%0Amission-critical%20gravitational%20modeling%20applications%20requiring%20high%20accuracy%2C%0Acomputational%20efficiency%2C%20and%20physical%20insight%20into%20internal%20mass%20distributions%0Aof%20irregular%20celestial%20bodies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08607v1&entry.124074799=Read"},
{"title": "Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation", "author": "Mathilde Monvoisin and Louise Piecuch and Blanche Texier and C\u00e9dric H\u00e9mon and Ana\u00efs Barateau and J\u00e9r\u00e9mie Huet and Antoine Nordez and Anne-Sophie Boureau and Jean-Claude Nunes and Diana Mateus", "abstract": "  The objective of this paper is to significantly reduce the manual workload\nrequired from medical professionals in complex 3D segmentation tasks that\ncannot be yet fully automated. For instance, in radiotherapy planning, organs\nat risk must be accurately identified in computed tomography (CT) or magnetic\nresonance imaging (MRI) scans to ensure they are spared from harmful radiation.\nSimilarly, diagnosing age-related degenerative diseases such as sarcopenia,\nwhich involve progressive muscle volume loss and strength, is commonly based on\nmuscular mass measurements often obtained from manual segmentation of medical\nvolumes. To alleviate the manual-segmentation burden, this paper introduces an\nimplicit shape prior to segment volumes from sparse slice manual annotations\ngeneralized to the multi-organ case, along with a simple framework for\nautomatically selecting the most informative slices to guide and minimize the\nnext interactions. The experimental validation shows the method's effectiveness\non two medical use cases: assisted segmentation in the context of at risks\norgans for brain cancer patients, and acceleration of the creation of a new\ndatabase with unseen muscle shapes for patients with sarcopenia.\n", "link": "http://arxiv.org/abs/2509.08580v1", "date": "2025-09-10", "relevancy": 2.2005, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5535}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5487}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Shape-Prior%20for%20Few-Shot%20Assisted%203D%20Segmentation&body=Title%3A%20Implicit%20Shape-Prior%20for%20Few-Shot%20Assisted%203D%20Segmentation%0AAuthor%3A%20Mathilde%20Monvoisin%20and%20Louise%20Piecuch%20and%20Blanche%20Texier%20and%20C%C3%A9dric%20H%C3%A9mon%20and%20Ana%C3%AFs%20Barateau%20and%20J%C3%A9r%C3%A9mie%20Huet%20and%20Antoine%20Nordez%20and%20Anne-Sophie%20Boureau%20and%20Jean-Claude%20Nunes%20and%20Diana%20Mateus%0AAbstract%3A%20%20%20The%20objective%20of%20this%20paper%20is%20to%20significantly%20reduce%20the%20manual%20workload%0Arequired%20from%20medical%20professionals%20in%20complex%203D%20segmentation%20tasks%20that%0Acannot%20be%20yet%20fully%20automated.%20For%20instance%2C%20in%20radiotherapy%20planning%2C%20organs%0Aat%20risk%20must%20be%20accurately%20identified%20in%20computed%20tomography%20%28CT%29%20or%20magnetic%0Aresonance%20imaging%20%28MRI%29%20scans%20to%20ensure%20they%20are%20spared%20from%20harmful%20radiation.%0ASimilarly%2C%20diagnosing%20age-related%20degenerative%20diseases%20such%20as%20sarcopenia%2C%0Awhich%20involve%20progressive%20muscle%20volume%20loss%20and%20strength%2C%20is%20commonly%20based%20on%0Amuscular%20mass%20measurements%20often%20obtained%20from%20manual%20segmentation%20of%20medical%0Avolumes.%20To%20alleviate%20the%20manual-segmentation%20burden%2C%20this%20paper%20introduces%20an%0Aimplicit%20shape%20prior%20to%20segment%20volumes%20from%20sparse%20slice%20manual%20annotations%0Ageneralized%20to%20the%20multi-organ%20case%2C%20along%20with%20a%20simple%20framework%20for%0Aautomatically%20selecting%20the%20most%20informative%20slices%20to%20guide%20and%20minimize%20the%0Anext%20interactions.%20The%20experimental%20validation%20shows%20the%20method%27s%20effectiveness%0Aon%20two%20medical%20use%20cases%3A%20assisted%20segmentation%20in%20the%20context%20of%20at%20risks%0Aorgans%20for%20brain%20cancer%20patients%2C%20and%20acceleration%20of%20the%20creation%20of%20a%20new%0Adatabase%20with%20unseen%20muscle%20shapes%20for%20patients%20with%20sarcopenia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Shape-Prior%2520for%2520Few-Shot%2520Assisted%25203D%2520Segmentation%26entry.906535625%3DMathilde%2520Monvoisin%2520and%2520Louise%2520Piecuch%2520and%2520Blanche%2520Texier%2520and%2520C%25C3%25A9dric%2520H%25C3%25A9mon%2520and%2520Ana%25C3%25AFs%2520Barateau%2520and%2520J%25C3%25A9r%25C3%25A9mie%2520Huet%2520and%2520Antoine%2520Nordez%2520and%2520Anne-Sophie%2520Boureau%2520and%2520Jean-Claude%2520Nunes%2520and%2520Diana%2520Mateus%26entry.1292438233%3D%2520%2520The%2520objective%2520of%2520this%2520paper%2520is%2520to%2520significantly%2520reduce%2520the%2520manual%2520workload%250Arequired%2520from%2520medical%2520professionals%2520in%2520complex%25203D%2520segmentation%2520tasks%2520that%250Acannot%2520be%2520yet%2520fully%2520automated.%2520For%2520instance%252C%2520in%2520radiotherapy%2520planning%252C%2520organs%250Aat%2520risk%2520must%2520be%2520accurately%2520identified%2520in%2520computed%2520tomography%2520%2528CT%2529%2520or%2520magnetic%250Aresonance%2520imaging%2520%2528MRI%2529%2520scans%2520to%2520ensure%2520they%2520are%2520spared%2520from%2520harmful%2520radiation.%250ASimilarly%252C%2520diagnosing%2520age-related%2520degenerative%2520diseases%2520such%2520as%2520sarcopenia%252C%250Awhich%2520involve%2520progressive%2520muscle%2520volume%2520loss%2520and%2520strength%252C%2520is%2520commonly%2520based%2520on%250Amuscular%2520mass%2520measurements%2520often%2520obtained%2520from%2520manual%2520segmentation%2520of%2520medical%250Avolumes.%2520To%2520alleviate%2520the%2520manual-segmentation%2520burden%252C%2520this%2520paper%2520introduces%2520an%250Aimplicit%2520shape%2520prior%2520to%2520segment%2520volumes%2520from%2520sparse%2520slice%2520manual%2520annotations%250Ageneralized%2520to%2520the%2520multi-organ%2520case%252C%2520along%2520with%2520a%2520simple%2520framework%2520for%250Aautomatically%2520selecting%2520the%2520most%2520informative%2520slices%2520to%2520guide%2520and%2520minimize%2520the%250Anext%2520interactions.%2520The%2520experimental%2520validation%2520shows%2520the%2520method%2527s%2520effectiveness%250Aon%2520two%2520medical%2520use%2520cases%253A%2520assisted%2520segmentation%2520in%2520the%2520context%2520of%2520at%2520risks%250Aorgans%2520for%2520brain%2520cancer%2520patients%252C%2520and%2520acceleration%2520of%2520the%2520creation%2520of%2520a%2520new%250Adatabase%2520with%2520unseen%2520muscle%2520shapes%2520for%2520patients%2520with%2520sarcopenia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Shape-Prior%20for%20Few-Shot%20Assisted%203D%20Segmentation&entry.906535625=Mathilde%20Monvoisin%20and%20Louise%20Piecuch%20and%20Blanche%20Texier%20and%20C%C3%A9dric%20H%C3%A9mon%20and%20Ana%C3%AFs%20Barateau%20and%20J%C3%A9r%C3%A9mie%20Huet%20and%20Antoine%20Nordez%20and%20Anne-Sophie%20Boureau%20and%20Jean-Claude%20Nunes%20and%20Diana%20Mateus&entry.1292438233=%20%20The%20objective%20of%20this%20paper%20is%20to%20significantly%20reduce%20the%20manual%20workload%0Arequired%20from%20medical%20professionals%20in%20complex%203D%20segmentation%20tasks%20that%0Acannot%20be%20yet%20fully%20automated.%20For%20instance%2C%20in%20radiotherapy%20planning%2C%20organs%0Aat%20risk%20must%20be%20accurately%20identified%20in%20computed%20tomography%20%28CT%29%20or%20magnetic%0Aresonance%20imaging%20%28MRI%29%20scans%20to%20ensure%20they%20are%20spared%20from%20harmful%20radiation.%0ASimilarly%2C%20diagnosing%20age-related%20degenerative%20diseases%20such%20as%20sarcopenia%2C%0Awhich%20involve%20progressive%20muscle%20volume%20loss%20and%20strength%2C%20is%20commonly%20based%20on%0Amuscular%20mass%20measurements%20often%20obtained%20from%20manual%20segmentation%20of%20medical%0Avolumes.%20To%20alleviate%20the%20manual-segmentation%20burden%2C%20this%20paper%20introduces%20an%0Aimplicit%20shape%20prior%20to%20segment%20volumes%20from%20sparse%20slice%20manual%20annotations%0Ageneralized%20to%20the%20multi-organ%20case%2C%20along%20with%20a%20simple%20framework%20for%0Aautomatically%20selecting%20the%20most%20informative%20slices%20to%20guide%20and%20minimize%20the%0Anext%20interactions.%20The%20experimental%20validation%20shows%20the%20method%27s%20effectiveness%0Aon%20two%20medical%20use%20cases%3A%20assisted%20segmentation%20in%20the%20context%20of%20at%20risks%0Aorgans%20for%20brain%20cancer%20patients%2C%20and%20acceleration%20of%20the%20creation%20of%20a%20new%0Adatabase%20with%20unseen%20muscle%20shapes%20for%20patients%20with%20sarcopenia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08580v1&entry.124074799=Read"},
{"title": "X-Part: high fidelity and structure coherent shape decomposition", "author": "Xinhao Yan and Jiachen Xu and Yang Li and Changfeng Ma and Yunhan Yang and Chunshi Wang and Zibo Zhao and Zeqiang Lai and Yunfei Zhao and Zhuo Chen and Chunchao Guo", "abstract": "  Generating 3D shapes at part level is pivotal for downstream applications\nsuch as mesh retopology, UV mapping, and 3D printing. However, existing\npart-based generation methods often lack sufficient controllability and suffer\nfrom poor semantically meaningful decomposition. To this end, we introduce\nX-Part, a controllable generative model designed to decompose a holistic 3D\nobject into semantically meaningful and structurally coherent parts with high\ngeometric fidelity. X-Part exploits the bounding box as prompts for the part\ngeneration and injects point-wise semantic features for meaningful\ndecomposition. Furthermore, we design an editable pipeline for interactive part\ngeneration. Extensive experimental results show that X-Part achieves\nstate-of-the-art performance in part-level shape generation. This work\nestablishes a new paradigm for creating production-ready, editable, and\nstructurally sound 3D assets. Codes will be released for public research.\n", "link": "http://arxiv.org/abs/2509.08643v1", "date": "2025-09-10", "relevancy": 2.1562, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5452}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5423}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-Part%3A%20high%20fidelity%20and%20structure%20coherent%20shape%20decomposition&body=Title%3A%20X-Part%3A%20high%20fidelity%20and%20structure%20coherent%20shape%20decomposition%0AAuthor%3A%20Xinhao%20Yan%20and%20Jiachen%20Xu%20and%20Yang%20Li%20and%20Changfeng%20Ma%20and%20Yunhan%20Yang%20and%20Chunshi%20Wang%20and%20Zibo%20Zhao%20and%20Zeqiang%20Lai%20and%20Yunfei%20Zhao%20and%20Zhuo%20Chen%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20Generating%203D%20shapes%20at%20part%20level%20is%20pivotal%20for%20downstream%20applications%0Asuch%20as%20mesh%20retopology%2C%20UV%20mapping%2C%20and%203D%20printing.%20However%2C%20existing%0Apart-based%20generation%20methods%20often%20lack%20sufficient%20controllability%20and%20suffer%0Afrom%20poor%20semantically%20meaningful%20decomposition.%20To%20this%20end%2C%20we%20introduce%0AX-Part%2C%20a%20controllable%20generative%20model%20designed%20to%20decompose%20a%20holistic%203D%0Aobject%20into%20semantically%20meaningful%20and%20structurally%20coherent%20parts%20with%20high%0Ageometric%20fidelity.%20X-Part%20exploits%20the%20bounding%20box%20as%20prompts%20for%20the%20part%0Ageneration%20and%20injects%20point-wise%20semantic%20features%20for%20meaningful%0Adecomposition.%20Furthermore%2C%20we%20design%20an%20editable%20pipeline%20for%20interactive%20part%0Ageneration.%20Extensive%20experimental%20results%20show%20that%20X-Part%20achieves%0Astate-of-the-art%20performance%20in%20part-level%20shape%20generation.%20This%20work%0Aestablishes%20a%20new%20paradigm%20for%20creating%20production-ready%2C%20editable%2C%20and%0Astructurally%20sound%203D%20assets.%20Codes%20will%20be%20released%20for%20public%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-Part%253A%2520high%2520fidelity%2520and%2520structure%2520coherent%2520shape%2520decomposition%26entry.906535625%3DXinhao%2520Yan%2520and%2520Jiachen%2520Xu%2520and%2520Yang%2520Li%2520and%2520Changfeng%2520Ma%2520and%2520Yunhan%2520Yang%2520and%2520Chunshi%2520Wang%2520and%2520Zibo%2520Zhao%2520and%2520Zeqiang%2520Lai%2520and%2520Yunfei%2520Zhao%2520and%2520Zhuo%2520Chen%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520Generating%25203D%2520shapes%2520at%2520part%2520level%2520is%2520pivotal%2520for%2520downstream%2520applications%250Asuch%2520as%2520mesh%2520retopology%252C%2520UV%2520mapping%252C%2520and%25203D%2520printing.%2520However%252C%2520existing%250Apart-based%2520generation%2520methods%2520often%2520lack%2520sufficient%2520controllability%2520and%2520suffer%250Afrom%2520poor%2520semantically%2520meaningful%2520decomposition.%2520To%2520this%2520end%252C%2520we%2520introduce%250AX-Part%252C%2520a%2520controllable%2520generative%2520model%2520designed%2520to%2520decompose%2520a%2520holistic%25203D%250Aobject%2520into%2520semantically%2520meaningful%2520and%2520structurally%2520coherent%2520parts%2520with%2520high%250Ageometric%2520fidelity.%2520X-Part%2520exploits%2520the%2520bounding%2520box%2520as%2520prompts%2520for%2520the%2520part%250Ageneration%2520and%2520injects%2520point-wise%2520semantic%2520features%2520for%2520meaningful%250Adecomposition.%2520Furthermore%252C%2520we%2520design%2520an%2520editable%2520pipeline%2520for%2520interactive%2520part%250Ageneration.%2520Extensive%2520experimental%2520results%2520show%2520that%2520X-Part%2520achieves%250Astate-of-the-art%2520performance%2520in%2520part-level%2520shape%2520generation.%2520This%2520work%250Aestablishes%2520a%2520new%2520paradigm%2520for%2520creating%2520production-ready%252C%2520editable%252C%2520and%250Astructurally%2520sound%25203D%2520assets.%2520Codes%2520will%2520be%2520released%2520for%2520public%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Part%3A%20high%20fidelity%20and%20structure%20coherent%20shape%20decomposition&entry.906535625=Xinhao%20Yan%20and%20Jiachen%20Xu%20and%20Yang%20Li%20and%20Changfeng%20Ma%20and%20Yunhan%20Yang%20and%20Chunshi%20Wang%20and%20Zibo%20Zhao%20and%20Zeqiang%20Lai%20and%20Yunfei%20Zhao%20and%20Zhuo%20Chen%20and%20Chunchao%20Guo&entry.1292438233=%20%20Generating%203D%20shapes%20at%20part%20level%20is%20pivotal%20for%20downstream%20applications%0Asuch%20as%20mesh%20retopology%2C%20UV%20mapping%2C%20and%203D%20printing.%20However%2C%20existing%0Apart-based%20generation%20methods%20often%20lack%20sufficient%20controllability%20and%20suffer%0Afrom%20poor%20semantically%20meaningful%20decomposition.%20To%20this%20end%2C%20we%20introduce%0AX-Part%2C%20a%20controllable%20generative%20model%20designed%20to%20decompose%20a%20holistic%203D%0Aobject%20into%20semantically%20meaningful%20and%20structurally%20coherent%20parts%20with%20high%0Ageometric%20fidelity.%20X-Part%20exploits%20the%20bounding%20box%20as%20prompts%20for%20the%20part%0Ageneration%20and%20injects%20point-wise%20semantic%20features%20for%20meaningful%0Adecomposition.%20Furthermore%2C%20we%20design%20an%20editable%20pipeline%20for%20interactive%20part%0Ageneration.%20Extensive%20experimental%20results%20show%20that%20X-Part%20achieves%0Astate-of-the-art%20performance%20in%20part-level%20shape%20generation.%20This%20work%0Aestablishes%20a%20new%20paradigm%20for%20creating%20production-ready%2C%20editable%2C%20and%0Astructurally%20sound%203D%20assets.%20Codes%20will%20be%20released%20for%20public%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08643v1&entry.124074799=Read"},
{"title": "CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection\n  in Crowded Scenes", "author": "Marius D\u00e4hling and Sebastian Krebs and J. Marius Z\u00f6llner", "abstract": "  This paper introduces a novel method for end-to-end crowd detection that\nleverages object density information to enhance existing transformer-based\ndetectors. We present CrowdQuery (CQ), whose core component is our CQ module\nthat predicts and subsequently embeds an object density map. The embedded\ndensity information is then systematically integrated into the decoder.\nExisting density map definitions typically depend on head positions or\nobject-based spatial statistics. Our method extends these definitions to\ninclude individual bounding box dimensions. By incorporating density\ninformation into object queries, our method utilizes density-guided queries to\nimprove detection in crowded scenes. CQ is universally applicable to both 2D\nand 3D detection without requiring additional data. Consequently, we are the\nfirst to design a method that effectively bridges 2D and 3D detection in\ncrowded environments. We demonstrate the integration of CQ into both a general\n2D and 3D transformer-based object detector, introducing the architectures CQ2D\nand CQ3D. CQ is not limited to the specific transformer models we selected.\nExperiments on the STCrowd dataset for both 2D and 3D domains show significant\nperformance improvements compared to the base models, outperforming most\nstate-of-the-art methods. When integrated into a state-of-the-art crowd\ndetector, CQ can further improve performance on the challenging CrowdHuman\ndataset, demonstrating its generalizability. The code is released at\nhttps://github.com/mdaehl/CrowdQuery.\n", "link": "http://arxiv.org/abs/2509.08738v1", "date": "2025-09-10", "relevancy": 2.1484, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5408}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5408}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrowdQuery%3A%20Density-Guided%20Query%20Module%20for%20Enhanced%202D%20and%203D%20Detection%0A%20%20in%20Crowded%20Scenes&body=Title%3A%20CrowdQuery%3A%20Density-Guided%20Query%20Module%20for%20Enhanced%202D%20and%203D%20Detection%0A%20%20in%20Crowded%20Scenes%0AAuthor%3A%20Marius%20D%C3%A4hling%20and%20Sebastian%20Krebs%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20method%20for%20end-to-end%20crowd%20detection%20that%0Aleverages%20object%20density%20information%20to%20enhance%20existing%20transformer-based%0Adetectors.%20We%20present%20CrowdQuery%20%28CQ%29%2C%20whose%20core%20component%20is%20our%20CQ%20module%0Athat%20predicts%20and%20subsequently%20embeds%20an%20object%20density%20map.%20The%20embedded%0Adensity%20information%20is%20then%20systematically%20integrated%20into%20the%20decoder.%0AExisting%20density%20map%20definitions%20typically%20depend%20on%20head%20positions%20or%0Aobject-based%20spatial%20statistics.%20Our%20method%20extends%20these%20definitions%20to%0Ainclude%20individual%20bounding%20box%20dimensions.%20By%20incorporating%20density%0Ainformation%20into%20object%20queries%2C%20our%20method%20utilizes%20density-guided%20queries%20to%0Aimprove%20detection%20in%20crowded%20scenes.%20CQ%20is%20universally%20applicable%20to%20both%202D%0Aand%203D%20detection%20without%20requiring%20additional%20data.%20Consequently%2C%20we%20are%20the%0Afirst%20to%20design%20a%20method%20that%20effectively%20bridges%202D%20and%203D%20detection%20in%0Acrowded%20environments.%20We%20demonstrate%20the%20integration%20of%20CQ%20into%20both%20a%20general%0A2D%20and%203D%20transformer-based%20object%20detector%2C%20introducing%20the%20architectures%20CQ2D%0Aand%20CQ3D.%20CQ%20is%20not%20limited%20to%20the%20specific%20transformer%20models%20we%20selected.%0AExperiments%20on%20the%20STCrowd%20dataset%20for%20both%202D%20and%203D%20domains%20show%20significant%0Aperformance%20improvements%20compared%20to%20the%20base%20models%2C%20outperforming%20most%0Astate-of-the-art%20methods.%20When%20integrated%20into%20a%20state-of-the-art%20crowd%0Adetector%2C%20CQ%20can%20further%20improve%20performance%20on%20the%20challenging%20CrowdHuman%0Adataset%2C%20demonstrating%20its%20generalizability.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/mdaehl/CrowdQuery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrowdQuery%253A%2520Density-Guided%2520Query%2520Module%2520for%2520Enhanced%25202D%2520and%25203D%2520Detection%250A%2520%2520in%2520Crowded%2520Scenes%26entry.906535625%3DMarius%2520D%25C3%25A4hling%2520and%2520Sebastian%2520Krebs%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520method%2520for%2520end-to-end%2520crowd%2520detection%2520that%250Aleverages%2520object%2520density%2520information%2520to%2520enhance%2520existing%2520transformer-based%250Adetectors.%2520We%2520present%2520CrowdQuery%2520%2528CQ%2529%252C%2520whose%2520core%2520component%2520is%2520our%2520CQ%2520module%250Athat%2520predicts%2520and%2520subsequently%2520embeds%2520an%2520object%2520density%2520map.%2520The%2520embedded%250Adensity%2520information%2520is%2520then%2520systematically%2520integrated%2520into%2520the%2520decoder.%250AExisting%2520density%2520map%2520definitions%2520typically%2520depend%2520on%2520head%2520positions%2520or%250Aobject-based%2520spatial%2520statistics.%2520Our%2520method%2520extends%2520these%2520definitions%2520to%250Ainclude%2520individual%2520bounding%2520box%2520dimensions.%2520By%2520incorporating%2520density%250Ainformation%2520into%2520object%2520queries%252C%2520our%2520method%2520utilizes%2520density-guided%2520queries%2520to%250Aimprove%2520detection%2520in%2520crowded%2520scenes.%2520CQ%2520is%2520universally%2520applicable%2520to%2520both%25202D%250Aand%25203D%2520detection%2520without%2520requiring%2520additional%2520data.%2520Consequently%252C%2520we%2520are%2520the%250Afirst%2520to%2520design%2520a%2520method%2520that%2520effectively%2520bridges%25202D%2520and%25203D%2520detection%2520in%250Acrowded%2520environments.%2520We%2520demonstrate%2520the%2520integration%2520of%2520CQ%2520into%2520both%2520a%2520general%250A2D%2520and%25203D%2520transformer-based%2520object%2520detector%252C%2520introducing%2520the%2520architectures%2520CQ2D%250Aand%2520CQ3D.%2520CQ%2520is%2520not%2520limited%2520to%2520the%2520specific%2520transformer%2520models%2520we%2520selected.%250AExperiments%2520on%2520the%2520STCrowd%2520dataset%2520for%2520both%25202D%2520and%25203D%2520domains%2520show%2520significant%250Aperformance%2520improvements%2520compared%2520to%2520the%2520base%2520models%252C%2520outperforming%2520most%250Astate-of-the-art%2520methods.%2520When%2520integrated%2520into%2520a%2520state-of-the-art%2520crowd%250Adetector%252C%2520CQ%2520can%2520further%2520improve%2520performance%2520on%2520the%2520challenging%2520CrowdHuman%250Adataset%252C%2520demonstrating%2520its%2520generalizability.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/mdaehl/CrowdQuery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrowdQuery%3A%20Density-Guided%20Query%20Module%20for%20Enhanced%202D%20and%203D%20Detection%0A%20%20in%20Crowded%20Scenes&entry.906535625=Marius%20D%C3%A4hling%20and%20Sebastian%20Krebs%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20method%20for%20end-to-end%20crowd%20detection%20that%0Aleverages%20object%20density%20information%20to%20enhance%20existing%20transformer-based%0Adetectors.%20We%20present%20CrowdQuery%20%28CQ%29%2C%20whose%20core%20component%20is%20our%20CQ%20module%0Athat%20predicts%20and%20subsequently%20embeds%20an%20object%20density%20map.%20The%20embedded%0Adensity%20information%20is%20then%20systematically%20integrated%20into%20the%20decoder.%0AExisting%20density%20map%20definitions%20typically%20depend%20on%20head%20positions%20or%0Aobject-based%20spatial%20statistics.%20Our%20method%20extends%20these%20definitions%20to%0Ainclude%20individual%20bounding%20box%20dimensions.%20By%20incorporating%20density%0Ainformation%20into%20object%20queries%2C%20our%20method%20utilizes%20density-guided%20queries%20to%0Aimprove%20detection%20in%20crowded%20scenes.%20CQ%20is%20universally%20applicable%20to%20both%202D%0Aand%203D%20detection%20without%20requiring%20additional%20data.%20Consequently%2C%20we%20are%20the%0Afirst%20to%20design%20a%20method%20that%20effectively%20bridges%202D%20and%203D%20detection%20in%0Acrowded%20environments.%20We%20demonstrate%20the%20integration%20of%20CQ%20into%20both%20a%20general%0A2D%20and%203D%20transformer-based%20object%20detector%2C%20introducing%20the%20architectures%20CQ2D%0Aand%20CQ3D.%20CQ%20is%20not%20limited%20to%20the%20specific%20transformer%20models%20we%20selected.%0AExperiments%20on%20the%20STCrowd%20dataset%20for%20both%202D%20and%203D%20domains%20show%20significant%0Aperformance%20improvements%20compared%20to%20the%20base%20models%2C%20outperforming%20most%0Astate-of-the-art%20methods.%20When%20integrated%20into%20a%20state-of-the-art%20crowd%0Adetector%2C%20CQ%20can%20further%20improve%20performance%20on%20the%20challenging%20CrowdHuman%0Adataset%2C%20demonstrating%20its%20generalizability.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/mdaehl/CrowdQuery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08738v1&entry.124074799=Read"},
{"title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT\n  Denoising", "author": "Yichao Liu and Hengzhi Xue and YueYang Teng", "abstract": "  Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications.\n", "link": "http://arxiv.org/abs/2509.06591v3", "date": "2025-09-10", "relevancy": 2.1483, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5602}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5265}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%0A%20%20Denoising&body=Title%3A%20Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%0A%20%20Denoising%0AAuthor%3A%20Yichao%20Liu%20and%20Hengzhi%20Xue%20and%20YueYang%20Teng%0AAbstract%3A%20%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20and%20positron%20emission%20tomography%20%28PET%29%0Ahave%20emerged%20as%20safer%20alternatives%20to%20conventional%20imaging%20modalities%20by%0Asignificantly%20reducing%20radiation%20exposure.%20However%2C%20this%20reduction%20often%0Aresults%20in%20increased%20noise%20and%20artifacts%2C%20which%20can%20compromise%20diagnostic%0Aaccuracy.%20Consequently%2C%20denoising%20for%20LDCT/PET%20has%20become%20a%20vital%20area%20of%0Aresearch%20aimed%20at%20enhancing%20image%20quality%20while%20maintaining%20radiation%20safety.%0AIn%20this%20study%2C%20we%20introduce%20a%20novel%20Hybrid%20Swin%20Attention%20Network%20%28HSANet%29%2C%0Awhich%20incorporates%20Efficient%20Global%20Attention%20%28EGA%29%20modules%20and%20a%20hybrid%0Aupsampling%20module.%20The%20EGA%20modules%20enhance%20both%20spatial%20and%20channel-wise%0Ainteraction%2C%20improving%20the%20network%27s%20capacity%20to%20capture%20relevant%20features%2C%0Awhile%20the%20hybrid%20upsampling%20module%20mitigates%20the%20risk%20of%20overfitting%20to%20noise.%0AWe%20validate%20the%20proposed%20approach%20using%20a%20publicly%20available%20LDCT/PET%20dataset.%0AExperimental%20results%20demonstrate%20that%20HSANet%20achieves%20superior%20denoising%0Aperformance%20compared%20to%20existing%20methods%2C%20while%20maintaining%20a%20lightweight%20model%0Asize%20suitable%20for%20deployment%20on%20GPUs%20with%20standard%20memory%20configurations.%20This%0Amakes%20our%20approach%20highly%20practical%20for%20real-world%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06591v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Swin%2520Attention%2520Networks%2520for%2520Simultaneously%2520Low-Dose%2520PET%2520and%2520CT%250A%2520%2520Denoising%26entry.906535625%3DYichao%2520Liu%2520and%2520Hengzhi%2520Xue%2520and%2520YueYang%2520Teng%26entry.1292438233%3D%2520%2520Low-dose%2520computed%2520tomography%2520%2528LDCT%2529%2520and%2520positron%2520emission%2520tomography%2520%2528PET%2529%250Ahave%2520emerged%2520as%2520safer%2520alternatives%2520to%2520conventional%2520imaging%2520modalities%2520by%250Asignificantly%2520reducing%2520radiation%2520exposure.%2520However%252C%2520this%2520reduction%2520often%250Aresults%2520in%2520increased%2520noise%2520and%2520artifacts%252C%2520which%2520can%2520compromise%2520diagnostic%250Aaccuracy.%2520Consequently%252C%2520denoising%2520for%2520LDCT/PET%2520has%2520become%2520a%2520vital%2520area%2520of%250Aresearch%2520aimed%2520at%2520enhancing%2520image%2520quality%2520while%2520maintaining%2520radiation%2520safety.%250AIn%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520Hybrid%2520Swin%2520Attention%2520Network%2520%2528HSANet%2529%252C%250Awhich%2520incorporates%2520Efficient%2520Global%2520Attention%2520%2528EGA%2529%2520modules%2520and%2520a%2520hybrid%250Aupsampling%2520module.%2520The%2520EGA%2520modules%2520enhance%2520both%2520spatial%2520and%2520channel-wise%250Ainteraction%252C%2520improving%2520the%2520network%2527s%2520capacity%2520to%2520capture%2520relevant%2520features%252C%250Awhile%2520the%2520hybrid%2520upsampling%2520module%2520mitigates%2520the%2520risk%2520of%2520overfitting%2520to%2520noise.%250AWe%2520validate%2520the%2520proposed%2520approach%2520using%2520a%2520publicly%2520available%2520LDCT/PET%2520dataset.%250AExperimental%2520results%2520demonstrate%2520that%2520HSANet%2520achieves%2520superior%2520denoising%250Aperformance%2520compared%2520to%2520existing%2520methods%252C%2520while%2520maintaining%2520a%2520lightweight%2520model%250Asize%2520suitable%2520for%2520deployment%2520on%2520GPUs%2520with%2520standard%2520memory%2520configurations.%2520This%250Amakes%2520our%2520approach%2520highly%2520practical%2520for%2520real-world%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06591v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%0A%20%20Denoising&entry.906535625=Yichao%20Liu%20and%20Hengzhi%20Xue%20and%20YueYang%20Teng&entry.1292438233=%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20and%20positron%20emission%20tomography%20%28PET%29%0Ahave%20emerged%20as%20safer%20alternatives%20to%20conventional%20imaging%20modalities%20by%0Asignificantly%20reducing%20radiation%20exposure.%20However%2C%20this%20reduction%20often%0Aresults%20in%20increased%20noise%20and%20artifacts%2C%20which%20can%20compromise%20diagnostic%0Aaccuracy.%20Consequently%2C%20denoising%20for%20LDCT/PET%20has%20become%20a%20vital%20area%20of%0Aresearch%20aimed%20at%20enhancing%20image%20quality%20while%20maintaining%20radiation%20safety.%0AIn%20this%20study%2C%20we%20introduce%20a%20novel%20Hybrid%20Swin%20Attention%20Network%20%28HSANet%29%2C%0Awhich%20incorporates%20Efficient%20Global%20Attention%20%28EGA%29%20modules%20and%20a%20hybrid%0Aupsampling%20module.%20The%20EGA%20modules%20enhance%20both%20spatial%20and%20channel-wise%0Ainteraction%2C%20improving%20the%20network%27s%20capacity%20to%20capture%20relevant%20features%2C%0Awhile%20the%20hybrid%20upsampling%20module%20mitigates%20the%20risk%20of%20overfitting%20to%20noise.%0AWe%20validate%20the%20proposed%20approach%20using%20a%20publicly%20available%20LDCT/PET%20dataset.%0AExperimental%20results%20demonstrate%20that%20HSANet%20achieves%20superior%20denoising%0Aperformance%20compared%20to%20existing%20methods%2C%20while%20maintaining%20a%20lightweight%20model%0Asize%20suitable%20for%20deployment%20on%20GPUs%20with%20standard%20memory%20configurations.%20This%0Amakes%20our%20approach%20highly%20practical%20for%20real-world%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06591v3&entry.124074799=Read"},
{"title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in\n  Diffusion Models", "author": "Yuming Li and Yikai Wang and Yuying Zhu and Zhongyu Zhao and Ming Lu and Qi She and Shanghang Zhang", "abstract": "  Recent advancements in aligning image and video generative models via GRPO\nhave achieved remarkable gains in enhancing human preference alignment.\nHowever, these methods still face high computational costs from on-policy\nrollouts and excessive SDE sampling steps, as well as training instability due\nto sparse rewards. In this paper, we propose BranchGRPO, a novel method that\nintroduces a branch sampling policy updating the SDE sampling process. By\nsharing computation across common prefixes and pruning low-reward paths and\nredundant depths, BranchGRPO substantially lowers the per-update compute cost\nwhile maintaining or improving exploration diversity. This work makes three\nmain contributions: (1) a branch sampling scheme that reduces rollout and\ntraining cost; (2) a tree-based advantage estimator incorporating dense\nprocess-level rewards; and (3) pruning strategies exploiting path and depth\nredundancy to accelerate convergence and boost performance. Experiments on\nimage and video preference alignment show that BranchGRPO improves alignment\nscores by 16% over strong baselines, while cutting training time by 50%.\n", "link": "http://arxiv.org/abs/2509.06040v3", "date": "2025-09-10", "relevancy": 2.1432, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5635}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5318}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BranchGRPO%3A%20Stable%20and%20Efficient%20GRPO%20with%20Structured%20Branching%20in%0A%20%20Diffusion%20Models&body=Title%3A%20BranchGRPO%3A%20Stable%20and%20Efficient%20GRPO%20with%20Structured%20Branching%20in%0A%20%20Diffusion%20Models%0AAuthor%3A%20Yuming%20Li%20and%20Yikai%20Wang%20and%20Yuying%20Zhu%20and%20Zhongyu%20Zhao%20and%20Ming%20Lu%20and%20Qi%20She%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20aligning%20image%20and%20video%20generative%20models%20via%20GRPO%0Ahave%20achieved%20remarkable%20gains%20in%20enhancing%20human%20preference%20alignment.%0AHowever%2C%20these%20methods%20still%20face%20high%20computational%20costs%20from%20on-policy%0Arollouts%20and%20excessive%20SDE%20sampling%20steps%2C%20as%20well%20as%20training%20instability%20due%0Ato%20sparse%20rewards.%20In%20this%20paper%2C%20we%20propose%20BranchGRPO%2C%20a%20novel%20method%20that%0Aintroduces%20a%20branch%20sampling%20policy%20updating%20the%20SDE%20sampling%20process.%20By%0Asharing%20computation%20across%20common%20prefixes%20and%20pruning%20low-reward%20paths%20and%0Aredundant%20depths%2C%20BranchGRPO%20substantially%20lowers%20the%20per-update%20compute%20cost%0Awhile%20maintaining%20or%20improving%20exploration%20diversity.%20This%20work%20makes%20three%0Amain%20contributions%3A%20%281%29%20a%20branch%20sampling%20scheme%20that%20reduces%20rollout%20and%0Atraining%20cost%3B%20%282%29%20a%20tree-based%20advantage%20estimator%20incorporating%20dense%0Aprocess-level%20rewards%3B%20and%20%283%29%20pruning%20strategies%20exploiting%20path%20and%20depth%0Aredundancy%20to%20accelerate%20convergence%20and%20boost%20performance.%20Experiments%20on%0Aimage%20and%20video%20preference%20alignment%20show%20that%20BranchGRPO%20improves%20alignment%0Ascores%20by%2016%25%20over%20strong%20baselines%2C%20while%20cutting%20training%20time%20by%2050%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06040v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBranchGRPO%253A%2520Stable%2520and%2520Efficient%2520GRPO%2520with%2520Structured%2520Branching%2520in%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DYuming%2520Li%2520and%2520Yikai%2520Wang%2520and%2520Yuying%2520Zhu%2520and%2520Zhongyu%2520Zhao%2520and%2520Ming%2520Lu%2520and%2520Qi%2520She%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520aligning%2520image%2520and%2520video%2520generative%2520models%2520via%2520GRPO%250Ahave%2520achieved%2520remarkable%2520gains%2520in%2520enhancing%2520human%2520preference%2520alignment.%250AHowever%252C%2520these%2520methods%2520still%2520face%2520high%2520computational%2520costs%2520from%2520on-policy%250Arollouts%2520and%2520excessive%2520SDE%2520sampling%2520steps%252C%2520as%2520well%2520as%2520training%2520instability%2520due%250Ato%2520sparse%2520rewards.%2520In%2520this%2520paper%252C%2520we%2520propose%2520BranchGRPO%252C%2520a%2520novel%2520method%2520that%250Aintroduces%2520a%2520branch%2520sampling%2520policy%2520updating%2520the%2520SDE%2520sampling%2520process.%2520By%250Asharing%2520computation%2520across%2520common%2520prefixes%2520and%2520pruning%2520low-reward%2520paths%2520and%250Aredundant%2520depths%252C%2520BranchGRPO%2520substantially%2520lowers%2520the%2520per-update%2520compute%2520cost%250Awhile%2520maintaining%2520or%2520improving%2520exploration%2520diversity.%2520This%2520work%2520makes%2520three%250Amain%2520contributions%253A%2520%25281%2529%2520a%2520branch%2520sampling%2520scheme%2520that%2520reduces%2520rollout%2520and%250Atraining%2520cost%253B%2520%25282%2529%2520a%2520tree-based%2520advantage%2520estimator%2520incorporating%2520dense%250Aprocess-level%2520rewards%253B%2520and%2520%25283%2529%2520pruning%2520strategies%2520exploiting%2520path%2520and%2520depth%250Aredundancy%2520to%2520accelerate%2520convergence%2520and%2520boost%2520performance.%2520Experiments%2520on%250Aimage%2520and%2520video%2520preference%2520alignment%2520show%2520that%2520BranchGRPO%2520improves%2520alignment%250Ascores%2520by%252016%2525%2520over%2520strong%2520baselines%252C%2520while%2520cutting%2520training%2520time%2520by%252050%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06040v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BranchGRPO%3A%20Stable%20and%20Efficient%20GRPO%20with%20Structured%20Branching%20in%0A%20%20Diffusion%20Models&entry.906535625=Yuming%20Li%20and%20Yikai%20Wang%20and%20Yuying%20Zhu%20and%20Zhongyu%20Zhao%20and%20Ming%20Lu%20and%20Qi%20She%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20aligning%20image%20and%20video%20generative%20models%20via%20GRPO%0Ahave%20achieved%20remarkable%20gains%20in%20enhancing%20human%20preference%20alignment.%0AHowever%2C%20these%20methods%20still%20face%20high%20computational%20costs%20from%20on-policy%0Arollouts%20and%20excessive%20SDE%20sampling%20steps%2C%20as%20well%20as%20training%20instability%20due%0Ato%20sparse%20rewards.%20In%20this%20paper%2C%20we%20propose%20BranchGRPO%2C%20a%20novel%20method%20that%0Aintroduces%20a%20branch%20sampling%20policy%20updating%20the%20SDE%20sampling%20process.%20By%0Asharing%20computation%20across%20common%20prefixes%20and%20pruning%20low-reward%20paths%20and%0Aredundant%20depths%2C%20BranchGRPO%20substantially%20lowers%20the%20per-update%20compute%20cost%0Awhile%20maintaining%20or%20improving%20exploration%20diversity.%20This%20work%20makes%20three%0Amain%20contributions%3A%20%281%29%20a%20branch%20sampling%20scheme%20that%20reduces%20rollout%20and%0Atraining%20cost%3B%20%282%29%20a%20tree-based%20advantage%20estimator%20incorporating%20dense%0Aprocess-level%20rewards%3B%20and%20%283%29%20pruning%20strategies%20exploiting%20path%20and%20depth%0Aredundancy%20to%20accelerate%20convergence%20and%20boost%20performance.%20Experiments%20on%0Aimage%20and%20video%20preference%20alignment%20show%20that%20BranchGRPO%20improves%20alignment%0Ascores%20by%2016%25%20over%20strong%20baselines%2C%20while%20cutting%20training%20time%20by%2050%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06040v3&entry.124074799=Read"},
{"title": "Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations", "author": "Daniele Barolo and Chiara Valentin and Fariba Karimi and Luis Gal\u00e1rraga and Gonzalo G. M\u00e9ndez and Lisette Esp\u00edn-Noboa", "abstract": "  This paper evaluates the performance of six open-weight LLMs (llama3-8b,\nllama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending\nexperts in physics across five tasks: top-k experts by field, influential\nscientists by discipline, epoch, seniority, and scholar counterparts. The\nevaluation examines consistency, factuality, and biases related to gender,\nethnicity, academic popularity, and scholar similarity. Using ground-truth data\nfrom the American Physical Society and OpenAlex, we establish scholarly\nbenchmarks by comparing model outputs to real-world academic records. Our\nanalysis reveals inconsistencies and biases across all models. mixtral-8x7b\nproduces the most stable outputs, while llama3.1-70b shows the highest\nvariability. Many models exhibit duplication, and some, particularly gemma2-9b\nand llama3.1-8b, struggle with formatting errors. LLMs generally recommend real\nscientists, but accuracy drops in field-, epoch-, and seniority-specific\nqueries, consistently favoring senior scholars. Representation biases persist,\nreplicating gender imbalances (reflecting male predominance),\nunder-representing Asian scientists, and over-representing White scholars.\nDespite some diversity in institutional and collaboration networks, models\nfavor highly cited and productive scholars, reinforcing the rich-getricher\neffect while offering limited geographical representation. These findings\nhighlight the need to improve LLMs for more reliable and equitable scholarly\nrecommendations.\n", "link": "http://arxiv.org/abs/2506.00074v2", "date": "2025-09-10", "relevancy": 2.1288, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4284}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whose%20Name%20Comes%20Up%3F%20Auditing%20LLM-Based%20Scholar%20Recommendations&body=Title%3A%20Whose%20Name%20Comes%20Up%3F%20Auditing%20LLM-Based%20Scholar%20Recommendations%0AAuthor%3A%20Daniele%20Barolo%20and%20Chiara%20Valentin%20and%20Fariba%20Karimi%20and%20Luis%20Gal%C3%A1rraga%20and%20Gonzalo%20G.%20M%C3%A9ndez%20and%20Lisette%20Esp%C3%ADn-Noboa%0AAbstract%3A%20%20%20This%20paper%20evaluates%20the%20performance%20of%20six%20open-weight%20LLMs%20%28llama3-8b%2C%0Allama3.1-8b%2C%20gemma2-9b%2C%20mixtral-8x7b%2C%20llama3-70b%2C%20llama3.1-70b%29%20in%20recommending%0Aexperts%20in%20physics%20across%20five%20tasks%3A%20top-k%20experts%20by%20field%2C%20influential%0Ascientists%20by%20discipline%2C%20epoch%2C%20seniority%2C%20and%20scholar%20counterparts.%20The%0Aevaluation%20examines%20consistency%2C%20factuality%2C%20and%20biases%20related%20to%20gender%2C%0Aethnicity%2C%20academic%20popularity%2C%20and%20scholar%20similarity.%20Using%20ground-truth%20data%0Afrom%20the%20American%20Physical%20Society%20and%20OpenAlex%2C%20we%20establish%20scholarly%0Abenchmarks%20by%20comparing%20model%20outputs%20to%20real-world%20academic%20records.%20Our%0Aanalysis%20reveals%20inconsistencies%20and%20biases%20across%20all%20models.%20mixtral-8x7b%0Aproduces%20the%20most%20stable%20outputs%2C%20while%20llama3.1-70b%20shows%20the%20highest%0Avariability.%20Many%20models%20exhibit%20duplication%2C%20and%20some%2C%20particularly%20gemma2-9b%0Aand%20llama3.1-8b%2C%20struggle%20with%20formatting%20errors.%20LLMs%20generally%20recommend%20real%0Ascientists%2C%20but%20accuracy%20drops%20in%20field-%2C%20epoch-%2C%20and%20seniority-specific%0Aqueries%2C%20consistently%20favoring%20senior%20scholars.%20Representation%20biases%20persist%2C%0Areplicating%20gender%20imbalances%20%28reflecting%20male%20predominance%29%2C%0Aunder-representing%20Asian%20scientists%2C%20and%20over-representing%20White%20scholars.%0ADespite%20some%20diversity%20in%20institutional%20and%20collaboration%20networks%2C%20models%0Afavor%20highly%20cited%20and%20productive%20scholars%2C%20reinforcing%20the%20rich-getricher%0Aeffect%20while%20offering%20limited%20geographical%20representation.%20These%20findings%0Ahighlight%20the%20need%20to%20improve%20LLMs%20for%20more%20reliable%20and%20equitable%20scholarly%0Arecommendations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00074v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhose%2520Name%2520Comes%2520Up%253F%2520Auditing%2520LLM-Based%2520Scholar%2520Recommendations%26entry.906535625%3DDaniele%2520Barolo%2520and%2520Chiara%2520Valentin%2520and%2520Fariba%2520Karimi%2520and%2520Luis%2520Gal%25C3%25A1rraga%2520and%2520Gonzalo%2520G.%2520M%25C3%25A9ndez%2520and%2520Lisette%2520Esp%25C3%25ADn-Noboa%26entry.1292438233%3D%2520%2520This%2520paper%2520evaluates%2520the%2520performance%2520of%2520six%2520open-weight%2520LLMs%2520%2528llama3-8b%252C%250Allama3.1-8b%252C%2520gemma2-9b%252C%2520mixtral-8x7b%252C%2520llama3-70b%252C%2520llama3.1-70b%2529%2520in%2520recommending%250Aexperts%2520in%2520physics%2520across%2520five%2520tasks%253A%2520top-k%2520experts%2520by%2520field%252C%2520influential%250Ascientists%2520by%2520discipline%252C%2520epoch%252C%2520seniority%252C%2520and%2520scholar%2520counterparts.%2520The%250Aevaluation%2520examines%2520consistency%252C%2520factuality%252C%2520and%2520biases%2520related%2520to%2520gender%252C%250Aethnicity%252C%2520academic%2520popularity%252C%2520and%2520scholar%2520similarity.%2520Using%2520ground-truth%2520data%250Afrom%2520the%2520American%2520Physical%2520Society%2520and%2520OpenAlex%252C%2520we%2520establish%2520scholarly%250Abenchmarks%2520by%2520comparing%2520model%2520outputs%2520to%2520real-world%2520academic%2520records.%2520Our%250Aanalysis%2520reveals%2520inconsistencies%2520and%2520biases%2520across%2520all%2520models.%2520mixtral-8x7b%250Aproduces%2520the%2520most%2520stable%2520outputs%252C%2520while%2520llama3.1-70b%2520shows%2520the%2520highest%250Avariability.%2520Many%2520models%2520exhibit%2520duplication%252C%2520and%2520some%252C%2520particularly%2520gemma2-9b%250Aand%2520llama3.1-8b%252C%2520struggle%2520with%2520formatting%2520errors.%2520LLMs%2520generally%2520recommend%2520real%250Ascientists%252C%2520but%2520accuracy%2520drops%2520in%2520field-%252C%2520epoch-%252C%2520and%2520seniority-specific%250Aqueries%252C%2520consistently%2520favoring%2520senior%2520scholars.%2520Representation%2520biases%2520persist%252C%250Areplicating%2520gender%2520imbalances%2520%2528reflecting%2520male%2520predominance%2529%252C%250Aunder-representing%2520Asian%2520scientists%252C%2520and%2520over-representing%2520White%2520scholars.%250ADespite%2520some%2520diversity%2520in%2520institutional%2520and%2520collaboration%2520networks%252C%2520models%250Afavor%2520highly%2520cited%2520and%2520productive%2520scholars%252C%2520reinforcing%2520the%2520rich-getricher%250Aeffect%2520while%2520offering%2520limited%2520geographical%2520representation.%2520These%2520findings%250Ahighlight%2520the%2520need%2520to%2520improve%2520LLMs%2520for%2520more%2520reliable%2520and%2520equitable%2520scholarly%250Arecommendations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00074v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whose%20Name%20Comes%20Up%3F%20Auditing%20LLM-Based%20Scholar%20Recommendations&entry.906535625=Daniele%20Barolo%20and%20Chiara%20Valentin%20and%20Fariba%20Karimi%20and%20Luis%20Gal%C3%A1rraga%20and%20Gonzalo%20G.%20M%C3%A9ndez%20and%20Lisette%20Esp%C3%ADn-Noboa&entry.1292438233=%20%20This%20paper%20evaluates%20the%20performance%20of%20six%20open-weight%20LLMs%20%28llama3-8b%2C%0Allama3.1-8b%2C%20gemma2-9b%2C%20mixtral-8x7b%2C%20llama3-70b%2C%20llama3.1-70b%29%20in%20recommending%0Aexperts%20in%20physics%20across%20five%20tasks%3A%20top-k%20experts%20by%20field%2C%20influential%0Ascientists%20by%20discipline%2C%20epoch%2C%20seniority%2C%20and%20scholar%20counterparts.%20The%0Aevaluation%20examines%20consistency%2C%20factuality%2C%20and%20biases%20related%20to%20gender%2C%0Aethnicity%2C%20academic%20popularity%2C%20and%20scholar%20similarity.%20Using%20ground-truth%20data%0Afrom%20the%20American%20Physical%20Society%20and%20OpenAlex%2C%20we%20establish%20scholarly%0Abenchmarks%20by%20comparing%20model%20outputs%20to%20real-world%20academic%20records.%20Our%0Aanalysis%20reveals%20inconsistencies%20and%20biases%20across%20all%20models.%20mixtral-8x7b%0Aproduces%20the%20most%20stable%20outputs%2C%20while%20llama3.1-70b%20shows%20the%20highest%0Avariability.%20Many%20models%20exhibit%20duplication%2C%20and%20some%2C%20particularly%20gemma2-9b%0Aand%20llama3.1-8b%2C%20struggle%20with%20formatting%20errors.%20LLMs%20generally%20recommend%20real%0Ascientists%2C%20but%20accuracy%20drops%20in%20field-%2C%20epoch-%2C%20and%20seniority-specific%0Aqueries%2C%20consistently%20favoring%20senior%20scholars.%20Representation%20biases%20persist%2C%0Areplicating%20gender%20imbalances%20%28reflecting%20male%20predominance%29%2C%0Aunder-representing%20Asian%20scientists%2C%20and%20over-representing%20White%20scholars.%0ADespite%20some%20diversity%20in%20institutional%20and%20collaboration%20networks%2C%20models%0Afavor%20highly%20cited%20and%20productive%20scholars%2C%20reinforcing%20the%20rich-getricher%0Aeffect%20while%20offering%20limited%20geographical%20representation.%20These%20findings%0Ahighlight%20the%20need%20to%20improve%20LLMs%20for%20more%20reliable%20and%20equitable%20scholarly%0Arecommendations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00074v2&entry.124074799=Read"},
{"title": "F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking\n  Face Generation, Customization, and Restoration", "author": "Lu Liu and Huiyu Duan and Qiang Hu and Liu Yang and Chunlei Cai and Tianxiao Ye and Huayu Liu and Xiaoyun Zhang and Guangtao Zhai", "abstract": "  Artificial intelligence generative models exhibit remarkable capabilities in\ncontent creation, particularly in face image generation, customization, and\nrestoration. However, current AI-generated faces (AIGFs) often fall short of\nhuman preferences due to unique distortions, unrealistic details, and\nunexpected identity shifts, underscoring the need for a comprehensive quality\nevaluation framework for AIGFs. To address this need, we introduce FaceQ, a\nlarge-scale, comprehensive database of AI-generated Face images with\nfine-grained Quality annotations reflecting human preferences. The FaceQ\ndatabase comprises 12,255 images generated by 29 models across three tasks: (1)\nface generation, (2) face customization, and (3) face restoration. It includes\n32,742 mean opinion scores (MOSs) from 180 annotators, assessed across multiple\ndimensions: quality, authenticity, identity (ID) fidelity, and text-image\ncorrespondence. Using the FaceQ database, we establish F-Bench, a benchmark for\ncomparing and evaluating face generation, customization, and restoration\nmodels, highlighting strengths and weaknesses across various prompts and\nevaluation dimensions. Additionally, we assess the performance of existing\nimage quality assessment (IQA), face quality assessment (FQA), AI-generated\ncontent image quality assessment (AIGCIQA), and preference evaluation metrics,\nmanifesting that these standard metrics are relatively ineffective in\nevaluating authenticity, ID fidelity, and text-image correspondence. The FaceQ\ndatabase will be publicly available upon publication.\n", "link": "http://arxiv.org/abs/2412.13155v2", "date": "2025-09-10", "relevancy": 2.1226, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5498}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5231}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F-Bench%3A%20Rethinking%20Human%20Preference%20Evaluation%20Metrics%20for%20Benchmarking%0A%20%20Face%20Generation%2C%20Customization%2C%20and%20Restoration&body=Title%3A%20F-Bench%3A%20Rethinking%20Human%20Preference%20Evaluation%20Metrics%20for%20Benchmarking%0A%20%20Face%20Generation%2C%20Customization%2C%20and%20Restoration%0AAuthor%3A%20Lu%20Liu%20and%20Huiyu%20Duan%20and%20Qiang%20Hu%20and%20Liu%20Yang%20and%20Chunlei%20Cai%20and%20Tianxiao%20Ye%20and%20Huayu%20Liu%20and%20Xiaoyun%20Zhang%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20Artificial%20intelligence%20generative%20models%20exhibit%20remarkable%20capabilities%20in%0Acontent%20creation%2C%20particularly%20in%20face%20image%20generation%2C%20customization%2C%20and%0Arestoration.%20However%2C%20current%20AI-generated%20faces%20%28AIGFs%29%20often%20fall%20short%20of%0Ahuman%20preferences%20due%20to%20unique%20distortions%2C%20unrealistic%20details%2C%20and%0Aunexpected%20identity%20shifts%2C%20underscoring%20the%20need%20for%20a%20comprehensive%20quality%0Aevaluation%20framework%20for%20AIGFs.%20To%20address%20this%20need%2C%20we%20introduce%20FaceQ%2C%20a%0Alarge-scale%2C%20comprehensive%20database%20of%20AI-generated%20Face%20images%20with%0Afine-grained%20Quality%20annotations%20reflecting%20human%20preferences.%20The%20FaceQ%0Adatabase%20comprises%2012%2C255%20images%20generated%20by%2029%20models%20across%20three%20tasks%3A%20%281%29%0Aface%20generation%2C%20%282%29%20face%20customization%2C%20and%20%283%29%20face%20restoration.%20It%20includes%0A32%2C742%20mean%20opinion%20scores%20%28MOSs%29%20from%20180%20annotators%2C%20assessed%20across%20multiple%0Adimensions%3A%20quality%2C%20authenticity%2C%20identity%20%28ID%29%20fidelity%2C%20and%20text-image%0Acorrespondence.%20Using%20the%20FaceQ%20database%2C%20we%20establish%20F-Bench%2C%20a%20benchmark%20for%0Acomparing%20and%20evaluating%20face%20generation%2C%20customization%2C%20and%20restoration%0Amodels%2C%20highlighting%20strengths%20and%20weaknesses%20across%20various%20prompts%20and%0Aevaluation%20dimensions.%20Additionally%2C%20we%20assess%20the%20performance%20of%20existing%0Aimage%20quality%20assessment%20%28IQA%29%2C%20face%20quality%20assessment%20%28FQA%29%2C%20AI-generated%0Acontent%20image%20quality%20assessment%20%28AIGCIQA%29%2C%20and%20preference%20evaluation%20metrics%2C%0Amanifesting%20that%20these%20standard%20metrics%20are%20relatively%20ineffective%20in%0Aevaluating%20authenticity%2C%20ID%20fidelity%2C%20and%20text-image%20correspondence.%20The%20FaceQ%0Adatabase%20will%20be%20publicly%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13155v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF-Bench%253A%2520Rethinking%2520Human%2520Preference%2520Evaluation%2520Metrics%2520for%2520Benchmarking%250A%2520%2520Face%2520Generation%252C%2520Customization%252C%2520and%2520Restoration%26entry.906535625%3DLu%2520Liu%2520and%2520Huiyu%2520Duan%2520and%2520Qiang%2520Hu%2520and%2520Liu%2520Yang%2520and%2520Chunlei%2520Cai%2520and%2520Tianxiao%2520Ye%2520and%2520Huayu%2520Liu%2520and%2520Xiaoyun%2520Zhang%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520generative%2520models%2520exhibit%2520remarkable%2520capabilities%2520in%250Acontent%2520creation%252C%2520particularly%2520in%2520face%2520image%2520generation%252C%2520customization%252C%2520and%250Arestoration.%2520However%252C%2520current%2520AI-generated%2520faces%2520%2528AIGFs%2529%2520often%2520fall%2520short%2520of%250Ahuman%2520preferences%2520due%2520to%2520unique%2520distortions%252C%2520unrealistic%2520details%252C%2520and%250Aunexpected%2520identity%2520shifts%252C%2520underscoring%2520the%2520need%2520for%2520a%2520comprehensive%2520quality%250Aevaluation%2520framework%2520for%2520AIGFs.%2520To%2520address%2520this%2520need%252C%2520we%2520introduce%2520FaceQ%252C%2520a%250Alarge-scale%252C%2520comprehensive%2520database%2520of%2520AI-generated%2520Face%2520images%2520with%250Afine-grained%2520Quality%2520annotations%2520reflecting%2520human%2520preferences.%2520The%2520FaceQ%250Adatabase%2520comprises%252012%252C255%2520images%2520generated%2520by%252029%2520models%2520across%2520three%2520tasks%253A%2520%25281%2529%250Aface%2520generation%252C%2520%25282%2529%2520face%2520customization%252C%2520and%2520%25283%2529%2520face%2520restoration.%2520It%2520includes%250A32%252C742%2520mean%2520opinion%2520scores%2520%2528MOSs%2529%2520from%2520180%2520annotators%252C%2520assessed%2520across%2520multiple%250Adimensions%253A%2520quality%252C%2520authenticity%252C%2520identity%2520%2528ID%2529%2520fidelity%252C%2520and%2520text-image%250Acorrespondence.%2520Using%2520the%2520FaceQ%2520database%252C%2520we%2520establish%2520F-Bench%252C%2520a%2520benchmark%2520for%250Acomparing%2520and%2520evaluating%2520face%2520generation%252C%2520customization%252C%2520and%2520restoration%250Amodels%252C%2520highlighting%2520strengths%2520and%2520weaknesses%2520across%2520various%2520prompts%2520and%250Aevaluation%2520dimensions.%2520Additionally%252C%2520we%2520assess%2520the%2520performance%2520of%2520existing%250Aimage%2520quality%2520assessment%2520%2528IQA%2529%252C%2520face%2520quality%2520assessment%2520%2528FQA%2529%252C%2520AI-generated%250Acontent%2520image%2520quality%2520assessment%2520%2528AIGCIQA%2529%252C%2520and%2520preference%2520evaluation%2520metrics%252C%250Amanifesting%2520that%2520these%2520standard%2520metrics%2520are%2520relatively%2520ineffective%2520in%250Aevaluating%2520authenticity%252C%2520ID%2520fidelity%252C%2520and%2520text-image%2520correspondence.%2520The%2520FaceQ%250Adatabase%2520will%2520be%2520publicly%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13155v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F-Bench%3A%20Rethinking%20Human%20Preference%20Evaluation%20Metrics%20for%20Benchmarking%0A%20%20Face%20Generation%2C%20Customization%2C%20and%20Restoration&entry.906535625=Lu%20Liu%20and%20Huiyu%20Duan%20and%20Qiang%20Hu%20and%20Liu%20Yang%20and%20Chunlei%20Cai%20and%20Tianxiao%20Ye%20and%20Huayu%20Liu%20and%20Xiaoyun%20Zhang%20and%20Guangtao%20Zhai&entry.1292438233=%20%20Artificial%20intelligence%20generative%20models%20exhibit%20remarkable%20capabilities%20in%0Acontent%20creation%2C%20particularly%20in%20face%20image%20generation%2C%20customization%2C%20and%0Arestoration.%20However%2C%20current%20AI-generated%20faces%20%28AIGFs%29%20often%20fall%20short%20of%0Ahuman%20preferences%20due%20to%20unique%20distortions%2C%20unrealistic%20details%2C%20and%0Aunexpected%20identity%20shifts%2C%20underscoring%20the%20need%20for%20a%20comprehensive%20quality%0Aevaluation%20framework%20for%20AIGFs.%20To%20address%20this%20need%2C%20we%20introduce%20FaceQ%2C%20a%0Alarge-scale%2C%20comprehensive%20database%20of%20AI-generated%20Face%20images%20with%0Afine-grained%20Quality%20annotations%20reflecting%20human%20preferences.%20The%20FaceQ%0Adatabase%20comprises%2012%2C255%20images%20generated%20by%2029%20models%20across%20three%20tasks%3A%20%281%29%0Aface%20generation%2C%20%282%29%20face%20customization%2C%20and%20%283%29%20face%20restoration.%20It%20includes%0A32%2C742%20mean%20opinion%20scores%20%28MOSs%29%20from%20180%20annotators%2C%20assessed%20across%20multiple%0Adimensions%3A%20quality%2C%20authenticity%2C%20identity%20%28ID%29%20fidelity%2C%20and%20text-image%0Acorrespondence.%20Using%20the%20FaceQ%20database%2C%20we%20establish%20F-Bench%2C%20a%20benchmark%20for%0Acomparing%20and%20evaluating%20face%20generation%2C%20customization%2C%20and%20restoration%0Amodels%2C%20highlighting%20strengths%20and%20weaknesses%20across%20various%20prompts%20and%0Aevaluation%20dimensions.%20Additionally%2C%20we%20assess%20the%20performance%20of%20existing%0Aimage%20quality%20assessment%20%28IQA%29%2C%20face%20quality%20assessment%20%28FQA%29%2C%20AI-generated%0Acontent%20image%20quality%20assessment%20%28AIGCIQA%29%2C%20and%20preference%20evaluation%20metrics%2C%0Amanifesting%20that%20these%20standard%20metrics%20are%20relatively%20ineffective%20in%0Aevaluating%20authenticity%2C%20ID%20fidelity%2C%20and%20text-image%20correspondence.%20The%20FaceQ%0Adatabase%20will%20be%20publicly%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13155v2&entry.124074799=Read"},
{"title": "Multi-Modal Robust Enhancement for Coastal Water Segmentation: A\n  Systematic HSV-Guided Framework", "author": "Zhen Tian and Christos Anagnostopoulos and Qiyuan Wang and Zhiwei Gao", "abstract": "  Coastal water segmentation from satellite imagery presents unique challenges\ndue to complex spectral characteristics and irregular boundary patterns.\nTraditional RGB-based approaches often suffer from training instability and\npoor generalization in diverse maritime environments. This paper introduces a\nsystematic robust enhancement framework, referred to as Robust U-Net, that\nleverages HSV color space supervision and multi-modal constraints for improved\ncoastal water segmentation. Our approach integrates five synergistic\ncomponents: HSV-guided color supervision, gradient-based coastline\noptimization, morphological post-processing, sea area cleanup, and connectivity\ncontrol. Through comprehensive ablation studies, we demonstrate that HSV\nsupervision provides the highest impact (0.85 influence score), while the\ncomplete framework achieves superior training stability (84\\% variance\nreduction) and enhanced segmentation quality. Our method shows consistent\nimprovements across multiple evaluation metrics while maintaining computational\nefficiency. For reproducibility, our training configurations and code are\navailable here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.\n", "link": "http://arxiv.org/abs/2509.08694v1", "date": "2025-09-10", "relevancy": 2.1053, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5292}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Robust%20Enhancement%20for%20Coastal%20Water%20Segmentation%3A%20A%0A%20%20Systematic%20HSV-Guided%20Framework&body=Title%3A%20Multi-Modal%20Robust%20Enhancement%20for%20Coastal%20Water%20Segmentation%3A%20A%0A%20%20Systematic%20HSV-Guided%20Framework%0AAuthor%3A%20Zhen%20Tian%20and%20Christos%20Anagnostopoulos%20and%20Qiyuan%20Wang%20and%20Zhiwei%20Gao%0AAbstract%3A%20%20%20Coastal%20water%20segmentation%20from%20satellite%20imagery%20presents%20unique%20challenges%0Adue%20to%20complex%20spectral%20characteristics%20and%20irregular%20boundary%20patterns.%0ATraditional%20RGB-based%20approaches%20often%20suffer%20from%20training%20instability%20and%0Apoor%20generalization%20in%20diverse%20maritime%20environments.%20This%20paper%20introduces%20a%0Asystematic%20robust%20enhancement%20framework%2C%20referred%20to%20as%20Robust%20U-Net%2C%20that%0Aleverages%20HSV%20color%20space%20supervision%20and%20multi-modal%20constraints%20for%20improved%0Acoastal%20water%20segmentation.%20Our%20approach%20integrates%20five%20synergistic%0Acomponents%3A%20HSV-guided%20color%20supervision%2C%20gradient-based%20coastline%0Aoptimization%2C%20morphological%20post-processing%2C%20sea%20area%20cleanup%2C%20and%20connectivity%0Acontrol.%20Through%20comprehensive%20ablation%20studies%2C%20we%20demonstrate%20that%20HSV%0Asupervision%20provides%20the%20highest%20impact%20%280.85%20influence%20score%29%2C%20while%20the%0Acomplete%20framework%20achieves%20superior%20training%20stability%20%2884%5C%25%20variance%0Areduction%29%20and%20enhanced%20segmentation%20quality.%20Our%20method%20shows%20consistent%0Aimprovements%20across%20multiple%20evaluation%20metrics%20while%20maintaining%20computational%0Aefficiency.%20For%20reproducibility%2C%20our%20training%20configurations%20and%20code%20are%0Aavailable%20here%3A%20https%3A//github.com/UofgCoastline/ICASSP-2026-Robust-Unet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Robust%2520Enhancement%2520for%2520Coastal%2520Water%2520Segmentation%253A%2520A%250A%2520%2520Systematic%2520HSV-Guided%2520Framework%26entry.906535625%3DZhen%2520Tian%2520and%2520Christos%2520Anagnostopoulos%2520and%2520Qiyuan%2520Wang%2520and%2520Zhiwei%2520Gao%26entry.1292438233%3D%2520%2520Coastal%2520water%2520segmentation%2520from%2520satellite%2520imagery%2520presents%2520unique%2520challenges%250Adue%2520to%2520complex%2520spectral%2520characteristics%2520and%2520irregular%2520boundary%2520patterns.%250ATraditional%2520RGB-based%2520approaches%2520often%2520suffer%2520from%2520training%2520instability%2520and%250Apoor%2520generalization%2520in%2520diverse%2520maritime%2520environments.%2520This%2520paper%2520introduces%2520a%250Asystematic%2520robust%2520enhancement%2520framework%252C%2520referred%2520to%2520as%2520Robust%2520U-Net%252C%2520that%250Aleverages%2520HSV%2520color%2520space%2520supervision%2520and%2520multi-modal%2520constraints%2520for%2520improved%250Acoastal%2520water%2520segmentation.%2520Our%2520approach%2520integrates%2520five%2520synergistic%250Acomponents%253A%2520HSV-guided%2520color%2520supervision%252C%2520gradient-based%2520coastline%250Aoptimization%252C%2520morphological%2520post-processing%252C%2520sea%2520area%2520cleanup%252C%2520and%2520connectivity%250Acontrol.%2520Through%2520comprehensive%2520ablation%2520studies%252C%2520we%2520demonstrate%2520that%2520HSV%250Asupervision%2520provides%2520the%2520highest%2520impact%2520%25280.85%2520influence%2520score%2529%252C%2520while%2520the%250Acomplete%2520framework%2520achieves%2520superior%2520training%2520stability%2520%252884%255C%2525%2520variance%250Areduction%2529%2520and%2520enhanced%2520segmentation%2520quality.%2520Our%2520method%2520shows%2520consistent%250Aimprovements%2520across%2520multiple%2520evaluation%2520metrics%2520while%2520maintaining%2520computational%250Aefficiency.%2520For%2520reproducibility%252C%2520our%2520training%2520configurations%2520and%2520code%2520are%250Aavailable%2520here%253A%2520https%253A//github.com/UofgCoastline/ICASSP-2026-Robust-Unet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Robust%20Enhancement%20for%20Coastal%20Water%20Segmentation%3A%20A%0A%20%20Systematic%20HSV-Guided%20Framework&entry.906535625=Zhen%20Tian%20and%20Christos%20Anagnostopoulos%20and%20Qiyuan%20Wang%20and%20Zhiwei%20Gao&entry.1292438233=%20%20Coastal%20water%20segmentation%20from%20satellite%20imagery%20presents%20unique%20challenges%0Adue%20to%20complex%20spectral%20characteristics%20and%20irregular%20boundary%20patterns.%0ATraditional%20RGB-based%20approaches%20often%20suffer%20from%20training%20instability%20and%0Apoor%20generalization%20in%20diverse%20maritime%20environments.%20This%20paper%20introduces%20a%0Asystematic%20robust%20enhancement%20framework%2C%20referred%20to%20as%20Robust%20U-Net%2C%20that%0Aleverages%20HSV%20color%20space%20supervision%20and%20multi-modal%20constraints%20for%20improved%0Acoastal%20water%20segmentation.%20Our%20approach%20integrates%20five%20synergistic%0Acomponents%3A%20HSV-guided%20color%20supervision%2C%20gradient-based%20coastline%0Aoptimization%2C%20morphological%20post-processing%2C%20sea%20area%20cleanup%2C%20and%20connectivity%0Acontrol.%20Through%20comprehensive%20ablation%20studies%2C%20we%20demonstrate%20that%20HSV%0Asupervision%20provides%20the%20highest%20impact%20%280.85%20influence%20score%29%2C%20while%20the%0Acomplete%20framework%20achieves%20superior%20training%20stability%20%2884%5C%25%20variance%0Areduction%29%20and%20enhanced%20segmentation%20quality.%20Our%20method%20shows%20consistent%0Aimprovements%20across%20multiple%20evaluation%20metrics%20while%20maintaining%20computational%0Aefficiency.%20For%20reproducibility%2C%20our%20training%20configurations%20and%20code%20are%0Aavailable%20here%3A%20https%3A//github.com/UofgCoastline/ICASSP-2026-Robust-Unet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08694v1&entry.124074799=Read"},
{"title": "Variational Rank Reduction Autoencoders for Generative", "author": "Alicia Tierz and Jad Mounayer and Beatriz Moya and Francisco Chinesta", "abstract": "  Generative thermal design for complex geometries is fundamental in many areas\nof engineering, yet it faces two main challenges: the high computational cost\nof high-fidelity simulations and the limitations of conventional generative\nmodels. Approaches such as autoencoders (AEs) and variational autoencoders\n(VAEs) often produce unstructured latent spaces with discontinuities, which\nrestricts their capacity to explore designs and generate physically consistent\nsolutions.\n  To address these limitations, we propose a hybrid framework that combines\nVariational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks\n(DeepONets). The VRRAE introduces a truncated SVD within the latent space,\nleading to continuous, interpretable, and well-structured representations that\nmitigate posterior collapse and improve geometric reconstruction. The DeepONet\nthen exploits this compact latent encoding in its branch network, together with\nspatial coordinates in the trunk network, to predict temperature gradients\nefficiently and accurately.\n  This hybrid approach not only enhances the quality of generated geometries\nand the accuracy of gradient prediction, but also provides a substantial\nadvantage in inference efficiency compared to traditional numerical solvers.\nOverall, the study underscores the importance of structured latent\nrepresentations for operator learning and highlights the potential of combining\ngenerative models and operator networks in thermal design and broader\nengineering applications.\n", "link": "http://arxiv.org/abs/2509.08515v1", "date": "2025-09-10", "relevancy": 2.0961, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5404}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5369}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Rank%20Reduction%20Autoencoders%20for%20Generative&body=Title%3A%20Variational%20Rank%20Reduction%20Autoencoders%20for%20Generative%0AAuthor%3A%20Alicia%20Tierz%20and%20Jad%20Mounayer%20and%20Beatriz%20Moya%20and%20Francisco%20Chinesta%0AAbstract%3A%20%20%20Generative%20thermal%20design%20for%20complex%20geometries%20is%20fundamental%20in%20many%20areas%0Aof%20engineering%2C%20yet%20it%20faces%20two%20main%20challenges%3A%20the%20high%20computational%20cost%0Aof%20high-fidelity%20simulations%20and%20the%20limitations%20of%20conventional%20generative%0Amodels.%20Approaches%20such%20as%20autoencoders%20%28AEs%29%20and%20variational%20autoencoders%0A%28VAEs%29%20often%20produce%20unstructured%20latent%20spaces%20with%20discontinuities%2C%20which%0Arestricts%20their%20capacity%20to%20explore%20designs%20and%20generate%20physically%20consistent%0Asolutions.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20a%20hybrid%20framework%20that%20combines%0AVariational%20Rank-Reduction%20Autoencoders%20%28VRRAEs%29%20with%20Deep%20Operator%20Networks%0A%28DeepONets%29.%20The%20VRRAE%20introduces%20a%20truncated%20SVD%20within%20the%20latent%20space%2C%0Aleading%20to%20continuous%2C%20interpretable%2C%20and%20well-structured%20representations%20that%0Amitigate%20posterior%20collapse%20and%20improve%20geometric%20reconstruction.%20The%20DeepONet%0Athen%20exploits%20this%20compact%20latent%20encoding%20in%20its%20branch%20network%2C%20together%20with%0Aspatial%20coordinates%20in%20the%20trunk%20network%2C%20to%20predict%20temperature%20gradients%0Aefficiently%20and%20accurately.%0A%20%20This%20hybrid%20approach%20not%20only%20enhances%20the%20quality%20of%20generated%20geometries%0Aand%20the%20accuracy%20of%20gradient%20prediction%2C%20but%20also%20provides%20a%20substantial%0Aadvantage%20in%20inference%20efficiency%20compared%20to%20traditional%20numerical%20solvers.%0AOverall%2C%20the%20study%20underscores%20the%20importance%20of%20structured%20latent%0Arepresentations%20for%20operator%20learning%20and%20highlights%20the%20potential%20of%20combining%0Agenerative%20models%20and%20operator%20networks%20in%20thermal%20design%20and%20broader%0Aengineering%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Rank%2520Reduction%2520Autoencoders%2520for%2520Generative%26entry.906535625%3DAlicia%2520Tierz%2520and%2520Jad%2520Mounayer%2520and%2520Beatriz%2520Moya%2520and%2520Francisco%2520Chinesta%26entry.1292438233%3D%2520%2520Generative%2520thermal%2520design%2520for%2520complex%2520geometries%2520is%2520fundamental%2520in%2520many%2520areas%250Aof%2520engineering%252C%2520yet%2520it%2520faces%2520two%2520main%2520challenges%253A%2520the%2520high%2520computational%2520cost%250Aof%2520high-fidelity%2520simulations%2520and%2520the%2520limitations%2520of%2520conventional%2520generative%250Amodels.%2520Approaches%2520such%2520as%2520autoencoders%2520%2528AEs%2529%2520and%2520variational%2520autoencoders%250A%2528VAEs%2529%2520often%2520produce%2520unstructured%2520latent%2520spaces%2520with%2520discontinuities%252C%2520which%250Arestricts%2520their%2520capacity%2520to%2520explore%2520designs%2520and%2520generate%2520physically%2520consistent%250Asolutions.%250A%2520%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520hybrid%2520framework%2520that%2520combines%250AVariational%2520Rank-Reduction%2520Autoencoders%2520%2528VRRAEs%2529%2520with%2520Deep%2520Operator%2520Networks%250A%2528DeepONets%2529.%2520The%2520VRRAE%2520introduces%2520a%2520truncated%2520SVD%2520within%2520the%2520latent%2520space%252C%250Aleading%2520to%2520continuous%252C%2520interpretable%252C%2520and%2520well-structured%2520representations%2520that%250Amitigate%2520posterior%2520collapse%2520and%2520improve%2520geometric%2520reconstruction.%2520The%2520DeepONet%250Athen%2520exploits%2520this%2520compact%2520latent%2520encoding%2520in%2520its%2520branch%2520network%252C%2520together%2520with%250Aspatial%2520coordinates%2520in%2520the%2520trunk%2520network%252C%2520to%2520predict%2520temperature%2520gradients%250Aefficiently%2520and%2520accurately.%250A%2520%2520This%2520hybrid%2520approach%2520not%2520only%2520enhances%2520the%2520quality%2520of%2520generated%2520geometries%250Aand%2520the%2520accuracy%2520of%2520gradient%2520prediction%252C%2520but%2520also%2520provides%2520a%2520substantial%250Aadvantage%2520in%2520inference%2520efficiency%2520compared%2520to%2520traditional%2520numerical%2520solvers.%250AOverall%252C%2520the%2520study%2520underscores%2520the%2520importance%2520of%2520structured%2520latent%250Arepresentations%2520for%2520operator%2520learning%2520and%2520highlights%2520the%2520potential%2520of%2520combining%250Agenerative%2520models%2520and%2520operator%2520networks%2520in%2520thermal%2520design%2520and%2520broader%250Aengineering%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Rank%20Reduction%20Autoencoders%20for%20Generative&entry.906535625=Alicia%20Tierz%20and%20Jad%20Mounayer%20and%20Beatriz%20Moya%20and%20Francisco%20Chinesta&entry.1292438233=%20%20Generative%20thermal%20design%20for%20complex%20geometries%20is%20fundamental%20in%20many%20areas%0Aof%20engineering%2C%20yet%20it%20faces%20two%20main%20challenges%3A%20the%20high%20computational%20cost%0Aof%20high-fidelity%20simulations%20and%20the%20limitations%20of%20conventional%20generative%0Amodels.%20Approaches%20such%20as%20autoencoders%20%28AEs%29%20and%20variational%20autoencoders%0A%28VAEs%29%20often%20produce%20unstructured%20latent%20spaces%20with%20discontinuities%2C%20which%0Arestricts%20their%20capacity%20to%20explore%20designs%20and%20generate%20physically%20consistent%0Asolutions.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20a%20hybrid%20framework%20that%20combines%0AVariational%20Rank-Reduction%20Autoencoders%20%28VRRAEs%29%20with%20Deep%20Operator%20Networks%0A%28DeepONets%29.%20The%20VRRAE%20introduces%20a%20truncated%20SVD%20within%20the%20latent%20space%2C%0Aleading%20to%20continuous%2C%20interpretable%2C%20and%20well-structured%20representations%20that%0Amitigate%20posterior%20collapse%20and%20improve%20geometric%20reconstruction.%20The%20DeepONet%0Athen%20exploits%20this%20compact%20latent%20encoding%20in%20its%20branch%20network%2C%20together%20with%0Aspatial%20coordinates%20in%20the%20trunk%20network%2C%20to%20predict%20temperature%20gradients%0Aefficiently%20and%20accurately.%0A%20%20This%20hybrid%20approach%20not%20only%20enhances%20the%20quality%20of%20generated%20geometries%0Aand%20the%20accuracy%20of%20gradient%20prediction%2C%20but%20also%20provides%20a%20substantial%0Aadvantage%20in%20inference%20efficiency%20compared%20to%20traditional%20numerical%20solvers.%0AOverall%2C%20the%20study%20underscores%20the%20importance%20of%20structured%20latent%0Arepresentations%20for%20operator%20learning%20and%20highlights%20the%20potential%20of%20combining%0Agenerative%20models%20and%20operator%20networks%20in%20thermal%20design%20and%20broader%0Aengineering%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08515v1&entry.124074799=Read"},
{"title": "ADHDeepNet From Raw EEG to Diagnosis: Improving ADHD Diagnosis through\n  Temporal-Spatial Processing, Adaptive Attention Mechanisms, and\n  Explainability in Raw EEG Signals", "author": "Ali Amini and Mohammad Alijanpour and Behnam Latifi and Ali Motie Nasrabadi", "abstract": "  Attention Deficit Hyperactivity Disorder (ADHD) is a common brain disorder in\nchildren that can persist into adulthood, affecting social, academic, and\ncareer life. Early diagnosis is crucial for managing these impacts on patients\nand the healthcare system but is often labor-intensive and time-consuming. This\npaper presents a novel method to improve ADHD diagnosis precision and\ntimeliness by leveraging Deep Learning (DL) approaches and electroencephalogram\n(EEG) signals. We introduce ADHDeepNet, a DL model that utilizes comprehensive\ntemporal-spatial characterization, attention modules, and explainability\ntechniques optimized for EEG signals. ADHDeepNet integrates feature extraction\nand refinement processes to enhance ADHD diagnosis. The model was trained and\nvalidated on a dataset of 121 participants (61 ADHD, 60 Healthy Controls),\nemploying nested cross-validation for robust performance. The proposed\ntwo-stage methodology uses a 10-fold cross-subject validation strategy.\nInitially, each iteration optimizes the model's hyper-parameters with inner\n2-fold cross-validation. Then, Additive Gaussian Noise (AGN) with various\nstandard deviations and magnification levels is applied for data augmentation.\nADHDeepNet achieved 100% sensitivity and 99.17% accuracy in classifying ADHD/HC\nsubjects. To clarify model explainability and identify key brain regions and\nfrequency bands for ADHD diagnosis, we analyzed the learned weights and\nactivation patterns of the model's primary layers. Additionally, t-distributed\nStochastic Neighbor Embedding (t-SNE) visualized high-dimensional data, aiding\nin interpreting the model's decisions. This study highlights the potential of\nDL and EEG in enhancing ADHD diagnosis accuracy and efficiency.\n", "link": "http://arxiv.org/abs/2509.08779v1", "date": "2025-09-10", "relevancy": 2.0941, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5434}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5127}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADHDeepNet%20From%20Raw%20EEG%20to%20Diagnosis%3A%20Improving%20ADHD%20Diagnosis%20through%0A%20%20Temporal-Spatial%20Processing%2C%20Adaptive%20Attention%20Mechanisms%2C%20and%0A%20%20Explainability%20in%20Raw%20EEG%20Signals&body=Title%3A%20ADHDeepNet%20From%20Raw%20EEG%20to%20Diagnosis%3A%20Improving%20ADHD%20Diagnosis%20through%0A%20%20Temporal-Spatial%20Processing%2C%20Adaptive%20Attention%20Mechanisms%2C%20and%0A%20%20Explainability%20in%20Raw%20EEG%20Signals%0AAuthor%3A%20Ali%20Amini%20and%20Mohammad%20Alijanpour%20and%20Behnam%20Latifi%20and%20Ali%20Motie%20Nasrabadi%0AAbstract%3A%20%20%20Attention%20Deficit%20Hyperactivity%20Disorder%20%28ADHD%29%20is%20a%20common%20brain%20disorder%20in%0Achildren%20that%20can%20persist%20into%20adulthood%2C%20affecting%20social%2C%20academic%2C%20and%0Acareer%20life.%20Early%20diagnosis%20is%20crucial%20for%20managing%20these%20impacts%20on%20patients%0Aand%20the%20healthcare%20system%20but%20is%20often%20labor-intensive%20and%20time-consuming.%20This%0Apaper%20presents%20a%20novel%20method%20to%20improve%20ADHD%20diagnosis%20precision%20and%0Atimeliness%20by%20leveraging%20Deep%20Learning%20%28DL%29%20approaches%20and%20electroencephalogram%0A%28EEG%29%20signals.%20We%20introduce%20ADHDeepNet%2C%20a%20DL%20model%20that%20utilizes%20comprehensive%0Atemporal-spatial%20characterization%2C%20attention%20modules%2C%20and%20explainability%0Atechniques%20optimized%20for%20EEG%20signals.%20ADHDeepNet%20integrates%20feature%20extraction%0Aand%20refinement%20processes%20to%20enhance%20ADHD%20diagnosis.%20The%20model%20was%20trained%20and%0Avalidated%20on%20a%20dataset%20of%20121%20participants%20%2861%20ADHD%2C%2060%20Healthy%20Controls%29%2C%0Aemploying%20nested%20cross-validation%20for%20robust%20performance.%20The%20proposed%0Atwo-stage%20methodology%20uses%20a%2010-fold%20cross-subject%20validation%20strategy.%0AInitially%2C%20each%20iteration%20optimizes%20the%20model%27s%20hyper-parameters%20with%20inner%0A2-fold%20cross-validation.%20Then%2C%20Additive%20Gaussian%20Noise%20%28AGN%29%20with%20various%0Astandard%20deviations%20and%20magnification%20levels%20is%20applied%20for%20data%20augmentation.%0AADHDeepNet%20achieved%20100%25%20sensitivity%20and%2099.17%25%20accuracy%20in%20classifying%20ADHD/HC%0Asubjects.%20To%20clarify%20model%20explainability%20and%20identify%20key%20brain%20regions%20and%0Afrequency%20bands%20for%20ADHD%20diagnosis%2C%20we%20analyzed%20the%20learned%20weights%20and%0Aactivation%20patterns%20of%20the%20model%27s%20primary%20layers.%20Additionally%2C%20t-distributed%0AStochastic%20Neighbor%20Embedding%20%28t-SNE%29%20visualized%20high-dimensional%20data%2C%20aiding%0Ain%20interpreting%20the%20model%27s%20decisions.%20This%20study%20highlights%20the%20potential%20of%0ADL%20and%20EEG%20in%20enhancing%20ADHD%20diagnosis%20accuracy%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADHDeepNet%2520From%2520Raw%2520EEG%2520to%2520Diagnosis%253A%2520Improving%2520ADHD%2520Diagnosis%2520through%250A%2520%2520Temporal-Spatial%2520Processing%252C%2520Adaptive%2520Attention%2520Mechanisms%252C%2520and%250A%2520%2520Explainability%2520in%2520Raw%2520EEG%2520Signals%26entry.906535625%3DAli%2520Amini%2520and%2520Mohammad%2520Alijanpour%2520and%2520Behnam%2520Latifi%2520and%2520Ali%2520Motie%2520Nasrabadi%26entry.1292438233%3D%2520%2520Attention%2520Deficit%2520Hyperactivity%2520Disorder%2520%2528ADHD%2529%2520is%2520a%2520common%2520brain%2520disorder%2520in%250Achildren%2520that%2520can%2520persist%2520into%2520adulthood%252C%2520affecting%2520social%252C%2520academic%252C%2520and%250Acareer%2520life.%2520Early%2520diagnosis%2520is%2520crucial%2520for%2520managing%2520these%2520impacts%2520on%2520patients%250Aand%2520the%2520healthcare%2520system%2520but%2520is%2520often%2520labor-intensive%2520and%2520time-consuming.%2520This%250Apaper%2520presents%2520a%2520novel%2520method%2520to%2520improve%2520ADHD%2520diagnosis%2520precision%2520and%250Atimeliness%2520by%2520leveraging%2520Deep%2520Learning%2520%2528DL%2529%2520approaches%2520and%2520electroencephalogram%250A%2528EEG%2529%2520signals.%2520We%2520introduce%2520ADHDeepNet%252C%2520a%2520DL%2520model%2520that%2520utilizes%2520comprehensive%250Atemporal-spatial%2520characterization%252C%2520attention%2520modules%252C%2520and%2520explainability%250Atechniques%2520optimized%2520for%2520EEG%2520signals.%2520ADHDeepNet%2520integrates%2520feature%2520extraction%250Aand%2520refinement%2520processes%2520to%2520enhance%2520ADHD%2520diagnosis.%2520The%2520model%2520was%2520trained%2520and%250Avalidated%2520on%2520a%2520dataset%2520of%2520121%2520participants%2520%252861%2520ADHD%252C%252060%2520Healthy%2520Controls%2529%252C%250Aemploying%2520nested%2520cross-validation%2520for%2520robust%2520performance.%2520The%2520proposed%250Atwo-stage%2520methodology%2520uses%2520a%252010-fold%2520cross-subject%2520validation%2520strategy.%250AInitially%252C%2520each%2520iteration%2520optimizes%2520the%2520model%2527s%2520hyper-parameters%2520with%2520inner%250A2-fold%2520cross-validation.%2520Then%252C%2520Additive%2520Gaussian%2520Noise%2520%2528AGN%2529%2520with%2520various%250Astandard%2520deviations%2520and%2520magnification%2520levels%2520is%2520applied%2520for%2520data%2520augmentation.%250AADHDeepNet%2520achieved%2520100%2525%2520sensitivity%2520and%252099.17%2525%2520accuracy%2520in%2520classifying%2520ADHD/HC%250Asubjects.%2520To%2520clarify%2520model%2520explainability%2520and%2520identify%2520key%2520brain%2520regions%2520and%250Afrequency%2520bands%2520for%2520ADHD%2520diagnosis%252C%2520we%2520analyzed%2520the%2520learned%2520weights%2520and%250Aactivation%2520patterns%2520of%2520the%2520model%2527s%2520primary%2520layers.%2520Additionally%252C%2520t-distributed%250AStochastic%2520Neighbor%2520Embedding%2520%2528t-SNE%2529%2520visualized%2520high-dimensional%2520data%252C%2520aiding%250Ain%2520interpreting%2520the%2520model%2527s%2520decisions.%2520This%2520study%2520highlights%2520the%2520potential%2520of%250ADL%2520and%2520EEG%2520in%2520enhancing%2520ADHD%2520diagnosis%2520accuracy%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADHDeepNet%20From%20Raw%20EEG%20to%20Diagnosis%3A%20Improving%20ADHD%20Diagnosis%20through%0A%20%20Temporal-Spatial%20Processing%2C%20Adaptive%20Attention%20Mechanisms%2C%20and%0A%20%20Explainability%20in%20Raw%20EEG%20Signals&entry.906535625=Ali%20Amini%20and%20Mohammad%20Alijanpour%20and%20Behnam%20Latifi%20and%20Ali%20Motie%20Nasrabadi&entry.1292438233=%20%20Attention%20Deficit%20Hyperactivity%20Disorder%20%28ADHD%29%20is%20a%20common%20brain%20disorder%20in%0Achildren%20that%20can%20persist%20into%20adulthood%2C%20affecting%20social%2C%20academic%2C%20and%0Acareer%20life.%20Early%20diagnosis%20is%20crucial%20for%20managing%20these%20impacts%20on%20patients%0Aand%20the%20healthcare%20system%20but%20is%20often%20labor-intensive%20and%20time-consuming.%20This%0Apaper%20presents%20a%20novel%20method%20to%20improve%20ADHD%20diagnosis%20precision%20and%0Atimeliness%20by%20leveraging%20Deep%20Learning%20%28DL%29%20approaches%20and%20electroencephalogram%0A%28EEG%29%20signals.%20We%20introduce%20ADHDeepNet%2C%20a%20DL%20model%20that%20utilizes%20comprehensive%0Atemporal-spatial%20characterization%2C%20attention%20modules%2C%20and%20explainability%0Atechniques%20optimized%20for%20EEG%20signals.%20ADHDeepNet%20integrates%20feature%20extraction%0Aand%20refinement%20processes%20to%20enhance%20ADHD%20diagnosis.%20The%20model%20was%20trained%20and%0Avalidated%20on%20a%20dataset%20of%20121%20participants%20%2861%20ADHD%2C%2060%20Healthy%20Controls%29%2C%0Aemploying%20nested%20cross-validation%20for%20robust%20performance.%20The%20proposed%0Atwo-stage%20methodology%20uses%20a%2010-fold%20cross-subject%20validation%20strategy.%0AInitially%2C%20each%20iteration%20optimizes%20the%20model%27s%20hyper-parameters%20with%20inner%0A2-fold%20cross-validation.%20Then%2C%20Additive%20Gaussian%20Noise%20%28AGN%29%20with%20various%0Astandard%20deviations%20and%20magnification%20levels%20is%20applied%20for%20data%20augmentation.%0AADHDeepNet%20achieved%20100%25%20sensitivity%20and%2099.17%25%20accuracy%20in%20classifying%20ADHD/HC%0Asubjects.%20To%20clarify%20model%20explainability%20and%20identify%20key%20brain%20regions%20and%0Afrequency%20bands%20for%20ADHD%20diagnosis%2C%20we%20analyzed%20the%20learned%20weights%20and%0Aactivation%20patterns%20of%20the%20model%27s%20primary%20layers.%20Additionally%2C%20t-distributed%0AStochastic%20Neighbor%20Embedding%20%28t-SNE%29%20visualized%20high-dimensional%20data%2C%20aiding%0Ain%20interpreting%20the%20model%27s%20decisions.%20This%20study%20highlights%20the%20potential%20of%0ADL%20and%20EEG%20in%20enhancing%20ADHD%20diagnosis%20accuracy%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08779v1&entry.124074799=Read"},
{"title": "One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human\n  Biases", "author": "Shalima Binta Manir and Tim Oates", "abstract": "  We introduce a novel Theory of Mind (ToM) framework inspired by dual-process\ntheories from cognitive science, integrating a fast, habitual graph-based\nreasoning system (System 1), implemented via graph convolutional networks\n(GCNs), and a slower, context-sensitive meta-adaptive learning system (System\n2), driven by meta-learning techniques. Our model dynamically balances\nintuitive and deliberative reasoning through a learned context gate mechanism.\nWe validate our architecture on canonical false-belief tasks and systematically\nexplore its capacity to replicate hallmark cognitive biases associated with\ndual-process theory, including anchoring, cognitive-load fatigue, framing\neffects, and priming effects. Experimental results demonstrate that our\ndual-process approach closely mirrors human adaptive behavior, achieves robust\ngeneralization to unseen contexts, and elucidates cognitive mechanisms\nunderlying reasoning biases. This work bridges artificial intelligence and\ncognitive theory, paving the way for AI systems exhibiting nuanced, human-like\nsocial cognition and adaptive decision-making capabilities.\n", "link": "http://arxiv.org/abs/2509.08705v1", "date": "2025-09-10", "relevancy": 2.0908, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5156}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Model%2C%20Two%20Minds%3A%20A%20Context-Gated%20Graph%20Learner%20that%20Recreates%20Human%0A%20%20Biases&body=Title%3A%20One%20Model%2C%20Two%20Minds%3A%20A%20Context-Gated%20Graph%20Learner%20that%20Recreates%20Human%0A%20%20Biases%0AAuthor%3A%20Shalima%20Binta%20Manir%20and%20Tim%20Oates%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20Theory%20of%20Mind%20%28ToM%29%20framework%20inspired%20by%20dual-process%0Atheories%20from%20cognitive%20science%2C%20integrating%20a%20fast%2C%20habitual%20graph-based%0Areasoning%20system%20%28System%201%29%2C%20implemented%20via%20graph%20convolutional%20networks%0A%28GCNs%29%2C%20and%20a%20slower%2C%20context-sensitive%20meta-adaptive%20learning%20system%20%28System%0A2%29%2C%20driven%20by%20meta-learning%20techniques.%20Our%20model%20dynamically%20balances%0Aintuitive%20and%20deliberative%20reasoning%20through%20a%20learned%20context%20gate%20mechanism.%0AWe%20validate%20our%20architecture%20on%20canonical%20false-belief%20tasks%20and%20systematically%0Aexplore%20its%20capacity%20to%20replicate%20hallmark%20cognitive%20biases%20associated%20with%0Adual-process%20theory%2C%20including%20anchoring%2C%20cognitive-load%20fatigue%2C%20framing%0Aeffects%2C%20and%20priming%20effects.%20Experimental%20results%20demonstrate%20that%20our%0Adual-process%20approach%20closely%20mirrors%20human%20adaptive%20behavior%2C%20achieves%20robust%0Ageneralization%20to%20unseen%20contexts%2C%20and%20elucidates%20cognitive%20mechanisms%0Aunderlying%20reasoning%20biases.%20This%20work%20bridges%20artificial%20intelligence%20and%0Acognitive%20theory%2C%20paving%20the%20way%20for%20AI%20systems%20exhibiting%20nuanced%2C%20human-like%0Asocial%20cognition%20and%20adaptive%20decision-making%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Model%252C%2520Two%2520Minds%253A%2520A%2520Context-Gated%2520Graph%2520Learner%2520that%2520Recreates%2520Human%250A%2520%2520Biases%26entry.906535625%3DShalima%2520Binta%2520Manir%2520and%2520Tim%2520Oates%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520Theory%2520of%2520Mind%2520%2528ToM%2529%2520framework%2520inspired%2520by%2520dual-process%250Atheories%2520from%2520cognitive%2520science%252C%2520integrating%2520a%2520fast%252C%2520habitual%2520graph-based%250Areasoning%2520system%2520%2528System%25201%2529%252C%2520implemented%2520via%2520graph%2520convolutional%2520networks%250A%2528GCNs%2529%252C%2520and%2520a%2520slower%252C%2520context-sensitive%2520meta-adaptive%2520learning%2520system%2520%2528System%250A2%2529%252C%2520driven%2520by%2520meta-learning%2520techniques.%2520Our%2520model%2520dynamically%2520balances%250Aintuitive%2520and%2520deliberative%2520reasoning%2520through%2520a%2520learned%2520context%2520gate%2520mechanism.%250AWe%2520validate%2520our%2520architecture%2520on%2520canonical%2520false-belief%2520tasks%2520and%2520systematically%250Aexplore%2520its%2520capacity%2520to%2520replicate%2520hallmark%2520cognitive%2520biases%2520associated%2520with%250Adual-process%2520theory%252C%2520including%2520anchoring%252C%2520cognitive-load%2520fatigue%252C%2520framing%250Aeffects%252C%2520and%2520priming%2520effects.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Adual-process%2520approach%2520closely%2520mirrors%2520human%2520adaptive%2520behavior%252C%2520achieves%2520robust%250Ageneralization%2520to%2520unseen%2520contexts%252C%2520and%2520elucidates%2520cognitive%2520mechanisms%250Aunderlying%2520reasoning%2520biases.%2520This%2520work%2520bridges%2520artificial%2520intelligence%2520and%250Acognitive%2520theory%252C%2520paving%2520the%2520way%2520for%2520AI%2520systems%2520exhibiting%2520nuanced%252C%2520human-like%250Asocial%2520cognition%2520and%2520adaptive%2520decision-making%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Model%2C%20Two%20Minds%3A%20A%20Context-Gated%20Graph%20Learner%20that%20Recreates%20Human%0A%20%20Biases&entry.906535625=Shalima%20Binta%20Manir%20and%20Tim%20Oates&entry.1292438233=%20%20We%20introduce%20a%20novel%20Theory%20of%20Mind%20%28ToM%29%20framework%20inspired%20by%20dual-process%0Atheories%20from%20cognitive%20science%2C%20integrating%20a%20fast%2C%20habitual%20graph-based%0Areasoning%20system%20%28System%201%29%2C%20implemented%20via%20graph%20convolutional%20networks%0A%28GCNs%29%2C%20and%20a%20slower%2C%20context-sensitive%20meta-adaptive%20learning%20system%20%28System%0A2%29%2C%20driven%20by%20meta-learning%20techniques.%20Our%20model%20dynamically%20balances%0Aintuitive%20and%20deliberative%20reasoning%20through%20a%20learned%20context%20gate%20mechanism.%0AWe%20validate%20our%20architecture%20on%20canonical%20false-belief%20tasks%20and%20systematically%0Aexplore%20its%20capacity%20to%20replicate%20hallmark%20cognitive%20biases%20associated%20with%0Adual-process%20theory%2C%20including%20anchoring%2C%20cognitive-load%20fatigue%2C%20framing%0Aeffects%2C%20and%20priming%20effects.%20Experimental%20results%20demonstrate%20that%20our%0Adual-process%20approach%20closely%20mirrors%20human%20adaptive%20behavior%2C%20achieves%20robust%0Ageneralization%20to%20unseen%20contexts%2C%20and%20elucidates%20cognitive%20mechanisms%0Aunderlying%20reasoning%20biases.%20This%20work%20bridges%20artificial%20intelligence%20and%0Acognitive%20theory%2C%20paving%20the%20way%20for%20AI%20systems%20exhibiting%20nuanced%2C%20human-like%0Asocial%20cognition%20and%20adaptive%20decision-making%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08705v1&entry.124074799=Read"},
{"title": "GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward\n  Reasoning", "author": "Chenglong Wang and Yongyu Mu and Hang Zhou and Yifu Huo and Ziming Zhu and Jiali Zeng and Murun Yang and Bei Li and Tong Xiao and Xiaoyang Hao and Chunliang Zhang and Fandong Meng and Jingbo Zhu", "abstract": "  Significant progress in reward modeling over recent years has been driven by\na paradigm shift from task-specific designs towards generalist reward models.\nDespite this trend, developing effective reward models remains a fundamental\nchallenge: the heavy reliance on large-scale labeled preference data.\nPre-training on abundant unlabeled data offers a promising direction, but\nexisting approaches fall short of instilling explicit reasoning into reward\nmodels. To bridge this gap, we propose a self-training approach that leverages\nunlabeled data to elicit reward reasoning in reward models. Based on this\napproach, we develop GRAM-R$^2$, a generative reward model trained to produce\nnot only preference labels but also accompanying reward rationales. GRAM-R$^2$\ncan serve as a foundation model for reward reasoning and can be applied to a\nwide range of tasks with minimal or no additional fine-tuning. It can support\ndownstream applications such as response ranking and task-specific reward\ntuning. Experiments on response ranking, task adaptation, and reinforcement\nlearning from human feedback demonstrate that GRAM-R$^2$ consistently delivers\nstrong performance, outperforming several strong discriminative and generative\nbaselines.\n", "link": "http://arxiv.org/abs/2509.02492v2", "date": "2025-09-10", "relevancy": 2.0828, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5455}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5097}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAM-R%24%5E2%24%3A%20Self-Training%20Generative%20Foundation%20Reward%20Models%20for%20Reward%0A%20%20Reasoning&body=Title%3A%20GRAM-R%24%5E2%24%3A%20Self-Training%20Generative%20Foundation%20Reward%20Models%20for%20Reward%0A%20%20Reasoning%0AAuthor%3A%20Chenglong%20Wang%20and%20Yongyu%20Mu%20and%20Hang%20Zhou%20and%20Yifu%20Huo%20and%20Ziming%20Zhu%20and%20Jiali%20Zeng%20and%20Murun%20Yang%20and%20Bei%20Li%20and%20Tong%20Xiao%20and%20Xiaoyang%20Hao%20and%20Chunliang%20Zhang%20and%20Fandong%20Meng%20and%20Jingbo%20Zhu%0AAbstract%3A%20%20%20Significant%20progress%20in%20reward%20modeling%20over%20recent%20years%20has%20been%20driven%20by%0Aa%20paradigm%20shift%20from%20task-specific%20designs%20towards%20generalist%20reward%20models.%0ADespite%20this%20trend%2C%20developing%20effective%20reward%20models%20remains%20a%20fundamental%0Achallenge%3A%20the%20heavy%20reliance%20on%20large-scale%20labeled%20preference%20data.%0APre-training%20on%20abundant%20unlabeled%20data%20offers%20a%20promising%20direction%2C%20but%0Aexisting%20approaches%20fall%20short%20of%20instilling%20explicit%20reasoning%20into%20reward%0Amodels.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20self-training%20approach%20that%20leverages%0Aunlabeled%20data%20to%20elicit%20reward%20reasoning%20in%20reward%20models.%20Based%20on%20this%0Aapproach%2C%20we%20develop%20GRAM-R%24%5E2%24%2C%20a%20generative%20reward%20model%20trained%20to%20produce%0Anot%20only%20preference%20labels%20but%20also%20accompanying%20reward%20rationales.%20GRAM-R%24%5E2%24%0Acan%20serve%20as%20a%20foundation%20model%20for%20reward%20reasoning%20and%20can%20be%20applied%20to%20a%0Awide%20range%20of%20tasks%20with%20minimal%20or%20no%20additional%20fine-tuning.%20It%20can%20support%0Adownstream%20applications%20such%20as%20response%20ranking%20and%20task-specific%20reward%0Atuning.%20Experiments%20on%20response%20ranking%2C%20task%20adaptation%2C%20and%20reinforcement%0Alearning%20from%20human%20feedback%20demonstrate%20that%20GRAM-R%24%5E2%24%20consistently%20delivers%0Astrong%20performance%2C%20outperforming%20several%20strong%20discriminative%20and%20generative%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAM-R%2524%255E2%2524%253A%2520Self-Training%2520Generative%2520Foundation%2520Reward%2520Models%2520for%2520Reward%250A%2520%2520Reasoning%26entry.906535625%3DChenglong%2520Wang%2520and%2520Yongyu%2520Mu%2520and%2520Hang%2520Zhou%2520and%2520Yifu%2520Huo%2520and%2520Ziming%2520Zhu%2520and%2520Jiali%2520Zeng%2520and%2520Murun%2520Yang%2520and%2520Bei%2520Li%2520and%2520Tong%2520Xiao%2520and%2520Xiaoyang%2520Hao%2520and%2520Chunliang%2520Zhang%2520and%2520Fandong%2520Meng%2520and%2520Jingbo%2520Zhu%26entry.1292438233%3D%2520%2520Significant%2520progress%2520in%2520reward%2520modeling%2520over%2520recent%2520years%2520has%2520been%2520driven%2520by%250Aa%2520paradigm%2520shift%2520from%2520task-specific%2520designs%2520towards%2520generalist%2520reward%2520models.%250ADespite%2520this%2520trend%252C%2520developing%2520effective%2520reward%2520models%2520remains%2520a%2520fundamental%250Achallenge%253A%2520the%2520heavy%2520reliance%2520on%2520large-scale%2520labeled%2520preference%2520data.%250APre-training%2520on%2520abundant%2520unlabeled%2520data%2520offers%2520a%2520promising%2520direction%252C%2520but%250Aexisting%2520approaches%2520fall%2520short%2520of%2520instilling%2520explicit%2520reasoning%2520into%2520reward%250Amodels.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520self-training%2520approach%2520that%2520leverages%250Aunlabeled%2520data%2520to%2520elicit%2520reward%2520reasoning%2520in%2520reward%2520models.%2520Based%2520on%2520this%250Aapproach%252C%2520we%2520develop%2520GRAM-R%2524%255E2%2524%252C%2520a%2520generative%2520reward%2520model%2520trained%2520to%2520produce%250Anot%2520only%2520preference%2520labels%2520but%2520also%2520accompanying%2520reward%2520rationales.%2520GRAM-R%2524%255E2%2524%250Acan%2520serve%2520as%2520a%2520foundation%2520model%2520for%2520reward%2520reasoning%2520and%2520can%2520be%2520applied%2520to%2520a%250Awide%2520range%2520of%2520tasks%2520with%2520minimal%2520or%2520no%2520additional%2520fine-tuning.%2520It%2520can%2520support%250Adownstream%2520applications%2520such%2520as%2520response%2520ranking%2520and%2520task-specific%2520reward%250Atuning.%2520Experiments%2520on%2520response%2520ranking%252C%2520task%2520adaptation%252C%2520and%2520reinforcement%250Alearning%2520from%2520human%2520feedback%2520demonstrate%2520that%2520GRAM-R%2524%255E2%2524%2520consistently%2520delivers%250Astrong%2520performance%252C%2520outperforming%2520several%2520strong%2520discriminative%2520and%2520generative%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAM-R%24%5E2%24%3A%20Self-Training%20Generative%20Foundation%20Reward%20Models%20for%20Reward%0A%20%20Reasoning&entry.906535625=Chenglong%20Wang%20and%20Yongyu%20Mu%20and%20Hang%20Zhou%20and%20Yifu%20Huo%20and%20Ziming%20Zhu%20and%20Jiali%20Zeng%20and%20Murun%20Yang%20and%20Bei%20Li%20and%20Tong%20Xiao%20and%20Xiaoyang%20Hao%20and%20Chunliang%20Zhang%20and%20Fandong%20Meng%20and%20Jingbo%20Zhu&entry.1292438233=%20%20Significant%20progress%20in%20reward%20modeling%20over%20recent%20years%20has%20been%20driven%20by%0Aa%20paradigm%20shift%20from%20task-specific%20designs%20towards%20generalist%20reward%20models.%0ADespite%20this%20trend%2C%20developing%20effective%20reward%20models%20remains%20a%20fundamental%0Achallenge%3A%20the%20heavy%20reliance%20on%20large-scale%20labeled%20preference%20data.%0APre-training%20on%20abundant%20unlabeled%20data%20offers%20a%20promising%20direction%2C%20but%0Aexisting%20approaches%20fall%20short%20of%20instilling%20explicit%20reasoning%20into%20reward%0Amodels.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20self-training%20approach%20that%20leverages%0Aunlabeled%20data%20to%20elicit%20reward%20reasoning%20in%20reward%20models.%20Based%20on%20this%0Aapproach%2C%20we%20develop%20GRAM-R%24%5E2%24%2C%20a%20generative%20reward%20model%20trained%20to%20produce%0Anot%20only%20preference%20labels%20but%20also%20accompanying%20reward%20rationales.%20GRAM-R%24%5E2%24%0Acan%20serve%20as%20a%20foundation%20model%20for%20reward%20reasoning%20and%20can%20be%20applied%20to%20a%0Awide%20range%20of%20tasks%20with%20minimal%20or%20no%20additional%20fine-tuning.%20It%20can%20support%0Adownstream%20applications%20such%20as%20response%20ranking%20and%20task-specific%20reward%0Atuning.%20Experiments%20on%20response%20ranking%2C%20task%20adaptation%2C%20and%20reinforcement%0Alearning%20from%20human%20feedback%20demonstrate%20that%20GRAM-R%24%5E2%24%20consistently%20delivers%0Astrong%20performance%2C%20outperforming%20several%20strong%20discriminative%20and%20generative%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02492v2&entry.124074799=Read"},
{"title": "A Survey of Reinforcement Learning for Large Reasoning Models", "author": "Kaiyan Zhang and Yuxin Zuo and Bingxiang He and Youbang Sun and Runze Liu and Che Jiang and Yuchen Fan and Kai Tian and Guoli Jia and Pengfei Li and Yu Fu and Xingtai Lv and Yuchen Zhang and Sihang Zeng and Shang Qu and Haozhan Li and Shijie Wang and Yuru Wang and Xinwei Long and Fangfu Liu and Xiang Xu and Jiaze Ma and Xuekai Zhu and Ermo Hua and Yihao Liu and Zonglin Li and Huayu Chen and Xiaoye Qu and Yafu Li and Weize Chen and Zhenzhao Yuan and Junqi Gao and Dong Li and Zhiyuan Ma and Ganqu Cui and Zhiyuan Liu and Biqing Qi and Ning Ding and Bowen Zhou", "abstract": "  In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs\n", "link": "http://arxiv.org/abs/2509.08827v1", "date": "2025-09-10", "relevancy": 2.0545, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Reinforcement%20Learning%20for%20Large%20Reasoning%20Models&body=Title%3A%20A%20Survey%20of%20Reinforcement%20Learning%20for%20Large%20Reasoning%20Models%0AAuthor%3A%20Kaiyan%20Zhang%20and%20Yuxin%20Zuo%20and%20Bingxiang%20He%20and%20Youbang%20Sun%20and%20Runze%20Liu%20and%20Che%20Jiang%20and%20Yuchen%20Fan%20and%20Kai%20Tian%20and%20Guoli%20Jia%20and%20Pengfei%20Li%20and%20Yu%20Fu%20and%20Xingtai%20Lv%20and%20Yuchen%20Zhang%20and%20Sihang%20Zeng%20and%20Shang%20Qu%20and%20Haozhan%20Li%20and%20Shijie%20Wang%20and%20Yuru%20Wang%20and%20Xinwei%20Long%20and%20Fangfu%20Liu%20and%20Xiang%20Xu%20and%20Jiaze%20Ma%20and%20Xuekai%20Zhu%20and%20Ermo%20Hua%20and%20Yihao%20Liu%20and%20Zonglin%20Li%20and%20Huayu%20Chen%20and%20Xiaoye%20Qu%20and%20Yafu%20Li%20and%20Weize%20Chen%20and%20Zhenzhao%20Yuan%20and%20Junqi%20Gao%20and%20Dong%20Li%20and%20Zhiyuan%20Ma%20and%20Ganqu%20Cui%20and%20Zhiyuan%20Liu%20and%20Biqing%20Qi%20and%20Ning%20Ding%20and%20Bowen%20Zhou%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20survey%20recent%20advances%20in%20Reinforcement%20Learning%20%28RL%29%20for%0Areasoning%20with%20Large%20Language%20Models%20%28LLMs%29.%20RL%20has%20achieved%20remarkable%20success%0Ain%20advancing%20the%20frontier%20of%20LLM%20capabilities%2C%20particularly%20in%20addressing%0Acomplex%20logical%20tasks%20such%20as%20mathematics%20and%20coding.%20As%20a%20result%2C%20RL%20has%0Aemerged%20as%20a%20foundational%20methodology%20for%20transforming%20LLMs%20into%20LRMs.%20With%20the%0Arapid%20progress%20of%20the%20field%2C%20further%20scaling%20of%20RL%20for%20LRMs%20now%20faces%0Afoundational%20challenges%20not%20only%20in%20computational%20resources%20but%20also%20in%0Aalgorithm%20design%2C%20training%20data%2C%20and%20infrastructure.%20To%20this%20end%2C%20it%20is%20timely%0Ato%20revisit%20the%20development%20of%20this%20domain%2C%20reassess%20its%20trajectory%2C%20and%20explore%0Astrategies%20to%20enhance%20the%20scalability%20of%20RL%20toward%20Artificial%20SuperIntelligence%0A%28ASI%29.%20In%20particular%2C%20we%20examine%20research%20applying%20RL%20to%20LLMs%20and%20LRMs%20for%0Areasoning%20abilities%2C%20especially%20since%20the%20release%20of%20DeepSeek-R1%2C%20including%0Afoundational%20components%2C%20core%20problems%2C%20training%20resources%2C%20and%20downstream%0Aapplications%2C%20to%20identify%20future%20opportunities%20and%20directions%20for%20this%20rapidly%0Aevolving%20area.%20We%20hope%20this%20review%20will%20promote%20future%20research%20on%20RL%20for%0Abroader%20reasoning%20models.%20Github%3A%0Ahttps%3A//github.com/TsinghuaC3I/Awesome-RL-for-LRMs%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Reinforcement%2520Learning%2520for%2520Large%2520Reasoning%2520Models%26entry.906535625%3DKaiyan%2520Zhang%2520and%2520Yuxin%2520Zuo%2520and%2520Bingxiang%2520He%2520and%2520Youbang%2520Sun%2520and%2520Runze%2520Liu%2520and%2520Che%2520Jiang%2520and%2520Yuchen%2520Fan%2520and%2520Kai%2520Tian%2520and%2520Guoli%2520Jia%2520and%2520Pengfei%2520Li%2520and%2520Yu%2520Fu%2520and%2520Xingtai%2520Lv%2520and%2520Yuchen%2520Zhang%2520and%2520Sihang%2520Zeng%2520and%2520Shang%2520Qu%2520and%2520Haozhan%2520Li%2520and%2520Shijie%2520Wang%2520and%2520Yuru%2520Wang%2520and%2520Xinwei%2520Long%2520and%2520Fangfu%2520Liu%2520and%2520Xiang%2520Xu%2520and%2520Jiaze%2520Ma%2520and%2520Xuekai%2520Zhu%2520and%2520Ermo%2520Hua%2520and%2520Yihao%2520Liu%2520and%2520Zonglin%2520Li%2520and%2520Huayu%2520Chen%2520and%2520Xiaoye%2520Qu%2520and%2520Yafu%2520Li%2520and%2520Weize%2520Chen%2520and%2520Zhenzhao%2520Yuan%2520and%2520Junqi%2520Gao%2520and%2520Dong%2520Li%2520and%2520Zhiyuan%2520Ma%2520and%2520Ganqu%2520Cui%2520and%2520Zhiyuan%2520Liu%2520and%2520Biqing%2520Qi%2520and%2520Ning%2520Ding%2520and%2520Bowen%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520survey%2520recent%2520advances%2520in%2520Reinforcement%2520Learning%2520%2528RL%2529%2520for%250Areasoning%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520RL%2520has%2520achieved%2520remarkable%2520success%250Ain%2520advancing%2520the%2520frontier%2520of%2520LLM%2520capabilities%252C%2520particularly%2520in%2520addressing%250Acomplex%2520logical%2520tasks%2520such%2520as%2520mathematics%2520and%2520coding.%2520As%2520a%2520result%252C%2520RL%2520has%250Aemerged%2520as%2520a%2520foundational%2520methodology%2520for%2520transforming%2520LLMs%2520into%2520LRMs.%2520With%2520the%250Arapid%2520progress%2520of%2520the%2520field%252C%2520further%2520scaling%2520of%2520RL%2520for%2520LRMs%2520now%2520faces%250Afoundational%2520challenges%2520not%2520only%2520in%2520computational%2520resources%2520but%2520also%2520in%250Aalgorithm%2520design%252C%2520training%2520data%252C%2520and%2520infrastructure.%2520To%2520this%2520end%252C%2520it%2520is%2520timely%250Ato%2520revisit%2520the%2520development%2520of%2520this%2520domain%252C%2520reassess%2520its%2520trajectory%252C%2520and%2520explore%250Astrategies%2520to%2520enhance%2520the%2520scalability%2520of%2520RL%2520toward%2520Artificial%2520SuperIntelligence%250A%2528ASI%2529.%2520In%2520particular%252C%2520we%2520examine%2520research%2520applying%2520RL%2520to%2520LLMs%2520and%2520LRMs%2520for%250Areasoning%2520abilities%252C%2520especially%2520since%2520the%2520release%2520of%2520DeepSeek-R1%252C%2520including%250Afoundational%2520components%252C%2520core%2520problems%252C%2520training%2520resources%252C%2520and%2520downstream%250Aapplications%252C%2520to%2520identify%2520future%2520opportunities%2520and%2520directions%2520for%2520this%2520rapidly%250Aevolving%2520area.%2520We%2520hope%2520this%2520review%2520will%2520promote%2520future%2520research%2520on%2520RL%2520for%250Abroader%2520reasoning%2520models.%2520Github%253A%250Ahttps%253A//github.com/TsinghuaC3I/Awesome-RL-for-LRMs%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Reinforcement%20Learning%20for%20Large%20Reasoning%20Models&entry.906535625=Kaiyan%20Zhang%20and%20Yuxin%20Zuo%20and%20Bingxiang%20He%20and%20Youbang%20Sun%20and%20Runze%20Liu%20and%20Che%20Jiang%20and%20Yuchen%20Fan%20and%20Kai%20Tian%20and%20Guoli%20Jia%20and%20Pengfei%20Li%20and%20Yu%20Fu%20and%20Xingtai%20Lv%20and%20Yuchen%20Zhang%20and%20Sihang%20Zeng%20and%20Shang%20Qu%20and%20Haozhan%20Li%20and%20Shijie%20Wang%20and%20Yuru%20Wang%20and%20Xinwei%20Long%20and%20Fangfu%20Liu%20and%20Xiang%20Xu%20and%20Jiaze%20Ma%20and%20Xuekai%20Zhu%20and%20Ermo%20Hua%20and%20Yihao%20Liu%20and%20Zonglin%20Li%20and%20Huayu%20Chen%20and%20Xiaoye%20Qu%20and%20Yafu%20Li%20and%20Weize%20Chen%20and%20Zhenzhao%20Yuan%20and%20Junqi%20Gao%20and%20Dong%20Li%20and%20Zhiyuan%20Ma%20and%20Ganqu%20Cui%20and%20Zhiyuan%20Liu%20and%20Biqing%20Qi%20and%20Ning%20Ding%20and%20Bowen%20Zhou&entry.1292438233=%20%20In%20this%20paper%2C%20we%20survey%20recent%20advances%20in%20Reinforcement%20Learning%20%28RL%29%20for%0Areasoning%20with%20Large%20Language%20Models%20%28LLMs%29.%20RL%20has%20achieved%20remarkable%20success%0Ain%20advancing%20the%20frontier%20of%20LLM%20capabilities%2C%20particularly%20in%20addressing%0Acomplex%20logical%20tasks%20such%20as%20mathematics%20and%20coding.%20As%20a%20result%2C%20RL%20has%0Aemerged%20as%20a%20foundational%20methodology%20for%20transforming%20LLMs%20into%20LRMs.%20With%20the%0Arapid%20progress%20of%20the%20field%2C%20further%20scaling%20of%20RL%20for%20LRMs%20now%20faces%0Afoundational%20challenges%20not%20only%20in%20computational%20resources%20but%20also%20in%0Aalgorithm%20design%2C%20training%20data%2C%20and%20infrastructure.%20To%20this%20end%2C%20it%20is%20timely%0Ato%20revisit%20the%20development%20of%20this%20domain%2C%20reassess%20its%20trajectory%2C%20and%20explore%0Astrategies%20to%20enhance%20the%20scalability%20of%20RL%20toward%20Artificial%20SuperIntelligence%0A%28ASI%29.%20In%20particular%2C%20we%20examine%20research%20applying%20RL%20to%20LLMs%20and%20LRMs%20for%0Areasoning%20abilities%2C%20especially%20since%20the%20release%20of%20DeepSeek-R1%2C%20including%0Afoundational%20components%2C%20core%20problems%2C%20training%20resources%2C%20and%20downstream%0Aapplications%2C%20to%20identify%20future%20opportunities%20and%20directions%20for%20this%20rapidly%0Aevolving%20area.%20We%20hope%20this%20review%20will%20promote%20future%20research%20on%20RL%20for%0Abroader%20reasoning%20models.%20Github%3A%0Ahttps%3A//github.com/TsinghuaC3I/Awesome-RL-for-LRMs%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08827v1&entry.124074799=Read"},
{"title": "Investigating Compositional Reasoning in Time Series Foundation Models", "author": "Willa Potosnak and Cristian Challu and Mononito Goswami and Kin G. Olivares and Micha\u0142 Wili\u0144ski and Nina \u017bukowska and Artur Dubrawski", "abstract": "  Large pre-trained time series foundation models (TSFMs) have demonstrated\npromising zero-shot performance across a wide range of domains. However, a\nquestion remains: Do TSFMs succeed by memorizing patterns in training data, or\ndo they possess the ability to reason about such patterns? While reasoning is a\ntopic of great interest in the study of Large Language Models (LLMs), it is\nundefined and largely unexplored in the context of TSFMs. In this work,\ninspired by language modeling literature, we formally define compositional\nreasoning in forecasting and distinguish it from in-distribution\ngeneralization. We evaluate the reasoning and generalization capabilities of 16\npopular deep learning forecasting models on multiple synthetic and real-world\ndatasets. Additionally, through controlled studies, we systematically examine\nwhich design choices in 7 popular open-source TSFMs contribute to improved\nreasoning capabilities. Our study yields key insights into the impact of TSFM\narchitecture design on compositional reasoning and generalization. We find that\npatch-based Transformers have the best reasoning performance, closely followed\nby residualized MLP-based architectures, which are 97\\% less computationally\ncomplex in terms of FLOPs and 86\\% smaller in terms of the number of trainable\nparameters. Interestingly, in some zero-shot out-of-distribution scenarios,\nthese models can outperform moving average and exponential smoothing\nstatistical baselines trained on in-distribution data. Only a few design\nchoices, such as the tokenization method, had a significant (negative) impact\non Transformer model performance.\n", "link": "http://arxiv.org/abs/2502.06037v2", "date": "2025-09-10", "relevancy": 2.0523, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Compositional%20Reasoning%20in%20Time%20Series%20Foundation%20Models&body=Title%3A%20Investigating%20Compositional%20Reasoning%20in%20Time%20Series%20Foundation%20Models%0AAuthor%3A%20Willa%20Potosnak%20and%20Cristian%20Challu%20and%20Mononito%20Goswami%20and%20Kin%20G.%20Olivares%20and%20Micha%C5%82%20Wili%C5%84ski%20and%20Nina%20%C5%BBukowska%20and%20Artur%20Dubrawski%0AAbstract%3A%20%20%20Large%20pre-trained%20time%20series%20foundation%20models%20%28TSFMs%29%20have%20demonstrated%0Apromising%20zero-shot%20performance%20across%20a%20wide%20range%20of%20domains.%20However%2C%20a%0Aquestion%20remains%3A%20Do%20TSFMs%20succeed%20by%20memorizing%20patterns%20in%20training%20data%2C%20or%0Ado%20they%20possess%20the%20ability%20to%20reason%20about%20such%20patterns%3F%20While%20reasoning%20is%20a%0Atopic%20of%20great%20interest%20in%20the%20study%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20it%20is%0Aundefined%20and%20largely%20unexplored%20in%20the%20context%20of%20TSFMs.%20In%20this%20work%2C%0Ainspired%20by%20language%20modeling%20literature%2C%20we%20formally%20define%20compositional%0Areasoning%20in%20forecasting%20and%20distinguish%20it%20from%20in-distribution%0Ageneralization.%20We%20evaluate%20the%20reasoning%20and%20generalization%20capabilities%20of%2016%0Apopular%20deep%20learning%20forecasting%20models%20on%20multiple%20synthetic%20and%20real-world%0Adatasets.%20Additionally%2C%20through%20controlled%20studies%2C%20we%20systematically%20examine%0Awhich%20design%20choices%20in%207%20popular%20open-source%20TSFMs%20contribute%20to%20improved%0Areasoning%20capabilities.%20Our%20study%20yields%20key%20insights%20into%20the%20impact%20of%20TSFM%0Aarchitecture%20design%20on%20compositional%20reasoning%20and%20generalization.%20We%20find%20that%0Apatch-based%20Transformers%20have%20the%20best%20reasoning%20performance%2C%20closely%20followed%0Aby%20residualized%20MLP-based%20architectures%2C%20which%20are%2097%5C%25%20less%20computationally%0Acomplex%20in%20terms%20of%20FLOPs%20and%2086%5C%25%20smaller%20in%20terms%20of%20the%20number%20of%20trainable%0Aparameters.%20Interestingly%2C%20in%20some%20zero-shot%20out-of-distribution%20scenarios%2C%0Athese%20models%20can%20outperform%20moving%20average%20and%20exponential%20smoothing%0Astatistical%20baselines%20trained%20on%20in-distribution%20data.%20Only%20a%20few%20design%0Achoices%2C%20such%20as%20the%20tokenization%20method%2C%20had%20a%20significant%20%28negative%29%20impact%0Aon%20Transformer%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Compositional%2520Reasoning%2520in%2520Time%2520Series%2520Foundation%2520Models%26entry.906535625%3DWilla%2520Potosnak%2520and%2520Cristian%2520Challu%2520and%2520Mononito%2520Goswami%2520and%2520Kin%2520G.%2520Olivares%2520and%2520Micha%25C5%2582%2520Wili%25C5%2584ski%2520and%2520Nina%2520%25C5%25BBukowska%2520and%2520Artur%2520Dubrawski%26entry.1292438233%3D%2520%2520Large%2520pre-trained%2520time%2520series%2520foundation%2520models%2520%2528TSFMs%2529%2520have%2520demonstrated%250Apromising%2520zero-shot%2520performance%2520across%2520a%2520wide%2520range%2520of%2520domains.%2520However%252C%2520a%250Aquestion%2520remains%253A%2520Do%2520TSFMs%2520succeed%2520by%2520memorizing%2520patterns%2520in%2520training%2520data%252C%2520or%250Ado%2520they%2520possess%2520the%2520ability%2520to%2520reason%2520about%2520such%2520patterns%253F%2520While%2520reasoning%2520is%2520a%250Atopic%2520of%2520great%2520interest%2520in%2520the%2520study%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520it%2520is%250Aundefined%2520and%2520largely%2520unexplored%2520in%2520the%2520context%2520of%2520TSFMs.%2520In%2520this%2520work%252C%250Ainspired%2520by%2520language%2520modeling%2520literature%252C%2520we%2520formally%2520define%2520compositional%250Areasoning%2520in%2520forecasting%2520and%2520distinguish%2520it%2520from%2520in-distribution%250Ageneralization.%2520We%2520evaluate%2520the%2520reasoning%2520and%2520generalization%2520capabilities%2520of%252016%250Apopular%2520deep%2520learning%2520forecasting%2520models%2520on%2520multiple%2520synthetic%2520and%2520real-world%250Adatasets.%2520Additionally%252C%2520through%2520controlled%2520studies%252C%2520we%2520systematically%2520examine%250Awhich%2520design%2520choices%2520in%25207%2520popular%2520open-source%2520TSFMs%2520contribute%2520to%2520improved%250Areasoning%2520capabilities.%2520Our%2520study%2520yields%2520key%2520insights%2520into%2520the%2520impact%2520of%2520TSFM%250Aarchitecture%2520design%2520on%2520compositional%2520reasoning%2520and%2520generalization.%2520We%2520find%2520that%250Apatch-based%2520Transformers%2520have%2520the%2520best%2520reasoning%2520performance%252C%2520closely%2520followed%250Aby%2520residualized%2520MLP-based%2520architectures%252C%2520which%2520are%252097%255C%2525%2520less%2520computationally%250Acomplex%2520in%2520terms%2520of%2520FLOPs%2520and%252086%255C%2525%2520smaller%2520in%2520terms%2520of%2520the%2520number%2520of%2520trainable%250Aparameters.%2520Interestingly%252C%2520in%2520some%2520zero-shot%2520out-of-distribution%2520scenarios%252C%250Athese%2520models%2520can%2520outperform%2520moving%2520average%2520and%2520exponential%2520smoothing%250Astatistical%2520baselines%2520trained%2520on%2520in-distribution%2520data.%2520Only%2520a%2520few%2520design%250Achoices%252C%2520such%2520as%2520the%2520tokenization%2520method%252C%2520had%2520a%2520significant%2520%2528negative%2529%2520impact%250Aon%2520Transformer%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Compositional%20Reasoning%20in%20Time%20Series%20Foundation%20Models&entry.906535625=Willa%20Potosnak%20and%20Cristian%20Challu%20and%20Mononito%20Goswami%20and%20Kin%20G.%20Olivares%20and%20Micha%C5%82%20Wili%C5%84ski%20and%20Nina%20%C5%BBukowska%20and%20Artur%20Dubrawski&entry.1292438233=%20%20Large%20pre-trained%20time%20series%20foundation%20models%20%28TSFMs%29%20have%20demonstrated%0Apromising%20zero-shot%20performance%20across%20a%20wide%20range%20of%20domains.%20However%2C%20a%0Aquestion%20remains%3A%20Do%20TSFMs%20succeed%20by%20memorizing%20patterns%20in%20training%20data%2C%20or%0Ado%20they%20possess%20the%20ability%20to%20reason%20about%20such%20patterns%3F%20While%20reasoning%20is%20a%0Atopic%20of%20great%20interest%20in%20the%20study%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20it%20is%0Aundefined%20and%20largely%20unexplored%20in%20the%20context%20of%20TSFMs.%20In%20this%20work%2C%0Ainspired%20by%20language%20modeling%20literature%2C%20we%20formally%20define%20compositional%0Areasoning%20in%20forecasting%20and%20distinguish%20it%20from%20in-distribution%0Ageneralization.%20We%20evaluate%20the%20reasoning%20and%20generalization%20capabilities%20of%2016%0Apopular%20deep%20learning%20forecasting%20models%20on%20multiple%20synthetic%20and%20real-world%0Adatasets.%20Additionally%2C%20through%20controlled%20studies%2C%20we%20systematically%20examine%0Awhich%20design%20choices%20in%207%20popular%20open-source%20TSFMs%20contribute%20to%20improved%0Areasoning%20capabilities.%20Our%20study%20yields%20key%20insights%20into%20the%20impact%20of%20TSFM%0Aarchitecture%20design%20on%20compositional%20reasoning%20and%20generalization.%20We%20find%20that%0Apatch-based%20Transformers%20have%20the%20best%20reasoning%20performance%2C%20closely%20followed%0Aby%20residualized%20MLP-based%20architectures%2C%20which%20are%2097%5C%25%20less%20computationally%0Acomplex%20in%20terms%20of%20FLOPs%20and%2086%5C%25%20smaller%20in%20terms%20of%20the%20number%20of%20trainable%0Aparameters.%20Interestingly%2C%20in%20some%20zero-shot%20out-of-distribution%20scenarios%2C%0Athese%20models%20can%20outperform%20moving%20average%20and%20exponential%20smoothing%0Astatistical%20baselines%20trained%20on%20in-distribution%20data.%20Only%20a%20few%20design%0Achoices%2C%20such%20as%20the%20tokenization%20method%2C%20had%20a%20significant%20%28negative%29%20impact%0Aon%20Transformer%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06037v2&entry.124074799=Read"},
{"title": "Linear Convergence of the Frank-Wolfe Algorithm over Product Polytopes", "author": "Gabriele Iommazzo and David Mart\u00ednez-Rubio and Francisco Criado and Elias Wirth and Sebastian Pokutta", "abstract": "  We study the linear convergence of Frank-Wolfe algorithms over product\npolytopes. We analyze two condition numbers for the product polytope, namely\nthe \\emph{pyramidal width} and the \\emph{vertex-facet distance}, based on the\ncondition numbers of individual polytope components. As a result, for convex\nobjectives that are $\\mu$-Polyak-{\\L}ojasiewicz, we show linear convergence\nrates quantified in terms of the resulting condition numbers. We apply our\nresults to the problem of approximately finding a feasible point in a polytope\nintersection in high-dimensions, and demonstrate the practical efficiency of\nour algorithms through empirical results.\n", "link": "http://arxiv.org/abs/2505.11259v2", "date": "2025-09-10", "relevancy": 2.0503, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4281}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.419}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20Convergence%20of%20the%20Frank-Wolfe%20Algorithm%20over%20Product%20Polytopes&body=Title%3A%20Linear%20Convergence%20of%20the%20Frank-Wolfe%20Algorithm%20over%20Product%20Polytopes%0AAuthor%3A%20Gabriele%20Iommazzo%20and%20David%20Mart%C3%ADnez-Rubio%20and%20Francisco%20Criado%20and%20Elias%20Wirth%20and%20Sebastian%20Pokutta%0AAbstract%3A%20%20%20We%20study%20the%20linear%20convergence%20of%20Frank-Wolfe%20algorithms%20over%20product%0Apolytopes.%20We%20analyze%20two%20condition%20numbers%20for%20the%20product%20polytope%2C%20namely%0Athe%20%5Cemph%7Bpyramidal%20width%7D%20and%20the%20%5Cemph%7Bvertex-facet%20distance%7D%2C%20based%20on%20the%0Acondition%20numbers%20of%20individual%20polytope%20components.%20As%20a%20result%2C%20for%20convex%0Aobjectives%20that%20are%20%24%5Cmu%24-Polyak-%7B%5CL%7Dojasiewicz%2C%20we%20show%20linear%20convergence%0Arates%20quantified%20in%20terms%20of%20the%20resulting%20condition%20numbers.%20We%20apply%20our%0Aresults%20to%20the%20problem%20of%20approximately%20finding%20a%20feasible%20point%20in%20a%20polytope%0Aintersection%20in%20high-dimensions%2C%20and%20demonstrate%20the%20practical%20efficiency%20of%0Aour%20algorithms%20through%20empirical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11259v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520Convergence%2520of%2520the%2520Frank-Wolfe%2520Algorithm%2520over%2520Product%2520Polytopes%26entry.906535625%3DGabriele%2520Iommazzo%2520and%2520David%2520Mart%25C3%25ADnez-Rubio%2520and%2520Francisco%2520Criado%2520and%2520Elias%2520Wirth%2520and%2520Sebastian%2520Pokutta%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520linear%2520convergence%2520of%2520Frank-Wolfe%2520algorithms%2520over%2520product%250Apolytopes.%2520We%2520analyze%2520two%2520condition%2520numbers%2520for%2520the%2520product%2520polytope%252C%2520namely%250Athe%2520%255Cemph%257Bpyramidal%2520width%257D%2520and%2520the%2520%255Cemph%257Bvertex-facet%2520distance%257D%252C%2520based%2520on%2520the%250Acondition%2520numbers%2520of%2520individual%2520polytope%2520components.%2520As%2520a%2520result%252C%2520for%2520convex%250Aobjectives%2520that%2520are%2520%2524%255Cmu%2524-Polyak-%257B%255CL%257Dojasiewicz%252C%2520we%2520show%2520linear%2520convergence%250Arates%2520quantified%2520in%2520terms%2520of%2520the%2520resulting%2520condition%2520numbers.%2520We%2520apply%2520our%250Aresults%2520to%2520the%2520problem%2520of%2520approximately%2520finding%2520a%2520feasible%2520point%2520in%2520a%2520polytope%250Aintersection%2520in%2520high-dimensions%252C%2520and%2520demonstrate%2520the%2520practical%2520efficiency%2520of%250Aour%2520algorithms%2520through%2520empirical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11259v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Convergence%20of%20the%20Frank-Wolfe%20Algorithm%20over%20Product%20Polytopes&entry.906535625=Gabriele%20Iommazzo%20and%20David%20Mart%C3%ADnez-Rubio%20and%20Francisco%20Criado%20and%20Elias%20Wirth%20and%20Sebastian%20Pokutta&entry.1292438233=%20%20We%20study%20the%20linear%20convergence%20of%20Frank-Wolfe%20algorithms%20over%20product%0Apolytopes.%20We%20analyze%20two%20condition%20numbers%20for%20the%20product%20polytope%2C%20namely%0Athe%20%5Cemph%7Bpyramidal%20width%7D%20and%20the%20%5Cemph%7Bvertex-facet%20distance%7D%2C%20based%20on%20the%0Acondition%20numbers%20of%20individual%20polytope%20components.%20As%20a%20result%2C%20for%20convex%0Aobjectives%20that%20are%20%24%5Cmu%24-Polyak-%7B%5CL%7Dojasiewicz%2C%20we%20show%20linear%20convergence%0Arates%20quantified%20in%20terms%20of%20the%20resulting%20condition%20numbers.%20We%20apply%20our%0Aresults%20to%20the%20problem%20of%20approximately%20finding%20a%20feasible%20point%20in%20a%20polytope%0Aintersection%20in%20high-dimensions%2C%20and%20demonstrate%20the%20practical%20efficiency%20of%0Aour%20algorithms%20through%20empirical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11259v2&entry.124074799=Read"},
{"title": "Motion-Based User Identification across XR and Metaverse Applications by\n  Deep Classification and Similarity Learning", "author": "Lukas Schach and Christian Rack and Ryan P. McMahan and Marc Erich Latoschik", "abstract": "  This paper examines the generalization capacity of two state-of-the-art\nclassification and similarity learning models in reliably identifying users\nbased on their motions in various Extended Reality (XR) applications. We\ndeveloped a novel dataset containing a wide range of motion data from 49 users\nin five different XR applications: four XR games with distinct tasks and action\npatterns, and an additional social XR application with no predefined task sets.\nThe dataset is used to evaluate the performance and, in particular, the\ngeneralization capacity of the two models across applications. Our results\nindicate that while the models can accurately identify individuals within the\nsame application, their ability to identify users across different XR\napplications remains limited. Overall, our results provide insight into current\nmodels generalization capabilities and suitability as biometric methods for\nuser verification and identification. The results also serve as a much-needed\nrisk assessment of hazardous and unwanted user identification in XR and\nMetaverse applications. Our cross-application XR motion dataset and code are\nmade available to the public to encourage similar research on the\ngeneralization of motion-based user identification in typical Metaverse\napplication use cases.\n", "link": "http://arxiv.org/abs/2509.08539v1", "date": "2025-09-10", "relevancy": 2.0294, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5179}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5003}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion-Based%20User%20Identification%20across%20XR%20and%20Metaverse%20Applications%20by%0A%20%20Deep%20Classification%20and%20Similarity%20Learning&body=Title%3A%20Motion-Based%20User%20Identification%20across%20XR%20and%20Metaverse%20Applications%20by%0A%20%20Deep%20Classification%20and%20Similarity%20Learning%0AAuthor%3A%20Lukas%20Schach%20and%20Christian%20Rack%20and%20Ryan%20P.%20McMahan%20and%20Marc%20Erich%20Latoschik%0AAbstract%3A%20%20%20This%20paper%20examines%20the%20generalization%20capacity%20of%20two%20state-of-the-art%0Aclassification%20and%20similarity%20learning%20models%20in%20reliably%20identifying%20users%0Abased%20on%20their%20motions%20in%20various%20Extended%20Reality%20%28XR%29%20applications.%20We%0Adeveloped%20a%20novel%20dataset%20containing%20a%20wide%20range%20of%20motion%20data%20from%2049%20users%0Ain%20five%20different%20XR%20applications%3A%20four%20XR%20games%20with%20distinct%20tasks%20and%20action%0Apatterns%2C%20and%20an%20additional%20social%20XR%20application%20with%20no%20predefined%20task%20sets.%0AThe%20dataset%20is%20used%20to%20evaluate%20the%20performance%20and%2C%20in%20particular%2C%20the%0Ageneralization%20capacity%20of%20the%20two%20models%20across%20applications.%20Our%20results%0Aindicate%20that%20while%20the%20models%20can%20accurately%20identify%20individuals%20within%20the%0Asame%20application%2C%20their%20ability%20to%20identify%20users%20across%20different%20XR%0Aapplications%20remains%20limited.%20Overall%2C%20our%20results%20provide%20insight%20into%20current%0Amodels%20generalization%20capabilities%20and%20suitability%20as%20biometric%20methods%20for%0Auser%20verification%20and%20identification.%20The%20results%20also%20serve%20as%20a%20much-needed%0Arisk%20assessment%20of%20hazardous%20and%20unwanted%20user%20identification%20in%20XR%20and%0AMetaverse%20applications.%20Our%20cross-application%20XR%20motion%20dataset%20and%20code%20are%0Amade%20available%20to%20the%20public%20to%20encourage%20similar%20research%20on%20the%0Ageneralization%20of%20motion-based%20user%20identification%20in%20typical%20Metaverse%0Aapplication%20use%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion-Based%2520User%2520Identification%2520across%2520XR%2520and%2520Metaverse%2520Applications%2520by%250A%2520%2520Deep%2520Classification%2520and%2520Similarity%2520Learning%26entry.906535625%3DLukas%2520Schach%2520and%2520Christian%2520Rack%2520and%2520Ryan%2520P.%2520McMahan%2520and%2520Marc%2520Erich%2520Latoschik%26entry.1292438233%3D%2520%2520This%2520paper%2520examines%2520the%2520generalization%2520capacity%2520of%2520two%2520state-of-the-art%250Aclassification%2520and%2520similarity%2520learning%2520models%2520in%2520reliably%2520identifying%2520users%250Abased%2520on%2520their%2520motions%2520in%2520various%2520Extended%2520Reality%2520%2528XR%2529%2520applications.%2520We%250Adeveloped%2520a%2520novel%2520dataset%2520containing%2520a%2520wide%2520range%2520of%2520motion%2520data%2520from%252049%2520users%250Ain%2520five%2520different%2520XR%2520applications%253A%2520four%2520XR%2520games%2520with%2520distinct%2520tasks%2520and%2520action%250Apatterns%252C%2520and%2520an%2520additional%2520social%2520XR%2520application%2520with%2520no%2520predefined%2520task%2520sets.%250AThe%2520dataset%2520is%2520used%2520to%2520evaluate%2520the%2520performance%2520and%252C%2520in%2520particular%252C%2520the%250Ageneralization%2520capacity%2520of%2520the%2520two%2520models%2520across%2520applications.%2520Our%2520results%250Aindicate%2520that%2520while%2520the%2520models%2520can%2520accurately%2520identify%2520individuals%2520within%2520the%250Asame%2520application%252C%2520their%2520ability%2520to%2520identify%2520users%2520across%2520different%2520XR%250Aapplications%2520remains%2520limited.%2520Overall%252C%2520our%2520results%2520provide%2520insight%2520into%2520current%250Amodels%2520generalization%2520capabilities%2520and%2520suitability%2520as%2520biometric%2520methods%2520for%250Auser%2520verification%2520and%2520identification.%2520The%2520results%2520also%2520serve%2520as%2520a%2520much-needed%250Arisk%2520assessment%2520of%2520hazardous%2520and%2520unwanted%2520user%2520identification%2520in%2520XR%2520and%250AMetaverse%2520applications.%2520Our%2520cross-application%2520XR%2520motion%2520dataset%2520and%2520code%2520are%250Amade%2520available%2520to%2520the%2520public%2520to%2520encourage%2520similar%2520research%2520on%2520the%250Ageneralization%2520of%2520motion-based%2520user%2520identification%2520in%2520typical%2520Metaverse%250Aapplication%2520use%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion-Based%20User%20Identification%20across%20XR%20and%20Metaverse%20Applications%20by%0A%20%20Deep%20Classification%20and%20Similarity%20Learning&entry.906535625=Lukas%20Schach%20and%20Christian%20Rack%20and%20Ryan%20P.%20McMahan%20and%20Marc%20Erich%20Latoschik&entry.1292438233=%20%20This%20paper%20examines%20the%20generalization%20capacity%20of%20two%20state-of-the-art%0Aclassification%20and%20similarity%20learning%20models%20in%20reliably%20identifying%20users%0Abased%20on%20their%20motions%20in%20various%20Extended%20Reality%20%28XR%29%20applications.%20We%0Adeveloped%20a%20novel%20dataset%20containing%20a%20wide%20range%20of%20motion%20data%20from%2049%20users%0Ain%20five%20different%20XR%20applications%3A%20four%20XR%20games%20with%20distinct%20tasks%20and%20action%0Apatterns%2C%20and%20an%20additional%20social%20XR%20application%20with%20no%20predefined%20task%20sets.%0AThe%20dataset%20is%20used%20to%20evaluate%20the%20performance%20and%2C%20in%20particular%2C%20the%0Ageneralization%20capacity%20of%20the%20two%20models%20across%20applications.%20Our%20results%0Aindicate%20that%20while%20the%20models%20can%20accurately%20identify%20individuals%20within%20the%0Asame%20application%2C%20their%20ability%20to%20identify%20users%20across%20different%20XR%0Aapplications%20remains%20limited.%20Overall%2C%20our%20results%20provide%20insight%20into%20current%0Amodels%20generalization%20capabilities%20and%20suitability%20as%20biometric%20methods%20for%0Auser%20verification%20and%20identification.%20The%20results%20also%20serve%20as%20a%20much-needed%0Arisk%20assessment%20of%20hazardous%20and%20unwanted%20user%20identification%20in%20XR%20and%0AMetaverse%20applications.%20Our%20cross-application%20XR%20motion%20dataset%20and%20code%20are%0Amade%20available%20to%20the%20public%20to%20encourage%20similar%20research%20on%20the%0Ageneralization%20of%20motion-based%20user%20identification%20in%20typical%20Metaverse%0Aapplication%20use%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08539v1&entry.124074799=Read"},
{"title": "Adversarial Attacks Against Automated Fact-Checking: A Survey", "author": "Fanzhen Liu and Alsharif Abuadbba and Kristen Moore and Surya Nepal and Cecile Paris and Jia Wu and Jian Yang and Quan Z. Sheng", "abstract": "  In an era where misinformation spreads freely, fact-checking (FC) plays a\ncrucial role in verifying claims and promoting reliable information. While\nautomated fact-checking (AFC) has advanced significantly, existing systems\nremain vulnerable to adversarial attacks that manipulate or generate claims,\nevidence, or claim-evidence pairs. These attacks can distort the truth, mislead\ndecision-makers, and ultimately undermine the reliability of FC models. Despite\ngrowing research interest in adversarial attacks against AFC systems, a\ncomprehensive, holistic overview of key challenges remains lacking. These\nchallenges include understanding attack strategies, assessing the resilience of\ncurrent models, and identifying ways to enhance robustness. This survey\nprovides the first in-depth review of adversarial attacks targeting FC,\ncategorizing existing attack methodologies and evaluating their impact on AFC\nsystems. Additionally, we examine recent advancements in adversary-aware\ndefenses and highlight open research questions that require further\nexploration. Our findings underscore the urgent need for resilient FC\nframeworks capable of withstanding adversarial manipulations in pursuit of\npreserving high verification accuracy.\n", "link": "http://arxiv.org/abs/2509.08463v1", "date": "2025-09-10", "relevancy": 2.0246, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4116}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4097}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Attacks%20Against%20Automated%20Fact-Checking%3A%20A%20Survey&body=Title%3A%20Adversarial%20Attacks%20Against%20Automated%20Fact-Checking%3A%20A%20Survey%0AAuthor%3A%20Fanzhen%20Liu%20and%20Alsharif%20Abuadbba%20and%20Kristen%20Moore%20and%20Surya%20Nepal%20and%20Cecile%20Paris%20and%20Jia%20Wu%20and%20Jian%20Yang%20and%20Quan%20Z.%20Sheng%0AAbstract%3A%20%20%20In%20an%20era%20where%20misinformation%20spreads%20freely%2C%20fact-checking%20%28FC%29%20plays%20a%0Acrucial%20role%20in%20verifying%20claims%20and%20promoting%20reliable%20information.%20While%0Aautomated%20fact-checking%20%28AFC%29%20has%20advanced%20significantly%2C%20existing%20systems%0Aremain%20vulnerable%20to%20adversarial%20attacks%20that%20manipulate%20or%20generate%20claims%2C%0Aevidence%2C%20or%20claim-evidence%20pairs.%20These%20attacks%20can%20distort%20the%20truth%2C%20mislead%0Adecision-makers%2C%20and%20ultimately%20undermine%20the%20reliability%20of%20FC%20models.%20Despite%0Agrowing%20research%20interest%20in%20adversarial%20attacks%20against%20AFC%20systems%2C%20a%0Acomprehensive%2C%20holistic%20overview%20of%20key%20challenges%20remains%20lacking.%20These%0Achallenges%20include%20understanding%20attack%20strategies%2C%20assessing%20the%20resilience%20of%0Acurrent%20models%2C%20and%20identifying%20ways%20to%20enhance%20robustness.%20This%20survey%0Aprovides%20the%20first%20in-depth%20review%20of%20adversarial%20attacks%20targeting%20FC%2C%0Acategorizing%20existing%20attack%20methodologies%20and%20evaluating%20their%20impact%20on%20AFC%0Asystems.%20Additionally%2C%20we%20examine%20recent%20advancements%20in%20adversary-aware%0Adefenses%20and%20highlight%20open%20research%20questions%20that%20require%20further%0Aexploration.%20Our%20findings%20underscore%20the%20urgent%20need%20for%20resilient%20FC%0Aframeworks%20capable%20of%20withstanding%20adversarial%20manipulations%20in%20pursuit%20of%0Apreserving%20high%20verification%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Attacks%2520Against%2520Automated%2520Fact-Checking%253A%2520A%2520Survey%26entry.906535625%3DFanzhen%2520Liu%2520and%2520Alsharif%2520Abuadbba%2520and%2520Kristen%2520Moore%2520and%2520Surya%2520Nepal%2520and%2520Cecile%2520Paris%2520and%2520Jia%2520Wu%2520and%2520Jian%2520Yang%2520and%2520Quan%2520Z.%2520Sheng%26entry.1292438233%3D%2520%2520In%2520an%2520era%2520where%2520misinformation%2520spreads%2520freely%252C%2520fact-checking%2520%2528FC%2529%2520plays%2520a%250Acrucial%2520role%2520in%2520verifying%2520claims%2520and%2520promoting%2520reliable%2520information.%2520While%250Aautomated%2520fact-checking%2520%2528AFC%2529%2520has%2520advanced%2520significantly%252C%2520existing%2520systems%250Aremain%2520vulnerable%2520to%2520adversarial%2520attacks%2520that%2520manipulate%2520or%2520generate%2520claims%252C%250Aevidence%252C%2520or%2520claim-evidence%2520pairs.%2520These%2520attacks%2520can%2520distort%2520the%2520truth%252C%2520mislead%250Adecision-makers%252C%2520and%2520ultimately%2520undermine%2520the%2520reliability%2520of%2520FC%2520models.%2520Despite%250Agrowing%2520research%2520interest%2520in%2520adversarial%2520attacks%2520against%2520AFC%2520systems%252C%2520a%250Acomprehensive%252C%2520holistic%2520overview%2520of%2520key%2520challenges%2520remains%2520lacking.%2520These%250Achallenges%2520include%2520understanding%2520attack%2520strategies%252C%2520assessing%2520the%2520resilience%2520of%250Acurrent%2520models%252C%2520and%2520identifying%2520ways%2520to%2520enhance%2520robustness.%2520This%2520survey%250Aprovides%2520the%2520first%2520in-depth%2520review%2520of%2520adversarial%2520attacks%2520targeting%2520FC%252C%250Acategorizing%2520existing%2520attack%2520methodologies%2520and%2520evaluating%2520their%2520impact%2520on%2520AFC%250Asystems.%2520Additionally%252C%2520we%2520examine%2520recent%2520advancements%2520in%2520adversary-aware%250Adefenses%2520and%2520highlight%2520open%2520research%2520questions%2520that%2520require%2520further%250Aexploration.%2520Our%2520findings%2520underscore%2520the%2520urgent%2520need%2520for%2520resilient%2520FC%250Aframeworks%2520capable%2520of%2520withstanding%2520adversarial%2520manipulations%2520in%2520pursuit%2520of%250Apreserving%2520high%2520verification%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Attacks%20Against%20Automated%20Fact-Checking%3A%20A%20Survey&entry.906535625=Fanzhen%20Liu%20and%20Alsharif%20Abuadbba%20and%20Kristen%20Moore%20and%20Surya%20Nepal%20and%20Cecile%20Paris%20and%20Jia%20Wu%20and%20Jian%20Yang%20and%20Quan%20Z.%20Sheng&entry.1292438233=%20%20In%20an%20era%20where%20misinformation%20spreads%20freely%2C%20fact-checking%20%28FC%29%20plays%20a%0Acrucial%20role%20in%20verifying%20claims%20and%20promoting%20reliable%20information.%20While%0Aautomated%20fact-checking%20%28AFC%29%20has%20advanced%20significantly%2C%20existing%20systems%0Aremain%20vulnerable%20to%20adversarial%20attacks%20that%20manipulate%20or%20generate%20claims%2C%0Aevidence%2C%20or%20claim-evidence%20pairs.%20These%20attacks%20can%20distort%20the%20truth%2C%20mislead%0Adecision-makers%2C%20and%20ultimately%20undermine%20the%20reliability%20of%20FC%20models.%20Despite%0Agrowing%20research%20interest%20in%20adversarial%20attacks%20against%20AFC%20systems%2C%20a%0Acomprehensive%2C%20holistic%20overview%20of%20key%20challenges%20remains%20lacking.%20These%0Achallenges%20include%20understanding%20attack%20strategies%2C%20assessing%20the%20resilience%20of%0Acurrent%20models%2C%20and%20identifying%20ways%20to%20enhance%20robustness.%20This%20survey%0Aprovides%20the%20first%20in-depth%20review%20of%20adversarial%20attacks%20targeting%20FC%2C%0Acategorizing%20existing%20attack%20methodologies%20and%20evaluating%20their%20impact%20on%20AFC%0Asystems.%20Additionally%2C%20we%20examine%20recent%20advancements%20in%20adversary-aware%0Adefenses%20and%20highlight%20open%20research%20questions%20that%20require%20further%0Aexploration.%20Our%20findings%20underscore%20the%20urgent%20need%20for%20resilient%20FC%0Aframeworks%20capable%20of%20withstanding%20adversarial%20manipulations%20in%20pursuit%20of%0Apreserving%20high%20verification%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08463v1&entry.124074799=Read"},
{"title": "Sharing is Caring: Efficient LM Post-Training with Collective RL\n  Experience Sharing", "author": "Jeffrey Amico and Gabriel Passamani Andrade and John Donaghy and Ben Fielding and Tristin Forbus and Harry Grieve and Semih Kara and Jari Kolehmainen and Yihua Lou and Christopher Nies and Edward Phillip Flores Nu\u00f1o and Diogo Ortega and Shikhar Rastogi and Austin Virts and Matthew J. Wright", "abstract": "  Post-training language models (LMs) with reinforcement learning (RL) can\nenhance their complex reasoning capabilities without supervised fine-tuning, as\ndemonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs\nrequires significant parallelization to scale-up inference, which introduces\nnon-trivial technical challenges (e.g. latency, memory, and reliability)\nalongside ever-growing financial costs. We present Swarm sAmpling Policy\nOptimization (SAPO), a fully decentralized and asynchronous RL post-training\nalgorithm. SAPO is designed for decentralized networks of heterogenous compute\nnodes, where each node manages its own policy model(s) while \"sharing\" rollouts\nwith others in the network; no explicit assumptions about latency, model\nhomogeneity, or hardware are required and nodes can operate in silo if desired.\nAs a result, the algorithm avoids common bottlenecks in scaling RL\npost-training while also allowing (and even encouraging) new possibilities. By\nsampling rollouts \"shared\" across the network, it enables \"Aha moments\" to\npropagate, thereby bootstrapping the learning process. In this paper we show\nSAPO achieved cumulative reward gains of up to 94% in controlled experiments.\nWe also share insights from tests on a network with thousands of nodes\ncontributed by Gensyn community members running the algorithm on diverse\nhardware and models during an open-source demo.\n", "link": "http://arxiv.org/abs/2509.08721v1", "date": "2025-09-10", "relevancy": 2.0091, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5248}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4875}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharing%20is%20Caring%3A%20Efficient%20LM%20Post-Training%20with%20Collective%20RL%0A%20%20Experience%20Sharing&body=Title%3A%20Sharing%20is%20Caring%3A%20Efficient%20LM%20Post-Training%20with%20Collective%20RL%0A%20%20Experience%20Sharing%0AAuthor%3A%20Jeffrey%20Amico%20and%20Gabriel%20Passamani%20Andrade%20and%20John%20Donaghy%20and%20Ben%20Fielding%20and%20Tristin%20Forbus%20and%20Harry%20Grieve%20and%20Semih%20Kara%20and%20Jari%20Kolehmainen%20and%20Yihua%20Lou%20and%20Christopher%20Nies%20and%20Edward%20Phillip%20Flores%20Nu%C3%B1o%20and%20Diogo%20Ortega%20and%20Shikhar%20Rastogi%20and%20Austin%20Virts%20and%20Matthew%20J.%20Wright%0AAbstract%3A%20%20%20Post-training%20language%20models%20%28LMs%29%20with%20reinforcement%20learning%20%28RL%29%20can%0Aenhance%20their%20complex%20reasoning%20capabilities%20without%20supervised%20fine-tuning%2C%20as%0Ademonstrated%20by%20DeepSeek-R1-Zero.%20However%2C%20effectively%20utilizing%20RL%20for%20LMs%0Arequires%20significant%20parallelization%20to%20scale-up%20inference%2C%20which%20introduces%0Anon-trivial%20technical%20challenges%20%28e.g.%20latency%2C%20memory%2C%20and%20reliability%29%0Aalongside%20ever-growing%20financial%20costs.%20We%20present%20Swarm%20sAmpling%20Policy%0AOptimization%20%28SAPO%29%2C%20a%20fully%20decentralized%20and%20asynchronous%20RL%20post-training%0Aalgorithm.%20SAPO%20is%20designed%20for%20decentralized%20networks%20of%20heterogenous%20compute%0Anodes%2C%20where%20each%20node%20manages%20its%20own%20policy%20model%28s%29%20while%20%22sharing%22%20rollouts%0Awith%20others%20in%20the%20network%3B%20no%20explicit%20assumptions%20about%20latency%2C%20model%0Ahomogeneity%2C%20or%20hardware%20are%20required%20and%20nodes%20can%20operate%20in%20silo%20if%20desired.%0AAs%20a%20result%2C%20the%20algorithm%20avoids%20common%20bottlenecks%20in%20scaling%20RL%0Apost-training%20while%20also%20allowing%20%28and%20even%20encouraging%29%20new%20possibilities.%20By%0Asampling%20rollouts%20%22shared%22%20across%20the%20network%2C%20it%20enables%20%22Aha%20moments%22%20to%0Apropagate%2C%20thereby%20bootstrapping%20the%20learning%20process.%20In%20this%20paper%20we%20show%0ASAPO%20achieved%20cumulative%20reward%20gains%20of%20up%20to%2094%25%20in%20controlled%20experiments.%0AWe%20also%20share%20insights%20from%20tests%20on%20a%20network%20with%20thousands%20of%20nodes%0Acontributed%20by%20Gensyn%20community%20members%20running%20the%20algorithm%20on%20diverse%0Ahardware%20and%20models%20during%20an%20open-source%20demo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharing%2520is%2520Caring%253A%2520Efficient%2520LM%2520Post-Training%2520with%2520Collective%2520RL%250A%2520%2520Experience%2520Sharing%26entry.906535625%3DJeffrey%2520Amico%2520and%2520Gabriel%2520Passamani%2520Andrade%2520and%2520John%2520Donaghy%2520and%2520Ben%2520Fielding%2520and%2520Tristin%2520Forbus%2520and%2520Harry%2520Grieve%2520and%2520Semih%2520Kara%2520and%2520Jari%2520Kolehmainen%2520and%2520Yihua%2520Lou%2520and%2520Christopher%2520Nies%2520and%2520Edward%2520Phillip%2520Flores%2520Nu%25C3%25B1o%2520and%2520Diogo%2520Ortega%2520and%2520Shikhar%2520Rastogi%2520and%2520Austin%2520Virts%2520and%2520Matthew%2520J.%2520Wright%26entry.1292438233%3D%2520%2520Post-training%2520language%2520models%2520%2528LMs%2529%2520with%2520reinforcement%2520learning%2520%2528RL%2529%2520can%250Aenhance%2520their%2520complex%2520reasoning%2520capabilities%2520without%2520supervised%2520fine-tuning%252C%2520as%250Ademonstrated%2520by%2520DeepSeek-R1-Zero.%2520However%252C%2520effectively%2520utilizing%2520RL%2520for%2520LMs%250Arequires%2520significant%2520parallelization%2520to%2520scale-up%2520inference%252C%2520which%2520introduces%250Anon-trivial%2520technical%2520challenges%2520%2528e.g.%2520latency%252C%2520memory%252C%2520and%2520reliability%2529%250Aalongside%2520ever-growing%2520financial%2520costs.%2520We%2520present%2520Swarm%2520sAmpling%2520Policy%250AOptimization%2520%2528SAPO%2529%252C%2520a%2520fully%2520decentralized%2520and%2520asynchronous%2520RL%2520post-training%250Aalgorithm.%2520SAPO%2520is%2520designed%2520for%2520decentralized%2520networks%2520of%2520heterogenous%2520compute%250Anodes%252C%2520where%2520each%2520node%2520manages%2520its%2520own%2520policy%2520model%2528s%2529%2520while%2520%2522sharing%2522%2520rollouts%250Awith%2520others%2520in%2520the%2520network%253B%2520no%2520explicit%2520assumptions%2520about%2520latency%252C%2520model%250Ahomogeneity%252C%2520or%2520hardware%2520are%2520required%2520and%2520nodes%2520can%2520operate%2520in%2520silo%2520if%2520desired.%250AAs%2520a%2520result%252C%2520the%2520algorithm%2520avoids%2520common%2520bottlenecks%2520in%2520scaling%2520RL%250Apost-training%2520while%2520also%2520allowing%2520%2528and%2520even%2520encouraging%2529%2520new%2520possibilities.%2520By%250Asampling%2520rollouts%2520%2522shared%2522%2520across%2520the%2520network%252C%2520it%2520enables%2520%2522Aha%2520moments%2522%2520to%250Apropagate%252C%2520thereby%2520bootstrapping%2520the%2520learning%2520process.%2520In%2520this%2520paper%2520we%2520show%250ASAPO%2520achieved%2520cumulative%2520reward%2520gains%2520of%2520up%2520to%252094%2525%2520in%2520controlled%2520experiments.%250AWe%2520also%2520share%2520insights%2520from%2520tests%2520on%2520a%2520network%2520with%2520thousands%2520of%2520nodes%250Acontributed%2520by%2520Gensyn%2520community%2520members%2520running%2520the%2520algorithm%2520on%2520diverse%250Ahardware%2520and%2520models%2520during%2520an%2520open-source%2520demo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharing%20is%20Caring%3A%20Efficient%20LM%20Post-Training%20with%20Collective%20RL%0A%20%20Experience%20Sharing&entry.906535625=Jeffrey%20Amico%20and%20Gabriel%20Passamani%20Andrade%20and%20John%20Donaghy%20and%20Ben%20Fielding%20and%20Tristin%20Forbus%20and%20Harry%20Grieve%20and%20Semih%20Kara%20and%20Jari%20Kolehmainen%20and%20Yihua%20Lou%20and%20Christopher%20Nies%20and%20Edward%20Phillip%20Flores%20Nu%C3%B1o%20and%20Diogo%20Ortega%20and%20Shikhar%20Rastogi%20and%20Austin%20Virts%20and%20Matthew%20J.%20Wright&entry.1292438233=%20%20Post-training%20language%20models%20%28LMs%29%20with%20reinforcement%20learning%20%28RL%29%20can%0Aenhance%20their%20complex%20reasoning%20capabilities%20without%20supervised%20fine-tuning%2C%20as%0Ademonstrated%20by%20DeepSeek-R1-Zero.%20However%2C%20effectively%20utilizing%20RL%20for%20LMs%0Arequires%20significant%20parallelization%20to%20scale-up%20inference%2C%20which%20introduces%0Anon-trivial%20technical%20challenges%20%28e.g.%20latency%2C%20memory%2C%20and%20reliability%29%0Aalongside%20ever-growing%20financial%20costs.%20We%20present%20Swarm%20sAmpling%20Policy%0AOptimization%20%28SAPO%29%2C%20a%20fully%20decentralized%20and%20asynchronous%20RL%20post-training%0Aalgorithm.%20SAPO%20is%20designed%20for%20decentralized%20networks%20of%20heterogenous%20compute%0Anodes%2C%20where%20each%20node%20manages%20its%20own%20policy%20model%28s%29%20while%20%22sharing%22%20rollouts%0Awith%20others%20in%20the%20network%3B%20no%20explicit%20assumptions%20about%20latency%2C%20model%0Ahomogeneity%2C%20or%20hardware%20are%20required%20and%20nodes%20can%20operate%20in%20silo%20if%20desired.%0AAs%20a%20result%2C%20the%20algorithm%20avoids%20common%20bottlenecks%20in%20scaling%20RL%0Apost-training%20while%20also%20allowing%20%28and%20even%20encouraging%29%20new%20possibilities.%20By%0Asampling%20rollouts%20%22shared%22%20across%20the%20network%2C%20it%20enables%20%22Aha%20moments%22%20to%0Apropagate%2C%20thereby%20bootstrapping%20the%20learning%20process.%20In%20this%20paper%20we%20show%0ASAPO%20achieved%20cumulative%20reward%20gains%20of%20up%20to%2094%25%20in%20controlled%20experiments.%0AWe%20also%20share%20insights%20from%20tests%20on%20a%20network%20with%20thousands%20of%20nodes%0Acontributed%20by%20Gensyn%20community%20members%20running%20the%20algorithm%20on%20diverse%0Ahardware%20and%20models%20during%20an%20open-source%20demo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08721v1&entry.124074799=Read"},
{"title": "Towards properties of adversarial image perturbations", "author": "Egor Kuznetsov and Kirill Aistov and Maxim Koroteev", "abstract": "  Using stochastic gradient approach we study the properties of adversarial\nperturbations resulting in noticeable growth of VMAF image quality metric. The\nstructure of the perturbations is investigated depending on the acceptable PSNR\nvalues and based on the Fourier power spectrum computations for the\nperturbations. It is demonstrated that moderate variation of image brightness\n($\\sim 10$ pixel units in a restricted region of an image can result in VMAF\ngrowth by $\\sim 60\\%$). Unlike some other methods demonstrating similar VMAF\ngrowth, the subjective quality of an image remains almost unchanged. It is also\nshown that the adversarial perturbations may demonstrate approximately linear\ndependence of perturbation amplitudes on the image brightness. The\nperturbations are studied based on the direct VMAF optimization in PyTorch. The\nsignificant discrepancies between the metric values and subjective judgements\nare also demonstrated when image restoration from noise is carried out using\nthe same direct VMAF optimization.\n", "link": "http://arxiv.org/abs/2503.14111v2", "date": "2025-09-10", "relevancy": 1.9949, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5171}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4896}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20properties%20of%20adversarial%20image%20perturbations&body=Title%3A%20Towards%20properties%20of%20adversarial%20image%20perturbations%0AAuthor%3A%20Egor%20Kuznetsov%20and%20Kirill%20Aistov%20and%20Maxim%20Koroteev%0AAbstract%3A%20%20%20Using%20stochastic%20gradient%20approach%20we%20study%20the%20properties%20of%20adversarial%0Aperturbations%20resulting%20in%20noticeable%20growth%20of%20VMAF%20image%20quality%20metric.%20The%0Astructure%20of%20the%20perturbations%20is%20investigated%20depending%20on%20the%20acceptable%20PSNR%0Avalues%20and%20based%20on%20the%20Fourier%20power%20spectrum%20computations%20for%20the%0Aperturbations.%20It%20is%20demonstrated%20that%20moderate%20variation%20of%20image%20brightness%0A%28%24%5Csim%2010%24%20pixel%20units%20in%20a%20restricted%20region%20of%20an%20image%20can%20result%20in%20VMAF%0Agrowth%20by%20%24%5Csim%2060%5C%25%24%29.%20Unlike%20some%20other%20methods%20demonstrating%20similar%20VMAF%0Agrowth%2C%20the%20subjective%20quality%20of%20an%20image%20remains%20almost%20unchanged.%20It%20is%20also%0Ashown%20that%20the%20adversarial%20perturbations%20may%20demonstrate%20approximately%20linear%0Adependence%20of%20perturbation%20amplitudes%20on%20the%20image%20brightness.%20The%0Aperturbations%20are%20studied%20based%20on%20the%20direct%20VMAF%20optimization%20in%20PyTorch.%20The%0Asignificant%20discrepancies%20between%20the%20metric%20values%20and%20subjective%20judgements%0Aare%20also%20demonstrated%20when%20image%20restoration%20from%20noise%20is%20carried%20out%20using%0Athe%20same%20direct%20VMAF%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14111v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520properties%2520of%2520adversarial%2520image%2520perturbations%26entry.906535625%3DEgor%2520Kuznetsov%2520and%2520Kirill%2520Aistov%2520and%2520Maxim%2520Koroteev%26entry.1292438233%3D%2520%2520Using%2520stochastic%2520gradient%2520approach%2520we%2520study%2520the%2520properties%2520of%2520adversarial%250Aperturbations%2520resulting%2520in%2520noticeable%2520growth%2520of%2520VMAF%2520image%2520quality%2520metric.%2520The%250Astructure%2520of%2520the%2520perturbations%2520is%2520investigated%2520depending%2520on%2520the%2520acceptable%2520PSNR%250Avalues%2520and%2520based%2520on%2520the%2520Fourier%2520power%2520spectrum%2520computations%2520for%2520the%250Aperturbations.%2520It%2520is%2520demonstrated%2520that%2520moderate%2520variation%2520of%2520image%2520brightness%250A%2528%2524%255Csim%252010%2524%2520pixel%2520units%2520in%2520a%2520restricted%2520region%2520of%2520an%2520image%2520can%2520result%2520in%2520VMAF%250Agrowth%2520by%2520%2524%255Csim%252060%255C%2525%2524%2529.%2520Unlike%2520some%2520other%2520methods%2520demonstrating%2520similar%2520VMAF%250Agrowth%252C%2520the%2520subjective%2520quality%2520of%2520an%2520image%2520remains%2520almost%2520unchanged.%2520It%2520is%2520also%250Ashown%2520that%2520the%2520adversarial%2520perturbations%2520may%2520demonstrate%2520approximately%2520linear%250Adependence%2520of%2520perturbation%2520amplitudes%2520on%2520the%2520image%2520brightness.%2520The%250Aperturbations%2520are%2520studied%2520based%2520on%2520the%2520direct%2520VMAF%2520optimization%2520in%2520PyTorch.%2520The%250Asignificant%2520discrepancies%2520between%2520the%2520metric%2520values%2520and%2520subjective%2520judgements%250Aare%2520also%2520demonstrated%2520when%2520image%2520restoration%2520from%2520noise%2520is%2520carried%2520out%2520using%250Athe%2520same%2520direct%2520VMAF%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14111v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20properties%20of%20adversarial%20image%20perturbations&entry.906535625=Egor%20Kuznetsov%20and%20Kirill%20Aistov%20and%20Maxim%20Koroteev&entry.1292438233=%20%20Using%20stochastic%20gradient%20approach%20we%20study%20the%20properties%20of%20adversarial%0Aperturbations%20resulting%20in%20noticeable%20growth%20of%20VMAF%20image%20quality%20metric.%20The%0Astructure%20of%20the%20perturbations%20is%20investigated%20depending%20on%20the%20acceptable%20PSNR%0Avalues%20and%20based%20on%20the%20Fourier%20power%20spectrum%20computations%20for%20the%0Aperturbations.%20It%20is%20demonstrated%20that%20moderate%20variation%20of%20image%20brightness%0A%28%24%5Csim%2010%24%20pixel%20units%20in%20a%20restricted%20region%20of%20an%20image%20can%20result%20in%20VMAF%0Agrowth%20by%20%24%5Csim%2060%5C%25%24%29.%20Unlike%20some%20other%20methods%20demonstrating%20similar%20VMAF%0Agrowth%2C%20the%20subjective%20quality%20of%20an%20image%20remains%20almost%20unchanged.%20It%20is%20also%0Ashown%20that%20the%20adversarial%20perturbations%20may%20demonstrate%20approximately%20linear%0Adependence%20of%20perturbation%20amplitudes%20on%20the%20image%20brightness.%20The%0Aperturbations%20are%20studied%20based%20on%20the%20direct%20VMAF%20optimization%20in%20PyTorch.%20The%0Asignificant%20discrepancies%20between%20the%20metric%20values%20and%20subjective%20judgements%0Aare%20also%20demonstrated%20when%20image%20restoration%20from%20noise%20is%20carried%20out%20using%0Athe%20same%20direct%20VMAF%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14111v2&entry.124074799=Read"},
{"title": "First-order State Space Model for Lightweight Image Super-resolution", "author": "Yujie Zhu and Xinyi Zhang and Yekai Lu and Guang Yang and Faming Fang and Guixu Zhang", "abstract": "  State space models (SSMs), particularly Mamba, have shown promise in NLP\ntasks and are increasingly applied to vision tasks. However, most Mamba-based\nvision models focus on network architecture and scan paths, with little\nattention to the SSM module. In order to explore the potential of SSMs, we\nmodified the calculation process of SSM without increasing the number of\nparameters to improve the performance on lightweight super-resolution tasks. In\nthis paper, we introduce the First-order State Space Model (FSSM) to improve\nthe original Mamba module, enhancing performance by incorporating token\ncorrelations. We apply a first-order hold condition in SSMs, derive the new\ndiscretized form, and analyzed cumulative error. Extensive experimental results\ndemonstrate that FSSM improves the performance of MambaIR on five benchmark\ndatasets without additionally increasing the number of parameters, and\nsurpasses current lightweight SR methods, achieving state-of-the-art results.\n", "link": "http://arxiv.org/abs/2509.08458v1", "date": "2025-09-10", "relevancy": 1.973, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5351}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.486}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20First-order%20State%20Space%20Model%20for%20Lightweight%20Image%20Super-resolution&body=Title%3A%20First-order%20State%20Space%20Model%20for%20Lightweight%20Image%20Super-resolution%0AAuthor%3A%20Yujie%20Zhu%20and%20Xinyi%20Zhang%20and%20Yekai%20Lu%20and%20Guang%20Yang%20and%20Faming%20Fang%20and%20Guixu%20Zhang%0AAbstract%3A%20%20%20State%20space%20models%20%28SSMs%29%2C%20particularly%20Mamba%2C%20have%20shown%20promise%20in%20NLP%0Atasks%20and%20are%20increasingly%20applied%20to%20vision%20tasks.%20However%2C%20most%20Mamba-based%0Avision%20models%20focus%20on%20network%20architecture%20and%20scan%20paths%2C%20with%20little%0Aattention%20to%20the%20SSM%20module.%20In%20order%20to%20explore%20the%20potential%20of%20SSMs%2C%20we%0Amodified%20the%20calculation%20process%20of%20SSM%20without%20increasing%20the%20number%20of%0Aparameters%20to%20improve%20the%20performance%20on%20lightweight%20super-resolution%20tasks.%20In%0Athis%20paper%2C%20we%20introduce%20the%20First-order%20State%20Space%20Model%20%28FSSM%29%20to%20improve%0Athe%20original%20Mamba%20module%2C%20enhancing%20performance%20by%20incorporating%20token%0Acorrelations.%20We%20apply%20a%20first-order%20hold%20condition%20in%20SSMs%2C%20derive%20the%20new%0Adiscretized%20form%2C%20and%20analyzed%20cumulative%20error.%20Extensive%20experimental%20results%0Ademonstrate%20that%20FSSM%20improves%20the%20performance%20of%20MambaIR%20on%20five%20benchmark%0Adatasets%20without%20additionally%20increasing%20the%20number%20of%20parameters%2C%20and%0Asurpasses%20current%20lightweight%20SR%20methods%2C%20achieving%20state-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFirst-order%2520State%2520Space%2520Model%2520for%2520Lightweight%2520Image%2520Super-resolution%26entry.906535625%3DYujie%2520Zhu%2520and%2520Xinyi%2520Zhang%2520and%2520Yekai%2520Lu%2520and%2520Guang%2520Yang%2520and%2520Faming%2520Fang%2520and%2520Guixu%2520Zhang%26entry.1292438233%3D%2520%2520State%2520space%2520models%2520%2528SSMs%2529%252C%2520particularly%2520Mamba%252C%2520have%2520shown%2520promise%2520in%2520NLP%250Atasks%2520and%2520are%2520increasingly%2520applied%2520to%2520vision%2520tasks.%2520However%252C%2520most%2520Mamba-based%250Avision%2520models%2520focus%2520on%2520network%2520architecture%2520and%2520scan%2520paths%252C%2520with%2520little%250Aattention%2520to%2520the%2520SSM%2520module.%2520In%2520order%2520to%2520explore%2520the%2520potential%2520of%2520SSMs%252C%2520we%250Amodified%2520the%2520calculation%2520process%2520of%2520SSM%2520without%2520increasing%2520the%2520number%2520of%250Aparameters%2520to%2520improve%2520the%2520performance%2520on%2520lightweight%2520super-resolution%2520tasks.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520the%2520First-order%2520State%2520Space%2520Model%2520%2528FSSM%2529%2520to%2520improve%250Athe%2520original%2520Mamba%2520module%252C%2520enhancing%2520performance%2520by%2520incorporating%2520token%250Acorrelations.%2520We%2520apply%2520a%2520first-order%2520hold%2520condition%2520in%2520SSMs%252C%2520derive%2520the%2520new%250Adiscretized%2520form%252C%2520and%2520analyzed%2520cumulative%2520error.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520FSSM%2520improves%2520the%2520performance%2520of%2520MambaIR%2520on%2520five%2520benchmark%250Adatasets%2520without%2520additionally%2520increasing%2520the%2520number%2520of%2520parameters%252C%2520and%250Asurpasses%2520current%2520lightweight%2520SR%2520methods%252C%2520achieving%2520state-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=First-order%20State%20Space%20Model%20for%20Lightweight%20Image%20Super-resolution&entry.906535625=Yujie%20Zhu%20and%20Xinyi%20Zhang%20and%20Yekai%20Lu%20and%20Guang%20Yang%20and%20Faming%20Fang%20and%20Guixu%20Zhang&entry.1292438233=%20%20State%20space%20models%20%28SSMs%29%2C%20particularly%20Mamba%2C%20have%20shown%20promise%20in%20NLP%0Atasks%20and%20are%20increasingly%20applied%20to%20vision%20tasks.%20However%2C%20most%20Mamba-based%0Avision%20models%20focus%20on%20network%20architecture%20and%20scan%20paths%2C%20with%20little%0Aattention%20to%20the%20SSM%20module.%20In%20order%20to%20explore%20the%20potential%20of%20SSMs%2C%20we%0Amodified%20the%20calculation%20process%20of%20SSM%20without%20increasing%20the%20number%20of%0Aparameters%20to%20improve%20the%20performance%20on%20lightweight%20super-resolution%20tasks.%20In%0Athis%20paper%2C%20we%20introduce%20the%20First-order%20State%20Space%20Model%20%28FSSM%29%20to%20improve%0Athe%20original%20Mamba%20module%2C%20enhancing%20performance%20by%20incorporating%20token%0Acorrelations.%20We%20apply%20a%20first-order%20hold%20condition%20in%20SSMs%2C%20derive%20the%20new%0Adiscretized%20form%2C%20and%20analyzed%20cumulative%20error.%20Extensive%20experimental%20results%0Ademonstrate%20that%20FSSM%20improves%20the%20performance%20of%20MambaIR%20on%20five%20benchmark%0Adatasets%20without%20additionally%20increasing%20the%20number%20of%20parameters%2C%20and%0Asurpasses%20current%20lightweight%20SR%20methods%2C%20achieving%20state-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08458v1&entry.124074799=Read"},
{"title": "Joint Learning using Mixture-of-Expert-Based Representation for Enhanced\n  Speech Generation and Robust Emotion Recognition", "author": "Jing-Tong Tzeng and Carlos Busso and Chi-Chun Lee", "abstract": "  Speech emotion recognition (SER) plays a critical role in building\nemotion-aware speech systems, but its performance degrades significantly under\nnoisy conditions. Although speech enhancement (SE) can improve robustness, it\noften introduces artifacts that obscure emotional cues and adds computational\noverhead to the pipeline. Multi-task learning (MTL) offers an alternative by\njointly optimizing SE and SER tasks. However, conventional shared-backbone\nmodels frequently suffer from gradient interference and representational\nconflicts between tasks. To address these challenges, we propose the Sparse\nMixture-of-Experts Representation Integration Technique (Sparse MERIT), a\nflexible MTL framework that applies frame-wise expert routing over\nself-supervised speech representations. Sparse MERIT incorporates task-specific\ngating networks that dynamically select from a shared pool of experts for each\nframe, enabling parameter-efficient and task-adaptive representation learning.\nExperiments on the MSP-Podcast corpus show that Sparse MERIT consistently\noutperforms baseline models on both SER and SE tasks. Under the most\nchallenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERIT\nimproves SER F1-macro by an average of 12.0% over a baseline relying on a SE\npre-processing strategy, and by 3.4% over a naive MTL baseline, with\nstatistical significance on unseen noise conditions. For SE, Sparse MERIT\nimproves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline and\nby 20.0% over the naive MTL baseline. These results demonstrate that Sparse\nMERIT provides robust and generalizable performance for both emotion\nrecognition and enhancement tasks in noisy environments.\n", "link": "http://arxiv.org/abs/2509.08470v1", "date": "2025-09-10", "relevancy": 1.9709, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5098}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4926}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Learning%20using%20Mixture-of-Expert-Based%20Representation%20for%20Enhanced%0A%20%20Speech%20Generation%20and%20Robust%20Emotion%20Recognition&body=Title%3A%20Joint%20Learning%20using%20Mixture-of-Expert-Based%20Representation%20for%20Enhanced%0A%20%20Speech%20Generation%20and%20Robust%20Emotion%20Recognition%0AAuthor%3A%20Jing-Tong%20Tzeng%20and%20Carlos%20Busso%20and%20Chi-Chun%20Lee%0AAbstract%3A%20%20%20Speech%20emotion%20recognition%20%28SER%29%20plays%20a%20critical%20role%20in%20building%0Aemotion-aware%20speech%20systems%2C%20but%20its%20performance%20degrades%20significantly%20under%0Anoisy%20conditions.%20Although%20speech%20enhancement%20%28SE%29%20can%20improve%20robustness%2C%20it%0Aoften%20introduces%20artifacts%20that%20obscure%20emotional%20cues%20and%20adds%20computational%0Aoverhead%20to%20the%20pipeline.%20Multi-task%20learning%20%28MTL%29%20offers%20an%20alternative%20by%0Ajointly%20optimizing%20SE%20and%20SER%20tasks.%20However%2C%20conventional%20shared-backbone%0Amodels%20frequently%20suffer%20from%20gradient%20interference%20and%20representational%0Aconflicts%20between%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20Sparse%0AMixture-of-Experts%20Representation%20Integration%20Technique%20%28Sparse%20MERIT%29%2C%20a%0Aflexible%20MTL%20framework%20that%20applies%20frame-wise%20expert%20routing%20over%0Aself-supervised%20speech%20representations.%20Sparse%20MERIT%20incorporates%20task-specific%0Agating%20networks%20that%20dynamically%20select%20from%20a%20shared%20pool%20of%20experts%20for%20each%0Aframe%2C%20enabling%20parameter-efficient%20and%20task-adaptive%20representation%20learning.%0AExperiments%20on%20the%20MSP-Podcast%20corpus%20show%20that%20Sparse%20MERIT%20consistently%0Aoutperforms%20baseline%20models%20on%20both%20SER%20and%20SE%20tasks.%20Under%20the%20most%0Achallenging%20condition%20of%20-5%20dB%20signal-to-noise%20ratio%20%28SNR%29%2C%20Sparse%20MERIT%0Aimproves%20SER%20F1-macro%20by%20an%20average%20of%2012.0%25%20over%20a%20baseline%20relying%20on%20a%20SE%0Apre-processing%20strategy%2C%20and%20by%203.4%25%20over%20a%20naive%20MTL%20baseline%2C%20with%0Astatistical%20significance%20on%20unseen%20noise%20conditions.%20For%20SE%2C%20Sparse%20MERIT%0Aimproves%20segmental%20SNR%20%28SSNR%29%20by%2028.2%25%20over%20the%20SE%20pre-processing%20baseline%20and%0Aby%2020.0%25%20over%20the%20naive%20MTL%20baseline.%20These%20results%20demonstrate%20that%20Sparse%0AMERIT%20provides%20robust%20and%20generalizable%20performance%20for%20both%20emotion%0Arecognition%20and%20enhancement%20tasks%20in%20noisy%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Learning%2520using%2520Mixture-of-Expert-Based%2520Representation%2520for%2520Enhanced%250A%2520%2520Speech%2520Generation%2520and%2520Robust%2520Emotion%2520Recognition%26entry.906535625%3DJing-Tong%2520Tzeng%2520and%2520Carlos%2520Busso%2520and%2520Chi-Chun%2520Lee%26entry.1292438233%3D%2520%2520Speech%2520emotion%2520recognition%2520%2528SER%2529%2520plays%2520a%2520critical%2520role%2520in%2520building%250Aemotion-aware%2520speech%2520systems%252C%2520but%2520its%2520performance%2520degrades%2520significantly%2520under%250Anoisy%2520conditions.%2520Although%2520speech%2520enhancement%2520%2528SE%2529%2520can%2520improve%2520robustness%252C%2520it%250Aoften%2520introduces%2520artifacts%2520that%2520obscure%2520emotional%2520cues%2520and%2520adds%2520computational%250Aoverhead%2520to%2520the%2520pipeline.%2520Multi-task%2520learning%2520%2528MTL%2529%2520offers%2520an%2520alternative%2520by%250Ajointly%2520optimizing%2520SE%2520and%2520SER%2520tasks.%2520However%252C%2520conventional%2520shared-backbone%250Amodels%2520frequently%2520suffer%2520from%2520gradient%2520interference%2520and%2520representational%250Aconflicts%2520between%2520tasks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%2520Sparse%250AMixture-of-Experts%2520Representation%2520Integration%2520Technique%2520%2528Sparse%2520MERIT%2529%252C%2520a%250Aflexible%2520MTL%2520framework%2520that%2520applies%2520frame-wise%2520expert%2520routing%2520over%250Aself-supervised%2520speech%2520representations.%2520Sparse%2520MERIT%2520incorporates%2520task-specific%250Agating%2520networks%2520that%2520dynamically%2520select%2520from%2520a%2520shared%2520pool%2520of%2520experts%2520for%2520each%250Aframe%252C%2520enabling%2520parameter-efficient%2520and%2520task-adaptive%2520representation%2520learning.%250AExperiments%2520on%2520the%2520MSP-Podcast%2520corpus%2520show%2520that%2520Sparse%2520MERIT%2520consistently%250Aoutperforms%2520baseline%2520models%2520on%2520both%2520SER%2520and%2520SE%2520tasks.%2520Under%2520the%2520most%250Achallenging%2520condition%2520of%2520-5%2520dB%2520signal-to-noise%2520ratio%2520%2528SNR%2529%252C%2520Sparse%2520MERIT%250Aimproves%2520SER%2520F1-macro%2520by%2520an%2520average%2520of%252012.0%2525%2520over%2520a%2520baseline%2520relying%2520on%2520a%2520SE%250Apre-processing%2520strategy%252C%2520and%2520by%25203.4%2525%2520over%2520a%2520naive%2520MTL%2520baseline%252C%2520with%250Astatistical%2520significance%2520on%2520unseen%2520noise%2520conditions.%2520For%2520SE%252C%2520Sparse%2520MERIT%250Aimproves%2520segmental%2520SNR%2520%2528SSNR%2529%2520by%252028.2%2525%2520over%2520the%2520SE%2520pre-processing%2520baseline%2520and%250Aby%252020.0%2525%2520over%2520the%2520naive%2520MTL%2520baseline.%2520These%2520results%2520demonstrate%2520that%2520Sparse%250AMERIT%2520provides%2520robust%2520and%2520generalizable%2520performance%2520for%2520both%2520emotion%250Arecognition%2520and%2520enhancement%2520tasks%2520in%2520noisy%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Learning%20using%20Mixture-of-Expert-Based%20Representation%20for%20Enhanced%0A%20%20Speech%20Generation%20and%20Robust%20Emotion%20Recognition&entry.906535625=Jing-Tong%20Tzeng%20and%20Carlos%20Busso%20and%20Chi-Chun%20Lee&entry.1292438233=%20%20Speech%20emotion%20recognition%20%28SER%29%20plays%20a%20critical%20role%20in%20building%0Aemotion-aware%20speech%20systems%2C%20but%20its%20performance%20degrades%20significantly%20under%0Anoisy%20conditions.%20Although%20speech%20enhancement%20%28SE%29%20can%20improve%20robustness%2C%20it%0Aoften%20introduces%20artifacts%20that%20obscure%20emotional%20cues%20and%20adds%20computational%0Aoverhead%20to%20the%20pipeline.%20Multi-task%20learning%20%28MTL%29%20offers%20an%20alternative%20by%0Ajointly%20optimizing%20SE%20and%20SER%20tasks.%20However%2C%20conventional%20shared-backbone%0Amodels%20frequently%20suffer%20from%20gradient%20interference%20and%20representational%0Aconflicts%20between%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20Sparse%0AMixture-of-Experts%20Representation%20Integration%20Technique%20%28Sparse%20MERIT%29%2C%20a%0Aflexible%20MTL%20framework%20that%20applies%20frame-wise%20expert%20routing%20over%0Aself-supervised%20speech%20representations.%20Sparse%20MERIT%20incorporates%20task-specific%0Agating%20networks%20that%20dynamically%20select%20from%20a%20shared%20pool%20of%20experts%20for%20each%0Aframe%2C%20enabling%20parameter-efficient%20and%20task-adaptive%20representation%20learning.%0AExperiments%20on%20the%20MSP-Podcast%20corpus%20show%20that%20Sparse%20MERIT%20consistently%0Aoutperforms%20baseline%20models%20on%20both%20SER%20and%20SE%20tasks.%20Under%20the%20most%0Achallenging%20condition%20of%20-5%20dB%20signal-to-noise%20ratio%20%28SNR%29%2C%20Sparse%20MERIT%0Aimproves%20SER%20F1-macro%20by%20an%20average%20of%2012.0%25%20over%20a%20baseline%20relying%20on%20a%20SE%0Apre-processing%20strategy%2C%20and%20by%203.4%25%20over%20a%20naive%20MTL%20baseline%2C%20with%0Astatistical%20significance%20on%20unseen%20noise%20conditions.%20For%20SE%2C%20Sparse%20MERIT%0Aimproves%20segmental%20SNR%20%28SSNR%29%20by%2028.2%25%20over%20the%20SE%20pre-processing%20baseline%20and%0Aby%2020.0%25%20over%20the%20naive%20MTL%20baseline.%20These%20results%20demonstrate%20that%20Sparse%0AMERIT%20provides%20robust%20and%20generalizable%20performance%20for%20both%20emotion%0Arecognition%20and%20enhancement%20tasks%20in%20noisy%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08470v1&entry.124074799=Read"},
{"title": "PrediTree: A Multi-Temporal Sub-meter Dataset of Multi-Spectral Imagery\n  Aligned With Canopy Height Maps", "author": "Hiyam Debary and Mustansar Fiaz and Levente Klein", "abstract": "  We present PrediTree, the first comprehensive open-source dataset designed\nfor training and evaluating tree height prediction models at sub-meter\nresolution. This dataset combines very high-resolution (0.5m) LiDAR-derived\ncanopy height maps, spatially aligned with multi-temporal and multi-spectral\nimagery, across diverse forest ecosystems in France, totaling 3,141,568 images.\nPrediTree addresses a critical gap in forest monitoring capabilities by\nenabling the training of deep learning methods that can predict tree growth\nbased on multiple past observations. To make use of this PrediTree dataset, we\npropose an encoder-decoder framework that requires the multi-temporal\nmulti-spectral imagery and the relative time differences in years between the\ncanopy height map timestamp (target) and each image acquisition date for which\nthis framework predicts the canopy height. The conducted experiments\ndemonstrate that a U-Net architecture trained on the PrediTree dataset provides\nthe highest masked mean squared error of $11.78\\%$, outperforming the next-best\narchitecture, ResNet-50, by around $12\\%$, and cutting the error of the same\nexperiments but on fewer bands (red, green, blue only), by around $30\\%$. This\ndataset is publicly available on\nhttps://huggingface.co/datasets/hiyam-d/PrediTree, and both processing and\ntraining codebases are available on {GitHub}.\n", "link": "http://arxiv.org/abs/2509.01202v2", "date": "2025-09-10", "relevancy": 1.9686, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5004}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.488}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrediTree%3A%20A%20Multi-Temporal%20Sub-meter%20Dataset%20of%20Multi-Spectral%20Imagery%0A%20%20Aligned%20With%20Canopy%20Height%20Maps&body=Title%3A%20PrediTree%3A%20A%20Multi-Temporal%20Sub-meter%20Dataset%20of%20Multi-Spectral%20Imagery%0A%20%20Aligned%20With%20Canopy%20Height%20Maps%0AAuthor%3A%20Hiyam%20Debary%20and%20Mustansar%20Fiaz%20and%20Levente%20Klein%0AAbstract%3A%20%20%20We%20present%20PrediTree%2C%20the%20first%20comprehensive%20open-source%20dataset%20designed%0Afor%20training%20and%20evaluating%20tree%20height%20prediction%20models%20at%20sub-meter%0Aresolution.%20This%20dataset%20combines%20very%20high-resolution%20%280.5m%29%20LiDAR-derived%0Acanopy%20height%20maps%2C%20spatially%20aligned%20with%20multi-temporal%20and%20multi-spectral%0Aimagery%2C%20across%20diverse%20forest%20ecosystems%20in%20France%2C%20totaling%203%2C141%2C568%20images.%0APrediTree%20addresses%20a%20critical%20gap%20in%20forest%20monitoring%20capabilities%20by%0Aenabling%20the%20training%20of%20deep%20learning%20methods%20that%20can%20predict%20tree%20growth%0Abased%20on%20multiple%20past%20observations.%20To%20make%20use%20of%20this%20PrediTree%20dataset%2C%20we%0Apropose%20an%20encoder-decoder%20framework%20that%20requires%20the%20multi-temporal%0Amulti-spectral%20imagery%20and%20the%20relative%20time%20differences%20in%20years%20between%20the%0Acanopy%20height%20map%20timestamp%20%28target%29%20and%20each%20image%20acquisition%20date%20for%20which%0Athis%20framework%20predicts%20the%20canopy%20height.%20The%20conducted%20experiments%0Ademonstrate%20that%20a%20U-Net%20architecture%20trained%20on%20the%20PrediTree%20dataset%20provides%0Athe%20highest%20masked%20mean%20squared%20error%20of%20%2411.78%5C%25%24%2C%20outperforming%20the%20next-best%0Aarchitecture%2C%20ResNet-50%2C%20by%20around%20%2412%5C%25%24%2C%20and%20cutting%20the%20error%20of%20the%20same%0Aexperiments%20but%20on%20fewer%20bands%20%28red%2C%20green%2C%20blue%20only%29%2C%20by%20around%20%2430%5C%25%24.%20This%0Adataset%20is%20publicly%20available%20on%0Ahttps%3A//huggingface.co/datasets/hiyam-d/PrediTree%2C%20and%20both%20processing%20and%0Atraining%20codebases%20are%20available%20on%20%7BGitHub%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediTree%253A%2520A%2520Multi-Temporal%2520Sub-meter%2520Dataset%2520of%2520Multi-Spectral%2520Imagery%250A%2520%2520Aligned%2520With%2520Canopy%2520Height%2520Maps%26entry.906535625%3DHiyam%2520Debary%2520and%2520Mustansar%2520Fiaz%2520and%2520Levente%2520Klein%26entry.1292438233%3D%2520%2520We%2520present%2520PrediTree%252C%2520the%2520first%2520comprehensive%2520open-source%2520dataset%2520designed%250Afor%2520training%2520and%2520evaluating%2520tree%2520height%2520prediction%2520models%2520at%2520sub-meter%250Aresolution.%2520This%2520dataset%2520combines%2520very%2520high-resolution%2520%25280.5m%2529%2520LiDAR-derived%250Acanopy%2520height%2520maps%252C%2520spatially%2520aligned%2520with%2520multi-temporal%2520and%2520multi-spectral%250Aimagery%252C%2520across%2520diverse%2520forest%2520ecosystems%2520in%2520France%252C%2520totaling%25203%252C141%252C568%2520images.%250APrediTree%2520addresses%2520a%2520critical%2520gap%2520in%2520forest%2520monitoring%2520capabilities%2520by%250Aenabling%2520the%2520training%2520of%2520deep%2520learning%2520methods%2520that%2520can%2520predict%2520tree%2520growth%250Abased%2520on%2520multiple%2520past%2520observations.%2520To%2520make%2520use%2520of%2520this%2520PrediTree%2520dataset%252C%2520we%250Apropose%2520an%2520encoder-decoder%2520framework%2520that%2520requires%2520the%2520multi-temporal%250Amulti-spectral%2520imagery%2520and%2520the%2520relative%2520time%2520differences%2520in%2520years%2520between%2520the%250Acanopy%2520height%2520map%2520timestamp%2520%2528target%2529%2520and%2520each%2520image%2520acquisition%2520date%2520for%2520which%250Athis%2520framework%2520predicts%2520the%2520canopy%2520height.%2520The%2520conducted%2520experiments%250Ademonstrate%2520that%2520a%2520U-Net%2520architecture%2520trained%2520on%2520the%2520PrediTree%2520dataset%2520provides%250Athe%2520highest%2520masked%2520mean%2520squared%2520error%2520of%2520%252411.78%255C%2525%2524%252C%2520outperforming%2520the%2520next-best%250Aarchitecture%252C%2520ResNet-50%252C%2520by%2520around%2520%252412%255C%2525%2524%252C%2520and%2520cutting%2520the%2520error%2520of%2520the%2520same%250Aexperiments%2520but%2520on%2520fewer%2520bands%2520%2528red%252C%2520green%252C%2520blue%2520only%2529%252C%2520by%2520around%2520%252430%255C%2525%2524.%2520This%250Adataset%2520is%2520publicly%2520available%2520on%250Ahttps%253A//huggingface.co/datasets/hiyam-d/PrediTree%252C%2520and%2520both%2520processing%2520and%250Atraining%2520codebases%2520are%2520available%2520on%2520%257BGitHub%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrediTree%3A%20A%20Multi-Temporal%20Sub-meter%20Dataset%20of%20Multi-Spectral%20Imagery%0A%20%20Aligned%20With%20Canopy%20Height%20Maps&entry.906535625=Hiyam%20Debary%20and%20Mustansar%20Fiaz%20and%20Levente%20Klein&entry.1292438233=%20%20We%20present%20PrediTree%2C%20the%20first%20comprehensive%20open-source%20dataset%20designed%0Afor%20training%20and%20evaluating%20tree%20height%20prediction%20models%20at%20sub-meter%0Aresolution.%20This%20dataset%20combines%20very%20high-resolution%20%280.5m%29%20LiDAR-derived%0Acanopy%20height%20maps%2C%20spatially%20aligned%20with%20multi-temporal%20and%20multi-spectral%0Aimagery%2C%20across%20diverse%20forest%20ecosystems%20in%20France%2C%20totaling%203%2C141%2C568%20images.%0APrediTree%20addresses%20a%20critical%20gap%20in%20forest%20monitoring%20capabilities%20by%0Aenabling%20the%20training%20of%20deep%20learning%20methods%20that%20can%20predict%20tree%20growth%0Abased%20on%20multiple%20past%20observations.%20To%20make%20use%20of%20this%20PrediTree%20dataset%2C%20we%0Apropose%20an%20encoder-decoder%20framework%20that%20requires%20the%20multi-temporal%0Amulti-spectral%20imagery%20and%20the%20relative%20time%20differences%20in%20years%20between%20the%0Acanopy%20height%20map%20timestamp%20%28target%29%20and%20each%20image%20acquisition%20date%20for%20which%0Athis%20framework%20predicts%20the%20canopy%20height.%20The%20conducted%20experiments%0Ademonstrate%20that%20a%20U-Net%20architecture%20trained%20on%20the%20PrediTree%20dataset%20provides%0Athe%20highest%20masked%20mean%20squared%20error%20of%20%2411.78%5C%25%24%2C%20outperforming%20the%20next-best%0Aarchitecture%2C%20ResNet-50%2C%20by%20around%20%2412%5C%25%24%2C%20and%20cutting%20the%20error%20of%20the%20same%0Aexperiments%20but%20on%20fewer%20bands%20%28red%2C%20green%2C%20blue%20only%29%2C%20by%20around%20%2430%5C%25%24.%20This%0Adataset%20is%20publicly%20available%20on%0Ahttps%3A//huggingface.co/datasets/hiyam-d/PrediTree%2C%20and%20both%20processing%20and%0Atraining%20codebases%20are%20available%20on%20%7BGitHub%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01202v2&entry.124074799=Read"},
{"title": "Learning Fluid-Structure Interaction Dynamics with Physics-Informed\n  Neural Networks and Immersed Boundary Methods", "author": "Afrah Farea and Saiful Khan and Reza Daryani and Emre Cenk Ersan and Mustafa Serdar Celebi", "abstract": "  Physics-informed neural networks (PINNs) have emerged as a promising approach\nfor solving complex fluid dynamics problems, yet their application to\nfluid-structure interaction (FSI) problems with moving boundaries remains\nlargely unexplored. This work addresses the critical challenge of modeling FSI\nsystems with deformable interfaces, where traditional unified PINN\narchitectures struggle to capture the distinct physics governing fluid and\nstructural domains simultaneously. We present an innovative Eulerian-Lagrangian\nPINN architecture that integrates immersed boundary method (IBM) principles to\nsolve FSI problems with moving boundary conditions. Our approach fundamentally\ndeparts from conventional unified architectures by introducing domain-specific\nneural networks: an Eulerian network for fluid dynamics and a Lagrangian\nnetwork for structural interfaces, coupled through physics-based constraints.\nAdditionally, we incorporate learnable B-spline activation functions with SiLU\nto capture both localized high-gradient features near interfaces and global\nflow patterns. Empirical studies on a 2D cavity flow problem involving a moving\nsolid structure show that while baseline unified PINNs achieve reasonable\nvelocity predictions, they suffer from substantial pressure errors (12.9%) in\nstructural regions. Our Eulerian-Lagrangian architecture with learnable\nactivations (EL-L) achieves better performance across all metrics, improving\naccuracy by 24.1-91.4% and particularly reducing pressure errors from 12.9% to\n2.39%. These results demonstrate that domain decomposition aligned with\nphysical principles, combined with locality-aware activation functions, is\nessential for accurate FSI modeling within the PINN framework.\n", "link": "http://arxiv.org/abs/2505.18565v4", "date": "2025-09-10", "relevancy": 1.9582, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4936}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Fluid-Structure%20Interaction%20Dynamics%20with%20Physics-Informed%0A%20%20Neural%20Networks%20and%20Immersed%20Boundary%20Methods&body=Title%3A%20Learning%20Fluid-Structure%20Interaction%20Dynamics%20with%20Physics-Informed%0A%20%20Neural%20Networks%20and%20Immersed%20Boundary%20Methods%0AAuthor%3A%20Afrah%20Farea%20and%20Saiful%20Khan%20and%20Reza%20Daryani%20and%20Emre%20Cenk%20Ersan%20and%20Mustafa%20Serdar%20Celebi%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20have%20emerged%20as%20a%20promising%20approach%0Afor%20solving%20complex%20fluid%20dynamics%20problems%2C%20yet%20their%20application%20to%0Afluid-structure%20interaction%20%28FSI%29%20problems%20with%20moving%20boundaries%20remains%0Alargely%20unexplored.%20This%20work%20addresses%20the%20critical%20challenge%20of%20modeling%20FSI%0Asystems%20with%20deformable%20interfaces%2C%20where%20traditional%20unified%20PINN%0Aarchitectures%20struggle%20to%20capture%20the%20distinct%20physics%20governing%20fluid%20and%0Astructural%20domains%20simultaneously.%20We%20present%20an%20innovative%20Eulerian-Lagrangian%0APINN%20architecture%20that%20integrates%20immersed%20boundary%20method%20%28IBM%29%20principles%20to%0Asolve%20FSI%20problems%20with%20moving%20boundary%20conditions.%20Our%20approach%20fundamentally%0Adeparts%20from%20conventional%20unified%20architectures%20by%20introducing%20domain-specific%0Aneural%20networks%3A%20an%20Eulerian%20network%20for%20fluid%20dynamics%20and%20a%20Lagrangian%0Anetwork%20for%20structural%20interfaces%2C%20coupled%20through%20physics-based%20constraints.%0AAdditionally%2C%20we%20incorporate%20learnable%20B-spline%20activation%20functions%20with%20SiLU%0Ato%20capture%20both%20localized%20high-gradient%20features%20near%20interfaces%20and%20global%0Aflow%20patterns.%20Empirical%20studies%20on%20a%202D%20cavity%20flow%20problem%20involving%20a%20moving%0Asolid%20structure%20show%20that%20while%20baseline%20unified%20PINNs%20achieve%20reasonable%0Avelocity%20predictions%2C%20they%20suffer%20from%20substantial%20pressure%20errors%20%2812.9%25%29%20in%0Astructural%20regions.%20Our%20Eulerian-Lagrangian%20architecture%20with%20learnable%0Aactivations%20%28EL-L%29%20achieves%20better%20performance%20across%20all%20metrics%2C%20improving%0Aaccuracy%20by%2024.1-91.4%25%20and%20particularly%20reducing%20pressure%20errors%20from%2012.9%25%20to%0A2.39%25.%20These%20results%20demonstrate%20that%20domain%20decomposition%20aligned%20with%0Aphysical%20principles%2C%20combined%20with%20locality-aware%20activation%20functions%2C%20is%0Aessential%20for%20accurate%20FSI%20modeling%20within%20the%20PINN%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18565v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Fluid-Structure%2520Interaction%2520Dynamics%2520with%2520Physics-Informed%250A%2520%2520Neural%2520Networks%2520and%2520Immersed%2520Boundary%2520Methods%26entry.906535625%3DAfrah%2520Farea%2520and%2520Saiful%2520Khan%2520and%2520Reza%2520Daryani%2520and%2520Emre%2520Cenk%2520Ersan%2520and%2520Mustafa%2520Serdar%2520Celebi%26entry.1292438233%3D%2520%2520Physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520approach%250Afor%2520solving%2520complex%2520fluid%2520dynamics%2520problems%252C%2520yet%2520their%2520application%2520to%250Afluid-structure%2520interaction%2520%2528FSI%2529%2520problems%2520with%2520moving%2520boundaries%2520remains%250Alargely%2520unexplored.%2520This%2520work%2520addresses%2520the%2520critical%2520challenge%2520of%2520modeling%2520FSI%250Asystems%2520with%2520deformable%2520interfaces%252C%2520where%2520traditional%2520unified%2520PINN%250Aarchitectures%2520struggle%2520to%2520capture%2520the%2520distinct%2520physics%2520governing%2520fluid%2520and%250Astructural%2520domains%2520simultaneously.%2520We%2520present%2520an%2520innovative%2520Eulerian-Lagrangian%250APINN%2520architecture%2520that%2520integrates%2520immersed%2520boundary%2520method%2520%2528IBM%2529%2520principles%2520to%250Asolve%2520FSI%2520problems%2520with%2520moving%2520boundary%2520conditions.%2520Our%2520approach%2520fundamentally%250Adeparts%2520from%2520conventional%2520unified%2520architectures%2520by%2520introducing%2520domain-specific%250Aneural%2520networks%253A%2520an%2520Eulerian%2520network%2520for%2520fluid%2520dynamics%2520and%2520a%2520Lagrangian%250Anetwork%2520for%2520structural%2520interfaces%252C%2520coupled%2520through%2520physics-based%2520constraints.%250AAdditionally%252C%2520we%2520incorporate%2520learnable%2520B-spline%2520activation%2520functions%2520with%2520SiLU%250Ato%2520capture%2520both%2520localized%2520high-gradient%2520features%2520near%2520interfaces%2520and%2520global%250Aflow%2520patterns.%2520Empirical%2520studies%2520on%2520a%25202D%2520cavity%2520flow%2520problem%2520involving%2520a%2520moving%250Asolid%2520structure%2520show%2520that%2520while%2520baseline%2520unified%2520PINNs%2520achieve%2520reasonable%250Avelocity%2520predictions%252C%2520they%2520suffer%2520from%2520substantial%2520pressure%2520errors%2520%252812.9%2525%2529%2520in%250Astructural%2520regions.%2520Our%2520Eulerian-Lagrangian%2520architecture%2520with%2520learnable%250Aactivations%2520%2528EL-L%2529%2520achieves%2520better%2520performance%2520across%2520all%2520metrics%252C%2520improving%250Aaccuracy%2520by%252024.1-91.4%2525%2520and%2520particularly%2520reducing%2520pressure%2520errors%2520from%252012.9%2525%2520to%250A2.39%2525.%2520These%2520results%2520demonstrate%2520that%2520domain%2520decomposition%2520aligned%2520with%250Aphysical%2520principles%252C%2520combined%2520with%2520locality-aware%2520activation%2520functions%252C%2520is%250Aessential%2520for%2520accurate%2520FSI%2520modeling%2520within%2520the%2520PINN%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18565v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Fluid-Structure%20Interaction%20Dynamics%20with%20Physics-Informed%0A%20%20Neural%20Networks%20and%20Immersed%20Boundary%20Methods&entry.906535625=Afrah%20Farea%20and%20Saiful%20Khan%20and%20Reza%20Daryani%20and%20Emre%20Cenk%20Ersan%20and%20Mustafa%20Serdar%20Celebi&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20have%20emerged%20as%20a%20promising%20approach%0Afor%20solving%20complex%20fluid%20dynamics%20problems%2C%20yet%20their%20application%20to%0Afluid-structure%20interaction%20%28FSI%29%20problems%20with%20moving%20boundaries%20remains%0Alargely%20unexplored.%20This%20work%20addresses%20the%20critical%20challenge%20of%20modeling%20FSI%0Asystems%20with%20deformable%20interfaces%2C%20where%20traditional%20unified%20PINN%0Aarchitectures%20struggle%20to%20capture%20the%20distinct%20physics%20governing%20fluid%20and%0Astructural%20domains%20simultaneously.%20We%20present%20an%20innovative%20Eulerian-Lagrangian%0APINN%20architecture%20that%20integrates%20immersed%20boundary%20method%20%28IBM%29%20principles%20to%0Asolve%20FSI%20problems%20with%20moving%20boundary%20conditions.%20Our%20approach%20fundamentally%0Adeparts%20from%20conventional%20unified%20architectures%20by%20introducing%20domain-specific%0Aneural%20networks%3A%20an%20Eulerian%20network%20for%20fluid%20dynamics%20and%20a%20Lagrangian%0Anetwork%20for%20structural%20interfaces%2C%20coupled%20through%20physics-based%20constraints.%0AAdditionally%2C%20we%20incorporate%20learnable%20B-spline%20activation%20functions%20with%20SiLU%0Ato%20capture%20both%20localized%20high-gradient%20features%20near%20interfaces%20and%20global%0Aflow%20patterns.%20Empirical%20studies%20on%20a%202D%20cavity%20flow%20problem%20involving%20a%20moving%0Asolid%20structure%20show%20that%20while%20baseline%20unified%20PINNs%20achieve%20reasonable%0Avelocity%20predictions%2C%20they%20suffer%20from%20substantial%20pressure%20errors%20%2812.9%25%29%20in%0Astructural%20regions.%20Our%20Eulerian-Lagrangian%20architecture%20with%20learnable%0Aactivations%20%28EL-L%29%20achieves%20better%20performance%20across%20all%20metrics%2C%20improving%0Aaccuracy%20by%2024.1-91.4%25%20and%20particularly%20reducing%20pressure%20errors%20from%2012.9%25%20to%0A2.39%25.%20These%20results%20demonstrate%20that%20domain%20decomposition%20aligned%20with%0Aphysical%20principles%2C%20combined%20with%20locality-aware%20activation%20functions%2C%20is%0Aessential%20for%20accurate%20FSI%20modeling%20within%20the%20PINN%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18565v4&entry.124074799=Read"},
{"title": "Heart Disease Prediction: A Comparative Study of Optimisers Performance\n  in Deep Neural Networks", "author": "Chisom Chibuike and Adeyinka Ogunsanya", "abstract": "  Optimization has been an important factor and topic of interest in training\ndeep learning models, yet less attention has been given to how we select the\noptimizers we use to train these models. Hence, there is a need to dive deeper\ninto how we select the optimizers we use for training and the metrics that\ndetermine this selection. In this work, we compare the performance of 10\ndifferent optimizers in training a simple Multi-layer Perceptron model using a\nheart disease dataset from Kaggle. We set up a consistent training paradigm and\nevaluate the optimizers based on metrics such as convergence speed and\nstability. We also include some other Machine Learning Evaluation metrics such\nas AUC, Precision, and Recall, which are central metrics to classification\nproblems. Our results show that there are trade-offs between convergence speed\nand stability, as optimizers like Adagrad and Adadelta, which are more stable,\ntook longer time to converge. Across all our metrics, we chose RMSProp to be\nthe most effective optimizer for this heart disease prediction task because it\noffered a balanced performance across key metrics. It achieved a precision of\n0.765, a recall of 0.827, and an AUC of 0.841, along with faster training time.\nHowever, it was not the most stable. We recommend that, in less\ncompute-constrained environments, this method of choosing optimizers through a\nthorough evaluation should be adopted to increase the scientific nature and\nperformance in training deep learning models.\n", "link": "http://arxiv.org/abs/2509.08499v1", "date": "2025-09-10", "relevancy": 1.9369, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5244}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4573}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heart%20Disease%20Prediction%3A%20A%20Comparative%20Study%20of%20Optimisers%20Performance%0A%20%20in%20Deep%20Neural%20Networks&body=Title%3A%20Heart%20Disease%20Prediction%3A%20A%20Comparative%20Study%20of%20Optimisers%20Performance%0A%20%20in%20Deep%20Neural%20Networks%0AAuthor%3A%20Chisom%20Chibuike%20and%20Adeyinka%20Ogunsanya%0AAbstract%3A%20%20%20Optimization%20has%20been%20an%20important%20factor%20and%20topic%20of%20interest%20in%20training%0Adeep%20learning%20models%2C%20yet%20less%20attention%20has%20been%20given%20to%20how%20we%20select%20the%0Aoptimizers%20we%20use%20to%20train%20these%20models.%20Hence%2C%20there%20is%20a%20need%20to%20dive%20deeper%0Ainto%20how%20we%20select%20the%20optimizers%20we%20use%20for%20training%20and%20the%20metrics%20that%0Adetermine%20this%20selection.%20In%20this%20work%2C%20we%20compare%20the%20performance%20of%2010%0Adifferent%20optimizers%20in%20training%20a%20simple%20Multi-layer%20Perceptron%20model%20using%20a%0Aheart%20disease%20dataset%20from%20Kaggle.%20We%20set%20up%20a%20consistent%20training%20paradigm%20and%0Aevaluate%20the%20optimizers%20based%20on%20metrics%20such%20as%20convergence%20speed%20and%0Astability.%20We%20also%20include%20some%20other%20Machine%20Learning%20Evaluation%20metrics%20such%0Aas%20AUC%2C%20Precision%2C%20and%20Recall%2C%20which%20are%20central%20metrics%20to%20classification%0Aproblems.%20Our%20results%20show%20that%20there%20are%20trade-offs%20between%20convergence%20speed%0Aand%20stability%2C%20as%20optimizers%20like%20Adagrad%20and%20Adadelta%2C%20which%20are%20more%20stable%2C%0Atook%20longer%20time%20to%20converge.%20Across%20all%20our%20metrics%2C%20we%20chose%20RMSProp%20to%20be%0Athe%20most%20effective%20optimizer%20for%20this%20heart%20disease%20prediction%20task%20because%20it%0Aoffered%20a%20balanced%20performance%20across%20key%20metrics.%20It%20achieved%20a%20precision%20of%0A0.765%2C%20a%20recall%20of%200.827%2C%20and%20an%20AUC%20of%200.841%2C%20along%20with%20faster%20training%20time.%0AHowever%2C%20it%20was%20not%20the%20most%20stable.%20We%20recommend%20that%2C%20in%20less%0Acompute-constrained%20environments%2C%20this%20method%20of%20choosing%20optimizers%20through%20a%0Athorough%20evaluation%20should%20be%20adopted%20to%20increase%20the%20scientific%20nature%20and%0Aperformance%20in%20training%20deep%20learning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeart%2520Disease%2520Prediction%253A%2520A%2520Comparative%2520Study%2520of%2520Optimisers%2520Performance%250A%2520%2520in%2520Deep%2520Neural%2520Networks%26entry.906535625%3DChisom%2520Chibuike%2520and%2520Adeyinka%2520Ogunsanya%26entry.1292438233%3D%2520%2520Optimization%2520has%2520been%2520an%2520important%2520factor%2520and%2520topic%2520of%2520interest%2520in%2520training%250Adeep%2520learning%2520models%252C%2520yet%2520less%2520attention%2520has%2520been%2520given%2520to%2520how%2520we%2520select%2520the%250Aoptimizers%2520we%2520use%2520to%2520train%2520these%2520models.%2520Hence%252C%2520there%2520is%2520a%2520need%2520to%2520dive%2520deeper%250Ainto%2520how%2520we%2520select%2520the%2520optimizers%2520we%2520use%2520for%2520training%2520and%2520the%2520metrics%2520that%250Adetermine%2520this%2520selection.%2520In%2520this%2520work%252C%2520we%2520compare%2520the%2520performance%2520of%252010%250Adifferent%2520optimizers%2520in%2520training%2520a%2520simple%2520Multi-layer%2520Perceptron%2520model%2520using%2520a%250Aheart%2520disease%2520dataset%2520from%2520Kaggle.%2520We%2520set%2520up%2520a%2520consistent%2520training%2520paradigm%2520and%250Aevaluate%2520the%2520optimizers%2520based%2520on%2520metrics%2520such%2520as%2520convergence%2520speed%2520and%250Astability.%2520We%2520also%2520include%2520some%2520other%2520Machine%2520Learning%2520Evaluation%2520metrics%2520such%250Aas%2520AUC%252C%2520Precision%252C%2520and%2520Recall%252C%2520which%2520are%2520central%2520metrics%2520to%2520classification%250Aproblems.%2520Our%2520results%2520show%2520that%2520there%2520are%2520trade-offs%2520between%2520convergence%2520speed%250Aand%2520stability%252C%2520as%2520optimizers%2520like%2520Adagrad%2520and%2520Adadelta%252C%2520which%2520are%2520more%2520stable%252C%250Atook%2520longer%2520time%2520to%2520converge.%2520Across%2520all%2520our%2520metrics%252C%2520we%2520chose%2520RMSProp%2520to%2520be%250Athe%2520most%2520effective%2520optimizer%2520for%2520this%2520heart%2520disease%2520prediction%2520task%2520because%2520it%250Aoffered%2520a%2520balanced%2520performance%2520across%2520key%2520metrics.%2520It%2520achieved%2520a%2520precision%2520of%250A0.765%252C%2520a%2520recall%2520of%25200.827%252C%2520and%2520an%2520AUC%2520of%25200.841%252C%2520along%2520with%2520faster%2520training%2520time.%250AHowever%252C%2520it%2520was%2520not%2520the%2520most%2520stable.%2520We%2520recommend%2520that%252C%2520in%2520less%250Acompute-constrained%2520environments%252C%2520this%2520method%2520of%2520choosing%2520optimizers%2520through%2520a%250Athorough%2520evaluation%2520should%2520be%2520adopted%2520to%2520increase%2520the%2520scientific%2520nature%2520and%250Aperformance%2520in%2520training%2520deep%2520learning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heart%20Disease%20Prediction%3A%20A%20Comparative%20Study%20of%20Optimisers%20Performance%0A%20%20in%20Deep%20Neural%20Networks&entry.906535625=Chisom%20Chibuike%20and%20Adeyinka%20Ogunsanya&entry.1292438233=%20%20Optimization%20has%20been%20an%20important%20factor%20and%20topic%20of%20interest%20in%20training%0Adeep%20learning%20models%2C%20yet%20less%20attention%20has%20been%20given%20to%20how%20we%20select%20the%0Aoptimizers%20we%20use%20to%20train%20these%20models.%20Hence%2C%20there%20is%20a%20need%20to%20dive%20deeper%0Ainto%20how%20we%20select%20the%20optimizers%20we%20use%20for%20training%20and%20the%20metrics%20that%0Adetermine%20this%20selection.%20In%20this%20work%2C%20we%20compare%20the%20performance%20of%2010%0Adifferent%20optimizers%20in%20training%20a%20simple%20Multi-layer%20Perceptron%20model%20using%20a%0Aheart%20disease%20dataset%20from%20Kaggle.%20We%20set%20up%20a%20consistent%20training%20paradigm%20and%0Aevaluate%20the%20optimizers%20based%20on%20metrics%20such%20as%20convergence%20speed%20and%0Astability.%20We%20also%20include%20some%20other%20Machine%20Learning%20Evaluation%20metrics%20such%0Aas%20AUC%2C%20Precision%2C%20and%20Recall%2C%20which%20are%20central%20metrics%20to%20classification%0Aproblems.%20Our%20results%20show%20that%20there%20are%20trade-offs%20between%20convergence%20speed%0Aand%20stability%2C%20as%20optimizers%20like%20Adagrad%20and%20Adadelta%2C%20which%20are%20more%20stable%2C%0Atook%20longer%20time%20to%20converge.%20Across%20all%20our%20metrics%2C%20we%20chose%20RMSProp%20to%20be%0Athe%20most%20effective%20optimizer%20for%20this%20heart%20disease%20prediction%20task%20because%20it%0Aoffered%20a%20balanced%20performance%20across%20key%20metrics.%20It%20achieved%20a%20precision%20of%0A0.765%2C%20a%20recall%20of%200.827%2C%20and%20an%20AUC%20of%200.841%2C%20along%20with%20faster%20training%20time.%0AHowever%2C%20it%20was%20not%20the%20most%20stable.%20We%20recommend%20that%2C%20in%20less%0Acompute-constrained%20environments%2C%20this%20method%20of%20choosing%20optimizers%20through%20a%0Athorough%20evaluation%20should%20be%20adopted%20to%20increase%20the%20scientific%20nature%20and%0Aperformance%20in%20training%20deep%20learning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08499v1&entry.124074799=Read"},
{"title": "Data Skeleton Learning: Scalable Active Clustering with Sparse Graph\n  Structures", "author": "Wen-Bo Xie and Xun Fu and Bin Chen and Yan-Li Lee and Tao Deng and Tian Zou and Xin Wang and Zhen Liu and Jaideep Srivastavad", "abstract": "  In this work, we focus on the efficiency and scalability of pairwise\nconstraint-based active clustering, crucial for processing large-scale data in\napplications such as data mining, knowledge annotation, and AI model\npre-training. Our goals are threefold: (1) to reduce computational costs for\niterative clustering updates; (2) to enhance the impact of user-provided\nconstraints to minimize annotation requirements for precise clustering; and (3)\nto cut down memory usage in practical deployments. To achieve these aims, we\npropose a graph-based active clustering algorithm that utilizes two sparse\ngraphs: one for representing relationships between data (our proposed data\nskeleton) and another for updating this data skeleton. These two graphs work in\nconcert, enabling the refinement of connected subgraphs within the data\nskeleton to create nested clusters. Our empirical analysis confirms that the\nproposed algorithm consistently facilitates more accurate clustering with\ndramatically less input of user-provided constraints, and outperforms its\ncounterparts in terms of computational performance and scalability, while\nmaintaining robustness across various distance metrics.\n", "link": "http://arxiv.org/abs/2509.08530v1", "date": "2025-09-10", "relevancy": 1.9357, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5021}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4928}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Skeleton%20Learning%3A%20Scalable%20Active%20Clustering%20with%20Sparse%20Graph%0A%20%20Structures&body=Title%3A%20Data%20Skeleton%20Learning%3A%20Scalable%20Active%20Clustering%20with%20Sparse%20Graph%0A%20%20Structures%0AAuthor%3A%20Wen-Bo%20Xie%20and%20Xun%20Fu%20and%20Bin%20Chen%20and%20Yan-Li%20Lee%20and%20Tao%20Deng%20and%20Tian%20Zou%20and%20Xin%20Wang%20and%20Zhen%20Liu%20and%20Jaideep%20Srivastavad%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20focus%20on%20the%20efficiency%20and%20scalability%20of%20pairwise%0Aconstraint-based%20active%20clustering%2C%20crucial%20for%20processing%20large-scale%20data%20in%0Aapplications%20such%20as%20data%20mining%2C%20knowledge%20annotation%2C%20and%20AI%20model%0Apre-training.%20Our%20goals%20are%20threefold%3A%20%281%29%20to%20reduce%20computational%20costs%20for%0Aiterative%20clustering%20updates%3B%20%282%29%20to%20enhance%20the%20impact%20of%20user-provided%0Aconstraints%20to%20minimize%20annotation%20requirements%20for%20precise%20clustering%3B%20and%20%283%29%0Ato%20cut%20down%20memory%20usage%20in%20practical%20deployments.%20To%20achieve%20these%20aims%2C%20we%0Apropose%20a%20graph-based%20active%20clustering%20algorithm%20that%20utilizes%20two%20sparse%0Agraphs%3A%20one%20for%20representing%20relationships%20between%20data%20%28our%20proposed%20data%0Askeleton%29%20and%20another%20for%20updating%20this%20data%20skeleton.%20These%20two%20graphs%20work%20in%0Aconcert%2C%20enabling%20the%20refinement%20of%20connected%20subgraphs%20within%20the%20data%0Askeleton%20to%20create%20nested%20clusters.%20Our%20empirical%20analysis%20confirms%20that%20the%0Aproposed%20algorithm%20consistently%20facilitates%20more%20accurate%20clustering%20with%0Adramatically%20less%20input%20of%20user-provided%20constraints%2C%20and%20outperforms%20its%0Acounterparts%20in%20terms%20of%20computational%20performance%20and%20scalability%2C%20while%0Amaintaining%20robustness%20across%20various%20distance%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Skeleton%2520Learning%253A%2520Scalable%2520Active%2520Clustering%2520with%2520Sparse%2520Graph%250A%2520%2520Structures%26entry.906535625%3DWen-Bo%2520Xie%2520and%2520Xun%2520Fu%2520and%2520Bin%2520Chen%2520and%2520Yan-Li%2520Lee%2520and%2520Tao%2520Deng%2520and%2520Tian%2520Zou%2520and%2520Xin%2520Wang%2520and%2520Zhen%2520Liu%2520and%2520Jaideep%2520Srivastavad%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%2520efficiency%2520and%2520scalability%2520of%2520pairwise%250Aconstraint-based%2520active%2520clustering%252C%2520crucial%2520for%2520processing%2520large-scale%2520data%2520in%250Aapplications%2520such%2520as%2520data%2520mining%252C%2520knowledge%2520annotation%252C%2520and%2520AI%2520model%250Apre-training.%2520Our%2520goals%2520are%2520threefold%253A%2520%25281%2529%2520to%2520reduce%2520computational%2520costs%2520for%250Aiterative%2520clustering%2520updates%253B%2520%25282%2529%2520to%2520enhance%2520the%2520impact%2520of%2520user-provided%250Aconstraints%2520to%2520minimize%2520annotation%2520requirements%2520for%2520precise%2520clustering%253B%2520and%2520%25283%2529%250Ato%2520cut%2520down%2520memory%2520usage%2520in%2520practical%2520deployments.%2520To%2520achieve%2520these%2520aims%252C%2520we%250Apropose%2520a%2520graph-based%2520active%2520clustering%2520algorithm%2520that%2520utilizes%2520two%2520sparse%250Agraphs%253A%2520one%2520for%2520representing%2520relationships%2520between%2520data%2520%2528our%2520proposed%2520data%250Askeleton%2529%2520and%2520another%2520for%2520updating%2520this%2520data%2520skeleton.%2520These%2520two%2520graphs%2520work%2520in%250Aconcert%252C%2520enabling%2520the%2520refinement%2520of%2520connected%2520subgraphs%2520within%2520the%2520data%250Askeleton%2520to%2520create%2520nested%2520clusters.%2520Our%2520empirical%2520analysis%2520confirms%2520that%2520the%250Aproposed%2520algorithm%2520consistently%2520facilitates%2520more%2520accurate%2520clustering%2520with%250Adramatically%2520less%2520input%2520of%2520user-provided%2520constraints%252C%2520and%2520outperforms%2520its%250Acounterparts%2520in%2520terms%2520of%2520computational%2520performance%2520and%2520scalability%252C%2520while%250Amaintaining%2520robustness%2520across%2520various%2520distance%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Skeleton%20Learning%3A%20Scalable%20Active%20Clustering%20with%20Sparse%20Graph%0A%20%20Structures&entry.906535625=Wen-Bo%20Xie%20and%20Xun%20Fu%20and%20Bin%20Chen%20and%20Yan-Li%20Lee%20and%20Tao%20Deng%20and%20Tian%20Zou%20and%20Xin%20Wang%20and%20Zhen%20Liu%20and%20Jaideep%20Srivastavad&entry.1292438233=%20%20In%20this%20work%2C%20we%20focus%20on%20the%20efficiency%20and%20scalability%20of%20pairwise%0Aconstraint-based%20active%20clustering%2C%20crucial%20for%20processing%20large-scale%20data%20in%0Aapplications%20such%20as%20data%20mining%2C%20knowledge%20annotation%2C%20and%20AI%20model%0Apre-training.%20Our%20goals%20are%20threefold%3A%20%281%29%20to%20reduce%20computational%20costs%20for%0Aiterative%20clustering%20updates%3B%20%282%29%20to%20enhance%20the%20impact%20of%20user-provided%0Aconstraints%20to%20minimize%20annotation%20requirements%20for%20precise%20clustering%3B%20and%20%283%29%0Ato%20cut%20down%20memory%20usage%20in%20practical%20deployments.%20To%20achieve%20these%20aims%2C%20we%0Apropose%20a%20graph-based%20active%20clustering%20algorithm%20that%20utilizes%20two%20sparse%0Agraphs%3A%20one%20for%20representing%20relationships%20between%20data%20%28our%20proposed%20data%0Askeleton%29%20and%20another%20for%20updating%20this%20data%20skeleton.%20These%20two%20graphs%20work%20in%0Aconcert%2C%20enabling%20the%20refinement%20of%20connected%20subgraphs%20within%20the%20data%0Askeleton%20to%20create%20nested%20clusters.%20Our%20empirical%20analysis%20confirms%20that%20the%0Aproposed%20algorithm%20consistently%20facilitates%20more%20accurate%20clustering%20with%0Adramatically%20less%20input%20of%20user-provided%20constraints%2C%20and%20outperforms%20its%0Acounterparts%20in%20terms%20of%20computational%20performance%20and%20scalability%2C%20while%0Amaintaining%20robustness%20across%20various%20distance%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08530v1&entry.124074799=Read"},
{"title": "PCGBandit: One-shot acceleration of transient PDE solvers via\n  online-learned preconditioners", "author": "Mikhail Khodak and Min Ki Jung and Brian Wynne and Edmond chow and Egemen Kolemen", "abstract": "  Data-driven acceleration of scientific computing workflows has been a\nhigh-profile aim of machine learning (ML) for science, with numerical\nsimulation of transient partial differential equations (PDEs) being one of the\nmain applications. The focus thus far has been on methods that require\nclassical simulations to train, which when combined with the data-hungriness\nand optimization challenges of neural networks has caused difficulties in\ndemonstrating a convincing advantage against strong classical baselines. We\nconsider an alternative paradigm in which the learner uses a classical solver's\nown data to accelerate it, enabling a one-shot speedup of the simulation.\nConcretely, since transient PDEs often require solving a sequence of related\nlinear systems, the feedback from repeated calls to a linear solver such as\npreconditioned conjugate gradient (PCG) can be used by a bandit algorithm to\nonline-learn an adaptive sequence of solver configurations (e.g.\npreconditioners). The method we develop, PCGBandit, is implemented directly on\ntop of the popular open source software OpenFOAM, which we use to show its\neffectiveness on a set of fluid and magnetohydrodynamics (MHD) problems.\n", "link": "http://arxiv.org/abs/2509.08765v1", "date": "2025-09-10", "relevancy": 1.9352, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4946}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4825}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCGBandit%3A%20One-shot%20acceleration%20of%20transient%20PDE%20solvers%20via%0A%20%20online-learned%20preconditioners&body=Title%3A%20PCGBandit%3A%20One-shot%20acceleration%20of%20transient%20PDE%20solvers%20via%0A%20%20online-learned%20preconditioners%0AAuthor%3A%20Mikhail%20Khodak%20and%20Min%20Ki%20Jung%20and%20Brian%20Wynne%20and%20Edmond%20chow%20and%20Egemen%20Kolemen%0AAbstract%3A%20%20%20Data-driven%20acceleration%20of%20scientific%20computing%20workflows%20has%20been%20a%0Ahigh-profile%20aim%20of%20machine%20learning%20%28ML%29%20for%20science%2C%20with%20numerical%0Asimulation%20of%20transient%20partial%20differential%20equations%20%28PDEs%29%20being%20one%20of%20the%0Amain%20applications.%20The%20focus%20thus%20far%20has%20been%20on%20methods%20that%20require%0Aclassical%20simulations%20to%20train%2C%20which%20when%20combined%20with%20the%20data-hungriness%0Aand%20optimization%20challenges%20of%20neural%20networks%20has%20caused%20difficulties%20in%0Ademonstrating%20a%20convincing%20advantage%20against%20strong%20classical%20baselines.%20We%0Aconsider%20an%20alternative%20paradigm%20in%20which%20the%20learner%20uses%20a%20classical%20solver%27s%0Aown%20data%20to%20accelerate%20it%2C%20enabling%20a%20one-shot%20speedup%20of%20the%20simulation.%0AConcretely%2C%20since%20transient%20PDEs%20often%20require%20solving%20a%20sequence%20of%20related%0Alinear%20systems%2C%20the%20feedback%20from%20repeated%20calls%20to%20a%20linear%20solver%20such%20as%0Apreconditioned%20conjugate%20gradient%20%28PCG%29%20can%20be%20used%20by%20a%20bandit%20algorithm%20to%0Aonline-learn%20an%20adaptive%20sequence%20of%20solver%20configurations%20%28e.g.%0Apreconditioners%29.%20The%20method%20we%20develop%2C%20PCGBandit%2C%20is%20implemented%20directly%20on%0Atop%20of%20the%20popular%20open%20source%20software%20OpenFOAM%2C%20which%20we%20use%20to%20show%20its%0Aeffectiveness%20on%20a%20set%20of%20fluid%20and%20magnetohydrodynamics%20%28MHD%29%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCGBandit%253A%2520One-shot%2520acceleration%2520of%2520transient%2520PDE%2520solvers%2520via%250A%2520%2520online-learned%2520preconditioners%26entry.906535625%3DMikhail%2520Khodak%2520and%2520Min%2520Ki%2520Jung%2520and%2520Brian%2520Wynne%2520and%2520Edmond%2520chow%2520and%2520Egemen%2520Kolemen%26entry.1292438233%3D%2520%2520Data-driven%2520acceleration%2520of%2520scientific%2520computing%2520workflows%2520has%2520been%2520a%250Ahigh-profile%2520aim%2520of%2520machine%2520learning%2520%2528ML%2529%2520for%2520science%252C%2520with%2520numerical%250Asimulation%2520of%2520transient%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520being%2520one%2520of%2520the%250Amain%2520applications.%2520The%2520focus%2520thus%2520far%2520has%2520been%2520on%2520methods%2520that%2520require%250Aclassical%2520simulations%2520to%2520train%252C%2520which%2520when%2520combined%2520with%2520the%2520data-hungriness%250Aand%2520optimization%2520challenges%2520of%2520neural%2520networks%2520has%2520caused%2520difficulties%2520in%250Ademonstrating%2520a%2520convincing%2520advantage%2520against%2520strong%2520classical%2520baselines.%2520We%250Aconsider%2520an%2520alternative%2520paradigm%2520in%2520which%2520the%2520learner%2520uses%2520a%2520classical%2520solver%2527s%250Aown%2520data%2520to%2520accelerate%2520it%252C%2520enabling%2520a%2520one-shot%2520speedup%2520of%2520the%2520simulation.%250AConcretely%252C%2520since%2520transient%2520PDEs%2520often%2520require%2520solving%2520a%2520sequence%2520of%2520related%250Alinear%2520systems%252C%2520the%2520feedback%2520from%2520repeated%2520calls%2520to%2520a%2520linear%2520solver%2520such%2520as%250Apreconditioned%2520conjugate%2520gradient%2520%2528PCG%2529%2520can%2520be%2520used%2520by%2520a%2520bandit%2520algorithm%2520to%250Aonline-learn%2520an%2520adaptive%2520sequence%2520of%2520solver%2520configurations%2520%2528e.g.%250Apreconditioners%2529.%2520The%2520method%2520we%2520develop%252C%2520PCGBandit%252C%2520is%2520implemented%2520directly%2520on%250Atop%2520of%2520the%2520popular%2520open%2520source%2520software%2520OpenFOAM%252C%2520which%2520we%2520use%2520to%2520show%2520its%250Aeffectiveness%2520on%2520a%2520set%2520of%2520fluid%2520and%2520magnetohydrodynamics%2520%2528MHD%2529%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCGBandit%3A%20One-shot%20acceleration%20of%20transient%20PDE%20solvers%20via%0A%20%20online-learned%20preconditioners&entry.906535625=Mikhail%20Khodak%20and%20Min%20Ki%20Jung%20and%20Brian%20Wynne%20and%20Edmond%20chow%20and%20Egemen%20Kolemen&entry.1292438233=%20%20Data-driven%20acceleration%20of%20scientific%20computing%20workflows%20has%20been%20a%0Ahigh-profile%20aim%20of%20machine%20learning%20%28ML%29%20for%20science%2C%20with%20numerical%0Asimulation%20of%20transient%20partial%20differential%20equations%20%28PDEs%29%20being%20one%20of%20the%0Amain%20applications.%20The%20focus%20thus%20far%20has%20been%20on%20methods%20that%20require%0Aclassical%20simulations%20to%20train%2C%20which%20when%20combined%20with%20the%20data-hungriness%0Aand%20optimization%20challenges%20of%20neural%20networks%20has%20caused%20difficulties%20in%0Ademonstrating%20a%20convincing%20advantage%20against%20strong%20classical%20baselines.%20We%0Aconsider%20an%20alternative%20paradigm%20in%20which%20the%20learner%20uses%20a%20classical%20solver%27s%0Aown%20data%20to%20accelerate%20it%2C%20enabling%20a%20one-shot%20speedup%20of%20the%20simulation.%0AConcretely%2C%20since%20transient%20PDEs%20often%20require%20solving%20a%20sequence%20of%20related%0Alinear%20systems%2C%20the%20feedback%20from%20repeated%20calls%20to%20a%20linear%20solver%20such%20as%0Apreconditioned%20conjugate%20gradient%20%28PCG%29%20can%20be%20used%20by%20a%20bandit%20algorithm%20to%0Aonline-learn%20an%20adaptive%20sequence%20of%20solver%20configurations%20%28e.g.%0Apreconditioners%29.%20The%20method%20we%20develop%2C%20PCGBandit%2C%20is%20implemented%20directly%20on%0Atop%20of%20the%20popular%20open%20source%20software%20OpenFOAM%2C%20which%20we%20use%20to%20show%20its%0Aeffectiveness%20on%20a%20set%20of%20fluid%20and%20magnetohydrodynamics%20%28MHD%29%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08765v1&entry.124074799=Read"},
{"title": "Meta-Semantics Augmented Few-Shot Relational Learning", "author": "Han Wu and Jie Yin", "abstract": "  Few-shot relational learning on knowledge graph (KGs) aims to perform\nreasoning over relations with only a few training examples. While existing\nmethods have primarily focused on leveraging specific relational information,\nrich semantics inherent in KGs have been largely overlooked. To address this\ncritical gap, we propose a novel prompted meta-learning (PromptMeta) framework\nthat seamlessly integrates meta-semantics with relational information for\nfew-shot relational learning. PromptMeta has two key innovations: (1) a\nMeta-Semantic Prompt (MSP) pool that learns and consolidates high-level\nmeta-semantics, enabling effective knowledge transfer and adaptation to rare\nand newly emerging relations; and (2) a learnable fusion token that dynamically\ncombines meta-semantics with task-specific relational information tailored to\ndifferent few-shot tasks. Both components are optimized jointly with model\nparameters within a meta-learning framework. Extensive experiments and analyses\non two real-world KG datasets demonstrate the effectiveness of PromptMeta in\nadapting to new relations with limited data.\n", "link": "http://arxiv.org/abs/2505.05684v2", "date": "2025-09-10", "relevancy": 1.9177, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4897}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.479}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Semantics%20Augmented%20Few-Shot%20Relational%20Learning&body=Title%3A%20Meta-Semantics%20Augmented%20Few-Shot%20Relational%20Learning%0AAuthor%3A%20Han%20Wu%20and%20Jie%20Yin%0AAbstract%3A%20%20%20Few-shot%20relational%20learning%20on%20knowledge%20graph%20%28KGs%29%20aims%20to%20perform%0Areasoning%20over%20relations%20with%20only%20a%20few%20training%20examples.%20While%20existing%0Amethods%20have%20primarily%20focused%20on%20leveraging%20specific%20relational%20information%2C%0Arich%20semantics%20inherent%20in%20KGs%20have%20been%20largely%20overlooked.%20To%20address%20this%0Acritical%20gap%2C%20we%20propose%20a%20novel%20prompted%20meta-learning%20%28PromptMeta%29%20framework%0Athat%20seamlessly%20integrates%20meta-semantics%20with%20relational%20information%20for%0Afew-shot%20relational%20learning.%20PromptMeta%20has%20two%20key%20innovations%3A%20%281%29%20a%0AMeta-Semantic%20Prompt%20%28MSP%29%20pool%20that%20learns%20and%20consolidates%20high-level%0Ameta-semantics%2C%20enabling%20effective%20knowledge%20transfer%20and%20adaptation%20to%20rare%0Aand%20newly%20emerging%20relations%3B%20and%20%282%29%20a%20learnable%20fusion%20token%20that%20dynamically%0Acombines%20meta-semantics%20with%20task-specific%20relational%20information%20tailored%20to%0Adifferent%20few-shot%20tasks.%20Both%20components%20are%20optimized%20jointly%20with%20model%0Aparameters%20within%20a%20meta-learning%20framework.%20Extensive%20experiments%20and%20analyses%0Aon%20two%20real-world%20KG%20datasets%20demonstrate%20the%20effectiveness%20of%20PromptMeta%20in%0Aadapting%20to%20new%20relations%20with%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05684v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Semantics%2520Augmented%2520Few-Shot%2520Relational%2520Learning%26entry.906535625%3DHan%2520Wu%2520and%2520Jie%2520Yin%26entry.1292438233%3D%2520%2520Few-shot%2520relational%2520learning%2520on%2520knowledge%2520graph%2520%2528KGs%2529%2520aims%2520to%2520perform%250Areasoning%2520over%2520relations%2520with%2520only%2520a%2520few%2520training%2520examples.%2520While%2520existing%250Amethods%2520have%2520primarily%2520focused%2520on%2520leveraging%2520specific%2520relational%2520information%252C%250Arich%2520semantics%2520inherent%2520in%2520KGs%2520have%2520been%2520largely%2520overlooked.%2520To%2520address%2520this%250Acritical%2520gap%252C%2520we%2520propose%2520a%2520novel%2520prompted%2520meta-learning%2520%2528PromptMeta%2529%2520framework%250Athat%2520seamlessly%2520integrates%2520meta-semantics%2520with%2520relational%2520information%2520for%250Afew-shot%2520relational%2520learning.%2520PromptMeta%2520has%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%250AMeta-Semantic%2520Prompt%2520%2528MSP%2529%2520pool%2520that%2520learns%2520and%2520consolidates%2520high-level%250Ameta-semantics%252C%2520enabling%2520effective%2520knowledge%2520transfer%2520and%2520adaptation%2520to%2520rare%250Aand%2520newly%2520emerging%2520relations%253B%2520and%2520%25282%2529%2520a%2520learnable%2520fusion%2520token%2520that%2520dynamically%250Acombines%2520meta-semantics%2520with%2520task-specific%2520relational%2520information%2520tailored%2520to%250Adifferent%2520few-shot%2520tasks.%2520Both%2520components%2520are%2520optimized%2520jointly%2520with%2520model%250Aparameters%2520within%2520a%2520meta-learning%2520framework.%2520Extensive%2520experiments%2520and%2520analyses%250Aon%2520two%2520real-world%2520KG%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520PromptMeta%2520in%250Aadapting%2520to%2520new%2520relations%2520with%2520limited%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05684v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Semantics%20Augmented%20Few-Shot%20Relational%20Learning&entry.906535625=Han%20Wu%20and%20Jie%20Yin&entry.1292438233=%20%20Few-shot%20relational%20learning%20on%20knowledge%20graph%20%28KGs%29%20aims%20to%20perform%0Areasoning%20over%20relations%20with%20only%20a%20few%20training%20examples.%20While%20existing%0Amethods%20have%20primarily%20focused%20on%20leveraging%20specific%20relational%20information%2C%0Arich%20semantics%20inherent%20in%20KGs%20have%20been%20largely%20overlooked.%20To%20address%20this%0Acritical%20gap%2C%20we%20propose%20a%20novel%20prompted%20meta-learning%20%28PromptMeta%29%20framework%0Athat%20seamlessly%20integrates%20meta-semantics%20with%20relational%20information%20for%0Afew-shot%20relational%20learning.%20PromptMeta%20has%20two%20key%20innovations%3A%20%281%29%20a%0AMeta-Semantic%20Prompt%20%28MSP%29%20pool%20that%20learns%20and%20consolidates%20high-level%0Ameta-semantics%2C%20enabling%20effective%20knowledge%20transfer%20and%20adaptation%20to%20rare%0Aand%20newly%20emerging%20relations%3B%20and%20%282%29%20a%20learnable%20fusion%20token%20that%20dynamically%0Acombines%20meta-semantics%20with%20task-specific%20relational%20information%20tailored%20to%0Adifferent%20few-shot%20tasks.%20Both%20components%20are%20optimized%20jointly%20with%20model%0Aparameters%20within%20a%20meta-learning%20framework.%20Extensive%20experiments%20and%20analyses%0Aon%20two%20real-world%20KG%20datasets%20demonstrate%20the%20effectiveness%20of%20PromptMeta%20in%0Aadapting%20to%20new%20relations%20with%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05684v2&entry.124074799=Read"},
{"title": "FinZero: Launching Multi-modal Financial Time Series Forecast with Large\n  Reasoning Model", "author": "Yanlong Wang and Jian Xu and Fei Ma and Hongkang Zhang and Hang Yu and Tiantian Gao and Yu Wang and Haochen You and Shao-Lun Huang and Danny Dongning Sun and Xiao-Ping Zhang", "abstract": "  Financial time series forecasting is both highly significant and challenging.\nPrevious approaches typically standardized time series data before feeding it\ninto forecasting models, but this encoding process inherently leads to a loss\nof important information. Moreover, past time series models generally require\nfixed numbers of variables or lookback window lengths, which further limits the\nscalability of time series forecasting. Besides, the interpretability and the\nuncertainty in forecasting remain areas requiring further research, as these\nfactors directly impact the reliability and practical value of predictions. To\naddress these issues, we first construct a diverse financial image-text dataset\n(FVLDB) and develop the Uncertainty-adjusted Group Relative Policy Optimization\n(UARPO) method to enable the model not only output predictions but also analyze\nthe uncertainty of those predictions. We then proposed FinZero, a multimodal\npre-trained model finetuned by UARPO to perform reasoning, prediction, and\nanalytical understanding on the FVLDB financial time series. Extensive\nexperiments validate that FinZero exhibits strong adaptability and scalability.\nAfter fine-tuning with UARPO, FinZero achieves an approximate 13.48\\%\nimprovement in prediction accuracy over GPT-4o in the high-confidence group,\ndemonstrating the effectiveness of reinforcement learning fine-tuning in\nmultimodal large model, including in financial time series forecasting tasks.\n", "link": "http://arxiv.org/abs/2509.08742v1", "date": "2025-09-10", "relevancy": 1.9094, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FinZero%3A%20Launching%20Multi-modal%20Financial%20Time%20Series%20Forecast%20with%20Large%0A%20%20Reasoning%20Model&body=Title%3A%20FinZero%3A%20Launching%20Multi-modal%20Financial%20Time%20Series%20Forecast%20with%20Large%0A%20%20Reasoning%20Model%0AAuthor%3A%20Yanlong%20Wang%20and%20Jian%20Xu%20and%20Fei%20Ma%20and%20Hongkang%20Zhang%20and%20Hang%20Yu%20and%20Tiantian%20Gao%20and%20Yu%20Wang%20and%20Haochen%20You%20and%20Shao-Lun%20Huang%20and%20Danny%20Dongning%20Sun%20and%20Xiao-Ping%20Zhang%0AAbstract%3A%20%20%20Financial%20time%20series%20forecasting%20is%20both%20highly%20significant%20and%20challenging.%0APrevious%20approaches%20typically%20standardized%20time%20series%20data%20before%20feeding%20it%0Ainto%20forecasting%20models%2C%20but%20this%20encoding%20process%20inherently%20leads%20to%20a%20loss%0Aof%20important%20information.%20Moreover%2C%20past%20time%20series%20models%20generally%20require%0Afixed%20numbers%20of%20variables%20or%20lookback%20window%20lengths%2C%20which%20further%20limits%20the%0Ascalability%20of%20time%20series%20forecasting.%20Besides%2C%20the%20interpretability%20and%20the%0Auncertainty%20in%20forecasting%20remain%20areas%20requiring%20further%20research%2C%20as%20these%0Afactors%20directly%20impact%20the%20reliability%20and%20practical%20value%20of%20predictions.%20To%0Aaddress%20these%20issues%2C%20we%20first%20construct%20a%20diverse%20financial%20image-text%20dataset%0A%28FVLDB%29%20and%20develop%20the%20Uncertainty-adjusted%20Group%20Relative%20Policy%20Optimization%0A%28UARPO%29%20method%20to%20enable%20the%20model%20not%20only%20output%20predictions%20but%20also%20analyze%0Athe%20uncertainty%20of%20those%20predictions.%20We%20then%20proposed%20FinZero%2C%20a%20multimodal%0Apre-trained%20model%20finetuned%20by%20UARPO%20to%20perform%20reasoning%2C%20prediction%2C%20and%0Aanalytical%20understanding%20on%20the%20FVLDB%20financial%20time%20series.%20Extensive%0Aexperiments%20validate%20that%20FinZero%20exhibits%20strong%20adaptability%20and%20scalability.%0AAfter%20fine-tuning%20with%20UARPO%2C%20FinZero%20achieves%20an%20approximate%2013.48%5C%25%0Aimprovement%20in%20prediction%20accuracy%20over%20GPT-4o%20in%20the%20high-confidence%20group%2C%0Ademonstrating%20the%20effectiveness%20of%20reinforcement%20learning%20fine-tuning%20in%0Amultimodal%20large%20model%2C%20including%20in%20financial%20time%20series%20forecasting%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinZero%253A%2520Launching%2520Multi-modal%2520Financial%2520Time%2520Series%2520Forecast%2520with%2520Large%250A%2520%2520Reasoning%2520Model%26entry.906535625%3DYanlong%2520Wang%2520and%2520Jian%2520Xu%2520and%2520Fei%2520Ma%2520and%2520Hongkang%2520Zhang%2520and%2520Hang%2520Yu%2520and%2520Tiantian%2520Gao%2520and%2520Yu%2520Wang%2520and%2520Haochen%2520You%2520and%2520Shao-Lun%2520Huang%2520and%2520Danny%2520Dongning%2520Sun%2520and%2520Xiao-Ping%2520Zhang%26entry.1292438233%3D%2520%2520Financial%2520time%2520series%2520forecasting%2520is%2520both%2520highly%2520significant%2520and%2520challenging.%250APrevious%2520approaches%2520typically%2520standardized%2520time%2520series%2520data%2520before%2520feeding%2520it%250Ainto%2520forecasting%2520models%252C%2520but%2520this%2520encoding%2520process%2520inherently%2520leads%2520to%2520a%2520loss%250Aof%2520important%2520information.%2520Moreover%252C%2520past%2520time%2520series%2520models%2520generally%2520require%250Afixed%2520numbers%2520of%2520variables%2520or%2520lookback%2520window%2520lengths%252C%2520which%2520further%2520limits%2520the%250Ascalability%2520of%2520time%2520series%2520forecasting.%2520Besides%252C%2520the%2520interpretability%2520and%2520the%250Auncertainty%2520in%2520forecasting%2520remain%2520areas%2520requiring%2520further%2520research%252C%2520as%2520these%250Afactors%2520directly%2520impact%2520the%2520reliability%2520and%2520practical%2520value%2520of%2520predictions.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520first%2520construct%2520a%2520diverse%2520financial%2520image-text%2520dataset%250A%2528FVLDB%2529%2520and%2520develop%2520the%2520Uncertainty-adjusted%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528UARPO%2529%2520method%2520to%2520enable%2520the%2520model%2520not%2520only%2520output%2520predictions%2520but%2520also%2520analyze%250Athe%2520uncertainty%2520of%2520those%2520predictions.%2520We%2520then%2520proposed%2520FinZero%252C%2520a%2520multimodal%250Apre-trained%2520model%2520finetuned%2520by%2520UARPO%2520to%2520perform%2520reasoning%252C%2520prediction%252C%2520and%250Aanalytical%2520understanding%2520on%2520the%2520FVLDB%2520financial%2520time%2520series.%2520Extensive%250Aexperiments%2520validate%2520that%2520FinZero%2520exhibits%2520strong%2520adaptability%2520and%2520scalability.%250AAfter%2520fine-tuning%2520with%2520UARPO%252C%2520FinZero%2520achieves%2520an%2520approximate%252013.48%255C%2525%250Aimprovement%2520in%2520prediction%2520accuracy%2520over%2520GPT-4o%2520in%2520the%2520high-confidence%2520group%252C%250Ademonstrating%2520the%2520effectiveness%2520of%2520reinforcement%2520learning%2520fine-tuning%2520in%250Amultimodal%2520large%2520model%252C%2520including%2520in%2520financial%2520time%2520series%2520forecasting%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FinZero%3A%20Launching%20Multi-modal%20Financial%20Time%20Series%20Forecast%20with%20Large%0A%20%20Reasoning%20Model&entry.906535625=Yanlong%20Wang%20and%20Jian%20Xu%20and%20Fei%20Ma%20and%20Hongkang%20Zhang%20and%20Hang%20Yu%20and%20Tiantian%20Gao%20and%20Yu%20Wang%20and%20Haochen%20You%20and%20Shao-Lun%20Huang%20and%20Danny%20Dongning%20Sun%20and%20Xiao-Ping%20Zhang&entry.1292438233=%20%20Financial%20time%20series%20forecasting%20is%20both%20highly%20significant%20and%20challenging.%0APrevious%20approaches%20typically%20standardized%20time%20series%20data%20before%20feeding%20it%0Ainto%20forecasting%20models%2C%20but%20this%20encoding%20process%20inherently%20leads%20to%20a%20loss%0Aof%20important%20information.%20Moreover%2C%20past%20time%20series%20models%20generally%20require%0Afixed%20numbers%20of%20variables%20or%20lookback%20window%20lengths%2C%20which%20further%20limits%20the%0Ascalability%20of%20time%20series%20forecasting.%20Besides%2C%20the%20interpretability%20and%20the%0Auncertainty%20in%20forecasting%20remain%20areas%20requiring%20further%20research%2C%20as%20these%0Afactors%20directly%20impact%20the%20reliability%20and%20practical%20value%20of%20predictions.%20To%0Aaddress%20these%20issues%2C%20we%20first%20construct%20a%20diverse%20financial%20image-text%20dataset%0A%28FVLDB%29%20and%20develop%20the%20Uncertainty-adjusted%20Group%20Relative%20Policy%20Optimization%0A%28UARPO%29%20method%20to%20enable%20the%20model%20not%20only%20output%20predictions%20but%20also%20analyze%0Athe%20uncertainty%20of%20those%20predictions.%20We%20then%20proposed%20FinZero%2C%20a%20multimodal%0Apre-trained%20model%20finetuned%20by%20UARPO%20to%20perform%20reasoning%2C%20prediction%2C%20and%0Aanalytical%20understanding%20on%20the%20FVLDB%20financial%20time%20series.%20Extensive%0Aexperiments%20validate%20that%20FinZero%20exhibits%20strong%20adaptability%20and%20scalability.%0AAfter%20fine-tuning%20with%20UARPO%2C%20FinZero%20achieves%20an%20approximate%2013.48%5C%25%0Aimprovement%20in%20prediction%20accuracy%20over%20GPT-4o%20in%20the%20high-confidence%20group%2C%0Ademonstrating%20the%20effectiveness%20of%20reinforcement%20learning%20fine-tuning%20in%0Amultimodal%20large%20model%2C%20including%20in%20financial%20time%20series%20forecasting%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08742v1&entry.124074799=Read"},
{"title": "Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation", "author": "Dennis Melamed and Connor Hashemi and Scott McCloskey", "abstract": "  Event-based cameras (EBCs) are a promising new technology for star\ntracking-based attitude determination, but prior studies have struggled to\ndetermine accurate ground truth for real data. We analyze the accuracy of an\nEBC star tracking system utilizing the Earth's motion as the ground truth for\ncomparison. The Earth rotates in a regular way with very small irregularities\nwhich are measured to the level of milli-arcseconds. By keeping an event camera\nstatic and pointing it through a ground-based telescope at the night sky, we\ncreate a system where the only camera motion in the celestial reference frame\nis that induced by the Earth's rotation. The resulting event stream is\nprocessed to generate estimates of orientation which we compare to the\nInternational Earth Rotation and Reference System (IERS) measured orientation\nof the Earth. The event camera system is able to achieve a root mean squared\nacross error of 18.47 arcseconds and an about error of 78.84 arcseconds.\nCombined with the other benefits of event cameras over framing sensors (reduced\ncomputation due to sparser data streams, higher dynamic range, lower energy\nconsumption, faster update rates), this level of accuracy suggests the utility\nof event cameras for low-cost and low-latency star tracking. We provide all\ncode and data used to generate our results:\nhttps://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.\n", "link": "http://arxiv.org/abs/2509.08794v1", "date": "2025-09-10", "relevancy": 1.8892, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4904}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4718}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Accuracy%20of%20an%20Event-Based%20Star%20Tracker%20via%20Earth%27s%20Rotation&body=Title%3A%20Quantifying%20Accuracy%20of%20an%20Event-Based%20Star%20Tracker%20via%20Earth%27s%20Rotation%0AAuthor%3A%20Dennis%20Melamed%20and%20Connor%20Hashemi%20and%20Scott%20McCloskey%0AAbstract%3A%20%20%20Event-based%20cameras%20%28EBCs%29%20are%20a%20promising%20new%20technology%20for%20star%0Atracking-based%20attitude%20determination%2C%20but%20prior%20studies%20have%20struggled%20to%0Adetermine%20accurate%20ground%20truth%20for%20real%20data.%20We%20analyze%20the%20accuracy%20of%20an%0AEBC%20star%20tracking%20system%20utilizing%20the%20Earth%27s%20motion%20as%20the%20ground%20truth%20for%0Acomparison.%20The%20Earth%20rotates%20in%20a%20regular%20way%20with%20very%20small%20irregularities%0Awhich%20are%20measured%20to%20the%20level%20of%20milli-arcseconds.%20By%20keeping%20an%20event%20camera%0Astatic%20and%20pointing%20it%20through%20a%20ground-based%20telescope%20at%20the%20night%20sky%2C%20we%0Acreate%20a%20system%20where%20the%20only%20camera%20motion%20in%20the%20celestial%20reference%20frame%0Ais%20that%20induced%20by%20the%20Earth%27s%20rotation.%20The%20resulting%20event%20stream%20is%0Aprocessed%20to%20generate%20estimates%20of%20orientation%20which%20we%20compare%20to%20the%0AInternational%20Earth%20Rotation%20and%20Reference%20System%20%28IERS%29%20measured%20orientation%0Aof%20the%20Earth.%20The%20event%20camera%20system%20is%20able%20to%20achieve%20a%20root%20mean%20squared%0Aacross%20error%20of%2018.47%20arcseconds%20and%20an%20about%20error%20of%2078.84%20arcseconds.%0ACombined%20with%20the%20other%20benefits%20of%20event%20cameras%20over%20framing%20sensors%20%28reduced%0Acomputation%20due%20to%20sparser%20data%20streams%2C%20higher%20dynamic%20range%2C%20lower%20energy%0Aconsumption%2C%20faster%20update%20rates%29%2C%20this%20level%20of%20accuracy%20suggests%20the%20utility%0Aof%20event%20cameras%20for%20low-cost%20and%20low-latency%20star%20tracking.%20We%20provide%20all%0Acode%20and%20data%20used%20to%20generate%20our%20results%3A%0Ahttps%3A//gitlab.kitware.com/nest-public/telescope_accuracy_quantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Accuracy%2520of%2520an%2520Event-Based%2520Star%2520Tracker%2520via%2520Earth%2527s%2520Rotation%26entry.906535625%3DDennis%2520Melamed%2520and%2520Connor%2520Hashemi%2520and%2520Scott%2520McCloskey%26entry.1292438233%3D%2520%2520Event-based%2520cameras%2520%2528EBCs%2529%2520are%2520a%2520promising%2520new%2520technology%2520for%2520star%250Atracking-based%2520attitude%2520determination%252C%2520but%2520prior%2520studies%2520have%2520struggled%2520to%250Adetermine%2520accurate%2520ground%2520truth%2520for%2520real%2520data.%2520We%2520analyze%2520the%2520accuracy%2520of%2520an%250AEBC%2520star%2520tracking%2520system%2520utilizing%2520the%2520Earth%2527s%2520motion%2520as%2520the%2520ground%2520truth%2520for%250Acomparison.%2520The%2520Earth%2520rotates%2520in%2520a%2520regular%2520way%2520with%2520very%2520small%2520irregularities%250Awhich%2520are%2520measured%2520to%2520the%2520level%2520of%2520milli-arcseconds.%2520By%2520keeping%2520an%2520event%2520camera%250Astatic%2520and%2520pointing%2520it%2520through%2520a%2520ground-based%2520telescope%2520at%2520the%2520night%2520sky%252C%2520we%250Acreate%2520a%2520system%2520where%2520the%2520only%2520camera%2520motion%2520in%2520the%2520celestial%2520reference%2520frame%250Ais%2520that%2520induced%2520by%2520the%2520Earth%2527s%2520rotation.%2520The%2520resulting%2520event%2520stream%2520is%250Aprocessed%2520to%2520generate%2520estimates%2520of%2520orientation%2520which%2520we%2520compare%2520to%2520the%250AInternational%2520Earth%2520Rotation%2520and%2520Reference%2520System%2520%2528IERS%2529%2520measured%2520orientation%250Aof%2520the%2520Earth.%2520The%2520event%2520camera%2520system%2520is%2520able%2520to%2520achieve%2520a%2520root%2520mean%2520squared%250Aacross%2520error%2520of%252018.47%2520arcseconds%2520and%2520an%2520about%2520error%2520of%252078.84%2520arcseconds.%250ACombined%2520with%2520the%2520other%2520benefits%2520of%2520event%2520cameras%2520over%2520framing%2520sensors%2520%2528reduced%250Acomputation%2520due%2520to%2520sparser%2520data%2520streams%252C%2520higher%2520dynamic%2520range%252C%2520lower%2520energy%250Aconsumption%252C%2520faster%2520update%2520rates%2529%252C%2520this%2520level%2520of%2520accuracy%2520suggests%2520the%2520utility%250Aof%2520event%2520cameras%2520for%2520low-cost%2520and%2520low-latency%2520star%2520tracking.%2520We%2520provide%2520all%250Acode%2520and%2520data%2520used%2520to%2520generate%2520our%2520results%253A%250Ahttps%253A//gitlab.kitware.com/nest-public/telescope_accuracy_quantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Accuracy%20of%20an%20Event-Based%20Star%20Tracker%20via%20Earth%27s%20Rotation&entry.906535625=Dennis%20Melamed%20and%20Connor%20Hashemi%20and%20Scott%20McCloskey&entry.1292438233=%20%20Event-based%20cameras%20%28EBCs%29%20are%20a%20promising%20new%20technology%20for%20star%0Atracking-based%20attitude%20determination%2C%20but%20prior%20studies%20have%20struggled%20to%0Adetermine%20accurate%20ground%20truth%20for%20real%20data.%20We%20analyze%20the%20accuracy%20of%20an%0AEBC%20star%20tracking%20system%20utilizing%20the%20Earth%27s%20motion%20as%20the%20ground%20truth%20for%0Acomparison.%20The%20Earth%20rotates%20in%20a%20regular%20way%20with%20very%20small%20irregularities%0Awhich%20are%20measured%20to%20the%20level%20of%20milli-arcseconds.%20By%20keeping%20an%20event%20camera%0Astatic%20and%20pointing%20it%20through%20a%20ground-based%20telescope%20at%20the%20night%20sky%2C%20we%0Acreate%20a%20system%20where%20the%20only%20camera%20motion%20in%20the%20celestial%20reference%20frame%0Ais%20that%20induced%20by%20the%20Earth%27s%20rotation.%20The%20resulting%20event%20stream%20is%0Aprocessed%20to%20generate%20estimates%20of%20orientation%20which%20we%20compare%20to%20the%0AInternational%20Earth%20Rotation%20and%20Reference%20System%20%28IERS%29%20measured%20orientation%0Aof%20the%20Earth.%20The%20event%20camera%20system%20is%20able%20to%20achieve%20a%20root%20mean%20squared%0Aacross%20error%20of%2018.47%20arcseconds%20and%20an%20about%20error%20of%2078.84%20arcseconds.%0ACombined%20with%20the%20other%20benefits%20of%20event%20cameras%20over%20framing%20sensors%20%28reduced%0Acomputation%20due%20to%20sparser%20data%20streams%2C%20higher%20dynamic%20range%2C%20lower%20energy%0Aconsumption%2C%20faster%20update%20rates%29%2C%20this%20level%20of%20accuracy%20suggests%20the%20utility%0Aof%20event%20cameras%20for%20low-cost%20and%20low-latency%20star%20tracking.%20We%20provide%20all%0Acode%20and%20data%20used%20to%20generate%20our%20results%3A%0Ahttps%3A//gitlab.kitware.com/nest-public/telescope_accuracy_quantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08794v1&entry.124074799=Read"},
{"title": "Compressing CNN models for resource-constrained systems by channel and\n  layer pruning", "author": "Ahmed Sadaqa and Di Liu", "abstract": "  Convolutional Neural Networks (CNNs) have achieved significant breakthroughs\nin various fields. However, these advancements have led to a substantial\nincrease in the complexity and size of these networks. This poses a challenge\nwhen deploying large and complex networks on edge devices. Consequently, model\ncompression has emerged as a research field aimed at reducing the size and\ncomplexity of CNNs. One prominent technique in model compression is model\npruning. This paper will present a new technique of pruning that combines both\nchannel and layer pruning in what is called a \"hybrid pruning framework\".\nInspired by EfficientNet, a renowned CNN architecture known for scaling up\nnetworks from both channel and layer perspectives, this hybrid approach applies\nthe same principles but in reverse, where it scales down the network through\npruning. Experiments on the hybrid approach demonstrated a notable decrease in\nthe overall complexity of the model, with only a minimal reduction in accuracy\ncompared to the baseline model. This complexity reduction translates into\nreduced latency when deploying the pruned models on an NVIDIA JETSON TX2\nembedded AI device.\n", "link": "http://arxiv.org/abs/2509.08714v1", "date": "2025-09-10", "relevancy": 1.8886, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4877}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4708}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressing%20CNN%20models%20for%20resource-constrained%20systems%20by%20channel%20and%0A%20%20layer%20pruning&body=Title%3A%20Compressing%20CNN%20models%20for%20resource-constrained%20systems%20by%20channel%20and%0A%20%20layer%20pruning%0AAuthor%3A%20Ahmed%20Sadaqa%20and%20Di%20Liu%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20achieved%20significant%20breakthroughs%0Ain%20various%20fields.%20However%2C%20these%20advancements%20have%20led%20to%20a%20substantial%0Aincrease%20in%20the%20complexity%20and%20size%20of%20these%20networks.%20This%20poses%20a%20challenge%0Awhen%20deploying%20large%20and%20complex%20networks%20on%20edge%20devices.%20Consequently%2C%20model%0Acompression%20has%20emerged%20as%20a%20research%20field%20aimed%20at%20reducing%20the%20size%20and%0Acomplexity%20of%20CNNs.%20One%20prominent%20technique%20in%20model%20compression%20is%20model%0Apruning.%20This%20paper%20will%20present%20a%20new%20technique%20of%20pruning%20that%20combines%20both%0Achannel%20and%20layer%20pruning%20in%20what%20is%20called%20a%20%22hybrid%20pruning%20framework%22.%0AInspired%20by%20EfficientNet%2C%20a%20renowned%20CNN%20architecture%20known%20for%20scaling%20up%0Anetworks%20from%20both%20channel%20and%20layer%20perspectives%2C%20this%20hybrid%20approach%20applies%0Athe%20same%20principles%20but%20in%20reverse%2C%20where%20it%20scales%20down%20the%20network%20through%0Apruning.%20Experiments%20on%20the%20hybrid%20approach%20demonstrated%20a%20notable%20decrease%20in%0Athe%20overall%20complexity%20of%20the%20model%2C%20with%20only%20a%20minimal%20reduction%20in%20accuracy%0Acompared%20to%20the%20baseline%20model.%20This%20complexity%20reduction%20translates%20into%0Areduced%20latency%20when%20deploying%20the%20pruned%20models%20on%20an%20NVIDIA%20JETSON%20TX2%0Aembedded%20AI%20device.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressing%2520CNN%2520models%2520for%2520resource-constrained%2520systems%2520by%2520channel%2520and%250A%2520%2520layer%2520pruning%26entry.906535625%3DAhmed%2520Sadaqa%2520and%2520Di%2520Liu%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520achieved%2520significant%2520breakthroughs%250Ain%2520various%2520fields.%2520However%252C%2520these%2520advancements%2520have%2520led%2520to%2520a%2520substantial%250Aincrease%2520in%2520the%2520complexity%2520and%2520size%2520of%2520these%2520networks.%2520This%2520poses%2520a%2520challenge%250Awhen%2520deploying%2520large%2520and%2520complex%2520networks%2520on%2520edge%2520devices.%2520Consequently%252C%2520model%250Acompression%2520has%2520emerged%2520as%2520a%2520research%2520field%2520aimed%2520at%2520reducing%2520the%2520size%2520and%250Acomplexity%2520of%2520CNNs.%2520One%2520prominent%2520technique%2520in%2520model%2520compression%2520is%2520model%250Apruning.%2520This%2520paper%2520will%2520present%2520a%2520new%2520technique%2520of%2520pruning%2520that%2520combines%2520both%250Achannel%2520and%2520layer%2520pruning%2520in%2520what%2520is%2520called%2520a%2520%2522hybrid%2520pruning%2520framework%2522.%250AInspired%2520by%2520EfficientNet%252C%2520a%2520renowned%2520CNN%2520architecture%2520known%2520for%2520scaling%2520up%250Anetworks%2520from%2520both%2520channel%2520and%2520layer%2520perspectives%252C%2520this%2520hybrid%2520approach%2520applies%250Athe%2520same%2520principles%2520but%2520in%2520reverse%252C%2520where%2520it%2520scales%2520down%2520the%2520network%2520through%250Apruning.%2520Experiments%2520on%2520the%2520hybrid%2520approach%2520demonstrated%2520a%2520notable%2520decrease%2520in%250Athe%2520overall%2520complexity%2520of%2520the%2520model%252C%2520with%2520only%2520a%2520minimal%2520reduction%2520in%2520accuracy%250Acompared%2520to%2520the%2520baseline%2520model.%2520This%2520complexity%2520reduction%2520translates%2520into%250Areduced%2520latency%2520when%2520deploying%2520the%2520pruned%2520models%2520on%2520an%2520NVIDIA%2520JETSON%2520TX2%250Aembedded%2520AI%2520device.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressing%20CNN%20models%20for%20resource-constrained%20systems%20by%20channel%20and%0A%20%20layer%20pruning&entry.906535625=Ahmed%20Sadaqa%20and%20Di%20Liu&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20achieved%20significant%20breakthroughs%0Ain%20various%20fields.%20However%2C%20these%20advancements%20have%20led%20to%20a%20substantial%0Aincrease%20in%20the%20complexity%20and%20size%20of%20these%20networks.%20This%20poses%20a%20challenge%0Awhen%20deploying%20large%20and%20complex%20networks%20on%20edge%20devices.%20Consequently%2C%20model%0Acompression%20has%20emerged%20as%20a%20research%20field%20aimed%20at%20reducing%20the%20size%20and%0Acomplexity%20of%20CNNs.%20One%20prominent%20technique%20in%20model%20compression%20is%20model%0Apruning.%20This%20paper%20will%20present%20a%20new%20technique%20of%20pruning%20that%20combines%20both%0Achannel%20and%20layer%20pruning%20in%20what%20is%20called%20a%20%22hybrid%20pruning%20framework%22.%0AInspired%20by%20EfficientNet%2C%20a%20renowned%20CNN%20architecture%20known%20for%20scaling%20up%0Anetworks%20from%20both%20channel%20and%20layer%20perspectives%2C%20this%20hybrid%20approach%20applies%0Athe%20same%20principles%20but%20in%20reverse%2C%20where%20it%20scales%20down%20the%20network%20through%0Apruning.%20Experiments%20on%20the%20hybrid%20approach%20demonstrated%20a%20notable%20decrease%20in%0Athe%20overall%20complexity%20of%20the%20model%2C%20with%20only%20a%20minimal%20reduction%20in%20accuracy%0Acompared%20to%20the%20baseline%20model.%20This%20complexity%20reduction%20translates%20into%0Areduced%20latency%20when%20deploying%20the%20pruned%20models%20on%20an%20NVIDIA%20JETSON%20TX2%0Aembedded%20AI%20device.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08714v1&entry.124074799=Read"},
{"title": "Interpretability as Alignment: Making Internal Understanding a Design\n  Principle", "author": "Aadit Sengupta and Pratinav Seth and Vinay Kumar Sankarapu", "abstract": "  Large neural models are increasingly deployed in high-stakes settings,\nraising concerns about whether their behavior reliably aligns with human\nvalues. Interpretability provides a route to internal transparency by revealing\nthe computations that drive outputs. We argue that interpretability especially\nmechanistic approaches should be treated as a design principle for alignment,\nnot an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer\nintuitive but correlational explanations, while mechanistic techniques like\ncircuit tracing or activation patching yield causal insight into internal\nfailures, including deceptive or misaligned reasoning that behavioral methods\nlike RLHF, red teaming, or Constitutional AI may overlook. Despite these\nadvantages, interpretability faces challenges of scalability, epistemic\nuncertainty, and mismatches between learned representations and human concepts.\nOur position is that progress on safe and trustworthy AI will depend on making\ninterpretability a first-class objective of AI research and development,\nensuring that systems are not only effective but also auditable, transparent,\nand aligned with human intent.\n", "link": "http://arxiv.org/abs/2509.08592v1", "date": "2025-09-10", "relevancy": 1.8802, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4742}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4742}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretability%20as%20Alignment%3A%20Making%20Internal%20Understanding%20a%20Design%0A%20%20Principle&body=Title%3A%20Interpretability%20as%20Alignment%3A%20Making%20Internal%20Understanding%20a%20Design%0A%20%20Principle%0AAuthor%3A%20Aadit%20Sengupta%20and%20Pratinav%20Seth%20and%20Vinay%20Kumar%20Sankarapu%0AAbstract%3A%20%20%20Large%20neural%20models%20are%20increasingly%20deployed%20in%20high-stakes%20settings%2C%0Araising%20concerns%20about%20whether%20their%20behavior%20reliably%20aligns%20with%20human%0Avalues.%20Interpretability%20provides%20a%20route%20to%20internal%20transparency%20by%20revealing%0Athe%20computations%20that%20drive%20outputs.%20We%20argue%20that%20interpretability%20especially%0Amechanistic%20approaches%20should%20be%20treated%20as%20a%20design%20principle%20for%20alignment%2C%0Anot%20an%20auxiliary%20diagnostic%20tool.%20Post-hoc%20methods%20such%20as%20LIME%20or%20SHAP%20offer%0Aintuitive%20but%20correlational%20explanations%2C%20while%20mechanistic%20techniques%20like%0Acircuit%20tracing%20or%20activation%20patching%20yield%20causal%20insight%20into%20internal%0Afailures%2C%20including%20deceptive%20or%20misaligned%20reasoning%20that%20behavioral%20methods%0Alike%20RLHF%2C%20red%20teaming%2C%20or%20Constitutional%20AI%20may%20overlook.%20Despite%20these%0Aadvantages%2C%20interpretability%20faces%20challenges%20of%20scalability%2C%20epistemic%0Auncertainty%2C%20and%20mismatches%20between%20learned%20representations%20and%20human%20concepts.%0AOur%20position%20is%20that%20progress%20on%20safe%20and%20trustworthy%20AI%20will%20depend%20on%20making%0Ainterpretability%20a%20first-class%20objective%20of%20AI%20research%20and%20development%2C%0Aensuring%20that%20systems%20are%20not%20only%20effective%20but%20also%20auditable%2C%20transparent%2C%0Aand%20aligned%20with%20human%20intent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretability%2520as%2520Alignment%253A%2520Making%2520Internal%2520Understanding%2520a%2520Design%250A%2520%2520Principle%26entry.906535625%3DAadit%2520Sengupta%2520and%2520Pratinav%2520Seth%2520and%2520Vinay%2520Kumar%2520Sankarapu%26entry.1292438233%3D%2520%2520Large%2520neural%2520models%2520are%2520increasingly%2520deployed%2520in%2520high-stakes%2520settings%252C%250Araising%2520concerns%2520about%2520whether%2520their%2520behavior%2520reliably%2520aligns%2520with%2520human%250Avalues.%2520Interpretability%2520provides%2520a%2520route%2520to%2520internal%2520transparency%2520by%2520revealing%250Athe%2520computations%2520that%2520drive%2520outputs.%2520We%2520argue%2520that%2520interpretability%2520especially%250Amechanistic%2520approaches%2520should%2520be%2520treated%2520as%2520a%2520design%2520principle%2520for%2520alignment%252C%250Anot%2520an%2520auxiliary%2520diagnostic%2520tool.%2520Post-hoc%2520methods%2520such%2520as%2520LIME%2520or%2520SHAP%2520offer%250Aintuitive%2520but%2520correlational%2520explanations%252C%2520while%2520mechanistic%2520techniques%2520like%250Acircuit%2520tracing%2520or%2520activation%2520patching%2520yield%2520causal%2520insight%2520into%2520internal%250Afailures%252C%2520including%2520deceptive%2520or%2520misaligned%2520reasoning%2520that%2520behavioral%2520methods%250Alike%2520RLHF%252C%2520red%2520teaming%252C%2520or%2520Constitutional%2520AI%2520may%2520overlook.%2520Despite%2520these%250Aadvantages%252C%2520interpretability%2520faces%2520challenges%2520of%2520scalability%252C%2520epistemic%250Auncertainty%252C%2520and%2520mismatches%2520between%2520learned%2520representations%2520and%2520human%2520concepts.%250AOur%2520position%2520is%2520that%2520progress%2520on%2520safe%2520and%2520trustworthy%2520AI%2520will%2520depend%2520on%2520making%250Ainterpretability%2520a%2520first-class%2520objective%2520of%2520AI%2520research%2520and%2520development%252C%250Aensuring%2520that%2520systems%2520are%2520not%2520only%2520effective%2520but%2520also%2520auditable%252C%2520transparent%252C%250Aand%2520aligned%2520with%2520human%2520intent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretability%20as%20Alignment%3A%20Making%20Internal%20Understanding%20a%20Design%0A%20%20Principle&entry.906535625=Aadit%20Sengupta%20and%20Pratinav%20Seth%20and%20Vinay%20Kumar%20Sankarapu&entry.1292438233=%20%20Large%20neural%20models%20are%20increasingly%20deployed%20in%20high-stakes%20settings%2C%0Araising%20concerns%20about%20whether%20their%20behavior%20reliably%20aligns%20with%20human%0Avalues.%20Interpretability%20provides%20a%20route%20to%20internal%20transparency%20by%20revealing%0Athe%20computations%20that%20drive%20outputs.%20We%20argue%20that%20interpretability%20especially%0Amechanistic%20approaches%20should%20be%20treated%20as%20a%20design%20principle%20for%20alignment%2C%0Anot%20an%20auxiliary%20diagnostic%20tool.%20Post-hoc%20methods%20such%20as%20LIME%20or%20SHAP%20offer%0Aintuitive%20but%20correlational%20explanations%2C%20while%20mechanistic%20techniques%20like%0Acircuit%20tracing%20or%20activation%20patching%20yield%20causal%20insight%20into%20internal%0Afailures%2C%20including%20deceptive%20or%20misaligned%20reasoning%20that%20behavioral%20methods%0Alike%20RLHF%2C%20red%20teaming%2C%20or%20Constitutional%20AI%20may%20overlook.%20Despite%20these%0Aadvantages%2C%20interpretability%20faces%20challenges%20of%20scalability%2C%20epistemic%0Auncertainty%2C%20and%20mismatches%20between%20learned%20representations%20and%20human%20concepts.%0AOur%20position%20is%20that%20progress%20on%20safe%20and%20trustworthy%20AI%20will%20depend%20on%20making%0Ainterpretability%20a%20first-class%20objective%20of%20AI%20research%20and%20development%2C%0Aensuring%20that%20systems%20are%20not%20only%20effective%20but%20also%20auditable%2C%20transparent%2C%0Aand%20aligned%20with%20human%20intent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08592v1&entry.124074799=Read"},
{"title": "Automatic Failure Attribution and Critical Step Prediction Method for\n  Multi-Agent Systems Based on Causal Inference", "author": "Guoqing Ma and Jia Zhu and Hanghui Guo and Weijie Shi and Jiawei Shen and Jingjiang Liu and Yidan Liang", "abstract": "  Multi-agent systems (MAS) are critical for automating complex tasks, yet\ntheir practical deployment is severely hampered by the challenge of failure\nattribution. Current diagnostic tools, which rely on statistical correlations,\nare fundamentally inadequate; on challenging benchmarks like Who\\&When,\nstate-of-the-art methods achieve less than 15\\% accuracy in locating the\nroot-cause step of a failure. To address this critical gap, we introduce the\nfirst failure attribution framework for MAS grounded in multi-granularity\ncausal inference. Our approach makes two key technical contributions: (1) a\nperformance causal inversion principle, which correctly models performance\ndependencies by reversing the data flow in execution logs, combined with\nShapley values to accurately assign agent-level blame; (2) a novel causal\ndiscovery algorithm, CDC-MAS, that robustly identifies critical failure steps\nby tackling the non-stationary nature of MAS interaction data. The framework's\nattribution results directly fuel an automated optimization loop, generating\ntargeted suggestions whose efficacy is validated via counterfactual\nsimulations. Evaluations on the Who\\&When and TRAIL benchmarks demonstrate a\nsignificant leap in performance. Our method achieves up to 36.2\\% step-level\naccuracy. Crucially, the generated optimizations boost overall task success\nrates by an average of 22.4\\%. This work provides a principled and effective\nsolution for debugging complex agent interactions, paving the way for more\nreliable and interpretable multi-agent systems.\n", "link": "http://arxiv.org/abs/2509.08682v1", "date": "2025-09-10", "relevancy": 1.4661, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5469}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4783}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Failure%20Attribution%20and%20Critical%20Step%20Prediction%20Method%20for%0A%20%20Multi-Agent%20Systems%20Based%20on%20Causal%20Inference&body=Title%3A%20Automatic%20Failure%20Attribution%20and%20Critical%20Step%20Prediction%20Method%20for%0A%20%20Multi-Agent%20Systems%20Based%20on%20Causal%20Inference%0AAuthor%3A%20Guoqing%20Ma%20and%20Jia%20Zhu%20and%20Hanghui%20Guo%20and%20Weijie%20Shi%20and%20Jiawei%20Shen%20and%20Jingjiang%20Liu%20and%20Yidan%20Liang%0AAbstract%3A%20%20%20Multi-agent%20systems%20%28MAS%29%20are%20critical%20for%20automating%20complex%20tasks%2C%20yet%0Atheir%20practical%20deployment%20is%20severely%20hampered%20by%20the%20challenge%20of%20failure%0Aattribution.%20Current%20diagnostic%20tools%2C%20which%20rely%20on%20statistical%20correlations%2C%0Aare%20fundamentally%20inadequate%3B%20on%20challenging%20benchmarks%20like%20Who%5C%26When%2C%0Astate-of-the-art%20methods%20achieve%20less%20than%2015%5C%25%20accuracy%20in%20locating%20the%0Aroot-cause%20step%20of%20a%20failure.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20the%0Afirst%20failure%20attribution%20framework%20for%20MAS%20grounded%20in%20multi-granularity%0Acausal%20inference.%20Our%20approach%20makes%20two%20key%20technical%20contributions%3A%20%281%29%20a%0Aperformance%20causal%20inversion%20principle%2C%20which%20correctly%20models%20performance%0Adependencies%20by%20reversing%20the%20data%20flow%20in%20execution%20logs%2C%20combined%20with%0AShapley%20values%20to%20accurately%20assign%20agent-level%20blame%3B%20%282%29%20a%20novel%20causal%0Adiscovery%20algorithm%2C%20CDC-MAS%2C%20that%20robustly%20identifies%20critical%20failure%20steps%0Aby%20tackling%20the%20non-stationary%20nature%20of%20MAS%20interaction%20data.%20The%20framework%27s%0Aattribution%20results%20directly%20fuel%20an%20automated%20optimization%20loop%2C%20generating%0Atargeted%20suggestions%20whose%20efficacy%20is%20validated%20via%20counterfactual%0Asimulations.%20Evaluations%20on%20the%20Who%5C%26When%20and%20TRAIL%20benchmarks%20demonstrate%20a%0Asignificant%20leap%20in%20performance.%20Our%20method%20achieves%20up%20to%2036.2%5C%25%20step-level%0Aaccuracy.%20Crucially%2C%20the%20generated%20optimizations%20boost%20overall%20task%20success%0Arates%20by%20an%20average%20of%2022.4%5C%25.%20This%20work%20provides%20a%20principled%20and%20effective%0Asolution%20for%20debugging%20complex%20agent%20interactions%2C%20paving%20the%20way%20for%20more%0Areliable%20and%20interpretable%20multi-agent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Failure%2520Attribution%2520and%2520Critical%2520Step%2520Prediction%2520Method%2520for%250A%2520%2520Multi-Agent%2520Systems%2520Based%2520on%2520Causal%2520Inference%26entry.906535625%3DGuoqing%2520Ma%2520and%2520Jia%2520Zhu%2520and%2520Hanghui%2520Guo%2520and%2520Weijie%2520Shi%2520and%2520Jiawei%2520Shen%2520and%2520Jingjiang%2520Liu%2520and%2520Yidan%2520Liang%26entry.1292438233%3D%2520%2520Multi-agent%2520systems%2520%2528MAS%2529%2520are%2520critical%2520for%2520automating%2520complex%2520tasks%252C%2520yet%250Atheir%2520practical%2520deployment%2520is%2520severely%2520hampered%2520by%2520the%2520challenge%2520of%2520failure%250Aattribution.%2520Current%2520diagnostic%2520tools%252C%2520which%2520rely%2520on%2520statistical%2520correlations%252C%250Aare%2520fundamentally%2520inadequate%253B%2520on%2520challenging%2520benchmarks%2520like%2520Who%255C%2526When%252C%250Astate-of-the-art%2520methods%2520achieve%2520less%2520than%252015%255C%2525%2520accuracy%2520in%2520locating%2520the%250Aroot-cause%2520step%2520of%2520a%2520failure.%2520To%2520address%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520the%250Afirst%2520failure%2520attribution%2520framework%2520for%2520MAS%2520grounded%2520in%2520multi-granularity%250Acausal%2520inference.%2520Our%2520approach%2520makes%2520two%2520key%2520technical%2520contributions%253A%2520%25281%2529%2520a%250Aperformance%2520causal%2520inversion%2520principle%252C%2520which%2520correctly%2520models%2520performance%250Adependencies%2520by%2520reversing%2520the%2520data%2520flow%2520in%2520execution%2520logs%252C%2520combined%2520with%250AShapley%2520values%2520to%2520accurately%2520assign%2520agent-level%2520blame%253B%2520%25282%2529%2520a%2520novel%2520causal%250Adiscovery%2520algorithm%252C%2520CDC-MAS%252C%2520that%2520robustly%2520identifies%2520critical%2520failure%2520steps%250Aby%2520tackling%2520the%2520non-stationary%2520nature%2520of%2520MAS%2520interaction%2520data.%2520The%2520framework%2527s%250Aattribution%2520results%2520directly%2520fuel%2520an%2520automated%2520optimization%2520loop%252C%2520generating%250Atargeted%2520suggestions%2520whose%2520efficacy%2520is%2520validated%2520via%2520counterfactual%250Asimulations.%2520Evaluations%2520on%2520the%2520Who%255C%2526When%2520and%2520TRAIL%2520benchmarks%2520demonstrate%2520a%250Asignificant%2520leap%2520in%2520performance.%2520Our%2520method%2520achieves%2520up%2520to%252036.2%255C%2525%2520step-level%250Aaccuracy.%2520Crucially%252C%2520the%2520generated%2520optimizations%2520boost%2520overall%2520task%2520success%250Arates%2520by%2520an%2520average%2520of%252022.4%255C%2525.%2520This%2520work%2520provides%2520a%2520principled%2520and%2520effective%250Asolution%2520for%2520debugging%2520complex%2520agent%2520interactions%252C%2520paving%2520the%2520way%2520for%2520more%250Areliable%2520and%2520interpretable%2520multi-agent%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Failure%20Attribution%20and%20Critical%20Step%20Prediction%20Method%20for%0A%20%20Multi-Agent%20Systems%20Based%20on%20Causal%20Inference&entry.906535625=Guoqing%20Ma%20and%20Jia%20Zhu%20and%20Hanghui%20Guo%20and%20Weijie%20Shi%20and%20Jiawei%20Shen%20and%20Jingjiang%20Liu%20and%20Yidan%20Liang&entry.1292438233=%20%20Multi-agent%20systems%20%28MAS%29%20are%20critical%20for%20automating%20complex%20tasks%2C%20yet%0Atheir%20practical%20deployment%20is%20severely%20hampered%20by%20the%20challenge%20of%20failure%0Aattribution.%20Current%20diagnostic%20tools%2C%20which%20rely%20on%20statistical%20correlations%2C%0Aare%20fundamentally%20inadequate%3B%20on%20challenging%20benchmarks%20like%20Who%5C%26When%2C%0Astate-of-the-art%20methods%20achieve%20less%20than%2015%5C%25%20accuracy%20in%20locating%20the%0Aroot-cause%20step%20of%20a%20failure.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20the%0Afirst%20failure%20attribution%20framework%20for%20MAS%20grounded%20in%20multi-granularity%0Acausal%20inference.%20Our%20approach%20makes%20two%20key%20technical%20contributions%3A%20%281%29%20a%0Aperformance%20causal%20inversion%20principle%2C%20which%20correctly%20models%20performance%0Adependencies%20by%20reversing%20the%20data%20flow%20in%20execution%20logs%2C%20combined%20with%0AShapley%20values%20to%20accurately%20assign%20agent-level%20blame%3B%20%282%29%20a%20novel%20causal%0Adiscovery%20algorithm%2C%20CDC-MAS%2C%20that%20robustly%20identifies%20critical%20failure%20steps%0Aby%20tackling%20the%20non-stationary%20nature%20of%20MAS%20interaction%20data.%20The%20framework%27s%0Aattribution%20results%20directly%20fuel%20an%20automated%20optimization%20loop%2C%20generating%0Atargeted%20suggestions%20whose%20efficacy%20is%20validated%20via%20counterfactual%0Asimulations.%20Evaluations%20on%20the%20Who%5C%26When%20and%20TRAIL%20benchmarks%20demonstrate%20a%0Asignificant%20leap%20in%20performance.%20Our%20method%20achieves%20up%20to%2036.2%5C%25%20step-level%0Aaccuracy.%20Crucially%2C%20the%20generated%20optimizations%20boost%20overall%20task%20success%0Arates%20by%20an%20average%20of%2022.4%5C%25.%20This%20work%20provides%20a%20principled%20and%20effective%0Asolution%20for%20debugging%20complex%20agent%20interactions%2C%20paving%20the%20way%20for%20more%0Areliable%20and%20interpretable%20multi-agent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08682v1&entry.124074799=Read"},
{"title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation", "author": "Joachim Baumann and Paul R\u00f6ttger and Aleksandra Urman and Albert Wendsj\u00f6 and Flor Miriam Plaza-del-Arco and Johannes B. Gruber and Dirk Hovy", "abstract": "  Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant.\n", "link": "http://arxiv.org/abs/2509.08825v1", "date": "2025-09-10", "relevancy": 1.786, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.436}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20Hacking%3A%20Quantifying%20the%20Hidden%20Risks%20of%20Using%20LLMs%0A%20%20for%20Text%20Annotation&body=Title%3A%20Large%20Language%20Model%20Hacking%3A%20Quantifying%20the%20Hidden%20Risks%20of%20Using%20LLMs%0A%20%20for%20Text%20Annotation%0AAuthor%3A%20Joachim%20Baumann%20and%20Paul%20R%C3%B6ttger%20and%20Aleksandra%20Urman%20and%20Albert%20Wendsj%C3%B6%20and%20Flor%20Miriam%20Plaza-del-Arco%20and%20Johannes%20B.%20Gruber%20and%20Dirk%20Hovy%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20rapidly%20transforming%20social%20science%20research%0Aby%20enabling%20the%20automation%20of%20labor-intensive%20tasks%20like%20data%20annotation%20and%0Atext%20analysis.%20However%2C%20LLM%20outputs%20vary%20significantly%20depending%20on%20the%0Aimplementation%20choices%20made%20by%20researchers%20%28e.g.%2C%20model%20selection%2C%20prompting%0Astrategy%2C%20or%20temperature%20settings%29.%20Such%20variation%20can%20introduce%20systematic%0Abiases%20and%20random%20errors%2C%20which%20propagate%20to%20downstream%20analyses%20and%20cause%20Type%0AI%2C%20Type%20II%2C%20Type%20S%2C%20or%20Type%20M%20errors.%20We%20call%20this%20LLM%20hacking.%0A%20%20We%20quantify%20the%20risk%20of%20LLM%20hacking%20by%20replicating%2037%20data%20annotation%20tasks%0Afrom%2021%20published%20social%20science%20research%20studies%20with%2018%20different%20models.%0AAnalyzing%2013%20million%20LLM%20labels%2C%20we%20test%202%2C361%20realistic%20hypotheses%20to%20measure%0Ahow%20plausible%20researcher%20choices%20affect%20statistical%20conclusions.%20We%20find%0Aincorrect%20conclusions%20based%20on%20LLM-annotated%20data%20in%20approximately%20one%20in%20three%0Ahypotheses%20for%20state-of-the-art%20models%2C%20and%20in%20half%20the%20hypotheses%20for%20small%0Alanguage%20models.%20While%20our%20findings%20show%20that%20higher%20task%20performance%20and%0Abetter%20general%20model%20capabilities%20reduce%20LLM%20hacking%20risk%2C%20even%20highly%20accurate%0Amodels%20do%20not%20completely%20eliminate%20it.%20The%20risk%20of%20LLM%20hacking%20decreases%20as%0Aeffect%20sizes%20increase%2C%20indicating%20the%20need%20for%20more%20rigorous%20verification%20of%0Afindings%20near%20significance%20thresholds.%20Our%20extensive%20analysis%20of%20LLM%20hacking%0Amitigation%20techniques%20emphasizes%20the%20importance%20of%20human%20annotations%20in%0Areducing%20false%20positive%20findings%20and%20improving%20model%20selection.%20Surprisingly%2C%0Acommon%20regression%20estimator%20correction%20techniques%20are%20largely%20ineffective%20in%0Areducing%20LLM%20hacking%20risk%2C%20as%20they%20heavily%20trade%20off%20Type%20I%20vs.%20Type%20II%20errors.%0A%20%20Beyond%20accidental%20errors%2C%20we%20find%20that%20intentional%20LLM%20hacking%20is%0Aunacceptably%20simple.%20With%20few%20LLMs%20and%20just%20a%20handful%20of%20prompt%20paraphrases%2C%0Aanything%20can%20be%20presented%20as%20statistically%20significant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520Hacking%253A%2520Quantifying%2520the%2520Hidden%2520Risks%2520of%2520Using%2520LLMs%250A%2520%2520for%2520Text%2520Annotation%26entry.906535625%3DJoachim%2520Baumann%2520and%2520Paul%2520R%25C3%25B6ttger%2520and%2520Aleksandra%2520Urman%2520and%2520Albert%2520Wendsj%25C3%25B6%2520and%2520Flor%2520Miriam%2520Plaza-del-Arco%2520and%2520Johannes%2520B.%2520Gruber%2520and%2520Dirk%2520Hovy%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520rapidly%2520transforming%2520social%2520science%2520research%250Aby%2520enabling%2520the%2520automation%2520of%2520labor-intensive%2520tasks%2520like%2520data%2520annotation%2520and%250Atext%2520analysis.%2520However%252C%2520LLM%2520outputs%2520vary%2520significantly%2520depending%2520on%2520the%250Aimplementation%2520choices%2520made%2520by%2520researchers%2520%2528e.g.%252C%2520model%2520selection%252C%2520prompting%250Astrategy%252C%2520or%2520temperature%2520settings%2529.%2520Such%2520variation%2520can%2520introduce%2520systematic%250Abiases%2520and%2520random%2520errors%252C%2520which%2520propagate%2520to%2520downstream%2520analyses%2520and%2520cause%2520Type%250AI%252C%2520Type%2520II%252C%2520Type%2520S%252C%2520or%2520Type%2520M%2520errors.%2520We%2520call%2520this%2520LLM%2520hacking.%250A%2520%2520We%2520quantify%2520the%2520risk%2520of%2520LLM%2520hacking%2520by%2520replicating%252037%2520data%2520annotation%2520tasks%250Afrom%252021%2520published%2520social%2520science%2520research%2520studies%2520with%252018%2520different%2520models.%250AAnalyzing%252013%2520million%2520LLM%2520labels%252C%2520we%2520test%25202%252C361%2520realistic%2520hypotheses%2520to%2520measure%250Ahow%2520plausible%2520researcher%2520choices%2520affect%2520statistical%2520conclusions.%2520We%2520find%250Aincorrect%2520conclusions%2520based%2520on%2520LLM-annotated%2520data%2520in%2520approximately%2520one%2520in%2520three%250Ahypotheses%2520for%2520state-of-the-art%2520models%252C%2520and%2520in%2520half%2520the%2520hypotheses%2520for%2520small%250Alanguage%2520models.%2520While%2520our%2520findings%2520show%2520that%2520higher%2520task%2520performance%2520and%250Abetter%2520general%2520model%2520capabilities%2520reduce%2520LLM%2520hacking%2520risk%252C%2520even%2520highly%2520accurate%250Amodels%2520do%2520not%2520completely%2520eliminate%2520it.%2520The%2520risk%2520of%2520LLM%2520hacking%2520decreases%2520as%250Aeffect%2520sizes%2520increase%252C%2520indicating%2520the%2520need%2520for%2520more%2520rigorous%2520verification%2520of%250Afindings%2520near%2520significance%2520thresholds.%2520Our%2520extensive%2520analysis%2520of%2520LLM%2520hacking%250Amitigation%2520techniques%2520emphasizes%2520the%2520importance%2520of%2520human%2520annotations%2520in%250Areducing%2520false%2520positive%2520findings%2520and%2520improving%2520model%2520selection.%2520Surprisingly%252C%250Acommon%2520regression%2520estimator%2520correction%2520techniques%2520are%2520largely%2520ineffective%2520in%250Areducing%2520LLM%2520hacking%2520risk%252C%2520as%2520they%2520heavily%2520trade%2520off%2520Type%2520I%2520vs.%2520Type%2520II%2520errors.%250A%2520%2520Beyond%2520accidental%2520errors%252C%2520we%2520find%2520that%2520intentional%2520LLM%2520hacking%2520is%250Aunacceptably%2520simple.%2520With%2520few%2520LLMs%2520and%2520just%2520a%2520handful%2520of%2520prompt%2520paraphrases%252C%250Aanything%2520can%2520be%2520presented%2520as%2520statistically%2520significant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20Hacking%3A%20Quantifying%20the%20Hidden%20Risks%20of%20Using%20LLMs%0A%20%20for%20Text%20Annotation&entry.906535625=Joachim%20Baumann%20and%20Paul%20R%C3%B6ttger%20and%20Aleksandra%20Urman%20and%20Albert%20Wendsj%C3%B6%20and%20Flor%20Miriam%20Plaza-del-Arco%20and%20Johannes%20B.%20Gruber%20and%20Dirk%20Hovy&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20rapidly%20transforming%20social%20science%20research%0Aby%20enabling%20the%20automation%20of%20labor-intensive%20tasks%20like%20data%20annotation%20and%0Atext%20analysis.%20However%2C%20LLM%20outputs%20vary%20significantly%20depending%20on%20the%0Aimplementation%20choices%20made%20by%20researchers%20%28e.g.%2C%20model%20selection%2C%20prompting%0Astrategy%2C%20or%20temperature%20settings%29.%20Such%20variation%20can%20introduce%20systematic%0Abiases%20and%20random%20errors%2C%20which%20propagate%20to%20downstream%20analyses%20and%20cause%20Type%0AI%2C%20Type%20II%2C%20Type%20S%2C%20or%20Type%20M%20errors.%20We%20call%20this%20LLM%20hacking.%0A%20%20We%20quantify%20the%20risk%20of%20LLM%20hacking%20by%20replicating%2037%20data%20annotation%20tasks%0Afrom%2021%20published%20social%20science%20research%20studies%20with%2018%20different%20models.%0AAnalyzing%2013%20million%20LLM%20labels%2C%20we%20test%202%2C361%20realistic%20hypotheses%20to%20measure%0Ahow%20plausible%20researcher%20choices%20affect%20statistical%20conclusions.%20We%20find%0Aincorrect%20conclusions%20based%20on%20LLM-annotated%20data%20in%20approximately%20one%20in%20three%0Ahypotheses%20for%20state-of-the-art%20models%2C%20and%20in%20half%20the%20hypotheses%20for%20small%0Alanguage%20models.%20While%20our%20findings%20show%20that%20higher%20task%20performance%20and%0Abetter%20general%20model%20capabilities%20reduce%20LLM%20hacking%20risk%2C%20even%20highly%20accurate%0Amodels%20do%20not%20completely%20eliminate%20it.%20The%20risk%20of%20LLM%20hacking%20decreases%20as%0Aeffect%20sizes%20increase%2C%20indicating%20the%20need%20for%20more%20rigorous%20verification%20of%0Afindings%20near%20significance%20thresholds.%20Our%20extensive%20analysis%20of%20LLM%20hacking%0Amitigation%20techniques%20emphasizes%20the%20importance%20of%20human%20annotations%20in%0Areducing%20false%20positive%20findings%20and%20improving%20model%20selection.%20Surprisingly%2C%0Acommon%20regression%20estimator%20correction%20techniques%20are%20largely%20ineffective%20in%0Areducing%20LLM%20hacking%20risk%2C%20as%20they%20heavily%20trade%20off%20Type%20I%20vs.%20Type%20II%20errors.%0A%20%20Beyond%20accidental%20errors%2C%20we%20find%20that%20intentional%20LLM%20hacking%20is%0Aunacceptably%20simple.%20With%20few%20LLMs%20and%20just%20a%20handful%20of%20prompt%20paraphrases%2C%0Aanything%20can%20be%20presented%20as%20statistically%20significant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08825v1&entry.124074799=Read"},
{"title": "PQMass: Probabilistic Assessment of the Quality of Generative Models\n  using Probability Mass Estimation", "author": "Pablo Lemos and Sammy Sharief and Nikolay Malkin and Salma Salhi and Connor Stone and Laurence Perreault-Levasseur and Yashar Hezaveh", "abstract": "  We propose a likelihood-free method for comparing two distributions given\nsamples from each, with the goal of assessing the quality of generative models.\nThe proposed approach, PQMass, provides a statistically rigorous method for\nassessing the performance of a single generative model or the comparison of\nmultiple competing models. PQMass divides the sample space into non-overlapping\nregions and applies chi-squared tests to the number of data samples that fall\nwithin each region, giving a p-value that measures the probability that the bin\ncounts derived from two sets of samples are drawn from the same multinomial\ndistribution. PQMass does not depend on assumptions regarding the density of\nthe true distribution, nor does it rely on training or fitting any auxiliary\nmodels. We evaluate PQMass on data of various modalities and dimensions,\ndemonstrating its effectiveness in assessing the quality, novelty, and\ndiversity of generated samples. We further show that PQMass scales well to\nmoderately high-dimensional data and thus obviates the need for feature\nextraction in practical applications.\n", "link": "http://arxiv.org/abs/2402.04355v3", "date": "2025-09-10", "relevancy": 0.9959, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5109}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4974}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PQMass%3A%20Probabilistic%20Assessment%20of%20the%20Quality%20of%20Generative%20Models%0A%20%20using%20Probability%20Mass%20Estimation&body=Title%3A%20PQMass%3A%20Probabilistic%20Assessment%20of%20the%20Quality%20of%20Generative%20Models%0A%20%20using%20Probability%20Mass%20Estimation%0AAuthor%3A%20Pablo%20Lemos%20and%20Sammy%20Sharief%20and%20Nikolay%20Malkin%20and%20Salma%20Salhi%20and%20Connor%20Stone%20and%20Laurence%20Perreault-Levasseur%20and%20Yashar%20Hezaveh%0AAbstract%3A%20%20%20We%20propose%20a%20likelihood-free%20method%20for%20comparing%20two%20distributions%20given%0Asamples%20from%20each%2C%20with%20the%20goal%20of%20assessing%20the%20quality%20of%20generative%20models.%0AThe%20proposed%20approach%2C%20PQMass%2C%20provides%20a%20statistically%20rigorous%20method%20for%0Aassessing%20the%20performance%20of%20a%20single%20generative%20model%20or%20the%20comparison%20of%0Amultiple%20competing%20models.%20PQMass%20divides%20the%20sample%20space%20into%20non-overlapping%0Aregions%20and%20applies%20chi-squared%20tests%20to%20the%20number%20of%20data%20samples%20that%20fall%0Awithin%20each%20region%2C%20giving%20a%20p-value%20that%20measures%20the%20probability%20that%20the%20bin%0Acounts%20derived%20from%20two%20sets%20of%20samples%20are%20drawn%20from%20the%20same%20multinomial%0Adistribution.%20PQMass%20does%20not%20depend%20on%20assumptions%20regarding%20the%20density%20of%0Athe%20true%20distribution%2C%20nor%20does%20it%20rely%20on%20training%20or%20fitting%20any%20auxiliary%0Amodels.%20We%20evaluate%20PQMass%20on%20data%20of%20various%20modalities%20and%20dimensions%2C%0Ademonstrating%20its%20effectiveness%20in%20assessing%20the%20quality%2C%20novelty%2C%20and%0Adiversity%20of%20generated%20samples.%20We%20further%20show%20that%20PQMass%20scales%20well%20to%0Amoderately%20high-dimensional%20data%20and%20thus%20obviates%20the%20need%20for%20feature%0Aextraction%20in%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04355v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPQMass%253A%2520Probabilistic%2520Assessment%2520of%2520the%2520Quality%2520of%2520Generative%2520Models%250A%2520%2520using%2520Probability%2520Mass%2520Estimation%26entry.906535625%3DPablo%2520Lemos%2520and%2520Sammy%2520Sharief%2520and%2520Nikolay%2520Malkin%2520and%2520Salma%2520Salhi%2520and%2520Connor%2520Stone%2520and%2520Laurence%2520Perreault-Levasseur%2520and%2520Yashar%2520Hezaveh%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520likelihood-free%2520method%2520for%2520comparing%2520two%2520distributions%2520given%250Asamples%2520from%2520each%252C%2520with%2520the%2520goal%2520of%2520assessing%2520the%2520quality%2520of%2520generative%2520models.%250AThe%2520proposed%2520approach%252C%2520PQMass%252C%2520provides%2520a%2520statistically%2520rigorous%2520method%2520for%250Aassessing%2520the%2520performance%2520of%2520a%2520single%2520generative%2520model%2520or%2520the%2520comparison%2520of%250Amultiple%2520competing%2520models.%2520PQMass%2520divides%2520the%2520sample%2520space%2520into%2520non-overlapping%250Aregions%2520and%2520applies%2520chi-squared%2520tests%2520to%2520the%2520number%2520of%2520data%2520samples%2520that%2520fall%250Awithin%2520each%2520region%252C%2520giving%2520a%2520p-value%2520that%2520measures%2520the%2520probability%2520that%2520the%2520bin%250Acounts%2520derived%2520from%2520two%2520sets%2520of%2520samples%2520are%2520drawn%2520from%2520the%2520same%2520multinomial%250Adistribution.%2520PQMass%2520does%2520not%2520depend%2520on%2520assumptions%2520regarding%2520the%2520density%2520of%250Athe%2520true%2520distribution%252C%2520nor%2520does%2520it%2520rely%2520on%2520training%2520or%2520fitting%2520any%2520auxiliary%250Amodels.%2520We%2520evaluate%2520PQMass%2520on%2520data%2520of%2520various%2520modalities%2520and%2520dimensions%252C%250Ademonstrating%2520its%2520effectiveness%2520in%2520assessing%2520the%2520quality%252C%2520novelty%252C%2520and%250Adiversity%2520of%2520generated%2520samples.%2520We%2520further%2520show%2520that%2520PQMass%2520scales%2520well%2520to%250Amoderately%2520high-dimensional%2520data%2520and%2520thus%2520obviates%2520the%2520need%2520for%2520feature%250Aextraction%2520in%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04355v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PQMass%3A%20Probabilistic%20Assessment%20of%20the%20Quality%20of%20Generative%20Models%0A%20%20using%20Probability%20Mass%20Estimation&entry.906535625=Pablo%20Lemos%20and%20Sammy%20Sharief%20and%20Nikolay%20Malkin%20and%20Salma%20Salhi%20and%20Connor%20Stone%20and%20Laurence%20Perreault-Levasseur%20and%20Yashar%20Hezaveh&entry.1292438233=%20%20We%20propose%20a%20likelihood-free%20method%20for%20comparing%20two%20distributions%20given%0Asamples%20from%20each%2C%20with%20the%20goal%20of%20assessing%20the%20quality%20of%20generative%20models.%0AThe%20proposed%20approach%2C%20PQMass%2C%20provides%20a%20statistically%20rigorous%20method%20for%0Aassessing%20the%20performance%20of%20a%20single%20generative%20model%20or%20the%20comparison%20of%0Amultiple%20competing%20models.%20PQMass%20divides%20the%20sample%20space%20into%20non-overlapping%0Aregions%20and%20applies%20chi-squared%20tests%20to%20the%20number%20of%20data%20samples%20that%20fall%0Awithin%20each%20region%2C%20giving%20a%20p-value%20that%20measures%20the%20probability%20that%20the%20bin%0Acounts%20derived%20from%20two%20sets%20of%20samples%20are%20drawn%20from%20the%20same%20multinomial%0Adistribution.%20PQMass%20does%20not%20depend%20on%20assumptions%20regarding%20the%20density%20of%0Athe%20true%20distribution%2C%20nor%20does%20it%20rely%20on%20training%20or%20fitting%20any%20auxiliary%0Amodels.%20We%20evaluate%20PQMass%20on%20data%20of%20various%20modalities%20and%20dimensions%2C%0Ademonstrating%20its%20effectiveness%20in%20assessing%20the%20quality%2C%20novelty%2C%20and%0Adiversity%20of%20generated%20samples.%20We%20further%20show%20that%20PQMass%20scales%20well%20to%0Amoderately%20high-dimensional%20data%20and%20thus%20obviates%20the%20need%20for%20feature%0Aextraction%20in%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04355v3&entry.124074799=Read"},
{"title": "Second-Order Tensorial Partial Differential Equations on Graphs", "author": "Aref Einizade and Fragkiskos D. Malliaros and Jhony H. Giraldo", "abstract": "  Processing data on multiple interacting graphs is crucial for many\napplications, but existing approaches rely mostly on discrete filtering or\nfirst-order continuous models that dampen high frequencies and propagate\ninformation slowly. We introduce second-order tensorial partial differential\nequations on graphs (So-TPDEGs) and propose the first theoretically grounded\nframework for second-order continuous product graph neural networks. Our method\nexploits the separability of cosine kernels in Cartesian product graphs to\nenable efficient spectral decomposition while preserving high-frequency\nsignals. We further provide rigorous analyses of stability under graph\nperturbations and over-smoothing, establishing a solid theoretical foundation\nfor continuous graph learning.\n", "link": "http://arxiv.org/abs/2509.02015v2", "date": "2025-09-10", "relevancy": 1.4136, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4882}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4678}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Second-Order%20Tensorial%20Partial%20Differential%20Equations%20on%20Graphs&body=Title%3A%20Second-Order%20Tensorial%20Partial%20Differential%20Equations%20on%20Graphs%0AAuthor%3A%20Aref%20Einizade%20and%20Fragkiskos%20D.%20Malliaros%20and%20Jhony%20H.%20Giraldo%0AAbstract%3A%20%20%20Processing%20data%20on%20multiple%20interacting%20graphs%20is%20crucial%20for%20many%0Aapplications%2C%20but%20existing%20approaches%20rely%20mostly%20on%20discrete%20filtering%20or%0Afirst-order%20continuous%20models%20that%20dampen%20high%20frequencies%20and%20propagate%0Ainformation%20slowly.%20We%20introduce%20second-order%20tensorial%20partial%20differential%0Aequations%20on%20graphs%20%28So-TPDEGs%29%20and%20propose%20the%20first%20theoretically%20grounded%0Aframework%20for%20second-order%20continuous%20product%20graph%20neural%20networks.%20Our%20method%0Aexploits%20the%20separability%20of%20cosine%20kernels%20in%20Cartesian%20product%20graphs%20to%0Aenable%20efficient%20spectral%20decomposition%20while%20preserving%20high-frequency%0Asignals.%20We%20further%20provide%20rigorous%20analyses%20of%20stability%20under%20graph%0Aperturbations%20and%20over-smoothing%2C%20establishing%20a%20solid%20theoretical%20foundation%0Afor%20continuous%20graph%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecond-Order%2520Tensorial%2520Partial%2520Differential%2520Equations%2520on%2520Graphs%26entry.906535625%3DAref%2520Einizade%2520and%2520Fragkiskos%2520D.%2520Malliaros%2520and%2520Jhony%2520H.%2520Giraldo%26entry.1292438233%3D%2520%2520Processing%2520data%2520on%2520multiple%2520interacting%2520graphs%2520is%2520crucial%2520for%2520many%250Aapplications%252C%2520but%2520existing%2520approaches%2520rely%2520mostly%2520on%2520discrete%2520filtering%2520or%250Afirst-order%2520continuous%2520models%2520that%2520dampen%2520high%2520frequencies%2520and%2520propagate%250Ainformation%2520slowly.%2520We%2520introduce%2520second-order%2520tensorial%2520partial%2520differential%250Aequations%2520on%2520graphs%2520%2528So-TPDEGs%2529%2520and%2520propose%2520the%2520first%2520theoretically%2520grounded%250Aframework%2520for%2520second-order%2520continuous%2520product%2520graph%2520neural%2520networks.%2520Our%2520method%250Aexploits%2520the%2520separability%2520of%2520cosine%2520kernels%2520in%2520Cartesian%2520product%2520graphs%2520to%250Aenable%2520efficient%2520spectral%2520decomposition%2520while%2520preserving%2520high-frequency%250Asignals.%2520We%2520further%2520provide%2520rigorous%2520analyses%2520of%2520stability%2520under%2520graph%250Aperturbations%2520and%2520over-smoothing%252C%2520establishing%2520a%2520solid%2520theoretical%2520foundation%250Afor%2520continuous%2520graph%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Second-Order%20Tensorial%20Partial%20Differential%20Equations%20on%20Graphs&entry.906535625=Aref%20Einizade%20and%20Fragkiskos%20D.%20Malliaros%20and%20Jhony%20H.%20Giraldo&entry.1292438233=%20%20Processing%20data%20on%20multiple%20interacting%20graphs%20is%20crucial%20for%20many%0Aapplications%2C%20but%20existing%20approaches%20rely%20mostly%20on%20discrete%20filtering%20or%0Afirst-order%20continuous%20models%20that%20dampen%20high%20frequencies%20and%20propagate%0Ainformation%20slowly.%20We%20introduce%20second-order%20tensorial%20partial%20differential%0Aequations%20on%20graphs%20%28So-TPDEGs%29%20and%20propose%20the%20first%20theoretically%20grounded%0Aframework%20for%20second-order%20continuous%20product%20graph%20neural%20networks.%20Our%20method%0Aexploits%20the%20separability%20of%20cosine%20kernels%20in%20Cartesian%20product%20graphs%20to%0Aenable%20efficient%20spectral%20decomposition%20while%20preserving%20high-frequency%0Asignals.%20We%20further%20provide%20rigorous%20analyses%20of%20stability%20under%20graph%0Aperturbations%20and%20over-smoothing%2C%20establishing%20a%20solid%20theoretical%20foundation%0Afor%20continuous%20graph%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02015v2&entry.124074799=Read"},
{"title": "Merge-of-Thought Distillation", "author": "Zhanming Shen and Zeyu Qin and Zenan Huang and Hao Chen and Jiaqi Hu and Yihong Zhuang and Guoshan Lu and Gang Chen and Junbo Zhao", "abstract": "  Efficient reasoning distillation for long chain-of-thought (CoT) models is\nincreasingly constrained by the assumption of a single oracle teacher, despite\npractical availability of multiple candidate teachers and growing CoT corpora.\nWe revisit teacher selection and observe that different students have different\n\"best teachers,\" and even for the same student the best teacher can vary across\ndatasets. Therefore, to unify multiple teachers' reasoning abilities into\nstudent with overcoming conflicts among various teachers' supervision, we\npropose Merge-of-Thought Distillation (MoT), a lightweight framework that\nalternates between teacher-specific supervised fine-tuning branches and\nweight-space merging of the resulting student variants. On competition math\nbenchmarks, using only about 200 high-quality CoT samples, applying MoT to a\nQwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,\nQWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT\nconsistently outperforms the best single-teacher distillation and the naive\nmulti-teacher union, raises the performance ceiling while mitigating\noverfitting, and shows robustness to distribution-shifted and peer-level\nteachers. Moreover, MoT reduces catastrophic forgetting, improves general\nreasoning beyond mathematics and even cultivates a better teacher, indicating\nthat consensus-filtered reasoning features transfer broadly. These results\nposition MoT as a simple, scalable route to efficiently distilling long CoT\ncapabilities from diverse teachers into compact students.\n", "link": "http://arxiv.org/abs/2509.08814v1", "date": "2025-09-10", "relevancy": 1.8743, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4855}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4575}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Merge-of-Thought%20Distillation&body=Title%3A%20Merge-of-Thought%20Distillation%0AAuthor%3A%20Zhanming%20Shen%20and%20Zeyu%20Qin%20and%20Zenan%20Huang%20and%20Hao%20Chen%20and%20Jiaqi%20Hu%20and%20Yihong%20Zhuang%20and%20Guoshan%20Lu%20and%20Gang%20Chen%20and%20Junbo%20Zhao%0AAbstract%3A%20%20%20Efficient%20reasoning%20distillation%20for%20long%20chain-of-thought%20%28CoT%29%20models%20is%0Aincreasingly%20constrained%20by%20the%20assumption%20of%20a%20single%20oracle%20teacher%2C%20despite%0Apractical%20availability%20of%20multiple%20candidate%20teachers%20and%20growing%20CoT%20corpora.%0AWe%20revisit%20teacher%20selection%20and%20observe%20that%20different%20students%20have%20different%0A%22best%20teachers%2C%22%20and%20even%20for%20the%20same%20student%20the%20best%20teacher%20can%20vary%20across%0Adatasets.%20Therefore%2C%20to%20unify%20multiple%20teachers%27%20reasoning%20abilities%20into%0Astudent%20with%20overcoming%20conflicts%20among%20various%20teachers%27%20supervision%2C%20we%0Apropose%20Merge-of-Thought%20Distillation%20%28MoT%29%2C%20a%20lightweight%20framework%20that%0Aalternates%20between%20teacher-specific%20supervised%20fine-tuning%20branches%20and%0Aweight-space%20merging%20of%20the%20resulting%20student%20variants.%20On%20competition%20math%0Abenchmarks%2C%20using%20only%20about%20200%20high-quality%20CoT%20samples%2C%20applying%20MoT%20to%20a%0AQwen3-14B%20student%20surpasses%20strong%20models%20including%20DEEPSEEK-R1%2C%20QWEN3-30B-A3B%2C%0AQWEN3-32B%2C%20and%20OPENAI-O1%2C%20demonstrating%20substantial%20gains.%20Besides%2C%20MoT%0Aconsistently%20outperforms%20the%20best%20single-teacher%20distillation%20and%20the%20naive%0Amulti-teacher%20union%2C%20raises%20the%20performance%20ceiling%20while%20mitigating%0Aoverfitting%2C%20and%20shows%20robustness%20to%20distribution-shifted%20and%20peer-level%0Ateachers.%20Moreover%2C%20MoT%20reduces%20catastrophic%20forgetting%2C%20improves%20general%0Areasoning%20beyond%20mathematics%20and%20even%20cultivates%20a%20better%20teacher%2C%20indicating%0Athat%20consensus-filtered%20reasoning%20features%20transfer%20broadly.%20These%20results%0Aposition%20MoT%20as%20a%20simple%2C%20scalable%20route%20to%20efficiently%20distilling%20long%20CoT%0Acapabilities%20from%20diverse%20teachers%20into%20compact%20students.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMerge-of-Thought%2520Distillation%26entry.906535625%3DZhanming%2520Shen%2520and%2520Zeyu%2520Qin%2520and%2520Zenan%2520Huang%2520and%2520Hao%2520Chen%2520and%2520Jiaqi%2520Hu%2520and%2520Yihong%2520Zhuang%2520and%2520Guoshan%2520Lu%2520and%2520Gang%2520Chen%2520and%2520Junbo%2520Zhao%26entry.1292438233%3D%2520%2520Efficient%2520reasoning%2520distillation%2520for%2520long%2520chain-of-thought%2520%2528CoT%2529%2520models%2520is%250Aincreasingly%2520constrained%2520by%2520the%2520assumption%2520of%2520a%2520single%2520oracle%2520teacher%252C%2520despite%250Apractical%2520availability%2520of%2520multiple%2520candidate%2520teachers%2520and%2520growing%2520CoT%2520corpora.%250AWe%2520revisit%2520teacher%2520selection%2520and%2520observe%2520that%2520different%2520students%2520have%2520different%250A%2522best%2520teachers%252C%2522%2520and%2520even%2520for%2520the%2520same%2520student%2520the%2520best%2520teacher%2520can%2520vary%2520across%250Adatasets.%2520Therefore%252C%2520to%2520unify%2520multiple%2520teachers%2527%2520reasoning%2520abilities%2520into%250Astudent%2520with%2520overcoming%2520conflicts%2520among%2520various%2520teachers%2527%2520supervision%252C%2520we%250Apropose%2520Merge-of-Thought%2520Distillation%2520%2528MoT%2529%252C%2520a%2520lightweight%2520framework%2520that%250Aalternates%2520between%2520teacher-specific%2520supervised%2520fine-tuning%2520branches%2520and%250Aweight-space%2520merging%2520of%2520the%2520resulting%2520student%2520variants.%2520On%2520competition%2520math%250Abenchmarks%252C%2520using%2520only%2520about%2520200%2520high-quality%2520CoT%2520samples%252C%2520applying%2520MoT%2520to%2520a%250AQwen3-14B%2520student%2520surpasses%2520strong%2520models%2520including%2520DEEPSEEK-R1%252C%2520QWEN3-30B-A3B%252C%250AQWEN3-32B%252C%2520and%2520OPENAI-O1%252C%2520demonstrating%2520substantial%2520gains.%2520Besides%252C%2520MoT%250Aconsistently%2520outperforms%2520the%2520best%2520single-teacher%2520distillation%2520and%2520the%2520naive%250Amulti-teacher%2520union%252C%2520raises%2520the%2520performance%2520ceiling%2520while%2520mitigating%250Aoverfitting%252C%2520and%2520shows%2520robustness%2520to%2520distribution-shifted%2520and%2520peer-level%250Ateachers.%2520Moreover%252C%2520MoT%2520reduces%2520catastrophic%2520forgetting%252C%2520improves%2520general%250Areasoning%2520beyond%2520mathematics%2520and%2520even%2520cultivates%2520a%2520better%2520teacher%252C%2520indicating%250Athat%2520consensus-filtered%2520reasoning%2520features%2520transfer%2520broadly.%2520These%2520results%250Aposition%2520MoT%2520as%2520a%2520simple%252C%2520scalable%2520route%2520to%2520efficiently%2520distilling%2520long%2520CoT%250Acapabilities%2520from%2520diverse%2520teachers%2520into%2520compact%2520students.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Merge-of-Thought%20Distillation&entry.906535625=Zhanming%20Shen%20and%20Zeyu%20Qin%20and%20Zenan%20Huang%20and%20Hao%20Chen%20and%20Jiaqi%20Hu%20and%20Yihong%20Zhuang%20and%20Guoshan%20Lu%20and%20Gang%20Chen%20and%20Junbo%20Zhao&entry.1292438233=%20%20Efficient%20reasoning%20distillation%20for%20long%20chain-of-thought%20%28CoT%29%20models%20is%0Aincreasingly%20constrained%20by%20the%20assumption%20of%20a%20single%20oracle%20teacher%2C%20despite%0Apractical%20availability%20of%20multiple%20candidate%20teachers%20and%20growing%20CoT%20corpora.%0AWe%20revisit%20teacher%20selection%20and%20observe%20that%20different%20students%20have%20different%0A%22best%20teachers%2C%22%20and%20even%20for%20the%20same%20student%20the%20best%20teacher%20can%20vary%20across%0Adatasets.%20Therefore%2C%20to%20unify%20multiple%20teachers%27%20reasoning%20abilities%20into%0Astudent%20with%20overcoming%20conflicts%20among%20various%20teachers%27%20supervision%2C%20we%0Apropose%20Merge-of-Thought%20Distillation%20%28MoT%29%2C%20a%20lightweight%20framework%20that%0Aalternates%20between%20teacher-specific%20supervised%20fine-tuning%20branches%20and%0Aweight-space%20merging%20of%20the%20resulting%20student%20variants.%20On%20competition%20math%0Abenchmarks%2C%20using%20only%20about%20200%20high-quality%20CoT%20samples%2C%20applying%20MoT%20to%20a%0AQwen3-14B%20student%20surpasses%20strong%20models%20including%20DEEPSEEK-R1%2C%20QWEN3-30B-A3B%2C%0AQWEN3-32B%2C%20and%20OPENAI-O1%2C%20demonstrating%20substantial%20gains.%20Besides%2C%20MoT%0Aconsistently%20outperforms%20the%20best%20single-teacher%20distillation%20and%20the%20naive%0Amulti-teacher%20union%2C%20raises%20the%20performance%20ceiling%20while%20mitigating%0Aoverfitting%2C%20and%20shows%20robustness%20to%20distribution-shifted%20and%20peer-level%0Ateachers.%20Moreover%2C%20MoT%20reduces%20catastrophic%20forgetting%2C%20improves%20general%0Areasoning%20beyond%20mathematics%20and%20even%20cultivates%20a%20better%20teacher%2C%20indicating%0Athat%20consensus-filtered%20reasoning%20features%20transfer%20broadly.%20These%20results%0Aposition%20MoT%20as%20a%20simple%2C%20scalable%20route%20to%20efficiently%20distilling%20long%20CoT%0Acapabilities%20from%20diverse%20teachers%20into%20compact%20students.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08814v1&entry.124074799=Read"},
{"title": "Data-driven generative simulation of SDEs using diffusion models", "author": "Xuefeng Gao and Jiale Zha and Xun Yu Zhou", "abstract": "  This paper introduces a new approach to generating sample paths of unknown\nstochastic differential equations (SDEs) using diffusion models, a class of\ngenerative AI models commonly employed in image and video applications. Unlike\nthe traditional Monte Carlo methods for simulating SDEs, which require explicit\nspecifications of the drift and diffusion coefficients, our method takes a\nmodel-free, data-driven approach. Given a finite set of sample paths from an\nSDE, we utilize conditional diffusion models to generate new, synthetic paths\nof the same SDE. To demonstrate the effectiveness of our approach, we conduct a\nsimulation experiment to compare our method with alternative benchmark ones\nincluding neural SDEs. Furthermore, in an empirical study we leverage these\nsynthetically generated sample paths to enhance the performance of\nreinforcement learning algorithms for continuous-time mean-variance portfolio\nselection, hinting promising applications of diffusion models in financial\nanalysis and decision-making.\n", "link": "http://arxiv.org/abs/2509.08731v1", "date": "2025-09-10", "relevancy": 1.1038, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5822}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5374}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-driven%20generative%20simulation%20of%20SDEs%20using%20diffusion%20models&body=Title%3A%20Data-driven%20generative%20simulation%20of%20SDEs%20using%20diffusion%20models%0AAuthor%3A%20Xuefeng%20Gao%20and%20Jiale%20Zha%20and%20Xun%20Yu%20Zhou%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20new%20approach%20to%20generating%20sample%20paths%20of%20unknown%0Astochastic%20differential%20equations%20%28SDEs%29%20using%20diffusion%20models%2C%20a%20class%20of%0Agenerative%20AI%20models%20commonly%20employed%20in%20image%20and%20video%20applications.%20Unlike%0Athe%20traditional%20Monte%20Carlo%20methods%20for%20simulating%20SDEs%2C%20which%20require%20explicit%0Aspecifications%20of%20the%20drift%20and%20diffusion%20coefficients%2C%20our%20method%20takes%20a%0Amodel-free%2C%20data-driven%20approach.%20Given%20a%20finite%20set%20of%20sample%20paths%20from%20an%0ASDE%2C%20we%20utilize%20conditional%20diffusion%20models%20to%20generate%20new%2C%20synthetic%20paths%0Aof%20the%20same%20SDE.%20To%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20we%20conduct%20a%0Asimulation%20experiment%20to%20compare%20our%20method%20with%20alternative%20benchmark%20ones%0Aincluding%20neural%20SDEs.%20Furthermore%2C%20in%20an%20empirical%20study%20we%20leverage%20these%0Asynthetically%20generated%20sample%20paths%20to%20enhance%20the%20performance%20of%0Areinforcement%20learning%20algorithms%20for%20continuous-time%20mean-variance%20portfolio%0Aselection%2C%20hinting%20promising%20applications%20of%20diffusion%20models%20in%20financial%0Aanalysis%20and%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-driven%2520generative%2520simulation%2520of%2520SDEs%2520using%2520diffusion%2520models%26entry.906535625%3DXuefeng%2520Gao%2520and%2520Jiale%2520Zha%2520and%2520Xun%2520Yu%2520Zhou%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520new%2520approach%2520to%2520generating%2520sample%2520paths%2520of%2520unknown%250Astochastic%2520differential%2520equations%2520%2528SDEs%2529%2520using%2520diffusion%2520models%252C%2520a%2520class%2520of%250Agenerative%2520AI%2520models%2520commonly%2520employed%2520in%2520image%2520and%2520video%2520applications.%2520Unlike%250Athe%2520traditional%2520Monte%2520Carlo%2520methods%2520for%2520simulating%2520SDEs%252C%2520which%2520require%2520explicit%250Aspecifications%2520of%2520the%2520drift%2520and%2520diffusion%2520coefficients%252C%2520our%2520method%2520takes%2520a%250Amodel-free%252C%2520data-driven%2520approach.%2520Given%2520a%2520finite%2520set%2520of%2520sample%2520paths%2520from%2520an%250ASDE%252C%2520we%2520utilize%2520conditional%2520diffusion%2520models%2520to%2520generate%2520new%252C%2520synthetic%2520paths%250Aof%2520the%2520same%2520SDE.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520we%2520conduct%2520a%250Asimulation%2520experiment%2520to%2520compare%2520our%2520method%2520with%2520alternative%2520benchmark%2520ones%250Aincluding%2520neural%2520SDEs.%2520Furthermore%252C%2520in%2520an%2520empirical%2520study%2520we%2520leverage%2520these%250Asynthetically%2520generated%2520sample%2520paths%2520to%2520enhance%2520the%2520performance%2520of%250Areinforcement%2520learning%2520algorithms%2520for%2520continuous-time%2520mean-variance%2520portfolio%250Aselection%252C%2520hinting%2520promising%2520applications%2520of%2520diffusion%2520models%2520in%2520financial%250Aanalysis%2520and%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-driven%20generative%20simulation%20of%20SDEs%20using%20diffusion%20models&entry.906535625=Xuefeng%20Gao%20and%20Jiale%20Zha%20and%20Xun%20Yu%20Zhou&entry.1292438233=%20%20This%20paper%20introduces%20a%20new%20approach%20to%20generating%20sample%20paths%20of%20unknown%0Astochastic%20differential%20equations%20%28SDEs%29%20using%20diffusion%20models%2C%20a%20class%20of%0Agenerative%20AI%20models%20commonly%20employed%20in%20image%20and%20video%20applications.%20Unlike%0Athe%20traditional%20Monte%20Carlo%20methods%20for%20simulating%20SDEs%2C%20which%20require%20explicit%0Aspecifications%20of%20the%20drift%20and%20diffusion%20coefficients%2C%20our%20method%20takes%20a%0Amodel-free%2C%20data-driven%20approach.%20Given%20a%20finite%20set%20of%20sample%20paths%20from%20an%0ASDE%2C%20we%20utilize%20conditional%20diffusion%20models%20to%20generate%20new%2C%20synthetic%20paths%0Aof%20the%20same%20SDE.%20To%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20we%20conduct%20a%0Asimulation%20experiment%20to%20compare%20our%20method%20with%20alternative%20benchmark%20ones%0Aincluding%20neural%20SDEs.%20Furthermore%2C%20in%20an%20empirical%20study%20we%20leverage%20these%0Asynthetically%20generated%20sample%20paths%20to%20enhance%20the%20performance%20of%0Areinforcement%20learning%20algorithms%20for%20continuous-time%20mean-variance%20portfolio%0Aselection%2C%20hinting%20promising%20applications%20of%20diffusion%20models%20in%20financial%0Aanalysis%20and%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08731v1&entry.124074799=Read"},
{"title": "FlexFringe: Modeling Software Behavior by Learning Probabilistic\n  Automata", "author": "Sicco Verwer and Christian Hammerschmidt", "abstract": "  We present the efficient implementations of probabilistic deterministic\nfinite automaton learning methods available in FlexFringe. These implement\nwell-known strategies for state-merging including several modifications to\nimprove their performance in practice. We show experimentally that these\nalgorithms obtain competitive results and significant improvements over a\ndefault implementation. We also demonstrate how to use FlexFringe to learn\ninterpretable models from software logs and use these for anomaly detection.\nAlthough less interpretable, we show that learning smaller more convoluted\nmodels improves the performance of FlexFringe on anomaly detection,\noutperforming an existing solution based on neural nets.\n", "link": "http://arxiv.org/abs/2203.16331v5", "date": "2025-09-10", "relevancy": 1.4741, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5049}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5026}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexFringe%3A%20Modeling%20Software%20Behavior%20by%20Learning%20Probabilistic%0A%20%20Automata&body=Title%3A%20FlexFringe%3A%20Modeling%20Software%20Behavior%20by%20Learning%20Probabilistic%0A%20%20Automata%0AAuthor%3A%20Sicco%20Verwer%20and%20Christian%20Hammerschmidt%0AAbstract%3A%20%20%20We%20present%20the%20efficient%20implementations%20of%20probabilistic%20deterministic%0Afinite%20automaton%20learning%20methods%20available%20in%20FlexFringe.%20These%20implement%0Awell-known%20strategies%20for%20state-merging%20including%20several%20modifications%20to%0Aimprove%20their%20performance%20in%20practice.%20We%20show%20experimentally%20that%20these%0Aalgorithms%20obtain%20competitive%20results%20and%20significant%20improvements%20over%20a%0Adefault%20implementation.%20We%20also%20demonstrate%20how%20to%20use%20FlexFringe%20to%20learn%0Ainterpretable%20models%20from%20software%20logs%20and%20use%20these%20for%20anomaly%20detection.%0AAlthough%20less%20interpretable%2C%20we%20show%20that%20learning%20smaller%20more%20convoluted%0Amodels%20improves%20the%20performance%20of%20FlexFringe%20on%20anomaly%20detection%2C%0Aoutperforming%20an%20existing%20solution%20based%20on%20neural%20nets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.16331v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexFringe%253A%2520Modeling%2520Software%2520Behavior%2520by%2520Learning%2520Probabilistic%250A%2520%2520Automata%26entry.906535625%3DSicco%2520Verwer%2520and%2520Christian%2520Hammerschmidt%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520efficient%2520implementations%2520of%2520probabilistic%2520deterministic%250Afinite%2520automaton%2520learning%2520methods%2520available%2520in%2520FlexFringe.%2520These%2520implement%250Awell-known%2520strategies%2520for%2520state-merging%2520including%2520several%2520modifications%2520to%250Aimprove%2520their%2520performance%2520in%2520practice.%2520We%2520show%2520experimentally%2520that%2520these%250Aalgorithms%2520obtain%2520competitive%2520results%2520and%2520significant%2520improvements%2520over%2520a%250Adefault%2520implementation.%2520We%2520also%2520demonstrate%2520how%2520to%2520use%2520FlexFringe%2520to%2520learn%250Ainterpretable%2520models%2520from%2520software%2520logs%2520and%2520use%2520these%2520for%2520anomaly%2520detection.%250AAlthough%2520less%2520interpretable%252C%2520we%2520show%2520that%2520learning%2520smaller%2520more%2520convoluted%250Amodels%2520improves%2520the%2520performance%2520of%2520FlexFringe%2520on%2520anomaly%2520detection%252C%250Aoutperforming%2520an%2520existing%2520solution%2520based%2520on%2520neural%2520nets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.16331v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexFringe%3A%20Modeling%20Software%20Behavior%20by%20Learning%20Probabilistic%0A%20%20Automata&entry.906535625=Sicco%20Verwer%20and%20Christian%20Hammerschmidt&entry.1292438233=%20%20We%20present%20the%20efficient%20implementations%20of%20probabilistic%20deterministic%0Afinite%20automaton%20learning%20methods%20available%20in%20FlexFringe.%20These%20implement%0Awell-known%20strategies%20for%20state-merging%20including%20several%20modifications%20to%0Aimprove%20their%20performance%20in%20practice.%20We%20show%20experimentally%20that%20these%0Aalgorithms%20obtain%20competitive%20results%20and%20significant%20improvements%20over%20a%0Adefault%20implementation.%20We%20also%20demonstrate%20how%20to%20use%20FlexFringe%20to%20learn%0Ainterpretable%20models%20from%20software%20logs%20and%20use%20these%20for%20anomaly%20detection.%0AAlthough%20less%20interpretable%2C%20we%20show%20that%20learning%20smaller%20more%20convoluted%0Amodels%20improves%20the%20performance%20of%20FlexFringe%20on%20anomaly%20detection%2C%0Aoutperforming%20an%20existing%20solution%20based%20on%20neural%20nets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.16331v5&entry.124074799=Read"},
{"title": "Calibrating Transformers via Sparse Gaussian Processes", "author": "Wenlong Chen and Yingzhen Li", "abstract": "  Transformer models have achieved profound success in prediction tasks in a\nwide range of applications in natural language processing, speech recognition\nand computer vision. Extending Transformer's success to safety-critical domains\nrequires calibrated uncertainty estimation which remains under-explored. To\naddress this, we propose Sparse Gaussian Process attention (SGPA), which\nperforms Bayesian inference directly in the output space of multi-head\nattention blocks (MHAs) in transformer to calibrate its uncertainty. It\nreplaces the scaled dot-product operation with a valid symmetric kernel and\nuses sparse Gaussian processes (SGP) techniques to approximate the posterior\nprocesses of MHA outputs. Empirically, on a suite of prediction tasks on text,\nimages and graphs, SGPA-based Transformers achieve competitive predictive\naccuracy, while noticeably improving both in-distribution calibration and\nout-of-distribution robustness and detection.\n", "link": "http://arxiv.org/abs/2303.02444v4", "date": "2025-09-10", "relevancy": 1.7148, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6012}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5694}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrating%20Transformers%20via%20Sparse%20Gaussian%20Processes&body=Title%3A%20Calibrating%20Transformers%20via%20Sparse%20Gaussian%20Processes%0AAuthor%3A%20Wenlong%20Chen%20and%20Yingzhen%20Li%0AAbstract%3A%20%20%20Transformer%20models%20have%20achieved%20profound%20success%20in%20prediction%20tasks%20in%20a%0Awide%20range%20of%20applications%20in%20natural%20language%20processing%2C%20speech%20recognition%0Aand%20computer%20vision.%20Extending%20Transformer%27s%20success%20to%20safety-critical%20domains%0Arequires%20calibrated%20uncertainty%20estimation%20which%20remains%20under-explored.%20To%0Aaddress%20this%2C%20we%20propose%20Sparse%20Gaussian%20Process%20attention%20%28SGPA%29%2C%20which%0Aperforms%20Bayesian%20inference%20directly%20in%20the%20output%20space%20of%20multi-head%0Aattention%20blocks%20%28MHAs%29%20in%20transformer%20to%20calibrate%20its%20uncertainty.%20It%0Areplaces%20the%20scaled%20dot-product%20operation%20with%20a%20valid%20symmetric%20kernel%20and%0Auses%20sparse%20Gaussian%20processes%20%28SGP%29%20techniques%20to%20approximate%20the%20posterior%0Aprocesses%20of%20MHA%20outputs.%20Empirically%2C%20on%20a%20suite%20of%20prediction%20tasks%20on%20text%2C%0Aimages%20and%20graphs%2C%20SGPA-based%20Transformers%20achieve%20competitive%20predictive%0Aaccuracy%2C%20while%20noticeably%20improving%20both%20in-distribution%20calibration%20and%0Aout-of-distribution%20robustness%20and%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.02444v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrating%2520Transformers%2520via%2520Sparse%2520Gaussian%2520Processes%26entry.906535625%3DWenlong%2520Chen%2520and%2520Yingzhen%2520Li%26entry.1292438233%3D%2520%2520Transformer%2520models%2520have%2520achieved%2520profound%2520success%2520in%2520prediction%2520tasks%2520in%2520a%250Awide%2520range%2520of%2520applications%2520in%2520natural%2520language%2520processing%252C%2520speech%2520recognition%250Aand%2520computer%2520vision.%2520Extending%2520Transformer%2527s%2520success%2520to%2520safety-critical%2520domains%250Arequires%2520calibrated%2520uncertainty%2520estimation%2520which%2520remains%2520under-explored.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520Sparse%2520Gaussian%2520Process%2520attention%2520%2528SGPA%2529%252C%2520which%250Aperforms%2520Bayesian%2520inference%2520directly%2520in%2520the%2520output%2520space%2520of%2520multi-head%250Aattention%2520blocks%2520%2528MHAs%2529%2520in%2520transformer%2520to%2520calibrate%2520its%2520uncertainty.%2520It%250Areplaces%2520the%2520scaled%2520dot-product%2520operation%2520with%2520a%2520valid%2520symmetric%2520kernel%2520and%250Auses%2520sparse%2520Gaussian%2520processes%2520%2528SGP%2529%2520techniques%2520to%2520approximate%2520the%2520posterior%250Aprocesses%2520of%2520MHA%2520outputs.%2520Empirically%252C%2520on%2520a%2520suite%2520of%2520prediction%2520tasks%2520on%2520text%252C%250Aimages%2520and%2520graphs%252C%2520SGPA-based%2520Transformers%2520achieve%2520competitive%2520predictive%250Aaccuracy%252C%2520while%2520noticeably%2520improving%2520both%2520in-distribution%2520calibration%2520and%250Aout-of-distribution%2520robustness%2520and%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.02444v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrating%20Transformers%20via%20Sparse%20Gaussian%20Processes&entry.906535625=Wenlong%20Chen%20and%20Yingzhen%20Li&entry.1292438233=%20%20Transformer%20models%20have%20achieved%20profound%20success%20in%20prediction%20tasks%20in%20a%0Awide%20range%20of%20applications%20in%20natural%20language%20processing%2C%20speech%20recognition%0Aand%20computer%20vision.%20Extending%20Transformer%27s%20success%20to%20safety-critical%20domains%0Arequires%20calibrated%20uncertainty%20estimation%20which%20remains%20under-explored.%20To%0Aaddress%20this%2C%20we%20propose%20Sparse%20Gaussian%20Process%20attention%20%28SGPA%29%2C%20which%0Aperforms%20Bayesian%20inference%20directly%20in%20the%20output%20space%20of%20multi-head%0Aattention%20blocks%20%28MHAs%29%20in%20transformer%20to%20calibrate%20its%20uncertainty.%20It%0Areplaces%20the%20scaled%20dot-product%20operation%20with%20a%20valid%20symmetric%20kernel%20and%0Auses%20sparse%20Gaussian%20processes%20%28SGP%29%20techniques%20to%20approximate%20the%20posterior%0Aprocesses%20of%20MHA%20outputs.%20Empirically%2C%20on%20a%20suite%20of%20prediction%20tasks%20on%20text%2C%0Aimages%20and%20graphs%2C%20SGPA-based%20Transformers%20achieve%20competitive%20predictive%0Aaccuracy%2C%20while%20noticeably%20improving%20both%20in-distribution%20calibration%20and%0Aout-of-distribution%20robustness%20and%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.02444v4&entry.124074799=Read"},
{"title": "BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for\n  Aerodynamic Predictions", "author": "Nicholas Sung and Steven Spreizer and Mohamed Elrefaie and Kaira Samuel and Matthew C. Jones and Faez Ahmed", "abstract": "  BlendedNet is a publicly available aerodynamic dataset of 999 blended wing\nbody (BWB) geometries. Each geometry is simulated across about nine flight\nconditions, yielding 8830 converged RANS cases with the Spalart-Allmaras model\nand 9 to 14 million cells per case. The dataset is generated by sampling\ngeometric design parameters and flight conditions, and includes detailed\npointwise surface quantities needed to study lift and drag. We also introduce\nan end-to-end surrogate framework for pointwise aerodynamic prediction. The\npipeline first uses a permutation-invariant PointNet regressor to predict\ngeometric parameters from sampled surface point clouds, then conditions a\nFeature-wise Linear Modulation (FiLM) network on the predicted parameters and\nflight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.\nExperiments show low errors in surface predictions across diverse BWBs.\nBlendedNet addresses data scarcity for unconventional configurations and\nenables research on data-driven surrogate modeling for aerodynamic design.\n", "link": "http://arxiv.org/abs/2509.07209v2", "date": "2025-09-10", "relevancy": 1.4177, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4772}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4764}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BlendedNet%3A%20A%20Blended%20Wing%20Body%20Aircraft%20Dataset%20and%20Surrogate%20Model%20for%0A%20%20Aerodynamic%20Predictions&body=Title%3A%20BlendedNet%3A%20A%20Blended%20Wing%20Body%20Aircraft%20Dataset%20and%20Surrogate%20Model%20for%0A%20%20Aerodynamic%20Predictions%0AAuthor%3A%20Nicholas%20Sung%20and%20Steven%20Spreizer%20and%20Mohamed%20Elrefaie%20and%20Kaira%20Samuel%20and%20Matthew%20C.%20Jones%20and%20Faez%20Ahmed%0AAbstract%3A%20%20%20BlendedNet%20is%20a%20publicly%20available%20aerodynamic%20dataset%20of%20999%20blended%20wing%0Abody%20%28BWB%29%20geometries.%20Each%20geometry%20is%20simulated%20across%20about%20nine%20flight%0Aconditions%2C%20yielding%208830%20converged%20RANS%20cases%20with%20the%20Spalart-Allmaras%20model%0Aand%209%20to%2014%20million%20cells%20per%20case.%20The%20dataset%20is%20generated%20by%20sampling%0Ageometric%20design%20parameters%20and%20flight%20conditions%2C%20and%20includes%20detailed%0Apointwise%20surface%20quantities%20needed%20to%20study%20lift%20and%20drag.%20We%20also%20introduce%0Aan%20end-to-end%20surrogate%20framework%20for%20pointwise%20aerodynamic%20prediction.%20The%0Apipeline%20first%20uses%20a%20permutation-invariant%20PointNet%20regressor%20to%20predict%0Ageometric%20parameters%20from%20sampled%20surface%20point%20clouds%2C%20then%20conditions%20a%0AFeature-wise%20Linear%20Modulation%20%28FiLM%29%20network%20on%20the%20predicted%20parameters%20and%0Aflight%20conditions%20to%20predict%20pointwise%20coefficients%20Cp%2C%20Cfx%2C%20and%20Cfz.%0AExperiments%20show%20low%20errors%20in%20surface%20predictions%20across%20diverse%20BWBs.%0ABlendedNet%20addresses%20data%20scarcity%20for%20unconventional%20configurations%20and%0Aenables%20research%20on%20data-driven%20surrogate%20modeling%20for%20aerodynamic%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlendedNet%253A%2520A%2520Blended%2520Wing%2520Body%2520Aircraft%2520Dataset%2520and%2520Surrogate%2520Model%2520for%250A%2520%2520Aerodynamic%2520Predictions%26entry.906535625%3DNicholas%2520Sung%2520and%2520Steven%2520Spreizer%2520and%2520Mohamed%2520Elrefaie%2520and%2520Kaira%2520Samuel%2520and%2520Matthew%2520C.%2520Jones%2520and%2520Faez%2520Ahmed%26entry.1292438233%3D%2520%2520BlendedNet%2520is%2520a%2520publicly%2520available%2520aerodynamic%2520dataset%2520of%2520999%2520blended%2520wing%250Abody%2520%2528BWB%2529%2520geometries.%2520Each%2520geometry%2520is%2520simulated%2520across%2520about%2520nine%2520flight%250Aconditions%252C%2520yielding%25208830%2520converged%2520RANS%2520cases%2520with%2520the%2520Spalart-Allmaras%2520model%250Aand%25209%2520to%252014%2520million%2520cells%2520per%2520case.%2520The%2520dataset%2520is%2520generated%2520by%2520sampling%250Ageometric%2520design%2520parameters%2520and%2520flight%2520conditions%252C%2520and%2520includes%2520detailed%250Apointwise%2520surface%2520quantities%2520needed%2520to%2520study%2520lift%2520and%2520drag.%2520We%2520also%2520introduce%250Aan%2520end-to-end%2520surrogate%2520framework%2520for%2520pointwise%2520aerodynamic%2520prediction.%2520The%250Apipeline%2520first%2520uses%2520a%2520permutation-invariant%2520PointNet%2520regressor%2520to%2520predict%250Ageometric%2520parameters%2520from%2520sampled%2520surface%2520point%2520clouds%252C%2520then%2520conditions%2520a%250AFeature-wise%2520Linear%2520Modulation%2520%2528FiLM%2529%2520network%2520on%2520the%2520predicted%2520parameters%2520and%250Aflight%2520conditions%2520to%2520predict%2520pointwise%2520coefficients%2520Cp%252C%2520Cfx%252C%2520and%2520Cfz.%250AExperiments%2520show%2520low%2520errors%2520in%2520surface%2520predictions%2520across%2520diverse%2520BWBs.%250ABlendedNet%2520addresses%2520data%2520scarcity%2520for%2520unconventional%2520configurations%2520and%250Aenables%2520research%2520on%2520data-driven%2520surrogate%2520modeling%2520for%2520aerodynamic%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BlendedNet%3A%20A%20Blended%20Wing%20Body%20Aircraft%20Dataset%20and%20Surrogate%20Model%20for%0A%20%20Aerodynamic%20Predictions&entry.906535625=Nicholas%20Sung%20and%20Steven%20Spreizer%20and%20Mohamed%20Elrefaie%20and%20Kaira%20Samuel%20and%20Matthew%20C.%20Jones%20and%20Faez%20Ahmed&entry.1292438233=%20%20BlendedNet%20is%20a%20publicly%20available%20aerodynamic%20dataset%20of%20999%20blended%20wing%0Abody%20%28BWB%29%20geometries.%20Each%20geometry%20is%20simulated%20across%20about%20nine%20flight%0Aconditions%2C%20yielding%208830%20converged%20RANS%20cases%20with%20the%20Spalart-Allmaras%20model%0Aand%209%20to%2014%20million%20cells%20per%20case.%20The%20dataset%20is%20generated%20by%20sampling%0Ageometric%20design%20parameters%20and%20flight%20conditions%2C%20and%20includes%20detailed%0Apointwise%20surface%20quantities%20needed%20to%20study%20lift%20and%20drag.%20We%20also%20introduce%0Aan%20end-to-end%20surrogate%20framework%20for%20pointwise%20aerodynamic%20prediction.%20The%0Apipeline%20first%20uses%20a%20permutation-invariant%20PointNet%20regressor%20to%20predict%0Ageometric%20parameters%20from%20sampled%20surface%20point%20clouds%2C%20then%20conditions%20a%0AFeature-wise%20Linear%20Modulation%20%28FiLM%29%20network%20on%20the%20predicted%20parameters%20and%0Aflight%20conditions%20to%20predict%20pointwise%20coefficients%20Cp%2C%20Cfx%2C%20and%20Cfz.%0AExperiments%20show%20low%20errors%20in%20surface%20predictions%20across%20diverse%20BWBs.%0ABlendedNet%20addresses%20data%20scarcity%20for%20unconventional%20configurations%20and%0Aenables%20research%20on%20data-driven%20surrogate%20modeling%20for%20aerodynamic%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07209v2&entry.124074799=Read"},
{"title": "Guiding Soft Robots with Motor-Imagery Brain Signals and Impedance\n  Control", "author": "Maximilian St\u00f6lzle and Sonal Santosh Baberwal and Daniela Rus and Shirley Coyle and Cosimo Della Santina", "abstract": "  Integrating Brain-Machine Interfaces into non-clinical applications like\nrobot motion control remains difficult - despite remarkable advancements in\nclinical settings. Specifically, EEG-based motor imagery systems are still\nerror-prone, posing safety risks when rigid robots operate near humans. This\nwork presents an alternative pathway towards safe and effective operation by\ncombining wearable EEG with physically embodied safety in soft robots. We\nintroduce and test a pipeline that allows a user to move a soft robot's end\neffector in real time via brain waves that are measured by as few as three EEG\nchannels. A robust motor imagery algorithm interprets the user's intentions to\nmove the position of a virtual attractor to which the end effector is\nattracted, thanks to a new Cartesian impedance controller. We specifically\nfocus here on planar soft robot-based architected metamaterials, which require\nthe development of a novel control architecture to deal with the peculiar\nnonlinearities - e.g., non-affinity in control. We preliminarily but\nquantitatively evaluate the approach on the task of setpoint regulation. We\nobserve that the user reaches the proximity of the setpoint in 66% of steps and\nthat for successful steps, the average response time is 21.5s. We also\ndemonstrate the execution of simple real-world tasks involving interaction with\nthe environment, which would be extremely hard to perform if it were not for\nthe robot's softness.\n", "link": "http://arxiv.org/abs/2401.13441v2", "date": "2025-09-10", "relevancy": 1.8617, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6275}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6224}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guiding%20Soft%20Robots%20with%20Motor-Imagery%20Brain%20Signals%20and%20Impedance%0A%20%20Control&body=Title%3A%20Guiding%20Soft%20Robots%20with%20Motor-Imagery%20Brain%20Signals%20and%20Impedance%0A%20%20Control%0AAuthor%3A%20Maximilian%20St%C3%B6lzle%20and%20Sonal%20Santosh%20Baberwal%20and%20Daniela%20Rus%20and%20Shirley%20Coyle%20and%20Cosimo%20Della%20Santina%0AAbstract%3A%20%20%20Integrating%20Brain-Machine%20Interfaces%20into%20non-clinical%20applications%20like%0Arobot%20motion%20control%20remains%20difficult%20-%20despite%20remarkable%20advancements%20in%0Aclinical%20settings.%20Specifically%2C%20EEG-based%20motor%20imagery%20systems%20are%20still%0Aerror-prone%2C%20posing%20safety%20risks%20when%20rigid%20robots%20operate%20near%20humans.%20This%0Awork%20presents%20an%20alternative%20pathway%20towards%20safe%20and%20effective%20operation%20by%0Acombining%20wearable%20EEG%20with%20physically%20embodied%20safety%20in%20soft%20robots.%20We%0Aintroduce%20and%20test%20a%20pipeline%20that%20allows%20a%20user%20to%20move%20a%20soft%20robot%27s%20end%0Aeffector%20in%20real%20time%20via%20brain%20waves%20that%20are%20measured%20by%20as%20few%20as%20three%20EEG%0Achannels.%20A%20robust%20motor%20imagery%20algorithm%20interprets%20the%20user%27s%20intentions%20to%0Amove%20the%20position%20of%20a%20virtual%20attractor%20to%20which%20the%20end%20effector%20is%0Aattracted%2C%20thanks%20to%20a%20new%20Cartesian%20impedance%20controller.%20We%20specifically%0Afocus%20here%20on%20planar%20soft%20robot-based%20architected%20metamaterials%2C%20which%20require%0Athe%20development%20of%20a%20novel%20control%20architecture%20to%20deal%20with%20the%20peculiar%0Anonlinearities%20-%20e.g.%2C%20non-affinity%20in%20control.%20We%20preliminarily%20but%0Aquantitatively%20evaluate%20the%20approach%20on%20the%20task%20of%20setpoint%20regulation.%20We%0Aobserve%20that%20the%20user%20reaches%20the%20proximity%20of%20the%20setpoint%20in%2066%25%20of%20steps%20and%0Athat%20for%20successful%20steps%2C%20the%20average%20response%20time%20is%2021.5s.%20We%20also%0Ademonstrate%20the%20execution%20of%20simple%20real-world%20tasks%20involving%20interaction%20with%0Athe%20environment%2C%20which%20would%20be%20extremely%20hard%20to%20perform%20if%20it%20were%20not%20for%0Athe%20robot%27s%20softness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13441v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuiding%2520Soft%2520Robots%2520with%2520Motor-Imagery%2520Brain%2520Signals%2520and%2520Impedance%250A%2520%2520Control%26entry.906535625%3DMaximilian%2520St%25C3%25B6lzle%2520and%2520Sonal%2520Santosh%2520Baberwal%2520and%2520Daniela%2520Rus%2520and%2520Shirley%2520Coyle%2520and%2520Cosimo%2520Della%2520Santina%26entry.1292438233%3D%2520%2520Integrating%2520Brain-Machine%2520Interfaces%2520into%2520non-clinical%2520applications%2520like%250Arobot%2520motion%2520control%2520remains%2520difficult%2520-%2520despite%2520remarkable%2520advancements%2520in%250Aclinical%2520settings.%2520Specifically%252C%2520EEG-based%2520motor%2520imagery%2520systems%2520are%2520still%250Aerror-prone%252C%2520posing%2520safety%2520risks%2520when%2520rigid%2520robots%2520operate%2520near%2520humans.%2520This%250Awork%2520presents%2520an%2520alternative%2520pathway%2520towards%2520safe%2520and%2520effective%2520operation%2520by%250Acombining%2520wearable%2520EEG%2520with%2520physically%2520embodied%2520safety%2520in%2520soft%2520robots.%2520We%250Aintroduce%2520and%2520test%2520a%2520pipeline%2520that%2520allows%2520a%2520user%2520to%2520move%2520a%2520soft%2520robot%2527s%2520end%250Aeffector%2520in%2520real%2520time%2520via%2520brain%2520waves%2520that%2520are%2520measured%2520by%2520as%2520few%2520as%2520three%2520EEG%250Achannels.%2520A%2520robust%2520motor%2520imagery%2520algorithm%2520interprets%2520the%2520user%2527s%2520intentions%2520to%250Amove%2520the%2520position%2520of%2520a%2520virtual%2520attractor%2520to%2520which%2520the%2520end%2520effector%2520is%250Aattracted%252C%2520thanks%2520to%2520a%2520new%2520Cartesian%2520impedance%2520controller.%2520We%2520specifically%250Afocus%2520here%2520on%2520planar%2520soft%2520robot-based%2520architected%2520metamaterials%252C%2520which%2520require%250Athe%2520development%2520of%2520a%2520novel%2520control%2520architecture%2520to%2520deal%2520with%2520the%2520peculiar%250Anonlinearities%2520-%2520e.g.%252C%2520non-affinity%2520in%2520control.%2520We%2520preliminarily%2520but%250Aquantitatively%2520evaluate%2520the%2520approach%2520on%2520the%2520task%2520of%2520setpoint%2520regulation.%2520We%250Aobserve%2520that%2520the%2520user%2520reaches%2520the%2520proximity%2520of%2520the%2520setpoint%2520in%252066%2525%2520of%2520steps%2520and%250Athat%2520for%2520successful%2520steps%252C%2520the%2520average%2520response%2520time%2520is%252021.5s.%2520We%2520also%250Ademonstrate%2520the%2520execution%2520of%2520simple%2520real-world%2520tasks%2520involving%2520interaction%2520with%250Athe%2520environment%252C%2520which%2520would%2520be%2520extremely%2520hard%2520to%2520perform%2520if%2520it%2520were%2520not%2520for%250Athe%2520robot%2527s%2520softness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13441v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20Soft%20Robots%20with%20Motor-Imagery%20Brain%20Signals%20and%20Impedance%0A%20%20Control&entry.906535625=Maximilian%20St%C3%B6lzle%20and%20Sonal%20Santosh%20Baberwal%20and%20Daniela%20Rus%20and%20Shirley%20Coyle%20and%20Cosimo%20Della%20Santina&entry.1292438233=%20%20Integrating%20Brain-Machine%20Interfaces%20into%20non-clinical%20applications%20like%0Arobot%20motion%20control%20remains%20difficult%20-%20despite%20remarkable%20advancements%20in%0Aclinical%20settings.%20Specifically%2C%20EEG-based%20motor%20imagery%20systems%20are%20still%0Aerror-prone%2C%20posing%20safety%20risks%20when%20rigid%20robots%20operate%20near%20humans.%20This%0Awork%20presents%20an%20alternative%20pathway%20towards%20safe%20and%20effective%20operation%20by%0Acombining%20wearable%20EEG%20with%20physically%20embodied%20safety%20in%20soft%20robots.%20We%0Aintroduce%20and%20test%20a%20pipeline%20that%20allows%20a%20user%20to%20move%20a%20soft%20robot%27s%20end%0Aeffector%20in%20real%20time%20via%20brain%20waves%20that%20are%20measured%20by%20as%20few%20as%20three%20EEG%0Achannels.%20A%20robust%20motor%20imagery%20algorithm%20interprets%20the%20user%27s%20intentions%20to%0Amove%20the%20position%20of%20a%20virtual%20attractor%20to%20which%20the%20end%20effector%20is%0Aattracted%2C%20thanks%20to%20a%20new%20Cartesian%20impedance%20controller.%20We%20specifically%0Afocus%20here%20on%20planar%20soft%20robot-based%20architected%20metamaterials%2C%20which%20require%0Athe%20development%20of%20a%20novel%20control%20architecture%20to%20deal%20with%20the%20peculiar%0Anonlinearities%20-%20e.g.%2C%20non-affinity%20in%20control.%20We%20preliminarily%20but%0Aquantitatively%20evaluate%20the%20approach%20on%20the%20task%20of%20setpoint%20regulation.%20We%0Aobserve%20that%20the%20user%20reaches%20the%20proximity%20of%20the%20setpoint%20in%2066%25%20of%20steps%20and%0Athat%20for%20successful%20steps%2C%20the%20average%20response%20time%20is%2021.5s.%20We%20also%0Ademonstrate%20the%20execution%20of%20simple%20real-world%20tasks%20involving%20interaction%20with%0Athe%20environment%2C%20which%20would%20be%20extremely%20hard%20to%20perform%20if%20it%20were%20not%20for%0Athe%20robot%27s%20softness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13441v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


