<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240415.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Geometrically-driven Aggregation for Zero-shot 3D Point Cloud\n  Understanding", "author": "Guofeng Mei and Luigi Riz and Yiming Wang and Fabio Poiesi", "abstract": "  Zero-shot 3D point cloud understanding can be achieved via 2D Vision-Language\nModels (VLMs). Existing strategies directly map Vision-Language Models from 2D\npixels of rendered or captured views to 3D points, overlooking the inherent and\nexpressible point cloud geometric structure. Geometrically similar or close\nregions can be exploited for bolstering point cloud understanding as they are\nlikely to share semantic information. To this end, we introduce the first\ntraining-free aggregation technique that leverages the point cloud's 3D\ngeometric structure to improve the quality of the transferred Vision-Language\nModels. Our approach operates iteratively, performing local-to-global\naggregation based on geometric and semantic point-level reasoning. We benchmark\nour approach on three downstream tasks, including classification, part\nsegmentation, and semantic segmentation, with a variety of datasets\nrepresenting both synthetic/real-world, and indoor/outdoor scenarios. Our\napproach achieves new state-of-the-art results in all benchmarks. Our approach\noperates iteratively, performing local-to-global aggregation based on geometric\nand semantic point-level reasoning. Code and dataset are available at\nhttps://luigiriz.github.io/geoze-website/\n", "link": "http://arxiv.org/abs/2312.02244v3", "date": "2024-04-15", "relevancy": 2.8049, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5734}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5477}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Geometrically-driven%20Aggregation%20for%20Zero-shot%203D%20Point%20Cloud%0A%20%20Understanding&body=Title%3A%20Geometrically-driven%20Aggregation%20for%20Zero-shot%203D%20Point%20Cloud%0A%20%20Understanding%0AAuthor%3A%20Guofeng%20Mei%20and%20Luigi%20Riz%20and%20Yiming%20Wang%20and%20Fabio%20Poiesi%0AAbstract%3A%20%20%20Zero-shot%203D%20point%20cloud%20understanding%20can%20be%20achieved%20via%202D%20Vision-Language%0AModels%20%28VLMs%29.%20Existing%20strategies%20directly%20map%20Vision-Language%20Models%20from%202D%0Apixels%20of%20rendered%20or%20captured%20views%20to%203D%20points%2C%20overlooking%20the%20inherent%20and%0Aexpressible%20point%20cloud%20geometric%20structure.%20Geometrically%20similar%20or%20close%0Aregions%20can%20be%20exploited%20for%20bolstering%20point%20cloud%20understanding%20as%20they%20are%0Alikely%20to%20share%20semantic%20information.%20To%20this%20end%2C%20we%20introduce%20the%20first%0Atraining-free%20aggregation%20technique%20that%20leverages%20the%20point%20cloud%27s%203D%0Ageometric%20structure%20to%20improve%20the%20quality%20of%20the%20transferred%20Vision-Language%0AModels.%20Our%20approach%20operates%20iteratively%2C%20performing%20local-to-global%0Aaggregation%20based%20on%20geometric%20and%20semantic%20point-level%20reasoning.%20We%20benchmark%0Aour%20approach%20on%20three%20downstream%20tasks%2C%20including%20classification%2C%20part%0Asegmentation%2C%20and%20semantic%20segmentation%2C%20with%20a%20variety%20of%20datasets%0Arepresenting%20both%20synthetic/real-world%2C%20and%20indoor/outdoor%20scenarios.%20Our%0Aapproach%20achieves%20new%20state-of-the-art%20results%20in%20all%20benchmarks.%20Our%20approach%0Aoperates%20iteratively%2C%20performing%20local-to-global%20aggregation%20based%20on%20geometric%0Aand%20semantic%20point-level%20reasoning.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//luigiriz.github.io/geoze-website/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02244v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometrically-driven%20Aggregation%20for%20Zero-shot%203D%20Point%20Cloud%0A%20%20Understanding&entry.906535625=Guofeng%20Mei%20and%20Luigi%20Riz%20and%20Yiming%20Wang%20and%20Fabio%20Poiesi&entry.1292438233=%20%20Zero-shot%203D%20point%20cloud%20understanding%20can%20be%20achieved%20via%202D%20Vision-Language%0AModels%20%28VLMs%29.%20Existing%20strategies%20directly%20map%20Vision-Language%20Models%20from%202D%0Apixels%20of%20rendered%20or%20captured%20views%20to%203D%20points%2C%20overlooking%20the%20inherent%20and%0Aexpressible%20point%20cloud%20geometric%20structure.%20Geometrically%20similar%20or%20close%0Aregions%20can%20be%20exploited%20for%20bolstering%20point%20cloud%20understanding%20as%20they%20are%0Alikely%20to%20share%20semantic%20information.%20To%20this%20end%2C%20we%20introduce%20the%20first%0Atraining-free%20aggregation%20technique%20that%20leverages%20the%20point%20cloud%27s%203D%0Ageometric%20structure%20to%20improve%20the%20quality%20of%20the%20transferred%20Vision-Language%0AModels.%20Our%20approach%20operates%20iteratively%2C%20performing%20local-to-global%0Aaggregation%20based%20on%20geometric%20and%20semantic%20point-level%20reasoning.%20We%20benchmark%0Aour%20approach%20on%20three%20downstream%20tasks%2C%20including%20classification%2C%20part%0Asegmentation%2C%20and%20semantic%20segmentation%2C%20with%20a%20variety%20of%20datasets%0Arepresenting%20both%20synthetic/real-world%2C%20and%20indoor/outdoor%20scenarios.%20Our%0Aapproach%20achieves%20new%20state-of-the-art%20results%20in%20all%20benchmarks.%20Our%20approach%0Aoperates%20iteratively%2C%20performing%20local-to-global%20aggregation%20based%20on%20geometric%0Aand%20semantic%20point-level%20reasoning.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//luigiriz.github.io/geoze-website/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02244v3&entry.124074799=Read"},
{"title": "UAV Navigation in Tunnels with 2D tilted LiDARs", "author": "Danilo Tardioli and Lorenzo Cano and Alejandro R. Mosteo", "abstract": "  Navigation of UAVs in challenging environments like tunnels or mines, where\nit is not possible to use GNSS methods to self-localize, illumination may be\nuneven or nonexistent, and wall features are likely to be scarce, is a complex\ntask, especially if the navigation has to be done at high speed. In this paper\nwe propose a novel proof-of-concept navigation technique for UAVs based on the\nuse of LiDAR information through the joint use of geometric and\nmachine-learning algorithms. The perceived information is processed by a deep\nneural network to establish the yaw of the UAV with respect to the tunnel's\nlongitudinal axis, in order to adjust the direction of navigation.\nAdditionally, a geometric method is used to compute the safest location inside\nthe tunnel (i.e. the one that maximizes the distance to the closest obstacle).\nThis information proves to be sufficient for simple yet effective navigation in\nstraight and curved tunnels.\n", "link": "http://arxiv.org/abs/2404.09688v1", "date": "2024-04-15", "relevancy": 2.7193, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5693}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5385}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5238}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UAV%20Navigation%20in%20Tunnels%20with%202D%20tilted%20LiDARs&body=Title%3A%20UAV%20Navigation%20in%20Tunnels%20with%202D%20tilted%20LiDARs%0AAuthor%3A%20Danilo%20Tardioli%20and%20Lorenzo%20Cano%20and%20Alejandro%20R.%20Mosteo%0AAbstract%3A%20%20%20Navigation%20of%20UAVs%20in%20challenging%20environments%20like%20tunnels%20or%20mines%2C%20where%0Ait%20is%20not%20possible%20to%20use%20GNSS%20methods%20to%20self-localize%2C%20illumination%20may%20be%0Auneven%20or%20nonexistent%2C%20and%20wall%20features%20are%20likely%20to%20be%20scarce%2C%20is%20a%20complex%0Atask%2C%20especially%20if%20the%20navigation%20has%20to%20be%20done%20at%20high%20speed.%20In%20this%20paper%0Awe%20propose%20a%20novel%20proof-of-concept%20navigation%20technique%20for%20UAVs%20based%20on%20the%0Ause%20of%20LiDAR%20information%20through%20the%20joint%20use%20of%20geometric%20and%0Amachine-learning%20algorithms.%20The%20perceived%20information%20is%20processed%20by%20a%20deep%0Aneural%20network%20to%20establish%20the%20yaw%20of%20the%20UAV%20with%20respect%20to%20the%20tunnel%27s%0Alongitudinal%20axis%2C%20in%20order%20to%20adjust%20the%20direction%20of%20navigation.%0AAdditionally%2C%20a%20geometric%20method%20is%20used%20to%20compute%20the%20safest%20location%20inside%0Athe%20tunnel%20%28i.e.%20the%20one%20that%20maximizes%20the%20distance%20to%20the%20closest%20obstacle%29.%0AThis%20information%20proves%20to%20be%20sufficient%20for%20simple%20yet%20effective%20navigation%20in%0Astraight%20and%20curved%20tunnels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09688v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAV%20Navigation%20in%20Tunnels%20with%202D%20tilted%20LiDARs&entry.906535625=Danilo%20Tardioli%20and%20Lorenzo%20Cano%20and%20Alejandro%20R.%20Mosteo&entry.1292438233=%20%20Navigation%20of%20UAVs%20in%20challenging%20environments%20like%20tunnels%20or%20mines%2C%20where%0Ait%20is%20not%20possible%20to%20use%20GNSS%20methods%20to%20self-localize%2C%20illumination%20may%20be%0Auneven%20or%20nonexistent%2C%20and%20wall%20features%20are%20likely%20to%20be%20scarce%2C%20is%20a%20complex%0Atask%2C%20especially%20if%20the%20navigation%20has%20to%20be%20done%20at%20high%20speed.%20In%20this%20paper%0Awe%20propose%20a%20novel%20proof-of-concept%20navigation%20technique%20for%20UAVs%20based%20on%20the%0Ause%20of%20LiDAR%20information%20through%20the%20joint%20use%20of%20geometric%20and%0Amachine-learning%20algorithms.%20The%20perceived%20information%20is%20processed%20by%20a%20deep%0Aneural%20network%20to%20establish%20the%20yaw%20of%20the%20UAV%20with%20respect%20to%20the%20tunnel%27s%0Alongitudinal%20axis%2C%20in%20order%20to%20adjust%20the%20direction%20of%20navigation.%0AAdditionally%2C%20a%20geometric%20method%20is%20used%20to%20compute%20the%20safest%20location%20inside%0Athe%20tunnel%20%28i.e.%20the%20one%20that%20maximizes%20the%20distance%20to%20the%20closest%20obstacle%29.%0AThis%20information%20proves%20to%20be%20sufficient%20for%20simple%20yet%20effective%20navigation%20in%0Astraight%20and%20curved%20tunnels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09688v1&entry.124074799=Read"},
{"title": "The Devil is in the Few Shots: Iterative Visual Knowledge Completion for\n  Few-shot Learning", "author": "Yaohui Li and Qifeng Zhou and Haoxing Chen and Jianbing Zhang and Xinyu Dai and Hao Zhou", "abstract": "  Contrastive Language-Image Pre-training (CLIP) has shown powerful zero-shot\nlearning performance. Few-shot learning aims to further enhance the transfer\ncapability of CLIP by giving few images in each class, aka 'few shots'. Most\nexisting methods either implicitly learn from the few shots by incorporating\nlearnable prompts or adapters, or explicitly embed them in a cache model for\ninference. However, the narrow distribution of few shots often contains\nincomplete class information, leading to biased visual knowledge with high risk\nof misclassification. To tackle this problem, recent methods propose to\nsupplement visual knowledge by generative models or extra databases, which can\nbe costly and time-consuming. In this paper, we propose an Iterative Visual\nKnowledge CompLetion (KCL) method to complement visual knowledge by properly\ntaking advantages of unlabeled samples without access to any auxiliary or\nsynthetic data. Specifically, KCL first measures the similarities between\nunlabeled samples and each category. Then, the samples with top confidence to\neach category is selected and collected by a designed confidence criterion.\nFinally, the collected samples are treated as labeled ones and added to few\nshots to jointly re-estimate the remaining unlabeled ones. The above procedures\nwill be repeated for a certain number of iterations with more and more samples\nbeing collected until convergence, ensuring a progressive and robust knowledge\ncompletion process. Extensive experiments on 11 benchmark datasets demonstrate\nthe effectiveness and efficiency of KCL as a plug-and-play module under both\nfew-shot and zero-shot learning settings. Code is available at\nhttps://github.com/Mark-Sky/KCL.\n", "link": "http://arxiv.org/abs/2404.09778v1", "date": "2024-04-15", "relevancy": 2.6532, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5678}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5195}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5047}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Devil%20is%20in%20the%20Few%20Shots%3A%20Iterative%20Visual%20Knowledge%20Completion%20for%0A%20%20Few-shot%20Learning&body=Title%3A%20The%20Devil%20is%20in%20the%20Few%20Shots%3A%20Iterative%20Visual%20Knowledge%20Completion%20for%0A%20%20Few-shot%20Learning%0AAuthor%3A%20Yaohui%20Li%20and%20Qifeng%20Zhou%20and%20Haoxing%20Chen%20and%20Jianbing%20Zhang%20and%20Xinyu%20Dai%20and%20Hao%20Zhou%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20shown%20powerful%20zero-shot%0Alearning%20performance.%20Few-shot%20learning%20aims%20to%20further%20enhance%20the%20transfer%0Acapability%20of%20CLIP%20by%20giving%20few%20images%20in%20each%20class%2C%20aka%20%27few%20shots%27.%20Most%0Aexisting%20methods%20either%20implicitly%20learn%20from%20the%20few%20shots%20by%20incorporating%0Alearnable%20prompts%20or%20adapters%2C%20or%20explicitly%20embed%20them%20in%20a%20cache%20model%20for%0Ainference.%20However%2C%20the%20narrow%20distribution%20of%20few%20shots%20often%20contains%0Aincomplete%20class%20information%2C%20leading%20to%20biased%20visual%20knowledge%20with%20high%20risk%0Aof%20misclassification.%20To%20tackle%20this%20problem%2C%20recent%20methods%20propose%20to%0Asupplement%20visual%20knowledge%20by%20generative%20models%20or%20extra%20databases%2C%20which%20can%0Abe%20costly%20and%20time-consuming.%20In%20this%20paper%2C%20we%20propose%20an%20Iterative%20Visual%0AKnowledge%20CompLetion%20%28KCL%29%20method%20to%20complement%20visual%20knowledge%20by%20properly%0Ataking%20advantages%20of%20unlabeled%20samples%20without%20access%20to%20any%20auxiliary%20or%0Asynthetic%20data.%20Specifically%2C%20KCL%20first%20measures%20the%20similarities%20between%0Aunlabeled%20samples%20and%20each%20category.%20Then%2C%20the%20samples%20with%20top%20confidence%20to%0Aeach%20category%20is%20selected%20and%20collected%20by%20a%20designed%20confidence%20criterion.%0AFinally%2C%20the%20collected%20samples%20are%20treated%20as%20labeled%20ones%20and%20added%20to%20few%0Ashots%20to%20jointly%20re-estimate%20the%20remaining%20unlabeled%20ones.%20The%20above%20procedures%0Awill%20be%20repeated%20for%20a%20certain%20number%20of%20iterations%20with%20more%20and%20more%20samples%0Abeing%20collected%20until%20convergence%2C%20ensuring%20a%20progressive%20and%20robust%20knowledge%0Acompletion%20process.%20Extensive%20experiments%20on%2011%20benchmark%20datasets%20demonstrate%0Athe%20effectiveness%20and%20efficiency%20of%20KCL%20as%20a%20plug-and-play%20module%20under%20both%0Afew-shot%20and%20zero-shot%20learning%20settings.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Mark-Sky/KCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09778v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Devil%20is%20in%20the%20Few%20Shots%3A%20Iterative%20Visual%20Knowledge%20Completion%20for%0A%20%20Few-shot%20Learning&entry.906535625=Yaohui%20Li%20and%20Qifeng%20Zhou%20and%20Haoxing%20Chen%20and%20Jianbing%20Zhang%20and%20Xinyu%20Dai%20and%20Hao%20Zhou&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20shown%20powerful%20zero-shot%0Alearning%20performance.%20Few-shot%20learning%20aims%20to%20further%20enhance%20the%20transfer%0Acapability%20of%20CLIP%20by%20giving%20few%20images%20in%20each%20class%2C%20aka%20%27few%20shots%27.%20Most%0Aexisting%20methods%20either%20implicitly%20learn%20from%20the%20few%20shots%20by%20incorporating%0Alearnable%20prompts%20or%20adapters%2C%20or%20explicitly%20embed%20them%20in%20a%20cache%20model%20for%0Ainference.%20However%2C%20the%20narrow%20distribution%20of%20few%20shots%20often%20contains%0Aincomplete%20class%20information%2C%20leading%20to%20biased%20visual%20knowledge%20with%20high%20risk%0Aof%20misclassification.%20To%20tackle%20this%20problem%2C%20recent%20methods%20propose%20to%0Asupplement%20visual%20knowledge%20by%20generative%20models%20or%20extra%20databases%2C%20which%20can%0Abe%20costly%20and%20time-consuming.%20In%20this%20paper%2C%20we%20propose%20an%20Iterative%20Visual%0AKnowledge%20CompLetion%20%28KCL%29%20method%20to%20complement%20visual%20knowledge%20by%20properly%0Ataking%20advantages%20of%20unlabeled%20samples%20without%20access%20to%20any%20auxiliary%20or%0Asynthetic%20data.%20Specifically%2C%20KCL%20first%20measures%20the%20similarities%20between%0Aunlabeled%20samples%20and%20each%20category.%20Then%2C%20the%20samples%20with%20top%20confidence%20to%0Aeach%20category%20is%20selected%20and%20collected%20by%20a%20designed%20confidence%20criterion.%0AFinally%2C%20the%20collected%20samples%20are%20treated%20as%20labeled%20ones%20and%20added%20to%20few%0Ashots%20to%20jointly%20re-estimate%20the%20remaining%20unlabeled%20ones.%20The%20above%20procedures%0Awill%20be%20repeated%20for%20a%20certain%20number%20of%20iterations%20with%20more%20and%20more%20samples%0Abeing%20collected%20until%20convergence%2C%20ensuring%20a%20progressive%20and%20robust%20knowledge%0Acompletion%20process.%20Extensive%20experiments%20on%2011%20benchmark%20datasets%20demonstrate%0Athe%20effectiveness%20and%20efficiency%20of%20KCL%20as%20a%20plug-and-play%20module%20under%20both%0Afew-shot%20and%20zero-shot%20learning%20settings.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Mark-Sky/KCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09778v1&entry.124074799=Read"},
{"title": "A Survey of Neural Network Robustness Assessment in Image Recognition", "author": "Jie Wang and Jun Ai and Minyan Lu and Haoran Su and Dan Yu and Yutao Zhang and Junda Zhu and Jingyu Liu", "abstract": "  In recent years, there has been significant attention given to the robustness\nassessment of neural networks. Robustness plays a critical role in ensuring\nreliable operation of artificial intelligence (AI) systems in complex and\nuncertain environments. Deep learning's robustness problem is particularly\nsignificant, highlighted by the discovery of adversarial attacks on image\nclassification models. Researchers have dedicated efforts to evaluate\nrobustness in diverse perturbation conditions for image recognition tasks.\nRobustness assessment encompasses two main techniques: robustness verification/\ncertification for deliberate adversarial attacks and robustness testing for\nrandom data corruptions. In this survey, we present a detailed examination of\nboth adversarial robustness (AR) and corruption robustness (CR) in neural\nnetwork assessment. Analyzing current research papers and standards, we provide\nan extensive overview of robustness assessment in image recognition. Three\nessential aspects are analyzed: concepts, metrics, and assessment methods. We\ninvestigate the perturbation metrics and range representations used to measure\nthe degree of perturbations on images, as well as the robustness metrics\nspecifically for the robustness conditions of classification models. The\nstrengths and limitations of the existing methods are also discussed, and some\npotential directions for future research are provided.\n", "link": "http://arxiv.org/abs/2404.08285v2", "date": "2024-04-15", "relevancy": 2.4583, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5219}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4841}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.469}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Neural%20Network%20Robustness%20Assessment%20in%20Image%20Recognition&body=Title%3A%20A%20Survey%20of%20Neural%20Network%20Robustness%20Assessment%20in%20Image%20Recognition%0AAuthor%3A%20Jie%20Wang%20and%20Jun%20Ai%20and%20Minyan%20Lu%20and%20Haoran%20Su%20and%20Dan%20Yu%20and%20Yutao%20Zhang%20and%20Junda%20Zhu%20and%20Jingyu%20Liu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20significant%20attention%20given%20to%20the%20robustness%0Aassessment%20of%20neural%20networks.%20Robustness%20plays%20a%20critical%20role%20in%20ensuring%0Areliable%20operation%20of%20artificial%20intelligence%20%28AI%29%20systems%20in%20complex%20and%0Auncertain%20environments.%20Deep%20learning%27s%20robustness%20problem%20is%20particularly%0Asignificant%2C%20highlighted%20by%20the%20discovery%20of%20adversarial%20attacks%20on%20image%0Aclassification%20models.%20Researchers%20have%20dedicated%20efforts%20to%20evaluate%0Arobustness%20in%20diverse%20perturbation%20conditions%20for%20image%20recognition%20tasks.%0ARobustness%20assessment%20encompasses%20two%20main%20techniques%3A%20robustness%20verification/%0Acertification%20for%20deliberate%20adversarial%20attacks%20and%20robustness%20testing%20for%0Arandom%20data%20corruptions.%20In%20this%20survey%2C%20we%20present%20a%20detailed%20examination%20of%0Aboth%20adversarial%20robustness%20%28AR%29%20and%20corruption%20robustness%20%28CR%29%20in%20neural%0Anetwork%20assessment.%20Analyzing%20current%20research%20papers%20and%20standards%2C%20we%20provide%0Aan%20extensive%20overview%20of%20robustness%20assessment%20in%20image%20recognition.%20Three%0Aessential%20aspects%20are%20analyzed%3A%20concepts%2C%20metrics%2C%20and%20assessment%20methods.%20We%0Ainvestigate%20the%20perturbation%20metrics%20and%20range%20representations%20used%20to%20measure%0Athe%20degree%20of%20perturbations%20on%20images%2C%20as%20well%20as%20the%20robustness%20metrics%0Aspecifically%20for%20the%20robustness%20conditions%20of%20classification%20models.%20The%0Astrengths%20and%20limitations%20of%20the%20existing%20methods%20are%20also%20discussed%2C%20and%20some%0Apotential%20directions%20for%20future%20research%20are%20provided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08285v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Neural%20Network%20Robustness%20Assessment%20in%20Image%20Recognition&entry.906535625=Jie%20Wang%20and%20Jun%20Ai%20and%20Minyan%20Lu%20and%20Haoran%20Su%20and%20Dan%20Yu%20and%20Yutao%20Zhang%20and%20Junda%20Zhu%20and%20Jingyu%20Liu&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20significant%20attention%20given%20to%20the%20robustness%0Aassessment%20of%20neural%20networks.%20Robustness%20plays%20a%20critical%20role%20in%20ensuring%0Areliable%20operation%20of%20artificial%20intelligence%20%28AI%29%20systems%20in%20complex%20and%0Auncertain%20environments.%20Deep%20learning%27s%20robustness%20problem%20is%20particularly%0Asignificant%2C%20highlighted%20by%20the%20discovery%20of%20adversarial%20attacks%20on%20image%0Aclassification%20models.%20Researchers%20have%20dedicated%20efforts%20to%20evaluate%0Arobustness%20in%20diverse%20perturbation%20conditions%20for%20image%20recognition%20tasks.%0ARobustness%20assessment%20encompasses%20two%20main%20techniques%3A%20robustness%20verification/%0Acertification%20for%20deliberate%20adversarial%20attacks%20and%20robustness%20testing%20for%0Arandom%20data%20corruptions.%20In%20this%20survey%2C%20we%20present%20a%20detailed%20examination%20of%0Aboth%20adversarial%20robustness%20%28AR%29%20and%20corruption%20robustness%20%28CR%29%20in%20neural%0Anetwork%20assessment.%20Analyzing%20current%20research%20papers%20and%20standards%2C%20we%20provide%0Aan%20extensive%20overview%20of%20robustness%20assessment%20in%20image%20recognition.%20Three%0Aessential%20aspects%20are%20analyzed%3A%20concepts%2C%20metrics%2C%20and%20assessment%20methods.%20We%0Ainvestigate%20the%20perturbation%20metrics%20and%20range%20representations%20used%20to%20measure%0Athe%20degree%20of%20perturbations%20on%20images%2C%20as%20well%20as%20the%20robustness%20metrics%0Aspecifically%20for%20the%20robustness%20conditions%20of%20classification%20models.%20The%0Astrengths%20and%20limitations%20of%20the%20existing%20methods%20are%20also%20discussed%2C%20and%20some%0Apotential%20directions%20for%20future%20research%20are%20provided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08285v2&entry.124074799=Read"},
{"title": "Object Instance Retrieval in Assistive Robotics: Leveraging Fine-Tuned\n  SimSiam with Multi-View Images Based on 3D Semantic Map", "author": "Taichi Sakaguchi and Akira Taniguchi and Yoshinobu Hagiwara and Lotfi El Hafi and Shoichi Hasegawa and Tadahiro Taniguchi", "abstract": "  Robots that assist in daily life are required to locate specific instances of\nobjects that match the user's desired object in the environment. This task is\nknown as Instance-Specific Image Goal Navigation (InstanceImageNav), which\nrequires a model capable of distinguishing between different instances within\nthe same class. One significant challenge in robotics is that when a robot\nobserves the same object from various 3D viewpoints, its appearance may differ\ngreatly, making it difficult to recognize and locate the object accurately. In\nthis study, we introduce a method, SimView, that leverages multi-view images\nbased on a 3D semantic map of the environment and self-supervised learning by\nSimSiam to train an instance identification model on-site. The effectiveness of\nour approach is validated using a photorealistic simulator, Habitat Matterport\n3D, created by scanning real home environments. Our results demonstrate a\n1.7-fold improvement in task accuracy compared to CLIP, which is pre-trained\nmultimodal contrastive learning for object search. This improvement highlights\nthe benefits of our proposed fine-tuning method in enhancing the performance of\nassistive robots in InstanceImageNav tasks. The project website is\nhttps://emergentsystemlabstudent.github.io/MultiViewRetrieve/.\n", "link": "http://arxiv.org/abs/2404.09647v1", "date": "2024-04-15", "relevancy": 2.3941, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6353}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.602}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5804}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Object%20Instance%20Retrieval%20in%20Assistive%20Robotics%3A%20Leveraging%20Fine-Tuned%0A%20%20SimSiam%20with%20Multi-View%20Images%20Based%20on%203D%20Semantic%20Map&body=Title%3A%20Object%20Instance%20Retrieval%20in%20Assistive%20Robotics%3A%20Leveraging%20Fine-Tuned%0A%20%20SimSiam%20with%20Multi-View%20Images%20Based%20on%203D%20Semantic%20Map%0AAuthor%3A%20Taichi%20Sakaguchi%20and%20Akira%20Taniguchi%20and%20Yoshinobu%20Hagiwara%20and%20Lotfi%20El%20Hafi%20and%20Shoichi%20Hasegawa%20and%20Tadahiro%20Taniguchi%0AAbstract%3A%20%20%20Robots%20that%20assist%20in%20daily%20life%20are%20required%20to%20locate%20specific%20instances%20of%0Aobjects%20that%20match%20the%20user%27s%20desired%20object%20in%20the%20environment.%20This%20task%20is%0Aknown%20as%20Instance-Specific%20Image%20Goal%20Navigation%20%28InstanceImageNav%29%2C%20which%0Arequires%20a%20model%20capable%20of%20distinguishing%20between%20different%20instances%20within%0Athe%20same%20class.%20One%20significant%20challenge%20in%20robotics%20is%20that%20when%20a%20robot%0Aobserves%20the%20same%20object%20from%20various%203D%20viewpoints%2C%20its%20appearance%20may%20differ%0Agreatly%2C%20making%20it%20difficult%20to%20recognize%20and%20locate%20the%20object%20accurately.%20In%0Athis%20study%2C%20we%20introduce%20a%20method%2C%20SimView%2C%20that%20leverages%20multi-view%20images%0Abased%20on%20a%203D%20semantic%20map%20of%20the%20environment%20and%20self-supervised%20learning%20by%0ASimSiam%20to%20train%20an%20instance%20identification%20model%20on-site.%20The%20effectiveness%20of%0Aour%20approach%20is%20validated%20using%20a%20photorealistic%20simulator%2C%20Habitat%20Matterport%0A3D%2C%20created%20by%20scanning%20real%20home%20environments.%20Our%20results%20demonstrate%20a%0A1.7-fold%20improvement%20in%20task%20accuracy%20compared%20to%20CLIP%2C%20which%20is%20pre-trained%0Amultimodal%20contrastive%20learning%20for%20object%20search.%20This%20improvement%20highlights%0Athe%20benefits%20of%20our%20proposed%20fine-tuning%20method%20in%20enhancing%20the%20performance%20of%0Aassistive%20robots%20in%20InstanceImageNav%20tasks.%20The%20project%20website%20is%0Ahttps%3A//emergentsystemlabstudent.github.io/MultiViewRetrieve/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09647v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Instance%20Retrieval%20in%20Assistive%20Robotics%3A%20Leveraging%20Fine-Tuned%0A%20%20SimSiam%20with%20Multi-View%20Images%20Based%20on%203D%20Semantic%20Map&entry.906535625=Taichi%20Sakaguchi%20and%20Akira%20Taniguchi%20and%20Yoshinobu%20Hagiwara%20and%20Lotfi%20El%20Hafi%20and%20Shoichi%20Hasegawa%20and%20Tadahiro%20Taniguchi&entry.1292438233=%20%20Robots%20that%20assist%20in%20daily%20life%20are%20required%20to%20locate%20specific%20instances%20of%0Aobjects%20that%20match%20the%20user%27s%20desired%20object%20in%20the%20environment.%20This%20task%20is%0Aknown%20as%20Instance-Specific%20Image%20Goal%20Navigation%20%28InstanceImageNav%29%2C%20which%0Arequires%20a%20model%20capable%20of%20distinguishing%20between%20different%20instances%20within%0Athe%20same%20class.%20One%20significant%20challenge%20in%20robotics%20is%20that%20when%20a%20robot%0Aobserves%20the%20same%20object%20from%20various%203D%20viewpoints%2C%20its%20appearance%20may%20differ%0Agreatly%2C%20making%20it%20difficult%20to%20recognize%20and%20locate%20the%20object%20accurately.%20In%0Athis%20study%2C%20we%20introduce%20a%20method%2C%20SimView%2C%20that%20leverages%20multi-view%20images%0Abased%20on%20a%203D%20semantic%20map%20of%20the%20environment%20and%20self-supervised%20learning%20by%0ASimSiam%20to%20train%20an%20instance%20identification%20model%20on-site.%20The%20effectiveness%20of%0Aour%20approach%20is%20validated%20using%20a%20photorealistic%20simulator%2C%20Habitat%20Matterport%0A3D%2C%20created%20by%20scanning%20real%20home%20environments.%20Our%20results%20demonstrate%20a%0A1.7-fold%20improvement%20in%20task%20accuracy%20compared%20to%20CLIP%2C%20which%20is%20pre-trained%0Amultimodal%20contrastive%20learning%20for%20object%20search.%20This%20improvement%20highlights%0Athe%20benefits%20of%20our%20proposed%20fine-tuning%20method%20in%20enhancing%20the%20performance%20of%0Aassistive%20robots%20in%20InstanceImageNav%20tasks.%20The%20project%20website%20is%0Ahttps%3A//emergentsystemlabstudent.github.io/MultiViewRetrieve/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09647v1&entry.124074799=Read"},
{"title": "2DLIW-SLAM:2D LiDAR-Inertial-Wheel Odometry with Real-Time Loop Closure", "author": "Bin Zhang and Zexin Peng and Bi Zeng and Junjie Lu", "abstract": "  Due to budgetary constraints, indoor navigation typically employs 2D LiDAR\nrather than 3D LiDAR. However, the utilization of 2D LiDAR in Simultaneous\nLocalization And Mapping (SLAM) frequently encounters challenges related to\nmotion degeneracy, particularly in geometrically similar environments. To\naddress this problem, this paper proposes a robust, accurate, and\nmulti-sensor-fused 2D LiDAR SLAM system specifically designed for indoor mobile\nrobots. To commence, the original LiDAR data undergoes meticulous processing\nthrough point and line extraction. Leveraging the distinctive characteristics\nof indoor environments, line-line constraints are established to complement\nother sensor data effectively, thereby augmenting the overall robustness and\nprecision of the system. Concurrently, a tightly-coupled front-end is created,\nintegrating data from the 2D LiDAR, IMU, and wheel odometry, thus enabling\nreal-time state estimation. Building upon this solid foundation, a novel global\nfeature point matching-based loop closure detection algorithm is proposed. This\nalgorithm proves highly effective in mitigating front-end accumulated errors\nand ultimately constructs a globally consistent map. The experimental results\nindicate that our system fully meets real-time requirements. When compared to\nCartographer, our system not only exhibits lower trajectory errors but also\ndemonstrates stronger robustness, particularly in degeneracy problem.\n", "link": "http://arxiv.org/abs/2404.07644v3", "date": "2024-04-15", "relevancy": 2.3776, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6206}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5878}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5708}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%202DLIW-SLAM%3A2D%20LiDAR-Inertial-Wheel%20Odometry%20with%20Real-Time%20Loop%20Closure&body=Title%3A%202DLIW-SLAM%3A2D%20LiDAR-Inertial-Wheel%20Odometry%20with%20Real-Time%20Loop%20Closure%0AAuthor%3A%20Bin%20Zhang%20and%20Zexin%20Peng%20and%20Bi%20Zeng%20and%20Junjie%20Lu%0AAbstract%3A%20%20%20Due%20to%20budgetary%20constraints%2C%20indoor%20navigation%20typically%20employs%202D%20LiDAR%0Arather%20than%203D%20LiDAR.%20However%2C%20the%20utilization%20of%202D%20LiDAR%20in%20Simultaneous%0ALocalization%20And%20Mapping%20%28SLAM%29%20frequently%20encounters%20challenges%20related%20to%0Amotion%20degeneracy%2C%20particularly%20in%20geometrically%20similar%20environments.%20To%0Aaddress%20this%20problem%2C%20this%20paper%20proposes%20a%20robust%2C%20accurate%2C%20and%0Amulti-sensor-fused%202D%20LiDAR%20SLAM%20system%20specifically%20designed%20for%20indoor%20mobile%0Arobots.%20To%20commence%2C%20the%20original%20LiDAR%20data%20undergoes%20meticulous%20processing%0Athrough%20point%20and%20line%20extraction.%20Leveraging%20the%20distinctive%20characteristics%0Aof%20indoor%20environments%2C%20line-line%20constraints%20are%20established%20to%20complement%0Aother%20sensor%20data%20effectively%2C%20thereby%20augmenting%20the%20overall%20robustness%20and%0Aprecision%20of%20the%20system.%20Concurrently%2C%20a%20tightly-coupled%20front-end%20is%20created%2C%0Aintegrating%20data%20from%20the%202D%20LiDAR%2C%20IMU%2C%20and%20wheel%20odometry%2C%20thus%20enabling%0Areal-time%20state%20estimation.%20Building%20upon%20this%20solid%20foundation%2C%20a%20novel%20global%0Afeature%20point%20matching-based%20loop%20closure%20detection%20algorithm%20is%20proposed.%20This%0Aalgorithm%20proves%20highly%20effective%20in%20mitigating%20front-end%20accumulated%20errors%0Aand%20ultimately%20constructs%20a%20globally%20consistent%20map.%20The%20experimental%20results%0Aindicate%20that%20our%20system%20fully%20meets%20real-time%20requirements.%20When%20compared%20to%0ACartographer%2C%20our%20system%20not%20only%20exhibits%20lower%20trajectory%20errors%20but%20also%0Ademonstrates%20stronger%20robustness%2C%20particularly%20in%20degeneracy%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07644v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2DLIW-SLAM%3A2D%20LiDAR-Inertial-Wheel%20Odometry%20with%20Real-Time%20Loop%20Closure&entry.906535625=Bin%20Zhang%20and%20Zexin%20Peng%20and%20Bi%20Zeng%20and%20Junjie%20Lu&entry.1292438233=%20%20Due%20to%20budgetary%20constraints%2C%20indoor%20navigation%20typically%20employs%202D%20LiDAR%0Arather%20than%203D%20LiDAR.%20However%2C%20the%20utilization%20of%202D%20LiDAR%20in%20Simultaneous%0ALocalization%20And%20Mapping%20%28SLAM%29%20frequently%20encounters%20challenges%20related%20to%0Amotion%20degeneracy%2C%20particularly%20in%20geometrically%20similar%20environments.%20To%0Aaddress%20this%20problem%2C%20this%20paper%20proposes%20a%20robust%2C%20accurate%2C%20and%0Amulti-sensor-fused%202D%20LiDAR%20SLAM%20system%20specifically%20designed%20for%20indoor%20mobile%0Arobots.%20To%20commence%2C%20the%20original%20LiDAR%20data%20undergoes%20meticulous%20processing%0Athrough%20point%20and%20line%20extraction.%20Leveraging%20the%20distinctive%20characteristics%0Aof%20indoor%20environments%2C%20line-line%20constraints%20are%20established%20to%20complement%0Aother%20sensor%20data%20effectively%2C%20thereby%20augmenting%20the%20overall%20robustness%20and%0Aprecision%20of%20the%20system.%20Concurrently%2C%20a%20tightly-coupled%20front-end%20is%20created%2C%0Aintegrating%20data%20from%20the%202D%20LiDAR%2C%20IMU%2C%20and%20wheel%20odometry%2C%20thus%20enabling%0Areal-time%20state%20estimation.%20Building%20upon%20this%20solid%20foundation%2C%20a%20novel%20global%0Afeature%20point%20matching-based%20loop%20closure%20detection%20algorithm%20is%20proposed.%20This%0Aalgorithm%20proves%20highly%20effective%20in%20mitigating%20front-end%20accumulated%20errors%0Aand%20ultimately%20constructs%20a%20globally%20consistent%20map.%20The%20experimental%20results%0Aindicate%20that%20our%20system%20fully%20meets%20real-time%20requirements.%20When%20compared%20to%0ACartographer%2C%20our%20system%20not%20only%20exhibits%20lower%20trajectory%20errors%20but%20also%0Ademonstrates%20stronger%20robustness%2C%20particularly%20in%20degeneracy%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07644v3&entry.124074799=Read"},
{"title": "RoHM: Robust Human Motion Reconstruction via Diffusion", "author": "Siwei Zhang and Bharat Lal Bhatnagar and Yuanlu Xu and Alexander Winkler and Petr Kadlecek and Siyu Tang and Federica Bogo", "abstract": "  We propose RoHM, an approach for robust 3D human motion reconstruction from\nmonocular RGB(-D) videos in the presence of noise and occlusions. Most previous\napproaches either train neural networks to directly regress motion in 3D or\nlearn data-driven motion priors and combine them with optimization at test\ntime. The former do not recover globally coherent motion and fail under\nocclusions; the latter are time-consuming, prone to local minima, and require\nmanual tuning. To overcome these shortcomings, we exploit the iterative,\ndenoising nature of diffusion models. RoHM is a novel diffusion-based motion\nmodel that, conditioned on noisy and occluded input data, reconstructs\ncomplete, plausible motions in consistent global coordinates. Given the\ncomplexity of the problem -- requiring one to address different tasks\n(denoising and infilling) in different solution spaces (local and global\nmotion) -- we decompose it into two sub-tasks and learn two models, one for\nglobal trajectory and one for local motion. To capture the correlations between\nthe two, we then introduce a novel conditioning module, combining it with an\niterative inference scheme. We apply RoHM to a variety of tasks -- from motion\nreconstruction and denoising to spatial and temporal infilling. Extensive\nexperiments on three popular datasets show that our method outperforms\nstate-of-the-art approaches qualitatively and quantitatively, while being\nfaster at test time. The code is available at\nhttps://sanweiliti.github.io/ROHM/ROHM.html.\n", "link": "http://arxiv.org/abs/2401.08570v2", "date": "2024-04-15", "relevancy": 2.3467, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6006}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5927}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5751}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RoHM%3A%20Robust%20Human%20Motion%20Reconstruction%20via%20Diffusion&body=Title%3A%20RoHM%3A%20Robust%20Human%20Motion%20Reconstruction%20via%20Diffusion%0AAuthor%3A%20Siwei%20Zhang%20and%20Bharat%20Lal%20Bhatnagar%20and%20Yuanlu%20Xu%20and%20Alexander%20Winkler%20and%20Petr%20Kadlecek%20and%20Siyu%20Tang%20and%20Federica%20Bogo%0AAbstract%3A%20%20%20We%20propose%20RoHM%2C%20an%20approach%20for%20robust%203D%20human%20motion%20reconstruction%20from%0Amonocular%20RGB%28-D%29%20videos%20in%20the%20presence%20of%20noise%20and%20occlusions.%20Most%20previous%0Aapproaches%20either%20train%20neural%20networks%20to%20directly%20regress%20motion%20in%203D%20or%0Alearn%20data-driven%20motion%20priors%20and%20combine%20them%20with%20optimization%20at%20test%0Atime.%20The%20former%20do%20not%20recover%20globally%20coherent%20motion%20and%20fail%20under%0Aocclusions%3B%20the%20latter%20are%20time-consuming%2C%20prone%20to%20local%20minima%2C%20and%20require%0Amanual%20tuning.%20To%20overcome%20these%20shortcomings%2C%20we%20exploit%20the%20iterative%2C%0Adenoising%20nature%20of%20diffusion%20models.%20RoHM%20is%20a%20novel%20diffusion-based%20motion%0Amodel%20that%2C%20conditioned%20on%20noisy%20and%20occluded%20input%20data%2C%20reconstructs%0Acomplete%2C%20plausible%20motions%20in%20consistent%20global%20coordinates.%20Given%20the%0Acomplexity%20of%20the%20problem%20--%20requiring%20one%20to%20address%20different%20tasks%0A%28denoising%20and%20infilling%29%20in%20different%20solution%20spaces%20%28local%20and%20global%0Amotion%29%20--%20we%20decompose%20it%20into%20two%20sub-tasks%20and%20learn%20two%20models%2C%20one%20for%0Aglobal%20trajectory%20and%20one%20for%20local%20motion.%20To%20capture%20the%20correlations%20between%0Athe%20two%2C%20we%20then%20introduce%20a%20novel%20conditioning%20module%2C%20combining%20it%20with%20an%0Aiterative%20inference%20scheme.%20We%20apply%20RoHM%20to%20a%20variety%20of%20tasks%20--%20from%20motion%0Areconstruction%20and%20denoising%20to%20spatial%20and%20temporal%20infilling.%20Extensive%0Aexperiments%20on%20three%20popular%20datasets%20show%20that%20our%20method%20outperforms%0Astate-of-the-art%20approaches%20qualitatively%20and%20quantitatively%2C%20while%20being%0Afaster%20at%20test%20time.%20The%20code%20is%20available%20at%0Ahttps%3A//sanweiliti.github.io/ROHM/ROHM.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08570v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoHM%3A%20Robust%20Human%20Motion%20Reconstruction%20via%20Diffusion&entry.906535625=Siwei%20Zhang%20and%20Bharat%20Lal%20Bhatnagar%20and%20Yuanlu%20Xu%20and%20Alexander%20Winkler%20and%20Petr%20Kadlecek%20and%20Siyu%20Tang%20and%20Federica%20Bogo&entry.1292438233=%20%20We%20propose%20RoHM%2C%20an%20approach%20for%20robust%203D%20human%20motion%20reconstruction%20from%0Amonocular%20RGB%28-D%29%20videos%20in%20the%20presence%20of%20noise%20and%20occlusions.%20Most%20previous%0Aapproaches%20either%20train%20neural%20networks%20to%20directly%20regress%20motion%20in%203D%20or%0Alearn%20data-driven%20motion%20priors%20and%20combine%20them%20with%20optimization%20at%20test%0Atime.%20The%20former%20do%20not%20recover%20globally%20coherent%20motion%20and%20fail%20under%0Aocclusions%3B%20the%20latter%20are%20time-consuming%2C%20prone%20to%20local%20minima%2C%20and%20require%0Amanual%20tuning.%20To%20overcome%20these%20shortcomings%2C%20we%20exploit%20the%20iterative%2C%0Adenoising%20nature%20of%20diffusion%20models.%20RoHM%20is%20a%20novel%20diffusion-based%20motion%0Amodel%20that%2C%20conditioned%20on%20noisy%20and%20occluded%20input%20data%2C%20reconstructs%0Acomplete%2C%20plausible%20motions%20in%20consistent%20global%20coordinates.%20Given%20the%0Acomplexity%20of%20the%20problem%20--%20requiring%20one%20to%20address%20different%20tasks%0A%28denoising%20and%20infilling%29%20in%20different%20solution%20spaces%20%28local%20and%20global%0Amotion%29%20--%20we%20decompose%20it%20into%20two%20sub-tasks%20and%20learn%20two%20models%2C%20one%20for%0Aglobal%20trajectory%20and%20one%20for%20local%20motion.%20To%20capture%20the%20correlations%20between%0Athe%20two%2C%20we%20then%20introduce%20a%20novel%20conditioning%20module%2C%20combining%20it%20with%20an%0Aiterative%20inference%20scheme.%20We%20apply%20RoHM%20to%20a%20variety%20of%20tasks%20--%20from%20motion%0Areconstruction%20and%20denoising%20to%20spatial%20and%20temporal%20infilling.%20Extensive%0Aexperiments%20on%20three%20popular%20datasets%20show%20that%20our%20method%20outperforms%0Astate-of-the-art%20approaches%20qualitatively%20and%20quantitatively%2C%20while%20being%0Afaster%20at%20test%20time.%20The%20code%20is%20available%20at%0Ahttps%3A//sanweiliti.github.io/ROHM/ROHM.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08570v2&entry.124074799=Read"},
{"title": "Bridging Vision and Language Spaces with Assignment Prediction", "author": "Jungin Park and Jiyoung Lee and Kwanghoon Sohn", "abstract": "  This paper introduces VLAP, a novel approach that bridges pretrained vision\nmodels and large language models (LLMs) to make frozen LLMs understand the\nvisual world. VLAP transforms the embedding space of pretrained vision models\ninto the LLMs' word embedding space using a single linear layer for efficient\nand general-purpose visual and language understanding. Specifically, we harness\nwell-established word embeddings to bridge two modality embedding spaces. The\nvisual and text representations are simultaneously assigned to a set of word\nembeddings within pretrained LLMs by formulating the assigning procedure as an\noptimal transport problem. We predict the assignment of one modality from the\nrepresentation of another modality data, enforcing consistent assignments for\npaired multimodal data. This allows vision and language representations to\ncontain the same information, grounding the frozen LLMs' word embedding space\nin visual data. Moreover, a robust semantic taxonomy of LLMs can be preserved\nwith visual data since the LLMs interpret and reason linguistic information\nfrom correlations between word embeddings. Experimental results show that VLAP\nachieves substantial improvements over the previous linear transformation-based\napproaches across a range of vision-language tasks, including image captioning,\nvisual question answering, and cross-modal retrieval. We also demonstrate the\nlearned visual representations hold a semantic taxonomy of LLMs, making visual\nsemantic arithmetic possible.\n", "link": "http://arxiv.org/abs/2404.09632v1", "date": "2024-04-15", "relevancy": 2.3253, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6201}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.549}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bridging%20Vision%20and%20Language%20Spaces%20with%20Assignment%20Prediction&body=Title%3A%20Bridging%20Vision%20and%20Language%20Spaces%20with%20Assignment%20Prediction%0AAuthor%3A%20Jungin%20Park%20and%20Jiyoung%20Lee%20and%20Kwanghoon%20Sohn%0AAbstract%3A%20%20%20This%20paper%20introduces%20VLAP%2C%20a%20novel%20approach%20that%20bridges%20pretrained%20vision%0Amodels%20and%20large%20language%20models%20%28LLMs%29%20to%20make%20frozen%20LLMs%20understand%20the%0Avisual%20world.%20VLAP%20transforms%20the%20embedding%20space%20of%20pretrained%20vision%20models%0Ainto%20the%20LLMs%27%20word%20embedding%20space%20using%20a%20single%20linear%20layer%20for%20efficient%0Aand%20general-purpose%20visual%20and%20language%20understanding.%20Specifically%2C%20we%20harness%0Awell-established%20word%20embeddings%20to%20bridge%20two%20modality%20embedding%20spaces.%20The%0Avisual%20and%20text%20representations%20are%20simultaneously%20assigned%20to%20a%20set%20of%20word%0Aembeddings%20within%20pretrained%20LLMs%20by%20formulating%20the%20assigning%20procedure%20as%20an%0Aoptimal%20transport%20problem.%20We%20predict%20the%20assignment%20of%20one%20modality%20from%20the%0Arepresentation%20of%20another%20modality%20data%2C%20enforcing%20consistent%20assignments%20for%0Apaired%20multimodal%20data.%20This%20allows%20vision%20and%20language%20representations%20to%0Acontain%20the%20same%20information%2C%20grounding%20the%20frozen%20LLMs%27%20word%20embedding%20space%0Ain%20visual%20data.%20Moreover%2C%20a%20robust%20semantic%20taxonomy%20of%20LLMs%20can%20be%20preserved%0Awith%20visual%20data%20since%20the%20LLMs%20interpret%20and%20reason%20linguistic%20information%0Afrom%20correlations%20between%20word%20embeddings.%20Experimental%20results%20show%20that%20VLAP%0Aachieves%20substantial%20improvements%20over%20the%20previous%20linear%20transformation-based%0Aapproaches%20across%20a%20range%20of%20vision-language%20tasks%2C%20including%20image%20captioning%2C%0Avisual%20question%20answering%2C%20and%20cross-modal%20retrieval.%20We%20also%20demonstrate%20the%0Alearned%20visual%20representations%20hold%20a%20semantic%20taxonomy%20of%20LLMs%2C%20making%20visual%0Asemantic%20arithmetic%20possible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09632v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Vision%20and%20Language%20Spaces%20with%20Assignment%20Prediction&entry.906535625=Jungin%20Park%20and%20Jiyoung%20Lee%20and%20Kwanghoon%20Sohn&entry.1292438233=%20%20This%20paper%20introduces%20VLAP%2C%20a%20novel%20approach%20that%20bridges%20pretrained%20vision%0Amodels%20and%20large%20language%20models%20%28LLMs%29%20to%20make%20frozen%20LLMs%20understand%20the%0Avisual%20world.%20VLAP%20transforms%20the%20embedding%20space%20of%20pretrained%20vision%20models%0Ainto%20the%20LLMs%27%20word%20embedding%20space%20using%20a%20single%20linear%20layer%20for%20efficient%0Aand%20general-purpose%20visual%20and%20language%20understanding.%20Specifically%2C%20we%20harness%0Awell-established%20word%20embeddings%20to%20bridge%20two%20modality%20embedding%20spaces.%20The%0Avisual%20and%20text%20representations%20are%20simultaneously%20assigned%20to%20a%20set%20of%20word%0Aembeddings%20within%20pretrained%20LLMs%20by%20formulating%20the%20assigning%20procedure%20as%20an%0Aoptimal%20transport%20problem.%20We%20predict%20the%20assignment%20of%20one%20modality%20from%20the%0Arepresentation%20of%20another%20modality%20data%2C%20enforcing%20consistent%20assignments%20for%0Apaired%20multimodal%20data.%20This%20allows%20vision%20and%20language%20representations%20to%0Acontain%20the%20same%20information%2C%20grounding%20the%20frozen%20LLMs%27%20word%20embedding%20space%0Ain%20visual%20data.%20Moreover%2C%20a%20robust%20semantic%20taxonomy%20of%20LLMs%20can%20be%20preserved%0Awith%20visual%20data%20since%20the%20LLMs%20interpret%20and%20reason%20linguistic%20information%0Afrom%20correlations%20between%20word%20embeddings.%20Experimental%20results%20show%20that%20VLAP%0Aachieves%20substantial%20improvements%20over%20the%20previous%20linear%20transformation-based%0Aapproaches%20across%20a%20range%20of%20vision-language%20tasks%2C%20including%20image%20captioning%2C%0Avisual%20question%20answering%2C%20and%20cross-modal%20retrieval.%20We%20also%20demonstrate%20the%0Alearned%20visual%20representations%20hold%20a%20semantic%20taxonomy%20of%20LLMs%2C%20making%20visual%0Asemantic%20arithmetic%20possible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09632v1&entry.124074799=Read"},
{"title": "Adaptive Patching for High-resolution Image Segmentation with\n  Transformers", "author": "Enzhi Zhang and Isaac Lyngaas and Peng Chen and Xiao Wang and Jun Igarashi and Yuankai Huo and Mohamed Wahib and Masaharu Munetomo", "abstract": "  Attention-based models are proliferating in the space of image analytics,\nincluding segmentation. The standard method of feeding images to transformer\nencoders is to divide the images into patches and then feed the patches to the\nmodel as a linear sequence of tokens. For high-resolution images, e.g.\nmicroscopic pathology images, the quadratic compute and memory cost prohibits\nthe use of an attention-based model, if we are to use smaller patch sizes that\nare favorable in segmentation. The solution is to either use custom complex\nmulti-resolution models or approximate attention schemes. We take inspiration\nfrom Adapative Mesh Refinement (AMR) methods in HPC by adaptively patching the\nimages, as a pre-processing step, based on the image details to reduce the\nnumber of patches being fed to the model, by orders of magnitude. This method\nhas a negligible overhead, and works seamlessly with any attention-based model,\ni.e. it is a pre-processing step that can be adopted by any attention-based\nmodel without friction. We demonstrate superior segmentation quality over SoTA\nsegmentation models for real-world pathology datasets while gaining a geomean\nspeedup of $6.9\\times$ for resolutions up to $64K^2$, on up to $2,048$ GPUs.\n", "link": "http://arxiv.org/abs/2404.09707v1", "date": "2024-04-15", "relevancy": 2.3183, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5902}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.583}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5676}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Patching%20for%20High-resolution%20Image%20Segmentation%20with%0A%20%20Transformers&body=Title%3A%20Adaptive%20Patching%20for%20High-resolution%20Image%20Segmentation%20with%0A%20%20Transformers%0AAuthor%3A%20Enzhi%20Zhang%20and%20Isaac%20Lyngaas%20and%20Peng%20Chen%20and%20Xiao%20Wang%20and%20Jun%20Igarashi%20and%20Yuankai%20Huo%20and%20Mohamed%20Wahib%20and%20Masaharu%20Munetomo%0AAbstract%3A%20%20%20Attention-based%20models%20are%20proliferating%20in%20the%20space%20of%20image%20analytics%2C%0Aincluding%20segmentation.%20The%20standard%20method%20of%20feeding%20images%20to%20transformer%0Aencoders%20is%20to%20divide%20the%20images%20into%20patches%20and%20then%20feed%20the%20patches%20to%20the%0Amodel%20as%20a%20linear%20sequence%20of%20tokens.%20For%20high-resolution%20images%2C%20e.g.%0Amicroscopic%20pathology%20images%2C%20the%20quadratic%20compute%20and%20memory%20cost%20prohibits%0Athe%20use%20of%20an%20attention-based%20model%2C%20if%20we%20are%20to%20use%20smaller%20patch%20sizes%20that%0Aare%20favorable%20in%20segmentation.%20The%20solution%20is%20to%20either%20use%20custom%20complex%0Amulti-resolution%20models%20or%20approximate%20attention%20schemes.%20We%20take%20inspiration%0Afrom%20Adapative%20Mesh%20Refinement%20%28AMR%29%20methods%20in%20HPC%20by%20adaptively%20patching%20the%0Aimages%2C%20as%20a%20pre-processing%20step%2C%20based%20on%20the%20image%20details%20to%20reduce%20the%0Anumber%20of%20patches%20being%20fed%20to%20the%20model%2C%20by%20orders%20of%20magnitude.%20This%20method%0Ahas%20a%20negligible%20overhead%2C%20and%20works%20seamlessly%20with%20any%20attention-based%20model%2C%0Ai.e.%20it%20is%20a%20pre-processing%20step%20that%20can%20be%20adopted%20by%20any%20attention-based%0Amodel%20without%20friction.%20We%20demonstrate%20superior%20segmentation%20quality%20over%20SoTA%0Asegmentation%20models%20for%20real-world%20pathology%20datasets%20while%20gaining%20a%20geomean%0Aspeedup%20of%20%246.9%5Ctimes%24%20for%20resolutions%20up%20to%20%2464K%5E2%24%2C%20on%20up%20to%20%242%2C048%24%20GPUs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09707v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Patching%20for%20High-resolution%20Image%20Segmentation%20with%0A%20%20Transformers&entry.906535625=Enzhi%20Zhang%20and%20Isaac%20Lyngaas%20and%20Peng%20Chen%20and%20Xiao%20Wang%20and%20Jun%20Igarashi%20and%20Yuankai%20Huo%20and%20Mohamed%20Wahib%20and%20Masaharu%20Munetomo&entry.1292438233=%20%20Attention-based%20models%20are%20proliferating%20in%20the%20space%20of%20image%20analytics%2C%0Aincluding%20segmentation.%20The%20standard%20method%20of%20feeding%20images%20to%20transformer%0Aencoders%20is%20to%20divide%20the%20images%20into%20patches%20and%20then%20feed%20the%20patches%20to%20the%0Amodel%20as%20a%20linear%20sequence%20of%20tokens.%20For%20high-resolution%20images%2C%20e.g.%0Amicroscopic%20pathology%20images%2C%20the%20quadratic%20compute%20and%20memory%20cost%20prohibits%0Athe%20use%20of%20an%20attention-based%20model%2C%20if%20we%20are%20to%20use%20smaller%20patch%20sizes%20that%0Aare%20favorable%20in%20segmentation.%20The%20solution%20is%20to%20either%20use%20custom%20complex%0Amulti-resolution%20models%20or%20approximate%20attention%20schemes.%20We%20take%20inspiration%0Afrom%20Adapative%20Mesh%20Refinement%20%28AMR%29%20methods%20in%20HPC%20by%20adaptively%20patching%20the%0Aimages%2C%20as%20a%20pre-processing%20step%2C%20based%20on%20the%20image%20details%20to%20reduce%20the%0Anumber%20of%20patches%20being%20fed%20to%20the%20model%2C%20by%20orders%20of%20magnitude.%20This%20method%0Ahas%20a%20negligible%20overhead%2C%20and%20works%20seamlessly%20with%20any%20attention-based%20model%2C%0Ai.e.%20it%20is%20a%20pre-processing%20step%20that%20can%20be%20adopted%20by%20any%20attention-based%0Amodel%20without%20friction.%20We%20demonstrate%20superior%20segmentation%20quality%20over%20SoTA%0Asegmentation%20models%20for%20real-world%20pathology%20datasets%20while%20gaining%20a%20geomean%0Aspeedup%20of%20%246.9%5Ctimes%24%20for%20resolutions%20up%20to%20%2464K%5E2%24%2C%20on%20up%20to%20%242%2C048%24%20GPUs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09707v1&entry.124074799=Read"},
{"title": "Dancing with Still Images: Video Distillation via Static-Dynamic\n  Disentanglement", "author": "Ziyu Wang and Yue Xu and Cewu Lu and Yong-Lu Li", "abstract": "  Recently, dataset distillation has paved the way towards efficient machine\nlearning, especially for image datasets. However, the distillation for videos,\ncharacterized by an exclusive temporal dimension, remains an underexplored\ndomain. In this work, we provide the first systematic study of video\ndistillation and introduce a taxonomy to categorize temporal compression. Our\ninvestigation reveals that the temporal information is usually not well learned\nduring distillation, and the temporal dimension of synthetic data contributes\nlittle. The observations motivate our unified framework of disentangling the\ndynamic and static information in the videos. It first distills the videos into\nstill images as static memory and then compensates the dynamic and motion\ninformation with a learnable dynamic memory block. Our method achieves\nstate-of-the-art on video datasets at different scales, with a notably smaller\nmemory storage budget. Our code is available at\nhttps://github.com/yuz1wan/video_distillation.\n", "link": "http://arxiv.org/abs/2312.00362v2", "date": "2024-04-15", "relevancy": 2.3157, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5854}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5851}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5702}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dancing%20with%20Still%20Images%3A%20Video%20Distillation%20via%20Static-Dynamic%0A%20%20Disentanglement&body=Title%3A%20Dancing%20with%20Still%20Images%3A%20Video%20Distillation%20via%20Static-Dynamic%0A%20%20Disentanglement%0AAuthor%3A%20Ziyu%20Wang%20and%20Yue%20Xu%20and%20Cewu%20Lu%20and%20Yong-Lu%20Li%0AAbstract%3A%20%20%20Recently%2C%20dataset%20distillation%20has%20paved%20the%20way%20towards%20efficient%20machine%0Alearning%2C%20especially%20for%20image%20datasets.%20However%2C%20the%20distillation%20for%20videos%2C%0Acharacterized%20by%20an%20exclusive%20temporal%20dimension%2C%20remains%20an%20underexplored%0Adomain.%20In%20this%20work%2C%20we%20provide%20the%20first%20systematic%20study%20of%20video%0Adistillation%20and%20introduce%20a%20taxonomy%20to%20categorize%20temporal%20compression.%20Our%0Ainvestigation%20reveals%20that%20the%20temporal%20information%20is%20usually%20not%20well%20learned%0Aduring%20distillation%2C%20and%20the%20temporal%20dimension%20of%20synthetic%20data%20contributes%0Alittle.%20The%20observations%20motivate%20our%20unified%20framework%20of%20disentangling%20the%0Adynamic%20and%20static%20information%20in%20the%20videos.%20It%20first%20distills%20the%20videos%20into%0Astill%20images%20as%20static%20memory%20and%20then%20compensates%20the%20dynamic%20and%20motion%0Ainformation%20with%20a%20learnable%20dynamic%20memory%20block.%20Our%20method%20achieves%0Astate-of-the-art%20on%20video%20datasets%20at%20different%20scales%2C%20with%20a%20notably%20smaller%0Amemory%20storage%20budget.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/yuz1wan/video_distillation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00362v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dancing%20with%20Still%20Images%3A%20Video%20Distillation%20via%20Static-Dynamic%0A%20%20Disentanglement&entry.906535625=Ziyu%20Wang%20and%20Yue%20Xu%20and%20Cewu%20Lu%20and%20Yong-Lu%20Li&entry.1292438233=%20%20Recently%2C%20dataset%20distillation%20has%20paved%20the%20way%20towards%20efficient%20machine%0Alearning%2C%20especially%20for%20image%20datasets.%20However%2C%20the%20distillation%20for%20videos%2C%0Acharacterized%20by%20an%20exclusive%20temporal%20dimension%2C%20remains%20an%20underexplored%0Adomain.%20In%20this%20work%2C%20we%20provide%20the%20first%20systematic%20study%20of%20video%0Adistillation%20and%20introduce%20a%20taxonomy%20to%20categorize%20temporal%20compression.%20Our%0Ainvestigation%20reveals%20that%20the%20temporal%20information%20is%20usually%20not%20well%20learned%0Aduring%20distillation%2C%20and%20the%20temporal%20dimension%20of%20synthetic%20data%20contributes%0Alittle.%20The%20observations%20motivate%20our%20unified%20framework%20of%20disentangling%20the%0Adynamic%20and%20static%20information%20in%20the%20videos.%20It%20first%20distills%20the%20videos%20into%0Astill%20images%20as%20static%20memory%20and%20then%20compensates%20the%20dynamic%20and%20motion%0Ainformation%20with%20a%20learnable%20dynamic%20memory%20block.%20Our%20method%20achieves%0Astate-of-the-art%20on%20video%20datasets%20at%20different%20scales%2C%20with%20a%20notably%20smaller%0Amemory%20storage%20budget.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/yuz1wan/video_distillation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00362v2&entry.124074799=Read"},
{"title": "LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\n  Compression for Large Language Models", "author": "Guangyan Li and Yongqiang Tang and Wensheng Zhang", "abstract": "  Large language models (LLMs) show excellent performance in difficult tasks,\nbut they often require massive memories and computational resources. How to\nreduce the parameter scale of LLMs has become research hotspots. In this study,\nwe make an important observation that the multi-head self-attention (MHA)\nsub-layer of Transformer exhibits noticeable low-rank structure, while the\nfeed-forward network (FFN) sub-layer does not. With this regard, we design a\nmixed compression model, which organically combines Low-Rank matrix\napproximation And structured Pruning (LoRAP). For the MHA sub-layer, we propose\nan input activation weighted singular value decomposition method to strengthen\nthe low-rank characteristic. Furthermore, we discover that the weight matrices\nin MHA sub-layer have different low-rank degrees. Thus, a novel parameter\nallocation scheme according to the discrepancy of low-rank degrees is devised.\nFor the FFN sub-layer, we propose a gradient-free structured channel pruning\nmethod. During the pruning, we get an interesting finding that the least\nimportant 1% of parameter actually play a vital role in model performance.\nExtensive evaluations on zero-shot perplexity and zero-shot task classification\nindicate that our proposal is superior to previous structured compression\nrivals under multiple compression ratios.\n", "link": "http://arxiv.org/abs/2404.09695v1", "date": "2024-04-15", "relevancy": 2.2611, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5779}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5655}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5525}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LoRAP%3A%20Transformer%20Sub-Layers%20Deserve%20Differentiated%20Structured%0A%20%20Compression%20for%20Large%20Language%20Models&body=Title%3A%20LoRAP%3A%20Transformer%20Sub-Layers%20Deserve%20Differentiated%20Structured%0A%20%20Compression%20for%20Large%20Language%20Models%0AAuthor%3A%20Guangyan%20Li%20and%20Yongqiang%20Tang%20and%20Wensheng%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20show%20excellent%20performance%20in%20difficult%20tasks%2C%0Abut%20they%20often%20require%20massive%20memories%20and%20computational%20resources.%20How%20to%0Areduce%20the%20parameter%20scale%20of%20LLMs%20has%20become%20research%20hotspots.%20In%20this%20study%2C%0Awe%20make%20an%20important%20observation%20that%20the%20multi-head%20self-attention%20%28MHA%29%0Asub-layer%20of%20Transformer%20exhibits%20noticeable%20low-rank%20structure%2C%20while%20the%0Afeed-forward%20network%20%28FFN%29%20sub-layer%20does%20not.%20With%20this%20regard%2C%20we%20design%20a%0Amixed%20compression%20model%2C%20which%20organically%20combines%20Low-Rank%20matrix%0Aapproximation%20And%20structured%20Pruning%20%28LoRAP%29.%20For%20the%20MHA%20sub-layer%2C%20we%20propose%0Aan%20input%20activation%20weighted%20singular%20value%20decomposition%20method%20to%20strengthen%0Athe%20low-rank%20characteristic.%20Furthermore%2C%20we%20discover%20that%20the%20weight%20matrices%0Ain%20MHA%20sub-layer%20have%20different%20low-rank%20degrees.%20Thus%2C%20a%20novel%20parameter%0Aallocation%20scheme%20according%20to%20the%20discrepancy%20of%20low-rank%20degrees%20is%20devised.%0AFor%20the%20FFN%20sub-layer%2C%20we%20propose%20a%20gradient-free%20structured%20channel%20pruning%0Amethod.%20During%20the%20pruning%2C%20we%20get%20an%20interesting%20finding%20that%20the%20least%0Aimportant%201%25%20of%20parameter%20actually%20play%20a%20vital%20role%20in%20model%20performance.%0AExtensive%20evaluations%20on%20zero-shot%20perplexity%20and%20zero-shot%20task%20classification%0Aindicate%20that%20our%20proposal%20is%20superior%20to%20previous%20structured%20compression%0Arivals%20under%20multiple%20compression%20ratios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09695v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRAP%3A%20Transformer%20Sub-Layers%20Deserve%20Differentiated%20Structured%0A%20%20Compression%20for%20Large%20Language%20Models&entry.906535625=Guangyan%20Li%20and%20Yongqiang%20Tang%20and%20Wensheng%20Zhang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20show%20excellent%20performance%20in%20difficult%20tasks%2C%0Abut%20they%20often%20require%20massive%20memories%20and%20computational%20resources.%20How%20to%0Areduce%20the%20parameter%20scale%20of%20LLMs%20has%20become%20research%20hotspots.%20In%20this%20study%2C%0Awe%20make%20an%20important%20observation%20that%20the%20multi-head%20self-attention%20%28MHA%29%0Asub-layer%20of%20Transformer%20exhibits%20noticeable%20low-rank%20structure%2C%20while%20the%0Afeed-forward%20network%20%28FFN%29%20sub-layer%20does%20not.%20With%20this%20regard%2C%20we%20design%20a%0Amixed%20compression%20model%2C%20which%20organically%20combines%20Low-Rank%20matrix%0Aapproximation%20And%20structured%20Pruning%20%28LoRAP%29.%20For%20the%20MHA%20sub-layer%2C%20we%20propose%0Aan%20input%20activation%20weighted%20singular%20value%20decomposition%20method%20to%20strengthen%0Athe%20low-rank%20characteristic.%20Furthermore%2C%20we%20discover%20that%20the%20weight%20matrices%0Ain%20MHA%20sub-layer%20have%20different%20low-rank%20degrees.%20Thus%2C%20a%20novel%20parameter%0Aallocation%20scheme%20according%20to%20the%20discrepancy%20of%20low-rank%20degrees%20is%20devised.%0AFor%20the%20FFN%20sub-layer%2C%20we%20propose%20a%20gradient-free%20structured%20channel%20pruning%0Amethod.%20During%20the%20pruning%2C%20we%20get%20an%20interesting%20finding%20that%20the%20least%0Aimportant%201%25%20of%20parameter%20actually%20play%20a%20vital%20role%20in%20model%20performance.%0AExtensive%20evaluations%20on%20zero-shot%20perplexity%20and%20zero-shot%20task%20classification%0Aindicate%20that%20our%20proposal%20is%20superior%20to%20previous%20structured%20compression%0Arivals%20under%20multiple%20compression%20ratios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09695v1&entry.124074799=Read"},
{"title": "CREST: Cross-modal Resonance through Evidential Deep Learning for\n  Enhanced Zero-Shot Learning", "author": "Haojian Huang and Xiaozhen Qiao and Zhuo Chen and Haodong Chen and Bingyu Li and Zhe Sun and Mulin Chen and Xuelong Li", "abstract": "  Zero-shot learning (ZSL) enables the recognition of novel classes by\nleveraging semantic knowledge transfer from known to unknown categories. This\nknowledge, typically encapsulated in attribute descriptions, aids in\nidentifying class-specific visual features, thus facilitating visual-semantic\nalignment and improving ZSL performance. However, real-world challenges such as\ndistribution imbalances and attribute co-occurrence among instances often\nhinder the discernment of local variances in images, a problem exacerbated by\nthe scarcity of fine-grained, region-specific attribute annotations. Moreover,\nthe variability in visual presentation within categories can also skew\nattribute-category associations. In response, we propose a bidirectional\ncross-modal ZSL approach CREST. It begins by extracting representations for\nattribute and visual localization and employs Evidential Deep Learning (EDL) to\nmeasure underlying epistemic uncertainty, thereby enhancing the model's\nresilience against hard negatives. CREST incorporates dual learning pathways,\nfocusing on both visual-category and attribute-category alignments, to ensure\nrobust correlation between latent and observable spaces. Moreover, we introduce\nan uncertainty-informed cross-modal fusion technique to refine visual-attribute\ninference. Extensive experiments demonstrate our model's effectiveness and\nunique explainability across multiple datasets. Our code and data are available\nat: Comments: Ongoing work; 10 pages, 2 Tables, 9 Figures; Repo is available at\nhttps://github.com/JethroJames/CREST.\n", "link": "http://arxiv.org/abs/2404.09640v1", "date": "2024-04-15", "relevancy": 2.2227, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5868}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5534}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5455}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CREST%3A%20Cross-modal%20Resonance%20through%20Evidential%20Deep%20Learning%20for%0A%20%20Enhanced%20Zero-Shot%20Learning&body=Title%3A%20CREST%3A%20Cross-modal%20Resonance%20through%20Evidential%20Deep%20Learning%20for%0A%20%20Enhanced%20Zero-Shot%20Learning%0AAuthor%3A%20Haojian%20Huang%20and%20Xiaozhen%20Qiao%20and%20Zhuo%20Chen%20and%20Haodong%20Chen%20and%20Bingyu%20Li%20and%20Zhe%20Sun%20and%20Mulin%20Chen%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Zero-shot%20learning%20%28ZSL%29%20enables%20the%20recognition%20of%20novel%20classes%20by%0Aleveraging%20semantic%20knowledge%20transfer%20from%20known%20to%20unknown%20categories.%20This%0Aknowledge%2C%20typically%20encapsulated%20in%20attribute%20descriptions%2C%20aids%20in%0Aidentifying%20class-specific%20visual%20features%2C%20thus%20facilitating%20visual-semantic%0Aalignment%20and%20improving%20ZSL%20performance.%20However%2C%20real-world%20challenges%20such%20as%0Adistribution%20imbalances%20and%20attribute%20co-occurrence%20among%20instances%20often%0Ahinder%20the%20discernment%20of%20local%20variances%20in%20images%2C%20a%20problem%20exacerbated%20by%0Athe%20scarcity%20of%20fine-grained%2C%20region-specific%20attribute%20annotations.%20Moreover%2C%0Athe%20variability%20in%20visual%20presentation%20within%20categories%20can%20also%20skew%0Aattribute-category%20associations.%20In%20response%2C%20we%20propose%20a%20bidirectional%0Across-modal%20ZSL%20approach%20CREST.%20It%20begins%20by%20extracting%20representations%20for%0Aattribute%20and%20visual%20localization%20and%20employs%20Evidential%20Deep%20Learning%20%28EDL%29%20to%0Ameasure%20underlying%20epistemic%20uncertainty%2C%20thereby%20enhancing%20the%20model%27s%0Aresilience%20against%20hard%20negatives.%20CREST%20incorporates%20dual%20learning%20pathways%2C%0Afocusing%20on%20both%20visual-category%20and%20attribute-category%20alignments%2C%20to%20ensure%0Arobust%20correlation%20between%20latent%20and%20observable%20spaces.%20Moreover%2C%20we%20introduce%0Aan%20uncertainty-informed%20cross-modal%20fusion%20technique%20to%20refine%20visual-attribute%0Ainference.%20Extensive%20experiments%20demonstrate%20our%20model%27s%20effectiveness%20and%0Aunique%20explainability%20across%20multiple%20datasets.%20Our%20code%20and%20data%20are%20available%0Aat%3A%20Comments%3A%20Ongoing%20work%3B%2010%20pages%2C%202%20Tables%2C%209%20Figures%3B%20Repo%20is%20available%20at%0Ahttps%3A//github.com/JethroJames/CREST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09640v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CREST%3A%20Cross-modal%20Resonance%20through%20Evidential%20Deep%20Learning%20for%0A%20%20Enhanced%20Zero-Shot%20Learning&entry.906535625=Haojian%20Huang%20and%20Xiaozhen%20Qiao%20and%20Zhuo%20Chen%20and%20Haodong%20Chen%20and%20Bingyu%20Li%20and%20Zhe%20Sun%20and%20Mulin%20Chen%20and%20Xuelong%20Li&entry.1292438233=%20%20Zero-shot%20learning%20%28ZSL%29%20enables%20the%20recognition%20of%20novel%20classes%20by%0Aleveraging%20semantic%20knowledge%20transfer%20from%20known%20to%20unknown%20categories.%20This%0Aknowledge%2C%20typically%20encapsulated%20in%20attribute%20descriptions%2C%20aids%20in%0Aidentifying%20class-specific%20visual%20features%2C%20thus%20facilitating%20visual-semantic%0Aalignment%20and%20improving%20ZSL%20performance.%20However%2C%20real-world%20challenges%20such%20as%0Adistribution%20imbalances%20and%20attribute%20co-occurrence%20among%20instances%20often%0Ahinder%20the%20discernment%20of%20local%20variances%20in%20images%2C%20a%20problem%20exacerbated%20by%0Athe%20scarcity%20of%20fine-grained%2C%20region-specific%20attribute%20annotations.%20Moreover%2C%0Athe%20variability%20in%20visual%20presentation%20within%20categories%20can%20also%20skew%0Aattribute-category%20associations.%20In%20response%2C%20we%20propose%20a%20bidirectional%0Across-modal%20ZSL%20approach%20CREST.%20It%20begins%20by%20extracting%20representations%20for%0Aattribute%20and%20visual%20localization%20and%20employs%20Evidential%20Deep%20Learning%20%28EDL%29%20to%0Ameasure%20underlying%20epistemic%20uncertainty%2C%20thereby%20enhancing%20the%20model%27s%0Aresilience%20against%20hard%20negatives.%20CREST%20incorporates%20dual%20learning%20pathways%2C%0Afocusing%20on%20both%20visual-category%20and%20attribute-category%20alignments%2C%20to%20ensure%0Arobust%20correlation%20between%20latent%20and%20observable%20spaces.%20Moreover%2C%20we%20introduce%0Aan%20uncertainty-informed%20cross-modal%20fusion%20technique%20to%20refine%20visual-attribute%0Ainference.%20Extensive%20experiments%20demonstrate%20our%20model%27s%20effectiveness%20and%0Aunique%20explainability%20across%20multiple%20datasets.%20Our%20code%20and%20data%20are%20available%0Aat%3A%20Comments%3A%20Ongoing%20work%3B%2010%20pages%2C%202%20Tables%2C%209%20Figures%3B%20Repo%20is%20available%20at%0Ahttps%3A//github.com/JethroJames/CREST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09640v1&entry.124074799=Read"},
{"title": "Are NeRFs ready for autonomous driving? Towards closing the\n  real-to-simulation gap", "author": "Carl Lindstr\u00f6m and Georg Hess and Adam Lilja and Maryam Fatemi and Lars Hammarstrand and Christoffer Petersson and Lennart Svensson", "abstract": "  Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing\nautonomous driving (AD) research, offering scalable closed-loop simulation and\ndata augmentation capabilities. However, to trust the results achieved in\nsimulation, one needs to ensure that AD systems perceive real and rendered data\nin the same way. Although the performance of rendering methods is increasing,\nmany scenarios will remain inherently challenging to reconstruct faithfully. To\nthis end, we propose a novel perspective for addressing the real-to-simulated\ndata gap. Rather than solely focusing on improving rendering fidelity, we\nexplore simple yet effective methods to enhance perception model robustness to\nNeRF artifacts without compromising performance on real data. Moreover, we\nconduct the first large-scale investigation into the real-to-simulated data gap\nin an AD setting using a state-of-the-art neural rendering technique.\nSpecifically, we evaluate object detectors and an online mapping model on real\nand simulated data, and study the effects of different fine-tuning\nstrategies.Our results show notable improvements in model robustness to\nsimulated data, even improving real-world performance in some cases. Last, we\ndelve into the correlation between the real-to-simulated gap and image\nreconstruction metrics, identifying FID and LPIPS as strong indicators. See\nhttps://research.zenseact.com/publications/closing-real2sim-gap for our project\npage.\n", "link": "http://arxiv.org/abs/2403.16092v2", "date": "2024-04-15", "relevancy": 2.2169, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5569}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5451}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Are%20NeRFs%20ready%20for%20autonomous%20driving%3F%20Towards%20closing%20the%0A%20%20real-to-simulation%20gap&body=Title%3A%20Are%20NeRFs%20ready%20for%20autonomous%20driving%3F%20Towards%20closing%20the%0A%20%20real-to-simulation%20gap%0AAuthor%3A%20Carl%20Lindstr%C3%B6m%20and%20Georg%20Hess%20and%20Adam%20Lilja%20and%20Maryam%20Fatemi%20and%20Lars%20Hammarstrand%20and%20Christoffer%20Petersson%20and%20Lennart%20Svensson%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20emerged%20as%20promising%20tools%20for%20advancing%0Aautonomous%20driving%20%28AD%29%20research%2C%20offering%20scalable%20closed-loop%20simulation%20and%0Adata%20augmentation%20capabilities.%20However%2C%20to%20trust%20the%20results%20achieved%20in%0Asimulation%2C%20one%20needs%20to%20ensure%20that%20AD%20systems%20perceive%20real%20and%20rendered%20data%0Ain%20the%20same%20way.%20Although%20the%20performance%20of%20rendering%20methods%20is%20increasing%2C%0Amany%20scenarios%20will%20remain%20inherently%20challenging%20to%20reconstruct%20faithfully.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20perspective%20for%20addressing%20the%20real-to-simulated%0Adata%20gap.%20Rather%20than%20solely%20focusing%20on%20improving%20rendering%20fidelity%2C%20we%0Aexplore%20simple%20yet%20effective%20methods%20to%20enhance%20perception%20model%20robustness%20to%0ANeRF%20artifacts%20without%20compromising%20performance%20on%20real%20data.%20Moreover%2C%20we%0Aconduct%20the%20first%20large-scale%20investigation%20into%20the%20real-to-simulated%20data%20gap%0Ain%20an%20AD%20setting%20using%20a%20state-of-the-art%20neural%20rendering%20technique.%0ASpecifically%2C%20we%20evaluate%20object%20detectors%20and%20an%20online%20mapping%20model%20on%20real%0Aand%20simulated%20data%2C%20and%20study%20the%20effects%20of%20different%20fine-tuning%0Astrategies.Our%20results%20show%20notable%20improvements%20in%20model%20robustness%20to%0Asimulated%20data%2C%20even%20improving%20real-world%20performance%20in%20some%20cases.%20Last%2C%20we%0Adelve%20into%20the%20correlation%20between%20the%20real-to-simulated%20gap%20and%20image%0Areconstruction%20metrics%2C%20identifying%20FID%20and%20LPIPS%20as%20strong%20indicators.%20See%0Ahttps%3A//research.zenseact.com/publications/closing-real2sim-gap%20for%20our%20project%0Apage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16092v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20NeRFs%20ready%20for%20autonomous%20driving%3F%20Towards%20closing%20the%0A%20%20real-to-simulation%20gap&entry.906535625=Carl%20Lindstr%C3%B6m%20and%20Georg%20Hess%20and%20Adam%20Lilja%20and%20Maryam%20Fatemi%20and%20Lars%20Hammarstrand%20and%20Christoffer%20Petersson%20and%20Lennart%20Svensson&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20emerged%20as%20promising%20tools%20for%20advancing%0Aautonomous%20driving%20%28AD%29%20research%2C%20offering%20scalable%20closed-loop%20simulation%20and%0Adata%20augmentation%20capabilities.%20However%2C%20to%20trust%20the%20results%20achieved%20in%0Asimulation%2C%20one%20needs%20to%20ensure%20that%20AD%20systems%20perceive%20real%20and%20rendered%20data%0Ain%20the%20same%20way.%20Although%20the%20performance%20of%20rendering%20methods%20is%20increasing%2C%0Amany%20scenarios%20will%20remain%20inherently%20challenging%20to%20reconstruct%20faithfully.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20perspective%20for%20addressing%20the%20real-to-simulated%0Adata%20gap.%20Rather%20than%20solely%20focusing%20on%20improving%20rendering%20fidelity%2C%20we%0Aexplore%20simple%20yet%20effective%20methods%20to%20enhance%20perception%20model%20robustness%20to%0ANeRF%20artifacts%20without%20compromising%20performance%20on%20real%20data.%20Moreover%2C%20we%0Aconduct%20the%20first%20large-scale%20investigation%20into%20the%20real-to-simulated%20data%20gap%0Ain%20an%20AD%20setting%20using%20a%20state-of-the-art%20neural%20rendering%20technique.%0ASpecifically%2C%20we%20evaluate%20object%20detectors%20and%20an%20online%20mapping%20model%20on%20real%0Aand%20simulated%20data%2C%20and%20study%20the%20effects%20of%20different%20fine-tuning%0Astrategies.Our%20results%20show%20notable%20improvements%20in%20model%20robustness%20to%0Asimulated%20data%2C%20even%20improving%20real-world%20performance%20in%20some%20cases.%20Last%2C%20we%0Adelve%20into%20the%20correlation%20between%20the%20real-to-simulated%20gap%20and%20image%0Areconstruction%20metrics%2C%20identifying%20FID%20and%20LPIPS%20as%20strong%20indicators.%20See%0Ahttps%3A//research.zenseact.com/publications/closing-real2sim-gap%20for%20our%20project%0Apage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16092v2&entry.124074799=Read"},
{"title": "Do LLMs Understand Visual Anomalies? Uncovering LLM Capabilities in\n  Zero-shot Anomaly Detection", "author": "Jiaqi Zhu and Shaofeng Cai and Fang Deng and Junran Wu", "abstract": "  Large vision-language models (LVLMs) are markedly proficient in deriving\nvisual representations guided by natural language. Recent explorations have\nutilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by\npairing images with textual descriptions indicative of normal and abnormal\nconditions, referred to as anomaly prompts. However, existing approaches depend\non static anomaly prompts that are prone to cross-semantic ambiguity, and\nprioritize global image-level representations over crucial local pixel-level\nimage-to-text alignment that is necessary for accurate anomaly localization. In\nthis paper, we present ALFA, a training-free approach designed to address these\nchallenges via a unified model. We propose a run-time prompt adaptation\nstrategy, which first generates informative anomaly prompts to leverage the\ncapabilities of a large language model (LLM). This strategy is enhanced by a\ncontextual scoring mechanism for per-image anomaly prompt adaptation and\ncross-semantic ambiguity mitigation. We further introduce a novel fine-grained\naligner to fuse local pixel-level semantics for precise anomaly localization,\nby projecting the image-text alignment from global to local semantic spaces.\nExtensive evaluations on the challenging MVTec and VisA datasets confirm ALFA's\neffectiveness in harnessing the language potential for zero-shot VAD, achieving\nsignificant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to\nstate-of-the-art zero-shot VAD approaches.\n", "link": "http://arxiv.org/abs/2404.09654v1", "date": "2024-04-15", "relevancy": 2.2068, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5757}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5578}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5253}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20Understand%20Visual%20Anomalies%3F%20Uncovering%20LLM%20Capabilities%20in%0A%20%20Zero-shot%20Anomaly%20Detection&body=Title%3A%20Do%20LLMs%20Understand%20Visual%20Anomalies%3F%20Uncovering%20LLM%20Capabilities%20in%0A%20%20Zero-shot%20Anomaly%20Detection%0AAuthor%3A%20Jiaqi%20Zhu%20and%20Shaofeng%20Cai%20and%20Fang%20Deng%20and%20Junran%20Wu%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20are%20markedly%20proficient%20in%20deriving%0Avisual%20representations%20guided%20by%20natural%20language.%20Recent%20explorations%20have%0Autilized%20LVLMs%20to%20tackle%20zero-shot%20visual%20anomaly%20detection%20%28VAD%29%20challenges%20by%0Apairing%20images%20with%20textual%20descriptions%20indicative%20of%20normal%20and%20abnormal%0Aconditions%2C%20referred%20to%20as%20anomaly%20prompts.%20However%2C%20existing%20approaches%20depend%0Aon%20static%20anomaly%20prompts%20that%20are%20prone%20to%20cross-semantic%20ambiguity%2C%20and%0Aprioritize%20global%20image-level%20representations%20over%20crucial%20local%20pixel-level%0Aimage-to-text%20alignment%20that%20is%20necessary%20for%20accurate%20anomaly%20localization.%20In%0Athis%20paper%2C%20we%20present%20ALFA%2C%20a%20training-free%20approach%20designed%20to%20address%20these%0Achallenges%20via%20a%20unified%20model.%20We%20propose%20a%20run-time%20prompt%20adaptation%0Astrategy%2C%20which%20first%20generates%20informative%20anomaly%20prompts%20to%20leverage%20the%0Acapabilities%20of%20a%20large%20language%20model%20%28LLM%29.%20This%20strategy%20is%20enhanced%20by%20a%0Acontextual%20scoring%20mechanism%20for%20per-image%20anomaly%20prompt%20adaptation%20and%0Across-semantic%20ambiguity%20mitigation.%20We%20further%20introduce%20a%20novel%20fine-grained%0Aaligner%20to%20fuse%20local%20pixel-level%20semantics%20for%20precise%20anomaly%20localization%2C%0Aby%20projecting%20the%20image-text%20alignment%20from%20global%20to%20local%20semantic%20spaces.%0AExtensive%20evaluations%20on%20the%20challenging%20MVTec%20and%20VisA%20datasets%20confirm%20ALFA%27s%0Aeffectiveness%20in%20harnessing%20the%20language%20potential%20for%20zero-shot%20VAD%2C%20achieving%0Asignificant%20PRO%20improvements%20of%2012.1%25%20on%20MVTec%20AD%20and%208.9%25%20on%20VisA%20compared%20to%0Astate-of-the-art%20zero-shot%20VAD%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09654v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20Understand%20Visual%20Anomalies%3F%20Uncovering%20LLM%20Capabilities%20in%0A%20%20Zero-shot%20Anomaly%20Detection&entry.906535625=Jiaqi%20Zhu%20and%20Shaofeng%20Cai%20and%20Fang%20Deng%20and%20Junran%20Wu&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20are%20markedly%20proficient%20in%20deriving%0Avisual%20representations%20guided%20by%20natural%20language.%20Recent%20explorations%20have%0Autilized%20LVLMs%20to%20tackle%20zero-shot%20visual%20anomaly%20detection%20%28VAD%29%20challenges%20by%0Apairing%20images%20with%20textual%20descriptions%20indicative%20of%20normal%20and%20abnormal%0Aconditions%2C%20referred%20to%20as%20anomaly%20prompts.%20However%2C%20existing%20approaches%20depend%0Aon%20static%20anomaly%20prompts%20that%20are%20prone%20to%20cross-semantic%20ambiguity%2C%20and%0Aprioritize%20global%20image-level%20representations%20over%20crucial%20local%20pixel-level%0Aimage-to-text%20alignment%20that%20is%20necessary%20for%20accurate%20anomaly%20localization.%20In%0Athis%20paper%2C%20we%20present%20ALFA%2C%20a%20training-free%20approach%20designed%20to%20address%20these%0Achallenges%20via%20a%20unified%20model.%20We%20propose%20a%20run-time%20prompt%20adaptation%0Astrategy%2C%20which%20first%20generates%20informative%20anomaly%20prompts%20to%20leverage%20the%0Acapabilities%20of%20a%20large%20language%20model%20%28LLM%29.%20This%20strategy%20is%20enhanced%20by%20a%0Acontextual%20scoring%20mechanism%20for%20per-image%20anomaly%20prompt%20adaptation%20and%0Across-semantic%20ambiguity%20mitigation.%20We%20further%20introduce%20a%20novel%20fine-grained%0Aaligner%20to%20fuse%20local%20pixel-level%20semantics%20for%20precise%20anomaly%20localization%2C%0Aby%20projecting%20the%20image-text%20alignment%20from%20global%20to%20local%20semantic%20spaces.%0AExtensive%20evaluations%20on%20the%20challenging%20MVTec%20and%20VisA%20datasets%20confirm%20ALFA%27s%0Aeffectiveness%20in%20harnessing%20the%20language%20potential%20for%20zero-shot%20VAD%2C%20achieving%0Asignificant%20PRO%20improvements%20of%2012.1%25%20on%20MVTec%20AD%20and%208.9%25%20on%20VisA%20compared%20to%0Astate-of-the-art%20zero-shot%20VAD%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09654v1&entry.124074799=Read"},
{"title": "Post-Training Network Compression for 3D Medical Image Segmentation:\n  Reducing Computational Efforts via Tucker Decomposition", "author": "Tobias Weber and Jakob Dexl and David R\u00fcgamer and Michael Ingrisch", "abstract": "  We address the computational barrier of deploying advanced deep learning\nsegmentation models in clinical settings by studying the efficacy of network\ncompression through tensor decomposition. We propose a post-training Tucker\nfactorization that enables the decomposition of pre-existing models to reduce\ncomputational requirements without impeding segmentation accuracy. We applied\nTucker decomposition to the convolutional kernels of the TotalSegmentator (TS)\nmodel, an nnU-Net model trained on a comprehensive dataset for automatic\nsegmentation of 117 anatomical structures. Our approach reduced the\nfloating-point operations (FLOPs) and memory required during inference,\noffering an adjustable trade-off between computational efficiency and\nsegmentation quality. This study utilized the publicly available TS dataset,\nemploying various downsampling factors to explore the relationship between\nmodel size, inference speed, and segmentation performance. The application of\nTucker decomposition to the TS model substantially reduced the model parameters\nand FLOPs across various compression rates, with limited loss in segmentation\naccuracy. We removed up to 88% of the model's parameters with no significant\nperformance changes in the majority of classes after fine-tuning. Practical\nbenefits varied across different graphics processing unit (GPU) architectures,\nwith more distinct speed-ups on less powerful hardware. Post-hoc network\ncompression via Tucker decomposition presents a viable strategy for reducing\nthe computational demand of medical image segmentation models without\nsubstantially sacrificing accuracy. This approach enables the broader adoption\nof advanced deep learning technologies in clinical practice, offering a way to\nnavigate the constraints of hardware capabilities.\n", "link": "http://arxiv.org/abs/2404.09683v1", "date": "2024-04-15", "relevancy": 2.1862, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.563}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5441}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5424}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Post-Training%20Network%20Compression%20for%203D%20Medical%20Image%20Segmentation%3A%0A%20%20Reducing%20Computational%20Efforts%20via%20Tucker%20Decomposition&body=Title%3A%20Post-Training%20Network%20Compression%20for%203D%20Medical%20Image%20Segmentation%3A%0A%20%20Reducing%20Computational%20Efforts%20via%20Tucker%20Decomposition%0AAuthor%3A%20Tobias%20Weber%20and%20Jakob%20Dexl%20and%20David%20R%C3%BCgamer%20and%20Michael%20Ingrisch%0AAbstract%3A%20%20%20We%20address%20the%20computational%20barrier%20of%20deploying%20advanced%20deep%20learning%0Asegmentation%20models%20in%20clinical%20settings%20by%20studying%20the%20efficacy%20of%20network%0Acompression%20through%20tensor%20decomposition.%20We%20propose%20a%20post-training%20Tucker%0Afactorization%20that%20enables%20the%20decomposition%20of%20pre-existing%20models%20to%20reduce%0Acomputational%20requirements%20without%20impeding%20segmentation%20accuracy.%20We%20applied%0ATucker%20decomposition%20to%20the%20convolutional%20kernels%20of%20the%20TotalSegmentator%20%28TS%29%0Amodel%2C%20an%20nnU-Net%20model%20trained%20on%20a%20comprehensive%20dataset%20for%20automatic%0Asegmentation%20of%20117%20anatomical%20structures.%20Our%20approach%20reduced%20the%0Afloating-point%20operations%20%28FLOPs%29%20and%20memory%20required%20during%20inference%2C%0Aoffering%20an%20adjustable%20trade-off%20between%20computational%20efficiency%20and%0Asegmentation%20quality.%20This%20study%20utilized%20the%20publicly%20available%20TS%20dataset%2C%0Aemploying%20various%20downsampling%20factors%20to%20explore%20the%20relationship%20between%0Amodel%20size%2C%20inference%20speed%2C%20and%20segmentation%20performance.%20The%20application%20of%0ATucker%20decomposition%20to%20the%20TS%20model%20substantially%20reduced%20the%20model%20parameters%0Aand%20FLOPs%20across%20various%20compression%20rates%2C%20with%20limited%20loss%20in%20segmentation%0Aaccuracy.%20We%20removed%20up%20to%2088%25%20of%20the%20model%27s%20parameters%20with%20no%20significant%0Aperformance%20changes%20in%20the%20majority%20of%20classes%20after%20fine-tuning.%20Practical%0Abenefits%20varied%20across%20different%20graphics%20processing%20unit%20%28GPU%29%20architectures%2C%0Awith%20more%20distinct%20speed-ups%20on%20less%20powerful%20hardware.%20Post-hoc%20network%0Acompression%20via%20Tucker%20decomposition%20presents%20a%20viable%20strategy%20for%20reducing%0Athe%20computational%20demand%20of%20medical%20image%20segmentation%20models%20without%0Asubstantially%20sacrificing%20accuracy.%20This%20approach%20enables%20the%20broader%20adoption%0Aof%20advanced%20deep%20learning%20technologies%20in%20clinical%20practice%2C%20offering%20a%20way%20to%0Anavigate%20the%20constraints%20of%20hardware%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09683v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-Training%20Network%20Compression%20for%203D%20Medical%20Image%20Segmentation%3A%0A%20%20Reducing%20Computational%20Efforts%20via%20Tucker%20Decomposition&entry.906535625=Tobias%20Weber%20and%20Jakob%20Dexl%20and%20David%20R%C3%BCgamer%20and%20Michael%20Ingrisch&entry.1292438233=%20%20We%20address%20the%20computational%20barrier%20of%20deploying%20advanced%20deep%20learning%0Asegmentation%20models%20in%20clinical%20settings%20by%20studying%20the%20efficacy%20of%20network%0Acompression%20through%20tensor%20decomposition.%20We%20propose%20a%20post-training%20Tucker%0Afactorization%20that%20enables%20the%20decomposition%20of%20pre-existing%20models%20to%20reduce%0Acomputational%20requirements%20without%20impeding%20segmentation%20accuracy.%20We%20applied%0ATucker%20decomposition%20to%20the%20convolutional%20kernels%20of%20the%20TotalSegmentator%20%28TS%29%0Amodel%2C%20an%20nnU-Net%20model%20trained%20on%20a%20comprehensive%20dataset%20for%20automatic%0Asegmentation%20of%20117%20anatomical%20structures.%20Our%20approach%20reduced%20the%0Afloating-point%20operations%20%28FLOPs%29%20and%20memory%20required%20during%20inference%2C%0Aoffering%20an%20adjustable%20trade-off%20between%20computational%20efficiency%20and%0Asegmentation%20quality.%20This%20study%20utilized%20the%20publicly%20available%20TS%20dataset%2C%0Aemploying%20various%20downsampling%20factors%20to%20explore%20the%20relationship%20between%0Amodel%20size%2C%20inference%20speed%2C%20and%20segmentation%20performance.%20The%20application%20of%0ATucker%20decomposition%20to%20the%20TS%20model%20substantially%20reduced%20the%20model%20parameters%0Aand%20FLOPs%20across%20various%20compression%20rates%2C%20with%20limited%20loss%20in%20segmentation%0Aaccuracy.%20We%20removed%20up%20to%2088%25%20of%20the%20model%27s%20parameters%20with%20no%20significant%0Aperformance%20changes%20in%20the%20majority%20of%20classes%20after%20fine-tuning.%20Practical%0Abenefits%20varied%20across%20different%20graphics%20processing%20unit%20%28GPU%29%20architectures%2C%0Awith%20more%20distinct%20speed-ups%20on%20less%20powerful%20hardware.%20Post-hoc%20network%0Acompression%20via%20Tucker%20decomposition%20presents%20a%20viable%20strategy%20for%20reducing%0Athe%20computational%20demand%20of%20medical%20image%20segmentation%20models%20without%0Asubstantially%20sacrificing%20accuracy.%20This%20approach%20enables%20the%20broader%20adoption%0Aof%20advanced%20deep%20learning%20technologies%20in%20clinical%20practice%2C%20offering%20a%20way%20to%0Anavigate%20the%20constraints%20of%20hardware%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09683v1&entry.124074799=Read"},
{"title": "Physics-guided Shape-from-Template: Monocular Video Perception through\n  Neural Surrogate Models", "author": "David Stotko and Nils Wandel and Reinhard Klein", "abstract": "  3D reconstruction of dynamic scenes is a long-standing problem in computer\ngraphics and increasingly difficult the less information is available.\nShape-from-Template (SfT) methods aim to reconstruct a template-based geometry\nfrom RGB images or video sequences, often leveraging just a single monocular\ncamera without depth information, such as regular smartphone recordings.\nUnfortunately, existing reconstruction methods are either unphysical and noisy\nor slow in optimization. To solve this problem, we propose a novel SfT\nreconstruction algorithm for cloth using a pre-trained neural surrogate model\nthat is fast to evaluate, stable, and produces smooth reconstructions due to a\nregularizing physics simulation. Differentiable rendering of the simulated mesh\nenables pixel-wise comparisons between the reconstruction and a target video\nsequence that can be used for a gradient-based optimization procedure to\nextract not only shape information but also physical parameters such as\nstretching, shearing, or bending stiffness of the cloth. This allows to retain\na precise, stable, and smooth reconstructed geometry while reducing the runtime\nby a factor of 400-500 compared to $\\phi$-SfT, a state-of-the-art physics-based\nSfT approach.\n", "link": "http://arxiv.org/abs/2311.12796v3", "date": "2024-04-15", "relevancy": 2.1775, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5466}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5449}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5429}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Physics-guided%20Shape-from-Template%3A%20Monocular%20Video%20Perception%20through%0A%20%20Neural%20Surrogate%20Models&body=Title%3A%20Physics-guided%20Shape-from-Template%3A%20Monocular%20Video%20Perception%20through%0A%20%20Neural%20Surrogate%20Models%0AAuthor%3A%20David%20Stotko%20and%20Nils%20Wandel%20and%20Reinhard%20Klein%0AAbstract%3A%20%20%203D%20reconstruction%20of%20dynamic%20scenes%20is%20a%20long-standing%20problem%20in%20computer%0Agraphics%20and%20increasingly%20difficult%20the%20less%20information%20is%20available.%0AShape-from-Template%20%28SfT%29%20methods%20aim%20to%20reconstruct%20a%20template-based%20geometry%0Afrom%20RGB%20images%20or%20video%20sequences%2C%20often%20leveraging%20just%20a%20single%20monocular%0Acamera%20without%20depth%20information%2C%20such%20as%20regular%20smartphone%20recordings.%0AUnfortunately%2C%20existing%20reconstruction%20methods%20are%20either%20unphysical%20and%20noisy%0Aor%20slow%20in%20optimization.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20novel%20SfT%0Areconstruction%20algorithm%20for%20cloth%20using%20a%20pre-trained%20neural%20surrogate%20model%0Athat%20is%20fast%20to%20evaluate%2C%20stable%2C%20and%20produces%20smooth%20reconstructions%20due%20to%20a%0Aregularizing%20physics%20simulation.%20Differentiable%20rendering%20of%20the%20simulated%20mesh%0Aenables%20pixel-wise%20comparisons%20between%20the%20reconstruction%20and%20a%20target%20video%0Asequence%20that%20can%20be%20used%20for%20a%20gradient-based%20optimization%20procedure%20to%0Aextract%20not%20only%20shape%20information%20but%20also%20physical%20parameters%20such%20as%0Astretching%2C%20shearing%2C%20or%20bending%20stiffness%20of%20the%20cloth.%20This%20allows%20to%20retain%0Aa%20precise%2C%20stable%2C%20and%20smooth%20reconstructed%20geometry%20while%20reducing%20the%20runtime%0Aby%20a%20factor%20of%20400-500%20compared%20to%20%24%5Cphi%24-SfT%2C%20a%20state-of-the-art%20physics-based%0ASfT%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12796v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-guided%20Shape-from-Template%3A%20Monocular%20Video%20Perception%20through%0A%20%20Neural%20Surrogate%20Models&entry.906535625=David%20Stotko%20and%20Nils%20Wandel%20and%20Reinhard%20Klein&entry.1292438233=%20%203D%20reconstruction%20of%20dynamic%20scenes%20is%20a%20long-standing%20problem%20in%20computer%0Agraphics%20and%20increasingly%20difficult%20the%20less%20information%20is%20available.%0AShape-from-Template%20%28SfT%29%20methods%20aim%20to%20reconstruct%20a%20template-based%20geometry%0Afrom%20RGB%20images%20or%20video%20sequences%2C%20often%20leveraging%20just%20a%20single%20monocular%0Acamera%20without%20depth%20information%2C%20such%20as%20regular%20smartphone%20recordings.%0AUnfortunately%2C%20existing%20reconstruction%20methods%20are%20either%20unphysical%20and%20noisy%0Aor%20slow%20in%20optimization.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20novel%20SfT%0Areconstruction%20algorithm%20for%20cloth%20using%20a%20pre-trained%20neural%20surrogate%20model%0Athat%20is%20fast%20to%20evaluate%2C%20stable%2C%20and%20produces%20smooth%20reconstructions%20due%20to%20a%0Aregularizing%20physics%20simulation.%20Differentiable%20rendering%20of%20the%20simulated%20mesh%0Aenables%20pixel-wise%20comparisons%20between%20the%20reconstruction%20and%20a%20target%20video%0Asequence%20that%20can%20be%20used%20for%20a%20gradient-based%20optimization%20procedure%20to%0Aextract%20not%20only%20shape%20information%20but%20also%20physical%20parameters%20such%20as%0Astretching%2C%20shearing%2C%20or%20bending%20stiffness%20of%20the%20cloth.%20This%20allows%20to%20retain%0Aa%20precise%2C%20stable%2C%20and%20smooth%20reconstructed%20geometry%20while%20reducing%20the%20runtime%0Aby%20a%20factor%20of%20400-500%20compared%20to%20%24%5Cphi%24-SfT%2C%20a%20state-of-the-art%20physics-based%0ASfT%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12796v3&entry.124074799=Read"},
{"title": "LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted\n  Gaussian Primitives", "author": "Jiadi Cui and Junming Cao and Yuhui Zhong and Liao Wang and Fuqiang Zhao and Penghao Wang and Yifan Chen and Zhipeng He and Lan Xu and Yujiao Shi and Yingliang Zhang and Jingyi Yu", "abstract": "  Large garages are ubiquitous yet intricate scenes in our daily lives, posing\nchallenges characterized by monotonous colors, repetitive patterns, reflective\nsurfaces, and transparent vehicle glass. Conventional Structure from Motion\n(SfM) methods for camera pose estimation and 3D reconstruction fail in these\nenvironments due to poor correspondence construction. To address these\nchallenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting\napproach for large-scale garage modeling and rendering. We develop a handheld\nscanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate\naccurate LiDAR and image data scanning. With this Polar device, we present a\nGarageWorld dataset consisting of five expansive garage scenes with diverse\ngeometric structures and will release the dataset to the community for further\nresearch. We demonstrate that the collected LiDAR point cloud by the Polar\ndevice enhances a suite of 3D Gaussian splatting algorithms for garage scene\nmodeling and rendering. We also propose a novel depth regularizer for 3D\nGaussian splatting algorithm training, effectively eliminating floating\nartifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian\nrenderer for real-time viewing on web-based devices. Additionally, we explore a\nhybrid representation that combines the advantages of traditional mesh in\ndepicting simple geometry and colors (e.g., walls and the ground) with modern\n3D Gaussian representations capturing complex details and high-frequency\ntextures. This strategy achieves an optimal balance between memory performance\nand rendering quality. Experimental results on our dataset, along with\nScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering\nquality and resource efficiency.\n", "link": "http://arxiv.org/abs/2404.09748v1", "date": "2024-04-15", "relevancy": 2.1198, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.543}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5222}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5167}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LetsGo%3A%20Large-Scale%20Garage%20Modeling%20and%20Rendering%20via%20LiDAR-Assisted%0A%20%20Gaussian%20Primitives&body=Title%3A%20LetsGo%3A%20Large-Scale%20Garage%20Modeling%20and%20Rendering%20via%20LiDAR-Assisted%0A%20%20Gaussian%20Primitives%0AAuthor%3A%20Jiadi%20Cui%20and%20Junming%20Cao%20and%20Yuhui%20Zhong%20and%20Liao%20Wang%20and%20Fuqiang%20Zhao%20and%20Penghao%20Wang%20and%20Yifan%20Chen%20and%20Zhipeng%20He%20and%20Lan%20Xu%20and%20Yujiao%20Shi%20and%20Yingliang%20Zhang%20and%20Jingyi%20Yu%0AAbstract%3A%20%20%20Large%20garages%20are%20ubiquitous%20yet%20intricate%20scenes%20in%20our%20daily%20lives%2C%20posing%0Achallenges%20characterized%20by%20monotonous%20colors%2C%20repetitive%20patterns%2C%20reflective%0Asurfaces%2C%20and%20transparent%20vehicle%20glass.%20Conventional%20Structure%20from%20Motion%0A%28SfM%29%20methods%20for%20camera%20pose%20estimation%20and%203D%20reconstruction%20fail%20in%20these%0Aenvironments%20due%20to%20poor%20correspondence%20construction.%20To%20address%20these%0Achallenges%2C%20this%20paper%20introduces%20LetsGo%2C%20a%20LiDAR-assisted%20Gaussian%20splatting%0Aapproach%20for%20large-scale%20garage%20modeling%20and%20rendering.%20We%20develop%20a%20handheld%0Ascanner%2C%20Polar%2C%20equipped%20with%20IMU%2C%20LiDAR%2C%20and%20a%20fisheye%20camera%2C%20to%20facilitate%0Aaccurate%20LiDAR%20and%20image%20data%20scanning.%20With%20this%20Polar%20device%2C%20we%20present%20a%0AGarageWorld%20dataset%20consisting%20of%20five%20expansive%20garage%20scenes%20with%20diverse%0Ageometric%20structures%20and%20will%20release%20the%20dataset%20to%20the%20community%20for%20further%0Aresearch.%20We%20demonstrate%20that%20the%20collected%20LiDAR%20point%20cloud%20by%20the%20Polar%0Adevice%20enhances%20a%20suite%20of%203D%20Gaussian%20splatting%20algorithms%20for%20garage%20scene%0Amodeling%20and%20rendering.%20We%20also%20propose%20a%20novel%20depth%20regularizer%20for%203D%0AGaussian%20splatting%20algorithm%20training%2C%20effectively%20eliminating%20floating%0Aartifacts%20in%20rendered%20images%2C%20and%20a%20lightweight%20Level%20of%20Detail%20%28LOD%29%20Gaussian%0Arenderer%20for%20real-time%20viewing%20on%20web-based%20devices.%20Additionally%2C%20we%20explore%20a%0Ahybrid%20representation%20that%20combines%20the%20advantages%20of%20traditional%20mesh%20in%0Adepicting%20simple%20geometry%20and%20colors%20%28e.g.%2C%20walls%20and%20the%20ground%29%20with%20modern%0A3D%20Gaussian%20representations%20capturing%20complex%20details%20and%20high-frequency%0Atextures.%20This%20strategy%20achieves%20an%20optimal%20balance%20between%20memory%20performance%0Aand%20rendering%20quality.%20Experimental%20results%20on%20our%20dataset%2C%20along%20with%0AScanNet%2B%2B%20and%20KITTI-360%2C%20demonstrate%20the%20superiority%20of%20our%20method%20in%20rendering%0Aquality%20and%20resource%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09748v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LetsGo%3A%20Large-Scale%20Garage%20Modeling%20and%20Rendering%20via%20LiDAR-Assisted%0A%20%20Gaussian%20Primitives&entry.906535625=Jiadi%20Cui%20and%20Junming%20Cao%20and%20Yuhui%20Zhong%20and%20Liao%20Wang%20and%20Fuqiang%20Zhao%20and%20Penghao%20Wang%20and%20Yifan%20Chen%20and%20Zhipeng%20He%20and%20Lan%20Xu%20and%20Yujiao%20Shi%20and%20Yingliang%20Zhang%20and%20Jingyi%20Yu&entry.1292438233=%20%20Large%20garages%20are%20ubiquitous%20yet%20intricate%20scenes%20in%20our%20daily%20lives%2C%20posing%0Achallenges%20characterized%20by%20monotonous%20colors%2C%20repetitive%20patterns%2C%20reflective%0Asurfaces%2C%20and%20transparent%20vehicle%20glass.%20Conventional%20Structure%20from%20Motion%0A%28SfM%29%20methods%20for%20camera%20pose%20estimation%20and%203D%20reconstruction%20fail%20in%20these%0Aenvironments%20due%20to%20poor%20correspondence%20construction.%20To%20address%20these%0Achallenges%2C%20this%20paper%20introduces%20LetsGo%2C%20a%20LiDAR-assisted%20Gaussian%20splatting%0Aapproach%20for%20large-scale%20garage%20modeling%20and%20rendering.%20We%20develop%20a%20handheld%0Ascanner%2C%20Polar%2C%20equipped%20with%20IMU%2C%20LiDAR%2C%20and%20a%20fisheye%20camera%2C%20to%20facilitate%0Aaccurate%20LiDAR%20and%20image%20data%20scanning.%20With%20this%20Polar%20device%2C%20we%20present%20a%0AGarageWorld%20dataset%20consisting%20of%20five%20expansive%20garage%20scenes%20with%20diverse%0Ageometric%20structures%20and%20will%20release%20the%20dataset%20to%20the%20community%20for%20further%0Aresearch.%20We%20demonstrate%20that%20the%20collected%20LiDAR%20point%20cloud%20by%20the%20Polar%0Adevice%20enhances%20a%20suite%20of%203D%20Gaussian%20splatting%20algorithms%20for%20garage%20scene%0Amodeling%20and%20rendering.%20We%20also%20propose%20a%20novel%20depth%20regularizer%20for%203D%0AGaussian%20splatting%20algorithm%20training%2C%20effectively%20eliminating%20floating%0Aartifacts%20in%20rendered%20images%2C%20and%20a%20lightweight%20Level%20of%20Detail%20%28LOD%29%20Gaussian%0Arenderer%20for%20real-time%20viewing%20on%20web-based%20devices.%20Additionally%2C%20we%20explore%20a%0Ahybrid%20representation%20that%20combines%20the%20advantages%20of%20traditional%20mesh%20in%0Adepicting%20simple%20geometry%20and%20colors%20%28e.g.%2C%20walls%20and%20the%20ground%29%20with%20modern%0A3D%20Gaussian%20representations%20capturing%20complex%20details%20and%20high-frequency%0Atextures.%20This%20strategy%20achieves%20an%20optimal%20balance%20between%20memory%20performance%0Aand%20rendering%20quality.%20Experimental%20results%20on%20our%20dataset%2C%20along%20with%0AScanNet%2B%2B%20and%20KITTI-360%2C%20demonstrate%20the%20superiority%20of%20our%20method%20in%20rendering%0Aquality%20and%20resource%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09748v1&entry.124074799=Read"},
{"title": "Sparse Global Matching for Video Frame Interpolation with Large Motion", "author": "Chunxu Liu and Guozhen Zhang and Rui Zhao and Limin Wang", "abstract": "  Large motion poses a critical challenge in Video Frame Interpolation (VFI)\ntask. Existing methods are often constrained by limited receptive fields,\nresulting in sub-optimal performance when handling scenarios with large motion.\nIn this paper, we introduce a new pipeline for VFI, which can effectively\nintegrate global-level information to alleviate issues associated with large\nmotion. Specifically, we first estimate a pair of initial intermediate flows\nusing a high-resolution feature map for extracting local details. Then, we\nincorporate a sparse global matching branch to compensate for flow estimation,\nwhich consists of identifying flaws in initial flows and generating sparse flow\ncompensation with a global receptive field. Finally, we adaptively merge the\ninitial flow estimation with global flow compensation, yielding a more accurate\nintermediate flow. To evaluate the effectiveness of our method in handling\nlarge motion, we carefully curate a more challenging subset from commonly used\nbenchmarks. Our method demonstrates the state-of-the-art performance on these\nVFI subsets with large motion.\n", "link": "http://arxiv.org/abs/2404.06913v2", "date": "2024-04-15", "relevancy": 2.108, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5455}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5401}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5032}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sparse%20Global%20Matching%20for%20Video%20Frame%20Interpolation%20with%20Large%20Motion&body=Title%3A%20Sparse%20Global%20Matching%20for%20Video%20Frame%20Interpolation%20with%20Large%20Motion%0AAuthor%3A%20Chunxu%20Liu%20and%20Guozhen%20Zhang%20and%20Rui%20Zhao%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Large%20motion%20poses%20a%20critical%20challenge%20in%20Video%20Frame%20Interpolation%20%28VFI%29%0Atask.%20Existing%20methods%20are%20often%20constrained%20by%20limited%20receptive%20fields%2C%0Aresulting%20in%20sub-optimal%20performance%20when%20handling%20scenarios%20with%20large%20motion.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20pipeline%20for%20VFI%2C%20which%20can%20effectively%0Aintegrate%20global-level%20information%20to%20alleviate%20issues%20associated%20with%20large%0Amotion.%20Specifically%2C%20we%20first%20estimate%20a%20pair%20of%20initial%20intermediate%20flows%0Ausing%20a%20high-resolution%20feature%20map%20for%20extracting%20local%20details.%20Then%2C%20we%0Aincorporate%20a%20sparse%20global%20matching%20branch%20to%20compensate%20for%20flow%20estimation%2C%0Awhich%20consists%20of%20identifying%20flaws%20in%20initial%20flows%20and%20generating%20sparse%20flow%0Acompensation%20with%20a%20global%20receptive%20field.%20Finally%2C%20we%20adaptively%20merge%20the%0Ainitial%20flow%20estimation%20with%20global%20flow%20compensation%2C%20yielding%20a%20more%20accurate%0Aintermediate%20flow.%20To%20evaluate%20the%20effectiveness%20of%20our%20method%20in%20handling%0Alarge%20motion%2C%20we%20carefully%20curate%20a%20more%20challenging%20subset%20from%20commonly%20used%0Abenchmarks.%20Our%20method%20demonstrates%20the%20state-of-the-art%20performance%20on%20these%0AVFI%20subsets%20with%20large%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06913v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Global%20Matching%20for%20Video%20Frame%20Interpolation%20with%20Large%20Motion&entry.906535625=Chunxu%20Liu%20and%20Guozhen%20Zhang%20and%20Rui%20Zhao%20and%20Limin%20Wang&entry.1292438233=%20%20Large%20motion%20poses%20a%20critical%20challenge%20in%20Video%20Frame%20Interpolation%20%28VFI%29%0Atask.%20Existing%20methods%20are%20often%20constrained%20by%20limited%20receptive%20fields%2C%0Aresulting%20in%20sub-optimal%20performance%20when%20handling%20scenarios%20with%20large%20motion.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20pipeline%20for%20VFI%2C%20which%20can%20effectively%0Aintegrate%20global-level%20information%20to%20alleviate%20issues%20associated%20with%20large%0Amotion.%20Specifically%2C%20we%20first%20estimate%20a%20pair%20of%20initial%20intermediate%20flows%0Ausing%20a%20high-resolution%20feature%20map%20for%20extracting%20local%20details.%20Then%2C%20we%0Aincorporate%20a%20sparse%20global%20matching%20branch%20to%20compensate%20for%20flow%20estimation%2C%0Awhich%20consists%20of%20identifying%20flaws%20in%20initial%20flows%20and%20generating%20sparse%20flow%0Acompensation%20with%20a%20global%20receptive%20field.%20Finally%2C%20we%20adaptively%20merge%20the%0Ainitial%20flow%20estimation%20with%20global%20flow%20compensation%2C%20yielding%20a%20more%20accurate%0Aintermediate%20flow.%20To%20evaluate%20the%20effectiveness%20of%20our%20method%20in%20handling%0Alarge%20motion%2C%20we%20carefully%20curate%20a%20more%20challenging%20subset%20from%20commonly%20used%0Abenchmarks.%20Our%20method%20demonstrates%20the%20state-of-the-art%20performance%20on%20these%0AVFI%20subsets%20with%20large%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06913v2&entry.124074799=Read"},
{"title": "XoFTR: Cross-modal Feature Matching Transformer", "author": "\u00d6nder Tuzcuo\u011flu and Aybora K\u00f6ksal and Bu\u011fra Sofu and Sinan Kalkan and A. Ayd\u0131n Alatan", "abstract": "  We introduce, XoFTR, a cross-modal cross-view method for local feature\nmatching between thermal infrared (TIR) and visible images. Unlike visible\nimages, TIR images are less susceptible to adverse lighting and weather\nconditions but present difficulties in matching due to significant texture and\nintensity differences. Current hand-crafted and learning-based methods for\nvisible-TIR matching fall short in handling viewpoint, scale, and texture\ndiversities. To address this, XoFTR incorporates masked image modeling\npre-training and fine-tuning with pseudo-thermal image augmentation to handle\nthe modality differences. Additionally, we introduce a refined matching\npipeline that adjusts for scale discrepancies and enhances match reliability\nthrough sub-pixel level refinement. To validate our approach, we collect a\ncomprehensive visible-thermal dataset, and show that our method outperforms\nexisting methods on many benchmarks.\n", "link": "http://arxiv.org/abs/2404.09692v1", "date": "2024-04-15", "relevancy": 2.1076, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5483}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5247}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4789}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20XoFTR%3A%20Cross-modal%20Feature%20Matching%20Transformer&body=Title%3A%20XoFTR%3A%20Cross-modal%20Feature%20Matching%20Transformer%0AAuthor%3A%20%C3%96nder%20Tuzcuo%C4%9Flu%20and%20Aybora%20K%C3%B6ksal%20and%20Bu%C4%9Fra%20Sofu%20and%20Sinan%20Kalkan%20and%20A.%20Ayd%C4%B1n%20Alatan%0AAbstract%3A%20%20%20We%20introduce%2C%20XoFTR%2C%20a%20cross-modal%20cross-view%20method%20for%20local%20feature%0Amatching%20between%20thermal%20infrared%20%28TIR%29%20and%20visible%20images.%20Unlike%20visible%0Aimages%2C%20TIR%20images%20are%20less%20susceptible%20to%20adverse%20lighting%20and%20weather%0Aconditions%20but%20present%20difficulties%20in%20matching%20due%20to%20significant%20texture%20and%0Aintensity%20differences.%20Current%20hand-crafted%20and%20learning-based%20methods%20for%0Avisible-TIR%20matching%20fall%20short%20in%20handling%20viewpoint%2C%20scale%2C%20and%20texture%0Adiversities.%20To%20address%20this%2C%20XoFTR%20incorporates%20masked%20image%20modeling%0Apre-training%20and%20fine-tuning%20with%20pseudo-thermal%20image%20augmentation%20to%20handle%0Athe%20modality%20differences.%20Additionally%2C%20we%20introduce%20a%20refined%20matching%0Apipeline%20that%20adjusts%20for%20scale%20discrepancies%20and%20enhances%20match%20reliability%0Athrough%20sub-pixel%20level%20refinement.%20To%20validate%20our%20approach%2C%20we%20collect%20a%0Acomprehensive%20visible-thermal%20dataset%2C%20and%20show%20that%20our%20method%20outperforms%0Aexisting%20methods%20on%20many%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09692v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XoFTR%3A%20Cross-modal%20Feature%20Matching%20Transformer&entry.906535625=%C3%96nder%20Tuzcuo%C4%9Flu%20and%20Aybora%20K%C3%B6ksal%20and%20Bu%C4%9Fra%20Sofu%20and%20Sinan%20Kalkan%20and%20A.%20Ayd%C4%B1n%20Alatan&entry.1292438233=%20%20We%20introduce%2C%20XoFTR%2C%20a%20cross-modal%20cross-view%20method%20for%20local%20feature%0Amatching%20between%20thermal%20infrared%20%28TIR%29%20and%20visible%20images.%20Unlike%20visible%0Aimages%2C%20TIR%20images%20are%20less%20susceptible%20to%20adverse%20lighting%20and%20weather%0Aconditions%20but%20present%20difficulties%20in%20matching%20due%20to%20significant%20texture%20and%0Aintensity%20differences.%20Current%20hand-crafted%20and%20learning-based%20methods%20for%0Avisible-TIR%20matching%20fall%20short%20in%20handling%20viewpoint%2C%20scale%2C%20and%20texture%0Adiversities.%20To%20address%20this%2C%20XoFTR%20incorporates%20masked%20image%20modeling%0Apre-training%20and%20fine-tuning%20with%20pseudo-thermal%20image%20augmentation%20to%20handle%0Athe%20modality%20differences.%20Additionally%2C%20we%20introduce%20a%20refined%20matching%0Apipeline%20that%20adjusts%20for%20scale%20discrepancies%20and%20enhances%20match%20reliability%0Athrough%20sub-pixel%20level%20refinement.%20To%20validate%20our%20approach%2C%20we%20collect%20a%0Acomprehensive%20visible-thermal%20dataset%2C%20and%20show%20that%20our%20method%20outperforms%0Aexisting%20methods%20on%20many%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09692v1&entry.124074799=Read"},
{"title": "TTK is Getting MPI-Ready", "author": "Eve Le Guillou and Michael Will and Pierre Guillou and Jonas Lukasczyk and Pierre Fortin and Christoph Garth and Julien Tierny", "abstract": "  This system paper documents the technical foundations for the extension of\nthe Topology ToolKit (TTK) to distributed-memory parallelism with the Message\nPassing Interface (MPI). While several recent papers introduced topology-based\napproaches for distributed-memory environments, these were reporting\nexperiments obtained with tailored, mono-algorithm implementations. In\ncontrast, we describe in this paper a versatile approach (supporting both\ntriangulated domains and regular grids) for the support of topological analysis\npipelines, i.e. a sequence of topological algorithms interacting together.\nWhile developing this extension, we faced several algorithmic and software\nengineering challenges, which we document in this paper. We describe an MPI\nextension of TTK's data structure for triangulation representation and\ntraversal, a central component to the global performance and generality of\nTTK's topological implementations. We also introduce an intermediate interface\nbetween TTK and MPI, both at the global pipeline level, and at the fine-grain\nalgorithmic level. We provide a taxonomy for the distributed-memory topological\nalgorithms supported by TTK, depending on their communication needs and provide\nexamples of hybrid MPI+thread parallelizations. Performance analyses show that\nparallel efficiencies range from 20% to 80% (depending on the algorithms), and\nthat the MPI-specific preconditioning introduced by our framework induces a\nnegligible computation time overhead. We illustrate the new distributed-memory\ncapabilities of TTK with an example of advanced analysis pipeline, combining\nmultiple algorithms, run on the largest publicly available dataset we have\nfound (120 billion vertices) on a cluster with 64 nodes (for a total of 1536\ncores). Finally, we provide a roadmap for the completion of TTK's MPI\nextension, along with generic recommendations for each algorithm communication\ncategory.\n", "link": "http://arxiv.org/abs/2310.08339v2", "date": "2024-04-15", "relevancy": 2.0841, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.459}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4008}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3906}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TTK%20is%20Getting%20MPI-Ready&body=Title%3A%20TTK%20is%20Getting%20MPI-Ready%0AAuthor%3A%20Eve%20Le%20Guillou%20and%20Michael%20Will%20and%20Pierre%20Guillou%20and%20Jonas%20Lukasczyk%20and%20Pierre%20Fortin%20and%20Christoph%20Garth%20and%20Julien%20Tierny%0AAbstract%3A%20%20%20This%20system%20paper%20documents%20the%20technical%20foundations%20for%20the%20extension%20of%0Athe%20Topology%20ToolKit%20%28TTK%29%20to%20distributed-memory%20parallelism%20with%20the%20Message%0APassing%20Interface%20%28MPI%29.%20While%20several%20recent%20papers%20introduced%20topology-based%0Aapproaches%20for%20distributed-memory%20environments%2C%20these%20were%20reporting%0Aexperiments%20obtained%20with%20tailored%2C%20mono-algorithm%20implementations.%20In%0Acontrast%2C%20we%20describe%20in%20this%20paper%20a%20versatile%20approach%20%28supporting%20both%0Atriangulated%20domains%20and%20regular%20grids%29%20for%20the%20support%20of%20topological%20analysis%0Apipelines%2C%20i.e.%20a%20sequence%20of%20topological%20algorithms%20interacting%20together.%0AWhile%20developing%20this%20extension%2C%20we%20faced%20several%20algorithmic%20and%20software%0Aengineering%20challenges%2C%20which%20we%20document%20in%20this%20paper.%20We%20describe%20an%20MPI%0Aextension%20of%20TTK%27s%20data%20structure%20for%20triangulation%20representation%20and%0Atraversal%2C%20a%20central%20component%20to%20the%20global%20performance%20and%20generality%20of%0ATTK%27s%20topological%20implementations.%20We%20also%20introduce%20an%20intermediate%20interface%0Abetween%20TTK%20and%20MPI%2C%20both%20at%20the%20global%20pipeline%20level%2C%20and%20at%20the%20fine-grain%0Aalgorithmic%20level.%20We%20provide%20a%20taxonomy%20for%20the%20distributed-memory%20topological%0Aalgorithms%20supported%20by%20TTK%2C%20depending%20on%20their%20communication%20needs%20and%20provide%0Aexamples%20of%20hybrid%20MPI%2Bthread%20parallelizations.%20Performance%20analyses%20show%20that%0Aparallel%20efficiencies%20range%20from%2020%25%20to%2080%25%20%28depending%20on%20the%20algorithms%29%2C%20and%0Athat%20the%20MPI-specific%20preconditioning%20introduced%20by%20our%20framework%20induces%20a%0Anegligible%20computation%20time%20overhead.%20We%20illustrate%20the%20new%20distributed-memory%0Acapabilities%20of%20TTK%20with%20an%20example%20of%20advanced%20analysis%20pipeline%2C%20combining%0Amultiple%20algorithms%2C%20run%20on%20the%20largest%20publicly%20available%20dataset%20we%20have%0Afound%20%28120%20billion%20vertices%29%20on%20a%20cluster%20with%2064%20nodes%20%28for%20a%20total%20of%201536%0Acores%29.%20Finally%2C%20we%20provide%20a%20roadmap%20for%20the%20completion%20of%20TTK%27s%20MPI%0Aextension%2C%20along%20with%20generic%20recommendations%20for%20each%20algorithm%20communication%0Acategory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08339v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTK%20is%20Getting%20MPI-Ready&entry.906535625=Eve%20Le%20Guillou%20and%20Michael%20Will%20and%20Pierre%20Guillou%20and%20Jonas%20Lukasczyk%20and%20Pierre%20Fortin%20and%20Christoph%20Garth%20and%20Julien%20Tierny&entry.1292438233=%20%20This%20system%20paper%20documents%20the%20technical%20foundations%20for%20the%20extension%20of%0Athe%20Topology%20ToolKit%20%28TTK%29%20to%20distributed-memory%20parallelism%20with%20the%20Message%0APassing%20Interface%20%28MPI%29.%20While%20several%20recent%20papers%20introduced%20topology-based%0Aapproaches%20for%20distributed-memory%20environments%2C%20these%20were%20reporting%0Aexperiments%20obtained%20with%20tailored%2C%20mono-algorithm%20implementations.%20In%0Acontrast%2C%20we%20describe%20in%20this%20paper%20a%20versatile%20approach%20%28supporting%20both%0Atriangulated%20domains%20and%20regular%20grids%29%20for%20the%20support%20of%20topological%20analysis%0Apipelines%2C%20i.e.%20a%20sequence%20of%20topological%20algorithms%20interacting%20together.%0AWhile%20developing%20this%20extension%2C%20we%20faced%20several%20algorithmic%20and%20software%0Aengineering%20challenges%2C%20which%20we%20document%20in%20this%20paper.%20We%20describe%20an%20MPI%0Aextension%20of%20TTK%27s%20data%20structure%20for%20triangulation%20representation%20and%0Atraversal%2C%20a%20central%20component%20to%20the%20global%20performance%20and%20generality%20of%0ATTK%27s%20topological%20implementations.%20We%20also%20introduce%20an%20intermediate%20interface%0Abetween%20TTK%20and%20MPI%2C%20both%20at%20the%20global%20pipeline%20level%2C%20and%20at%20the%20fine-grain%0Aalgorithmic%20level.%20We%20provide%20a%20taxonomy%20for%20the%20distributed-memory%20topological%0Aalgorithms%20supported%20by%20TTK%2C%20depending%20on%20their%20communication%20needs%20and%20provide%0Aexamples%20of%20hybrid%20MPI%2Bthread%20parallelizations.%20Performance%20analyses%20show%20that%0Aparallel%20efficiencies%20range%20from%2020%25%20to%2080%25%20%28depending%20on%20the%20algorithms%29%2C%20and%0Athat%20the%20MPI-specific%20preconditioning%20introduced%20by%20our%20framework%20induces%20a%0Anegligible%20computation%20time%20overhead.%20We%20illustrate%20the%20new%20distributed-memory%0Acapabilities%20of%20TTK%20with%20an%20example%20of%20advanced%20analysis%20pipeline%2C%20combining%0Amultiple%20algorithms%2C%20run%20on%20the%20largest%20publicly%20available%20dataset%20we%20have%0Afound%20%28120%20billion%20vertices%29%20on%20a%20cluster%20with%2064%20nodes%20%28for%20a%20total%20of%201536%0Acores%29.%20Finally%2C%20we%20provide%20a%20roadmap%20for%20the%20completion%20of%20TTK%27s%20MPI%0Aextension%2C%20along%20with%20generic%20recommendations%20for%20each%20algorithm%20communication%0Acategory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08339v2&entry.124074799=Read"},
{"title": "Hilti SLAM Challenge 2023: Benchmarking Single + Multi-session SLAM\n  across Sensor Constellations in Construction", "author": "Ashish Devadas Nair and Julien Kindle and Plamen Levchev and Davide Scaramuzza", "abstract": "  Simultaneous Localization and Mapping systems are a key enabler for\npositioning in both handheld and robotic applications. The Hilti SLAM\nChallenges organized over the past years have been successful at benchmarking\nsome of the world's best SLAM Systems with high accuracy. However, more\ncapabilities of these systems are yet to be explored, such as platform\nagnosticism across varying sensor suites and multi-session SLAM. These factors\nindirectly serve as an indicator of robustness and ease of deployment in\nreal-world applications. There exists no dataset plus benchmark combination\npublicly available, which considers these factors combined. The Hilti SLAM\nChallenge 2023 Dataset and Benchmark addresses this issue. Additionally, we\npropose a novel fiducial marker design for a pre-surveyed point on the ground\nto be observable from an off-the-shelf LiDAR mounted on a robot, and an\nalgorithm to estimate its position at mm-level accuracy. Results from the\nchallenge show an increase in overall participation, single-session SLAM\nsystems getting increasingly accurate, successfully operating across varying\nsensor suites, but relatively few participants performing multi-session SLAM.\n", "link": "http://arxiv.org/abs/2404.09765v1", "date": "2024-04-15", "relevancy": 2.0737, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5485}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5128}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4906}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hilti%20SLAM%20Challenge%202023%3A%20Benchmarking%20Single%20%2B%20Multi-session%20SLAM%0A%20%20across%20Sensor%20Constellations%20in%20Construction&body=Title%3A%20Hilti%20SLAM%20Challenge%202023%3A%20Benchmarking%20Single%20%2B%20Multi-session%20SLAM%0A%20%20across%20Sensor%20Constellations%20in%20Construction%0AAuthor%3A%20Ashish%20Devadas%20Nair%20and%20Julien%20Kindle%20and%20Plamen%20Levchev%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Simultaneous%20Localization%20and%20Mapping%20systems%20are%20a%20key%20enabler%20for%0Apositioning%20in%20both%20handheld%20and%20robotic%20applications.%20The%20Hilti%20SLAM%0AChallenges%20organized%20over%20the%20past%20years%20have%20been%20successful%20at%20benchmarking%0Asome%20of%20the%20world%27s%20best%20SLAM%20Systems%20with%20high%20accuracy.%20However%2C%20more%0Acapabilities%20of%20these%20systems%20are%20yet%20to%20be%20explored%2C%20such%20as%20platform%0Aagnosticism%20across%20varying%20sensor%20suites%20and%20multi-session%20SLAM.%20These%20factors%0Aindirectly%20serve%20as%20an%20indicator%20of%20robustness%20and%20ease%20of%20deployment%20in%0Areal-world%20applications.%20There%20exists%20no%20dataset%20plus%20benchmark%20combination%0Apublicly%20available%2C%20which%20considers%20these%20factors%20combined.%20The%20Hilti%20SLAM%0AChallenge%202023%20Dataset%20and%20Benchmark%20addresses%20this%20issue.%20Additionally%2C%20we%0Apropose%20a%20novel%20fiducial%20marker%20design%20for%20a%20pre-surveyed%20point%20on%20the%20ground%0Ato%20be%20observable%20from%20an%20off-the-shelf%20LiDAR%20mounted%20on%20a%20robot%2C%20and%20an%0Aalgorithm%20to%20estimate%20its%20position%20at%20mm-level%20accuracy.%20Results%20from%20the%0Achallenge%20show%20an%20increase%20in%20overall%20participation%2C%20single-session%20SLAM%0Asystems%20getting%20increasingly%20accurate%2C%20successfully%20operating%20across%20varying%0Asensor%20suites%2C%20but%20relatively%20few%20participants%20performing%20multi-session%20SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09765v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hilti%20SLAM%20Challenge%202023%3A%20Benchmarking%20Single%20%2B%20Multi-session%20SLAM%0A%20%20across%20Sensor%20Constellations%20in%20Construction&entry.906535625=Ashish%20Devadas%20Nair%20and%20Julien%20Kindle%20and%20Plamen%20Levchev%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Simultaneous%20Localization%20and%20Mapping%20systems%20are%20a%20key%20enabler%20for%0Apositioning%20in%20both%20handheld%20and%20robotic%20applications.%20The%20Hilti%20SLAM%0AChallenges%20organized%20over%20the%20past%20years%20have%20been%20successful%20at%20benchmarking%0Asome%20of%20the%20world%27s%20best%20SLAM%20Systems%20with%20high%20accuracy.%20However%2C%20more%0Acapabilities%20of%20these%20systems%20are%20yet%20to%20be%20explored%2C%20such%20as%20platform%0Aagnosticism%20across%20varying%20sensor%20suites%20and%20multi-session%20SLAM.%20These%20factors%0Aindirectly%20serve%20as%20an%20indicator%20of%20robustness%20and%20ease%20of%20deployment%20in%0Areal-world%20applications.%20There%20exists%20no%20dataset%20plus%20benchmark%20combination%0Apublicly%20available%2C%20which%20considers%20these%20factors%20combined.%20The%20Hilti%20SLAM%0AChallenge%202023%20Dataset%20and%20Benchmark%20addresses%20this%20issue.%20Additionally%2C%20we%0Apropose%20a%20novel%20fiducial%20marker%20design%20for%20a%20pre-surveyed%20point%20on%20the%20ground%0Ato%20be%20observable%20from%20an%20off-the-shelf%20LiDAR%20mounted%20on%20a%20robot%2C%20and%20an%0Aalgorithm%20to%20estimate%20its%20position%20at%20mm-level%20accuracy.%20Results%20from%20the%0Achallenge%20show%20an%20increase%20in%20overall%20participation%2C%20single-session%20SLAM%0Asystems%20getting%20increasingly%20accurate%2C%20successfully%20operating%20across%20varying%0Asensor%20suites%2C%20but%20relatively%20few%20participants%20performing%20multi-session%20SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09765v1&entry.124074799=Read"},
{"title": "VFLGAN: Vertical Federated Learning-based Generative Adversarial Network\n  for Vertically Partitioned Data Publication", "author": "Xun Yuan and Yang Yang and Prosanta Gope and Aryan Pasikhani and Biplab Sikdar", "abstract": "  In the current artificial intelligence (AI) era, the scale and quality of the\ndataset play a crucial role in training a high-quality AI model. However, good\ndata is not a free lunch and is always hard to access due to privacy\nregulations like the General Data Protection Regulation (GDPR). A potential\nsolution is to release a synthetic dataset with a similar distribution to that\nof the private dataset. Nevertheless, in some scenarios, it has been found that\nthe attributes needed to train an AI model belong to different parties, and\nthey cannot share the raw data for synthetic data publication due to privacy\nregulations. In PETS 2023, Xue et al. proposed the first generative adversary\nnetwork-based model, VertiGAN, for vertically partitioned data publication.\nHowever, after thoroughly investigating, we found that VertiGAN is less\neffective in preserving the correlation among the attributes of different\nparties. This article proposes a Vertical Federated Learning-based Generative\nAdversarial Network, VFLGAN, for vertically partitioned data publication to\naddress the above issues. Our experimental results show that compared with\nVertiGAN, VFLGAN significantly improves the quality of synthetic data. Taking\nthe MNIST dataset as an example, the quality of the synthetic dataset generated\nby VFLGAN is 3.2 times better than that generated by VertiGAN w.r.t. the\nFr\\'echet Distance. We also designed a more efficient and effective Gaussian\nmechanism for the proposed VFLGAN to provide the synthetic dataset with a\ndifferential privacy guarantee. On the other hand, differential privacy only\ngives the upper bound of the worst-case privacy guarantee. This article also\nproposes a practical auditing scheme that applies membership inference attacks\nto estimate privacy leakage through the synthetic dataset.\n", "link": "http://arxiv.org/abs/2404.09722v1", "date": "2024-04-15", "relevancy": 2.0683, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5319}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5135}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5036}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VFLGAN%3A%20Vertical%20Federated%20Learning-based%20Generative%20Adversarial%20Network%0A%20%20for%20Vertically%20Partitioned%20Data%20Publication&body=Title%3A%20VFLGAN%3A%20Vertical%20Federated%20Learning-based%20Generative%20Adversarial%20Network%0A%20%20for%20Vertically%20Partitioned%20Data%20Publication%0AAuthor%3A%20Xun%20Yuan%20and%20Yang%20Yang%20and%20Prosanta%20Gope%20and%20Aryan%20Pasikhani%20and%20Biplab%20Sikdar%0AAbstract%3A%20%20%20In%20the%20current%20artificial%20intelligence%20%28AI%29%20era%2C%20the%20scale%20and%20quality%20of%20the%0Adataset%20play%20a%20crucial%20role%20in%20training%20a%20high-quality%20AI%20model.%20However%2C%20good%0Adata%20is%20not%20a%20free%20lunch%20and%20is%20always%20hard%20to%20access%20due%20to%20privacy%0Aregulations%20like%20the%20General%20Data%20Protection%20Regulation%20%28GDPR%29.%20A%20potential%0Asolution%20is%20to%20release%20a%20synthetic%20dataset%20with%20a%20similar%20distribution%20to%20that%0Aof%20the%20private%20dataset.%20Nevertheless%2C%20in%20some%20scenarios%2C%20it%20has%20been%20found%20that%0Athe%20attributes%20needed%20to%20train%20an%20AI%20model%20belong%20to%20different%20parties%2C%20and%0Athey%20cannot%20share%20the%20raw%20data%20for%20synthetic%20data%20publication%20due%20to%20privacy%0Aregulations.%20In%20PETS%202023%2C%20Xue%20et%20al.%20proposed%20the%20first%20generative%20adversary%0Anetwork-based%20model%2C%20VertiGAN%2C%20for%20vertically%20partitioned%20data%20publication.%0AHowever%2C%20after%20thoroughly%20investigating%2C%20we%20found%20that%20VertiGAN%20is%20less%0Aeffective%20in%20preserving%20the%20correlation%20among%20the%20attributes%20of%20different%0Aparties.%20This%20article%20proposes%20a%20Vertical%20Federated%20Learning-based%20Generative%0AAdversarial%20Network%2C%20VFLGAN%2C%20for%20vertically%20partitioned%20data%20publication%20to%0Aaddress%20the%20above%20issues.%20Our%20experimental%20results%20show%20that%20compared%20with%0AVertiGAN%2C%20VFLGAN%20significantly%20improves%20the%20quality%20of%20synthetic%20data.%20Taking%0Athe%20MNIST%20dataset%20as%20an%20example%2C%20the%20quality%20of%20the%20synthetic%20dataset%20generated%0Aby%20VFLGAN%20is%203.2%20times%20better%20than%20that%20generated%20by%20VertiGAN%20w.r.t.%20the%0AFr%5C%27echet%20Distance.%20We%20also%20designed%20a%20more%20efficient%20and%20effective%20Gaussian%0Amechanism%20for%20the%20proposed%20VFLGAN%20to%20provide%20the%20synthetic%20dataset%20with%20a%0Adifferential%20privacy%20guarantee.%20On%20the%20other%20hand%2C%20differential%20privacy%20only%0Agives%20the%20upper%20bound%20of%20the%20worst-case%20privacy%20guarantee.%20This%20article%20also%0Aproposes%20a%20practical%20auditing%20scheme%20that%20applies%20membership%20inference%20attacks%0Ato%20estimate%20privacy%20leakage%20through%20the%20synthetic%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09722v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VFLGAN%3A%20Vertical%20Federated%20Learning-based%20Generative%20Adversarial%20Network%0A%20%20for%20Vertically%20Partitioned%20Data%20Publication&entry.906535625=Xun%20Yuan%20and%20Yang%20Yang%20and%20Prosanta%20Gope%20and%20Aryan%20Pasikhani%20and%20Biplab%20Sikdar&entry.1292438233=%20%20In%20the%20current%20artificial%20intelligence%20%28AI%29%20era%2C%20the%20scale%20and%20quality%20of%20the%0Adataset%20play%20a%20crucial%20role%20in%20training%20a%20high-quality%20AI%20model.%20However%2C%20good%0Adata%20is%20not%20a%20free%20lunch%20and%20is%20always%20hard%20to%20access%20due%20to%20privacy%0Aregulations%20like%20the%20General%20Data%20Protection%20Regulation%20%28GDPR%29.%20A%20potential%0Asolution%20is%20to%20release%20a%20synthetic%20dataset%20with%20a%20similar%20distribution%20to%20that%0Aof%20the%20private%20dataset.%20Nevertheless%2C%20in%20some%20scenarios%2C%20it%20has%20been%20found%20that%0Athe%20attributes%20needed%20to%20train%20an%20AI%20model%20belong%20to%20different%20parties%2C%20and%0Athey%20cannot%20share%20the%20raw%20data%20for%20synthetic%20data%20publication%20due%20to%20privacy%0Aregulations.%20In%20PETS%202023%2C%20Xue%20et%20al.%20proposed%20the%20first%20generative%20adversary%0Anetwork-based%20model%2C%20VertiGAN%2C%20for%20vertically%20partitioned%20data%20publication.%0AHowever%2C%20after%20thoroughly%20investigating%2C%20we%20found%20that%20VertiGAN%20is%20less%0Aeffective%20in%20preserving%20the%20correlation%20among%20the%20attributes%20of%20different%0Aparties.%20This%20article%20proposes%20a%20Vertical%20Federated%20Learning-based%20Generative%0AAdversarial%20Network%2C%20VFLGAN%2C%20for%20vertically%20partitioned%20data%20publication%20to%0Aaddress%20the%20above%20issues.%20Our%20experimental%20results%20show%20that%20compared%20with%0AVertiGAN%2C%20VFLGAN%20significantly%20improves%20the%20quality%20of%20synthetic%20data.%20Taking%0Athe%20MNIST%20dataset%20as%20an%20example%2C%20the%20quality%20of%20the%20synthetic%20dataset%20generated%0Aby%20VFLGAN%20is%203.2%20times%20better%20than%20that%20generated%20by%20VertiGAN%20w.r.t.%20the%0AFr%5C%27echet%20Distance.%20We%20also%20designed%20a%20more%20efficient%20and%20effective%20Gaussian%0Amechanism%20for%20the%20proposed%20VFLGAN%20to%20provide%20the%20synthetic%20dataset%20with%20a%0Adifferential%20privacy%20guarantee.%20On%20the%20other%20hand%2C%20differential%20privacy%20only%0Agives%20the%20upper%20bound%20of%20the%20worst-case%20privacy%20guarantee.%20This%20article%20also%0Aproposes%20a%20practical%20auditing%20scheme%20that%20applies%20membership%20inference%20attacks%0Ato%20estimate%20privacy%20leakage%20through%20the%20synthetic%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09722v1&entry.124074799=Read"},
{"title": "AntDT: A Self-Adaptive Distributed Training Framework for Leader and\n  Straggler Nodes", "author": "Youshao Xiao and Lin Ju and Zhenglei Zhou and Siyuan Li and Zhaoxin Huan and Dalong Zhang and Rujie Jiang and Lin Wang and Xiaolu Zhang and Lei Liang and Jun Zhou", "abstract": "  Many distributed training techniques like Parameter Server and AllReduce have\nbeen proposed to take advantage of the increasingly large data and rich\nfeatures. However, stragglers frequently occur in distributed training due to\nresource contention and hardware heterogeneity, which significantly hampers the\ntraining efficiency. Previous works only address part of the stragglers and\ncould not adaptively solve various stragglers in practice. Additionally, it is\nchallenging to use a systematic framework to address all stragglers because\ndifferent stragglers require diverse data allocation and fault-tolerance\nmechanisms. Therefore, this paper proposes a unified distributed training\nframework called AntDT (Ant Distributed Training Framework) to adaptively solve\nthe straggler problems. Firstly, the framework consists of four components,\nincluding the Stateful Dynamic Data Sharding service, Monitor, Controller, and\nAgent. These components work collaboratively to efficiently distribute\nworkloads and provide a range of pre-defined straggler mitigation methods with\nfault tolerance, thereby hiding messy details of data allocation and fault\nhandling. Secondly, the framework provides a high degree of flexibility,\nallowing for the customization of straggler mitigation solutions based on the\nspecific circumstances of the cluster. Leveraging this flexibility, we\nintroduce two straggler mitigation solutions, namely AntDT-ND for non-dedicated\nclusters and AntDT-DD for dedicated clusters, as practical examples to resolve\nvarious types of stragglers at Ant Group. Justified by our comprehensive\nexperiments and industrial deployment statistics, AntDT outperforms other SOTA\nmethods more than 3x in terms of training efficiency. Additionally, in Alipay's\nhomepage recommendation scenario, using AntDT reduces the training duration of\nthe ranking model from 27.8 hours to just 5.4 hours.\n", "link": "http://arxiv.org/abs/2404.09679v1", "date": "2024-04-15", "relevancy": 2.0339, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5394}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4887}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4806}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AntDT%3A%20A%20Self-Adaptive%20Distributed%20Training%20Framework%20for%20Leader%20and%0A%20%20Straggler%20Nodes&body=Title%3A%20AntDT%3A%20A%20Self-Adaptive%20Distributed%20Training%20Framework%20for%20Leader%20and%0A%20%20Straggler%20Nodes%0AAuthor%3A%20Youshao%20Xiao%20and%20Lin%20Ju%20and%20Zhenglei%20Zhou%20and%20Siyuan%20Li%20and%20Zhaoxin%20Huan%20and%20Dalong%20Zhang%20and%20Rujie%20Jiang%20and%20Lin%20Wang%20and%20Xiaolu%20Zhang%20and%20Lei%20Liang%20and%20Jun%20Zhou%0AAbstract%3A%20%20%20Many%20distributed%20training%20techniques%20like%20Parameter%20Server%20and%20AllReduce%20have%0Abeen%20proposed%20to%20take%20advantage%20of%20the%20increasingly%20large%20data%20and%20rich%0Afeatures.%20However%2C%20stragglers%20frequently%20occur%20in%20distributed%20training%20due%20to%0Aresource%20contention%20and%20hardware%20heterogeneity%2C%20which%20significantly%20hampers%20the%0Atraining%20efficiency.%20Previous%20works%20only%20address%20part%20of%20the%20stragglers%20and%0Acould%20not%20adaptively%20solve%20various%20stragglers%20in%20practice.%20Additionally%2C%20it%20is%0Achallenging%20to%20use%20a%20systematic%20framework%20to%20address%20all%20stragglers%20because%0Adifferent%20stragglers%20require%20diverse%20data%20allocation%20and%20fault-tolerance%0Amechanisms.%20Therefore%2C%20this%20paper%20proposes%20a%20unified%20distributed%20training%0Aframework%20called%20AntDT%20%28Ant%20Distributed%20Training%20Framework%29%20to%20adaptively%20solve%0Athe%20straggler%20problems.%20Firstly%2C%20the%20framework%20consists%20of%20four%20components%2C%0Aincluding%20the%20Stateful%20Dynamic%20Data%20Sharding%20service%2C%20Monitor%2C%20Controller%2C%20and%0AAgent.%20These%20components%20work%20collaboratively%20to%20efficiently%20distribute%0Aworkloads%20and%20provide%20a%20range%20of%20pre-defined%20straggler%20mitigation%20methods%20with%0Afault%20tolerance%2C%20thereby%20hiding%20messy%20details%20of%20data%20allocation%20and%20fault%0Ahandling.%20Secondly%2C%20the%20framework%20provides%20a%20high%20degree%20of%20flexibility%2C%0Aallowing%20for%20the%20customization%20of%20straggler%20mitigation%20solutions%20based%20on%20the%0Aspecific%20circumstances%20of%20the%20cluster.%20Leveraging%20this%20flexibility%2C%20we%0Aintroduce%20two%20straggler%20mitigation%20solutions%2C%20namely%20AntDT-ND%20for%20non-dedicated%0Aclusters%20and%20AntDT-DD%20for%20dedicated%20clusters%2C%20as%20practical%20examples%20to%20resolve%0Avarious%20types%20of%20stragglers%20at%20Ant%20Group.%20Justified%20by%20our%20comprehensive%0Aexperiments%20and%20industrial%20deployment%20statistics%2C%20AntDT%20outperforms%20other%20SOTA%0Amethods%20more%20than%203x%20in%20terms%20of%20training%20efficiency.%20Additionally%2C%20in%20Alipay%27s%0Ahomepage%20recommendation%20scenario%2C%20using%20AntDT%20reduces%20the%20training%20duration%20of%0Athe%20ranking%20model%20from%2027.8%20hours%20to%20just%205.4%20hours.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09679v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AntDT%3A%20A%20Self-Adaptive%20Distributed%20Training%20Framework%20for%20Leader%20and%0A%20%20Straggler%20Nodes&entry.906535625=Youshao%20Xiao%20and%20Lin%20Ju%20and%20Zhenglei%20Zhou%20and%20Siyuan%20Li%20and%20Zhaoxin%20Huan%20and%20Dalong%20Zhang%20and%20Rujie%20Jiang%20and%20Lin%20Wang%20and%20Xiaolu%20Zhang%20and%20Lei%20Liang%20and%20Jun%20Zhou&entry.1292438233=%20%20Many%20distributed%20training%20techniques%20like%20Parameter%20Server%20and%20AllReduce%20have%0Abeen%20proposed%20to%20take%20advantage%20of%20the%20increasingly%20large%20data%20and%20rich%0Afeatures.%20However%2C%20stragglers%20frequently%20occur%20in%20distributed%20training%20due%20to%0Aresource%20contention%20and%20hardware%20heterogeneity%2C%20which%20significantly%20hampers%20the%0Atraining%20efficiency.%20Previous%20works%20only%20address%20part%20of%20the%20stragglers%20and%0Acould%20not%20adaptively%20solve%20various%20stragglers%20in%20practice.%20Additionally%2C%20it%20is%0Achallenging%20to%20use%20a%20systematic%20framework%20to%20address%20all%20stragglers%20because%0Adifferent%20stragglers%20require%20diverse%20data%20allocation%20and%20fault-tolerance%0Amechanisms.%20Therefore%2C%20this%20paper%20proposes%20a%20unified%20distributed%20training%0Aframework%20called%20AntDT%20%28Ant%20Distributed%20Training%20Framework%29%20to%20adaptively%20solve%0Athe%20straggler%20problems.%20Firstly%2C%20the%20framework%20consists%20of%20four%20components%2C%0Aincluding%20the%20Stateful%20Dynamic%20Data%20Sharding%20service%2C%20Monitor%2C%20Controller%2C%20and%0AAgent.%20These%20components%20work%20collaboratively%20to%20efficiently%20distribute%0Aworkloads%20and%20provide%20a%20range%20of%20pre-defined%20straggler%20mitigation%20methods%20with%0Afault%20tolerance%2C%20thereby%20hiding%20messy%20details%20of%20data%20allocation%20and%20fault%0Ahandling.%20Secondly%2C%20the%20framework%20provides%20a%20high%20degree%20of%20flexibility%2C%0Aallowing%20for%20the%20customization%20of%20straggler%20mitigation%20solutions%20based%20on%20the%0Aspecific%20circumstances%20of%20the%20cluster.%20Leveraging%20this%20flexibility%2C%20we%0Aintroduce%20two%20straggler%20mitigation%20solutions%2C%20namely%20AntDT-ND%20for%20non-dedicated%0Aclusters%20and%20AntDT-DD%20for%20dedicated%20clusters%2C%20as%20practical%20examples%20to%20resolve%0Avarious%20types%20of%20stragglers%20at%20Ant%20Group.%20Justified%20by%20our%20comprehensive%0Aexperiments%20and%20industrial%20deployment%20statistics%2C%20AntDT%20outperforms%20other%20SOTA%0Amethods%20more%20than%203x%20in%20terms%20of%20training%20efficiency.%20Additionally%2C%20in%20Alipay%27s%0Ahomepage%20recommendation%20scenario%2C%20using%20AntDT%20reduces%20the%20training%20duration%20of%0Athe%20ranking%20model%20from%2027.8%20hours%20to%20just%205.4%20hours.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09679v1&entry.124074799=Read"},
{"title": "Can We Break Free from Strong Data Augmentations in Self-Supervised\n  Learning?", "author": "Shruthi Gowda and Elahe Arani and Bahram Zonooz", "abstract": "  Self-supervised learning (SSL) has emerged as a promising solution for\naddressing the challenge of limited labeled data in deep neural networks\n(DNNs), offering scalability potential. However, the impact of design\ndependencies within the SSL framework remains insufficiently investigated. In\nthis study, we comprehensively explore SSL behavior across a spectrum of\naugmentations, revealing their crucial role in shaping SSL model performance\nand learning mechanisms. Leveraging these insights, we propose a novel learning\napproach that integrates prior knowledge, with the aim of curtailing the need\nfor extensive data augmentations and thereby amplifying the efficacy of learned\nrepresentations. Notably, our findings underscore that SSL models imbued with\nprior knowledge exhibit reduced texture bias, diminished reliance on shortcuts\nand augmentations, and improved robustness against both natural and adversarial\ncorruptions. These findings not only illuminate a new direction in SSL\nresearch, but also pave the way for enhancing DNN performance while\nconcurrently alleviating the imperative for intensive data augmentation,\nthereby enhancing scalability and real-world problem-solving capabilities.\n", "link": "http://arxiv.org/abs/2404.09752v1", "date": "2024-04-15", "relevancy": 2.0062, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5395}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4799}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4723}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Can%20We%20Break%20Free%20from%20Strong%20Data%20Augmentations%20in%20Self-Supervised%0A%20%20Learning%3F&body=Title%3A%20Can%20We%20Break%20Free%20from%20Strong%20Data%20Augmentations%20in%20Self-Supervised%0A%20%20Learning%3F%0AAuthor%3A%20Shruthi%20Gowda%20and%20Elahe%20Arani%20and%20Bahram%20Zonooz%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20promising%20solution%20for%0Aaddressing%20the%20challenge%20of%20limited%20labeled%20data%20in%20deep%20neural%20networks%0A%28DNNs%29%2C%20offering%20scalability%20potential.%20However%2C%20the%20impact%20of%20design%0Adependencies%20within%20the%20SSL%20framework%20remains%20insufficiently%20investigated.%20In%0Athis%20study%2C%20we%20comprehensively%20explore%20SSL%20behavior%20across%20a%20spectrum%20of%0Aaugmentations%2C%20revealing%20their%20crucial%20role%20in%20shaping%20SSL%20model%20performance%0Aand%20learning%20mechanisms.%20Leveraging%20these%20insights%2C%20we%20propose%20a%20novel%20learning%0Aapproach%20that%20integrates%20prior%20knowledge%2C%20with%20the%20aim%20of%20curtailing%20the%20need%0Afor%20extensive%20data%20augmentations%20and%20thereby%20amplifying%20the%20efficacy%20of%20learned%0Arepresentations.%20Notably%2C%20our%20findings%20underscore%20that%20SSL%20models%20imbued%20with%0Aprior%20knowledge%20exhibit%20reduced%20texture%20bias%2C%20diminished%20reliance%20on%20shortcuts%0Aand%20augmentations%2C%20and%20improved%20robustness%20against%20both%20natural%20and%20adversarial%0Acorruptions.%20These%20findings%20not%20only%20illuminate%20a%20new%20direction%20in%20SSL%0Aresearch%2C%20but%20also%20pave%20the%20way%20for%20enhancing%20DNN%20performance%20while%0Aconcurrently%20alleviating%20the%20imperative%20for%20intensive%20data%20augmentation%2C%0Athereby%20enhancing%20scalability%20and%20real-world%20problem-solving%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09752v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20We%20Break%20Free%20from%20Strong%20Data%20Augmentations%20in%20Self-Supervised%0A%20%20Learning%3F&entry.906535625=Shruthi%20Gowda%20and%20Elahe%20Arani%20and%20Bahram%20Zonooz&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20promising%20solution%20for%0Aaddressing%20the%20challenge%20of%20limited%20labeled%20data%20in%20deep%20neural%20networks%0A%28DNNs%29%2C%20offering%20scalability%20potential.%20However%2C%20the%20impact%20of%20design%0Adependencies%20within%20the%20SSL%20framework%20remains%20insufficiently%20investigated.%20In%0Athis%20study%2C%20we%20comprehensively%20explore%20SSL%20behavior%20across%20a%20spectrum%20of%0Aaugmentations%2C%20revealing%20their%20crucial%20role%20in%20shaping%20SSL%20model%20performance%0Aand%20learning%20mechanisms.%20Leveraging%20these%20insights%2C%20we%20propose%20a%20novel%20learning%0Aapproach%20that%20integrates%20prior%20knowledge%2C%20with%20the%20aim%20of%20curtailing%20the%20need%0Afor%20extensive%20data%20augmentations%20and%20thereby%20amplifying%20the%20efficacy%20of%20learned%0Arepresentations.%20Notably%2C%20our%20findings%20underscore%20that%20SSL%20models%20imbued%20with%0Aprior%20knowledge%20exhibit%20reduced%20texture%20bias%2C%20diminished%20reliance%20on%20shortcuts%0Aand%20augmentations%2C%20and%20improved%20robustness%20against%20both%20natural%20and%20adversarial%0Acorruptions.%20These%20findings%20not%20only%20illuminate%20a%20new%20direction%20in%20SSL%0Aresearch%2C%20but%20also%20pave%20the%20way%20for%20enhancing%20DNN%20performance%20while%0Aconcurrently%20alleviating%20the%20imperative%20for%20intensive%20data%20augmentation%2C%0Athereby%20enhancing%20scalability%20and%20real-world%20problem-solving%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09752v1&entry.124074799=Read"},
{"title": "Kernel-based learning with guarantees for multi-agent applications", "author": "Krzysztof Kowalczyk and Pawe\u0142 Wachel and Cristian R. Rojas", "abstract": "  This paper addresses a kernel-based learning problem for a network of agents\nlocally observing a latent multidimensional, nonlinear phenomenon in a noisy\nenvironment. We propose a learning algorithm that requires only mild a priori\nknowledge about the phenomenon under investigation and delivers a model with\ncorresponding non-asymptotic high probability error bounds. Both non-asymptotic\nanalysis of the method and numerical simulation results are presented and\ndiscussed in the paper.\n", "link": "http://arxiv.org/abs/2404.09708v1", "date": "2024-04-15", "relevancy": 1.9988, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.529}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4999}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4878}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Kernel-based%20learning%20with%20guarantees%20for%20multi-agent%20applications&body=Title%3A%20Kernel-based%20learning%20with%20guarantees%20for%20multi-agent%20applications%0AAuthor%3A%20Krzysztof%20Kowalczyk%20and%20Pawe%C5%82%20Wachel%20and%20Cristian%20R.%20Rojas%0AAbstract%3A%20%20%20This%20paper%20addresses%20a%20kernel-based%20learning%20problem%20for%20a%20network%20of%20agents%0Alocally%20observing%20a%20latent%20multidimensional%2C%20nonlinear%20phenomenon%20in%20a%20noisy%0Aenvironment.%20We%20propose%20a%20learning%20algorithm%20that%20requires%20only%20mild%20a%20priori%0Aknowledge%20about%20the%20phenomenon%20under%20investigation%20and%20delivers%20a%20model%20with%0Acorresponding%20non-asymptotic%20high%20probability%20error%20bounds.%20Both%20non-asymptotic%0Aanalysis%20of%20the%20method%20and%20numerical%20simulation%20results%20are%20presented%20and%0Adiscussed%20in%20the%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09708v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel-based%20learning%20with%20guarantees%20for%20multi-agent%20applications&entry.906535625=Krzysztof%20Kowalczyk%20and%20Pawe%C5%82%20Wachel%20and%20Cristian%20R.%20Rojas&entry.1292438233=%20%20This%20paper%20addresses%20a%20kernel-based%20learning%20problem%20for%20a%20network%20of%20agents%0Alocally%20observing%20a%20latent%20multidimensional%2C%20nonlinear%20phenomenon%20in%20a%20noisy%0Aenvironment.%20We%20propose%20a%20learning%20algorithm%20that%20requires%20only%20mild%20a%20priori%0Aknowledge%20about%20the%20phenomenon%20under%20investigation%20and%20delivers%20a%20model%20with%0Acorresponding%20non-asymptotic%20high%20probability%20error%20bounds.%20Both%20non-asymptotic%0Aanalysis%20of%20the%20method%20and%20numerical%20simulation%20results%20are%20presented%20and%0Adiscussed%20in%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09708v1&entry.124074799=Read"},
{"title": "Deformable MRI Sequence Registration for AI-based Prostate Cancer\n  Diagnosis", "author": "Alessa Hering and Sarah de Boer and Anindo Saha and Jasper J. Twilt and Derya Yakar and Maarten de Rooij and Henkjan Huisman and Joeran S. Bosma", "abstract": "  The PI-CAI (Prostate Imaging: Cancer AI) challenge led to expert-level\ndiagnostic algorithms for clinically significant prostate cancer detection. The\nalgorithms receive biparametric MRI scans as input, which consist of\nT2-weighted and diffusion-weighted scans. These scans can be misaligned due to\nmultiple factors in the scanning process. Image registration can alleviate this\nissue by predicting the deformation between the sequences. We investigate the\neffect of image registration on the diagnostic performance of AI-based prostate\ncancer diagnosis. First, the image registration algorithm, developed in\nMeVisLab, is analyzed using a dataset with paired lesion annotations. Second,\nthe effect on diagnosis is evaluated by comparing case-level cancer diagnosis\nperformance between using the original dataset, rigidly aligned\ndiffusion-weighted scans, or deformably aligned diffusion-weighted scans. Rigid\nregistration showed no improvement. Deformable registration demonstrated a\nsubstantial improvement in lesion overlap (+10% median Dice score) and a\npositive yet non-significant improvement in diagnostic performance (+0.3%\nAUROC, p=0.18). Our investigation shows that a substantial improvement in\nlesion alignment does not directly lead to a significant improvement in\ndiagnostic performance. Qualitative analysis indicated that jointly developing\nimage registration methods and diagnostic AI algorithms could enhance\ndiagnostic accuracy and patient outcomes.\n", "link": "http://arxiv.org/abs/2404.09666v1", "date": "2024-04-15", "relevancy": 1.9881, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5067}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4967}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4935}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deformable%20MRI%20Sequence%20Registration%20for%20AI-based%20Prostate%20Cancer%0A%20%20Diagnosis&body=Title%3A%20Deformable%20MRI%20Sequence%20Registration%20for%20AI-based%20Prostate%20Cancer%0A%20%20Diagnosis%0AAuthor%3A%20Alessa%20Hering%20and%20Sarah%20de%20Boer%20and%20Anindo%20Saha%20and%20Jasper%20J.%20Twilt%20and%20Derya%20Yakar%20and%20Maarten%20de%20Rooij%20and%20Henkjan%20Huisman%20and%20Joeran%20S.%20Bosma%0AAbstract%3A%20%20%20The%20PI-CAI%20%28Prostate%20Imaging%3A%20Cancer%20AI%29%20challenge%20led%20to%20expert-level%0Adiagnostic%20algorithms%20for%20clinically%20significant%20prostate%20cancer%20detection.%20The%0Aalgorithms%20receive%20biparametric%20MRI%20scans%20as%20input%2C%20which%20consist%20of%0AT2-weighted%20and%20diffusion-weighted%20scans.%20These%20scans%20can%20be%20misaligned%20due%20to%0Amultiple%20factors%20in%20the%20scanning%20process.%20Image%20registration%20can%20alleviate%20this%0Aissue%20by%20predicting%20the%20deformation%20between%20the%20sequences.%20We%20investigate%20the%0Aeffect%20of%20image%20registration%20on%20the%20diagnostic%20performance%20of%20AI-based%20prostate%0Acancer%20diagnosis.%20First%2C%20the%20image%20registration%20algorithm%2C%20developed%20in%0AMeVisLab%2C%20is%20analyzed%20using%20a%20dataset%20with%20paired%20lesion%20annotations.%20Second%2C%0Athe%20effect%20on%20diagnosis%20is%20evaluated%20by%20comparing%20case-level%20cancer%20diagnosis%0Aperformance%20between%20using%20the%20original%20dataset%2C%20rigidly%20aligned%0Adiffusion-weighted%20scans%2C%20or%20deformably%20aligned%20diffusion-weighted%20scans.%20Rigid%0Aregistration%20showed%20no%20improvement.%20Deformable%20registration%20demonstrated%20a%0Asubstantial%20improvement%20in%20lesion%20overlap%20%28%2B10%25%20median%20Dice%20score%29%20and%20a%0Apositive%20yet%20non-significant%20improvement%20in%20diagnostic%20performance%20%28%2B0.3%25%0AAUROC%2C%20p%3D0.18%29.%20Our%20investigation%20shows%20that%20a%20substantial%20improvement%20in%0Alesion%20alignment%20does%20not%20directly%20lead%20to%20a%20significant%20improvement%20in%0Adiagnostic%20performance.%20Qualitative%20analysis%20indicated%20that%20jointly%20developing%0Aimage%20registration%20methods%20and%20diagnostic%20AI%20algorithms%20could%20enhance%0Adiagnostic%20accuracy%20and%20patient%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09666v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deformable%20MRI%20Sequence%20Registration%20for%20AI-based%20Prostate%20Cancer%0A%20%20Diagnosis&entry.906535625=Alessa%20Hering%20and%20Sarah%20de%20Boer%20and%20Anindo%20Saha%20and%20Jasper%20J.%20Twilt%20and%20Derya%20Yakar%20and%20Maarten%20de%20Rooij%20and%20Henkjan%20Huisman%20and%20Joeran%20S.%20Bosma&entry.1292438233=%20%20The%20PI-CAI%20%28Prostate%20Imaging%3A%20Cancer%20AI%29%20challenge%20led%20to%20expert-level%0Adiagnostic%20algorithms%20for%20clinically%20significant%20prostate%20cancer%20detection.%20The%0Aalgorithms%20receive%20biparametric%20MRI%20scans%20as%20input%2C%20which%20consist%20of%0AT2-weighted%20and%20diffusion-weighted%20scans.%20These%20scans%20can%20be%20misaligned%20due%20to%0Amultiple%20factors%20in%20the%20scanning%20process.%20Image%20registration%20can%20alleviate%20this%0Aissue%20by%20predicting%20the%20deformation%20between%20the%20sequences.%20We%20investigate%20the%0Aeffect%20of%20image%20registration%20on%20the%20diagnostic%20performance%20of%20AI-based%20prostate%0Acancer%20diagnosis.%20First%2C%20the%20image%20registration%20algorithm%2C%20developed%20in%0AMeVisLab%2C%20is%20analyzed%20using%20a%20dataset%20with%20paired%20lesion%20annotations.%20Second%2C%0Athe%20effect%20on%20diagnosis%20is%20evaluated%20by%20comparing%20case-level%20cancer%20diagnosis%0Aperformance%20between%20using%20the%20original%20dataset%2C%20rigidly%20aligned%0Adiffusion-weighted%20scans%2C%20or%20deformably%20aligned%20diffusion-weighted%20scans.%20Rigid%0Aregistration%20showed%20no%20improvement.%20Deformable%20registration%20demonstrated%20a%0Asubstantial%20improvement%20in%20lesion%20overlap%20%28%2B10%25%20median%20Dice%20score%29%20and%20a%0Apositive%20yet%20non-significant%20improvement%20in%20diagnostic%20performance%20%28%2B0.3%25%0AAUROC%2C%20p%3D0.18%29.%20Our%20investigation%20shows%20that%20a%20substantial%20improvement%20in%0Alesion%20alignment%20does%20not%20directly%20lead%20to%20a%20significant%20improvement%20in%0Adiagnostic%20performance.%20Qualitative%20analysis%20indicated%20that%20jointly%20developing%0Aimage%20registration%20methods%20and%20diagnostic%20AI%20algorithms%20could%20enhance%0Adiagnostic%20accuracy%20and%20patient%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09666v1&entry.124074799=Read"},
{"title": "Harnessing GPT-4V(ision) for Insurance: A Preliminary Exploration", "author": "Chenwei Lin and Hanjia Lyu and Jiebo Luo and Xian Xu", "abstract": "  The emergence of Large Multimodal Models (LMMs) marks a significant milestone\nin the development of artificial intelligence. Insurance, as a vast and complex\ndiscipline, involves a wide variety of data forms in its operational processes,\nincluding text, images, and videos, thereby giving rise to diverse multimodal\ntasks. Despite this, there has been limited systematic exploration of\nmultimodal tasks specific to insurance, nor a thorough investigation into how\nLMMs can address these challenges. In this paper, we explore GPT-4V's\ncapabilities in the insurance domain. We categorize multimodal tasks by\nfocusing primarily on visual aspects based on types of insurance (e.g., auto,\nhousehold/commercial property, health, and agricultural insurance) and\ninsurance stages (e.g., risk assessment, risk monitoring, and claims\nprocessing). Our experiment reveals that GPT-4V exhibits remarkable abilities\nin insurance-related tasks, demonstrating not only a robust understanding of\nmultimodal content in the insurance domain but also a comprehensive knowledge\nof insurance scenarios. However, there are notable shortcomings: GPT-4V\nstruggles with detailed risk rating and loss assessment, suffers from\nhallucination in image understanding, and shows variable support for different\nlanguages. Through this work, we aim to bridge the insurance domain with\ncutting-edge LMM technology, facilitate interdisciplinary exchange and\ndevelopment, and provide a foundation for the continued advancement and\nevolution of future research endeavors.\n", "link": "http://arxiv.org/abs/2404.09690v1", "date": "2024-04-15", "relevancy": 1.9851, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5452}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4876}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4854}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Harnessing%20GPT-4V%28ision%29%20for%20Insurance%3A%20A%20Preliminary%20Exploration&body=Title%3A%20Harnessing%20GPT-4V%28ision%29%20for%20Insurance%3A%20A%20Preliminary%20Exploration%0AAuthor%3A%20Chenwei%20Lin%20and%20Hanjia%20Lyu%20and%20Jiebo%20Luo%20and%20Xian%20Xu%0AAbstract%3A%20%20%20The%20emergence%20of%20Large%20Multimodal%20Models%20%28LMMs%29%20marks%20a%20significant%20milestone%0Ain%20the%20development%20of%20artificial%20intelligence.%20Insurance%2C%20as%20a%20vast%20and%20complex%0Adiscipline%2C%20involves%20a%20wide%20variety%20of%20data%20forms%20in%20its%20operational%20processes%2C%0Aincluding%20text%2C%20images%2C%20and%20videos%2C%20thereby%20giving%20rise%20to%20diverse%20multimodal%0Atasks.%20Despite%20this%2C%20there%20has%20been%20limited%20systematic%20exploration%20of%0Amultimodal%20tasks%20specific%20to%20insurance%2C%20nor%20a%20thorough%20investigation%20into%20how%0ALMMs%20can%20address%20these%20challenges.%20In%20this%20paper%2C%20we%20explore%20GPT-4V%27s%0Acapabilities%20in%20the%20insurance%20domain.%20We%20categorize%20multimodal%20tasks%20by%0Afocusing%20primarily%20on%20visual%20aspects%20based%20on%20types%20of%20insurance%20%28e.g.%2C%20auto%2C%0Ahousehold/commercial%20property%2C%20health%2C%20and%20agricultural%20insurance%29%20and%0Ainsurance%20stages%20%28e.g.%2C%20risk%20assessment%2C%20risk%20monitoring%2C%20and%20claims%0Aprocessing%29.%20Our%20experiment%20reveals%20that%20GPT-4V%20exhibits%20remarkable%20abilities%0Ain%20insurance-related%20tasks%2C%20demonstrating%20not%20only%20a%20robust%20understanding%20of%0Amultimodal%20content%20in%20the%20insurance%20domain%20but%20also%20a%20comprehensive%20knowledge%0Aof%20insurance%20scenarios.%20However%2C%20there%20are%20notable%20shortcomings%3A%20GPT-4V%0Astruggles%20with%20detailed%20risk%20rating%20and%20loss%20assessment%2C%20suffers%20from%0Ahallucination%20in%20image%20understanding%2C%20and%20shows%20variable%20support%20for%20different%0Alanguages.%20Through%20this%20work%2C%20we%20aim%20to%20bridge%20the%20insurance%20domain%20with%0Acutting-edge%20LMM%20technology%2C%20facilitate%20interdisciplinary%20exchange%20and%0Adevelopment%2C%20and%20provide%20a%20foundation%20for%20the%20continued%20advancement%20and%0Aevolution%20of%20future%20research%20endeavors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09690v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20GPT-4V%28ision%29%20for%20Insurance%3A%20A%20Preliminary%20Exploration&entry.906535625=Chenwei%20Lin%20and%20Hanjia%20Lyu%20and%20Jiebo%20Luo%20and%20Xian%20Xu&entry.1292438233=%20%20The%20emergence%20of%20Large%20Multimodal%20Models%20%28LMMs%29%20marks%20a%20significant%20milestone%0Ain%20the%20development%20of%20artificial%20intelligence.%20Insurance%2C%20as%20a%20vast%20and%20complex%0Adiscipline%2C%20involves%20a%20wide%20variety%20of%20data%20forms%20in%20its%20operational%20processes%2C%0Aincluding%20text%2C%20images%2C%20and%20videos%2C%20thereby%20giving%20rise%20to%20diverse%20multimodal%0Atasks.%20Despite%20this%2C%20there%20has%20been%20limited%20systematic%20exploration%20of%0Amultimodal%20tasks%20specific%20to%20insurance%2C%20nor%20a%20thorough%20investigation%20into%20how%0ALMMs%20can%20address%20these%20challenges.%20In%20this%20paper%2C%20we%20explore%20GPT-4V%27s%0Acapabilities%20in%20the%20insurance%20domain.%20We%20categorize%20multimodal%20tasks%20by%0Afocusing%20primarily%20on%20visual%20aspects%20based%20on%20types%20of%20insurance%20%28e.g.%2C%20auto%2C%0Ahousehold/commercial%20property%2C%20health%2C%20and%20agricultural%20insurance%29%20and%0Ainsurance%20stages%20%28e.g.%2C%20risk%20assessment%2C%20risk%20monitoring%2C%20and%20claims%0Aprocessing%29.%20Our%20experiment%20reveals%20that%20GPT-4V%20exhibits%20remarkable%20abilities%0Ain%20insurance-related%20tasks%2C%20demonstrating%20not%20only%20a%20robust%20understanding%20of%0Amultimodal%20content%20in%20the%20insurance%20domain%20but%20also%20a%20comprehensive%20knowledge%0Aof%20insurance%20scenarios.%20However%2C%20there%20are%20notable%20shortcomings%3A%20GPT-4V%0Astruggles%20with%20detailed%20risk%20rating%20and%20loss%20assessment%2C%20suffers%20from%0Ahallucination%20in%20image%20understanding%2C%20and%20shows%20variable%20support%20for%20different%0Alanguages.%20Through%20this%20work%2C%20we%20aim%20to%20bridge%20the%20insurance%20domain%20with%0Acutting-edge%20LMM%20technology%2C%20facilitate%20interdisciplinary%20exchange%20and%0Adevelopment%2C%20and%20provide%20a%20foundation%20for%20the%20continued%20advancement%20and%0Aevolution%20of%20future%20research%20endeavors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09690v1&entry.124074799=Read"},
{"title": "Higher Replay Ratio Empowers Sample-Efficient Multi-Agent Reinforcement\n  Learning", "author": "Linjie Xu and Zichuan Liu and Alexander Dockhorn and Diego Perez-Liebana and Jinyu Wang and Lei Song and Jiang Bian", "abstract": "  One of the notorious issues for Reinforcement Learning (RL) is poor sample\nefficiency. Compared to single agent RL, the sample efficiency for Multi-Agent\nReinforcement Learning (MARL) is more challenging because of its inherent\npartial observability, non-stationary training, and enormous strategy space.\nAlthough much effort has been devoted to developing new methods and enhancing\nsample efficiency, we look at the widely used episodic training mechanism. In\neach training step, tens of frames are collected, but only one gradient step is\nmade. We argue that this episodic training could be a source of poor sample\nefficiency. To better exploit the data already collected, we propose to\nincrease the frequency of the gradient updates per environment interaction\n(a.k.a. Replay Ratio or Update-To-Data ratio). To show its generality, we\nevaluate $3$ MARL methods on $6$ SMAC tasks. The empirical results validate\nthat a higher replay ratio significantly improves the sample efficiency for\nMARL algorithms. The codes to reimplement the results presented in this paper\nare open-sourced at https://anonymous.4open.science/r/rr_for_MARL-0D83/.\n", "link": "http://arxiv.org/abs/2404.09715v1", "date": "2024-04-15", "relevancy": 1.9691, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4994}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4933}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4847}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Higher%20Replay%20Ratio%20Empowers%20Sample-Efficient%20Multi-Agent%20Reinforcement%0A%20%20Learning&body=Title%3A%20Higher%20Replay%20Ratio%20Empowers%20Sample-Efficient%20Multi-Agent%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Linjie%20Xu%20and%20Zichuan%20Liu%20and%20Alexander%20Dockhorn%20and%20Diego%20Perez-Liebana%20and%20Jinyu%20Wang%20and%20Lei%20Song%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20One%20of%20the%20notorious%20issues%20for%20Reinforcement%20Learning%20%28RL%29%20is%20poor%20sample%0Aefficiency.%20Compared%20to%20single%20agent%20RL%2C%20the%20sample%20efficiency%20for%20Multi-Agent%0AReinforcement%20Learning%20%28MARL%29%20is%20more%20challenging%20because%20of%20its%20inherent%0Apartial%20observability%2C%20non-stationary%20training%2C%20and%20enormous%20strategy%20space.%0AAlthough%20much%20effort%20has%20been%20devoted%20to%20developing%20new%20methods%20and%20enhancing%0Asample%20efficiency%2C%20we%20look%20at%20the%20widely%20used%20episodic%20training%20mechanism.%20In%0Aeach%20training%20step%2C%20tens%20of%20frames%20are%20collected%2C%20but%20only%20one%20gradient%20step%20is%0Amade.%20We%20argue%20that%20this%20episodic%20training%20could%20be%20a%20source%20of%20poor%20sample%0Aefficiency.%20To%20better%20exploit%20the%20data%20already%20collected%2C%20we%20propose%20to%0Aincrease%20the%20frequency%20of%20the%20gradient%20updates%20per%20environment%20interaction%0A%28a.k.a.%20Replay%20Ratio%20or%20Update-To-Data%20ratio%29.%20To%20show%20its%20generality%2C%20we%0Aevaluate%20%243%24%20MARL%20methods%20on%20%246%24%20SMAC%20tasks.%20The%20empirical%20results%20validate%0Athat%20a%20higher%20replay%20ratio%20significantly%20improves%20the%20sample%20efficiency%20for%0AMARL%20algorithms.%20The%20codes%20to%20reimplement%20the%20results%20presented%20in%20this%20paper%0Aare%20open-sourced%20at%20https%3A//anonymous.4open.science/r/rr_for_MARL-0D83/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09715v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher%20Replay%20Ratio%20Empowers%20Sample-Efficient%20Multi-Agent%20Reinforcement%0A%20%20Learning&entry.906535625=Linjie%20Xu%20and%20Zichuan%20Liu%20and%20Alexander%20Dockhorn%20and%20Diego%20Perez-Liebana%20and%20Jinyu%20Wang%20and%20Lei%20Song%20and%20Jiang%20Bian&entry.1292438233=%20%20One%20of%20the%20notorious%20issues%20for%20Reinforcement%20Learning%20%28RL%29%20is%20poor%20sample%0Aefficiency.%20Compared%20to%20single%20agent%20RL%2C%20the%20sample%20efficiency%20for%20Multi-Agent%0AReinforcement%20Learning%20%28MARL%29%20is%20more%20challenging%20because%20of%20its%20inherent%0Apartial%20observability%2C%20non-stationary%20training%2C%20and%20enormous%20strategy%20space.%0AAlthough%20much%20effort%20has%20been%20devoted%20to%20developing%20new%20methods%20and%20enhancing%0Asample%20efficiency%2C%20we%20look%20at%20the%20widely%20used%20episodic%20training%20mechanism.%20In%0Aeach%20training%20step%2C%20tens%20of%20frames%20are%20collected%2C%20but%20only%20one%20gradient%20step%20is%0Amade.%20We%20argue%20that%20this%20episodic%20training%20could%20be%20a%20source%20of%20poor%20sample%0Aefficiency.%20To%20better%20exploit%20the%20data%20already%20collected%2C%20we%20propose%20to%0Aincrease%20the%20frequency%20of%20the%20gradient%20updates%20per%20environment%20interaction%0A%28a.k.a.%20Replay%20Ratio%20or%20Update-To-Data%20ratio%29.%20To%20show%20its%20generality%2C%20we%0Aevaluate%20%243%24%20MARL%20methods%20on%20%246%24%20SMAC%20tasks.%20The%20empirical%20results%20validate%0Athat%20a%20higher%20replay%20ratio%20significantly%20improves%20the%20sample%20efficiency%20for%0AMARL%20algorithms.%20The%20codes%20to%20reimplement%20the%20results%20presented%20in%20this%20paper%0Aare%20open-sourced%20at%20https%3A//anonymous.4open.science/r/rr_for_MARL-0D83/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09715v1&entry.124074799=Read"},
{"title": "Deep Learning-Based Segmentation of Tumors in PET/CT Volumes: Benchmark\n  of Different Architectures and Training Strategies", "author": "Monika G\u00f3rka and Daniel Jaworek and Marek Wodzinski", "abstract": "  Cancer is one of the leading causes of death globally, and early diagnosis is\ncrucial for patient survival. Deep learning algorithms have great potential for\nautomatic cancer analysis. Artificial intelligence has achieved high\nperformance in recognizing and segmenting single lesions. However, diagnosing\nmultiple lesions remains a challenge. This study examines and compares various\nneural network architectures and training strategies for automatically\nsegmentation of cancer lesions using PET/CT images from the head, neck, and\nwhole body. The authors analyzed datasets from the AutoPET and HECKTOR\nchallenges, exploring popular single-step segmentation architectures and\npresenting a two-step approach. The results indicate that the V-Net and nnU-Net\nmodels were the most effective for their respective datasets. The results for\nthe HECKTOR dataset ranged from 0.75 to 0.76 for the aggregated Dice\ncoefficient. Eliminating cancer-free cases from the AutoPET dataset was found\nto improve the performance of most models. In the case of AutoPET data, the\naverage segmentation efficiency after training only on images containing cancer\nlesions increased from 0.55 to 0.66 for the classic Dice coefficient and from\n0.65 to 0.73 for the aggregated Dice coefficient. The research demonstrates the\npotential of artificial intelligence in precise oncological diagnostics and may\ncontribute to the development of more targeted and effective cancer assessment\ntechniques.\n", "link": "http://arxiv.org/abs/2404.09761v1", "date": "2024-04-15", "relevancy": 1.9492, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4957}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4928}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4784}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-Based%20Segmentation%20of%20Tumors%20in%20PET/CT%20Volumes%3A%20Benchmark%0A%20%20of%20Different%20Architectures%20and%20Training%20Strategies&body=Title%3A%20Deep%20Learning-Based%20Segmentation%20of%20Tumors%20in%20PET/CT%20Volumes%3A%20Benchmark%0A%20%20of%20Different%20Architectures%20and%20Training%20Strategies%0AAuthor%3A%20Monika%20G%C3%B3rka%20and%20Daniel%20Jaworek%20and%20Marek%20Wodzinski%0AAbstract%3A%20%20%20Cancer%20is%20one%20of%20the%20leading%20causes%20of%20death%20globally%2C%20and%20early%20diagnosis%20is%0Acrucial%20for%20patient%20survival.%20Deep%20learning%20algorithms%20have%20great%20potential%20for%0Aautomatic%20cancer%20analysis.%20Artificial%20intelligence%20has%20achieved%20high%0Aperformance%20in%20recognizing%20and%20segmenting%20single%20lesions.%20However%2C%20diagnosing%0Amultiple%20lesions%20remains%20a%20challenge.%20This%20study%20examines%20and%20compares%20various%0Aneural%20network%20architectures%20and%20training%20strategies%20for%20automatically%0Asegmentation%20of%20cancer%20lesions%20using%20PET/CT%20images%20from%20the%20head%2C%20neck%2C%20and%0Awhole%20body.%20The%20authors%20analyzed%20datasets%20from%20the%20AutoPET%20and%20HECKTOR%0Achallenges%2C%20exploring%20popular%20single-step%20segmentation%20architectures%20and%0Apresenting%20a%20two-step%20approach.%20The%20results%20indicate%20that%20the%20V-Net%20and%20nnU-Net%0Amodels%20were%20the%20most%20effective%20for%20their%20respective%20datasets.%20The%20results%20for%0Athe%20HECKTOR%20dataset%20ranged%20from%200.75%20to%200.76%20for%20the%20aggregated%20Dice%0Acoefficient.%20Eliminating%20cancer-free%20cases%20from%20the%20AutoPET%20dataset%20was%20found%0Ato%20improve%20the%20performance%20of%20most%20models.%20In%20the%20case%20of%20AutoPET%20data%2C%20the%0Aaverage%20segmentation%20efficiency%20after%20training%20only%20on%20images%20containing%20cancer%0Alesions%20increased%20from%200.55%20to%200.66%20for%20the%20classic%20Dice%20coefficient%20and%20from%0A0.65%20to%200.73%20for%20the%20aggregated%20Dice%20coefficient.%20The%20research%20demonstrates%20the%0Apotential%20of%20artificial%20intelligence%20in%20precise%20oncological%20diagnostics%20and%20may%0Acontribute%20to%20the%20development%20of%20more%20targeted%20and%20effective%20cancer%20assessment%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09761v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-Based%20Segmentation%20of%20Tumors%20in%20PET/CT%20Volumes%3A%20Benchmark%0A%20%20of%20Different%20Architectures%20and%20Training%20Strategies&entry.906535625=Monika%20G%C3%B3rka%20and%20Daniel%20Jaworek%20and%20Marek%20Wodzinski&entry.1292438233=%20%20Cancer%20is%20one%20of%20the%20leading%20causes%20of%20death%20globally%2C%20and%20early%20diagnosis%20is%0Acrucial%20for%20patient%20survival.%20Deep%20learning%20algorithms%20have%20great%20potential%20for%0Aautomatic%20cancer%20analysis.%20Artificial%20intelligence%20has%20achieved%20high%0Aperformance%20in%20recognizing%20and%20segmenting%20single%20lesions.%20However%2C%20diagnosing%0Amultiple%20lesions%20remains%20a%20challenge.%20This%20study%20examines%20and%20compares%20various%0Aneural%20network%20architectures%20and%20training%20strategies%20for%20automatically%0Asegmentation%20of%20cancer%20lesions%20using%20PET/CT%20images%20from%20the%20head%2C%20neck%2C%20and%0Awhole%20body.%20The%20authors%20analyzed%20datasets%20from%20the%20AutoPET%20and%20HECKTOR%0Achallenges%2C%20exploring%20popular%20single-step%20segmentation%20architectures%20and%0Apresenting%20a%20two-step%20approach.%20The%20results%20indicate%20that%20the%20V-Net%20and%20nnU-Net%0Amodels%20were%20the%20most%20effective%20for%20their%20respective%20datasets.%20The%20results%20for%0Athe%20HECKTOR%20dataset%20ranged%20from%200.75%20to%200.76%20for%20the%20aggregated%20Dice%0Acoefficient.%20Eliminating%20cancer-free%20cases%20from%20the%20AutoPET%20dataset%20was%20found%0Ato%20improve%20the%20performance%20of%20most%20models.%20In%20the%20case%20of%20AutoPET%20data%2C%20the%0Aaverage%20segmentation%20efficiency%20after%20training%20only%20on%20images%20containing%20cancer%0Alesions%20increased%20from%200.55%20to%200.66%20for%20the%20classic%20Dice%20coefficient%20and%20from%0A0.65%20to%200.73%20for%20the%20aggregated%20Dice%20coefficient.%20The%20research%20demonstrates%20the%0Apotential%20of%20artificial%20intelligence%20in%20precise%20oncological%20diagnostics%20and%20may%0Acontribute%20to%20the%20development%20of%20more%20targeted%20and%20effective%20cancer%20assessment%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09761v1&entry.124074799=Read"},
{"title": "Dynamic Ego-Velocity estimation Using Moving mmWave Radar: A Phase-Based\n  Approach", "author": "Argha Sen and Soham Chakraborty and Soham Tripathy and Sandip Chakraborty", "abstract": "  Precise ego-motion measurement is crucial for various applications, including\nrobotics, augmented reality, and autonomous navigation. In this poster, we\npropose mmPhase, an odometry framework based on single-chip millimetre-wave\n(mmWave) radar for robust ego-motion estimation in mobile platforms without\nrequiring additional modalities like the visual, wheel, or inertial odometry.\nmmPhase leverages a phase-based velocity estimation approach to overcome the\nlimitations of conventional doppler resolution. For real-world evaluations of\nmmPhase we have developed an ego-vehicle prototype. Compared to the\nstate-of-the-art baselines, mmPhase shows superior performance in ego-velocity\nestimation.\n", "link": "http://arxiv.org/abs/2404.09691v1", "date": "2024-04-15", "relevancy": 1.9401, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4873}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4687}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Ego-Velocity%20estimation%20Using%20Moving%20mmWave%20Radar%3A%20A%20Phase-Based%0A%20%20Approach&body=Title%3A%20Dynamic%20Ego-Velocity%20estimation%20Using%20Moving%20mmWave%20Radar%3A%20A%20Phase-Based%0A%20%20Approach%0AAuthor%3A%20Argha%20Sen%20and%20Soham%20Chakraborty%20and%20Soham%20Tripathy%20and%20Sandip%20Chakraborty%0AAbstract%3A%20%20%20Precise%20ego-motion%20measurement%20is%20crucial%20for%20various%20applications%2C%20including%0Arobotics%2C%20augmented%20reality%2C%20and%20autonomous%20navigation.%20In%20this%20poster%2C%20we%0Apropose%20mmPhase%2C%20an%20odometry%20framework%20based%20on%20single-chip%20millimetre-wave%0A%28mmWave%29%20radar%20for%20robust%20ego-motion%20estimation%20in%20mobile%20platforms%20without%0Arequiring%20additional%20modalities%20like%20the%20visual%2C%20wheel%2C%20or%20inertial%20odometry.%0AmmPhase%20leverages%20a%20phase-based%20velocity%20estimation%20approach%20to%20overcome%20the%0Alimitations%20of%20conventional%20doppler%20resolution.%20For%20real-world%20evaluations%20of%0AmmPhase%20we%20have%20developed%20an%20ego-vehicle%20prototype.%20Compared%20to%20the%0Astate-of-the-art%20baselines%2C%20mmPhase%20shows%20superior%20performance%20in%20ego-velocity%0Aestimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09691v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Ego-Velocity%20estimation%20Using%20Moving%20mmWave%20Radar%3A%20A%20Phase-Based%0A%20%20Approach&entry.906535625=Argha%20Sen%20and%20Soham%20Chakraborty%20and%20Soham%20Tripathy%20and%20Sandip%20Chakraborty&entry.1292438233=%20%20Precise%20ego-motion%20measurement%20is%20crucial%20for%20various%20applications%2C%20including%0Arobotics%2C%20augmented%20reality%2C%20and%20autonomous%20navigation.%20In%20this%20poster%2C%20we%0Apropose%20mmPhase%2C%20an%20odometry%20framework%20based%20on%20single-chip%20millimetre-wave%0A%28mmWave%29%20radar%20for%20robust%20ego-motion%20estimation%20in%20mobile%20platforms%20without%0Arequiring%20additional%20modalities%20like%20the%20visual%2C%20wheel%2C%20or%20inertial%20odometry.%0AmmPhase%20leverages%20a%20phase-based%20velocity%20estimation%20approach%20to%20overcome%20the%0Alimitations%20of%20conventional%20doppler%20resolution.%20For%20real-world%20evaluations%20of%0AmmPhase%20we%20have%20developed%20an%20ego-vehicle%20prototype.%20Compared%20to%20the%0Astate-of-the-art%20baselines%2C%20mmPhase%20shows%20superior%20performance%20in%20ego-velocity%0Aestimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09691v1&entry.124074799=Read"},
{"title": "RandAlign: A Parameter-Free Method for Regularizing Graph Convolutional\n  Networks", "author": "Haimin Zhang and Min Xu", "abstract": "  Studies continually find that message-passing graph convolutional networks\nsuffer from the over-smoothing issue. Basically, the issue of over-smoothing\nrefers to the phenomenon that the learned embeddings for all nodes can become\nvery similar to one another and therefore are uninformative after repeatedly\napplying message passing iterations. Intuitively, we can expect the generated\nembeddings become smooth asymptotically layerwisely, that is each layer of\ngraph convolution generates a smoothed version of embeddings as compared to\nthat generated by the previous layer. Based on this intuition, we propose\nRandAlign, a stochastic regularization method for graph convolutional networks.\nThe idea of RandAlign is to randomly align the learned embedding for each node\nwith that of the previous layer using randomly interpolation in each graph\nconvolution layer. Through alignment, the smoothness of the generated\nembeddings is explicitly reduced. To better maintain the benefit yielded by the\ngraph convolution, in the alignment step we introduce to first scale the\nembedding of the previous layer to the same norm as the generated embedding and\nthen perform random interpolation for aligning the generated embedding.\nRandAlign is a parameter-free method and can be directly applied without\nintroducing additional trainable weights or hyper-parameters. We experimentally\nevaluate RandAlign on different graph domain tasks on seven benchmark datasets.\nThe experimental results show that RandAlign is a general method that improves\nthe generalization performance of various graph convolutional network models\nand also improves the numerical stability of optimization, advancing the state\nof the art performance for graph representation learning.\n", "link": "http://arxiv.org/abs/2404.09774v1", "date": "2024-04-15", "relevancy": 1.9292, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5092}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4933}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4605}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RandAlign%3A%20A%20Parameter-Free%20Method%20for%20Regularizing%20Graph%20Convolutional%0A%20%20Networks&body=Title%3A%20RandAlign%3A%20A%20Parameter-Free%20Method%20for%20Regularizing%20Graph%20Convolutional%0A%20%20Networks%0AAuthor%3A%20Haimin%20Zhang%20and%20Min%20Xu%0AAbstract%3A%20%20%20Studies%20continually%20find%20that%20message-passing%20graph%20convolutional%20networks%0Asuffer%20from%20the%20over-smoothing%20issue.%20Basically%2C%20the%20issue%20of%20over-smoothing%0Arefers%20to%20the%20phenomenon%20that%20the%20learned%20embeddings%20for%20all%20nodes%20can%20become%0Avery%20similar%20to%20one%20another%20and%20therefore%20are%20uninformative%20after%20repeatedly%0Aapplying%20message%20passing%20iterations.%20Intuitively%2C%20we%20can%20expect%20the%20generated%0Aembeddings%20become%20smooth%20asymptotically%20layerwisely%2C%20that%20is%20each%20layer%20of%0Agraph%20convolution%20generates%20a%20smoothed%20version%20of%20embeddings%20as%20compared%20to%0Athat%20generated%20by%20the%20previous%20layer.%20Based%20on%20this%20intuition%2C%20we%20propose%0ARandAlign%2C%20a%20stochastic%20regularization%20method%20for%20graph%20convolutional%20networks.%0AThe%20idea%20of%20RandAlign%20is%20to%20randomly%20align%20the%20learned%20embedding%20for%20each%20node%0Awith%20that%20of%20the%20previous%20layer%20using%20randomly%20interpolation%20in%20each%20graph%0Aconvolution%20layer.%20Through%20alignment%2C%20the%20smoothness%20of%20the%20generated%0Aembeddings%20is%20explicitly%20reduced.%20To%20better%20maintain%20the%20benefit%20yielded%20by%20the%0Agraph%20convolution%2C%20in%20the%20alignment%20step%20we%20introduce%20to%20first%20scale%20the%0Aembedding%20of%20the%20previous%20layer%20to%20the%20same%20norm%20as%20the%20generated%20embedding%20and%0Athen%20perform%20random%20interpolation%20for%20aligning%20the%20generated%20embedding.%0ARandAlign%20is%20a%20parameter-free%20method%20and%20can%20be%20directly%20applied%20without%0Aintroducing%20additional%20trainable%20weights%20or%20hyper-parameters.%20We%20experimentally%0Aevaluate%20RandAlign%20on%20different%20graph%20domain%20tasks%20on%20seven%20benchmark%20datasets.%0AThe%20experimental%20results%20show%20that%20RandAlign%20is%20a%20general%20method%20that%20improves%0Athe%20generalization%20performance%20of%20various%20graph%20convolutional%20network%20models%0Aand%20also%20improves%20the%20numerical%20stability%20of%20optimization%2C%20advancing%20the%20state%0Aof%20the%20art%20performance%20for%20graph%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09774v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RandAlign%3A%20A%20Parameter-Free%20Method%20for%20Regularizing%20Graph%20Convolutional%0A%20%20Networks&entry.906535625=Haimin%20Zhang%20and%20Min%20Xu&entry.1292438233=%20%20Studies%20continually%20find%20that%20message-passing%20graph%20convolutional%20networks%0Asuffer%20from%20the%20over-smoothing%20issue.%20Basically%2C%20the%20issue%20of%20over-smoothing%0Arefers%20to%20the%20phenomenon%20that%20the%20learned%20embeddings%20for%20all%20nodes%20can%20become%0Avery%20similar%20to%20one%20another%20and%20therefore%20are%20uninformative%20after%20repeatedly%0Aapplying%20message%20passing%20iterations.%20Intuitively%2C%20we%20can%20expect%20the%20generated%0Aembeddings%20become%20smooth%20asymptotically%20layerwisely%2C%20that%20is%20each%20layer%20of%0Agraph%20convolution%20generates%20a%20smoothed%20version%20of%20embeddings%20as%20compared%20to%0Athat%20generated%20by%20the%20previous%20layer.%20Based%20on%20this%20intuition%2C%20we%20propose%0ARandAlign%2C%20a%20stochastic%20regularization%20method%20for%20graph%20convolutional%20networks.%0AThe%20idea%20of%20RandAlign%20is%20to%20randomly%20align%20the%20learned%20embedding%20for%20each%20node%0Awith%20that%20of%20the%20previous%20layer%20using%20randomly%20interpolation%20in%20each%20graph%0Aconvolution%20layer.%20Through%20alignment%2C%20the%20smoothness%20of%20the%20generated%0Aembeddings%20is%20explicitly%20reduced.%20To%20better%20maintain%20the%20benefit%20yielded%20by%20the%0Agraph%20convolution%2C%20in%20the%20alignment%20step%20we%20introduce%20to%20first%20scale%20the%0Aembedding%20of%20the%20previous%20layer%20to%20the%20same%20norm%20as%20the%20generated%20embedding%20and%0Athen%20perform%20random%20interpolation%20for%20aligning%20the%20generated%20embedding.%0ARandAlign%20is%20a%20parameter-free%20method%20and%20can%20be%20directly%20applied%20without%0Aintroducing%20additional%20trainable%20weights%20or%20hyper-parameters.%20We%20experimentally%0Aevaluate%20RandAlign%20on%20different%20graph%20domain%20tasks%20on%20seven%20benchmark%20datasets.%0AThe%20experimental%20results%20show%20that%20RandAlign%20is%20a%20general%20method%20that%20improves%0Athe%20generalization%20performance%20of%20various%20graph%20convolutional%20network%20models%0Aand%20also%20improves%20the%20numerical%20stability%20of%20optimization%2C%20advancing%20the%20state%0Aof%20the%20art%20performance%20for%20graph%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09774v1&entry.124074799=Read"},
{"title": "Flattening the Parent Bias: Hierarchical Semantic Segmentation in the\n  Poincar\u00e9 Ball", "author": "Simon Weber and Bar\u0131\u015f Z\u00f6ng\u00fcr and Nikita Araslanov and Daniel Cremers", "abstract": "  Hierarchy is a natural representation of semantic taxonomies, including the\nones routinely used in image segmentation. Indeed, recent work on semantic\nsegmentation reports improved accuracy from supervised training leveraging\nhierarchical label structures. Encouraged by these results, we revisit the\nfundamental assumptions behind that work. We postulate and then empirically\nverify that the reasons for the observed improvement in segmentation accuracy\nmay be entirely unrelated to the use of the semantic hierarchy. To demonstrate\nthis, we design a range of cross-domain experiments with a representative\nhierarchical approach. We find that on the new testing domains, a flat\n(non-hierarchical) segmentation network, in which the parents are inferred from\nthe children, has superior segmentation accuracy to the hierarchical approach\nacross the board. Complementing these findings and inspired by the intrinsic\nproperties of hyperbolic spaces, we study a more principled approach to\nhierarchical segmentation using the Poincar\\'e ball model. The hyperbolic\nrepresentation largely outperforms the previous (Euclidean) hierarchical\napproach as well and is on par with our flat Euclidean baseline in terms of\nsegmentation accuracy. However, it additionally exhibits surprisingly strong\ncalibration quality of the parent nodes in the semantic hierarchy, especially\non the more challenging domains. Our combined analysis suggests that the\nestablished practice of hierarchical segmentation may be limited to in-domain\nsettings, whereas flat classifiers generalize substantially better, especially\nif they are modeled in the hyperbolic space.\n", "link": "http://arxiv.org/abs/2404.03778v3", "date": "2024-04-15", "relevancy": 1.9274, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5137}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4756}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4754}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Flattening%20the%20Parent%20Bias%3A%20Hierarchical%20Semantic%20Segmentation%20in%20the%0A%20%20Poincar%C3%A9%20Ball&body=Title%3A%20Flattening%20the%20Parent%20Bias%3A%20Hierarchical%20Semantic%20Segmentation%20in%20the%0A%20%20Poincar%C3%A9%20Ball%0AAuthor%3A%20Simon%20Weber%20and%20Bar%C4%B1%C5%9F%20Z%C3%B6ng%C3%BCr%20and%20Nikita%20Araslanov%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Hierarchy%20is%20a%20natural%20representation%20of%20semantic%20taxonomies%2C%20including%20the%0Aones%20routinely%20used%20in%20image%20segmentation.%20Indeed%2C%20recent%20work%20on%20semantic%0Asegmentation%20reports%20improved%20accuracy%20from%20supervised%20training%20leveraging%0Ahierarchical%20label%20structures.%20Encouraged%20by%20these%20results%2C%20we%20revisit%20the%0Afundamental%20assumptions%20behind%20that%20work.%20We%20postulate%20and%20then%20empirically%0Averify%20that%20the%20reasons%20for%20the%20observed%20improvement%20in%20segmentation%20accuracy%0Amay%20be%20entirely%20unrelated%20to%20the%20use%20of%20the%20semantic%20hierarchy.%20To%20demonstrate%0Athis%2C%20we%20design%20a%20range%20of%20cross-domain%20experiments%20with%20a%20representative%0Ahierarchical%20approach.%20We%20find%20that%20on%20the%20new%20testing%20domains%2C%20a%20flat%0A%28non-hierarchical%29%20segmentation%20network%2C%20in%20which%20the%20parents%20are%20inferred%20from%0Athe%20children%2C%20has%20superior%20segmentation%20accuracy%20to%20the%20hierarchical%20approach%0Aacross%20the%20board.%20Complementing%20these%20findings%20and%20inspired%20by%20the%20intrinsic%0Aproperties%20of%20hyperbolic%20spaces%2C%20we%20study%20a%20more%20principled%20approach%20to%0Ahierarchical%20segmentation%20using%20the%20Poincar%5C%27e%20ball%20model.%20The%20hyperbolic%0Arepresentation%20largely%20outperforms%20the%20previous%20%28Euclidean%29%20hierarchical%0Aapproach%20as%20well%20and%20is%20on%20par%20with%20our%20flat%20Euclidean%20baseline%20in%20terms%20of%0Asegmentation%20accuracy.%20However%2C%20it%20additionally%20exhibits%20surprisingly%20strong%0Acalibration%20quality%20of%20the%20parent%20nodes%20in%20the%20semantic%20hierarchy%2C%20especially%0Aon%20the%20more%20challenging%20domains.%20Our%20combined%20analysis%20suggests%20that%20the%0Aestablished%20practice%20of%20hierarchical%20segmentation%20may%20be%20limited%20to%20in-domain%0Asettings%2C%20whereas%20flat%20classifiers%20generalize%20substantially%20better%2C%20especially%0Aif%20they%20are%20modeled%20in%20the%20hyperbolic%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03778v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flattening%20the%20Parent%20Bias%3A%20Hierarchical%20Semantic%20Segmentation%20in%20the%0A%20%20Poincar%C3%A9%20Ball&entry.906535625=Simon%20Weber%20and%20Bar%C4%B1%C5%9F%20Z%C3%B6ng%C3%BCr%20and%20Nikita%20Araslanov%20and%20Daniel%20Cremers&entry.1292438233=%20%20Hierarchy%20is%20a%20natural%20representation%20of%20semantic%20taxonomies%2C%20including%20the%0Aones%20routinely%20used%20in%20image%20segmentation.%20Indeed%2C%20recent%20work%20on%20semantic%0Asegmentation%20reports%20improved%20accuracy%20from%20supervised%20training%20leveraging%0Ahierarchical%20label%20structures.%20Encouraged%20by%20these%20results%2C%20we%20revisit%20the%0Afundamental%20assumptions%20behind%20that%20work.%20We%20postulate%20and%20then%20empirically%0Averify%20that%20the%20reasons%20for%20the%20observed%20improvement%20in%20segmentation%20accuracy%0Amay%20be%20entirely%20unrelated%20to%20the%20use%20of%20the%20semantic%20hierarchy.%20To%20demonstrate%0Athis%2C%20we%20design%20a%20range%20of%20cross-domain%20experiments%20with%20a%20representative%0Ahierarchical%20approach.%20We%20find%20that%20on%20the%20new%20testing%20domains%2C%20a%20flat%0A%28non-hierarchical%29%20segmentation%20network%2C%20in%20which%20the%20parents%20are%20inferred%20from%0Athe%20children%2C%20has%20superior%20segmentation%20accuracy%20to%20the%20hierarchical%20approach%0Aacross%20the%20board.%20Complementing%20these%20findings%20and%20inspired%20by%20the%20intrinsic%0Aproperties%20of%20hyperbolic%20spaces%2C%20we%20study%20a%20more%20principled%20approach%20to%0Ahierarchical%20segmentation%20using%20the%20Poincar%5C%27e%20ball%20model.%20The%20hyperbolic%0Arepresentation%20largely%20outperforms%20the%20previous%20%28Euclidean%29%20hierarchical%0Aapproach%20as%20well%20and%20is%20on%20par%20with%20our%20flat%20Euclidean%20baseline%20in%20terms%20of%0Asegmentation%20accuracy.%20However%2C%20it%20additionally%20exhibits%20surprisingly%20strong%0Acalibration%20quality%20of%20the%20parent%20nodes%20in%20the%20semantic%20hierarchy%2C%20especially%0Aon%20the%20more%20challenging%20domains.%20Our%20combined%20analysis%20suggests%20that%20the%0Aestablished%20practice%20of%20hierarchical%20segmentation%20may%20be%20limited%20to%20in-domain%0Asettings%2C%20whereas%20flat%20classifiers%20generalize%20substantially%20better%2C%20especially%0Aif%20they%20are%20modeled%20in%20the%20hyperbolic%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03778v3&entry.124074799=Read"},
{"title": "A replica analysis of under-bagging", "author": "Takashi Takahashi", "abstract": "  A sharp asymptotics of the under-bagging (UB) method, which is a popular\nensemble learning method for training classifiers from an imbalanced data, is\nderived and used to compare with several other standard methods for learning\nfrom imbalanced data, in the scenario where a linear classifier is trained from\na binary mixture data. The methods compared include the under-sampling (US)\nmethod, which trains a model using a single realization of the subsampled\ndataset, and the simple weighting (SW) method, which trains a model with a\nweighted loss on the entire data. It is shown that the performance of UB is\nimproved by increasing the size of the majority class, even if the class\nimbalance can be large, especially when the size of the minority class is\nsmall. This is in contrast to US, whose performance does not change as the size\nof the majority class increases, and SW, whose performance decreases as the\nimbalance increases. These results are different from the case of the naive\nbagging in training generalized linear models without considering the structure\nof class imbalance, indicating the intrinsic difference between the ensembling\nand the direct regularization on the parameters.\n", "link": "http://arxiv.org/abs/2404.09779v1", "date": "2024-04-15", "relevancy": 1.912, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5368}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4438}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4329}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20replica%20analysis%20of%20under-bagging&body=Title%3A%20A%20replica%20analysis%20of%20under-bagging%0AAuthor%3A%20Takashi%20Takahashi%0AAbstract%3A%20%20%20A%20sharp%20asymptotics%20of%20the%20under-bagging%20%28UB%29%20method%2C%20which%20is%20a%20popular%0Aensemble%20learning%20method%20for%20training%20classifiers%20from%20an%20imbalanced%20data%2C%20is%0Aderived%20and%20used%20to%20compare%20with%20several%20other%20standard%20methods%20for%20learning%0Afrom%20imbalanced%20data%2C%20in%20the%20scenario%20where%20a%20linear%20classifier%20is%20trained%20from%0Aa%20binary%20mixture%20data.%20The%20methods%20compared%20include%20the%20under-sampling%20%28US%29%0Amethod%2C%20which%20trains%20a%20model%20using%20a%20single%20realization%20of%20the%20subsampled%0Adataset%2C%20and%20the%20simple%20weighting%20%28SW%29%20method%2C%20which%20trains%20a%20model%20with%20a%0Aweighted%20loss%20on%20the%20entire%20data.%20It%20is%20shown%20that%20the%20performance%20of%20UB%20is%0Aimproved%20by%20increasing%20the%20size%20of%20the%20majority%20class%2C%20even%20if%20the%20class%0Aimbalance%20can%20be%20large%2C%20especially%20when%20the%20size%20of%20the%20minority%20class%20is%0Asmall.%20This%20is%20in%20contrast%20to%20US%2C%20whose%20performance%20does%20not%20change%20as%20the%20size%0Aof%20the%20majority%20class%20increases%2C%20and%20SW%2C%20whose%20performance%20decreases%20as%20the%0Aimbalance%20increases.%20These%20results%20are%20different%20from%20the%20case%20of%20the%20naive%0Abagging%20in%20training%20generalized%20linear%20models%20without%20considering%20the%20structure%0Aof%20class%20imbalance%2C%20indicating%20the%20intrinsic%20difference%20between%20the%20ensembling%0Aand%20the%20direct%20regularization%20on%20the%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09779v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20replica%20analysis%20of%20under-bagging&entry.906535625=Takashi%20Takahashi&entry.1292438233=%20%20A%20sharp%20asymptotics%20of%20the%20under-bagging%20%28UB%29%20method%2C%20which%20is%20a%20popular%0Aensemble%20learning%20method%20for%20training%20classifiers%20from%20an%20imbalanced%20data%2C%20is%0Aderived%20and%20used%20to%20compare%20with%20several%20other%20standard%20methods%20for%20learning%0Afrom%20imbalanced%20data%2C%20in%20the%20scenario%20where%20a%20linear%20classifier%20is%20trained%20from%0Aa%20binary%20mixture%20data.%20The%20methods%20compared%20include%20the%20under-sampling%20%28US%29%0Amethod%2C%20which%20trains%20a%20model%20using%20a%20single%20realization%20of%20the%20subsampled%0Adataset%2C%20and%20the%20simple%20weighting%20%28SW%29%20method%2C%20which%20trains%20a%20model%20with%20a%0Aweighted%20loss%20on%20the%20entire%20data.%20It%20is%20shown%20that%20the%20performance%20of%20UB%20is%0Aimproved%20by%20increasing%20the%20size%20of%20the%20majority%20class%2C%20even%20if%20the%20class%0Aimbalance%20can%20be%20large%2C%20especially%20when%20the%20size%20of%20the%20minority%20class%20is%0Asmall.%20This%20is%20in%20contrast%20to%20US%2C%20whose%20performance%20does%20not%20change%20as%20the%20size%0Aof%20the%20majority%20class%20increases%2C%20and%20SW%2C%20whose%20performance%20decreases%20as%20the%0Aimbalance%20increases.%20These%20results%20are%20different%20from%20the%20case%20of%20the%20naive%0Abagging%20in%20training%20generalized%20linear%20models%20without%20considering%20the%20structure%0Aof%20class%20imbalance%2C%20indicating%20the%20intrinsic%20difference%20between%20the%20ensembling%0Aand%20the%20direct%20regularization%20on%20the%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09779v1&entry.124074799=Read"},
{"title": "Sampling for Model Predictive Trajectory Planning in Autonomous Driving\n  using Normalizing Flows", "author": "Georg Rabenstein and Lars Ullrich and Knut Graichen", "abstract": "  Alongside optimization-based planners, sampling-based approaches are often\nused in trajectory planning for autonomous driving due to their simplicity.\nModel predictive path integral control is a framework that builds upon\noptimization principles while incorporating stochastic sampling of input\ntrajectories. This paper investigates several sampling approaches for\ntrajectory generation. In this context, normalizing flows originating from the\nfield of variational inference are considered for the generation of sampling\ndistributions, as they model transformations of simple to more complex\ndistributions. Accordingly, learning-based normalizing flow models are trained\nfor a more efficient exploration of the input domain for the task at hand. The\ndeveloped algorithm and the proposed sampling distributions are evaluated in\ntwo simulation scenarios.\n", "link": "http://arxiv.org/abs/2404.09657v1", "date": "2024-04-15", "relevancy": 1.901, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.498}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4779}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4635}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sampling%20for%20Model%20Predictive%20Trajectory%20Planning%20in%20Autonomous%20Driving%0A%20%20using%20Normalizing%20Flows&body=Title%3A%20Sampling%20for%20Model%20Predictive%20Trajectory%20Planning%20in%20Autonomous%20Driving%0A%20%20using%20Normalizing%20Flows%0AAuthor%3A%20Georg%20Rabenstein%20and%20Lars%20Ullrich%20and%20Knut%20Graichen%0AAbstract%3A%20%20%20Alongside%20optimization-based%20planners%2C%20sampling-based%20approaches%20are%20often%0Aused%20in%20trajectory%20planning%20for%20autonomous%20driving%20due%20to%20their%20simplicity.%0AModel%20predictive%20path%20integral%20control%20is%20a%20framework%20that%20builds%20upon%0Aoptimization%20principles%20while%20incorporating%20stochastic%20sampling%20of%20input%0Atrajectories.%20This%20paper%20investigates%20several%20sampling%20approaches%20for%0Atrajectory%20generation.%20In%20this%20context%2C%20normalizing%20flows%20originating%20from%20the%0Afield%20of%20variational%20inference%20are%20considered%20for%20the%20generation%20of%20sampling%0Adistributions%2C%20as%20they%20model%20transformations%20of%20simple%20to%20more%20complex%0Adistributions.%20Accordingly%2C%20learning-based%20normalizing%20flow%20models%20are%20trained%0Afor%20a%20more%20efficient%20exploration%20of%20the%20input%20domain%20for%20the%20task%20at%20hand.%20The%0Adeveloped%20algorithm%20and%20the%20proposed%20sampling%20distributions%20are%20evaluated%20in%0Atwo%20simulation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09657v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling%20for%20Model%20Predictive%20Trajectory%20Planning%20in%20Autonomous%20Driving%0A%20%20using%20Normalizing%20Flows&entry.906535625=Georg%20Rabenstein%20and%20Lars%20Ullrich%20and%20Knut%20Graichen&entry.1292438233=%20%20Alongside%20optimization-based%20planners%2C%20sampling-based%20approaches%20are%20often%0Aused%20in%20trajectory%20planning%20for%20autonomous%20driving%20due%20to%20their%20simplicity.%0AModel%20predictive%20path%20integral%20control%20is%20a%20framework%20that%20builds%20upon%0Aoptimization%20principles%20while%20incorporating%20stochastic%20sampling%20of%20input%0Atrajectories.%20This%20paper%20investigates%20several%20sampling%20approaches%20for%0Atrajectory%20generation.%20In%20this%20context%2C%20normalizing%20flows%20originating%20from%20the%0Afield%20of%20variational%20inference%20are%20considered%20for%20the%20generation%20of%20sampling%0Adistributions%2C%20as%20they%20model%20transformations%20of%20simple%20to%20more%20complex%0Adistributions.%20Accordingly%2C%20learning-based%20normalizing%20flow%20models%20are%20trained%0Afor%20a%20more%20efficient%20exploration%20of%20the%20input%20domain%20for%20the%20task%20at%20hand.%20The%0Adeveloped%20algorithm%20and%20the%20proposed%20sampling%20distributions%20are%20evaluated%20in%0Atwo%20simulation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09657v1&entry.124074799=Read"},
{"title": "AI Competitions and Benchmarks: Dataset Development", "author": "Romain Egele and Julio C. S. Jacques Junior and Jan N. van Rijn and Isabelle Guyon and Xavier Bar\u00f3 and Albert Clap\u00e9s and Prasanna Balaprakash and Sergio Escalera and Thomas Moeslund and Jun Wan", "abstract": "  Machine learning is now used in many applications thanks to its ability to\npredict, generate, or discover patterns from large quantities of data. However,\nthe process of collecting and transforming data for practical use is intricate.\nEven in today's digital era, where substantial data is generated daily, it is\nuncommon for it to be readily usable; most often, it necessitates meticulous\nmanual data preparation. The haste in developing new models can frequently\nresult in various shortcomings, potentially posing risks when deployed in\nreal-world scenarios (eg social discrimination, critical failures), leading to\nthe failure or substantial escalation of costs in AI-based projects. This\nchapter provides a comprehensive overview of established methodological tools,\nenriched by our practical experience, in the development of datasets for\nmachine learning. Initially, we develop the tasks involved in dataset\ndevelopment and offer insights into their effective management (including\nrequirements, design, implementation, evaluation, distribution, and\nmaintenance). Then, we provide more details about the implementation process\nwhich includes data collection, transformation, and quality evaluation.\nFinally, we address practical considerations regarding dataset distribution and\nmaintenance.\n", "link": "http://arxiv.org/abs/2404.09703v1", "date": "2024-04-15", "relevancy": 1.8114, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4676}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.465}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4333}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AI%20Competitions%20and%20Benchmarks%3A%20Dataset%20Development&body=Title%3A%20AI%20Competitions%20and%20Benchmarks%3A%20Dataset%20Development%0AAuthor%3A%20Romain%20Egele%20and%20Julio%20C.%20S.%20Jacques%20Junior%20and%20Jan%20N.%20van%20Rijn%20and%20Isabelle%20Guyon%20and%20Xavier%20Bar%C3%B3%20and%20Albert%20Clap%C3%A9s%20and%20Prasanna%20Balaprakash%20and%20Sergio%20Escalera%20and%20Thomas%20Moeslund%20and%20Jun%20Wan%0AAbstract%3A%20%20%20Machine%20learning%20is%20now%20used%20in%20many%20applications%20thanks%20to%20its%20ability%20to%0Apredict%2C%20generate%2C%20or%20discover%20patterns%20from%20large%20quantities%20of%20data.%20However%2C%0Athe%20process%20of%20collecting%20and%20transforming%20data%20for%20practical%20use%20is%20intricate.%0AEven%20in%20today%27s%20digital%20era%2C%20where%20substantial%20data%20is%20generated%20daily%2C%20it%20is%0Auncommon%20for%20it%20to%20be%20readily%20usable%3B%20most%20often%2C%20it%20necessitates%20meticulous%0Amanual%20data%20preparation.%20The%20haste%20in%20developing%20new%20models%20can%20frequently%0Aresult%20in%20various%20shortcomings%2C%20potentially%20posing%20risks%20when%20deployed%20in%0Areal-world%20scenarios%20%28eg%20social%20discrimination%2C%20critical%20failures%29%2C%20leading%20to%0Athe%20failure%20or%20substantial%20escalation%20of%20costs%20in%20AI-based%20projects.%20This%0Achapter%20provides%20a%20comprehensive%20overview%20of%20established%20methodological%20tools%2C%0Aenriched%20by%20our%20practical%20experience%2C%20in%20the%20development%20of%20datasets%20for%0Amachine%20learning.%20Initially%2C%20we%20develop%20the%20tasks%20involved%20in%20dataset%0Adevelopment%20and%20offer%20insights%20into%20their%20effective%20management%20%28including%0Arequirements%2C%20design%2C%20implementation%2C%20evaluation%2C%20distribution%2C%20and%0Amaintenance%29.%20Then%2C%20we%20provide%20more%20details%20about%20the%20implementation%20process%0Awhich%20includes%20data%20collection%2C%20transformation%2C%20and%20quality%20evaluation.%0AFinally%2C%20we%20address%20practical%20considerations%20regarding%20dataset%20distribution%20and%0Amaintenance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09703v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Competitions%20and%20Benchmarks%3A%20Dataset%20Development&entry.906535625=Romain%20Egele%20and%20Julio%20C.%20S.%20Jacques%20Junior%20and%20Jan%20N.%20van%20Rijn%20and%20Isabelle%20Guyon%20and%20Xavier%20Bar%C3%B3%20and%20Albert%20Clap%C3%A9s%20and%20Prasanna%20Balaprakash%20and%20Sergio%20Escalera%20and%20Thomas%20Moeslund%20and%20Jun%20Wan&entry.1292438233=%20%20Machine%20learning%20is%20now%20used%20in%20many%20applications%20thanks%20to%20its%20ability%20to%0Apredict%2C%20generate%2C%20or%20discover%20patterns%20from%20large%20quantities%20of%20data.%20However%2C%0Athe%20process%20of%20collecting%20and%20transforming%20data%20for%20practical%20use%20is%20intricate.%0AEven%20in%20today%27s%20digital%20era%2C%20where%20substantial%20data%20is%20generated%20daily%2C%20it%20is%0Auncommon%20for%20it%20to%20be%20readily%20usable%3B%20most%20often%2C%20it%20necessitates%20meticulous%0Amanual%20data%20preparation.%20The%20haste%20in%20developing%20new%20models%20can%20frequently%0Aresult%20in%20various%20shortcomings%2C%20potentially%20posing%20risks%20when%20deployed%20in%0Areal-world%20scenarios%20%28eg%20social%20discrimination%2C%20critical%20failures%29%2C%20leading%20to%0Athe%20failure%20or%20substantial%20escalation%20of%20costs%20in%20AI-based%20projects.%20This%0Achapter%20provides%20a%20comprehensive%20overview%20of%20established%20methodological%20tools%2C%0Aenriched%20by%20our%20practical%20experience%2C%20in%20the%20development%20of%20datasets%20for%0Amachine%20learning.%20Initially%2C%20we%20develop%20the%20tasks%20involved%20in%20dataset%0Adevelopment%20and%20offer%20insights%20into%20their%20effective%20management%20%28including%0Arequirements%2C%20design%2C%20implementation%2C%20evaluation%2C%20distribution%2C%20and%0Amaintenance%29.%20Then%2C%20we%20provide%20more%20details%20about%20the%20implementation%20process%0Awhich%20includes%20data%20collection%2C%20transformation%2C%20and%20quality%20evaluation.%0AFinally%2C%20we%20address%20practical%20considerations%20regarding%20dataset%20distribution%20and%0Amaintenance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09703v1&entry.124074799=Read"},
{"title": "KG-CTG: Citation Generation through Knowledge Graph-guided Large\n  Language Models", "author": "Avinash Anand and Mohit Gupta and Kritarth Prasad and Ujjwal Goel and Naman Lal and Astha Verma and Rajiv Ratn Shah", "abstract": "  Citation Text Generation (CTG) is a task in natural language processing (NLP)\nthat aims to produce text that accurately cites or references a cited document\nwithin a source document. In CTG, the generated text draws upon contextual cues\nfrom both the source document and the cited paper, ensuring accurate and\nrelevant citation information is provided. Previous work in the field of\ncitation generation is mainly based on the text summarization of documents.\nFollowing this, this paper presents a framework, and a comparative study to\ndemonstrate the use of Large Language Models (LLMs) for the task of citation\ngeneration. Also, we have shown the improvement in the results of citation\ngeneration by incorporating the knowledge graph relations of the papers in the\nprompt for the LLM to better learn the relationship between the papers. To\nassess how well our model is performing, we have used a subset of standard\nS2ORC dataset, which only consists of computer science academic research papers\nin the English Language. Vicuna performs best for this task with 14.15 Meteor,\n12.88 Rouge-1, 1.52 Rouge-2, and 10.94 Rouge-L. Also, Alpaca performs best, and\nimproves the performance by 36.98% in Rouge-1, and 33.14% in Meteor by\nincluding knowledge graphs.\n", "link": "http://arxiv.org/abs/2404.09763v1", "date": "2024-04-15", "relevancy": 1.8075, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.466}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.457}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4358}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20KG-CTG%3A%20Citation%20Generation%20through%20Knowledge%20Graph-guided%20Large%0A%20%20Language%20Models&body=Title%3A%20KG-CTG%3A%20Citation%20Generation%20through%20Knowledge%20Graph-guided%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Avinash%20Anand%20and%20Mohit%20Gupta%20and%20Kritarth%20Prasad%20and%20Ujjwal%20Goel%20and%20Naman%20Lal%20and%20Astha%20Verma%20and%20Rajiv%20Ratn%20Shah%0AAbstract%3A%20%20%20Citation%20Text%20Generation%20%28CTG%29%20is%20a%20task%20in%20natural%20language%20processing%20%28NLP%29%0Athat%20aims%20to%20produce%20text%20that%20accurately%20cites%20or%20references%20a%20cited%20document%0Awithin%20a%20source%20document.%20In%20CTG%2C%20the%20generated%20text%20draws%20upon%20contextual%20cues%0Afrom%20both%20the%20source%20document%20and%20the%20cited%20paper%2C%20ensuring%20accurate%20and%0Arelevant%20citation%20information%20is%20provided.%20Previous%20work%20in%20the%20field%20of%0Acitation%20generation%20is%20mainly%20based%20on%20the%20text%20summarization%20of%20documents.%0AFollowing%20this%2C%20this%20paper%20presents%20a%20framework%2C%20and%20a%20comparative%20study%20to%0Ademonstrate%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20the%20task%20of%20citation%0Ageneration.%20Also%2C%20we%20have%20shown%20the%20improvement%20in%20the%20results%20of%20citation%0Ageneration%20by%20incorporating%20the%20knowledge%20graph%20relations%20of%20the%20papers%20in%20the%0Aprompt%20for%20the%20LLM%20to%20better%20learn%20the%20relationship%20between%20the%20papers.%20To%0Aassess%20how%20well%20our%20model%20is%20performing%2C%20we%20have%20used%20a%20subset%20of%20standard%0AS2ORC%20dataset%2C%20which%20only%20consists%20of%20computer%20science%20academic%20research%20papers%0Ain%20the%20English%20Language.%20Vicuna%20performs%20best%20for%20this%20task%20with%2014.15%20Meteor%2C%0A12.88%20Rouge-1%2C%201.52%20Rouge-2%2C%20and%2010.94%20Rouge-L.%20Also%2C%20Alpaca%20performs%20best%2C%20and%0Aimproves%20the%20performance%20by%2036.98%25%20in%20Rouge-1%2C%20and%2033.14%25%20in%20Meteor%20by%0Aincluding%20knowledge%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09763v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KG-CTG%3A%20Citation%20Generation%20through%20Knowledge%20Graph-guided%20Large%0A%20%20Language%20Models&entry.906535625=Avinash%20Anand%20and%20Mohit%20Gupta%20and%20Kritarth%20Prasad%20and%20Ujjwal%20Goel%20and%20Naman%20Lal%20and%20Astha%20Verma%20and%20Rajiv%20Ratn%20Shah&entry.1292438233=%20%20Citation%20Text%20Generation%20%28CTG%29%20is%20a%20task%20in%20natural%20language%20processing%20%28NLP%29%0Athat%20aims%20to%20produce%20text%20that%20accurately%20cites%20or%20references%20a%20cited%20document%0Awithin%20a%20source%20document.%20In%20CTG%2C%20the%20generated%20text%20draws%20upon%20contextual%20cues%0Afrom%20both%20the%20source%20document%20and%20the%20cited%20paper%2C%20ensuring%20accurate%20and%0Arelevant%20citation%20information%20is%20provided.%20Previous%20work%20in%20the%20field%20of%0Acitation%20generation%20is%20mainly%20based%20on%20the%20text%20summarization%20of%20documents.%0AFollowing%20this%2C%20this%20paper%20presents%20a%20framework%2C%20and%20a%20comparative%20study%20to%0Ademonstrate%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20the%20task%20of%20citation%0Ageneration.%20Also%2C%20we%20have%20shown%20the%20improvement%20in%20the%20results%20of%20citation%0Ageneration%20by%20incorporating%20the%20knowledge%20graph%20relations%20of%20the%20papers%20in%20the%0Aprompt%20for%20the%20LLM%20to%20better%20learn%20the%20relationship%20between%20the%20papers.%20To%0Aassess%20how%20well%20our%20model%20is%20performing%2C%20we%20have%20used%20a%20subset%20of%20standard%0AS2ORC%20dataset%2C%20which%20only%20consists%20of%20computer%20science%20academic%20research%20papers%0Ain%20the%20English%20Language.%20Vicuna%20performs%20best%20for%20this%20task%20with%2014.15%20Meteor%2C%0A12.88%20Rouge-1%2C%201.52%20Rouge-2%2C%20and%2010.94%20Rouge-L.%20Also%2C%20Alpaca%20performs%20best%2C%20and%0Aimproves%20the%20performance%20by%2036.98%25%20in%20Rouge-1%2C%20and%2033.14%25%20in%20Meteor%20by%0Aincluding%20knowledge%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09763v1&entry.124074799=Read"},
{"title": "SyncDreamer: Generating Multiview-consistent Images from a Single-view\n  Image", "author": "Yuan Liu and Cheng Lin and Zijiao Zeng and Xiaoxiao Long and Lingjie Liu and Taku Komura and Wenping Wang", "abstract": "  In this paper, we present a novel diffusion model called that generates\nmultiview-consistent images from a single-view image. Using pretrained\nlarge-scale 2D diffusion models, recent work Zero123 demonstrates the ability\nto generate plausible novel views from a single-view image of an object.\nHowever, maintaining consistency in geometry and colors for the generated\nimages remains a challenge. To address this issue, we propose a synchronized\nmultiview diffusion model that models the joint probability distribution of\nmultiview images, enabling the generation of multiview-consistent images in a\nsingle reverse process. SyncDreamer synchronizes the intermediate states of all\nthe generated images at every step of the reverse process through a 3D-aware\nfeature attention mechanism that correlates the corresponding features across\ndifferent views. Experiments show that SyncDreamer generates images with high\nconsistency across different views, thus making it well-suited for various 3D\ngeneration tasks such as novel-view-synthesis, text-to-3D, and image-to-3D.\n", "link": "http://arxiv.org/abs/2309.03453v2", "date": "2024-04-15", "relevancy": 1.7623, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6104}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5865}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5668}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SyncDreamer%3A%20Generating%20Multiview-consistent%20Images%20from%20a%20Single-view%0A%20%20Image&body=Title%3A%20SyncDreamer%3A%20Generating%20Multiview-consistent%20Images%20from%20a%20Single-view%0A%20%20Image%0AAuthor%3A%20Yuan%20Liu%20and%20Cheng%20Lin%20and%20Zijiao%20Zeng%20and%20Xiaoxiao%20Long%20and%20Lingjie%20Liu%20and%20Taku%20Komura%20and%20Wenping%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20diffusion%20model%20called%20that%20generates%0Amultiview-consistent%20images%20from%20a%20single-view%20image.%20Using%20pretrained%0Alarge-scale%202D%20diffusion%20models%2C%20recent%20work%20Zero123%20demonstrates%20the%20ability%0Ato%20generate%20plausible%20novel%20views%20from%20a%20single-view%20image%20of%20an%20object.%0AHowever%2C%20maintaining%20consistency%20in%20geometry%20and%20colors%20for%20the%20generated%0Aimages%20remains%20a%20challenge.%20To%20address%20this%20issue%2C%20we%20propose%20a%20synchronized%0Amultiview%20diffusion%20model%20that%20models%20the%20joint%20probability%20distribution%20of%0Amultiview%20images%2C%20enabling%20the%20generation%20of%20multiview-consistent%20images%20in%20a%0Asingle%20reverse%20process.%20SyncDreamer%20synchronizes%20the%20intermediate%20states%20of%20all%0Athe%20generated%20images%20at%20every%20step%20of%20the%20reverse%20process%20through%20a%203D-aware%0Afeature%20attention%20mechanism%20that%20correlates%20the%20corresponding%20features%20across%0Adifferent%20views.%20Experiments%20show%20that%20SyncDreamer%20generates%20images%20with%20high%0Aconsistency%20across%20different%20views%2C%20thus%20making%20it%20well-suited%20for%20various%203D%0Ageneration%20tasks%20such%20as%20novel-view-synthesis%2C%20text-to-3D%2C%20and%20image-to-3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.03453v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyncDreamer%3A%20Generating%20Multiview-consistent%20Images%20from%20a%20Single-view%0A%20%20Image&entry.906535625=Yuan%20Liu%20and%20Cheng%20Lin%20and%20Zijiao%20Zeng%20and%20Xiaoxiao%20Long%20and%20Lingjie%20Liu%20and%20Taku%20Komura%20and%20Wenping%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20diffusion%20model%20called%20that%20generates%0Amultiview-consistent%20images%20from%20a%20single-view%20image.%20Using%20pretrained%0Alarge-scale%202D%20diffusion%20models%2C%20recent%20work%20Zero123%20demonstrates%20the%20ability%0Ato%20generate%20plausible%20novel%20views%20from%20a%20single-view%20image%20of%20an%20object.%0AHowever%2C%20maintaining%20consistency%20in%20geometry%20and%20colors%20for%20the%20generated%0Aimages%20remains%20a%20challenge.%20To%20address%20this%20issue%2C%20we%20propose%20a%20synchronized%0Amultiview%20diffusion%20model%20that%20models%20the%20joint%20probability%20distribution%20of%0Amultiview%20images%2C%20enabling%20the%20generation%20of%20multiview-consistent%20images%20in%20a%0Asingle%20reverse%20process.%20SyncDreamer%20synchronizes%20the%20intermediate%20states%20of%20all%0Athe%20generated%20images%20at%20every%20step%20of%20the%20reverse%20process%20through%20a%203D-aware%0Afeature%20attention%20mechanism%20that%20correlates%20the%20corresponding%20features%20across%0Adifferent%20views.%20Experiments%20show%20that%20SyncDreamer%20generates%20images%20with%20high%0Aconsistency%20across%20different%20views%2C%20thus%20making%20it%20well-suited%20for%20various%203D%0Ageneration%20tasks%20such%20as%20novel-view-synthesis%2C%20text-to-3D%2C%20and%20image-to-3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.03453v2&entry.124074799=Read"},
{"title": "Equipping Diffusion Models with Differentiable Spatial Entropy for\n  Low-Light Image Enhancement", "author": "Wenyi Lian and Wenjing Lian and Ziwei Luo", "abstract": "  Image restoration, which aims to recover high-quality images from their\ncorrupted counterparts, often faces the challenge of being an ill-posed problem\nthat allows multiple solutions for a single input. However, most deep learning\nbased works simply employ l1 loss to train their network in a deterministic\nway, resulting in over-smoothed predictions with inferior perceptual quality.\nIn this work, we propose a novel method that shifts the focus from a\ndeterministic pixel-by-pixel comparison to a statistical perspective,\nemphasizing the learning of distributions rather than individual pixel values.\nThe core idea is to introduce spatial entropy into the loss function to measure\nthe distribution difference between predictions and targets. To make this\nspatial entropy differentiable, we employ kernel density estimation (KDE) to\napproximate the probabilities for specific intensity values of each pixel with\ntheir neighbor areas. Specifically, we equip the entropy with diffusion models\nand aim for superior accuracy and enhanced perceptual quality over l1 based\nnoise matching loss. In the experiments, we evaluate the proposed method for\nlow light enhancement on two datasets and the NTIRE challenge 2024. All these\nresults illustrate the effectiveness of our statistic-based entropy loss. Code\nis available at https://github.com/shermanlian/spatial-entropy-loss.\n", "link": "http://arxiv.org/abs/2404.09735v1", "date": "2024-04-15", "relevancy": 1.7528, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6305}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5711}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.571}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Equipping%20Diffusion%20Models%20with%20Differentiable%20Spatial%20Entropy%20for%0A%20%20Low-Light%20Image%20Enhancement&body=Title%3A%20Equipping%20Diffusion%20Models%20with%20Differentiable%20Spatial%20Entropy%20for%0A%20%20Low-Light%20Image%20Enhancement%0AAuthor%3A%20Wenyi%20Lian%20and%20Wenjing%20Lian%20and%20Ziwei%20Luo%0AAbstract%3A%20%20%20Image%20restoration%2C%20which%20aims%20to%20recover%20high-quality%20images%20from%20their%0Acorrupted%20counterparts%2C%20often%20faces%20the%20challenge%20of%20being%20an%20ill-posed%20problem%0Athat%20allows%20multiple%20solutions%20for%20a%20single%20input.%20However%2C%20most%20deep%20learning%0Abased%20works%20simply%20employ%20l1%20loss%20to%20train%20their%20network%20in%20a%20deterministic%0Away%2C%20resulting%20in%20over-smoothed%20predictions%20with%20inferior%20perceptual%20quality.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20method%20that%20shifts%20the%20focus%20from%20a%0Adeterministic%20pixel-by-pixel%20comparison%20to%20a%20statistical%20perspective%2C%0Aemphasizing%20the%20learning%20of%20distributions%20rather%20than%20individual%20pixel%20values.%0AThe%20core%20idea%20is%20to%20introduce%20spatial%20entropy%20into%20the%20loss%20function%20to%20measure%0Athe%20distribution%20difference%20between%20predictions%20and%20targets.%20To%20make%20this%0Aspatial%20entropy%20differentiable%2C%20we%20employ%20kernel%20density%20estimation%20%28KDE%29%20to%0Aapproximate%20the%20probabilities%20for%20specific%20intensity%20values%20of%20each%20pixel%20with%0Atheir%20neighbor%20areas.%20Specifically%2C%20we%20equip%20the%20entropy%20with%20diffusion%20models%0Aand%20aim%20for%20superior%20accuracy%20and%20enhanced%20perceptual%20quality%20over%20l1%20based%0Anoise%20matching%20loss.%20In%20the%20experiments%2C%20we%20evaluate%20the%20proposed%20method%20for%0Alow%20light%20enhancement%20on%20two%20datasets%20and%20the%20NTIRE%20challenge%202024.%20All%20these%0Aresults%20illustrate%20the%20effectiveness%20of%20our%20statistic-based%20entropy%20loss.%20Code%0Ais%20available%20at%20https%3A//github.com/shermanlian/spatial-entropy-loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09735v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equipping%20Diffusion%20Models%20with%20Differentiable%20Spatial%20Entropy%20for%0A%20%20Low-Light%20Image%20Enhancement&entry.906535625=Wenyi%20Lian%20and%20Wenjing%20Lian%20and%20Ziwei%20Luo&entry.1292438233=%20%20Image%20restoration%2C%20which%20aims%20to%20recover%20high-quality%20images%20from%20their%0Acorrupted%20counterparts%2C%20often%20faces%20the%20challenge%20of%20being%20an%20ill-posed%20problem%0Athat%20allows%20multiple%20solutions%20for%20a%20single%20input.%20However%2C%20most%20deep%20learning%0Abased%20works%20simply%20employ%20l1%20loss%20to%20train%20their%20network%20in%20a%20deterministic%0Away%2C%20resulting%20in%20over-smoothed%20predictions%20with%20inferior%20perceptual%20quality.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20method%20that%20shifts%20the%20focus%20from%20a%0Adeterministic%20pixel-by-pixel%20comparison%20to%20a%20statistical%20perspective%2C%0Aemphasizing%20the%20learning%20of%20distributions%20rather%20than%20individual%20pixel%20values.%0AThe%20core%20idea%20is%20to%20introduce%20spatial%20entropy%20into%20the%20loss%20function%20to%20measure%0Athe%20distribution%20difference%20between%20predictions%20and%20targets.%20To%20make%20this%0Aspatial%20entropy%20differentiable%2C%20we%20employ%20kernel%20density%20estimation%20%28KDE%29%20to%0Aapproximate%20the%20probabilities%20for%20specific%20intensity%20values%20of%20each%20pixel%20with%0Atheir%20neighbor%20areas.%20Specifically%2C%20we%20equip%20the%20entropy%20with%20diffusion%20models%0Aand%20aim%20for%20superior%20accuracy%20and%20enhanced%20perceptual%20quality%20over%20l1%20based%0Anoise%20matching%20loss.%20In%20the%20experiments%2C%20we%20evaluate%20the%20proposed%20method%20for%0Alow%20light%20enhancement%20on%20two%20datasets%20and%20the%20NTIRE%20challenge%202024.%20All%20these%0Aresults%20illustrate%20the%20effectiveness%20of%20our%20statistic-based%20entropy%20loss.%20Code%0Ais%20available%20at%20https%3A//github.com/shermanlian/spatial-entropy-loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09735v1&entry.124074799=Read"},
{"title": "Enhancing Robot Explanation Capabilities through Vision-Language Models:\n  a Preliminary Study by Interpreting Visual Inputs for Improved Human-Robot\n  Interaction", "author": "David Sobr\u00edn-Hidalgo and Miguel \u00c1ngel Gonz\u00e1lez-Santamarta and \u00c1ngel Manuel Guerrero-Higueras and Francisco Javier Rodr\u00edguez-Lera and Vicente Matell\u00e1n-Olivera", "abstract": "  This paper presents an improved system based on our prior work, designed to\ncreate explanations for autonomous robot actions during Human-Robot Interaction\n(HRI). Previously, we developed a system that used Large Language Models (LLMs)\nto interpret logs and produce natural language explanations. In this study, we\nexpand our approach by incorporating Vision-Language Models (VLMs), enabling\nthe system to analyze textual logs with the added context of visual input. This\nmethod allows for generating explanations that combine data from the robot's\nlogs and the images it captures. We tested this enhanced system on a basic\nnavigation task where the robot needs to avoid a human obstacle. The findings\nfrom this preliminary study indicate that adding visual interpretation improves\nour system's explanations by precisely identifying obstacles and increasing the\naccuracy of the explanations provided.\n", "link": "http://arxiv.org/abs/2404.09705v1", "date": "2024-04-15", "relevancy": 1.7224, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6053}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5723}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5624}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Robot%20Explanation%20Capabilities%20through%20Vision-Language%20Models%3A%0A%20%20a%20Preliminary%20Study%20by%20Interpreting%20Visual%20Inputs%20for%20Improved%20Human-Robot%0A%20%20Interaction&body=Title%3A%20Enhancing%20Robot%20Explanation%20Capabilities%20through%20Vision-Language%20Models%3A%0A%20%20a%20Preliminary%20Study%20by%20Interpreting%20Visual%20Inputs%20for%20Improved%20Human-Robot%0A%20%20Interaction%0AAuthor%3A%20David%20Sobr%C3%ADn-Hidalgo%20and%20Miguel%20%C3%81ngel%20Gonz%C3%A1lez-Santamarta%20and%20%C3%81ngel%20Manuel%20Guerrero-Higueras%20and%20Francisco%20Javier%20Rodr%C3%ADguez-Lera%20and%20Vicente%20Matell%C3%A1n-Olivera%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20improved%20system%20based%20on%20our%20prior%20work%2C%20designed%20to%0Acreate%20explanations%20for%20autonomous%20robot%20actions%20during%20Human-Robot%20Interaction%0A%28HRI%29.%20Previously%2C%20we%20developed%20a%20system%20that%20used%20Large%20Language%20Models%20%28LLMs%29%0Ato%20interpret%20logs%20and%20produce%20natural%20language%20explanations.%20In%20this%20study%2C%20we%0Aexpand%20our%20approach%20by%20incorporating%20Vision-Language%20Models%20%28VLMs%29%2C%20enabling%0Athe%20system%20to%20analyze%20textual%20logs%20with%20the%20added%20context%20of%20visual%20input.%20This%0Amethod%20allows%20for%20generating%20explanations%20that%20combine%20data%20from%20the%20robot%27s%0Alogs%20and%20the%20images%20it%20captures.%20We%20tested%20this%20enhanced%20system%20on%20a%20basic%0Anavigation%20task%20where%20the%20robot%20needs%20to%20avoid%20a%20human%20obstacle.%20The%20findings%0Afrom%20this%20preliminary%20study%20indicate%20that%20adding%20visual%20interpretation%20improves%0Aour%20system%27s%20explanations%20by%20precisely%20identifying%20obstacles%20and%20increasing%20the%0Aaccuracy%20of%20the%20explanations%20provided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09705v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Robot%20Explanation%20Capabilities%20through%20Vision-Language%20Models%3A%0A%20%20a%20Preliminary%20Study%20by%20Interpreting%20Visual%20Inputs%20for%20Improved%20Human-Robot%0A%20%20Interaction&entry.906535625=David%20Sobr%C3%ADn-Hidalgo%20and%20Miguel%20%C3%81ngel%20Gonz%C3%A1lez-Santamarta%20and%20%C3%81ngel%20Manuel%20Guerrero-Higueras%20and%20Francisco%20Javier%20Rodr%C3%ADguez-Lera%20and%20Vicente%20Matell%C3%A1n-Olivera&entry.1292438233=%20%20This%20paper%20presents%20an%20improved%20system%20based%20on%20our%20prior%20work%2C%20designed%20to%0Acreate%20explanations%20for%20autonomous%20robot%20actions%20during%20Human-Robot%20Interaction%0A%28HRI%29.%20Previously%2C%20we%20developed%20a%20system%20that%20used%20Large%20Language%20Models%20%28LLMs%29%0Ato%20interpret%20logs%20and%20produce%20natural%20language%20explanations.%20In%20this%20study%2C%20we%0Aexpand%20our%20approach%20by%20incorporating%20Vision-Language%20Models%20%28VLMs%29%2C%20enabling%0Athe%20system%20to%20analyze%20textual%20logs%20with%20the%20added%20context%20of%20visual%20input.%20This%0Amethod%20allows%20for%20generating%20explanations%20that%20combine%20data%20from%20the%20robot%27s%0Alogs%20and%20the%20images%20it%20captures.%20We%20tested%20this%20enhanced%20system%20on%20a%20basic%0Anavigation%20task%20where%20the%20robot%20needs%20to%20avoid%20a%20human%20obstacle.%20The%20findings%0Afrom%20this%20preliminary%20study%20indicate%20that%20adding%20visual%20interpretation%20improves%0Aour%20system%27s%20explanations%20by%20precisely%20identifying%20obstacles%20and%20increasing%20the%0Aaccuracy%20of%20the%20explanations%20provided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09705v1&entry.124074799=Read"},
{"title": "Amplitude-Phase Fusion for Enhanced Electrocardiogram Morphological\n  Analysis", "author": "Shuaicong Hu and Yanan Wang and Jian Liu and Jingyu Lin and Shengmei Qin and Zhenning Nie and Zhifeng Yao and Wenjie Cai and Cuiwei Yang", "abstract": "  Considering the variability of amplitude and phase patterns in\nelectrocardiogram (ECG) signals due to cardiac activity and individual\ndifferences, existing entropy-based studies have not fully utilized these two\npatterns and lack integration. To address this gap, this paper proposes a novel\nfusion entropy metric, morphological ECG entropy (MEE) for the first time,\nspecifically designed for ECG morphology, to comprehensively describe the\nfusion of amplitude and phase patterns. MEE is computed based on beat-level\nsamples, enabling detailed analysis of each cardiac cycle. Experimental results\ndemonstrate that MEE achieves rapid, accurate, and label-free localization of\nabnormal ECG arrhythmia regions. Furthermore, MEE provides a method for\nassessing sample diversity, facilitating compression of imbalanced training\nsets (via representative sample selection), and outperforms random pruning.\nAdditionally, MEE exhibits the ability to describe areas of poor quality. By\ndiscussing, it proves the robustness of MEE value calculation to noise\ninterference and its low computational complexity. Finally, we integrate this\nmethod into a clinical interactive interface to provide a more convenient and\nintuitive user experience. These findings indicate that MEE serves as a\nvaluable clinical descriptor for ECG characterization. The implementation code\ncan be referenced at the following link:\nhttps://github.com/fdu-harry/ECG-MEE-metric.\n", "link": "http://arxiv.org/abs/2404.09729v1", "date": "2024-04-15", "relevancy": 1.7019, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4327}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.427}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4177}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Amplitude-Phase%20Fusion%20for%20Enhanced%20Electrocardiogram%20Morphological%0A%20%20Analysis&body=Title%3A%20Amplitude-Phase%20Fusion%20for%20Enhanced%20Electrocardiogram%20Morphological%0A%20%20Analysis%0AAuthor%3A%20Shuaicong%20Hu%20and%20Yanan%20Wang%20and%20Jian%20Liu%20and%20Jingyu%20Lin%20and%20Shengmei%20Qin%20and%20Zhenning%20Nie%20and%20Zhifeng%20Yao%20and%20Wenjie%20Cai%20and%20Cuiwei%20Yang%0AAbstract%3A%20%20%20Considering%20the%20variability%20of%20amplitude%20and%20phase%20patterns%20in%0Aelectrocardiogram%20%28ECG%29%20signals%20due%20to%20cardiac%20activity%20and%20individual%0Adifferences%2C%20existing%20entropy-based%20studies%20have%20not%20fully%20utilized%20these%20two%0Apatterns%20and%20lack%20integration.%20To%20address%20this%20gap%2C%20this%20paper%20proposes%20a%20novel%0Afusion%20entropy%20metric%2C%20morphological%20ECG%20entropy%20%28MEE%29%20for%20the%20first%20time%2C%0Aspecifically%20designed%20for%20ECG%20morphology%2C%20to%20comprehensively%20describe%20the%0Afusion%20of%20amplitude%20and%20phase%20patterns.%20MEE%20is%20computed%20based%20on%20beat-level%0Asamples%2C%20enabling%20detailed%20analysis%20of%20each%20cardiac%20cycle.%20Experimental%20results%0Ademonstrate%20that%20MEE%20achieves%20rapid%2C%20accurate%2C%20and%20label-free%20localization%20of%0Aabnormal%20ECG%20arrhythmia%20regions.%20Furthermore%2C%20MEE%20provides%20a%20method%20for%0Aassessing%20sample%20diversity%2C%20facilitating%20compression%20of%20imbalanced%20training%0Asets%20%28via%20representative%20sample%20selection%29%2C%20and%20outperforms%20random%20pruning.%0AAdditionally%2C%20MEE%20exhibits%20the%20ability%20to%20describe%20areas%20of%20poor%20quality.%20By%0Adiscussing%2C%20it%20proves%20the%20robustness%20of%20MEE%20value%20calculation%20to%20noise%0Ainterference%20and%20its%20low%20computational%20complexity.%20Finally%2C%20we%20integrate%20this%0Amethod%20into%20a%20clinical%20interactive%20interface%20to%20provide%20a%20more%20convenient%20and%0Aintuitive%20user%20experience.%20These%20findings%20indicate%20that%20MEE%20serves%20as%20a%0Avaluable%20clinical%20descriptor%20for%20ECG%20characterization.%20The%20implementation%20code%0Acan%20be%20referenced%20at%20the%20following%20link%3A%0Ahttps%3A//github.com/fdu-harry/ECG-MEE-metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09729v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Amplitude-Phase%20Fusion%20for%20Enhanced%20Electrocardiogram%20Morphological%0A%20%20Analysis&entry.906535625=Shuaicong%20Hu%20and%20Yanan%20Wang%20and%20Jian%20Liu%20and%20Jingyu%20Lin%20and%20Shengmei%20Qin%20and%20Zhenning%20Nie%20and%20Zhifeng%20Yao%20and%20Wenjie%20Cai%20and%20Cuiwei%20Yang&entry.1292438233=%20%20Considering%20the%20variability%20of%20amplitude%20and%20phase%20patterns%20in%0Aelectrocardiogram%20%28ECG%29%20signals%20due%20to%20cardiac%20activity%20and%20individual%0Adifferences%2C%20existing%20entropy-based%20studies%20have%20not%20fully%20utilized%20these%20two%0Apatterns%20and%20lack%20integration.%20To%20address%20this%20gap%2C%20this%20paper%20proposes%20a%20novel%0Afusion%20entropy%20metric%2C%20morphological%20ECG%20entropy%20%28MEE%29%20for%20the%20first%20time%2C%0Aspecifically%20designed%20for%20ECG%20morphology%2C%20to%20comprehensively%20describe%20the%0Afusion%20of%20amplitude%20and%20phase%20patterns.%20MEE%20is%20computed%20based%20on%20beat-level%0Asamples%2C%20enabling%20detailed%20analysis%20of%20each%20cardiac%20cycle.%20Experimental%20results%0Ademonstrate%20that%20MEE%20achieves%20rapid%2C%20accurate%2C%20and%20label-free%20localization%20of%0Aabnormal%20ECG%20arrhythmia%20regions.%20Furthermore%2C%20MEE%20provides%20a%20method%20for%0Aassessing%20sample%20diversity%2C%20facilitating%20compression%20of%20imbalanced%20training%0Asets%20%28via%20representative%20sample%20selection%29%2C%20and%20outperforms%20random%20pruning.%0AAdditionally%2C%20MEE%20exhibits%20the%20ability%20to%20describe%20areas%20of%20poor%20quality.%20By%0Adiscussing%2C%20it%20proves%20the%20robustness%20of%20MEE%20value%20calculation%20to%20noise%0Ainterference%20and%20its%20low%20computational%20complexity.%20Finally%2C%20we%20integrate%20this%0Amethod%20into%20a%20clinical%20interactive%20interface%20to%20provide%20a%20more%20convenient%20and%0Aintuitive%20user%20experience.%20These%20findings%20indicate%20that%20MEE%20serves%20as%20a%0Avaluable%20clinical%20descriptor%20for%20ECG%20characterization.%20The%20implementation%20code%0Acan%20be%20referenced%20at%20the%20following%20link%3A%0Ahttps%3A//github.com/fdu-harry/ECG-MEE-metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09729v1&entry.124074799=Read"},
{"title": "Are Large Language Models Reliable Argument Quality Annotators?", "author": "Nailia Mirzakhmedova and Marcel Gohsen and Chia Hao Chang and Benno Stein", "abstract": "  Evaluating the quality of arguments is a crucial aspect of any system\nleveraging argument mining. However, it is a challenge to obtain reliable and\nconsistent annotations regarding argument quality, as this usually requires\ndomain-specific expertise of the annotators. Even among experts, the assessment\nof argument quality is often inconsistent due to the inherent subjectivity of\nthis task. In this paper, we study the potential of using state-of-the-art\nlarge language models (LLMs) as proxies for argument quality annotators. To\nassess the capability of LLMs in this regard, we analyze the agreement between\nmodel, human expert, and human novice annotators based on an established\ntaxonomy of argument quality dimensions. Our findings highlight that LLMs can\nproduce consistent annotations, with a moderately high agreement with human\nexperts across most of the quality dimensions. Moreover, we show that using\nLLMs as additional annotators can significantly improve the agreement between\nannotators. These results suggest that LLMs can serve as a valuable tool for\nautomated argument quality assessment, thus streamlining and accelerating the\nevaluation of large argument datasets.\n", "link": "http://arxiv.org/abs/2404.09696v1", "date": "2024-04-15", "relevancy": 1.6917, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5095}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4068}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4044}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Are%20Large%20Language%20Models%20Reliable%20Argument%20Quality%20Annotators%3F&body=Title%3A%20Are%20Large%20Language%20Models%20Reliable%20Argument%20Quality%20Annotators%3F%0AAuthor%3A%20Nailia%20Mirzakhmedova%20and%20Marcel%20Gohsen%20and%20Chia%20Hao%20Chang%20and%20Benno%20Stein%0AAbstract%3A%20%20%20Evaluating%20the%20quality%20of%20arguments%20is%20a%20crucial%20aspect%20of%20any%20system%0Aleveraging%20argument%20mining.%20However%2C%20it%20is%20a%20challenge%20to%20obtain%20reliable%20and%0Aconsistent%20annotations%20regarding%20argument%20quality%2C%20as%20this%20usually%20requires%0Adomain-specific%20expertise%20of%20the%20annotators.%20Even%20among%20experts%2C%20the%20assessment%0Aof%20argument%20quality%20is%20often%20inconsistent%20due%20to%20the%20inherent%20subjectivity%20of%0Athis%20task.%20In%20this%20paper%2C%20we%20study%20the%20potential%20of%20using%20state-of-the-art%0Alarge%20language%20models%20%28LLMs%29%20as%20proxies%20for%20argument%20quality%20annotators.%20To%0Aassess%20the%20capability%20of%20LLMs%20in%20this%20regard%2C%20we%20analyze%20the%20agreement%20between%0Amodel%2C%20human%20expert%2C%20and%20human%20novice%20annotators%20based%20on%20an%20established%0Ataxonomy%20of%20argument%20quality%20dimensions.%20Our%20findings%20highlight%20that%20LLMs%20can%0Aproduce%20consistent%20annotations%2C%20with%20a%20moderately%20high%20agreement%20with%20human%0Aexperts%20across%20most%20of%20the%20quality%20dimensions.%20Moreover%2C%20we%20show%20that%20using%0ALLMs%20as%20additional%20annotators%20can%20significantly%20improve%20the%20agreement%20between%0Aannotators.%20These%20results%20suggest%20that%20LLMs%20can%20serve%20as%20a%20valuable%20tool%20for%0Aautomated%20argument%20quality%20assessment%2C%20thus%20streamlining%20and%20accelerating%20the%0Aevaluation%20of%20large%20argument%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09696v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Large%20Language%20Models%20Reliable%20Argument%20Quality%20Annotators%3F&entry.906535625=Nailia%20Mirzakhmedova%20and%20Marcel%20Gohsen%20and%20Chia%20Hao%20Chang%20and%20Benno%20Stein&entry.1292438233=%20%20Evaluating%20the%20quality%20of%20arguments%20is%20a%20crucial%20aspect%20of%20any%20system%0Aleveraging%20argument%20mining.%20However%2C%20it%20is%20a%20challenge%20to%20obtain%20reliable%20and%0Aconsistent%20annotations%20regarding%20argument%20quality%2C%20as%20this%20usually%20requires%0Adomain-specific%20expertise%20of%20the%20annotators.%20Even%20among%20experts%2C%20the%20assessment%0Aof%20argument%20quality%20is%20often%20inconsistent%20due%20to%20the%20inherent%20subjectivity%20of%0Athis%20task.%20In%20this%20paper%2C%20we%20study%20the%20potential%20of%20using%20state-of-the-art%0Alarge%20language%20models%20%28LLMs%29%20as%20proxies%20for%20argument%20quality%20annotators.%20To%0Aassess%20the%20capability%20of%20LLMs%20in%20this%20regard%2C%20we%20analyze%20the%20agreement%20between%0Amodel%2C%20human%20expert%2C%20and%20human%20novice%20annotators%20based%20on%20an%20established%0Ataxonomy%20of%20argument%20quality%20dimensions.%20Our%20findings%20highlight%20that%20LLMs%20can%0Aproduce%20consistent%20annotations%2C%20with%20a%20moderately%20high%20agreement%20with%20human%0Aexperts%20across%20most%20of%20the%20quality%20dimensions.%20Moreover%2C%20we%20show%20that%20using%0ALLMs%20as%20additional%20annotators%20can%20significantly%20improve%20the%20agreement%20between%0Aannotators.%20These%20results%20suggest%20that%20LLMs%20can%20serve%20as%20a%20valuable%20tool%20for%0Aautomated%20argument%20quality%20assessment%2C%20thus%20streamlining%20and%20accelerating%20the%0Aevaluation%20of%20large%20argument%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09696v1&entry.124074799=Read"},
{"title": "AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics\n  Perception", "author": "Yipo Huang and Xiangfei Sheng and Zhichao Yang and Quan Yuan and Zhichao Duan and Pengfei Chen and Leida Li and Weisi Lin and Guangming Shi", "abstract": "  The highly abstract nature of image aesthetics perception (IAP) poses\nsignificant challenge for current multimodal large language models (MLLMs). The\nlack of human-annotated multi-modality aesthetic data further exacerbates this\ndilemma, resulting in MLLMs falling short of aesthetics perception\ncapabilities. To address the above challenge, we first introduce a\ncomprehensively annotated Aesthetic Multi-Modality Instruction Tuning (AesMMIT)\ndataset, which serves as the footstone for building multi-modality aesthetics\nfoundation models. Specifically, to align MLLMs with human aesthetics\nperception, we construct a corpus-rich aesthetic critique database with 21,904\ndiverse-sourced images and 88K human natural language feedbacks, which are\ncollected via progressive questions, ranging from coarse-grained aesthetic\ngrades to fine-grained aesthetic descriptions. To ensure that MLLMs can handle\ndiverse queries, we further prompt GPT to refine the aesthetic critiques and\nassemble the large-scale aesthetic instruction tuning dataset, i.e. AesMMIT,\nwhich consists of 409K multi-typed instructions to activate stronger aesthetic\ncapabilities. Based on the AesMMIT database, we fine-tune the open-sourced\ngeneral foundation models, achieving multi-modality Aesthetic Expert models,\ndubbed AesExpert. Extensive experiments demonstrate that the proposed AesExpert\nmodels deliver significantly better aesthetic perception performances than the\nstate-of-the-art MLLMs, including the most advanced GPT-4V and\nGemini-Pro-Vision. Source data will be available at\nhttps://github.com/yipoh/AesExpert.\n", "link": "http://arxiv.org/abs/2404.09624v1", "date": "2024-04-15", "relevancy": 1.6898, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5652}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5162}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AesExpert%3A%20Towards%20Multi-modality%20Foundation%20Model%20for%20Image%20Aesthetics%0A%20%20Perception&body=Title%3A%20AesExpert%3A%20Towards%20Multi-modality%20Foundation%20Model%20for%20Image%20Aesthetics%0A%20%20Perception%0AAuthor%3A%20Yipo%20Huang%20and%20Xiangfei%20Sheng%20and%20Zhichao%20Yang%20and%20Quan%20Yuan%20and%20Zhichao%20Duan%20and%20Pengfei%20Chen%20and%20Leida%20Li%20and%20Weisi%20Lin%20and%20Guangming%20Shi%0AAbstract%3A%20%20%20The%20highly%20abstract%20nature%20of%20image%20aesthetics%20perception%20%28IAP%29%20poses%0Asignificant%20challenge%20for%20current%20multimodal%20large%20language%20models%20%28MLLMs%29.%20The%0Alack%20of%20human-annotated%20multi-modality%20aesthetic%20data%20further%20exacerbates%20this%0Adilemma%2C%20resulting%20in%20MLLMs%20falling%20short%20of%20aesthetics%20perception%0Acapabilities.%20To%20address%20the%20above%20challenge%2C%20we%20first%20introduce%20a%0Acomprehensively%20annotated%20Aesthetic%20Multi-Modality%20Instruction%20Tuning%20%28AesMMIT%29%0Adataset%2C%20which%20serves%20as%20the%20footstone%20for%20building%20multi-modality%20aesthetics%0Afoundation%20models.%20Specifically%2C%20to%20align%20MLLMs%20with%20human%20aesthetics%0Aperception%2C%20we%20construct%20a%20corpus-rich%20aesthetic%20critique%20database%20with%2021%2C904%0Adiverse-sourced%20images%20and%2088K%20human%20natural%20language%20feedbacks%2C%20which%20are%0Acollected%20via%20progressive%20questions%2C%20ranging%20from%20coarse-grained%20aesthetic%0Agrades%20to%20fine-grained%20aesthetic%20descriptions.%20To%20ensure%20that%20MLLMs%20can%20handle%0Adiverse%20queries%2C%20we%20further%20prompt%20GPT%20to%20refine%20the%20aesthetic%20critiques%20and%0Aassemble%20the%20large-scale%20aesthetic%20instruction%20tuning%20dataset%2C%20i.e.%20AesMMIT%2C%0Awhich%20consists%20of%20409K%20multi-typed%20instructions%20to%20activate%20stronger%20aesthetic%0Acapabilities.%20Based%20on%20the%20AesMMIT%20database%2C%20we%20fine-tune%20the%20open-sourced%0Ageneral%20foundation%20models%2C%20achieving%20multi-modality%20Aesthetic%20Expert%20models%2C%0Adubbed%20AesExpert.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20AesExpert%0Amodels%20deliver%20significantly%20better%20aesthetic%20perception%20performances%20than%20the%0Astate-of-the-art%20MLLMs%2C%20including%20the%20most%20advanced%20GPT-4V%20and%0AGemini-Pro-Vision.%20Source%20data%20will%20be%20available%20at%0Ahttps%3A//github.com/yipoh/AesExpert.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09624v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AesExpert%3A%20Towards%20Multi-modality%20Foundation%20Model%20for%20Image%20Aesthetics%0A%20%20Perception&entry.906535625=Yipo%20Huang%20and%20Xiangfei%20Sheng%20and%20Zhichao%20Yang%20and%20Quan%20Yuan%20and%20Zhichao%20Duan%20and%20Pengfei%20Chen%20and%20Leida%20Li%20and%20Weisi%20Lin%20and%20Guangming%20Shi&entry.1292438233=%20%20The%20highly%20abstract%20nature%20of%20image%20aesthetics%20perception%20%28IAP%29%20poses%0Asignificant%20challenge%20for%20current%20multimodal%20large%20language%20models%20%28MLLMs%29.%20The%0Alack%20of%20human-annotated%20multi-modality%20aesthetic%20data%20further%20exacerbates%20this%0Adilemma%2C%20resulting%20in%20MLLMs%20falling%20short%20of%20aesthetics%20perception%0Acapabilities.%20To%20address%20the%20above%20challenge%2C%20we%20first%20introduce%20a%0Acomprehensively%20annotated%20Aesthetic%20Multi-Modality%20Instruction%20Tuning%20%28AesMMIT%29%0Adataset%2C%20which%20serves%20as%20the%20footstone%20for%20building%20multi-modality%20aesthetics%0Afoundation%20models.%20Specifically%2C%20to%20align%20MLLMs%20with%20human%20aesthetics%0Aperception%2C%20we%20construct%20a%20corpus-rich%20aesthetic%20critique%20database%20with%2021%2C904%0Adiverse-sourced%20images%20and%2088K%20human%20natural%20language%20feedbacks%2C%20which%20are%0Acollected%20via%20progressive%20questions%2C%20ranging%20from%20coarse-grained%20aesthetic%0Agrades%20to%20fine-grained%20aesthetic%20descriptions.%20To%20ensure%20that%20MLLMs%20can%20handle%0Adiverse%20queries%2C%20we%20further%20prompt%20GPT%20to%20refine%20the%20aesthetic%20critiques%20and%0Aassemble%20the%20large-scale%20aesthetic%20instruction%20tuning%20dataset%2C%20i.e.%20AesMMIT%2C%0Awhich%20consists%20of%20409K%20multi-typed%20instructions%20to%20activate%20stronger%20aesthetic%0Acapabilities.%20Based%20on%20the%20AesMMIT%20database%2C%20we%20fine-tune%20the%20open-sourced%0Ageneral%20foundation%20models%2C%20achieving%20multi-modality%20Aesthetic%20Expert%20models%2C%0Adubbed%20AesExpert.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20AesExpert%0Amodels%20deliver%20significantly%20better%20aesthetic%20perception%20performances%20than%20the%0Astate-of-the-art%20MLLMs%2C%20including%20the%20most%20advanced%20GPT-4V%20and%0AGemini-Pro-Vision.%20Source%20data%20will%20be%20available%20at%0Ahttps%3A//github.com/yipoh/AesExpert.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09624v1&entry.124074799=Read"},
{"title": "FSRT: Facial Scene Representation Transformer for Face Reenactment from\n  Factorized Appearance, Head-pose, and Facial Expression Features", "author": "Andre Rochow and Max Schwarz and Sven Behnke", "abstract": "  The task of face reenactment is to transfer the head motion and facial\nexpressions from a driving video to the appearance of a source image, which may\nbe of a different person (cross-reenactment). Most existing methods are\nCNN-based and estimate optical flow from the source image to the current\ndriving frame, which is then inpainted and refined to produce the output\nanimation. We propose a transformer-based encoder for computing a set-latent\nrepresentation of the source image(s). We then predict the output color of a\nquery pixel using a transformer-based decoder, which is conditioned with\nkeypoints and a facial expression vector extracted from the driving frame.\nLatent representations of the source person are learned in a self-supervised\nmanner that factorize their appearance, head pose, and facial expressions.\nThus, they are perfectly suited for cross-reenactment. In contrast to most\nrelated work, our method naturally extends to multiple source images and can\nthus adapt to person-specific facial dynamics. We also propose data\naugmentation and regularization schemes that are necessary to prevent\noverfitting and support generalizability of the learned representations. We\nevaluated our approach in a randomized user study. The results indicate\nsuperior performance compared to the state-of-the-art in terms of motion\ntransfer quality and temporal consistency.\n", "link": "http://arxiv.org/abs/2404.09736v1", "date": "2024-04-15", "relevancy": 1.6766, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5863}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5517}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5508}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FSRT%3A%20Facial%20Scene%20Representation%20Transformer%20for%20Face%20Reenactment%20from%0A%20%20Factorized%20Appearance%2C%20Head-pose%2C%20and%20Facial%20Expression%20Features&body=Title%3A%20FSRT%3A%20Facial%20Scene%20Representation%20Transformer%20for%20Face%20Reenactment%20from%0A%20%20Factorized%20Appearance%2C%20Head-pose%2C%20and%20Facial%20Expression%20Features%0AAuthor%3A%20Andre%20Rochow%20and%20Max%20Schwarz%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20The%20task%20of%20face%20reenactment%20is%20to%20transfer%20the%20head%20motion%20and%20facial%0Aexpressions%20from%20a%20driving%20video%20to%20the%20appearance%20of%20a%20source%20image%2C%20which%20may%0Abe%20of%20a%20different%20person%20%28cross-reenactment%29.%20Most%20existing%20methods%20are%0ACNN-based%20and%20estimate%20optical%20flow%20from%20the%20source%20image%20to%20the%20current%0Adriving%20frame%2C%20which%20is%20then%20inpainted%20and%20refined%20to%20produce%20the%20output%0Aanimation.%20We%20propose%20a%20transformer-based%20encoder%20for%20computing%20a%20set-latent%0Arepresentation%20of%20the%20source%20image%28s%29.%20We%20then%20predict%20the%20output%20color%20of%20a%0Aquery%20pixel%20using%20a%20transformer-based%20decoder%2C%20which%20is%20conditioned%20with%0Akeypoints%20and%20a%20facial%20expression%20vector%20extracted%20from%20the%20driving%20frame.%0ALatent%20representations%20of%20the%20source%20person%20are%20learned%20in%20a%20self-supervised%0Amanner%20that%20factorize%20their%20appearance%2C%20head%20pose%2C%20and%20facial%20expressions.%0AThus%2C%20they%20are%20perfectly%20suited%20for%20cross-reenactment.%20In%20contrast%20to%20most%0Arelated%20work%2C%20our%20method%20naturally%20extends%20to%20multiple%20source%20images%20and%20can%0Athus%20adapt%20to%20person-specific%20facial%20dynamics.%20We%20also%20propose%20data%0Aaugmentation%20and%20regularization%20schemes%20that%20are%20necessary%20to%20prevent%0Aoverfitting%20and%20support%20generalizability%20of%20the%20learned%20representations.%20We%0Aevaluated%20our%20approach%20in%20a%20randomized%20user%20study.%20The%20results%20indicate%0Asuperior%20performance%20compared%20to%20the%20state-of-the-art%20in%20terms%20of%20motion%0Atransfer%20quality%20and%20temporal%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09736v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FSRT%3A%20Facial%20Scene%20Representation%20Transformer%20for%20Face%20Reenactment%20from%0A%20%20Factorized%20Appearance%2C%20Head-pose%2C%20and%20Facial%20Expression%20Features&entry.906535625=Andre%20Rochow%20and%20Max%20Schwarz%20and%20Sven%20Behnke&entry.1292438233=%20%20The%20task%20of%20face%20reenactment%20is%20to%20transfer%20the%20head%20motion%20and%20facial%0Aexpressions%20from%20a%20driving%20video%20to%20the%20appearance%20of%20a%20source%20image%2C%20which%20may%0Abe%20of%20a%20different%20person%20%28cross-reenactment%29.%20Most%20existing%20methods%20are%0ACNN-based%20and%20estimate%20optical%20flow%20from%20the%20source%20image%20to%20the%20current%0Adriving%20frame%2C%20which%20is%20then%20inpainted%20and%20refined%20to%20produce%20the%20output%0Aanimation.%20We%20propose%20a%20transformer-based%20encoder%20for%20computing%20a%20set-latent%0Arepresentation%20of%20the%20source%20image%28s%29.%20We%20then%20predict%20the%20output%20color%20of%20a%0Aquery%20pixel%20using%20a%20transformer-based%20decoder%2C%20which%20is%20conditioned%20with%0Akeypoints%20and%20a%20facial%20expression%20vector%20extracted%20from%20the%20driving%20frame.%0ALatent%20representations%20of%20the%20source%20person%20are%20learned%20in%20a%20self-supervised%0Amanner%20that%20factorize%20their%20appearance%2C%20head%20pose%2C%20and%20facial%20expressions.%0AThus%2C%20they%20are%20perfectly%20suited%20for%20cross-reenactment.%20In%20contrast%20to%20most%0Arelated%20work%2C%20our%20method%20naturally%20extends%20to%20multiple%20source%20images%20and%20can%0Athus%20adapt%20to%20person-specific%20facial%20dynamics.%20We%20also%20propose%20data%0Aaugmentation%20and%20regularization%20schemes%20that%20are%20necessary%20to%20prevent%0Aoverfitting%20and%20support%20generalizability%20of%20the%20learned%20representations.%20We%0Aevaluated%20our%20approach%20in%20a%20randomized%20user%20study.%20The%20results%20indicate%0Asuperior%20performance%20compared%20to%20the%20state-of-the-art%20in%20terms%20of%20motion%0Atransfer%20quality%20and%20temporal%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09736v1&entry.124074799=Read"},
{"title": "In-Context Translation: Towards Unifying Image Recognition, Processing,\n  and Generation", "author": "Han Xue and Qianru Sun and Li Song and Wenjun Zhang and Zhiwu Huang", "abstract": "  We propose In-Context Translation (ICT), a general learning framework to\nunify visual recognition (e.g., semantic segmentation), low-level image\nprocessing (e.g., denoising), and conditional image generation (e.g.,\nedge-to-image synthesis). Thanks to unification, ICT significantly reduces the\ninherent inductive bias that comes with designing models for specific tasks,\nand it maximizes mutual enhancement across similar tasks. However, the\nunification across a large number of tasks is non-trivial due to various data\nformats and training pipelines. To this end, ICT introduces two designs.\nFirstly, it standardizes input-output data of different tasks into RGB image\npairs, e.g., semantic segmentation data pairs an RGB image with its\nsegmentation mask in the same RGB format. This turns different tasks into a\ngeneral translation task between two RGB images. Secondly, it standardizes the\ntraining of different tasks into a general in-context learning, where\n\"in-context\" means the input comprises an example input-output pair of the\ntarget task and a query image. The learning objective is to generate the\n\"missing\" data paired with the query. The implicit translation process is thus\nbetween the query and the generated image. In experiments, ICT unifies ten\nvision tasks and showcases impressive performance on their respective\nbenchmarks. Notably, compared to its competitors, e.g., Painter and\nPromptDiffusion, ICT trained on only 4 RTX 3090 GPUs is shown to be more\nefficient and less costly in training.\n", "link": "http://arxiv.org/abs/2404.09633v1", "date": "2024-04-15", "relevancy": 1.6661, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5597}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5564}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5435}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20In-Context%20Translation%3A%20Towards%20Unifying%20Image%20Recognition%2C%20Processing%2C%0A%20%20and%20Generation&body=Title%3A%20In-Context%20Translation%3A%20Towards%20Unifying%20Image%20Recognition%2C%20Processing%2C%0A%20%20and%20Generation%0AAuthor%3A%20Han%20Xue%20and%20Qianru%20Sun%20and%20Li%20Song%20and%20Wenjun%20Zhang%20and%20Zhiwu%20Huang%0AAbstract%3A%20%20%20We%20propose%20In-Context%20Translation%20%28ICT%29%2C%20a%20general%20learning%20framework%20to%0Aunify%20visual%20recognition%20%28e.g.%2C%20semantic%20segmentation%29%2C%20low-level%20image%0Aprocessing%20%28e.g.%2C%20denoising%29%2C%20and%20conditional%20image%20generation%20%28e.g.%2C%0Aedge-to-image%20synthesis%29.%20Thanks%20to%20unification%2C%20ICT%20significantly%20reduces%20the%0Ainherent%20inductive%20bias%20that%20comes%20with%20designing%20models%20for%20specific%20tasks%2C%0Aand%20it%20maximizes%20mutual%20enhancement%20across%20similar%20tasks.%20However%2C%20the%0Aunification%20across%20a%20large%20number%20of%20tasks%20is%20non-trivial%20due%20to%20various%20data%0Aformats%20and%20training%20pipelines.%20To%20this%20end%2C%20ICT%20introduces%20two%20designs.%0AFirstly%2C%20it%20standardizes%20input-output%20data%20of%20different%20tasks%20into%20RGB%20image%0Apairs%2C%20e.g.%2C%20semantic%20segmentation%20data%20pairs%20an%20RGB%20image%20with%20its%0Asegmentation%20mask%20in%20the%20same%20RGB%20format.%20This%20turns%20different%20tasks%20into%20a%0Ageneral%20translation%20task%20between%20two%20RGB%20images.%20Secondly%2C%20it%20standardizes%20the%0Atraining%20of%20different%20tasks%20into%20a%20general%20in-context%20learning%2C%20where%0A%22in-context%22%20means%20the%20input%20comprises%20an%20example%20input-output%20pair%20of%20the%0Atarget%20task%20and%20a%20query%20image.%20The%20learning%20objective%20is%20to%20generate%20the%0A%22missing%22%20data%20paired%20with%20the%20query.%20The%20implicit%20translation%20process%20is%20thus%0Abetween%20the%20query%20and%20the%20generated%20image.%20In%20experiments%2C%20ICT%20unifies%20ten%0Avision%20tasks%20and%20showcases%20impressive%20performance%20on%20their%20respective%0Abenchmarks.%20Notably%2C%20compared%20to%20its%20competitors%2C%20e.g.%2C%20Painter%20and%0APromptDiffusion%2C%20ICT%20trained%20on%20only%204%20RTX%203090%20GPUs%20is%20shown%20to%20be%20more%0Aefficient%20and%20less%20costly%20in%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09633v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Translation%3A%20Towards%20Unifying%20Image%20Recognition%2C%20Processing%2C%0A%20%20and%20Generation&entry.906535625=Han%20Xue%20and%20Qianru%20Sun%20and%20Li%20Song%20and%20Wenjun%20Zhang%20and%20Zhiwu%20Huang&entry.1292438233=%20%20We%20propose%20In-Context%20Translation%20%28ICT%29%2C%20a%20general%20learning%20framework%20to%0Aunify%20visual%20recognition%20%28e.g.%2C%20semantic%20segmentation%29%2C%20low-level%20image%0Aprocessing%20%28e.g.%2C%20denoising%29%2C%20and%20conditional%20image%20generation%20%28e.g.%2C%0Aedge-to-image%20synthesis%29.%20Thanks%20to%20unification%2C%20ICT%20significantly%20reduces%20the%0Ainherent%20inductive%20bias%20that%20comes%20with%20designing%20models%20for%20specific%20tasks%2C%0Aand%20it%20maximizes%20mutual%20enhancement%20across%20similar%20tasks.%20However%2C%20the%0Aunification%20across%20a%20large%20number%20of%20tasks%20is%20non-trivial%20due%20to%20various%20data%0Aformats%20and%20training%20pipelines.%20To%20this%20end%2C%20ICT%20introduces%20two%20designs.%0AFirstly%2C%20it%20standardizes%20input-output%20data%20of%20different%20tasks%20into%20RGB%20image%0Apairs%2C%20e.g.%2C%20semantic%20segmentation%20data%20pairs%20an%20RGB%20image%20with%20its%0Asegmentation%20mask%20in%20the%20same%20RGB%20format.%20This%20turns%20different%20tasks%20into%20a%0Ageneral%20translation%20task%20between%20two%20RGB%20images.%20Secondly%2C%20it%20standardizes%20the%0Atraining%20of%20different%20tasks%20into%20a%20general%20in-context%20learning%2C%20where%0A%22in-context%22%20means%20the%20input%20comprises%20an%20example%20input-output%20pair%20of%20the%0Atarget%20task%20and%20a%20query%20image.%20The%20learning%20objective%20is%20to%20generate%20the%0A%22missing%22%20data%20paired%20with%20the%20query.%20The%20implicit%20translation%20process%20is%20thus%0Abetween%20the%20query%20and%20the%20generated%20image.%20In%20experiments%2C%20ICT%20unifies%20ten%0Avision%20tasks%20and%20showcases%20impressive%20performance%20on%20their%20respective%0Abenchmarks.%20Notably%2C%20compared%20to%20its%20competitors%2C%20e.g.%2C%20Painter%20and%0APromptDiffusion%2C%20ICT%20trained%20on%20only%204%20RTX%203090%20GPUs%20is%20shown%20to%20be%20more%0Aefficient%20and%20less%20costly%20in%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09633v1&entry.124074799=Read"},
{"title": "Real-world Instance-specific Image Goal Navigation for Service Robots:\n  Bridging the Domain Gap with Contrastive Learning", "author": "Taichi Sakaguchi and Akira Taniguchi and Yoshinobu Hagiwara and Lotfi El Hafi and Shoichi Hasegawa and Tadahiro Taniguchi", "abstract": "  Improving instance-specific image goal navigation (InstanceImageNav), which\nlocates the identical object in a real-world environment from a query image, is\nessential for robotic systems to assist users in finding desired objects. The\nchallenge lies in the domain gap between low-quality images observed by the\nmoving robot, characterized by motion blur and low-resolution, and high-quality\nquery images provided by the user. Such domain gaps could significantly reduce\nthe task success rate but have not been the focus of previous work. To address\nthis, we propose a novel method called Few-shot Cross-quality Instance-aware\nAdaptation (CrossIA), which employs contrastive learning with an instance\nclassifier to align features between massive low- and few high-quality images.\nThis approach effectively reduces the domain gap by bringing the latent\nrepresentations of cross-quality images closer on an instance basis.\nAdditionally, the system integrates an object image collection with a\npre-trained deblurring model to enhance the observed image quality. Our method\nfine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA. We\nevaluated our method's effectiveness through an InstanceImageNav task with 20\ndifferent types of instances, where the robot identifies the same instance in a\nreal-world environment as a high-quality query image. Our experiments showed\nthat our method improves the task success rate by up to three times compared to\nthe baseline, a conventional approach based on SuperGlue. These findings\nhighlight the potential of leveraging contrastive learning and image\nenhancement techniques to bridge the domain gap and improve object localization\nin robotic applications. The project website is\nhttps://emergentsystemlabstudent.github.io/DomainBridgingNav/.\n", "link": "http://arxiv.org/abs/2404.09645v1", "date": "2024-04-15", "relevancy": 1.6577, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5628}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5501}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5485}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Real-world%20Instance-specific%20Image%20Goal%20Navigation%20for%20Service%20Robots%3A%0A%20%20Bridging%20the%20Domain%20Gap%20with%20Contrastive%20Learning&body=Title%3A%20Real-world%20Instance-specific%20Image%20Goal%20Navigation%20for%20Service%20Robots%3A%0A%20%20Bridging%20the%20Domain%20Gap%20with%20Contrastive%20Learning%0AAuthor%3A%20Taichi%20Sakaguchi%20and%20Akira%20Taniguchi%20and%20Yoshinobu%20Hagiwara%20and%20Lotfi%20El%20Hafi%20and%20Shoichi%20Hasegawa%20and%20Tadahiro%20Taniguchi%0AAbstract%3A%20%20%20Improving%20instance-specific%20image%20goal%20navigation%20%28InstanceImageNav%29%2C%20which%0Alocates%20the%20identical%20object%20in%20a%20real-world%20environment%20from%20a%20query%20image%2C%20is%0Aessential%20for%20robotic%20systems%20to%20assist%20users%20in%20finding%20desired%20objects.%20The%0Achallenge%20lies%20in%20the%20domain%20gap%20between%20low-quality%20images%20observed%20by%20the%0Amoving%20robot%2C%20characterized%20by%20motion%20blur%20and%20low-resolution%2C%20and%20high-quality%0Aquery%20images%20provided%20by%20the%20user.%20Such%20domain%20gaps%20could%20significantly%20reduce%0Athe%20task%20success%20rate%20but%20have%20not%20been%20the%20focus%20of%20previous%20work.%20To%20address%0Athis%2C%20we%20propose%20a%20novel%20method%20called%20Few-shot%20Cross-quality%20Instance-aware%0AAdaptation%20%28CrossIA%29%2C%20which%20employs%20contrastive%20learning%20with%20an%20instance%0Aclassifier%20to%20align%20features%20between%20massive%20low-%20and%20few%20high-quality%20images.%0AThis%20approach%20effectively%20reduces%20the%20domain%20gap%20by%20bringing%20the%20latent%0Arepresentations%20of%20cross-quality%20images%20closer%20on%20an%20instance%20basis.%0AAdditionally%2C%20the%20system%20integrates%20an%20object%20image%20collection%20with%20a%0Apre-trained%20deblurring%20model%20to%20enhance%20the%20observed%20image%20quality.%20Our%20method%0Afine-tunes%20the%20SimSiam%20model%2C%20pre-trained%20on%20ImageNet%2C%20using%20CrossIA.%20We%0Aevaluated%20our%20method%27s%20effectiveness%20through%20an%20InstanceImageNav%20task%20with%2020%0Adifferent%20types%20of%20instances%2C%20where%20the%20robot%20identifies%20the%20same%20instance%20in%20a%0Areal-world%20environment%20as%20a%20high-quality%20query%20image.%20Our%20experiments%20showed%0Athat%20our%20method%20improves%20the%20task%20success%20rate%20by%20up%20to%20three%20times%20compared%20to%0Athe%20baseline%2C%20a%20conventional%20approach%20based%20on%20SuperGlue.%20These%20findings%0Ahighlight%20the%20potential%20of%20leveraging%20contrastive%20learning%20and%20image%0Aenhancement%20techniques%20to%20bridge%20the%20domain%20gap%20and%20improve%20object%20localization%0Ain%20robotic%20applications.%20The%20project%20website%20is%0Ahttps%3A//emergentsystemlabstudent.github.io/DomainBridgingNav/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09645v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-world%20Instance-specific%20Image%20Goal%20Navigation%20for%20Service%20Robots%3A%0A%20%20Bridging%20the%20Domain%20Gap%20with%20Contrastive%20Learning&entry.906535625=Taichi%20Sakaguchi%20and%20Akira%20Taniguchi%20and%20Yoshinobu%20Hagiwara%20and%20Lotfi%20El%20Hafi%20and%20Shoichi%20Hasegawa%20and%20Tadahiro%20Taniguchi&entry.1292438233=%20%20Improving%20instance-specific%20image%20goal%20navigation%20%28InstanceImageNav%29%2C%20which%0Alocates%20the%20identical%20object%20in%20a%20real-world%20environment%20from%20a%20query%20image%2C%20is%0Aessential%20for%20robotic%20systems%20to%20assist%20users%20in%20finding%20desired%20objects.%20The%0Achallenge%20lies%20in%20the%20domain%20gap%20between%20low-quality%20images%20observed%20by%20the%0Amoving%20robot%2C%20characterized%20by%20motion%20blur%20and%20low-resolution%2C%20and%20high-quality%0Aquery%20images%20provided%20by%20the%20user.%20Such%20domain%20gaps%20could%20significantly%20reduce%0Athe%20task%20success%20rate%20but%20have%20not%20been%20the%20focus%20of%20previous%20work.%20To%20address%0Athis%2C%20we%20propose%20a%20novel%20method%20called%20Few-shot%20Cross-quality%20Instance-aware%0AAdaptation%20%28CrossIA%29%2C%20which%20employs%20contrastive%20learning%20with%20an%20instance%0Aclassifier%20to%20align%20features%20between%20massive%20low-%20and%20few%20high-quality%20images.%0AThis%20approach%20effectively%20reduces%20the%20domain%20gap%20by%20bringing%20the%20latent%0Arepresentations%20of%20cross-quality%20images%20closer%20on%20an%20instance%20basis.%0AAdditionally%2C%20the%20system%20integrates%20an%20object%20image%20collection%20with%20a%0Apre-trained%20deblurring%20model%20to%20enhance%20the%20observed%20image%20quality.%20Our%20method%0Afine-tunes%20the%20SimSiam%20model%2C%20pre-trained%20on%20ImageNet%2C%20using%20CrossIA.%20We%0Aevaluated%20our%20method%27s%20effectiveness%20through%20an%20InstanceImageNav%20task%20with%2020%0Adifferent%20types%20of%20instances%2C%20where%20the%20robot%20identifies%20the%20same%20instance%20in%20a%0Areal-world%20environment%20as%20a%20high-quality%20query%20image.%20Our%20experiments%20showed%0Athat%20our%20method%20improves%20the%20task%20success%20rate%20by%20up%20to%20three%20times%20compared%20to%0Athe%20baseline%2C%20a%20conventional%20approach%20based%20on%20SuperGlue.%20These%20findings%0Ahighlight%20the%20potential%20of%20leveraging%20contrastive%20learning%20and%20image%0Aenhancement%20techniques%20to%20bridge%20the%20domain%20gap%20and%20improve%20object%20localization%0Ain%20robotic%20applications.%20The%20project%20website%20is%0Ahttps%3A//emergentsystemlabstudent.github.io/DomainBridgingNav/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09645v1&entry.124074799=Read"},
{"title": "AMPCliff: quantitative definition and benchmarking of activity cliffs in\n  antimicrobial peptides", "author": "Kewei Li and Yuqian Wu and Yutong Guo and Yinheng Li and Yusi Fan and Ruochi Zhang and Lan Huang and Fengfeng Zhou", "abstract": "  Activity cliff (AC) is a phenomenon that a pair of similar molecules differ\nby a small structural alternation but exhibit a large difference in their\nbiochemical activities. The AC of small molecules has been extensively\ninvestigated but limited knowledge is accumulated about the AC phenomenon in\npeptides with canonical amino acids. This study introduces a quantitative\ndefinition and benchmarking framework AMPCliff for the AC phenomenon in\nantimicrobial peptides (AMPs) composed by canonical amino acids. A\ncomprehensive analysis of the existing AMP dataset reveals a significant\nprevalence of AC within AMPs. AMPCliff quantifies the activities of AMPs by the\nmetric minimum inhibitory concentration (MIC), and defines 0.9 as the minimum\nthreshold for the normalized BLOSUM62 similarity score between a pair of\naligned peptides with at least two-fold MIC changes. This study establishes a\nbenchmark dataset of paired AMPs in Staphylococcus aureus from the publicly\navailable AMP dataset GRAMPA, and conducts a rigorous procedure to evaluate\nvarious AMP AC prediction models, including nine machine learning, four deep\nlearning algorithms, four masked language models, and four generative language\nmodels. Our analysis reveals that these models are capable of detecting AMP AC\nevents and the pre-trained protein language ESM2 model demonstrates superior\nperformance across the evaluations. The predictive performance of AMP activity\ncliffs remains to be further improved, considering that ESM2 with 33 layers\nonly achieves the Spearman correlation coefficient=0.50 for the regression task\nof the MIC values on the benchmark dataset. Source code and additional\nresources are available at https://www.healthinformaticslab.org/supp/ or\nhttps://github.com/Kewei2023/AMPCliff-generation.\n", "link": "http://arxiv.org/abs/2404.09738v1", "date": "2024-04-15", "relevancy": 1.6438, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4184}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4083}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.399}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AMPCliff%3A%20quantitative%20definition%20and%20benchmarking%20of%20activity%20cliffs%20in%0A%20%20antimicrobial%20peptides&body=Title%3A%20AMPCliff%3A%20quantitative%20definition%20and%20benchmarking%20of%20activity%20cliffs%20in%0A%20%20antimicrobial%20peptides%0AAuthor%3A%20Kewei%20Li%20and%20Yuqian%20Wu%20and%20Yutong%20Guo%20and%20Yinheng%20Li%20and%20Yusi%20Fan%20and%20Ruochi%20Zhang%20and%20Lan%20Huang%20and%20Fengfeng%20Zhou%0AAbstract%3A%20%20%20Activity%20cliff%20%28AC%29%20is%20a%20phenomenon%20that%20a%20pair%20of%20similar%20molecules%20differ%0Aby%20a%20small%20structural%20alternation%20but%20exhibit%20a%20large%20difference%20in%20their%0Abiochemical%20activities.%20The%20AC%20of%20small%20molecules%20has%20been%20extensively%0Ainvestigated%20but%20limited%20knowledge%20is%20accumulated%20about%20the%20AC%20phenomenon%20in%0Apeptides%20with%20canonical%20amino%20acids.%20This%20study%20introduces%20a%20quantitative%0Adefinition%20and%20benchmarking%20framework%20AMPCliff%20for%20the%20AC%20phenomenon%20in%0Aantimicrobial%20peptides%20%28AMPs%29%20composed%20by%20canonical%20amino%20acids.%20A%0Acomprehensive%20analysis%20of%20the%20existing%20AMP%20dataset%20reveals%20a%20significant%0Aprevalence%20of%20AC%20within%20AMPs.%20AMPCliff%20quantifies%20the%20activities%20of%20AMPs%20by%20the%0Ametric%20minimum%20inhibitory%20concentration%20%28MIC%29%2C%20and%20defines%200.9%20as%20the%20minimum%0Athreshold%20for%20the%20normalized%20BLOSUM62%20similarity%20score%20between%20a%20pair%20of%0Aaligned%20peptides%20with%20at%20least%20two-fold%20MIC%20changes.%20This%20study%20establishes%20a%0Abenchmark%20dataset%20of%20paired%20AMPs%20in%20Staphylococcus%20aureus%20from%20the%20publicly%0Aavailable%20AMP%20dataset%20GRAMPA%2C%20and%20conducts%20a%20rigorous%20procedure%20to%20evaluate%0Avarious%20AMP%20AC%20prediction%20models%2C%20including%20nine%20machine%20learning%2C%20four%20deep%0Alearning%20algorithms%2C%20four%20masked%20language%20models%2C%20and%20four%20generative%20language%0Amodels.%20Our%20analysis%20reveals%20that%20these%20models%20are%20capable%20of%20detecting%20AMP%20AC%0Aevents%20and%20the%20pre-trained%20protein%20language%20ESM2%20model%20demonstrates%20superior%0Aperformance%20across%20the%20evaluations.%20The%20predictive%20performance%20of%20AMP%20activity%0Acliffs%20remains%20to%20be%20further%20improved%2C%20considering%20that%20ESM2%20with%2033%20layers%0Aonly%20achieves%20the%20Spearman%20correlation%20coefficient%3D0.50%20for%20the%20regression%20task%0Aof%20the%20MIC%20values%20on%20the%20benchmark%20dataset.%20Source%20code%20and%20additional%0Aresources%20are%20available%20at%20https%3A//www.healthinformaticslab.org/supp/%20or%0Ahttps%3A//github.com/Kewei2023/AMPCliff-generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09738v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMPCliff%3A%20quantitative%20definition%20and%20benchmarking%20of%20activity%20cliffs%20in%0A%20%20antimicrobial%20peptides&entry.906535625=Kewei%20Li%20and%20Yuqian%20Wu%20and%20Yutong%20Guo%20and%20Yinheng%20Li%20and%20Yusi%20Fan%20and%20Ruochi%20Zhang%20and%20Lan%20Huang%20and%20Fengfeng%20Zhou&entry.1292438233=%20%20Activity%20cliff%20%28AC%29%20is%20a%20phenomenon%20that%20a%20pair%20of%20similar%20molecules%20differ%0Aby%20a%20small%20structural%20alternation%20but%20exhibit%20a%20large%20difference%20in%20their%0Abiochemical%20activities.%20The%20AC%20of%20small%20molecules%20has%20been%20extensively%0Ainvestigated%20but%20limited%20knowledge%20is%20accumulated%20about%20the%20AC%20phenomenon%20in%0Apeptides%20with%20canonical%20amino%20acids.%20This%20study%20introduces%20a%20quantitative%0Adefinition%20and%20benchmarking%20framework%20AMPCliff%20for%20the%20AC%20phenomenon%20in%0Aantimicrobial%20peptides%20%28AMPs%29%20composed%20by%20canonical%20amino%20acids.%20A%0Acomprehensive%20analysis%20of%20the%20existing%20AMP%20dataset%20reveals%20a%20significant%0Aprevalence%20of%20AC%20within%20AMPs.%20AMPCliff%20quantifies%20the%20activities%20of%20AMPs%20by%20the%0Ametric%20minimum%20inhibitory%20concentration%20%28MIC%29%2C%20and%20defines%200.9%20as%20the%20minimum%0Athreshold%20for%20the%20normalized%20BLOSUM62%20similarity%20score%20between%20a%20pair%20of%0Aaligned%20peptides%20with%20at%20least%20two-fold%20MIC%20changes.%20This%20study%20establishes%20a%0Abenchmark%20dataset%20of%20paired%20AMPs%20in%20Staphylococcus%20aureus%20from%20the%20publicly%0Aavailable%20AMP%20dataset%20GRAMPA%2C%20and%20conducts%20a%20rigorous%20procedure%20to%20evaluate%0Avarious%20AMP%20AC%20prediction%20models%2C%20including%20nine%20machine%20learning%2C%20four%20deep%0Alearning%20algorithms%2C%20four%20masked%20language%20models%2C%20and%20four%20generative%20language%0Amodels.%20Our%20analysis%20reveals%20that%20these%20models%20are%20capable%20of%20detecting%20AMP%20AC%0Aevents%20and%20the%20pre-trained%20protein%20language%20ESM2%20model%20demonstrates%20superior%0Aperformance%20across%20the%20evaluations.%20The%20predictive%20performance%20of%20AMP%20activity%0Acliffs%20remains%20to%20be%20further%20improved%2C%20considering%20that%20ESM2%20with%2033%20layers%0Aonly%20achieves%20the%20Spearman%20correlation%20coefficient%3D0.50%20for%20the%20regression%20task%0Aof%20the%20MIC%20values%20on%20the%20benchmark%20dataset.%20Source%20code%20and%20additional%0Aresources%20are%20available%20at%20https%3A//www.healthinformaticslab.org/supp/%20or%0Ahttps%3A//github.com/Kewei2023/AMPCliff-generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09738v1&entry.124074799=Read"},
{"title": "Plus Strategies are Exponentially Slower for Planted Optima of Random\n  Height", "author": "Johannes Lengler and Leon Schiller and Oliver Sieberling", "abstract": "  We compare the $(1,\\lambda)$-EA and the $(1 + \\lambda)$-EA on the recently\nintroduced benchmark DisOM, which is the OneMax function with randomly planted\nlocal optima. Previous work showed that if all local optima have the same\nrelative height, then the plus strategy never loses more than a factor $O(n\\log\nn)$ compared to the comma strategy. Here we show that even small random\nfluctuations in the heights of the local optima have a devastating effect for\nthe plus strategy and lead to super-polynomial runtimes. On the other hand, due\nto their ability to escape local optima, comma strategies are unaffected by the\nheight of the local optima and remain efficient. Our results hold for a broad\nclass of possible distortions and show that the plus strategy, but not the\ncomma strategy, is generally deceived by sparse unstructured fluctuations of a\nsmooth landscape.\n", "link": "http://arxiv.org/abs/2404.09687v1", "date": "2024-04-15", "relevancy": 1.6231, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4097}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4084}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3894}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Plus%20Strategies%20are%20Exponentially%20Slower%20for%20Planted%20Optima%20of%20Random%0A%20%20Height&body=Title%3A%20Plus%20Strategies%20are%20Exponentially%20Slower%20for%20Planted%20Optima%20of%20Random%0A%20%20Height%0AAuthor%3A%20Johannes%20Lengler%20and%20Leon%20Schiller%20and%20Oliver%20Sieberling%0AAbstract%3A%20%20%20We%20compare%20the%20%24%281%2C%5Clambda%29%24-EA%20and%20the%20%24%281%20%2B%20%5Clambda%29%24-EA%20on%20the%20recently%0Aintroduced%20benchmark%20DisOM%2C%20which%20is%20the%20OneMax%20function%20with%20randomly%20planted%0Alocal%20optima.%20Previous%20work%20showed%20that%20if%20all%20local%20optima%20have%20the%20same%0Arelative%20height%2C%20then%20the%20plus%20strategy%20never%20loses%20more%20than%20a%20factor%20%24O%28n%5Clog%0An%29%24%20compared%20to%20the%20comma%20strategy.%20Here%20we%20show%20that%20even%20small%20random%0Afluctuations%20in%20the%20heights%20of%20the%20local%20optima%20have%20a%20devastating%20effect%20for%0Athe%20plus%20strategy%20and%20lead%20to%20super-polynomial%20runtimes.%20On%20the%20other%20hand%2C%20due%0Ato%20their%20ability%20to%20escape%20local%20optima%2C%20comma%20strategies%20are%20unaffected%20by%20the%0Aheight%20of%20the%20local%20optima%20and%20remain%20efficient.%20Our%20results%20hold%20for%20a%20broad%0Aclass%20of%20possible%20distortions%20and%20show%20that%20the%20plus%20strategy%2C%20but%20not%20the%0Acomma%20strategy%2C%20is%20generally%20deceived%20by%20sparse%20unstructured%20fluctuations%20of%20a%0Asmooth%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09687v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plus%20Strategies%20are%20Exponentially%20Slower%20for%20Planted%20Optima%20of%20Random%0A%20%20Height&entry.906535625=Johannes%20Lengler%20and%20Leon%20Schiller%20and%20Oliver%20Sieberling&entry.1292438233=%20%20We%20compare%20the%20%24%281%2C%5Clambda%29%24-EA%20and%20the%20%24%281%20%2B%20%5Clambda%29%24-EA%20on%20the%20recently%0Aintroduced%20benchmark%20DisOM%2C%20which%20is%20the%20OneMax%20function%20with%20randomly%20planted%0Alocal%20optima.%20Previous%20work%20showed%20that%20if%20all%20local%20optima%20have%20the%20same%0Arelative%20height%2C%20then%20the%20plus%20strategy%20never%20loses%20more%20than%20a%20factor%20%24O%28n%5Clog%0An%29%24%20compared%20to%20the%20comma%20strategy.%20Here%20we%20show%20that%20even%20small%20random%0Afluctuations%20in%20the%20heights%20of%20the%20local%20optima%20have%20a%20devastating%20effect%20for%0Athe%20plus%20strategy%20and%20lead%20to%20super-polynomial%20runtimes.%20On%20the%20other%20hand%2C%20due%0Ato%20their%20ability%20to%20escape%20local%20optima%2C%20comma%20strategies%20are%20unaffected%20by%20the%0Aheight%20of%20the%20local%20optima%20and%20remain%20efficient.%20Our%20results%20hold%20for%20a%20broad%0Aclass%20of%20possible%20distortions%20and%20show%20that%20the%20plus%20strategy%2C%20but%20not%20the%0Acomma%20strategy%2C%20is%20generally%20deceived%20by%20sparse%20unstructured%20fluctuations%20of%20a%0Asmooth%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09687v1&entry.124074799=Read"},
{"title": "Towards Variable and Coordinated Holistic Co-Speech Motion Generation", "author": "Yifei Liu and Qiong Cao and Yandong Wen and Huaiguang Jiang and Changxing Ding", "abstract": "  This paper addresses the problem of generating lifelike holistic co-speech\nmotions for 3D avatars, focusing on two key aspects: variability and\ncoordination. Variability allows the avatar to exhibit a wide range of motions\neven with similar speech content, while coordination ensures a harmonious\nalignment among facial expressions, hand gestures, and body poses. We aim to\nachieve both with ProbTalk, a unified probabilistic framework designed to\njointly model facial, hand, and body movements in speech. ProbTalk builds on\nthe variational autoencoder (VAE) architecture and incorporates three core\ndesigns. First, we introduce product quantization (PQ) to the VAE, which\nenriches the representation of complex holistic motion. Second, we devise a\nnovel non-autoregressive model that embeds 2D positional encoding into the\nproduct-quantized representation, thereby preserving essential structure\ninformation of the PQ codes. Last, we employ a secondary stage to refine the\npreliminary prediction, further sharpening the high-frequency details. Coupling\nthese three designs enables ProbTalk to generate natural and diverse holistic\nco-speech motions, outperforming several state-of-the-art methods in\nqualitative and quantitative evaluations, particularly in terms of realism. Our\ncode and model will be released for research purposes at\nhttps://feifeifeiliu.github.io/probtalk/.\n", "link": "http://arxiv.org/abs/2404.00368v2", "date": "2024-04-15", "relevancy": 1.6079, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5553}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5513}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5221}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Variable%20and%20Coordinated%20Holistic%20Co-Speech%20Motion%20Generation&body=Title%3A%20Towards%20Variable%20and%20Coordinated%20Holistic%20Co-Speech%20Motion%20Generation%0AAuthor%3A%20Yifei%20Liu%20and%20Qiong%20Cao%20and%20Yandong%20Wen%20and%20Huaiguang%20Jiang%20and%20Changxing%20Ding%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20problem%20of%20generating%20lifelike%20holistic%20co-speech%0Amotions%20for%203D%20avatars%2C%20focusing%20on%20two%20key%20aspects%3A%20variability%20and%0Acoordination.%20Variability%20allows%20the%20avatar%20to%20exhibit%20a%20wide%20range%20of%20motions%0Aeven%20with%20similar%20speech%20content%2C%20while%20coordination%20ensures%20a%20harmonious%0Aalignment%20among%20facial%20expressions%2C%20hand%20gestures%2C%20and%20body%20poses.%20We%20aim%20to%0Aachieve%20both%20with%20ProbTalk%2C%20a%20unified%20probabilistic%20framework%20designed%20to%0Ajointly%20model%20facial%2C%20hand%2C%20and%20body%20movements%20in%20speech.%20ProbTalk%20builds%20on%0Athe%20variational%20autoencoder%20%28VAE%29%20architecture%20and%20incorporates%20three%20core%0Adesigns.%20First%2C%20we%20introduce%20product%20quantization%20%28PQ%29%20to%20the%20VAE%2C%20which%0Aenriches%20the%20representation%20of%20complex%20holistic%20motion.%20Second%2C%20we%20devise%20a%0Anovel%20non-autoregressive%20model%20that%20embeds%202D%20positional%20encoding%20into%20the%0Aproduct-quantized%20representation%2C%20thereby%20preserving%20essential%20structure%0Ainformation%20of%20the%20PQ%20codes.%20Last%2C%20we%20employ%20a%20secondary%20stage%20to%20refine%20the%0Apreliminary%20prediction%2C%20further%20sharpening%20the%20high-frequency%20details.%20Coupling%0Athese%20three%20designs%20enables%20ProbTalk%20to%20generate%20natural%20and%20diverse%20holistic%0Aco-speech%20motions%2C%20outperforming%20several%20state-of-the-art%20methods%20in%0Aqualitative%20and%20quantitative%20evaluations%2C%20particularly%20in%20terms%20of%20realism.%20Our%0Acode%20and%20model%20will%20be%20released%20for%20research%20purposes%20at%0Ahttps%3A//feifeifeiliu.github.io/probtalk/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00368v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Variable%20and%20Coordinated%20Holistic%20Co-Speech%20Motion%20Generation&entry.906535625=Yifei%20Liu%20and%20Qiong%20Cao%20and%20Yandong%20Wen%20and%20Huaiguang%20Jiang%20and%20Changxing%20Ding&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20generating%20lifelike%20holistic%20co-speech%0Amotions%20for%203D%20avatars%2C%20focusing%20on%20two%20key%20aspects%3A%20variability%20and%0Acoordination.%20Variability%20allows%20the%20avatar%20to%20exhibit%20a%20wide%20range%20of%20motions%0Aeven%20with%20similar%20speech%20content%2C%20while%20coordination%20ensures%20a%20harmonious%0Aalignment%20among%20facial%20expressions%2C%20hand%20gestures%2C%20and%20body%20poses.%20We%20aim%20to%0Aachieve%20both%20with%20ProbTalk%2C%20a%20unified%20probabilistic%20framework%20designed%20to%0Ajointly%20model%20facial%2C%20hand%2C%20and%20body%20movements%20in%20speech.%20ProbTalk%20builds%20on%0Athe%20variational%20autoencoder%20%28VAE%29%20architecture%20and%20incorporates%20three%20core%0Adesigns.%20First%2C%20we%20introduce%20product%20quantization%20%28PQ%29%20to%20the%20VAE%2C%20which%0Aenriches%20the%20representation%20of%20complex%20holistic%20motion.%20Second%2C%20we%20devise%20a%0Anovel%20non-autoregressive%20model%20that%20embeds%202D%20positional%20encoding%20into%20the%0Aproduct-quantized%20representation%2C%20thereby%20preserving%20essential%20structure%0Ainformation%20of%20the%20PQ%20codes.%20Last%2C%20we%20employ%20a%20secondary%20stage%20to%20refine%20the%0Apreliminary%20prediction%2C%20further%20sharpening%20the%20high-frequency%20details.%20Coupling%0Athese%20three%20designs%20enables%20ProbTalk%20to%20generate%20natural%20and%20diverse%20holistic%0Aco-speech%20motions%2C%20outperforming%20several%20state-of-the-art%20methods%20in%0Aqualitative%20and%20quantitative%20evaluations%2C%20particularly%20in%20terms%20of%20realism.%20Our%0Acode%20and%20model%20will%20be%20released%20for%20research%20purposes%20at%0Ahttps%3A//feifeifeiliu.github.io/probtalk/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00368v2&entry.124074799=Read"},
{"title": "A Generic Trajectory Planning Method for Constrained All-Wheel-Steering\n  Robots", "author": "Ren Xin and Hongji Liu and Yingbing Chen and Sheng Wang and Ming Liu", "abstract": "  This paper presents a trajectory planning method for wheeled robots with\nfixed steering axes while the steering angle of each wheel is constrained. In\nthe past, All-Wheel-Steering(AWS) robots, incorporating modes such as\nrotation-free translation maneuvers, in-situ rotational maneuvers, and\nproportional steering, exhibited inefficient performance due to time-consuming\nmode switches. This inefficiency arises from wheel rotation constraints and\ninter-wheel cooperation requirements. The direct application of a holonomic\nmoving strategy can lead to significant slip angles or even structural failure.\nAdditionally, the limited steering range of AWS wheeled robots exacerbates\nnonlinearity issues, thereby complicating control processes. To address these\nchallenges, we developed a novel planning method termed Constrained AWS(C-AWS),\nwhich integrates second-order discrete search with predictive control\ntechniques. Experimental results demonstrate that our method adeptly generates\nfeasible and smooth trajectories for C-AWS while adhering to steering angle\nconstraints.\n", "link": "http://arxiv.org/abs/2404.09677v1", "date": "2024-04-15", "relevancy": 1.5926, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.55}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5095}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5043}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Generic%20Trajectory%20Planning%20Method%20for%20Constrained%20All-Wheel-Steering%0A%20%20Robots&body=Title%3A%20A%20Generic%20Trajectory%20Planning%20Method%20for%20Constrained%20All-Wheel-Steering%0A%20%20Robots%0AAuthor%3A%20Ren%20Xin%20and%20Hongji%20Liu%20and%20Yingbing%20Chen%20and%20Sheng%20Wang%20and%20Ming%20Liu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20trajectory%20planning%20method%20for%20wheeled%20robots%20with%0Afixed%20steering%20axes%20while%20the%20steering%20angle%20of%20each%20wheel%20is%20constrained.%20In%0Athe%20past%2C%20All-Wheel-Steering%28AWS%29%20robots%2C%20incorporating%20modes%20such%20as%0Arotation-free%20translation%20maneuvers%2C%20in-situ%20rotational%20maneuvers%2C%20and%0Aproportional%20steering%2C%20exhibited%20inefficient%20performance%20due%20to%20time-consuming%0Amode%20switches.%20This%20inefficiency%20arises%20from%20wheel%20rotation%20constraints%20and%0Ainter-wheel%20cooperation%20requirements.%20The%20direct%20application%20of%20a%20holonomic%0Amoving%20strategy%20can%20lead%20to%20significant%20slip%20angles%20or%20even%20structural%20failure.%0AAdditionally%2C%20the%20limited%20steering%20range%20of%20AWS%20wheeled%20robots%20exacerbates%0Anonlinearity%20issues%2C%20thereby%20complicating%20control%20processes.%20To%20address%20these%0Achallenges%2C%20we%20developed%20a%20novel%20planning%20method%20termed%20Constrained%20AWS%28C-AWS%29%2C%0Awhich%20integrates%20second-order%20discrete%20search%20with%20predictive%20control%0Atechniques.%20Experimental%20results%20demonstrate%20that%20our%20method%20adeptly%20generates%0Afeasible%20and%20smooth%20trajectories%20for%20C-AWS%20while%20adhering%20to%20steering%20angle%0Aconstraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09677v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generic%20Trajectory%20Planning%20Method%20for%20Constrained%20All-Wheel-Steering%0A%20%20Robots&entry.906535625=Ren%20Xin%20and%20Hongji%20Liu%20and%20Yingbing%20Chen%20and%20Sheng%20Wang%20and%20Ming%20Liu&entry.1292438233=%20%20This%20paper%20presents%20a%20trajectory%20planning%20method%20for%20wheeled%20robots%20with%0Afixed%20steering%20axes%20while%20the%20steering%20angle%20of%20each%20wheel%20is%20constrained.%20In%0Athe%20past%2C%20All-Wheel-Steering%28AWS%29%20robots%2C%20incorporating%20modes%20such%20as%0Arotation-free%20translation%20maneuvers%2C%20in-situ%20rotational%20maneuvers%2C%20and%0Aproportional%20steering%2C%20exhibited%20inefficient%20performance%20due%20to%20time-consuming%0Amode%20switches.%20This%20inefficiency%20arises%20from%20wheel%20rotation%20constraints%20and%0Ainter-wheel%20cooperation%20requirements.%20The%20direct%20application%20of%20a%20holonomic%0Amoving%20strategy%20can%20lead%20to%20significant%20slip%20angles%20or%20even%20structural%20failure.%0AAdditionally%2C%20the%20limited%20steering%20range%20of%20AWS%20wheeled%20robots%20exacerbates%0Anonlinearity%20issues%2C%20thereby%20complicating%20control%20processes.%20To%20address%20these%0Achallenges%2C%20we%20developed%20a%20novel%20planning%20method%20termed%20Constrained%20AWS%28C-AWS%29%2C%0Awhich%20integrates%20second-order%20discrete%20search%20with%20predictive%20control%0Atechniques.%20Experimental%20results%20demonstrate%20that%20our%20method%20adeptly%20generates%0Afeasible%20and%20smooth%20trajectories%20for%20C-AWS%20while%20adhering%20to%20steering%20angle%0Aconstraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09677v1&entry.124074799=Read"},
{"title": "Personalized Collaborative Fine-Tuning for On-Device Large Language\n  Models", "author": "Nicolas Wagner and Dongyang Fan and Martin Jaggi", "abstract": "  We explore on-device self-supervised collaborative fine-tuning of large\nlanguage models with limited local data availability. Taking inspiration from\nthe collaborative learning community, we introduce three distinct\ntrust-weighted gradient aggregation schemes: weight similarity-based,\nprediction similarity-based and validation performance-based. To minimize\ncommunication overhead, we integrate Low-Rank Adaptation (LoRA) and only\nexchange LoRA weight updates. Our protocols, driven by prediction and\nperformance metrics, surpass both FedAvg and local fine-tuning methods, which\nis particularly evident in realistic scenarios with more diverse local data\ndistributions. The results underscore the effectiveness of our approach in\naddressing heterogeneity and scarcity within local datasets.\n", "link": "http://arxiv.org/abs/2404.09753v1", "date": "2024-04-15", "relevancy": 1.5775, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5301}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5294}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5115}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Personalized%20Collaborative%20Fine-Tuning%20for%20On-Device%20Large%20Language%0A%20%20Models&body=Title%3A%20Personalized%20Collaborative%20Fine-Tuning%20for%20On-Device%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Nicolas%20Wagner%20and%20Dongyang%20Fan%20and%20Martin%20Jaggi%0AAbstract%3A%20%20%20We%20explore%20on-device%20self-supervised%20collaborative%20fine-tuning%20of%20large%0Alanguage%20models%20with%20limited%20local%20data%20availability.%20Taking%20inspiration%20from%0Athe%20collaborative%20learning%20community%2C%20we%20introduce%20three%20distinct%0Atrust-weighted%20gradient%20aggregation%20schemes%3A%20weight%20similarity-based%2C%0Aprediction%20similarity-based%20and%20validation%20performance-based.%20To%20minimize%0Acommunication%20overhead%2C%20we%20integrate%20Low-Rank%20Adaptation%20%28LoRA%29%20and%20only%0Aexchange%20LoRA%20weight%20updates.%20Our%20protocols%2C%20driven%20by%20prediction%20and%0Aperformance%20metrics%2C%20surpass%20both%20FedAvg%20and%20local%20fine-tuning%20methods%2C%20which%0Ais%20particularly%20evident%20in%20realistic%20scenarios%20with%20more%20diverse%20local%20data%0Adistributions.%20The%20results%20underscore%20the%20effectiveness%20of%20our%20approach%20in%0Aaddressing%20heterogeneity%20and%20scarcity%20within%20local%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09753v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Collaborative%20Fine-Tuning%20for%20On-Device%20Large%20Language%0A%20%20Models&entry.906535625=Nicolas%20Wagner%20and%20Dongyang%20Fan%20and%20Martin%20Jaggi&entry.1292438233=%20%20We%20explore%20on-device%20self-supervised%20collaborative%20fine-tuning%20of%20large%0Alanguage%20models%20with%20limited%20local%20data%20availability.%20Taking%20inspiration%20from%0Athe%20collaborative%20learning%20community%2C%20we%20introduce%20three%20distinct%0Atrust-weighted%20gradient%20aggregation%20schemes%3A%20weight%20similarity-based%2C%0Aprediction%20similarity-based%20and%20validation%20performance-based.%20To%20minimize%0Acommunication%20overhead%2C%20we%20integrate%20Low-Rank%20Adaptation%20%28LoRA%29%20and%20only%0Aexchange%20LoRA%20weight%20updates.%20Our%20protocols%2C%20driven%20by%20prediction%20and%0Aperformance%20metrics%2C%20surpass%20both%20FedAvg%20and%20local%20fine-tuning%20methods%2C%20which%0Ais%20particularly%20evident%20in%20realistic%20scenarios%20with%20more%20diverse%20local%20data%0Adistributions.%20The%20results%20underscore%20the%20effectiveness%20of%20our%20approach%20in%0Aaddressing%20heterogeneity%20and%20scarcity%20within%20local%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09753v1&entry.124074799=Read"},
{"title": "Effective Reinforcement Learning Based on Structural Information\n  Principles", "author": "Xianghua Zeng and Hao Peng and Dingli Su and Angsheng Li", "abstract": "  Although Reinforcement Learning (RL) algorithms acquire sequential behavioral\npatterns through interactions with the environment, their effectiveness in\nnoisy and high-dimensional scenarios typically relies on specific structural\npriors. In this paper, we propose a novel and general Structural Information\nprinciples-based framework for effective Decision-Making, namely SIDM,\napproached from an information-theoretic perspective. This paper presents a\nspecific unsupervised partitioning method that forms vertex communities in the\nstate and action spaces based on their feature similarities. An aggregation\nfunction, which utilizes structural entropy as the vertex weight, is devised\nwithin each community to obtain its embedding, thereby facilitating\nhierarchical state and action abstractions. By extracting abstract elements\nfrom historical trajectories, a directed, weighted, homogeneous transition\ngraph is constructed. The minimization of this graph's high-dimensional entropy\nleads to the generation of an optimal encoding tree. An innovative two-layer\nskill-based learning mechanism is introduced to compute the common path entropy\nof each state transition as its identified probability, thereby obviating the\nrequirement for expert knowledge. Moreover, SIDM can be flexibly incorporated\ninto various single-agent and multi-agent RL algorithms, enhancing their\nperformance. Finally, extensive evaluations on challenging benchmarks\ndemonstrate that, compared with SOTA baselines, our framework significantly and\nconsistently improves the policy's quality, stability, and efficiency up to\n32.70%, 88.26%, and 64.86%, respectively.\n", "link": "http://arxiv.org/abs/2404.09760v1", "date": "2024-04-15", "relevancy": 1.5481, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5434}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5382}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4962}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Effective%20Reinforcement%20Learning%20Based%20on%20Structural%20Information%0A%20%20Principles&body=Title%3A%20Effective%20Reinforcement%20Learning%20Based%20on%20Structural%20Information%0A%20%20Principles%0AAuthor%3A%20Xianghua%20Zeng%20and%20Hao%20Peng%20and%20Dingli%20Su%20and%20Angsheng%20Li%0AAbstract%3A%20%20%20Although%20Reinforcement%20Learning%20%28RL%29%20algorithms%20acquire%20sequential%20behavioral%0Apatterns%20through%20interactions%20with%20the%20environment%2C%20their%20effectiveness%20in%0Anoisy%20and%20high-dimensional%20scenarios%20typically%20relies%20on%20specific%20structural%0Apriors.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20general%20Structural%20Information%0Aprinciples-based%20framework%20for%20effective%20Decision-Making%2C%20namely%20SIDM%2C%0Aapproached%20from%20an%20information-theoretic%20perspective.%20This%20paper%20presents%20a%0Aspecific%20unsupervised%20partitioning%20method%20that%20forms%20vertex%20communities%20in%20the%0Astate%20and%20action%20spaces%20based%20on%20their%20feature%20similarities.%20An%20aggregation%0Afunction%2C%20which%20utilizes%20structural%20entropy%20as%20the%20vertex%20weight%2C%20is%20devised%0Awithin%20each%20community%20to%20obtain%20its%20embedding%2C%20thereby%20facilitating%0Ahierarchical%20state%20and%20action%20abstractions.%20By%20extracting%20abstract%20elements%0Afrom%20historical%20trajectories%2C%20a%20directed%2C%20weighted%2C%20homogeneous%20transition%0Agraph%20is%20constructed.%20The%20minimization%20of%20this%20graph%27s%20high-dimensional%20entropy%0Aleads%20to%20the%20generation%20of%20an%20optimal%20encoding%20tree.%20An%20innovative%20two-layer%0Askill-based%20learning%20mechanism%20is%20introduced%20to%20compute%20the%20common%20path%20entropy%0Aof%20each%20state%20transition%20as%20its%20identified%20probability%2C%20thereby%20obviating%20the%0Arequirement%20for%20expert%20knowledge.%20Moreover%2C%20SIDM%20can%20be%20flexibly%20incorporated%0Ainto%20various%20single-agent%20and%20multi-agent%20RL%20algorithms%2C%20enhancing%20their%0Aperformance.%20Finally%2C%20extensive%20evaluations%20on%20challenging%20benchmarks%0Ademonstrate%20that%2C%20compared%20with%20SOTA%20baselines%2C%20our%20framework%20significantly%20and%0Aconsistently%20improves%20the%20policy%27s%20quality%2C%20stability%2C%20and%20efficiency%20up%20to%0A32.70%25%2C%2088.26%25%2C%20and%2064.86%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09760v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20Reinforcement%20Learning%20Based%20on%20Structural%20Information%0A%20%20Principles&entry.906535625=Xianghua%20Zeng%20and%20Hao%20Peng%20and%20Dingli%20Su%20and%20Angsheng%20Li&entry.1292438233=%20%20Although%20Reinforcement%20Learning%20%28RL%29%20algorithms%20acquire%20sequential%20behavioral%0Apatterns%20through%20interactions%20with%20the%20environment%2C%20their%20effectiveness%20in%0Anoisy%20and%20high-dimensional%20scenarios%20typically%20relies%20on%20specific%20structural%0Apriors.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20general%20Structural%20Information%0Aprinciples-based%20framework%20for%20effective%20Decision-Making%2C%20namely%20SIDM%2C%0Aapproached%20from%20an%20information-theoretic%20perspective.%20This%20paper%20presents%20a%0Aspecific%20unsupervised%20partitioning%20method%20that%20forms%20vertex%20communities%20in%20the%0Astate%20and%20action%20spaces%20based%20on%20their%20feature%20similarities.%20An%20aggregation%0Afunction%2C%20which%20utilizes%20structural%20entropy%20as%20the%20vertex%20weight%2C%20is%20devised%0Awithin%20each%20community%20to%20obtain%20its%20embedding%2C%20thereby%20facilitating%0Ahierarchical%20state%20and%20action%20abstractions.%20By%20extracting%20abstract%20elements%0Afrom%20historical%20trajectories%2C%20a%20directed%2C%20weighted%2C%20homogeneous%20transition%0Agraph%20is%20constructed.%20The%20minimization%20of%20this%20graph%27s%20high-dimensional%20entropy%0Aleads%20to%20the%20generation%20of%20an%20optimal%20encoding%20tree.%20An%20innovative%20two-layer%0Askill-based%20learning%20mechanism%20is%20introduced%20to%20compute%20the%20common%20path%20entropy%0Aof%20each%20state%20transition%20as%20its%20identified%20probability%2C%20thereby%20obviating%20the%0Arequirement%20for%20expert%20knowledge.%20Moreover%2C%20SIDM%20can%20be%20flexibly%20incorporated%0Ainto%20various%20single-agent%20and%20multi-agent%20RL%20algorithms%2C%20enhancing%20their%0Aperformance.%20Finally%2C%20extensive%20evaluations%20on%20challenging%20benchmarks%0Ademonstrate%20that%2C%20compared%20with%20SOTA%20baselines%2C%20our%20framework%20significantly%20and%0Aconsistently%20improves%20the%20policy%27s%20quality%2C%20stability%2C%20and%20efficiency%20up%20to%0A32.70%25%2C%2088.26%25%2C%20and%2064.86%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09760v1&entry.124074799=Read"},
{"title": "New methods for drug synergy prediction: a mini-review", "author": "Fatemeh Abbasi and Juho Rousu", "abstract": "  In this mini-review, we explore the new prediction methods for drug\ncombination synergy relying on high-throughput combinatorial screens. The fast\nprogress of the field is witnessed in the more than thirty original machine\nlearning methods published since 2021, a clear majority of them based on deep\nlearning techniques. We aim to put these papers under a unifying lens by\nhighlighting the core technologies, the data sources, the input data types and\nsynergy scores used in the methods, as well as the prediction scenarios and\nevaluation protocols that the papers deal with. Our finding is that the best\nmethods accurately solve the synergy prediction scenarios involving known drugs\nor cell lines while the scenarios involving new drugs or cell lines still fall\nshort of an accurate prediction level.\n", "link": "http://arxiv.org/abs/2404.02484v2", "date": "2024-04-15", "relevancy": 1.543, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4258}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3916}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3639}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20New%20methods%20for%20drug%20synergy%20prediction%3A%20a%20mini-review&body=Title%3A%20New%20methods%20for%20drug%20synergy%20prediction%3A%20a%20mini-review%0AAuthor%3A%20Fatemeh%20Abbasi%20and%20Juho%20Rousu%0AAbstract%3A%20%20%20In%20this%20mini-review%2C%20we%20explore%20the%20new%20prediction%20methods%20for%20drug%0Acombination%20synergy%20relying%20on%20high-throughput%20combinatorial%20screens.%20The%20fast%0Aprogress%20of%20the%20field%20is%20witnessed%20in%20the%20more%20than%20thirty%20original%20machine%0Alearning%20methods%20published%20since%202021%2C%20a%20clear%20majority%20of%20them%20based%20on%20deep%0Alearning%20techniques.%20We%20aim%20to%20put%20these%20papers%20under%20a%20unifying%20lens%20by%0Ahighlighting%20the%20core%20technologies%2C%20the%20data%20sources%2C%20the%20input%20data%20types%20and%0Asynergy%20scores%20used%20in%20the%20methods%2C%20as%20well%20as%20the%20prediction%20scenarios%20and%0Aevaluation%20protocols%20that%20the%20papers%20deal%20with.%20Our%20finding%20is%20that%20the%20best%0Amethods%20accurately%20solve%20the%20synergy%20prediction%20scenarios%20involving%20known%20drugs%0Aor%20cell%20lines%20while%20the%20scenarios%20involving%20new%20drugs%20or%20cell%20lines%20still%20fall%0Ashort%20of%20an%20accurate%20prediction%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02484v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20methods%20for%20drug%20synergy%20prediction%3A%20a%20mini-review&entry.906535625=Fatemeh%20Abbasi%20and%20Juho%20Rousu&entry.1292438233=%20%20In%20this%20mini-review%2C%20we%20explore%20the%20new%20prediction%20methods%20for%20drug%0Acombination%20synergy%20relying%20on%20high-throughput%20combinatorial%20screens.%20The%20fast%0Aprogress%20of%20the%20field%20is%20witnessed%20in%20the%20more%20than%20thirty%20original%20machine%0Alearning%20methods%20published%20since%202021%2C%20a%20clear%20majority%20of%20them%20based%20on%20deep%0Alearning%20techniques.%20We%20aim%20to%20put%20these%20papers%20under%20a%20unifying%20lens%20by%0Ahighlighting%20the%20core%20technologies%2C%20the%20data%20sources%2C%20the%20input%20data%20types%20and%0Asynergy%20scores%20used%20in%20the%20methods%2C%20as%20well%20as%20the%20prediction%20scenarios%20and%0Aevaluation%20protocols%20that%20the%20papers%20deal%20with.%20Our%20finding%20is%20that%20the%20best%0Amethods%20accurately%20solve%20the%20synergy%20prediction%20scenarios%20involving%20known%20drugs%0Aor%20cell%20lines%20while%20the%20scenarios%20involving%20new%20drugs%20or%20cell%20lines%20still%20fall%0Ashort%20of%20an%20accurate%20prediction%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02484v2&entry.124074799=Read"},
{"title": "Learn Your Reference Model for Real Good Alignment", "author": "Alexey Gorbatovski and Boris Shaposhnikov and Alexey Malakhov and Nikita Surnachev and Yaroslav Aksenov and Ian Maksimov and Nikita Balagansky and Daniil Gavrilov", "abstract": "  The complexity of the alignment problem stems from the fact that existing\nmethods are unstable. Researchers continuously invent various tricks to address\nthis shortcoming. For instance, in the fundamental Reinforcement Learning From\nHuman Feedback (RLHF) technique of Language Model alignment, in addition to\nreward maximization, the Kullback-Leibler divergence between the trainable\npolicy and the SFT policy is minimized. This addition prevents the model from\nbeing overfitted to the Reward Model (RM) and generating texts that are\nout-of-domain for the RM. The Direct Preference Optimization (DPO) method\nreformulates the optimization task of RLHF and eliminates the Reward Model\nwhile tacitly maintaining the requirement for the policy to be close to the SFT\npolicy. In our paper, we argue that this implicit limitation in the DPO method\nleads to sub-optimal results. We propose a new method called Trust Region DPO\n(TR-DPO), which updates the reference policy during training. With such a\nstraightforward update, we demonstrate the effectiveness of TR-DPO against DPO\non the Anthropic HH and TLDR datasets. We show that TR-DPO outperforms DPO by\nup to 19%, measured by automatic evaluation with GPT-4. The new alignment\napproach that we propose allows us to improve the quality of models across\nseveral parameters at once, such as coherence, correctness, level of detail,\nhelpfulness, and harmlessness.\n", "link": "http://arxiv.org/abs/2404.09656v1", "date": "2024-04-15", "relevancy": 1.5265, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.491}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4859}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learn%20Your%20Reference%20Model%20for%20Real%20Good%20Alignment&body=Title%3A%20Learn%20Your%20Reference%20Model%20for%20Real%20Good%20Alignment%0AAuthor%3A%20Alexey%20Gorbatovski%20and%20Boris%20Shaposhnikov%20and%20Alexey%20Malakhov%20and%20Nikita%20Surnachev%20and%20Yaroslav%20Aksenov%20and%20Ian%20Maksimov%20and%20Nikita%20Balagansky%20and%20Daniil%20Gavrilov%0AAbstract%3A%20%20%20The%20complexity%20of%20the%20alignment%20problem%20stems%20from%20the%20fact%20that%20existing%0Amethods%20are%20unstable.%20Researchers%20continuously%20invent%20various%20tricks%20to%20address%0Athis%20shortcoming.%20For%20instance%2C%20in%20the%20fundamental%20Reinforcement%20Learning%20From%0AHuman%20Feedback%20%28RLHF%29%20technique%20of%20Language%20Model%20alignment%2C%20in%20addition%20to%0Areward%20maximization%2C%20the%20Kullback-Leibler%20divergence%20between%20the%20trainable%0Apolicy%20and%20the%20SFT%20policy%20is%20minimized.%20This%20addition%20prevents%20the%20model%20from%0Abeing%20overfitted%20to%20the%20Reward%20Model%20%28RM%29%20and%20generating%20texts%20that%20are%0Aout-of-domain%20for%20the%20RM.%20The%20Direct%20Preference%20Optimization%20%28DPO%29%20method%0Areformulates%20the%20optimization%20task%20of%20RLHF%20and%20eliminates%20the%20Reward%20Model%0Awhile%20tacitly%20maintaining%20the%20requirement%20for%20the%20policy%20to%20be%20close%20to%20the%20SFT%0Apolicy.%20In%20our%20paper%2C%20we%20argue%20that%20this%20implicit%20limitation%20in%20the%20DPO%20method%0Aleads%20to%20sub-optimal%20results.%20We%20propose%20a%20new%20method%20called%20Trust%20Region%20DPO%0A%28TR-DPO%29%2C%20which%20updates%20the%20reference%20policy%20during%20training.%20With%20such%20a%0Astraightforward%20update%2C%20we%20demonstrate%20the%20effectiveness%20of%20TR-DPO%20against%20DPO%0Aon%20the%20Anthropic%20HH%20and%20TLDR%20datasets.%20We%20show%20that%20TR-DPO%20outperforms%20DPO%20by%0Aup%20to%2019%25%2C%20measured%20by%20automatic%20evaluation%20with%20GPT-4.%20The%20new%20alignment%0Aapproach%20that%20we%20propose%20allows%20us%20to%20improve%20the%20quality%20of%20models%20across%0Aseveral%20parameters%20at%20once%2C%20such%20as%20coherence%2C%20correctness%2C%20level%20of%20detail%2C%0Ahelpfulness%2C%20and%20harmlessness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09656v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20Your%20Reference%20Model%20for%20Real%20Good%20Alignment&entry.906535625=Alexey%20Gorbatovski%20and%20Boris%20Shaposhnikov%20and%20Alexey%20Malakhov%20and%20Nikita%20Surnachev%20and%20Yaroslav%20Aksenov%20and%20Ian%20Maksimov%20and%20Nikita%20Balagansky%20and%20Daniil%20Gavrilov&entry.1292438233=%20%20The%20complexity%20of%20the%20alignment%20problem%20stems%20from%20the%20fact%20that%20existing%0Amethods%20are%20unstable.%20Researchers%20continuously%20invent%20various%20tricks%20to%20address%0Athis%20shortcoming.%20For%20instance%2C%20in%20the%20fundamental%20Reinforcement%20Learning%20From%0AHuman%20Feedback%20%28RLHF%29%20technique%20of%20Language%20Model%20alignment%2C%20in%20addition%20to%0Areward%20maximization%2C%20the%20Kullback-Leibler%20divergence%20between%20the%20trainable%0Apolicy%20and%20the%20SFT%20policy%20is%20minimized.%20This%20addition%20prevents%20the%20model%20from%0Abeing%20overfitted%20to%20the%20Reward%20Model%20%28RM%29%20and%20generating%20texts%20that%20are%0Aout-of-domain%20for%20the%20RM.%20The%20Direct%20Preference%20Optimization%20%28DPO%29%20method%0Areformulates%20the%20optimization%20task%20of%20RLHF%20and%20eliminates%20the%20Reward%20Model%0Awhile%20tacitly%20maintaining%20the%20requirement%20for%20the%20policy%20to%20be%20close%20to%20the%20SFT%0Apolicy.%20In%20our%20paper%2C%20we%20argue%20that%20this%20implicit%20limitation%20in%20the%20DPO%20method%0Aleads%20to%20sub-optimal%20results.%20We%20propose%20a%20new%20method%20called%20Trust%20Region%20DPO%0A%28TR-DPO%29%2C%20which%20updates%20the%20reference%20policy%20during%20training.%20With%20such%20a%0Astraightforward%20update%2C%20we%20demonstrate%20the%20effectiveness%20of%20TR-DPO%20against%20DPO%0Aon%20the%20Anthropic%20HH%20and%20TLDR%20datasets.%20We%20show%20that%20TR-DPO%20outperforms%20DPO%20by%0Aup%20to%2019%25%2C%20measured%20by%20automatic%20evaluation%20with%20GPT-4.%20The%20new%20alignment%0Aapproach%20that%20we%20propose%20allows%20us%20to%20improve%20the%20quality%20of%20models%20across%0Aseveral%20parameters%20at%20once%2C%20such%20as%20coherence%2C%20correctness%2C%20level%20of%20detail%2C%0Ahelpfulness%2C%20and%20harmlessness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09656v1&entry.124074799=Read"},
{"title": "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data\n  Annotation", "author": "Juhwan Choi and Jungmin Yun and Kyohoon Jin and YoungBin Kim", "abstract": "  The quality of the dataset is crucial for ensuring optimal performance and\nreliability of downstream task models. However, datasets often contain noisy\ndata inadvertently included during the construction process. Numerous attempts\nhave been made to correct this issue through human annotators. However, hiring\nand managing human annotators is expensive and time-consuming. As an\nalternative, recent studies are exploring the use of large language models\n(LLMs) for data annotation.\n  In this study, we present a case study that extends the application of\nLLM-based data annotation to enhance the quality of existing datasets through a\ncleansing strategy. Specifically, we leverage approaches such as\nchain-of-thought (CoT) and majority voting to imitate human annotation and\nclassify unrelated documents from the Multi-News dataset, which is widely used\nfor the multi-document summarization task. Through our proposed cleansing\nmethod, we introduce an enhanced Multi-News+. By employing LLMs for data\ncleansing, we demonstrate an efficient and effective approach to improving\ndataset quality without relying on expensive human annotation efforts.\n", "link": "http://arxiv.org/abs/2404.09682v1", "date": "2024-04-15", "relevancy": 1.5253, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5376}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5017}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-News%2B%3A%20Cost-efficient%20Dataset%20Cleansing%20via%20LLM-based%20Data%0A%20%20Annotation&body=Title%3A%20Multi-News%2B%3A%20Cost-efficient%20Dataset%20Cleansing%20via%20LLM-based%20Data%0A%20%20Annotation%0AAuthor%3A%20Juhwan%20Choi%20and%20Jungmin%20Yun%20and%20Kyohoon%20Jin%20and%20YoungBin%20Kim%0AAbstract%3A%20%20%20The%20quality%20of%20the%20dataset%20is%20crucial%20for%20ensuring%20optimal%20performance%20and%0Areliability%20of%20downstream%20task%20models.%20However%2C%20datasets%20often%20contain%20noisy%0Adata%20inadvertently%20included%20during%20the%20construction%20process.%20Numerous%20attempts%0Ahave%20been%20made%20to%20correct%20this%20issue%20through%20human%20annotators.%20However%2C%20hiring%0Aand%20managing%20human%20annotators%20is%20expensive%20and%20time-consuming.%20As%20an%0Aalternative%2C%20recent%20studies%20are%20exploring%20the%20use%20of%20large%20language%20models%0A%28LLMs%29%20for%20data%20annotation.%0A%20%20In%20this%20study%2C%20we%20present%20a%20case%20study%20that%20extends%20the%20application%20of%0ALLM-based%20data%20annotation%20to%20enhance%20the%20quality%20of%20existing%20datasets%20through%20a%0Acleansing%20strategy.%20Specifically%2C%20we%20leverage%20approaches%20such%20as%0Achain-of-thought%20%28CoT%29%20and%20majority%20voting%20to%20imitate%20human%20annotation%20and%0Aclassify%20unrelated%20documents%20from%20the%20Multi-News%20dataset%2C%20which%20is%20widely%20used%0Afor%20the%20multi-document%20summarization%20task.%20Through%20our%20proposed%20cleansing%0Amethod%2C%20we%20introduce%20an%20enhanced%20Multi-News%2B.%20By%20employing%20LLMs%20for%20data%0Acleansing%2C%20we%20demonstrate%20an%20efficient%20and%20effective%20approach%20to%20improving%0Adataset%20quality%20without%20relying%20on%20expensive%20human%20annotation%20efforts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09682v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-News%2B%3A%20Cost-efficient%20Dataset%20Cleansing%20via%20LLM-based%20Data%0A%20%20Annotation&entry.906535625=Juhwan%20Choi%20and%20Jungmin%20Yun%20and%20Kyohoon%20Jin%20and%20YoungBin%20Kim&entry.1292438233=%20%20The%20quality%20of%20the%20dataset%20is%20crucial%20for%20ensuring%20optimal%20performance%20and%0Areliability%20of%20downstream%20task%20models.%20However%2C%20datasets%20often%20contain%20noisy%0Adata%20inadvertently%20included%20during%20the%20construction%20process.%20Numerous%20attempts%0Ahave%20been%20made%20to%20correct%20this%20issue%20through%20human%20annotators.%20However%2C%20hiring%0Aand%20managing%20human%20annotators%20is%20expensive%20and%20time-consuming.%20As%20an%0Aalternative%2C%20recent%20studies%20are%20exploring%20the%20use%20of%20large%20language%20models%0A%28LLMs%29%20for%20data%20annotation.%0A%20%20In%20this%20study%2C%20we%20present%20a%20case%20study%20that%20extends%20the%20application%20of%0ALLM-based%20data%20annotation%20to%20enhance%20the%20quality%20of%20existing%20datasets%20through%20a%0Acleansing%20strategy.%20Specifically%2C%20we%20leverage%20approaches%20such%20as%0Achain-of-thought%20%28CoT%29%20and%20majority%20voting%20to%20imitate%20human%20annotation%20and%0Aclassify%20unrelated%20documents%20from%20the%20Multi-News%20dataset%2C%20which%20is%20widely%20used%0Afor%20the%20multi-document%20summarization%20task.%20Through%20our%20proposed%20cleansing%0Amethod%2C%20we%20introduce%20an%20enhanced%20Multi-News%2B.%20By%20employing%20LLMs%20for%20data%0Acleansing%2C%20we%20demonstrate%20an%20efficient%20and%20effective%20approach%20to%20improving%0Adataset%20quality%20without%20relying%20on%20expensive%20human%20annotation%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09682v1&entry.124074799=Read"},
{"title": "An Origami-Inspired Variable Friction Surface for Increasing the\n  Dexterity of Robotic Grippers", "author": "Qiujie Lu and Angus B. Clark and Matthew Shen and Nicolas Rojas", "abstract": "  While the grasping capability of robotic grippers has shown significant\ndevelopment, the ability to manipulate objects within the hand is still\nlimited. One explanation for this limitation is the lack of controlled contact\nvariation between the grasped object and the gripper. For instance, human hands\nhave the ability to firmly grip object surfaces, as well as slide over object\nfaces, an aspect that aids the enhanced manipulation of objects within the hand\nwithout losing contact. In this letter, we present a parametric,\norigami-inspired thin surface capable of transitioning between a high friction\nand a low friction state, suitable for implementation as an epidermis in\nrobotic fingers. A numerical analysis of the proposed surface based on its\ndesign parameters, force analysis, and performance in in-hand manipulation\ntasks is presented. Through the development of a simple two-fingered\ntwo-degree-of-freedom gripper utilizing the proposed variable-friction surfaces\nwith different parameters, we experimentally demonstrate the improved\nmanipulation capabilities of the hand when compared to the same gripper without\nchangeable friction. Results show that the pattern density and valley gap are\nthe main parameters that effect the in-hand manipulation performance. The\norigami-inspired thin surface with a higher pattern density generated a smaller\nvalley gap and smaller height change, producing a more stable improvement of\nthe manipulation capabilities of the hand.\n", "link": "http://arxiv.org/abs/2404.09644v1", "date": "2024-04-15", "relevancy": 1.5202, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5179}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4976}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Origami-Inspired%20Variable%20Friction%20Surface%20for%20Increasing%20the%0A%20%20Dexterity%20of%20Robotic%20Grippers&body=Title%3A%20An%20Origami-Inspired%20Variable%20Friction%20Surface%20for%20Increasing%20the%0A%20%20Dexterity%20of%20Robotic%20Grippers%0AAuthor%3A%20Qiujie%20Lu%20and%20Angus%20B.%20Clark%20and%20Matthew%20Shen%20and%20Nicolas%20Rojas%0AAbstract%3A%20%20%20While%20the%20grasping%20capability%20of%20robotic%20grippers%20has%20shown%20significant%0Adevelopment%2C%20the%20ability%20to%20manipulate%20objects%20within%20the%20hand%20is%20still%0Alimited.%20One%20explanation%20for%20this%20limitation%20is%20the%20lack%20of%20controlled%20contact%0Avariation%20between%20the%20grasped%20object%20and%20the%20gripper.%20For%20instance%2C%20human%20hands%0Ahave%20the%20ability%20to%20firmly%20grip%20object%20surfaces%2C%20as%20well%20as%20slide%20over%20object%0Afaces%2C%20an%20aspect%20that%20aids%20the%20enhanced%20manipulation%20of%20objects%20within%20the%20hand%0Awithout%20losing%20contact.%20In%20this%20letter%2C%20we%20present%20a%20parametric%2C%0Aorigami-inspired%20thin%20surface%20capable%20of%20transitioning%20between%20a%20high%20friction%0Aand%20a%20low%20friction%20state%2C%20suitable%20for%20implementation%20as%20an%20epidermis%20in%0Arobotic%20fingers.%20A%20numerical%20analysis%20of%20the%20proposed%20surface%20based%20on%20its%0Adesign%20parameters%2C%20force%20analysis%2C%20and%20performance%20in%20in-hand%20manipulation%0Atasks%20is%20presented.%20Through%20the%20development%20of%20a%20simple%20two-fingered%0Atwo-degree-of-freedom%20gripper%20utilizing%20the%20proposed%20variable-friction%20surfaces%0Awith%20different%20parameters%2C%20we%20experimentally%20demonstrate%20the%20improved%0Amanipulation%20capabilities%20of%20the%20hand%20when%20compared%20to%20the%20same%20gripper%20without%0Achangeable%20friction.%20Results%20show%20that%20the%20pattern%20density%20and%20valley%20gap%20are%0Athe%20main%20parameters%20that%20effect%20the%20in-hand%20manipulation%20performance.%20The%0Aorigami-inspired%20thin%20surface%20with%20a%20higher%20pattern%20density%20generated%20a%20smaller%0Avalley%20gap%20and%20smaller%20height%20change%2C%20producing%20a%20more%20stable%20improvement%20of%0Athe%20manipulation%20capabilities%20of%20the%20hand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09644v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Origami-Inspired%20Variable%20Friction%20Surface%20for%20Increasing%20the%0A%20%20Dexterity%20of%20Robotic%20Grippers&entry.906535625=Qiujie%20Lu%20and%20Angus%20B.%20Clark%20and%20Matthew%20Shen%20and%20Nicolas%20Rojas&entry.1292438233=%20%20While%20the%20grasping%20capability%20of%20robotic%20grippers%20has%20shown%20significant%0Adevelopment%2C%20the%20ability%20to%20manipulate%20objects%20within%20the%20hand%20is%20still%0Alimited.%20One%20explanation%20for%20this%20limitation%20is%20the%20lack%20of%20controlled%20contact%0Avariation%20between%20the%20grasped%20object%20and%20the%20gripper.%20For%20instance%2C%20human%20hands%0Ahave%20the%20ability%20to%20firmly%20grip%20object%20surfaces%2C%20as%20well%20as%20slide%20over%20object%0Afaces%2C%20an%20aspect%20that%20aids%20the%20enhanced%20manipulation%20of%20objects%20within%20the%20hand%0Awithout%20losing%20contact.%20In%20this%20letter%2C%20we%20present%20a%20parametric%2C%0Aorigami-inspired%20thin%20surface%20capable%20of%20transitioning%20between%20a%20high%20friction%0Aand%20a%20low%20friction%20state%2C%20suitable%20for%20implementation%20as%20an%20epidermis%20in%0Arobotic%20fingers.%20A%20numerical%20analysis%20of%20the%20proposed%20surface%20based%20on%20its%0Adesign%20parameters%2C%20force%20analysis%2C%20and%20performance%20in%20in-hand%20manipulation%0Atasks%20is%20presented.%20Through%20the%20development%20of%20a%20simple%20two-fingered%0Atwo-degree-of-freedom%20gripper%20utilizing%20the%20proposed%20variable-friction%20surfaces%0Awith%20different%20parameters%2C%20we%20experimentally%20demonstrate%20the%20improved%0Amanipulation%20capabilities%20of%20the%20hand%20when%20compared%20to%20the%20same%20gripper%20without%0Achangeable%20friction.%20Results%20show%20that%20the%20pattern%20density%20and%20valley%20gap%20are%0Athe%20main%20parameters%20that%20effect%20the%20in-hand%20manipulation%20performance.%20The%0Aorigami-inspired%20thin%20surface%20with%20a%20higher%20pattern%20density%20generated%20a%20smaller%0Avalley%20gap%20and%20smaller%20height%20change%2C%20producing%20a%20more%20stable%20improvement%20of%0Athe%20manipulation%20capabilities%20of%20the%20hand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09644v1&entry.124074799=Read"},
{"title": "Contrastive Pretraining for Visual Concept Explanations of Socioeconomic\n  Outcomes", "author": "Ivica Obadic and Alex Levering and Lars Pennig and Dario Oliveira and Diego Marcos and Xiaoxiang Zhu", "abstract": "  Predicting socioeconomic indicators from satellite imagery with deep learning\nhas become an increasingly popular research direction. Post-hoc concept-based\nexplanations can be an important step towards broader adoption of these models\nin policy-making as they enable the interpretation of socioeconomic outcomes\nbased on visual concepts that are intuitive to humans. In this paper, we study\nthe interplay between representation learning using an additional task-specific\ncontrastive loss and post-hoc concept explainability for socioeconomic studies.\nOur results on two different geographical locations and tasks indicate that the\ntask-specific pretraining imposes a continuous ordering of the latent space\nembeddings according to the socioeconomic outcomes. This improves the model's\ninterpretability as it enables the latent space of the model to associate urban\nconcepts with continuous intervals of socioeconomic outcomes. Further, we\nillustrate how analyzing the model's conceptual sensitivity for the intervals\nof socioeconomic outcomes can shed light on new insights for urban studies.\n", "link": "http://arxiv.org/abs/2404.09768v1", "date": "2024-04-15", "relevancy": 1.5121, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5083}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5024}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.495}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Pretraining%20for%20Visual%20Concept%20Explanations%20of%20Socioeconomic%0A%20%20Outcomes&body=Title%3A%20Contrastive%20Pretraining%20for%20Visual%20Concept%20Explanations%20of%20Socioeconomic%0A%20%20Outcomes%0AAuthor%3A%20Ivica%20Obadic%20and%20Alex%20Levering%20and%20Lars%20Pennig%20and%20Dario%20Oliveira%20and%20Diego%20Marcos%20and%20Xiaoxiang%20Zhu%0AAbstract%3A%20%20%20Predicting%20socioeconomic%20indicators%20from%20satellite%20imagery%20with%20deep%20learning%0Ahas%20become%20an%20increasingly%20popular%20research%20direction.%20Post-hoc%20concept-based%0Aexplanations%20can%20be%20an%20important%20step%20towards%20broader%20adoption%20of%20these%20models%0Ain%20policy-making%20as%20they%20enable%20the%20interpretation%20of%20socioeconomic%20outcomes%0Abased%20on%20visual%20concepts%20that%20are%20intuitive%20to%20humans.%20In%20this%20paper%2C%20we%20study%0Athe%20interplay%20between%20representation%20learning%20using%20an%20additional%20task-specific%0Acontrastive%20loss%20and%20post-hoc%20concept%20explainability%20for%20socioeconomic%20studies.%0AOur%20results%20on%20two%20different%20geographical%20locations%20and%20tasks%20indicate%20that%20the%0Atask-specific%20pretraining%20imposes%20a%20continuous%20ordering%20of%20the%20latent%20space%0Aembeddings%20according%20to%20the%20socioeconomic%20outcomes.%20This%20improves%20the%20model%27s%0Ainterpretability%20as%20it%20enables%20the%20latent%20space%20of%20the%20model%20to%20associate%20urban%0Aconcepts%20with%20continuous%20intervals%20of%20socioeconomic%20outcomes.%20Further%2C%20we%0Aillustrate%20how%20analyzing%20the%20model%27s%20conceptual%20sensitivity%20for%20the%20intervals%0Aof%20socioeconomic%20outcomes%20can%20shed%20light%20on%20new%20insights%20for%20urban%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09768v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Pretraining%20for%20Visual%20Concept%20Explanations%20of%20Socioeconomic%0A%20%20Outcomes&entry.906535625=Ivica%20Obadic%20and%20Alex%20Levering%20and%20Lars%20Pennig%20and%20Dario%20Oliveira%20and%20Diego%20Marcos%20and%20Xiaoxiang%20Zhu&entry.1292438233=%20%20Predicting%20socioeconomic%20indicators%20from%20satellite%20imagery%20with%20deep%20learning%0Ahas%20become%20an%20increasingly%20popular%20research%20direction.%20Post-hoc%20concept-based%0Aexplanations%20can%20be%20an%20important%20step%20towards%20broader%20adoption%20of%20these%20models%0Ain%20policy-making%20as%20they%20enable%20the%20interpretation%20of%20socioeconomic%20outcomes%0Abased%20on%20visual%20concepts%20that%20are%20intuitive%20to%20humans.%20In%20this%20paper%2C%20we%20study%0Athe%20interplay%20between%20representation%20learning%20using%20an%20additional%20task-specific%0Acontrastive%20loss%20and%20post-hoc%20concept%20explainability%20for%20socioeconomic%20studies.%0AOur%20results%20on%20two%20different%20geographical%20locations%20and%20tasks%20indicate%20that%20the%0Atask-specific%20pretraining%20imposes%20a%20continuous%20ordering%20of%20the%20latent%20space%0Aembeddings%20according%20to%20the%20socioeconomic%20outcomes.%20This%20improves%20the%20model%27s%0Ainterpretability%20as%20it%20enables%20the%20latent%20space%20of%20the%20model%20to%20associate%20urban%0Aconcepts%20with%20continuous%20intervals%20of%20socioeconomic%20outcomes.%20Further%2C%20we%0Aillustrate%20how%20analyzing%20the%20model%27s%20conceptual%20sensitivity%20for%20the%20intervals%0Aof%20socioeconomic%20outcomes%20can%20shed%20light%20on%20new%20insights%20for%20urban%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09768v1&entry.124074799=Read"},
{"title": "HSIDMamba: Exploring Bidirectional State-Space Models for Hyperspectral\n  Denoising", "author": "Yang Liu and Jiahua Xiao and Yu Guo and Peilin Jiang and Haiwei Yang and Fei Wang", "abstract": "  Effectively discerning spatial-spectral dependencies in HSI denoising is\ncrucial, but prevailing methods using convolution or transformers still face\ncomputational efficiency limitations. Recently, the emerging Selective State\nSpace Model(Mamba) has risen with its nearly linear computational complexity in\nprocessing natural language sequences, which inspired us to explore its\npotential in handling long spectral sequences. In this paper, we propose\nHSIDMamba(HSDM), tailored to exploit the linear complexity for effectively\ncapturing spatial-spectral dependencies in HSI denoising. In particular, HSDM\ncomprises multiple Hyperspectral Continuous Scan Blocks, incorporating\nBCSM(Bidirectional Continuous Scanning Mechanism), scale residual, and spectral\nattention mechanisms to enhance the capture of long-range and local\nspatial-spectral information. BCSM strengthens spatial-spectral interactions by\nlinking forward and backward scans and enhancing information from eight\ndirections through SSM, significantly enhancing the perceptual capability of\nHSDM and improving denoising performance more effectively. Extensive\nevaluations against HSI denoising benchmarks validate the superior performance\nof HSDM, achieving state-of-the-art results in performance and surpassing the\nefficiency of the latest transformer architectures by $30\\%$.\n", "link": "http://arxiv.org/abs/2404.09697v1", "date": "2024-04-15", "relevancy": 1.5121, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5539}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.492}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4889}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HSIDMamba%3A%20Exploring%20Bidirectional%20State-Space%20Models%20for%20Hyperspectral%0A%20%20Denoising&body=Title%3A%20HSIDMamba%3A%20Exploring%20Bidirectional%20State-Space%20Models%20for%20Hyperspectral%0A%20%20Denoising%0AAuthor%3A%20Yang%20Liu%20and%20Jiahua%20Xiao%20and%20Yu%20Guo%20and%20Peilin%20Jiang%20and%20Haiwei%20Yang%20and%20Fei%20Wang%0AAbstract%3A%20%20%20Effectively%20discerning%20spatial-spectral%20dependencies%20in%20HSI%20denoising%20is%0Acrucial%2C%20but%20prevailing%20methods%20using%20convolution%20or%20transformers%20still%20face%0Acomputational%20efficiency%20limitations.%20Recently%2C%20the%20emerging%20Selective%20State%0ASpace%20Model%28Mamba%29%20has%20risen%20with%20its%20nearly%20linear%20computational%20complexity%20in%0Aprocessing%20natural%20language%20sequences%2C%20which%20inspired%20us%20to%20explore%20its%0Apotential%20in%20handling%20long%20spectral%20sequences.%20In%20this%20paper%2C%20we%20propose%0AHSIDMamba%28HSDM%29%2C%20tailored%20to%20exploit%20the%20linear%20complexity%20for%20effectively%0Acapturing%20spatial-spectral%20dependencies%20in%20HSI%20denoising.%20In%20particular%2C%20HSDM%0Acomprises%20multiple%20Hyperspectral%20Continuous%20Scan%20Blocks%2C%20incorporating%0ABCSM%28Bidirectional%20Continuous%20Scanning%20Mechanism%29%2C%20scale%20residual%2C%20and%20spectral%0Aattention%20mechanisms%20to%20enhance%20the%20capture%20of%20long-range%20and%20local%0Aspatial-spectral%20information.%20BCSM%20strengthens%20spatial-spectral%20interactions%20by%0Alinking%20forward%20and%20backward%20scans%20and%20enhancing%20information%20from%20eight%0Adirections%20through%20SSM%2C%20significantly%20enhancing%20the%20perceptual%20capability%20of%0AHSDM%20and%20improving%20denoising%20performance%20more%20effectively.%20Extensive%0Aevaluations%20against%20HSI%20denoising%20benchmarks%20validate%20the%20superior%20performance%0Aof%20HSDM%2C%20achieving%20state-of-the-art%20results%20in%20performance%20and%20surpassing%20the%0Aefficiency%20of%20the%20latest%20transformer%20architectures%20by%20%2430%5C%25%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09697v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HSIDMamba%3A%20Exploring%20Bidirectional%20State-Space%20Models%20for%20Hyperspectral%0A%20%20Denoising&entry.906535625=Yang%20Liu%20and%20Jiahua%20Xiao%20and%20Yu%20Guo%20and%20Peilin%20Jiang%20and%20Haiwei%20Yang%20and%20Fei%20Wang&entry.1292438233=%20%20Effectively%20discerning%20spatial-spectral%20dependencies%20in%20HSI%20denoising%20is%0Acrucial%2C%20but%20prevailing%20methods%20using%20convolution%20or%20transformers%20still%20face%0Acomputational%20efficiency%20limitations.%20Recently%2C%20the%20emerging%20Selective%20State%0ASpace%20Model%28Mamba%29%20has%20risen%20with%20its%20nearly%20linear%20computational%20complexity%20in%0Aprocessing%20natural%20language%20sequences%2C%20which%20inspired%20us%20to%20explore%20its%0Apotential%20in%20handling%20long%20spectral%20sequences.%20In%20this%20paper%2C%20we%20propose%0AHSIDMamba%28HSDM%29%2C%20tailored%20to%20exploit%20the%20linear%20complexity%20for%20effectively%0Acapturing%20spatial-spectral%20dependencies%20in%20HSI%20denoising.%20In%20particular%2C%20HSDM%0Acomprises%20multiple%20Hyperspectral%20Continuous%20Scan%20Blocks%2C%20incorporating%0ABCSM%28Bidirectional%20Continuous%20Scanning%20Mechanism%29%2C%20scale%20residual%2C%20and%20spectral%0Aattention%20mechanisms%20to%20enhance%20the%20capture%20of%20long-range%20and%20local%0Aspatial-spectral%20information.%20BCSM%20strengthens%20spatial-spectral%20interactions%20by%0Alinking%20forward%20and%20backward%20scans%20and%20enhancing%20information%20from%20eight%0Adirections%20through%20SSM%2C%20significantly%20enhancing%20the%20perceptual%20capability%20of%0AHSDM%20and%20improving%20denoising%20performance%20more%20effectively.%20Extensive%0Aevaluations%20against%20HSI%20denoising%20benchmarks%20validate%20the%20superior%20performance%0Aof%20HSDM%2C%20achieving%20state-of-the-art%20results%20in%20performance%20and%20surpassing%20the%0Aefficiency%20of%20the%20latest%20transformer%20architectures%20by%20%2430%5C%25%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09697v1&entry.124074799=Read"},
{"title": "Mind-to-Image: Projecting Visual Mental Imagination of the Brain from\n  fMRI", "author": "Hugo Caselles-Dupr\u00e9 and Charles Mellerio and Paul H\u00e9rent and Aliz\u00e9e Lopez-Persem and Benoit B\u00e9ranger and Mathieu Soularue and Pierre Fautrel and Gauthier Vernier and Matthieu Cord", "abstract": "  The reconstruction of images observed by subjects from fMRI data collected\nduring visual stimuli has made significant strides in the past decade, thanks\nto the availability of extensive fMRI datasets and advancements in generative\nmodels for image generation. However, the application of visual reconstruction\nhas remained limited. Reconstructing visual imagination presents a greater\nchallenge, with potentially revolutionary applications ranging from aiding\nindividuals with disabilities to verifying witness accounts in court. The\nprimary hurdles in this field are the absence of data collection protocols for\nvisual imagery and the lack of datasets on the subject. Traditionally,\nfMRI-to-image relies on data collected from subjects exposed to visual stimuli,\nwhich poses issues for generating visual imagery based on the difference of\nbrain activity between visual stimulation and visual imagery. For the first\ntime, we have compiled a substantial dataset (around 6h of scans) on visual\nimagery along with a proposed data collection protocol. We then train a\nmodified version of an fMRI-to-image model and demonstrate the feasibility of\nreconstructing images from two modes of imagination: from memory and from pure\nimagination. This marks an important step towards creating a technology that\nallow direct reconstruction of visual imagery.\n", "link": "http://arxiv.org/abs/2404.05468v2", "date": "2024-04-15", "relevancy": 1.5056, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5206}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4966}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4965}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mind-to-Image%3A%20Projecting%20Visual%20Mental%20Imagination%20of%20the%20Brain%20from%0A%20%20fMRI&body=Title%3A%20Mind-to-Image%3A%20Projecting%20Visual%20Mental%20Imagination%20of%20the%20Brain%20from%0A%20%20fMRI%0AAuthor%3A%20Hugo%20Caselles-Dupr%C3%A9%20and%20Charles%20Mellerio%20and%20Paul%20H%C3%A9rent%20and%20Aliz%C3%A9e%20Lopez-Persem%20and%20Benoit%20B%C3%A9ranger%20and%20Mathieu%20Soularue%20and%20Pierre%20Fautrel%20and%20Gauthier%20Vernier%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20The%20reconstruction%20of%20images%20observed%20by%20subjects%20from%20fMRI%20data%20collected%0Aduring%20visual%20stimuli%20has%20made%20significant%20strides%20in%20the%20past%20decade%2C%20thanks%0Ato%20the%20availability%20of%20extensive%20fMRI%20datasets%20and%20advancements%20in%20generative%0Amodels%20for%20image%20generation.%20However%2C%20the%20application%20of%20visual%20reconstruction%0Ahas%20remained%20limited.%20Reconstructing%20visual%20imagination%20presents%20a%20greater%0Achallenge%2C%20with%20potentially%20revolutionary%20applications%20ranging%20from%20aiding%0Aindividuals%20with%20disabilities%20to%20verifying%20witness%20accounts%20in%20court.%20The%0Aprimary%20hurdles%20in%20this%20field%20are%20the%20absence%20of%20data%20collection%20protocols%20for%0Avisual%20imagery%20and%20the%20lack%20of%20datasets%20on%20the%20subject.%20Traditionally%2C%0AfMRI-to-image%20relies%20on%20data%20collected%20from%20subjects%20exposed%20to%20visual%20stimuli%2C%0Awhich%20poses%20issues%20for%20generating%20visual%20imagery%20based%20on%20the%20difference%20of%0Abrain%20activity%20between%20visual%20stimulation%20and%20visual%20imagery.%20For%20the%20first%0Atime%2C%20we%20have%20compiled%20a%20substantial%20dataset%20%28around%206h%20of%20scans%29%20on%20visual%0Aimagery%20along%20with%20a%20proposed%20data%20collection%20protocol.%20We%20then%20train%20a%0Amodified%20version%20of%20an%20fMRI-to-image%20model%20and%20demonstrate%20the%20feasibility%20of%0Areconstructing%20images%20from%20two%20modes%20of%20imagination%3A%20from%20memory%20and%20from%20pure%0Aimagination.%20This%20marks%20an%20important%20step%20towards%20creating%20a%20technology%20that%0Aallow%20direct%20reconstruction%20of%20visual%20imagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05468v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind-to-Image%3A%20Projecting%20Visual%20Mental%20Imagination%20of%20the%20Brain%20from%0A%20%20fMRI&entry.906535625=Hugo%20Caselles-Dupr%C3%A9%20and%20Charles%20Mellerio%20and%20Paul%20H%C3%A9rent%20and%20Aliz%C3%A9e%20Lopez-Persem%20and%20Benoit%20B%C3%A9ranger%20and%20Mathieu%20Soularue%20and%20Pierre%20Fautrel%20and%20Gauthier%20Vernier%20and%20Matthieu%20Cord&entry.1292438233=%20%20The%20reconstruction%20of%20images%20observed%20by%20subjects%20from%20fMRI%20data%20collected%0Aduring%20visual%20stimuli%20has%20made%20significant%20strides%20in%20the%20past%20decade%2C%20thanks%0Ato%20the%20availability%20of%20extensive%20fMRI%20datasets%20and%20advancements%20in%20generative%0Amodels%20for%20image%20generation.%20However%2C%20the%20application%20of%20visual%20reconstruction%0Ahas%20remained%20limited.%20Reconstructing%20visual%20imagination%20presents%20a%20greater%0Achallenge%2C%20with%20potentially%20revolutionary%20applications%20ranging%20from%20aiding%0Aindividuals%20with%20disabilities%20to%20verifying%20witness%20accounts%20in%20court.%20The%0Aprimary%20hurdles%20in%20this%20field%20are%20the%20absence%20of%20data%20collection%20protocols%20for%0Avisual%20imagery%20and%20the%20lack%20of%20datasets%20on%20the%20subject.%20Traditionally%2C%0AfMRI-to-image%20relies%20on%20data%20collected%20from%20subjects%20exposed%20to%20visual%20stimuli%2C%0Awhich%20poses%20issues%20for%20generating%20visual%20imagery%20based%20on%20the%20difference%20of%0Abrain%20activity%20between%20visual%20stimulation%20and%20visual%20imagery.%20For%20the%20first%0Atime%2C%20we%20have%20compiled%20a%20substantial%20dataset%20%28around%206h%20of%20scans%29%20on%20visual%0Aimagery%20along%20with%20a%20proposed%20data%20collection%20protocol.%20We%20then%20train%20a%0Amodified%20version%20of%20an%20fMRI-to-image%20model%20and%20demonstrate%20the%20feasibility%20of%0Areconstructing%20images%20from%20two%20modes%20of%20imagination%3A%20from%20memory%20and%20from%20pure%0Aimagination.%20This%20marks%20an%20important%20step%20towards%20creating%20a%20technology%20that%0Aallow%20direct%20reconstruction%20of%20visual%20imagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05468v2&entry.124074799=Read"},
{"title": "Closing the Gap in the Trade-off between Fair Representations and\n  Accuracy", "author": "Biswajit Rout and Ananya B. Sai and Arun Rajkumar", "abstract": "  The rapid developments of various machine learning models and their\ndeployments in several applications has led to discussions around the\nimportance of looking beyond the accuracies of these models. Fairness of such\nmodels is one such aspect that is deservedly gaining more attention. In this\nwork, we analyse the natural language representations of documents and\nsentences (i.e., encodings) for any embedding-level bias that could potentially\nalso affect the fairness of the downstream tasks that rely on them. We identify\nbias in these encodings either towards or against different sub-groups based on\nthe difference in their reconstruction errors along various subsets of\nprincipal components. We explore and recommend ways to mitigate such bias in\nthe encodings while also maintaining a decent accuracy in classification models\nthat use them.\n", "link": "http://arxiv.org/abs/2404.09664v1", "date": "2024-04-15", "relevancy": 1.5036, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.504}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.498}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4974}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20Gap%20in%20the%20Trade-off%20between%20Fair%20Representations%20and%0A%20%20Accuracy&body=Title%3A%20Closing%20the%20Gap%20in%20the%20Trade-off%20between%20Fair%20Representations%20and%0A%20%20Accuracy%0AAuthor%3A%20Biswajit%20Rout%20and%20Ananya%20B.%20Sai%20and%20Arun%20Rajkumar%0AAbstract%3A%20%20%20The%20rapid%20developments%20of%20various%20machine%20learning%20models%20and%20their%0Adeployments%20in%20several%20applications%20has%20led%20to%20discussions%20around%20the%0Aimportance%20of%20looking%20beyond%20the%20accuracies%20of%20these%20models.%20Fairness%20of%20such%0Amodels%20is%20one%20such%20aspect%20that%20is%20deservedly%20gaining%20more%20attention.%20In%20this%0Awork%2C%20we%20analyse%20the%20natural%20language%20representations%20of%20documents%20and%0Asentences%20%28i.e.%2C%20encodings%29%20for%20any%20embedding-level%20bias%20that%20could%20potentially%0Aalso%20affect%20the%20fairness%20of%20the%20downstream%20tasks%20that%20rely%20on%20them.%20We%20identify%0Abias%20in%20these%20encodings%20either%20towards%20or%20against%20different%20sub-groups%20based%20on%0Athe%20difference%20in%20their%20reconstruction%20errors%20along%20various%20subsets%20of%0Aprincipal%20components.%20We%20explore%20and%20recommend%20ways%20to%20mitigate%20such%20bias%20in%0Athe%20encodings%20while%20also%20maintaining%20a%20decent%20accuracy%20in%20classification%20models%0Athat%20use%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09664v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20Gap%20in%20the%20Trade-off%20between%20Fair%20Representations%20and%0A%20%20Accuracy&entry.906535625=Biswajit%20Rout%20and%20Ananya%20B.%20Sai%20and%20Arun%20Rajkumar&entry.1292438233=%20%20The%20rapid%20developments%20of%20various%20machine%20learning%20models%20and%20their%0Adeployments%20in%20several%20applications%20has%20led%20to%20discussions%20around%20the%0Aimportance%20of%20looking%20beyond%20the%20accuracies%20of%20these%20models.%20Fairness%20of%20such%0Amodels%20is%20one%20such%20aspect%20that%20is%20deservedly%20gaining%20more%20attention.%20In%20this%0Awork%2C%20we%20analyse%20the%20natural%20language%20representations%20of%20documents%20and%0Asentences%20%28i.e.%2C%20encodings%29%20for%20any%20embedding-level%20bias%20that%20could%20potentially%0Aalso%20affect%20the%20fairness%20of%20the%20downstream%20tasks%20that%20rely%20on%20them.%20We%20identify%0Abias%20in%20these%20encodings%20either%20towards%20or%20against%20different%20sub-groups%20based%20on%0Athe%20difference%20in%20their%20reconstruction%20errors%20along%20various%20subsets%20of%0Aprincipal%20components.%20We%20explore%20and%20recommend%20ways%20to%20mitigate%20such%20bias%20in%0Athe%20encodings%20while%20also%20maintaining%20a%20decent%20accuracy%20in%20classification%20models%0Athat%20use%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09664v1&entry.124074799=Read"},
{"title": "Action Model Learning with Guarantees", "author": "Diego Aineto and Enrico Scala", "abstract": "  This paper studies the problem of action model learning with full\nobservability. Following the learning by search paradigm by Mitchell, we\ndevelop a theory for action model learning based on version spaces that\ninterprets the task as search for hypothesis that are consistent with the\nlearning examples. Our theoretical findings are instantiated in an online\nalgorithm that maintains a compact representation of all solutions of the\nproblem. Among these range of solutions, we bring attention to actions models\napproximating the actual transition system from below (sound models) and from\nabove (complete models). We show how to manipulate the output of our learning\nalgorithm to build deterministic and non-deterministic formulations of the\nsound and complete models and prove that, given enough examples, both\nformulations converge into the very same true model. Our experiments reveal\ntheir usefulness over a range of planning domains.\n", "link": "http://arxiv.org/abs/2404.09631v1", "date": "2024-04-15", "relevancy": 1.4908, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.513}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5089}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4857}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Action%20Model%20Learning%20with%20Guarantees&body=Title%3A%20Action%20Model%20Learning%20with%20Guarantees%0AAuthor%3A%20Diego%20Aineto%20and%20Enrico%20Scala%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20problem%20of%20action%20model%20learning%20with%20full%0Aobservability.%20Following%20the%20learning%20by%20search%20paradigm%20by%20Mitchell%2C%20we%0Adevelop%20a%20theory%20for%20action%20model%20learning%20based%20on%20version%20spaces%20that%0Ainterprets%20the%20task%20as%20search%20for%20hypothesis%20that%20are%20consistent%20with%20the%0Alearning%20examples.%20Our%20theoretical%20findings%20are%20instantiated%20in%20an%20online%0Aalgorithm%20that%20maintains%20a%20compact%20representation%20of%20all%20solutions%20of%20the%0Aproblem.%20Among%20these%20range%20of%20solutions%2C%20we%20bring%20attention%20to%20actions%20models%0Aapproximating%20the%20actual%20transition%20system%20from%20below%20%28sound%20models%29%20and%20from%0Aabove%20%28complete%20models%29.%20We%20show%20how%20to%20manipulate%20the%20output%20of%20our%20learning%0Aalgorithm%20to%20build%20deterministic%20and%20non-deterministic%20formulations%20of%20the%0Asound%20and%20complete%20models%20and%20prove%20that%2C%20given%20enough%20examples%2C%20both%0Aformulations%20converge%20into%20the%20very%20same%20true%20model.%20Our%20experiments%20reveal%0Atheir%20usefulness%20over%20a%20range%20of%20planning%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09631v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action%20Model%20Learning%20with%20Guarantees&entry.906535625=Diego%20Aineto%20and%20Enrico%20Scala&entry.1292438233=%20%20This%20paper%20studies%20the%20problem%20of%20action%20model%20learning%20with%20full%0Aobservability.%20Following%20the%20learning%20by%20search%20paradigm%20by%20Mitchell%2C%20we%0Adevelop%20a%20theory%20for%20action%20model%20learning%20based%20on%20version%20spaces%20that%0Ainterprets%20the%20task%20as%20search%20for%20hypothesis%20that%20are%20consistent%20with%20the%0Alearning%20examples.%20Our%20theoretical%20findings%20are%20instantiated%20in%20an%20online%0Aalgorithm%20that%20maintains%20a%20compact%20representation%20of%20all%20solutions%20of%20the%0Aproblem.%20Among%20these%20range%20of%20solutions%2C%20we%20bring%20attention%20to%20actions%20models%0Aapproximating%20the%20actual%20transition%20system%20from%20below%20%28sound%20models%29%20and%20from%0Aabove%20%28complete%20models%29.%20We%20show%20how%20to%20manipulate%20the%20output%20of%20our%20learning%0Aalgorithm%20to%20build%20deterministic%20and%20non-deterministic%20formulations%20of%20the%0Asound%20and%20complete%20models%20and%20prove%20that%2C%20given%20enough%20examples%2C%20both%0Aformulations%20converge%20into%20the%20very%20same%20true%20model.%20Our%20experiments%20reveal%0Atheir%20usefulness%20over%20a%20range%20of%20planning%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09631v1&entry.124074799=Read"},
{"title": "Human vs. LMMs: Exploring the Discrepancy in Emoji Interpretation and\n  Usage in Digital Communication", "author": "Hanjia Lyu and Weihong Qi and Zhongyu Wei and Jiebo Luo", "abstract": "  Leveraging Large Multimodal Models (LMMs) to simulate human behaviors when\nprocessing multimodal information, especially in the context of social media,\nhas garnered immense interest due to its broad potential and far-reaching\nimplications. Emojis, as one of the most unique aspects of digital\ncommunication, are pivotal in enriching and often clarifying the emotional and\ntonal dimensions. Yet, there is a notable gap in understanding how these\nadvanced models, such as GPT-4V, interpret and employ emojis in the nuanced\ncontext of online interaction. This study intends to bridge this gap by\nexamining the behavior of GPT-4V in replicating human-like use of emojis. The\nfindings reveal a discernible discrepancy between human and GPT-4V behaviors,\nlikely due to the subjective nature of human interpretation and the limitations\nof GPT-4V's English-centric training, suggesting cultural biases and inadequate\nrepresentation of non-English cultures.\n", "link": "http://arxiv.org/abs/2401.08212v2", "date": "2024-04-15", "relevancy": 1.4834, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5016}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.49}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.481}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Human%20vs.%20LMMs%3A%20Exploring%20the%20Discrepancy%20in%20Emoji%20Interpretation%20and%0A%20%20Usage%20in%20Digital%20Communication&body=Title%3A%20Human%20vs.%20LMMs%3A%20Exploring%20the%20Discrepancy%20in%20Emoji%20Interpretation%20and%0A%20%20Usage%20in%20Digital%20Communication%0AAuthor%3A%20Hanjia%20Lyu%20and%20Weihong%20Qi%20and%20Zhongyu%20Wei%20and%20Jiebo%20Luo%0AAbstract%3A%20%20%20Leveraging%20Large%20Multimodal%20Models%20%28LMMs%29%20to%20simulate%20human%20behaviors%20when%0Aprocessing%20multimodal%20information%2C%20especially%20in%20the%20context%20of%20social%20media%2C%0Ahas%20garnered%20immense%20interest%20due%20to%20its%20broad%20potential%20and%20far-reaching%0Aimplications.%20Emojis%2C%20as%20one%20of%20the%20most%20unique%20aspects%20of%20digital%0Acommunication%2C%20are%20pivotal%20in%20enriching%20and%20often%20clarifying%20the%20emotional%20and%0Atonal%20dimensions.%20Yet%2C%20there%20is%20a%20notable%20gap%20in%20understanding%20how%20these%0Aadvanced%20models%2C%20such%20as%20GPT-4V%2C%20interpret%20and%20employ%20emojis%20in%20the%20nuanced%0Acontext%20of%20online%20interaction.%20This%20study%20intends%20to%20bridge%20this%20gap%20by%0Aexamining%20the%20behavior%20of%20GPT-4V%20in%20replicating%20human-like%20use%20of%20emojis.%20The%0Afindings%20reveal%20a%20discernible%20discrepancy%20between%20human%20and%20GPT-4V%20behaviors%2C%0Alikely%20due%20to%20the%20subjective%20nature%20of%20human%20interpretation%20and%20the%20limitations%0Aof%20GPT-4V%27s%20English-centric%20training%2C%20suggesting%20cultural%20biases%20and%20inadequate%0Arepresentation%20of%20non-English%20cultures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08212v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20vs.%20LMMs%3A%20Exploring%20the%20Discrepancy%20in%20Emoji%20Interpretation%20and%0A%20%20Usage%20in%20Digital%20Communication&entry.906535625=Hanjia%20Lyu%20and%20Weihong%20Qi%20and%20Zhongyu%20Wei%20and%20Jiebo%20Luo&entry.1292438233=%20%20Leveraging%20Large%20Multimodal%20Models%20%28LMMs%29%20to%20simulate%20human%20behaviors%20when%0Aprocessing%20multimodal%20information%2C%20especially%20in%20the%20context%20of%20social%20media%2C%0Ahas%20garnered%20immense%20interest%20due%20to%20its%20broad%20potential%20and%20far-reaching%0Aimplications.%20Emojis%2C%20as%20one%20of%20the%20most%20unique%20aspects%20of%20digital%0Acommunication%2C%20are%20pivotal%20in%20enriching%20and%20often%20clarifying%20the%20emotional%20and%0Atonal%20dimensions.%20Yet%2C%20there%20is%20a%20notable%20gap%20in%20understanding%20how%20these%0Aadvanced%20models%2C%20such%20as%20GPT-4V%2C%20interpret%20and%20employ%20emojis%20in%20the%20nuanced%0Acontext%20of%20online%20interaction.%20This%20study%20intends%20to%20bridge%20this%20gap%20by%0Aexamining%20the%20behavior%20of%20GPT-4V%20in%20replicating%20human-like%20use%20of%20emojis.%20The%0Afindings%20reveal%20a%20discernible%20discrepancy%20between%20human%20and%20GPT-4V%20behaviors%2C%0Alikely%20due%20to%20the%20subjective%20nature%20of%20human%20interpretation%20and%20the%20limitations%0Aof%20GPT-4V%27s%20English-centric%20training%2C%20suggesting%20cultural%20biases%20and%20inadequate%0Arepresentation%20of%20non-English%20cultures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08212v2&entry.124074799=Read"},
{"title": "Convergence Analysis of Probability Flow ODE for Score-based Generative\n  Models", "author": "Daniel Zhengyu Huang and Jiaoyang Huang and Zhengjiang Lin", "abstract": "  Score-based generative models have emerged as a powerful approach for\nsampling high-dimensional probability distributions. Despite their\neffectiveness, their theoretical underpinnings remain relatively\nunderdeveloped. In this work, we study the convergence properties of\ndeterministic samplers based on probability flow ODEs from both theoretical and\nnumerical perspectives. Assuming access to $L^2$-accurate estimates of the\nscore function, we prove the total variation between the target and the\ngenerated data distributions can be bounded above by\n$\\mathcal{O}(d\\sqrt{\\delta})$ in the continuous time level, where $d$ denotes\nthe data dimension and $\\delta$ represents the $L^2$-score matching error. For\npractical implementations using a $p$-th order Runge-Kutta integrator with step\nsize $h$, we establish error bounds of $\\mathcal{O}(d(\\sqrt{\\delta} + (dh)^p))$\nat the discrete level. Finally, we present numerical studies on problems up to\n$128$ dimensions to verify our theory, which indicate a better score matching\nerror and dimension dependence.\n", "link": "http://arxiv.org/abs/2404.09730v1", "date": "2024-04-15", "relevancy": 1.476, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5314}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4891}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4599}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Convergence%20Analysis%20of%20Probability%20Flow%20ODE%20for%20Score-based%20Generative%0A%20%20Models&body=Title%3A%20Convergence%20Analysis%20of%20Probability%20Flow%20ODE%20for%20Score-based%20Generative%0A%20%20Models%0AAuthor%3A%20Daniel%20Zhengyu%20Huang%20and%20Jiaoyang%20Huang%20and%20Zhengjiang%20Lin%0AAbstract%3A%20%20%20Score-based%20generative%20models%20have%20emerged%20as%20a%20powerful%20approach%20for%0Asampling%20high-dimensional%20probability%20distributions.%20Despite%20their%0Aeffectiveness%2C%20their%20theoretical%20underpinnings%20remain%20relatively%0Aunderdeveloped.%20In%20this%20work%2C%20we%20study%20the%20convergence%20properties%20of%0Adeterministic%20samplers%20based%20on%20probability%20flow%20ODEs%20from%20both%20theoretical%20and%0Anumerical%20perspectives.%20Assuming%20access%20to%20%24L%5E2%24-accurate%20estimates%20of%20the%0Ascore%20function%2C%20we%20prove%20the%20total%20variation%20between%20the%20target%20and%20the%0Agenerated%20data%20distributions%20can%20be%20bounded%20above%20by%0A%24%5Cmathcal%7BO%7D%28d%5Csqrt%7B%5Cdelta%7D%29%24%20in%20the%20continuous%20time%20level%2C%20where%20%24d%24%20denotes%0Athe%20data%20dimension%20and%20%24%5Cdelta%24%20represents%20the%20%24L%5E2%24-score%20matching%20error.%20For%0Apractical%20implementations%20using%20a%20%24p%24-th%20order%20Runge-Kutta%20integrator%20with%20step%0Asize%20%24h%24%2C%20we%20establish%20error%20bounds%20of%20%24%5Cmathcal%7BO%7D%28d%28%5Csqrt%7B%5Cdelta%7D%20%2B%20%28dh%29%5Ep%29%29%24%0Aat%20the%20discrete%20level.%20Finally%2C%20we%20present%20numerical%20studies%20on%20problems%20up%20to%0A%24128%24%20dimensions%20to%20verify%20our%20theory%2C%20which%20indicate%20a%20better%20score%20matching%0Aerror%20and%20dimension%20dependence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09730v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20Analysis%20of%20Probability%20Flow%20ODE%20for%20Score-based%20Generative%0A%20%20Models&entry.906535625=Daniel%20Zhengyu%20Huang%20and%20Jiaoyang%20Huang%20and%20Zhengjiang%20Lin&entry.1292438233=%20%20Score-based%20generative%20models%20have%20emerged%20as%20a%20powerful%20approach%20for%0Asampling%20high-dimensional%20probability%20distributions.%20Despite%20their%0Aeffectiveness%2C%20their%20theoretical%20underpinnings%20remain%20relatively%0Aunderdeveloped.%20In%20this%20work%2C%20we%20study%20the%20convergence%20properties%20of%0Adeterministic%20samplers%20based%20on%20probability%20flow%20ODEs%20from%20both%20theoretical%20and%0Anumerical%20perspectives.%20Assuming%20access%20to%20%24L%5E2%24-accurate%20estimates%20of%20the%0Ascore%20function%2C%20we%20prove%20the%20total%20variation%20between%20the%20target%20and%20the%0Agenerated%20data%20distributions%20can%20be%20bounded%20above%20by%0A%24%5Cmathcal%7BO%7D%28d%5Csqrt%7B%5Cdelta%7D%29%24%20in%20the%20continuous%20time%20level%2C%20where%20%24d%24%20denotes%0Athe%20data%20dimension%20and%20%24%5Cdelta%24%20represents%20the%20%24L%5E2%24-score%20matching%20error.%20For%0Apractical%20implementations%20using%20a%20%24p%24-th%20order%20Runge-Kutta%20integrator%20with%20step%0Asize%20%24h%24%2C%20we%20establish%20error%20bounds%20of%20%24%5Cmathcal%7BO%7D%28d%28%5Csqrt%7B%5Cdelta%7D%20%2B%20%28dh%29%5Ep%29%29%24%0Aat%20the%20discrete%20level.%20Finally%2C%20we%20present%20numerical%20studies%20on%20problems%20up%20to%0A%24128%24%20dimensions%20to%20verify%20our%20theory%2C%20which%20indicate%20a%20better%20score%20matching%0Aerror%20and%20dimension%20dependence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09730v1&entry.124074799=Read"},
{"title": "Quantization of Large Language Models with an Overdetermined Basis", "author": "Daniil Merkulov and Daria Cherniuk and Alexander Rudikov and Ivan Oseledets and Ekaterina Muravleva and Aleksandr Mikhalev and Boris Kashin", "abstract": "  In this paper, we introduce an algorithm for data quantization based on the\nprinciples of Kashin representation. This approach hinges on decomposing any\ngiven vector, matrix, or tensor into two factors. The first factor maintains a\nsmall infinity norm, while the second exhibits a similarly constrained norm\nwhen multiplied by an orthogonal matrix. Surprisingly, the entries of factors\nafter decomposition are well-concentrated around several peaks, which allows us\nto efficiently replace them with corresponding centroids for quantization\npurposes. We study the theoretical properties of the proposed approach and\nrigorously evaluate our compression algorithm in the context of next-word\nprediction tasks and on a set of downstream tasks for text classification. Our\nfindings demonstrate that Kashin Quantization achieves competitive or superior\nquality in model performance while ensuring data compression, marking a\nsignificant advancement in the field of data quantization.\n", "link": "http://arxiv.org/abs/2404.09737v1", "date": "2024-04-15", "relevancy": 1.4499, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4878}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4873}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4679}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Quantization%20of%20Large%20Language%20Models%20with%20an%20Overdetermined%20Basis&body=Title%3A%20Quantization%20of%20Large%20Language%20Models%20with%20an%20Overdetermined%20Basis%0AAuthor%3A%20Daniil%20Merkulov%20and%20Daria%20Cherniuk%20and%20Alexander%20Rudikov%20and%20Ivan%20Oseledets%20and%20Ekaterina%20Muravleva%20and%20Aleksandr%20Mikhalev%20and%20Boris%20Kashin%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20an%20algorithm%20for%20data%20quantization%20based%20on%20the%0Aprinciples%20of%20Kashin%20representation.%20This%20approach%20hinges%20on%20decomposing%20any%0Agiven%20vector%2C%20matrix%2C%20or%20tensor%20into%20two%20factors.%20The%20first%20factor%20maintains%20a%0Asmall%20infinity%20norm%2C%20while%20the%20second%20exhibits%20a%20similarly%20constrained%20norm%0Awhen%20multiplied%20by%20an%20orthogonal%20matrix.%20Surprisingly%2C%20the%20entries%20of%20factors%0Aafter%20decomposition%20are%20well-concentrated%20around%20several%20peaks%2C%20which%20allows%20us%0Ato%20efficiently%20replace%20them%20with%20corresponding%20centroids%20for%20quantization%0Apurposes.%20We%20study%20the%20theoretical%20properties%20of%20the%20proposed%20approach%20and%0Arigorously%20evaluate%20our%20compression%20algorithm%20in%20the%20context%20of%20next-word%0Aprediction%20tasks%20and%20on%20a%20set%20of%20downstream%20tasks%20for%20text%20classification.%20Our%0Afindings%20demonstrate%20that%20Kashin%20Quantization%20achieves%20competitive%20or%20superior%0Aquality%20in%20model%20performance%20while%20ensuring%20data%20compression%2C%20marking%20a%0Asignificant%20advancement%20in%20the%20field%20of%20data%20quantization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09737v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantization%20of%20Large%20Language%20Models%20with%20an%20Overdetermined%20Basis&entry.906535625=Daniil%20Merkulov%20and%20Daria%20Cherniuk%20and%20Alexander%20Rudikov%20and%20Ivan%20Oseledets%20and%20Ekaterina%20Muravleva%20and%20Aleksandr%20Mikhalev%20and%20Boris%20Kashin&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20an%20algorithm%20for%20data%20quantization%20based%20on%20the%0Aprinciples%20of%20Kashin%20representation.%20This%20approach%20hinges%20on%20decomposing%20any%0Agiven%20vector%2C%20matrix%2C%20or%20tensor%20into%20two%20factors.%20The%20first%20factor%20maintains%20a%0Asmall%20infinity%20norm%2C%20while%20the%20second%20exhibits%20a%20similarly%20constrained%20norm%0Awhen%20multiplied%20by%20an%20orthogonal%20matrix.%20Surprisingly%2C%20the%20entries%20of%20factors%0Aafter%20decomposition%20are%20well-concentrated%20around%20several%20peaks%2C%20which%20allows%20us%0Ato%20efficiently%20replace%20them%20with%20corresponding%20centroids%20for%20quantization%0Apurposes.%20We%20study%20the%20theoretical%20properties%20of%20the%20proposed%20approach%20and%0Arigorously%20evaluate%20our%20compression%20algorithm%20in%20the%20context%20of%20next-word%0Aprediction%20tasks%20and%20on%20a%20set%20of%20downstream%20tasks%20for%20text%20classification.%20Our%0Afindings%20demonstrate%20that%20Kashin%20Quantization%20achieves%20competitive%20or%20superior%0Aquality%20in%20model%20performance%20while%20ensuring%20data%20compression%2C%20marking%20a%0Asignificant%20advancement%20in%20the%20field%20of%20data%20quantization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09737v1&entry.124074799=Read"},
{"title": "Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\n  Large Language Model", "author": "Hyunsoo Cho", "abstract": "  Many recent studies endeavor to improve open-source language models through\nimitation learning, and re-training on the synthetic instruction data from\nstate-of-the-art proprietary models like ChatGPT and GPT-4. However, the innate\nnature of synthetic data inherently contains noisy data, giving rise to a\nsubstantial presence of low-quality data replete with erroneous responses, and\nflawed reasoning. Although we intuitively grasp the potential harm of noisy\ndata, we lack a quantitative understanding of its impact. To this end, this\npaper explores the correlation between the degree of noise and its impact on\nlanguage models through instruction tuning. We first introduce the\nFalsity-Controllable (FACO) dataset, which comprises pairs of true answers with\ncorresponding reasoning, as well as false pairs to manually control the falsity\nratio of the dataset.Through our extensive experiments, we found multiple\nintriguing findings of the correlation between the factuality of the dataset\nand instruction tuning: Specifically, we verified falsity of the instruction is\nhighly relevant to various benchmark scores. Moreover, when LLMs are trained\nwith false instructions, they learn to lie and generate fake unfaithful\nanswers, even though they know the correct answer for the user request.\nAdditionally, we noted that once the language model is trained with a dataset\ncontaminated by noise, restoring its original performance is possible, but it\nfailed to reach full performance.\n", "link": "http://arxiv.org/abs/2404.09717v1", "date": "2024-04-15", "relevancy": 1.4451, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.493}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4803}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4777}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Imitation%20Learning%3A%20Exploring%20the%20Impact%20of%20Data%20Falsity%20to%0A%20%20Large%20Language%20Model&body=Title%3A%20Unveiling%20Imitation%20Learning%3A%20Exploring%20the%20Impact%20of%20Data%20Falsity%20to%0A%20%20Large%20Language%20Model%0AAuthor%3A%20Hyunsoo%20Cho%0AAbstract%3A%20%20%20Many%20recent%20studies%20endeavor%20to%20improve%20open-source%20language%20models%20through%0Aimitation%20learning%2C%20and%20re-training%20on%20the%20synthetic%20instruction%20data%20from%0Astate-of-the-art%20proprietary%20models%20like%20ChatGPT%20and%20GPT-4.%20However%2C%20the%20innate%0Anature%20of%20synthetic%20data%20inherently%20contains%20noisy%20data%2C%20giving%20rise%20to%20a%0Asubstantial%20presence%20of%20low-quality%20data%20replete%20with%20erroneous%20responses%2C%20and%0Aflawed%20reasoning.%20Although%20we%20intuitively%20grasp%20the%20potential%20harm%20of%20noisy%0Adata%2C%20we%20lack%20a%20quantitative%20understanding%20of%20its%20impact.%20To%20this%20end%2C%20this%0Apaper%20explores%20the%20correlation%20between%20the%20degree%20of%20noise%20and%20its%20impact%20on%0Alanguage%20models%20through%20instruction%20tuning.%20We%20first%20introduce%20the%0AFalsity-Controllable%20%28FACO%29%20dataset%2C%20which%20comprises%20pairs%20of%20true%20answers%20with%0Acorresponding%20reasoning%2C%20as%20well%20as%20false%20pairs%20to%20manually%20control%20the%20falsity%0Aratio%20of%20the%20dataset.Through%20our%20extensive%20experiments%2C%20we%20found%20multiple%0Aintriguing%20findings%20of%20the%20correlation%20between%20the%20factuality%20of%20the%20dataset%0Aand%20instruction%20tuning%3A%20Specifically%2C%20we%20verified%20falsity%20of%20the%20instruction%20is%0Ahighly%20relevant%20to%20various%20benchmark%20scores.%20Moreover%2C%20when%20LLMs%20are%20trained%0Awith%20false%20instructions%2C%20they%20learn%20to%20lie%20and%20generate%20fake%20unfaithful%0Aanswers%2C%20even%20though%20they%20know%20the%20correct%20answer%20for%20the%20user%20request.%0AAdditionally%2C%20we%20noted%20that%20once%20the%20language%20model%20is%20trained%20with%20a%20dataset%0Acontaminated%20by%20noise%2C%20restoring%20its%20original%20performance%20is%20possible%2C%20but%20it%0Afailed%20to%20reach%20full%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09717v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Imitation%20Learning%3A%20Exploring%20the%20Impact%20of%20Data%20Falsity%20to%0A%20%20Large%20Language%20Model&entry.906535625=Hyunsoo%20Cho&entry.1292438233=%20%20Many%20recent%20studies%20endeavor%20to%20improve%20open-source%20language%20models%20through%0Aimitation%20learning%2C%20and%20re-training%20on%20the%20synthetic%20instruction%20data%20from%0Astate-of-the-art%20proprietary%20models%20like%20ChatGPT%20and%20GPT-4.%20However%2C%20the%20innate%0Anature%20of%20synthetic%20data%20inherently%20contains%20noisy%20data%2C%20giving%20rise%20to%20a%0Asubstantial%20presence%20of%20low-quality%20data%20replete%20with%20erroneous%20responses%2C%20and%0Aflawed%20reasoning.%20Although%20we%20intuitively%20grasp%20the%20potential%20harm%20of%20noisy%0Adata%2C%20we%20lack%20a%20quantitative%20understanding%20of%20its%20impact.%20To%20this%20end%2C%20this%0Apaper%20explores%20the%20correlation%20between%20the%20degree%20of%20noise%20and%20its%20impact%20on%0Alanguage%20models%20through%20instruction%20tuning.%20We%20first%20introduce%20the%0AFalsity-Controllable%20%28FACO%29%20dataset%2C%20which%20comprises%20pairs%20of%20true%20answers%20with%0Acorresponding%20reasoning%2C%20as%20well%20as%20false%20pairs%20to%20manually%20control%20the%20falsity%0Aratio%20of%20the%20dataset.Through%20our%20extensive%20experiments%2C%20we%20found%20multiple%0Aintriguing%20findings%20of%20the%20correlation%20between%20the%20factuality%20of%20the%20dataset%0Aand%20instruction%20tuning%3A%20Specifically%2C%20we%20verified%20falsity%20of%20the%20instruction%20is%0Ahighly%20relevant%20to%20various%20benchmark%20scores.%20Moreover%2C%20when%20LLMs%20are%20trained%0Awith%20false%20instructions%2C%20they%20learn%20to%20lie%20and%20generate%20fake%20unfaithful%0Aanswers%2C%20even%20though%20they%20know%20the%20correct%20answer%20for%20the%20user%20request.%0AAdditionally%2C%20we%20noted%20that%20once%20the%20language%20model%20is%20trained%20with%20a%20dataset%0Acontaminated%20by%20noise%2C%20restoring%20its%20original%20performance%20is%20possible%2C%20but%20it%0Afailed%20to%20reach%20full%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09717v1&entry.124074799=Read"},
{"title": "CBQ: Cross-Block Quantization for Large Language Models", "author": "Xin Ding and Xiaoyu Liu and Zhijun Tu and Yun Zhang and Wei Li and Jie Hu and Hanting Chen and Yehui Tang and Zhiwei Xiong and Baoqun Yin and Yunhe Wang", "abstract": "  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n", "link": "http://arxiv.org/abs/2312.07950v4", "date": "2024-04-15", "relevancy": 1.4219, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4805}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4665}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4651}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CBQ%3A%20Cross-Block%20Quantization%20for%20Large%20Language%20Models&body=Title%3A%20CBQ%3A%20Cross-Block%20Quantization%20for%20Large%20Language%20Models%0AAuthor%3A%20Xin%20Ding%20and%20Xiaoyu%20Liu%20and%20Zhijun%20Tu%20and%20Yun%20Zhang%20and%20Wei%20Li%20and%20Jie%20Hu%20and%20Hanting%20Chen%20and%20Yehui%20Tang%20and%20Zhiwei%20Xiong%20and%20Baoqun%20Yin%20and%20Yunhe%20Wang%0AAbstract%3A%20%20%20Post-training%20quantization%20%28PTQ%29%20has%20played%20a%20key%20role%20in%20compressing%20large%0Alanguage%20models%20%28LLMs%29%20with%20ultra-low%20costs.%20However%2C%20existing%20PTQ%20methods%20only%0Afocus%20on%20handling%20the%20outliers%20within%20one%20layer%20or%20one%20block%2C%20which%20ignores%20the%0Adependency%20of%20blocks%20and%20leads%20to%20severe%20performance%20degradation%20in%20low-bit%0Asettings.%20In%20this%20paper%2C%20we%20propose%20CBQ%2C%20a%20cross-block%20reconstruction-based%20PTQ%0Amethod%20for%20LLMs.%20CBQ%20employs%20a%20cross-block%20dependency%20using%20a%20homologous%0Areconstruction%20scheme%2C%20establishing%20long-range%20dependencies%20across%20multiple%0Ablocks%20to%20minimize%20error%20accumulation.%20Furthermore%2C%20CBQ%20incorporates%20a%0Acoarse-to-fine%20preprocessing%20%28CFP%29%20strategy%20for%20suppressing%20weight%20and%0Aactivation%20outliers%2C%20coupled%20with%20an%20adaptive%20LoRA-Rounding%20technique%20for%0Aprecise%20weight%20quantization.%20These%20innovations%20enable%20CBQ%20to%20not%20only%20handle%0Aextreme%20outliers%20effectively%20but%20also%20improve%20overall%20quantization%20accuracy.%0AExtensive%20experiments%20show%20that%20CBQ%20achieves%20superior%20low-bit%20quantization%0A%28W4A4%2C%20W4A8%2C%20W2A16%29%20and%20outperforms%20existing%20state-of-the-art%20methods%20across%0Avarious%20LLMs%20and%20datasets.%20Notably%2C%20CBQ%20quantizes%20the%204-bit%20LLAMA1-65B%20model%0Awithin%20only%204.3%20hours%20on%20a%20single%20GPU%2C%20achieving%20a%20commendable%20tradeoff%20between%0Aperformance%20and%20quantization%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07950v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CBQ%3A%20Cross-Block%20Quantization%20for%20Large%20Language%20Models&entry.906535625=Xin%20Ding%20and%20Xiaoyu%20Liu%20and%20Zhijun%20Tu%20and%20Yun%20Zhang%20and%20Wei%20Li%20and%20Jie%20Hu%20and%20Hanting%20Chen%20and%20Yehui%20Tang%20and%20Zhiwei%20Xiong%20and%20Baoqun%20Yin%20and%20Yunhe%20Wang&entry.1292438233=%20%20Post-training%20quantization%20%28PTQ%29%20has%20played%20a%20key%20role%20in%20compressing%20large%0Alanguage%20models%20%28LLMs%29%20with%20ultra-low%20costs.%20However%2C%20existing%20PTQ%20methods%20only%0Afocus%20on%20handling%20the%20outliers%20within%20one%20layer%20or%20one%20block%2C%20which%20ignores%20the%0Adependency%20of%20blocks%20and%20leads%20to%20severe%20performance%20degradation%20in%20low-bit%0Asettings.%20In%20this%20paper%2C%20we%20propose%20CBQ%2C%20a%20cross-block%20reconstruction-based%20PTQ%0Amethod%20for%20LLMs.%20CBQ%20employs%20a%20cross-block%20dependency%20using%20a%20homologous%0Areconstruction%20scheme%2C%20establishing%20long-range%20dependencies%20across%20multiple%0Ablocks%20to%20minimize%20error%20accumulation.%20Furthermore%2C%20CBQ%20incorporates%20a%0Acoarse-to-fine%20preprocessing%20%28CFP%29%20strategy%20for%20suppressing%20weight%20and%0Aactivation%20outliers%2C%20coupled%20with%20an%20adaptive%20LoRA-Rounding%20technique%20for%0Aprecise%20weight%20quantization.%20These%20innovations%20enable%20CBQ%20to%20not%20only%20handle%0Aextreme%20outliers%20effectively%20but%20also%20improve%20overall%20quantization%20accuracy.%0AExtensive%20experiments%20show%20that%20CBQ%20achieves%20superior%20low-bit%20quantization%0A%28W4A4%2C%20W4A8%2C%20W2A16%29%20and%20outperforms%20existing%20state-of-the-art%20methods%20across%0Avarious%20LLMs%20and%20datasets.%20Notably%2C%20CBQ%20quantizes%20the%204-bit%20LLAMA1-65B%20model%0Awithin%20only%204.3%20hours%20on%20a%20single%20GPU%2C%20achieving%20a%20commendable%20tradeoff%20between%0Aperformance%20and%20quantization%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07950v4&entry.124074799=Read"},
{"title": "AntBatchInfer: Elastic Batch Inference in the Kubernetes Cluster", "author": "Siyuan Li and Youshao Xiao and Fanzhuang Meng and Lin Ju and Lei Liang and Lin Wang and Jun Zhou", "abstract": "  Offline batch inference is a common task in the industry for deep learning\napplications, but it can be challenging to ensure stability and performance\nwhen dealing with large amounts of data and complicated inference pipelines.\nThis paper demonstrated AntBatchInfer, an elastic batch inference framework,\nwhich is specially optimized for the non-dedicated cluster. AntBatchInfer\naddresses these challenges by providing multi-level fault-tolerant\ncapabilities, enabling the stable execution of versatile and long-running\ninference tasks. It also improves inference efficiency by pipelining,\nintra-node, and inter-node scaling. It further optimizes the performance in\ncomplicated multiple-model batch inference scenarios. Through extensive\nexperiments and real-world statistics, we demonstrate the superiority of our\nframework in terms of stability and efficiency. In the experiment, it\noutperforms the baseline by at least $2\\times$ and $6\\times$ in the\nsingle-model or multiple-model batch inference. Also, it is widely used at Ant\nGroup, with thousands of daily jobs from various scenarios, including DLRM, CV,\nand NLP, which proves its practicability in the industry.\n", "link": "http://arxiv.org/abs/2404.09686v1", "date": "2024-04-15", "relevancy": 1.3981, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4832}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4495}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4398}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AntBatchInfer%3A%20Elastic%20Batch%20Inference%20in%20the%20Kubernetes%20Cluster&body=Title%3A%20AntBatchInfer%3A%20Elastic%20Batch%20Inference%20in%20the%20Kubernetes%20Cluster%0AAuthor%3A%20Siyuan%20Li%20and%20Youshao%20Xiao%20and%20Fanzhuang%20Meng%20and%20Lin%20Ju%20and%20Lei%20Liang%20and%20Lin%20Wang%20and%20Jun%20Zhou%0AAbstract%3A%20%20%20Offline%20batch%20inference%20is%20a%20common%20task%20in%20the%20industry%20for%20deep%20learning%0Aapplications%2C%20but%20it%20can%20be%20challenging%20to%20ensure%20stability%20and%20performance%0Awhen%20dealing%20with%20large%20amounts%20of%20data%20and%20complicated%20inference%20pipelines.%0AThis%20paper%20demonstrated%20AntBatchInfer%2C%20an%20elastic%20batch%20inference%20framework%2C%0Awhich%20is%20specially%20optimized%20for%20the%20non-dedicated%20cluster.%20AntBatchInfer%0Aaddresses%20these%20challenges%20by%20providing%20multi-level%20fault-tolerant%0Acapabilities%2C%20enabling%20the%20stable%20execution%20of%20versatile%20and%20long-running%0Ainference%20tasks.%20It%20also%20improves%20inference%20efficiency%20by%20pipelining%2C%0Aintra-node%2C%20and%20inter-node%20scaling.%20It%20further%20optimizes%20the%20performance%20in%0Acomplicated%20multiple-model%20batch%20inference%20scenarios.%20Through%20extensive%0Aexperiments%20and%20real-world%20statistics%2C%20we%20demonstrate%20the%20superiority%20of%20our%0Aframework%20in%20terms%20of%20stability%20and%20efficiency.%20In%20the%20experiment%2C%20it%0Aoutperforms%20the%20baseline%20by%20at%20least%20%242%5Ctimes%24%20and%20%246%5Ctimes%24%20in%20the%0Asingle-model%20or%20multiple-model%20batch%20inference.%20Also%2C%20it%20is%20widely%20used%20at%20Ant%0AGroup%2C%20with%20thousands%20of%20daily%20jobs%20from%20various%20scenarios%2C%20including%20DLRM%2C%20CV%2C%0Aand%20NLP%2C%20which%20proves%20its%20practicability%20in%20the%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09686v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AntBatchInfer%3A%20Elastic%20Batch%20Inference%20in%20the%20Kubernetes%20Cluster&entry.906535625=Siyuan%20Li%20and%20Youshao%20Xiao%20and%20Fanzhuang%20Meng%20and%20Lin%20Ju%20and%20Lei%20Liang%20and%20Lin%20Wang%20and%20Jun%20Zhou&entry.1292438233=%20%20Offline%20batch%20inference%20is%20a%20common%20task%20in%20the%20industry%20for%20deep%20learning%0Aapplications%2C%20but%20it%20can%20be%20challenging%20to%20ensure%20stability%20and%20performance%0Awhen%20dealing%20with%20large%20amounts%20of%20data%20and%20complicated%20inference%20pipelines.%0AThis%20paper%20demonstrated%20AntBatchInfer%2C%20an%20elastic%20batch%20inference%20framework%2C%0Awhich%20is%20specially%20optimized%20for%20the%20non-dedicated%20cluster.%20AntBatchInfer%0Aaddresses%20these%20challenges%20by%20providing%20multi-level%20fault-tolerant%0Acapabilities%2C%20enabling%20the%20stable%20execution%20of%20versatile%20and%20long-running%0Ainference%20tasks.%20It%20also%20improves%20inference%20efficiency%20by%20pipelining%2C%0Aintra-node%2C%20and%20inter-node%20scaling.%20It%20further%20optimizes%20the%20performance%20in%0Acomplicated%20multiple-model%20batch%20inference%20scenarios.%20Through%20extensive%0Aexperiments%20and%20real-world%20statistics%2C%20we%20demonstrate%20the%20superiority%20of%20our%0Aframework%20in%20terms%20of%20stability%20and%20efficiency.%20In%20the%20experiment%2C%20it%0Aoutperforms%20the%20baseline%20by%20at%20least%20%242%5Ctimes%24%20and%20%246%5Ctimes%24%20in%20the%0Asingle-model%20or%20multiple-model%20batch%20inference.%20Also%2C%20it%20is%20widely%20used%20at%20Ant%0AGroup%2C%20with%20thousands%20of%20daily%20jobs%20from%20various%20scenarios%2C%20including%20DLRM%2C%20CV%2C%0Aand%20NLP%2C%20which%20proves%20its%20practicability%20in%20the%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09686v1&entry.124074799=Read"},
{"title": "Psychometric Predictive Power of Large Language Models", "author": "Tatsuki Kuribayashi and Yohei Oseki and Timothy Baldwin", "abstract": "  Instruction tuning aligns the response of large language models (LLMs) with\nhuman preferences. Despite such efforts in human--LLM alignment, we find that\ninstruction tuning does not always make LLMs human-like from a cognitive\nmodeling perspective. More specifically, next-word probabilities estimated by\ninstruction-tuned LLMs are often worse at simulating human reading behavior\nthan those estimated by base LLMs. In addition, we explore prompting\nmethodologies for simulating human reading behavior with LLMs. Our results show\nthat prompts reflecting a particular linguistic hypothesis improve psychometric\npredictive power, but are still inferior to small base models. These findings\nhighlight that recent advancements in LLMs, i.e., instruction tuning and\nprompting, do not offer better estimates than direct probability measurements\nfrom base LLMs in cognitive modeling. In other words, pure next-word\nprobability remains a strong predictor for human reading behavior, even in the\nage of LLMs.\n", "link": "http://arxiv.org/abs/2311.07484v3", "date": "2024-04-15", "relevancy": 1.3477, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4625}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4457}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4447}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Psychometric%20Predictive%20Power%20of%20Large%20Language%20Models&body=Title%3A%20Psychometric%20Predictive%20Power%20of%20Large%20Language%20Models%0AAuthor%3A%20Tatsuki%20Kuribayashi%20and%20Yohei%20Oseki%20and%20Timothy%20Baldwin%0AAbstract%3A%20%20%20Instruction%20tuning%20aligns%20the%20response%20of%20large%20language%20models%20%28LLMs%29%20with%0Ahuman%20preferences.%20Despite%20such%20efforts%20in%20human--LLM%20alignment%2C%20we%20find%20that%0Ainstruction%20tuning%20does%20not%20always%20make%20LLMs%20human-like%20from%20a%20cognitive%0Amodeling%20perspective.%20More%20specifically%2C%20next-word%20probabilities%20estimated%20by%0Ainstruction-tuned%20LLMs%20are%20often%20worse%20at%20simulating%20human%20reading%20behavior%0Athan%20those%20estimated%20by%20base%20LLMs.%20In%20addition%2C%20we%20explore%20prompting%0Amethodologies%20for%20simulating%20human%20reading%20behavior%20with%20LLMs.%20Our%20results%20show%0Athat%20prompts%20reflecting%20a%20particular%20linguistic%20hypothesis%20improve%20psychometric%0Apredictive%20power%2C%20but%20are%20still%20inferior%20to%20small%20base%20models.%20These%20findings%0Ahighlight%20that%20recent%20advancements%20in%20LLMs%2C%20i.e.%2C%20instruction%20tuning%20and%0Aprompting%2C%20do%20not%20offer%20better%20estimates%20than%20direct%20probability%20measurements%0Afrom%20base%20LLMs%20in%20cognitive%20modeling.%20In%20other%20words%2C%20pure%20next-word%0Aprobability%20remains%20a%20strong%20predictor%20for%20human%20reading%20behavior%2C%20even%20in%20the%0Aage%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07484v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Psychometric%20Predictive%20Power%20of%20Large%20Language%20Models&entry.906535625=Tatsuki%20Kuribayashi%20and%20Yohei%20Oseki%20and%20Timothy%20Baldwin&entry.1292438233=%20%20Instruction%20tuning%20aligns%20the%20response%20of%20large%20language%20models%20%28LLMs%29%20with%0Ahuman%20preferences.%20Despite%20such%20efforts%20in%20human--LLM%20alignment%2C%20we%20find%20that%0Ainstruction%20tuning%20does%20not%20always%20make%20LLMs%20human-like%20from%20a%20cognitive%0Amodeling%20perspective.%20More%20specifically%2C%20next-word%20probabilities%20estimated%20by%0Ainstruction-tuned%20LLMs%20are%20often%20worse%20at%20simulating%20human%20reading%20behavior%0Athan%20those%20estimated%20by%20base%20LLMs.%20In%20addition%2C%20we%20explore%20prompting%0Amethodologies%20for%20simulating%20human%20reading%20behavior%20with%20LLMs.%20Our%20results%20show%0Athat%20prompts%20reflecting%20a%20particular%20linguistic%20hypothesis%20improve%20psychometric%0Apredictive%20power%2C%20but%20are%20still%20inferior%20to%20small%20base%20models.%20These%20findings%0Ahighlight%20that%20recent%20advancements%20in%20LLMs%2C%20i.e.%2C%20instruction%20tuning%20and%0Aprompting%2C%20do%20not%20offer%20better%20estimates%20than%20direct%20probability%20measurements%0Afrom%20base%20LLMs%20in%20cognitive%20modeling.%20In%20other%20words%2C%20pure%20next-word%0Aprobability%20remains%20a%20strong%20predictor%20for%20human%20reading%20behavior%2C%20even%20in%20the%0Aage%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07484v3&entry.124074799=Read"},
{"title": "PerkwE_COQA: Enhanced Persian Conversational Question Answering by\n  combining contextual keyword extraction with Large Language Models", "author": "Pardis Moradbeiki and Nasser Ghadiri", "abstract": "  Smart cities need the involvement of their residents to enhance quality of\nlife. Conversational query-answering is an emerging approach for user\nengagement. There is an increasing demand of an advanced conversational\nquestion-answering that goes beyond classic systems. Existing approaches have\nshown that LLMs offer promising capabilities for CQA, but may struggle to\ncapture the nuances of conversational contexts. The new approach involves\nunderstanding the content and engaging in a multi-step conversation with the\nuser to fulfill their needs. This paper presents a novel method to elevate the\nperformance of Persian Conversational question-answering (CQA) systems. It\ncombines the strengths of Large Language Models (LLMs) with contextual keyword\nextraction. Our method extracts keywords specific to the conversational flow,\nproviding the LLM with additional context to understand the user's intent and\ngenerate more relevant and coherent responses. We evaluated the effectiveness\nof this combined approach through various metrics, demonstrating significant\nimprovements in CQA performance compared to an LLM-only baseline. The proposed\nmethod effectively handles implicit questions, delivers contextually relevant\nanswers, and tackles complex questions that rely heavily on conversational\ncontext. The findings indicate that our method outperformed the evaluation\nbenchmarks up to 8% higher than existing methods and the LLM-only baseline.\n", "link": "http://arxiv.org/abs/2404.05406v2", "date": "2024-04-15", "relevancy": 1.3451, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4542}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4515}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4448}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PerkwE_COQA%3A%20Enhanced%20Persian%20Conversational%20Question%20Answering%20by%0A%20%20combining%20contextual%20keyword%20extraction%20with%20Large%20Language%20Models&body=Title%3A%20PerkwE_COQA%3A%20Enhanced%20Persian%20Conversational%20Question%20Answering%20by%0A%20%20combining%20contextual%20keyword%20extraction%20with%20Large%20Language%20Models%0AAuthor%3A%20Pardis%20Moradbeiki%20and%20Nasser%20Ghadiri%0AAbstract%3A%20%20%20Smart%20cities%20need%20the%20involvement%20of%20their%20residents%20to%20enhance%20quality%20of%0Alife.%20Conversational%20query-answering%20is%20an%20emerging%20approach%20for%20user%0Aengagement.%20There%20is%20an%20increasing%20demand%20of%20an%20advanced%20conversational%0Aquestion-answering%20that%20goes%20beyond%20classic%20systems.%20Existing%20approaches%20have%0Ashown%20that%20LLMs%20offer%20promising%20capabilities%20for%20CQA%2C%20but%20may%20struggle%20to%0Acapture%20the%20nuances%20of%20conversational%20contexts.%20The%20new%20approach%20involves%0Aunderstanding%20the%20content%20and%20engaging%20in%20a%20multi-step%20conversation%20with%20the%0Auser%20to%20fulfill%20their%20needs.%20This%20paper%20presents%20a%20novel%20method%20to%20elevate%20the%0Aperformance%20of%20Persian%20Conversational%20question-answering%20%28CQA%29%20systems.%20It%0Acombines%20the%20strengths%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20contextual%20keyword%0Aextraction.%20Our%20method%20extracts%20keywords%20specific%20to%20the%20conversational%20flow%2C%0Aproviding%20the%20LLM%20with%20additional%20context%20to%20understand%20the%20user%27s%20intent%20and%0Agenerate%20more%20relevant%20and%20coherent%20responses.%20We%20evaluated%20the%20effectiveness%0Aof%20this%20combined%20approach%20through%20various%20metrics%2C%20demonstrating%20significant%0Aimprovements%20in%20CQA%20performance%20compared%20to%20an%20LLM-only%20baseline.%20The%20proposed%0Amethod%20effectively%20handles%20implicit%20questions%2C%20delivers%20contextually%20relevant%0Aanswers%2C%20and%20tackles%20complex%20questions%20that%20rely%20heavily%20on%20conversational%0Acontext.%20The%20findings%20indicate%20that%20our%20method%20outperformed%20the%20evaluation%0Abenchmarks%20up%20to%208%25%20higher%20than%20existing%20methods%20and%20the%20LLM-only%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05406v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerkwE_COQA%3A%20Enhanced%20Persian%20Conversational%20Question%20Answering%20by%0A%20%20combining%20contextual%20keyword%20extraction%20with%20Large%20Language%20Models&entry.906535625=Pardis%20Moradbeiki%20and%20Nasser%20Ghadiri&entry.1292438233=%20%20Smart%20cities%20need%20the%20involvement%20of%20their%20residents%20to%20enhance%20quality%20of%0Alife.%20Conversational%20query-answering%20is%20an%20emerging%20approach%20for%20user%0Aengagement.%20There%20is%20an%20increasing%20demand%20of%20an%20advanced%20conversational%0Aquestion-answering%20that%20goes%20beyond%20classic%20systems.%20Existing%20approaches%20have%0Ashown%20that%20LLMs%20offer%20promising%20capabilities%20for%20CQA%2C%20but%20may%20struggle%20to%0Acapture%20the%20nuances%20of%20conversational%20contexts.%20The%20new%20approach%20involves%0Aunderstanding%20the%20content%20and%20engaging%20in%20a%20multi-step%20conversation%20with%20the%0Auser%20to%20fulfill%20their%20needs.%20This%20paper%20presents%20a%20novel%20method%20to%20elevate%20the%0Aperformance%20of%20Persian%20Conversational%20question-answering%20%28CQA%29%20systems.%20It%0Acombines%20the%20strengths%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20contextual%20keyword%0Aextraction.%20Our%20method%20extracts%20keywords%20specific%20to%20the%20conversational%20flow%2C%0Aproviding%20the%20LLM%20with%20additional%20context%20to%20understand%20the%20user%27s%20intent%20and%0Agenerate%20more%20relevant%20and%20coherent%20responses.%20We%20evaluated%20the%20effectiveness%0Aof%20this%20combined%20approach%20through%20various%20metrics%2C%20demonstrating%20significant%0Aimprovements%20in%20CQA%20performance%20compared%20to%20an%20LLM-only%20baseline.%20The%20proposed%0Amethod%20effectively%20handles%20implicit%20questions%2C%20delivers%20contextually%20relevant%0Aanswers%2C%20and%20tackles%20complex%20questions%20that%20rely%20heavily%20on%20conversational%0Acontext.%20The%20findings%20indicate%20that%20our%20method%20outperformed%20the%20evaluation%0Abenchmarks%20up%20to%208%25%20higher%20than%20existing%20methods%20and%20the%20LLM-only%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05406v2&entry.124074799=Read"},
{"title": "Stiffness-Tuneable Limb Segment with Flexible Spine for Malleable Robots", "author": "Angus B. Clark and Nicolas Rojas", "abstract": "  Robotic arms built from stiffness-adjustable, continuously bending segments\nserially connected with revolute joints have the ability to change their\nmechanical architecture and workspace, thus allowing high flexibility and\nadaptation to different tasks with less than six degrees of freedom, a concept\nthat we call malleable robots. Known stiffening mechanisms may be used to\nimplement suitable links for these novel robotic manipulators; however, these\nsolutions usually show a reduced performance when bending due to structural\ndeformation. By including an inner support structure this deformation can be\nminimised, resulting in an increased stiffening performance. This paper\npresents a new multi-material spine-inspired flexible structure for providing\nsupport in stiffness-controllable layer-jamming-based robotic links of large\ndiameter. The proposed spine mechanism is highly movable with type and range of\nmotions that match those of a robotic link using solely layer jamming, whilst\nmaintaining a hollow and light structure. The mechanics and design of the\nflexible spine are explored, and a prototype of a link utilising it is\ndeveloped and compared with limb segments based on granular jamming and layer\njamming without support structure. Results of experiments verify the advantages\nof the proposed design, demonstrating that it maintains a constant central\ndiameter across bending angles and presents an improvement of more than 203% of\nresisting force at 180 degrees.\n", "link": "http://arxiv.org/abs/2404.09653v1", "date": "2024-04-15", "relevancy": 1.3258, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4834}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4527}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.421}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Stiffness-Tuneable%20Limb%20Segment%20with%20Flexible%20Spine%20for%20Malleable%20Robots&body=Title%3A%20Stiffness-Tuneable%20Limb%20Segment%20with%20Flexible%20Spine%20for%20Malleable%20Robots%0AAuthor%3A%20Angus%20B.%20Clark%20and%20Nicolas%20Rojas%0AAbstract%3A%20%20%20Robotic%20arms%20built%20from%20stiffness-adjustable%2C%20continuously%20bending%20segments%0Aserially%20connected%20with%20revolute%20joints%20have%20the%20ability%20to%20change%20their%0Amechanical%20architecture%20and%20workspace%2C%20thus%20allowing%20high%20flexibility%20and%0Aadaptation%20to%20different%20tasks%20with%20less%20than%20six%20degrees%20of%20freedom%2C%20a%20concept%0Athat%20we%20call%20malleable%20robots.%20Known%20stiffening%20mechanisms%20may%20be%20used%20to%0Aimplement%20suitable%20links%20for%20these%20novel%20robotic%20manipulators%3B%20however%2C%20these%0Asolutions%20usually%20show%20a%20reduced%20performance%20when%20bending%20due%20to%20structural%0Adeformation.%20By%20including%20an%20inner%20support%20structure%20this%20deformation%20can%20be%0Aminimised%2C%20resulting%20in%20an%20increased%20stiffening%20performance.%20This%20paper%0Apresents%20a%20new%20multi-material%20spine-inspired%20flexible%20structure%20for%20providing%0Asupport%20in%20stiffness-controllable%20layer-jamming-based%20robotic%20links%20of%20large%0Adiameter.%20The%20proposed%20spine%20mechanism%20is%20highly%20movable%20with%20type%20and%20range%20of%0Amotions%20that%20match%20those%20of%20a%20robotic%20link%20using%20solely%20layer%20jamming%2C%20whilst%0Amaintaining%20a%20hollow%20and%20light%20structure.%20The%20mechanics%20and%20design%20of%20the%0Aflexible%20spine%20are%20explored%2C%20and%20a%20prototype%20of%20a%20link%20utilising%20it%20is%0Adeveloped%20and%20compared%20with%20limb%20segments%20based%20on%20granular%20jamming%20and%20layer%0Ajamming%20without%20support%20structure.%20Results%20of%20experiments%20verify%20the%20advantages%0Aof%20the%20proposed%20design%2C%20demonstrating%20that%20it%20maintains%20a%20constant%20central%0Adiameter%20across%20bending%20angles%20and%20presents%20an%20improvement%20of%20more%20than%20203%25%20of%0Aresisting%20force%20at%20180%20degrees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09653v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stiffness-Tuneable%20Limb%20Segment%20with%20Flexible%20Spine%20for%20Malleable%20Robots&entry.906535625=Angus%20B.%20Clark%20and%20Nicolas%20Rojas&entry.1292438233=%20%20Robotic%20arms%20built%20from%20stiffness-adjustable%2C%20continuously%20bending%20segments%0Aserially%20connected%20with%20revolute%20joints%20have%20the%20ability%20to%20change%20their%0Amechanical%20architecture%20and%20workspace%2C%20thus%20allowing%20high%20flexibility%20and%0Aadaptation%20to%20different%20tasks%20with%20less%20than%20six%20degrees%20of%20freedom%2C%20a%20concept%0Athat%20we%20call%20malleable%20robots.%20Known%20stiffening%20mechanisms%20may%20be%20used%20to%0Aimplement%20suitable%20links%20for%20these%20novel%20robotic%20manipulators%3B%20however%2C%20these%0Asolutions%20usually%20show%20a%20reduced%20performance%20when%20bending%20due%20to%20structural%0Adeformation.%20By%20including%20an%20inner%20support%20structure%20this%20deformation%20can%20be%0Aminimised%2C%20resulting%20in%20an%20increased%20stiffening%20performance.%20This%20paper%0Apresents%20a%20new%20multi-material%20spine-inspired%20flexible%20structure%20for%20providing%0Asupport%20in%20stiffness-controllable%20layer-jamming-based%20robotic%20links%20of%20large%0Adiameter.%20The%20proposed%20spine%20mechanism%20is%20highly%20movable%20with%20type%20and%20range%20of%0Amotions%20that%20match%20those%20of%20a%20robotic%20link%20using%20solely%20layer%20jamming%2C%20whilst%0Amaintaining%20a%20hollow%20and%20light%20structure.%20The%20mechanics%20and%20design%20of%20the%0Aflexible%20spine%20are%20explored%2C%20and%20a%20prototype%20of%20a%20link%20utilising%20it%20is%0Adeveloped%20and%20compared%20with%20limb%20segments%20based%20on%20granular%20jamming%20and%20layer%0Ajamming%20without%20support%20structure.%20Results%20of%20experiments%20verify%20the%20advantages%0Aof%20the%20proposed%20design%2C%20demonstrating%20that%20it%20maintains%20a%20constant%20central%0Adiameter%20across%20bending%20angles%20and%20presents%20an%20improvement%20of%20more%20than%20203%25%20of%0Aresisting%20force%20at%20180%20degrees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09653v1&entry.124074799=Read"},
{"title": "On the $O(\\frac{\\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its\n  Momentum Extension Measured by $\\ell_1$ Norm", "author": "Huan Li and Zhouchen Lin", "abstract": "  Although adaptive gradient methods have been extensively used in deep\nlearning, their convergence rates proved in the literature are all slower than\nthat of SGD, particularly with respect to their dependence on the dimension.\nThis paper considers the classical RMSProp and its momentum extension and\nestablishes the convergence rate of $\\frac{1}{T}\\sum_{k=1}^T E\\left[\\|\\nabla\nf(x^k)\\|_1\\right]\\leq O(\\frac{\\sqrt{d}C}{T^{1/4}})$ measured by $\\ell_1$ norm\nwithout the bounded gradient assumption, where $d$ is the dimension of the\noptimization variable, $T$ is the iteration number, and $C$ is a constant\nidentical to that appeared in the optimal convergence rate of SGD. Our\nconvergence rate matches the lower bound with respect to all the coefficients\nexcept the dimension $d$. Since $\\|x\\|_2\\ll\\|x\\|_1\\leq\\sqrt{d}\\|x\\|_2$ for\nproblems with extremely large $d$, our convergence rate can be considered to be\nanalogous to the $\\frac{1}{T}\\sum_{k=1}^T E\\left[\\|\\nabla f(x^k)\\|_2\\right]\\leq\nO(\\frac{C}{T^{1/4}})$ rate of SGD in the ideal case of $\\|\\nabla\nf(x)\\|_1=\\varTheta(\\sqrt{d}\\|\\nabla f(x)\\|_2)$.\n", "link": "http://arxiv.org/abs/2402.00389v3", "date": "2024-04-15", "relevancy": 1.3249, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4575}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4388}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4328}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20%24O%28%5Cfrac%7B%5Csqrt%7Bd%7D%7D%7BT%5E%7B1/4%7D%7D%29%24%20Convergence%20Rate%20of%20RMSProp%20and%20Its%0A%20%20Momentum%20Extension%20Measured%20by%20%24%5Cell_1%24%20Norm&body=Title%3A%20On%20the%20%24O%28%5Cfrac%7B%5Csqrt%7Bd%7D%7D%7BT%5E%7B1/4%7D%7D%29%24%20Convergence%20Rate%20of%20RMSProp%20and%20Its%0A%20%20Momentum%20Extension%20Measured%20by%20%24%5Cell_1%24%20Norm%0AAuthor%3A%20Huan%20Li%20and%20Zhouchen%20Lin%0AAbstract%3A%20%20%20Although%20adaptive%20gradient%20methods%20have%20been%20extensively%20used%20in%20deep%0Alearning%2C%20their%20convergence%20rates%20proved%20in%20the%20literature%20are%20all%20slower%20than%0Athat%20of%20SGD%2C%20particularly%20with%20respect%20to%20their%20dependence%20on%20the%20dimension.%0AThis%20paper%20considers%20the%20classical%20RMSProp%20and%20its%20momentum%20extension%20and%0Aestablishes%20the%20convergence%20rate%20of%20%24%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bk%3D1%7D%5ET%20E%5Cleft%5B%5C%7C%5Cnabla%0Af%28x%5Ek%29%5C%7C_1%5Cright%5D%5Cleq%20O%28%5Cfrac%7B%5Csqrt%7Bd%7DC%7D%7BT%5E%7B1/4%7D%7D%29%24%20measured%20by%20%24%5Cell_1%24%20norm%0Awithout%20the%20bounded%20gradient%20assumption%2C%20where%20%24d%24%20is%20the%20dimension%20of%20the%0Aoptimization%20variable%2C%20%24T%24%20is%20the%20iteration%20number%2C%20and%20%24C%24%20is%20a%20constant%0Aidentical%20to%20that%20appeared%20in%20the%20optimal%20convergence%20rate%20of%20SGD.%20Our%0Aconvergence%20rate%20matches%20the%20lower%20bound%20with%20respect%20to%20all%20the%20coefficients%0Aexcept%20the%20dimension%20%24d%24.%20Since%20%24%5C%7Cx%5C%7C_2%5Cll%5C%7Cx%5C%7C_1%5Cleq%5Csqrt%7Bd%7D%5C%7Cx%5C%7C_2%24%20for%0Aproblems%20with%20extremely%20large%20%24d%24%2C%20our%20convergence%20rate%20can%20be%20considered%20to%20be%0Aanalogous%20to%20the%20%24%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bk%3D1%7D%5ET%20E%5Cleft%5B%5C%7C%5Cnabla%20f%28x%5Ek%29%5C%7C_2%5Cright%5D%5Cleq%0AO%28%5Cfrac%7BC%7D%7BT%5E%7B1/4%7D%7D%29%24%20rate%20of%20SGD%20in%20the%20ideal%20case%20of%20%24%5C%7C%5Cnabla%0Af%28x%29%5C%7C_1%3D%5CvarTheta%28%5Csqrt%7Bd%7D%5C%7C%5Cnabla%20f%28x%29%5C%7C_2%29%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00389v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20%24O%28%5Cfrac%7B%5Csqrt%7Bd%7D%7D%7BT%5E%7B1/4%7D%7D%29%24%20Convergence%20Rate%20of%20RMSProp%20and%20Its%0A%20%20Momentum%20Extension%20Measured%20by%20%24%5Cell_1%24%20Norm&entry.906535625=Huan%20Li%20and%20Zhouchen%20Lin&entry.1292438233=%20%20Although%20adaptive%20gradient%20methods%20have%20been%20extensively%20used%20in%20deep%0Alearning%2C%20their%20convergence%20rates%20proved%20in%20the%20literature%20are%20all%20slower%20than%0Athat%20of%20SGD%2C%20particularly%20with%20respect%20to%20their%20dependence%20on%20the%20dimension.%0AThis%20paper%20considers%20the%20classical%20RMSProp%20and%20its%20momentum%20extension%20and%0Aestablishes%20the%20convergence%20rate%20of%20%24%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bk%3D1%7D%5ET%20E%5Cleft%5B%5C%7C%5Cnabla%0Af%28x%5Ek%29%5C%7C_1%5Cright%5D%5Cleq%20O%28%5Cfrac%7B%5Csqrt%7Bd%7DC%7D%7BT%5E%7B1/4%7D%7D%29%24%20measured%20by%20%24%5Cell_1%24%20norm%0Awithout%20the%20bounded%20gradient%20assumption%2C%20where%20%24d%24%20is%20the%20dimension%20of%20the%0Aoptimization%20variable%2C%20%24T%24%20is%20the%20iteration%20number%2C%20and%20%24C%24%20is%20a%20constant%0Aidentical%20to%20that%20appeared%20in%20the%20optimal%20convergence%20rate%20of%20SGD.%20Our%0Aconvergence%20rate%20matches%20the%20lower%20bound%20with%20respect%20to%20all%20the%20coefficients%0Aexcept%20the%20dimension%20%24d%24.%20Since%20%24%5C%7Cx%5C%7C_2%5Cll%5C%7Cx%5C%7C_1%5Cleq%5Csqrt%7Bd%7D%5C%7Cx%5C%7C_2%24%20for%0Aproblems%20with%20extremely%20large%20%24d%24%2C%20our%20convergence%20rate%20can%20be%20considered%20to%20be%0Aanalogous%20to%20the%20%24%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bk%3D1%7D%5ET%20E%5Cleft%5B%5C%7C%5Cnabla%20f%28x%5Ek%29%5C%7C_2%5Cright%5D%5Cleq%0AO%28%5Cfrac%7BC%7D%7BT%5E%7B1/4%7D%7D%29%24%20rate%20of%20SGD%20in%20the%20ideal%20case%20of%20%24%5C%7C%5Cnabla%0Af%28x%29%5C%7C_1%3D%5CvarTheta%28%5Csqrt%7Bd%7D%5C%7C%5Cnabla%20f%28x%29%5C%7C_2%29%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00389v3&entry.124074799=Read"},
{"title": "Privacy-Preserving Intrusion Detection using Convolutional Neural\n  Networks", "author": "Martin Kodys and Zhongmin Dai and Vrizlynn L. L. Thing", "abstract": "  Privacy-preserving analytics is designed to protect valuable assets. A common\nservice provision involves the input data from the client and the model on the\nanalyst's side. The importance of the privacy preservation is fuelled by legal\nobligations and intellectual property concerns. We explore the use case of a\nmodel owner providing an analytic service on customer's private data. No\ninformation about the data shall be revealed to the analyst and no information\nabout the model shall be leaked to the customer. Current methods involve costs:\naccuracy deterioration and computational complexity. The complexity, in turn,\nresults in a longer processing time, increased requirement on computing\nresources, and involves data communication between the client and the server.\nIn order to deploy such service architecture, we need to evaluate the optimal\nsetting that fits the constraints. And that is what this paper addresses. In\nthis work, we enhance an attack detection system based on Convolutional Neural\nNetworks with privacy-preserving technology based on PriMIA framework that is\ninitially designed for medical data.\n", "link": "http://arxiv.org/abs/2404.09625v1", "date": "2024-04-15", "relevancy": 1.3241, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4497}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4335}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4285}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Privacy-Preserving%20Intrusion%20Detection%20using%20Convolutional%20Neural%0A%20%20Networks&body=Title%3A%20Privacy-Preserving%20Intrusion%20Detection%20using%20Convolutional%20Neural%0A%20%20Networks%0AAuthor%3A%20Martin%20Kodys%20and%20Zhongmin%20Dai%20and%20Vrizlynn%20L.%20L.%20Thing%0AAbstract%3A%20%20%20Privacy-preserving%20analytics%20is%20designed%20to%20protect%20valuable%20assets.%20A%20common%0Aservice%20provision%20involves%20the%20input%20data%20from%20the%20client%20and%20the%20model%20on%20the%0Aanalyst%27s%20side.%20The%20importance%20of%20the%20privacy%20preservation%20is%20fuelled%20by%20legal%0Aobligations%20and%20intellectual%20property%20concerns.%20We%20explore%20the%20use%20case%20of%20a%0Amodel%20owner%20providing%20an%20analytic%20service%20on%20customer%27s%20private%20data.%20No%0Ainformation%20about%20the%20data%20shall%20be%20revealed%20to%20the%20analyst%20and%20no%20information%0Aabout%20the%20model%20shall%20be%20leaked%20to%20the%20customer.%20Current%20methods%20involve%20costs%3A%0Aaccuracy%20deterioration%20and%20computational%20complexity.%20The%20complexity%2C%20in%20turn%2C%0Aresults%20in%20a%20longer%20processing%20time%2C%20increased%20requirement%20on%20computing%0Aresources%2C%20and%20involves%20data%20communication%20between%20the%20client%20and%20the%20server.%0AIn%20order%20to%20deploy%20such%20service%20architecture%2C%20we%20need%20to%20evaluate%20the%20optimal%0Asetting%20that%20fits%20the%20constraints.%20And%20that%20is%20what%20this%20paper%20addresses.%20In%0Athis%20work%2C%20we%20enhance%20an%20attack%20detection%20system%20based%20on%20Convolutional%20Neural%0ANetworks%20with%20privacy-preserving%20technology%20based%20on%20PriMIA%20framework%20that%20is%0Ainitially%20designed%20for%20medical%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09625v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy-Preserving%20Intrusion%20Detection%20using%20Convolutional%20Neural%0A%20%20Networks&entry.906535625=Martin%20Kodys%20and%20Zhongmin%20Dai%20and%20Vrizlynn%20L.%20L.%20Thing&entry.1292438233=%20%20Privacy-preserving%20analytics%20is%20designed%20to%20protect%20valuable%20assets.%20A%20common%0Aservice%20provision%20involves%20the%20input%20data%20from%20the%20client%20and%20the%20model%20on%20the%0Aanalyst%27s%20side.%20The%20importance%20of%20the%20privacy%20preservation%20is%20fuelled%20by%20legal%0Aobligations%20and%20intellectual%20property%20concerns.%20We%20explore%20the%20use%20case%20of%20a%0Amodel%20owner%20providing%20an%20analytic%20service%20on%20customer%27s%20private%20data.%20No%0Ainformation%20about%20the%20data%20shall%20be%20revealed%20to%20the%20analyst%20and%20no%20information%0Aabout%20the%20model%20shall%20be%20leaked%20to%20the%20customer.%20Current%20methods%20involve%20costs%3A%0Aaccuracy%20deterioration%20and%20computational%20complexity.%20The%20complexity%2C%20in%20turn%2C%0Aresults%20in%20a%20longer%20processing%20time%2C%20increased%20requirement%20on%20computing%0Aresources%2C%20and%20involves%20data%20communication%20between%20the%20client%20and%20the%20server.%0AIn%20order%20to%20deploy%20such%20service%20architecture%2C%20we%20need%20to%20evaluate%20the%20optimal%0Asetting%20that%20fits%20the%20constraints.%20And%20that%20is%20what%20this%20paper%20addresses.%20In%0Athis%20work%2C%20we%20enhance%20an%20attack%20detection%20system%20based%20on%20Convolutional%20Neural%0ANetworks%20with%20privacy-preserving%20technology%20based%20on%20PriMIA%20framework%20that%20is%0Ainitially%20designed%20for%20medical%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09625v1&entry.124074799=Read"},
{"title": "Exploring Non-Regular Extensions of Propositional Dynamic Logic with\n  Description-Logics Features", "author": "Bartosz Bednarczyk", "abstract": "  We investigate the impact of non-regular path expressions on the decidability\nof satisfiability checking and querying in description logics extending ALC.\nOur primary objects of interest are ALCreg and ALCvpl, the extensions of with\npath expressions employing, respectively, regular and visibly-pushdown\nlanguages. The first one, ALCreg, is a notational variant of the well-known\nPropositional Dynamic Logic of Fischer and Ladner. The second one, ALCvpl, was\nintroduced and investigated by Loding and Serre in 2007. The logic ALCvpl\ngeneralises many known decidable non-regular extensions of ALCreg.\n  We provide a series of undecidability results. First, we show that\ndecidability of the concept satisfiability problem for ALCvpl is lost upon\nadding the seemingly innocent Self operator. Second, we establish\nundecidability for the concept satisfiability problem for ALCvpl extended with\nnominals. Interestingly, our undecidability proof relies only on one single\nnon-regular (visibly-pushdown) language, namely on r#s# := { r^n s^n | n in N }\nfor fixed role names r and s. Finally, in contrast to the classical database\nsetting, we establish undecidability of query entailment for queries involving\nnon-regular atoms from r#s#, already in the case of ALC-TBoxes.\n", "link": "http://arxiv.org/abs/2307.09913v4", "date": "2024-04-15", "relevancy": 1.2972, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4578}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4411}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4188}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20Non-Regular%20Extensions%20of%20Propositional%20Dynamic%20Logic%20with%0A%20%20Description-Logics%20Features&body=Title%3A%20Exploring%20Non-Regular%20Extensions%20of%20Propositional%20Dynamic%20Logic%20with%0A%20%20Description-Logics%20Features%0AAuthor%3A%20Bartosz%20Bednarczyk%0AAbstract%3A%20%20%20We%20investigate%20the%20impact%20of%20non-regular%20path%20expressions%20on%20the%20decidability%0Aof%20satisfiability%20checking%20and%20querying%20in%20description%20logics%20extending%20ALC.%0AOur%20primary%20objects%20of%20interest%20are%20ALCreg%20and%20ALCvpl%2C%20the%20extensions%20of%20with%0Apath%20expressions%20employing%2C%20respectively%2C%20regular%20and%20visibly-pushdown%0Alanguages.%20The%20first%20one%2C%20ALCreg%2C%20is%20a%20notational%20variant%20of%20the%20well-known%0APropositional%20Dynamic%20Logic%20of%20Fischer%20and%20Ladner.%20The%20second%20one%2C%20ALCvpl%2C%20was%0Aintroduced%20and%20investigated%20by%20Loding%20and%20Serre%20in%202007.%20The%20logic%20ALCvpl%0Ageneralises%20many%20known%20decidable%20non-regular%20extensions%20of%20ALCreg.%0A%20%20We%20provide%20a%20series%20of%20undecidability%20results.%20First%2C%20we%20show%20that%0Adecidability%20of%20the%20concept%20satisfiability%20problem%20for%20ALCvpl%20is%20lost%20upon%0Aadding%20the%20seemingly%20innocent%20Self%20operator.%20Second%2C%20we%20establish%0Aundecidability%20for%20the%20concept%20satisfiability%20problem%20for%20ALCvpl%20extended%20with%0Anominals.%20Interestingly%2C%20our%20undecidability%20proof%20relies%20only%20on%20one%20single%0Anon-regular%20%28visibly-pushdown%29%20language%2C%20namely%20on%20r%23s%23%20%3A%3D%20%7B%20r%5En%20s%5En%20%7C%20n%20in%20N%20%7D%0Afor%20fixed%20role%20names%20r%20and%20s.%20Finally%2C%20in%20contrast%20to%20the%20classical%20database%0Asetting%2C%20we%20establish%20undecidability%20of%20query%20entailment%20for%20queries%20involving%0Anon-regular%20atoms%20from%20r%23s%23%2C%20already%20in%20the%20case%20of%20ALC-TBoxes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09913v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Non-Regular%20Extensions%20of%20Propositional%20Dynamic%20Logic%20with%0A%20%20Description-Logics%20Features&entry.906535625=Bartosz%20Bednarczyk&entry.1292438233=%20%20We%20investigate%20the%20impact%20of%20non-regular%20path%20expressions%20on%20the%20decidability%0Aof%20satisfiability%20checking%20and%20querying%20in%20description%20logics%20extending%20ALC.%0AOur%20primary%20objects%20of%20interest%20are%20ALCreg%20and%20ALCvpl%2C%20the%20extensions%20of%20with%0Apath%20expressions%20employing%2C%20respectively%2C%20regular%20and%20visibly-pushdown%0Alanguages.%20The%20first%20one%2C%20ALCreg%2C%20is%20a%20notational%20variant%20of%20the%20well-known%0APropositional%20Dynamic%20Logic%20of%20Fischer%20and%20Ladner.%20The%20second%20one%2C%20ALCvpl%2C%20was%0Aintroduced%20and%20investigated%20by%20Loding%20and%20Serre%20in%202007.%20The%20logic%20ALCvpl%0Ageneralises%20many%20known%20decidable%20non-regular%20extensions%20of%20ALCreg.%0A%20%20We%20provide%20a%20series%20of%20undecidability%20results.%20First%2C%20we%20show%20that%0Adecidability%20of%20the%20concept%20satisfiability%20problem%20for%20ALCvpl%20is%20lost%20upon%0Aadding%20the%20seemingly%20innocent%20Self%20operator.%20Second%2C%20we%20establish%0Aundecidability%20for%20the%20concept%20satisfiability%20problem%20for%20ALCvpl%20extended%20with%0Anominals.%20Interestingly%2C%20our%20undecidability%20proof%20relies%20only%20on%20one%20single%0Anon-regular%20%28visibly-pushdown%29%20language%2C%20namely%20on%20r%23s%23%20%3A%3D%20%7B%20r%5En%20s%5En%20%7C%20n%20in%20N%20%7D%0Afor%20fixed%20role%20names%20r%20and%20s.%20Finally%2C%20in%20contrast%20to%20the%20classical%20database%0Asetting%2C%20we%20establish%20undecidability%20of%20query%20entailment%20for%20queries%20involving%0Anon-regular%20atoms%20from%20r%23s%23%2C%20already%20in%20the%20case%20of%20ALC-TBoxes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09913v4&entry.124074799=Read"},
{"title": "Photo-Realistic Image Restoration in the Wild with Controlled\n  Vision-Language Models", "author": "Ziwei Luo and Fredrik K. Gustafsson and Zheng Zhao and Jens Sj\u00f6lund and Thomas B. Sch\u00f6n", "abstract": "  Though diffusion models have been successfully applied to various image\nrestoration (IR) tasks, their performance is sensitive to the choice of\ntraining datasets. Typically, diffusion models trained in specific datasets\nfail to recover images that have out-of-distribution degradations. To address\nthis problem, this work leverages a capable vision-language model and a\nsynthetic degradation pipeline to learn image restoration in the wild (wild\nIR). More specifically, all low-quality images are simulated with a synthetic\ndegradation pipeline that contains multiple common degradations such as blur,\nresize, noise, and JPEG compression. Then we introduce robust training for a\ndegradation-aware CLIP model to extract enriched image content features to\nassist high-quality image restoration. Our base diffusion model is the image\nrestoration SDE (IR-SDE). Built upon it, we further present a posterior\nsampling strategy for fast noise-free image generation. We evaluate our model\non both synthetic and real-world degradation datasets. Moreover, experiments on\nthe unified image restoration task illustrate that the proposed posterior\nsampling improves image generation quality for various degradations.\n", "link": "http://arxiv.org/abs/2404.09732v1", "date": "2024-04-15", "relevancy": 1.2356, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6382}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6093}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6059}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Photo-Realistic%20Image%20Restoration%20in%20the%20Wild%20with%20Controlled%0A%20%20Vision-Language%20Models&body=Title%3A%20Photo-Realistic%20Image%20Restoration%20in%20the%20Wild%20with%20Controlled%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Ziwei%20Luo%20and%20Fredrik%20K.%20Gustafsson%20and%20Zheng%20Zhao%20and%20Jens%20Sj%C3%B6lund%20and%20Thomas%20B.%20Sch%C3%B6n%0AAbstract%3A%20%20%20Though%20diffusion%20models%20have%20been%20successfully%20applied%20to%20various%20image%0Arestoration%20%28IR%29%20tasks%2C%20their%20performance%20is%20sensitive%20to%20the%20choice%20of%0Atraining%20datasets.%20Typically%2C%20diffusion%20models%20trained%20in%20specific%20datasets%0Afail%20to%20recover%20images%20that%20have%20out-of-distribution%20degradations.%20To%20address%0Athis%20problem%2C%20this%20work%20leverages%20a%20capable%20vision-language%20model%20and%20a%0Asynthetic%20degradation%20pipeline%20to%20learn%20image%20restoration%20in%20the%20wild%20%28wild%0AIR%29.%20More%20specifically%2C%20all%20low-quality%20images%20are%20simulated%20with%20a%20synthetic%0Adegradation%20pipeline%20that%20contains%20multiple%20common%20degradations%20such%20as%20blur%2C%0Aresize%2C%20noise%2C%20and%20JPEG%20compression.%20Then%20we%20introduce%20robust%20training%20for%20a%0Adegradation-aware%20CLIP%20model%20to%20extract%20enriched%20image%20content%20features%20to%0Aassist%20high-quality%20image%20restoration.%20Our%20base%20diffusion%20model%20is%20the%20image%0Arestoration%20SDE%20%28IR-SDE%29.%20Built%20upon%20it%2C%20we%20further%20present%20a%20posterior%0Asampling%20strategy%20for%20fast%20noise-free%20image%20generation.%20We%20evaluate%20our%20model%0Aon%20both%20synthetic%20and%20real-world%20degradation%20datasets.%20Moreover%2C%20experiments%20on%0Athe%20unified%20image%20restoration%20task%20illustrate%20that%20the%20proposed%20posterior%0Asampling%20improves%20image%20generation%20quality%20for%20various%20degradations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09732v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Photo-Realistic%20Image%20Restoration%20in%20the%20Wild%20with%20Controlled%0A%20%20Vision-Language%20Models&entry.906535625=Ziwei%20Luo%20and%20Fredrik%20K.%20Gustafsson%20and%20Zheng%20Zhao%20and%20Jens%20Sj%C3%B6lund%20and%20Thomas%20B.%20Sch%C3%B6n&entry.1292438233=%20%20Though%20diffusion%20models%20have%20been%20successfully%20applied%20to%20various%20image%0Arestoration%20%28IR%29%20tasks%2C%20their%20performance%20is%20sensitive%20to%20the%20choice%20of%0Atraining%20datasets.%20Typically%2C%20diffusion%20models%20trained%20in%20specific%20datasets%0Afail%20to%20recover%20images%20that%20have%20out-of-distribution%20degradations.%20To%20address%0Athis%20problem%2C%20this%20work%20leverages%20a%20capable%20vision-language%20model%20and%20a%0Asynthetic%20degradation%20pipeline%20to%20learn%20image%20restoration%20in%20the%20wild%20%28wild%0AIR%29.%20More%20specifically%2C%20all%20low-quality%20images%20are%20simulated%20with%20a%20synthetic%0Adegradation%20pipeline%20that%20contains%20multiple%20common%20degradations%20such%20as%20blur%2C%0Aresize%2C%20noise%2C%20and%20JPEG%20compression.%20Then%20we%20introduce%20robust%20training%20for%20a%0Adegradation-aware%20CLIP%20model%20to%20extract%20enriched%20image%20content%20features%20to%0Aassist%20high-quality%20image%20restoration.%20Our%20base%20diffusion%20model%20is%20the%20image%0Arestoration%20SDE%20%28IR-SDE%29.%20Built%20upon%20it%2C%20we%20further%20present%20a%20posterior%0Asampling%20strategy%20for%20fast%20noise-free%20image%20generation.%20We%20evaluate%20our%20model%0Aon%20both%20synthetic%20and%20real-world%20degradation%20datasets.%20Moreover%2C%20experiments%20on%0Athe%20unified%20image%20restoration%20task%20illustrate%20that%20the%20proposed%20posterior%0Asampling%20improves%20image%20generation%20quality%20for%20various%20degradations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09732v1&entry.124074799=Read"},
{"title": "Monitoring Second-Order Hyperproperties", "author": "Raven Beutner and Bernd Finkbeiner and Hadar Frenkel and Niklas Metzger", "abstract": "  Hyperproperties express the relationship between multiple executions of a\nsystem. This is needed in many AI-related fields, such as knowledge\nrepresentation and planning, to capture system properties related to knowledge,\ninformation flow, and privacy. In this paper, we study the monitoring of\ncomplex hyperproperties at runtime. Previous work in this area has either\nfocused on the simpler problem of monitoring trace properties (which are sets\nof traces, while hyperproperties are sets of sets of traces) or on monitoring\nfirst-order hyperproperties, which are expressible in temporal logics with\nfirst-order quantification over traces, such as HyperLTL. We present the first\nmonitoring algorithm for the much more expressive class of second-order\nhyperproperties. Second-order hyperproperties include system properties like\ncommon knowledge, which cannot be expressed in first-order logics like\nHyperLTL.\n  We introduce Hyper$^2$LTL$_f$, a temporal logic over finite traces that\nallows for second-order quantification over sets of traces. We study the\nmonitoring problem in two fundamental execution models: (1) the parallel model,\nwhere a fixed number of traces is monitored in parallel, and (2) the sequential\nmodel, where an unbounded number of traces is observed sequentially, one trace\nafter the other. For the parallel model, we show that the monitoring of the\nsecond-order hyperproperties of Hyper$^2$LTL$_f$ can be reduced to monitoring\nfirst-order hyperproperties. For the sequential model, we present a monitoring\nalgorithm that handles second-order quantification efficiently, exploiting\noptimizations based on the monotonicity of subformulas, graph-based storing of\nexecutions, and fixpoint hashing. We present experimental results from a range\nof benchmarks, including examples from common knowledge and planning.\n", "link": "http://arxiv.org/abs/2404.09652v1", "date": "2024-04-15", "relevancy": 1.167, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4549}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3725}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3692}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Monitoring%20Second-Order%20Hyperproperties&body=Title%3A%20Monitoring%20Second-Order%20Hyperproperties%0AAuthor%3A%20Raven%20Beutner%20and%20Bernd%20Finkbeiner%20and%20Hadar%20Frenkel%20and%20Niklas%20Metzger%0AAbstract%3A%20%20%20Hyperproperties%20express%20the%20relationship%20between%20multiple%20executions%20of%20a%0Asystem.%20This%20is%20needed%20in%20many%20AI-related%20fields%2C%20such%20as%20knowledge%0Arepresentation%20and%20planning%2C%20to%20capture%20system%20properties%20related%20to%20knowledge%2C%0Ainformation%20flow%2C%20and%20privacy.%20In%20this%20paper%2C%20we%20study%20the%20monitoring%20of%0Acomplex%20hyperproperties%20at%20runtime.%20Previous%20work%20in%20this%20area%20has%20either%0Afocused%20on%20the%20simpler%20problem%20of%20monitoring%20trace%20properties%20%28which%20are%20sets%0Aof%20traces%2C%20while%20hyperproperties%20are%20sets%20of%20sets%20of%20traces%29%20or%20on%20monitoring%0Afirst-order%20hyperproperties%2C%20which%20are%20expressible%20in%20temporal%20logics%20with%0Afirst-order%20quantification%20over%20traces%2C%20such%20as%20HyperLTL.%20We%20present%20the%20first%0Amonitoring%20algorithm%20for%20the%20much%20more%20expressive%20class%20of%20second-order%0Ahyperproperties.%20Second-order%20hyperproperties%20include%20system%20properties%20like%0Acommon%20knowledge%2C%20which%20cannot%20be%20expressed%20in%20first-order%20logics%20like%0AHyperLTL.%0A%20%20We%20introduce%20Hyper%24%5E2%24LTL%24_f%24%2C%20a%20temporal%20logic%20over%20finite%20traces%20that%0Aallows%20for%20second-order%20quantification%20over%20sets%20of%20traces.%20We%20study%20the%0Amonitoring%20problem%20in%20two%20fundamental%20execution%20models%3A%20%281%29%20the%20parallel%20model%2C%0Awhere%20a%20fixed%20number%20of%20traces%20is%20monitored%20in%20parallel%2C%20and%20%282%29%20the%20sequential%0Amodel%2C%20where%20an%20unbounded%20number%20of%20traces%20is%20observed%20sequentially%2C%20one%20trace%0Aafter%20the%20other.%20For%20the%20parallel%20model%2C%20we%20show%20that%20the%20monitoring%20of%20the%0Asecond-order%20hyperproperties%20of%20Hyper%24%5E2%24LTL%24_f%24%20can%20be%20reduced%20to%20monitoring%0Afirst-order%20hyperproperties.%20For%20the%20sequential%20model%2C%20we%20present%20a%20monitoring%0Aalgorithm%20that%20handles%20second-order%20quantification%20efficiently%2C%20exploiting%0Aoptimizations%20based%20on%20the%20monotonicity%20of%20subformulas%2C%20graph-based%20storing%20of%0Aexecutions%2C%20and%20fixpoint%20hashing.%20We%20present%20experimental%20results%20from%20a%20range%0Aof%20benchmarks%2C%20including%20examples%20from%20common%20knowledge%20and%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09652v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monitoring%20Second-Order%20Hyperproperties&entry.906535625=Raven%20Beutner%20and%20Bernd%20Finkbeiner%20and%20Hadar%20Frenkel%20and%20Niklas%20Metzger&entry.1292438233=%20%20Hyperproperties%20express%20the%20relationship%20between%20multiple%20executions%20of%20a%0Asystem.%20This%20is%20needed%20in%20many%20AI-related%20fields%2C%20such%20as%20knowledge%0Arepresentation%20and%20planning%2C%20to%20capture%20system%20properties%20related%20to%20knowledge%2C%0Ainformation%20flow%2C%20and%20privacy.%20In%20this%20paper%2C%20we%20study%20the%20monitoring%20of%0Acomplex%20hyperproperties%20at%20runtime.%20Previous%20work%20in%20this%20area%20has%20either%0Afocused%20on%20the%20simpler%20problem%20of%20monitoring%20trace%20properties%20%28which%20are%20sets%0Aof%20traces%2C%20while%20hyperproperties%20are%20sets%20of%20sets%20of%20traces%29%20or%20on%20monitoring%0Afirst-order%20hyperproperties%2C%20which%20are%20expressible%20in%20temporal%20logics%20with%0Afirst-order%20quantification%20over%20traces%2C%20such%20as%20HyperLTL.%20We%20present%20the%20first%0Amonitoring%20algorithm%20for%20the%20much%20more%20expressive%20class%20of%20second-order%0Ahyperproperties.%20Second-order%20hyperproperties%20include%20system%20properties%20like%0Acommon%20knowledge%2C%20which%20cannot%20be%20expressed%20in%20first-order%20logics%20like%0AHyperLTL.%0A%20%20We%20introduce%20Hyper%24%5E2%24LTL%24_f%24%2C%20a%20temporal%20logic%20over%20finite%20traces%20that%0Aallows%20for%20second-order%20quantification%20over%20sets%20of%20traces.%20We%20study%20the%0Amonitoring%20problem%20in%20two%20fundamental%20execution%20models%3A%20%281%29%20the%20parallel%20model%2C%0Awhere%20a%20fixed%20number%20of%20traces%20is%20monitored%20in%20parallel%2C%20and%20%282%29%20the%20sequential%0Amodel%2C%20where%20an%20unbounded%20number%20of%20traces%20is%20observed%20sequentially%2C%20one%20trace%0Aafter%20the%20other.%20For%20the%20parallel%20model%2C%20we%20show%20that%20the%20monitoring%20of%20the%0Asecond-order%20hyperproperties%20of%20Hyper%24%5E2%24LTL%24_f%24%20can%20be%20reduced%20to%20monitoring%0Afirst-order%20hyperproperties.%20For%20the%20sequential%20model%2C%20we%20present%20a%20monitoring%0Aalgorithm%20that%20handles%20second-order%20quantification%20efficiently%2C%20exploiting%0Aoptimizations%20based%20on%20the%20monotonicity%20of%20subformulas%2C%20graph-based%20storing%20of%0Aexecutions%2C%20and%20fixpoint%20hashing.%20We%20present%20experimental%20results%20from%20a%20range%0Aof%20benchmarks%2C%20including%20examples%20from%20common%20knowledge%20and%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09652v1&entry.124074799=Read"},
{"title": "Image-based Deep Learning for the time-dependent prediction of fresh\n  concrete properties", "author": "Max Meyer and Amadeus Langer and Max Mehltretter and Dries Beyer and Max Coenen and Tobias Schack and Michael Haist and Christian Heipke", "abstract": "  Increasing the degree of digitisation and automation in the concrete\nproduction process can play a crucial role in reducing the CO$_2$ emissions\nthat are associated with the production of concrete. In this paper, a method is\npresented that makes it possible to predict the properties of fresh concrete\nduring the mixing process based on stereoscopic image sequences of the\nconcretes flow behaviour. A Convolutional Neural Network (CNN) is used for the\nprediction, which receives the images supported by information on the mix\ndesign as input. In addition, the network receives temporal information in the\nform of the time difference between the time at which the images are taken and\nthe time at which the reference values of the concretes are carried out. With\nthis temporal information, the network implicitly learns the time-dependent\nbehaviour of the concretes properties. The network predicts the slump flow\ndiameter, the yield stress and the plastic viscosity. The time-dependent\nprediction potentially opens up the pathway to determine the temporal\ndevelopment of the fresh concrete properties already during mixing. This\nprovides a huge advantage for the concrete industry. As a result,\ncountermeasures can be taken in a timely manner. It is shown that an approach\nbased on depth and optical flow images, supported by information of the mix\ndesign, achieves the best results.\n", "link": "http://arxiv.org/abs/2402.06611v2", "date": "2024-04-15", "relevancy": 1.108, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5891}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5477}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5252}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Image-based%20Deep%20Learning%20for%20the%20time-dependent%20prediction%20of%20fresh%0A%20%20concrete%20properties&body=Title%3A%20Image-based%20Deep%20Learning%20for%20the%20time-dependent%20prediction%20of%20fresh%0A%20%20concrete%20properties%0AAuthor%3A%20Max%20Meyer%20and%20Amadeus%20Langer%20and%20Max%20Mehltretter%20and%20Dries%20Beyer%20and%20Max%20Coenen%20and%20Tobias%20Schack%20and%20Michael%20Haist%20and%20Christian%20Heipke%0AAbstract%3A%20%20%20Increasing%20the%20degree%20of%20digitisation%20and%20automation%20in%20the%20concrete%0Aproduction%20process%20can%20play%20a%20crucial%20role%20in%20reducing%20the%20CO%24_2%24%20emissions%0Athat%20are%20associated%20with%20the%20production%20of%20concrete.%20In%20this%20paper%2C%20a%20method%20is%0Apresented%20that%20makes%20it%20possible%20to%20predict%20the%20properties%20of%20fresh%20concrete%0Aduring%20the%20mixing%20process%20based%20on%20stereoscopic%20image%20sequences%20of%20the%0Aconcretes%20flow%20behaviour.%20A%20Convolutional%20Neural%20Network%20%28CNN%29%20is%20used%20for%20the%0Aprediction%2C%20which%20receives%20the%20images%20supported%20by%20information%20on%20the%20mix%0Adesign%20as%20input.%20In%20addition%2C%20the%20network%20receives%20temporal%20information%20in%20the%0Aform%20of%20the%20time%20difference%20between%20the%20time%20at%20which%20the%20images%20are%20taken%20and%0Athe%20time%20at%20which%20the%20reference%20values%20of%20the%20concretes%20are%20carried%20out.%20With%0Athis%20temporal%20information%2C%20the%20network%20implicitly%20learns%20the%20time-dependent%0Abehaviour%20of%20the%20concretes%20properties.%20The%20network%20predicts%20the%20slump%20flow%0Adiameter%2C%20the%20yield%20stress%20and%20the%20plastic%20viscosity.%20The%20time-dependent%0Aprediction%20potentially%20opens%20up%20the%20pathway%20to%20determine%20the%20temporal%0Adevelopment%20of%20the%20fresh%20concrete%20properties%20already%20during%20mixing.%20This%0Aprovides%20a%20huge%20advantage%20for%20the%20concrete%20industry.%20As%20a%20result%2C%0Acountermeasures%20can%20be%20taken%20in%20a%20timely%20manner.%20It%20is%20shown%20that%20an%20approach%0Abased%20on%20depth%20and%20optical%20flow%20images%2C%20supported%20by%20information%20of%20the%20mix%0Adesign%2C%20achieves%20the%20best%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06611v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-based%20Deep%20Learning%20for%20the%20time-dependent%20prediction%20of%20fresh%0A%20%20concrete%20properties&entry.906535625=Max%20Meyer%20and%20Amadeus%20Langer%20and%20Max%20Mehltretter%20and%20Dries%20Beyer%20and%20Max%20Coenen%20and%20Tobias%20Schack%20and%20Michael%20Haist%20and%20Christian%20Heipke&entry.1292438233=%20%20Increasing%20the%20degree%20of%20digitisation%20and%20automation%20in%20the%20concrete%0Aproduction%20process%20can%20play%20a%20crucial%20role%20in%20reducing%20the%20CO%24_2%24%20emissions%0Athat%20are%20associated%20with%20the%20production%20of%20concrete.%20In%20this%20paper%2C%20a%20method%20is%0Apresented%20that%20makes%20it%20possible%20to%20predict%20the%20properties%20of%20fresh%20concrete%0Aduring%20the%20mixing%20process%20based%20on%20stereoscopic%20image%20sequences%20of%20the%0Aconcretes%20flow%20behaviour.%20A%20Convolutional%20Neural%20Network%20%28CNN%29%20is%20used%20for%20the%0Aprediction%2C%20which%20receives%20the%20images%20supported%20by%20information%20on%20the%20mix%0Adesign%20as%20input.%20In%20addition%2C%20the%20network%20receives%20temporal%20information%20in%20the%0Aform%20of%20the%20time%20difference%20between%20the%20time%20at%20which%20the%20images%20are%20taken%20and%0Athe%20time%20at%20which%20the%20reference%20values%20of%20the%20concretes%20are%20carried%20out.%20With%0Athis%20temporal%20information%2C%20the%20network%20implicitly%20learns%20the%20time-dependent%0Abehaviour%20of%20the%20concretes%20properties.%20The%20network%20predicts%20the%20slump%20flow%0Adiameter%2C%20the%20yield%20stress%20and%20the%20plastic%20viscosity.%20The%20time-dependent%0Aprediction%20potentially%20opens%20up%20the%20pathway%20to%20determine%20the%20temporal%0Adevelopment%20of%20the%20fresh%20concrete%20properties%20already%20during%20mixing.%20This%0Aprovides%20a%20huge%20advantage%20for%20the%20concrete%20industry.%20As%20a%20result%2C%0Acountermeasures%20can%20be%20taken%20in%20a%20timely%20manner.%20It%20is%20shown%20that%20an%20approach%0Abased%20on%20depth%20and%20optical%20flow%20images%2C%20supported%20by%20information%20of%20the%20mix%0Adesign%2C%20achieves%20the%20best%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06611v2&entry.124074799=Read"},
{"title": "All-in-one simulation-based inference", "author": "Manuel Gloeckler and Michael Deistler and Christian Weilbach and Frank Wood and Jakob H. Macke", "abstract": "  Amortized Bayesian inference trains neural networks to solve stochastic\ninference problems using model simulations, thereby making it possible to\nrapidly perform Bayesian inference for any newly observed data. However,\ncurrent simulation-based amortized inference methods are simulation-hungry and\ninflexible: They require the specification of a fixed parametric prior,\nsimulator, and inference tasks ahead of time. Here, we present a new amortized\ninference method -- the Simformer -- which overcomes these limitations. By\ntraining a probabilistic diffusion model with transformer architectures, the\nSimformer outperforms current state-of-the-art amortized inference approaches\non benchmark tasks and is substantially more flexible: It can be applied to\nmodels with function-valued parameters, it can handle inference scenarios with\nmissing or unstructured data, and it can sample arbitrary conditionals of the\njoint distribution of parameters and data, including both posterior and\nlikelihood. We showcase the performance and flexibility of the Simformer on\nsimulators from ecology, epidemiology, and neuroscience, and demonstrate that\nit opens up new possibilities and application domains for amortized Bayesian\ninference on simulation-based models.\n", "link": "http://arxiv.org/abs/2404.09636v1", "date": "2024-04-15", "relevancy": 1.0585, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5246}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5153}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20All-in-one%20simulation-based%20inference&body=Title%3A%20All-in-one%20simulation-based%20inference%0AAuthor%3A%20Manuel%20Gloeckler%20and%20Michael%20Deistler%20and%20Christian%20Weilbach%20and%20Frank%20Wood%20and%20Jakob%20H.%20Macke%0AAbstract%3A%20%20%20Amortized%20Bayesian%20inference%20trains%20neural%20networks%20to%20solve%20stochastic%0Ainference%20problems%20using%20model%20simulations%2C%20thereby%20making%20it%20possible%20to%0Arapidly%20perform%20Bayesian%20inference%20for%20any%20newly%20observed%20data.%20However%2C%0Acurrent%20simulation-based%20amortized%20inference%20methods%20are%20simulation-hungry%20and%0Ainflexible%3A%20They%20require%20the%20specification%20of%20a%20fixed%20parametric%20prior%2C%0Asimulator%2C%20and%20inference%20tasks%20ahead%20of%20time.%20Here%2C%20we%20present%20a%20new%20amortized%0Ainference%20method%20--%20the%20Simformer%20--%20which%20overcomes%20these%20limitations.%20By%0Atraining%20a%20probabilistic%20diffusion%20model%20with%20transformer%20architectures%2C%20the%0ASimformer%20outperforms%20current%20state-of-the-art%20amortized%20inference%20approaches%0Aon%20benchmark%20tasks%20and%20is%20substantially%20more%20flexible%3A%20It%20can%20be%20applied%20to%0Amodels%20with%20function-valued%20parameters%2C%20it%20can%20handle%20inference%20scenarios%20with%0Amissing%20or%20unstructured%20data%2C%20and%20it%20can%20sample%20arbitrary%20conditionals%20of%20the%0Ajoint%20distribution%20of%20parameters%20and%20data%2C%20including%20both%20posterior%20and%0Alikelihood.%20We%20showcase%20the%20performance%20and%20flexibility%20of%20the%20Simformer%20on%0Asimulators%20from%20ecology%2C%20epidemiology%2C%20and%20neuroscience%2C%20and%20demonstrate%20that%0Ait%20opens%20up%20new%20possibilities%20and%20application%20domains%20for%20amortized%20Bayesian%0Ainference%20on%20simulation-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09636v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All-in-one%20simulation-based%20inference&entry.906535625=Manuel%20Gloeckler%20and%20Michael%20Deistler%20and%20Christian%20Weilbach%20and%20Frank%20Wood%20and%20Jakob%20H.%20Macke&entry.1292438233=%20%20Amortized%20Bayesian%20inference%20trains%20neural%20networks%20to%20solve%20stochastic%0Ainference%20problems%20using%20model%20simulations%2C%20thereby%20making%20it%20possible%20to%0Arapidly%20perform%20Bayesian%20inference%20for%20any%20newly%20observed%20data.%20However%2C%0Acurrent%20simulation-based%20amortized%20inference%20methods%20are%20simulation-hungry%20and%0Ainflexible%3A%20They%20require%20the%20specification%20of%20a%20fixed%20parametric%20prior%2C%0Asimulator%2C%20and%20inference%20tasks%20ahead%20of%20time.%20Here%2C%20we%20present%20a%20new%20amortized%0Ainference%20method%20--%20the%20Simformer%20--%20which%20overcomes%20these%20limitations.%20By%0Atraining%20a%20probabilistic%20diffusion%20model%20with%20transformer%20architectures%2C%20the%0ASimformer%20outperforms%20current%20state-of-the-art%20amortized%20inference%20approaches%0Aon%20benchmark%20tasks%20and%20is%20substantially%20more%20flexible%3A%20It%20can%20be%20applied%20to%0Amodels%20with%20function-valued%20parameters%2C%20it%20can%20handle%20inference%20scenarios%20with%0Amissing%20or%20unstructured%20data%2C%20and%20it%20can%20sample%20arbitrary%20conditionals%20of%20the%0Ajoint%20distribution%20of%20parameters%20and%20data%2C%20including%20both%20posterior%20and%0Alikelihood.%20We%20showcase%20the%20performance%20and%20flexibility%20of%20the%20Simformer%20on%0Asimulators%20from%20ecology%2C%20epidemiology%2C%20and%20neuroscience%2C%20and%20demonstrate%20that%0Ait%20opens%20up%20new%20possibilities%20and%20application%20domains%20for%20amortized%20Bayesian%0Ainference%20on%20simulation-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09636v1&entry.124074799=Read"},
{"title": "Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User\n  Behavior Representation to the Scenario Context", "author": "Moyu Zhang and Yongxiang Tang and Jinxin Hu and Yu Zhang", "abstract": "  Existing methods often adjust representations adaptively only after\naggregating user behavior sequences. This coarse-grained approach to\nre-weighting the entire user sequence hampers the model's ability to accurately\nmodel the user interest migration across different scenarios. To enhance the\nmodel's capacity to capture user interests from historical behavior sequences\nin each scenario, we develop a ranking framework named the Scenario-Adaptive\nFine-Grained Personalization Network (SFPNet), which designs a kind of\nfine-grained method for multi-scenario personalized recommendations.\nSpecifically, SFPNet comprises a series of blocks named as Scenario-Tailoring\nBlock, stacked sequentially. Each block initially deploys a parameter\npersonalization unit to integrate scenario information at a coarse-grained\nlevel by redefining fundamental features. Subsequently, we consolidate\nscenario-adaptively adjusted feature representations to serve as context\ninformation. By employing residual connection, we incorporate this context into\nthe representation of each historical behavior, allowing for context-aware\nfine-grained customization of the behavior representations at the\nscenario-level, which in turn supports scenario-aware user interest modeling.\n", "link": "http://arxiv.org/abs/2404.09709v1", "date": "2024-04-15", "relevancy": 0.9382, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4766}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.467}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4638}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scenario-Adaptive%20Fine-Grained%20Personalization%20Network%3A%20Tailoring%20User%0A%20%20Behavior%20Representation%20to%20the%20Scenario%20Context&body=Title%3A%20Scenario-Adaptive%20Fine-Grained%20Personalization%20Network%3A%20Tailoring%20User%0A%20%20Behavior%20Representation%20to%20the%20Scenario%20Context%0AAuthor%3A%20Moyu%20Zhang%20and%20Yongxiang%20Tang%20and%20Jinxin%20Hu%20and%20Yu%20Zhang%0AAbstract%3A%20%20%20Existing%20methods%20often%20adjust%20representations%20adaptively%20only%20after%0Aaggregating%20user%20behavior%20sequences.%20This%20coarse-grained%20approach%20to%0Are-weighting%20the%20entire%20user%20sequence%20hampers%20the%20model%27s%20ability%20to%20accurately%0Amodel%20the%20user%20interest%20migration%20across%20different%20scenarios.%20To%20enhance%20the%0Amodel%27s%20capacity%20to%20capture%20user%20interests%20from%20historical%20behavior%20sequences%0Ain%20each%20scenario%2C%20we%20develop%20a%20ranking%20framework%20named%20the%20Scenario-Adaptive%0AFine-Grained%20Personalization%20Network%20%28SFPNet%29%2C%20which%20designs%20a%20kind%20of%0Afine-grained%20method%20for%20multi-scenario%20personalized%20recommendations.%0ASpecifically%2C%20SFPNet%20comprises%20a%20series%20of%20blocks%20named%20as%20Scenario-Tailoring%0ABlock%2C%20stacked%20sequentially.%20Each%20block%20initially%20deploys%20a%20parameter%0Apersonalization%20unit%20to%20integrate%20scenario%20information%20at%20a%20coarse-grained%0Alevel%20by%20redefining%20fundamental%20features.%20Subsequently%2C%20we%20consolidate%0Ascenario-adaptively%20adjusted%20feature%20representations%20to%20serve%20as%20context%0Ainformation.%20By%20employing%20residual%20connection%2C%20we%20incorporate%20this%20context%20into%0Athe%20representation%20of%20each%20historical%20behavior%2C%20allowing%20for%20context-aware%0Afine-grained%20customization%20of%20the%20behavior%20representations%20at%20the%0Ascenario-level%2C%20which%20in%20turn%20supports%20scenario-aware%20user%20interest%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09709v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scenario-Adaptive%20Fine-Grained%20Personalization%20Network%3A%20Tailoring%20User%0A%20%20Behavior%20Representation%20to%20the%20Scenario%20Context&entry.906535625=Moyu%20Zhang%20and%20Yongxiang%20Tang%20and%20Jinxin%20Hu%20and%20Yu%20Zhang&entry.1292438233=%20%20Existing%20methods%20often%20adjust%20representations%20adaptively%20only%20after%0Aaggregating%20user%20behavior%20sequences.%20This%20coarse-grained%20approach%20to%0Are-weighting%20the%20entire%20user%20sequence%20hampers%20the%20model%27s%20ability%20to%20accurately%0Amodel%20the%20user%20interest%20migration%20across%20different%20scenarios.%20To%20enhance%20the%0Amodel%27s%20capacity%20to%20capture%20user%20interests%20from%20historical%20behavior%20sequences%0Ain%20each%20scenario%2C%20we%20develop%20a%20ranking%20framework%20named%20the%20Scenario-Adaptive%0AFine-Grained%20Personalization%20Network%20%28SFPNet%29%2C%20which%20designs%20a%20kind%20of%0Afine-grained%20method%20for%20multi-scenario%20personalized%20recommendations.%0ASpecifically%2C%20SFPNet%20comprises%20a%20series%20of%20blocks%20named%20as%20Scenario-Tailoring%0ABlock%2C%20stacked%20sequentially.%20Each%20block%20initially%20deploys%20a%20parameter%0Apersonalization%20unit%20to%20integrate%20scenario%20information%20at%20a%20coarse-grained%0Alevel%20by%20redefining%20fundamental%20features.%20Subsequently%2C%20we%20consolidate%0Ascenario-adaptively%20adjusted%20feature%20representations%20to%20serve%20as%20context%0Ainformation.%20By%20employing%20residual%20connection%2C%20we%20incorporate%20this%20context%20into%0Athe%20representation%20of%20each%20historical%20behavior%2C%20allowing%20for%20context-aware%0Afine-grained%20customization%20of%20the%20behavior%20representations%20at%20the%0Ascenario-level%2C%20which%20in%20turn%20supports%20scenario-aware%20user%20interest%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09709v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


