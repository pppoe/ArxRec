<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250710.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance\n  Transfer and Reflection", "author": "Yongyang Zhou and Fang-Lue Zhang and Zichen Wang and Lei Zhang", "abstract": "  3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in\nnovel view synthesis. However, rendering reflective objects remains a\nsignificant challenge, particularly in inverse rendering and relighting. We\nintroduce RTR-GS, a novel inverse rendering framework capable of robustly\nrendering objects with arbitrary reflectance properties, decomposing BRDF and\nlighting, and delivering credible relighting results. Given a collection of\nmulti-view images, our method effectively recovers geometric structure through\na hybrid rendering model that combines forward rendering for radiance transfer\nwith deferred rendering for reflections. This approach successfully separates\nhigh-frequency and low-frequency appearances, mitigating floating artifacts\ncaused by spherical harmonic overfitting when handling high-frequency details.\nWe further refine BRDF and lighting decomposition using an additional\nphysically-based deferred rendering branch. Experimental results show that our\nmethod enhances novel view synthesis, normal estimation, decomposition, and\nrelighting while maintaining efficient training inference process.\n", "link": "http://arxiv.org/abs/2507.07733v1", "date": "2025-07-10", "relevancy": 3.2584, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7096}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6505}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RTR-GS%3A%203D%20Gaussian%20Splatting%20for%20Inverse%20Rendering%20with%20Radiance%0A%20%20Transfer%20and%20Reflection&body=Title%3A%20RTR-GS%3A%203D%20Gaussian%20Splatting%20for%20Inverse%20Rendering%20with%20Radiance%0A%20%20Transfer%20and%20Reflection%0AAuthor%3A%20Yongyang%20Zhou%20and%20Fang-Lue%20Zhang%20and%20Zichen%20Wang%20and%20Lei%20Zhang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20impressive%20capabilities%20in%0Anovel%20view%20synthesis.%20However%2C%20rendering%20reflective%20objects%20remains%20a%0Asignificant%20challenge%2C%20particularly%20in%20inverse%20rendering%20and%20relighting.%20We%0Aintroduce%20RTR-GS%2C%20a%20novel%20inverse%20rendering%20framework%20capable%20of%20robustly%0Arendering%20objects%20with%20arbitrary%20reflectance%20properties%2C%20decomposing%20BRDF%20and%0Alighting%2C%20and%20delivering%20credible%20relighting%20results.%20Given%20a%20collection%20of%0Amulti-view%20images%2C%20our%20method%20effectively%20recovers%20geometric%20structure%20through%0Aa%20hybrid%20rendering%20model%20that%20combines%20forward%20rendering%20for%20radiance%20transfer%0Awith%20deferred%20rendering%20for%20reflections.%20This%20approach%20successfully%20separates%0Ahigh-frequency%20and%20low-frequency%20appearances%2C%20mitigating%20floating%20artifacts%0Acaused%20by%20spherical%20harmonic%20overfitting%20when%20handling%20high-frequency%20details.%0AWe%20further%20refine%20BRDF%20and%20lighting%20decomposition%20using%20an%20additional%0Aphysically-based%20deferred%20rendering%20branch.%20Experimental%20results%20show%20that%20our%0Amethod%20enhances%20novel%20view%20synthesis%2C%20normal%20estimation%2C%20decomposition%2C%20and%0Arelighting%20while%20maintaining%20efficient%20training%20inference%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRTR-GS%253A%25203D%2520Gaussian%2520Splatting%2520for%2520Inverse%2520Rendering%2520with%2520Radiance%250A%2520%2520Transfer%2520and%2520Reflection%26entry.906535625%3DYongyang%2520Zhou%2520and%2520Fang-Lue%2520Zhang%2520and%2520Zichen%2520Wang%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520demonstrated%2520impressive%2520capabilities%2520in%250Anovel%2520view%2520synthesis.%2520However%252C%2520rendering%2520reflective%2520objects%2520remains%2520a%250Asignificant%2520challenge%252C%2520particularly%2520in%2520inverse%2520rendering%2520and%2520relighting.%2520We%250Aintroduce%2520RTR-GS%252C%2520a%2520novel%2520inverse%2520rendering%2520framework%2520capable%2520of%2520robustly%250Arendering%2520objects%2520with%2520arbitrary%2520reflectance%2520properties%252C%2520decomposing%2520BRDF%2520and%250Alighting%252C%2520and%2520delivering%2520credible%2520relighting%2520results.%2520Given%2520a%2520collection%2520of%250Amulti-view%2520images%252C%2520our%2520method%2520effectively%2520recovers%2520geometric%2520structure%2520through%250Aa%2520hybrid%2520rendering%2520model%2520that%2520combines%2520forward%2520rendering%2520for%2520radiance%2520transfer%250Awith%2520deferred%2520rendering%2520for%2520reflections.%2520This%2520approach%2520successfully%2520separates%250Ahigh-frequency%2520and%2520low-frequency%2520appearances%252C%2520mitigating%2520floating%2520artifacts%250Acaused%2520by%2520spherical%2520harmonic%2520overfitting%2520when%2520handling%2520high-frequency%2520details.%250AWe%2520further%2520refine%2520BRDF%2520and%2520lighting%2520decomposition%2520using%2520an%2520additional%250Aphysically-based%2520deferred%2520rendering%2520branch.%2520Experimental%2520results%2520show%2520that%2520our%250Amethod%2520enhances%2520novel%2520view%2520synthesis%252C%2520normal%2520estimation%252C%2520decomposition%252C%2520and%250Arelighting%2520while%2520maintaining%2520efficient%2520training%2520inference%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTR-GS%3A%203D%20Gaussian%20Splatting%20for%20Inverse%20Rendering%20with%20Radiance%0A%20%20Transfer%20and%20Reflection&entry.906535625=Yongyang%20Zhou%20and%20Fang-Lue%20Zhang%20and%20Zichen%20Wang%20and%20Lei%20Zhang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20impressive%20capabilities%20in%0Anovel%20view%20synthesis.%20However%2C%20rendering%20reflective%20objects%20remains%20a%0Asignificant%20challenge%2C%20particularly%20in%20inverse%20rendering%20and%20relighting.%20We%0Aintroduce%20RTR-GS%2C%20a%20novel%20inverse%20rendering%20framework%20capable%20of%20robustly%0Arendering%20objects%20with%20arbitrary%20reflectance%20properties%2C%20decomposing%20BRDF%20and%0Alighting%2C%20and%20delivering%20credible%20relighting%20results.%20Given%20a%20collection%20of%0Amulti-view%20images%2C%20our%20method%20effectively%20recovers%20geometric%20structure%20through%0Aa%20hybrid%20rendering%20model%20that%20combines%20forward%20rendering%20for%20radiance%20transfer%0Awith%20deferred%20rendering%20for%20reflections.%20This%20approach%20successfully%20separates%0Ahigh-frequency%20and%20low-frequency%20appearances%2C%20mitigating%20floating%20artifacts%0Acaused%20by%20spherical%20harmonic%20overfitting%20when%20handling%20high-frequency%20details.%0AWe%20further%20refine%20BRDF%20and%20lighting%20decomposition%20using%20an%20additional%0Aphysically-based%20deferred%20rendering%20branch.%20Experimental%20results%20show%20that%20our%0Amethod%20enhances%20novel%20view%20synthesis%2C%20normal%20estimation%2C%20decomposition%2C%20and%0Arelighting%20while%20maintaining%20efficient%20training%20inference%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07733v1&entry.124074799=Read"},
{"title": "E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models", "author": "Wenyan Cong and Yiqing Liang and Yancheng Zhang and Ziyi Yang and Yan Wang and Boris Ivanovic and Marco Pavone and Chen Chen and Zhangyang Wang and Zhiwen Fan", "abstract": "  Spatial intelligence, encompassing 3D reconstruction, perception, and\nreasoning, is fundamental to applications such as robotics, aerial imaging, and\nextended reality. A key enabler is the real-time, accurate estimation of core\n3D attributes (camera parameters, point clouds, depth maps, and 3D point\ntracks) from unstructured or streaming imagery. Inspired by the success of\nlarge foundation models in language and 2D vision, a new class of end-to-end 3D\ngeometric foundation models (GFMs) has emerged, directly predicting dense 3D\nrepresentations in a single feed-forward pass, eliminating the need for slow or\nunavailable precomputed camera parameters. Since late 2023, the field has\nexploded with diverse variants, but systematic evaluation is lacking. In this\nwork, we present the first comprehensive benchmark for 3D GFMs, covering five\ncore tasks: sparse-view depth estimation, video depth estimation, 3D\nreconstruction, multi-view pose estimation, novel view synthesis, and spanning\nboth standard and challenging out-of-distribution datasets. Our standardized\ntoolkit automates dataset handling, evaluation protocols, and metric\ncomputation to ensure fair, reproducible comparisons. We evaluate 16\nstate-of-the-art GFMs, revealing their strengths and limitations across tasks\nand domains, and derive key insights to guide future model scaling and\noptimization. All code, evaluation scripts, and processed data will be publicly\nreleased to accelerate research in 3D spatial intelligence.\n", "link": "http://arxiv.org/abs/2506.01933v3", "date": "2025-07-10", "relevancy": 3.1546, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6382}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E3D-Bench%3A%20A%20Benchmark%20for%20End-to-End%203D%20Geometric%20Foundation%20Models&body=Title%3A%20E3D-Bench%3A%20A%20Benchmark%20for%20End-to-End%203D%20Geometric%20Foundation%20Models%0AAuthor%3A%20Wenyan%20Cong%20and%20Yiqing%20Liang%20and%20Yancheng%20Zhang%20and%20Ziyi%20Yang%20and%20Yan%20Wang%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Chen%20Chen%20and%20Zhangyang%20Wang%20and%20Zhiwen%20Fan%0AAbstract%3A%20%20%20Spatial%20intelligence%2C%20encompassing%203D%20reconstruction%2C%20perception%2C%20and%0Areasoning%2C%20is%20fundamental%20to%20applications%20such%20as%20robotics%2C%20aerial%20imaging%2C%20and%0Aextended%20reality.%20A%20key%20enabler%20is%20the%20real-time%2C%20accurate%20estimation%20of%20core%0A3D%20attributes%20%28camera%20parameters%2C%20point%20clouds%2C%20depth%20maps%2C%20and%203D%20point%0Atracks%29%20from%20unstructured%20or%20streaming%20imagery.%20Inspired%20by%20the%20success%20of%0Alarge%20foundation%20models%20in%20language%20and%202D%20vision%2C%20a%20new%20class%20of%20end-to-end%203D%0Ageometric%20foundation%20models%20%28GFMs%29%20has%20emerged%2C%20directly%20predicting%20dense%203D%0Arepresentations%20in%20a%20single%20feed-forward%20pass%2C%20eliminating%20the%20need%20for%20slow%20or%0Aunavailable%20precomputed%20camera%20parameters.%20Since%20late%202023%2C%20the%20field%20has%0Aexploded%20with%20diverse%20variants%2C%20but%20systematic%20evaluation%20is%20lacking.%20In%20this%0Awork%2C%20we%20present%20the%20first%20comprehensive%20benchmark%20for%203D%20GFMs%2C%20covering%20five%0Acore%20tasks%3A%20sparse-view%20depth%20estimation%2C%20video%20depth%20estimation%2C%203D%0Areconstruction%2C%20multi-view%20pose%20estimation%2C%20novel%20view%20synthesis%2C%20and%20spanning%0Aboth%20standard%20and%20challenging%20out-of-distribution%20datasets.%20Our%20standardized%0Atoolkit%20automates%20dataset%20handling%2C%20evaluation%20protocols%2C%20and%20metric%0Acomputation%20to%20ensure%20fair%2C%20reproducible%20comparisons.%20We%20evaluate%2016%0Astate-of-the-art%20GFMs%2C%20revealing%20their%20strengths%20and%20limitations%20across%20tasks%0Aand%20domains%2C%20and%20derive%20key%20insights%20to%20guide%20future%20model%20scaling%20and%0Aoptimization.%20All%20code%2C%20evaluation%20scripts%2C%20and%20processed%20data%20will%20be%20publicly%0Areleased%20to%20accelerate%20research%20in%203D%20spatial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01933v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE3D-Bench%253A%2520A%2520Benchmark%2520for%2520End-to-End%25203D%2520Geometric%2520Foundation%2520Models%26entry.906535625%3DWenyan%2520Cong%2520and%2520Yiqing%2520Liang%2520and%2520Yancheng%2520Zhang%2520and%2520Ziyi%2520Yang%2520and%2520Yan%2520Wang%2520and%2520Boris%2520Ivanovic%2520and%2520Marco%2520Pavone%2520and%2520Chen%2520Chen%2520and%2520Zhangyang%2520Wang%2520and%2520Zhiwen%2520Fan%26entry.1292438233%3D%2520%2520Spatial%2520intelligence%252C%2520encompassing%25203D%2520reconstruction%252C%2520perception%252C%2520and%250Areasoning%252C%2520is%2520fundamental%2520to%2520applications%2520such%2520as%2520robotics%252C%2520aerial%2520imaging%252C%2520and%250Aextended%2520reality.%2520A%2520key%2520enabler%2520is%2520the%2520real-time%252C%2520accurate%2520estimation%2520of%2520core%250A3D%2520attributes%2520%2528camera%2520parameters%252C%2520point%2520clouds%252C%2520depth%2520maps%252C%2520and%25203D%2520point%250Atracks%2529%2520from%2520unstructured%2520or%2520streaming%2520imagery.%2520Inspired%2520by%2520the%2520success%2520of%250Alarge%2520foundation%2520models%2520in%2520language%2520and%25202D%2520vision%252C%2520a%2520new%2520class%2520of%2520end-to-end%25203D%250Ageometric%2520foundation%2520models%2520%2528GFMs%2529%2520has%2520emerged%252C%2520directly%2520predicting%2520dense%25203D%250Arepresentations%2520in%2520a%2520single%2520feed-forward%2520pass%252C%2520eliminating%2520the%2520need%2520for%2520slow%2520or%250Aunavailable%2520precomputed%2520camera%2520parameters.%2520Since%2520late%25202023%252C%2520the%2520field%2520has%250Aexploded%2520with%2520diverse%2520variants%252C%2520but%2520systematic%2520evaluation%2520is%2520lacking.%2520In%2520this%250Awork%252C%2520we%2520present%2520the%2520first%2520comprehensive%2520benchmark%2520for%25203D%2520GFMs%252C%2520covering%2520five%250Acore%2520tasks%253A%2520sparse-view%2520depth%2520estimation%252C%2520video%2520depth%2520estimation%252C%25203D%250Areconstruction%252C%2520multi-view%2520pose%2520estimation%252C%2520novel%2520view%2520synthesis%252C%2520and%2520spanning%250Aboth%2520standard%2520and%2520challenging%2520out-of-distribution%2520datasets.%2520Our%2520standardized%250Atoolkit%2520automates%2520dataset%2520handling%252C%2520evaluation%2520protocols%252C%2520and%2520metric%250Acomputation%2520to%2520ensure%2520fair%252C%2520reproducible%2520comparisons.%2520We%2520evaluate%252016%250Astate-of-the-art%2520GFMs%252C%2520revealing%2520their%2520strengths%2520and%2520limitations%2520across%2520tasks%250Aand%2520domains%252C%2520and%2520derive%2520key%2520insights%2520to%2520guide%2520future%2520model%2520scaling%2520and%250Aoptimization.%2520All%2520code%252C%2520evaluation%2520scripts%252C%2520and%2520processed%2520data%2520will%2520be%2520publicly%250Areleased%2520to%2520accelerate%2520research%2520in%25203D%2520spatial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01933v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E3D-Bench%3A%20A%20Benchmark%20for%20End-to-End%203D%20Geometric%20Foundation%20Models&entry.906535625=Wenyan%20Cong%20and%20Yiqing%20Liang%20and%20Yancheng%20Zhang%20and%20Ziyi%20Yang%20and%20Yan%20Wang%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Chen%20Chen%20and%20Zhangyang%20Wang%20and%20Zhiwen%20Fan&entry.1292438233=%20%20Spatial%20intelligence%2C%20encompassing%203D%20reconstruction%2C%20perception%2C%20and%0Areasoning%2C%20is%20fundamental%20to%20applications%20such%20as%20robotics%2C%20aerial%20imaging%2C%20and%0Aextended%20reality.%20A%20key%20enabler%20is%20the%20real-time%2C%20accurate%20estimation%20of%20core%0A3D%20attributes%20%28camera%20parameters%2C%20point%20clouds%2C%20depth%20maps%2C%20and%203D%20point%0Atracks%29%20from%20unstructured%20or%20streaming%20imagery.%20Inspired%20by%20the%20success%20of%0Alarge%20foundation%20models%20in%20language%20and%202D%20vision%2C%20a%20new%20class%20of%20end-to-end%203D%0Ageometric%20foundation%20models%20%28GFMs%29%20has%20emerged%2C%20directly%20predicting%20dense%203D%0Arepresentations%20in%20a%20single%20feed-forward%20pass%2C%20eliminating%20the%20need%20for%20slow%20or%0Aunavailable%20precomputed%20camera%20parameters.%20Since%20late%202023%2C%20the%20field%20has%0Aexploded%20with%20diverse%20variants%2C%20but%20systematic%20evaluation%20is%20lacking.%20In%20this%0Awork%2C%20we%20present%20the%20first%20comprehensive%20benchmark%20for%203D%20GFMs%2C%20covering%20five%0Acore%20tasks%3A%20sparse-view%20depth%20estimation%2C%20video%20depth%20estimation%2C%203D%0Areconstruction%2C%20multi-view%20pose%20estimation%2C%20novel%20view%20synthesis%2C%20and%20spanning%0Aboth%20standard%20and%20challenging%20out-of-distribution%20datasets.%20Our%20standardized%0Atoolkit%20automates%20dataset%20handling%2C%20evaluation%20protocols%2C%20and%20metric%0Acomputation%20to%20ensure%20fair%2C%20reproducible%20comparisons.%20We%20evaluate%2016%0Astate-of-the-art%20GFMs%2C%20revealing%20their%20strengths%20and%20limitations%20across%20tasks%0Aand%20domains%2C%20and%20derive%20key%20insights%20to%20guide%20future%20model%20scaling%20and%0Aoptimization.%20All%20code%2C%20evaluation%20scripts%2C%20and%20processed%20data%20will%20be%20publicly%0Areleased%20to%20accelerate%20research%20in%203D%20spatial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01933v3&entry.124074799=Read"},
{"title": "Martian World Models: Controllable Video Synthesis with Physically\n  Accurate 3D Reconstructions", "author": "Longfei Li and Zhiwen Fan and Wenyan Cong and Xinhang Liu and Yuyang Yin and Matt Foutter and Panwang Pan and Chenyu You and Yue Wang and Zhangyang Wang and Yao Zhao and Marco Pavone and Yunchao Wei", "abstract": "  Synthesizing realistic Martian landscape videos is crucial for mission\nrehearsal and robotic simulation. However, this task poses unique challenges\ndue to the scarcity of high-quality Martian data and the significant domain gap\nbetween Martian and terrestrial imagery. To address these challenges, we\npropose a holistic solution composed of two key components: 1) A data curation\npipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian\nenvironments from real stereo navigation images, sourced from NASA's Planetary\nData System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A\nMartian terrain video generator, MarsGen, which synthesizes novel videos\nvisually realistic and geometrically consistent with the 3D structure encoded\nin the data. Our M3arsSynth engine spans a wide range of Martian terrains and\nacquisition dates, enabling the generation of physically accurate 3D surface\nmodels at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data,\nsynthesizes videos conditioned on an initial image frame and, optionally,\ncamera trajectories or textual prompts, allowing for video generation in novel\nenvironments. Experimental results show that our approach outperforms video\nsynthesis models trained on terrestrial datasets, achieving superior visual\nfidelity and 3D structural consistency.\n", "link": "http://arxiv.org/abs/2507.07978v1", "date": "2025-07-10", "relevancy": 3.0799, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6169}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6169}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Martian%20World%20Models%3A%20Controllable%20Video%20Synthesis%20with%20Physically%0A%20%20Accurate%203D%20Reconstructions&body=Title%3A%20Martian%20World%20Models%3A%20Controllable%20Video%20Synthesis%20with%20Physically%0A%20%20Accurate%203D%20Reconstructions%0AAuthor%3A%20Longfei%20Li%20and%20Zhiwen%20Fan%20and%20Wenyan%20Cong%20and%20Xinhang%20Liu%20and%20Yuyang%20Yin%20and%20Matt%20Foutter%20and%20Panwang%20Pan%20and%20Chenyu%20You%20and%20Yue%20Wang%20and%20Zhangyang%20Wang%20and%20Yao%20Zhao%20and%20Marco%20Pavone%20and%20Yunchao%20Wei%0AAbstract%3A%20%20%20Synthesizing%20realistic%20Martian%20landscape%20videos%20is%20crucial%20for%20mission%0Arehearsal%20and%20robotic%20simulation.%20However%2C%20this%20task%20poses%20unique%20challenges%0Adue%20to%20the%20scarcity%20of%20high-quality%20Martian%20data%20and%20the%20significant%20domain%20gap%0Abetween%20Martian%20and%20terrestrial%20imagery.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20holistic%20solution%20composed%20of%20two%20key%20components%3A%201%29%20A%20data%20curation%0Apipeline%20Multimodal%20Mars%20Synthesis%20%28M3arsSynth%29%2C%20which%20reconstructs%203D%20Martian%0Aenvironments%20from%20real%20stereo%20navigation%20images%2C%20sourced%20from%20NASA%27s%20Planetary%0AData%20System%20%28PDS%29%2C%20and%20renders%20high-fidelity%20multiview%203D%20video%20sequences.%202%29%20A%0AMartian%20terrain%20video%20generator%2C%20MarsGen%2C%20which%20synthesizes%20novel%20videos%0Avisually%20realistic%20and%20geometrically%20consistent%20with%20the%203D%20structure%20encoded%0Ain%20the%20data.%20Our%20M3arsSynth%20engine%20spans%20a%20wide%20range%20of%20Martian%20terrains%20and%0Aacquisition%20dates%2C%20enabling%20the%20generation%20of%20physically%20accurate%203D%20surface%0Amodels%20at%20metric-scale%20resolution.%20MarsGen%2C%20fine-tuned%20on%20M3arsSynth%20data%2C%0Asynthesizes%20videos%20conditioned%20on%20an%20initial%20image%20frame%20and%2C%20optionally%2C%0Acamera%20trajectories%20or%20textual%20prompts%2C%20allowing%20for%20video%20generation%20in%20novel%0Aenvironments.%20Experimental%20results%20show%20that%20our%20approach%20outperforms%20video%0Asynthesis%20models%20trained%20on%20terrestrial%20datasets%2C%20achieving%20superior%20visual%0Afidelity%20and%203D%20structural%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMartian%2520World%2520Models%253A%2520Controllable%2520Video%2520Synthesis%2520with%2520Physically%250A%2520%2520Accurate%25203D%2520Reconstructions%26entry.906535625%3DLongfei%2520Li%2520and%2520Zhiwen%2520Fan%2520and%2520Wenyan%2520Cong%2520and%2520Xinhang%2520Liu%2520and%2520Yuyang%2520Yin%2520and%2520Matt%2520Foutter%2520and%2520Panwang%2520Pan%2520and%2520Chenyu%2520You%2520and%2520Yue%2520Wang%2520and%2520Zhangyang%2520Wang%2520and%2520Yao%2520Zhao%2520and%2520Marco%2520Pavone%2520and%2520Yunchao%2520Wei%26entry.1292438233%3D%2520%2520Synthesizing%2520realistic%2520Martian%2520landscape%2520videos%2520is%2520crucial%2520for%2520mission%250Arehearsal%2520and%2520robotic%2520simulation.%2520However%252C%2520this%2520task%2520poses%2520unique%2520challenges%250Adue%2520to%2520the%2520scarcity%2520of%2520high-quality%2520Martian%2520data%2520and%2520the%2520significant%2520domain%2520gap%250Abetween%2520Martian%2520and%2520terrestrial%2520imagery.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520holistic%2520solution%2520composed%2520of%2520two%2520key%2520components%253A%25201%2529%2520A%2520data%2520curation%250Apipeline%2520Multimodal%2520Mars%2520Synthesis%2520%2528M3arsSynth%2529%252C%2520which%2520reconstructs%25203D%2520Martian%250Aenvironments%2520from%2520real%2520stereo%2520navigation%2520images%252C%2520sourced%2520from%2520NASA%2527s%2520Planetary%250AData%2520System%2520%2528PDS%2529%252C%2520and%2520renders%2520high-fidelity%2520multiview%25203D%2520video%2520sequences.%25202%2529%2520A%250AMartian%2520terrain%2520video%2520generator%252C%2520MarsGen%252C%2520which%2520synthesizes%2520novel%2520videos%250Avisually%2520realistic%2520and%2520geometrically%2520consistent%2520with%2520the%25203D%2520structure%2520encoded%250Ain%2520the%2520data.%2520Our%2520M3arsSynth%2520engine%2520spans%2520a%2520wide%2520range%2520of%2520Martian%2520terrains%2520and%250Aacquisition%2520dates%252C%2520enabling%2520the%2520generation%2520of%2520physically%2520accurate%25203D%2520surface%250Amodels%2520at%2520metric-scale%2520resolution.%2520MarsGen%252C%2520fine-tuned%2520on%2520M3arsSynth%2520data%252C%250Asynthesizes%2520videos%2520conditioned%2520on%2520an%2520initial%2520image%2520frame%2520and%252C%2520optionally%252C%250Acamera%2520trajectories%2520or%2520textual%2520prompts%252C%2520allowing%2520for%2520video%2520generation%2520in%2520novel%250Aenvironments.%2520Experimental%2520results%2520show%2520that%2520our%2520approach%2520outperforms%2520video%250Asynthesis%2520models%2520trained%2520on%2520terrestrial%2520datasets%252C%2520achieving%2520superior%2520visual%250Afidelity%2520and%25203D%2520structural%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Martian%20World%20Models%3A%20Controllable%20Video%20Synthesis%20with%20Physically%0A%20%20Accurate%203D%20Reconstructions&entry.906535625=Longfei%20Li%20and%20Zhiwen%20Fan%20and%20Wenyan%20Cong%20and%20Xinhang%20Liu%20and%20Yuyang%20Yin%20and%20Matt%20Foutter%20and%20Panwang%20Pan%20and%20Chenyu%20You%20and%20Yue%20Wang%20and%20Zhangyang%20Wang%20and%20Yao%20Zhao%20and%20Marco%20Pavone%20and%20Yunchao%20Wei&entry.1292438233=%20%20Synthesizing%20realistic%20Martian%20landscape%20videos%20is%20crucial%20for%20mission%0Arehearsal%20and%20robotic%20simulation.%20However%2C%20this%20task%20poses%20unique%20challenges%0Adue%20to%20the%20scarcity%20of%20high-quality%20Martian%20data%20and%20the%20significant%20domain%20gap%0Abetween%20Martian%20and%20terrestrial%20imagery.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20holistic%20solution%20composed%20of%20two%20key%20components%3A%201%29%20A%20data%20curation%0Apipeline%20Multimodal%20Mars%20Synthesis%20%28M3arsSynth%29%2C%20which%20reconstructs%203D%20Martian%0Aenvironments%20from%20real%20stereo%20navigation%20images%2C%20sourced%20from%20NASA%27s%20Planetary%0AData%20System%20%28PDS%29%2C%20and%20renders%20high-fidelity%20multiview%203D%20video%20sequences.%202%29%20A%0AMartian%20terrain%20video%20generator%2C%20MarsGen%2C%20which%20synthesizes%20novel%20videos%0Avisually%20realistic%20and%20geometrically%20consistent%20with%20the%203D%20structure%20encoded%0Ain%20the%20data.%20Our%20M3arsSynth%20engine%20spans%20a%20wide%20range%20of%20Martian%20terrains%20and%0Aacquisition%20dates%2C%20enabling%20the%20generation%20of%20physically%20accurate%203D%20surface%0Amodels%20at%20metric-scale%20resolution.%20MarsGen%2C%20fine-tuned%20on%20M3arsSynth%20data%2C%0Asynthesizes%20videos%20conditioned%20on%20an%20initial%20image%20frame%20and%2C%20optionally%2C%0Acamera%20trajectories%20or%20textual%20prompts%2C%20allowing%20for%20video%20generation%20in%20novel%0Aenvironments.%20Experimental%20results%20show%20that%20our%20approach%20outperforms%20video%0Asynthesis%20models%20trained%20on%20terrestrial%20datasets%2C%20achieving%20superior%20visual%0Afidelity%20and%203D%20structural%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07978v1&entry.124074799=Read"},
{"title": "LOSC: LiDAR Open-voc Segmentation Consolidator", "author": "Nermin Samet and Gilles Puy and Renaud Marlet", "abstract": "  We study the use of image-based Vision-Language Models (VLMs) for\nopen-vocabulary segmentation of lidar scans in driving settings. Classically,\nimage semantics can be back-projected onto 3D point clouds. Yet, resulting\npoint labels are noisy and sparse. We consolidate these labels to enforce both\nspatio-temporal consistency and robustness to image-level augmentations. We\nthen train a 3D network based on these refined labels. This simple method,\ncalled LOSC, outperforms the SOTA of zero-shot open-vocabulary semantic and\npanoptic segmentation on both nuScenes and SemanticKITTI, with significant\nmargins.\n", "link": "http://arxiv.org/abs/2507.07605v1", "date": "2025-07-10", "relevancy": 3.0137, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6113}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6113}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOSC%3A%20LiDAR%20Open-voc%20Segmentation%20Consolidator&body=Title%3A%20LOSC%3A%20LiDAR%20Open-voc%20Segmentation%20Consolidator%0AAuthor%3A%20Nermin%20Samet%20and%20Gilles%20Puy%20and%20Renaud%20Marlet%0AAbstract%3A%20%20%20We%20study%20the%20use%20of%20image-based%20Vision-Language%20Models%20%28VLMs%29%20for%0Aopen-vocabulary%20segmentation%20of%20lidar%20scans%20in%20driving%20settings.%20Classically%2C%0Aimage%20semantics%20can%20be%20back-projected%20onto%203D%20point%20clouds.%20Yet%2C%20resulting%0Apoint%20labels%20are%20noisy%20and%20sparse.%20We%20consolidate%20these%20labels%20to%20enforce%20both%0Aspatio-temporal%20consistency%20and%20robustness%20to%20image-level%20augmentations.%20We%0Athen%20train%20a%203D%20network%20based%20on%20these%20refined%20labels.%20This%20simple%20method%2C%0Acalled%20LOSC%2C%20outperforms%20the%20SOTA%20of%20zero-shot%20open-vocabulary%20semantic%20and%0Apanoptic%20segmentation%20on%20both%20nuScenes%20and%20SemanticKITTI%2C%20with%20significant%0Amargins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOSC%253A%2520LiDAR%2520Open-voc%2520Segmentation%2520Consolidator%26entry.906535625%3DNermin%2520Samet%2520and%2520Gilles%2520Puy%2520and%2520Renaud%2520Marlet%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520use%2520of%2520image-based%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520for%250Aopen-vocabulary%2520segmentation%2520of%2520lidar%2520scans%2520in%2520driving%2520settings.%2520Classically%252C%250Aimage%2520semantics%2520can%2520be%2520back-projected%2520onto%25203D%2520point%2520clouds.%2520Yet%252C%2520resulting%250Apoint%2520labels%2520are%2520noisy%2520and%2520sparse.%2520We%2520consolidate%2520these%2520labels%2520to%2520enforce%2520both%250Aspatio-temporal%2520consistency%2520and%2520robustness%2520to%2520image-level%2520augmentations.%2520We%250Athen%2520train%2520a%25203D%2520network%2520based%2520on%2520these%2520refined%2520labels.%2520This%2520simple%2520method%252C%250Acalled%2520LOSC%252C%2520outperforms%2520the%2520SOTA%2520of%2520zero-shot%2520open-vocabulary%2520semantic%2520and%250Apanoptic%2520segmentation%2520on%2520both%2520nuScenes%2520and%2520SemanticKITTI%252C%2520with%2520significant%250Amargins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOSC%3A%20LiDAR%20Open-voc%20Segmentation%20Consolidator&entry.906535625=Nermin%20Samet%20and%20Gilles%20Puy%20and%20Renaud%20Marlet&entry.1292438233=%20%20We%20study%20the%20use%20of%20image-based%20Vision-Language%20Models%20%28VLMs%29%20for%0Aopen-vocabulary%20segmentation%20of%20lidar%20scans%20in%20driving%20settings.%20Classically%2C%0Aimage%20semantics%20can%20be%20back-projected%20onto%203D%20point%20clouds.%20Yet%2C%20resulting%0Apoint%20labels%20are%20noisy%20and%20sparse.%20We%20consolidate%20these%20labels%20to%20enforce%20both%0Aspatio-temporal%20consistency%20and%20robustness%20to%20image-level%20augmentations.%20We%0Athen%20train%20a%203D%20network%20based%20on%20these%20refined%20labels.%20This%20simple%20method%2C%0Acalled%20LOSC%2C%20outperforms%20the%20SOTA%20of%20zero-shot%20open-vocabulary%20semantic%20and%0Apanoptic%20segmentation%20on%20both%20nuScenes%20and%20SemanticKITTI%2C%20with%20significant%0Amargins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07605v1&entry.124074799=Read"},
{"title": "EEPNet-V2: Patch-to-Pixel Solution for Efficient Cross-Modal\n  Registration between LiDAR Point Cloud and Camera Image", "author": "Yuanchao Yue and Hui Yuan and Zhengxin Li and Shuai Li and Wei Zhang", "abstract": "  The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds effectively but also bridge the domain gap\nbetween the point cloud and image. Moreover, to tackle the challenges of cross\nmodal differences and the limited overlap between LiDAR point clouds and images\nin the image matching task, we introduce a multi-scale feature extraction\nnetwork to effectively extract features from both camera images and the\nprojection maps of LiDAR point cloud. Additionally, we propose a patch-to-pixel\nmatching network to provide more effective supervision and achieve high\naccuracy. We validate the performance of our model through experiments on the\nKITTI and nuScenes datasets. Experimental results demonstrate the the proposed\nmethod achieves real-time performance and extremely high registration accuracy.\nSpecifically, on the KITTI dataset, our model achieves a registration accuracy\nrate of over 99\\%. Our code is released at:\nhttps://github.com/ESRSchao/EEPNet-V2.\n", "link": "http://arxiv.org/abs/2503.15285v2", "date": "2025-07-10", "relevancy": 3.0055, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6423}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5942}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEPNet-V2%3A%20Patch-to-Pixel%20Solution%20for%20Efficient%20Cross-Modal%0A%20%20Registration%20between%20LiDAR%20Point%20Cloud%20and%20Camera%20Image&body=Title%3A%20EEPNet-V2%3A%20Patch-to-Pixel%20Solution%20for%20Efficient%20Cross-Modal%0A%20%20Registration%20between%20LiDAR%20Point%20Cloud%20and%20Camera%20Image%0AAuthor%3A%20Yuanchao%20Yue%20and%20Hui%20Yuan%20and%20Zhengxin%20Li%20and%20Shuai%20Li%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20The%20primary%20requirement%20for%20cross-modal%20data%20fusion%20is%20the%20precise%20alignment%0Aof%20data%20from%20different%20sensors.%20However%2C%20the%20calibration%20between%20LiDAR%20point%0Aclouds%20and%20camera%20images%20is%20typically%20time-consuming%20and%20needs%20external%0Acalibration%20board%20or%20specific%20environmental%20features.%20Cross-modal%20registration%0Aeffectively%20solves%20this%20problem%20by%20aligning%20the%20data%20directly%20without%20requiring%0Aexternal%20calibration.%20However%2C%20due%20to%20the%20domain%20gap%20between%20the%20point%20cloud%0Aand%20the%20image%2C%20existing%20methods%20rarely%20achieve%20satisfactory%20registration%0Aaccuracy%20while%20maintaining%20real-time%20performance.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20framework%20that%20projects%20point%20clouds%20into%20several%202D%20representations%0Afor%20matching%20with%20camera%20images%2C%20which%20not%20only%20leverages%20the%20geometric%0Acharacteristic%20of%20LiDAR%20point%20clouds%20effectively%20but%20also%20bridge%20the%20domain%20gap%0Abetween%20the%20point%20cloud%20and%20image.%20Moreover%2C%20to%20tackle%20the%20challenges%20of%20cross%0Amodal%20differences%20and%20the%20limited%20overlap%20between%20LiDAR%20point%20clouds%20and%20images%0Ain%20the%20image%20matching%20task%2C%20we%20introduce%20a%20multi-scale%20feature%20extraction%0Anetwork%20to%20effectively%20extract%20features%20from%20both%20camera%20images%20and%20the%0Aprojection%20maps%20of%20LiDAR%20point%20cloud.%20Additionally%2C%20we%20propose%20a%20patch-to-pixel%0Amatching%20network%20to%20provide%20more%20effective%20supervision%20and%20achieve%20high%0Aaccuracy.%20We%20validate%20the%20performance%20of%20our%20model%20through%20experiments%20on%20the%0AKITTI%20and%20nuScenes%20datasets.%20Experimental%20results%20demonstrate%20the%20the%20proposed%0Amethod%20achieves%20real-time%20performance%20and%20extremely%20high%20registration%20accuracy.%0ASpecifically%2C%20on%20the%20KITTI%20dataset%2C%20our%20model%20achieves%20a%20registration%20accuracy%0Arate%20of%20over%2099%5C%25.%20Our%20code%20is%20released%20at%3A%0Ahttps%3A//github.com/ESRSchao/EEPNet-V2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.15285v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEPNet-V2%253A%2520Patch-to-Pixel%2520Solution%2520for%2520Efficient%2520Cross-Modal%250A%2520%2520Registration%2520between%2520LiDAR%2520Point%2520Cloud%2520and%2520Camera%2520Image%26entry.906535625%3DYuanchao%2520Yue%2520and%2520Hui%2520Yuan%2520and%2520Zhengxin%2520Li%2520and%2520Shuai%2520Li%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520primary%2520requirement%2520for%2520cross-modal%2520data%2520fusion%2520is%2520the%2520precise%2520alignment%250Aof%2520data%2520from%2520different%2520sensors.%2520However%252C%2520the%2520calibration%2520between%2520LiDAR%2520point%250Aclouds%2520and%2520camera%2520images%2520is%2520typically%2520time-consuming%2520and%2520needs%2520external%250Acalibration%2520board%2520or%2520specific%2520environmental%2520features.%2520Cross-modal%2520registration%250Aeffectively%2520solves%2520this%2520problem%2520by%2520aligning%2520the%2520data%2520directly%2520without%2520requiring%250Aexternal%2520calibration.%2520However%252C%2520due%2520to%2520the%2520domain%2520gap%2520between%2520the%2520point%2520cloud%250Aand%2520the%2520image%252C%2520existing%2520methods%2520rarely%2520achieve%2520satisfactory%2520registration%250Aaccuracy%2520while%2520maintaining%2520real-time%2520performance.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520a%2520framework%2520that%2520projects%2520point%2520clouds%2520into%2520several%25202D%2520representations%250Afor%2520matching%2520with%2520camera%2520images%252C%2520which%2520not%2520only%2520leverages%2520the%2520geometric%250Acharacteristic%2520of%2520LiDAR%2520point%2520clouds%2520effectively%2520but%2520also%2520bridge%2520the%2520domain%2520gap%250Abetween%2520the%2520point%2520cloud%2520and%2520image.%2520Moreover%252C%2520to%2520tackle%2520the%2520challenges%2520of%2520cross%250Amodal%2520differences%2520and%2520the%2520limited%2520overlap%2520between%2520LiDAR%2520point%2520clouds%2520and%2520images%250Ain%2520the%2520image%2520matching%2520task%252C%2520we%2520introduce%2520a%2520multi-scale%2520feature%2520extraction%250Anetwork%2520to%2520effectively%2520extract%2520features%2520from%2520both%2520camera%2520images%2520and%2520the%250Aprojection%2520maps%2520of%2520LiDAR%2520point%2520cloud.%2520Additionally%252C%2520we%2520propose%2520a%2520patch-to-pixel%250Amatching%2520network%2520to%2520provide%2520more%2520effective%2520supervision%2520and%2520achieve%2520high%250Aaccuracy.%2520We%2520validate%2520the%2520performance%2520of%2520our%2520model%2520through%2520experiments%2520on%2520the%250AKITTI%2520and%2520nuScenes%2520datasets.%2520Experimental%2520results%2520demonstrate%2520the%2520the%2520proposed%250Amethod%2520achieves%2520real-time%2520performance%2520and%2520extremely%2520high%2520registration%2520accuracy.%250ASpecifically%252C%2520on%2520the%2520KITTI%2520dataset%252C%2520our%2520model%2520achieves%2520a%2520registration%2520accuracy%250Arate%2520of%2520over%252099%255C%2525.%2520Our%2520code%2520is%2520released%2520at%253A%250Ahttps%253A//github.com/ESRSchao/EEPNet-V2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15285v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEPNet-V2%3A%20Patch-to-Pixel%20Solution%20for%20Efficient%20Cross-Modal%0A%20%20Registration%20between%20LiDAR%20Point%20Cloud%20and%20Camera%20Image&entry.906535625=Yuanchao%20Yue%20and%20Hui%20Yuan%20and%20Zhengxin%20Li%20and%20Shuai%20Li%20and%20Wei%20Zhang&entry.1292438233=%20%20The%20primary%20requirement%20for%20cross-modal%20data%20fusion%20is%20the%20precise%20alignment%0Aof%20data%20from%20different%20sensors.%20However%2C%20the%20calibration%20between%20LiDAR%20point%0Aclouds%20and%20camera%20images%20is%20typically%20time-consuming%20and%20needs%20external%0Acalibration%20board%20or%20specific%20environmental%20features.%20Cross-modal%20registration%0Aeffectively%20solves%20this%20problem%20by%20aligning%20the%20data%20directly%20without%20requiring%0Aexternal%20calibration.%20However%2C%20due%20to%20the%20domain%20gap%20between%20the%20point%20cloud%0Aand%20the%20image%2C%20existing%20methods%20rarely%20achieve%20satisfactory%20registration%0Aaccuracy%20while%20maintaining%20real-time%20performance.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20framework%20that%20projects%20point%20clouds%20into%20several%202D%20representations%0Afor%20matching%20with%20camera%20images%2C%20which%20not%20only%20leverages%20the%20geometric%0Acharacteristic%20of%20LiDAR%20point%20clouds%20effectively%20but%20also%20bridge%20the%20domain%20gap%0Abetween%20the%20point%20cloud%20and%20image.%20Moreover%2C%20to%20tackle%20the%20challenges%20of%20cross%0Amodal%20differences%20and%20the%20limited%20overlap%20between%20LiDAR%20point%20clouds%20and%20images%0Ain%20the%20image%20matching%20task%2C%20we%20introduce%20a%20multi-scale%20feature%20extraction%0Anetwork%20to%20effectively%20extract%20features%20from%20both%20camera%20images%20and%20the%0Aprojection%20maps%20of%20LiDAR%20point%20cloud.%20Additionally%2C%20we%20propose%20a%20patch-to-pixel%0Amatching%20network%20to%20provide%20more%20effective%20supervision%20and%20achieve%20high%0Aaccuracy.%20We%20validate%20the%20performance%20of%20our%20model%20through%20experiments%20on%20the%0AKITTI%20and%20nuScenes%20datasets.%20Experimental%20results%20demonstrate%20the%20the%20proposed%0Amethod%20achieves%20real-time%20performance%20and%20extremely%20high%20registration%20accuracy.%0ASpecifically%2C%20on%20the%20KITTI%20dataset%2C%20our%20model%20achieves%20a%20registration%20accuracy%0Arate%20of%20over%2099%5C%25.%20Our%20code%20is%20released%20at%3A%0Ahttps%3A//github.com/ESRSchao/EEPNet-V2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.15285v2&entry.124074799=Read"},
{"title": "RAPS-3D: Efficient interactive segmentation for 3D radiological imaging", "author": "Th\u00e9o Danielou and Daniel Tordjman and Pierre Manceron and Corentin Dancette", "abstract": "  Promptable segmentation, introduced by the Segment Anything Model (SAM), is a\npromising approach for medical imaging, as it enables clinicians to guide and\nrefine model predictions interactively. However, SAM's architecture is designed\nfor 2D images and does not extend naturally to 3D volumetric data such as CT or\nMRI scans. Adapting 2D models to 3D typically involves autoregressive\nstrategies, where predictions are propagated slice by slice, resulting in\nincreased inference complexity. Processing large 3D volumes also requires\nsignificant computational resources, often leading existing 3D methods to also\nadopt complex strategies like sliding-window inference to manage memory usage,\nat the cost of longer inference times and greater implementation complexity. In\nthis paper, we present a simplified 3D promptable segmentation method, inspired\nby SegVol, designed to reduce inference time and eliminate prompt management\ncomplexities associated with sliding windows while achieving state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2507.07730v1", "date": "2025-07-10", "relevancy": 2.9832, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6072}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6072}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAPS-3D%3A%20Efficient%20interactive%20segmentation%20for%203D%20radiological%20imaging&body=Title%3A%20RAPS-3D%3A%20Efficient%20interactive%20segmentation%20for%203D%20radiological%20imaging%0AAuthor%3A%20Th%C3%A9o%20Danielou%20and%20Daniel%20Tordjman%20and%20Pierre%20Manceron%20and%20Corentin%20Dancette%0AAbstract%3A%20%20%20Promptable%20segmentation%2C%20introduced%20by%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%20is%20a%0Apromising%20approach%20for%20medical%20imaging%2C%20as%20it%20enables%20clinicians%20to%20guide%20and%0Arefine%20model%20predictions%20interactively.%20However%2C%20SAM%27s%20architecture%20is%20designed%0Afor%202D%20images%20and%20does%20not%20extend%20naturally%20to%203D%20volumetric%20data%20such%20as%20CT%20or%0AMRI%20scans.%20Adapting%202D%20models%20to%203D%20typically%20involves%20autoregressive%0Astrategies%2C%20where%20predictions%20are%20propagated%20slice%20by%20slice%2C%20resulting%20in%0Aincreased%20inference%20complexity.%20Processing%20large%203D%20volumes%20also%20requires%0Asignificant%20computational%20resources%2C%20often%20leading%20existing%203D%20methods%20to%20also%0Aadopt%20complex%20strategies%20like%20sliding-window%20inference%20to%20manage%20memory%20usage%2C%0Aat%20the%20cost%20of%20longer%20inference%20times%20and%20greater%20implementation%20complexity.%20In%0Athis%20paper%2C%20we%20present%20a%20simplified%203D%20promptable%20segmentation%20method%2C%20inspired%0Aby%20SegVol%2C%20designed%20to%20reduce%20inference%20time%20and%20eliminate%20prompt%20management%0Acomplexities%20associated%20with%20sliding%20windows%20while%20achieving%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAPS-3D%253A%2520Efficient%2520interactive%2520segmentation%2520for%25203D%2520radiological%2520imaging%26entry.906535625%3DTh%25C3%25A9o%2520Danielou%2520and%2520Daniel%2520Tordjman%2520and%2520Pierre%2520Manceron%2520and%2520Corentin%2520Dancette%26entry.1292438233%3D%2520%2520Promptable%2520segmentation%252C%2520introduced%2520by%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520is%2520a%250Apromising%2520approach%2520for%2520medical%2520imaging%252C%2520as%2520it%2520enables%2520clinicians%2520to%2520guide%2520and%250Arefine%2520model%2520predictions%2520interactively.%2520However%252C%2520SAM%2527s%2520architecture%2520is%2520designed%250Afor%25202D%2520images%2520and%2520does%2520not%2520extend%2520naturally%2520to%25203D%2520volumetric%2520data%2520such%2520as%2520CT%2520or%250AMRI%2520scans.%2520Adapting%25202D%2520models%2520to%25203D%2520typically%2520involves%2520autoregressive%250Astrategies%252C%2520where%2520predictions%2520are%2520propagated%2520slice%2520by%2520slice%252C%2520resulting%2520in%250Aincreased%2520inference%2520complexity.%2520Processing%2520large%25203D%2520volumes%2520also%2520requires%250Asignificant%2520computational%2520resources%252C%2520often%2520leading%2520existing%25203D%2520methods%2520to%2520also%250Aadopt%2520complex%2520strategies%2520like%2520sliding-window%2520inference%2520to%2520manage%2520memory%2520usage%252C%250Aat%2520the%2520cost%2520of%2520longer%2520inference%2520times%2520and%2520greater%2520implementation%2520complexity.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520simplified%25203D%2520promptable%2520segmentation%2520method%252C%2520inspired%250Aby%2520SegVol%252C%2520designed%2520to%2520reduce%2520inference%2520time%2520and%2520eliminate%2520prompt%2520management%250Acomplexities%2520associated%2520with%2520sliding%2520windows%2520while%2520achieving%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAPS-3D%3A%20Efficient%20interactive%20segmentation%20for%203D%20radiological%20imaging&entry.906535625=Th%C3%A9o%20Danielou%20and%20Daniel%20Tordjman%20and%20Pierre%20Manceron%20and%20Corentin%20Dancette&entry.1292438233=%20%20Promptable%20segmentation%2C%20introduced%20by%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%20is%20a%0Apromising%20approach%20for%20medical%20imaging%2C%20as%20it%20enables%20clinicians%20to%20guide%20and%0Arefine%20model%20predictions%20interactively.%20However%2C%20SAM%27s%20architecture%20is%20designed%0Afor%202D%20images%20and%20does%20not%20extend%20naturally%20to%203D%20volumetric%20data%20such%20as%20CT%20or%0AMRI%20scans.%20Adapting%202D%20models%20to%203D%20typically%20involves%20autoregressive%0Astrategies%2C%20where%20predictions%20are%20propagated%20slice%20by%20slice%2C%20resulting%20in%0Aincreased%20inference%20complexity.%20Processing%20large%203D%20volumes%20also%20requires%0Asignificant%20computational%20resources%2C%20often%20leading%20existing%203D%20methods%20to%20also%0Aadopt%20complex%20strategies%20like%20sliding-window%20inference%20to%20manage%20memory%20usage%2C%0Aat%20the%20cost%20of%20longer%20inference%20times%20and%20greater%20implementation%20complexity.%20In%0Athis%20paper%2C%20we%20present%20a%20simplified%203D%20promptable%20segmentation%20method%2C%20inspired%0Aby%20SegVol%2C%20designed%20to%20reduce%20inference%20time%20and%20eliminate%20prompt%20management%0Acomplexities%20associated%20with%20sliding%20windows%20while%20achieving%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07730v1&entry.124074799=Read"},
{"title": "FunHOI: Annotation-Free 3D Hand-Object Interaction Generation via\n  Functional Text Guidanc", "author": "Yongqi Tian and Xueyu Sun and Haoyuan He and Linji Hao and Ning Ding and Caigui Jiang", "abstract": "  Hand-object interaction(HOI) is the fundamental link between human and\nenvironment, yet its dexterous and complex pose significantly challenges for\ngesture control. Despite significant advances in AI and robotics, enabling\nmachines to understand and simulate hand-object interactions, capturing the\nsemantics of functional grasping tasks remains a considerable challenge. While\nprevious work can generate stable and correct 3D grasps, they are still far\nfrom achieving functional grasps due to unconsidered grasp semantics. To\naddress this challenge, we propose an innovative two-stage framework,\nFunctional Grasp Synthesis Net (FGS-Net), for generating 3D HOI driven by\nfunctional text. This framework consists of a text-guided 3D model generator,\nFunctional Grasp Generator (FGG), and a pose optimization strategy, Functional\nGrasp Refiner (FGR). FGG generates 3D models of hands and objects based on text\ninput, while FGR fine-tunes the poses using Object Pose Approximator and energy\nfunctions to ensure the relative position between the hand and object aligns\nwith human intent and remains physically plausible. Extensive experiments\ndemonstrate that our approach achieves precise and high-quality HOI generation\nwithout requiring additional 3D annotation data.\n", "link": "http://arxiv.org/abs/2502.20805v2", "date": "2025-07-10", "relevancy": 2.9715, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6046}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5931}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FunHOI%3A%20Annotation-Free%203D%20Hand-Object%20Interaction%20Generation%20via%0A%20%20Functional%20Text%20Guidanc&body=Title%3A%20FunHOI%3A%20Annotation-Free%203D%20Hand-Object%20Interaction%20Generation%20via%0A%20%20Functional%20Text%20Guidanc%0AAuthor%3A%20Yongqi%20Tian%20and%20Xueyu%20Sun%20and%20Haoyuan%20He%20and%20Linji%20Hao%20and%20Ning%20Ding%20and%20Caigui%20Jiang%0AAbstract%3A%20%20%20Hand-object%20interaction%28HOI%29%20is%20the%20fundamental%20link%20between%20human%20and%0Aenvironment%2C%20yet%20its%20dexterous%20and%20complex%20pose%20significantly%20challenges%20for%0Agesture%20control.%20Despite%20significant%20advances%20in%20AI%20and%20robotics%2C%20enabling%0Amachines%20to%20understand%20and%20simulate%20hand-object%20interactions%2C%20capturing%20the%0Asemantics%20of%20functional%20grasping%20tasks%20remains%20a%20considerable%20challenge.%20While%0Aprevious%20work%20can%20generate%20stable%20and%20correct%203D%20grasps%2C%20they%20are%20still%20far%0Afrom%20achieving%20functional%20grasps%20due%20to%20unconsidered%20grasp%20semantics.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20an%20innovative%20two-stage%20framework%2C%0AFunctional%20Grasp%20Synthesis%20Net%20%28FGS-Net%29%2C%20for%20generating%203D%20HOI%20driven%20by%0Afunctional%20text.%20This%20framework%20consists%20of%20a%20text-guided%203D%20model%20generator%2C%0AFunctional%20Grasp%20Generator%20%28FGG%29%2C%20and%20a%20pose%20optimization%20strategy%2C%20Functional%0AGrasp%20Refiner%20%28FGR%29.%20FGG%20generates%203D%20models%20of%20hands%20and%20objects%20based%20on%20text%0Ainput%2C%20while%20FGR%20fine-tunes%20the%20poses%20using%20Object%20Pose%20Approximator%20and%20energy%0Afunctions%20to%20ensure%20the%20relative%20position%20between%20the%20hand%20and%20object%20aligns%0Awith%20human%20intent%20and%20remains%20physically%20plausible.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20achieves%20precise%20and%20high-quality%20HOI%20generation%0Awithout%20requiring%20additional%203D%20annotation%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20805v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunHOI%253A%2520Annotation-Free%25203D%2520Hand-Object%2520Interaction%2520Generation%2520via%250A%2520%2520Functional%2520Text%2520Guidanc%26entry.906535625%3DYongqi%2520Tian%2520and%2520Xueyu%2520Sun%2520and%2520Haoyuan%2520He%2520and%2520Linji%2520Hao%2520and%2520Ning%2520Ding%2520and%2520Caigui%2520Jiang%26entry.1292438233%3D%2520%2520Hand-object%2520interaction%2528HOI%2529%2520is%2520the%2520fundamental%2520link%2520between%2520human%2520and%250Aenvironment%252C%2520yet%2520its%2520dexterous%2520and%2520complex%2520pose%2520significantly%2520challenges%2520for%250Agesture%2520control.%2520Despite%2520significant%2520advances%2520in%2520AI%2520and%2520robotics%252C%2520enabling%250Amachines%2520to%2520understand%2520and%2520simulate%2520hand-object%2520interactions%252C%2520capturing%2520the%250Asemantics%2520of%2520functional%2520grasping%2520tasks%2520remains%2520a%2520considerable%2520challenge.%2520While%250Aprevious%2520work%2520can%2520generate%2520stable%2520and%2520correct%25203D%2520grasps%252C%2520they%2520are%2520still%2520far%250Afrom%2520achieving%2520functional%2520grasps%2520due%2520to%2520unconsidered%2520grasp%2520semantics.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520an%2520innovative%2520two-stage%2520framework%252C%250AFunctional%2520Grasp%2520Synthesis%2520Net%2520%2528FGS-Net%2529%252C%2520for%2520generating%25203D%2520HOI%2520driven%2520by%250Afunctional%2520text.%2520This%2520framework%2520consists%2520of%2520a%2520text-guided%25203D%2520model%2520generator%252C%250AFunctional%2520Grasp%2520Generator%2520%2528FGG%2529%252C%2520and%2520a%2520pose%2520optimization%2520strategy%252C%2520Functional%250AGrasp%2520Refiner%2520%2528FGR%2529.%2520FGG%2520generates%25203D%2520models%2520of%2520hands%2520and%2520objects%2520based%2520on%2520text%250Ainput%252C%2520while%2520FGR%2520fine-tunes%2520the%2520poses%2520using%2520Object%2520Pose%2520Approximator%2520and%2520energy%250Afunctions%2520to%2520ensure%2520the%2520relative%2520position%2520between%2520the%2520hand%2520and%2520object%2520aligns%250Awith%2520human%2520intent%2520and%2520remains%2520physically%2520plausible.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520approach%2520achieves%2520precise%2520and%2520high-quality%2520HOI%2520generation%250Awithout%2520requiring%2520additional%25203D%2520annotation%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20805v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FunHOI%3A%20Annotation-Free%203D%20Hand-Object%20Interaction%20Generation%20via%0A%20%20Functional%20Text%20Guidanc&entry.906535625=Yongqi%20Tian%20and%20Xueyu%20Sun%20and%20Haoyuan%20He%20and%20Linji%20Hao%20and%20Ning%20Ding%20and%20Caigui%20Jiang&entry.1292438233=%20%20Hand-object%20interaction%28HOI%29%20is%20the%20fundamental%20link%20between%20human%20and%0Aenvironment%2C%20yet%20its%20dexterous%20and%20complex%20pose%20significantly%20challenges%20for%0Agesture%20control.%20Despite%20significant%20advances%20in%20AI%20and%20robotics%2C%20enabling%0Amachines%20to%20understand%20and%20simulate%20hand-object%20interactions%2C%20capturing%20the%0Asemantics%20of%20functional%20grasping%20tasks%20remains%20a%20considerable%20challenge.%20While%0Aprevious%20work%20can%20generate%20stable%20and%20correct%203D%20grasps%2C%20they%20are%20still%20far%0Afrom%20achieving%20functional%20grasps%20due%20to%20unconsidered%20grasp%20semantics.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20an%20innovative%20two-stage%20framework%2C%0AFunctional%20Grasp%20Synthesis%20Net%20%28FGS-Net%29%2C%20for%20generating%203D%20HOI%20driven%20by%0Afunctional%20text.%20This%20framework%20consists%20of%20a%20text-guided%203D%20model%20generator%2C%0AFunctional%20Grasp%20Generator%20%28FGG%29%2C%20and%20a%20pose%20optimization%20strategy%2C%20Functional%0AGrasp%20Refiner%20%28FGR%29.%20FGG%20generates%203D%20models%20of%20hands%20and%20objects%20based%20on%20text%0Ainput%2C%20while%20FGR%20fine-tunes%20the%20poses%20using%20Object%20Pose%20Approximator%20and%20energy%0Afunctions%20to%20ensure%20the%20relative%20position%20between%20the%20hand%20and%20object%20aligns%0Awith%20human%20intent%20and%20remains%20physically%20plausible.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20achieves%20precise%20and%20high-quality%20HOI%20generation%0Awithout%20requiring%20additional%203D%20annotation%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20805v2&entry.124074799=Read"},
{"title": "IRAF-SLAM: An Illumination-Robust and Adaptive Feature-Culling Front-End\n  for Visual SLAM in Challenging Environments", "author": "Thanh Nguyen Canh and Bao Nguyen Quoc and Haolan Zhang and Bupesh Rethinam Veeraiah and Xiem HoangVan and Nak Young Chong", "abstract": "  Robust Visual SLAM (vSLAM) is essential for autonomous systems operating in\nreal-world environments, where challenges such as dynamic objects, low texture,\nand critically, varying illumination conditions often degrade performance.\nExisting feature-based SLAM systems rely on fixed front-end parameters, making\nthem vulnerable to sudden lighting changes and unstable feature tracking. To\naddress these challenges, we propose ``IRAF-SLAM'', an Illumination-Robust and\nAdaptive Feature-Culling front-end designed to enhance vSLAM resilience in\ncomplex and challenging environments. Our approach introduces: (1) an image\nenhancement scheme to preprocess and adjust image quality under varying\nlighting conditions; (2) an adaptive feature extraction mechanism that\ndynamically adjusts detection sensitivity based on image entropy, pixel\nintensity, and gradient analysis; and (3) a feature culling strategy that\nfilters out unreliable feature points using density distribution analysis and a\nlighting impact factor. Comprehensive evaluations on the TUM-VI and European\nRobotics Challenge (EuRoC) datasets demonstrate that IRAF-SLAM significantly\nreduces tracking failures and achieves superior trajectory accuracy compared to\nstate-of-the-art vSLAM methods under adverse illumination conditions. These\nresults highlight the effectiveness of adaptive front-end strategies in\nimproving vSLAM robustness without incurring significant computational\noverhead. The implementation of IRAF-SLAM is publicly available at\nhttps://thanhnguyencanh. github.io/IRAF-SLAM/.\n", "link": "http://arxiv.org/abs/2507.07752v1", "date": "2025-07-10", "relevancy": 2.9634, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6423}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5912}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRAF-SLAM%3A%20An%20Illumination-Robust%20and%20Adaptive%20Feature-Culling%20Front-End%0A%20%20for%20Visual%20SLAM%20in%20Challenging%20Environments&body=Title%3A%20IRAF-SLAM%3A%20An%20Illumination-Robust%20and%20Adaptive%20Feature-Culling%20Front-End%0A%20%20for%20Visual%20SLAM%20in%20Challenging%20Environments%0AAuthor%3A%20Thanh%20Nguyen%20Canh%20and%20Bao%20Nguyen%20Quoc%20and%20Haolan%20Zhang%20and%20Bupesh%20Rethinam%20Veeraiah%20and%20Xiem%20HoangVan%20and%20Nak%20Young%20Chong%0AAbstract%3A%20%20%20Robust%20Visual%20SLAM%20%28vSLAM%29%20is%20essential%20for%20autonomous%20systems%20operating%20in%0Areal-world%20environments%2C%20where%20challenges%20such%20as%20dynamic%20objects%2C%20low%20texture%2C%0Aand%20critically%2C%20varying%20illumination%20conditions%20often%20degrade%20performance.%0AExisting%20feature-based%20SLAM%20systems%20rely%20on%20fixed%20front-end%20parameters%2C%20making%0Athem%20vulnerable%20to%20sudden%20lighting%20changes%20and%20unstable%20feature%20tracking.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20%60%60IRAF-SLAM%27%27%2C%20an%20Illumination-Robust%20and%0AAdaptive%20Feature-Culling%20front-end%20designed%20to%20enhance%20vSLAM%20resilience%20in%0Acomplex%20and%20challenging%20environments.%20Our%20approach%20introduces%3A%20%281%29%20an%20image%0Aenhancement%20scheme%20to%20preprocess%20and%20adjust%20image%20quality%20under%20varying%0Alighting%20conditions%3B%20%282%29%20an%20adaptive%20feature%20extraction%20mechanism%20that%0Adynamically%20adjusts%20detection%20sensitivity%20based%20on%20image%20entropy%2C%20pixel%0Aintensity%2C%20and%20gradient%20analysis%3B%20and%20%283%29%20a%20feature%20culling%20strategy%20that%0Afilters%20out%20unreliable%20feature%20points%20using%20density%20distribution%20analysis%20and%20a%0Alighting%20impact%20factor.%20Comprehensive%20evaluations%20on%20the%20TUM-VI%20and%20European%0ARobotics%20Challenge%20%28EuRoC%29%20datasets%20demonstrate%20that%20IRAF-SLAM%20significantly%0Areduces%20tracking%20failures%20and%20achieves%20superior%20trajectory%20accuracy%20compared%20to%0Astate-of-the-art%20vSLAM%20methods%20under%20adverse%20illumination%20conditions.%20These%0Aresults%20highlight%20the%20effectiveness%20of%20adaptive%20front-end%20strategies%20in%0Aimproving%20vSLAM%20robustness%20without%20incurring%20significant%20computational%0Aoverhead.%20The%20implementation%20of%20IRAF-SLAM%20is%20publicly%20available%20at%0Ahttps%3A//thanhnguyencanh.%20github.io/IRAF-SLAM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRAF-SLAM%253A%2520An%2520Illumination-Robust%2520and%2520Adaptive%2520Feature-Culling%2520Front-End%250A%2520%2520for%2520Visual%2520SLAM%2520in%2520Challenging%2520Environments%26entry.906535625%3DThanh%2520Nguyen%2520Canh%2520and%2520Bao%2520Nguyen%2520Quoc%2520and%2520Haolan%2520Zhang%2520and%2520Bupesh%2520Rethinam%2520Veeraiah%2520and%2520Xiem%2520HoangVan%2520and%2520Nak%2520Young%2520Chong%26entry.1292438233%3D%2520%2520Robust%2520Visual%2520SLAM%2520%2528vSLAM%2529%2520is%2520essential%2520for%2520autonomous%2520systems%2520operating%2520in%250Areal-world%2520environments%252C%2520where%2520challenges%2520such%2520as%2520dynamic%2520objects%252C%2520low%2520texture%252C%250Aand%2520critically%252C%2520varying%2520illumination%2520conditions%2520often%2520degrade%2520performance.%250AExisting%2520feature-based%2520SLAM%2520systems%2520rely%2520on%2520fixed%2520front-end%2520parameters%252C%2520making%250Athem%2520vulnerable%2520to%2520sudden%2520lighting%2520changes%2520and%2520unstable%2520feature%2520tracking.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520%2560%2560IRAF-SLAM%2527%2527%252C%2520an%2520Illumination-Robust%2520and%250AAdaptive%2520Feature-Culling%2520front-end%2520designed%2520to%2520enhance%2520vSLAM%2520resilience%2520in%250Acomplex%2520and%2520challenging%2520environments.%2520Our%2520approach%2520introduces%253A%2520%25281%2529%2520an%2520image%250Aenhancement%2520scheme%2520to%2520preprocess%2520and%2520adjust%2520image%2520quality%2520under%2520varying%250Alighting%2520conditions%253B%2520%25282%2529%2520an%2520adaptive%2520feature%2520extraction%2520mechanism%2520that%250Adynamically%2520adjusts%2520detection%2520sensitivity%2520based%2520on%2520image%2520entropy%252C%2520pixel%250Aintensity%252C%2520and%2520gradient%2520analysis%253B%2520and%2520%25283%2529%2520a%2520feature%2520culling%2520strategy%2520that%250Afilters%2520out%2520unreliable%2520feature%2520points%2520using%2520density%2520distribution%2520analysis%2520and%2520a%250Alighting%2520impact%2520factor.%2520Comprehensive%2520evaluations%2520on%2520the%2520TUM-VI%2520and%2520European%250ARobotics%2520Challenge%2520%2528EuRoC%2529%2520datasets%2520demonstrate%2520that%2520IRAF-SLAM%2520significantly%250Areduces%2520tracking%2520failures%2520and%2520achieves%2520superior%2520trajectory%2520accuracy%2520compared%2520to%250Astate-of-the-art%2520vSLAM%2520methods%2520under%2520adverse%2520illumination%2520conditions.%2520These%250Aresults%2520highlight%2520the%2520effectiveness%2520of%2520adaptive%2520front-end%2520strategies%2520in%250Aimproving%2520vSLAM%2520robustness%2520without%2520incurring%2520significant%2520computational%250Aoverhead.%2520The%2520implementation%2520of%2520IRAF-SLAM%2520is%2520publicly%2520available%2520at%250Ahttps%253A//thanhnguyencanh.%2520github.io/IRAF-SLAM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRAF-SLAM%3A%20An%20Illumination-Robust%20and%20Adaptive%20Feature-Culling%20Front-End%0A%20%20for%20Visual%20SLAM%20in%20Challenging%20Environments&entry.906535625=Thanh%20Nguyen%20Canh%20and%20Bao%20Nguyen%20Quoc%20and%20Haolan%20Zhang%20and%20Bupesh%20Rethinam%20Veeraiah%20and%20Xiem%20HoangVan%20and%20Nak%20Young%20Chong&entry.1292438233=%20%20Robust%20Visual%20SLAM%20%28vSLAM%29%20is%20essential%20for%20autonomous%20systems%20operating%20in%0Areal-world%20environments%2C%20where%20challenges%20such%20as%20dynamic%20objects%2C%20low%20texture%2C%0Aand%20critically%2C%20varying%20illumination%20conditions%20often%20degrade%20performance.%0AExisting%20feature-based%20SLAM%20systems%20rely%20on%20fixed%20front-end%20parameters%2C%20making%0Athem%20vulnerable%20to%20sudden%20lighting%20changes%20and%20unstable%20feature%20tracking.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20%60%60IRAF-SLAM%27%27%2C%20an%20Illumination-Robust%20and%0AAdaptive%20Feature-Culling%20front-end%20designed%20to%20enhance%20vSLAM%20resilience%20in%0Acomplex%20and%20challenging%20environments.%20Our%20approach%20introduces%3A%20%281%29%20an%20image%0Aenhancement%20scheme%20to%20preprocess%20and%20adjust%20image%20quality%20under%20varying%0Alighting%20conditions%3B%20%282%29%20an%20adaptive%20feature%20extraction%20mechanism%20that%0Adynamically%20adjusts%20detection%20sensitivity%20based%20on%20image%20entropy%2C%20pixel%0Aintensity%2C%20and%20gradient%20analysis%3B%20and%20%283%29%20a%20feature%20culling%20strategy%20that%0Afilters%20out%20unreliable%20feature%20points%20using%20density%20distribution%20analysis%20and%20a%0Alighting%20impact%20factor.%20Comprehensive%20evaluations%20on%20the%20TUM-VI%20and%20European%0ARobotics%20Challenge%20%28EuRoC%29%20datasets%20demonstrate%20that%20IRAF-SLAM%20significantly%0Areduces%20tracking%20failures%20and%20achieves%20superior%20trajectory%20accuracy%20compared%20to%0Astate-of-the-art%20vSLAM%20methods%20under%20adverse%20illumination%20conditions.%20These%0Aresults%20highlight%20the%20effectiveness%20of%20adaptive%20front-end%20strategies%20in%0Aimproving%20vSLAM%20robustness%20without%20incurring%20significant%20computational%0Aoverhead.%20The%20implementation%20of%20IRAF-SLAM%20is%20publicly%20available%20at%0Ahttps%3A//thanhnguyencanh.%20github.io/IRAF-SLAM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07752v1&entry.124074799=Read"},
{"title": "Sparse-Dense Side-Tuner for efficient Video Temporal Grounding", "author": "David Pujol-Perich and Sergio Escalera and Albert Clap\u00e9s", "abstract": "  Video Temporal Grounding (VTG) involves Moment Retrieval (MR) and Highlight\nDetection (HD) based on textual queries. For this, most methods rely solely on\nfinal-layer features of frozen large pre-trained backbones, limiting their\nadaptability to new domains. While full fine-tuning is often impractical,\nparameter-efficient fine-tuning -- and particularly side-tuning (ST) -- has\nemerged as an effective alternative. However, prior ST approaches this problem\nfrom a frame-level refinement perspective, overlooking the inherent sparse\nnature of MR. To address this, we propose the Sparse-Dense Side-Tuner (SDST),\nthe first anchor-free ST architecture for VTG. We also introduce the\nReference-based Deformable Self-Attention, a novel mechanism that enhances the\ncontext modeling of the deformable attention -- a key limitation of existing\nanchor-free methods. Additionally, we present the first effective integration\nof InternVideo2 backbone into an ST framework, showing its profound\nimplications in performance. Overall, our method significantly improves\nexisting ST methods, achieving highly competitive or SOTA results on\nQVHighlights, TACoS, and Charades-STA, while reducing up to a 73% the parameter\ncount w.r.t. the existing SOTA methods. The code is publicly accessible at\nhttps://github.com/davidpujol/SDST.\n", "link": "http://arxiv.org/abs/2507.07744v1", "date": "2025-07-10", "relevancy": 2.9416, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5936}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5867}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse-Dense%20Side-Tuner%20for%20efficient%20Video%20Temporal%20Grounding&body=Title%3A%20Sparse-Dense%20Side-Tuner%20for%20efficient%20Video%20Temporal%20Grounding%0AAuthor%3A%20David%20Pujol-Perich%20and%20Sergio%20Escalera%20and%20Albert%20Clap%C3%A9s%0AAbstract%3A%20%20%20Video%20Temporal%20Grounding%20%28VTG%29%20involves%20Moment%20Retrieval%20%28MR%29%20and%20Highlight%0ADetection%20%28HD%29%20based%20on%20textual%20queries.%20For%20this%2C%20most%20methods%20rely%20solely%20on%0Afinal-layer%20features%20of%20frozen%20large%20pre-trained%20backbones%2C%20limiting%20their%0Aadaptability%20to%20new%20domains.%20While%20full%20fine-tuning%20is%20often%20impractical%2C%0Aparameter-efficient%20fine-tuning%20--%20and%20particularly%20side-tuning%20%28ST%29%20--%20has%0Aemerged%20as%20an%20effective%20alternative.%20However%2C%20prior%20ST%20approaches%20this%20problem%0Afrom%20a%20frame-level%20refinement%20perspective%2C%20overlooking%20the%20inherent%20sparse%0Anature%20of%20MR.%20To%20address%20this%2C%20we%20propose%20the%20Sparse-Dense%20Side-Tuner%20%28SDST%29%2C%0Athe%20first%20anchor-free%20ST%20architecture%20for%20VTG.%20We%20also%20introduce%20the%0AReference-based%20Deformable%20Self-Attention%2C%20a%20novel%20mechanism%20that%20enhances%20the%0Acontext%20modeling%20of%20the%20deformable%20attention%20--%20a%20key%20limitation%20of%20existing%0Aanchor-free%20methods.%20Additionally%2C%20we%20present%20the%20first%20effective%20integration%0Aof%20InternVideo2%20backbone%20into%20an%20ST%20framework%2C%20showing%20its%20profound%0Aimplications%20in%20performance.%20Overall%2C%20our%20method%20significantly%20improves%0Aexisting%20ST%20methods%2C%20achieving%20highly%20competitive%20or%20SOTA%20results%20on%0AQVHighlights%2C%20TACoS%2C%20and%20Charades-STA%2C%20while%20reducing%20up%20to%20a%2073%25%20the%20parameter%0Acount%20w.r.t.%20the%20existing%20SOTA%20methods.%20The%20code%20is%20publicly%20accessible%20at%0Ahttps%3A//github.com/davidpujol/SDST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse-Dense%2520Side-Tuner%2520for%2520efficient%2520Video%2520Temporal%2520Grounding%26entry.906535625%3DDavid%2520Pujol-Perich%2520and%2520Sergio%2520Escalera%2520and%2520Albert%2520Clap%25C3%25A9s%26entry.1292438233%3D%2520%2520Video%2520Temporal%2520Grounding%2520%2528VTG%2529%2520involves%2520Moment%2520Retrieval%2520%2528MR%2529%2520and%2520Highlight%250ADetection%2520%2528HD%2529%2520based%2520on%2520textual%2520queries.%2520For%2520this%252C%2520most%2520methods%2520rely%2520solely%2520on%250Afinal-layer%2520features%2520of%2520frozen%2520large%2520pre-trained%2520backbones%252C%2520limiting%2520their%250Aadaptability%2520to%2520new%2520domains.%2520While%2520full%2520fine-tuning%2520is%2520often%2520impractical%252C%250Aparameter-efficient%2520fine-tuning%2520--%2520and%2520particularly%2520side-tuning%2520%2528ST%2529%2520--%2520has%250Aemerged%2520as%2520an%2520effective%2520alternative.%2520However%252C%2520prior%2520ST%2520approaches%2520this%2520problem%250Afrom%2520a%2520frame-level%2520refinement%2520perspective%252C%2520overlooking%2520the%2520inherent%2520sparse%250Anature%2520of%2520MR.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Sparse-Dense%2520Side-Tuner%2520%2528SDST%2529%252C%250Athe%2520first%2520anchor-free%2520ST%2520architecture%2520for%2520VTG.%2520We%2520also%2520introduce%2520the%250AReference-based%2520Deformable%2520Self-Attention%252C%2520a%2520novel%2520mechanism%2520that%2520enhances%2520the%250Acontext%2520modeling%2520of%2520the%2520deformable%2520attention%2520--%2520a%2520key%2520limitation%2520of%2520existing%250Aanchor-free%2520methods.%2520Additionally%252C%2520we%2520present%2520the%2520first%2520effective%2520integration%250Aof%2520InternVideo2%2520backbone%2520into%2520an%2520ST%2520framework%252C%2520showing%2520its%2520profound%250Aimplications%2520in%2520performance.%2520Overall%252C%2520our%2520method%2520significantly%2520improves%250Aexisting%2520ST%2520methods%252C%2520achieving%2520highly%2520competitive%2520or%2520SOTA%2520results%2520on%250AQVHighlights%252C%2520TACoS%252C%2520and%2520Charades-STA%252C%2520while%2520reducing%2520up%2520to%2520a%252073%2525%2520the%2520parameter%250Acount%2520w.r.t.%2520the%2520existing%2520SOTA%2520methods.%2520The%2520code%2520is%2520publicly%2520accessible%2520at%250Ahttps%253A//github.com/davidpujol/SDST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse-Dense%20Side-Tuner%20for%20efficient%20Video%20Temporal%20Grounding&entry.906535625=David%20Pujol-Perich%20and%20Sergio%20Escalera%20and%20Albert%20Clap%C3%A9s&entry.1292438233=%20%20Video%20Temporal%20Grounding%20%28VTG%29%20involves%20Moment%20Retrieval%20%28MR%29%20and%20Highlight%0ADetection%20%28HD%29%20based%20on%20textual%20queries.%20For%20this%2C%20most%20methods%20rely%20solely%20on%0Afinal-layer%20features%20of%20frozen%20large%20pre-trained%20backbones%2C%20limiting%20their%0Aadaptability%20to%20new%20domains.%20While%20full%20fine-tuning%20is%20often%20impractical%2C%0Aparameter-efficient%20fine-tuning%20--%20and%20particularly%20side-tuning%20%28ST%29%20--%20has%0Aemerged%20as%20an%20effective%20alternative.%20However%2C%20prior%20ST%20approaches%20this%20problem%0Afrom%20a%20frame-level%20refinement%20perspective%2C%20overlooking%20the%20inherent%20sparse%0Anature%20of%20MR.%20To%20address%20this%2C%20we%20propose%20the%20Sparse-Dense%20Side-Tuner%20%28SDST%29%2C%0Athe%20first%20anchor-free%20ST%20architecture%20for%20VTG.%20We%20also%20introduce%20the%0AReference-based%20Deformable%20Self-Attention%2C%20a%20novel%20mechanism%20that%20enhances%20the%0Acontext%20modeling%20of%20the%20deformable%20attention%20--%20a%20key%20limitation%20of%20existing%0Aanchor-free%20methods.%20Additionally%2C%20we%20present%20the%20first%20effective%20integration%0Aof%20InternVideo2%20backbone%20into%20an%20ST%20framework%2C%20showing%20its%20profound%0Aimplications%20in%20performance.%20Overall%2C%20our%20method%20significantly%20improves%0Aexisting%20ST%20methods%2C%20achieving%20highly%20competitive%20or%20SOTA%20results%20on%0AQVHighlights%2C%20TACoS%2C%20and%20Charades-STA%2C%20while%20reducing%20up%20to%20a%2073%25%20the%20parameter%0Acount%20w.r.t.%20the%20existing%20SOTA%20methods.%20The%20code%20is%20publicly%20accessible%20at%0Ahttps%3A//github.com/davidpujol/SDST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07744v1&entry.124074799=Read"},
{"title": "STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs", "author": "Zongzhao Li and Zongyang Ma and Mingze Li and Songyou Li and Yu Rong and Tingyang Xu and Ziqi Zhang and Deli Zhao and Wenbing Huang", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1.\n", "link": "http://arxiv.org/abs/2505.15804v3", "date": "2025-07-10", "relevancy": 2.8751, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAR-R1%3A%20Spatial%20TrAnsformation%20Reasoning%20by%20Reinforcing%20Multimodal%20LLMs&body=Title%3A%20STAR-R1%3A%20Spatial%20TrAnsformation%20Reasoning%20by%20Reinforcing%20Multimodal%20LLMs%0AAuthor%3A%20Zongzhao%20Li%20and%20Zongyang%20Ma%20and%20Mingze%20Li%20and%20Songyou%20Li%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Ziqi%20Zhang%20and%20Deli%20Zhao%20and%20Wenbing%20Huang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20across%20diverse%20tasks%2C%20yet%20they%20lag%20significantly%20behind%20humans%20in%0Aspatial%20reasoning.%20We%20investigate%20this%20gap%20through%20Transformation-Driven%20Visual%0AReasoning%20%28TVR%29%2C%20a%20challenging%20task%20requiring%20identification%20of%20object%0Atransformations%20across%20images%20under%20varying%20viewpoints.%20While%20traditional%0ASupervised%20Fine-Tuning%20%28SFT%29%20fails%20to%20generate%20coherent%20reasoning%20paths%20in%0Across-view%20settings%2C%20sparse-reward%20Reinforcement%20Learning%20%28RL%29%20suffers%20from%0Ainefficient%20exploration%20and%20slow%20convergence.%20To%20address%20these%20limitations%2C%20we%0Apropose%20STAR-R1%2C%20a%20novel%20framework%20that%20integrates%20a%20single-stage%20RL%20paradigm%0Awith%20a%20fine-grained%20reward%20mechanism%20tailored%20for%20TVR.%20Specifically%2C%20STAR-R1%0Arewards%20partial%20correctness%20while%20penalizing%20excessive%20enumeration%20and%20passive%0Ainaction%2C%20enabling%20efficient%20exploration%20and%20precise%20reasoning.%20Comprehensive%0Aevaluations%20demonstrate%20that%20STAR-R1%20achieves%20state-of-the-art%20performance%0Aacross%20all%2011%20metrics%2C%20outperforming%20SFT%20by%2023%25%20in%20cross-view%20scenarios.%0AFurther%20analysis%20reveals%20STAR-R1%27s%20anthropomorphic%20behavior%20and%20highlights%20its%0Aunique%20ability%20to%20compare%20all%20objects%20for%20improving%20spatial%20reasoning.%20Our%20work%0Aprovides%20critical%20insights%20in%20advancing%20the%20research%20of%20MLLMs%20and%20reasoning%0Amodels.%20The%20codes%2C%20model%20weights%2C%20and%20data%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/zongzhao23/STAR-R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15804v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAR-R1%253A%2520Spatial%2520TrAnsformation%2520Reasoning%2520by%2520Reinforcing%2520Multimodal%2520LLMs%26entry.906535625%3DZongzhao%2520Li%2520and%2520Zongyang%2520Ma%2520and%2520Mingze%2520Li%2520and%2520Songyou%2520Li%2520and%2520Yu%2520Rong%2520and%2520Tingyang%2520Xu%2520and%2520Ziqi%2520Zhang%2520and%2520Deli%2520Zhao%2520and%2520Wenbing%2520Huang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520across%2520diverse%2520tasks%252C%2520yet%2520they%2520lag%2520significantly%2520behind%2520humans%2520in%250Aspatial%2520reasoning.%2520We%2520investigate%2520this%2520gap%2520through%2520Transformation-Driven%2520Visual%250AReasoning%2520%2528TVR%2529%252C%2520a%2520challenging%2520task%2520requiring%2520identification%2520of%2520object%250Atransformations%2520across%2520images%2520under%2520varying%2520viewpoints.%2520While%2520traditional%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%2520fails%2520to%2520generate%2520coherent%2520reasoning%2520paths%2520in%250Across-view%2520settings%252C%2520sparse-reward%2520Reinforcement%2520Learning%2520%2528RL%2529%2520suffers%2520from%250Ainefficient%2520exploration%2520and%2520slow%2520convergence.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520STAR-R1%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520a%2520single-stage%2520RL%2520paradigm%250Awith%2520a%2520fine-grained%2520reward%2520mechanism%2520tailored%2520for%2520TVR.%2520Specifically%252C%2520STAR-R1%250Arewards%2520partial%2520correctness%2520while%2520penalizing%2520excessive%2520enumeration%2520and%2520passive%250Ainaction%252C%2520enabling%2520efficient%2520exploration%2520and%2520precise%2520reasoning.%2520Comprehensive%250Aevaluations%2520demonstrate%2520that%2520STAR-R1%2520achieves%2520state-of-the-art%2520performance%250Aacross%2520all%252011%2520metrics%252C%2520outperforming%2520SFT%2520by%252023%2525%2520in%2520cross-view%2520scenarios.%250AFurther%2520analysis%2520reveals%2520STAR-R1%2527s%2520anthropomorphic%2520behavior%2520and%2520highlights%2520its%250Aunique%2520ability%2520to%2520compare%2520all%2520objects%2520for%2520improving%2520spatial%2520reasoning.%2520Our%2520work%250Aprovides%2520critical%2520insights%2520in%2520advancing%2520the%2520research%2520of%2520MLLMs%2520and%2520reasoning%250Amodels.%2520The%2520codes%252C%2520model%2520weights%252C%2520and%2520data%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/zongzhao23/STAR-R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15804v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAR-R1%3A%20Spatial%20TrAnsformation%20Reasoning%20by%20Reinforcing%20Multimodal%20LLMs&entry.906535625=Zongzhao%20Li%20and%20Zongyang%20Ma%20and%20Mingze%20Li%20and%20Songyou%20Li%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Ziqi%20Zhang%20and%20Deli%20Zhao%20and%20Wenbing%20Huang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20across%20diverse%20tasks%2C%20yet%20they%20lag%20significantly%20behind%20humans%20in%0Aspatial%20reasoning.%20We%20investigate%20this%20gap%20through%20Transformation-Driven%20Visual%0AReasoning%20%28TVR%29%2C%20a%20challenging%20task%20requiring%20identification%20of%20object%0Atransformations%20across%20images%20under%20varying%20viewpoints.%20While%20traditional%0ASupervised%20Fine-Tuning%20%28SFT%29%20fails%20to%20generate%20coherent%20reasoning%20paths%20in%0Across-view%20settings%2C%20sparse-reward%20Reinforcement%20Learning%20%28RL%29%20suffers%20from%0Ainefficient%20exploration%20and%20slow%20convergence.%20To%20address%20these%20limitations%2C%20we%0Apropose%20STAR-R1%2C%20a%20novel%20framework%20that%20integrates%20a%20single-stage%20RL%20paradigm%0Awith%20a%20fine-grained%20reward%20mechanism%20tailored%20for%20TVR.%20Specifically%2C%20STAR-R1%0Arewards%20partial%20correctness%20while%20penalizing%20excessive%20enumeration%20and%20passive%0Ainaction%2C%20enabling%20efficient%20exploration%20and%20precise%20reasoning.%20Comprehensive%0Aevaluations%20demonstrate%20that%20STAR-R1%20achieves%20state-of-the-art%20performance%0Aacross%20all%2011%20metrics%2C%20outperforming%20SFT%20by%2023%25%20in%20cross-view%20scenarios.%0AFurther%20analysis%20reveals%20STAR-R1%27s%20anthropomorphic%20behavior%20and%20highlights%20its%0Aunique%20ability%20to%20compare%20all%20objects%20for%20improving%20spatial%20reasoning.%20Our%20work%0Aprovides%20critical%20insights%20in%20advancing%20the%20research%20of%20MLLMs%20and%20reasoning%0Amodels.%20The%20codes%2C%20model%20weights%2C%20and%20data%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/zongzhao23/STAR-R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15804v3&entry.124074799=Read"},
{"title": "CLIP Won't Learn Object-Attribute Binding from Natural Data and Here is\n  Why", "author": "Bijay Gurung and David T. Hoffmann and Thomas Brox", "abstract": "  Contrastive vision-language models like CLIP are used for a large variety of\napplications, such as zero-shot classification or as vision encoder for\nmulti-modal models. Despite their popularity, their representations show major\nlimitations. For instance, CLIP models learn bag-of-words representations and,\nas a consequence, fail to distinguish whether an image is of \"a yellow\nsubmarine and a blue bus\" or \"a blue submarine and a yellow bus\". Previous\nattempts to fix this issue added hard negatives during training or modified the\narchitecture, but failed to resolve the problem in its entirety. We suspect\nthat the missing insights to solve the binding problem for CLIP are hidden in\nthe arguably most important part of learning algorithms: the data. In this\nwork, we fill this gap by rigorously identifying the influence of data\nproperties on CLIP's ability to learn binding using a synthetic dataset. We\nfind that common properties of natural data such as low attribute density,\nincomplete captions, and the saliency bias, a tendency of human captioners to\ndescribe the object that is \"most salient\" to them have a detrimental effect on\nbinding performance. In contrast to common belief, we find that neither scaling\nthe batch size, i.e., implicitly adding more hard negatives, nor explicitly\ncreating hard negatives enables CLIP to learn reliable binding. Only when the\ndata expresses our identified data properties CLIP learns almost perfect\nbinding.\n", "link": "http://arxiv.org/abs/2507.07985v1", "date": "2025-07-10", "relevancy": 2.8006, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP%20Won%27t%20Learn%20Object-Attribute%20Binding%20from%20Natural%20Data%20and%20Here%20is%0A%20%20Why&body=Title%3A%20CLIP%20Won%27t%20Learn%20Object-Attribute%20Binding%20from%20Natural%20Data%20and%20Here%20is%0A%20%20Why%0AAuthor%3A%20Bijay%20Gurung%20and%20David%20T.%20Hoffmann%20and%20Thomas%20Brox%0AAbstract%3A%20%20%20Contrastive%20vision-language%20models%20like%20CLIP%20are%20used%20for%20a%20large%20variety%20of%0Aapplications%2C%20such%20as%20zero-shot%20classification%20or%20as%20vision%20encoder%20for%0Amulti-modal%20models.%20Despite%20their%20popularity%2C%20their%20representations%20show%20major%0Alimitations.%20For%20instance%2C%20CLIP%20models%20learn%20bag-of-words%20representations%20and%2C%0Aas%20a%20consequence%2C%20fail%20to%20distinguish%20whether%20an%20image%20is%20of%20%22a%20yellow%0Asubmarine%20and%20a%20blue%20bus%22%20or%20%22a%20blue%20submarine%20and%20a%20yellow%20bus%22.%20Previous%0Aattempts%20to%20fix%20this%20issue%20added%20hard%20negatives%20during%20training%20or%20modified%20the%0Aarchitecture%2C%20but%20failed%20to%20resolve%20the%20problem%20in%20its%20entirety.%20We%20suspect%0Athat%20the%20missing%20insights%20to%20solve%20the%20binding%20problem%20for%20CLIP%20are%20hidden%20in%0Athe%20arguably%20most%20important%20part%20of%20learning%20algorithms%3A%20the%20data.%20In%20this%0Awork%2C%20we%20fill%20this%20gap%20by%20rigorously%20identifying%20the%20influence%20of%20data%0Aproperties%20on%20CLIP%27s%20ability%20to%20learn%20binding%20using%20a%20synthetic%20dataset.%20We%0Afind%20that%20common%20properties%20of%20natural%20data%20such%20as%20low%20attribute%20density%2C%0Aincomplete%20captions%2C%20and%20the%20saliency%20bias%2C%20a%20tendency%20of%20human%20captioners%20to%0Adescribe%20the%20object%20that%20is%20%22most%20salient%22%20to%20them%20have%20a%20detrimental%20effect%20on%0Abinding%20performance.%20In%20contrast%20to%20common%20belief%2C%20we%20find%20that%20neither%20scaling%0Athe%20batch%20size%2C%20i.e.%2C%20implicitly%20adding%20more%20hard%20negatives%2C%20nor%20explicitly%0Acreating%20hard%20negatives%20enables%20CLIP%20to%20learn%20reliable%20binding.%20Only%20when%20the%0Adata%20expresses%20our%20identified%20data%20properties%20CLIP%20learns%20almost%20perfect%0Abinding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP%2520Won%2527t%2520Learn%2520Object-Attribute%2520Binding%2520from%2520Natural%2520Data%2520and%2520Here%2520is%250A%2520%2520Why%26entry.906535625%3DBijay%2520Gurung%2520and%2520David%2520T.%2520Hoffmann%2520and%2520Thomas%2520Brox%26entry.1292438233%3D%2520%2520Contrastive%2520vision-language%2520models%2520like%2520CLIP%2520are%2520used%2520for%2520a%2520large%2520variety%2520of%250Aapplications%252C%2520such%2520as%2520zero-shot%2520classification%2520or%2520as%2520vision%2520encoder%2520for%250Amulti-modal%2520models.%2520Despite%2520their%2520popularity%252C%2520their%2520representations%2520show%2520major%250Alimitations.%2520For%2520instance%252C%2520CLIP%2520models%2520learn%2520bag-of-words%2520representations%2520and%252C%250Aas%2520a%2520consequence%252C%2520fail%2520to%2520distinguish%2520whether%2520an%2520image%2520is%2520of%2520%2522a%2520yellow%250Asubmarine%2520and%2520a%2520blue%2520bus%2522%2520or%2520%2522a%2520blue%2520submarine%2520and%2520a%2520yellow%2520bus%2522.%2520Previous%250Aattempts%2520to%2520fix%2520this%2520issue%2520added%2520hard%2520negatives%2520during%2520training%2520or%2520modified%2520the%250Aarchitecture%252C%2520but%2520failed%2520to%2520resolve%2520the%2520problem%2520in%2520its%2520entirety.%2520We%2520suspect%250Athat%2520the%2520missing%2520insights%2520to%2520solve%2520the%2520binding%2520problem%2520for%2520CLIP%2520are%2520hidden%2520in%250Athe%2520arguably%2520most%2520important%2520part%2520of%2520learning%2520algorithms%253A%2520the%2520data.%2520In%2520this%250Awork%252C%2520we%2520fill%2520this%2520gap%2520by%2520rigorously%2520identifying%2520the%2520influence%2520of%2520data%250Aproperties%2520on%2520CLIP%2527s%2520ability%2520to%2520learn%2520binding%2520using%2520a%2520synthetic%2520dataset.%2520We%250Afind%2520that%2520common%2520properties%2520of%2520natural%2520data%2520such%2520as%2520low%2520attribute%2520density%252C%250Aincomplete%2520captions%252C%2520and%2520the%2520saliency%2520bias%252C%2520a%2520tendency%2520of%2520human%2520captioners%2520to%250Adescribe%2520the%2520object%2520that%2520is%2520%2522most%2520salient%2522%2520to%2520them%2520have%2520a%2520detrimental%2520effect%2520on%250Abinding%2520performance.%2520In%2520contrast%2520to%2520common%2520belief%252C%2520we%2520find%2520that%2520neither%2520scaling%250Athe%2520batch%2520size%252C%2520i.e.%252C%2520implicitly%2520adding%2520more%2520hard%2520negatives%252C%2520nor%2520explicitly%250Acreating%2520hard%2520negatives%2520enables%2520CLIP%2520to%2520learn%2520reliable%2520binding.%2520Only%2520when%2520the%250Adata%2520expresses%2520our%2520identified%2520data%2520properties%2520CLIP%2520learns%2520almost%2520perfect%250Abinding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP%20Won%27t%20Learn%20Object-Attribute%20Binding%20from%20Natural%20Data%20and%20Here%20is%0A%20%20Why&entry.906535625=Bijay%20Gurung%20and%20David%20T.%20Hoffmann%20and%20Thomas%20Brox&entry.1292438233=%20%20Contrastive%20vision-language%20models%20like%20CLIP%20are%20used%20for%20a%20large%20variety%20of%0Aapplications%2C%20such%20as%20zero-shot%20classification%20or%20as%20vision%20encoder%20for%0Amulti-modal%20models.%20Despite%20their%20popularity%2C%20their%20representations%20show%20major%0Alimitations.%20For%20instance%2C%20CLIP%20models%20learn%20bag-of-words%20representations%20and%2C%0Aas%20a%20consequence%2C%20fail%20to%20distinguish%20whether%20an%20image%20is%20of%20%22a%20yellow%0Asubmarine%20and%20a%20blue%20bus%22%20or%20%22a%20blue%20submarine%20and%20a%20yellow%20bus%22.%20Previous%0Aattempts%20to%20fix%20this%20issue%20added%20hard%20negatives%20during%20training%20or%20modified%20the%0Aarchitecture%2C%20but%20failed%20to%20resolve%20the%20problem%20in%20its%20entirety.%20We%20suspect%0Athat%20the%20missing%20insights%20to%20solve%20the%20binding%20problem%20for%20CLIP%20are%20hidden%20in%0Athe%20arguably%20most%20important%20part%20of%20learning%20algorithms%3A%20the%20data.%20In%20this%0Awork%2C%20we%20fill%20this%20gap%20by%20rigorously%20identifying%20the%20influence%20of%20data%0Aproperties%20on%20CLIP%27s%20ability%20to%20learn%20binding%20using%20a%20synthetic%20dataset.%20We%0Afind%20that%20common%20properties%20of%20natural%20data%20such%20as%20low%20attribute%20density%2C%0Aincomplete%20captions%2C%20and%20the%20saliency%20bias%2C%20a%20tendency%20of%20human%20captioners%20to%0Adescribe%20the%20object%20that%20is%20%22most%20salient%22%20to%20them%20have%20a%20detrimental%20effect%20on%0Abinding%20performance.%20In%20contrast%20to%20common%20belief%2C%20we%20find%20that%20neither%20scaling%0Athe%20batch%20size%2C%20i.e.%2C%20implicitly%20adding%20more%20hard%20negatives%2C%20nor%20explicitly%0Acreating%20hard%20negatives%20enables%20CLIP%20to%20learn%20reliable%20binding.%20Only%20when%20the%0Adata%20expresses%20our%20identified%20data%20properties%20CLIP%20learns%20almost%20perfect%0Abinding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07985v1&entry.124074799=Read"},
{"title": "Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection", "author": "Subhajit Maity and Ayan Kumar Bhunia and Subhadeep Koley and Pinaki Nath Chowdhury and Aneeshan Sain and Yi-Zhe Song", "abstract": "  Keypoint detection, integral to modern machine perception, faces challenges\nin few-shot learning, particularly when source data from the same distribution\nas the query is unavailable. This gap is addressed by leveraging sketches, a\npopular form of human expression, providing a source-free alternative. However,\nchallenges arise in mastering cross-modal embeddings and handling user-specific\nsketch styles. Our proposed framework overcomes these hurdles with a\nprototypical setup, combined with a grid-based locator and prototypical domain\nadaptation. We also demonstrate success in few-shot convergence across novel\nkeypoints and classes through extensive experiments.\n", "link": "http://arxiv.org/abs/2507.07994v1", "date": "2025-07-10", "relevancy": 2.7785, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6103}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5398}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Doodle%20Your%20Keypoints%3A%20Sketch-Based%20Few-Shot%20Keypoint%20Detection&body=Title%3A%20Doodle%20Your%20Keypoints%3A%20Sketch-Based%20Few-Shot%20Keypoint%20Detection%0AAuthor%3A%20Subhajit%20Maity%20and%20Ayan%20Kumar%20Bhunia%20and%20Subhadeep%20Koley%20and%20Pinaki%20Nath%20Chowdhury%20and%20Aneeshan%20Sain%20and%20Yi-Zhe%20Song%0AAbstract%3A%20%20%20Keypoint%20detection%2C%20integral%20to%20modern%20machine%20perception%2C%20faces%20challenges%0Ain%20few-shot%20learning%2C%20particularly%20when%20source%20data%20from%20the%20same%20distribution%0Aas%20the%20query%20is%20unavailable.%20This%20gap%20is%20addressed%20by%20leveraging%20sketches%2C%20a%0Apopular%20form%20of%20human%20expression%2C%20providing%20a%20source-free%20alternative.%20However%2C%0Achallenges%20arise%20in%20mastering%20cross-modal%20embeddings%20and%20handling%20user-specific%0Asketch%20styles.%20Our%20proposed%20framework%20overcomes%20these%20hurdles%20with%20a%0Aprototypical%20setup%2C%20combined%20with%20a%20grid-based%20locator%20and%20prototypical%20domain%0Aadaptation.%20We%20also%20demonstrate%20success%20in%20few-shot%20convergence%20across%20novel%0Akeypoints%20and%20classes%20through%20extensive%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoodle%2520Your%2520Keypoints%253A%2520Sketch-Based%2520Few-Shot%2520Keypoint%2520Detection%26entry.906535625%3DSubhajit%2520Maity%2520and%2520Ayan%2520Kumar%2520Bhunia%2520and%2520Subhadeep%2520Koley%2520and%2520Pinaki%2520Nath%2520Chowdhury%2520and%2520Aneeshan%2520Sain%2520and%2520Yi-Zhe%2520Song%26entry.1292438233%3D%2520%2520Keypoint%2520detection%252C%2520integral%2520to%2520modern%2520machine%2520perception%252C%2520faces%2520challenges%250Ain%2520few-shot%2520learning%252C%2520particularly%2520when%2520source%2520data%2520from%2520the%2520same%2520distribution%250Aas%2520the%2520query%2520is%2520unavailable.%2520This%2520gap%2520is%2520addressed%2520by%2520leveraging%2520sketches%252C%2520a%250Apopular%2520form%2520of%2520human%2520expression%252C%2520providing%2520a%2520source-free%2520alternative.%2520However%252C%250Achallenges%2520arise%2520in%2520mastering%2520cross-modal%2520embeddings%2520and%2520handling%2520user-specific%250Asketch%2520styles.%2520Our%2520proposed%2520framework%2520overcomes%2520these%2520hurdles%2520with%2520a%250Aprototypical%2520setup%252C%2520combined%2520with%2520a%2520grid-based%2520locator%2520and%2520prototypical%2520domain%250Aadaptation.%2520We%2520also%2520demonstrate%2520success%2520in%2520few-shot%2520convergence%2520across%2520novel%250Akeypoints%2520and%2520classes%2520through%2520extensive%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Doodle%20Your%20Keypoints%3A%20Sketch-Based%20Few-Shot%20Keypoint%20Detection&entry.906535625=Subhajit%20Maity%20and%20Ayan%20Kumar%20Bhunia%20and%20Subhadeep%20Koley%20and%20Pinaki%20Nath%20Chowdhury%20and%20Aneeshan%20Sain%20and%20Yi-Zhe%20Song&entry.1292438233=%20%20Keypoint%20detection%2C%20integral%20to%20modern%20machine%20perception%2C%20faces%20challenges%0Ain%20few-shot%20learning%2C%20particularly%20when%20source%20data%20from%20the%20same%20distribution%0Aas%20the%20query%20is%20unavailable.%20This%20gap%20is%20addressed%20by%20leveraging%20sketches%2C%20a%0Apopular%20form%20of%20human%20expression%2C%20providing%20a%20source-free%20alternative.%20However%2C%0Achallenges%20arise%20in%20mastering%20cross-modal%20embeddings%20and%20handling%20user-specific%0Asketch%20styles.%20Our%20proposed%20framework%20overcomes%20these%20hurdles%20with%20a%0Aprototypical%20setup%2C%20combined%20with%20a%20grid-based%20locator%20and%20prototypical%20domain%0Aadaptation.%20We%20also%20demonstrate%20success%20in%20few-shot%20convergence%20across%20novel%0Akeypoints%20and%20classes%20through%20extensive%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07994v1&entry.124074799=Read"},
{"title": "Deep Learning based 3D Volume Correlation for Additive Manufacturing\n  Using High-Resolution Industrial X-ray Computed Tomography", "author": "Keerthana Chand and Tobias Fritsch and Bardia Hejazi and Konstantin Poka and Giovanni Bruno", "abstract": "  Quality control in additive manufacturing (AM) is vital for industrial\napplications in areas such as the automotive, medical and aerospace sectors.\nGeometric inaccuracies caused by shrinkage and deformations can compromise the\nlife and performance of additively manufactured components. Such deviations can\nbe quantified using Digital Volume Correlation (DVC), which compares the\ncomputer-aided design (CAD) model with the X-ray Computed Tomography (XCT)\ngeometry of the components produced. However, accurate registration between the\ntwo modalities is challenging due to the absence of a ground truth or reference\ndeformation field. In addition, the extremely large data size of\nhigh-resolution XCT volumes makes computation difficult. In this work, we\npresent a deep learning-based approach for estimating voxel-wise deformations\nbetween CAD and XCT volumes. Our method uses a dynamic patch-based processing\nstrategy to handle high-resolution volumes. In addition to the Dice Score, we\nintroduce a Binary Difference Map (BDM) that quantifies voxel-wise mismatches\nbetween binarized CAD and XCT volumes to evaluate the accuracy of the\nregistration. Our approach shows a 9.2\\% improvement in the Dice Score and a\n9.9\\% improvement in the voxel match rate compared to classic DVC methods,\nwhile reducing the interaction time from days to minutes. This work sets the\nfoundation for deep learning-based DVC methods to generate compensation meshes\nthat can then be used in closed-loop correlations during the AM production\nprocess. Such a system would be of great interest to industries since the\nmanufacturing process will become more reliable and efficient, saving time and\nmaterial.\n", "link": "http://arxiv.org/abs/2507.07757v1", "date": "2025-07-10", "relevancy": 2.7672, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5634}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5634}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20based%203D%20Volume%20Correlation%20for%20Additive%20Manufacturing%0A%20%20Using%20High-Resolution%20Industrial%20X-ray%20Computed%20Tomography&body=Title%3A%20Deep%20Learning%20based%203D%20Volume%20Correlation%20for%20Additive%20Manufacturing%0A%20%20Using%20High-Resolution%20Industrial%20X-ray%20Computed%20Tomography%0AAuthor%3A%20Keerthana%20Chand%20and%20Tobias%20Fritsch%20and%20Bardia%20Hejazi%20and%20Konstantin%20Poka%20and%20Giovanni%20Bruno%0AAbstract%3A%20%20%20Quality%20control%20in%20additive%20manufacturing%20%28AM%29%20is%20vital%20for%20industrial%0Aapplications%20in%20areas%20such%20as%20the%20automotive%2C%20medical%20and%20aerospace%20sectors.%0AGeometric%20inaccuracies%20caused%20by%20shrinkage%20and%20deformations%20can%20compromise%20the%0Alife%20and%20performance%20of%20additively%20manufactured%20components.%20Such%20deviations%20can%0Abe%20quantified%20using%20Digital%20Volume%20Correlation%20%28DVC%29%2C%20which%20compares%20the%0Acomputer-aided%20design%20%28CAD%29%20model%20with%20the%20X-ray%20Computed%20Tomography%20%28XCT%29%0Ageometry%20of%20the%20components%20produced.%20However%2C%20accurate%20registration%20between%20the%0Atwo%20modalities%20is%20challenging%20due%20to%20the%20absence%20of%20a%20ground%20truth%20or%20reference%0Adeformation%20field.%20In%20addition%2C%20the%20extremely%20large%20data%20size%20of%0Ahigh-resolution%20XCT%20volumes%20makes%20computation%20difficult.%20In%20this%20work%2C%20we%0Apresent%20a%20deep%20learning-based%20approach%20for%20estimating%20voxel-wise%20deformations%0Abetween%20CAD%20and%20XCT%20volumes.%20Our%20method%20uses%20a%20dynamic%20patch-based%20processing%0Astrategy%20to%20handle%20high-resolution%20volumes.%20In%20addition%20to%20the%20Dice%20Score%2C%20we%0Aintroduce%20a%20Binary%20Difference%20Map%20%28BDM%29%20that%20quantifies%20voxel-wise%20mismatches%0Abetween%20binarized%20CAD%20and%20XCT%20volumes%20to%20evaluate%20the%20accuracy%20of%20the%0Aregistration.%20Our%20approach%20shows%20a%209.2%5C%25%20improvement%20in%20the%20Dice%20Score%20and%20a%0A9.9%5C%25%20improvement%20in%20the%20voxel%20match%20rate%20compared%20to%20classic%20DVC%20methods%2C%0Awhile%20reducing%20the%20interaction%20time%20from%20days%20to%20minutes.%20This%20work%20sets%20the%0Afoundation%20for%20deep%20learning-based%20DVC%20methods%20to%20generate%20compensation%20meshes%0Athat%20can%20then%20be%20used%20in%20closed-loop%20correlations%20during%20the%20AM%20production%0Aprocess.%20Such%20a%20system%20would%20be%20of%20great%20interest%20to%20industries%20since%20the%0Amanufacturing%20process%20will%20become%20more%20reliable%20and%20efficient%2C%20saving%20time%20and%0Amaterial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520based%25203D%2520Volume%2520Correlation%2520for%2520Additive%2520Manufacturing%250A%2520%2520Using%2520High-Resolution%2520Industrial%2520X-ray%2520Computed%2520Tomography%26entry.906535625%3DKeerthana%2520Chand%2520and%2520Tobias%2520Fritsch%2520and%2520Bardia%2520Hejazi%2520and%2520Konstantin%2520Poka%2520and%2520Giovanni%2520Bruno%26entry.1292438233%3D%2520%2520Quality%2520control%2520in%2520additive%2520manufacturing%2520%2528AM%2529%2520is%2520vital%2520for%2520industrial%250Aapplications%2520in%2520areas%2520such%2520as%2520the%2520automotive%252C%2520medical%2520and%2520aerospace%2520sectors.%250AGeometric%2520inaccuracies%2520caused%2520by%2520shrinkage%2520and%2520deformations%2520can%2520compromise%2520the%250Alife%2520and%2520performance%2520of%2520additively%2520manufactured%2520components.%2520Such%2520deviations%2520can%250Abe%2520quantified%2520using%2520Digital%2520Volume%2520Correlation%2520%2528DVC%2529%252C%2520which%2520compares%2520the%250Acomputer-aided%2520design%2520%2528CAD%2529%2520model%2520with%2520the%2520X-ray%2520Computed%2520Tomography%2520%2528XCT%2529%250Ageometry%2520of%2520the%2520components%2520produced.%2520However%252C%2520accurate%2520registration%2520between%2520the%250Atwo%2520modalities%2520is%2520challenging%2520due%2520to%2520the%2520absence%2520of%2520a%2520ground%2520truth%2520or%2520reference%250Adeformation%2520field.%2520In%2520addition%252C%2520the%2520extremely%2520large%2520data%2520size%2520of%250Ahigh-resolution%2520XCT%2520volumes%2520makes%2520computation%2520difficult.%2520In%2520this%2520work%252C%2520we%250Apresent%2520a%2520deep%2520learning-based%2520approach%2520for%2520estimating%2520voxel-wise%2520deformations%250Abetween%2520CAD%2520and%2520XCT%2520volumes.%2520Our%2520method%2520uses%2520a%2520dynamic%2520patch-based%2520processing%250Astrategy%2520to%2520handle%2520high-resolution%2520volumes.%2520In%2520addition%2520to%2520the%2520Dice%2520Score%252C%2520we%250Aintroduce%2520a%2520Binary%2520Difference%2520Map%2520%2528BDM%2529%2520that%2520quantifies%2520voxel-wise%2520mismatches%250Abetween%2520binarized%2520CAD%2520and%2520XCT%2520volumes%2520to%2520evaluate%2520the%2520accuracy%2520of%2520the%250Aregistration.%2520Our%2520approach%2520shows%2520a%25209.2%255C%2525%2520improvement%2520in%2520the%2520Dice%2520Score%2520and%2520a%250A9.9%255C%2525%2520improvement%2520in%2520the%2520voxel%2520match%2520rate%2520compared%2520to%2520classic%2520DVC%2520methods%252C%250Awhile%2520reducing%2520the%2520interaction%2520time%2520from%2520days%2520to%2520minutes.%2520This%2520work%2520sets%2520the%250Afoundation%2520for%2520deep%2520learning-based%2520DVC%2520methods%2520to%2520generate%2520compensation%2520meshes%250Athat%2520can%2520then%2520be%2520used%2520in%2520closed-loop%2520correlations%2520during%2520the%2520AM%2520production%250Aprocess.%2520Such%2520a%2520system%2520would%2520be%2520of%2520great%2520interest%2520to%2520industries%2520since%2520the%250Amanufacturing%2520process%2520will%2520become%2520more%2520reliable%2520and%2520efficient%252C%2520saving%2520time%2520and%250Amaterial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20based%203D%20Volume%20Correlation%20for%20Additive%20Manufacturing%0A%20%20Using%20High-Resolution%20Industrial%20X-ray%20Computed%20Tomography&entry.906535625=Keerthana%20Chand%20and%20Tobias%20Fritsch%20and%20Bardia%20Hejazi%20and%20Konstantin%20Poka%20and%20Giovanni%20Bruno&entry.1292438233=%20%20Quality%20control%20in%20additive%20manufacturing%20%28AM%29%20is%20vital%20for%20industrial%0Aapplications%20in%20areas%20such%20as%20the%20automotive%2C%20medical%20and%20aerospace%20sectors.%0AGeometric%20inaccuracies%20caused%20by%20shrinkage%20and%20deformations%20can%20compromise%20the%0Alife%20and%20performance%20of%20additively%20manufactured%20components.%20Such%20deviations%20can%0Abe%20quantified%20using%20Digital%20Volume%20Correlation%20%28DVC%29%2C%20which%20compares%20the%0Acomputer-aided%20design%20%28CAD%29%20model%20with%20the%20X-ray%20Computed%20Tomography%20%28XCT%29%0Ageometry%20of%20the%20components%20produced.%20However%2C%20accurate%20registration%20between%20the%0Atwo%20modalities%20is%20challenging%20due%20to%20the%20absence%20of%20a%20ground%20truth%20or%20reference%0Adeformation%20field.%20In%20addition%2C%20the%20extremely%20large%20data%20size%20of%0Ahigh-resolution%20XCT%20volumes%20makes%20computation%20difficult.%20In%20this%20work%2C%20we%0Apresent%20a%20deep%20learning-based%20approach%20for%20estimating%20voxel-wise%20deformations%0Abetween%20CAD%20and%20XCT%20volumes.%20Our%20method%20uses%20a%20dynamic%20patch-based%20processing%0Astrategy%20to%20handle%20high-resolution%20volumes.%20In%20addition%20to%20the%20Dice%20Score%2C%20we%0Aintroduce%20a%20Binary%20Difference%20Map%20%28BDM%29%20that%20quantifies%20voxel-wise%20mismatches%0Abetween%20binarized%20CAD%20and%20XCT%20volumes%20to%20evaluate%20the%20accuracy%20of%20the%0Aregistration.%20Our%20approach%20shows%20a%209.2%5C%25%20improvement%20in%20the%20Dice%20Score%20and%20a%0A9.9%5C%25%20improvement%20in%20the%20voxel%20match%20rate%20compared%20to%20classic%20DVC%20methods%2C%0Awhile%20reducing%20the%20interaction%20time%20from%20days%20to%20minutes.%20This%20work%20sets%20the%0Afoundation%20for%20deep%20learning-based%20DVC%20methods%20to%20generate%20compensation%20meshes%0Athat%20can%20then%20be%20used%20in%20closed-loop%20correlations%20during%20the%20AM%20production%0Aprocess.%20Such%20a%20system%20would%20be%20of%20great%20interest%20to%20industries%20since%20the%0Amanufacturing%20process%20will%20become%20more%20reliable%20and%20efficient%2C%20saving%20time%20and%0Amaterial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07757v1&entry.124074799=Read"},
{"title": "Information-driven design of imaging systems", "author": "Henry Pinkard and Leyla Kabuli and Eric Markley and Tiffany Chien and Jiantao Jiao and Laura Waller", "abstract": "  In modern imaging systems that computationally process raw measurements\nbefore or instead of human viewing, information content matters more than\nvisual appearance. However, developing information estimators that can handle\nthe complexity of real-world measurements yet remain practical enough for\nwidespread use has proven challenging. We introduce a data-driven approach for\nestimating mutual information between unknown objects and their noisy\nmeasurements. Our technique fits probabilistic models to measurements and their\nnoise processes, quantifying information content without requiring ground truth\ndata or making assumptions about object structure. We validate our approach\nacross diverse applications-color photography, radio astronomy, lensless\nimaging, and microscopy-demonstrating that information estimates reliably\npredict system performance. Finally, we introduce Information-Driven Encoder\nAnalysis Learning (IDEAL), which optimizes imaging systems to maximize\ninformation capture. Our work unlocks information theory as a powerful,\npractical tool for analyzing and designing imaging systems across a broad range\nof applications.\n  A video summarizing this work can be found at:\nhttps://waller-lab.github.io/EncodingInformationWebsite/\n", "link": "http://arxiv.org/abs/2405.20559v4", "date": "2025-07-10", "relevancy": 2.7606, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information-driven%20design%20of%20imaging%20systems&body=Title%3A%20Information-driven%20design%20of%20imaging%20systems%0AAuthor%3A%20Henry%20Pinkard%20and%20Leyla%20Kabuli%20and%20Eric%20Markley%20and%20Tiffany%20Chien%20and%20Jiantao%20Jiao%20and%20Laura%20Waller%0AAbstract%3A%20%20%20In%20modern%20imaging%20systems%20that%20computationally%20process%20raw%20measurements%0Abefore%20or%20instead%20of%20human%20viewing%2C%20information%20content%20matters%20more%20than%0Avisual%20appearance.%20However%2C%20developing%20information%20estimators%20that%20can%20handle%0Athe%20complexity%20of%20real-world%20measurements%20yet%20remain%20practical%20enough%20for%0Awidespread%20use%20has%20proven%20challenging.%20We%20introduce%20a%20data-driven%20approach%20for%0Aestimating%20mutual%20information%20between%20unknown%20objects%20and%20their%20noisy%0Ameasurements.%20Our%20technique%20fits%20probabilistic%20models%20to%20measurements%20and%20their%0Anoise%20processes%2C%20quantifying%20information%20content%20without%20requiring%20ground%20truth%0Adata%20or%20making%20assumptions%20about%20object%20structure.%20We%20validate%20our%20approach%0Aacross%20diverse%20applications-color%20photography%2C%20radio%20astronomy%2C%20lensless%0Aimaging%2C%20and%20microscopy-demonstrating%20that%20information%20estimates%20reliably%0Apredict%20system%20performance.%20Finally%2C%20we%20introduce%20Information-Driven%20Encoder%0AAnalysis%20Learning%20%28IDEAL%29%2C%20which%20optimizes%20imaging%20systems%20to%20maximize%0Ainformation%20capture.%20Our%20work%20unlocks%20information%20theory%20as%20a%20powerful%2C%0Apractical%20tool%20for%20analyzing%20and%20designing%20imaging%20systems%20across%20a%20broad%20range%0Aof%20applications.%0A%20%20A%20video%20summarizing%20this%20work%20can%20be%20found%20at%3A%0Ahttps%3A//waller-lab.github.io/EncodingInformationWebsite/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20559v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation-driven%2520design%2520of%2520imaging%2520systems%26entry.906535625%3DHenry%2520Pinkard%2520and%2520Leyla%2520Kabuli%2520and%2520Eric%2520Markley%2520and%2520Tiffany%2520Chien%2520and%2520Jiantao%2520Jiao%2520and%2520Laura%2520Waller%26entry.1292438233%3D%2520%2520In%2520modern%2520imaging%2520systems%2520that%2520computationally%2520process%2520raw%2520measurements%250Abefore%2520or%2520instead%2520of%2520human%2520viewing%252C%2520information%2520content%2520matters%2520more%2520than%250Avisual%2520appearance.%2520However%252C%2520developing%2520information%2520estimators%2520that%2520can%2520handle%250Athe%2520complexity%2520of%2520real-world%2520measurements%2520yet%2520remain%2520practical%2520enough%2520for%250Awidespread%2520use%2520has%2520proven%2520challenging.%2520We%2520introduce%2520a%2520data-driven%2520approach%2520for%250Aestimating%2520mutual%2520information%2520between%2520unknown%2520objects%2520and%2520their%2520noisy%250Ameasurements.%2520Our%2520technique%2520fits%2520probabilistic%2520models%2520to%2520measurements%2520and%2520their%250Anoise%2520processes%252C%2520quantifying%2520information%2520content%2520without%2520requiring%2520ground%2520truth%250Adata%2520or%2520making%2520assumptions%2520about%2520object%2520structure.%2520We%2520validate%2520our%2520approach%250Aacross%2520diverse%2520applications-color%2520photography%252C%2520radio%2520astronomy%252C%2520lensless%250Aimaging%252C%2520and%2520microscopy-demonstrating%2520that%2520information%2520estimates%2520reliably%250Apredict%2520system%2520performance.%2520Finally%252C%2520we%2520introduce%2520Information-Driven%2520Encoder%250AAnalysis%2520Learning%2520%2528IDEAL%2529%252C%2520which%2520optimizes%2520imaging%2520systems%2520to%2520maximize%250Ainformation%2520capture.%2520Our%2520work%2520unlocks%2520information%2520theory%2520as%2520a%2520powerful%252C%250Apractical%2520tool%2520for%2520analyzing%2520and%2520designing%2520imaging%2520systems%2520across%2520a%2520broad%2520range%250Aof%2520applications.%250A%2520%2520A%2520video%2520summarizing%2520this%2520work%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//waller-lab.github.io/EncodingInformationWebsite/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20559v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information-driven%20design%20of%20imaging%20systems&entry.906535625=Henry%20Pinkard%20and%20Leyla%20Kabuli%20and%20Eric%20Markley%20and%20Tiffany%20Chien%20and%20Jiantao%20Jiao%20and%20Laura%20Waller&entry.1292438233=%20%20In%20modern%20imaging%20systems%20that%20computationally%20process%20raw%20measurements%0Abefore%20or%20instead%20of%20human%20viewing%2C%20information%20content%20matters%20more%20than%0Avisual%20appearance.%20However%2C%20developing%20information%20estimators%20that%20can%20handle%0Athe%20complexity%20of%20real-world%20measurements%20yet%20remain%20practical%20enough%20for%0Awidespread%20use%20has%20proven%20challenging.%20We%20introduce%20a%20data-driven%20approach%20for%0Aestimating%20mutual%20information%20between%20unknown%20objects%20and%20their%20noisy%0Ameasurements.%20Our%20technique%20fits%20probabilistic%20models%20to%20measurements%20and%20their%0Anoise%20processes%2C%20quantifying%20information%20content%20without%20requiring%20ground%20truth%0Adata%20or%20making%20assumptions%20about%20object%20structure.%20We%20validate%20our%20approach%0Aacross%20diverse%20applications-color%20photography%2C%20radio%20astronomy%2C%20lensless%0Aimaging%2C%20and%20microscopy-demonstrating%20that%20information%20estimates%20reliably%0Apredict%20system%20performance.%20Finally%2C%20we%20introduce%20Information-Driven%20Encoder%0AAnalysis%20Learning%20%28IDEAL%29%2C%20which%20optimizes%20imaging%20systems%20to%20maximize%0Ainformation%20capture.%20Our%20work%20unlocks%20information%20theory%20as%20a%20powerful%2C%0Apractical%20tool%20for%20analyzing%20and%20designing%20imaging%20systems%20across%20a%20broad%20range%0Aof%20applications.%0A%20%20A%20video%20summarizing%20this%20work%20can%20be%20found%20at%3A%0Ahttps%3A//waller-lab.github.io/EncodingInformationWebsite/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20559v4&entry.124074799=Read"},
{"title": "PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured\n  Representations", "author": "Fedor Rodionov and Abdelrahman Eldesokey and Michael Birsak and John Femiani and Bernard Ghanem and Peter Wonka", "abstract": "  We introduce PlanQA, a diagnostic benchmark for evaluating geometric and\nspatial reasoning in large-language models (LLMs). PlanQA is grounded in\nstructured representations of indoor scenes, such as kitchens, living rooms,\nand bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The\nbenchmark includes diverse question types that test not only metric and\ntopological reasoning (e.g., distance, visibility, shortest paths) but also\ninterior design constraints such as affordance, clearance, balance, and\nusability. Our results across a variety of frontier open-source and commercial\nLLMs show that while models may succeed in shallow queries, they often fail to\nsimulate physical constraints, preserve spatial coherence, or generalize under\nlayout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they\ndo not consistently reason about real-world layouts. We hope that this\nbenchmark inspires new work on language models that can accurately infer and\nmanipulate spatial and geometric properties in practical settings.\n", "link": "http://arxiv.org/abs/2507.07644v1", "date": "2025-07-10", "relevancy": 2.7567, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PlanQA%3A%20A%20Benchmark%20for%20Spatial%20Reasoning%20in%20LLMs%20using%20Structured%0A%20%20Representations&body=Title%3A%20PlanQA%3A%20A%20Benchmark%20for%20Spatial%20Reasoning%20in%20LLMs%20using%20Structured%0A%20%20Representations%0AAuthor%3A%20Fedor%20Rodionov%20and%20Abdelrahman%20Eldesokey%20and%20Michael%20Birsak%20and%20John%20Femiani%20and%20Bernard%20Ghanem%20and%20Peter%20Wonka%0AAbstract%3A%20%20%20We%20introduce%20PlanQA%2C%20a%20diagnostic%20benchmark%20for%20evaluating%20geometric%20and%0Aspatial%20reasoning%20in%20large-language%20models%20%28LLMs%29.%20PlanQA%20is%20grounded%20in%0Astructured%20representations%20of%20indoor%20scenes%2C%20such%20as%20kitchens%2C%20living%20rooms%2C%0Aand%20bedrooms%2C%20encoded%20in%20a%20symbolic%20format%20%28e.g.%2C%20JSON%2C%20XML%20layouts%29.%20The%0Abenchmark%20includes%20diverse%20question%20types%20that%20test%20not%20only%20metric%20and%0Atopological%20reasoning%20%28e.g.%2C%20distance%2C%20visibility%2C%20shortest%20paths%29%20but%20also%0Ainterior%20design%20constraints%20such%20as%20affordance%2C%20clearance%2C%20balance%2C%20and%0Ausability.%20Our%20results%20across%20a%20variety%20of%20frontier%20open-source%20and%20commercial%0ALLMs%20show%20that%20while%20models%20may%20succeed%20in%20shallow%20queries%2C%20they%20often%20fail%20to%0Asimulate%20physical%20constraints%2C%20preserve%20spatial%20coherence%2C%20or%20generalize%20under%0Alayout%20perturbation.%20PlanQA%20uncovers%20a%20clear%20blind%20spot%20in%20today%27s%20LLMs%3A%20they%0Ado%20not%20consistently%20reason%20about%20real-world%20layouts.%20We%20hope%20that%20this%0Abenchmark%20inspires%20new%20work%20on%20language%20models%20that%20can%20accurately%20infer%20and%0Amanipulate%20spatial%20and%20geometric%20properties%20in%20practical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanQA%253A%2520A%2520Benchmark%2520for%2520Spatial%2520Reasoning%2520in%2520LLMs%2520using%2520Structured%250A%2520%2520Representations%26entry.906535625%3DFedor%2520Rodionov%2520and%2520Abdelrahman%2520Eldesokey%2520and%2520Michael%2520Birsak%2520and%2520John%2520Femiani%2520and%2520Bernard%2520Ghanem%2520and%2520Peter%2520Wonka%26entry.1292438233%3D%2520%2520We%2520introduce%2520PlanQA%252C%2520a%2520diagnostic%2520benchmark%2520for%2520evaluating%2520geometric%2520and%250Aspatial%2520reasoning%2520in%2520large-language%2520models%2520%2528LLMs%2529.%2520PlanQA%2520is%2520grounded%2520in%250Astructured%2520representations%2520of%2520indoor%2520scenes%252C%2520such%2520as%2520kitchens%252C%2520living%2520rooms%252C%250Aand%2520bedrooms%252C%2520encoded%2520in%2520a%2520symbolic%2520format%2520%2528e.g.%252C%2520JSON%252C%2520XML%2520layouts%2529.%2520The%250Abenchmark%2520includes%2520diverse%2520question%2520types%2520that%2520test%2520not%2520only%2520metric%2520and%250Atopological%2520reasoning%2520%2528e.g.%252C%2520distance%252C%2520visibility%252C%2520shortest%2520paths%2529%2520but%2520also%250Ainterior%2520design%2520constraints%2520such%2520as%2520affordance%252C%2520clearance%252C%2520balance%252C%2520and%250Ausability.%2520Our%2520results%2520across%2520a%2520variety%2520of%2520frontier%2520open-source%2520and%2520commercial%250ALLMs%2520show%2520that%2520while%2520models%2520may%2520succeed%2520in%2520shallow%2520queries%252C%2520they%2520often%2520fail%2520to%250Asimulate%2520physical%2520constraints%252C%2520preserve%2520spatial%2520coherence%252C%2520or%2520generalize%2520under%250Alayout%2520perturbation.%2520PlanQA%2520uncovers%2520a%2520clear%2520blind%2520spot%2520in%2520today%2527s%2520LLMs%253A%2520they%250Ado%2520not%2520consistently%2520reason%2520about%2520real-world%2520layouts.%2520We%2520hope%2520that%2520this%250Abenchmark%2520inspires%2520new%2520work%2520on%2520language%2520models%2520that%2520can%2520accurately%2520infer%2520and%250Amanipulate%2520spatial%2520and%2520geometric%2520properties%2520in%2520practical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlanQA%3A%20A%20Benchmark%20for%20Spatial%20Reasoning%20in%20LLMs%20using%20Structured%0A%20%20Representations&entry.906535625=Fedor%20Rodionov%20and%20Abdelrahman%20Eldesokey%20and%20Michael%20Birsak%20and%20John%20Femiani%20and%20Bernard%20Ghanem%20and%20Peter%20Wonka&entry.1292438233=%20%20We%20introduce%20PlanQA%2C%20a%20diagnostic%20benchmark%20for%20evaluating%20geometric%20and%0Aspatial%20reasoning%20in%20large-language%20models%20%28LLMs%29.%20PlanQA%20is%20grounded%20in%0Astructured%20representations%20of%20indoor%20scenes%2C%20such%20as%20kitchens%2C%20living%20rooms%2C%0Aand%20bedrooms%2C%20encoded%20in%20a%20symbolic%20format%20%28e.g.%2C%20JSON%2C%20XML%20layouts%29.%20The%0Abenchmark%20includes%20diverse%20question%20types%20that%20test%20not%20only%20metric%20and%0Atopological%20reasoning%20%28e.g.%2C%20distance%2C%20visibility%2C%20shortest%20paths%29%20but%20also%0Ainterior%20design%20constraints%20such%20as%20affordance%2C%20clearance%2C%20balance%2C%20and%0Ausability.%20Our%20results%20across%20a%20variety%20of%20frontier%20open-source%20and%20commercial%0ALLMs%20show%20that%20while%20models%20may%20succeed%20in%20shallow%20queries%2C%20they%20often%20fail%20to%0Asimulate%20physical%20constraints%2C%20preserve%20spatial%20coherence%2C%20or%20generalize%20under%0Alayout%20perturbation.%20PlanQA%20uncovers%20a%20clear%20blind%20spot%20in%20today%27s%20LLMs%3A%20they%0Ado%20not%20consistently%20reason%20about%20real-world%20layouts.%20We%20hope%20that%20this%0Abenchmark%20inspires%20new%20work%20on%20language%20models%20that%20can%20accurately%20infer%20and%0Amanipulate%20spatial%20and%20geometric%20properties%20in%20practical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07644v1&entry.124074799=Read"},
{"title": "MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group\n  Quantization", "author": "Mingkai Jia and Wei Yin and Xiaotao Hu and Jiaxin Guo and Xiaoyang Guo and Qian Zhang and Xiao-Xiao Long and Ping Tan", "abstract": "  Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models\nthat compress continuous visual data into discrete tokens. Existing methods\nhave tried to improve the quantization strategy for better reconstruction\nquality, however, there still exists a large gap between VQ-VAEs and VAEs. To\nnarrow this gap, we propose \\NickName, a novel method to augment the\nrepresentation capability of discrete codebooks, facilitating easier\noptimization for codebooks and minimizing information loss, thereby enhancing\nreconstruction quality. Specifically, we propose to retain the latent dimension\nto preserve encoded features and incorporate a set of sub-codebooks for\nquantization. Furthermore, we construct comprehensive zero-shot benchmarks\nfeaturing resolutions of 512p and 2k to evaluate the reconstruction performance\nof existing methods rigorously. \\NickName~achieves the \\textbf{state-of-the-art\nperformance on both ImageNet and $8$ zero-shot benchmarks} across all VQ-VAEs.\nNotably, compared with SD-VAE, we outperform them on ImageNet significantly,\nwith rFID $\\textbf{0.49}$ v.s. $\\textbf{0.91}$, and achieve superior PSNR on\nall zero-shot benchmarks. These results highlight the superiority of\n\\NickName~in reconstruction and pave the way for preserving fidelity in HD\nimage processing tasks. Code will be publicly available at\nhttps://github.com/MKJia/MGVQ.\n", "link": "http://arxiv.org/abs/2507.07997v1", "date": "2025-07-10", "relevancy": 2.7319, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5834}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5289}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGVQ%3A%20Could%20VQ-VAE%20Beat%20VAE%3F%20A%20Generalizable%20Tokenizer%20with%20Multi-group%0A%20%20Quantization&body=Title%3A%20MGVQ%3A%20Could%20VQ-VAE%20Beat%20VAE%3F%20A%20Generalizable%20Tokenizer%20with%20Multi-group%0A%20%20Quantization%0AAuthor%3A%20Mingkai%20Jia%20and%20Wei%20Yin%20and%20Xiaotao%20Hu%20and%20Jiaxin%20Guo%20and%20Xiaoyang%20Guo%20and%20Qian%20Zhang%20and%20Xiao-Xiao%20Long%20and%20Ping%20Tan%0AAbstract%3A%20%20%20Vector%20Quantized%20Variational%20Autoencoders%20%28VQ-VAEs%29%20are%20fundamental%20models%0Athat%20compress%20continuous%20visual%20data%20into%20discrete%20tokens.%20Existing%20methods%0Ahave%20tried%20to%20improve%20the%20quantization%20strategy%20for%20better%20reconstruction%0Aquality%2C%20however%2C%20there%20still%20exists%20a%20large%20gap%20between%20VQ-VAEs%20and%20VAEs.%20To%0Anarrow%20this%20gap%2C%20we%20propose%20%5CNickName%2C%20a%20novel%20method%20to%20augment%20the%0Arepresentation%20capability%20of%20discrete%20codebooks%2C%20facilitating%20easier%0Aoptimization%20for%20codebooks%20and%20minimizing%20information%20loss%2C%20thereby%20enhancing%0Areconstruction%20quality.%20Specifically%2C%20we%20propose%20to%20retain%20the%20latent%20dimension%0Ato%20preserve%20encoded%20features%20and%20incorporate%20a%20set%20of%20sub-codebooks%20for%0Aquantization.%20Furthermore%2C%20we%20construct%20comprehensive%20zero-shot%20benchmarks%0Afeaturing%20resolutions%20of%20512p%20and%202k%20to%20evaluate%20the%20reconstruction%20performance%0Aof%20existing%20methods%20rigorously.%20%5CNickName~achieves%20the%20%5Ctextbf%7Bstate-of-the-art%0Aperformance%20on%20both%20ImageNet%20and%20%248%24%20zero-shot%20benchmarks%7D%20across%20all%20VQ-VAEs.%0ANotably%2C%20compared%20with%20SD-VAE%2C%20we%20outperform%20them%20on%20ImageNet%20significantly%2C%0Awith%20rFID%20%24%5Ctextbf%7B0.49%7D%24%20v.s.%20%24%5Ctextbf%7B0.91%7D%24%2C%20and%20achieve%20superior%20PSNR%20on%0Aall%20zero-shot%20benchmarks.%20These%20results%20highlight%20the%20superiority%20of%0A%5CNickName~in%20reconstruction%20and%20pave%20the%20way%20for%20preserving%20fidelity%20in%20HD%0Aimage%20processing%20tasks.%20Code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/MKJia/MGVQ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGVQ%253A%2520Could%2520VQ-VAE%2520Beat%2520VAE%253F%2520A%2520Generalizable%2520Tokenizer%2520with%2520Multi-group%250A%2520%2520Quantization%26entry.906535625%3DMingkai%2520Jia%2520and%2520Wei%2520Yin%2520and%2520Xiaotao%2520Hu%2520and%2520Jiaxin%2520Guo%2520and%2520Xiaoyang%2520Guo%2520and%2520Qian%2520Zhang%2520and%2520Xiao-Xiao%2520Long%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520Vector%2520Quantized%2520Variational%2520Autoencoders%2520%2528VQ-VAEs%2529%2520are%2520fundamental%2520models%250Athat%2520compress%2520continuous%2520visual%2520data%2520into%2520discrete%2520tokens.%2520Existing%2520methods%250Ahave%2520tried%2520to%2520improve%2520the%2520quantization%2520strategy%2520for%2520better%2520reconstruction%250Aquality%252C%2520however%252C%2520there%2520still%2520exists%2520a%2520large%2520gap%2520between%2520VQ-VAEs%2520and%2520VAEs.%2520To%250Anarrow%2520this%2520gap%252C%2520we%2520propose%2520%255CNickName%252C%2520a%2520novel%2520method%2520to%2520augment%2520the%250Arepresentation%2520capability%2520of%2520discrete%2520codebooks%252C%2520facilitating%2520easier%250Aoptimization%2520for%2520codebooks%2520and%2520minimizing%2520information%2520loss%252C%2520thereby%2520enhancing%250Areconstruction%2520quality.%2520Specifically%252C%2520we%2520propose%2520to%2520retain%2520the%2520latent%2520dimension%250Ato%2520preserve%2520encoded%2520features%2520and%2520incorporate%2520a%2520set%2520of%2520sub-codebooks%2520for%250Aquantization.%2520Furthermore%252C%2520we%2520construct%2520comprehensive%2520zero-shot%2520benchmarks%250Afeaturing%2520resolutions%2520of%2520512p%2520and%25202k%2520to%2520evaluate%2520the%2520reconstruction%2520performance%250Aof%2520existing%2520methods%2520rigorously.%2520%255CNickName~achieves%2520the%2520%255Ctextbf%257Bstate-of-the-art%250Aperformance%2520on%2520both%2520ImageNet%2520and%2520%25248%2524%2520zero-shot%2520benchmarks%257D%2520across%2520all%2520VQ-VAEs.%250ANotably%252C%2520compared%2520with%2520SD-VAE%252C%2520we%2520outperform%2520them%2520on%2520ImageNet%2520significantly%252C%250Awith%2520rFID%2520%2524%255Ctextbf%257B0.49%257D%2524%2520v.s.%2520%2524%255Ctextbf%257B0.91%257D%2524%252C%2520and%2520achieve%2520superior%2520PSNR%2520on%250Aall%2520zero-shot%2520benchmarks.%2520These%2520results%2520highlight%2520the%2520superiority%2520of%250A%255CNickName~in%2520reconstruction%2520and%2520pave%2520the%2520way%2520for%2520preserving%2520fidelity%2520in%2520HD%250Aimage%2520processing%2520tasks.%2520Code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/MKJia/MGVQ.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGVQ%3A%20Could%20VQ-VAE%20Beat%20VAE%3F%20A%20Generalizable%20Tokenizer%20with%20Multi-group%0A%20%20Quantization&entry.906535625=Mingkai%20Jia%20and%20Wei%20Yin%20and%20Xiaotao%20Hu%20and%20Jiaxin%20Guo%20and%20Xiaoyang%20Guo%20and%20Qian%20Zhang%20and%20Xiao-Xiao%20Long%20and%20Ping%20Tan&entry.1292438233=%20%20Vector%20Quantized%20Variational%20Autoencoders%20%28VQ-VAEs%29%20are%20fundamental%20models%0Athat%20compress%20continuous%20visual%20data%20into%20discrete%20tokens.%20Existing%20methods%0Ahave%20tried%20to%20improve%20the%20quantization%20strategy%20for%20better%20reconstruction%0Aquality%2C%20however%2C%20there%20still%20exists%20a%20large%20gap%20between%20VQ-VAEs%20and%20VAEs.%20To%0Anarrow%20this%20gap%2C%20we%20propose%20%5CNickName%2C%20a%20novel%20method%20to%20augment%20the%0Arepresentation%20capability%20of%20discrete%20codebooks%2C%20facilitating%20easier%0Aoptimization%20for%20codebooks%20and%20minimizing%20information%20loss%2C%20thereby%20enhancing%0Areconstruction%20quality.%20Specifically%2C%20we%20propose%20to%20retain%20the%20latent%20dimension%0Ato%20preserve%20encoded%20features%20and%20incorporate%20a%20set%20of%20sub-codebooks%20for%0Aquantization.%20Furthermore%2C%20we%20construct%20comprehensive%20zero-shot%20benchmarks%0Afeaturing%20resolutions%20of%20512p%20and%202k%20to%20evaluate%20the%20reconstruction%20performance%0Aof%20existing%20methods%20rigorously.%20%5CNickName~achieves%20the%20%5Ctextbf%7Bstate-of-the-art%0Aperformance%20on%20both%20ImageNet%20and%20%248%24%20zero-shot%20benchmarks%7D%20across%20all%20VQ-VAEs.%0ANotably%2C%20compared%20with%20SD-VAE%2C%20we%20outperform%20them%20on%20ImageNet%20significantly%2C%0Awith%20rFID%20%24%5Ctextbf%7B0.49%7D%24%20v.s.%20%24%5Ctextbf%7B0.91%7D%24%2C%20and%20achieve%20superior%20PSNR%20on%0Aall%20zero-shot%20benchmarks.%20These%20results%20highlight%20the%20superiority%20of%0A%5CNickName~in%20reconstruction%20and%20pave%20the%20way%20for%20preserving%20fidelity%20in%20HD%0Aimage%20processing%20tasks.%20Code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/MKJia/MGVQ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07997v1&entry.124074799=Read"},
{"title": "Robust and Generalizable Heart Rate Estimation via Deep Learning for\n  Remote Photoplethysmography in Complex Scenarios", "author": "Kang Cen and Chang-Hong Fu and Hong Hong", "abstract": "  Non-contact remote photoplethysmography (rPPG) technology enables heart rate\nmeasurement from facial videos. However, existing network models still face\nchallenges in accu racy, robustness, and generalization capability under\ncomplex scenarios. This paper proposes an end-to-end rPPG extraction network\nthat employs 3D convolutional neural networks to reconstruct accurate rPPG\nsignals from raw facial videos. We introduce a differential frame fusion module\nthat integrates differential frames with original frames, enabling frame-level\nrepresentations to capture blood volume pulse (BVP) variations. Additionally,\nwe incorporate Temporal Shift Module (TSM) with self-attention mechanisms,\nwhich effectively enhance rPPG features with minimal computational overhead.\nFurthermore, we propose a novel dynamic hybrid loss function that provides\nstronger supervision for the network, effectively mitigating over fitting.\nComprehensive experiments were conducted on not only the PURE and UBFC-rPPG\ndatasets but also the challenging MMPD dataset under complex scenarios,\ninvolving both intra dataset and cross-dataset evaluations, which demonstrate\nthe superior robustness and generalization capability of our network.\nSpecifically, after training on PURE, our model achieved a mean absolute error\n(MAE) of 7.58 on the MMPD test set, outperforming the state-of-the-art models.\n", "link": "http://arxiv.org/abs/2507.07795v1", "date": "2025-07-10", "relevancy": 2.7227, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5506}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5493}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20and%20Generalizable%20Heart%20Rate%20Estimation%20via%20Deep%20Learning%20for%0A%20%20Remote%20Photoplethysmography%20in%20Complex%20Scenarios&body=Title%3A%20Robust%20and%20Generalizable%20Heart%20Rate%20Estimation%20via%20Deep%20Learning%20for%0A%20%20Remote%20Photoplethysmography%20in%20Complex%20Scenarios%0AAuthor%3A%20Kang%20Cen%20and%20Chang-Hong%20Fu%20and%20Hong%20Hong%0AAbstract%3A%20%20%20Non-contact%20remote%20photoplethysmography%20%28rPPG%29%20technology%20enables%20heart%20rate%0Ameasurement%20from%20facial%20videos.%20However%2C%20existing%20network%20models%20still%20face%0Achallenges%20in%20accu%20racy%2C%20robustness%2C%20and%20generalization%20capability%20under%0Acomplex%20scenarios.%20This%20paper%20proposes%20an%20end-to-end%20rPPG%20extraction%20network%0Athat%20employs%203D%20convolutional%20neural%20networks%20to%20reconstruct%20accurate%20rPPG%0Asignals%20from%20raw%20facial%20videos.%20We%20introduce%20a%20differential%20frame%20fusion%20module%0Athat%20integrates%20differential%20frames%20with%20original%20frames%2C%20enabling%20frame-level%0Arepresentations%20to%20capture%20blood%20volume%20pulse%20%28BVP%29%20variations.%20Additionally%2C%0Awe%20incorporate%20Temporal%20Shift%20Module%20%28TSM%29%20with%20self-attention%20mechanisms%2C%0Awhich%20effectively%20enhance%20rPPG%20features%20with%20minimal%20computational%20overhead.%0AFurthermore%2C%20we%20propose%20a%20novel%20dynamic%20hybrid%20loss%20function%20that%20provides%0Astronger%20supervision%20for%20the%20network%2C%20effectively%20mitigating%20over%20fitting.%0AComprehensive%20experiments%20were%20conducted%20on%20not%20only%20the%20PURE%20and%20UBFC-rPPG%0Adatasets%20but%20also%20the%20challenging%20MMPD%20dataset%20under%20complex%20scenarios%2C%0Ainvolving%20both%20intra%20dataset%20and%20cross-dataset%20evaluations%2C%20which%20demonstrate%0Athe%20superior%20robustness%20and%20generalization%20capability%20of%20our%20network.%0ASpecifically%2C%20after%20training%20on%20PURE%2C%20our%20model%20achieved%20a%20mean%20absolute%20error%0A%28MAE%29%20of%207.58%20on%20the%20MMPD%20test%20set%2C%20outperforming%20the%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520and%2520Generalizable%2520Heart%2520Rate%2520Estimation%2520via%2520Deep%2520Learning%2520for%250A%2520%2520Remote%2520Photoplethysmography%2520in%2520Complex%2520Scenarios%26entry.906535625%3DKang%2520Cen%2520and%2520Chang-Hong%2520Fu%2520and%2520Hong%2520Hong%26entry.1292438233%3D%2520%2520Non-contact%2520remote%2520photoplethysmography%2520%2528rPPG%2529%2520technology%2520enables%2520heart%2520rate%250Ameasurement%2520from%2520facial%2520videos.%2520However%252C%2520existing%2520network%2520models%2520still%2520face%250Achallenges%2520in%2520accu%2520racy%252C%2520robustness%252C%2520and%2520generalization%2520capability%2520under%250Acomplex%2520scenarios.%2520This%2520paper%2520proposes%2520an%2520end-to-end%2520rPPG%2520extraction%2520network%250Athat%2520employs%25203D%2520convolutional%2520neural%2520networks%2520to%2520reconstruct%2520accurate%2520rPPG%250Asignals%2520from%2520raw%2520facial%2520videos.%2520We%2520introduce%2520a%2520differential%2520frame%2520fusion%2520module%250Athat%2520integrates%2520differential%2520frames%2520with%2520original%2520frames%252C%2520enabling%2520frame-level%250Arepresentations%2520to%2520capture%2520blood%2520volume%2520pulse%2520%2528BVP%2529%2520variations.%2520Additionally%252C%250Awe%2520incorporate%2520Temporal%2520Shift%2520Module%2520%2528TSM%2529%2520with%2520self-attention%2520mechanisms%252C%250Awhich%2520effectively%2520enhance%2520rPPG%2520features%2520with%2520minimal%2520computational%2520overhead.%250AFurthermore%252C%2520we%2520propose%2520a%2520novel%2520dynamic%2520hybrid%2520loss%2520function%2520that%2520provides%250Astronger%2520supervision%2520for%2520the%2520network%252C%2520effectively%2520mitigating%2520over%2520fitting.%250AComprehensive%2520experiments%2520were%2520conducted%2520on%2520not%2520only%2520the%2520PURE%2520and%2520UBFC-rPPG%250Adatasets%2520but%2520also%2520the%2520challenging%2520MMPD%2520dataset%2520under%2520complex%2520scenarios%252C%250Ainvolving%2520both%2520intra%2520dataset%2520and%2520cross-dataset%2520evaluations%252C%2520which%2520demonstrate%250Athe%2520superior%2520robustness%2520and%2520generalization%2520capability%2520of%2520our%2520network.%250ASpecifically%252C%2520after%2520training%2520on%2520PURE%252C%2520our%2520model%2520achieved%2520a%2520mean%2520absolute%2520error%250A%2528MAE%2529%2520of%25207.58%2520on%2520the%2520MMPD%2520test%2520set%252C%2520outperforming%2520the%2520state-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20and%20Generalizable%20Heart%20Rate%20Estimation%20via%20Deep%20Learning%20for%0A%20%20Remote%20Photoplethysmography%20in%20Complex%20Scenarios&entry.906535625=Kang%20Cen%20and%20Chang-Hong%20Fu%20and%20Hong%20Hong&entry.1292438233=%20%20Non-contact%20remote%20photoplethysmography%20%28rPPG%29%20technology%20enables%20heart%20rate%0Ameasurement%20from%20facial%20videos.%20However%2C%20existing%20network%20models%20still%20face%0Achallenges%20in%20accu%20racy%2C%20robustness%2C%20and%20generalization%20capability%20under%0Acomplex%20scenarios.%20This%20paper%20proposes%20an%20end-to-end%20rPPG%20extraction%20network%0Athat%20employs%203D%20convolutional%20neural%20networks%20to%20reconstruct%20accurate%20rPPG%0Asignals%20from%20raw%20facial%20videos.%20We%20introduce%20a%20differential%20frame%20fusion%20module%0Athat%20integrates%20differential%20frames%20with%20original%20frames%2C%20enabling%20frame-level%0Arepresentations%20to%20capture%20blood%20volume%20pulse%20%28BVP%29%20variations.%20Additionally%2C%0Awe%20incorporate%20Temporal%20Shift%20Module%20%28TSM%29%20with%20self-attention%20mechanisms%2C%0Awhich%20effectively%20enhance%20rPPG%20features%20with%20minimal%20computational%20overhead.%0AFurthermore%2C%20we%20propose%20a%20novel%20dynamic%20hybrid%20loss%20function%20that%20provides%0Astronger%20supervision%20for%20the%20network%2C%20effectively%20mitigating%20over%20fitting.%0AComprehensive%20experiments%20were%20conducted%20on%20not%20only%20the%20PURE%20and%20UBFC-rPPG%0Adatasets%20but%20also%20the%20challenging%20MMPD%20dataset%20under%20complex%20scenarios%2C%0Ainvolving%20both%20intra%20dataset%20and%20cross-dataset%20evaluations%2C%20which%20demonstrate%0Athe%20superior%20robustness%20and%20generalization%20capability%20of%20our%20network.%0ASpecifically%2C%20after%20training%20on%20PURE%2C%20our%20model%20achieved%20a%20mean%20absolute%20error%0A%28MAE%29%20of%207.58%20on%20the%20MMPD%20test%20set%2C%20outperforming%20the%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07795v1&entry.124074799=Read"},
{"title": "Dynamic Chunking for End-to-End Hierarchical Sequence Modeling", "author": "Sukjun Hwang and Brandon Wang and Albert Gu", "abstract": "  Despite incredible progress in language models (LMs) in recent years, largely\nresulting from moving away from specialized models designed for specific tasks\nto general models based on powerful architectures (e.g. the Transformer) that\nlearn everything from raw data, pre-processing steps such as tokenization\nremain a barrier to true end-to-end foundation models. We introduce a\ncollection of new techniques that enable a dynamic chunking mechanism which\nautomatically learns content -- and context -- dependent segmentation\nstrategies learned jointly with the rest of the model. Incorporating this into\nan explicit hierarchical network (H-Net) allows replacing the (implicitly\nhierarchical) tokenization-LM-detokenization pipeline with a single model\nlearned fully end-to-end. When compute- and data- matched, an H-Net with one\nstage of hierarchy operating at the byte level outperforms a strong Transformer\nlanguage model operating over BPE tokens. Iterating the hierarchy to multiple\nstages further increases its performance by modeling multiple levels of\nabstraction, demonstrating significantly better scaling with data and matching\na token-based Transformer of twice its size. H-Nets pretrained on English show\nsignificantly increased character-level robustness, and qualitatively learn\nmeaningful data-dependent chunking strategies without any heuristics or\nexplicit supervision. Finally, the H-Net's improvement over tokenized pipelines\nis further increased in languages and modalities with weaker tokenization\nheuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement\nin data efficiency over baselines), showing the potential of true end-to-end\nmodels that learn and scale better from unprocessed data.\n", "link": "http://arxiv.org/abs/2507.07955v1", "date": "2025-07-10", "relevancy": 2.7184, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Chunking%20for%20End-to-End%20Hierarchical%20Sequence%20Modeling&body=Title%3A%20Dynamic%20Chunking%20for%20End-to-End%20Hierarchical%20Sequence%20Modeling%0AAuthor%3A%20Sukjun%20Hwang%20and%20Brandon%20Wang%20and%20Albert%20Gu%0AAbstract%3A%20%20%20Despite%20incredible%20progress%20in%20language%20models%20%28LMs%29%20in%20recent%20years%2C%20largely%0Aresulting%20from%20moving%20away%20from%20specialized%20models%20designed%20for%20specific%20tasks%0Ato%20general%20models%20based%20on%20powerful%20architectures%20%28e.g.%20the%20Transformer%29%20that%0Alearn%20everything%20from%20raw%20data%2C%20pre-processing%20steps%20such%20as%20tokenization%0Aremain%20a%20barrier%20to%20true%20end-to-end%20foundation%20models.%20We%20introduce%20a%0Acollection%20of%20new%20techniques%20that%20enable%20a%20dynamic%20chunking%20mechanism%20which%0Aautomatically%20learns%20content%20--%20and%20context%20--%20dependent%20segmentation%0Astrategies%20learned%20jointly%20with%20the%20rest%20of%20the%20model.%20Incorporating%20this%20into%0Aan%20explicit%20hierarchical%20network%20%28H-Net%29%20allows%20replacing%20the%20%28implicitly%0Ahierarchical%29%20tokenization-LM-detokenization%20pipeline%20with%20a%20single%20model%0Alearned%20fully%20end-to-end.%20When%20compute-%20and%20data-%20matched%2C%20an%20H-Net%20with%20one%0Astage%20of%20hierarchy%20operating%20at%20the%20byte%20level%20outperforms%20a%20strong%20Transformer%0Alanguage%20model%20operating%20over%20BPE%20tokens.%20Iterating%20the%20hierarchy%20to%20multiple%0Astages%20further%20increases%20its%20performance%20by%20modeling%20multiple%20levels%20of%0Aabstraction%2C%20demonstrating%20significantly%20better%20scaling%20with%20data%20and%20matching%0Aa%20token-based%20Transformer%20of%20twice%20its%20size.%20H-Nets%20pretrained%20on%20English%20show%0Asignificantly%20increased%20character-level%20robustness%2C%20and%20qualitatively%20learn%0Ameaningful%20data-dependent%20chunking%20strategies%20without%20any%20heuristics%20or%0Aexplicit%20supervision.%20Finally%2C%20the%20H-Net%27s%20improvement%20over%20tokenized%20pipelines%0Ais%20further%20increased%20in%20languages%20and%20modalities%20with%20weaker%20tokenization%0Aheuristics%2C%20such%20as%20Chinese%20and%20code%2C%20or%20DNA%20sequences%20%28nearly%204x%20improvement%0Ain%20data%20efficiency%20over%20baselines%29%2C%20showing%20the%20potential%20of%20true%20end-to-end%0Amodels%20that%20learn%20and%20scale%20better%20from%20unprocessed%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Chunking%2520for%2520End-to-End%2520Hierarchical%2520Sequence%2520Modeling%26entry.906535625%3DSukjun%2520Hwang%2520and%2520Brandon%2520Wang%2520and%2520Albert%2520Gu%26entry.1292438233%3D%2520%2520Despite%2520incredible%2520progress%2520in%2520language%2520models%2520%2528LMs%2529%2520in%2520recent%2520years%252C%2520largely%250Aresulting%2520from%2520moving%2520away%2520from%2520specialized%2520models%2520designed%2520for%2520specific%2520tasks%250Ato%2520general%2520models%2520based%2520on%2520powerful%2520architectures%2520%2528e.g.%2520the%2520Transformer%2529%2520that%250Alearn%2520everything%2520from%2520raw%2520data%252C%2520pre-processing%2520steps%2520such%2520as%2520tokenization%250Aremain%2520a%2520barrier%2520to%2520true%2520end-to-end%2520foundation%2520models.%2520We%2520introduce%2520a%250Acollection%2520of%2520new%2520techniques%2520that%2520enable%2520a%2520dynamic%2520chunking%2520mechanism%2520which%250Aautomatically%2520learns%2520content%2520--%2520and%2520context%2520--%2520dependent%2520segmentation%250Astrategies%2520learned%2520jointly%2520with%2520the%2520rest%2520of%2520the%2520model.%2520Incorporating%2520this%2520into%250Aan%2520explicit%2520hierarchical%2520network%2520%2528H-Net%2529%2520allows%2520replacing%2520the%2520%2528implicitly%250Ahierarchical%2529%2520tokenization-LM-detokenization%2520pipeline%2520with%2520a%2520single%2520model%250Alearned%2520fully%2520end-to-end.%2520When%2520compute-%2520and%2520data-%2520matched%252C%2520an%2520H-Net%2520with%2520one%250Astage%2520of%2520hierarchy%2520operating%2520at%2520the%2520byte%2520level%2520outperforms%2520a%2520strong%2520Transformer%250Alanguage%2520model%2520operating%2520over%2520BPE%2520tokens.%2520Iterating%2520the%2520hierarchy%2520to%2520multiple%250Astages%2520further%2520increases%2520its%2520performance%2520by%2520modeling%2520multiple%2520levels%2520of%250Aabstraction%252C%2520demonstrating%2520significantly%2520better%2520scaling%2520with%2520data%2520and%2520matching%250Aa%2520token-based%2520Transformer%2520of%2520twice%2520its%2520size.%2520H-Nets%2520pretrained%2520on%2520English%2520show%250Asignificantly%2520increased%2520character-level%2520robustness%252C%2520and%2520qualitatively%2520learn%250Ameaningful%2520data-dependent%2520chunking%2520strategies%2520without%2520any%2520heuristics%2520or%250Aexplicit%2520supervision.%2520Finally%252C%2520the%2520H-Net%2527s%2520improvement%2520over%2520tokenized%2520pipelines%250Ais%2520further%2520increased%2520in%2520languages%2520and%2520modalities%2520with%2520weaker%2520tokenization%250Aheuristics%252C%2520such%2520as%2520Chinese%2520and%2520code%252C%2520or%2520DNA%2520sequences%2520%2528nearly%25204x%2520improvement%250Ain%2520data%2520efficiency%2520over%2520baselines%2529%252C%2520showing%2520the%2520potential%2520of%2520true%2520end-to-end%250Amodels%2520that%2520learn%2520and%2520scale%2520better%2520from%2520unprocessed%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Chunking%20for%20End-to-End%20Hierarchical%20Sequence%20Modeling&entry.906535625=Sukjun%20Hwang%20and%20Brandon%20Wang%20and%20Albert%20Gu&entry.1292438233=%20%20Despite%20incredible%20progress%20in%20language%20models%20%28LMs%29%20in%20recent%20years%2C%20largely%0Aresulting%20from%20moving%20away%20from%20specialized%20models%20designed%20for%20specific%20tasks%0Ato%20general%20models%20based%20on%20powerful%20architectures%20%28e.g.%20the%20Transformer%29%20that%0Alearn%20everything%20from%20raw%20data%2C%20pre-processing%20steps%20such%20as%20tokenization%0Aremain%20a%20barrier%20to%20true%20end-to-end%20foundation%20models.%20We%20introduce%20a%0Acollection%20of%20new%20techniques%20that%20enable%20a%20dynamic%20chunking%20mechanism%20which%0Aautomatically%20learns%20content%20--%20and%20context%20--%20dependent%20segmentation%0Astrategies%20learned%20jointly%20with%20the%20rest%20of%20the%20model.%20Incorporating%20this%20into%0Aan%20explicit%20hierarchical%20network%20%28H-Net%29%20allows%20replacing%20the%20%28implicitly%0Ahierarchical%29%20tokenization-LM-detokenization%20pipeline%20with%20a%20single%20model%0Alearned%20fully%20end-to-end.%20When%20compute-%20and%20data-%20matched%2C%20an%20H-Net%20with%20one%0Astage%20of%20hierarchy%20operating%20at%20the%20byte%20level%20outperforms%20a%20strong%20Transformer%0Alanguage%20model%20operating%20over%20BPE%20tokens.%20Iterating%20the%20hierarchy%20to%20multiple%0Astages%20further%20increases%20its%20performance%20by%20modeling%20multiple%20levels%20of%0Aabstraction%2C%20demonstrating%20significantly%20better%20scaling%20with%20data%20and%20matching%0Aa%20token-based%20Transformer%20of%20twice%20its%20size.%20H-Nets%20pretrained%20on%20English%20show%0Asignificantly%20increased%20character-level%20robustness%2C%20and%20qualitatively%20learn%0Ameaningful%20data-dependent%20chunking%20strategies%20without%20any%20heuristics%20or%0Aexplicit%20supervision.%20Finally%2C%20the%20H-Net%27s%20improvement%20over%20tokenized%20pipelines%0Ais%20further%20increased%20in%20languages%20and%20modalities%20with%20weaker%20tokenization%0Aheuristics%2C%20such%20as%20Chinese%20and%20code%2C%20or%20DNA%20sequences%20%28nearly%204x%20improvement%0Ain%20data%20efficiency%20over%20baselines%29%2C%20showing%20the%20potential%20of%20true%20end-to-end%0Amodels%20that%20learn%20and%20scale%20better%20from%20unprocessed%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07955v1&entry.124074799=Read"},
{"title": "Compressive Imaging Reconstruction via Tensor Decomposed\n  Multi-Resolution Grid Encoding", "author": "Zhenyu Jin and Yisi Luo and Xile Zhao and Deyu Meng", "abstract": "  Compressive imaging (CI) reconstruction, such as snapshot compressive imaging\n(SCI) and compressive sensing magnetic resonance imaging (MRI), aims to recover\nhigh-dimensional images from low-dimensional compressed measurements. This\nprocess critically relies on learning an accurate representation of the\nunderlying high-dimensional image. However, existing unsupervised\nrepresentations may struggle to achieve a desired balance between\nrepresentation ability and efficiency. To overcome this limitation, we propose\nTensor Decomposed multi-resolution Grid encoding (GridTD), an unsupervised\ncontinuous representation framework for CI reconstruction. GridTD optimizes a\nlightweight neural network and the input tensor decomposition model whose\nparameters are learned via multi-resolution hash grid encoding. It inherently\nenjoys the hierarchical modeling ability of multi-resolution grid encoding and\nthe compactness of tensor decomposition, enabling effective and efficient\nreconstruction of high-dimensional images. Theoretical analyses for the\nalgorithm's Lipschitz property, generalization error bound, and fixed-point\nconvergence reveal the intrinsic superiority of GridTD as compared with\nexisting continuous representation models. Extensive experiments across diverse\nCI tasks, including video SCI, spectral SCI, and compressive dynamic MRI\nreconstruction, consistently demonstrate the superiority of GridTD over\nexisting methods, positioning GridTD as a versatile and state-of-the-art CI\nreconstruction method.\n", "link": "http://arxiv.org/abs/2507.07707v1", "date": "2025-07-10", "relevancy": 2.714, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5468}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5468}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressive%20Imaging%20Reconstruction%20via%20Tensor%20Decomposed%0A%20%20Multi-Resolution%20Grid%20Encoding&body=Title%3A%20Compressive%20Imaging%20Reconstruction%20via%20Tensor%20Decomposed%0A%20%20Multi-Resolution%20Grid%20Encoding%0AAuthor%3A%20Zhenyu%20Jin%20and%20Yisi%20Luo%20and%20Xile%20Zhao%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20Compressive%20imaging%20%28CI%29%20reconstruction%2C%20such%20as%20snapshot%20compressive%20imaging%0A%28SCI%29%20and%20compressive%20sensing%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20aims%20to%20recover%0Ahigh-dimensional%20images%20from%20low-dimensional%20compressed%20measurements.%20This%0Aprocess%20critically%20relies%20on%20learning%20an%20accurate%20representation%20of%20the%0Aunderlying%20high-dimensional%20image.%20However%2C%20existing%20unsupervised%0Arepresentations%20may%20struggle%20to%20achieve%20a%20desired%20balance%20between%0Arepresentation%20ability%20and%20efficiency.%20To%20overcome%20this%20limitation%2C%20we%20propose%0ATensor%20Decomposed%20multi-resolution%20Grid%20encoding%20%28GridTD%29%2C%20an%20unsupervised%0Acontinuous%20representation%20framework%20for%20CI%20reconstruction.%20GridTD%20optimizes%20a%0Alightweight%20neural%20network%20and%20the%20input%20tensor%20decomposition%20model%20whose%0Aparameters%20are%20learned%20via%20multi-resolution%20hash%20grid%20encoding.%20It%20inherently%0Aenjoys%20the%20hierarchical%20modeling%20ability%20of%20multi-resolution%20grid%20encoding%20and%0Athe%20compactness%20of%20tensor%20decomposition%2C%20enabling%20effective%20and%20efficient%0Areconstruction%20of%20high-dimensional%20images.%20Theoretical%20analyses%20for%20the%0Aalgorithm%27s%20Lipschitz%20property%2C%20generalization%20error%20bound%2C%20and%20fixed-point%0Aconvergence%20reveal%20the%20intrinsic%20superiority%20of%20GridTD%20as%20compared%20with%0Aexisting%20continuous%20representation%20models.%20Extensive%20experiments%20across%20diverse%0ACI%20tasks%2C%20including%20video%20SCI%2C%20spectral%20SCI%2C%20and%20compressive%20dynamic%20MRI%0Areconstruction%2C%20consistently%20demonstrate%20the%20superiority%20of%20GridTD%20over%0Aexisting%20methods%2C%20positioning%20GridTD%20as%20a%20versatile%20and%20state-of-the-art%20CI%0Areconstruction%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressive%2520Imaging%2520Reconstruction%2520via%2520Tensor%2520Decomposed%250A%2520%2520Multi-Resolution%2520Grid%2520Encoding%26entry.906535625%3DZhenyu%2520Jin%2520and%2520Yisi%2520Luo%2520and%2520Xile%2520Zhao%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520Compressive%2520imaging%2520%2528CI%2529%2520reconstruction%252C%2520such%2520as%2520snapshot%2520compressive%2520imaging%250A%2528SCI%2529%2520and%2520compressive%2520sensing%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%252C%2520aims%2520to%2520recover%250Ahigh-dimensional%2520images%2520from%2520low-dimensional%2520compressed%2520measurements.%2520This%250Aprocess%2520critically%2520relies%2520on%2520learning%2520an%2520accurate%2520representation%2520of%2520the%250Aunderlying%2520high-dimensional%2520image.%2520However%252C%2520existing%2520unsupervised%250Arepresentations%2520may%2520struggle%2520to%2520achieve%2520a%2520desired%2520balance%2520between%250Arepresentation%2520ability%2520and%2520efficiency.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%250ATensor%2520Decomposed%2520multi-resolution%2520Grid%2520encoding%2520%2528GridTD%2529%252C%2520an%2520unsupervised%250Acontinuous%2520representation%2520framework%2520for%2520CI%2520reconstruction.%2520GridTD%2520optimizes%2520a%250Alightweight%2520neural%2520network%2520and%2520the%2520input%2520tensor%2520decomposition%2520model%2520whose%250Aparameters%2520are%2520learned%2520via%2520multi-resolution%2520hash%2520grid%2520encoding.%2520It%2520inherently%250Aenjoys%2520the%2520hierarchical%2520modeling%2520ability%2520of%2520multi-resolution%2520grid%2520encoding%2520and%250Athe%2520compactness%2520of%2520tensor%2520decomposition%252C%2520enabling%2520effective%2520and%2520efficient%250Areconstruction%2520of%2520high-dimensional%2520images.%2520Theoretical%2520analyses%2520for%2520the%250Aalgorithm%2527s%2520Lipschitz%2520property%252C%2520generalization%2520error%2520bound%252C%2520and%2520fixed-point%250Aconvergence%2520reveal%2520the%2520intrinsic%2520superiority%2520of%2520GridTD%2520as%2520compared%2520with%250Aexisting%2520continuous%2520representation%2520models.%2520Extensive%2520experiments%2520across%2520diverse%250ACI%2520tasks%252C%2520including%2520video%2520SCI%252C%2520spectral%2520SCI%252C%2520and%2520compressive%2520dynamic%2520MRI%250Areconstruction%252C%2520consistently%2520demonstrate%2520the%2520superiority%2520of%2520GridTD%2520over%250Aexisting%2520methods%252C%2520positioning%2520GridTD%2520as%2520a%2520versatile%2520and%2520state-of-the-art%2520CI%250Areconstruction%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressive%20Imaging%20Reconstruction%20via%20Tensor%20Decomposed%0A%20%20Multi-Resolution%20Grid%20Encoding&entry.906535625=Zhenyu%20Jin%20and%20Yisi%20Luo%20and%20Xile%20Zhao%20and%20Deyu%20Meng&entry.1292438233=%20%20Compressive%20imaging%20%28CI%29%20reconstruction%2C%20such%20as%20snapshot%20compressive%20imaging%0A%28SCI%29%20and%20compressive%20sensing%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20aims%20to%20recover%0Ahigh-dimensional%20images%20from%20low-dimensional%20compressed%20measurements.%20This%0Aprocess%20critically%20relies%20on%20learning%20an%20accurate%20representation%20of%20the%0Aunderlying%20high-dimensional%20image.%20However%2C%20existing%20unsupervised%0Arepresentations%20may%20struggle%20to%20achieve%20a%20desired%20balance%20between%0Arepresentation%20ability%20and%20efficiency.%20To%20overcome%20this%20limitation%2C%20we%20propose%0ATensor%20Decomposed%20multi-resolution%20Grid%20encoding%20%28GridTD%29%2C%20an%20unsupervised%0Acontinuous%20representation%20framework%20for%20CI%20reconstruction.%20GridTD%20optimizes%20a%0Alightweight%20neural%20network%20and%20the%20input%20tensor%20decomposition%20model%20whose%0Aparameters%20are%20learned%20via%20multi-resolution%20hash%20grid%20encoding.%20It%20inherently%0Aenjoys%20the%20hierarchical%20modeling%20ability%20of%20multi-resolution%20grid%20encoding%20and%0Athe%20compactness%20of%20tensor%20decomposition%2C%20enabling%20effective%20and%20efficient%0Areconstruction%20of%20high-dimensional%20images.%20Theoretical%20analyses%20for%20the%0Aalgorithm%27s%20Lipschitz%20property%2C%20generalization%20error%20bound%2C%20and%20fixed-point%0Aconvergence%20reveal%20the%20intrinsic%20superiority%20of%20GridTD%20as%20compared%20with%0Aexisting%20continuous%20representation%20models.%20Extensive%20experiments%20across%20diverse%0ACI%20tasks%2C%20including%20video%20SCI%2C%20spectral%20SCI%2C%20and%20compressive%20dynamic%20MRI%0Areconstruction%2C%20consistently%20demonstrate%20the%20superiority%20of%20GridTD%20over%0Aexisting%20methods%2C%20positioning%20GridTD%20as%20a%20versatile%20and%20state-of-the-art%20CI%0Areconstruction%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07707v1&entry.124074799=Read"},
{"title": "One Object, Multiple Lies: A Benchmark for Cross-task Adversarial Attack\n  on Unified Vision-Language Models", "author": "Jiale Zhao and Xinyang Jiang and Junyao Gao and Yuhao Xue and Cairong Zhao", "abstract": "  Unified vision-language models(VLMs) have recently shown remarkable progress,\nenabling a single model to flexibly address diverse tasks through different\ninstructions within a shared computational architecture. This instruction-based\ncontrol mechanism creates unique security challenges, as adversarial inputs\nmust remain effective across multiple task instructions that may be\nunpredictably applied to process the same malicious content. In this paper, we\nintroduce CrossVLAD, a new benchmark dataset carefully curated from MSCOCO with\nGPT-4-assisted annotations for systematically evaluating cross-task adversarial\nattacks on unified VLMs. CrossVLAD centers on the object-change\nobjective-consistently manipulating a target object's classification across\nfour downstream tasks-and proposes a novel success rate metric that measures\nsimultaneous misclassification across all tasks, providing a rigorous\nevaluation of adversarial transferability. To tackle this challenge, we present\nCRAFT (Cross-task Region-based Attack Framework with Token-alignment), an\nefficient region-centric attack method. Extensive experiments on Florence-2 and\nother popular unified VLMs demonstrate that our method outperforms existing\napproaches in both overall cross-task attack performance and targeted\nobject-change success rates, highlighting its effectiveness in adversarially\ninfluencing unified VLMs across diverse tasks.\n", "link": "http://arxiv.org/abs/2507.07709v1", "date": "2025-07-10", "relevancy": 2.7053, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5695}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Object%2C%20Multiple%20Lies%3A%20A%20Benchmark%20for%20Cross-task%20Adversarial%20Attack%0A%20%20on%20Unified%20Vision-Language%20Models&body=Title%3A%20One%20Object%2C%20Multiple%20Lies%3A%20A%20Benchmark%20for%20Cross-task%20Adversarial%20Attack%0A%20%20on%20Unified%20Vision-Language%20Models%0AAuthor%3A%20Jiale%20Zhao%20and%20Xinyang%20Jiang%20and%20Junyao%20Gao%20and%20Yuhao%20Xue%20and%20Cairong%20Zhao%0AAbstract%3A%20%20%20Unified%20vision-language%20models%28VLMs%29%20have%20recently%20shown%20remarkable%20progress%2C%0Aenabling%20a%20single%20model%20to%20flexibly%20address%20diverse%20tasks%20through%20different%0Ainstructions%20within%20a%20shared%20computational%20architecture.%20This%20instruction-based%0Acontrol%20mechanism%20creates%20unique%20security%20challenges%2C%20as%20adversarial%20inputs%0Amust%20remain%20effective%20across%20multiple%20task%20instructions%20that%20may%20be%0Aunpredictably%20applied%20to%20process%20the%20same%20malicious%20content.%20In%20this%20paper%2C%20we%0Aintroduce%20CrossVLAD%2C%20a%20new%20benchmark%20dataset%20carefully%20curated%20from%20MSCOCO%20with%0AGPT-4-assisted%20annotations%20for%20systematically%20evaluating%20cross-task%20adversarial%0Aattacks%20on%20unified%20VLMs.%20CrossVLAD%20centers%20on%20the%20object-change%0Aobjective-consistently%20manipulating%20a%20target%20object%27s%20classification%20across%0Afour%20downstream%20tasks-and%20proposes%20a%20novel%20success%20rate%20metric%20that%20measures%0Asimultaneous%20misclassification%20across%20all%20tasks%2C%20providing%20a%20rigorous%0Aevaluation%20of%20adversarial%20transferability.%20To%20tackle%20this%20challenge%2C%20we%20present%0ACRAFT%20%28Cross-task%20Region-based%20Attack%20Framework%20with%20Token-alignment%29%2C%20an%0Aefficient%20region-centric%20attack%20method.%20Extensive%20experiments%20on%20Florence-2%20and%0Aother%20popular%20unified%20VLMs%20demonstrate%20that%20our%20method%20outperforms%20existing%0Aapproaches%20in%20both%20overall%20cross-task%20attack%20performance%20and%20targeted%0Aobject-change%20success%20rates%2C%20highlighting%20its%20effectiveness%20in%20adversarially%0Ainfluencing%20unified%20VLMs%20across%20diverse%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Object%252C%2520Multiple%2520Lies%253A%2520A%2520Benchmark%2520for%2520Cross-task%2520Adversarial%2520Attack%250A%2520%2520on%2520Unified%2520Vision-Language%2520Models%26entry.906535625%3DJiale%2520Zhao%2520and%2520Xinyang%2520Jiang%2520and%2520Junyao%2520Gao%2520and%2520Yuhao%2520Xue%2520and%2520Cairong%2520Zhao%26entry.1292438233%3D%2520%2520Unified%2520vision-language%2520models%2528VLMs%2529%2520have%2520recently%2520shown%2520remarkable%2520progress%252C%250Aenabling%2520a%2520single%2520model%2520to%2520flexibly%2520address%2520diverse%2520tasks%2520through%2520different%250Ainstructions%2520within%2520a%2520shared%2520computational%2520architecture.%2520This%2520instruction-based%250Acontrol%2520mechanism%2520creates%2520unique%2520security%2520challenges%252C%2520as%2520adversarial%2520inputs%250Amust%2520remain%2520effective%2520across%2520multiple%2520task%2520instructions%2520that%2520may%2520be%250Aunpredictably%2520applied%2520to%2520process%2520the%2520same%2520malicious%2520content.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520CrossVLAD%252C%2520a%2520new%2520benchmark%2520dataset%2520carefully%2520curated%2520from%2520MSCOCO%2520with%250AGPT-4-assisted%2520annotations%2520for%2520systematically%2520evaluating%2520cross-task%2520adversarial%250Aattacks%2520on%2520unified%2520VLMs.%2520CrossVLAD%2520centers%2520on%2520the%2520object-change%250Aobjective-consistently%2520manipulating%2520a%2520target%2520object%2527s%2520classification%2520across%250Afour%2520downstream%2520tasks-and%2520proposes%2520a%2520novel%2520success%2520rate%2520metric%2520that%2520measures%250Asimultaneous%2520misclassification%2520across%2520all%2520tasks%252C%2520providing%2520a%2520rigorous%250Aevaluation%2520of%2520adversarial%2520transferability.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520present%250ACRAFT%2520%2528Cross-task%2520Region-based%2520Attack%2520Framework%2520with%2520Token-alignment%2529%252C%2520an%250Aefficient%2520region-centric%2520attack%2520method.%2520Extensive%2520experiments%2520on%2520Florence-2%2520and%250Aother%2520popular%2520unified%2520VLMs%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%250Aapproaches%2520in%2520both%2520overall%2520cross-task%2520attack%2520performance%2520and%2520targeted%250Aobject-change%2520success%2520rates%252C%2520highlighting%2520its%2520effectiveness%2520in%2520adversarially%250Ainfluencing%2520unified%2520VLMs%2520across%2520diverse%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Object%2C%20Multiple%20Lies%3A%20A%20Benchmark%20for%20Cross-task%20Adversarial%20Attack%0A%20%20on%20Unified%20Vision-Language%20Models&entry.906535625=Jiale%20Zhao%20and%20Xinyang%20Jiang%20and%20Junyao%20Gao%20and%20Yuhao%20Xue%20and%20Cairong%20Zhao&entry.1292438233=%20%20Unified%20vision-language%20models%28VLMs%29%20have%20recently%20shown%20remarkable%20progress%2C%0Aenabling%20a%20single%20model%20to%20flexibly%20address%20diverse%20tasks%20through%20different%0Ainstructions%20within%20a%20shared%20computational%20architecture.%20This%20instruction-based%0Acontrol%20mechanism%20creates%20unique%20security%20challenges%2C%20as%20adversarial%20inputs%0Amust%20remain%20effective%20across%20multiple%20task%20instructions%20that%20may%20be%0Aunpredictably%20applied%20to%20process%20the%20same%20malicious%20content.%20In%20this%20paper%2C%20we%0Aintroduce%20CrossVLAD%2C%20a%20new%20benchmark%20dataset%20carefully%20curated%20from%20MSCOCO%20with%0AGPT-4-assisted%20annotations%20for%20systematically%20evaluating%20cross-task%20adversarial%0Aattacks%20on%20unified%20VLMs.%20CrossVLAD%20centers%20on%20the%20object-change%0Aobjective-consistently%20manipulating%20a%20target%20object%27s%20classification%20across%0Afour%20downstream%20tasks-and%20proposes%20a%20novel%20success%20rate%20metric%20that%20measures%0Asimultaneous%20misclassification%20across%20all%20tasks%2C%20providing%20a%20rigorous%0Aevaluation%20of%20adversarial%20transferability.%20To%20tackle%20this%20challenge%2C%20we%20present%0ACRAFT%20%28Cross-task%20Region-based%20Attack%20Framework%20with%20Token-alignment%29%2C%20an%0Aefficient%20region-centric%20attack%20method.%20Extensive%20experiments%20on%20Florence-2%20and%0Aother%20popular%20unified%20VLMs%20demonstrate%20that%20our%20method%20outperforms%20existing%0Aapproaches%20in%20both%20overall%20cross-task%20attack%20performance%20and%20targeted%0Aobject-change%20success%20rates%2C%20highlighting%20its%20effectiveness%20in%20adversarially%0Ainfluencing%20unified%20VLMs%20across%20diverse%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07709v1&entry.124074799=Read"},
{"title": "HiM2SAM: Enhancing SAM2 with Hierarchical Motion Estimation and Memory\n  Optimization towards Long-term Tracking", "author": "Ruixiang Chen and Guolei Sun and Yawei Li and Jie Qin and Luca Benini", "abstract": "  This paper presents enhancements to the SAM2 framework for video object\ntracking task, addressing challenges such as occlusions, background clutter,\nand target reappearance. We introduce a hierarchical motion estimation\nstrategy, combining lightweight linear prediction with selective non-linear\nrefinement to improve tracking accuracy without requiring additional training.\nIn addition, we optimize the memory bank by distinguishing long-term and\nshort-term memory frames, enabling more reliable tracking under long-term\nocclusions and appearance changes. Experimental results show consistent\nimprovements across different model scales. Our method achieves\nstate-of-the-art performance on LaSOT and LaSOText with the large model,\nachieving 9.6% and 7.2% relative improvements in AUC over the original SAM2,\nand demonstrates even larger relative gains on smaller models, highlighting the\neffectiveness of our trainless, low-overhead improvements for boosting\nlong-term tracking performance. The code is available at\nhttps://github.com/LouisFinner/HiM2SAM.\n", "link": "http://arxiv.org/abs/2507.07603v1", "date": "2025-07-10", "relevancy": 2.6631, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5335}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5329}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiM2SAM%3A%20Enhancing%20SAM2%20with%20Hierarchical%20Motion%20Estimation%20and%20Memory%0A%20%20Optimization%20towards%20Long-term%20Tracking&body=Title%3A%20HiM2SAM%3A%20Enhancing%20SAM2%20with%20Hierarchical%20Motion%20Estimation%20and%20Memory%0A%20%20Optimization%20towards%20Long-term%20Tracking%0AAuthor%3A%20Ruixiang%20Chen%20and%20Guolei%20Sun%20and%20Yawei%20Li%20and%20Jie%20Qin%20and%20Luca%20Benini%0AAbstract%3A%20%20%20This%20paper%20presents%20enhancements%20to%20the%20SAM2%20framework%20for%20video%20object%0Atracking%20task%2C%20addressing%20challenges%20such%20as%20occlusions%2C%20background%20clutter%2C%0Aand%20target%20reappearance.%20We%20introduce%20a%20hierarchical%20motion%20estimation%0Astrategy%2C%20combining%20lightweight%20linear%20prediction%20with%20selective%20non-linear%0Arefinement%20to%20improve%20tracking%20accuracy%20without%20requiring%20additional%20training.%0AIn%20addition%2C%20we%20optimize%20the%20memory%20bank%20by%20distinguishing%20long-term%20and%0Ashort-term%20memory%20frames%2C%20enabling%20more%20reliable%20tracking%20under%20long-term%0Aocclusions%20and%20appearance%20changes.%20Experimental%20results%20show%20consistent%0Aimprovements%20across%20different%20model%20scales.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20on%20LaSOT%20and%20LaSOText%20with%20the%20large%20model%2C%0Aachieving%209.6%25%20and%207.2%25%20relative%20improvements%20in%20AUC%20over%20the%20original%20SAM2%2C%0Aand%20demonstrates%20even%20larger%20relative%20gains%20on%20smaller%20models%2C%20highlighting%20the%0Aeffectiveness%20of%20our%20trainless%2C%20low-overhead%20improvements%20for%20boosting%0Along-term%20tracking%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LouisFinner/HiM2SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiM2SAM%253A%2520Enhancing%2520SAM2%2520with%2520Hierarchical%2520Motion%2520Estimation%2520and%2520Memory%250A%2520%2520Optimization%2520towards%2520Long-term%2520Tracking%26entry.906535625%3DRuixiang%2520Chen%2520and%2520Guolei%2520Sun%2520and%2520Yawei%2520Li%2520and%2520Jie%2520Qin%2520and%2520Luca%2520Benini%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520enhancements%2520to%2520the%2520SAM2%2520framework%2520for%2520video%2520object%250Atracking%2520task%252C%2520addressing%2520challenges%2520such%2520as%2520occlusions%252C%2520background%2520clutter%252C%250Aand%2520target%2520reappearance.%2520We%2520introduce%2520a%2520hierarchical%2520motion%2520estimation%250Astrategy%252C%2520combining%2520lightweight%2520linear%2520prediction%2520with%2520selective%2520non-linear%250Arefinement%2520to%2520improve%2520tracking%2520accuracy%2520without%2520requiring%2520additional%2520training.%250AIn%2520addition%252C%2520we%2520optimize%2520the%2520memory%2520bank%2520by%2520distinguishing%2520long-term%2520and%250Ashort-term%2520memory%2520frames%252C%2520enabling%2520more%2520reliable%2520tracking%2520under%2520long-term%250Aocclusions%2520and%2520appearance%2520changes.%2520Experimental%2520results%2520show%2520consistent%250Aimprovements%2520across%2520different%2520model%2520scales.%2520Our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520on%2520LaSOT%2520and%2520LaSOText%2520with%2520the%2520large%2520model%252C%250Aachieving%25209.6%2525%2520and%25207.2%2525%2520relative%2520improvements%2520in%2520AUC%2520over%2520the%2520original%2520SAM2%252C%250Aand%2520demonstrates%2520even%2520larger%2520relative%2520gains%2520on%2520smaller%2520models%252C%2520highlighting%2520the%250Aeffectiveness%2520of%2520our%2520trainless%252C%2520low-overhead%2520improvements%2520for%2520boosting%250Along-term%2520tracking%2520performance.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LouisFinner/HiM2SAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiM2SAM%3A%20Enhancing%20SAM2%20with%20Hierarchical%20Motion%20Estimation%20and%20Memory%0A%20%20Optimization%20towards%20Long-term%20Tracking&entry.906535625=Ruixiang%20Chen%20and%20Guolei%20Sun%20and%20Yawei%20Li%20and%20Jie%20Qin%20and%20Luca%20Benini&entry.1292438233=%20%20This%20paper%20presents%20enhancements%20to%20the%20SAM2%20framework%20for%20video%20object%0Atracking%20task%2C%20addressing%20challenges%20such%20as%20occlusions%2C%20background%20clutter%2C%0Aand%20target%20reappearance.%20We%20introduce%20a%20hierarchical%20motion%20estimation%0Astrategy%2C%20combining%20lightweight%20linear%20prediction%20with%20selective%20non-linear%0Arefinement%20to%20improve%20tracking%20accuracy%20without%20requiring%20additional%20training.%0AIn%20addition%2C%20we%20optimize%20the%20memory%20bank%20by%20distinguishing%20long-term%20and%0Ashort-term%20memory%20frames%2C%20enabling%20more%20reliable%20tracking%20under%20long-term%0Aocclusions%20and%20appearance%20changes.%20Experimental%20results%20show%20consistent%0Aimprovements%20across%20different%20model%20scales.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20on%20LaSOT%20and%20LaSOText%20with%20the%20large%20model%2C%0Aachieving%209.6%25%20and%207.2%25%20relative%20improvements%20in%20AUC%20over%20the%20original%20SAM2%2C%0Aand%20demonstrates%20even%20larger%20relative%20gains%20on%20smaller%20models%2C%20highlighting%20the%0Aeffectiveness%20of%20our%20trainless%2C%20low-overhead%20improvements%20for%20boosting%0Along-term%20tracking%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LouisFinner/HiM2SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07603v1&entry.124074799=Read"},
{"title": "SVIP: Semantically Contextualized Visual Patches for Zero-Shot Learning", "author": "Zhi Chen and Zecheng Zhao and Jingcai Guo and Jingjing Li and Zi Huang", "abstract": "  Zero-shot learning (ZSL) aims to recognize unseen classes without labeled\ntraining examples by leveraging class-level semantic descriptors such as\nattributes. A fundamental challenge in ZSL is semantic misalignment, where\nsemantic-unrelated information involved in visual features introduce ambiguity\nto visual-semantic interaction. Unlike existing methods that suppress\nsemantic-unrelated information post hoc either in the feature space or the\nmodel space, we propose addressing this issue at the input stage, preventing\nsemantic-unrelated patches from propagating through the network. To this end,\nwe introduce Semantically contextualized VIsual Patches (SVIP) for ZSL, a\ntransformer-based framework designed to enhance visual-semantic alignment.\nSpecifically, we propose a self-supervised patch selection mechanism that\npreemptively learns to identify semantic-unrelated patches in the input space.\nThis is trained with the supervision from aggregated attention scores across\nall transformer layers, which estimate each patch's semantic score. As removing\nsemantic-unrelated patches from the input sequence may disrupt object\nstructure, we replace them with learnable patch embeddings. With initialization\nfrom word embeddings, we can ensure they remain semantically meaningful\nthroughout feature extraction. Extensive experiments on ZSL benchmarks\ndemonstrate that SVIP achieves state-of-the-art performance results while\nproviding more interpretable and semantically rich feature representations.\nCode is available at https://github.com/uqzhichen/SVIP.\n", "link": "http://arxiv.org/abs/2503.10252v2", "date": "2025-07-10", "relevancy": 2.6584, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVIP%3A%20Semantically%20Contextualized%20Visual%20Patches%20for%20Zero-Shot%20Learning&body=Title%3A%20SVIP%3A%20Semantically%20Contextualized%20Visual%20Patches%20for%20Zero-Shot%20Learning%0AAuthor%3A%20Zhi%20Chen%20and%20Zecheng%20Zhao%20and%20Jingcai%20Guo%20and%20Jingjing%20Li%20and%20Zi%20Huang%0AAbstract%3A%20%20%20Zero-shot%20learning%20%28ZSL%29%20aims%20to%20recognize%20unseen%20classes%20without%20labeled%0Atraining%20examples%20by%20leveraging%20class-level%20semantic%20descriptors%20such%20as%0Aattributes.%20A%20fundamental%20challenge%20in%20ZSL%20is%20semantic%20misalignment%2C%20where%0Asemantic-unrelated%20information%20involved%20in%20visual%20features%20introduce%20ambiguity%0Ato%20visual-semantic%20interaction.%20Unlike%20existing%20methods%20that%20suppress%0Asemantic-unrelated%20information%20post%20hoc%20either%20in%20the%20feature%20space%20or%20the%0Amodel%20space%2C%20we%20propose%20addressing%20this%20issue%20at%20the%20input%20stage%2C%20preventing%0Asemantic-unrelated%20patches%20from%20propagating%20through%20the%20network.%20To%20this%20end%2C%0Awe%20introduce%20Semantically%20contextualized%20VIsual%20Patches%20%28SVIP%29%20for%20ZSL%2C%20a%0Atransformer-based%20framework%20designed%20to%20enhance%20visual-semantic%20alignment.%0ASpecifically%2C%20we%20propose%20a%20self-supervised%20patch%20selection%20mechanism%20that%0Apreemptively%20learns%20to%20identify%20semantic-unrelated%20patches%20in%20the%20input%20space.%0AThis%20is%20trained%20with%20the%20supervision%20from%20aggregated%20attention%20scores%20across%0Aall%20transformer%20layers%2C%20which%20estimate%20each%20patch%27s%20semantic%20score.%20As%20removing%0Asemantic-unrelated%20patches%20from%20the%20input%20sequence%20may%20disrupt%20object%0Astructure%2C%20we%20replace%20them%20with%20learnable%20patch%20embeddings.%20With%20initialization%0Afrom%20word%20embeddings%2C%20we%20can%20ensure%20they%20remain%20semantically%20meaningful%0Athroughout%20feature%20extraction.%20Extensive%20experiments%20on%20ZSL%20benchmarks%0Ademonstrate%20that%20SVIP%20achieves%20state-of-the-art%20performance%20results%20while%0Aproviding%20more%20interpretable%20and%20semantically%20rich%20feature%20representations.%0ACode%20is%20available%20at%20https%3A//github.com/uqzhichen/SVIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVIP%253A%2520Semantically%2520Contextualized%2520Visual%2520Patches%2520for%2520Zero-Shot%2520Learning%26entry.906535625%3DZhi%2520Chen%2520and%2520Zecheng%2520Zhao%2520and%2520Jingcai%2520Guo%2520and%2520Jingjing%2520Li%2520and%2520Zi%2520Huang%26entry.1292438233%3D%2520%2520Zero-shot%2520learning%2520%2528ZSL%2529%2520aims%2520to%2520recognize%2520unseen%2520classes%2520without%2520labeled%250Atraining%2520examples%2520by%2520leveraging%2520class-level%2520semantic%2520descriptors%2520such%2520as%250Aattributes.%2520A%2520fundamental%2520challenge%2520in%2520ZSL%2520is%2520semantic%2520misalignment%252C%2520where%250Asemantic-unrelated%2520information%2520involved%2520in%2520visual%2520features%2520introduce%2520ambiguity%250Ato%2520visual-semantic%2520interaction.%2520Unlike%2520existing%2520methods%2520that%2520suppress%250Asemantic-unrelated%2520information%2520post%2520hoc%2520either%2520in%2520the%2520feature%2520space%2520or%2520the%250Amodel%2520space%252C%2520we%2520propose%2520addressing%2520this%2520issue%2520at%2520the%2520input%2520stage%252C%2520preventing%250Asemantic-unrelated%2520patches%2520from%2520propagating%2520through%2520the%2520network.%2520To%2520this%2520end%252C%250Awe%2520introduce%2520Semantically%2520contextualized%2520VIsual%2520Patches%2520%2528SVIP%2529%2520for%2520ZSL%252C%2520a%250Atransformer-based%2520framework%2520designed%2520to%2520enhance%2520visual-semantic%2520alignment.%250ASpecifically%252C%2520we%2520propose%2520a%2520self-supervised%2520patch%2520selection%2520mechanism%2520that%250Apreemptively%2520learns%2520to%2520identify%2520semantic-unrelated%2520patches%2520in%2520the%2520input%2520space.%250AThis%2520is%2520trained%2520with%2520the%2520supervision%2520from%2520aggregated%2520attention%2520scores%2520across%250Aall%2520transformer%2520layers%252C%2520which%2520estimate%2520each%2520patch%2527s%2520semantic%2520score.%2520As%2520removing%250Asemantic-unrelated%2520patches%2520from%2520the%2520input%2520sequence%2520may%2520disrupt%2520object%250Astructure%252C%2520we%2520replace%2520them%2520with%2520learnable%2520patch%2520embeddings.%2520With%2520initialization%250Afrom%2520word%2520embeddings%252C%2520we%2520can%2520ensure%2520they%2520remain%2520semantically%2520meaningful%250Athroughout%2520feature%2520extraction.%2520Extensive%2520experiments%2520on%2520ZSL%2520benchmarks%250Ademonstrate%2520that%2520SVIP%2520achieves%2520state-of-the-art%2520performance%2520results%2520while%250Aproviding%2520more%2520interpretable%2520and%2520semantically%2520rich%2520feature%2520representations.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/uqzhichen/SVIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVIP%3A%20Semantically%20Contextualized%20Visual%20Patches%20for%20Zero-Shot%20Learning&entry.906535625=Zhi%20Chen%20and%20Zecheng%20Zhao%20and%20Jingcai%20Guo%20and%20Jingjing%20Li%20and%20Zi%20Huang&entry.1292438233=%20%20Zero-shot%20learning%20%28ZSL%29%20aims%20to%20recognize%20unseen%20classes%20without%20labeled%0Atraining%20examples%20by%20leveraging%20class-level%20semantic%20descriptors%20such%20as%0Aattributes.%20A%20fundamental%20challenge%20in%20ZSL%20is%20semantic%20misalignment%2C%20where%0Asemantic-unrelated%20information%20involved%20in%20visual%20features%20introduce%20ambiguity%0Ato%20visual-semantic%20interaction.%20Unlike%20existing%20methods%20that%20suppress%0Asemantic-unrelated%20information%20post%20hoc%20either%20in%20the%20feature%20space%20or%20the%0Amodel%20space%2C%20we%20propose%20addressing%20this%20issue%20at%20the%20input%20stage%2C%20preventing%0Asemantic-unrelated%20patches%20from%20propagating%20through%20the%20network.%20To%20this%20end%2C%0Awe%20introduce%20Semantically%20contextualized%20VIsual%20Patches%20%28SVIP%29%20for%20ZSL%2C%20a%0Atransformer-based%20framework%20designed%20to%20enhance%20visual-semantic%20alignment.%0ASpecifically%2C%20we%20propose%20a%20self-supervised%20patch%20selection%20mechanism%20that%0Apreemptively%20learns%20to%20identify%20semantic-unrelated%20patches%20in%20the%20input%20space.%0AThis%20is%20trained%20with%20the%20supervision%20from%20aggregated%20attention%20scores%20across%0Aall%20transformer%20layers%2C%20which%20estimate%20each%20patch%27s%20semantic%20score.%20As%20removing%0Asemantic-unrelated%20patches%20from%20the%20input%20sequence%20may%20disrupt%20object%0Astructure%2C%20we%20replace%20them%20with%20learnable%20patch%20embeddings.%20With%20initialization%0Afrom%20word%20embeddings%2C%20we%20can%20ensure%20they%20remain%20semantically%20meaningful%0Athroughout%20feature%20extraction.%20Extensive%20experiments%20on%20ZSL%20benchmarks%0Ademonstrate%20that%20SVIP%20achieves%20state-of-the-art%20performance%20results%20while%0Aproviding%20more%20interpretable%20and%20semantically%20rich%20feature%20representations.%0ACode%20is%20available%20at%20https%3A//github.com/uqzhichen/SVIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10252v2&entry.124074799=Read"},
{"title": "EEvAct: Early Event-Based Action Recognition with High-Rate Two-Stream\n  Spiking Neural Networks", "author": "Michael Neumeier and Jules Lecomte and Nils Kazinski and Soubarna Banik and Bing Li and Axel von Arnim", "abstract": "  Recognizing human activities early is crucial for the safety and\nresponsiveness of human-robot and human-machine interfaces. Due to their high\ntemporal resolution and low latency, event-based vision sensors are a perfect\nmatch for this early recognition demand. However, most existing processing\napproaches accumulate events to low-rate frames or space-time voxels which\nlimits the early prediction capabilities. In contrast, spiking neural networks\n(SNNs) can process the events at a high-rate for early predictions, but most\nworks still fall short on final accuracy. In this work, we introduce a\nhigh-rate two-stream SNN which closes this gap by outperforming previous work\nby 2% in final accuracy on the large-scale THU EACT-50 dataset. We benchmark\nthe SNNs within a novel early event-based recognition framework by reporting\nTop-1 and Top-5 recognition scores for growing observation time. Finally, we\nexemplify the impact of these methods on a real-world task of early action\ntriggering for human motion capture in sports.\n", "link": "http://arxiv.org/abs/2507.07734v1", "date": "2025-07-10", "relevancy": 2.6451, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5363}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5353}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEvAct%3A%20Early%20Event-Based%20Action%20Recognition%20with%20High-Rate%20Two-Stream%0A%20%20Spiking%20Neural%20Networks&body=Title%3A%20EEvAct%3A%20Early%20Event-Based%20Action%20Recognition%20with%20High-Rate%20Two-Stream%0A%20%20Spiking%20Neural%20Networks%0AAuthor%3A%20Michael%20Neumeier%20and%20Jules%20Lecomte%20and%20Nils%20Kazinski%20and%20Soubarna%20Banik%20and%20Bing%20Li%20and%20Axel%20von%20Arnim%0AAbstract%3A%20%20%20Recognizing%20human%20activities%20early%20is%20crucial%20for%20the%20safety%20and%0Aresponsiveness%20of%20human-robot%20and%20human-machine%20interfaces.%20Due%20to%20their%20high%0Atemporal%20resolution%20and%20low%20latency%2C%20event-based%20vision%20sensors%20are%20a%20perfect%0Amatch%20for%20this%20early%20recognition%20demand.%20However%2C%20most%20existing%20processing%0Aapproaches%20accumulate%20events%20to%20low-rate%20frames%20or%20space-time%20voxels%20which%0Alimits%20the%20early%20prediction%20capabilities.%20In%20contrast%2C%20spiking%20neural%20networks%0A%28SNNs%29%20can%20process%20the%20events%20at%20a%20high-rate%20for%20early%20predictions%2C%20but%20most%0Aworks%20still%20fall%20short%20on%20final%20accuracy.%20In%20this%20work%2C%20we%20introduce%20a%0Ahigh-rate%20two-stream%20SNN%20which%20closes%20this%20gap%20by%20outperforming%20previous%20work%0Aby%202%25%20in%20final%20accuracy%20on%20the%20large-scale%20THU%20EACT-50%20dataset.%20We%20benchmark%0Athe%20SNNs%20within%20a%20novel%20early%20event-based%20recognition%20framework%20by%20reporting%0ATop-1%20and%20Top-5%20recognition%20scores%20for%20growing%20observation%20time.%20Finally%2C%20we%0Aexemplify%20the%20impact%20of%20these%20methods%20on%20a%20real-world%20task%20of%20early%20action%0Atriggering%20for%20human%20motion%20capture%20in%20sports.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEvAct%253A%2520Early%2520Event-Based%2520Action%2520Recognition%2520with%2520High-Rate%2520Two-Stream%250A%2520%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DMichael%2520Neumeier%2520and%2520Jules%2520Lecomte%2520and%2520Nils%2520Kazinski%2520and%2520Soubarna%2520Banik%2520and%2520Bing%2520Li%2520and%2520Axel%2520von%2520Arnim%26entry.1292438233%3D%2520%2520Recognizing%2520human%2520activities%2520early%2520is%2520crucial%2520for%2520the%2520safety%2520and%250Aresponsiveness%2520of%2520human-robot%2520and%2520human-machine%2520interfaces.%2520Due%2520to%2520their%2520high%250Atemporal%2520resolution%2520and%2520low%2520latency%252C%2520event-based%2520vision%2520sensors%2520are%2520a%2520perfect%250Amatch%2520for%2520this%2520early%2520recognition%2520demand.%2520However%252C%2520most%2520existing%2520processing%250Aapproaches%2520accumulate%2520events%2520to%2520low-rate%2520frames%2520or%2520space-time%2520voxels%2520which%250Alimits%2520the%2520early%2520prediction%2520capabilities.%2520In%2520contrast%252C%2520spiking%2520neural%2520networks%250A%2528SNNs%2529%2520can%2520process%2520the%2520events%2520at%2520a%2520high-rate%2520for%2520early%2520predictions%252C%2520but%2520most%250Aworks%2520still%2520fall%2520short%2520on%2520final%2520accuracy.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Ahigh-rate%2520two-stream%2520SNN%2520which%2520closes%2520this%2520gap%2520by%2520outperforming%2520previous%2520work%250Aby%25202%2525%2520in%2520final%2520accuracy%2520on%2520the%2520large-scale%2520THU%2520EACT-50%2520dataset.%2520We%2520benchmark%250Athe%2520SNNs%2520within%2520a%2520novel%2520early%2520event-based%2520recognition%2520framework%2520by%2520reporting%250ATop-1%2520and%2520Top-5%2520recognition%2520scores%2520for%2520growing%2520observation%2520time.%2520Finally%252C%2520we%250Aexemplify%2520the%2520impact%2520of%2520these%2520methods%2520on%2520a%2520real-world%2520task%2520of%2520early%2520action%250Atriggering%2520for%2520human%2520motion%2520capture%2520in%2520sports.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEvAct%3A%20Early%20Event-Based%20Action%20Recognition%20with%20High-Rate%20Two-Stream%0A%20%20Spiking%20Neural%20Networks&entry.906535625=Michael%20Neumeier%20and%20Jules%20Lecomte%20and%20Nils%20Kazinski%20and%20Soubarna%20Banik%20and%20Bing%20Li%20and%20Axel%20von%20Arnim&entry.1292438233=%20%20Recognizing%20human%20activities%20early%20is%20crucial%20for%20the%20safety%20and%0Aresponsiveness%20of%20human-robot%20and%20human-machine%20interfaces.%20Due%20to%20their%20high%0Atemporal%20resolution%20and%20low%20latency%2C%20event-based%20vision%20sensors%20are%20a%20perfect%0Amatch%20for%20this%20early%20recognition%20demand.%20However%2C%20most%20existing%20processing%0Aapproaches%20accumulate%20events%20to%20low-rate%20frames%20or%20space-time%20voxels%20which%0Alimits%20the%20early%20prediction%20capabilities.%20In%20contrast%2C%20spiking%20neural%20networks%0A%28SNNs%29%20can%20process%20the%20events%20at%20a%20high-rate%20for%20early%20predictions%2C%20but%20most%0Aworks%20still%20fall%20short%20on%20final%20accuracy.%20In%20this%20work%2C%20we%20introduce%20a%0Ahigh-rate%20two-stream%20SNN%20which%20closes%20this%20gap%20by%20outperforming%20previous%20work%0Aby%202%25%20in%20final%20accuracy%20on%20the%20large-scale%20THU%20EACT-50%20dataset.%20We%20benchmark%0Athe%20SNNs%20within%20a%20novel%20early%20event-based%20recognition%20framework%20by%20reporting%0ATop-1%20and%20Top-5%20recognition%20scores%20for%20growing%20observation%20time.%20Finally%2C%20we%0Aexemplify%20the%20impact%20of%20these%20methods%20on%20a%20real-world%20task%20of%20early%20action%0Atriggering%20for%20human%20motion%20capture%20in%20sports.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07734v1&entry.124074799=Read"},
{"title": "Sparse Causal Discovery with Generative Intervention for Unsupervised\n  Graph Domain Adaptation", "author": "Junyu Luo and Yuhao Tang and Yiwei Fu and Xiao Luo and Zhizhuo Kou and Zhiping Xiao and Wei Ju and Wentao Zhang and Ming Zhang", "abstract": "  Unsupervised Graph Domain Adaptation (UGDA) leverages labeled source domain\ngraphs to achieve effective performance in unlabeled target domains despite\ndistribution shifts. However, existing methods often yield suboptimal results\ndue to the entanglement of causal-spurious features and the failure of global\nalignment strategies. We propose SLOGAN (Sparse Causal Discovery with\nGenerative Intervention), a novel approach that achieves stable graph\nrepresentation transfer through sparse causal modeling and dynamic intervention\nmechanisms. Specifically, SLOGAN first constructs a sparse causal graph\nstructure, leveraging mutual information bottleneck constraints to disentangle\nsparse, stable causal features while compressing domain-dependent spurious\ncorrelations through variational inference. To address residual spurious\ncorrelations, we innovatively design a generative intervention mechanism that\nbreaks local spurious couplings through cross-domain feature recombination\nwhile maintaining causal feature semantic consistency via covariance\nconstraints. Furthermore, to mitigate error accumulation in target domain\npseudo-labels, we introduce a category-adaptive dynamic calibration strategy,\nensuring stable discriminative learning. Extensive experiments on multiple\nreal-world datasets demonstrate that SLOGAN significantly outperforms existing\nbaselines.\n", "link": "http://arxiv.org/abs/2507.07621v1", "date": "2025-07-10", "relevancy": 2.6272, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5359}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5276}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Causal%20Discovery%20with%20Generative%20Intervention%20for%20Unsupervised%0A%20%20Graph%20Domain%20Adaptation&body=Title%3A%20Sparse%20Causal%20Discovery%20with%20Generative%20Intervention%20for%20Unsupervised%0A%20%20Graph%20Domain%20Adaptation%0AAuthor%3A%20Junyu%20Luo%20and%20Yuhao%20Tang%20and%20Yiwei%20Fu%20and%20Xiao%20Luo%20and%20Zhizhuo%20Kou%20and%20Zhiping%20Xiao%20and%20Wei%20Ju%20and%20Wentao%20Zhang%20and%20Ming%20Zhang%0AAbstract%3A%20%20%20Unsupervised%20Graph%20Domain%20Adaptation%20%28UGDA%29%20leverages%20labeled%20source%20domain%0Agraphs%20to%20achieve%20effective%20performance%20in%20unlabeled%20target%20domains%20despite%0Adistribution%20shifts.%20However%2C%20existing%20methods%20often%20yield%20suboptimal%20results%0Adue%20to%20the%20entanglement%20of%20causal-spurious%20features%20and%20the%20failure%20of%20global%0Aalignment%20strategies.%20We%20propose%20SLOGAN%20%28Sparse%20Causal%20Discovery%20with%0AGenerative%20Intervention%29%2C%20a%20novel%20approach%20that%20achieves%20stable%20graph%0Arepresentation%20transfer%20through%20sparse%20causal%20modeling%20and%20dynamic%20intervention%0Amechanisms.%20Specifically%2C%20SLOGAN%20first%20constructs%20a%20sparse%20causal%20graph%0Astructure%2C%20leveraging%20mutual%20information%20bottleneck%20constraints%20to%20disentangle%0Asparse%2C%20stable%20causal%20features%20while%20compressing%20domain-dependent%20spurious%0Acorrelations%20through%20variational%20inference.%20To%20address%20residual%20spurious%0Acorrelations%2C%20we%20innovatively%20design%20a%20generative%20intervention%20mechanism%20that%0Abreaks%20local%20spurious%20couplings%20through%20cross-domain%20feature%20recombination%0Awhile%20maintaining%20causal%20feature%20semantic%20consistency%20via%20covariance%0Aconstraints.%20Furthermore%2C%20to%20mitigate%20error%20accumulation%20in%20target%20domain%0Apseudo-labels%2C%20we%20introduce%20a%20category-adaptive%20dynamic%20calibration%20strategy%2C%0Aensuring%20stable%20discriminative%20learning.%20Extensive%20experiments%20on%20multiple%0Areal-world%20datasets%20demonstrate%20that%20SLOGAN%20significantly%20outperforms%20existing%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Causal%2520Discovery%2520with%2520Generative%2520Intervention%2520for%2520Unsupervised%250A%2520%2520Graph%2520Domain%2520Adaptation%26entry.906535625%3DJunyu%2520Luo%2520and%2520Yuhao%2520Tang%2520and%2520Yiwei%2520Fu%2520and%2520Xiao%2520Luo%2520and%2520Zhizhuo%2520Kou%2520and%2520Zhiping%2520Xiao%2520and%2520Wei%2520Ju%2520and%2520Wentao%2520Zhang%2520and%2520Ming%2520Zhang%26entry.1292438233%3D%2520%2520Unsupervised%2520Graph%2520Domain%2520Adaptation%2520%2528UGDA%2529%2520leverages%2520labeled%2520source%2520domain%250Agraphs%2520to%2520achieve%2520effective%2520performance%2520in%2520unlabeled%2520target%2520domains%2520despite%250Adistribution%2520shifts.%2520However%252C%2520existing%2520methods%2520often%2520yield%2520suboptimal%2520results%250Adue%2520to%2520the%2520entanglement%2520of%2520causal-spurious%2520features%2520and%2520the%2520failure%2520of%2520global%250Aalignment%2520strategies.%2520We%2520propose%2520SLOGAN%2520%2528Sparse%2520Causal%2520Discovery%2520with%250AGenerative%2520Intervention%2529%252C%2520a%2520novel%2520approach%2520that%2520achieves%2520stable%2520graph%250Arepresentation%2520transfer%2520through%2520sparse%2520causal%2520modeling%2520and%2520dynamic%2520intervention%250Amechanisms.%2520Specifically%252C%2520SLOGAN%2520first%2520constructs%2520a%2520sparse%2520causal%2520graph%250Astructure%252C%2520leveraging%2520mutual%2520information%2520bottleneck%2520constraints%2520to%2520disentangle%250Asparse%252C%2520stable%2520causal%2520features%2520while%2520compressing%2520domain-dependent%2520spurious%250Acorrelations%2520through%2520variational%2520inference.%2520To%2520address%2520residual%2520spurious%250Acorrelations%252C%2520we%2520innovatively%2520design%2520a%2520generative%2520intervention%2520mechanism%2520that%250Abreaks%2520local%2520spurious%2520couplings%2520through%2520cross-domain%2520feature%2520recombination%250Awhile%2520maintaining%2520causal%2520feature%2520semantic%2520consistency%2520via%2520covariance%250Aconstraints.%2520Furthermore%252C%2520to%2520mitigate%2520error%2520accumulation%2520in%2520target%2520domain%250Apseudo-labels%252C%2520we%2520introduce%2520a%2520category-adaptive%2520dynamic%2520calibration%2520strategy%252C%250Aensuring%2520stable%2520discriminative%2520learning.%2520Extensive%2520experiments%2520on%2520multiple%250Areal-world%2520datasets%2520demonstrate%2520that%2520SLOGAN%2520significantly%2520outperforms%2520existing%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Causal%20Discovery%20with%20Generative%20Intervention%20for%20Unsupervised%0A%20%20Graph%20Domain%20Adaptation&entry.906535625=Junyu%20Luo%20and%20Yuhao%20Tang%20and%20Yiwei%20Fu%20and%20Xiao%20Luo%20and%20Zhizhuo%20Kou%20and%20Zhiping%20Xiao%20and%20Wei%20Ju%20and%20Wentao%20Zhang%20and%20Ming%20Zhang&entry.1292438233=%20%20Unsupervised%20Graph%20Domain%20Adaptation%20%28UGDA%29%20leverages%20labeled%20source%20domain%0Agraphs%20to%20achieve%20effective%20performance%20in%20unlabeled%20target%20domains%20despite%0Adistribution%20shifts.%20However%2C%20existing%20methods%20often%20yield%20suboptimal%20results%0Adue%20to%20the%20entanglement%20of%20causal-spurious%20features%20and%20the%20failure%20of%20global%0Aalignment%20strategies.%20We%20propose%20SLOGAN%20%28Sparse%20Causal%20Discovery%20with%0AGenerative%20Intervention%29%2C%20a%20novel%20approach%20that%20achieves%20stable%20graph%0Arepresentation%20transfer%20through%20sparse%20causal%20modeling%20and%20dynamic%20intervention%0Amechanisms.%20Specifically%2C%20SLOGAN%20first%20constructs%20a%20sparse%20causal%20graph%0Astructure%2C%20leveraging%20mutual%20information%20bottleneck%20constraints%20to%20disentangle%0Asparse%2C%20stable%20causal%20features%20while%20compressing%20domain-dependent%20spurious%0Acorrelations%20through%20variational%20inference.%20To%20address%20residual%20spurious%0Acorrelations%2C%20we%20innovatively%20design%20a%20generative%20intervention%20mechanism%20that%0Abreaks%20local%20spurious%20couplings%20through%20cross-domain%20feature%20recombination%0Awhile%20maintaining%20causal%20feature%20semantic%20consistency%20via%20covariance%0Aconstraints.%20Furthermore%2C%20to%20mitigate%20error%20accumulation%20in%20target%20domain%0Apseudo-labels%2C%20we%20introduce%20a%20category-adaptive%20dynamic%20calibration%20strategy%2C%0Aensuring%20stable%20discriminative%20learning.%20Extensive%20experiments%20on%20multiple%0Areal-world%20datasets%20demonstrate%20that%20SLOGAN%20significantly%20outperforms%20existing%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07621v1&entry.124074799=Read"},
{"title": "Scaling RL to Long Videos", "author": "Yukang Chen and Wei Huang and Baifeng Shi and Qinghao Hu and Hanrong Ye and Ligeng Zhu and Zhijian Liu and Pavlo Molchanov and Jan Kautz and Xiaojuan Qi and Sifei Liu and Hongxu Yin and Yao Lu and Song Han", "abstract": "  We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).\n", "link": "http://arxiv.org/abs/2507.07966v1", "date": "2025-07-10", "relevancy": 2.6177, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20RL%20to%20Long%20Videos&body=Title%3A%20Scaling%20RL%20to%20Long%20Videos%0AAuthor%3A%20Yukang%20Chen%20and%20Wei%20Huang%20and%20Baifeng%20Shi%20and%20Qinghao%20Hu%20and%20Hanrong%20Ye%20and%20Ligeng%20Zhu%20and%20Zhijian%20Liu%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Xiaojuan%20Qi%20and%20Sifei%20Liu%20and%20Hongxu%20Yin%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20We%20introduce%20a%20full-stack%20framework%20that%20scales%20up%20reasoning%20in%0Avision-language%20models%20%28VLMs%29%20to%20long%20videos%2C%20leveraging%20reinforcement%0Alearning.%20We%20address%20the%20unique%20challenges%20of%20long%20video%20reasoning%20by%0Aintegrating%20three%20critical%20components%3A%20%281%29%20a%20large-scale%20dataset%2C%0ALongVideo-Reason%2C%20comprising%2052K%20long%20video%20QA%20pairs%20with%20high-quality%0Areasoning%20annotations%20across%20diverse%20domains%20such%20as%20sports%2C%20games%2C%20and%20vlogs%3B%0A%282%29%20a%20two-stage%20training%20pipeline%20that%20extends%20VLMs%20with%20chain-of-thought%0Asupervised%20fine-tuning%20%28CoT-SFT%29%20and%20reinforcement%20learning%20%28RL%29%3B%20and%20%283%29%20a%0Atraining%20infrastructure%20for%20long%20video%20RL%2C%20named%20Multi-modal%20Reinforcement%0ASequence%20Parallelism%20%28MR-SP%29%2C%20which%20incorporates%20sequence%20parallelism%20and%20a%0AvLLM-based%20engine%20tailored%20for%20long%20video%2C%20using%20cached%20video%20embeddings%20for%0Aefficient%20rollout%20and%20prefilling.%20In%20experiments%2C%20LongVILA-R1-7B%20achieves%0Astrong%20performance%20on%20long%20video%20QA%20benchmarks%20such%20as%20VideoMME.%20It%20also%0Aoutperforms%20Video-R1-7B%20and%20even%20matches%20Gemini-1.5-Pro%20across%20temporal%0Areasoning%2C%20goal%20and%20purpose%20reasoning%2C%20spatial%20reasoning%2C%20and%20plot%20reasoning%20on%0Aour%20LongVideo-Reason-eval%20benchmark.%20Notably%2C%20our%20MR-SP%20system%20achieves%20up%20to%0A2.1x%20speedup%20on%20long%20video%20RL%20training.%20LongVILA-R1%20demonstrates%20consistent%0Aperformance%20gains%20as%20the%20number%20of%20input%20video%20frames%20scales.%20LongVILA-R1%20marks%0Aa%20firm%20step%20towards%20long%20video%20reasoning%20in%20VLMs.%20In%20addition%2C%20we%20release%20our%0Atraining%20system%20for%20public%20availability%20that%20supports%20RL%20training%20on%20various%0Amodalities%20%28video%2C%20text%2C%20and%20audio%29%2C%20various%20models%20%28VILA%20and%20Qwen%20series%29%2C%20and%0Aeven%20image%20and%20video%20generation%20models.%20On%20a%20single%20A100%20node%20%288%20GPUs%29%2C%20it%0Asupports%20RL%20training%20on%20hour-long%20videos%20%28e.g.%2C%203%2C600%20frames%20/%20around%20256k%0Atokens%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520RL%2520to%2520Long%2520Videos%26entry.906535625%3DYukang%2520Chen%2520and%2520Wei%2520Huang%2520and%2520Baifeng%2520Shi%2520and%2520Qinghao%2520Hu%2520and%2520Hanrong%2520Ye%2520and%2520Ligeng%2520Zhu%2520and%2520Zhijian%2520Liu%2520and%2520Pavlo%2520Molchanov%2520and%2520Jan%2520Kautz%2520and%2520Xiaojuan%2520Qi%2520and%2520Sifei%2520Liu%2520and%2520Hongxu%2520Yin%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520full-stack%2520framework%2520that%2520scales%2520up%2520reasoning%2520in%250Avision-language%2520models%2520%2528VLMs%2529%2520to%2520long%2520videos%252C%2520leveraging%2520reinforcement%250Alearning.%2520We%2520address%2520the%2520unique%2520challenges%2520of%2520long%2520video%2520reasoning%2520by%250Aintegrating%2520three%2520critical%2520components%253A%2520%25281%2529%2520a%2520large-scale%2520dataset%252C%250ALongVideo-Reason%252C%2520comprising%252052K%2520long%2520video%2520QA%2520pairs%2520with%2520high-quality%250Areasoning%2520annotations%2520across%2520diverse%2520domains%2520such%2520as%2520sports%252C%2520games%252C%2520and%2520vlogs%253B%250A%25282%2529%2520a%2520two-stage%2520training%2520pipeline%2520that%2520extends%2520VLMs%2520with%2520chain-of-thought%250Asupervised%2520fine-tuning%2520%2528CoT-SFT%2529%2520and%2520reinforcement%2520learning%2520%2528RL%2529%253B%2520and%2520%25283%2529%2520a%250Atraining%2520infrastructure%2520for%2520long%2520video%2520RL%252C%2520named%2520Multi-modal%2520Reinforcement%250ASequence%2520Parallelism%2520%2528MR-SP%2529%252C%2520which%2520incorporates%2520sequence%2520parallelism%2520and%2520a%250AvLLM-based%2520engine%2520tailored%2520for%2520long%2520video%252C%2520using%2520cached%2520video%2520embeddings%2520for%250Aefficient%2520rollout%2520and%2520prefilling.%2520In%2520experiments%252C%2520LongVILA-R1-7B%2520achieves%250Astrong%2520performance%2520on%2520long%2520video%2520QA%2520benchmarks%2520such%2520as%2520VideoMME.%2520It%2520also%250Aoutperforms%2520Video-R1-7B%2520and%2520even%2520matches%2520Gemini-1.5-Pro%2520across%2520temporal%250Areasoning%252C%2520goal%2520and%2520purpose%2520reasoning%252C%2520spatial%2520reasoning%252C%2520and%2520plot%2520reasoning%2520on%250Aour%2520LongVideo-Reason-eval%2520benchmark.%2520Notably%252C%2520our%2520MR-SP%2520system%2520achieves%2520up%2520to%250A2.1x%2520speedup%2520on%2520long%2520video%2520RL%2520training.%2520LongVILA-R1%2520demonstrates%2520consistent%250Aperformance%2520gains%2520as%2520the%2520number%2520of%2520input%2520video%2520frames%2520scales.%2520LongVILA-R1%2520marks%250Aa%2520firm%2520step%2520towards%2520long%2520video%2520reasoning%2520in%2520VLMs.%2520In%2520addition%252C%2520we%2520release%2520our%250Atraining%2520system%2520for%2520public%2520availability%2520that%2520supports%2520RL%2520training%2520on%2520various%250Amodalities%2520%2528video%252C%2520text%252C%2520and%2520audio%2529%252C%2520various%2520models%2520%2528VILA%2520and%2520Qwen%2520series%2529%252C%2520and%250Aeven%2520image%2520and%2520video%2520generation%2520models.%2520On%2520a%2520single%2520A100%2520node%2520%25288%2520GPUs%2529%252C%2520it%250Asupports%2520RL%2520training%2520on%2520hour-long%2520videos%2520%2528e.g.%252C%25203%252C600%2520frames%2520/%2520around%2520256k%250Atokens%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20RL%20to%20Long%20Videos&entry.906535625=Yukang%20Chen%20and%20Wei%20Huang%20and%20Baifeng%20Shi%20and%20Qinghao%20Hu%20and%20Hanrong%20Ye%20and%20Ligeng%20Zhu%20and%20Zhijian%20Liu%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Xiaojuan%20Qi%20and%20Sifei%20Liu%20and%20Hongxu%20Yin%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20We%20introduce%20a%20full-stack%20framework%20that%20scales%20up%20reasoning%20in%0Avision-language%20models%20%28VLMs%29%20to%20long%20videos%2C%20leveraging%20reinforcement%0Alearning.%20We%20address%20the%20unique%20challenges%20of%20long%20video%20reasoning%20by%0Aintegrating%20three%20critical%20components%3A%20%281%29%20a%20large-scale%20dataset%2C%0ALongVideo-Reason%2C%20comprising%2052K%20long%20video%20QA%20pairs%20with%20high-quality%0Areasoning%20annotations%20across%20diverse%20domains%20such%20as%20sports%2C%20games%2C%20and%20vlogs%3B%0A%282%29%20a%20two-stage%20training%20pipeline%20that%20extends%20VLMs%20with%20chain-of-thought%0Asupervised%20fine-tuning%20%28CoT-SFT%29%20and%20reinforcement%20learning%20%28RL%29%3B%20and%20%283%29%20a%0Atraining%20infrastructure%20for%20long%20video%20RL%2C%20named%20Multi-modal%20Reinforcement%0ASequence%20Parallelism%20%28MR-SP%29%2C%20which%20incorporates%20sequence%20parallelism%20and%20a%0AvLLM-based%20engine%20tailored%20for%20long%20video%2C%20using%20cached%20video%20embeddings%20for%0Aefficient%20rollout%20and%20prefilling.%20In%20experiments%2C%20LongVILA-R1-7B%20achieves%0Astrong%20performance%20on%20long%20video%20QA%20benchmarks%20such%20as%20VideoMME.%20It%20also%0Aoutperforms%20Video-R1-7B%20and%20even%20matches%20Gemini-1.5-Pro%20across%20temporal%0Areasoning%2C%20goal%20and%20purpose%20reasoning%2C%20spatial%20reasoning%2C%20and%20plot%20reasoning%20on%0Aour%20LongVideo-Reason-eval%20benchmark.%20Notably%2C%20our%20MR-SP%20system%20achieves%20up%20to%0A2.1x%20speedup%20on%20long%20video%20RL%20training.%20LongVILA-R1%20demonstrates%20consistent%0Aperformance%20gains%20as%20the%20number%20of%20input%20video%20frames%20scales.%20LongVILA-R1%20marks%0Aa%20firm%20step%20towards%20long%20video%20reasoning%20in%20VLMs.%20In%20addition%2C%20we%20release%20our%0Atraining%20system%20for%20public%20availability%20that%20supports%20RL%20training%20on%20various%0Amodalities%20%28video%2C%20text%2C%20and%20audio%29%2C%20various%20models%20%28VILA%20and%20Qwen%20series%29%2C%20and%0Aeven%20image%20and%20video%20generation%20models.%20On%20a%20single%20A100%20node%20%288%20GPUs%29%2C%20it%0Asupports%20RL%20training%20on%20hour-long%20videos%20%28e.g.%2C%203%2C600%20frames%20/%20around%20256k%0Atokens%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07966v1&entry.124074799=Read"},
{"title": "Curriculum Negative Mining For Temporal Networks", "author": "Ziyue Chen and Tongya Zheng and Mingli Song", "abstract": "  Temporal networks are effective in capturing the evolving interactions of\nnetworks over time, such as social networks and e-commerce networks. In recent\nyears, researchers have primarily concentrated on developing specific model\narchitectures for Temporal Graph Neural Networks (TGNNs) in order to improve\nthe representation quality of temporal nodes and edges. However, limited\nattention has been given to the quality of negative samples during the training\nof TGNNs. When compared with static networks, temporal networks present two\nspecific challenges for negative sampling: positive sparsity and positive\nshift. Positive sparsity refers to the presence of a single positive sample\namidst numerous negative samples at each timestamp, while positive shift\nrelates to the variations in positive samples across different timestamps. To\nrobustly address these challenges in training TGNNs, we introduce Curriculum\nNegative Mining (CurNM), a model-aware curriculum learning framework that\nadaptively adjusts the difficulty of negative samples. Within this framework,\nwe first establish a dynamically updated negative pool that balances random,\nhistorical, and hard negatives to address the challenges posed by positive\nsparsity. Secondly, we implement a temporal-aware negative selection module\nthat focuses on learning from the disentangled factors of recently active\nedges, thus accurately capturing shifting preferences. Finally, the selected\nnegatives are combined with annealing random negatives to support stable\ntraining. Extensive experiments on 12 datasets and 3 TGNNs demonstrate that our\nmethod outperforms baseline methods by a significant margin. Additionally,\nthorough ablation studies and parameter sensitivity experiments verify the\nusefulness and robustness of our approach.\n", "link": "http://arxiv.org/abs/2407.17070v2", "date": "2025-07-10", "relevancy": 2.5702, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5304}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.508}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curriculum%20Negative%20Mining%20For%20Temporal%20Networks&body=Title%3A%20Curriculum%20Negative%20Mining%20For%20Temporal%20Networks%0AAuthor%3A%20Ziyue%20Chen%20and%20Tongya%20Zheng%20and%20Mingli%20Song%0AAbstract%3A%20%20%20Temporal%20networks%20are%20effective%20in%20capturing%20the%20evolving%20interactions%20of%0Anetworks%20over%20time%2C%20such%20as%20social%20networks%20and%20e-commerce%20networks.%20In%20recent%0Ayears%2C%20researchers%20have%20primarily%20concentrated%20on%20developing%20specific%20model%0Aarchitectures%20for%20Temporal%20Graph%20Neural%20Networks%20%28TGNNs%29%20in%20order%20to%20improve%0Athe%20representation%20quality%20of%20temporal%20nodes%20and%20edges.%20However%2C%20limited%0Aattention%20has%20been%20given%20to%20the%20quality%20of%20negative%20samples%20during%20the%20training%0Aof%20TGNNs.%20When%20compared%20with%20static%20networks%2C%20temporal%20networks%20present%20two%0Aspecific%20challenges%20for%20negative%20sampling%3A%20positive%20sparsity%20and%20positive%0Ashift.%20Positive%20sparsity%20refers%20to%20the%20presence%20of%20a%20single%20positive%20sample%0Aamidst%20numerous%20negative%20samples%20at%20each%20timestamp%2C%20while%20positive%20shift%0Arelates%20to%20the%20variations%20in%20positive%20samples%20across%20different%20timestamps.%20To%0Arobustly%20address%20these%20challenges%20in%20training%20TGNNs%2C%20we%20introduce%20Curriculum%0ANegative%20Mining%20%28CurNM%29%2C%20a%20model-aware%20curriculum%20learning%20framework%20that%0Aadaptively%20adjusts%20the%20difficulty%20of%20negative%20samples.%20Within%20this%20framework%2C%0Awe%20first%20establish%20a%20dynamically%20updated%20negative%20pool%20that%20balances%20random%2C%0Ahistorical%2C%20and%20hard%20negatives%20to%20address%20the%20challenges%20posed%20by%20positive%0Asparsity.%20Secondly%2C%20we%20implement%20a%20temporal-aware%20negative%20selection%20module%0Athat%20focuses%20on%20learning%20from%20the%20disentangled%20factors%20of%20recently%20active%0Aedges%2C%20thus%20accurately%20capturing%20shifting%20preferences.%20Finally%2C%20the%20selected%0Anegatives%20are%20combined%20with%20annealing%20random%20negatives%20to%20support%20stable%0Atraining.%20Extensive%20experiments%20on%2012%20datasets%20and%203%20TGNNs%20demonstrate%20that%20our%0Amethod%20outperforms%20baseline%20methods%20by%20a%20significant%20margin.%20Additionally%2C%0Athorough%20ablation%20studies%20and%20parameter%20sensitivity%20experiments%20verify%20the%0Ausefulness%20and%20robustness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17070v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurriculum%2520Negative%2520Mining%2520For%2520Temporal%2520Networks%26entry.906535625%3DZiyue%2520Chen%2520and%2520Tongya%2520Zheng%2520and%2520Mingli%2520Song%26entry.1292438233%3D%2520%2520Temporal%2520networks%2520are%2520effective%2520in%2520capturing%2520the%2520evolving%2520interactions%2520of%250Anetworks%2520over%2520time%252C%2520such%2520as%2520social%2520networks%2520and%2520e-commerce%2520networks.%2520In%2520recent%250Ayears%252C%2520researchers%2520have%2520primarily%2520concentrated%2520on%2520developing%2520specific%2520model%250Aarchitectures%2520for%2520Temporal%2520Graph%2520Neural%2520Networks%2520%2528TGNNs%2529%2520in%2520order%2520to%2520improve%250Athe%2520representation%2520quality%2520of%2520temporal%2520nodes%2520and%2520edges.%2520However%252C%2520limited%250Aattention%2520has%2520been%2520given%2520to%2520the%2520quality%2520of%2520negative%2520samples%2520during%2520the%2520training%250Aof%2520TGNNs.%2520When%2520compared%2520with%2520static%2520networks%252C%2520temporal%2520networks%2520present%2520two%250Aspecific%2520challenges%2520for%2520negative%2520sampling%253A%2520positive%2520sparsity%2520and%2520positive%250Ashift.%2520Positive%2520sparsity%2520refers%2520to%2520the%2520presence%2520of%2520a%2520single%2520positive%2520sample%250Aamidst%2520numerous%2520negative%2520samples%2520at%2520each%2520timestamp%252C%2520while%2520positive%2520shift%250Arelates%2520to%2520the%2520variations%2520in%2520positive%2520samples%2520across%2520different%2520timestamps.%2520To%250Arobustly%2520address%2520these%2520challenges%2520in%2520training%2520TGNNs%252C%2520we%2520introduce%2520Curriculum%250ANegative%2520Mining%2520%2528CurNM%2529%252C%2520a%2520model-aware%2520curriculum%2520learning%2520framework%2520that%250Aadaptively%2520adjusts%2520the%2520difficulty%2520of%2520negative%2520samples.%2520Within%2520this%2520framework%252C%250Awe%2520first%2520establish%2520a%2520dynamically%2520updated%2520negative%2520pool%2520that%2520balances%2520random%252C%250Ahistorical%252C%2520and%2520hard%2520negatives%2520to%2520address%2520the%2520challenges%2520posed%2520by%2520positive%250Asparsity.%2520Secondly%252C%2520we%2520implement%2520a%2520temporal-aware%2520negative%2520selection%2520module%250Athat%2520focuses%2520on%2520learning%2520from%2520the%2520disentangled%2520factors%2520of%2520recently%2520active%250Aedges%252C%2520thus%2520accurately%2520capturing%2520shifting%2520preferences.%2520Finally%252C%2520the%2520selected%250Anegatives%2520are%2520combined%2520with%2520annealing%2520random%2520negatives%2520to%2520support%2520stable%250Atraining.%2520Extensive%2520experiments%2520on%252012%2520datasets%2520and%25203%2520TGNNs%2520demonstrate%2520that%2520our%250Amethod%2520outperforms%2520baseline%2520methods%2520by%2520a%2520significant%2520margin.%2520Additionally%252C%250Athorough%2520ablation%2520studies%2520and%2520parameter%2520sensitivity%2520experiments%2520verify%2520the%250Ausefulness%2520and%2520robustness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17070v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curriculum%20Negative%20Mining%20For%20Temporal%20Networks&entry.906535625=Ziyue%20Chen%20and%20Tongya%20Zheng%20and%20Mingli%20Song&entry.1292438233=%20%20Temporal%20networks%20are%20effective%20in%20capturing%20the%20evolving%20interactions%20of%0Anetworks%20over%20time%2C%20such%20as%20social%20networks%20and%20e-commerce%20networks.%20In%20recent%0Ayears%2C%20researchers%20have%20primarily%20concentrated%20on%20developing%20specific%20model%0Aarchitectures%20for%20Temporal%20Graph%20Neural%20Networks%20%28TGNNs%29%20in%20order%20to%20improve%0Athe%20representation%20quality%20of%20temporal%20nodes%20and%20edges.%20However%2C%20limited%0Aattention%20has%20been%20given%20to%20the%20quality%20of%20negative%20samples%20during%20the%20training%0Aof%20TGNNs.%20When%20compared%20with%20static%20networks%2C%20temporal%20networks%20present%20two%0Aspecific%20challenges%20for%20negative%20sampling%3A%20positive%20sparsity%20and%20positive%0Ashift.%20Positive%20sparsity%20refers%20to%20the%20presence%20of%20a%20single%20positive%20sample%0Aamidst%20numerous%20negative%20samples%20at%20each%20timestamp%2C%20while%20positive%20shift%0Arelates%20to%20the%20variations%20in%20positive%20samples%20across%20different%20timestamps.%20To%0Arobustly%20address%20these%20challenges%20in%20training%20TGNNs%2C%20we%20introduce%20Curriculum%0ANegative%20Mining%20%28CurNM%29%2C%20a%20model-aware%20curriculum%20learning%20framework%20that%0Aadaptively%20adjusts%20the%20difficulty%20of%20negative%20samples.%20Within%20this%20framework%2C%0Awe%20first%20establish%20a%20dynamically%20updated%20negative%20pool%20that%20balances%20random%2C%0Ahistorical%2C%20and%20hard%20negatives%20to%20address%20the%20challenges%20posed%20by%20positive%0Asparsity.%20Secondly%2C%20we%20implement%20a%20temporal-aware%20negative%20selection%20module%0Athat%20focuses%20on%20learning%20from%20the%20disentangled%20factors%20of%20recently%20active%0Aedges%2C%20thus%20accurately%20capturing%20shifting%20preferences.%20Finally%2C%20the%20selected%0Anegatives%20are%20combined%20with%20annealing%20random%20negatives%20to%20support%20stable%0Atraining.%20Extensive%20experiments%20on%2012%20datasets%20and%203%20TGNNs%20demonstrate%20that%20our%0Amethod%20outperforms%20baseline%20methods%20by%20a%20significant%20margin.%20Additionally%2C%0Athorough%20ablation%20studies%20and%20parameter%20sensitivity%20experiments%20verify%20the%0Ausefulness%20and%20robustness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17070v2&entry.124074799=Read"},
{"title": "An Algorithm for Learning Smaller Representations of Models With Scarce\n  Data", "author": "Adrian de Wynter", "abstract": "  We present an algorithm for solving binary classification problems when the\ndataset is not fully representative of the problem being solved, and obtaining\nmore data is not possible. It relies on a trained model with loose accuracy\nconstraints, an iterative hyperparameter searching-and-pruning procedure over a\nsearch space $\\Theta$, and a data-generating function. Our algorithm works by\nreconstructing up to homology the manifold on which lies the support of the\nunderlying distribution. We provide an analysis on correctness and runtime\ncomplexity under ideal conditions and an extension to deep neural networks. In\nthe former case, if $\\size{\\Theta}$ is the number of hyperparameter sets in the\nsearch space, this algorithm returns a solution that is up to $2(1 -\n{2^{-\\size{\\Theta}}})$ times better than simply training with an enumeration of\n$\\Theta$ and picking the best model. As part of our analysis we also prove that\nan open cover of a dataset has the same homology as the manifold on which lies\nthe support of the underlying probability distribution, if and only said\ndataset is learnable. This latter result acts as a formal argument to explain\nthe effectiveness of data expansion techniques.\n", "link": "http://arxiv.org/abs/2010.07990v2", "date": "2025-07-10", "relevancy": 2.5683, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5194}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5142}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Algorithm%20for%20Learning%20Smaller%20Representations%20of%20Models%20With%20Scarce%0A%20%20Data&body=Title%3A%20An%20Algorithm%20for%20Learning%20Smaller%20Representations%20of%20Models%20With%20Scarce%0A%20%20Data%0AAuthor%3A%20Adrian%20de%20Wynter%0AAbstract%3A%20%20%20We%20present%20an%20algorithm%20for%20solving%20binary%20classification%20problems%20when%20the%0Adataset%20is%20not%20fully%20representative%20of%20the%20problem%20being%20solved%2C%20and%20obtaining%0Amore%20data%20is%20not%20possible.%20It%20relies%20on%20a%20trained%20model%20with%20loose%20accuracy%0Aconstraints%2C%20an%20iterative%20hyperparameter%20searching-and-pruning%20procedure%20over%20a%0Asearch%20space%20%24%5CTheta%24%2C%20and%20a%20data-generating%20function.%20Our%20algorithm%20works%20by%0Areconstructing%20up%20to%20homology%20the%20manifold%20on%20which%20lies%20the%20support%20of%20the%0Aunderlying%20distribution.%20We%20provide%20an%20analysis%20on%20correctness%20and%20runtime%0Acomplexity%20under%20ideal%20conditions%20and%20an%20extension%20to%20deep%20neural%20networks.%20In%0Athe%20former%20case%2C%20if%20%24%5Csize%7B%5CTheta%7D%24%20is%20the%20number%20of%20hyperparameter%20sets%20in%20the%0Asearch%20space%2C%20this%20algorithm%20returns%20a%20solution%20that%20is%20up%20to%20%242%281%20-%0A%7B2%5E%7B-%5Csize%7B%5CTheta%7D%7D%7D%29%24%20times%20better%20than%20simply%20training%20with%20an%20enumeration%20of%0A%24%5CTheta%24%20and%20picking%20the%20best%20model.%20As%20part%20of%20our%20analysis%20we%20also%20prove%20that%0Aan%20open%20cover%20of%20a%20dataset%20has%20the%20same%20homology%20as%20the%20manifold%20on%20which%20lies%0Athe%20support%20of%20the%20underlying%20probability%20distribution%2C%20if%20and%20only%20said%0Adataset%20is%20learnable.%20This%20latter%20result%20acts%20as%20a%20formal%20argument%20to%20explain%0Athe%20effectiveness%20of%20data%20expansion%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2010.07990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Algorithm%2520for%2520Learning%2520Smaller%2520Representations%2520of%2520Models%2520With%2520Scarce%250A%2520%2520Data%26entry.906535625%3DAdrian%2520de%2520Wynter%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520algorithm%2520for%2520solving%2520binary%2520classification%2520problems%2520when%2520the%250Adataset%2520is%2520not%2520fully%2520representative%2520of%2520the%2520problem%2520being%2520solved%252C%2520and%2520obtaining%250Amore%2520data%2520is%2520not%2520possible.%2520It%2520relies%2520on%2520a%2520trained%2520model%2520with%2520loose%2520accuracy%250Aconstraints%252C%2520an%2520iterative%2520hyperparameter%2520searching-and-pruning%2520procedure%2520over%2520a%250Asearch%2520space%2520%2524%255CTheta%2524%252C%2520and%2520a%2520data-generating%2520function.%2520Our%2520algorithm%2520works%2520by%250Areconstructing%2520up%2520to%2520homology%2520the%2520manifold%2520on%2520which%2520lies%2520the%2520support%2520of%2520the%250Aunderlying%2520distribution.%2520We%2520provide%2520an%2520analysis%2520on%2520correctness%2520and%2520runtime%250Acomplexity%2520under%2520ideal%2520conditions%2520and%2520an%2520extension%2520to%2520deep%2520neural%2520networks.%2520In%250Athe%2520former%2520case%252C%2520if%2520%2524%255Csize%257B%255CTheta%257D%2524%2520is%2520the%2520number%2520of%2520hyperparameter%2520sets%2520in%2520the%250Asearch%2520space%252C%2520this%2520algorithm%2520returns%2520a%2520solution%2520that%2520is%2520up%2520to%2520%25242%25281%2520-%250A%257B2%255E%257B-%255Csize%257B%255CTheta%257D%257D%257D%2529%2524%2520times%2520better%2520than%2520simply%2520training%2520with%2520an%2520enumeration%2520of%250A%2524%255CTheta%2524%2520and%2520picking%2520the%2520best%2520model.%2520As%2520part%2520of%2520our%2520analysis%2520we%2520also%2520prove%2520that%250Aan%2520open%2520cover%2520of%2520a%2520dataset%2520has%2520the%2520same%2520homology%2520as%2520the%2520manifold%2520on%2520which%2520lies%250Athe%2520support%2520of%2520the%2520underlying%2520probability%2520distribution%252C%2520if%2520and%2520only%2520said%250Adataset%2520is%2520learnable.%2520This%2520latter%2520result%2520acts%2520as%2520a%2520formal%2520argument%2520to%2520explain%250Athe%2520effectiveness%2520of%2520data%2520expansion%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2010.07990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Algorithm%20for%20Learning%20Smaller%20Representations%20of%20Models%20With%20Scarce%0A%20%20Data&entry.906535625=Adrian%20de%20Wynter&entry.1292438233=%20%20We%20present%20an%20algorithm%20for%20solving%20binary%20classification%20problems%20when%20the%0Adataset%20is%20not%20fully%20representative%20of%20the%20problem%20being%20solved%2C%20and%20obtaining%0Amore%20data%20is%20not%20possible.%20It%20relies%20on%20a%20trained%20model%20with%20loose%20accuracy%0Aconstraints%2C%20an%20iterative%20hyperparameter%20searching-and-pruning%20procedure%20over%20a%0Asearch%20space%20%24%5CTheta%24%2C%20and%20a%20data-generating%20function.%20Our%20algorithm%20works%20by%0Areconstructing%20up%20to%20homology%20the%20manifold%20on%20which%20lies%20the%20support%20of%20the%0Aunderlying%20distribution.%20We%20provide%20an%20analysis%20on%20correctness%20and%20runtime%0Acomplexity%20under%20ideal%20conditions%20and%20an%20extension%20to%20deep%20neural%20networks.%20In%0Athe%20former%20case%2C%20if%20%24%5Csize%7B%5CTheta%7D%24%20is%20the%20number%20of%20hyperparameter%20sets%20in%20the%0Asearch%20space%2C%20this%20algorithm%20returns%20a%20solution%20that%20is%20up%20to%20%242%281%20-%0A%7B2%5E%7B-%5Csize%7B%5CTheta%7D%7D%7D%29%24%20times%20better%20than%20simply%20training%20with%20an%20enumeration%20of%0A%24%5CTheta%24%20and%20picking%20the%20best%20model.%20As%20part%20of%20our%20analysis%20we%20also%20prove%20that%0Aan%20open%20cover%20of%20a%20dataset%20has%20the%20same%20homology%20as%20the%20manifold%20on%20which%20lies%0Athe%20support%20of%20the%20underlying%20probability%20distribution%2C%20if%20and%20only%20said%0Adataset%20is%20learnable.%20This%20latter%20result%20acts%20as%20a%20formal%20argument%20to%20explain%0Athe%20effectiveness%20of%20data%20expansion%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2010.07990v2&entry.124074799=Read"},
{"title": "D-CNN and VQ-VAE Autoencoders for Compression and Denoising of\n  Industrial X-ray Computed Tomography Images", "author": "Bardia Hejazi and Keerthana Chand and Tobias Fritsch and Giovanni Bruno", "abstract": "  The ever-growing volume of data in imaging sciences stemming from the\nadvancements in imaging technologies, necessitates efficient and reliable\nstorage solutions for such large datasets. This study investigates the\ncompression of industrial X-ray computed tomography (XCT) data using deep\nlearning autoencoders and examines how these compression algorithms affect the\nquality of the recovered data. Two network architectures with different\ncompression rates were used, a deep convolution neural network (D-CNN) and a\nvector quantized variational autoencoder (VQ-VAE). The XCT data used was from a\nsandstone sample with a complex internal pore network. The quality of the\ndecoded images obtained from the two different deep learning architectures with\ndifferent compression rates were quantified and compared to the original input\ndata. In addition, to improve image decoding quality metrics, we introduced a\nmetric sensitive to edge preservation, which is crucial for three-dimensional\ndata analysis. We showed that different architectures and compression rates are\nrequired depending on the specific characteristics needed to be preserved for\nlater analysis. The findings presented here can aid scientists to determine the\nrequirements and strategies for their data storage and analysis needs.\n", "link": "http://arxiv.org/abs/2507.07704v1", "date": "2025-07-10", "relevancy": 2.5676, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5211}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-CNN%20and%20VQ-VAE%20Autoencoders%20for%20Compression%20and%20Denoising%20of%0A%20%20Industrial%20X-ray%20Computed%20Tomography%20Images&body=Title%3A%20D-CNN%20and%20VQ-VAE%20Autoencoders%20for%20Compression%20and%20Denoising%20of%0A%20%20Industrial%20X-ray%20Computed%20Tomography%20Images%0AAuthor%3A%20Bardia%20Hejazi%20and%20Keerthana%20Chand%20and%20Tobias%20Fritsch%20and%20Giovanni%20Bruno%0AAbstract%3A%20%20%20The%20ever-growing%20volume%20of%20data%20in%20imaging%20sciences%20stemming%20from%20the%0Aadvancements%20in%20imaging%20technologies%2C%20necessitates%20efficient%20and%20reliable%0Astorage%20solutions%20for%20such%20large%20datasets.%20This%20study%20investigates%20the%0Acompression%20of%20industrial%20X-ray%20computed%20tomography%20%28XCT%29%20data%20using%20deep%0Alearning%20autoencoders%20and%20examines%20how%20these%20compression%20algorithms%20affect%20the%0Aquality%20of%20the%20recovered%20data.%20Two%20network%20architectures%20with%20different%0Acompression%20rates%20were%20used%2C%20a%20deep%20convolution%20neural%20network%20%28D-CNN%29%20and%20a%0Avector%20quantized%20variational%20autoencoder%20%28VQ-VAE%29.%20The%20XCT%20data%20used%20was%20from%20a%0Asandstone%20sample%20with%20a%20complex%20internal%20pore%20network.%20The%20quality%20of%20the%0Adecoded%20images%20obtained%20from%20the%20two%20different%20deep%20learning%20architectures%20with%0Adifferent%20compression%20rates%20were%20quantified%20and%20compared%20to%20the%20original%20input%0Adata.%20In%20addition%2C%20to%20improve%20image%20decoding%20quality%20metrics%2C%20we%20introduced%20a%0Ametric%20sensitive%20to%20edge%20preservation%2C%20which%20is%20crucial%20for%20three-dimensional%0Adata%20analysis.%20We%20showed%20that%20different%20architectures%20and%20compression%20rates%20are%0Arequired%20depending%20on%20the%20specific%20characteristics%20needed%20to%20be%20preserved%20for%0Alater%20analysis.%20The%20findings%20presented%20here%20can%20aid%20scientists%20to%20determine%20the%0Arequirements%20and%20strategies%20for%20their%20data%20storage%20and%20analysis%20needs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-CNN%2520and%2520VQ-VAE%2520Autoencoders%2520for%2520Compression%2520and%2520Denoising%2520of%250A%2520%2520Industrial%2520X-ray%2520Computed%2520Tomography%2520Images%26entry.906535625%3DBardia%2520Hejazi%2520and%2520Keerthana%2520Chand%2520and%2520Tobias%2520Fritsch%2520and%2520Giovanni%2520Bruno%26entry.1292438233%3D%2520%2520The%2520ever-growing%2520volume%2520of%2520data%2520in%2520imaging%2520sciences%2520stemming%2520from%2520the%250Aadvancements%2520in%2520imaging%2520technologies%252C%2520necessitates%2520efficient%2520and%2520reliable%250Astorage%2520solutions%2520for%2520such%2520large%2520datasets.%2520This%2520study%2520investigates%2520the%250Acompression%2520of%2520industrial%2520X-ray%2520computed%2520tomography%2520%2528XCT%2529%2520data%2520using%2520deep%250Alearning%2520autoencoders%2520and%2520examines%2520how%2520these%2520compression%2520algorithms%2520affect%2520the%250Aquality%2520of%2520the%2520recovered%2520data.%2520Two%2520network%2520architectures%2520with%2520different%250Acompression%2520rates%2520were%2520used%252C%2520a%2520deep%2520convolution%2520neural%2520network%2520%2528D-CNN%2529%2520and%2520a%250Avector%2520quantized%2520variational%2520autoencoder%2520%2528VQ-VAE%2529.%2520The%2520XCT%2520data%2520used%2520was%2520from%2520a%250Asandstone%2520sample%2520with%2520a%2520complex%2520internal%2520pore%2520network.%2520The%2520quality%2520of%2520the%250Adecoded%2520images%2520obtained%2520from%2520the%2520two%2520different%2520deep%2520learning%2520architectures%2520with%250Adifferent%2520compression%2520rates%2520were%2520quantified%2520and%2520compared%2520to%2520the%2520original%2520input%250Adata.%2520In%2520addition%252C%2520to%2520improve%2520image%2520decoding%2520quality%2520metrics%252C%2520we%2520introduced%2520a%250Ametric%2520sensitive%2520to%2520edge%2520preservation%252C%2520which%2520is%2520crucial%2520for%2520three-dimensional%250Adata%2520analysis.%2520We%2520showed%2520that%2520different%2520architectures%2520and%2520compression%2520rates%2520are%250Arequired%2520depending%2520on%2520the%2520specific%2520characteristics%2520needed%2520to%2520be%2520preserved%2520for%250Alater%2520analysis.%2520The%2520findings%2520presented%2520here%2520can%2520aid%2520scientists%2520to%2520determine%2520the%250Arequirements%2520and%2520strategies%2520for%2520their%2520data%2520storage%2520and%2520analysis%2520needs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-CNN%20and%20VQ-VAE%20Autoencoders%20for%20Compression%20and%20Denoising%20of%0A%20%20Industrial%20X-ray%20Computed%20Tomography%20Images&entry.906535625=Bardia%20Hejazi%20and%20Keerthana%20Chand%20and%20Tobias%20Fritsch%20and%20Giovanni%20Bruno&entry.1292438233=%20%20The%20ever-growing%20volume%20of%20data%20in%20imaging%20sciences%20stemming%20from%20the%0Aadvancements%20in%20imaging%20technologies%2C%20necessitates%20efficient%20and%20reliable%0Astorage%20solutions%20for%20such%20large%20datasets.%20This%20study%20investigates%20the%0Acompression%20of%20industrial%20X-ray%20computed%20tomography%20%28XCT%29%20data%20using%20deep%0Alearning%20autoencoders%20and%20examines%20how%20these%20compression%20algorithms%20affect%20the%0Aquality%20of%20the%20recovered%20data.%20Two%20network%20architectures%20with%20different%0Acompression%20rates%20were%20used%2C%20a%20deep%20convolution%20neural%20network%20%28D-CNN%29%20and%20a%0Avector%20quantized%20variational%20autoencoder%20%28VQ-VAE%29.%20The%20XCT%20data%20used%20was%20from%20a%0Asandstone%20sample%20with%20a%20complex%20internal%20pore%20network.%20The%20quality%20of%20the%0Adecoded%20images%20obtained%20from%20the%20two%20different%20deep%20learning%20architectures%20with%0Adifferent%20compression%20rates%20were%20quantified%20and%20compared%20to%20the%20original%20input%0Adata.%20In%20addition%2C%20to%20improve%20image%20decoding%20quality%20metrics%2C%20we%20introduced%20a%0Ametric%20sensitive%20to%20edge%20preservation%2C%20which%20is%20crucial%20for%20three-dimensional%0Adata%20analysis.%20We%20showed%20that%20different%20architectures%20and%20compression%20rates%20are%0Arequired%20depending%20on%20the%20specific%20characteristics%20needed%20to%20be%20preserved%20for%0Alater%20analysis.%20The%20findings%20presented%20here%20can%20aid%20scientists%20to%20determine%20the%0Arequirements%20and%20strategies%20for%20their%20data%20storage%20and%20analysis%20needs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07704v1&entry.124074799=Read"},
{"title": "BarcodeBERT: Transformers for Biodiversity Analysis", "author": "Pablo Millan Arias and Niousha Sadjadi and Monireh Safari and ZeMing Gong and Austin T. Wang and Joakim Bruslund Haurum and Iuliia Zarubiieva and Dirk Steinke and Lila Kari and Angel X. Chang and Scott C. Lowe and Graham W. Taylor", "abstract": "  In the global challenge of understanding and characterizing biodiversity,\nshort species-specific genomic sequences known as DNA barcodes play a critical\nrole, enabling fine-grained comparisons among organisms within the same kingdom\nof life. Although machine learning algorithms specifically designed for the\nanalysis of DNA barcodes are becoming more popular, most existing methodologies\nrely on generic supervised training algorithms. We introduce BarcodeBERT, a\nfamily of models tailored to biodiversity analysis and trained exclusively on\ndata from a reference library of 1.5M invertebrate DNA barcodes. We compared\nthe performance of BarcodeBERT on taxonomic identification tasks against a\nspectrum of machine learning approaches including supervised training of\nclassical neural architectures and fine-tuning of general DNA foundation\nmodels. Our self-supervised pretraining strategies on domain-specific data\noutperform fine-tuned foundation models, especially in identification tasks\ninvolving lower taxa such as genera and species. We also compared BarcodeBERT\nwith BLAST, one of the most widely used bioinformatics tools for sequence\nsearching, and found that our method matched BLAST's performance in\nspecies-level classification while being 55 times faster. Our analysis of\nmasking and tokenization strategies also provides practical guidance for\nbuilding customized DNA language models, emphasizing the importance of aligning\nmodel training strategies with dataset characteristics and domain knowledge.\nThe code repository is available at https://github.com/bioscan-ml/BarcodeBERT.\n", "link": "http://arxiv.org/abs/2311.02401v3", "date": "2025-07-10", "relevancy": 2.5579, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5218}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BarcodeBERT%3A%20Transformers%20for%20Biodiversity%20Analysis&body=Title%3A%20BarcodeBERT%3A%20Transformers%20for%20Biodiversity%20Analysis%0AAuthor%3A%20Pablo%20Millan%20Arias%20and%20Niousha%20Sadjadi%20and%20Monireh%20Safari%20and%20ZeMing%20Gong%20and%20Austin%20T.%20Wang%20and%20Joakim%20Bruslund%20Haurum%20and%20Iuliia%20Zarubiieva%20and%20Dirk%20Steinke%20and%20Lila%20Kari%20and%20Angel%20X.%20Chang%20and%20Scott%20C.%20Lowe%20and%20Graham%20W.%20Taylor%0AAbstract%3A%20%20%20In%20the%20global%20challenge%20of%20understanding%20and%20characterizing%20biodiversity%2C%0Ashort%20species-specific%20genomic%20sequences%20known%20as%20DNA%20barcodes%20play%20a%20critical%0Arole%2C%20enabling%20fine-grained%20comparisons%20among%20organisms%20within%20the%20same%20kingdom%0Aof%20life.%20Although%20machine%20learning%20algorithms%20specifically%20designed%20for%20the%0Aanalysis%20of%20DNA%20barcodes%20are%20becoming%20more%20popular%2C%20most%20existing%20methodologies%0Arely%20on%20generic%20supervised%20training%20algorithms.%20We%20introduce%20BarcodeBERT%2C%20a%0Afamily%20of%20models%20tailored%20to%20biodiversity%20analysis%20and%20trained%20exclusively%20on%0Adata%20from%20a%20reference%20library%20of%201.5M%20invertebrate%20DNA%20barcodes.%20We%20compared%0Athe%20performance%20of%20BarcodeBERT%20on%20taxonomic%20identification%20tasks%20against%20a%0Aspectrum%20of%20machine%20learning%20approaches%20including%20supervised%20training%20of%0Aclassical%20neural%20architectures%20and%20fine-tuning%20of%20general%20DNA%20foundation%0Amodels.%20Our%20self-supervised%20pretraining%20strategies%20on%20domain-specific%20data%0Aoutperform%20fine-tuned%20foundation%20models%2C%20especially%20in%20identification%20tasks%0Ainvolving%20lower%20taxa%20such%20as%20genera%20and%20species.%20We%20also%20compared%20BarcodeBERT%0Awith%20BLAST%2C%20one%20of%20the%20most%20widely%20used%20bioinformatics%20tools%20for%20sequence%0Asearching%2C%20and%20found%20that%20our%20method%20matched%20BLAST%27s%20performance%20in%0Aspecies-level%20classification%20while%20being%2055%20times%20faster.%20Our%20analysis%20of%0Amasking%20and%20tokenization%20strategies%20also%20provides%20practical%20guidance%20for%0Abuilding%20customized%20DNA%20language%20models%2C%20emphasizing%20the%20importance%20of%20aligning%0Amodel%20training%20strategies%20with%20dataset%20characteristics%20and%20domain%20knowledge.%0AThe%20code%20repository%20is%20available%20at%20https%3A//github.com/bioscan-ml/BarcodeBERT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02401v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBarcodeBERT%253A%2520Transformers%2520for%2520Biodiversity%2520Analysis%26entry.906535625%3DPablo%2520Millan%2520Arias%2520and%2520Niousha%2520Sadjadi%2520and%2520Monireh%2520Safari%2520and%2520ZeMing%2520Gong%2520and%2520Austin%2520T.%2520Wang%2520and%2520Joakim%2520Bruslund%2520Haurum%2520and%2520Iuliia%2520Zarubiieva%2520and%2520Dirk%2520Steinke%2520and%2520Lila%2520Kari%2520and%2520Angel%2520X.%2520Chang%2520and%2520Scott%2520C.%2520Lowe%2520and%2520Graham%2520W.%2520Taylor%26entry.1292438233%3D%2520%2520In%2520the%2520global%2520challenge%2520of%2520understanding%2520and%2520characterizing%2520biodiversity%252C%250Ashort%2520species-specific%2520genomic%2520sequences%2520known%2520as%2520DNA%2520barcodes%2520play%2520a%2520critical%250Arole%252C%2520enabling%2520fine-grained%2520comparisons%2520among%2520organisms%2520within%2520the%2520same%2520kingdom%250Aof%2520life.%2520Although%2520machine%2520learning%2520algorithms%2520specifically%2520designed%2520for%2520the%250Aanalysis%2520of%2520DNA%2520barcodes%2520are%2520becoming%2520more%2520popular%252C%2520most%2520existing%2520methodologies%250Arely%2520on%2520generic%2520supervised%2520training%2520algorithms.%2520We%2520introduce%2520BarcodeBERT%252C%2520a%250Afamily%2520of%2520models%2520tailored%2520to%2520biodiversity%2520analysis%2520and%2520trained%2520exclusively%2520on%250Adata%2520from%2520a%2520reference%2520library%2520of%25201.5M%2520invertebrate%2520DNA%2520barcodes.%2520We%2520compared%250Athe%2520performance%2520of%2520BarcodeBERT%2520on%2520taxonomic%2520identification%2520tasks%2520against%2520a%250Aspectrum%2520of%2520machine%2520learning%2520approaches%2520including%2520supervised%2520training%2520of%250Aclassical%2520neural%2520architectures%2520and%2520fine-tuning%2520of%2520general%2520DNA%2520foundation%250Amodels.%2520Our%2520self-supervised%2520pretraining%2520strategies%2520on%2520domain-specific%2520data%250Aoutperform%2520fine-tuned%2520foundation%2520models%252C%2520especially%2520in%2520identification%2520tasks%250Ainvolving%2520lower%2520taxa%2520such%2520as%2520genera%2520and%2520species.%2520We%2520also%2520compared%2520BarcodeBERT%250Awith%2520BLAST%252C%2520one%2520of%2520the%2520most%2520widely%2520used%2520bioinformatics%2520tools%2520for%2520sequence%250Asearching%252C%2520and%2520found%2520that%2520our%2520method%2520matched%2520BLAST%2527s%2520performance%2520in%250Aspecies-level%2520classification%2520while%2520being%252055%2520times%2520faster.%2520Our%2520analysis%2520of%250Amasking%2520and%2520tokenization%2520strategies%2520also%2520provides%2520practical%2520guidance%2520for%250Abuilding%2520customized%2520DNA%2520language%2520models%252C%2520emphasizing%2520the%2520importance%2520of%2520aligning%250Amodel%2520training%2520strategies%2520with%2520dataset%2520characteristics%2520and%2520domain%2520knowledge.%250AThe%2520code%2520repository%2520is%2520available%2520at%2520https%253A//github.com/bioscan-ml/BarcodeBERT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.02401v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BarcodeBERT%3A%20Transformers%20for%20Biodiversity%20Analysis&entry.906535625=Pablo%20Millan%20Arias%20and%20Niousha%20Sadjadi%20and%20Monireh%20Safari%20and%20ZeMing%20Gong%20and%20Austin%20T.%20Wang%20and%20Joakim%20Bruslund%20Haurum%20and%20Iuliia%20Zarubiieva%20and%20Dirk%20Steinke%20and%20Lila%20Kari%20and%20Angel%20X.%20Chang%20and%20Scott%20C.%20Lowe%20and%20Graham%20W.%20Taylor&entry.1292438233=%20%20In%20the%20global%20challenge%20of%20understanding%20and%20characterizing%20biodiversity%2C%0Ashort%20species-specific%20genomic%20sequences%20known%20as%20DNA%20barcodes%20play%20a%20critical%0Arole%2C%20enabling%20fine-grained%20comparisons%20among%20organisms%20within%20the%20same%20kingdom%0Aof%20life.%20Although%20machine%20learning%20algorithms%20specifically%20designed%20for%20the%0Aanalysis%20of%20DNA%20barcodes%20are%20becoming%20more%20popular%2C%20most%20existing%20methodologies%0Arely%20on%20generic%20supervised%20training%20algorithms.%20We%20introduce%20BarcodeBERT%2C%20a%0Afamily%20of%20models%20tailored%20to%20biodiversity%20analysis%20and%20trained%20exclusively%20on%0Adata%20from%20a%20reference%20library%20of%201.5M%20invertebrate%20DNA%20barcodes.%20We%20compared%0Athe%20performance%20of%20BarcodeBERT%20on%20taxonomic%20identification%20tasks%20against%20a%0Aspectrum%20of%20machine%20learning%20approaches%20including%20supervised%20training%20of%0Aclassical%20neural%20architectures%20and%20fine-tuning%20of%20general%20DNA%20foundation%0Amodels.%20Our%20self-supervised%20pretraining%20strategies%20on%20domain-specific%20data%0Aoutperform%20fine-tuned%20foundation%20models%2C%20especially%20in%20identification%20tasks%0Ainvolving%20lower%20taxa%20such%20as%20genera%20and%20species.%20We%20also%20compared%20BarcodeBERT%0Awith%20BLAST%2C%20one%20of%20the%20most%20widely%20used%20bioinformatics%20tools%20for%20sequence%0Asearching%2C%20and%20found%20that%20our%20method%20matched%20BLAST%27s%20performance%20in%0Aspecies-level%20classification%20while%20being%2055%20times%20faster.%20Our%20analysis%20of%0Amasking%20and%20tokenization%20strategies%20also%20provides%20practical%20guidance%20for%0Abuilding%20customized%20DNA%20language%20models%2C%20emphasizing%20the%20importance%20of%20aligning%0Amodel%20training%20strategies%20with%20dataset%20characteristics%20and%20domain%20knowledge.%0AThe%20code%20repository%20is%20available%20at%20https%3A//github.com/bioscan-ml/BarcodeBERT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02401v3&entry.124074799=Read"},
{"title": "Constrain Alignment with Sparse Autoencoders", "author": "Qingyu Yin and Chak Tou Leong and Minjun Zhu and Hanqi Yan and Qiang Zhang and Yulan He and Wenjie Li and Jun Wang and Yue Zhang and Linyi Yang", "abstract": "  The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments.\n", "link": "http://arxiv.org/abs/2411.07618v4", "date": "2025-07-10", "relevancy": 2.5513, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrain%20Alignment%20with%20Sparse%20Autoencoders&body=Title%3A%20Constrain%20Alignment%20with%20Sparse%20Autoencoders%0AAuthor%3A%20Qingyu%20Yin%20and%20Chak%20Tou%20Leong%20and%20Minjun%20Zhu%20and%20Hanqi%20Yan%20and%20Qiang%20Zhang%20and%20Yulan%20He%20and%20Wenjie%20Li%20and%20Jun%20Wang%20and%20Yue%20Zhang%20and%20Linyi%20Yang%0AAbstract%3A%20%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences%20remains%0Aa%20key%20challenge.%20While%20post-training%20techniques%20like%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20have%0Aachieved%20notable%20success%2C%20they%20often%20introduce%20computational%20inefficiencies%20and%0Atraining%20instability.%20In%20this%20paper%2C%20we%20propose%20Feature-level%20constrained%0APreference%20Optimization%20%28FPO%29%2C%20a%20novel%20method%20designed%20to%20simplify%20the%0Aalignment%20process%20while%20ensuring%20stability.%20FPO%20leverages%20pre-trained%20Sparse%0AAutoencoders%20%28SAEs%29%20and%20introduces%20feature-level%20constraints%2C%20allowing%20for%0Aefficient%2C%20sparsity-enforced%20alignment.%20Our%20approach%20enjoys%20efficiency%20by%20using%0Asparse%20features%20activated%20in%20a%20well-trained%20sparse%20autoencoder%20and%20the%20quality%0Aof%20sequential%20KL%20divergence%20by%20using%20the%20feature-level%20offline%20reference.%0AExperimental%20results%20on%20benchmark%20datasets%20demonstrate%20that%20FPO%20achieves%20a%0A5.08%25%20absolute%20improvement%20in%20win%20rate%20with%20much%20lower%20computational%20cost%0Acompared%20to%20state-of-the-art%20baselines%2C%20making%20it%20a%20promising%20solution%20for%0Aefficient%20and%20controllable%20LLM%20alignments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07618v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrain%2520Alignment%2520with%2520Sparse%2520Autoencoders%26entry.906535625%3DQingyu%2520Yin%2520and%2520Chak%2520Tou%2520Leong%2520and%2520Minjun%2520Zhu%2520and%2520Hanqi%2520Yan%2520and%2520Qiang%2520Zhang%2520and%2520Yulan%2520He%2520and%2520Wenjie%2520Li%2520and%2520Jun%2520Wang%2520and%2520Yue%2520Zhang%2520and%2520Linyi%2520Yang%26entry.1292438233%3D%2520%2520The%2520alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%2520preferences%2520remains%250Aa%2520key%2520challenge.%2520While%2520post-training%2520techniques%2520like%2520Reinforcement%2520Learning%250Afrom%2520Human%2520Feedback%2520%2528RLHF%2529%2520and%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520have%250Aachieved%2520notable%2520success%252C%2520they%2520often%2520introduce%2520computational%2520inefficiencies%2520and%250Atraining%2520instability.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Feature-level%2520constrained%250APreference%2520Optimization%2520%2528FPO%2529%252C%2520a%2520novel%2520method%2520designed%2520to%2520simplify%2520the%250Aalignment%2520process%2520while%2520ensuring%2520stability.%2520FPO%2520leverages%2520pre-trained%2520Sparse%250AAutoencoders%2520%2528SAEs%2529%2520and%2520introduces%2520feature-level%2520constraints%252C%2520allowing%2520for%250Aefficient%252C%2520sparsity-enforced%2520alignment.%2520Our%2520approach%2520enjoys%2520efficiency%2520by%2520using%250Asparse%2520features%2520activated%2520in%2520a%2520well-trained%2520sparse%2520autoencoder%2520and%2520the%2520quality%250Aof%2520sequential%2520KL%2520divergence%2520by%2520using%2520the%2520feature-level%2520offline%2520reference.%250AExperimental%2520results%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520FPO%2520achieves%2520a%250A5.08%2525%2520absolute%2520improvement%2520in%2520win%2520rate%2520with%2520much%2520lower%2520computational%2520cost%250Acompared%2520to%2520state-of-the-art%2520baselines%252C%2520making%2520it%2520a%2520promising%2520solution%2520for%250Aefficient%2520and%2520controllable%2520LLM%2520alignments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07618v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrain%20Alignment%20with%20Sparse%20Autoencoders&entry.906535625=Qingyu%20Yin%20and%20Chak%20Tou%20Leong%20and%20Minjun%20Zhu%20and%20Hanqi%20Yan%20and%20Qiang%20Zhang%20and%20Yulan%20He%20and%20Wenjie%20Li%20and%20Jun%20Wang%20and%20Yue%20Zhang%20and%20Linyi%20Yang&entry.1292438233=%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences%20remains%0Aa%20key%20challenge.%20While%20post-training%20techniques%20like%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20have%0Aachieved%20notable%20success%2C%20they%20often%20introduce%20computational%20inefficiencies%20and%0Atraining%20instability.%20In%20this%20paper%2C%20we%20propose%20Feature-level%20constrained%0APreference%20Optimization%20%28FPO%29%2C%20a%20novel%20method%20designed%20to%20simplify%20the%0Aalignment%20process%20while%20ensuring%20stability.%20FPO%20leverages%20pre-trained%20Sparse%0AAutoencoders%20%28SAEs%29%20and%20introduces%20feature-level%20constraints%2C%20allowing%20for%0Aefficient%2C%20sparsity-enforced%20alignment.%20Our%20approach%20enjoys%20efficiency%20by%20using%0Asparse%20features%20activated%20in%20a%20well-trained%20sparse%20autoencoder%20and%20the%20quality%0Aof%20sequential%20KL%20divergence%20by%20using%20the%20feature-level%20offline%20reference.%0AExperimental%20results%20on%20benchmark%20datasets%20demonstrate%20that%20FPO%20achieves%20a%0A5.08%25%20absolute%20improvement%20in%20win%20rate%20with%20much%20lower%20computational%20cost%0Acompared%20to%20state-of-the-art%20baselines%2C%20making%20it%20a%20promising%20solution%20for%0Aefficient%20and%20controllable%20LLM%20alignments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07618v4&entry.124074799=Read"},
{"title": "SURPRISE3D: A Dataset for Spatial Understanding and Reasoning in Complex\n  3D Scenes", "author": "Jiaxin Huang and Ziwen Li and Hanlve Zhang and Runnan Chen and Xiao He and Yandong Guo and Wenping Wang and Tongliang Liu and Mingming Gong", "abstract": "  The integration of language and 3D perception is critical for embodied AI and\nrobotic systems to perceive, understand, and interact with the physical world.\nSpatial reasoning, a key capability for understanding spatial relationships\nbetween objects, remains underexplored in current 3D vision-language research.\nExisting datasets often mix semantic cues (e.g., object name) with spatial\ncontext, leading models to rely on superficial shortcuts rather than genuinely\ninterpreting spatial relationships. To address this gap, we introduce\nS\\textsc{urprise}3D, a novel dataset designed to evaluate language-guided\nspatial reasoning segmentation in complex 3D scenes. S\\textsc{urprise}3D\nconsists of more than 200k vision language pairs across 900+ detailed indoor\nscenes from ScanNet++ v2, including more than 2.8k unique object classes. The\ndataset contains 89k+ human-annotated spatial queries deliberately crafted\nwithout object name, thereby mitigating shortcut biases in spatial\nunderstanding. These queries comprehensively cover various spatial reasoning\nskills, such as relative position, narrative perspective, parametric\nperspective, and absolute distance reasoning. Initial benchmarks demonstrate\nsignificant challenges for current state-of-the-art expert 3D visual grounding\nmethods and 3D-LLMs, underscoring the necessity of our dataset and the\naccompanying 3D Spatial Reasoning Segmentation (3D-SRS) benchmark suite.\nS\\textsc{urprise}3D and 3D-SRS aim to facilitate advancements in spatially\naware AI, paving the way for effective embodied interaction and robotic\nplanning. The code and datasets can be found in\nhttps://github.com/liziwennba/SUPRISE.\n", "link": "http://arxiv.org/abs/2507.07781v1", "date": "2025-07-10", "relevancy": 2.5499, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6524}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SURPRISE3D%3A%20A%20Dataset%20for%20Spatial%20Understanding%20and%20Reasoning%20in%20Complex%0A%20%203D%20Scenes&body=Title%3A%20SURPRISE3D%3A%20A%20Dataset%20for%20Spatial%20Understanding%20and%20Reasoning%20in%20Complex%0A%20%203D%20Scenes%0AAuthor%3A%20Jiaxin%20Huang%20and%20Ziwen%20Li%20and%20Hanlve%20Zhang%20and%20Runnan%20Chen%20and%20Xiao%20He%20and%20Yandong%20Guo%20and%20Wenping%20Wang%20and%20Tongliang%20Liu%20and%20Mingming%20Gong%0AAbstract%3A%20%20%20The%20integration%20of%20language%20and%203D%20perception%20is%20critical%20for%20embodied%20AI%20and%0Arobotic%20systems%20to%20perceive%2C%20understand%2C%20and%20interact%20with%20the%20physical%20world.%0ASpatial%20reasoning%2C%20a%20key%20capability%20for%20understanding%20spatial%20relationships%0Abetween%20objects%2C%20remains%20underexplored%20in%20current%203D%20vision-language%20research.%0AExisting%20datasets%20often%20mix%20semantic%20cues%20%28e.g.%2C%20object%20name%29%20with%20spatial%0Acontext%2C%20leading%20models%20to%20rely%20on%20superficial%20shortcuts%20rather%20than%20genuinely%0Ainterpreting%20spatial%20relationships.%20To%20address%20this%20gap%2C%20we%20introduce%0AS%5Ctextsc%7Burprise%7D3D%2C%20a%20novel%20dataset%20designed%20to%20evaluate%20language-guided%0Aspatial%20reasoning%20segmentation%20in%20complex%203D%20scenes.%20S%5Ctextsc%7Burprise%7D3D%0Aconsists%20of%20more%20than%20200k%20vision%20language%20pairs%20across%20900%2B%20detailed%20indoor%0Ascenes%20from%20ScanNet%2B%2B%20v2%2C%20including%20more%20than%202.8k%20unique%20object%20classes.%20The%0Adataset%20contains%2089k%2B%20human-annotated%20spatial%20queries%20deliberately%20crafted%0Awithout%20object%20name%2C%20thereby%20mitigating%20shortcut%20biases%20in%20spatial%0Aunderstanding.%20These%20queries%20comprehensively%20cover%20various%20spatial%20reasoning%0Askills%2C%20such%20as%20relative%20position%2C%20narrative%20perspective%2C%20parametric%0Aperspective%2C%20and%20absolute%20distance%20reasoning.%20Initial%20benchmarks%20demonstrate%0Asignificant%20challenges%20for%20current%20state-of-the-art%20expert%203D%20visual%20grounding%0Amethods%20and%203D-LLMs%2C%20underscoring%20the%20necessity%20of%20our%20dataset%20and%20the%0Aaccompanying%203D%20Spatial%20Reasoning%20Segmentation%20%283D-SRS%29%20benchmark%20suite.%0AS%5Ctextsc%7Burprise%7D3D%20and%203D-SRS%20aim%20to%20facilitate%20advancements%20in%20spatially%0Aaware%20AI%2C%20paving%20the%20way%20for%20effective%20embodied%20interaction%20and%20robotic%0Aplanning.%20The%20code%20and%20datasets%20can%20be%20found%20in%0Ahttps%3A//github.com/liziwennba/SUPRISE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSURPRISE3D%253A%2520A%2520Dataset%2520for%2520Spatial%2520Understanding%2520and%2520Reasoning%2520in%2520Complex%250A%2520%25203D%2520Scenes%26entry.906535625%3DJiaxin%2520Huang%2520and%2520Ziwen%2520Li%2520and%2520Hanlve%2520Zhang%2520and%2520Runnan%2520Chen%2520and%2520Xiao%2520He%2520and%2520Yandong%2520Guo%2520and%2520Wenping%2520Wang%2520and%2520Tongliang%2520Liu%2520and%2520Mingming%2520Gong%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520language%2520and%25203D%2520perception%2520is%2520critical%2520for%2520embodied%2520AI%2520and%250Arobotic%2520systems%2520to%2520perceive%252C%2520understand%252C%2520and%2520interact%2520with%2520the%2520physical%2520world.%250ASpatial%2520reasoning%252C%2520a%2520key%2520capability%2520for%2520understanding%2520spatial%2520relationships%250Abetween%2520objects%252C%2520remains%2520underexplored%2520in%2520current%25203D%2520vision-language%2520research.%250AExisting%2520datasets%2520often%2520mix%2520semantic%2520cues%2520%2528e.g.%252C%2520object%2520name%2529%2520with%2520spatial%250Acontext%252C%2520leading%2520models%2520to%2520rely%2520on%2520superficial%2520shortcuts%2520rather%2520than%2520genuinely%250Ainterpreting%2520spatial%2520relationships.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250AS%255Ctextsc%257Burprise%257D3D%252C%2520a%2520novel%2520dataset%2520designed%2520to%2520evaluate%2520language-guided%250Aspatial%2520reasoning%2520segmentation%2520in%2520complex%25203D%2520scenes.%2520S%255Ctextsc%257Burprise%257D3D%250Aconsists%2520of%2520more%2520than%2520200k%2520vision%2520language%2520pairs%2520across%2520900%252B%2520detailed%2520indoor%250Ascenes%2520from%2520ScanNet%252B%252B%2520v2%252C%2520including%2520more%2520than%25202.8k%2520unique%2520object%2520classes.%2520The%250Adataset%2520contains%252089k%252B%2520human-annotated%2520spatial%2520queries%2520deliberately%2520crafted%250Awithout%2520object%2520name%252C%2520thereby%2520mitigating%2520shortcut%2520biases%2520in%2520spatial%250Aunderstanding.%2520These%2520queries%2520comprehensively%2520cover%2520various%2520spatial%2520reasoning%250Askills%252C%2520such%2520as%2520relative%2520position%252C%2520narrative%2520perspective%252C%2520parametric%250Aperspective%252C%2520and%2520absolute%2520distance%2520reasoning.%2520Initial%2520benchmarks%2520demonstrate%250Asignificant%2520challenges%2520for%2520current%2520state-of-the-art%2520expert%25203D%2520visual%2520grounding%250Amethods%2520and%25203D-LLMs%252C%2520underscoring%2520the%2520necessity%2520of%2520our%2520dataset%2520and%2520the%250Aaccompanying%25203D%2520Spatial%2520Reasoning%2520Segmentation%2520%25283D-SRS%2529%2520benchmark%2520suite.%250AS%255Ctextsc%257Burprise%257D3D%2520and%25203D-SRS%2520aim%2520to%2520facilitate%2520advancements%2520in%2520spatially%250Aaware%2520AI%252C%2520paving%2520the%2520way%2520for%2520effective%2520embodied%2520interaction%2520and%2520robotic%250Aplanning.%2520The%2520code%2520and%2520datasets%2520can%2520be%2520found%2520in%250Ahttps%253A//github.com/liziwennba/SUPRISE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SURPRISE3D%3A%20A%20Dataset%20for%20Spatial%20Understanding%20and%20Reasoning%20in%20Complex%0A%20%203D%20Scenes&entry.906535625=Jiaxin%20Huang%20and%20Ziwen%20Li%20and%20Hanlve%20Zhang%20and%20Runnan%20Chen%20and%20Xiao%20He%20and%20Yandong%20Guo%20and%20Wenping%20Wang%20and%20Tongliang%20Liu%20and%20Mingming%20Gong&entry.1292438233=%20%20The%20integration%20of%20language%20and%203D%20perception%20is%20critical%20for%20embodied%20AI%20and%0Arobotic%20systems%20to%20perceive%2C%20understand%2C%20and%20interact%20with%20the%20physical%20world.%0ASpatial%20reasoning%2C%20a%20key%20capability%20for%20understanding%20spatial%20relationships%0Abetween%20objects%2C%20remains%20underexplored%20in%20current%203D%20vision-language%20research.%0AExisting%20datasets%20often%20mix%20semantic%20cues%20%28e.g.%2C%20object%20name%29%20with%20spatial%0Acontext%2C%20leading%20models%20to%20rely%20on%20superficial%20shortcuts%20rather%20than%20genuinely%0Ainterpreting%20spatial%20relationships.%20To%20address%20this%20gap%2C%20we%20introduce%0AS%5Ctextsc%7Burprise%7D3D%2C%20a%20novel%20dataset%20designed%20to%20evaluate%20language-guided%0Aspatial%20reasoning%20segmentation%20in%20complex%203D%20scenes.%20S%5Ctextsc%7Burprise%7D3D%0Aconsists%20of%20more%20than%20200k%20vision%20language%20pairs%20across%20900%2B%20detailed%20indoor%0Ascenes%20from%20ScanNet%2B%2B%20v2%2C%20including%20more%20than%202.8k%20unique%20object%20classes.%20The%0Adataset%20contains%2089k%2B%20human-annotated%20spatial%20queries%20deliberately%20crafted%0Awithout%20object%20name%2C%20thereby%20mitigating%20shortcut%20biases%20in%20spatial%0Aunderstanding.%20These%20queries%20comprehensively%20cover%20various%20spatial%20reasoning%0Askills%2C%20such%20as%20relative%20position%2C%20narrative%20perspective%2C%20parametric%0Aperspective%2C%20and%20absolute%20distance%20reasoning.%20Initial%20benchmarks%20demonstrate%0Asignificant%20challenges%20for%20current%20state-of-the-art%20expert%203D%20visual%20grounding%0Amethods%20and%203D-LLMs%2C%20underscoring%20the%20necessity%20of%20our%20dataset%20and%20the%0Aaccompanying%203D%20Spatial%20Reasoning%20Segmentation%20%283D-SRS%29%20benchmark%20suite.%0AS%5Ctextsc%7Burprise%7D3D%20and%203D-SRS%20aim%20to%20facilitate%20advancements%20in%20spatially%0Aaware%20AI%2C%20paving%20the%20way%20for%20effective%20embodied%20interaction%20and%20robotic%0Aplanning.%20The%20code%20and%20datasets%20can%20be%20found%20in%0Ahttps%3A//github.com/liziwennba/SUPRISE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07781v1&entry.124074799=Read"},
{"title": "TRIX- Trading Adversarial Fairness via Mixed Adversarial Training", "author": "Tejaswini Medi and Steffen Jung and Margret Keuper", "abstract": "  Adversarial Training (AT) is a widely adopted defense against adversarial\nexamples. However, existing approaches typically apply a uniform training\nobjective across all classes, overlooking disparities in class-wise\nvulnerability. This results in adversarial unfairness: classes with well\ndistinguishable features (strong classes) tend to become more robust, while\nclasses with overlapping or shared features(weak classes) remain\ndisproportionately susceptible to adversarial attacks. We observe that strong\nclasses do not require strong adversaries during training, as their non-robust\nfeatures are quickly suppressed. In contrast, weak classes benefit from\nstronger adversaries to effectively reduce their vulnerabilities. Motivated by\nthis, we introduce TRIX, a feature-aware adversarial training framework that\nadaptively assigns weaker targeted adversaries to strong classes, promoting\nfeature diversity via uniformly sampled targets, and stronger untargeted\nadversaries to weak classes, enhancing their focused robustness. TRIX further\nincorporates per-class loss weighting and perturbation strength adjustments,\nbuilding on prior work, to emphasize weak classes during the optimization.\nComprehensive experiments on standard image classification benchmarks,\nincluding evaluations under strong attacks such as PGD and AutoAttack,\ndemonstrate that TRIX significantly improves worst-case class accuracy on both\nclean and adversarial data, reducing inter-class robustness disparities, and\npreserves overall accuracy. Our results highlight TRIX as a practical step\ntoward fair and effective adversarial defense.\n", "link": "http://arxiv.org/abs/2507.07768v1", "date": "2025-07-10", "relevancy": 2.5493, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5387}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5186}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRIX-%20Trading%20Adversarial%20Fairness%20via%20Mixed%20Adversarial%20Training&body=Title%3A%20TRIX-%20Trading%20Adversarial%20Fairness%20via%20Mixed%20Adversarial%20Training%0AAuthor%3A%20Tejaswini%20Medi%20and%20Steffen%20Jung%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20Adversarial%20Training%20%28AT%29%20is%20a%20widely%20adopted%20defense%20against%20adversarial%0Aexamples.%20However%2C%20existing%20approaches%20typically%20apply%20a%20uniform%20training%0Aobjective%20across%20all%20classes%2C%20overlooking%20disparities%20in%20class-wise%0Avulnerability.%20This%20results%20in%20adversarial%20unfairness%3A%20classes%20with%20well%0Adistinguishable%20features%20%28strong%20classes%29%20tend%20to%20become%20more%20robust%2C%20while%0Aclasses%20with%20overlapping%20or%20shared%20features%28weak%20classes%29%20remain%0Adisproportionately%20susceptible%20to%20adversarial%20attacks.%20We%20observe%20that%20strong%0Aclasses%20do%20not%20require%20strong%20adversaries%20during%20training%2C%20as%20their%20non-robust%0Afeatures%20are%20quickly%20suppressed.%20In%20contrast%2C%20weak%20classes%20benefit%20from%0Astronger%20adversaries%20to%20effectively%20reduce%20their%20vulnerabilities.%20Motivated%20by%0Athis%2C%20we%20introduce%20TRIX%2C%20a%20feature-aware%20adversarial%20training%20framework%20that%0Aadaptively%20assigns%20weaker%20targeted%20adversaries%20to%20strong%20classes%2C%20promoting%0Afeature%20diversity%20via%20uniformly%20sampled%20targets%2C%20and%20stronger%20untargeted%0Aadversaries%20to%20weak%20classes%2C%20enhancing%20their%20focused%20robustness.%20TRIX%20further%0Aincorporates%20per-class%20loss%20weighting%20and%20perturbation%20strength%20adjustments%2C%0Abuilding%20on%20prior%20work%2C%20to%20emphasize%20weak%20classes%20during%20the%20optimization.%0AComprehensive%20experiments%20on%20standard%20image%20classification%20benchmarks%2C%0Aincluding%20evaluations%20under%20strong%20attacks%20such%20as%20PGD%20and%20AutoAttack%2C%0Ademonstrate%20that%20TRIX%20significantly%20improves%20worst-case%20class%20accuracy%20on%20both%0Aclean%20and%20adversarial%20data%2C%20reducing%20inter-class%20robustness%20disparities%2C%20and%0Apreserves%20overall%20accuracy.%20Our%20results%20highlight%20TRIX%20as%20a%20practical%20step%0Atoward%20fair%20and%20effective%20adversarial%20defense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRIX-%2520Trading%2520Adversarial%2520Fairness%2520via%2520Mixed%2520Adversarial%2520Training%26entry.906535625%3DTejaswini%2520Medi%2520and%2520Steffen%2520Jung%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520Adversarial%2520Training%2520%2528AT%2529%2520is%2520a%2520widely%2520adopted%2520defense%2520against%2520adversarial%250Aexamples.%2520However%252C%2520existing%2520approaches%2520typically%2520apply%2520a%2520uniform%2520training%250Aobjective%2520across%2520all%2520classes%252C%2520overlooking%2520disparities%2520in%2520class-wise%250Avulnerability.%2520This%2520results%2520in%2520adversarial%2520unfairness%253A%2520classes%2520with%2520well%250Adistinguishable%2520features%2520%2528strong%2520classes%2529%2520tend%2520to%2520become%2520more%2520robust%252C%2520while%250Aclasses%2520with%2520overlapping%2520or%2520shared%2520features%2528weak%2520classes%2529%2520remain%250Adisproportionately%2520susceptible%2520to%2520adversarial%2520attacks.%2520We%2520observe%2520that%2520strong%250Aclasses%2520do%2520not%2520require%2520strong%2520adversaries%2520during%2520training%252C%2520as%2520their%2520non-robust%250Afeatures%2520are%2520quickly%2520suppressed.%2520In%2520contrast%252C%2520weak%2520classes%2520benefit%2520from%250Astronger%2520adversaries%2520to%2520effectively%2520reduce%2520their%2520vulnerabilities.%2520Motivated%2520by%250Athis%252C%2520we%2520introduce%2520TRIX%252C%2520a%2520feature-aware%2520adversarial%2520training%2520framework%2520that%250Aadaptively%2520assigns%2520weaker%2520targeted%2520adversaries%2520to%2520strong%2520classes%252C%2520promoting%250Afeature%2520diversity%2520via%2520uniformly%2520sampled%2520targets%252C%2520and%2520stronger%2520untargeted%250Aadversaries%2520to%2520weak%2520classes%252C%2520enhancing%2520their%2520focused%2520robustness.%2520TRIX%2520further%250Aincorporates%2520per-class%2520loss%2520weighting%2520and%2520perturbation%2520strength%2520adjustments%252C%250Abuilding%2520on%2520prior%2520work%252C%2520to%2520emphasize%2520weak%2520classes%2520during%2520the%2520optimization.%250AComprehensive%2520experiments%2520on%2520standard%2520image%2520classification%2520benchmarks%252C%250Aincluding%2520evaluations%2520under%2520strong%2520attacks%2520such%2520as%2520PGD%2520and%2520AutoAttack%252C%250Ademonstrate%2520that%2520TRIX%2520significantly%2520improves%2520worst-case%2520class%2520accuracy%2520on%2520both%250Aclean%2520and%2520adversarial%2520data%252C%2520reducing%2520inter-class%2520robustness%2520disparities%252C%2520and%250Apreserves%2520overall%2520accuracy.%2520Our%2520results%2520highlight%2520TRIX%2520as%2520a%2520practical%2520step%250Atoward%2520fair%2520and%2520effective%2520adversarial%2520defense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRIX-%20Trading%20Adversarial%20Fairness%20via%20Mixed%20Adversarial%20Training&entry.906535625=Tejaswini%20Medi%20and%20Steffen%20Jung%20and%20Margret%20Keuper&entry.1292438233=%20%20Adversarial%20Training%20%28AT%29%20is%20a%20widely%20adopted%20defense%20against%20adversarial%0Aexamples.%20However%2C%20existing%20approaches%20typically%20apply%20a%20uniform%20training%0Aobjective%20across%20all%20classes%2C%20overlooking%20disparities%20in%20class-wise%0Avulnerability.%20This%20results%20in%20adversarial%20unfairness%3A%20classes%20with%20well%0Adistinguishable%20features%20%28strong%20classes%29%20tend%20to%20become%20more%20robust%2C%20while%0Aclasses%20with%20overlapping%20or%20shared%20features%28weak%20classes%29%20remain%0Adisproportionately%20susceptible%20to%20adversarial%20attacks.%20We%20observe%20that%20strong%0Aclasses%20do%20not%20require%20strong%20adversaries%20during%20training%2C%20as%20their%20non-robust%0Afeatures%20are%20quickly%20suppressed.%20In%20contrast%2C%20weak%20classes%20benefit%20from%0Astronger%20adversaries%20to%20effectively%20reduce%20their%20vulnerabilities.%20Motivated%20by%0Athis%2C%20we%20introduce%20TRIX%2C%20a%20feature-aware%20adversarial%20training%20framework%20that%0Aadaptively%20assigns%20weaker%20targeted%20adversaries%20to%20strong%20classes%2C%20promoting%0Afeature%20diversity%20via%20uniformly%20sampled%20targets%2C%20and%20stronger%20untargeted%0Aadversaries%20to%20weak%20classes%2C%20enhancing%20their%20focused%20robustness.%20TRIX%20further%0Aincorporates%20per-class%20loss%20weighting%20and%20perturbation%20strength%20adjustments%2C%0Abuilding%20on%20prior%20work%2C%20to%20emphasize%20weak%20classes%20during%20the%20optimization.%0AComprehensive%20experiments%20on%20standard%20image%20classification%20benchmarks%2C%0Aincluding%20evaluations%20under%20strong%20attacks%20such%20as%20PGD%20and%20AutoAttack%2C%0Ademonstrate%20that%20TRIX%20significantly%20improves%20worst-case%20class%20accuracy%20on%20both%0Aclean%20and%20adversarial%20data%2C%20reducing%20inter-class%20robustness%20disparities%2C%20and%0Apreserves%20overall%20accuracy.%20Our%20results%20highlight%20TRIX%20as%20a%20practical%20step%0Atoward%20fair%20and%20effective%20adversarial%20defense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07768v1&entry.124074799=Read"},
{"title": "DTECT: Dynamic Topic Explorer & Context Tracker", "author": "Suman Adhya and Debarshi Kumar Sanyal", "abstract": "  The explosive growth of textual data over time presents a significant\nchallenge in uncovering evolving themes and trends. Existing dynamic topic\nmodeling techniques, while powerful, often exist in fragmented pipelines that\nlack robust support for interpretation and user-friendly exploration. We\nintroduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end\nsystem that bridges the gap between raw textual data and meaningful temporal\ninsights. DTECT provides a unified workflow that supports data preprocessing,\nmultiple model architectures, and dedicated evaluation metrics to analyze the\ntopic quality of temporal topic models. It significantly enhances\ninterpretability by introducing LLM-driven automatic topic labeling, trend\nanalysis via temporally salient words, interactive visualizations with\ndocument-level summarization, and a natural language chat interface for\nintuitive data querying. By integrating these features into a single, cohesive\nplatform, DTECT empowers users to more effectively track and understand\nthematic dynamics. DTECT is open-source and available at\nhttps://github.com/AdhyaSuman/DTECT.\n", "link": "http://arxiv.org/abs/2507.07910v1", "date": "2025-07-10", "relevancy": 2.5434, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DTECT%3A%20Dynamic%20Topic%20Explorer%20%26%20Context%20Tracker&body=Title%3A%20DTECT%3A%20Dynamic%20Topic%20Explorer%20%26%20Context%20Tracker%0AAuthor%3A%20Suman%20Adhya%20and%20Debarshi%20Kumar%20Sanyal%0AAbstract%3A%20%20%20The%20explosive%20growth%20of%20textual%20data%20over%20time%20presents%20a%20significant%0Achallenge%20in%20uncovering%20evolving%20themes%20and%20trends.%20Existing%20dynamic%20topic%0Amodeling%20techniques%2C%20while%20powerful%2C%20often%20exist%20in%20fragmented%20pipelines%20that%0Alack%20robust%20support%20for%20interpretation%20and%20user-friendly%20exploration.%20We%0Aintroduce%20DTECT%20%28Dynamic%20Topic%20Explorer%20%26%20Context%20Tracker%29%2C%20an%20end-to-end%0Asystem%20that%20bridges%20the%20gap%20between%20raw%20textual%20data%20and%20meaningful%20temporal%0Ainsights.%20DTECT%20provides%20a%20unified%20workflow%20that%20supports%20data%20preprocessing%2C%0Amultiple%20model%20architectures%2C%20and%20dedicated%20evaluation%20metrics%20to%20analyze%20the%0Atopic%20quality%20of%20temporal%20topic%20models.%20It%20significantly%20enhances%0Ainterpretability%20by%20introducing%20LLM-driven%20automatic%20topic%20labeling%2C%20trend%0Aanalysis%20via%20temporally%20salient%20words%2C%20interactive%20visualizations%20with%0Adocument-level%20summarization%2C%20and%20a%20natural%20language%20chat%20interface%20for%0Aintuitive%20data%20querying.%20By%20integrating%20these%20features%20into%20a%20single%2C%20cohesive%0Aplatform%2C%20DTECT%20empowers%20users%20to%20more%20effectively%20track%20and%20understand%0Athematic%20dynamics.%20DTECT%20is%20open-source%20and%20available%20at%0Ahttps%3A//github.com/AdhyaSuman/DTECT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDTECT%253A%2520Dynamic%2520Topic%2520Explorer%2520%2526%2520Context%2520Tracker%26entry.906535625%3DSuman%2520Adhya%2520and%2520Debarshi%2520Kumar%2520Sanyal%26entry.1292438233%3D%2520%2520The%2520explosive%2520growth%2520of%2520textual%2520data%2520over%2520time%2520presents%2520a%2520significant%250Achallenge%2520in%2520uncovering%2520evolving%2520themes%2520and%2520trends.%2520Existing%2520dynamic%2520topic%250Amodeling%2520techniques%252C%2520while%2520powerful%252C%2520often%2520exist%2520in%2520fragmented%2520pipelines%2520that%250Alack%2520robust%2520support%2520for%2520interpretation%2520and%2520user-friendly%2520exploration.%2520We%250Aintroduce%2520DTECT%2520%2528Dynamic%2520Topic%2520Explorer%2520%2526%2520Context%2520Tracker%2529%252C%2520an%2520end-to-end%250Asystem%2520that%2520bridges%2520the%2520gap%2520between%2520raw%2520textual%2520data%2520and%2520meaningful%2520temporal%250Ainsights.%2520DTECT%2520provides%2520a%2520unified%2520workflow%2520that%2520supports%2520data%2520preprocessing%252C%250Amultiple%2520model%2520architectures%252C%2520and%2520dedicated%2520evaluation%2520metrics%2520to%2520analyze%2520the%250Atopic%2520quality%2520of%2520temporal%2520topic%2520models.%2520It%2520significantly%2520enhances%250Ainterpretability%2520by%2520introducing%2520LLM-driven%2520automatic%2520topic%2520labeling%252C%2520trend%250Aanalysis%2520via%2520temporally%2520salient%2520words%252C%2520interactive%2520visualizations%2520with%250Adocument-level%2520summarization%252C%2520and%2520a%2520natural%2520language%2520chat%2520interface%2520for%250Aintuitive%2520data%2520querying.%2520By%2520integrating%2520these%2520features%2520into%2520a%2520single%252C%2520cohesive%250Aplatform%252C%2520DTECT%2520empowers%2520users%2520to%2520more%2520effectively%2520track%2520and%2520understand%250Athematic%2520dynamics.%2520DTECT%2520is%2520open-source%2520and%2520available%2520at%250Ahttps%253A//github.com/AdhyaSuman/DTECT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DTECT%3A%20Dynamic%20Topic%20Explorer%20%26%20Context%20Tracker&entry.906535625=Suman%20Adhya%20and%20Debarshi%20Kumar%20Sanyal&entry.1292438233=%20%20The%20explosive%20growth%20of%20textual%20data%20over%20time%20presents%20a%20significant%0Achallenge%20in%20uncovering%20evolving%20themes%20and%20trends.%20Existing%20dynamic%20topic%0Amodeling%20techniques%2C%20while%20powerful%2C%20often%20exist%20in%20fragmented%20pipelines%20that%0Alack%20robust%20support%20for%20interpretation%20and%20user-friendly%20exploration.%20We%0Aintroduce%20DTECT%20%28Dynamic%20Topic%20Explorer%20%26%20Context%20Tracker%29%2C%20an%20end-to-end%0Asystem%20that%20bridges%20the%20gap%20between%20raw%20textual%20data%20and%20meaningful%20temporal%0Ainsights.%20DTECT%20provides%20a%20unified%20workflow%20that%20supports%20data%20preprocessing%2C%0Amultiple%20model%20architectures%2C%20and%20dedicated%20evaluation%20metrics%20to%20analyze%20the%0Atopic%20quality%20of%20temporal%20topic%20models.%20It%20significantly%20enhances%0Ainterpretability%20by%20introducing%20LLM-driven%20automatic%20topic%20labeling%2C%20trend%0Aanalysis%20via%20temporally%20salient%20words%2C%20interactive%20visualizations%20with%0Adocument-level%20summarization%2C%20and%20a%20natural%20language%20chat%20interface%20for%0Aintuitive%20data%20querying.%20By%20integrating%20these%20features%20into%20a%20single%2C%20cohesive%0Aplatform%2C%20DTECT%20empowers%20users%20to%20more%20effectively%20track%20and%20understand%0Athematic%20dynamics.%20DTECT%20is%20open-source%20and%20available%20at%0Ahttps%3A//github.com/AdhyaSuman/DTECT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07910v1&entry.124074799=Read"},
{"title": "Open-source automatic pipeline for efficient conversion of large-scale\n  point clouds to IFC format", "author": "Sl\u00e1vek Zbirovsk\u00fd and V\u00e1clav Ne\u017eerka", "abstract": "  Building Information Modeling (BIM) is an essential component in the\nsustainable reconstruction and revitalization of ageing structures. However,\nmodel creation usually relies on laborious manual transformation of the\nunstructured point cloud data provided by laser scans or photogrammetry. This\npaper presents Cloud2BIM, an open-source software tool designed to automate the\nconversion of point clouds into BIM models compliant with the Industry\nFoundation Classes (IFC) standard. Cloud2BIM integrates advanced algorithms for\nwall and slab segmentation, opening detection, and room zoning based on real\nwall surfaces, resulting in a comprehensive and fully automated workflow.\nUnlike existing tools, it avoids computationally- and calibration-intensive\ntechniques such as RANSAC, supports non-orthogonal geometries, and provides\nunprecedented processing speed-achieving results up to seven times faster than\nfastest competing solutions. Systematic validation using benchmark datasets\nconfirms that Cloud2BIM is an easy-to-use, efficient, and scalable solution for\ngenerating accurate BIM models, capable of converting extensive point cloud\ndatasets for entire buildings into IFC format with minimal user input.\n", "link": "http://arxiv.org/abs/2503.11498v3", "date": "2025-07-10", "relevancy": 2.5309, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5211}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5211}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-source%20automatic%20pipeline%20for%20efficient%20conversion%20of%20large-scale%0A%20%20point%20clouds%20to%20IFC%20format&body=Title%3A%20Open-source%20automatic%20pipeline%20for%20efficient%20conversion%20of%20large-scale%0A%20%20point%20clouds%20to%20IFC%20format%0AAuthor%3A%20Sl%C3%A1vek%20Zbirovsk%C3%BD%20and%20V%C3%A1clav%20Ne%C5%BEerka%0AAbstract%3A%20%20%20Building%20Information%20Modeling%20%28BIM%29%20is%20an%20essential%20component%20in%20the%0Asustainable%20reconstruction%20and%20revitalization%20of%20ageing%20structures.%20However%2C%0Amodel%20creation%20usually%20relies%20on%20laborious%20manual%20transformation%20of%20the%0Aunstructured%20point%20cloud%20data%20provided%20by%20laser%20scans%20or%20photogrammetry.%20This%0Apaper%20presents%20Cloud2BIM%2C%20an%20open-source%20software%20tool%20designed%20to%20automate%20the%0Aconversion%20of%20point%20clouds%20into%20BIM%20models%20compliant%20with%20the%20Industry%0AFoundation%20Classes%20%28IFC%29%20standard.%20Cloud2BIM%20integrates%20advanced%20algorithms%20for%0Awall%20and%20slab%20segmentation%2C%20opening%20detection%2C%20and%20room%20zoning%20based%20on%20real%0Awall%20surfaces%2C%20resulting%20in%20a%20comprehensive%20and%20fully%20automated%20workflow.%0AUnlike%20existing%20tools%2C%20it%20avoids%20computationally-%20and%20calibration-intensive%0Atechniques%20such%20as%20RANSAC%2C%20supports%20non-orthogonal%20geometries%2C%20and%20provides%0Aunprecedented%20processing%20speed-achieving%20results%20up%20to%20seven%20times%20faster%20than%0Afastest%20competing%20solutions.%20Systematic%20validation%20using%20benchmark%20datasets%0Aconfirms%20that%20Cloud2BIM%20is%20an%20easy-to-use%2C%20efficient%2C%20and%20scalable%20solution%20for%0Agenerating%20accurate%20BIM%20models%2C%20capable%20of%20converting%20extensive%20point%20cloud%0Adatasets%20for%20entire%20buildings%20into%20IFC%20format%20with%20minimal%20user%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.11498v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-source%2520automatic%2520pipeline%2520for%2520efficient%2520conversion%2520of%2520large-scale%250A%2520%2520point%2520clouds%2520to%2520IFC%2520format%26entry.906535625%3DSl%25C3%25A1vek%2520Zbirovsk%25C3%25BD%2520and%2520V%25C3%25A1clav%2520Ne%25C5%25BEerka%26entry.1292438233%3D%2520%2520Building%2520Information%2520Modeling%2520%2528BIM%2529%2520is%2520an%2520essential%2520component%2520in%2520the%250Asustainable%2520reconstruction%2520and%2520revitalization%2520of%2520ageing%2520structures.%2520However%252C%250Amodel%2520creation%2520usually%2520relies%2520on%2520laborious%2520manual%2520transformation%2520of%2520the%250Aunstructured%2520point%2520cloud%2520data%2520provided%2520by%2520laser%2520scans%2520or%2520photogrammetry.%2520This%250Apaper%2520presents%2520Cloud2BIM%252C%2520an%2520open-source%2520software%2520tool%2520designed%2520to%2520automate%2520the%250Aconversion%2520of%2520point%2520clouds%2520into%2520BIM%2520models%2520compliant%2520with%2520the%2520Industry%250AFoundation%2520Classes%2520%2528IFC%2529%2520standard.%2520Cloud2BIM%2520integrates%2520advanced%2520algorithms%2520for%250Awall%2520and%2520slab%2520segmentation%252C%2520opening%2520detection%252C%2520and%2520room%2520zoning%2520based%2520on%2520real%250Awall%2520surfaces%252C%2520resulting%2520in%2520a%2520comprehensive%2520and%2520fully%2520automated%2520workflow.%250AUnlike%2520existing%2520tools%252C%2520it%2520avoids%2520computationally-%2520and%2520calibration-intensive%250Atechniques%2520such%2520as%2520RANSAC%252C%2520supports%2520non-orthogonal%2520geometries%252C%2520and%2520provides%250Aunprecedented%2520processing%2520speed-achieving%2520results%2520up%2520to%2520seven%2520times%2520faster%2520than%250Afastest%2520competing%2520solutions.%2520Systematic%2520validation%2520using%2520benchmark%2520datasets%250Aconfirms%2520that%2520Cloud2BIM%2520is%2520an%2520easy-to-use%252C%2520efficient%252C%2520and%2520scalable%2520solution%2520for%250Agenerating%2520accurate%2520BIM%2520models%252C%2520capable%2520of%2520converting%2520extensive%2520point%2520cloud%250Adatasets%2520for%2520entire%2520buildings%2520into%2520IFC%2520format%2520with%2520minimal%2520user%2520input.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.11498v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-source%20automatic%20pipeline%20for%20efficient%20conversion%20of%20large-scale%0A%20%20point%20clouds%20to%20IFC%20format&entry.906535625=Sl%C3%A1vek%20Zbirovsk%C3%BD%20and%20V%C3%A1clav%20Ne%C5%BEerka&entry.1292438233=%20%20Building%20Information%20Modeling%20%28BIM%29%20is%20an%20essential%20component%20in%20the%0Asustainable%20reconstruction%20and%20revitalization%20of%20ageing%20structures.%20However%2C%0Amodel%20creation%20usually%20relies%20on%20laborious%20manual%20transformation%20of%20the%0Aunstructured%20point%20cloud%20data%20provided%20by%20laser%20scans%20or%20photogrammetry.%20This%0Apaper%20presents%20Cloud2BIM%2C%20an%20open-source%20software%20tool%20designed%20to%20automate%20the%0Aconversion%20of%20point%20clouds%20into%20BIM%20models%20compliant%20with%20the%20Industry%0AFoundation%20Classes%20%28IFC%29%20standard.%20Cloud2BIM%20integrates%20advanced%20algorithms%20for%0Awall%20and%20slab%20segmentation%2C%20opening%20detection%2C%20and%20room%20zoning%20based%20on%20real%0Awall%20surfaces%2C%20resulting%20in%20a%20comprehensive%20and%20fully%20automated%20workflow.%0AUnlike%20existing%20tools%2C%20it%20avoids%20computationally-%20and%20calibration-intensive%0Atechniques%20such%20as%20RANSAC%2C%20supports%20non-orthogonal%20geometries%2C%20and%20provides%0Aunprecedented%20processing%20speed-achieving%20results%20up%20to%20seven%20times%20faster%20than%0Afastest%20competing%20solutions.%20Systematic%20validation%20using%20benchmark%20datasets%0Aconfirms%20that%20Cloud2BIM%20is%20an%20easy-to-use%2C%20efficient%2C%20and%20scalable%20solution%20for%0Agenerating%20accurate%20BIM%20models%2C%20capable%20of%20converting%20extensive%20point%20cloud%0Adatasets%20for%20entire%20buildings%20into%20IFC%20format%20with%20minimal%20user%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.11498v3&entry.124074799=Read"},
{"title": "THUNDER: Tile-level Histopathology image UNDERstanding benchmark", "author": "Pierre Marza and Leo Fillioux and Sofi\u00e8ne Boutaj and Kunal Mahatha and Christian Desrosiers and Pablo Piantanida and Jose Dolz and Stergios Christodoulidis and Maria Vakalopoulou", "abstract": "  Progress in a research field can be hard to assess, in particular when many\nconcurrent methods are proposed in a short period of time. This is the case in\ndigital pathology, where many foundation models have been released recently to\nserve as feature extractors for tile-level images, being used in a variety of\ndownstream tasks, both for tile- and slide-level problems. Benchmarking\navailable methods then becomes paramount to get a clearer view of the research\nlandscape. In particular, in critical domains such as healthcare, a benchmark\nshould not only focus on evaluating downstream performance, but also provide\ninsights about the main differences between methods, and importantly, further\nconsider uncertainty and robustness to ensure a reliable usage of proposed\nmodels. For these reasons, we introduce THUNDER, a tile-level benchmark for\ndigital pathology foundation models, allowing for efficient comparison of many\nmodels on diverse datasets with a series of downstream tasks, studying their\nfeature spaces and assessing the robustness and uncertainty of predictions\ninformed by their embeddings. THUNDER is a fast, easy-to-use, dynamic benchmark\nthat can already support a large variety of state-of-the-art foundation, as\nwell as local user-defined models for direct tile-based comparison. In this\npaper, we provide a comprehensive comparison of 23 foundation models on 16\ndifferent datasets covering diverse tasks, feature analysis, and robustness.\nThe code for THUNDER is publicly available at\nhttps://github.com/MICS-Lab/thunder.\n", "link": "http://arxiv.org/abs/2507.07860v1", "date": "2025-07-10", "relevancy": 2.5294, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5098}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5098}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20THUNDER%3A%20Tile-level%20Histopathology%20image%20UNDERstanding%20benchmark&body=Title%3A%20THUNDER%3A%20Tile-level%20Histopathology%20image%20UNDERstanding%20benchmark%0AAuthor%3A%20Pierre%20Marza%20and%20Leo%20Fillioux%20and%20Sofi%C3%A8ne%20Boutaj%20and%20Kunal%20Mahatha%20and%20Christian%20Desrosiers%20and%20Pablo%20Piantanida%20and%20Jose%20Dolz%20and%20Stergios%20Christodoulidis%20and%20Maria%20Vakalopoulou%0AAbstract%3A%20%20%20Progress%20in%20a%20research%20field%20can%20be%20hard%20to%20assess%2C%20in%20particular%20when%20many%0Aconcurrent%20methods%20are%20proposed%20in%20a%20short%20period%20of%20time.%20This%20is%20the%20case%20in%0Adigital%20pathology%2C%20where%20many%20foundation%20models%20have%20been%20released%20recently%20to%0Aserve%20as%20feature%20extractors%20for%20tile-level%20images%2C%20being%20used%20in%20a%20variety%20of%0Adownstream%20tasks%2C%20both%20for%20tile-%20and%20slide-level%20problems.%20Benchmarking%0Aavailable%20methods%20then%20becomes%20paramount%20to%20get%20a%20clearer%20view%20of%20the%20research%0Alandscape.%20In%20particular%2C%20in%20critical%20domains%20such%20as%20healthcare%2C%20a%20benchmark%0Ashould%20not%20only%20focus%20on%20evaluating%20downstream%20performance%2C%20but%20also%20provide%0Ainsights%20about%20the%20main%20differences%20between%20methods%2C%20and%20importantly%2C%20further%0Aconsider%20uncertainty%20and%20robustness%20to%20ensure%20a%20reliable%20usage%20of%20proposed%0Amodels.%20For%20these%20reasons%2C%20we%20introduce%20THUNDER%2C%20a%20tile-level%20benchmark%20for%0Adigital%20pathology%20foundation%20models%2C%20allowing%20for%20efficient%20comparison%20of%20many%0Amodels%20on%20diverse%20datasets%20with%20a%20series%20of%20downstream%20tasks%2C%20studying%20their%0Afeature%20spaces%20and%20assessing%20the%20robustness%20and%20uncertainty%20of%20predictions%0Ainformed%20by%20their%20embeddings.%20THUNDER%20is%20a%20fast%2C%20easy-to-use%2C%20dynamic%20benchmark%0Athat%20can%20already%20support%20a%20large%20variety%20of%20state-of-the-art%20foundation%2C%20as%0Awell%20as%20local%20user-defined%20models%20for%20direct%20tile-based%20comparison.%20In%20this%0Apaper%2C%20we%20provide%20a%20comprehensive%20comparison%20of%2023%20foundation%20models%20on%2016%0Adifferent%20datasets%20covering%20diverse%20tasks%2C%20feature%20analysis%2C%20and%20robustness.%0AThe%20code%20for%20THUNDER%20is%20publicly%20available%20at%0Ahttps%3A//github.com/MICS-Lab/thunder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTHUNDER%253A%2520Tile-level%2520Histopathology%2520image%2520UNDERstanding%2520benchmark%26entry.906535625%3DPierre%2520Marza%2520and%2520Leo%2520Fillioux%2520and%2520Sofi%25C3%25A8ne%2520Boutaj%2520and%2520Kunal%2520Mahatha%2520and%2520Christian%2520Desrosiers%2520and%2520Pablo%2520Piantanida%2520and%2520Jose%2520Dolz%2520and%2520Stergios%2520Christodoulidis%2520and%2520Maria%2520Vakalopoulou%26entry.1292438233%3D%2520%2520Progress%2520in%2520a%2520research%2520field%2520can%2520be%2520hard%2520to%2520assess%252C%2520in%2520particular%2520when%2520many%250Aconcurrent%2520methods%2520are%2520proposed%2520in%2520a%2520short%2520period%2520of%2520time.%2520This%2520is%2520the%2520case%2520in%250Adigital%2520pathology%252C%2520where%2520many%2520foundation%2520models%2520have%2520been%2520released%2520recently%2520to%250Aserve%2520as%2520feature%2520extractors%2520for%2520tile-level%2520images%252C%2520being%2520used%2520in%2520a%2520variety%2520of%250Adownstream%2520tasks%252C%2520both%2520for%2520tile-%2520and%2520slide-level%2520problems.%2520Benchmarking%250Aavailable%2520methods%2520then%2520becomes%2520paramount%2520to%2520get%2520a%2520clearer%2520view%2520of%2520the%2520research%250Alandscape.%2520In%2520particular%252C%2520in%2520critical%2520domains%2520such%2520as%2520healthcare%252C%2520a%2520benchmark%250Ashould%2520not%2520only%2520focus%2520on%2520evaluating%2520downstream%2520performance%252C%2520but%2520also%2520provide%250Ainsights%2520about%2520the%2520main%2520differences%2520between%2520methods%252C%2520and%2520importantly%252C%2520further%250Aconsider%2520uncertainty%2520and%2520robustness%2520to%2520ensure%2520a%2520reliable%2520usage%2520of%2520proposed%250Amodels.%2520For%2520these%2520reasons%252C%2520we%2520introduce%2520THUNDER%252C%2520a%2520tile-level%2520benchmark%2520for%250Adigital%2520pathology%2520foundation%2520models%252C%2520allowing%2520for%2520efficient%2520comparison%2520of%2520many%250Amodels%2520on%2520diverse%2520datasets%2520with%2520a%2520series%2520of%2520downstream%2520tasks%252C%2520studying%2520their%250Afeature%2520spaces%2520and%2520assessing%2520the%2520robustness%2520and%2520uncertainty%2520of%2520predictions%250Ainformed%2520by%2520their%2520embeddings.%2520THUNDER%2520is%2520a%2520fast%252C%2520easy-to-use%252C%2520dynamic%2520benchmark%250Athat%2520can%2520already%2520support%2520a%2520large%2520variety%2520of%2520state-of-the-art%2520foundation%252C%2520as%250Awell%2520as%2520local%2520user-defined%2520models%2520for%2520direct%2520tile-based%2520comparison.%2520In%2520this%250Apaper%252C%2520we%2520provide%2520a%2520comprehensive%2520comparison%2520of%252023%2520foundation%2520models%2520on%252016%250Adifferent%2520datasets%2520covering%2520diverse%2520tasks%252C%2520feature%2520analysis%252C%2520and%2520robustness.%250AThe%2520code%2520for%2520THUNDER%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/MICS-Lab/thunder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=THUNDER%3A%20Tile-level%20Histopathology%20image%20UNDERstanding%20benchmark&entry.906535625=Pierre%20Marza%20and%20Leo%20Fillioux%20and%20Sofi%C3%A8ne%20Boutaj%20and%20Kunal%20Mahatha%20and%20Christian%20Desrosiers%20and%20Pablo%20Piantanida%20and%20Jose%20Dolz%20and%20Stergios%20Christodoulidis%20and%20Maria%20Vakalopoulou&entry.1292438233=%20%20Progress%20in%20a%20research%20field%20can%20be%20hard%20to%20assess%2C%20in%20particular%20when%20many%0Aconcurrent%20methods%20are%20proposed%20in%20a%20short%20period%20of%20time.%20This%20is%20the%20case%20in%0Adigital%20pathology%2C%20where%20many%20foundation%20models%20have%20been%20released%20recently%20to%0Aserve%20as%20feature%20extractors%20for%20tile-level%20images%2C%20being%20used%20in%20a%20variety%20of%0Adownstream%20tasks%2C%20both%20for%20tile-%20and%20slide-level%20problems.%20Benchmarking%0Aavailable%20methods%20then%20becomes%20paramount%20to%20get%20a%20clearer%20view%20of%20the%20research%0Alandscape.%20In%20particular%2C%20in%20critical%20domains%20such%20as%20healthcare%2C%20a%20benchmark%0Ashould%20not%20only%20focus%20on%20evaluating%20downstream%20performance%2C%20but%20also%20provide%0Ainsights%20about%20the%20main%20differences%20between%20methods%2C%20and%20importantly%2C%20further%0Aconsider%20uncertainty%20and%20robustness%20to%20ensure%20a%20reliable%20usage%20of%20proposed%0Amodels.%20For%20these%20reasons%2C%20we%20introduce%20THUNDER%2C%20a%20tile-level%20benchmark%20for%0Adigital%20pathology%20foundation%20models%2C%20allowing%20for%20efficient%20comparison%20of%20many%0Amodels%20on%20diverse%20datasets%20with%20a%20series%20of%20downstream%20tasks%2C%20studying%20their%0Afeature%20spaces%20and%20assessing%20the%20robustness%20and%20uncertainty%20of%20predictions%0Ainformed%20by%20their%20embeddings.%20THUNDER%20is%20a%20fast%2C%20easy-to-use%2C%20dynamic%20benchmark%0Athat%20can%20already%20support%20a%20large%20variety%20of%20state-of-the-art%20foundation%2C%20as%0Awell%20as%20local%20user-defined%20models%20for%20direct%20tile-based%20comparison.%20In%20this%0Apaper%2C%20we%20provide%20a%20comprehensive%20comparison%20of%2023%20foundation%20models%20on%2016%0Adifferent%20datasets%20covering%20diverse%20tasks%2C%20feature%20analysis%2C%20and%20robustness.%0AThe%20code%20for%20THUNDER%20is%20publicly%20available%20at%0Ahttps%3A//github.com/MICS-Lab/thunder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07860v1&entry.124074799=Read"},
{"title": "OPC: One-Point-Contraction Unlearning Toward Deep Feature Forgetting", "author": "Jaeheun Jung and Bosung Jung and Suhyun Bae and Donghun Lee", "abstract": "  Machine unlearning seeks to remove the influence of particular data or class\nfrom trained models to meet privacy, legal, or ethical requirements. Existing\nunlearning methods tend to forget shallowly: phenomenon of an unlearned model\npretend to forget by adjusting only the model response, while its internal\nrepresentations retain information sufficiently to restore the forgotten data\nor behavior. We empirically confirm the widespread shallowness by reverting the\nforgetting effect of various unlearning methods via training-free performance\nrecovery attack and gradient-inversion-based data reconstruction attack. To\naddress this vulnerability fundamentally, we define a theoretical criterion of\n``deep forgetting'' based on one-point-contraction of feature representations\nof data to forget. We also propose an efficient approximation algorithm, and\nuse it to construct a novel general-purpose unlearning algorithm:\nOne-Point-Contraction (OPC). Empirical evaluations on image classification\nunlearning benchmarks show that OPC achieves not only effective unlearning\nperformance but also superior resilience against both performance recovery\nattack and gradient-inversion attack. The distinctive unlearning performance of\nOPC arises from the deep feature forgetting enforced by its theoretical\nfoundation, and recaps the need for improved robustness of machine unlearning\nmethods.\n", "link": "http://arxiv.org/abs/2507.07754v1", "date": "2025-07-10", "relevancy": 2.5012, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.52}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5039}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OPC%3A%20One-Point-Contraction%20Unlearning%20Toward%20Deep%20Feature%20Forgetting&body=Title%3A%20OPC%3A%20One-Point-Contraction%20Unlearning%20Toward%20Deep%20Feature%20Forgetting%0AAuthor%3A%20Jaeheun%20Jung%20and%20Bosung%20Jung%20and%20Suhyun%20Bae%20and%20Donghun%20Lee%0AAbstract%3A%20%20%20Machine%20unlearning%20seeks%20to%20remove%20the%20influence%20of%20particular%20data%20or%20class%0Afrom%20trained%20models%20to%20meet%20privacy%2C%20legal%2C%20or%20ethical%20requirements.%20Existing%0Aunlearning%20methods%20tend%20to%20forget%20shallowly%3A%20phenomenon%20of%20an%20unlearned%20model%0Apretend%20to%20forget%20by%20adjusting%20only%20the%20model%20response%2C%20while%20its%20internal%0Arepresentations%20retain%20information%20sufficiently%20to%20restore%20the%20forgotten%20data%0Aor%20behavior.%20We%20empirically%20confirm%20the%20widespread%20shallowness%20by%20reverting%20the%0Aforgetting%20effect%20of%20various%20unlearning%20methods%20via%20training-free%20performance%0Arecovery%20attack%20and%20gradient-inversion-based%20data%20reconstruction%20attack.%20To%0Aaddress%20this%20vulnerability%20fundamentally%2C%20we%20define%20a%20theoretical%20criterion%20of%0A%60%60deep%20forgetting%27%27%20based%20on%20one-point-contraction%20of%20feature%20representations%0Aof%20data%20to%20forget.%20We%20also%20propose%20an%20efficient%20approximation%20algorithm%2C%20and%0Ause%20it%20to%20construct%20a%20novel%20general-purpose%20unlearning%20algorithm%3A%0AOne-Point-Contraction%20%28OPC%29.%20Empirical%20evaluations%20on%20image%20classification%0Aunlearning%20benchmarks%20show%20that%20OPC%20achieves%20not%20only%20effective%20unlearning%0Aperformance%20but%20also%20superior%20resilience%20against%20both%20performance%20recovery%0Aattack%20and%20gradient-inversion%20attack.%20The%20distinctive%20unlearning%20performance%20of%0AOPC%20arises%20from%20the%20deep%20feature%20forgetting%20enforced%20by%20its%20theoretical%0Afoundation%2C%20and%20recaps%20the%20need%20for%20improved%20robustness%20of%20machine%20unlearning%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOPC%253A%2520One-Point-Contraction%2520Unlearning%2520Toward%2520Deep%2520Feature%2520Forgetting%26entry.906535625%3DJaeheun%2520Jung%2520and%2520Bosung%2520Jung%2520and%2520Suhyun%2520Bae%2520and%2520Donghun%2520Lee%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520seeks%2520to%2520remove%2520the%2520influence%2520of%2520particular%2520data%2520or%2520class%250Afrom%2520trained%2520models%2520to%2520meet%2520privacy%252C%2520legal%252C%2520or%2520ethical%2520requirements.%2520Existing%250Aunlearning%2520methods%2520tend%2520to%2520forget%2520shallowly%253A%2520phenomenon%2520of%2520an%2520unlearned%2520model%250Apretend%2520to%2520forget%2520by%2520adjusting%2520only%2520the%2520model%2520response%252C%2520while%2520its%2520internal%250Arepresentations%2520retain%2520information%2520sufficiently%2520to%2520restore%2520the%2520forgotten%2520data%250Aor%2520behavior.%2520We%2520empirically%2520confirm%2520the%2520widespread%2520shallowness%2520by%2520reverting%2520the%250Aforgetting%2520effect%2520of%2520various%2520unlearning%2520methods%2520via%2520training-free%2520performance%250Arecovery%2520attack%2520and%2520gradient-inversion-based%2520data%2520reconstruction%2520attack.%2520To%250Aaddress%2520this%2520vulnerability%2520fundamentally%252C%2520we%2520define%2520a%2520theoretical%2520criterion%2520of%250A%2560%2560deep%2520forgetting%2527%2527%2520based%2520on%2520one-point-contraction%2520of%2520feature%2520representations%250Aof%2520data%2520to%2520forget.%2520We%2520also%2520propose%2520an%2520efficient%2520approximation%2520algorithm%252C%2520and%250Ause%2520it%2520to%2520construct%2520a%2520novel%2520general-purpose%2520unlearning%2520algorithm%253A%250AOne-Point-Contraction%2520%2528OPC%2529.%2520Empirical%2520evaluations%2520on%2520image%2520classification%250Aunlearning%2520benchmarks%2520show%2520that%2520OPC%2520achieves%2520not%2520only%2520effective%2520unlearning%250Aperformance%2520but%2520also%2520superior%2520resilience%2520against%2520both%2520performance%2520recovery%250Aattack%2520and%2520gradient-inversion%2520attack.%2520The%2520distinctive%2520unlearning%2520performance%2520of%250AOPC%2520arises%2520from%2520the%2520deep%2520feature%2520forgetting%2520enforced%2520by%2520its%2520theoretical%250Afoundation%252C%2520and%2520recaps%2520the%2520need%2520for%2520improved%2520robustness%2520of%2520machine%2520unlearning%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPC%3A%20One-Point-Contraction%20Unlearning%20Toward%20Deep%20Feature%20Forgetting&entry.906535625=Jaeheun%20Jung%20and%20Bosung%20Jung%20and%20Suhyun%20Bae%20and%20Donghun%20Lee&entry.1292438233=%20%20Machine%20unlearning%20seeks%20to%20remove%20the%20influence%20of%20particular%20data%20or%20class%0Afrom%20trained%20models%20to%20meet%20privacy%2C%20legal%2C%20or%20ethical%20requirements.%20Existing%0Aunlearning%20methods%20tend%20to%20forget%20shallowly%3A%20phenomenon%20of%20an%20unlearned%20model%0Apretend%20to%20forget%20by%20adjusting%20only%20the%20model%20response%2C%20while%20its%20internal%0Arepresentations%20retain%20information%20sufficiently%20to%20restore%20the%20forgotten%20data%0Aor%20behavior.%20We%20empirically%20confirm%20the%20widespread%20shallowness%20by%20reverting%20the%0Aforgetting%20effect%20of%20various%20unlearning%20methods%20via%20training-free%20performance%0Arecovery%20attack%20and%20gradient-inversion-based%20data%20reconstruction%20attack.%20To%0Aaddress%20this%20vulnerability%20fundamentally%2C%20we%20define%20a%20theoretical%20criterion%20of%0A%60%60deep%20forgetting%27%27%20based%20on%20one-point-contraction%20of%20feature%20representations%0Aof%20data%20to%20forget.%20We%20also%20propose%20an%20efficient%20approximation%20algorithm%2C%20and%0Ause%20it%20to%20construct%20a%20novel%20general-purpose%20unlearning%20algorithm%3A%0AOne-Point-Contraction%20%28OPC%29.%20Empirical%20evaluations%20on%20image%20classification%0Aunlearning%20benchmarks%20show%20that%20OPC%20achieves%20not%20only%20effective%20unlearning%0Aperformance%20but%20also%20superior%20resilience%20against%20both%20performance%20recovery%0Aattack%20and%20gradient-inversion%20attack.%20The%20distinctive%20unlearning%20performance%20of%0AOPC%20arises%20from%20the%20deep%20feature%20forgetting%20enforced%20by%20its%20theoretical%0Afoundation%2C%20and%20recaps%20the%20need%20for%20improved%20robustness%20of%20machine%20unlearning%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07754v1&entry.124074799=Read"},
{"title": "What do self-supervised speech models know about Dutch? Analyzing\n  advantages of language-specific pre-training", "author": "Marianne de Heer Kloots and Hosein Mohebbi and Charlotte Pouw and Gaofei Shen and Willem Zuidema and Martijn Bentum", "abstract": "  How language-specific are speech representations learned by self-supervised\nmodels? Existing work has shown that a range of linguistic features can be\nsuccessfully decoded from end-to-end models trained only on speech recordings.\nHowever, it's less clear to what extent pre-training on specific languages\nimproves language-specific linguistic information. Here we test the encoding of\nDutch phonetic and lexical information in internal representations of\nself-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the\nrepresentation of Dutch linguistic features as compared to pre-training on\nsimilar amounts of English or larger amounts of multilingual data. This\nlanguage-specific advantage is well-detected by trained clustering or\nclassification probes, and partially observable using zero-shot metrics.\nFurthermore, the language-specific benefit on linguistic feature encoding\naligns with downstream performance on Automatic Speech Recognition.\n", "link": "http://arxiv.org/abs/2506.00981v2", "date": "2025-07-10", "relevancy": 2.5008, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20do%20self-supervised%20speech%20models%20know%20about%20Dutch%3F%20Analyzing%0A%20%20advantages%20of%20language-specific%20pre-training&body=Title%3A%20What%20do%20self-supervised%20speech%20models%20know%20about%20Dutch%3F%20Analyzing%0A%20%20advantages%20of%20language-specific%20pre-training%0AAuthor%3A%20Marianne%20de%20Heer%20Kloots%20and%20Hosein%20Mohebbi%20and%20Charlotte%20Pouw%20and%20Gaofei%20Shen%20and%20Willem%20Zuidema%20and%20Martijn%20Bentum%0AAbstract%3A%20%20%20How%20language-specific%20are%20speech%20representations%20learned%20by%20self-supervised%0Amodels%3F%20Existing%20work%20has%20shown%20that%20a%20range%20of%20linguistic%20features%20can%20be%0Asuccessfully%20decoded%20from%20end-to-end%20models%20trained%20only%20on%20speech%20recordings.%0AHowever%2C%20it%27s%20less%20clear%20to%20what%20extent%20pre-training%20on%20specific%20languages%0Aimproves%20language-specific%20linguistic%20information.%20Here%20we%20test%20the%20encoding%20of%0ADutch%20phonetic%20and%20lexical%20information%20in%20internal%20representations%20of%0Aself-supervised%20Wav2Vec2%20models.%20Pre-training%20exclusively%20on%20Dutch%20improves%20the%0Arepresentation%20of%20Dutch%20linguistic%20features%20as%20compared%20to%20pre-training%20on%0Asimilar%20amounts%20of%20English%20or%20larger%20amounts%20of%20multilingual%20data.%20This%0Alanguage-specific%20advantage%20is%20well-detected%20by%20trained%20clustering%20or%0Aclassification%20probes%2C%20and%20partially%20observable%20using%20zero-shot%20metrics.%0AFurthermore%2C%20the%20language-specific%20benefit%20on%20linguistic%20feature%20encoding%0Aaligns%20with%20downstream%20performance%20on%20Automatic%20Speech%20Recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00981v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520do%2520self-supervised%2520speech%2520models%2520know%2520about%2520Dutch%253F%2520Analyzing%250A%2520%2520advantages%2520of%2520language-specific%2520pre-training%26entry.906535625%3DMarianne%2520de%2520Heer%2520Kloots%2520and%2520Hosein%2520Mohebbi%2520and%2520Charlotte%2520Pouw%2520and%2520Gaofei%2520Shen%2520and%2520Willem%2520Zuidema%2520and%2520Martijn%2520Bentum%26entry.1292438233%3D%2520%2520How%2520language-specific%2520are%2520speech%2520representations%2520learned%2520by%2520self-supervised%250Amodels%253F%2520Existing%2520work%2520has%2520shown%2520that%2520a%2520range%2520of%2520linguistic%2520features%2520can%2520be%250Asuccessfully%2520decoded%2520from%2520end-to-end%2520models%2520trained%2520only%2520on%2520speech%2520recordings.%250AHowever%252C%2520it%2527s%2520less%2520clear%2520to%2520what%2520extent%2520pre-training%2520on%2520specific%2520languages%250Aimproves%2520language-specific%2520linguistic%2520information.%2520Here%2520we%2520test%2520the%2520encoding%2520of%250ADutch%2520phonetic%2520and%2520lexical%2520information%2520in%2520internal%2520representations%2520of%250Aself-supervised%2520Wav2Vec2%2520models.%2520Pre-training%2520exclusively%2520on%2520Dutch%2520improves%2520the%250Arepresentation%2520of%2520Dutch%2520linguistic%2520features%2520as%2520compared%2520to%2520pre-training%2520on%250Asimilar%2520amounts%2520of%2520English%2520or%2520larger%2520amounts%2520of%2520multilingual%2520data.%2520This%250Alanguage-specific%2520advantage%2520is%2520well-detected%2520by%2520trained%2520clustering%2520or%250Aclassification%2520probes%252C%2520and%2520partially%2520observable%2520using%2520zero-shot%2520metrics.%250AFurthermore%252C%2520the%2520language-specific%2520benefit%2520on%2520linguistic%2520feature%2520encoding%250Aaligns%2520with%2520downstream%2520performance%2520on%2520Automatic%2520Speech%2520Recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00981v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20do%20self-supervised%20speech%20models%20know%20about%20Dutch%3F%20Analyzing%0A%20%20advantages%20of%20language-specific%20pre-training&entry.906535625=Marianne%20de%20Heer%20Kloots%20and%20Hosein%20Mohebbi%20and%20Charlotte%20Pouw%20and%20Gaofei%20Shen%20and%20Willem%20Zuidema%20and%20Martijn%20Bentum&entry.1292438233=%20%20How%20language-specific%20are%20speech%20representations%20learned%20by%20self-supervised%0Amodels%3F%20Existing%20work%20has%20shown%20that%20a%20range%20of%20linguistic%20features%20can%20be%0Asuccessfully%20decoded%20from%20end-to-end%20models%20trained%20only%20on%20speech%20recordings.%0AHowever%2C%20it%27s%20less%20clear%20to%20what%20extent%20pre-training%20on%20specific%20languages%0Aimproves%20language-specific%20linguistic%20information.%20Here%20we%20test%20the%20encoding%20of%0ADutch%20phonetic%20and%20lexical%20information%20in%20internal%20representations%20of%0Aself-supervised%20Wav2Vec2%20models.%20Pre-training%20exclusively%20on%20Dutch%20improves%20the%0Arepresentation%20of%20Dutch%20linguistic%20features%20as%20compared%20to%20pre-training%20on%0Asimilar%20amounts%20of%20English%20or%20larger%20amounts%20of%20multilingual%20data.%20This%0Alanguage-specific%20advantage%20is%20well-detected%20by%20trained%20clustering%20or%0Aclassification%20probes%2C%20and%20partially%20observable%20using%20zero-shot%20metrics.%0AFurthermore%2C%20the%20language-specific%20benefit%20on%20linguistic%20feature%20encoding%0Aaligns%20with%20downstream%20performance%20on%20Automatic%20Speech%20Recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00981v2&entry.124074799=Read"},
{"title": "Not All Preferences are What You Need for Post-Training: Selective\n  Alignment Strategy for Preference Optimization", "author": "Zhijin Dong", "abstract": "  Post-training alignment of large language models (LLMs) is a critical\nchallenge, as not all tokens contribute equally to model performance. This\npaper introduces a selective alignment strategy that prioritizes high-impact\ntokens within preference pairs, leveraging token-level log-probability\ndifferences between the current policy and a reference model. By focusing on\nthese informative tokens, our approach reduces computational overhead and\nenhances alignment fidelity. We further explore the role of reference model\nquality, demonstrating that stronger reference models significantly improve\ntoken selection accuracy and overall optimization effectiveness. Comprehensive\nexperiments on benchmarks such as Arena-Hard and MT-Bench validate the\nsuperiority of our Selective-DPO method over standard DPO and\ndistillation-based baselines. Our findings highlight the importance of\ntoken-level optimization and reference model selection in advancing preference\nalignment for LLMs. The code is available at\nhttps://github.com/Dongzhijin/SDPO.\n", "link": "http://arxiv.org/abs/2507.07725v1", "date": "2025-07-10", "relevancy": 2.4751, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5148}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Preferences%20are%20What%20You%20Need%20for%20Post-Training%3A%20Selective%0A%20%20Alignment%20Strategy%20for%20Preference%20Optimization&body=Title%3A%20Not%20All%20Preferences%20are%20What%20You%20Need%20for%20Post-Training%3A%20Selective%0A%20%20Alignment%20Strategy%20for%20Preference%20Optimization%0AAuthor%3A%20Zhijin%20Dong%0AAbstract%3A%20%20%20Post-training%20alignment%20of%20large%20language%20models%20%28LLMs%29%20is%20a%20critical%0Achallenge%2C%20as%20not%20all%20tokens%20contribute%20equally%20to%20model%20performance.%20This%0Apaper%20introduces%20a%20selective%20alignment%20strategy%20that%20prioritizes%20high-impact%0Atokens%20within%20preference%20pairs%2C%20leveraging%20token-level%20log-probability%0Adifferences%20between%20the%20current%20policy%20and%20a%20reference%20model.%20By%20focusing%20on%0Athese%20informative%20tokens%2C%20our%20approach%20reduces%20computational%20overhead%20and%0Aenhances%20alignment%20fidelity.%20We%20further%20explore%20the%20role%20of%20reference%20model%0Aquality%2C%20demonstrating%20that%20stronger%20reference%20models%20significantly%20improve%0Atoken%20selection%20accuracy%20and%20overall%20optimization%20effectiveness.%20Comprehensive%0Aexperiments%20on%20benchmarks%20such%20as%20Arena-Hard%20and%20MT-Bench%20validate%20the%0Asuperiority%20of%20our%20Selective-DPO%20method%20over%20standard%20DPO%20and%0Adistillation-based%20baselines.%20Our%20findings%20highlight%20the%20importance%20of%0Atoken-level%20optimization%20and%20reference%20model%20selection%20in%20advancing%20preference%0Aalignment%20for%20LLMs.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Dongzhijin/SDPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Preferences%2520are%2520What%2520You%2520Need%2520for%2520Post-Training%253A%2520Selective%250A%2520%2520Alignment%2520Strategy%2520for%2520Preference%2520Optimization%26entry.906535625%3DZhijin%2520Dong%26entry.1292438233%3D%2520%2520Post-training%2520alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520a%2520critical%250Achallenge%252C%2520as%2520not%2520all%2520tokens%2520contribute%2520equally%2520to%2520model%2520performance.%2520This%250Apaper%2520introduces%2520a%2520selective%2520alignment%2520strategy%2520that%2520prioritizes%2520high-impact%250Atokens%2520within%2520preference%2520pairs%252C%2520leveraging%2520token-level%2520log-probability%250Adifferences%2520between%2520the%2520current%2520policy%2520and%2520a%2520reference%2520model.%2520By%2520focusing%2520on%250Athese%2520informative%2520tokens%252C%2520our%2520approach%2520reduces%2520computational%2520overhead%2520and%250Aenhances%2520alignment%2520fidelity.%2520We%2520further%2520explore%2520the%2520role%2520of%2520reference%2520model%250Aquality%252C%2520demonstrating%2520that%2520stronger%2520reference%2520models%2520significantly%2520improve%250Atoken%2520selection%2520accuracy%2520and%2520overall%2520optimization%2520effectiveness.%2520Comprehensive%250Aexperiments%2520on%2520benchmarks%2520such%2520as%2520Arena-Hard%2520and%2520MT-Bench%2520validate%2520the%250Asuperiority%2520of%2520our%2520Selective-DPO%2520method%2520over%2520standard%2520DPO%2520and%250Adistillation-based%2520baselines.%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%250Atoken-level%2520optimization%2520and%2520reference%2520model%2520selection%2520in%2520advancing%2520preference%250Aalignment%2520for%2520LLMs.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Dongzhijin/SDPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Preferences%20are%20What%20You%20Need%20for%20Post-Training%3A%20Selective%0A%20%20Alignment%20Strategy%20for%20Preference%20Optimization&entry.906535625=Zhijin%20Dong&entry.1292438233=%20%20Post-training%20alignment%20of%20large%20language%20models%20%28LLMs%29%20is%20a%20critical%0Achallenge%2C%20as%20not%20all%20tokens%20contribute%20equally%20to%20model%20performance.%20This%0Apaper%20introduces%20a%20selective%20alignment%20strategy%20that%20prioritizes%20high-impact%0Atokens%20within%20preference%20pairs%2C%20leveraging%20token-level%20log-probability%0Adifferences%20between%20the%20current%20policy%20and%20a%20reference%20model.%20By%20focusing%20on%0Athese%20informative%20tokens%2C%20our%20approach%20reduces%20computational%20overhead%20and%0Aenhances%20alignment%20fidelity.%20We%20further%20explore%20the%20role%20of%20reference%20model%0Aquality%2C%20demonstrating%20that%20stronger%20reference%20models%20significantly%20improve%0Atoken%20selection%20accuracy%20and%20overall%20optimization%20effectiveness.%20Comprehensive%0Aexperiments%20on%20benchmarks%20such%20as%20Arena-Hard%20and%20MT-Bench%20validate%20the%0Asuperiority%20of%20our%20Selective-DPO%20method%20over%20standard%20DPO%20and%0Adistillation-based%20baselines.%20Our%20findings%20highlight%20the%20importance%20of%0Atoken-level%20optimization%20and%20reference%20model%20selection%20in%20advancing%20preference%0Aalignment%20for%20LLMs.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Dongzhijin/SDPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07725v1&entry.124074799=Read"},
{"title": "Optimization Guarantees for Square-Root Natural-Gradient Variational\n  Inference", "author": "Navish Kumar and Thomas M\u00f6llenhoff and Mohammad Emtiyaz Khan and Aurelien Lucchi", "abstract": "  Variational inference with natural-gradient descent often shows fast\nconvergence in practice, but its theoretical convergence guarantees have been\nchallenging to establish. This is true even for the simplest cases that involve\nconcave log-likelihoods and use a Gaussian approximation. We show that the\nchallenge can be circumvented for such cases using a square-root\nparameterization for the Gaussian covariance. This approach establishes novel\nconvergence guarantees for natural-gradient variational-Gaussian inference and\nits continuous-time gradient flow. Our experiments demonstrate the\neffectiveness of natural gradient methods and highlight their advantages over\nalgorithms that use Euclidean or Wasserstein geometries.\n", "link": "http://arxiv.org/abs/2507.07853v1", "date": "2025-07-10", "relevancy": 2.4698, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4989}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4944}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimization%20Guarantees%20for%20Square-Root%20Natural-Gradient%20Variational%0A%20%20Inference&body=Title%3A%20Optimization%20Guarantees%20for%20Square-Root%20Natural-Gradient%20Variational%0A%20%20Inference%0AAuthor%3A%20Navish%20Kumar%20and%20Thomas%20M%C3%B6llenhoff%20and%20Mohammad%20Emtiyaz%20Khan%20and%20Aurelien%20Lucchi%0AAbstract%3A%20%20%20Variational%20inference%20with%20natural-gradient%20descent%20often%20shows%20fast%0Aconvergence%20in%20practice%2C%20but%20its%20theoretical%20convergence%20guarantees%20have%20been%0Achallenging%20to%20establish.%20This%20is%20true%20even%20for%20the%20simplest%20cases%20that%20involve%0Aconcave%20log-likelihoods%20and%20use%20a%20Gaussian%20approximation.%20We%20show%20that%20the%0Achallenge%20can%20be%20circumvented%20for%20such%20cases%20using%20a%20square-root%0Aparameterization%20for%20the%20Gaussian%20covariance.%20This%20approach%20establishes%20novel%0Aconvergence%20guarantees%20for%20natural-gradient%20variational-Gaussian%20inference%20and%0Aits%20continuous-time%20gradient%20flow.%20Our%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20natural%20gradient%20methods%20and%20highlight%20their%20advantages%20over%0Aalgorithms%20that%20use%20Euclidean%20or%20Wasserstein%20geometries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimization%2520Guarantees%2520for%2520Square-Root%2520Natural-Gradient%2520Variational%250A%2520%2520Inference%26entry.906535625%3DNavish%2520Kumar%2520and%2520Thomas%2520M%25C3%25B6llenhoff%2520and%2520Mohammad%2520Emtiyaz%2520Khan%2520and%2520Aurelien%2520Lucchi%26entry.1292438233%3D%2520%2520Variational%2520inference%2520with%2520natural-gradient%2520descent%2520often%2520shows%2520fast%250Aconvergence%2520in%2520practice%252C%2520but%2520its%2520theoretical%2520convergence%2520guarantees%2520have%2520been%250Achallenging%2520to%2520establish.%2520This%2520is%2520true%2520even%2520for%2520the%2520simplest%2520cases%2520that%2520involve%250Aconcave%2520log-likelihoods%2520and%2520use%2520a%2520Gaussian%2520approximation.%2520We%2520show%2520that%2520the%250Achallenge%2520can%2520be%2520circumvented%2520for%2520such%2520cases%2520using%2520a%2520square-root%250Aparameterization%2520for%2520the%2520Gaussian%2520covariance.%2520This%2520approach%2520establishes%2520novel%250Aconvergence%2520guarantees%2520for%2520natural-gradient%2520variational-Gaussian%2520inference%2520and%250Aits%2520continuous-time%2520gradient%2520flow.%2520Our%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520natural%2520gradient%2520methods%2520and%2520highlight%2520their%2520advantages%2520over%250Aalgorithms%2520that%2520use%2520Euclidean%2520or%2520Wasserstein%2520geometries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimization%20Guarantees%20for%20Square-Root%20Natural-Gradient%20Variational%0A%20%20Inference&entry.906535625=Navish%20Kumar%20and%20Thomas%20M%C3%B6llenhoff%20and%20Mohammad%20Emtiyaz%20Khan%20and%20Aurelien%20Lucchi&entry.1292438233=%20%20Variational%20inference%20with%20natural-gradient%20descent%20often%20shows%20fast%0Aconvergence%20in%20practice%2C%20but%20its%20theoretical%20convergence%20guarantees%20have%20been%0Achallenging%20to%20establish.%20This%20is%20true%20even%20for%20the%20simplest%20cases%20that%20involve%0Aconcave%20log-likelihoods%20and%20use%20a%20Gaussian%20approximation.%20We%20show%20that%20the%0Achallenge%20can%20be%20circumvented%20for%20such%20cases%20using%20a%20square-root%0Aparameterization%20for%20the%20Gaussian%20covariance.%20This%20approach%20establishes%20novel%0Aconvergence%20guarantees%20for%20natural-gradient%20variational-Gaussian%20inference%20and%0Aits%20continuous-time%20gradient%20flow.%20Our%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20natural%20gradient%20methods%20and%20highlight%20their%20advantages%20over%0Aalgorithms%20that%20use%20Euclidean%20or%20Wasserstein%20geometries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07853v1&entry.124074799=Read"},
{"title": "A Bilevel Optimization Framework for Imbalanced Data Classification", "author": "Karen Medlin and Sven Leyffer and Krishnan Raghavan", "abstract": "  Data rebalancing techniques, including oversampling and undersampling, are a\ncommon approach to addressing the challenges of imbalanced data. To tackle\nunresolved problems related to both oversampling and undersampling, we propose\na new undersampling approach that: (i) avoids the pitfalls of noise and overlap\ncaused by synthetic data and (ii) avoids the pitfall of under-fitting caused by\nrandom undersampling. Instead of undersampling majority data randomly, our\nmethod undersamples datapoints based on their ability to improve model loss.\nUsing improved model loss as a proxy measurement for classification\nperformance, our technique assesses a datapoint's impact on loss and rejects\nthose unable to improve it. In so doing, our approach rejects majority\ndatapoints redundant to datapoints already accepted and, thereby, finds an\noptimal subset of majority training data for classification. The accept/reject\ncomponent of our algorithm is motivated by a bilevel optimization problem\nuniquely formulated to identify the optimal training set we seek. Experimental\nresults show our proposed technique with F1 scores up to 10% higher than\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2410.11171v3", "date": "2025-07-10", "relevancy": 2.465, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5453}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4671}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bilevel%20Optimization%20Framework%20for%20Imbalanced%20Data%20Classification&body=Title%3A%20A%20Bilevel%20Optimization%20Framework%20for%20Imbalanced%20Data%20Classification%0AAuthor%3A%20Karen%20Medlin%20and%20Sven%20Leyffer%20and%20Krishnan%20Raghavan%0AAbstract%3A%20%20%20Data%20rebalancing%20techniques%2C%20including%20oversampling%20and%20undersampling%2C%20are%20a%0Acommon%20approach%20to%20addressing%20the%20challenges%20of%20imbalanced%20data.%20To%20tackle%0Aunresolved%20problems%20related%20to%20both%20oversampling%20and%20undersampling%2C%20we%20propose%0Aa%20new%20undersampling%20approach%20that%3A%20%28i%29%20avoids%20the%20pitfalls%20of%20noise%20and%20overlap%0Acaused%20by%20synthetic%20data%20and%20%28ii%29%20avoids%20the%20pitfall%20of%20under-fitting%20caused%20by%0Arandom%20undersampling.%20Instead%20of%20undersampling%20majority%20data%20randomly%2C%20our%0Amethod%20undersamples%20datapoints%20based%20on%20their%20ability%20to%20improve%20model%20loss.%0AUsing%20improved%20model%20loss%20as%20a%20proxy%20measurement%20for%20classification%0Aperformance%2C%20our%20technique%20assesses%20a%20datapoint%27s%20impact%20on%20loss%20and%20rejects%0Athose%20unable%20to%20improve%20it.%20In%20so%20doing%2C%20our%20approach%20rejects%20majority%0Adatapoints%20redundant%20to%20datapoints%20already%20accepted%20and%2C%20thereby%2C%20finds%20an%0Aoptimal%20subset%20of%20majority%20training%20data%20for%20classification.%20The%20accept/reject%0Acomponent%20of%20our%20algorithm%20is%20motivated%20by%20a%20bilevel%20optimization%20problem%0Auniquely%20formulated%20to%20identify%20the%20optimal%20training%20set%20we%20seek.%20Experimental%0Aresults%20show%20our%20proposed%20technique%20with%20F1%20scores%20up%20to%2010%25%20higher%20than%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11171v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bilevel%2520Optimization%2520Framework%2520for%2520Imbalanced%2520Data%2520Classification%26entry.906535625%3DKaren%2520Medlin%2520and%2520Sven%2520Leyffer%2520and%2520Krishnan%2520Raghavan%26entry.1292438233%3D%2520%2520Data%2520rebalancing%2520techniques%252C%2520including%2520oversampling%2520and%2520undersampling%252C%2520are%2520a%250Acommon%2520approach%2520to%2520addressing%2520the%2520challenges%2520of%2520imbalanced%2520data.%2520To%2520tackle%250Aunresolved%2520problems%2520related%2520to%2520both%2520oversampling%2520and%2520undersampling%252C%2520we%2520propose%250Aa%2520new%2520undersampling%2520approach%2520that%253A%2520%2528i%2529%2520avoids%2520the%2520pitfalls%2520of%2520noise%2520and%2520overlap%250Acaused%2520by%2520synthetic%2520data%2520and%2520%2528ii%2529%2520avoids%2520the%2520pitfall%2520of%2520under-fitting%2520caused%2520by%250Arandom%2520undersampling.%2520Instead%2520of%2520undersampling%2520majority%2520data%2520randomly%252C%2520our%250Amethod%2520undersamples%2520datapoints%2520based%2520on%2520their%2520ability%2520to%2520improve%2520model%2520loss.%250AUsing%2520improved%2520model%2520loss%2520as%2520a%2520proxy%2520measurement%2520for%2520classification%250Aperformance%252C%2520our%2520technique%2520assesses%2520a%2520datapoint%2527s%2520impact%2520on%2520loss%2520and%2520rejects%250Athose%2520unable%2520to%2520improve%2520it.%2520In%2520so%2520doing%252C%2520our%2520approach%2520rejects%2520majority%250Adatapoints%2520redundant%2520to%2520datapoints%2520already%2520accepted%2520and%252C%2520thereby%252C%2520finds%2520an%250Aoptimal%2520subset%2520of%2520majority%2520training%2520data%2520for%2520classification.%2520The%2520accept/reject%250Acomponent%2520of%2520our%2520algorithm%2520is%2520motivated%2520by%2520a%2520bilevel%2520optimization%2520problem%250Auniquely%2520formulated%2520to%2520identify%2520the%2520optimal%2520training%2520set%2520we%2520seek.%2520Experimental%250Aresults%2520show%2520our%2520proposed%2520technique%2520with%2520F1%2520scores%2520up%2520to%252010%2525%2520higher%2520than%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11171v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bilevel%20Optimization%20Framework%20for%20Imbalanced%20Data%20Classification&entry.906535625=Karen%20Medlin%20and%20Sven%20Leyffer%20and%20Krishnan%20Raghavan&entry.1292438233=%20%20Data%20rebalancing%20techniques%2C%20including%20oversampling%20and%20undersampling%2C%20are%20a%0Acommon%20approach%20to%20addressing%20the%20challenges%20of%20imbalanced%20data.%20To%20tackle%0Aunresolved%20problems%20related%20to%20both%20oversampling%20and%20undersampling%2C%20we%20propose%0Aa%20new%20undersampling%20approach%20that%3A%20%28i%29%20avoids%20the%20pitfalls%20of%20noise%20and%20overlap%0Acaused%20by%20synthetic%20data%20and%20%28ii%29%20avoids%20the%20pitfall%20of%20under-fitting%20caused%20by%0Arandom%20undersampling.%20Instead%20of%20undersampling%20majority%20data%20randomly%2C%20our%0Amethod%20undersamples%20datapoints%20based%20on%20their%20ability%20to%20improve%20model%20loss.%0AUsing%20improved%20model%20loss%20as%20a%20proxy%20measurement%20for%20classification%0Aperformance%2C%20our%20technique%20assesses%20a%20datapoint%27s%20impact%20on%20loss%20and%20rejects%0Athose%20unable%20to%20improve%20it.%20In%20so%20doing%2C%20our%20approach%20rejects%20majority%0Adatapoints%20redundant%20to%20datapoints%20already%20accepted%20and%2C%20thereby%2C%20finds%20an%0Aoptimal%20subset%20of%20majority%20training%20data%20for%20classification.%20The%20accept/reject%0Acomponent%20of%20our%20algorithm%20is%20motivated%20by%20a%20bilevel%20optimization%20problem%0Auniquely%20formulated%20to%20identify%20the%20optimal%20training%20set%20we%20seek.%20Experimental%0Aresults%20show%20our%20proposed%20technique%20with%20F1%20scores%20up%20to%2010%25%20higher%20than%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11171v3&entry.124074799=Read"},
{"title": "Multigranular Evaluation for Brain Visual Decoding", "author": "Weihao Xia and Cengiz Oztireli", "abstract": "  Existing evaluation protocols for brain visual decoding predominantly rely on\ncoarse metrics that obscure inter-model differences, lack neuroscientific\nfoundation, and fail to capture fine-grained visual distinctions. To address\nthese limitations, we introduce BASIC, a unified, multigranular evaluation\nframework that jointly quantifies structural fidelity, inferential alignment,\nand contextual coherence between decoded and ground truth images. For the\nstructural level, we introduce a hierarchical suite of segmentation-based\nmetrics, including foreground, semantic, instance, and component masks,\nanchored in granularity-aware correspondence across mask structures. For the\nsemantic level, we extract structured scene representations encompassing\nobjects, attributes, and relationships using multimodal large language models,\nenabling detailed, scalable, and context-rich comparisons with ground-truth\nstimuli. We benchmark a diverse set of visual decoding methods across multiple\nstimulus-neuroimaging datasets within this unified evaluation framework.\nTogether, these criteria provide a more discriminative, interpretable, and\ncomprehensive foundation for measuring brain visual decoding methods.\n", "link": "http://arxiv.org/abs/2507.07993v1", "date": "2025-07-10", "relevancy": 2.4642, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.64}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.64}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multigranular%20Evaluation%20for%20Brain%20Visual%20Decoding&body=Title%3A%20Multigranular%20Evaluation%20for%20Brain%20Visual%20Decoding%0AAuthor%3A%20Weihao%20Xia%20and%20Cengiz%20Oztireli%0AAbstract%3A%20%20%20Existing%20evaluation%20protocols%20for%20brain%20visual%20decoding%20predominantly%20rely%20on%0Acoarse%20metrics%20that%20obscure%20inter-model%20differences%2C%20lack%20neuroscientific%0Afoundation%2C%20and%20fail%20to%20capture%20fine-grained%20visual%20distinctions.%20To%20address%0Athese%20limitations%2C%20we%20introduce%20BASIC%2C%20a%20unified%2C%20multigranular%20evaluation%0Aframework%20that%20jointly%20quantifies%20structural%20fidelity%2C%20inferential%20alignment%2C%0Aand%20contextual%20coherence%20between%20decoded%20and%20ground%20truth%20images.%20For%20the%0Astructural%20level%2C%20we%20introduce%20a%20hierarchical%20suite%20of%20segmentation-based%0Ametrics%2C%20including%20foreground%2C%20semantic%2C%20instance%2C%20and%20component%20masks%2C%0Aanchored%20in%20granularity-aware%20correspondence%20across%20mask%20structures.%20For%20the%0Asemantic%20level%2C%20we%20extract%20structured%20scene%20representations%20encompassing%0Aobjects%2C%20attributes%2C%20and%20relationships%20using%20multimodal%20large%20language%20models%2C%0Aenabling%20detailed%2C%20scalable%2C%20and%20context-rich%20comparisons%20with%20ground-truth%0Astimuli.%20We%20benchmark%20a%20diverse%20set%20of%20visual%20decoding%20methods%20across%20multiple%0Astimulus-neuroimaging%20datasets%20within%20this%20unified%20evaluation%20framework.%0ATogether%2C%20these%20criteria%20provide%20a%20more%20discriminative%2C%20interpretable%2C%20and%0Acomprehensive%20foundation%20for%20measuring%20brain%20visual%20decoding%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultigranular%2520Evaluation%2520for%2520Brain%2520Visual%2520Decoding%26entry.906535625%3DWeihao%2520Xia%2520and%2520Cengiz%2520Oztireli%26entry.1292438233%3D%2520%2520Existing%2520evaluation%2520protocols%2520for%2520brain%2520visual%2520decoding%2520predominantly%2520rely%2520on%250Acoarse%2520metrics%2520that%2520obscure%2520inter-model%2520differences%252C%2520lack%2520neuroscientific%250Afoundation%252C%2520and%2520fail%2520to%2520capture%2520fine-grained%2520visual%2520distinctions.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520introduce%2520BASIC%252C%2520a%2520unified%252C%2520multigranular%2520evaluation%250Aframework%2520that%2520jointly%2520quantifies%2520structural%2520fidelity%252C%2520inferential%2520alignment%252C%250Aand%2520contextual%2520coherence%2520between%2520decoded%2520and%2520ground%2520truth%2520images.%2520For%2520the%250Astructural%2520level%252C%2520we%2520introduce%2520a%2520hierarchical%2520suite%2520of%2520segmentation-based%250Ametrics%252C%2520including%2520foreground%252C%2520semantic%252C%2520instance%252C%2520and%2520component%2520masks%252C%250Aanchored%2520in%2520granularity-aware%2520correspondence%2520across%2520mask%2520structures.%2520For%2520the%250Asemantic%2520level%252C%2520we%2520extract%2520structured%2520scene%2520representations%2520encompassing%250Aobjects%252C%2520attributes%252C%2520and%2520relationships%2520using%2520multimodal%2520large%2520language%2520models%252C%250Aenabling%2520detailed%252C%2520scalable%252C%2520and%2520context-rich%2520comparisons%2520with%2520ground-truth%250Astimuli.%2520We%2520benchmark%2520a%2520diverse%2520set%2520of%2520visual%2520decoding%2520methods%2520across%2520multiple%250Astimulus-neuroimaging%2520datasets%2520within%2520this%2520unified%2520evaluation%2520framework.%250ATogether%252C%2520these%2520criteria%2520provide%2520a%2520more%2520discriminative%252C%2520interpretable%252C%2520and%250Acomprehensive%2520foundation%2520for%2520measuring%2520brain%2520visual%2520decoding%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multigranular%20Evaluation%20for%20Brain%20Visual%20Decoding&entry.906535625=Weihao%20Xia%20and%20Cengiz%20Oztireli&entry.1292438233=%20%20Existing%20evaluation%20protocols%20for%20brain%20visual%20decoding%20predominantly%20rely%20on%0Acoarse%20metrics%20that%20obscure%20inter-model%20differences%2C%20lack%20neuroscientific%0Afoundation%2C%20and%20fail%20to%20capture%20fine-grained%20visual%20distinctions.%20To%20address%0Athese%20limitations%2C%20we%20introduce%20BASIC%2C%20a%20unified%2C%20multigranular%20evaluation%0Aframework%20that%20jointly%20quantifies%20structural%20fidelity%2C%20inferential%20alignment%2C%0Aand%20contextual%20coherence%20between%20decoded%20and%20ground%20truth%20images.%20For%20the%0Astructural%20level%2C%20we%20introduce%20a%20hierarchical%20suite%20of%20segmentation-based%0Ametrics%2C%20including%20foreground%2C%20semantic%2C%20instance%2C%20and%20component%20masks%2C%0Aanchored%20in%20granularity-aware%20correspondence%20across%20mask%20structures.%20For%20the%0Asemantic%20level%2C%20we%20extract%20structured%20scene%20representations%20encompassing%0Aobjects%2C%20attributes%2C%20and%20relationships%20using%20multimodal%20large%20language%20models%2C%0Aenabling%20detailed%2C%20scalable%2C%20and%20context-rich%20comparisons%20with%20ground-truth%0Astimuli.%20We%20benchmark%20a%20diverse%20set%20of%20visual%20decoding%20methods%20across%20multiple%0Astimulus-neuroimaging%20datasets%20within%20this%20unified%20evaluation%20framework.%0ATogether%2C%20these%20criteria%20provide%20a%20more%20discriminative%2C%20interpretable%2C%20and%0Acomprehensive%20foundation%20for%20measuring%20brain%20visual%20decoding%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07993v1&entry.124074799=Read"},
{"title": "Uncertainty-Aware Gradient Stabilization for Small Object Detection", "author": "Huixin Sun and Yanjing Li and Linlin Yang and Xianbin Cao and Baochang Zhang", "abstract": "  Despite advances in generic object detection, there remains a performance gap\nin detecting small objects compared to normal-scale objects. We reveal that\nconventional object localization methods suffer from gradient instability in\nsmall objects due to sharper loss curvature, leading to a convergence\nchallenge. To address the issue, we propose Uncertainty-Aware Gradient\nStabilization (UGS), a framework that reformulates object localization as a\nclassification task to stabilize gradients. UGS quantizes continuous labels\ninto interval non-uniform discrete representations. Under a\nclassification-based objective, the localization branch generates bounded and\nconfidence-driven gradients, mitigating instability. Furthermore, UGS\nintegrates an uncertainty minimization (UM) loss that reduces prediction\nvariance and an uncertainty-guided refinement (UR) module that identifies and\nrefines high-uncertainty regions via perturbations. Evaluated on four\nbenchmarks, UGS consistently improves anchor-based, anchor-free, and leading\nsmall object detectors. Especially, UGS enhances DINO-5scale by 2.6 AP on\nVisDrone, surpassing previous state-of-the-art results.\n", "link": "http://arxiv.org/abs/2303.01803v2", "date": "2025-07-10", "relevancy": 2.4595, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.629}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6082}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Gradient%20Stabilization%20for%20Small%20Object%20Detection&body=Title%3A%20Uncertainty-Aware%20Gradient%20Stabilization%20for%20Small%20Object%20Detection%0AAuthor%3A%20Huixin%20Sun%20and%20Yanjing%20Li%20and%20Linlin%20Yang%20and%20Xianbin%20Cao%20and%20Baochang%20Zhang%0AAbstract%3A%20%20%20Despite%20advances%20in%20generic%20object%20detection%2C%20there%20remains%20a%20performance%20gap%0Ain%20detecting%20small%20objects%20compared%20to%20normal-scale%20objects.%20We%20reveal%20that%0Aconventional%20object%20localization%20methods%20suffer%20from%20gradient%20instability%20in%0Asmall%20objects%20due%20to%20sharper%20loss%20curvature%2C%20leading%20to%20a%20convergence%0Achallenge.%20To%20address%20the%20issue%2C%20we%20propose%20Uncertainty-Aware%20Gradient%0AStabilization%20%28UGS%29%2C%20a%20framework%20that%20reformulates%20object%20localization%20as%20a%0Aclassification%20task%20to%20stabilize%20gradients.%20UGS%20quantizes%20continuous%20labels%0Ainto%20interval%20non-uniform%20discrete%20representations.%20Under%20a%0Aclassification-based%20objective%2C%20the%20localization%20branch%20generates%20bounded%20and%0Aconfidence-driven%20gradients%2C%20mitigating%20instability.%20Furthermore%2C%20UGS%0Aintegrates%20an%20uncertainty%20minimization%20%28UM%29%20loss%20that%20reduces%20prediction%0Avariance%20and%20an%20uncertainty-guided%20refinement%20%28UR%29%20module%20that%20identifies%20and%0Arefines%20high-uncertainty%20regions%20via%20perturbations.%20Evaluated%20on%20four%0Abenchmarks%2C%20UGS%20consistently%20improves%20anchor-based%2C%20anchor-free%2C%20and%20leading%0Asmall%20object%20detectors.%20Especially%2C%20UGS%20enhances%20DINO-5scale%20by%202.6%20AP%20on%0AVisDrone%2C%20surpassing%20previous%20state-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.01803v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Gradient%2520Stabilization%2520for%2520Small%2520Object%2520Detection%26entry.906535625%3DHuixin%2520Sun%2520and%2520Yanjing%2520Li%2520and%2520Linlin%2520Yang%2520and%2520Xianbin%2520Cao%2520and%2520Baochang%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520advances%2520in%2520generic%2520object%2520detection%252C%2520there%2520remains%2520a%2520performance%2520gap%250Ain%2520detecting%2520small%2520objects%2520compared%2520to%2520normal-scale%2520objects.%2520We%2520reveal%2520that%250Aconventional%2520object%2520localization%2520methods%2520suffer%2520from%2520gradient%2520instability%2520in%250Asmall%2520objects%2520due%2520to%2520sharper%2520loss%2520curvature%252C%2520leading%2520to%2520a%2520convergence%250Achallenge.%2520To%2520address%2520the%2520issue%252C%2520we%2520propose%2520Uncertainty-Aware%2520Gradient%250AStabilization%2520%2528UGS%2529%252C%2520a%2520framework%2520that%2520reformulates%2520object%2520localization%2520as%2520a%250Aclassification%2520task%2520to%2520stabilize%2520gradients.%2520UGS%2520quantizes%2520continuous%2520labels%250Ainto%2520interval%2520non-uniform%2520discrete%2520representations.%2520Under%2520a%250Aclassification-based%2520objective%252C%2520the%2520localization%2520branch%2520generates%2520bounded%2520and%250Aconfidence-driven%2520gradients%252C%2520mitigating%2520instability.%2520Furthermore%252C%2520UGS%250Aintegrates%2520an%2520uncertainty%2520minimization%2520%2528UM%2529%2520loss%2520that%2520reduces%2520prediction%250Avariance%2520and%2520an%2520uncertainty-guided%2520refinement%2520%2528UR%2529%2520module%2520that%2520identifies%2520and%250Arefines%2520high-uncertainty%2520regions%2520via%2520perturbations.%2520Evaluated%2520on%2520four%250Abenchmarks%252C%2520UGS%2520consistently%2520improves%2520anchor-based%252C%2520anchor-free%252C%2520and%2520leading%250Asmall%2520object%2520detectors.%2520Especially%252C%2520UGS%2520enhances%2520DINO-5scale%2520by%25202.6%2520AP%2520on%250AVisDrone%252C%2520surpassing%2520previous%2520state-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.01803v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Gradient%20Stabilization%20for%20Small%20Object%20Detection&entry.906535625=Huixin%20Sun%20and%20Yanjing%20Li%20and%20Linlin%20Yang%20and%20Xianbin%20Cao%20and%20Baochang%20Zhang&entry.1292438233=%20%20Despite%20advances%20in%20generic%20object%20detection%2C%20there%20remains%20a%20performance%20gap%0Ain%20detecting%20small%20objects%20compared%20to%20normal-scale%20objects.%20We%20reveal%20that%0Aconventional%20object%20localization%20methods%20suffer%20from%20gradient%20instability%20in%0Asmall%20objects%20due%20to%20sharper%20loss%20curvature%2C%20leading%20to%20a%20convergence%0Achallenge.%20To%20address%20the%20issue%2C%20we%20propose%20Uncertainty-Aware%20Gradient%0AStabilization%20%28UGS%29%2C%20a%20framework%20that%20reformulates%20object%20localization%20as%20a%0Aclassification%20task%20to%20stabilize%20gradients.%20UGS%20quantizes%20continuous%20labels%0Ainto%20interval%20non-uniform%20discrete%20representations.%20Under%20a%0Aclassification-based%20objective%2C%20the%20localization%20branch%20generates%20bounded%20and%0Aconfidence-driven%20gradients%2C%20mitigating%20instability.%20Furthermore%2C%20UGS%0Aintegrates%20an%20uncertainty%20minimization%20%28UM%29%20loss%20that%20reduces%20prediction%0Avariance%20and%20an%20uncertainty-guided%20refinement%20%28UR%29%20module%20that%20identifies%20and%0Arefines%20high-uncertainty%20regions%20via%20perturbations.%20Evaluated%20on%20four%0Abenchmarks%2C%20UGS%20consistently%20improves%20anchor-based%2C%20anchor-free%2C%20and%20leading%0Asmall%20object%20detectors.%20Especially%2C%20UGS%20enhances%20DINO-5scale%20by%202.6%20AP%20on%0AVisDrone%2C%20surpassing%20previous%20state-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.01803v2&entry.124074799=Read"},
{"title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding", "author": "JingLi Lin and Chenming Zhu and Runsen Xu and Xiaohan Mao and Xihui Liu and Tai Wang and Jiangmiao Pang", "abstract": "  Recent advances in multimodal large language models (MLLMs) have shown\nremarkable capabilities in integrating vision and language for complex\nreasoning. While most existing benchmarks evaluate models under offline\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\nperspective of an agent actively exploring a scene. The Online aspect\nemphasizes the need to process and reason over incrementally acquired\nobservations, while the Spatio-Temporal component requires integrating current\nvisual inputs with historical memory to support dynamic spatial reasoning.\nOST-Bench better reflects the challenges of real-world embodied perception.\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\nonline setting, their accuracy declines as the exploration horizon extends and\nthe memory grows. Through further experimental analysis, we identify common\nerror patterns across models and find that both complex clue-based spatial\nreasoning demands and long-term memory retrieval requirements significantly\ndrop model performance along two separate axes, highlighting the core\nchallenges that must be addressed to improve online embodied reasoning. To\nfoster further research and development in the field, our codes, dataset, and\nbenchmark are available. Our project page is:\nhttps://rbler1234.github.io/OSTBench.github.io/\n", "link": "http://arxiv.org/abs/2507.07984v1", "date": "2025-07-10", "relevancy": 2.4583, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OST-Bench%3A%20Evaluating%20the%20Capabilities%20of%20MLLMs%20in%20Online%0A%20%20Spatio-temporal%20Scene%20Understanding&body=Title%3A%20OST-Bench%3A%20Evaluating%20the%20Capabilities%20of%20MLLMs%20in%20Online%0A%20%20Spatio-temporal%20Scene%20Understanding%0AAuthor%3A%20JingLi%20Lin%20and%20Chenming%20Zhu%20and%20Runsen%20Xu%20and%20Xiaohan%20Mao%20and%20Xihui%20Liu%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%0Aremarkable%20capabilities%20in%20integrating%20vision%20and%20language%20for%20complex%0Areasoning.%20While%20most%20existing%20benchmarks%20evaluate%20models%20under%20offline%0Asettings%20with%20a%20fixed%20set%20of%20pre-recorded%20inputs%2C%20we%20introduce%20OST-Bench%2C%20a%0Abenchmark%20designed%20to%20evaluate%20Online%20Spatio-Temporal%20understanding%20from%20the%0Aperspective%20of%20an%20agent%20actively%20exploring%20a%20scene.%20The%20Online%20aspect%0Aemphasizes%20the%20need%20to%20process%20and%20reason%20over%20incrementally%20acquired%0Aobservations%2C%20while%20the%20Spatio-Temporal%20component%20requires%20integrating%20current%0Avisual%20inputs%20with%20historical%20memory%20to%20support%20dynamic%20spatial%20reasoning.%0AOST-Bench%20better%20reflects%20the%20challenges%20of%20real-world%20embodied%20perception.%0ABuilt%20on%20an%20efficient%20data%20collection%20pipeline%2C%20OST-Bench%20consists%20of%201.4k%0Ascenes%20and%2010k%20question-answer%20pairs%20collected%20from%20ScanNet%2C%20Matterport3D%2C%20and%0AARKitScenes.%20We%20evaluate%20several%20leading%20MLLMs%20on%20OST-Bench%20and%20observe%20that%0Athey%20fall%20short%20on%20tasks%20requiring%20complex%20spatio-temporal%20reasoning.%20Under%20the%0Aonline%20setting%2C%20their%20accuracy%20declines%20as%20the%20exploration%20horizon%20extends%20and%0Athe%20memory%20grows.%20Through%20further%20experimental%20analysis%2C%20we%20identify%20common%0Aerror%20patterns%20across%20models%20and%20find%20that%20both%20complex%20clue-based%20spatial%0Areasoning%20demands%20and%20long-term%20memory%20retrieval%20requirements%20significantly%0Adrop%20model%20performance%20along%20two%20separate%20axes%2C%20highlighting%20the%20core%0Achallenges%20that%20must%20be%20addressed%20to%20improve%20online%20embodied%20reasoning.%20To%0Afoster%20further%20research%20and%20development%20in%20the%20field%2C%20our%20codes%2C%20dataset%2C%20and%0Abenchmark%20are%20available.%20Our%20project%20page%20is%3A%0Ahttps%3A//rbler1234.github.io/OSTBench.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOST-Bench%253A%2520Evaluating%2520the%2520Capabilities%2520of%2520MLLMs%2520in%2520Online%250A%2520%2520Spatio-temporal%2520Scene%2520Understanding%26entry.906535625%3DJingLi%2520Lin%2520and%2520Chenming%2520Zhu%2520and%2520Runsen%2520Xu%2520and%2520Xiaohan%2520Mao%2520and%2520Xihui%2520Liu%2520and%2520Tai%2520Wang%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520shown%250Aremarkable%2520capabilities%2520in%2520integrating%2520vision%2520and%2520language%2520for%2520complex%250Areasoning.%2520While%2520most%2520existing%2520benchmarks%2520evaluate%2520models%2520under%2520offline%250Asettings%2520with%2520a%2520fixed%2520set%2520of%2520pre-recorded%2520inputs%252C%2520we%2520introduce%2520OST-Bench%252C%2520a%250Abenchmark%2520designed%2520to%2520evaluate%2520Online%2520Spatio-Temporal%2520understanding%2520from%2520the%250Aperspective%2520of%2520an%2520agent%2520actively%2520exploring%2520a%2520scene.%2520The%2520Online%2520aspect%250Aemphasizes%2520the%2520need%2520to%2520process%2520and%2520reason%2520over%2520incrementally%2520acquired%250Aobservations%252C%2520while%2520the%2520Spatio-Temporal%2520component%2520requires%2520integrating%2520current%250Avisual%2520inputs%2520with%2520historical%2520memory%2520to%2520support%2520dynamic%2520spatial%2520reasoning.%250AOST-Bench%2520better%2520reflects%2520the%2520challenges%2520of%2520real-world%2520embodied%2520perception.%250ABuilt%2520on%2520an%2520efficient%2520data%2520collection%2520pipeline%252C%2520OST-Bench%2520consists%2520of%25201.4k%250Ascenes%2520and%252010k%2520question-answer%2520pairs%2520collected%2520from%2520ScanNet%252C%2520Matterport3D%252C%2520and%250AARKitScenes.%2520We%2520evaluate%2520several%2520leading%2520MLLMs%2520on%2520OST-Bench%2520and%2520observe%2520that%250Athey%2520fall%2520short%2520on%2520tasks%2520requiring%2520complex%2520spatio-temporal%2520reasoning.%2520Under%2520the%250Aonline%2520setting%252C%2520their%2520accuracy%2520declines%2520as%2520the%2520exploration%2520horizon%2520extends%2520and%250Athe%2520memory%2520grows.%2520Through%2520further%2520experimental%2520analysis%252C%2520we%2520identify%2520common%250Aerror%2520patterns%2520across%2520models%2520and%2520find%2520that%2520both%2520complex%2520clue-based%2520spatial%250Areasoning%2520demands%2520and%2520long-term%2520memory%2520retrieval%2520requirements%2520significantly%250Adrop%2520model%2520performance%2520along%2520two%2520separate%2520axes%252C%2520highlighting%2520the%2520core%250Achallenges%2520that%2520must%2520be%2520addressed%2520to%2520improve%2520online%2520embodied%2520reasoning.%2520To%250Afoster%2520further%2520research%2520and%2520development%2520in%2520the%2520field%252C%2520our%2520codes%252C%2520dataset%252C%2520and%250Abenchmark%2520are%2520available.%2520Our%2520project%2520page%2520is%253A%250Ahttps%253A//rbler1234.github.io/OSTBench.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OST-Bench%3A%20Evaluating%20the%20Capabilities%20of%20MLLMs%20in%20Online%0A%20%20Spatio-temporal%20Scene%20Understanding&entry.906535625=JingLi%20Lin%20and%20Chenming%20Zhu%20and%20Runsen%20Xu%20and%20Xiaohan%20Mao%20and%20Xihui%20Liu%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%0Aremarkable%20capabilities%20in%20integrating%20vision%20and%20language%20for%20complex%0Areasoning.%20While%20most%20existing%20benchmarks%20evaluate%20models%20under%20offline%0Asettings%20with%20a%20fixed%20set%20of%20pre-recorded%20inputs%2C%20we%20introduce%20OST-Bench%2C%20a%0Abenchmark%20designed%20to%20evaluate%20Online%20Spatio-Temporal%20understanding%20from%20the%0Aperspective%20of%20an%20agent%20actively%20exploring%20a%20scene.%20The%20Online%20aspect%0Aemphasizes%20the%20need%20to%20process%20and%20reason%20over%20incrementally%20acquired%0Aobservations%2C%20while%20the%20Spatio-Temporal%20component%20requires%20integrating%20current%0Avisual%20inputs%20with%20historical%20memory%20to%20support%20dynamic%20spatial%20reasoning.%0AOST-Bench%20better%20reflects%20the%20challenges%20of%20real-world%20embodied%20perception.%0ABuilt%20on%20an%20efficient%20data%20collection%20pipeline%2C%20OST-Bench%20consists%20of%201.4k%0Ascenes%20and%2010k%20question-answer%20pairs%20collected%20from%20ScanNet%2C%20Matterport3D%2C%20and%0AARKitScenes.%20We%20evaluate%20several%20leading%20MLLMs%20on%20OST-Bench%20and%20observe%20that%0Athey%20fall%20short%20on%20tasks%20requiring%20complex%20spatio-temporal%20reasoning.%20Under%20the%0Aonline%20setting%2C%20their%20accuracy%20declines%20as%20the%20exploration%20horizon%20extends%20and%0Athe%20memory%20grows.%20Through%20further%20experimental%20analysis%2C%20we%20identify%20common%0Aerror%20patterns%20across%20models%20and%20find%20that%20both%20complex%20clue-based%20spatial%0Areasoning%20demands%20and%20long-term%20memory%20retrieval%20requirements%20significantly%0Adrop%20model%20performance%20along%20two%20separate%20axes%2C%20highlighting%20the%20core%0Achallenges%20that%20must%20be%20addressed%20to%20improve%20online%20embodied%20reasoning.%20To%0Afoster%20further%20research%20and%20development%20in%20the%20field%2C%20our%20codes%2C%20dataset%2C%20and%0Abenchmark%20are%20available.%20Our%20project%20page%20is%3A%0Ahttps%3A//rbler1234.github.io/OSTBench.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07984v1&entry.124074799=Read"},
{"title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs", "author": "Ziyue Li and Yang Li and Tianyi Zhou", "abstract": "  Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.\n", "link": "http://arxiv.org/abs/2507.07996v1", "date": "2025-07-10", "relevancy": 2.4361, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skip%20a%20Layer%20or%20Loop%20it%3F%20Test-Time%20Depth%20Adaptation%20of%20Pretrained%20LLMs&body=Title%3A%20Skip%20a%20Layer%20or%20Loop%20it%3F%20Test-Time%20Depth%20Adaptation%20of%20Pretrained%20LLMs%0AAuthor%3A%20Ziyue%20Li%20and%20Yang%20Li%20and%20Tianyi%20Zhou%0AAbstract%3A%20%20%20Can%20a%20pretrained%20neural%20network%20adapt%20its%20architecture%20to%20different%20inputs%0Awithout%20any%20finetuning%3F%20Do%20we%20need%20all%20layers%20for%20simple%20tasks%2C%20and%20are%20they%0Aadequate%20for%20challenging%20tasks%3F%20We%20found%20that%20the%20layers%20of%20a%20pretrained%20large%0Alanguage%20model%20%28LLM%29%20can%20be%20manipulated%20as%20separate%20modules%20to%20build%20a%20better%0Aand%20even%20shallower%20model%20customized%20for%20each%20test%20sample.%20In%20particular%2C%20each%0Alayer%20from%20the%20pretrained%20model%20can%20be%20skipped/pruned%20or%20repeated%20multiple%0Atimes%20as%20recurrent%20neural%20networks%20%28RNN%29%2C%20and%20stacked%20with%20others%20in%20arbitrary%0Aorders%2C%20yielding%20a%20chain-of-layers%20%28CoLa%29%20per%20sample.%20This%20compositional%20space%0Agreatly%20expands%20the%20scope%20of%20existing%20works%20on%20looped/recurrent%20pretrained%0Amodules%2C%20layer%20pruning%2C%20or%20early-exit%20networks.%20We%20develop%20a%20Monte%20Carlo%20Tree%0ASearch%20%28MCTS%29%20protocol%20to%20explore%20and%20identify%20the%20optimal%20CoLa%20for%20each%20sample%0Afrom%20math%20and%20commonsense%20reasoning%20benchmarks.%20Compared%20to%20a%20static%20model%20of%20a%0Afixed%20depth%2C%20CoLa%20allows%20shortcut%20paths%20%28fast%20thinking%29%2C%20recurrence%20of%20the%20same%0Alayer%28s%29%20%28slow%20thinking%29%2C%20and%20combining%20both%2C%20offering%20more%20flexible%2C%20dynamic%0Aarchitectures%20for%20different%20inputs.%20We%20conduct%20an%20extensive%20analysis%20of%20the%0AMCTS-optimized%20CoLa%2C%20which%20leads%20to%20two%20key%20findings%3A%20%281%29%20For%20%3E75%25%20of%20samples%0Awith%20correct%20predictions%20by%20the%20original%20LLM%2C%20we%20can%20find%20shorter%20CoLa%2C%0Asuggesting%20a%20large%20space%20for%20improving%20inference%20efficiency%3B%20%282%29%20For%20%3E60%25%20of%0Asamples%20with%20originally%20incorrect%20predictions%2C%20we%20can%20identify%20CoLa%20achieving%0Acorrect%20predictions%2C%20suggesting%20a%20large%20space%20of%20performance%20enhancement.%20Our%0Aresults%20highlight%20the%20shortcomings%20of%20using%20a%20fixed%20architecture%20of%20pre-trained%0ALLMs%20for%20inference%20on%20different%20samples%20and%20pave%20the%20way%20to%20unlock%20the%0Ageneralization%20power%20of%20test-time%20depth%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkip%2520a%2520Layer%2520or%2520Loop%2520it%253F%2520Test-Time%2520Depth%2520Adaptation%2520of%2520Pretrained%2520LLMs%26entry.906535625%3DZiyue%2520Li%2520and%2520Yang%2520Li%2520and%2520Tianyi%2520Zhou%26entry.1292438233%3D%2520%2520Can%2520a%2520pretrained%2520neural%2520network%2520adapt%2520its%2520architecture%2520to%2520different%2520inputs%250Awithout%2520any%2520finetuning%253F%2520Do%2520we%2520need%2520all%2520layers%2520for%2520simple%2520tasks%252C%2520and%2520are%2520they%250Aadequate%2520for%2520challenging%2520tasks%253F%2520We%2520found%2520that%2520the%2520layers%2520of%2520a%2520pretrained%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520can%2520be%2520manipulated%2520as%2520separate%2520modules%2520to%2520build%2520a%2520better%250Aand%2520even%2520shallower%2520model%2520customized%2520for%2520each%2520test%2520sample.%2520In%2520particular%252C%2520each%250Alayer%2520from%2520the%2520pretrained%2520model%2520can%2520be%2520skipped/pruned%2520or%2520repeated%2520multiple%250Atimes%2520as%2520recurrent%2520neural%2520networks%2520%2528RNN%2529%252C%2520and%2520stacked%2520with%2520others%2520in%2520arbitrary%250Aorders%252C%2520yielding%2520a%2520chain-of-layers%2520%2528CoLa%2529%2520per%2520sample.%2520This%2520compositional%2520space%250Agreatly%2520expands%2520the%2520scope%2520of%2520existing%2520works%2520on%2520looped/recurrent%2520pretrained%250Amodules%252C%2520layer%2520pruning%252C%2520or%2520early-exit%2520networks.%2520We%2520develop%2520a%2520Monte%2520Carlo%2520Tree%250ASearch%2520%2528MCTS%2529%2520protocol%2520to%2520explore%2520and%2520identify%2520the%2520optimal%2520CoLa%2520for%2520each%2520sample%250Afrom%2520math%2520and%2520commonsense%2520reasoning%2520benchmarks.%2520Compared%2520to%2520a%2520static%2520model%2520of%2520a%250Afixed%2520depth%252C%2520CoLa%2520allows%2520shortcut%2520paths%2520%2528fast%2520thinking%2529%252C%2520recurrence%2520of%2520the%2520same%250Alayer%2528s%2529%2520%2528slow%2520thinking%2529%252C%2520and%2520combining%2520both%252C%2520offering%2520more%2520flexible%252C%2520dynamic%250Aarchitectures%2520for%2520different%2520inputs.%2520We%2520conduct%2520an%2520extensive%2520analysis%2520of%2520the%250AMCTS-optimized%2520CoLa%252C%2520which%2520leads%2520to%2520two%2520key%2520findings%253A%2520%25281%2529%2520For%2520%253E75%2525%2520of%2520samples%250Awith%2520correct%2520predictions%2520by%2520the%2520original%2520LLM%252C%2520we%2520can%2520find%2520shorter%2520CoLa%252C%250Asuggesting%2520a%2520large%2520space%2520for%2520improving%2520inference%2520efficiency%253B%2520%25282%2529%2520For%2520%253E60%2525%2520of%250Asamples%2520with%2520originally%2520incorrect%2520predictions%252C%2520we%2520can%2520identify%2520CoLa%2520achieving%250Acorrect%2520predictions%252C%2520suggesting%2520a%2520large%2520space%2520of%2520performance%2520enhancement.%2520Our%250Aresults%2520highlight%2520the%2520shortcomings%2520of%2520using%2520a%2520fixed%2520architecture%2520of%2520pre-trained%250ALLMs%2520for%2520inference%2520on%2520different%2520samples%2520and%2520pave%2520the%2520way%2520to%2520unlock%2520the%250Ageneralization%2520power%2520of%2520test-time%2520depth%2520adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skip%20a%20Layer%20or%20Loop%20it%3F%20Test-Time%20Depth%20Adaptation%20of%20Pretrained%20LLMs&entry.906535625=Ziyue%20Li%20and%20Yang%20Li%20and%20Tianyi%20Zhou&entry.1292438233=%20%20Can%20a%20pretrained%20neural%20network%20adapt%20its%20architecture%20to%20different%20inputs%0Awithout%20any%20finetuning%3F%20Do%20we%20need%20all%20layers%20for%20simple%20tasks%2C%20and%20are%20they%0Aadequate%20for%20challenging%20tasks%3F%20We%20found%20that%20the%20layers%20of%20a%20pretrained%20large%0Alanguage%20model%20%28LLM%29%20can%20be%20manipulated%20as%20separate%20modules%20to%20build%20a%20better%0Aand%20even%20shallower%20model%20customized%20for%20each%20test%20sample.%20In%20particular%2C%20each%0Alayer%20from%20the%20pretrained%20model%20can%20be%20skipped/pruned%20or%20repeated%20multiple%0Atimes%20as%20recurrent%20neural%20networks%20%28RNN%29%2C%20and%20stacked%20with%20others%20in%20arbitrary%0Aorders%2C%20yielding%20a%20chain-of-layers%20%28CoLa%29%20per%20sample.%20This%20compositional%20space%0Agreatly%20expands%20the%20scope%20of%20existing%20works%20on%20looped/recurrent%20pretrained%0Amodules%2C%20layer%20pruning%2C%20or%20early-exit%20networks.%20We%20develop%20a%20Monte%20Carlo%20Tree%0ASearch%20%28MCTS%29%20protocol%20to%20explore%20and%20identify%20the%20optimal%20CoLa%20for%20each%20sample%0Afrom%20math%20and%20commonsense%20reasoning%20benchmarks.%20Compared%20to%20a%20static%20model%20of%20a%0Afixed%20depth%2C%20CoLa%20allows%20shortcut%20paths%20%28fast%20thinking%29%2C%20recurrence%20of%20the%20same%0Alayer%28s%29%20%28slow%20thinking%29%2C%20and%20combining%20both%2C%20offering%20more%20flexible%2C%20dynamic%0Aarchitectures%20for%20different%20inputs.%20We%20conduct%20an%20extensive%20analysis%20of%20the%0AMCTS-optimized%20CoLa%2C%20which%20leads%20to%20two%20key%20findings%3A%20%281%29%20For%20%3E75%25%20of%20samples%0Awith%20correct%20predictions%20by%20the%20original%20LLM%2C%20we%20can%20find%20shorter%20CoLa%2C%0Asuggesting%20a%20large%20space%20for%20improving%20inference%20efficiency%3B%20%282%29%20For%20%3E60%25%20of%0Asamples%20with%20originally%20incorrect%20predictions%2C%20we%20can%20identify%20CoLa%20achieving%0Acorrect%20predictions%2C%20suggesting%20a%20large%20space%20of%20performance%20enhancement.%20Our%0Aresults%20highlight%20the%20shortcomings%20of%20using%20a%20fixed%20architecture%20of%20pre-trained%0ALLMs%20for%20inference%20on%20different%20samples%20and%20pave%20the%20way%20to%20unlock%20the%0Ageneralization%20power%20of%20test-time%20depth%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07996v1&entry.124074799=Read"},
{"title": "FiDTouch: A 3D Wearable Haptic Display for the Finger Pad", "author": "Daria Trinitatova and Dzmitry Tsetserukou", "abstract": "  The applications of fingertip haptic devices have spread to various fields\nfrom revolutionizing virtual reality and medical training simulations to\nfacilitating remote robotic operations, proposing great potential for enhancing\nuser experiences, improving training outcomes, and new forms of interaction. In\nthis work, we present FiDTouch, a 3D wearable haptic device that delivers\ncutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin\nstretch, and vibrotactile feedback. The application of a tiny inverted Delta\nrobot in the mechanism design allows providing accurate contact and fast\nchanging dynamic stimuli to the finger pad surface. The performance of the\ndeveloped display was evaluated in a two-stage user study of the perception of\nstatic spatial contact stimuli and skin stretch stimuli generated on the finger\npad. The proposed display, by providing users with precise touch and force\nstimuli, can enhance user immersion and efficiency in the fields of\nhuman-computer and human-robot interactions.\n", "link": "http://arxiv.org/abs/2507.07661v1", "date": "2025-07-10", "relevancy": 2.4345, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4987}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4963}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FiDTouch%3A%20A%203D%20Wearable%20Haptic%20Display%20for%20the%20Finger%20Pad&body=Title%3A%20FiDTouch%3A%20A%203D%20Wearable%20Haptic%20Display%20for%20the%20Finger%20Pad%0AAuthor%3A%20Daria%20Trinitatova%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20The%20applications%20of%20fingertip%20haptic%20devices%20have%20spread%20to%20various%20fields%0Afrom%20revolutionizing%20virtual%20reality%20and%20medical%20training%20simulations%20to%0Afacilitating%20remote%20robotic%20operations%2C%20proposing%20great%20potential%20for%20enhancing%0Auser%20experiences%2C%20improving%20training%20outcomes%2C%20and%20new%20forms%20of%20interaction.%20In%0Athis%20work%2C%20we%20present%20FiDTouch%2C%20a%203D%20wearable%20haptic%20device%20that%20delivers%0Acutaneous%20stimuli%20to%20the%20finger%20pad%2C%20such%20as%20contact%2C%20pressure%2C%20encounter%2C%20skin%0Astretch%2C%20and%20vibrotactile%20feedback.%20The%20application%20of%20a%20tiny%20inverted%20Delta%0Arobot%20in%20the%20mechanism%20design%20allows%20providing%20accurate%20contact%20and%20fast%0Achanging%20dynamic%20stimuli%20to%20the%20finger%20pad%20surface.%20The%20performance%20of%20the%0Adeveloped%20display%20was%20evaluated%20in%20a%20two-stage%20user%20study%20of%20the%20perception%20of%0Astatic%20spatial%20contact%20stimuli%20and%20skin%20stretch%20stimuli%20generated%20on%20the%20finger%0Apad.%20The%20proposed%20display%2C%20by%20providing%20users%20with%20precise%20touch%20and%20force%0Astimuli%2C%20can%20enhance%20user%20immersion%20and%20efficiency%20in%20the%20fields%20of%0Ahuman-computer%20and%20human-robot%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFiDTouch%253A%2520A%25203D%2520Wearable%2520Haptic%2520Display%2520for%2520the%2520Finger%2520Pad%26entry.906535625%3DDaria%2520Trinitatova%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520The%2520applications%2520of%2520fingertip%2520haptic%2520devices%2520have%2520spread%2520to%2520various%2520fields%250Afrom%2520revolutionizing%2520virtual%2520reality%2520and%2520medical%2520training%2520simulations%2520to%250Afacilitating%2520remote%2520robotic%2520operations%252C%2520proposing%2520great%2520potential%2520for%2520enhancing%250Auser%2520experiences%252C%2520improving%2520training%2520outcomes%252C%2520and%2520new%2520forms%2520of%2520interaction.%2520In%250Athis%2520work%252C%2520we%2520present%2520FiDTouch%252C%2520a%25203D%2520wearable%2520haptic%2520device%2520that%2520delivers%250Acutaneous%2520stimuli%2520to%2520the%2520finger%2520pad%252C%2520such%2520as%2520contact%252C%2520pressure%252C%2520encounter%252C%2520skin%250Astretch%252C%2520and%2520vibrotactile%2520feedback.%2520The%2520application%2520of%2520a%2520tiny%2520inverted%2520Delta%250Arobot%2520in%2520the%2520mechanism%2520design%2520allows%2520providing%2520accurate%2520contact%2520and%2520fast%250Achanging%2520dynamic%2520stimuli%2520to%2520the%2520finger%2520pad%2520surface.%2520The%2520performance%2520of%2520the%250Adeveloped%2520display%2520was%2520evaluated%2520in%2520a%2520two-stage%2520user%2520study%2520of%2520the%2520perception%2520of%250Astatic%2520spatial%2520contact%2520stimuli%2520and%2520skin%2520stretch%2520stimuli%2520generated%2520on%2520the%2520finger%250Apad.%2520The%2520proposed%2520display%252C%2520by%2520providing%2520users%2520with%2520precise%2520touch%2520and%2520force%250Astimuli%252C%2520can%2520enhance%2520user%2520immersion%2520and%2520efficiency%2520in%2520the%2520fields%2520of%250Ahuman-computer%2520and%2520human-robot%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FiDTouch%3A%20A%203D%20Wearable%20Haptic%20Display%20for%20the%20Finger%20Pad&entry.906535625=Daria%20Trinitatova%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20The%20applications%20of%20fingertip%20haptic%20devices%20have%20spread%20to%20various%20fields%0Afrom%20revolutionizing%20virtual%20reality%20and%20medical%20training%20simulations%20to%0Afacilitating%20remote%20robotic%20operations%2C%20proposing%20great%20potential%20for%20enhancing%0Auser%20experiences%2C%20improving%20training%20outcomes%2C%20and%20new%20forms%20of%20interaction.%20In%0Athis%20work%2C%20we%20present%20FiDTouch%2C%20a%203D%20wearable%20haptic%20device%20that%20delivers%0Acutaneous%20stimuli%20to%20the%20finger%20pad%2C%20such%20as%20contact%2C%20pressure%2C%20encounter%2C%20skin%0Astretch%2C%20and%20vibrotactile%20feedback.%20The%20application%20of%20a%20tiny%20inverted%20Delta%0Arobot%20in%20the%20mechanism%20design%20allows%20providing%20accurate%20contact%20and%20fast%0Achanging%20dynamic%20stimuli%20to%20the%20finger%20pad%20surface.%20The%20performance%20of%20the%0Adeveloped%20display%20was%20evaluated%20in%20a%20two-stage%20user%20study%20of%20the%20perception%20of%0Astatic%20spatial%20contact%20stimuli%20and%20skin%20stretch%20stimuli%20generated%20on%20the%20finger%0Apad.%20The%20proposed%20display%2C%20by%20providing%20users%20with%20precise%20touch%20and%20force%0Astimuli%2C%20can%20enhance%20user%20immersion%20and%20efficiency%20in%20the%20fields%20of%0Ahuman-computer%20and%20human-robot%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07661v1&entry.124074799=Read"},
{"title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling", "author": "Haoyu Wu and Diankun Wu and Tianyu He and Junliang Guo and Yang Ye and Yueqi Duan and Jiang Bian", "abstract": "  Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.\n", "link": "http://arxiv.org/abs/2507.07982v1", "date": "2025-07-10", "relevancy": 2.4213, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6154}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6023}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20Forcing%3A%20Marrying%20Video%20Diffusion%20and%203D%20Representation%20for%0A%20%20Consistent%20World%20Modeling&body=Title%3A%20Geometry%20Forcing%3A%20Marrying%20Video%20Diffusion%20and%203D%20Representation%20for%0A%20%20Consistent%20World%20Modeling%0AAuthor%3A%20Haoyu%20Wu%20and%20Diankun%20Wu%20and%20Tianyu%20He%20and%20Junliang%20Guo%20and%20Yang%20Ye%20and%20Yueqi%20Duan%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Videos%20inherently%20represent%202D%20projections%20of%20a%20dynamic%203D%20world.%20However%2C%0Aour%20analysis%20suggests%20that%20video%20diffusion%20models%20trained%20solely%20on%20raw%20video%0Adata%20often%20fail%20to%20capture%20meaningful%20geometric-aware%20structure%20in%20their%0Alearned%20representations.%20To%20bridge%20this%20gap%20between%20video%20diffusion%20models%20and%0Athe%20underlying%203D%20nature%20of%20the%20physical%20world%2C%20we%20propose%20Geometry%20Forcing%2C%20a%0Asimple%20yet%20effective%20method%20that%20encourages%20video%20diffusion%20models%20to%0Ainternalize%20latent%203D%20representations.%20Our%20key%20insight%20is%20to%20guide%20the%20model%27s%0Aintermediate%20representations%20toward%20geometry-aware%20structure%20by%20aligning%20them%0Awith%20features%20from%20a%20pretrained%20geometric%20foundation%20model.%20To%20this%20end%2C%20we%0Aintroduce%20two%20complementary%20alignment%20objectives%3A%20Angular%20Alignment%2C%20which%0Aenforces%20directional%20consistency%20via%20cosine%20similarity%2C%20and%20Scale%20Alignment%2C%0Awhich%20preserves%20scale-related%20information%20by%20regressing%20unnormalized%20geometric%0Afeatures%20from%20normalized%20diffusion%20representation.%20We%20evaluate%20Geometry%20Forcing%0Aon%20both%20camera%20view-conditioned%20and%20action-conditioned%20video%20generation%20tasks.%0AExperimental%20results%20demonstrate%20that%20our%20method%20substantially%20improves%20visual%0Aquality%20and%203D%20consistency%20over%20the%20baseline%20methods.%20Project%20page%3A%0Ahttps%3A//GeometryForcing.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520Forcing%253A%2520Marrying%2520Video%2520Diffusion%2520and%25203D%2520Representation%2520for%250A%2520%2520Consistent%2520World%2520Modeling%26entry.906535625%3DHaoyu%2520Wu%2520and%2520Diankun%2520Wu%2520and%2520Tianyu%2520He%2520and%2520Junliang%2520Guo%2520and%2520Yang%2520Ye%2520and%2520Yueqi%2520Duan%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Videos%2520inherently%2520represent%25202D%2520projections%2520of%2520a%2520dynamic%25203D%2520world.%2520However%252C%250Aour%2520analysis%2520suggests%2520that%2520video%2520diffusion%2520models%2520trained%2520solely%2520on%2520raw%2520video%250Adata%2520often%2520fail%2520to%2520capture%2520meaningful%2520geometric-aware%2520structure%2520in%2520their%250Alearned%2520representations.%2520To%2520bridge%2520this%2520gap%2520between%2520video%2520diffusion%2520models%2520and%250Athe%2520underlying%25203D%2520nature%2520of%2520the%2520physical%2520world%252C%2520we%2520propose%2520Geometry%2520Forcing%252C%2520a%250Asimple%2520yet%2520effective%2520method%2520that%2520encourages%2520video%2520diffusion%2520models%2520to%250Ainternalize%2520latent%25203D%2520representations.%2520Our%2520key%2520insight%2520is%2520to%2520guide%2520the%2520model%2527s%250Aintermediate%2520representations%2520toward%2520geometry-aware%2520structure%2520by%2520aligning%2520them%250Awith%2520features%2520from%2520a%2520pretrained%2520geometric%2520foundation%2520model.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520two%2520complementary%2520alignment%2520objectives%253A%2520Angular%2520Alignment%252C%2520which%250Aenforces%2520directional%2520consistency%2520via%2520cosine%2520similarity%252C%2520and%2520Scale%2520Alignment%252C%250Awhich%2520preserves%2520scale-related%2520information%2520by%2520regressing%2520unnormalized%2520geometric%250Afeatures%2520from%2520normalized%2520diffusion%2520representation.%2520We%2520evaluate%2520Geometry%2520Forcing%250Aon%2520both%2520camera%2520view-conditioned%2520and%2520action-conditioned%2520video%2520generation%2520tasks.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520substantially%2520improves%2520visual%250Aquality%2520and%25203D%2520consistency%2520over%2520the%2520baseline%2520methods.%2520Project%2520page%253A%250Ahttps%253A//GeometryForcing.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20Forcing%3A%20Marrying%20Video%20Diffusion%20and%203D%20Representation%20for%0A%20%20Consistent%20World%20Modeling&entry.906535625=Haoyu%20Wu%20and%20Diankun%20Wu%20and%20Tianyu%20He%20and%20Junliang%20Guo%20and%20Yang%20Ye%20and%20Yueqi%20Duan%20and%20Jiang%20Bian&entry.1292438233=%20%20Videos%20inherently%20represent%202D%20projections%20of%20a%20dynamic%203D%20world.%20However%2C%0Aour%20analysis%20suggests%20that%20video%20diffusion%20models%20trained%20solely%20on%20raw%20video%0Adata%20often%20fail%20to%20capture%20meaningful%20geometric-aware%20structure%20in%20their%0Alearned%20representations.%20To%20bridge%20this%20gap%20between%20video%20diffusion%20models%20and%0Athe%20underlying%203D%20nature%20of%20the%20physical%20world%2C%20we%20propose%20Geometry%20Forcing%2C%20a%0Asimple%20yet%20effective%20method%20that%20encourages%20video%20diffusion%20models%20to%0Ainternalize%20latent%203D%20representations.%20Our%20key%20insight%20is%20to%20guide%20the%20model%27s%0Aintermediate%20representations%20toward%20geometry-aware%20structure%20by%20aligning%20them%0Awith%20features%20from%20a%20pretrained%20geometric%20foundation%20model.%20To%20this%20end%2C%20we%0Aintroduce%20two%20complementary%20alignment%20objectives%3A%20Angular%20Alignment%2C%20which%0Aenforces%20directional%20consistency%20via%20cosine%20similarity%2C%20and%20Scale%20Alignment%2C%0Awhich%20preserves%20scale-related%20information%20by%20regressing%20unnormalized%20geometric%0Afeatures%20from%20normalized%20diffusion%20representation.%20We%20evaluate%20Geometry%20Forcing%0Aon%20both%20camera%20view-conditioned%20and%20action-conditioned%20video%20generation%20tasks.%0AExperimental%20results%20demonstrate%20that%20our%20method%20substantially%20improves%20visual%0Aquality%20and%203D%20consistency%20over%20the%20baseline%20methods.%20Project%20page%3A%0Ahttps%3A//GeometryForcing.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07982v1&entry.124074799=Read"},
{"title": "T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates", "author": "Zhitao Wang and Hengyu Man and Wenrui Li and Xingtao Wang and Xiaopeng Fan and Debin Zhao", "abstract": "  Recent advances in video generation techniques have given rise to an emerging\nparadigm of generative video coding, aiming to achieve semantically accurate\nreconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong\ngenerative priors. However, most existing methods are limited by domain\nspecificity (e.g., facial or human videos) or an excessive dependence on\nhigh-level text guidance, which often fails to capture motion details and\nresults in unrealistic reconstructions. To address these challenges, we propose\na Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC\nemploys a semantic-aware sparse motion sampling pipeline to effectively bridge\nlow-level motion tracking with high-level semantic understanding by extracting\npixel-wise motion as sparse trajectory points based on their semantic\nimportance, not only significantly reducing the bitrate but also preserving\ncritical temporal semantic information. In addition, by incorporating\ntrajectory-aligned loss constraints into diffusion processes, we introduce a\ntraining-free latent space guidance mechanism to ensure physically plausible\nmotion patterns without sacrificing the inherent capabilities of generative\nmodels. Experimental results demonstrate that our framework outperforms both\ntraditional codecs and state-of-the-art end-to-end video compression methods\nunder ULB conditions. Furthermore, additional experiments confirm that our\napproach achieves more precise motion control than existing text-guided\nmethods, paving the way for a novel direction of generative video coding guided\nby geometric motion modeling.\n", "link": "http://arxiv.org/abs/2507.07633v1", "date": "2025-07-10", "relevancy": 2.4086, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.619}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5986}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-GVC%3A%20Trajectory-Guided%20Generative%20Video%20Coding%20at%20Ultra-Low%20Bitrates&body=Title%3A%20T-GVC%3A%20Trajectory-Guided%20Generative%20Video%20Coding%20at%20Ultra-Low%20Bitrates%0AAuthor%3A%20Zhitao%20Wang%20and%20Hengyu%20Man%20and%20Wenrui%20Li%20and%20Xingtao%20Wang%20and%20Xiaopeng%20Fan%20and%20Debin%20Zhao%0AAbstract%3A%20%20%20Recent%20advances%20in%20video%20generation%20techniques%20have%20given%20rise%20to%20an%20emerging%0Aparadigm%20of%20generative%20video%20coding%2C%20aiming%20to%20achieve%20semantically%20accurate%0Areconstructions%20in%20Ultra-Low%20Bitrate%20%28ULB%29%20scenarios%20by%20leveraging%20strong%0Agenerative%20priors.%20However%2C%20most%20existing%20methods%20are%20limited%20by%20domain%0Aspecificity%20%28e.g.%2C%20facial%20or%20human%20videos%29%20or%20an%20excessive%20dependence%20on%0Ahigh-level%20text%20guidance%2C%20which%20often%20fails%20to%20capture%20motion%20details%20and%0Aresults%20in%20unrealistic%20reconstructions.%20To%20address%20these%20challenges%2C%20we%20propose%0Aa%20Trajectory-Guided%20Generative%20Video%20Coding%20framework%20%28dubbed%20T-GVC%29.%20T-GVC%0Aemploys%20a%20semantic-aware%20sparse%20motion%20sampling%20pipeline%20to%20effectively%20bridge%0Alow-level%20motion%20tracking%20with%20high-level%20semantic%20understanding%20by%20extracting%0Apixel-wise%20motion%20as%20sparse%20trajectory%20points%20based%20on%20their%20semantic%0Aimportance%2C%20not%20only%20significantly%20reducing%20the%20bitrate%20but%20also%20preserving%0Acritical%20temporal%20semantic%20information.%20In%20addition%2C%20by%20incorporating%0Atrajectory-aligned%20loss%20constraints%20into%20diffusion%20processes%2C%20we%20introduce%20a%0Atraining-free%20latent%20space%20guidance%20mechanism%20to%20ensure%20physically%20plausible%0Amotion%20patterns%20without%20sacrificing%20the%20inherent%20capabilities%20of%20generative%0Amodels.%20Experimental%20results%20demonstrate%20that%20our%20framework%20outperforms%20both%0Atraditional%20codecs%20and%20state-of-the-art%20end-to-end%20video%20compression%20methods%0Aunder%20ULB%20conditions.%20Furthermore%2C%20additional%20experiments%20confirm%20that%20our%0Aapproach%20achieves%20more%20precise%20motion%20control%20than%20existing%20text-guided%0Amethods%2C%20paving%20the%20way%20for%20a%20novel%20direction%20of%20generative%20video%20coding%20guided%0Aby%20geometric%20motion%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-GVC%253A%2520Trajectory-Guided%2520Generative%2520Video%2520Coding%2520at%2520Ultra-Low%2520Bitrates%26entry.906535625%3DZhitao%2520Wang%2520and%2520Hengyu%2520Man%2520and%2520Wenrui%2520Li%2520and%2520Xingtao%2520Wang%2520and%2520Xiaopeng%2520Fan%2520and%2520Debin%2520Zhao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520video%2520generation%2520techniques%2520have%2520given%2520rise%2520to%2520an%2520emerging%250Aparadigm%2520of%2520generative%2520video%2520coding%252C%2520aiming%2520to%2520achieve%2520semantically%2520accurate%250Areconstructions%2520in%2520Ultra-Low%2520Bitrate%2520%2528ULB%2529%2520scenarios%2520by%2520leveraging%2520strong%250Agenerative%2520priors.%2520However%252C%2520most%2520existing%2520methods%2520are%2520limited%2520by%2520domain%250Aspecificity%2520%2528e.g.%252C%2520facial%2520or%2520human%2520videos%2529%2520or%2520an%2520excessive%2520dependence%2520on%250Ahigh-level%2520text%2520guidance%252C%2520which%2520often%2520fails%2520to%2520capture%2520motion%2520details%2520and%250Aresults%2520in%2520unrealistic%2520reconstructions.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250Aa%2520Trajectory-Guided%2520Generative%2520Video%2520Coding%2520framework%2520%2528dubbed%2520T-GVC%2529.%2520T-GVC%250Aemploys%2520a%2520semantic-aware%2520sparse%2520motion%2520sampling%2520pipeline%2520to%2520effectively%2520bridge%250Alow-level%2520motion%2520tracking%2520with%2520high-level%2520semantic%2520understanding%2520by%2520extracting%250Apixel-wise%2520motion%2520as%2520sparse%2520trajectory%2520points%2520based%2520on%2520their%2520semantic%250Aimportance%252C%2520not%2520only%2520significantly%2520reducing%2520the%2520bitrate%2520but%2520also%2520preserving%250Acritical%2520temporal%2520semantic%2520information.%2520In%2520addition%252C%2520by%2520incorporating%250Atrajectory-aligned%2520loss%2520constraints%2520into%2520diffusion%2520processes%252C%2520we%2520introduce%2520a%250Atraining-free%2520latent%2520space%2520guidance%2520mechanism%2520to%2520ensure%2520physically%2520plausible%250Amotion%2520patterns%2520without%2520sacrificing%2520the%2520inherent%2520capabilities%2520of%2520generative%250Amodels.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520framework%2520outperforms%2520both%250Atraditional%2520codecs%2520and%2520state-of-the-art%2520end-to-end%2520video%2520compression%2520methods%250Aunder%2520ULB%2520conditions.%2520Furthermore%252C%2520additional%2520experiments%2520confirm%2520that%2520our%250Aapproach%2520achieves%2520more%2520precise%2520motion%2520control%2520than%2520existing%2520text-guided%250Amethods%252C%2520paving%2520the%2520way%2520for%2520a%2520novel%2520direction%2520of%2520generative%2520video%2520coding%2520guided%250Aby%2520geometric%2520motion%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-GVC%3A%20Trajectory-Guided%20Generative%20Video%20Coding%20at%20Ultra-Low%20Bitrates&entry.906535625=Zhitao%20Wang%20and%20Hengyu%20Man%20and%20Wenrui%20Li%20and%20Xingtao%20Wang%20and%20Xiaopeng%20Fan%20and%20Debin%20Zhao&entry.1292438233=%20%20Recent%20advances%20in%20video%20generation%20techniques%20have%20given%20rise%20to%20an%20emerging%0Aparadigm%20of%20generative%20video%20coding%2C%20aiming%20to%20achieve%20semantically%20accurate%0Areconstructions%20in%20Ultra-Low%20Bitrate%20%28ULB%29%20scenarios%20by%20leveraging%20strong%0Agenerative%20priors.%20However%2C%20most%20existing%20methods%20are%20limited%20by%20domain%0Aspecificity%20%28e.g.%2C%20facial%20or%20human%20videos%29%20or%20an%20excessive%20dependence%20on%0Ahigh-level%20text%20guidance%2C%20which%20often%20fails%20to%20capture%20motion%20details%20and%0Aresults%20in%20unrealistic%20reconstructions.%20To%20address%20these%20challenges%2C%20we%20propose%0Aa%20Trajectory-Guided%20Generative%20Video%20Coding%20framework%20%28dubbed%20T-GVC%29.%20T-GVC%0Aemploys%20a%20semantic-aware%20sparse%20motion%20sampling%20pipeline%20to%20effectively%20bridge%0Alow-level%20motion%20tracking%20with%20high-level%20semantic%20understanding%20by%20extracting%0Apixel-wise%20motion%20as%20sparse%20trajectory%20points%20based%20on%20their%20semantic%0Aimportance%2C%20not%20only%20significantly%20reducing%20the%20bitrate%20but%20also%20preserving%0Acritical%20temporal%20semantic%20information.%20In%20addition%2C%20by%20incorporating%0Atrajectory-aligned%20loss%20constraints%20into%20diffusion%20processes%2C%20we%20introduce%20a%0Atraining-free%20latent%20space%20guidance%20mechanism%20to%20ensure%20physically%20plausible%0Amotion%20patterns%20without%20sacrificing%20the%20inherent%20capabilities%20of%20generative%0Amodels.%20Experimental%20results%20demonstrate%20that%20our%20framework%20outperforms%20both%0Atraditional%20codecs%20and%20state-of-the-art%20end-to-end%20video%20compression%20methods%0Aunder%20ULB%20conditions.%20Furthermore%2C%20additional%20experiments%20confirm%20that%20our%0Aapproach%20achieves%20more%20precise%20motion%20control%20than%20existing%20text-guided%0Amethods%2C%20paving%20the%20way%20for%20a%20novel%20direction%20of%20generative%20video%20coding%20guided%0Aby%20geometric%20motion%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07633v1&entry.124074799=Read"},
{"title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology", "author": "Haochen Wang and Xiangtai Li and Zilong Huang and Anran Wang and Jiacong Wang and Tao Zhang and Jiani Zheng and Sule Bai and Zijian Kang and Jiashi Feng and Zhuochen Wang and Zhaoxiang Zhang", "abstract": "  Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\nreferencing visual regions, just like human \"thinking with images\". However, no\nbenchmark exists to evaluate these capabilities holistically. To bridge this\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\ndiagnostic benchmark built on three principles: (1) focused visual perception\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\nevaluation, and (3) second-order reasoning to test object interactions and\nspatial hierarchies beyond simple object localization. Prioritizing images with\ndense objects, we initially sample 1K high-quality images from SA-1B, and\nincorporate eight LMM experts to manually annotate questions, candidate\noptions, and answers for each image. After three stages of quality control,\nTreeBench consists of 405 challenging visual question-answering pairs, even the\nmost advanced models struggle with this benchmark, where none of them reach 60%\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\nsupervise localization and reasoning jointly with reinforcement learning,\nenabling accurate localizations and explainable reasoning pathways. Initialized\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.\n", "link": "http://arxiv.org/abs/2507.07999v1", "date": "2025-07-10", "relevancy": 2.4016, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6106}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6106}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Traceable%20Evidence%20Enhanced%20Visual%20Grounded%20Reasoning%3A%20Evaluation%20and%0A%20%20Methodology&body=Title%3A%20Traceable%20Evidence%20Enhanced%20Visual%20Grounded%20Reasoning%3A%20Evaluation%20and%0A%20%20Methodology%0AAuthor%3A%20Haochen%20Wang%20and%20Xiangtai%20Li%20and%20Zilong%20Huang%20and%20Anran%20Wang%20and%20Jiacong%20Wang%20and%20Tao%20Zhang%20and%20Jiani%20Zheng%20and%20Sule%20Bai%20and%20Zijian%20Kang%20and%20Jiashi%20Feng%20and%20Zhuochen%20Wang%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20Models%20like%20OpenAI-o3%20pioneer%20visual%20grounded%20reasoning%20by%20dynamically%0Areferencing%20visual%20regions%2C%20just%20like%20human%20%22thinking%20with%20images%22.%20However%2C%20no%0Abenchmark%20exists%20to%20evaluate%20these%20capabilities%20holistically.%20To%20bridge%20this%0Agap%2C%20we%20propose%20TreeBench%20%28Traceable%20Evidence%20Evaluation%20Benchmark%29%2C%20a%0Adiagnostic%20benchmark%20built%20on%20three%20principles%3A%20%281%29%20focused%20visual%20perception%0Aof%20subtle%20targets%20in%20complex%20scenes%2C%20%282%29%20traceable%20evidence%20via%20bounding%20box%0Aevaluation%2C%20and%20%283%29%20second-order%20reasoning%20to%20test%20object%20interactions%20and%0Aspatial%20hierarchies%20beyond%20simple%20object%20localization.%20Prioritizing%20images%20with%0Adense%20objects%2C%20we%20initially%20sample%201K%20high-quality%20images%20from%20SA-1B%2C%20and%0Aincorporate%20eight%20LMM%20experts%20to%20manually%20annotate%20questions%2C%20candidate%0Aoptions%2C%20and%20answers%20for%20each%20image.%20After%20three%20stages%20of%20quality%20control%2C%0ATreeBench%20consists%20of%20405%20challenging%20visual%20question-answering%20pairs%2C%20even%20the%0Amost%20advanced%20models%20struggle%20with%20this%20benchmark%2C%20where%20none%20of%20them%20reach%2060%25%0Aaccuracy%2C%20e.g.%2C%20OpenAI-o3%20scores%20only%2054.87.%20Furthermore%2C%20we%20introduce%20TreeVGR%0A%28Traceable%20Evidence%20Enhanced%20Visual%20Grounded%20Reasoning%29%2C%20a%20training%20paradigm%20to%0Asupervise%20localization%20and%20reasoning%20jointly%20with%20reinforcement%20learning%2C%0Aenabling%20accurate%20localizations%20and%20explainable%20reasoning%20pathways.%20Initialized%0Afrom%20Qwen2.5-VL-7B%2C%20it%20improves%20V%2A%20Bench%20%28%2B16.8%29%2C%20MME-RealWorld%20%28%2B12.6%29%2C%20and%0ATreeBench%20%28%2B13.4%29%2C%20proving%20traceability%20is%20key%20to%20advancing%20vision-grounded%0Areasoning.%20The%20code%20is%20available%20at%20https%3A//github.com/Haochen-Wang409/TreeVGR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraceable%2520Evidence%2520Enhanced%2520Visual%2520Grounded%2520Reasoning%253A%2520Evaluation%2520and%250A%2520%2520Methodology%26entry.906535625%3DHaochen%2520Wang%2520and%2520Xiangtai%2520Li%2520and%2520Zilong%2520Huang%2520and%2520Anran%2520Wang%2520and%2520Jiacong%2520Wang%2520and%2520Tao%2520Zhang%2520and%2520Jiani%2520Zheng%2520and%2520Sule%2520Bai%2520and%2520Zijian%2520Kang%2520and%2520Jiashi%2520Feng%2520and%2520Zhuochen%2520Wang%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520Models%2520like%2520OpenAI-o3%2520pioneer%2520visual%2520grounded%2520reasoning%2520by%2520dynamically%250Areferencing%2520visual%2520regions%252C%2520just%2520like%2520human%2520%2522thinking%2520with%2520images%2522.%2520However%252C%2520no%250Abenchmark%2520exists%2520to%2520evaluate%2520these%2520capabilities%2520holistically.%2520To%2520bridge%2520this%250Agap%252C%2520we%2520propose%2520TreeBench%2520%2528Traceable%2520Evidence%2520Evaluation%2520Benchmark%2529%252C%2520a%250Adiagnostic%2520benchmark%2520built%2520on%2520three%2520principles%253A%2520%25281%2529%2520focused%2520visual%2520perception%250Aof%2520subtle%2520targets%2520in%2520complex%2520scenes%252C%2520%25282%2529%2520traceable%2520evidence%2520via%2520bounding%2520box%250Aevaluation%252C%2520and%2520%25283%2529%2520second-order%2520reasoning%2520to%2520test%2520object%2520interactions%2520and%250Aspatial%2520hierarchies%2520beyond%2520simple%2520object%2520localization.%2520Prioritizing%2520images%2520with%250Adense%2520objects%252C%2520we%2520initially%2520sample%25201K%2520high-quality%2520images%2520from%2520SA-1B%252C%2520and%250Aincorporate%2520eight%2520LMM%2520experts%2520to%2520manually%2520annotate%2520questions%252C%2520candidate%250Aoptions%252C%2520and%2520answers%2520for%2520each%2520image.%2520After%2520three%2520stages%2520of%2520quality%2520control%252C%250ATreeBench%2520consists%2520of%2520405%2520challenging%2520visual%2520question-answering%2520pairs%252C%2520even%2520the%250Amost%2520advanced%2520models%2520struggle%2520with%2520this%2520benchmark%252C%2520where%2520none%2520of%2520them%2520reach%252060%2525%250Aaccuracy%252C%2520e.g.%252C%2520OpenAI-o3%2520scores%2520only%252054.87.%2520Furthermore%252C%2520we%2520introduce%2520TreeVGR%250A%2528Traceable%2520Evidence%2520Enhanced%2520Visual%2520Grounded%2520Reasoning%2529%252C%2520a%2520training%2520paradigm%2520to%250Asupervise%2520localization%2520and%2520reasoning%2520jointly%2520with%2520reinforcement%2520learning%252C%250Aenabling%2520accurate%2520localizations%2520and%2520explainable%2520reasoning%2520pathways.%2520Initialized%250Afrom%2520Qwen2.5-VL-7B%252C%2520it%2520improves%2520V%252A%2520Bench%2520%2528%252B16.8%2529%252C%2520MME-RealWorld%2520%2528%252B12.6%2529%252C%2520and%250ATreeBench%2520%2528%252B13.4%2529%252C%2520proving%2520traceability%2520is%2520key%2520to%2520advancing%2520vision-grounded%250Areasoning.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Haochen-Wang409/TreeVGR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Traceable%20Evidence%20Enhanced%20Visual%20Grounded%20Reasoning%3A%20Evaluation%20and%0A%20%20Methodology&entry.906535625=Haochen%20Wang%20and%20Xiangtai%20Li%20and%20Zilong%20Huang%20and%20Anran%20Wang%20and%20Jiacong%20Wang%20and%20Tao%20Zhang%20and%20Jiani%20Zheng%20and%20Sule%20Bai%20and%20Zijian%20Kang%20and%20Jiashi%20Feng%20and%20Zhuochen%20Wang%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20Models%20like%20OpenAI-o3%20pioneer%20visual%20grounded%20reasoning%20by%20dynamically%0Areferencing%20visual%20regions%2C%20just%20like%20human%20%22thinking%20with%20images%22.%20However%2C%20no%0Abenchmark%20exists%20to%20evaluate%20these%20capabilities%20holistically.%20To%20bridge%20this%0Agap%2C%20we%20propose%20TreeBench%20%28Traceable%20Evidence%20Evaluation%20Benchmark%29%2C%20a%0Adiagnostic%20benchmark%20built%20on%20three%20principles%3A%20%281%29%20focused%20visual%20perception%0Aof%20subtle%20targets%20in%20complex%20scenes%2C%20%282%29%20traceable%20evidence%20via%20bounding%20box%0Aevaluation%2C%20and%20%283%29%20second-order%20reasoning%20to%20test%20object%20interactions%20and%0Aspatial%20hierarchies%20beyond%20simple%20object%20localization.%20Prioritizing%20images%20with%0Adense%20objects%2C%20we%20initially%20sample%201K%20high-quality%20images%20from%20SA-1B%2C%20and%0Aincorporate%20eight%20LMM%20experts%20to%20manually%20annotate%20questions%2C%20candidate%0Aoptions%2C%20and%20answers%20for%20each%20image.%20After%20three%20stages%20of%20quality%20control%2C%0ATreeBench%20consists%20of%20405%20challenging%20visual%20question-answering%20pairs%2C%20even%20the%0Amost%20advanced%20models%20struggle%20with%20this%20benchmark%2C%20where%20none%20of%20them%20reach%2060%25%0Aaccuracy%2C%20e.g.%2C%20OpenAI-o3%20scores%20only%2054.87.%20Furthermore%2C%20we%20introduce%20TreeVGR%0A%28Traceable%20Evidence%20Enhanced%20Visual%20Grounded%20Reasoning%29%2C%20a%20training%20paradigm%20to%0Asupervise%20localization%20and%20reasoning%20jointly%20with%20reinforcement%20learning%2C%0Aenabling%20accurate%20localizations%20and%20explainable%20reasoning%20pathways.%20Initialized%0Afrom%20Qwen2.5-VL-7B%2C%20it%20improves%20V%2A%20Bench%20%28%2B16.8%29%2C%20MME-RealWorld%20%28%2B12.6%29%2C%20and%0ATreeBench%20%28%2B13.4%29%2C%20proving%20traceability%20is%20key%20to%20advancing%20vision-grounded%0Areasoning.%20The%20code%20is%20available%20at%20https%3A//github.com/Haochen-Wang409/TreeVGR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07999v1&entry.124074799=Read"},
{"title": "Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time\n  Training", "author": "Wooseong Jeong and Jegyeong Cho and Youngho Yoon and Kuk-Jin Yoon", "abstract": "  Generalizing neural networks to unseen target domains is a significant\nchallenge in real-world deployments. Test-time training (TTT) addresses this by\nusing an auxiliary self-supervised task to reduce the domain gap caused by\ndistribution shifts between the source and target. However, we find that when\nmodels are required to perform multiple tasks under domain shifts, conventional\nTTT methods suffer from unsynchronized task behavior, where the adaptation\nsteps needed for optimal performance in one task may not align with the\nrequirements of other tasks. To address this, we propose a novel TTT approach\ncalled Synchronizing Tasks for Test-time Training (S4T), which enables the\nconcurrent handling of multiple tasks. The core idea behind S4T is that\npredicting task relations across domain shifts is key to synchronizing tasks\nduring test time. To validate our approach, we apply S4T to conventional\nmulti-task benchmarks, integrating it with traditional TTT protocols. Our\nempirical results show that S4T outperforms state-of-the-art TTT methods across\nvarious benchmarks.\n", "link": "http://arxiv.org/abs/2507.07778v1", "date": "2025-07-10", "relevancy": 2.3994, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4941}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4765}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synchronizing%20Task%20Behavior%3A%20Aligning%20Multiple%20Tasks%20during%20Test-Time%0A%20%20Training&body=Title%3A%20Synchronizing%20Task%20Behavior%3A%20Aligning%20Multiple%20Tasks%20during%20Test-Time%0A%20%20Training%0AAuthor%3A%20Wooseong%20Jeong%20and%20Jegyeong%20Cho%20and%20Youngho%20Yoon%20and%20Kuk-Jin%20Yoon%0AAbstract%3A%20%20%20Generalizing%20neural%20networks%20to%20unseen%20target%20domains%20is%20a%20significant%0Achallenge%20in%20real-world%20deployments.%20Test-time%20training%20%28TTT%29%20addresses%20this%20by%0Ausing%20an%20auxiliary%20self-supervised%20task%20to%20reduce%20the%20domain%20gap%20caused%20by%0Adistribution%20shifts%20between%20the%20source%20and%20target.%20However%2C%20we%20find%20that%20when%0Amodels%20are%20required%20to%20perform%20multiple%20tasks%20under%20domain%20shifts%2C%20conventional%0ATTT%20methods%20suffer%20from%20unsynchronized%20task%20behavior%2C%20where%20the%20adaptation%0Asteps%20needed%20for%20optimal%20performance%20in%20one%20task%20may%20not%20align%20with%20the%0Arequirements%20of%20other%20tasks.%20To%20address%20this%2C%20we%20propose%20a%20novel%20TTT%20approach%0Acalled%20Synchronizing%20Tasks%20for%20Test-time%20Training%20%28S4T%29%2C%20which%20enables%20the%0Aconcurrent%20handling%20of%20multiple%20tasks.%20The%20core%20idea%20behind%20S4T%20is%20that%0Apredicting%20task%20relations%20across%20domain%20shifts%20is%20key%20to%20synchronizing%20tasks%0Aduring%20test%20time.%20To%20validate%20our%20approach%2C%20we%20apply%20S4T%20to%20conventional%0Amulti-task%20benchmarks%2C%20integrating%20it%20with%20traditional%20TTT%20protocols.%20Our%0Aempirical%20results%20show%20that%20S4T%20outperforms%20state-of-the-art%20TTT%20methods%20across%0Avarious%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynchronizing%2520Task%2520Behavior%253A%2520Aligning%2520Multiple%2520Tasks%2520during%2520Test-Time%250A%2520%2520Training%26entry.906535625%3DWooseong%2520Jeong%2520and%2520Jegyeong%2520Cho%2520and%2520Youngho%2520Yoon%2520and%2520Kuk-Jin%2520Yoon%26entry.1292438233%3D%2520%2520Generalizing%2520neural%2520networks%2520to%2520unseen%2520target%2520domains%2520is%2520a%2520significant%250Achallenge%2520in%2520real-world%2520deployments.%2520Test-time%2520training%2520%2528TTT%2529%2520addresses%2520this%2520by%250Ausing%2520an%2520auxiliary%2520self-supervised%2520task%2520to%2520reduce%2520the%2520domain%2520gap%2520caused%2520by%250Adistribution%2520shifts%2520between%2520the%2520source%2520and%2520target.%2520However%252C%2520we%2520find%2520that%2520when%250Amodels%2520are%2520required%2520to%2520perform%2520multiple%2520tasks%2520under%2520domain%2520shifts%252C%2520conventional%250ATTT%2520methods%2520suffer%2520from%2520unsynchronized%2520task%2520behavior%252C%2520where%2520the%2520adaptation%250Asteps%2520needed%2520for%2520optimal%2520performance%2520in%2520one%2520task%2520may%2520not%2520align%2520with%2520the%250Arequirements%2520of%2520other%2520tasks.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520TTT%2520approach%250Acalled%2520Synchronizing%2520Tasks%2520for%2520Test-time%2520Training%2520%2528S4T%2529%252C%2520which%2520enables%2520the%250Aconcurrent%2520handling%2520of%2520multiple%2520tasks.%2520The%2520core%2520idea%2520behind%2520S4T%2520is%2520that%250Apredicting%2520task%2520relations%2520across%2520domain%2520shifts%2520is%2520key%2520to%2520synchronizing%2520tasks%250Aduring%2520test%2520time.%2520To%2520validate%2520our%2520approach%252C%2520we%2520apply%2520S4T%2520to%2520conventional%250Amulti-task%2520benchmarks%252C%2520integrating%2520it%2520with%2520traditional%2520TTT%2520protocols.%2520Our%250Aempirical%2520results%2520show%2520that%2520S4T%2520outperforms%2520state-of-the-art%2520TTT%2520methods%2520across%250Avarious%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synchronizing%20Task%20Behavior%3A%20Aligning%20Multiple%20Tasks%20during%20Test-Time%0A%20%20Training&entry.906535625=Wooseong%20Jeong%20and%20Jegyeong%20Cho%20and%20Youngho%20Yoon%20and%20Kuk-Jin%20Yoon&entry.1292438233=%20%20Generalizing%20neural%20networks%20to%20unseen%20target%20domains%20is%20a%20significant%0Achallenge%20in%20real-world%20deployments.%20Test-time%20training%20%28TTT%29%20addresses%20this%20by%0Ausing%20an%20auxiliary%20self-supervised%20task%20to%20reduce%20the%20domain%20gap%20caused%20by%0Adistribution%20shifts%20between%20the%20source%20and%20target.%20However%2C%20we%20find%20that%20when%0Amodels%20are%20required%20to%20perform%20multiple%20tasks%20under%20domain%20shifts%2C%20conventional%0ATTT%20methods%20suffer%20from%20unsynchronized%20task%20behavior%2C%20where%20the%20adaptation%0Asteps%20needed%20for%20optimal%20performance%20in%20one%20task%20may%20not%20align%20with%20the%0Arequirements%20of%20other%20tasks.%20To%20address%20this%2C%20we%20propose%20a%20novel%20TTT%20approach%0Acalled%20Synchronizing%20Tasks%20for%20Test-time%20Training%20%28S4T%29%2C%20which%20enables%20the%0Aconcurrent%20handling%20of%20multiple%20tasks.%20The%20core%20idea%20behind%20S4T%20is%20that%0Apredicting%20task%20relations%20across%20domain%20shifts%20is%20key%20to%20synchronizing%20tasks%0Aduring%20test%20time.%20To%20validate%20our%20approach%2C%20we%20apply%20S4T%20to%20conventional%0Amulti-task%20benchmarks%2C%20integrating%20it%20with%20traditional%20TTT%20protocols.%20Our%0Aempirical%20results%20show%20that%20S4T%20outperforms%20state-of-the-art%20TTT%20methods%20across%0Avarious%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07778v1&entry.124074799=Read"},
{"title": "S2FGL: Spatial Spectral Federated Graph Learning", "author": "Zihan Tan and Suyuan Huang and Guancheng Wan and Wenke Huang and He Li and Mang Ye", "abstract": "  Federated Graph Learning (FGL) combines the privacy-preserving capabilities\nof federated learning (FL) with the strong graph modeling capability of Graph\nNeural Networks (GNNs). Current research addresses subgraph-FL only from the\nstructural perspective, neglecting the propagation of graph signals on spatial\nand spectral domains of the structure. From a spatial perspective, subgraph-FL\nintroduces edge disconnections between clients, leading to disruptions in label\nsignals and a degradation in the class knowledge of the global GNN. From a\nspectral perspective, spectral heterogeneity causes inconsistencies in signal\nfrequencies across subgraphs, which makes local GNNs overfit the local signal\npropagation schemes. As a result, spectral client drifts occur, undermining\nglobal generalizability. To tackle the challenges, we propose a global\nknowledge repository to mitigate label signal disruption and a frequency\nalignment to address spectral client drifts. The combination of spatial and\nspectral strategies forms our framework S2FGL. Extensive experiments on\nmultiple datasets demonstrate the superiority of S2FGL. The code is available\nat https://github.com/Wonder7racer/S2FGL.git.\n", "link": "http://arxiv.org/abs/2507.02409v2", "date": "2025-07-10", "relevancy": 2.3903, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.498}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4743}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S2FGL%3A%20Spatial%20Spectral%20Federated%20Graph%20Learning&body=Title%3A%20S2FGL%3A%20Spatial%20Spectral%20Federated%20Graph%20Learning%0AAuthor%3A%20Zihan%20Tan%20and%20Suyuan%20Huang%20and%20Guancheng%20Wan%20and%20Wenke%20Huang%20and%20He%20Li%20and%20Mang%20Ye%0AAbstract%3A%20%20%20Federated%20Graph%20Learning%20%28FGL%29%20combines%20the%20privacy-preserving%20capabilities%0Aof%20federated%20learning%20%28FL%29%20with%20the%20strong%20graph%20modeling%20capability%20of%20Graph%0ANeural%20Networks%20%28GNNs%29.%20Current%20research%20addresses%20subgraph-FL%20only%20from%20the%0Astructural%20perspective%2C%20neglecting%20the%20propagation%20of%20graph%20signals%20on%20spatial%0Aand%20spectral%20domains%20of%20the%20structure.%20From%20a%20spatial%20perspective%2C%20subgraph-FL%0Aintroduces%20edge%20disconnections%20between%20clients%2C%20leading%20to%20disruptions%20in%20label%0Asignals%20and%20a%20degradation%20in%20the%20class%20knowledge%20of%20the%20global%20GNN.%20From%20a%0Aspectral%20perspective%2C%20spectral%20heterogeneity%20causes%20inconsistencies%20in%20signal%0Afrequencies%20across%20subgraphs%2C%20which%20makes%20local%20GNNs%20overfit%20the%20local%20signal%0Apropagation%20schemes.%20As%20a%20result%2C%20spectral%20client%20drifts%20occur%2C%20undermining%0Aglobal%20generalizability.%20To%20tackle%20the%20challenges%2C%20we%20propose%20a%20global%0Aknowledge%20repository%20to%20mitigate%20label%20signal%20disruption%20and%20a%20frequency%0Aalignment%20to%20address%20spectral%20client%20drifts.%20The%20combination%20of%20spatial%20and%0Aspectral%20strategies%20forms%20our%20framework%20S2FGL.%20Extensive%20experiments%20on%0Amultiple%20datasets%20demonstrate%20the%20superiority%20of%20S2FGL.%20The%20code%20is%20available%0Aat%20https%3A//github.com/Wonder7racer/S2FGL.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02409v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS2FGL%253A%2520Spatial%2520Spectral%2520Federated%2520Graph%2520Learning%26entry.906535625%3DZihan%2520Tan%2520and%2520Suyuan%2520Huang%2520and%2520Guancheng%2520Wan%2520and%2520Wenke%2520Huang%2520and%2520He%2520Li%2520and%2520Mang%2520Ye%26entry.1292438233%3D%2520%2520Federated%2520Graph%2520Learning%2520%2528FGL%2529%2520combines%2520the%2520privacy-preserving%2520capabilities%250Aof%2520federated%2520learning%2520%2528FL%2529%2520with%2520the%2520strong%2520graph%2520modeling%2520capability%2520of%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529.%2520Current%2520research%2520addresses%2520subgraph-FL%2520only%2520from%2520the%250Astructural%2520perspective%252C%2520neglecting%2520the%2520propagation%2520of%2520graph%2520signals%2520on%2520spatial%250Aand%2520spectral%2520domains%2520of%2520the%2520structure.%2520From%2520a%2520spatial%2520perspective%252C%2520subgraph-FL%250Aintroduces%2520edge%2520disconnections%2520between%2520clients%252C%2520leading%2520to%2520disruptions%2520in%2520label%250Asignals%2520and%2520a%2520degradation%2520in%2520the%2520class%2520knowledge%2520of%2520the%2520global%2520GNN.%2520From%2520a%250Aspectral%2520perspective%252C%2520spectral%2520heterogeneity%2520causes%2520inconsistencies%2520in%2520signal%250Afrequencies%2520across%2520subgraphs%252C%2520which%2520makes%2520local%2520GNNs%2520overfit%2520the%2520local%2520signal%250Apropagation%2520schemes.%2520As%2520a%2520result%252C%2520spectral%2520client%2520drifts%2520occur%252C%2520undermining%250Aglobal%2520generalizability.%2520To%2520tackle%2520the%2520challenges%252C%2520we%2520propose%2520a%2520global%250Aknowledge%2520repository%2520to%2520mitigate%2520label%2520signal%2520disruption%2520and%2520a%2520frequency%250Aalignment%2520to%2520address%2520spectral%2520client%2520drifts.%2520The%2520combination%2520of%2520spatial%2520and%250Aspectral%2520strategies%2520forms%2520our%2520framework%2520S2FGL.%2520Extensive%2520experiments%2520on%250Amultiple%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520S2FGL.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/Wonder7racer/S2FGL.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02409v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S2FGL%3A%20Spatial%20Spectral%20Federated%20Graph%20Learning&entry.906535625=Zihan%20Tan%20and%20Suyuan%20Huang%20and%20Guancheng%20Wan%20and%20Wenke%20Huang%20and%20He%20Li%20and%20Mang%20Ye&entry.1292438233=%20%20Federated%20Graph%20Learning%20%28FGL%29%20combines%20the%20privacy-preserving%20capabilities%0Aof%20federated%20learning%20%28FL%29%20with%20the%20strong%20graph%20modeling%20capability%20of%20Graph%0ANeural%20Networks%20%28GNNs%29.%20Current%20research%20addresses%20subgraph-FL%20only%20from%20the%0Astructural%20perspective%2C%20neglecting%20the%20propagation%20of%20graph%20signals%20on%20spatial%0Aand%20spectral%20domains%20of%20the%20structure.%20From%20a%20spatial%20perspective%2C%20subgraph-FL%0Aintroduces%20edge%20disconnections%20between%20clients%2C%20leading%20to%20disruptions%20in%20label%0Asignals%20and%20a%20degradation%20in%20the%20class%20knowledge%20of%20the%20global%20GNN.%20From%20a%0Aspectral%20perspective%2C%20spectral%20heterogeneity%20causes%20inconsistencies%20in%20signal%0Afrequencies%20across%20subgraphs%2C%20which%20makes%20local%20GNNs%20overfit%20the%20local%20signal%0Apropagation%20schemes.%20As%20a%20result%2C%20spectral%20client%20drifts%20occur%2C%20undermining%0Aglobal%20generalizability.%20To%20tackle%20the%20challenges%2C%20we%20propose%20a%20global%0Aknowledge%20repository%20to%20mitigate%20label%20signal%20disruption%20and%20a%20frequency%0Aalignment%20to%20address%20spectral%20client%20drifts.%20The%20combination%20of%20spatial%20and%0Aspectral%20strategies%20forms%20our%20framework%20S2FGL.%20Extensive%20experiments%20on%0Amultiple%20datasets%20demonstrate%20the%20superiority%20of%20S2FGL.%20The%20code%20is%20available%0Aat%20https%3A//github.com/Wonder7racer/S2FGL.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02409v2&entry.124074799=Read"},
{"title": "ArteryX: Advancing Brain Artery Feature Extraction with Vessel-Fused\n  Networks and a Robust Validation Framework", "author": "Abrar Faiyaz and Nhat Hoang and Giovanni Schifitto and Md Nasir Uddin", "abstract": "  Cerebrovascular pathology significantly contributes to cognitive decline and\nneurological disorders, underscoring the need for advanced tools to assess\nvascular integrity. Three-dimensional Time-of-Flight Magnetic Resonance\nAngiography (3D TOF MRA) is widely used to visualize cerebral vasculature,\nhowever, clinical evaluations generally focus on major arterial abnormalities,\noverlooking quantitative metrics critical for understanding subtle vascular\nchanges. Existing methods for extracting structural, geometrical and\nmorphological arterial features from MRA - whether manual or automated - face\nchallenges including user-dependent variability, steep learning curves, and\nlack of standardized quantitative validations. We propose a novel\nsemi-supervised artery evaluation framework, named ArteryX, a MATLAB-based\ntoolbox that quantifies vascular features with high accuracy and efficiency,\nachieving processing times ~10-15 minutes per subject at 0.5 mm resolution with\nminimal user intervention. ArteryX employs a vessel-fused network based\nlandmarking approach to reliably track and manage tracings, effectively\naddressing the issue of dangling/disconnected vessels. Validation on human\nsubjects with cerebral small vessel disease demonstrated its improved\nsensitivity to subtle vascular changes and better performance than an existing\nsemi-automated method. Importantly, the ArteryX toolbox enables quantitative\nfeature validation by integrating an in-vivo like artery simulation framework\nutilizing vessel-fused graph nodes and predefined ground-truth features for\nspecific artery types. Thus, the ArteryX framework holds promise for\nbenchmarking feature extraction toolboxes and for seamless integration into\nclinical workflows, enabling early detection of cerebrovascular pathology and\nstandardized comparisons across patient cohorts to advance understanding of\nvascular contributions to brain health.\n", "link": "http://arxiv.org/abs/2507.07920v1", "date": "2025-07-10", "relevancy": 2.361, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArteryX%3A%20Advancing%20Brain%20Artery%20Feature%20Extraction%20with%20Vessel-Fused%0A%20%20Networks%20and%20a%20Robust%20Validation%20Framework&body=Title%3A%20ArteryX%3A%20Advancing%20Brain%20Artery%20Feature%20Extraction%20with%20Vessel-Fused%0A%20%20Networks%20and%20a%20Robust%20Validation%20Framework%0AAuthor%3A%20Abrar%20Faiyaz%20and%20Nhat%20Hoang%20and%20Giovanni%20Schifitto%20and%20Md%20Nasir%20Uddin%0AAbstract%3A%20%20%20Cerebrovascular%20pathology%20significantly%20contributes%20to%20cognitive%20decline%20and%0Aneurological%20disorders%2C%20underscoring%20the%20need%20for%20advanced%20tools%20to%20assess%0Avascular%20integrity.%20Three-dimensional%20Time-of-Flight%20Magnetic%20Resonance%0AAngiography%20%283D%20TOF%20MRA%29%20is%20widely%20used%20to%20visualize%20cerebral%20vasculature%2C%0Ahowever%2C%20clinical%20evaluations%20generally%20focus%20on%20major%20arterial%20abnormalities%2C%0Aoverlooking%20quantitative%20metrics%20critical%20for%20understanding%20subtle%20vascular%0Achanges.%20Existing%20methods%20for%20extracting%20structural%2C%20geometrical%20and%0Amorphological%20arterial%20features%20from%20MRA%20-%20whether%20manual%20or%20automated%20-%20face%0Achallenges%20including%20user-dependent%20variability%2C%20steep%20learning%20curves%2C%20and%0Alack%20of%20standardized%20quantitative%20validations.%20We%20propose%20a%20novel%0Asemi-supervised%20artery%20evaluation%20framework%2C%20named%20ArteryX%2C%20a%20MATLAB-based%0Atoolbox%20that%20quantifies%20vascular%20features%20with%20high%20accuracy%20and%20efficiency%2C%0Aachieving%20processing%20times%20~10-15%20minutes%20per%20subject%20at%200.5%20mm%20resolution%20with%0Aminimal%20user%20intervention.%20ArteryX%20employs%20a%20vessel-fused%20network%20based%0Alandmarking%20approach%20to%20reliably%20track%20and%20manage%20tracings%2C%20effectively%0Aaddressing%20the%20issue%20of%20dangling/disconnected%20vessels.%20Validation%20on%20human%0Asubjects%20with%20cerebral%20small%20vessel%20disease%20demonstrated%20its%20improved%0Asensitivity%20to%20subtle%20vascular%20changes%20and%20better%20performance%20than%20an%20existing%0Asemi-automated%20method.%20Importantly%2C%20the%20ArteryX%20toolbox%20enables%20quantitative%0Afeature%20validation%20by%20integrating%20an%20in-vivo%20like%20artery%20simulation%20framework%0Autilizing%20vessel-fused%20graph%20nodes%20and%20predefined%20ground-truth%20features%20for%0Aspecific%20artery%20types.%20Thus%2C%20the%20ArteryX%20framework%20holds%20promise%20for%0Abenchmarking%20feature%20extraction%20toolboxes%20and%20for%20seamless%20integration%20into%0Aclinical%20workflows%2C%20enabling%20early%20detection%20of%20cerebrovascular%20pathology%20and%0Astandardized%20comparisons%20across%20patient%20cohorts%20to%20advance%20understanding%20of%0Avascular%20contributions%20to%20brain%20health.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArteryX%253A%2520Advancing%2520Brain%2520Artery%2520Feature%2520Extraction%2520with%2520Vessel-Fused%250A%2520%2520Networks%2520and%2520a%2520Robust%2520Validation%2520Framework%26entry.906535625%3DAbrar%2520Faiyaz%2520and%2520Nhat%2520Hoang%2520and%2520Giovanni%2520Schifitto%2520and%2520Md%2520Nasir%2520Uddin%26entry.1292438233%3D%2520%2520Cerebrovascular%2520pathology%2520significantly%2520contributes%2520to%2520cognitive%2520decline%2520and%250Aneurological%2520disorders%252C%2520underscoring%2520the%2520need%2520for%2520advanced%2520tools%2520to%2520assess%250Avascular%2520integrity.%2520Three-dimensional%2520Time-of-Flight%2520Magnetic%2520Resonance%250AAngiography%2520%25283D%2520TOF%2520MRA%2529%2520is%2520widely%2520used%2520to%2520visualize%2520cerebral%2520vasculature%252C%250Ahowever%252C%2520clinical%2520evaluations%2520generally%2520focus%2520on%2520major%2520arterial%2520abnormalities%252C%250Aoverlooking%2520quantitative%2520metrics%2520critical%2520for%2520understanding%2520subtle%2520vascular%250Achanges.%2520Existing%2520methods%2520for%2520extracting%2520structural%252C%2520geometrical%2520and%250Amorphological%2520arterial%2520features%2520from%2520MRA%2520-%2520whether%2520manual%2520or%2520automated%2520-%2520face%250Achallenges%2520including%2520user-dependent%2520variability%252C%2520steep%2520learning%2520curves%252C%2520and%250Alack%2520of%2520standardized%2520quantitative%2520validations.%2520We%2520propose%2520a%2520novel%250Asemi-supervised%2520artery%2520evaluation%2520framework%252C%2520named%2520ArteryX%252C%2520a%2520MATLAB-based%250Atoolbox%2520that%2520quantifies%2520vascular%2520features%2520with%2520high%2520accuracy%2520and%2520efficiency%252C%250Aachieving%2520processing%2520times%2520~10-15%2520minutes%2520per%2520subject%2520at%25200.5%2520mm%2520resolution%2520with%250Aminimal%2520user%2520intervention.%2520ArteryX%2520employs%2520a%2520vessel-fused%2520network%2520based%250Alandmarking%2520approach%2520to%2520reliably%2520track%2520and%2520manage%2520tracings%252C%2520effectively%250Aaddressing%2520the%2520issue%2520of%2520dangling/disconnected%2520vessels.%2520Validation%2520on%2520human%250Asubjects%2520with%2520cerebral%2520small%2520vessel%2520disease%2520demonstrated%2520its%2520improved%250Asensitivity%2520to%2520subtle%2520vascular%2520changes%2520and%2520better%2520performance%2520than%2520an%2520existing%250Asemi-automated%2520method.%2520Importantly%252C%2520the%2520ArteryX%2520toolbox%2520enables%2520quantitative%250Afeature%2520validation%2520by%2520integrating%2520an%2520in-vivo%2520like%2520artery%2520simulation%2520framework%250Autilizing%2520vessel-fused%2520graph%2520nodes%2520and%2520predefined%2520ground-truth%2520features%2520for%250Aspecific%2520artery%2520types.%2520Thus%252C%2520the%2520ArteryX%2520framework%2520holds%2520promise%2520for%250Abenchmarking%2520feature%2520extraction%2520toolboxes%2520and%2520for%2520seamless%2520integration%2520into%250Aclinical%2520workflows%252C%2520enabling%2520early%2520detection%2520of%2520cerebrovascular%2520pathology%2520and%250Astandardized%2520comparisons%2520across%2520patient%2520cohorts%2520to%2520advance%2520understanding%2520of%250Avascular%2520contributions%2520to%2520brain%2520health.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArteryX%3A%20Advancing%20Brain%20Artery%20Feature%20Extraction%20with%20Vessel-Fused%0A%20%20Networks%20and%20a%20Robust%20Validation%20Framework&entry.906535625=Abrar%20Faiyaz%20and%20Nhat%20Hoang%20and%20Giovanni%20Schifitto%20and%20Md%20Nasir%20Uddin&entry.1292438233=%20%20Cerebrovascular%20pathology%20significantly%20contributes%20to%20cognitive%20decline%20and%0Aneurological%20disorders%2C%20underscoring%20the%20need%20for%20advanced%20tools%20to%20assess%0Avascular%20integrity.%20Three-dimensional%20Time-of-Flight%20Magnetic%20Resonance%0AAngiography%20%283D%20TOF%20MRA%29%20is%20widely%20used%20to%20visualize%20cerebral%20vasculature%2C%0Ahowever%2C%20clinical%20evaluations%20generally%20focus%20on%20major%20arterial%20abnormalities%2C%0Aoverlooking%20quantitative%20metrics%20critical%20for%20understanding%20subtle%20vascular%0Achanges.%20Existing%20methods%20for%20extracting%20structural%2C%20geometrical%20and%0Amorphological%20arterial%20features%20from%20MRA%20-%20whether%20manual%20or%20automated%20-%20face%0Achallenges%20including%20user-dependent%20variability%2C%20steep%20learning%20curves%2C%20and%0Alack%20of%20standardized%20quantitative%20validations.%20We%20propose%20a%20novel%0Asemi-supervised%20artery%20evaluation%20framework%2C%20named%20ArteryX%2C%20a%20MATLAB-based%0Atoolbox%20that%20quantifies%20vascular%20features%20with%20high%20accuracy%20and%20efficiency%2C%0Aachieving%20processing%20times%20~10-15%20minutes%20per%20subject%20at%200.5%20mm%20resolution%20with%0Aminimal%20user%20intervention.%20ArteryX%20employs%20a%20vessel-fused%20network%20based%0Alandmarking%20approach%20to%20reliably%20track%20and%20manage%20tracings%2C%20effectively%0Aaddressing%20the%20issue%20of%20dangling/disconnected%20vessels.%20Validation%20on%20human%0Asubjects%20with%20cerebral%20small%20vessel%20disease%20demonstrated%20its%20improved%0Asensitivity%20to%20subtle%20vascular%20changes%20and%20better%20performance%20than%20an%20existing%0Asemi-automated%20method.%20Importantly%2C%20the%20ArteryX%20toolbox%20enables%20quantitative%0Afeature%20validation%20by%20integrating%20an%20in-vivo%20like%20artery%20simulation%20framework%0Autilizing%20vessel-fused%20graph%20nodes%20and%20predefined%20ground-truth%20features%20for%0Aspecific%20artery%20types.%20Thus%2C%20the%20ArteryX%20framework%20holds%20promise%20for%0Abenchmarking%20feature%20extraction%20toolboxes%20and%20for%20seamless%20integration%20into%0Aclinical%20workflows%2C%20enabling%20early%20detection%20of%20cerebrovascular%20pathology%20and%0Astandardized%20comparisons%20across%20patient%20cohorts%20to%20advance%20understanding%20of%0Avascular%20contributions%20to%20brain%20health.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07920v1&entry.124074799=Read"},
{"title": "PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle\n  CT", "author": "Yi Liu and Yiyang Wen and Zekun Zhou and Junqi Ma and Linghang Wang and Yucheng Yao and Liu Shi and Qiegen Liu", "abstract": "  Generative diffusion models have received increasing attention in medical\nimaging, particularly in limited-angle computed tomography (LACT). Standard\ndiffusion models achieve high-quality image reconstruction but require a large\nnumber of sampling steps during inference, resulting in substantial\ncomputational overhead. Although skip-sampling strategies have been proposed to\nimprove efficiency, they often lead to loss of fine structural details. To\naddress this issue, we propose a prior information embedding and wavelet\nfeature fusion fast sampling diffusion model for LACT reconstruction. The PWD\nenables efficient sampling while preserving reconstruction fidelity in LACT,\nand effectively mitigates the degradation typically introduced by\nskip-sampling. Specifically, during the training phase, PWD maps the\ndistribution of LACT images to that of fully sampled target images, enabling\nthe model to learn structural correspondences between them. During inference,\nthe LACT image serves as an explicit prior to guide the sampling trajectory,\nallowing for high-quality reconstruction with significantly fewer steps. In\naddition, PWD performs multi-scale feature fusion in the wavelet domain,\neffectively enhancing the reconstruction of fine details by leveraging both\nlow-frequency and high-frequency information. Quantitative and qualitative\nevaluations on clinical dental arch CBCT and periapical datasets demonstrate\nthat PWD outperforms existing methods under the same sampling condition. Using\nonly 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and\n10% gain in SSIM.\n", "link": "http://arxiv.org/abs/2507.05317v2", "date": "2025-07-10", "relevancy": 2.3521, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6164}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5823}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PWD%3A%20Prior-Guided%20and%20Wavelet-Enhanced%20Diffusion%20Model%20for%20Limited-Angle%0A%20%20CT&body=Title%3A%20PWD%3A%20Prior-Guided%20and%20Wavelet-Enhanced%20Diffusion%20Model%20for%20Limited-Angle%0A%20%20CT%0AAuthor%3A%20Yi%20Liu%20and%20Yiyang%20Wen%20and%20Zekun%20Zhou%20and%20Junqi%20Ma%20and%20Linghang%20Wang%20and%20Yucheng%20Yao%20and%20Liu%20Shi%20and%20Qiegen%20Liu%0AAbstract%3A%20%20%20Generative%20diffusion%20models%20have%20received%20increasing%20attention%20in%20medical%0Aimaging%2C%20particularly%20in%20limited-angle%20computed%20tomography%20%28LACT%29.%20Standard%0Adiffusion%20models%20achieve%20high-quality%20image%20reconstruction%20but%20require%20a%20large%0Anumber%20of%20sampling%20steps%20during%20inference%2C%20resulting%20in%20substantial%0Acomputational%20overhead.%20Although%20skip-sampling%20strategies%20have%20been%20proposed%20to%0Aimprove%20efficiency%2C%20they%20often%20lead%20to%20loss%20of%20fine%20structural%20details.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20prior%20information%20embedding%20and%20wavelet%0Afeature%20fusion%20fast%20sampling%20diffusion%20model%20for%20LACT%20reconstruction.%20The%20PWD%0Aenables%20efficient%20sampling%20while%20preserving%20reconstruction%20fidelity%20in%20LACT%2C%0Aand%20effectively%20mitigates%20the%20degradation%20typically%20introduced%20by%0Askip-sampling.%20Specifically%2C%20during%20the%20training%20phase%2C%20PWD%20maps%20the%0Adistribution%20of%20LACT%20images%20to%20that%20of%20fully%20sampled%20target%20images%2C%20enabling%0Athe%20model%20to%20learn%20structural%20correspondences%20between%20them.%20During%20inference%2C%0Athe%20LACT%20image%20serves%20as%20an%20explicit%20prior%20to%20guide%20the%20sampling%20trajectory%2C%0Aallowing%20for%20high-quality%20reconstruction%20with%20significantly%20fewer%20steps.%20In%0Aaddition%2C%20PWD%20performs%20multi-scale%20feature%20fusion%20in%20the%20wavelet%20domain%2C%0Aeffectively%20enhancing%20the%20reconstruction%20of%20fine%20details%20by%20leveraging%20both%0Alow-frequency%20and%20high-frequency%20information.%20Quantitative%20and%20qualitative%0Aevaluations%20on%20clinical%20dental%20arch%20CBCT%20and%20periapical%20datasets%20demonstrate%0Athat%20PWD%20outperforms%20existing%20methods%20under%20the%20same%20sampling%20condition.%20Using%0Aonly%2050%20sampling%20steps%2C%20PWD%20achieves%20at%20least%201.7%20dB%20improvement%20in%20PSNR%20and%0A10%25%20gain%20in%20SSIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPWD%253A%2520Prior-Guided%2520and%2520Wavelet-Enhanced%2520Diffusion%2520Model%2520for%2520Limited-Angle%250A%2520%2520CT%26entry.906535625%3DYi%2520Liu%2520and%2520Yiyang%2520Wen%2520and%2520Zekun%2520Zhou%2520and%2520Junqi%2520Ma%2520and%2520Linghang%2520Wang%2520and%2520Yucheng%2520Yao%2520and%2520Liu%2520Shi%2520and%2520Qiegen%2520Liu%26entry.1292438233%3D%2520%2520Generative%2520diffusion%2520models%2520have%2520received%2520increasing%2520attention%2520in%2520medical%250Aimaging%252C%2520particularly%2520in%2520limited-angle%2520computed%2520tomography%2520%2528LACT%2529.%2520Standard%250Adiffusion%2520models%2520achieve%2520high-quality%2520image%2520reconstruction%2520but%2520require%2520a%2520large%250Anumber%2520of%2520sampling%2520steps%2520during%2520inference%252C%2520resulting%2520in%2520substantial%250Acomputational%2520overhead.%2520Although%2520skip-sampling%2520strategies%2520have%2520been%2520proposed%2520to%250Aimprove%2520efficiency%252C%2520they%2520often%2520lead%2520to%2520loss%2520of%2520fine%2520structural%2520details.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520prior%2520information%2520embedding%2520and%2520wavelet%250Afeature%2520fusion%2520fast%2520sampling%2520diffusion%2520model%2520for%2520LACT%2520reconstruction.%2520The%2520PWD%250Aenables%2520efficient%2520sampling%2520while%2520preserving%2520reconstruction%2520fidelity%2520in%2520LACT%252C%250Aand%2520effectively%2520mitigates%2520the%2520degradation%2520typically%2520introduced%2520by%250Askip-sampling.%2520Specifically%252C%2520during%2520the%2520training%2520phase%252C%2520PWD%2520maps%2520the%250Adistribution%2520of%2520LACT%2520images%2520to%2520that%2520of%2520fully%2520sampled%2520target%2520images%252C%2520enabling%250Athe%2520model%2520to%2520learn%2520structural%2520correspondences%2520between%2520them.%2520During%2520inference%252C%250Athe%2520LACT%2520image%2520serves%2520as%2520an%2520explicit%2520prior%2520to%2520guide%2520the%2520sampling%2520trajectory%252C%250Aallowing%2520for%2520high-quality%2520reconstruction%2520with%2520significantly%2520fewer%2520steps.%2520In%250Aaddition%252C%2520PWD%2520performs%2520multi-scale%2520feature%2520fusion%2520in%2520the%2520wavelet%2520domain%252C%250Aeffectively%2520enhancing%2520the%2520reconstruction%2520of%2520fine%2520details%2520by%2520leveraging%2520both%250Alow-frequency%2520and%2520high-frequency%2520information.%2520Quantitative%2520and%2520qualitative%250Aevaluations%2520on%2520clinical%2520dental%2520arch%2520CBCT%2520and%2520periapical%2520datasets%2520demonstrate%250Athat%2520PWD%2520outperforms%2520existing%2520methods%2520under%2520the%2520same%2520sampling%2520condition.%2520Using%250Aonly%252050%2520sampling%2520steps%252C%2520PWD%2520achieves%2520at%2520least%25201.7%2520dB%2520improvement%2520in%2520PSNR%2520and%250A10%2525%2520gain%2520in%2520SSIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PWD%3A%20Prior-Guided%20and%20Wavelet-Enhanced%20Diffusion%20Model%20for%20Limited-Angle%0A%20%20CT&entry.906535625=Yi%20Liu%20and%20Yiyang%20Wen%20and%20Zekun%20Zhou%20and%20Junqi%20Ma%20and%20Linghang%20Wang%20and%20Yucheng%20Yao%20and%20Liu%20Shi%20and%20Qiegen%20Liu&entry.1292438233=%20%20Generative%20diffusion%20models%20have%20received%20increasing%20attention%20in%20medical%0Aimaging%2C%20particularly%20in%20limited-angle%20computed%20tomography%20%28LACT%29.%20Standard%0Adiffusion%20models%20achieve%20high-quality%20image%20reconstruction%20but%20require%20a%20large%0Anumber%20of%20sampling%20steps%20during%20inference%2C%20resulting%20in%20substantial%0Acomputational%20overhead.%20Although%20skip-sampling%20strategies%20have%20been%20proposed%20to%0Aimprove%20efficiency%2C%20they%20often%20lead%20to%20loss%20of%20fine%20structural%20details.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20prior%20information%20embedding%20and%20wavelet%0Afeature%20fusion%20fast%20sampling%20diffusion%20model%20for%20LACT%20reconstruction.%20The%20PWD%0Aenables%20efficient%20sampling%20while%20preserving%20reconstruction%20fidelity%20in%20LACT%2C%0Aand%20effectively%20mitigates%20the%20degradation%20typically%20introduced%20by%0Askip-sampling.%20Specifically%2C%20during%20the%20training%20phase%2C%20PWD%20maps%20the%0Adistribution%20of%20LACT%20images%20to%20that%20of%20fully%20sampled%20target%20images%2C%20enabling%0Athe%20model%20to%20learn%20structural%20correspondences%20between%20them.%20During%20inference%2C%0Athe%20LACT%20image%20serves%20as%20an%20explicit%20prior%20to%20guide%20the%20sampling%20trajectory%2C%0Aallowing%20for%20high-quality%20reconstruction%20with%20significantly%20fewer%20steps.%20In%0Aaddition%2C%20PWD%20performs%20multi-scale%20feature%20fusion%20in%20the%20wavelet%20domain%2C%0Aeffectively%20enhancing%20the%20reconstruction%20of%20fine%20details%20by%20leveraging%20both%0Alow-frequency%20and%20high-frequency%20information.%20Quantitative%20and%20qualitative%0Aevaluations%20on%20clinical%20dental%20arch%20CBCT%20and%20periapical%20datasets%20demonstrate%0Athat%20PWD%20outperforms%20existing%20methods%20under%20the%20same%20sampling%20condition.%20Using%0Aonly%2050%20sampling%20steps%2C%20PWD%20achieves%20at%20least%201.7%20dB%20improvement%20in%20PSNR%20and%0A10%25%20gain%20in%20SSIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05317v2&entry.124074799=Read"},
{"title": "Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought", "author": "Shin'ya Yamaguchi and Kosuke Nishida and Daiki Chijiwa", "abstract": "  Large vision-language models (LVLMs) have demonstrated remarkable\ncapabilities by integrating pre-trained vision encoders with large language\nmodels (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting\nhas been adapted for LVLMs to enhance multi-modal reasoning by generating\nintermediate rationales based on visual and textual inputs. While CoT is\nassumed to improve grounding and accuracy in LVLMs, our experiments reveal a\nkey challenge: existing LVLMs often ignore the contents of generated rationales\nin CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as\na KL-constrained reward maximization focused on rationale-conditional\nlog-likelihood. As the optimal solution, we propose rationale-enhanced decoding\n(RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes\nvisual and rationale information by multiplying distinct image-conditional and\nrationale-conditional next token distributions. Extensive experiments show that\nRED consistently and significantly improves reasoning over standard CoT and\nother decoding methods across multiple benchmarks and LVLMs. Our work offers a\npractical and effective approach to improve both the faithfulness and accuracy\nof CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded\nmulti-modal systems.\n", "link": "http://arxiv.org/abs/2507.07685v1", "date": "2025-07-10", "relevancy": 2.3458, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5999}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rationale-Enhanced%20Decoding%20for%20Multi-modal%20Chain-of-Thought&body=Title%3A%20Rationale-Enhanced%20Decoding%20for%20Multi-modal%20Chain-of-Thought%0AAuthor%3A%20Shin%27ya%20Yamaguchi%20and%20Kosuke%20Nishida%20and%20Daiki%20Chijiwa%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20by%20integrating%20pre-trained%20vision%20encoders%20with%20large%20language%0Amodels%20%28LLMs%29.%20Similar%20to%20single-modal%20LLMs%2C%20chain-of-thought%20%28CoT%29%20prompting%0Ahas%20been%20adapted%20for%20LVLMs%20to%20enhance%20multi-modal%20reasoning%20by%20generating%0Aintermediate%20rationales%20based%20on%20visual%20and%20textual%20inputs.%20While%20CoT%20is%0Aassumed%20to%20improve%20grounding%20and%20accuracy%20in%20LVLMs%2C%20our%20experiments%20reveal%20a%0Akey%20challenge%3A%20existing%20LVLMs%20often%20ignore%20the%20contents%20of%20generated%20rationales%0Ain%20CoT%20reasoning.%20To%20address%20this%2C%20we%20re-formulate%20multi-modal%20CoT%20reasoning%20as%0Aa%20KL-constrained%20reward%20maximization%20focused%20on%20rationale-conditional%0Alog-likelihood.%20As%20the%20optimal%20solution%2C%20we%20propose%20rationale-enhanced%20decoding%0A%28RED%29%2C%20a%20novel%20plug-and-play%20inference-time%20decoding%20strategy.%20RED%20harmonizes%0Avisual%20and%20rationale%20information%20by%20multiplying%20distinct%20image-conditional%20and%0Arationale-conditional%20next%20token%20distributions.%20Extensive%20experiments%20show%20that%0ARED%20consistently%20and%20significantly%20improves%20reasoning%20over%20standard%20CoT%20and%0Aother%20decoding%20methods%20across%20multiple%20benchmarks%20and%20LVLMs.%20Our%20work%20offers%20a%0Apractical%20and%20effective%20approach%20to%20improve%20both%20the%20faithfulness%20and%20accuracy%0Aof%20CoT%20reasoning%20in%20LVLMs%2C%20paving%20the%20way%20for%20more%20reliable%20rationale-grounded%0Amulti-modal%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRationale-Enhanced%2520Decoding%2520for%2520Multi-modal%2520Chain-of-Thought%26entry.906535625%3DShin%2527ya%2520Yamaguchi%2520and%2520Kosuke%2520Nishida%2520and%2520Daiki%2520Chijiwa%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520by%2520integrating%2520pre-trained%2520vision%2520encoders%2520with%2520large%2520language%250Amodels%2520%2528LLMs%2529.%2520Similar%2520to%2520single-modal%2520LLMs%252C%2520chain-of-thought%2520%2528CoT%2529%2520prompting%250Ahas%2520been%2520adapted%2520for%2520LVLMs%2520to%2520enhance%2520multi-modal%2520reasoning%2520by%2520generating%250Aintermediate%2520rationales%2520based%2520on%2520visual%2520and%2520textual%2520inputs.%2520While%2520CoT%2520is%250Aassumed%2520to%2520improve%2520grounding%2520and%2520accuracy%2520in%2520LVLMs%252C%2520our%2520experiments%2520reveal%2520a%250Akey%2520challenge%253A%2520existing%2520LVLMs%2520often%2520ignore%2520the%2520contents%2520of%2520generated%2520rationales%250Ain%2520CoT%2520reasoning.%2520To%2520address%2520this%252C%2520we%2520re-formulate%2520multi-modal%2520CoT%2520reasoning%2520as%250Aa%2520KL-constrained%2520reward%2520maximization%2520focused%2520on%2520rationale-conditional%250Alog-likelihood.%2520As%2520the%2520optimal%2520solution%252C%2520we%2520propose%2520rationale-enhanced%2520decoding%250A%2528RED%2529%252C%2520a%2520novel%2520plug-and-play%2520inference-time%2520decoding%2520strategy.%2520RED%2520harmonizes%250Avisual%2520and%2520rationale%2520information%2520by%2520multiplying%2520distinct%2520image-conditional%2520and%250Arationale-conditional%2520next%2520token%2520distributions.%2520Extensive%2520experiments%2520show%2520that%250ARED%2520consistently%2520and%2520significantly%2520improves%2520reasoning%2520over%2520standard%2520CoT%2520and%250Aother%2520decoding%2520methods%2520across%2520multiple%2520benchmarks%2520and%2520LVLMs.%2520Our%2520work%2520offers%2520a%250Apractical%2520and%2520effective%2520approach%2520to%2520improve%2520both%2520the%2520faithfulness%2520and%2520accuracy%250Aof%2520CoT%2520reasoning%2520in%2520LVLMs%252C%2520paving%2520the%2520way%2520for%2520more%2520reliable%2520rationale-grounded%250Amulti-modal%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rationale-Enhanced%20Decoding%20for%20Multi-modal%20Chain-of-Thought&entry.906535625=Shin%27ya%20Yamaguchi%20and%20Kosuke%20Nishida%20and%20Daiki%20Chijiwa&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20by%20integrating%20pre-trained%20vision%20encoders%20with%20large%20language%0Amodels%20%28LLMs%29.%20Similar%20to%20single-modal%20LLMs%2C%20chain-of-thought%20%28CoT%29%20prompting%0Ahas%20been%20adapted%20for%20LVLMs%20to%20enhance%20multi-modal%20reasoning%20by%20generating%0Aintermediate%20rationales%20based%20on%20visual%20and%20textual%20inputs.%20While%20CoT%20is%0Aassumed%20to%20improve%20grounding%20and%20accuracy%20in%20LVLMs%2C%20our%20experiments%20reveal%20a%0Akey%20challenge%3A%20existing%20LVLMs%20often%20ignore%20the%20contents%20of%20generated%20rationales%0Ain%20CoT%20reasoning.%20To%20address%20this%2C%20we%20re-formulate%20multi-modal%20CoT%20reasoning%20as%0Aa%20KL-constrained%20reward%20maximization%20focused%20on%20rationale-conditional%0Alog-likelihood.%20As%20the%20optimal%20solution%2C%20we%20propose%20rationale-enhanced%20decoding%0A%28RED%29%2C%20a%20novel%20plug-and-play%20inference-time%20decoding%20strategy.%20RED%20harmonizes%0Avisual%20and%20rationale%20information%20by%20multiplying%20distinct%20image-conditional%20and%0Arationale-conditional%20next%20token%20distributions.%20Extensive%20experiments%20show%20that%0ARED%20consistently%20and%20significantly%20improves%20reasoning%20over%20standard%20CoT%20and%0Aother%20decoding%20methods%20across%20multiple%20benchmarks%20and%20LVLMs.%20Our%20work%20offers%20a%0Apractical%20and%20effective%20approach%20to%20improve%20both%20the%20faithfulness%20and%20accuracy%0Aof%20CoT%20reasoning%20in%20LVLMs%2C%20paving%20the%20way%20for%20more%20reliable%20rationale-grounded%0Amulti-modal%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07685v1&entry.124074799=Read"},
{"title": "Balancing the Past and Present: A Coordinated Replay Framework for\n  Federated Class-Incremental Learning", "author": "Zhuang Qi and Lei Meng and Han Yu", "abstract": "  Federated Class Incremental Learning (FCIL) aims to collaboratively process\ncontinuously increasing incoming tasks across multiple clients. Among various\napproaches, data replay has become a promising solution, which can alleviate\nforgetting by reintroducing representative samples from previous tasks.\nHowever, their performance is typically limited by class imbalance, both within\nthe replay buffer due to limited global awareness and between replayed and\nnewly arrived classes. To address this issue, we propose a class wise balancing\ndata replay method for FCIL (FedCBDR), which employs a global coordination\nmechanism for class-level memory construction and reweights the learning\nobjective to alleviate the aforementioned imbalances. Specifically, FedCBDR has\ntwo key components: 1) the global-perspective data replay module reconstructs\nglobal representations of prior task in a privacy-preserving manner, which then\nguides a class-aware and importance-sensitive sampling strategy to achieve\nbalanced replay; 2) Subsequently, to handle class imbalance across tasks, the\ntask aware temperature scaling module adaptively adjusts the temperature of\nlogits at both class and instance levels based on task dynamics, which reduces\nthe model's overconfidence in majority classes while enhancing its sensitivity\nto minority classes. Experimental results verified that FedCBDR achieves\nbalanced class-wise sampling under heterogeneous data distributions and\nimproves generalization under task imbalance between earlier and recent tasks,\nyielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2507.07712v1", "date": "2025-07-10", "relevancy": 2.3412, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.504}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.454}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balancing%20the%20Past%20and%20Present%3A%20A%20Coordinated%20Replay%20Framework%20for%0A%20%20Federated%20Class-Incremental%20Learning&body=Title%3A%20Balancing%20the%20Past%20and%20Present%3A%20A%20Coordinated%20Replay%20Framework%20for%0A%20%20Federated%20Class-Incremental%20Learning%0AAuthor%3A%20Zhuang%20Qi%20and%20Lei%20Meng%20and%20Han%20Yu%0AAbstract%3A%20%20%20Federated%20Class%20Incremental%20Learning%20%28FCIL%29%20aims%20to%20collaboratively%20process%0Acontinuously%20increasing%20incoming%20tasks%20across%20multiple%20clients.%20Among%20various%0Aapproaches%2C%20data%20replay%20has%20become%20a%20promising%20solution%2C%20which%20can%20alleviate%0Aforgetting%20by%20reintroducing%20representative%20samples%20from%20previous%20tasks.%0AHowever%2C%20their%20performance%20is%20typically%20limited%20by%20class%20imbalance%2C%20both%20within%0Athe%20replay%20buffer%20due%20to%20limited%20global%20awareness%20and%20between%20replayed%20and%0Anewly%20arrived%20classes.%20To%20address%20this%20issue%2C%20we%20propose%20a%20class%20wise%20balancing%0Adata%20replay%20method%20for%20FCIL%20%28FedCBDR%29%2C%20which%20employs%20a%20global%20coordination%0Amechanism%20for%20class-level%20memory%20construction%20and%20reweights%20the%20learning%0Aobjective%20to%20alleviate%20the%20aforementioned%20imbalances.%20Specifically%2C%20FedCBDR%20has%0Atwo%20key%20components%3A%201%29%20the%20global-perspective%20data%20replay%20module%20reconstructs%0Aglobal%20representations%20of%20prior%20task%20in%20a%20privacy-preserving%20manner%2C%20which%20then%0Aguides%20a%20class-aware%20and%20importance-sensitive%20sampling%20strategy%20to%20achieve%0Abalanced%20replay%3B%202%29%20Subsequently%2C%20to%20handle%20class%20imbalance%20across%20tasks%2C%20the%0Atask%20aware%20temperature%20scaling%20module%20adaptively%20adjusts%20the%20temperature%20of%0Alogits%20at%20both%20class%20and%20instance%20levels%20based%20on%20task%20dynamics%2C%20which%20reduces%0Athe%20model%27s%20overconfidence%20in%20majority%20classes%20while%20enhancing%20its%20sensitivity%0Ato%20minority%20classes.%20Experimental%20results%20verified%20that%20FedCBDR%20achieves%0Abalanced%20class-wise%20sampling%20under%20heterogeneous%20data%20distributions%20and%0Aimproves%20generalization%20under%20task%20imbalance%20between%20earlier%20and%20recent%20tasks%2C%0Ayielding%20a%202%25-15%25%20Top-1%20accuracy%20improvement%20over%20six%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancing%2520the%2520Past%2520and%2520Present%253A%2520A%2520Coordinated%2520Replay%2520Framework%2520for%250A%2520%2520Federated%2520Class-Incremental%2520Learning%26entry.906535625%3DZhuang%2520Qi%2520and%2520Lei%2520Meng%2520and%2520Han%2520Yu%26entry.1292438233%3D%2520%2520Federated%2520Class%2520Incremental%2520Learning%2520%2528FCIL%2529%2520aims%2520to%2520collaboratively%2520process%250Acontinuously%2520increasing%2520incoming%2520tasks%2520across%2520multiple%2520clients.%2520Among%2520various%250Aapproaches%252C%2520data%2520replay%2520has%2520become%2520a%2520promising%2520solution%252C%2520which%2520can%2520alleviate%250Aforgetting%2520by%2520reintroducing%2520representative%2520samples%2520from%2520previous%2520tasks.%250AHowever%252C%2520their%2520performance%2520is%2520typically%2520limited%2520by%2520class%2520imbalance%252C%2520both%2520within%250Athe%2520replay%2520buffer%2520due%2520to%2520limited%2520global%2520awareness%2520and%2520between%2520replayed%2520and%250Anewly%2520arrived%2520classes.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520class%2520wise%2520balancing%250Adata%2520replay%2520method%2520for%2520FCIL%2520%2528FedCBDR%2529%252C%2520which%2520employs%2520a%2520global%2520coordination%250Amechanism%2520for%2520class-level%2520memory%2520construction%2520and%2520reweights%2520the%2520learning%250Aobjective%2520to%2520alleviate%2520the%2520aforementioned%2520imbalances.%2520Specifically%252C%2520FedCBDR%2520has%250Atwo%2520key%2520components%253A%25201%2529%2520the%2520global-perspective%2520data%2520replay%2520module%2520reconstructs%250Aglobal%2520representations%2520of%2520prior%2520task%2520in%2520a%2520privacy-preserving%2520manner%252C%2520which%2520then%250Aguides%2520a%2520class-aware%2520and%2520importance-sensitive%2520sampling%2520strategy%2520to%2520achieve%250Abalanced%2520replay%253B%25202%2529%2520Subsequently%252C%2520to%2520handle%2520class%2520imbalance%2520across%2520tasks%252C%2520the%250Atask%2520aware%2520temperature%2520scaling%2520module%2520adaptively%2520adjusts%2520the%2520temperature%2520of%250Alogits%2520at%2520both%2520class%2520and%2520instance%2520levels%2520based%2520on%2520task%2520dynamics%252C%2520which%2520reduces%250Athe%2520model%2527s%2520overconfidence%2520in%2520majority%2520classes%2520while%2520enhancing%2520its%2520sensitivity%250Ato%2520minority%2520classes.%2520Experimental%2520results%2520verified%2520that%2520FedCBDR%2520achieves%250Abalanced%2520class-wise%2520sampling%2520under%2520heterogeneous%2520data%2520distributions%2520and%250Aimproves%2520generalization%2520under%2520task%2520imbalance%2520between%2520earlier%2520and%2520recent%2520tasks%252C%250Ayielding%2520a%25202%2525-15%2525%2520Top-1%2520accuracy%2520improvement%2520over%2520six%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20the%20Past%20and%20Present%3A%20A%20Coordinated%20Replay%20Framework%20for%0A%20%20Federated%20Class-Incremental%20Learning&entry.906535625=Zhuang%20Qi%20and%20Lei%20Meng%20and%20Han%20Yu&entry.1292438233=%20%20Federated%20Class%20Incremental%20Learning%20%28FCIL%29%20aims%20to%20collaboratively%20process%0Acontinuously%20increasing%20incoming%20tasks%20across%20multiple%20clients.%20Among%20various%0Aapproaches%2C%20data%20replay%20has%20become%20a%20promising%20solution%2C%20which%20can%20alleviate%0Aforgetting%20by%20reintroducing%20representative%20samples%20from%20previous%20tasks.%0AHowever%2C%20their%20performance%20is%20typically%20limited%20by%20class%20imbalance%2C%20both%20within%0Athe%20replay%20buffer%20due%20to%20limited%20global%20awareness%20and%20between%20replayed%20and%0Anewly%20arrived%20classes.%20To%20address%20this%20issue%2C%20we%20propose%20a%20class%20wise%20balancing%0Adata%20replay%20method%20for%20FCIL%20%28FedCBDR%29%2C%20which%20employs%20a%20global%20coordination%0Amechanism%20for%20class-level%20memory%20construction%20and%20reweights%20the%20learning%0Aobjective%20to%20alleviate%20the%20aforementioned%20imbalances.%20Specifically%2C%20FedCBDR%20has%0Atwo%20key%20components%3A%201%29%20the%20global-perspective%20data%20replay%20module%20reconstructs%0Aglobal%20representations%20of%20prior%20task%20in%20a%20privacy-preserving%20manner%2C%20which%20then%0Aguides%20a%20class-aware%20and%20importance-sensitive%20sampling%20strategy%20to%20achieve%0Abalanced%20replay%3B%202%29%20Subsequently%2C%20to%20handle%20class%20imbalance%20across%20tasks%2C%20the%0Atask%20aware%20temperature%20scaling%20module%20adaptively%20adjusts%20the%20temperature%20of%0Alogits%20at%20both%20class%20and%20instance%20levels%20based%20on%20task%20dynamics%2C%20which%20reduces%0Athe%20model%27s%20overconfidence%20in%20majority%20classes%20while%20enhancing%20its%20sensitivity%0Ato%20minority%20classes.%20Experimental%20results%20verified%20that%20FedCBDR%20achieves%0Abalanced%20class-wise%20sampling%20under%20heterogeneous%20data%20distributions%20and%0Aimproves%20generalization%20under%20task%20imbalance%20between%20earlier%20and%20recent%20tasks%2C%0Ayielding%20a%202%25-15%25%20Top-1%20accuracy%20improvement%20over%20six%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07712v1&entry.124074799=Read"},
{"title": "ViLU: Learning Vision-Language Uncertainties for Failure Prediction", "author": "Marc Lafon and Yannis Karmim and Julio Silva-Rodriguez and Paul Couairon and Cl\u00e9ment Rambour and Rapha\u00ebl Fournier-Sniehotta and Ismail Ben Ayed and Jose Dolz and Nicolas Thome", "abstract": "  Reliable Uncertainty Quantification (UQ) and failure prediction remain open\nchallenges for Vision-Language Models (VLMs). We introduce ViLU, a new\nVision-Language Uncertainty quantification framework that contextualizes\nuncertainty estimates by leveraging all task-relevant textual representations.\nViLU constructs an uncertainty-aware multi-modal representation by integrating\nthe visual embedding, the predicted textual embedding, and an image-conditioned\ntextual representation via cross-attention. Unlike traditional UQ methods based\non loss prediction, ViLU trains an uncertainty predictor as a binary classifier\nto distinguish correct from incorrect predictions using a weighted binary\ncross-entropy loss, making it loss-agnostic. In particular, our proposed\napproach is well-suited for post-hoc settings, where only vision and text\nembeddings are available without direct access to the model itself. Extensive\nexperiments on diverse datasets show the significant gains of our method\ncompared to state-of-the-art failure prediction methods. We apply our method to\nstandard classification datasets, such as ImageNet-1k, as well as large-scale\nimage-caption datasets like CC12M and LAION-400M. Ablation studies highlight\nthe critical role of our architecture and training in achieving effective\nuncertainty quantification. Our code is publicly available and can be found\nhere: https://github.com/ykrmm/ViLU.\n", "link": "http://arxiv.org/abs/2507.07620v1", "date": "2025-07-10", "relevancy": 2.3403, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6032}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6023}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViLU%3A%20Learning%20Vision-Language%20Uncertainties%20for%20Failure%20Prediction&body=Title%3A%20ViLU%3A%20Learning%20Vision-Language%20Uncertainties%20for%20Failure%20Prediction%0AAuthor%3A%20Marc%20Lafon%20and%20Yannis%20Karmim%20and%20Julio%20Silva-Rodriguez%20and%20Paul%20Couairon%20and%20Cl%C3%A9ment%20Rambour%20and%20Rapha%C3%ABl%20Fournier-Sniehotta%20and%20Ismail%20Ben%20Ayed%20and%20Jose%20Dolz%20and%20Nicolas%20Thome%0AAbstract%3A%20%20%20Reliable%20Uncertainty%20Quantification%20%28UQ%29%20and%20failure%20prediction%20remain%20open%0Achallenges%20for%20Vision-Language%20Models%20%28VLMs%29.%20We%20introduce%20ViLU%2C%20a%20new%0AVision-Language%20Uncertainty%20quantification%20framework%20that%20contextualizes%0Auncertainty%20estimates%20by%20leveraging%20all%20task-relevant%20textual%20representations.%0AViLU%20constructs%20an%20uncertainty-aware%20multi-modal%20representation%20by%20integrating%0Athe%20visual%20embedding%2C%20the%20predicted%20textual%20embedding%2C%20and%20an%20image-conditioned%0Atextual%20representation%20via%20cross-attention.%20Unlike%20traditional%20UQ%20methods%20based%0Aon%20loss%20prediction%2C%20ViLU%20trains%20an%20uncertainty%20predictor%20as%20a%20binary%20classifier%0Ato%20distinguish%20correct%20from%20incorrect%20predictions%20using%20a%20weighted%20binary%0Across-entropy%20loss%2C%20making%20it%20loss-agnostic.%20In%20particular%2C%20our%20proposed%0Aapproach%20is%20well-suited%20for%20post-hoc%20settings%2C%20where%20only%20vision%20and%20text%0Aembeddings%20are%20available%20without%20direct%20access%20to%20the%20model%20itself.%20Extensive%0Aexperiments%20on%20diverse%20datasets%20show%20the%20significant%20gains%20of%20our%20method%0Acompared%20to%20state-of-the-art%20failure%20prediction%20methods.%20We%20apply%20our%20method%20to%0Astandard%20classification%20datasets%2C%20such%20as%20ImageNet-1k%2C%20as%20well%20as%20large-scale%0Aimage-caption%20datasets%20like%20CC12M%20and%20LAION-400M.%20Ablation%20studies%20highlight%0Athe%20critical%20role%20of%20our%20architecture%20and%20training%20in%20achieving%20effective%0Auncertainty%20quantification.%20Our%20code%20is%20publicly%20available%20and%20can%20be%20found%0Ahere%3A%20https%3A//github.com/ykrmm/ViLU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViLU%253A%2520Learning%2520Vision-Language%2520Uncertainties%2520for%2520Failure%2520Prediction%26entry.906535625%3DMarc%2520Lafon%2520and%2520Yannis%2520Karmim%2520and%2520Julio%2520Silva-Rodriguez%2520and%2520Paul%2520Couairon%2520and%2520Cl%25C3%25A9ment%2520Rambour%2520and%2520Rapha%25C3%25ABl%2520Fournier-Sniehotta%2520and%2520Ismail%2520Ben%2520Ayed%2520and%2520Jose%2520Dolz%2520and%2520Nicolas%2520Thome%26entry.1292438233%3D%2520%2520Reliable%2520Uncertainty%2520Quantification%2520%2528UQ%2529%2520and%2520failure%2520prediction%2520remain%2520open%250Achallenges%2520for%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520We%2520introduce%2520ViLU%252C%2520a%2520new%250AVision-Language%2520Uncertainty%2520quantification%2520framework%2520that%2520contextualizes%250Auncertainty%2520estimates%2520by%2520leveraging%2520all%2520task-relevant%2520textual%2520representations.%250AViLU%2520constructs%2520an%2520uncertainty-aware%2520multi-modal%2520representation%2520by%2520integrating%250Athe%2520visual%2520embedding%252C%2520the%2520predicted%2520textual%2520embedding%252C%2520and%2520an%2520image-conditioned%250Atextual%2520representation%2520via%2520cross-attention.%2520Unlike%2520traditional%2520UQ%2520methods%2520based%250Aon%2520loss%2520prediction%252C%2520ViLU%2520trains%2520an%2520uncertainty%2520predictor%2520as%2520a%2520binary%2520classifier%250Ato%2520distinguish%2520correct%2520from%2520incorrect%2520predictions%2520using%2520a%2520weighted%2520binary%250Across-entropy%2520loss%252C%2520making%2520it%2520loss-agnostic.%2520In%2520particular%252C%2520our%2520proposed%250Aapproach%2520is%2520well-suited%2520for%2520post-hoc%2520settings%252C%2520where%2520only%2520vision%2520and%2520text%250Aembeddings%2520are%2520available%2520without%2520direct%2520access%2520to%2520the%2520model%2520itself.%2520Extensive%250Aexperiments%2520on%2520diverse%2520datasets%2520show%2520the%2520significant%2520gains%2520of%2520our%2520method%250Acompared%2520to%2520state-of-the-art%2520failure%2520prediction%2520methods.%2520We%2520apply%2520our%2520method%2520to%250Astandard%2520classification%2520datasets%252C%2520such%2520as%2520ImageNet-1k%252C%2520as%2520well%2520as%2520large-scale%250Aimage-caption%2520datasets%2520like%2520CC12M%2520and%2520LAION-400M.%2520Ablation%2520studies%2520highlight%250Athe%2520critical%2520role%2520of%2520our%2520architecture%2520and%2520training%2520in%2520achieving%2520effective%250Auncertainty%2520quantification.%2520Our%2520code%2520is%2520publicly%2520available%2520and%2520can%2520be%2520found%250Ahere%253A%2520https%253A//github.com/ykrmm/ViLU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViLU%3A%20Learning%20Vision-Language%20Uncertainties%20for%20Failure%20Prediction&entry.906535625=Marc%20Lafon%20and%20Yannis%20Karmim%20and%20Julio%20Silva-Rodriguez%20and%20Paul%20Couairon%20and%20Cl%C3%A9ment%20Rambour%20and%20Rapha%C3%ABl%20Fournier-Sniehotta%20and%20Ismail%20Ben%20Ayed%20and%20Jose%20Dolz%20and%20Nicolas%20Thome&entry.1292438233=%20%20Reliable%20Uncertainty%20Quantification%20%28UQ%29%20and%20failure%20prediction%20remain%20open%0Achallenges%20for%20Vision-Language%20Models%20%28VLMs%29.%20We%20introduce%20ViLU%2C%20a%20new%0AVision-Language%20Uncertainty%20quantification%20framework%20that%20contextualizes%0Auncertainty%20estimates%20by%20leveraging%20all%20task-relevant%20textual%20representations.%0AViLU%20constructs%20an%20uncertainty-aware%20multi-modal%20representation%20by%20integrating%0Athe%20visual%20embedding%2C%20the%20predicted%20textual%20embedding%2C%20and%20an%20image-conditioned%0Atextual%20representation%20via%20cross-attention.%20Unlike%20traditional%20UQ%20methods%20based%0Aon%20loss%20prediction%2C%20ViLU%20trains%20an%20uncertainty%20predictor%20as%20a%20binary%20classifier%0Ato%20distinguish%20correct%20from%20incorrect%20predictions%20using%20a%20weighted%20binary%0Across-entropy%20loss%2C%20making%20it%20loss-agnostic.%20In%20particular%2C%20our%20proposed%0Aapproach%20is%20well-suited%20for%20post-hoc%20settings%2C%20where%20only%20vision%20and%20text%0Aembeddings%20are%20available%20without%20direct%20access%20to%20the%20model%20itself.%20Extensive%0Aexperiments%20on%20diverse%20datasets%20show%20the%20significant%20gains%20of%20our%20method%0Acompared%20to%20state-of-the-art%20failure%20prediction%20methods.%20We%20apply%20our%20method%20to%0Astandard%20classification%20datasets%2C%20such%20as%20ImageNet-1k%2C%20as%20well%20as%20large-scale%0Aimage-caption%20datasets%20like%20CC12M%20and%20LAION-400M.%20Ablation%20studies%20highlight%0Athe%20critical%20role%20of%20our%20architecture%20and%20training%20in%20achieving%20effective%0Auncertainty%20quantification.%20Our%20code%20is%20publicly%20available%20and%20can%20be%20found%0Ahere%3A%20https%3A//github.com/ykrmm/ViLU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07620v1&entry.124074799=Read"},
{"title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for\n  World Models", "author": "Keyon Vafa and Peter G. Chang and Ashesh Rambachan and Sendhil Mullainathan", "abstract": "  Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.\n", "link": "http://arxiv.org/abs/2507.06952v2", "date": "2025-07-10", "relevancy": 2.3324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6001}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Has%20a%20Foundation%20Model%20Found%3F%20Using%20Inductive%20Bias%20to%20Probe%20for%0A%20%20World%20Models&body=Title%3A%20What%20Has%20a%20Foundation%20Model%20Found%3F%20Using%20Inductive%20Bias%20to%20Probe%20for%0A%20%20World%20Models%0AAuthor%3A%20Keyon%20Vafa%20and%20Peter%20G.%20Chang%20and%20Ashesh%20Rambachan%20and%20Sendhil%20Mullainathan%0AAbstract%3A%20%20%20Foundation%20models%20are%20premised%20on%20the%20idea%20that%20sequence%20prediction%20can%0Auncover%20deeper%20domain%20understanding%2C%20much%20like%20how%20Kepler%27s%20predictions%20of%0Aplanetary%20motion%20later%20led%20to%20the%20discovery%20of%20Newtonian%20mechanics.%20However%2C%0Aevaluating%20whether%20these%20models%20truly%20capture%20deeper%20structure%20remains%20a%0Achallenge.%20We%20develop%20a%20technique%20for%20evaluating%20foundation%20models%20that%0Aexamines%20how%20they%20adapt%20to%20synthetic%20datasets%20generated%20from%20some%20postulated%0Aworld%20model.%20Our%20technique%20measures%20whether%20the%20foundation%20model%27s%20inductive%0Abias%20aligns%20with%20the%20world%20model%2C%20and%20so%20we%20refer%20to%20it%20as%20an%20inductive%20bias%0Aprobe.%20Across%20multiple%20domains%2C%20we%20find%20that%20foundation%20models%20can%20excel%20at%0Atheir%20training%20tasks%20yet%20fail%20to%20develop%20inductive%20biases%20towards%20the%0Aunderlying%20world%20model%20when%20adapted%20to%20new%20tasks.%20We%20particularly%20find%20that%0Afoundation%20models%20trained%20on%20orbital%20trajectories%20consistently%20fail%20to%20apply%0ANewtonian%20mechanics%20when%20adapted%20to%20new%20physics%20tasks.%20Further%20analysis%20reveals%0Athat%20these%20models%20behave%20as%20if%20they%20develop%20task-specific%20heuristics%20that%20fail%0Ato%20generalize.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06952v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Has%2520a%2520Foundation%2520Model%2520Found%253F%2520Using%2520Inductive%2520Bias%2520to%2520Probe%2520for%250A%2520%2520World%2520Models%26entry.906535625%3DKeyon%2520Vafa%2520and%2520Peter%2520G.%2520Chang%2520and%2520Ashesh%2520Rambachan%2520and%2520Sendhil%2520Mullainathan%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520premised%2520on%2520the%2520idea%2520that%2520sequence%2520prediction%2520can%250Auncover%2520deeper%2520domain%2520understanding%252C%2520much%2520like%2520how%2520Kepler%2527s%2520predictions%2520of%250Aplanetary%2520motion%2520later%2520led%2520to%2520the%2520discovery%2520of%2520Newtonian%2520mechanics.%2520However%252C%250Aevaluating%2520whether%2520these%2520models%2520truly%2520capture%2520deeper%2520structure%2520remains%2520a%250Achallenge.%2520We%2520develop%2520a%2520technique%2520for%2520evaluating%2520foundation%2520models%2520that%250Aexamines%2520how%2520they%2520adapt%2520to%2520synthetic%2520datasets%2520generated%2520from%2520some%2520postulated%250Aworld%2520model.%2520Our%2520technique%2520measures%2520whether%2520the%2520foundation%2520model%2527s%2520inductive%250Abias%2520aligns%2520with%2520the%2520world%2520model%252C%2520and%2520so%2520we%2520refer%2520to%2520it%2520as%2520an%2520inductive%2520bias%250Aprobe.%2520Across%2520multiple%2520domains%252C%2520we%2520find%2520that%2520foundation%2520models%2520can%2520excel%2520at%250Atheir%2520training%2520tasks%2520yet%2520fail%2520to%2520develop%2520inductive%2520biases%2520towards%2520the%250Aunderlying%2520world%2520model%2520when%2520adapted%2520to%2520new%2520tasks.%2520We%2520particularly%2520find%2520that%250Afoundation%2520models%2520trained%2520on%2520orbital%2520trajectories%2520consistently%2520fail%2520to%2520apply%250ANewtonian%2520mechanics%2520when%2520adapted%2520to%2520new%2520physics%2520tasks.%2520Further%2520analysis%2520reveals%250Athat%2520these%2520models%2520behave%2520as%2520if%2520they%2520develop%2520task-specific%2520heuristics%2520that%2520fail%250Ato%2520generalize.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06952v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Has%20a%20Foundation%20Model%20Found%3F%20Using%20Inductive%20Bias%20to%20Probe%20for%0A%20%20World%20Models&entry.906535625=Keyon%20Vafa%20and%20Peter%20G.%20Chang%20and%20Ashesh%20Rambachan%20and%20Sendhil%20Mullainathan&entry.1292438233=%20%20Foundation%20models%20are%20premised%20on%20the%20idea%20that%20sequence%20prediction%20can%0Auncover%20deeper%20domain%20understanding%2C%20much%20like%20how%20Kepler%27s%20predictions%20of%0Aplanetary%20motion%20later%20led%20to%20the%20discovery%20of%20Newtonian%20mechanics.%20However%2C%0Aevaluating%20whether%20these%20models%20truly%20capture%20deeper%20structure%20remains%20a%0Achallenge.%20We%20develop%20a%20technique%20for%20evaluating%20foundation%20models%20that%0Aexamines%20how%20they%20adapt%20to%20synthetic%20datasets%20generated%20from%20some%20postulated%0Aworld%20model.%20Our%20technique%20measures%20whether%20the%20foundation%20model%27s%20inductive%0Abias%20aligns%20with%20the%20world%20model%2C%20and%20so%20we%20refer%20to%20it%20as%20an%20inductive%20bias%0Aprobe.%20Across%20multiple%20domains%2C%20we%20find%20that%20foundation%20models%20can%20excel%20at%0Atheir%20training%20tasks%20yet%20fail%20to%20develop%20inductive%20biases%20towards%20the%0Aunderlying%20world%20model%20when%20adapted%20to%20new%20tasks.%20We%20particularly%20find%20that%0Afoundation%20models%20trained%20on%20orbital%20trajectories%20consistently%20fail%20to%20apply%0ANewtonian%20mechanics%20when%20adapted%20to%20new%20physics%20tasks.%20Further%20analysis%20reveals%0Athat%20these%20models%20behave%20as%20if%20they%20develop%20task-specific%20heuristics%20that%20fail%0Ato%20generalize.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06952v2&entry.124074799=Read"},
{"title": "SpatialViz-Bench: Automatically Generated Spatial Visualization\n  Reasoning Tasks for MLLMs", "author": "Siting Wang and Luoyang Sun and Cheng Deng and Kun Shao and Minnan Pei and Zheng Tian and Haifeng Zhang and Jun Wang", "abstract": "  Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models exhibit unexpected behaviors\nby showing difficulty perception that misaligns with human intuition,\ndisplaying dramatic 2D-to-3D performance cliffs, and defaulting to formula\nderivation despite spatial tasks requiring visualization alone. SpatialVizBench\nempirically demonstrates that state-of-the-art MLLMs continue to exhibit\ndeficiencies in spatial visualization tasks, thereby addressing a significant\nlacuna in the field. The benchmark is publicly available.\n", "link": "http://arxiv.org/abs/2507.07610v1", "date": "2025-07-10", "relevancy": 2.3234, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5901}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5901}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialViz-Bench%3A%20Automatically%20Generated%20Spatial%20Visualization%0A%20%20Reasoning%20Tasks%20for%20MLLMs&body=Title%3A%20SpatialViz-Bench%3A%20Automatically%20Generated%20Spatial%20Visualization%0A%20%20Reasoning%20Tasks%20for%20MLLMs%0AAuthor%3A%20Siting%20Wang%20and%20Luoyang%20Sun%20and%20Cheng%20Deng%20and%20Kun%20Shao%20and%20Minnan%20Pei%20and%20Zheng%20Tian%20and%20Haifeng%20Zhang%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Humans%20can%20directly%20imagine%20and%20manipulate%20visual%20images%20in%20their%20minds%2C%20a%0Acapability%20known%20as%20spatial%20visualization.%20While%20multi-modal%20Large%20Language%0AModels%20%28MLLMs%29%20support%20imagination-based%20reasoning%2C%20spatial%20visualization%0Aremains%20insufficiently%20evaluated%2C%20typically%20embedded%20within%20broader%0Amathematical%20and%20logical%20assessments.%20Existing%20evaluations%20often%20rely%20on%20IQ%0Atests%20or%20math%20competitions%20that%20may%20overlap%20with%20training%20data%2C%20compromising%0Aassessment%20reliability.%20To%20this%20end%2C%20we%20introduce%20SpatialViz-Bench%2C%20a%0Acomprehensive%20multi-modal%20benchmark%20for%20spatial%20visualization%20with%2012%20tasks%0Aacross%204%20sub-abilities%2C%20comprising%201%2C180%20automatically%20generated%20problems.%20Our%0Aevaluation%20of%2033%20state-of-the-art%20MLLMs%20not%20only%20reveals%20wide%20performance%0Avariations%20and%20demonstrates%20the%20benchmark%27s%20strong%20discriminative%20power%2C%20but%0Aalso%20uncovers%20counter-intuitive%20findings%3A%20models%20exhibit%20unexpected%20behaviors%0Aby%20showing%20difficulty%20perception%20that%20misaligns%20with%20human%20intuition%2C%0Adisplaying%20dramatic%202D-to-3D%20performance%20cliffs%2C%20and%20defaulting%20to%20formula%0Aderivation%20despite%20spatial%20tasks%20requiring%20visualization%20alone.%20SpatialVizBench%0Aempirically%20demonstrates%20that%20state-of-the-art%20MLLMs%20continue%20to%20exhibit%0Adeficiencies%20in%20spatial%20visualization%20tasks%2C%20thereby%20addressing%20a%20significant%0Alacuna%20in%20the%20field.%20The%20benchmark%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialViz-Bench%253A%2520Automatically%2520Generated%2520Spatial%2520Visualization%250A%2520%2520Reasoning%2520Tasks%2520for%2520MLLMs%26entry.906535625%3DSiting%2520Wang%2520and%2520Luoyang%2520Sun%2520and%2520Cheng%2520Deng%2520and%2520Kun%2520Shao%2520and%2520Minnan%2520Pei%2520and%2520Zheng%2520Tian%2520and%2520Haifeng%2520Zhang%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Humans%2520can%2520directly%2520imagine%2520and%2520manipulate%2520visual%2520images%2520in%2520their%2520minds%252C%2520a%250Acapability%2520known%2520as%2520spatial%2520visualization.%2520While%2520multi-modal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520support%2520imagination-based%2520reasoning%252C%2520spatial%2520visualization%250Aremains%2520insufficiently%2520evaluated%252C%2520typically%2520embedded%2520within%2520broader%250Amathematical%2520and%2520logical%2520assessments.%2520Existing%2520evaluations%2520often%2520rely%2520on%2520IQ%250Atests%2520or%2520math%2520competitions%2520that%2520may%2520overlap%2520with%2520training%2520data%252C%2520compromising%250Aassessment%2520reliability.%2520To%2520this%2520end%252C%2520we%2520introduce%2520SpatialViz-Bench%252C%2520a%250Acomprehensive%2520multi-modal%2520benchmark%2520for%2520spatial%2520visualization%2520with%252012%2520tasks%250Aacross%25204%2520sub-abilities%252C%2520comprising%25201%252C180%2520automatically%2520generated%2520problems.%2520Our%250Aevaluation%2520of%252033%2520state-of-the-art%2520MLLMs%2520not%2520only%2520reveals%2520wide%2520performance%250Avariations%2520and%2520demonstrates%2520the%2520benchmark%2527s%2520strong%2520discriminative%2520power%252C%2520but%250Aalso%2520uncovers%2520counter-intuitive%2520findings%253A%2520models%2520exhibit%2520unexpected%2520behaviors%250Aby%2520showing%2520difficulty%2520perception%2520that%2520misaligns%2520with%2520human%2520intuition%252C%250Adisplaying%2520dramatic%25202D-to-3D%2520performance%2520cliffs%252C%2520and%2520defaulting%2520to%2520formula%250Aderivation%2520despite%2520spatial%2520tasks%2520requiring%2520visualization%2520alone.%2520SpatialVizBench%250Aempirically%2520demonstrates%2520that%2520state-of-the-art%2520MLLMs%2520continue%2520to%2520exhibit%250Adeficiencies%2520in%2520spatial%2520visualization%2520tasks%252C%2520thereby%2520addressing%2520a%2520significant%250Alacuna%2520in%2520the%2520field.%2520The%2520benchmark%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialViz-Bench%3A%20Automatically%20Generated%20Spatial%20Visualization%0A%20%20Reasoning%20Tasks%20for%20MLLMs&entry.906535625=Siting%20Wang%20and%20Luoyang%20Sun%20and%20Cheng%20Deng%20and%20Kun%20Shao%20and%20Minnan%20Pei%20and%20Zheng%20Tian%20and%20Haifeng%20Zhang%20and%20Jun%20Wang&entry.1292438233=%20%20Humans%20can%20directly%20imagine%20and%20manipulate%20visual%20images%20in%20their%20minds%2C%20a%0Acapability%20known%20as%20spatial%20visualization.%20While%20multi-modal%20Large%20Language%0AModels%20%28MLLMs%29%20support%20imagination-based%20reasoning%2C%20spatial%20visualization%0Aremains%20insufficiently%20evaluated%2C%20typically%20embedded%20within%20broader%0Amathematical%20and%20logical%20assessments.%20Existing%20evaluations%20often%20rely%20on%20IQ%0Atests%20or%20math%20competitions%20that%20may%20overlap%20with%20training%20data%2C%20compromising%0Aassessment%20reliability.%20To%20this%20end%2C%20we%20introduce%20SpatialViz-Bench%2C%20a%0Acomprehensive%20multi-modal%20benchmark%20for%20spatial%20visualization%20with%2012%20tasks%0Aacross%204%20sub-abilities%2C%20comprising%201%2C180%20automatically%20generated%20problems.%20Our%0Aevaluation%20of%2033%20state-of-the-art%20MLLMs%20not%20only%20reveals%20wide%20performance%0Avariations%20and%20demonstrates%20the%20benchmark%27s%20strong%20discriminative%20power%2C%20but%0Aalso%20uncovers%20counter-intuitive%20findings%3A%20models%20exhibit%20unexpected%20behaviors%0Aby%20showing%20difficulty%20perception%20that%20misaligns%20with%20human%20intuition%2C%0Adisplaying%20dramatic%202D-to-3D%20performance%20cliffs%2C%20and%20defaulting%20to%20formula%0Aderivation%20despite%20spatial%20tasks%20requiring%20visualization%20alone.%20SpatialVizBench%0Aempirically%20demonstrates%20that%20state-of-the-art%20MLLMs%20continue%20to%20exhibit%0Adeficiencies%20in%20spatial%20visualization%20tasks%2C%20thereby%20addressing%20a%20significant%0Alacuna%20in%20the%20field.%20The%20benchmark%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07610v1&entry.124074799=Read"},
{"title": "Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and\n  Synthetic Fine-Tuning with Vision Foundation Models", "author": "Zijie Cai and Christopher Metzler", "abstract": "  Monocular depth estimation has recently progressed beyond ordinal depth to\nprovide metric depth predictions. However, its reliability in underwater\nenvironments remains limited due to light attenuation and scattering, color\ndistortion, turbidity, and the lack of high-quality metric ground truth data.\nIn this paper, we present a comprehensive benchmark of zero-shot and fine-tuned\nmonocular metric depth estimation models on real-world underwater datasets with\nmetric depth annotations, including FLSea and SQUID. We evaluated a diverse set\nof state-of-the-art Vision Foundation Models across a range of underwater\nconditions and depth ranges. Our results show that large-scale models trained\non terrestrial data (real or synthetic) are effective in in-air settings, but\nperform poorly underwater due to significant domain shifts. To address this, we\nfine-tune Depth Anything V2 with a ViT-S backbone encoder on a synthetic\nunderwater variant of the Hypersim dataset, which we simulated using a\nphysically based underwater image formation model. Our fine-tuned model\nconsistently improves performance across all benchmarks and outperforms\nbaselines trained only on the clean in-air Hypersim dataset. This study\npresents a detailed evaluation and visualization of monocular metric depth\nestimation in underwater scenes, emphasizing the importance of domain\nadaptation and scale-aware supervision for achieving robust and generalizable\nmetric depth predictions using foundation models in challenging environments.\n", "link": "http://arxiv.org/abs/2507.02148v2", "date": "2025-07-10", "relevancy": 2.3125, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Underwater%20Monocular%20Metric%20Depth%20Estimation%3A%20Real-World%20Benchmarks%20and%0A%20%20Synthetic%20Fine-Tuning%20with%20Vision%20Foundation%20Models&body=Title%3A%20Underwater%20Monocular%20Metric%20Depth%20Estimation%3A%20Real-World%20Benchmarks%20and%0A%20%20Synthetic%20Fine-Tuning%20with%20Vision%20Foundation%20Models%0AAuthor%3A%20Zijie%20Cai%20and%20Christopher%20Metzler%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20has%20recently%20progressed%20beyond%20ordinal%20depth%20to%0Aprovide%20metric%20depth%20predictions.%20However%2C%20its%20reliability%20in%20underwater%0Aenvironments%20remains%20limited%20due%20to%20light%20attenuation%20and%20scattering%2C%20color%0Adistortion%2C%20turbidity%2C%20and%20the%20lack%20of%20high-quality%20metric%20ground%20truth%20data.%0AIn%20this%20paper%2C%20we%20present%20a%20comprehensive%20benchmark%20of%20zero-shot%20and%20fine-tuned%0Amonocular%20metric%20depth%20estimation%20models%20on%20real-world%20underwater%20datasets%20with%0Ametric%20depth%20annotations%2C%20including%20FLSea%20and%20SQUID.%20We%20evaluated%20a%20diverse%20set%0Aof%20state-of-the-art%20Vision%20Foundation%20Models%20across%20a%20range%20of%20underwater%0Aconditions%20and%20depth%20ranges.%20Our%20results%20show%20that%20large-scale%20models%20trained%0Aon%20terrestrial%20data%20%28real%20or%20synthetic%29%20are%20effective%20in%20in-air%20settings%2C%20but%0Aperform%20poorly%20underwater%20due%20to%20significant%20domain%20shifts.%20To%20address%20this%2C%20we%0Afine-tune%20Depth%20Anything%20V2%20with%20a%20ViT-S%20backbone%20encoder%20on%20a%20synthetic%0Aunderwater%20variant%20of%20the%20Hypersim%20dataset%2C%20which%20we%20simulated%20using%20a%0Aphysically%20based%20underwater%20image%20formation%20model.%20Our%20fine-tuned%20model%0Aconsistently%20improves%20performance%20across%20all%20benchmarks%20and%20outperforms%0Abaselines%20trained%20only%20on%20the%20clean%20in-air%20Hypersim%20dataset.%20This%20study%0Apresents%20a%20detailed%20evaluation%20and%20visualization%20of%20monocular%20metric%20depth%0Aestimation%20in%20underwater%20scenes%2C%20emphasizing%20the%20importance%20of%20domain%0Aadaptation%20and%20scale-aware%20supervision%20for%20achieving%20robust%20and%20generalizable%0Ametric%20depth%20predictions%20using%20foundation%20models%20in%20challenging%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02148v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderwater%2520Monocular%2520Metric%2520Depth%2520Estimation%253A%2520Real-World%2520Benchmarks%2520and%250A%2520%2520Synthetic%2520Fine-Tuning%2520with%2520Vision%2520Foundation%2520Models%26entry.906535625%3DZijie%2520Cai%2520and%2520Christopher%2520Metzler%26entry.1292438233%3D%2520%2520Monocular%2520depth%2520estimation%2520has%2520recently%2520progressed%2520beyond%2520ordinal%2520depth%2520to%250Aprovide%2520metric%2520depth%2520predictions.%2520However%252C%2520its%2520reliability%2520in%2520underwater%250Aenvironments%2520remains%2520limited%2520due%2520to%2520light%2520attenuation%2520and%2520scattering%252C%2520color%250Adistortion%252C%2520turbidity%252C%2520and%2520the%2520lack%2520of%2520high-quality%2520metric%2520ground%2520truth%2520data.%250AIn%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520benchmark%2520of%2520zero-shot%2520and%2520fine-tuned%250Amonocular%2520metric%2520depth%2520estimation%2520models%2520on%2520real-world%2520underwater%2520datasets%2520with%250Ametric%2520depth%2520annotations%252C%2520including%2520FLSea%2520and%2520SQUID.%2520We%2520evaluated%2520a%2520diverse%2520set%250Aof%2520state-of-the-art%2520Vision%2520Foundation%2520Models%2520across%2520a%2520range%2520of%2520underwater%250Aconditions%2520and%2520depth%2520ranges.%2520Our%2520results%2520show%2520that%2520large-scale%2520models%2520trained%250Aon%2520terrestrial%2520data%2520%2528real%2520or%2520synthetic%2529%2520are%2520effective%2520in%2520in-air%2520settings%252C%2520but%250Aperform%2520poorly%2520underwater%2520due%2520to%2520significant%2520domain%2520shifts.%2520To%2520address%2520this%252C%2520we%250Afine-tune%2520Depth%2520Anything%2520V2%2520with%2520a%2520ViT-S%2520backbone%2520encoder%2520on%2520a%2520synthetic%250Aunderwater%2520variant%2520of%2520the%2520Hypersim%2520dataset%252C%2520which%2520we%2520simulated%2520using%2520a%250Aphysically%2520based%2520underwater%2520image%2520formation%2520model.%2520Our%2520fine-tuned%2520model%250Aconsistently%2520improves%2520performance%2520across%2520all%2520benchmarks%2520and%2520outperforms%250Abaselines%2520trained%2520only%2520on%2520the%2520clean%2520in-air%2520Hypersim%2520dataset.%2520This%2520study%250Apresents%2520a%2520detailed%2520evaluation%2520and%2520visualization%2520of%2520monocular%2520metric%2520depth%250Aestimation%2520in%2520underwater%2520scenes%252C%2520emphasizing%2520the%2520importance%2520of%2520domain%250Aadaptation%2520and%2520scale-aware%2520supervision%2520for%2520achieving%2520robust%2520and%2520generalizable%250Ametric%2520depth%2520predictions%2520using%2520foundation%2520models%2520in%2520challenging%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02148v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Underwater%20Monocular%20Metric%20Depth%20Estimation%3A%20Real-World%20Benchmarks%20and%0A%20%20Synthetic%20Fine-Tuning%20with%20Vision%20Foundation%20Models&entry.906535625=Zijie%20Cai%20and%20Christopher%20Metzler&entry.1292438233=%20%20Monocular%20depth%20estimation%20has%20recently%20progressed%20beyond%20ordinal%20depth%20to%0Aprovide%20metric%20depth%20predictions.%20However%2C%20its%20reliability%20in%20underwater%0Aenvironments%20remains%20limited%20due%20to%20light%20attenuation%20and%20scattering%2C%20color%0Adistortion%2C%20turbidity%2C%20and%20the%20lack%20of%20high-quality%20metric%20ground%20truth%20data.%0AIn%20this%20paper%2C%20we%20present%20a%20comprehensive%20benchmark%20of%20zero-shot%20and%20fine-tuned%0Amonocular%20metric%20depth%20estimation%20models%20on%20real-world%20underwater%20datasets%20with%0Ametric%20depth%20annotations%2C%20including%20FLSea%20and%20SQUID.%20We%20evaluated%20a%20diverse%20set%0Aof%20state-of-the-art%20Vision%20Foundation%20Models%20across%20a%20range%20of%20underwater%0Aconditions%20and%20depth%20ranges.%20Our%20results%20show%20that%20large-scale%20models%20trained%0Aon%20terrestrial%20data%20%28real%20or%20synthetic%29%20are%20effective%20in%20in-air%20settings%2C%20but%0Aperform%20poorly%20underwater%20due%20to%20significant%20domain%20shifts.%20To%20address%20this%2C%20we%0Afine-tune%20Depth%20Anything%20V2%20with%20a%20ViT-S%20backbone%20encoder%20on%20a%20synthetic%0Aunderwater%20variant%20of%20the%20Hypersim%20dataset%2C%20which%20we%20simulated%20using%20a%0Aphysically%20based%20underwater%20image%20formation%20model.%20Our%20fine-tuned%20model%0Aconsistently%20improves%20performance%20across%20all%20benchmarks%20and%20outperforms%0Abaselines%20trained%20only%20on%20the%20clean%20in-air%20Hypersim%20dataset.%20This%20study%0Apresents%20a%20detailed%20evaluation%20and%20visualization%20of%20monocular%20metric%20depth%0Aestimation%20in%20underwater%20scenes%2C%20emphasizing%20the%20importance%20of%20domain%0Aadaptation%20and%20scale-aware%20supervision%20for%20achieving%20robust%20and%20generalizable%0Ametric%20depth%20predictions%20using%20foundation%20models%20in%20challenging%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02148v2&entry.124074799=Read"},
{"title": "Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound\n  Segmentation", "author": "Fangyijie Wang and Kevin Whelan and F\u00e9lix Balado and Kathleen M. Curran and Gu\u00e9nol\u00e9 Silvestre", "abstract": "  Medical image data is less accessible than in other domains due to privacy\nand regulatory constraints. In addition, labeling requires costly,\ntime-intensive manual image annotation by clinical experts. To overcome these\nchallenges, synthetic medical data generation offers a promising solution.\nGenerative AI (GenAI), employing generative deep learning models, has proven\neffective at producing realistic synthetic images. This study proposes a novel\nmask-guided GenAI approach using diffusion models to generate synthetic fetal\nhead ultrasound images paired with segmentation masks. These synthetic pairs\naugment real datasets for supervised fine-tuning of the Segment Anything Model\n(SAM). Our results show that the synthetic data captures real image features\neffectively, and this approach reaches state-of-the-art fetal head\nsegmentation, especially when trained with a limited number of real image-mask\npairs. In particular, the segmentation reaches Dice Scores of 94.66\\% and\n94.38\\% using a handful of ultrasound images from the Spanish and African\ncohorts, respectively. Our code, models, and data are available on GitHub.\n", "link": "http://arxiv.org/abs/2506.23664v2", "date": "2025-07-10", "relevancy": 2.3115, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5865}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5817}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Model-based%20Data%20Augmentation%20Method%20for%20Fetal%20Head%20Ultrasound%0A%20%20Segmentation&body=Title%3A%20Diffusion%20Model-based%20Data%20Augmentation%20Method%20for%20Fetal%20Head%20Ultrasound%0A%20%20Segmentation%0AAuthor%3A%20Fangyijie%20Wang%20and%20Kevin%20Whelan%20and%20F%C3%A9lix%20Balado%20and%20Kathleen%20M.%20Curran%20and%20Gu%C3%A9nol%C3%A9%20Silvestre%0AAbstract%3A%20%20%20Medical%20image%20data%20is%20less%20accessible%20than%20in%20other%20domains%20due%20to%20privacy%0Aand%20regulatory%20constraints.%20In%20addition%2C%20labeling%20requires%20costly%2C%0Atime-intensive%20manual%20image%20annotation%20by%20clinical%20experts.%20To%20overcome%20these%0Achallenges%2C%20synthetic%20medical%20data%20generation%20offers%20a%20promising%20solution.%0AGenerative%20AI%20%28GenAI%29%2C%20employing%20generative%20deep%20learning%20models%2C%20has%20proven%0Aeffective%20at%20producing%20realistic%20synthetic%20images.%20This%20study%20proposes%20a%20novel%0Amask-guided%20GenAI%20approach%20using%20diffusion%20models%20to%20generate%20synthetic%20fetal%0Ahead%20ultrasound%20images%20paired%20with%20segmentation%20masks.%20These%20synthetic%20pairs%0Aaugment%20real%20datasets%20for%20supervised%20fine-tuning%20of%20the%20Segment%20Anything%20Model%0A%28SAM%29.%20Our%20results%20show%20that%20the%20synthetic%20data%20captures%20real%20image%20features%0Aeffectively%2C%20and%20this%20approach%20reaches%20state-of-the-art%20fetal%20head%0Asegmentation%2C%20especially%20when%20trained%20with%20a%20limited%20number%20of%20real%20image-mask%0Apairs.%20In%20particular%2C%20the%20segmentation%20reaches%20Dice%20Scores%20of%2094.66%5C%25%20and%0A94.38%5C%25%20using%20a%20handful%20of%20ultrasound%20images%20from%20the%20Spanish%20and%20African%0Acohorts%2C%20respectively.%20Our%20code%2C%20models%2C%20and%20data%20are%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Model-based%2520Data%2520Augmentation%2520Method%2520for%2520Fetal%2520Head%2520Ultrasound%250A%2520%2520Segmentation%26entry.906535625%3DFangyijie%2520Wang%2520and%2520Kevin%2520Whelan%2520and%2520F%25C3%25A9lix%2520Balado%2520and%2520Kathleen%2520M.%2520Curran%2520and%2520Gu%25C3%25A9nol%25C3%25A9%2520Silvestre%26entry.1292438233%3D%2520%2520Medical%2520image%2520data%2520is%2520less%2520accessible%2520than%2520in%2520other%2520domains%2520due%2520to%2520privacy%250Aand%2520regulatory%2520constraints.%2520In%2520addition%252C%2520labeling%2520requires%2520costly%252C%250Atime-intensive%2520manual%2520image%2520annotation%2520by%2520clinical%2520experts.%2520To%2520overcome%2520these%250Achallenges%252C%2520synthetic%2520medical%2520data%2520generation%2520offers%2520a%2520promising%2520solution.%250AGenerative%2520AI%2520%2528GenAI%2529%252C%2520employing%2520generative%2520deep%2520learning%2520models%252C%2520has%2520proven%250Aeffective%2520at%2520producing%2520realistic%2520synthetic%2520images.%2520This%2520study%2520proposes%2520a%2520novel%250Amask-guided%2520GenAI%2520approach%2520using%2520diffusion%2520models%2520to%2520generate%2520synthetic%2520fetal%250Ahead%2520ultrasound%2520images%2520paired%2520with%2520segmentation%2520masks.%2520These%2520synthetic%2520pairs%250Aaugment%2520real%2520datasets%2520for%2520supervised%2520fine-tuning%2520of%2520the%2520Segment%2520Anything%2520Model%250A%2528SAM%2529.%2520Our%2520results%2520show%2520that%2520the%2520synthetic%2520data%2520captures%2520real%2520image%2520features%250Aeffectively%252C%2520and%2520this%2520approach%2520reaches%2520state-of-the-art%2520fetal%2520head%250Asegmentation%252C%2520especially%2520when%2520trained%2520with%2520a%2520limited%2520number%2520of%2520real%2520image-mask%250Apairs.%2520In%2520particular%252C%2520the%2520segmentation%2520reaches%2520Dice%2520Scores%2520of%252094.66%255C%2525%2520and%250A94.38%255C%2525%2520using%2520a%2520handful%2520of%2520ultrasound%2520images%2520from%2520the%2520Spanish%2520and%2520African%250Acohorts%252C%2520respectively.%2520Our%2520code%252C%2520models%252C%2520and%2520data%2520are%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Model-based%20Data%20Augmentation%20Method%20for%20Fetal%20Head%20Ultrasound%0A%20%20Segmentation&entry.906535625=Fangyijie%20Wang%20and%20Kevin%20Whelan%20and%20F%C3%A9lix%20Balado%20and%20Kathleen%20M.%20Curran%20and%20Gu%C3%A9nol%C3%A9%20Silvestre&entry.1292438233=%20%20Medical%20image%20data%20is%20less%20accessible%20than%20in%20other%20domains%20due%20to%20privacy%0Aand%20regulatory%20constraints.%20In%20addition%2C%20labeling%20requires%20costly%2C%0Atime-intensive%20manual%20image%20annotation%20by%20clinical%20experts.%20To%20overcome%20these%0Achallenges%2C%20synthetic%20medical%20data%20generation%20offers%20a%20promising%20solution.%0AGenerative%20AI%20%28GenAI%29%2C%20employing%20generative%20deep%20learning%20models%2C%20has%20proven%0Aeffective%20at%20producing%20realistic%20synthetic%20images.%20This%20study%20proposes%20a%20novel%0Amask-guided%20GenAI%20approach%20using%20diffusion%20models%20to%20generate%20synthetic%20fetal%0Ahead%20ultrasound%20images%20paired%20with%20segmentation%20masks.%20These%20synthetic%20pairs%0Aaugment%20real%20datasets%20for%20supervised%20fine-tuning%20of%20the%20Segment%20Anything%20Model%0A%28SAM%29.%20Our%20results%20show%20that%20the%20synthetic%20data%20captures%20real%20image%20features%0Aeffectively%2C%20and%20this%20approach%20reaches%20state-of-the-art%20fetal%20head%0Asegmentation%2C%20especially%20when%20trained%20with%20a%20limited%20number%20of%20real%20image-mask%0Apairs.%20In%20particular%2C%20the%20segmentation%20reaches%20Dice%20Scores%20of%2094.66%5C%25%20and%0A94.38%5C%25%20using%20a%20handful%20of%20ultrasound%20images%20from%20the%20Spanish%20and%20African%0Acohorts%2C%20respectively.%20Our%20code%2C%20models%2C%20and%20data%20are%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23664v2&entry.124074799=Read"},
{"title": "Unsupervised Morphological Tree Tokenizer", "author": "Qingyang Zhu and Xiang Hu and Pengyu Ji and Wei Wu and Kewei Tu", "abstract": "  As a cornerstone in language modeling, tokenization involves segmenting text\ninputs into pre-defined atomic units. Conventional statistical tokenizers often\ndisrupt constituent boundaries within words, thereby corrupting semantic\ninformation. To address this drawback, we introduce morphological structure\nguidance to tokenization and propose a deep model to induce character-level\nstructures of words. Specifically, the deep model jointly encodes internal\nstructures and representations of words with a mechanism named\n$\\textit{MorphOverriding}$ to ensure the indecomposability of morphemes. By\ntraining the model with self-supervised objectives, our method is capable of\ninducing character-level structures that align with morphological rules without\nannotated training data. Based on the induced structures, our algorithm\ntokenizes words through vocabulary matching in a top-down manner. Empirical\nresults indicate that the proposed method effectively retains complete\nmorphemes and outperforms widely adopted methods such as BPE and WordPiece on\nboth morphological segmentation tasks and language modeling tasks. Code is\navailable at https://github.com/martianmartina/TreeTokenizer.\n", "link": "http://arxiv.org/abs/2406.15245v2", "date": "2025-07-10", "relevancy": 2.3108, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4874}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4572}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Morphological%20Tree%20Tokenizer&body=Title%3A%20Unsupervised%20Morphological%20Tree%20Tokenizer%0AAuthor%3A%20Qingyang%20Zhu%20and%20Xiang%20Hu%20and%20Pengyu%20Ji%20and%20Wei%20Wu%20and%20Kewei%20Tu%0AAbstract%3A%20%20%20As%20a%20cornerstone%20in%20language%20modeling%2C%20tokenization%20involves%20segmenting%20text%0Ainputs%20into%20pre-defined%20atomic%20units.%20Conventional%20statistical%20tokenizers%20often%0Adisrupt%20constituent%20boundaries%20within%20words%2C%20thereby%20corrupting%20semantic%0Ainformation.%20To%20address%20this%20drawback%2C%20we%20introduce%20morphological%20structure%0Aguidance%20to%20tokenization%20and%20propose%20a%20deep%20model%20to%20induce%20character-level%0Astructures%20of%20words.%20Specifically%2C%20the%20deep%20model%20jointly%20encodes%20internal%0Astructures%20and%20representations%20of%20words%20with%20a%20mechanism%20named%0A%24%5Ctextit%7BMorphOverriding%7D%24%20to%20ensure%20the%20indecomposability%20of%20morphemes.%20By%0Atraining%20the%20model%20with%20self-supervised%20objectives%2C%20our%20method%20is%20capable%20of%0Ainducing%20character-level%20structures%20that%20align%20with%20morphological%20rules%20without%0Aannotated%20training%20data.%20Based%20on%20the%20induced%20structures%2C%20our%20algorithm%0Atokenizes%20words%20through%20vocabulary%20matching%20in%20a%20top-down%20manner.%20Empirical%0Aresults%20indicate%20that%20the%20proposed%20method%20effectively%20retains%20complete%0Amorphemes%20and%20outperforms%20widely%20adopted%20methods%20such%20as%20BPE%20and%20WordPiece%20on%0Aboth%20morphological%20segmentation%20tasks%20and%20language%20modeling%20tasks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/martianmartina/TreeTokenizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15245v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Morphological%2520Tree%2520Tokenizer%26entry.906535625%3DQingyang%2520Zhu%2520and%2520Xiang%2520Hu%2520and%2520Pengyu%2520Ji%2520and%2520Wei%2520Wu%2520and%2520Kewei%2520Tu%26entry.1292438233%3D%2520%2520As%2520a%2520cornerstone%2520in%2520language%2520modeling%252C%2520tokenization%2520involves%2520segmenting%2520text%250Ainputs%2520into%2520pre-defined%2520atomic%2520units.%2520Conventional%2520statistical%2520tokenizers%2520often%250Adisrupt%2520constituent%2520boundaries%2520within%2520words%252C%2520thereby%2520corrupting%2520semantic%250Ainformation.%2520To%2520address%2520this%2520drawback%252C%2520we%2520introduce%2520morphological%2520structure%250Aguidance%2520to%2520tokenization%2520and%2520propose%2520a%2520deep%2520model%2520to%2520induce%2520character-level%250Astructures%2520of%2520words.%2520Specifically%252C%2520the%2520deep%2520model%2520jointly%2520encodes%2520internal%250Astructures%2520and%2520representations%2520of%2520words%2520with%2520a%2520mechanism%2520named%250A%2524%255Ctextit%257BMorphOverriding%257D%2524%2520to%2520ensure%2520the%2520indecomposability%2520of%2520morphemes.%2520By%250Atraining%2520the%2520model%2520with%2520self-supervised%2520objectives%252C%2520our%2520method%2520is%2520capable%2520of%250Ainducing%2520character-level%2520structures%2520that%2520align%2520with%2520morphological%2520rules%2520without%250Aannotated%2520training%2520data.%2520Based%2520on%2520the%2520induced%2520structures%252C%2520our%2520algorithm%250Atokenizes%2520words%2520through%2520vocabulary%2520matching%2520in%2520a%2520top-down%2520manner.%2520Empirical%250Aresults%2520indicate%2520that%2520the%2520proposed%2520method%2520effectively%2520retains%2520complete%250Amorphemes%2520and%2520outperforms%2520widely%2520adopted%2520methods%2520such%2520as%2520BPE%2520and%2520WordPiece%2520on%250Aboth%2520morphological%2520segmentation%2520tasks%2520and%2520language%2520modeling%2520tasks.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/martianmartina/TreeTokenizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15245v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Morphological%20Tree%20Tokenizer&entry.906535625=Qingyang%20Zhu%20and%20Xiang%20Hu%20and%20Pengyu%20Ji%20and%20Wei%20Wu%20and%20Kewei%20Tu&entry.1292438233=%20%20As%20a%20cornerstone%20in%20language%20modeling%2C%20tokenization%20involves%20segmenting%20text%0Ainputs%20into%20pre-defined%20atomic%20units.%20Conventional%20statistical%20tokenizers%20often%0Adisrupt%20constituent%20boundaries%20within%20words%2C%20thereby%20corrupting%20semantic%0Ainformation.%20To%20address%20this%20drawback%2C%20we%20introduce%20morphological%20structure%0Aguidance%20to%20tokenization%20and%20propose%20a%20deep%20model%20to%20induce%20character-level%0Astructures%20of%20words.%20Specifically%2C%20the%20deep%20model%20jointly%20encodes%20internal%0Astructures%20and%20representations%20of%20words%20with%20a%20mechanism%20named%0A%24%5Ctextit%7BMorphOverriding%7D%24%20to%20ensure%20the%20indecomposability%20of%20morphemes.%20By%0Atraining%20the%20model%20with%20self-supervised%20objectives%2C%20our%20method%20is%20capable%20of%0Ainducing%20character-level%20structures%20that%20align%20with%20morphological%20rules%20without%0Aannotated%20training%20data.%20Based%20on%20the%20induced%20structures%2C%20our%20algorithm%0Atokenizes%20words%20through%20vocabulary%20matching%20in%20a%20top-down%20manner.%20Empirical%0Aresults%20indicate%20that%20the%20proposed%20method%20effectively%20retains%20complete%0Amorphemes%20and%20outperforms%20widely%20adopted%20methods%20such%20as%20BPE%20and%20WordPiece%20on%0Aboth%20morphological%20segmentation%20tasks%20and%20language%20modeling%20tasks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/martianmartina/TreeTokenizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15245v2&entry.124074799=Read"},
{"title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs", "author": "Jeongseok Hyun and Sukjun Hwang and Su Ho Han and Taeoh Kim and Inwoong Lee and Dongyoon Wee and Joon-Young Lee and Seon Joo Kim and Minho Shim", "abstract": "  Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.\n", "link": "http://arxiv.org/abs/2507.07990v1", "date": "2025-07-10", "relevancy": 2.3, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5825}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5768}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Granular%20Spatio-Temporal%20Token%20Merging%20for%20Training-Free%0A%20%20Acceleration%20of%20Video%20LLMs&body=Title%3A%20Multi-Granular%20Spatio-Temporal%20Token%20Merging%20for%20Training-Free%0A%20%20Acceleration%20of%20Video%20LLMs%0AAuthor%3A%20Jeongseok%20Hyun%20and%20Sukjun%20Hwang%20and%20Su%20Ho%20Han%20and%20Taeoh%20Kim%20and%20Inwoong%20Lee%20and%20Dongyoon%20Wee%20and%20Joon-Young%20Lee%20and%20Seon%20Joo%20Kim%20and%20Minho%20Shim%0AAbstract%3A%20%20%20Video%20large%20language%20models%20%28LLMs%29%20achieve%20strong%20video%20understanding%20by%0Aleveraging%20a%20large%20number%20of%20spatio-temporal%20tokens%2C%20but%20suffer%20from%20quadratic%0Acomputational%20scaling%20with%20token%20count.%20To%20address%20this%2C%20we%20propose%20a%0Atraining-free%20spatio-temporal%20token%20merging%20method%2C%20named%20STTM.%20Our%20key%20insight%0Ais%20to%20exploit%20local%20spatial%20and%20temporal%20redundancy%20in%20video%20data%20which%20has%0Abeen%20overlooked%20in%20prior%20work.%20STTM%20first%20transforms%20each%20frame%20into%0Amulti-granular%20spatial%20tokens%20using%20a%20coarse-to-fine%20search%20over%20a%20quadtree%0Astructure%2C%20then%20performs%20directed%20pairwise%20merging%20across%20the%20temporal%0Adimension.%20This%20decomposed%20merging%20approach%20outperforms%20existing%20token%0Areduction%20methods%20across%20six%20video%20QA%20benchmarks.%20Notably%2C%20STTM%20achieves%20a%0A2%24%5Ctimes%24%20speed-up%20with%20only%20a%200.5%25%20accuracy%20drop%20under%20a%2050%25%20token%20budget%2C%20and%0Aa%203%24%5Ctimes%24%20speed-up%20with%20just%20a%202%25%20drop%20under%20a%2030%25%20budget.%20Moreover%2C%20STTM%20is%0Aquery-agnostic%2C%20allowing%20KV%20cache%20reuse%20across%20different%20questions%20for%20the%20same%0Avideo.%20The%20project%20page%20is%20available%20at%20https%3A//www.jshyun.me/projects/sttm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Granular%2520Spatio-Temporal%2520Token%2520Merging%2520for%2520Training-Free%250A%2520%2520Acceleration%2520of%2520Video%2520LLMs%26entry.906535625%3DJeongseok%2520Hyun%2520and%2520Sukjun%2520Hwang%2520and%2520Su%2520Ho%2520Han%2520and%2520Taeoh%2520Kim%2520and%2520Inwoong%2520Lee%2520and%2520Dongyoon%2520Wee%2520and%2520Joon-Young%2520Lee%2520and%2520Seon%2520Joo%2520Kim%2520and%2520Minho%2520Shim%26entry.1292438233%3D%2520%2520Video%2520large%2520language%2520models%2520%2528LLMs%2529%2520achieve%2520strong%2520video%2520understanding%2520by%250Aleveraging%2520a%2520large%2520number%2520of%2520spatio-temporal%2520tokens%252C%2520but%2520suffer%2520from%2520quadratic%250Acomputational%2520scaling%2520with%2520token%2520count.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Atraining-free%2520spatio-temporal%2520token%2520merging%2520method%252C%2520named%2520STTM.%2520Our%2520key%2520insight%250Ais%2520to%2520exploit%2520local%2520spatial%2520and%2520temporal%2520redundancy%2520in%2520video%2520data%2520which%2520has%250Abeen%2520overlooked%2520in%2520prior%2520work.%2520STTM%2520first%2520transforms%2520each%2520frame%2520into%250Amulti-granular%2520spatial%2520tokens%2520using%2520a%2520coarse-to-fine%2520search%2520over%2520a%2520quadtree%250Astructure%252C%2520then%2520performs%2520directed%2520pairwise%2520merging%2520across%2520the%2520temporal%250Adimension.%2520This%2520decomposed%2520merging%2520approach%2520outperforms%2520existing%2520token%250Areduction%2520methods%2520across%2520six%2520video%2520QA%2520benchmarks.%2520Notably%252C%2520STTM%2520achieves%2520a%250A2%2524%255Ctimes%2524%2520speed-up%2520with%2520only%2520a%25200.5%2525%2520accuracy%2520drop%2520under%2520a%252050%2525%2520token%2520budget%252C%2520and%250Aa%25203%2524%255Ctimes%2524%2520speed-up%2520with%2520just%2520a%25202%2525%2520drop%2520under%2520a%252030%2525%2520budget.%2520Moreover%252C%2520STTM%2520is%250Aquery-agnostic%252C%2520allowing%2520KV%2520cache%2520reuse%2520across%2520different%2520questions%2520for%2520the%2520same%250Avideo.%2520The%2520project%2520page%2520is%2520available%2520at%2520https%253A//www.jshyun.me/projects/sttm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Granular%20Spatio-Temporal%20Token%20Merging%20for%20Training-Free%0A%20%20Acceleration%20of%20Video%20LLMs&entry.906535625=Jeongseok%20Hyun%20and%20Sukjun%20Hwang%20and%20Su%20Ho%20Han%20and%20Taeoh%20Kim%20and%20Inwoong%20Lee%20and%20Dongyoon%20Wee%20and%20Joon-Young%20Lee%20and%20Seon%20Joo%20Kim%20and%20Minho%20Shim&entry.1292438233=%20%20Video%20large%20language%20models%20%28LLMs%29%20achieve%20strong%20video%20understanding%20by%0Aleveraging%20a%20large%20number%20of%20spatio-temporal%20tokens%2C%20but%20suffer%20from%20quadratic%0Acomputational%20scaling%20with%20token%20count.%20To%20address%20this%2C%20we%20propose%20a%0Atraining-free%20spatio-temporal%20token%20merging%20method%2C%20named%20STTM.%20Our%20key%20insight%0Ais%20to%20exploit%20local%20spatial%20and%20temporal%20redundancy%20in%20video%20data%20which%20has%0Abeen%20overlooked%20in%20prior%20work.%20STTM%20first%20transforms%20each%20frame%20into%0Amulti-granular%20spatial%20tokens%20using%20a%20coarse-to-fine%20search%20over%20a%20quadtree%0Astructure%2C%20then%20performs%20directed%20pairwise%20merging%20across%20the%20temporal%0Adimension.%20This%20decomposed%20merging%20approach%20outperforms%20existing%20token%0Areduction%20methods%20across%20six%20video%20QA%20benchmarks.%20Notably%2C%20STTM%20achieves%20a%0A2%24%5Ctimes%24%20speed-up%20with%20only%20a%200.5%25%20accuracy%20drop%20under%20a%2050%25%20token%20budget%2C%20and%0Aa%203%24%5Ctimes%24%20speed-up%20with%20just%20a%202%25%20drop%20under%20a%2030%25%20budget.%20Moreover%2C%20STTM%20is%0Aquery-agnostic%2C%20allowing%20KV%20cache%20reuse%20across%20different%20questions%20for%20the%20same%0Avideo.%20The%20project%20page%20is%20available%20at%20https%3A//www.jshyun.me/projects/sttm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07990v1&entry.124074799=Read"},
{"title": "Skywork-R1V3 Technical Report", "author": "Wei Shen and Jiangbo Pei and Yi Peng and Xuchen Song and Yang Liu and Jian Peng and Haofeng Sun and Yunzhuo Hao and Peiyu Wang and Jianhao Zhang and Yahui Zhou", "abstract": "  We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.\n", "link": "http://arxiv.org/abs/2507.06167v3", "date": "2025-07-10", "relevancy": 2.2999, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skywork-R1V3%20Technical%20Report&body=Title%3A%20Skywork-R1V3%20Technical%20Report%0AAuthor%3A%20Wei%20Shen%20and%20Jiangbo%20Pei%20and%20Yi%20Peng%20and%20Xuchen%20Song%20and%20Yang%20Liu%20and%20Jian%20Peng%20and%20Haofeng%20Sun%20and%20Yunzhuo%20Hao%20and%20Peiyu%20Wang%20and%20Jianhao%20Zhang%20and%20Yahui%20Zhou%0AAbstract%3A%20%20%20We%20introduce%20Skywork-R1V3%2C%20an%20advanced%2C%20open-source%20vision-language%20model%0A%28VLM%29%20that%20pioneers%20a%20new%20approach%20to%20visual%20reasoning.%20Its%20key%20innovation%20lies%0Ain%20effectively%20transferring%20reasoning%20skills%20from%20text-only%20Large%20Language%0AModels%20%28LLMs%29%20to%20visual%20tasks.%20The%20strong%20performance%20of%20Skywork-R1V3%20primarily%0Astems%20from%20our%20elaborate%20post-training%20RL%20framework%2C%20which%20effectively%0Aactivates%20and%20enhances%20the%20model%27s%20reasoning%20ability%2C%20without%20the%20need%20for%0Aadditional%20continue%20pre-training.%20Through%20this%20framework%2C%20we%20further%20uncover%0Athe%20fundamental%20role%20of%20the%20connector%20module%20in%20achieving%20robust%20cross-modal%0Aalignment%20for%20multimodal%20reasoning%20models.%20In%20addition%2C%20we%20introduce%20a%20unique%0Aindicator%20of%20reasoning%20capability%2C%20the%20entropy%20of%20critical%20reasoning%20tokens%2C%0Awhich%20has%20proven%20highly%20effective%20for%20checkpoint%20selection%20during%20RL%20training.%0ASkywork-R1V3%20achieves%20state-of-the-art%20results%20on%20MMMU%2C%20significantly%20improving%0Afrom%2064.3%25%20to%2076.0%25.%20This%20performance%20matches%20entry-level%20human%20capabilities.%0ARemarkably%2C%20our%20RL-powered%20post-training%20approach%20enables%20even%20the%2038B%0Aparameter%20model%20to%20rival%20top%20closed-source%20VLMs.%20The%20implementation%0Asuccessfully%20transfers%20mathematical%20reasoning%20to%20other%20subject-related%0Areasoning%20tasks.%20We%20also%20include%20an%20analysis%20of%20curriculum%20learning%20and%0Areinforcement%20finetuning%20strategies%2C%20along%20with%20a%20broader%20discussion%20on%0Amultimodal%20reasoning.%20Skywork-R1V3%20represents%20a%20significant%20leap%20in%20multimodal%0Areasoning%2C%20showcasing%20RL%20as%20a%20powerful%20engine%20for%20advancing%20open-source%20VLM%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06167v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkywork-R1V3%2520Technical%2520Report%26entry.906535625%3DWei%2520Shen%2520and%2520Jiangbo%2520Pei%2520and%2520Yi%2520Peng%2520and%2520Xuchen%2520Song%2520and%2520Yang%2520Liu%2520and%2520Jian%2520Peng%2520and%2520Haofeng%2520Sun%2520and%2520Yunzhuo%2520Hao%2520and%2520Peiyu%2520Wang%2520and%2520Jianhao%2520Zhang%2520and%2520Yahui%2520Zhou%26entry.1292438233%3D%2520%2520We%2520introduce%2520Skywork-R1V3%252C%2520an%2520advanced%252C%2520open-source%2520vision-language%2520model%250A%2528VLM%2529%2520that%2520pioneers%2520a%2520new%2520approach%2520to%2520visual%2520reasoning.%2520Its%2520key%2520innovation%2520lies%250Ain%2520effectively%2520transferring%2520reasoning%2520skills%2520from%2520text-only%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520to%2520visual%2520tasks.%2520The%2520strong%2520performance%2520of%2520Skywork-R1V3%2520primarily%250Astems%2520from%2520our%2520elaborate%2520post-training%2520RL%2520framework%252C%2520which%2520effectively%250Aactivates%2520and%2520enhances%2520the%2520model%2527s%2520reasoning%2520ability%252C%2520without%2520the%2520need%2520for%250Aadditional%2520continue%2520pre-training.%2520Through%2520this%2520framework%252C%2520we%2520further%2520uncover%250Athe%2520fundamental%2520role%2520of%2520the%2520connector%2520module%2520in%2520achieving%2520robust%2520cross-modal%250Aalignment%2520for%2520multimodal%2520reasoning%2520models.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520unique%250Aindicator%2520of%2520reasoning%2520capability%252C%2520the%2520entropy%2520of%2520critical%2520reasoning%2520tokens%252C%250Awhich%2520has%2520proven%2520highly%2520effective%2520for%2520checkpoint%2520selection%2520during%2520RL%2520training.%250ASkywork-R1V3%2520achieves%2520state-of-the-art%2520results%2520on%2520MMMU%252C%2520significantly%2520improving%250Afrom%252064.3%2525%2520to%252076.0%2525.%2520This%2520performance%2520matches%2520entry-level%2520human%2520capabilities.%250ARemarkably%252C%2520our%2520RL-powered%2520post-training%2520approach%2520enables%2520even%2520the%252038B%250Aparameter%2520model%2520to%2520rival%2520top%2520closed-source%2520VLMs.%2520The%2520implementation%250Asuccessfully%2520transfers%2520mathematical%2520reasoning%2520to%2520other%2520subject-related%250Areasoning%2520tasks.%2520We%2520also%2520include%2520an%2520analysis%2520of%2520curriculum%2520learning%2520and%250Areinforcement%2520finetuning%2520strategies%252C%2520along%2520with%2520a%2520broader%2520discussion%2520on%250Amultimodal%2520reasoning.%2520Skywork-R1V3%2520represents%2520a%2520significant%2520leap%2520in%2520multimodal%250Areasoning%252C%2520showcasing%2520RL%2520as%2520a%2520powerful%2520engine%2520for%2520advancing%2520open-source%2520VLM%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06167v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skywork-R1V3%20Technical%20Report&entry.906535625=Wei%20Shen%20and%20Jiangbo%20Pei%20and%20Yi%20Peng%20and%20Xuchen%20Song%20and%20Yang%20Liu%20and%20Jian%20Peng%20and%20Haofeng%20Sun%20and%20Yunzhuo%20Hao%20and%20Peiyu%20Wang%20and%20Jianhao%20Zhang%20and%20Yahui%20Zhou&entry.1292438233=%20%20We%20introduce%20Skywork-R1V3%2C%20an%20advanced%2C%20open-source%20vision-language%20model%0A%28VLM%29%20that%20pioneers%20a%20new%20approach%20to%20visual%20reasoning.%20Its%20key%20innovation%20lies%0Ain%20effectively%20transferring%20reasoning%20skills%20from%20text-only%20Large%20Language%0AModels%20%28LLMs%29%20to%20visual%20tasks.%20The%20strong%20performance%20of%20Skywork-R1V3%20primarily%0Astems%20from%20our%20elaborate%20post-training%20RL%20framework%2C%20which%20effectively%0Aactivates%20and%20enhances%20the%20model%27s%20reasoning%20ability%2C%20without%20the%20need%20for%0Aadditional%20continue%20pre-training.%20Through%20this%20framework%2C%20we%20further%20uncover%0Athe%20fundamental%20role%20of%20the%20connector%20module%20in%20achieving%20robust%20cross-modal%0Aalignment%20for%20multimodal%20reasoning%20models.%20In%20addition%2C%20we%20introduce%20a%20unique%0Aindicator%20of%20reasoning%20capability%2C%20the%20entropy%20of%20critical%20reasoning%20tokens%2C%0Awhich%20has%20proven%20highly%20effective%20for%20checkpoint%20selection%20during%20RL%20training.%0ASkywork-R1V3%20achieves%20state-of-the-art%20results%20on%20MMMU%2C%20significantly%20improving%0Afrom%2064.3%25%20to%2076.0%25.%20This%20performance%20matches%20entry-level%20human%20capabilities.%0ARemarkably%2C%20our%20RL-powered%20post-training%20approach%20enables%20even%20the%2038B%0Aparameter%20model%20to%20rival%20top%20closed-source%20VLMs.%20The%20implementation%0Asuccessfully%20transfers%20mathematical%20reasoning%20to%20other%20subject-related%0Areasoning%20tasks.%20We%20also%20include%20an%20analysis%20of%20curriculum%20learning%20and%0Areinforcement%20finetuning%20strategies%2C%20along%20with%20a%20broader%20discussion%20on%0Amultimodal%20reasoning.%20Skywork-R1V3%20represents%20a%20significant%20leap%20in%20multimodal%0Areasoning%2C%20showcasing%20RL%20as%20a%20powerful%20engine%20for%20advancing%20open-source%20VLM%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06167v3&entry.124074799=Read"},
{"title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory", "author": "Runjia Li and Philip Torr and Andrea Vedaldi and Tomas Jakab", "abstract": "  We propose a novel memory mechanism to build video generators that can\nexplore environments interactively. Similar results have previously been\nachieved by out-painting 2D views of the scene while incrementally\nreconstructing its 3D geometry, which quickly accumulates errors, or by video\ngenerators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a mechanism that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables the efficient retrieval of the most relevant past\nviews when generating new ones. By focusing only on these relevant views, our\nmethod produces consistent explorations of imagined environments at a fraction\nof the computational cost of using all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.\n", "link": "http://arxiv.org/abs/2506.18903v2", "date": "2025-07-10", "relevancy": 2.2641, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5765}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5586}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VMem%3A%20Consistent%20Interactive%20Video%20Scene%20Generation%20with%20Surfel-Indexed%0A%20%20View%20Memory&body=Title%3A%20VMem%3A%20Consistent%20Interactive%20Video%20Scene%20Generation%20with%20Surfel-Indexed%0A%20%20View%20Memory%0AAuthor%3A%20Runjia%20Li%20and%20Philip%20Torr%20and%20Andrea%20Vedaldi%20and%20Tomas%20Jakab%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20memory%20mechanism%20to%20build%20video%20generators%20that%20can%0Aexplore%20environments%20interactively.%20Similar%20results%20have%20previously%20been%0Aachieved%20by%20out-painting%202D%20views%20of%20the%20scene%20while%20incrementally%0Areconstructing%20its%203D%20geometry%2C%20which%20quickly%20accumulates%20errors%2C%20or%20by%20video%0Agenerators%20with%20a%20short%20context%20window%2C%20which%20struggle%20to%20maintain%20scene%0Acoherence%20over%20the%20long%20term.%20To%20address%20these%20limitations%2C%20we%20introduce%0ASurfel-Indexed%20View%20Memory%20%28VMem%29%2C%20a%20mechanism%20that%20remembers%20past%20views%20by%0Aindexing%20them%20geometrically%20based%20on%20the%203D%20surface%20elements%20%28surfels%29%20they%0Ahave%20observed.%20VMem%20enables%20the%20efficient%20retrieval%20of%20the%20most%20relevant%20past%0Aviews%20when%20generating%20new%20ones.%20By%20focusing%20only%20on%20these%20relevant%20views%2C%20our%0Amethod%20produces%20consistent%20explorations%20of%20imagined%20environments%20at%20a%20fraction%0Aof%20the%20computational%20cost%20of%20using%20all%20past%20views%20as%20context.%20We%20evaluate%20our%0Aapproach%20on%20challenging%20long-term%20scene%20synthesis%20benchmarks%20and%20demonstrate%0Asuperior%20performance%20compared%20to%20existing%20methods%20in%20maintaining%20scene%0Acoherence%20and%20camera%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18903v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVMem%253A%2520Consistent%2520Interactive%2520Video%2520Scene%2520Generation%2520with%2520Surfel-Indexed%250A%2520%2520View%2520Memory%26entry.906535625%3DRunjia%2520Li%2520and%2520Philip%2520Torr%2520and%2520Andrea%2520Vedaldi%2520and%2520Tomas%2520Jakab%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520memory%2520mechanism%2520to%2520build%2520video%2520generators%2520that%2520can%250Aexplore%2520environments%2520interactively.%2520Similar%2520results%2520have%2520previously%2520been%250Aachieved%2520by%2520out-painting%25202D%2520views%2520of%2520the%2520scene%2520while%2520incrementally%250Areconstructing%2520its%25203D%2520geometry%252C%2520which%2520quickly%2520accumulates%2520errors%252C%2520or%2520by%2520video%250Agenerators%2520with%2520a%2520short%2520context%2520window%252C%2520which%2520struggle%2520to%2520maintain%2520scene%250Acoherence%2520over%2520the%2520long%2520term.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%250ASurfel-Indexed%2520View%2520Memory%2520%2528VMem%2529%252C%2520a%2520mechanism%2520that%2520remembers%2520past%2520views%2520by%250Aindexing%2520them%2520geometrically%2520based%2520on%2520the%25203D%2520surface%2520elements%2520%2528surfels%2529%2520they%250Ahave%2520observed.%2520VMem%2520enables%2520the%2520efficient%2520retrieval%2520of%2520the%2520most%2520relevant%2520past%250Aviews%2520when%2520generating%2520new%2520ones.%2520By%2520focusing%2520only%2520on%2520these%2520relevant%2520views%252C%2520our%250Amethod%2520produces%2520consistent%2520explorations%2520of%2520imagined%2520environments%2520at%2520a%2520fraction%250Aof%2520the%2520computational%2520cost%2520of%2520using%2520all%2520past%2520views%2520as%2520context.%2520We%2520evaluate%2520our%250Aapproach%2520on%2520challenging%2520long-term%2520scene%2520synthesis%2520benchmarks%2520and%2520demonstrate%250Asuperior%2520performance%2520compared%2520to%2520existing%2520methods%2520in%2520maintaining%2520scene%250Acoherence%2520and%2520camera%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18903v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VMem%3A%20Consistent%20Interactive%20Video%20Scene%20Generation%20with%20Surfel-Indexed%0A%20%20View%20Memory&entry.906535625=Runjia%20Li%20and%20Philip%20Torr%20and%20Andrea%20Vedaldi%20and%20Tomas%20Jakab&entry.1292438233=%20%20We%20propose%20a%20novel%20memory%20mechanism%20to%20build%20video%20generators%20that%20can%0Aexplore%20environments%20interactively.%20Similar%20results%20have%20previously%20been%0Aachieved%20by%20out-painting%202D%20views%20of%20the%20scene%20while%20incrementally%0Areconstructing%20its%203D%20geometry%2C%20which%20quickly%20accumulates%20errors%2C%20or%20by%20video%0Agenerators%20with%20a%20short%20context%20window%2C%20which%20struggle%20to%20maintain%20scene%0Acoherence%20over%20the%20long%20term.%20To%20address%20these%20limitations%2C%20we%20introduce%0ASurfel-Indexed%20View%20Memory%20%28VMem%29%2C%20a%20mechanism%20that%20remembers%20past%20views%20by%0Aindexing%20them%20geometrically%20based%20on%20the%203D%20surface%20elements%20%28surfels%29%20they%0Ahave%20observed.%20VMem%20enables%20the%20efficient%20retrieval%20of%20the%20most%20relevant%20past%0Aviews%20when%20generating%20new%20ones.%20By%20focusing%20only%20on%20these%20relevant%20views%2C%20our%0Amethod%20produces%20consistent%20explorations%20of%20imagined%20environments%20at%20a%20fraction%0Aof%20the%20computational%20cost%20of%20using%20all%20past%20views%20as%20context.%20We%20evaluate%20our%0Aapproach%20on%20challenging%20long-term%20scene%20synthesis%20benchmarks%20and%20demonstrate%0Asuperior%20performance%20compared%20to%20existing%20methods%20in%20maintaining%20scene%0Acoherence%20and%20camera%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18903v2&entry.124074799=Read"},
{"title": "Studying and Improving Graph Neural Network-based Motif Estimation", "author": "Pedro C. Vieira and Miguel E. P. Silva and Pedro Manuel Pinto Ribeiro", "abstract": "  Graph Neural Networks (GNNs) are a predominant method for graph\nrepresentation learning. However, beyond subgraph frequency estimation, their\napplication to network motif significance-profile (SP) prediction remains\nunder-explored, with no established benchmarks in the literature. We propose to\naddress this problem, framing SP estimation as a task independent of subgraph\nfrequency estimation. Our approach shifts from frequency counting to direct SP\nestimation and modulates the problem as multitarget regression. The\nreformulation is optimised for interpretability, stability and scalability on\nlarge graphs. We validate our method using a large synthetic dataset and\nfurther test it on real-world graphs. Our experiments reveal that 1-WL limited\nmodels struggle to make precise estimations of SPs. However, they can\ngeneralise to approximate the graph generation processes of networks by\ncomparing their predicted SP with the ones originating from synthetic\ngenerators. This first study on GNN-based motif estimation also hints at how\nusing direct SP estimation can help go past the theoretical limitations that\nmotif estimation faces when performed through subgraph counting.\n", "link": "http://arxiv.org/abs/2506.15709v3", "date": "2025-07-10", "relevancy": 2.2438, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4752}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4358}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Studying%20and%20Improving%20Graph%20Neural%20Network-based%20Motif%20Estimation&body=Title%3A%20Studying%20and%20Improving%20Graph%20Neural%20Network-based%20Motif%20Estimation%0AAuthor%3A%20Pedro%20C.%20Vieira%20and%20Miguel%20E.%20P.%20Silva%20and%20Pedro%20Manuel%20Pinto%20Ribeiro%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20predominant%20method%20for%20graph%0Arepresentation%20learning.%20However%2C%20beyond%20subgraph%20frequency%20estimation%2C%20their%0Aapplication%20to%20network%20motif%20significance-profile%20%28SP%29%20prediction%20remains%0Aunder-explored%2C%20with%20no%20established%20benchmarks%20in%20the%20literature.%20We%20propose%20to%0Aaddress%20this%20problem%2C%20framing%20SP%20estimation%20as%20a%20task%20independent%20of%20subgraph%0Afrequency%20estimation.%20Our%20approach%20shifts%20from%20frequency%20counting%20to%20direct%20SP%0Aestimation%20and%20modulates%20the%20problem%20as%20multitarget%20regression.%20The%0Areformulation%20is%20optimised%20for%20interpretability%2C%20stability%20and%20scalability%20on%0Alarge%20graphs.%20We%20validate%20our%20method%20using%20a%20large%20synthetic%20dataset%20and%0Afurther%20test%20it%20on%20real-world%20graphs.%20Our%20experiments%20reveal%20that%201-WL%20limited%0Amodels%20struggle%20to%20make%20precise%20estimations%20of%20SPs.%20However%2C%20they%20can%0Ageneralise%20to%20approximate%20the%20graph%20generation%20processes%20of%20networks%20by%0Acomparing%20their%20predicted%20SP%20with%20the%20ones%20originating%20from%20synthetic%0Agenerators.%20This%20first%20study%20on%20GNN-based%20motif%20estimation%20also%20hints%20at%20how%0Ausing%20direct%20SP%20estimation%20can%20help%20go%20past%20the%20theoretical%20limitations%20that%0Amotif%20estimation%20faces%20when%20performed%20through%20subgraph%20counting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15709v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudying%2520and%2520Improving%2520Graph%2520Neural%2520Network-based%2520Motif%2520Estimation%26entry.906535625%3DPedro%2520C.%2520Vieira%2520and%2520Miguel%2520E.%2520P.%2520Silva%2520and%2520Pedro%2520Manuel%2520Pinto%2520Ribeiro%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520a%2520predominant%2520method%2520for%2520graph%250Arepresentation%2520learning.%2520However%252C%2520beyond%2520subgraph%2520frequency%2520estimation%252C%2520their%250Aapplication%2520to%2520network%2520motif%2520significance-profile%2520%2528SP%2529%2520prediction%2520remains%250Aunder-explored%252C%2520with%2520no%2520established%2520benchmarks%2520in%2520the%2520literature.%2520We%2520propose%2520to%250Aaddress%2520this%2520problem%252C%2520framing%2520SP%2520estimation%2520as%2520a%2520task%2520independent%2520of%2520subgraph%250Afrequency%2520estimation.%2520Our%2520approach%2520shifts%2520from%2520frequency%2520counting%2520to%2520direct%2520SP%250Aestimation%2520and%2520modulates%2520the%2520problem%2520as%2520multitarget%2520regression.%2520The%250Areformulation%2520is%2520optimised%2520for%2520interpretability%252C%2520stability%2520and%2520scalability%2520on%250Alarge%2520graphs.%2520We%2520validate%2520our%2520method%2520using%2520a%2520large%2520synthetic%2520dataset%2520and%250Afurther%2520test%2520it%2520on%2520real-world%2520graphs.%2520Our%2520experiments%2520reveal%2520that%25201-WL%2520limited%250Amodels%2520struggle%2520to%2520make%2520precise%2520estimations%2520of%2520SPs.%2520However%252C%2520they%2520can%250Ageneralise%2520to%2520approximate%2520the%2520graph%2520generation%2520processes%2520of%2520networks%2520by%250Acomparing%2520their%2520predicted%2520SP%2520with%2520the%2520ones%2520originating%2520from%2520synthetic%250Agenerators.%2520This%2520first%2520study%2520on%2520GNN-based%2520motif%2520estimation%2520also%2520hints%2520at%2520how%250Ausing%2520direct%2520SP%2520estimation%2520can%2520help%2520go%2520past%2520the%2520theoretical%2520limitations%2520that%250Amotif%2520estimation%2520faces%2520when%2520performed%2520through%2520subgraph%2520counting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15709v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Studying%20and%20Improving%20Graph%20Neural%20Network-based%20Motif%20Estimation&entry.906535625=Pedro%20C.%20Vieira%20and%20Miguel%20E.%20P.%20Silva%20and%20Pedro%20Manuel%20Pinto%20Ribeiro&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20predominant%20method%20for%20graph%0Arepresentation%20learning.%20However%2C%20beyond%20subgraph%20frequency%20estimation%2C%20their%0Aapplication%20to%20network%20motif%20significance-profile%20%28SP%29%20prediction%20remains%0Aunder-explored%2C%20with%20no%20established%20benchmarks%20in%20the%20literature.%20We%20propose%20to%0Aaddress%20this%20problem%2C%20framing%20SP%20estimation%20as%20a%20task%20independent%20of%20subgraph%0Afrequency%20estimation.%20Our%20approach%20shifts%20from%20frequency%20counting%20to%20direct%20SP%0Aestimation%20and%20modulates%20the%20problem%20as%20multitarget%20regression.%20The%0Areformulation%20is%20optimised%20for%20interpretability%2C%20stability%20and%20scalability%20on%0Alarge%20graphs.%20We%20validate%20our%20method%20using%20a%20large%20synthetic%20dataset%20and%0Afurther%20test%20it%20on%20real-world%20graphs.%20Our%20experiments%20reveal%20that%201-WL%20limited%0Amodels%20struggle%20to%20make%20precise%20estimations%20of%20SPs.%20However%2C%20they%20can%0Ageneralise%20to%20approximate%20the%20graph%20generation%20processes%20of%20networks%20by%0Acomparing%20their%20predicted%20SP%20with%20the%20ones%20originating%20from%20synthetic%0Agenerators.%20This%20first%20study%20on%20GNN-based%20motif%20estimation%20also%20hints%20at%20how%0Ausing%20direct%20SP%20estimation%20can%20help%20go%20past%20the%20theoretical%20limitations%20that%0Amotif%20estimation%20faces%20when%20performed%20through%20subgraph%20counting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15709v3&entry.124074799=Read"},
{"title": "Energy-Guided Decoding for Object Hallucination Mitigation", "author": "Xixi Liu and Ailin Deng and Christopher Zach", "abstract": "  Mitigating object hallucination in large vision-language models (LVLMs) is\ncritical to their safe deployment. Existing methods either are restricted to\nspecific decoding methods, or demand sophisticated modifications to visual\ninputs, or rely on knowledge from external models. In this work, we first\nreveal the phenomenon that VLMs exhibit significant imbalance in the ``Yes''\nratio ( \\ie, the fraction of ``Yes'' answers among the total number of\nquestions) across three different visual question answering (VQA) datasets.\nFurthermore, we propose an energy-based decoding method, which dynamically\nselects the hidden states from the layer with minimal energy score. It is\nsimple yet effective in reducing the bias for the yes ratio while boosting\nperformance across three benchmarks (POPE, MME, and MMVP). Our method\nconsistently improves accuracy and F1 score on three VQA datasets across three\ncommonly used VLMs over several baseline methods. The average accuracy\nimprovement is 4.82% compared to greedy decoding. Moreover, the average\nyes-ratio gap reduction is 8.81%, meaning the proposed method is less biased as\nshown in Figure 1.\n", "link": "http://arxiv.org/abs/2507.07731v1", "date": "2025-07-10", "relevancy": 2.2413, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5674}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5674}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Guided%20Decoding%20for%20Object%20Hallucination%20Mitigation&body=Title%3A%20Energy-Guided%20Decoding%20for%20Object%20Hallucination%20Mitigation%0AAuthor%3A%20Xixi%20Liu%20and%20Ailin%20Deng%20and%20Christopher%20Zach%0AAbstract%3A%20%20%20Mitigating%20object%20hallucination%20in%20large%20vision-language%20models%20%28LVLMs%29%20is%0Acritical%20to%20their%20safe%20deployment.%20Existing%20methods%20either%20are%20restricted%20to%0Aspecific%20decoding%20methods%2C%20or%20demand%20sophisticated%20modifications%20to%20visual%0Ainputs%2C%20or%20rely%20on%20knowledge%20from%20external%20models.%20In%20this%20work%2C%20we%20first%0Areveal%20the%20phenomenon%20that%20VLMs%20exhibit%20significant%20imbalance%20in%20the%20%60%60Yes%27%27%0Aratio%20%28%20%5Cie%2C%20the%20fraction%20of%20%60%60Yes%27%27%20answers%20among%20the%20total%20number%20of%0Aquestions%29%20across%20three%20different%20visual%20question%20answering%20%28VQA%29%20datasets.%0AFurthermore%2C%20we%20propose%20an%20energy-based%20decoding%20method%2C%20which%20dynamically%0Aselects%20the%20hidden%20states%20from%20the%20layer%20with%20minimal%20energy%20score.%20It%20is%0Asimple%20yet%20effective%20in%20reducing%20the%20bias%20for%20the%20yes%20ratio%20while%20boosting%0Aperformance%20across%20three%20benchmarks%20%28POPE%2C%20MME%2C%20and%20MMVP%29.%20Our%20method%0Aconsistently%20improves%20accuracy%20and%20F1%20score%20on%20three%20VQA%20datasets%20across%20three%0Acommonly%20used%20VLMs%20over%20several%20baseline%20methods.%20The%20average%20accuracy%0Aimprovement%20is%204.82%25%20compared%20to%20greedy%20decoding.%20Moreover%2C%20the%20average%0Ayes-ratio%20gap%20reduction%20is%208.81%25%2C%20meaning%20the%20proposed%20method%20is%20less%20biased%20as%0Ashown%20in%20Figure%201.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Guided%2520Decoding%2520for%2520Object%2520Hallucination%2520Mitigation%26entry.906535625%3DXixi%2520Liu%2520and%2520Ailin%2520Deng%2520and%2520Christopher%2520Zach%26entry.1292438233%3D%2520%2520Mitigating%2520object%2520hallucination%2520in%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520is%250Acritical%2520to%2520their%2520safe%2520deployment.%2520Existing%2520methods%2520either%2520are%2520restricted%2520to%250Aspecific%2520decoding%2520methods%252C%2520or%2520demand%2520sophisticated%2520modifications%2520to%2520visual%250Ainputs%252C%2520or%2520rely%2520on%2520knowledge%2520from%2520external%2520models.%2520In%2520this%2520work%252C%2520we%2520first%250Areveal%2520the%2520phenomenon%2520that%2520VLMs%2520exhibit%2520significant%2520imbalance%2520in%2520the%2520%2560%2560Yes%2527%2527%250Aratio%2520%2528%2520%255Cie%252C%2520the%2520fraction%2520of%2520%2560%2560Yes%2527%2527%2520answers%2520among%2520the%2520total%2520number%2520of%250Aquestions%2529%2520across%2520three%2520different%2520visual%2520question%2520answering%2520%2528VQA%2529%2520datasets.%250AFurthermore%252C%2520we%2520propose%2520an%2520energy-based%2520decoding%2520method%252C%2520which%2520dynamically%250Aselects%2520the%2520hidden%2520states%2520from%2520the%2520layer%2520with%2520minimal%2520energy%2520score.%2520It%2520is%250Asimple%2520yet%2520effective%2520in%2520reducing%2520the%2520bias%2520for%2520the%2520yes%2520ratio%2520while%2520boosting%250Aperformance%2520across%2520three%2520benchmarks%2520%2528POPE%252C%2520MME%252C%2520and%2520MMVP%2529.%2520Our%2520method%250Aconsistently%2520improves%2520accuracy%2520and%2520F1%2520score%2520on%2520three%2520VQA%2520datasets%2520across%2520three%250Acommonly%2520used%2520VLMs%2520over%2520several%2520baseline%2520methods.%2520The%2520average%2520accuracy%250Aimprovement%2520is%25204.82%2525%2520compared%2520to%2520greedy%2520decoding.%2520Moreover%252C%2520the%2520average%250Ayes-ratio%2520gap%2520reduction%2520is%25208.81%2525%252C%2520meaning%2520the%2520proposed%2520method%2520is%2520less%2520biased%2520as%250Ashown%2520in%2520Figure%25201.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Guided%20Decoding%20for%20Object%20Hallucination%20Mitigation&entry.906535625=Xixi%20Liu%20and%20Ailin%20Deng%20and%20Christopher%20Zach&entry.1292438233=%20%20Mitigating%20object%20hallucination%20in%20large%20vision-language%20models%20%28LVLMs%29%20is%0Acritical%20to%20their%20safe%20deployment.%20Existing%20methods%20either%20are%20restricted%20to%0Aspecific%20decoding%20methods%2C%20or%20demand%20sophisticated%20modifications%20to%20visual%0Ainputs%2C%20or%20rely%20on%20knowledge%20from%20external%20models.%20In%20this%20work%2C%20we%20first%0Areveal%20the%20phenomenon%20that%20VLMs%20exhibit%20significant%20imbalance%20in%20the%20%60%60Yes%27%27%0Aratio%20%28%20%5Cie%2C%20the%20fraction%20of%20%60%60Yes%27%27%20answers%20among%20the%20total%20number%20of%0Aquestions%29%20across%20three%20different%20visual%20question%20answering%20%28VQA%29%20datasets.%0AFurthermore%2C%20we%20propose%20an%20energy-based%20decoding%20method%2C%20which%20dynamically%0Aselects%20the%20hidden%20states%20from%20the%20layer%20with%20minimal%20energy%20score.%20It%20is%0Asimple%20yet%20effective%20in%20reducing%20the%20bias%20for%20the%20yes%20ratio%20while%20boosting%0Aperformance%20across%20three%20benchmarks%20%28POPE%2C%20MME%2C%20and%20MMVP%29.%20Our%20method%0Aconsistently%20improves%20accuracy%20and%20F1%20score%20on%20three%20VQA%20datasets%20across%20three%0Acommonly%20used%20VLMs%20over%20several%20baseline%20methods.%20The%20average%20accuracy%0Aimprovement%20is%204.82%25%20compared%20to%20greedy%20decoding.%20Moreover%2C%20the%20average%0Ayes-ratio%20gap%20reduction%20is%208.81%25%2C%20meaning%20the%20proposed%20method%20is%20less%20biased%20as%0Ashown%20in%20Figure%201.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07731v1&entry.124074799=Read"},
{"title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble\n  Voting", "author": "Juyi Lin and Amir Taherin and Arash Akbari and Arman Akbari and Lei Lu and Guangyu Chen and Taskin Padir and Xiaomeng Yang and Weiwei Chen and Yiqian Li and Xue Lin and David Kaeli and Pu Zhao and Yanzhi Wang", "abstract": "  Recent large-scale Vision Language Action (VLA) models have shown superior\nperformance in robotic manipulation tasks guided by natural language. However,\ntheir generalization remains limited when applied to novel objects or\nunfamiliar environments that lie outside the training distribution. To address\nthis, many existing approaches integrate additional components such as depth\nestimation, segmentation, or even diffusion to improve generalization, at the\ncost of adding significant computation overhead, resulting in low efficiency.\nThis motivates the exploration of efficient action prediction methods, which\nare independent of additional high-level visual representations or diffusion\ntechniques. In this work, we propose VOTE, an efficient and general framework\nfor the optimization and acceleration of VLA models. In details, we propose a\nnovel tokenizer-free fine-tuning approach for parallel accurate action\nprediction, which reduces computational overhead and accelerates inference\nspeed. Additionally, we adopt an ensemble voting strategy for the action\nsampling, which significantly improves model performance and enhances\ngeneralization. Experimental results show that our method achieves\nstate-of-the-art performance with 35x faster inference and 145 Hz throughput.\nAll the details and codes will be open-sourced.\n", "link": "http://arxiv.org/abs/2507.05116v2", "date": "2025-07-10", "relevancy": 2.2356, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5606}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5606}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VOTE%3A%20Vision-Language-Action%20Optimization%20with%20Trajectory%20Ensemble%0A%20%20Voting&body=Title%3A%20VOTE%3A%20Vision-Language-Action%20Optimization%20with%20Trajectory%20Ensemble%0A%20%20Voting%0AAuthor%3A%20Juyi%20Lin%20and%20Amir%20Taherin%20and%20Arash%20Akbari%20and%20Arman%20Akbari%20and%20Lei%20Lu%20and%20Guangyu%20Chen%20and%20Taskin%20Padir%20and%20Xiaomeng%20Yang%20and%20Weiwei%20Chen%20and%20Yiqian%20Li%20and%20Xue%20Lin%20and%20David%20Kaeli%20and%20Pu%20Zhao%20and%20Yanzhi%20Wang%0AAbstract%3A%20%20%20Recent%20large-scale%20Vision%20Language%20Action%20%28VLA%29%20models%20have%20shown%20superior%0Aperformance%20in%20robotic%20manipulation%20tasks%20guided%20by%20natural%20language.%20However%2C%0Atheir%20generalization%20remains%20limited%20when%20applied%20to%20novel%20objects%20or%0Aunfamiliar%20environments%20that%20lie%20outside%20the%20training%20distribution.%20To%20address%0Athis%2C%20many%20existing%20approaches%20integrate%20additional%20components%20such%20as%20depth%0Aestimation%2C%20segmentation%2C%20or%20even%20diffusion%20to%20improve%20generalization%2C%20at%20the%0Acost%20of%20adding%20significant%20computation%20overhead%2C%20resulting%20in%20low%20efficiency.%0AThis%20motivates%20the%20exploration%20of%20efficient%20action%20prediction%20methods%2C%20which%0Aare%20independent%20of%20additional%20high-level%20visual%20representations%20or%20diffusion%0Atechniques.%20In%20this%20work%2C%20we%20propose%20VOTE%2C%20an%20efficient%20and%20general%20framework%0Afor%20the%20optimization%20and%20acceleration%20of%20VLA%20models.%20In%20details%2C%20we%20propose%20a%0Anovel%20tokenizer-free%20fine-tuning%20approach%20for%20parallel%20accurate%20action%0Aprediction%2C%20which%20reduces%20computational%20overhead%20and%20accelerates%20inference%0Aspeed.%20Additionally%2C%20we%20adopt%20an%20ensemble%20voting%20strategy%20for%20the%20action%0Asampling%2C%20which%20significantly%20improves%20model%20performance%20and%20enhances%0Ageneralization.%20Experimental%20results%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20with%2035x%20faster%20inference%20and%20145%20Hz%20throughput.%0AAll%20the%20details%20and%20codes%20will%20be%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05116v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVOTE%253A%2520Vision-Language-Action%2520Optimization%2520with%2520Trajectory%2520Ensemble%250A%2520%2520Voting%26entry.906535625%3DJuyi%2520Lin%2520and%2520Amir%2520Taherin%2520and%2520Arash%2520Akbari%2520and%2520Arman%2520Akbari%2520and%2520Lei%2520Lu%2520and%2520Guangyu%2520Chen%2520and%2520Taskin%2520Padir%2520and%2520Xiaomeng%2520Yang%2520and%2520Weiwei%2520Chen%2520and%2520Yiqian%2520Li%2520and%2520Xue%2520Lin%2520and%2520David%2520Kaeli%2520and%2520Pu%2520Zhao%2520and%2520Yanzhi%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520large-scale%2520Vision%2520Language%2520Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520superior%250Aperformance%2520in%2520robotic%2520manipulation%2520tasks%2520guided%2520by%2520natural%2520language.%2520However%252C%250Atheir%2520generalization%2520remains%2520limited%2520when%2520applied%2520to%2520novel%2520objects%2520or%250Aunfamiliar%2520environments%2520that%2520lie%2520outside%2520the%2520training%2520distribution.%2520To%2520address%250Athis%252C%2520many%2520existing%2520approaches%2520integrate%2520additional%2520components%2520such%2520as%2520depth%250Aestimation%252C%2520segmentation%252C%2520or%2520even%2520diffusion%2520to%2520improve%2520generalization%252C%2520at%2520the%250Acost%2520of%2520adding%2520significant%2520computation%2520overhead%252C%2520resulting%2520in%2520low%2520efficiency.%250AThis%2520motivates%2520the%2520exploration%2520of%2520efficient%2520action%2520prediction%2520methods%252C%2520which%250Aare%2520independent%2520of%2520additional%2520high-level%2520visual%2520representations%2520or%2520diffusion%250Atechniques.%2520In%2520this%2520work%252C%2520we%2520propose%2520VOTE%252C%2520an%2520efficient%2520and%2520general%2520framework%250Afor%2520the%2520optimization%2520and%2520acceleration%2520of%2520VLA%2520models.%2520In%2520details%252C%2520we%2520propose%2520a%250Anovel%2520tokenizer-free%2520fine-tuning%2520approach%2520for%2520parallel%2520accurate%2520action%250Aprediction%252C%2520which%2520reduces%2520computational%2520overhead%2520and%2520accelerates%2520inference%250Aspeed.%2520Additionally%252C%2520we%2520adopt%2520an%2520ensemble%2520voting%2520strategy%2520for%2520the%2520action%250Asampling%252C%2520which%2520significantly%2520improves%2520model%2520performance%2520and%2520enhances%250Ageneralization.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520with%252035x%2520faster%2520inference%2520and%2520145%2520Hz%2520throughput.%250AAll%2520the%2520details%2520and%2520codes%2520will%2520be%2520open-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05116v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VOTE%3A%20Vision-Language-Action%20Optimization%20with%20Trajectory%20Ensemble%0A%20%20Voting&entry.906535625=Juyi%20Lin%20and%20Amir%20Taherin%20and%20Arash%20Akbari%20and%20Arman%20Akbari%20and%20Lei%20Lu%20and%20Guangyu%20Chen%20and%20Taskin%20Padir%20and%20Xiaomeng%20Yang%20and%20Weiwei%20Chen%20and%20Yiqian%20Li%20and%20Xue%20Lin%20and%20David%20Kaeli%20and%20Pu%20Zhao%20and%20Yanzhi%20Wang&entry.1292438233=%20%20Recent%20large-scale%20Vision%20Language%20Action%20%28VLA%29%20models%20have%20shown%20superior%0Aperformance%20in%20robotic%20manipulation%20tasks%20guided%20by%20natural%20language.%20However%2C%0Atheir%20generalization%20remains%20limited%20when%20applied%20to%20novel%20objects%20or%0Aunfamiliar%20environments%20that%20lie%20outside%20the%20training%20distribution.%20To%20address%0Athis%2C%20many%20existing%20approaches%20integrate%20additional%20components%20such%20as%20depth%0Aestimation%2C%20segmentation%2C%20or%20even%20diffusion%20to%20improve%20generalization%2C%20at%20the%0Acost%20of%20adding%20significant%20computation%20overhead%2C%20resulting%20in%20low%20efficiency.%0AThis%20motivates%20the%20exploration%20of%20efficient%20action%20prediction%20methods%2C%20which%0Aare%20independent%20of%20additional%20high-level%20visual%20representations%20or%20diffusion%0Atechniques.%20In%20this%20work%2C%20we%20propose%20VOTE%2C%20an%20efficient%20and%20general%20framework%0Afor%20the%20optimization%20and%20acceleration%20of%20VLA%20models.%20In%20details%2C%20we%20propose%20a%0Anovel%20tokenizer-free%20fine-tuning%20approach%20for%20parallel%20accurate%20action%0Aprediction%2C%20which%20reduces%20computational%20overhead%20and%20accelerates%20inference%0Aspeed.%20Additionally%2C%20we%20adopt%20an%20ensemble%20voting%20strategy%20for%20the%20action%0Asampling%2C%20which%20significantly%20improves%20model%20performance%20and%20enhances%0Ageneralization.%20Experimental%20results%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20with%2035x%20faster%20inference%20and%20145%20Hz%20throughput.%0AAll%20the%20details%20and%20codes%20will%20be%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05116v2&entry.124074799=Read"},
{"title": "Single-pass Adaptive Image Tokenization for Minimum Program Search", "author": "Shivam Duggal and Sanghyun Byun and William T. Freeman and Antonio Torralba and Phillip Isola", "abstract": "  According to Algorithmic Information Theory (AIT) -- Intelligent\nrepresentations compress data into the shortest possible program that can\nreconstruct its content, exhibiting low Kolmogorov Complexity (KC). In\ncontrast, most visual representation learning systems use fixed-length\nrepresentations for all inputs, ignoring variations in complexity or\nfamiliarity. Recent adaptive tokenization methods address this by allocating\nvariable-length representations but typically require test-time search over\nmultiple encodings to find the most predictive one. Inspired by Kolmogorov\nComplexity principles, we propose a single-pass adaptive tokenizer, KARL, which\npredicts the appropriate number of tokens for an image in a single forward\npass, halting once its approximate KC is reached. The token count serves as a\nproxy for the minimum description length. KARL's training procedure closely\nresembles the Upside-Down Reinforcement Learning paradigm, as it learns to\nconditionally predict token halting based on a desired reconstruction quality.\nKARL matches the performance of recent adaptive tokenizers while operating in a\nsingle pass. We present scaling laws for KARL, analyzing the role of\nencoder/decoder size, continuous vs. discrete tokenization and more.\nAdditionally, we offer a conceptual study drawing an analogy between Adaptive\nImage Tokenization and Algorithmic Information Theory, examining the predicted\nimage complexity (KC) across axes such as structure vs. noise and in- vs.\nout-of-distribution familiarity -- revealing alignment with human intuition.\n", "link": "http://arxiv.org/abs/2507.07995v1", "date": "2025-07-10", "relevancy": 2.2343, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6155}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5189}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-pass%20Adaptive%20Image%20Tokenization%20for%20Minimum%20Program%20Search&body=Title%3A%20Single-pass%20Adaptive%20Image%20Tokenization%20for%20Minimum%20Program%20Search%0AAuthor%3A%20Shivam%20Duggal%20and%20Sanghyun%20Byun%20and%20William%20T.%20Freeman%20and%20Antonio%20Torralba%20and%20Phillip%20Isola%0AAbstract%3A%20%20%20According%20to%20Algorithmic%20Information%20Theory%20%28AIT%29%20--%20Intelligent%0Arepresentations%20compress%20data%20into%20the%20shortest%20possible%20program%20that%20can%0Areconstruct%20its%20content%2C%20exhibiting%20low%20Kolmogorov%20Complexity%20%28KC%29.%20In%0Acontrast%2C%20most%20visual%20representation%20learning%20systems%20use%20fixed-length%0Arepresentations%20for%20all%20inputs%2C%20ignoring%20variations%20in%20complexity%20or%0Afamiliarity.%20Recent%20adaptive%20tokenization%20methods%20address%20this%20by%20allocating%0Avariable-length%20representations%20but%20typically%20require%20test-time%20search%20over%0Amultiple%20encodings%20to%20find%20the%20most%20predictive%20one.%20Inspired%20by%20Kolmogorov%0AComplexity%20principles%2C%20we%20propose%20a%20single-pass%20adaptive%20tokenizer%2C%20KARL%2C%20which%0Apredicts%20the%20appropriate%20number%20of%20tokens%20for%20an%20image%20in%20a%20single%20forward%0Apass%2C%20halting%20once%20its%20approximate%20KC%20is%20reached.%20The%20token%20count%20serves%20as%20a%0Aproxy%20for%20the%20minimum%20description%20length.%20KARL%27s%20training%20procedure%20closely%0Aresembles%20the%20Upside-Down%20Reinforcement%20Learning%20paradigm%2C%20as%20it%20learns%20to%0Aconditionally%20predict%20token%20halting%20based%20on%20a%20desired%20reconstruction%20quality.%0AKARL%20matches%20the%20performance%20of%20recent%20adaptive%20tokenizers%20while%20operating%20in%20a%0Asingle%20pass.%20We%20present%20scaling%20laws%20for%20KARL%2C%20analyzing%20the%20role%20of%0Aencoder/decoder%20size%2C%20continuous%20vs.%20discrete%20tokenization%20and%20more.%0AAdditionally%2C%20we%20offer%20a%20conceptual%20study%20drawing%20an%20analogy%20between%20Adaptive%0AImage%20Tokenization%20and%20Algorithmic%20Information%20Theory%2C%20examining%20the%20predicted%0Aimage%20complexity%20%28KC%29%20across%20axes%20such%20as%20structure%20vs.%20noise%20and%20in-%20vs.%0Aout-of-distribution%20familiarity%20--%20revealing%20alignment%20with%20human%20intuition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-pass%2520Adaptive%2520Image%2520Tokenization%2520for%2520Minimum%2520Program%2520Search%26entry.906535625%3DShivam%2520Duggal%2520and%2520Sanghyun%2520Byun%2520and%2520William%2520T.%2520Freeman%2520and%2520Antonio%2520Torralba%2520and%2520Phillip%2520Isola%26entry.1292438233%3D%2520%2520According%2520to%2520Algorithmic%2520Information%2520Theory%2520%2528AIT%2529%2520--%2520Intelligent%250Arepresentations%2520compress%2520data%2520into%2520the%2520shortest%2520possible%2520program%2520that%2520can%250Areconstruct%2520its%2520content%252C%2520exhibiting%2520low%2520Kolmogorov%2520Complexity%2520%2528KC%2529.%2520In%250Acontrast%252C%2520most%2520visual%2520representation%2520learning%2520systems%2520use%2520fixed-length%250Arepresentations%2520for%2520all%2520inputs%252C%2520ignoring%2520variations%2520in%2520complexity%2520or%250Afamiliarity.%2520Recent%2520adaptive%2520tokenization%2520methods%2520address%2520this%2520by%2520allocating%250Avariable-length%2520representations%2520but%2520typically%2520require%2520test-time%2520search%2520over%250Amultiple%2520encodings%2520to%2520find%2520the%2520most%2520predictive%2520one.%2520Inspired%2520by%2520Kolmogorov%250AComplexity%2520principles%252C%2520we%2520propose%2520a%2520single-pass%2520adaptive%2520tokenizer%252C%2520KARL%252C%2520which%250Apredicts%2520the%2520appropriate%2520number%2520of%2520tokens%2520for%2520an%2520image%2520in%2520a%2520single%2520forward%250Apass%252C%2520halting%2520once%2520its%2520approximate%2520KC%2520is%2520reached.%2520The%2520token%2520count%2520serves%2520as%2520a%250Aproxy%2520for%2520the%2520minimum%2520description%2520length.%2520KARL%2527s%2520training%2520procedure%2520closely%250Aresembles%2520the%2520Upside-Down%2520Reinforcement%2520Learning%2520paradigm%252C%2520as%2520it%2520learns%2520to%250Aconditionally%2520predict%2520token%2520halting%2520based%2520on%2520a%2520desired%2520reconstruction%2520quality.%250AKARL%2520matches%2520the%2520performance%2520of%2520recent%2520adaptive%2520tokenizers%2520while%2520operating%2520in%2520a%250Asingle%2520pass.%2520We%2520present%2520scaling%2520laws%2520for%2520KARL%252C%2520analyzing%2520the%2520role%2520of%250Aencoder/decoder%2520size%252C%2520continuous%2520vs.%2520discrete%2520tokenization%2520and%2520more.%250AAdditionally%252C%2520we%2520offer%2520a%2520conceptual%2520study%2520drawing%2520an%2520analogy%2520between%2520Adaptive%250AImage%2520Tokenization%2520and%2520Algorithmic%2520Information%2520Theory%252C%2520examining%2520the%2520predicted%250Aimage%2520complexity%2520%2528KC%2529%2520across%2520axes%2520such%2520as%2520structure%2520vs.%2520noise%2520and%2520in-%2520vs.%250Aout-of-distribution%2520familiarity%2520--%2520revealing%2520alignment%2520with%2520human%2520intuition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-pass%20Adaptive%20Image%20Tokenization%20for%20Minimum%20Program%20Search&entry.906535625=Shivam%20Duggal%20and%20Sanghyun%20Byun%20and%20William%20T.%20Freeman%20and%20Antonio%20Torralba%20and%20Phillip%20Isola&entry.1292438233=%20%20According%20to%20Algorithmic%20Information%20Theory%20%28AIT%29%20--%20Intelligent%0Arepresentations%20compress%20data%20into%20the%20shortest%20possible%20program%20that%20can%0Areconstruct%20its%20content%2C%20exhibiting%20low%20Kolmogorov%20Complexity%20%28KC%29.%20In%0Acontrast%2C%20most%20visual%20representation%20learning%20systems%20use%20fixed-length%0Arepresentations%20for%20all%20inputs%2C%20ignoring%20variations%20in%20complexity%20or%0Afamiliarity.%20Recent%20adaptive%20tokenization%20methods%20address%20this%20by%20allocating%0Avariable-length%20representations%20but%20typically%20require%20test-time%20search%20over%0Amultiple%20encodings%20to%20find%20the%20most%20predictive%20one.%20Inspired%20by%20Kolmogorov%0AComplexity%20principles%2C%20we%20propose%20a%20single-pass%20adaptive%20tokenizer%2C%20KARL%2C%20which%0Apredicts%20the%20appropriate%20number%20of%20tokens%20for%20an%20image%20in%20a%20single%20forward%0Apass%2C%20halting%20once%20its%20approximate%20KC%20is%20reached.%20The%20token%20count%20serves%20as%20a%0Aproxy%20for%20the%20minimum%20description%20length.%20KARL%27s%20training%20procedure%20closely%0Aresembles%20the%20Upside-Down%20Reinforcement%20Learning%20paradigm%2C%20as%20it%20learns%20to%0Aconditionally%20predict%20token%20halting%20based%20on%20a%20desired%20reconstruction%20quality.%0AKARL%20matches%20the%20performance%20of%20recent%20adaptive%20tokenizers%20while%20operating%20in%20a%0Asingle%20pass.%20We%20present%20scaling%20laws%20for%20KARL%2C%20analyzing%20the%20role%20of%0Aencoder/decoder%20size%2C%20continuous%20vs.%20discrete%20tokenization%20and%20more.%0AAdditionally%2C%20we%20offer%20a%20conceptual%20study%20drawing%20an%20analogy%20between%20Adaptive%0AImage%20Tokenization%20and%20Algorithmic%20Information%20Theory%2C%20examining%20the%20predicted%0Aimage%20complexity%20%28KC%29%20across%20axes%20such%20as%20structure%20vs.%20noise%20and%20in-%20vs.%0Aout-of-distribution%20familiarity%20--%20revealing%20alignment%20with%20human%20intuition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07995v1&entry.124074799=Read"},
{"title": "Motion-Aware Adaptive Pixel Pruning for Efficient Local Motion\n  Deblurring", "author": "Wei Shang and Dongwei Ren and Wanying Zhang and Pengfei Zhu and Qinghua Hu and Wangmeng Zuo", "abstract": "  Local motion blur in digital images originates from the relative motion\nbetween dynamic objects and static imaging systems during exposure. Existing\ndeblurring methods face significant challenges in addressing this problem due\nto their inefficient allocation of computational resources and inadequate\nhandling of spatially varying blur patterns. To overcome these limitations, we\nfirst propose a trainable mask predictor that identifies blurred regions in the\nimage. During training, we employ blur masks to exclude sharp regions. For\ninference optimization, we implement structural reparameterization by\nconverting $3\\times 3$ convolutions to computationally efficient $1\\times 1$\nconvolutions, enabling pixel-level pruning of sharp areas to reduce\ncomputation. Second, we develop an intra-frame motion analyzer that translates\nrelative pixel displacements into motion trajectories, establishing adaptive\nguidance for region-specific blur restoration. Our method is trained end-to-end\nusing a combination of reconstruction loss, reblur loss, and mask loss guided\nby annotated blur masks. Extensive experiments demonstrate superior performance\nover state-of-the-art methods on both local and global blur datasets while\nreducing FLOPs by 49\\% compared to SOTA models (e.g., LMD-ViT). The source code\nis available at https://github.com/shangwei5/M2AENet.\n", "link": "http://arxiv.org/abs/2507.07708v1", "date": "2025-07-10", "relevancy": 2.2312, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5769}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5443}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion-Aware%20Adaptive%20Pixel%20Pruning%20for%20Efficient%20Local%20Motion%0A%20%20Deblurring&body=Title%3A%20Motion-Aware%20Adaptive%20Pixel%20Pruning%20for%20Efficient%20Local%20Motion%0A%20%20Deblurring%0AAuthor%3A%20Wei%20Shang%20and%20Dongwei%20Ren%20and%20Wanying%20Zhang%20and%20Pengfei%20Zhu%20and%20Qinghua%20Hu%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Local%20motion%20blur%20in%20digital%20images%20originates%20from%20the%20relative%20motion%0Abetween%20dynamic%20objects%20and%20static%20imaging%20systems%20during%20exposure.%20Existing%0Adeblurring%20methods%20face%20significant%20challenges%20in%20addressing%20this%20problem%20due%0Ato%20their%20inefficient%20allocation%20of%20computational%20resources%20and%20inadequate%0Ahandling%20of%20spatially%20varying%20blur%20patterns.%20To%20overcome%20these%20limitations%2C%20we%0Afirst%20propose%20a%20trainable%20mask%20predictor%20that%20identifies%20blurred%20regions%20in%20the%0Aimage.%20During%20training%2C%20we%20employ%20blur%20masks%20to%20exclude%20sharp%20regions.%20For%0Ainference%20optimization%2C%20we%20implement%20structural%20reparameterization%20by%0Aconverting%20%243%5Ctimes%203%24%20convolutions%20to%20computationally%20efficient%20%241%5Ctimes%201%24%0Aconvolutions%2C%20enabling%20pixel-level%20pruning%20of%20sharp%20areas%20to%20reduce%0Acomputation.%20Second%2C%20we%20develop%20an%20intra-frame%20motion%20analyzer%20that%20translates%0Arelative%20pixel%20displacements%20into%20motion%20trajectories%2C%20establishing%20adaptive%0Aguidance%20for%20region-specific%20blur%20restoration.%20Our%20method%20is%20trained%20end-to-end%0Ausing%20a%20combination%20of%20reconstruction%20loss%2C%20reblur%20loss%2C%20and%20mask%20loss%20guided%0Aby%20annotated%20blur%20masks.%20Extensive%20experiments%20demonstrate%20superior%20performance%0Aover%20state-of-the-art%20methods%20on%20both%20local%20and%20global%20blur%20datasets%20while%0Areducing%20FLOPs%20by%2049%5C%25%20compared%20to%20SOTA%20models%20%28e.g.%2C%20LMD-ViT%29.%20The%20source%20code%0Ais%20available%20at%20https%3A//github.com/shangwei5/M2AENet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion-Aware%2520Adaptive%2520Pixel%2520Pruning%2520for%2520Efficient%2520Local%2520Motion%250A%2520%2520Deblurring%26entry.906535625%3DWei%2520Shang%2520and%2520Dongwei%2520Ren%2520and%2520Wanying%2520Zhang%2520and%2520Pengfei%2520Zhu%2520and%2520Qinghua%2520Hu%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Local%2520motion%2520blur%2520in%2520digital%2520images%2520originates%2520from%2520the%2520relative%2520motion%250Abetween%2520dynamic%2520objects%2520and%2520static%2520imaging%2520systems%2520during%2520exposure.%2520Existing%250Adeblurring%2520methods%2520face%2520significant%2520challenges%2520in%2520addressing%2520this%2520problem%2520due%250Ato%2520their%2520inefficient%2520allocation%2520of%2520computational%2520resources%2520and%2520inadequate%250Ahandling%2520of%2520spatially%2520varying%2520blur%2520patterns.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Afirst%2520propose%2520a%2520trainable%2520mask%2520predictor%2520that%2520identifies%2520blurred%2520regions%2520in%2520the%250Aimage.%2520During%2520training%252C%2520we%2520employ%2520blur%2520masks%2520to%2520exclude%2520sharp%2520regions.%2520For%250Ainference%2520optimization%252C%2520we%2520implement%2520structural%2520reparameterization%2520by%250Aconverting%2520%25243%255Ctimes%25203%2524%2520convolutions%2520to%2520computationally%2520efficient%2520%25241%255Ctimes%25201%2524%250Aconvolutions%252C%2520enabling%2520pixel-level%2520pruning%2520of%2520sharp%2520areas%2520to%2520reduce%250Acomputation.%2520Second%252C%2520we%2520develop%2520an%2520intra-frame%2520motion%2520analyzer%2520that%2520translates%250Arelative%2520pixel%2520displacements%2520into%2520motion%2520trajectories%252C%2520establishing%2520adaptive%250Aguidance%2520for%2520region-specific%2520blur%2520restoration.%2520Our%2520method%2520is%2520trained%2520end-to-end%250Ausing%2520a%2520combination%2520of%2520reconstruction%2520loss%252C%2520reblur%2520loss%252C%2520and%2520mask%2520loss%2520guided%250Aby%2520annotated%2520blur%2520masks.%2520Extensive%2520experiments%2520demonstrate%2520superior%2520performance%250Aover%2520state-of-the-art%2520methods%2520on%2520both%2520local%2520and%2520global%2520blur%2520datasets%2520while%250Areducing%2520FLOPs%2520by%252049%255C%2525%2520compared%2520to%2520SOTA%2520models%2520%2528e.g.%252C%2520LMD-ViT%2529.%2520The%2520source%2520code%250Ais%2520available%2520at%2520https%253A//github.com/shangwei5/M2AENet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion-Aware%20Adaptive%20Pixel%20Pruning%20for%20Efficient%20Local%20Motion%0A%20%20Deblurring&entry.906535625=Wei%20Shang%20and%20Dongwei%20Ren%20and%20Wanying%20Zhang%20and%20Pengfei%20Zhu%20and%20Qinghua%20Hu%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Local%20motion%20blur%20in%20digital%20images%20originates%20from%20the%20relative%20motion%0Abetween%20dynamic%20objects%20and%20static%20imaging%20systems%20during%20exposure.%20Existing%0Adeblurring%20methods%20face%20significant%20challenges%20in%20addressing%20this%20problem%20due%0Ato%20their%20inefficient%20allocation%20of%20computational%20resources%20and%20inadequate%0Ahandling%20of%20spatially%20varying%20blur%20patterns.%20To%20overcome%20these%20limitations%2C%20we%0Afirst%20propose%20a%20trainable%20mask%20predictor%20that%20identifies%20blurred%20regions%20in%20the%0Aimage.%20During%20training%2C%20we%20employ%20blur%20masks%20to%20exclude%20sharp%20regions.%20For%0Ainference%20optimization%2C%20we%20implement%20structural%20reparameterization%20by%0Aconverting%20%243%5Ctimes%203%24%20convolutions%20to%20computationally%20efficient%20%241%5Ctimes%201%24%0Aconvolutions%2C%20enabling%20pixel-level%20pruning%20of%20sharp%20areas%20to%20reduce%0Acomputation.%20Second%2C%20we%20develop%20an%20intra-frame%20motion%20analyzer%20that%20translates%0Arelative%20pixel%20displacements%20into%20motion%20trajectories%2C%20establishing%20adaptive%0Aguidance%20for%20region-specific%20blur%20restoration.%20Our%20method%20is%20trained%20end-to-end%0Ausing%20a%20combination%20of%20reconstruction%20loss%2C%20reblur%20loss%2C%20and%20mask%20loss%20guided%0Aby%20annotated%20blur%20masks.%20Extensive%20experiments%20demonstrate%20superior%20performance%0Aover%20state-of-the-art%20methods%20on%20both%20local%20and%20global%20blur%20datasets%20while%0Areducing%20FLOPs%20by%2049%5C%25%20compared%20to%20SOTA%20models%20%28e.g.%2C%20LMD-ViT%29.%20The%20source%20code%0Ais%20available%20at%20https%3A//github.com/shangwei5/M2AENet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07708v1&entry.124074799=Read"},
{"title": "Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders", "author": "Dimitrios Bralios and Jonah Casebeer and Paris Smaragdis", "abstract": "  Neural audio codecs and autoencoders have emerged as versatile models for\naudio compression, transmission, feature-extraction, and latent-space\ngeneration. However, a key limitation is that most are trained to maximize\nreconstruction fidelity, often neglecting the specific latent structure\nnecessary for optimal performance in diverse downstream applications. We\npropose a simple, post-hoc framework to address this by modifying the\nbottleneck of a pre-trained autoencoder. Our method introduces a\n\"Re-Bottleneck\", an inner bottleneck trained exclusively through latent space\nlosses to instill user-defined structure. We demonstrate the framework's\neffectiveness in three experiments. First, we enforce an ordering on latent\nchannels without sacrificing reconstruction quality. Second, we align latents\nwith semantic embeddings, analyzing the impact on downstream diffusion\nmodeling. Third, we introduce equivariance, ensuring that a filtering operation\non the input waveform directly corresponds to a specific transformation in the\nlatent space. Ultimately, our Re-Bottleneck framework offers a flexible and\nefficient way to tailor representations of neural audio models, enabling them\nto seamlessly meet the varied demands of different applications with minimal\nadditional training.\n", "link": "http://arxiv.org/abs/2507.07867v1", "date": "2025-07-10", "relevancy": 2.2291, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5662}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5572}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-Bottleneck%3A%20Latent%20Re-Structuring%20for%20Neural%20Audio%20Autoencoders&body=Title%3A%20Re-Bottleneck%3A%20Latent%20Re-Structuring%20for%20Neural%20Audio%20Autoencoders%0AAuthor%3A%20Dimitrios%20Bralios%20and%20Jonah%20Casebeer%20and%20Paris%20Smaragdis%0AAbstract%3A%20%20%20Neural%20audio%20codecs%20and%20autoencoders%20have%20emerged%20as%20versatile%20models%20for%0Aaudio%20compression%2C%20transmission%2C%20feature-extraction%2C%20and%20latent-space%0Ageneration.%20However%2C%20a%20key%20limitation%20is%20that%20most%20are%20trained%20to%20maximize%0Areconstruction%20fidelity%2C%20often%20neglecting%20the%20specific%20latent%20structure%0Anecessary%20for%20optimal%20performance%20in%20diverse%20downstream%20applications.%20We%0Apropose%20a%20simple%2C%20post-hoc%20framework%20to%20address%20this%20by%20modifying%20the%0Abottleneck%20of%20a%20pre-trained%20autoencoder.%20Our%20method%20introduces%20a%0A%22Re-Bottleneck%22%2C%20an%20inner%20bottleneck%20trained%20exclusively%20through%20latent%20space%0Alosses%20to%20instill%20user-defined%20structure.%20We%20demonstrate%20the%20framework%27s%0Aeffectiveness%20in%20three%20experiments.%20First%2C%20we%20enforce%20an%20ordering%20on%20latent%0Achannels%20without%20sacrificing%20reconstruction%20quality.%20Second%2C%20we%20align%20latents%0Awith%20semantic%20embeddings%2C%20analyzing%20the%20impact%20on%20downstream%20diffusion%0Amodeling.%20Third%2C%20we%20introduce%20equivariance%2C%20ensuring%20that%20a%20filtering%20operation%0Aon%20the%20input%20waveform%20directly%20corresponds%20to%20a%20specific%20transformation%20in%20the%0Alatent%20space.%20Ultimately%2C%20our%20Re-Bottleneck%20framework%20offers%20a%20flexible%20and%0Aefficient%20way%20to%20tailor%20representations%20of%20neural%20audio%20models%2C%20enabling%20them%0Ato%20seamlessly%20meet%20the%20varied%20demands%20of%20different%20applications%20with%20minimal%0Aadditional%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-Bottleneck%253A%2520Latent%2520Re-Structuring%2520for%2520Neural%2520Audio%2520Autoencoders%26entry.906535625%3DDimitrios%2520Bralios%2520and%2520Jonah%2520Casebeer%2520and%2520Paris%2520Smaragdis%26entry.1292438233%3D%2520%2520Neural%2520audio%2520codecs%2520and%2520autoencoders%2520have%2520emerged%2520as%2520versatile%2520models%2520for%250Aaudio%2520compression%252C%2520transmission%252C%2520feature-extraction%252C%2520and%2520latent-space%250Ageneration.%2520However%252C%2520a%2520key%2520limitation%2520is%2520that%2520most%2520are%2520trained%2520to%2520maximize%250Areconstruction%2520fidelity%252C%2520often%2520neglecting%2520the%2520specific%2520latent%2520structure%250Anecessary%2520for%2520optimal%2520performance%2520in%2520diverse%2520downstream%2520applications.%2520We%250Apropose%2520a%2520simple%252C%2520post-hoc%2520framework%2520to%2520address%2520this%2520by%2520modifying%2520the%250Abottleneck%2520of%2520a%2520pre-trained%2520autoencoder.%2520Our%2520method%2520introduces%2520a%250A%2522Re-Bottleneck%2522%252C%2520an%2520inner%2520bottleneck%2520trained%2520exclusively%2520through%2520latent%2520space%250Alosses%2520to%2520instill%2520user-defined%2520structure.%2520We%2520demonstrate%2520the%2520framework%2527s%250Aeffectiveness%2520in%2520three%2520experiments.%2520First%252C%2520we%2520enforce%2520an%2520ordering%2520on%2520latent%250Achannels%2520without%2520sacrificing%2520reconstruction%2520quality.%2520Second%252C%2520we%2520align%2520latents%250Awith%2520semantic%2520embeddings%252C%2520analyzing%2520the%2520impact%2520on%2520downstream%2520diffusion%250Amodeling.%2520Third%252C%2520we%2520introduce%2520equivariance%252C%2520ensuring%2520that%2520a%2520filtering%2520operation%250Aon%2520the%2520input%2520waveform%2520directly%2520corresponds%2520to%2520a%2520specific%2520transformation%2520in%2520the%250Alatent%2520space.%2520Ultimately%252C%2520our%2520Re-Bottleneck%2520framework%2520offers%2520a%2520flexible%2520and%250Aefficient%2520way%2520to%2520tailor%2520representations%2520of%2520neural%2520audio%2520models%252C%2520enabling%2520them%250Ato%2520seamlessly%2520meet%2520the%2520varied%2520demands%2520of%2520different%2520applications%2520with%2520minimal%250Aadditional%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-Bottleneck%3A%20Latent%20Re-Structuring%20for%20Neural%20Audio%20Autoencoders&entry.906535625=Dimitrios%20Bralios%20and%20Jonah%20Casebeer%20and%20Paris%20Smaragdis&entry.1292438233=%20%20Neural%20audio%20codecs%20and%20autoencoders%20have%20emerged%20as%20versatile%20models%20for%0Aaudio%20compression%2C%20transmission%2C%20feature-extraction%2C%20and%20latent-space%0Ageneration.%20However%2C%20a%20key%20limitation%20is%20that%20most%20are%20trained%20to%20maximize%0Areconstruction%20fidelity%2C%20often%20neglecting%20the%20specific%20latent%20structure%0Anecessary%20for%20optimal%20performance%20in%20diverse%20downstream%20applications.%20We%0Apropose%20a%20simple%2C%20post-hoc%20framework%20to%20address%20this%20by%20modifying%20the%0Abottleneck%20of%20a%20pre-trained%20autoencoder.%20Our%20method%20introduces%20a%0A%22Re-Bottleneck%22%2C%20an%20inner%20bottleneck%20trained%20exclusively%20through%20latent%20space%0Alosses%20to%20instill%20user-defined%20structure.%20We%20demonstrate%20the%20framework%27s%0Aeffectiveness%20in%20three%20experiments.%20First%2C%20we%20enforce%20an%20ordering%20on%20latent%0Achannels%20without%20sacrificing%20reconstruction%20quality.%20Second%2C%20we%20align%20latents%0Awith%20semantic%20embeddings%2C%20analyzing%20the%20impact%20on%20downstream%20diffusion%0Amodeling.%20Third%2C%20we%20introduce%20equivariance%2C%20ensuring%20that%20a%20filtering%20operation%0Aon%20the%20input%20waveform%20directly%20corresponds%20to%20a%20specific%20transformation%20in%20the%0Alatent%20space.%20Ultimately%2C%20our%20Re-Bottleneck%20framework%20offers%20a%20flexible%20and%0Aefficient%20way%20to%20tailor%20representations%20of%20neural%20audio%20models%2C%20enabling%20them%0Ato%20seamlessly%20meet%20the%20varied%20demands%20of%20different%20applications%20with%20minimal%0Aadditional%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07867v1&entry.124074799=Read"},
{"title": "Distributed and Decentralised Training: Technical Governance Challenges\n  in a Shifting AI Landscape", "author": "Jakub Kry\u015b and Yashvardhan Sharma and Janet Egan", "abstract": "  Advances in low-communication training algorithms are enabling a shift from\ncentralised model training to compute setups that are either distributed across\nmultiple clusters or decentralised via community-driven contributions. This\npaper distinguishes these two scenarios - distributed and decentralised\ntraining - which are little understood and often conflated in policy discourse.\nWe discuss how they could impact technical AI governance through an increased\nrisk of compute structuring, capability proliferation, and the erosion of\ndetectability and shutdownability. While these trends foreshadow a possible new\nparadigm that could challenge key assumptions of compute governance, we\nemphasise that certain policy levers, like export controls, remain relevant. We\nalso acknowledge potential benefits of decentralised AI, including\nprivacy-preserving training runs that could unlock access to more data, and\nmitigating harmful power concentration. Our goal is to support more precise\npolicymaking around compute, capability proliferation, and decentralised AI\ndevelopment.\n", "link": "http://arxiv.org/abs/2507.07765v1", "date": "2025-07-10", "relevancy": 2.2215, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4851}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4267}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20and%20Decentralised%20Training%3A%20Technical%20Governance%20Challenges%0A%20%20in%20a%20Shifting%20AI%20Landscape&body=Title%3A%20Distributed%20and%20Decentralised%20Training%3A%20Technical%20Governance%20Challenges%0A%20%20in%20a%20Shifting%20AI%20Landscape%0AAuthor%3A%20Jakub%20Kry%C5%9B%20and%20Yashvardhan%20Sharma%20and%20Janet%20Egan%0AAbstract%3A%20%20%20Advances%20in%20low-communication%20training%20algorithms%20are%20enabling%20a%20shift%20from%0Acentralised%20model%20training%20to%20compute%20setups%20that%20are%20either%20distributed%20across%0Amultiple%20clusters%20or%20decentralised%20via%20community-driven%20contributions.%20This%0Apaper%20distinguishes%20these%20two%20scenarios%20-%20distributed%20and%20decentralised%0Atraining%20-%20which%20are%20little%20understood%20and%20often%20conflated%20in%20policy%20discourse.%0AWe%20discuss%20how%20they%20could%20impact%20technical%20AI%20governance%20through%20an%20increased%0Arisk%20of%20compute%20structuring%2C%20capability%20proliferation%2C%20and%20the%20erosion%20of%0Adetectability%20and%20shutdownability.%20While%20these%20trends%20foreshadow%20a%20possible%20new%0Aparadigm%20that%20could%20challenge%20key%20assumptions%20of%20compute%20governance%2C%20we%0Aemphasise%20that%20certain%20policy%20levers%2C%20like%20export%20controls%2C%20remain%20relevant.%20We%0Aalso%20acknowledge%20potential%20benefits%20of%20decentralised%20AI%2C%20including%0Aprivacy-preserving%20training%20runs%20that%20could%20unlock%20access%20to%20more%20data%2C%20and%0Amitigating%20harmful%20power%20concentration.%20Our%20goal%20is%20to%20support%20more%20precise%0Apolicymaking%20around%20compute%2C%20capability%20proliferation%2C%20and%20decentralised%20AI%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520and%2520Decentralised%2520Training%253A%2520Technical%2520Governance%2520Challenges%250A%2520%2520in%2520a%2520Shifting%2520AI%2520Landscape%26entry.906535625%3DJakub%2520Kry%25C5%259B%2520and%2520Yashvardhan%2520Sharma%2520and%2520Janet%2520Egan%26entry.1292438233%3D%2520%2520Advances%2520in%2520low-communication%2520training%2520algorithms%2520are%2520enabling%2520a%2520shift%2520from%250Acentralised%2520model%2520training%2520to%2520compute%2520setups%2520that%2520are%2520either%2520distributed%2520across%250Amultiple%2520clusters%2520or%2520decentralised%2520via%2520community-driven%2520contributions.%2520This%250Apaper%2520distinguishes%2520these%2520two%2520scenarios%2520-%2520distributed%2520and%2520decentralised%250Atraining%2520-%2520which%2520are%2520little%2520understood%2520and%2520often%2520conflated%2520in%2520policy%2520discourse.%250AWe%2520discuss%2520how%2520they%2520could%2520impact%2520technical%2520AI%2520governance%2520through%2520an%2520increased%250Arisk%2520of%2520compute%2520structuring%252C%2520capability%2520proliferation%252C%2520and%2520the%2520erosion%2520of%250Adetectability%2520and%2520shutdownability.%2520While%2520these%2520trends%2520foreshadow%2520a%2520possible%2520new%250Aparadigm%2520that%2520could%2520challenge%2520key%2520assumptions%2520of%2520compute%2520governance%252C%2520we%250Aemphasise%2520that%2520certain%2520policy%2520levers%252C%2520like%2520export%2520controls%252C%2520remain%2520relevant.%2520We%250Aalso%2520acknowledge%2520potential%2520benefits%2520of%2520decentralised%2520AI%252C%2520including%250Aprivacy-preserving%2520training%2520runs%2520that%2520could%2520unlock%2520access%2520to%2520more%2520data%252C%2520and%250Amitigating%2520harmful%2520power%2520concentration.%2520Our%2520goal%2520is%2520to%2520support%2520more%2520precise%250Apolicymaking%2520around%2520compute%252C%2520capability%2520proliferation%252C%2520and%2520decentralised%2520AI%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20and%20Decentralised%20Training%3A%20Technical%20Governance%20Challenges%0A%20%20in%20a%20Shifting%20AI%20Landscape&entry.906535625=Jakub%20Kry%C5%9B%20and%20Yashvardhan%20Sharma%20and%20Janet%20Egan&entry.1292438233=%20%20Advances%20in%20low-communication%20training%20algorithms%20are%20enabling%20a%20shift%20from%0Acentralised%20model%20training%20to%20compute%20setups%20that%20are%20either%20distributed%20across%0Amultiple%20clusters%20or%20decentralised%20via%20community-driven%20contributions.%20This%0Apaper%20distinguishes%20these%20two%20scenarios%20-%20distributed%20and%20decentralised%0Atraining%20-%20which%20are%20little%20understood%20and%20often%20conflated%20in%20policy%20discourse.%0AWe%20discuss%20how%20they%20could%20impact%20technical%20AI%20governance%20through%20an%20increased%0Arisk%20of%20compute%20structuring%2C%20capability%20proliferation%2C%20and%20the%20erosion%20of%0Adetectability%20and%20shutdownability.%20While%20these%20trends%20foreshadow%20a%20possible%20new%0Aparadigm%20that%20could%20challenge%20key%20assumptions%20of%20compute%20governance%2C%20we%0Aemphasise%20that%20certain%20policy%20levers%2C%20like%20export%20controls%2C%20remain%20relevant.%20We%0Aalso%20acknowledge%20potential%20benefits%20of%20decentralised%20AI%2C%20including%0Aprivacy-preserving%20training%20runs%20that%20could%20unlock%20access%20to%20more%20data%2C%20and%0Amitigating%20harmful%20power%20concentration.%20Our%20goal%20is%20to%20support%20more%20precise%0Apolicymaking%20around%20compute%2C%20capability%20proliferation%2C%20and%20decentralised%20AI%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07765v1&entry.124074799=Read"},
{"title": "Probing Experts' Perspectives on AI-Assisted Public Speaking Training", "author": "Nesrine Fourati and Alisa Barkar and Marion Drag\u00e9e and Liv Danthon-Lefebvre and Mathieu Chollet", "abstract": "  Background: Public speaking is a vital professional skill, yet it remains a\nsource of significant anxiety for many individuals. Traditional training relies\nheavily on expert coaching, but recent advances in AI has led to novel types of\ncommercial automated public speaking feedback tools. However, most research has\nfocused on prototypes rather than commercial applications, and little is known\nabout how public speaking experts perceive these tools.\n  Objectives: This study aims to evaluate expert opinions on the efficacy and\ndesign of commercial AI-based public speaking training tools and to propose\nguidelines for their improvement.\n  Methods: The research involved 16 semi-structured interviews and 2 focus\ngroups with public speaking experts. Participants discussed their views on\ncurrent commercial tools, their potential integration into traditional\ncoaching, and suggestions for enhancing these systems.\n  Results and Conclusions: Experts acknowledged the value of AI tools in\nhandling repetitive, technical aspects of training, allowing coaches to focus\non higher-level skills. However they found key issues in current tools,\nemphasising the need for personalised, understandable, carefully selected\nfeedback and clear instructional design. Overall, they supported a hybrid model\ncombining traditional coaching with AI-supported exercises.\n", "link": "http://arxiv.org/abs/2507.07930v1", "date": "2025-07-10", "relevancy": 2.2162, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4358}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20Experts%27%20Perspectives%20on%20AI-Assisted%20Public%20Speaking%20Training&body=Title%3A%20Probing%20Experts%27%20Perspectives%20on%20AI-Assisted%20Public%20Speaking%20Training%0AAuthor%3A%20Nesrine%20Fourati%20and%20Alisa%20Barkar%20and%20Marion%20Drag%C3%A9e%20and%20Liv%20Danthon-Lefebvre%20and%20Mathieu%20Chollet%0AAbstract%3A%20%20%20Background%3A%20Public%20speaking%20is%20a%20vital%20professional%20skill%2C%20yet%20it%20remains%20a%0Asource%20of%20significant%20anxiety%20for%20many%20individuals.%20Traditional%20training%20relies%0Aheavily%20on%20expert%20coaching%2C%20but%20recent%20advances%20in%20AI%20has%20led%20to%20novel%20types%20of%0Acommercial%20automated%20public%20speaking%20feedback%20tools.%20However%2C%20most%20research%20has%0Afocused%20on%20prototypes%20rather%20than%20commercial%20applications%2C%20and%20little%20is%20known%0Aabout%20how%20public%20speaking%20experts%20perceive%20these%20tools.%0A%20%20Objectives%3A%20This%20study%20aims%20to%20evaluate%20expert%20opinions%20on%20the%20efficacy%20and%0Adesign%20of%20commercial%20AI-based%20public%20speaking%20training%20tools%20and%20to%20propose%0Aguidelines%20for%20their%20improvement.%0A%20%20Methods%3A%20The%20research%20involved%2016%20semi-structured%20interviews%20and%202%20focus%0Agroups%20with%20public%20speaking%20experts.%20Participants%20discussed%20their%20views%20on%0Acurrent%20commercial%20tools%2C%20their%20potential%20integration%20into%20traditional%0Acoaching%2C%20and%20suggestions%20for%20enhancing%20these%20systems.%0A%20%20Results%20and%20Conclusions%3A%20Experts%20acknowledged%20the%20value%20of%20AI%20tools%20in%0Ahandling%20repetitive%2C%20technical%20aspects%20of%20training%2C%20allowing%20coaches%20to%20focus%0Aon%20higher-level%20skills.%20However%20they%20found%20key%20issues%20in%20current%20tools%2C%0Aemphasising%20the%20need%20for%20personalised%2C%20understandable%2C%20carefully%20selected%0Afeedback%20and%20clear%20instructional%20design.%20Overall%2C%20they%20supported%20a%20hybrid%20model%0Acombining%20traditional%20coaching%20with%20AI-supported%20exercises.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520Experts%2527%2520Perspectives%2520on%2520AI-Assisted%2520Public%2520Speaking%2520Training%26entry.906535625%3DNesrine%2520Fourati%2520and%2520Alisa%2520Barkar%2520and%2520Marion%2520Drag%25C3%25A9e%2520and%2520Liv%2520Danthon-Lefebvre%2520and%2520Mathieu%2520Chollet%26entry.1292438233%3D%2520%2520Background%253A%2520Public%2520speaking%2520is%2520a%2520vital%2520professional%2520skill%252C%2520yet%2520it%2520remains%2520a%250Asource%2520of%2520significant%2520anxiety%2520for%2520many%2520individuals.%2520Traditional%2520training%2520relies%250Aheavily%2520on%2520expert%2520coaching%252C%2520but%2520recent%2520advances%2520in%2520AI%2520has%2520led%2520to%2520novel%2520types%2520of%250Acommercial%2520automated%2520public%2520speaking%2520feedback%2520tools.%2520However%252C%2520most%2520research%2520has%250Afocused%2520on%2520prototypes%2520rather%2520than%2520commercial%2520applications%252C%2520and%2520little%2520is%2520known%250Aabout%2520how%2520public%2520speaking%2520experts%2520perceive%2520these%2520tools.%250A%2520%2520Objectives%253A%2520This%2520study%2520aims%2520to%2520evaluate%2520expert%2520opinions%2520on%2520the%2520efficacy%2520and%250Adesign%2520of%2520commercial%2520AI-based%2520public%2520speaking%2520training%2520tools%2520and%2520to%2520propose%250Aguidelines%2520for%2520their%2520improvement.%250A%2520%2520Methods%253A%2520The%2520research%2520involved%252016%2520semi-structured%2520interviews%2520and%25202%2520focus%250Agroups%2520with%2520public%2520speaking%2520experts.%2520Participants%2520discussed%2520their%2520views%2520on%250Acurrent%2520commercial%2520tools%252C%2520their%2520potential%2520integration%2520into%2520traditional%250Acoaching%252C%2520and%2520suggestions%2520for%2520enhancing%2520these%2520systems.%250A%2520%2520Results%2520and%2520Conclusions%253A%2520Experts%2520acknowledged%2520the%2520value%2520of%2520AI%2520tools%2520in%250Ahandling%2520repetitive%252C%2520technical%2520aspects%2520of%2520training%252C%2520allowing%2520coaches%2520to%2520focus%250Aon%2520higher-level%2520skills.%2520However%2520they%2520found%2520key%2520issues%2520in%2520current%2520tools%252C%250Aemphasising%2520the%2520need%2520for%2520personalised%252C%2520understandable%252C%2520carefully%2520selected%250Afeedback%2520and%2520clear%2520instructional%2520design.%2520Overall%252C%2520they%2520supported%2520a%2520hybrid%2520model%250Acombining%2520traditional%2520coaching%2520with%2520AI-supported%2520exercises.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20Experts%27%20Perspectives%20on%20AI-Assisted%20Public%20Speaking%20Training&entry.906535625=Nesrine%20Fourati%20and%20Alisa%20Barkar%20and%20Marion%20Drag%C3%A9e%20and%20Liv%20Danthon-Lefebvre%20and%20Mathieu%20Chollet&entry.1292438233=%20%20Background%3A%20Public%20speaking%20is%20a%20vital%20professional%20skill%2C%20yet%20it%20remains%20a%0Asource%20of%20significant%20anxiety%20for%20many%20individuals.%20Traditional%20training%20relies%0Aheavily%20on%20expert%20coaching%2C%20but%20recent%20advances%20in%20AI%20has%20led%20to%20novel%20types%20of%0Acommercial%20automated%20public%20speaking%20feedback%20tools.%20However%2C%20most%20research%20has%0Afocused%20on%20prototypes%20rather%20than%20commercial%20applications%2C%20and%20little%20is%20known%0Aabout%20how%20public%20speaking%20experts%20perceive%20these%20tools.%0A%20%20Objectives%3A%20This%20study%20aims%20to%20evaluate%20expert%20opinions%20on%20the%20efficacy%20and%0Adesign%20of%20commercial%20AI-based%20public%20speaking%20training%20tools%20and%20to%20propose%0Aguidelines%20for%20their%20improvement.%0A%20%20Methods%3A%20The%20research%20involved%2016%20semi-structured%20interviews%20and%202%20focus%0Agroups%20with%20public%20speaking%20experts.%20Participants%20discussed%20their%20views%20on%0Acurrent%20commercial%20tools%2C%20their%20potential%20integration%20into%20traditional%0Acoaching%2C%20and%20suggestions%20for%20enhancing%20these%20systems.%0A%20%20Results%20and%20Conclusions%3A%20Experts%20acknowledged%20the%20value%20of%20AI%20tools%20in%0Ahandling%20repetitive%2C%20technical%20aspects%20of%20training%2C%20allowing%20coaches%20to%20focus%0Aon%20higher-level%20skills.%20However%20they%20found%20key%20issues%20in%20current%20tools%2C%0Aemphasising%20the%20need%20for%20personalised%2C%20understandable%2C%20carefully%20selected%0Afeedback%20and%20clear%20instructional%20design.%20Overall%2C%20they%20supported%20a%20hybrid%20model%0Acombining%20traditional%20coaching%20with%20AI-supported%20exercises.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07930v1&entry.124074799=Read"},
{"title": "3D-ADAM: A Dataset for 3D Anomaly Detection in Advanced Manufacturing", "author": "Paul McHard and Florent P. Audonnet and Oliver Summerell and Sebastian Andraos and Paul Henderson and Gerardo Aragon-Camarasa", "abstract": "  Surface defects are one of the largest contributors to low yield in the\nmanufacturing sector. Accurate and reliable detection of defects during the\nmanufacturing process is therefore of great value across the sector.\nState-of-the-art approaches to automated defect detection yield impressive\nperformance on current datasets, yet still fall short in real-world\nmanufacturing settings and developing improved methods relies on large datasets\nrepresentative of real-world scenarios. Unfortunately, high-quality,\nhigh-precision RGB+3D industrial anomaly detection datasets are scarce, and\ntypically do not reflect real-world industrial deployment scenarios. To address\nthis, we introduce 3D-ADAM, the first large-scale industry-relevant dataset for\nhigh-precision 3D Anomaly Detection. 3D-ADAM comprises 14,120 high-resolution\nscans across 217 unique parts, captured using 4 industrial depth imaging\nsensors. It includes 27,346 annotated defect instances from 12 categories,\ncovering the breadth of industrial surface defects. 3D-ADAM uniquely captures\nan additional 8,110 annotations of machine element features, spanning the range\nof relevant mechanical design form factors. Unlike existing datasets, 3D-ADAM\nis captured in a real industrial environment with variations in part position\nand orientation, camera positioning, ambient lighting conditions, as well as\npartial occlusions. Our evaluation of SOTA models across various RGB+3D anomaly\ndetection tasks demonstrates the significant challenge this dataset presents to\ncurrent approaches. We further validated the industrial relevance and quality\nof the dataset through an expert labelling survey conducted by industry\npartners. By providing this challenging benchmark, 3D-ADAM aims to accelerate\nthe development of robust 3D Anomaly Detection models capable of meeting the\ndemands of modern manufacturing environments.\n", "link": "http://arxiv.org/abs/2507.07838v1", "date": "2025-07-10", "relevancy": 2.2012, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5543}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5495}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-ADAM%3A%20A%20Dataset%20for%203D%20Anomaly%20Detection%20in%20Advanced%20Manufacturing&body=Title%3A%203D-ADAM%3A%20A%20Dataset%20for%203D%20Anomaly%20Detection%20in%20Advanced%20Manufacturing%0AAuthor%3A%20Paul%20McHard%20and%20Florent%20P.%20Audonnet%20and%20Oliver%20Summerell%20and%20Sebastian%20Andraos%20and%20Paul%20Henderson%20and%20Gerardo%20Aragon-Camarasa%0AAbstract%3A%20%20%20Surface%20defects%20are%20one%20of%20the%20largest%20contributors%20to%20low%20yield%20in%20the%0Amanufacturing%20sector.%20Accurate%20and%20reliable%20detection%20of%20defects%20during%20the%0Amanufacturing%20process%20is%20therefore%20of%20great%20value%20across%20the%20sector.%0AState-of-the-art%20approaches%20to%20automated%20defect%20detection%20yield%20impressive%0Aperformance%20on%20current%20datasets%2C%20yet%20still%20fall%20short%20in%20real-world%0Amanufacturing%20settings%20and%20developing%20improved%20methods%20relies%20on%20large%20datasets%0Arepresentative%20of%20real-world%20scenarios.%20Unfortunately%2C%20high-quality%2C%0Ahigh-precision%20RGB%2B3D%20industrial%20anomaly%20detection%20datasets%20are%20scarce%2C%20and%0Atypically%20do%20not%20reflect%20real-world%20industrial%20deployment%20scenarios.%20To%20address%0Athis%2C%20we%20introduce%203D-ADAM%2C%20the%20first%20large-scale%20industry-relevant%20dataset%20for%0Ahigh-precision%203D%20Anomaly%20Detection.%203D-ADAM%20comprises%2014%2C120%20high-resolution%0Ascans%20across%20217%20unique%20parts%2C%20captured%20using%204%20industrial%20depth%20imaging%0Asensors.%20It%20includes%2027%2C346%20annotated%20defect%20instances%20from%2012%20categories%2C%0Acovering%20the%20breadth%20of%20industrial%20surface%20defects.%203D-ADAM%20uniquely%20captures%0Aan%20additional%208%2C110%20annotations%20of%20machine%20element%20features%2C%20spanning%20the%20range%0Aof%20relevant%20mechanical%20design%20form%20factors.%20Unlike%20existing%20datasets%2C%203D-ADAM%0Ais%20captured%20in%20a%20real%20industrial%20environment%20with%20variations%20in%20part%20position%0Aand%20orientation%2C%20camera%20positioning%2C%20ambient%20lighting%20conditions%2C%20as%20well%20as%0Apartial%20occlusions.%20Our%20evaluation%20of%20SOTA%20models%20across%20various%20RGB%2B3D%20anomaly%0Adetection%20tasks%20demonstrates%20the%20significant%20challenge%20this%20dataset%20presents%20to%0Acurrent%20approaches.%20We%20further%20validated%20the%20industrial%20relevance%20and%20quality%0Aof%20the%20dataset%20through%20an%20expert%20labelling%20survey%20conducted%20by%20industry%0Apartners.%20By%20providing%20this%20challenging%20benchmark%2C%203D-ADAM%20aims%20to%20accelerate%0Athe%20development%20of%20robust%203D%20Anomaly%20Detection%20models%20capable%20of%20meeting%20the%0Ademands%20of%20modern%20manufacturing%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-ADAM%253A%2520A%2520Dataset%2520for%25203D%2520Anomaly%2520Detection%2520in%2520Advanced%2520Manufacturing%26entry.906535625%3DPaul%2520McHard%2520and%2520Florent%2520P.%2520Audonnet%2520and%2520Oliver%2520Summerell%2520and%2520Sebastian%2520Andraos%2520and%2520Paul%2520Henderson%2520and%2520Gerardo%2520Aragon-Camarasa%26entry.1292438233%3D%2520%2520Surface%2520defects%2520are%2520one%2520of%2520the%2520largest%2520contributors%2520to%2520low%2520yield%2520in%2520the%250Amanufacturing%2520sector.%2520Accurate%2520and%2520reliable%2520detection%2520of%2520defects%2520during%2520the%250Amanufacturing%2520process%2520is%2520therefore%2520of%2520great%2520value%2520across%2520the%2520sector.%250AState-of-the-art%2520approaches%2520to%2520automated%2520defect%2520detection%2520yield%2520impressive%250Aperformance%2520on%2520current%2520datasets%252C%2520yet%2520still%2520fall%2520short%2520in%2520real-world%250Amanufacturing%2520settings%2520and%2520developing%2520improved%2520methods%2520relies%2520on%2520large%2520datasets%250Arepresentative%2520of%2520real-world%2520scenarios.%2520Unfortunately%252C%2520high-quality%252C%250Ahigh-precision%2520RGB%252B3D%2520industrial%2520anomaly%2520detection%2520datasets%2520are%2520scarce%252C%2520and%250Atypically%2520do%2520not%2520reflect%2520real-world%2520industrial%2520deployment%2520scenarios.%2520To%2520address%250Athis%252C%2520we%2520introduce%25203D-ADAM%252C%2520the%2520first%2520large-scale%2520industry-relevant%2520dataset%2520for%250Ahigh-precision%25203D%2520Anomaly%2520Detection.%25203D-ADAM%2520comprises%252014%252C120%2520high-resolution%250Ascans%2520across%2520217%2520unique%2520parts%252C%2520captured%2520using%25204%2520industrial%2520depth%2520imaging%250Asensors.%2520It%2520includes%252027%252C346%2520annotated%2520defect%2520instances%2520from%252012%2520categories%252C%250Acovering%2520the%2520breadth%2520of%2520industrial%2520surface%2520defects.%25203D-ADAM%2520uniquely%2520captures%250Aan%2520additional%25208%252C110%2520annotations%2520of%2520machine%2520element%2520features%252C%2520spanning%2520the%2520range%250Aof%2520relevant%2520mechanical%2520design%2520form%2520factors.%2520Unlike%2520existing%2520datasets%252C%25203D-ADAM%250Ais%2520captured%2520in%2520a%2520real%2520industrial%2520environment%2520with%2520variations%2520in%2520part%2520position%250Aand%2520orientation%252C%2520camera%2520positioning%252C%2520ambient%2520lighting%2520conditions%252C%2520as%2520well%2520as%250Apartial%2520occlusions.%2520Our%2520evaluation%2520of%2520SOTA%2520models%2520across%2520various%2520RGB%252B3D%2520anomaly%250Adetection%2520tasks%2520demonstrates%2520the%2520significant%2520challenge%2520this%2520dataset%2520presents%2520to%250Acurrent%2520approaches.%2520We%2520further%2520validated%2520the%2520industrial%2520relevance%2520and%2520quality%250Aof%2520the%2520dataset%2520through%2520an%2520expert%2520labelling%2520survey%2520conducted%2520by%2520industry%250Apartners.%2520By%2520providing%2520this%2520challenging%2520benchmark%252C%25203D-ADAM%2520aims%2520to%2520accelerate%250Athe%2520development%2520of%2520robust%25203D%2520Anomaly%2520Detection%2520models%2520capable%2520of%2520meeting%2520the%250Ademands%2520of%2520modern%2520manufacturing%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-ADAM%3A%20A%20Dataset%20for%203D%20Anomaly%20Detection%20in%20Advanced%20Manufacturing&entry.906535625=Paul%20McHard%20and%20Florent%20P.%20Audonnet%20and%20Oliver%20Summerell%20and%20Sebastian%20Andraos%20and%20Paul%20Henderson%20and%20Gerardo%20Aragon-Camarasa&entry.1292438233=%20%20Surface%20defects%20are%20one%20of%20the%20largest%20contributors%20to%20low%20yield%20in%20the%0Amanufacturing%20sector.%20Accurate%20and%20reliable%20detection%20of%20defects%20during%20the%0Amanufacturing%20process%20is%20therefore%20of%20great%20value%20across%20the%20sector.%0AState-of-the-art%20approaches%20to%20automated%20defect%20detection%20yield%20impressive%0Aperformance%20on%20current%20datasets%2C%20yet%20still%20fall%20short%20in%20real-world%0Amanufacturing%20settings%20and%20developing%20improved%20methods%20relies%20on%20large%20datasets%0Arepresentative%20of%20real-world%20scenarios.%20Unfortunately%2C%20high-quality%2C%0Ahigh-precision%20RGB%2B3D%20industrial%20anomaly%20detection%20datasets%20are%20scarce%2C%20and%0Atypically%20do%20not%20reflect%20real-world%20industrial%20deployment%20scenarios.%20To%20address%0Athis%2C%20we%20introduce%203D-ADAM%2C%20the%20first%20large-scale%20industry-relevant%20dataset%20for%0Ahigh-precision%203D%20Anomaly%20Detection.%203D-ADAM%20comprises%2014%2C120%20high-resolution%0Ascans%20across%20217%20unique%20parts%2C%20captured%20using%204%20industrial%20depth%20imaging%0Asensors.%20It%20includes%2027%2C346%20annotated%20defect%20instances%20from%2012%20categories%2C%0Acovering%20the%20breadth%20of%20industrial%20surface%20defects.%203D-ADAM%20uniquely%20captures%0Aan%20additional%208%2C110%20annotations%20of%20machine%20element%20features%2C%20spanning%20the%20range%0Aof%20relevant%20mechanical%20design%20form%20factors.%20Unlike%20existing%20datasets%2C%203D-ADAM%0Ais%20captured%20in%20a%20real%20industrial%20environment%20with%20variations%20in%20part%20position%0Aand%20orientation%2C%20camera%20positioning%2C%20ambient%20lighting%20conditions%2C%20as%20well%20as%0Apartial%20occlusions.%20Our%20evaluation%20of%20SOTA%20models%20across%20various%20RGB%2B3D%20anomaly%0Adetection%20tasks%20demonstrates%20the%20significant%20challenge%20this%20dataset%20presents%20to%0Acurrent%20approaches.%20We%20further%20validated%20the%20industrial%20relevance%20and%20quality%0Aof%20the%20dataset%20through%20an%20expert%20labelling%20survey%20conducted%20by%20industry%0Apartners.%20By%20providing%20this%20challenging%20benchmark%2C%203D-ADAM%20aims%20to%20accelerate%0Athe%20development%20of%20robust%203D%20Anomaly%20Detection%20models%20capable%20of%20meeting%20the%0Ademands%20of%20modern%20manufacturing%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07838v1&entry.124074799=Read"},
{"title": "Are Vision Transformer Representations Semantically Meaningful? A Case\n  Study in Medical Imaging", "author": "Montasir Shams and Chashi Mahiul Islam and Shaeke Salman and Phat Tran and Xiuwen Liu", "abstract": "  Vision transformers (ViTs) have rapidly gained prominence in medical imaging\ntasks such as disease classification, segmentation, and detection due to their\nsuperior accuracy compared to conventional deep learning models. However, due\nto their size and complex interactions via the self-attention mechanism, they\nare not well understood. In particular, it is unclear whether the\nrepresentations produced by such models are semantically meaningful. In this\npaper, using a projected gradient-based algorithm, we show that their\nrepresentations are not semantically meaningful and they are inherently\nvulnerable to small changes. Images with imperceptible differences can have\nvery different representations; on the other hand, images that should belong to\ndifferent semantic classes can have nearly identical representations. Such\nvulnerability can lead to unreliable classification results; for example,\nunnoticeable changes cause the classification accuracy to be reduced by over\n60\\%. %. To the best of our knowledge, this is the first work to systematically\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\nrepresentations for medical image classification, revealing a critical\nchallenge for their deployment in safety-critical systems.\n", "link": "http://arxiv.org/abs/2507.01788v2", "date": "2025-07-10", "relevancy": 2.1945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5527}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Vision%20Transformer%20Representations%20Semantically%20Meaningful%3F%20A%20Case%0A%20%20Study%20in%20Medical%20Imaging&body=Title%3A%20Are%20Vision%20Transformer%20Representations%20Semantically%20Meaningful%3F%20A%20Case%0A%20%20Study%20in%20Medical%20Imaging%0AAuthor%3A%20Montasir%20Shams%20and%20Chashi%20Mahiul%20Islam%20and%20Shaeke%20Salman%20and%20Phat%20Tran%20and%20Xiuwen%20Liu%0AAbstract%3A%20%20%20Vision%20transformers%20%28ViTs%29%20have%20rapidly%20gained%20prominence%20in%20medical%20imaging%0Atasks%20such%20as%20disease%20classification%2C%20segmentation%2C%20and%20detection%20due%20to%20their%0Asuperior%20accuracy%20compared%20to%20conventional%20deep%20learning%20models.%20However%2C%20due%0Ato%20their%20size%20and%20complex%20interactions%20via%20the%20self-attention%20mechanism%2C%20they%0Aare%20not%20well%20understood.%20In%20particular%2C%20it%20is%20unclear%20whether%20the%0Arepresentations%20produced%20by%20such%20models%20are%20semantically%20meaningful.%20In%20this%0Apaper%2C%20using%20a%20projected%20gradient-based%20algorithm%2C%20we%20show%20that%20their%0Arepresentations%20are%20not%20semantically%20meaningful%20and%20they%20are%20inherently%0Avulnerable%20to%20small%20changes.%20Images%20with%20imperceptible%20differences%20can%20have%0Avery%20different%20representations%3B%20on%20the%20other%20hand%2C%20images%20that%20should%20belong%20to%0Adifferent%20semantic%20classes%20can%20have%20nearly%20identical%20representations.%20Such%0Avulnerability%20can%20lead%20to%20unreliable%20classification%20results%3B%20for%20example%2C%0Aunnoticeable%20changes%20cause%20the%20classification%20accuracy%20to%20be%20reduced%20by%20over%0A60%5C%25.%20%25.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20systematically%0Ademonstrate%20this%20fundamental%20lack%20of%20semantic%20meaningfulness%20in%20ViT%0Arepresentations%20for%20medical%20image%20classification%2C%20revealing%20a%20critical%0Achallenge%20for%20their%20deployment%20in%20safety-critical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01788v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Vision%2520Transformer%2520Representations%2520Semantically%2520Meaningful%253F%2520A%2520Case%250A%2520%2520Study%2520in%2520Medical%2520Imaging%26entry.906535625%3DMontasir%2520Shams%2520and%2520Chashi%2520Mahiul%2520Islam%2520and%2520Shaeke%2520Salman%2520and%2520Phat%2520Tran%2520and%2520Xiuwen%2520Liu%26entry.1292438233%3D%2520%2520Vision%2520transformers%2520%2528ViTs%2529%2520have%2520rapidly%2520gained%2520prominence%2520in%2520medical%2520imaging%250Atasks%2520such%2520as%2520disease%2520classification%252C%2520segmentation%252C%2520and%2520detection%2520due%2520to%2520their%250Asuperior%2520accuracy%2520compared%2520to%2520conventional%2520deep%2520learning%2520models.%2520However%252C%2520due%250Ato%2520their%2520size%2520and%2520complex%2520interactions%2520via%2520the%2520self-attention%2520mechanism%252C%2520they%250Aare%2520not%2520well%2520understood.%2520In%2520particular%252C%2520it%2520is%2520unclear%2520whether%2520the%250Arepresentations%2520produced%2520by%2520such%2520models%2520are%2520semantically%2520meaningful.%2520In%2520this%250Apaper%252C%2520using%2520a%2520projected%2520gradient-based%2520algorithm%252C%2520we%2520show%2520that%2520their%250Arepresentations%2520are%2520not%2520semantically%2520meaningful%2520and%2520they%2520are%2520inherently%250Avulnerable%2520to%2520small%2520changes.%2520Images%2520with%2520imperceptible%2520differences%2520can%2520have%250Avery%2520different%2520representations%253B%2520on%2520the%2520other%2520hand%252C%2520images%2520that%2520should%2520belong%2520to%250Adifferent%2520semantic%2520classes%2520can%2520have%2520nearly%2520identical%2520representations.%2520Such%250Avulnerability%2520can%2520lead%2520to%2520unreliable%2520classification%2520results%253B%2520for%2520example%252C%250Aunnoticeable%2520changes%2520cause%2520the%2520classification%2520accuracy%2520to%2520be%2520reduced%2520by%2520over%250A60%255C%2525.%2520%2525.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520systematically%250Ademonstrate%2520this%2520fundamental%2520lack%2520of%2520semantic%2520meaningfulness%2520in%2520ViT%250Arepresentations%2520for%2520medical%2520image%2520classification%252C%2520revealing%2520a%2520critical%250Achallenge%2520for%2520their%2520deployment%2520in%2520safety-critical%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01788v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Vision%20Transformer%20Representations%20Semantically%20Meaningful%3F%20A%20Case%0A%20%20Study%20in%20Medical%20Imaging&entry.906535625=Montasir%20Shams%20and%20Chashi%20Mahiul%20Islam%20and%20Shaeke%20Salman%20and%20Phat%20Tran%20and%20Xiuwen%20Liu&entry.1292438233=%20%20Vision%20transformers%20%28ViTs%29%20have%20rapidly%20gained%20prominence%20in%20medical%20imaging%0Atasks%20such%20as%20disease%20classification%2C%20segmentation%2C%20and%20detection%20due%20to%20their%0Asuperior%20accuracy%20compared%20to%20conventional%20deep%20learning%20models.%20However%2C%20due%0Ato%20their%20size%20and%20complex%20interactions%20via%20the%20self-attention%20mechanism%2C%20they%0Aare%20not%20well%20understood.%20In%20particular%2C%20it%20is%20unclear%20whether%20the%0Arepresentations%20produced%20by%20such%20models%20are%20semantically%20meaningful.%20In%20this%0Apaper%2C%20using%20a%20projected%20gradient-based%20algorithm%2C%20we%20show%20that%20their%0Arepresentations%20are%20not%20semantically%20meaningful%20and%20they%20are%20inherently%0Avulnerable%20to%20small%20changes.%20Images%20with%20imperceptible%20differences%20can%20have%0Avery%20different%20representations%3B%20on%20the%20other%20hand%2C%20images%20that%20should%20belong%20to%0Adifferent%20semantic%20classes%20can%20have%20nearly%20identical%20representations.%20Such%0Avulnerability%20can%20lead%20to%20unreliable%20classification%20results%3B%20for%20example%2C%0Aunnoticeable%20changes%20cause%20the%20classification%20accuracy%20to%20be%20reduced%20by%20over%0A60%5C%25.%20%25.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20systematically%0Ademonstrate%20this%20fundamental%20lack%20of%20semantic%20meaningfulness%20in%20ViT%0Arepresentations%20for%20medical%20image%20classification%2C%20revealing%20a%20critical%0Achallenge%20for%20their%20deployment%20in%20safety-critical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01788v2&entry.124074799=Read"},
{"title": "UniTac: Whole-Robot Touch Sensing Without Tactile Sensors", "author": "Wanjia Fu and Hongyu Li and Ivy X. He and Stefanie Tellex and Srinath Sridhar", "abstract": "  Robots can better interact with humans and unstructured environments through\ntouch sensing. However, most commercial robots are not equipped with tactile\nskins, making it challenging to achieve even basic touch-sensing functions,\nsuch as contact localization. We present UniTac, a data-driven whole-body\ntouch-sensing approach that uses only proprioceptive joint sensors and does not\nrequire the installation of additional sensors. Our approach enables a robot\nequipped solely with joint sensors to localize contacts. Our goal is to\ndemocratize touch sensing and provide an off-the-shelf tool for HRI researchers\nto provide their robots with touch-sensing capabilities. We validate our\napproach on two platforms: the Franka robot arm and the Spot quadruped. On\nFranka, we can localize contact to within 8.0 centimeters, and on Spot, we can\nlocalize to within 7.2 centimeters at around 2,000 Hz on an RTX 3090 GPU\nwithout adding any additional sensors to the robot. Project website:\nhttps://ivl.cs.brown.edu/research/unitac.\n", "link": "http://arxiv.org/abs/2507.07980v1", "date": "2025-07-10", "relevancy": 2.1923, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.577}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5572}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniTac%3A%20Whole-Robot%20Touch%20Sensing%20Without%20Tactile%20Sensors&body=Title%3A%20UniTac%3A%20Whole-Robot%20Touch%20Sensing%20Without%20Tactile%20Sensors%0AAuthor%3A%20Wanjia%20Fu%20and%20Hongyu%20Li%20and%20Ivy%20X.%20He%20and%20Stefanie%20Tellex%20and%20Srinath%20Sridhar%0AAbstract%3A%20%20%20Robots%20can%20better%20interact%20with%20humans%20and%20unstructured%20environments%20through%0Atouch%20sensing.%20However%2C%20most%20commercial%20robots%20are%20not%20equipped%20with%20tactile%0Askins%2C%20making%20it%20challenging%20to%20achieve%20even%20basic%20touch-sensing%20functions%2C%0Asuch%20as%20contact%20localization.%20We%20present%20UniTac%2C%20a%20data-driven%20whole-body%0Atouch-sensing%20approach%20that%20uses%20only%20proprioceptive%20joint%20sensors%20and%20does%20not%0Arequire%20the%20installation%20of%20additional%20sensors.%20Our%20approach%20enables%20a%20robot%0Aequipped%20solely%20with%20joint%20sensors%20to%20localize%20contacts.%20Our%20goal%20is%20to%0Ademocratize%20touch%20sensing%20and%20provide%20an%20off-the-shelf%20tool%20for%20HRI%20researchers%0Ato%20provide%20their%20robots%20with%20touch-sensing%20capabilities.%20We%20validate%20our%0Aapproach%20on%20two%20platforms%3A%20the%20Franka%20robot%20arm%20and%20the%20Spot%20quadruped.%20On%0AFranka%2C%20we%20can%20localize%20contact%20to%20within%208.0%20centimeters%2C%20and%20on%20Spot%2C%20we%20can%0Alocalize%20to%20within%207.2%20centimeters%20at%20around%202%2C000%20Hz%20on%20an%20RTX%203090%20GPU%0Awithout%20adding%20any%20additional%20sensors%20to%20the%20robot.%20Project%20website%3A%0Ahttps%3A//ivl.cs.brown.edu/research/unitac.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniTac%253A%2520Whole-Robot%2520Touch%2520Sensing%2520Without%2520Tactile%2520Sensors%26entry.906535625%3DWanjia%2520Fu%2520and%2520Hongyu%2520Li%2520and%2520Ivy%2520X.%2520He%2520and%2520Stefanie%2520Tellex%2520and%2520Srinath%2520Sridhar%26entry.1292438233%3D%2520%2520Robots%2520can%2520better%2520interact%2520with%2520humans%2520and%2520unstructured%2520environments%2520through%250Atouch%2520sensing.%2520However%252C%2520most%2520commercial%2520robots%2520are%2520not%2520equipped%2520with%2520tactile%250Askins%252C%2520making%2520it%2520challenging%2520to%2520achieve%2520even%2520basic%2520touch-sensing%2520functions%252C%250Asuch%2520as%2520contact%2520localization.%2520We%2520present%2520UniTac%252C%2520a%2520data-driven%2520whole-body%250Atouch-sensing%2520approach%2520that%2520uses%2520only%2520proprioceptive%2520joint%2520sensors%2520and%2520does%2520not%250Arequire%2520the%2520installation%2520of%2520additional%2520sensors.%2520Our%2520approach%2520enables%2520a%2520robot%250Aequipped%2520solely%2520with%2520joint%2520sensors%2520to%2520localize%2520contacts.%2520Our%2520goal%2520is%2520to%250Ademocratize%2520touch%2520sensing%2520and%2520provide%2520an%2520off-the-shelf%2520tool%2520for%2520HRI%2520researchers%250Ato%2520provide%2520their%2520robots%2520with%2520touch-sensing%2520capabilities.%2520We%2520validate%2520our%250Aapproach%2520on%2520two%2520platforms%253A%2520the%2520Franka%2520robot%2520arm%2520and%2520the%2520Spot%2520quadruped.%2520On%250AFranka%252C%2520we%2520can%2520localize%2520contact%2520to%2520within%25208.0%2520centimeters%252C%2520and%2520on%2520Spot%252C%2520we%2520can%250Alocalize%2520to%2520within%25207.2%2520centimeters%2520at%2520around%25202%252C000%2520Hz%2520on%2520an%2520RTX%25203090%2520GPU%250Awithout%2520adding%2520any%2520additional%2520sensors%2520to%2520the%2520robot.%2520Project%2520website%253A%250Ahttps%253A//ivl.cs.brown.edu/research/unitac.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTac%3A%20Whole-Robot%20Touch%20Sensing%20Without%20Tactile%20Sensors&entry.906535625=Wanjia%20Fu%20and%20Hongyu%20Li%20and%20Ivy%20X.%20He%20and%20Stefanie%20Tellex%20and%20Srinath%20Sridhar&entry.1292438233=%20%20Robots%20can%20better%20interact%20with%20humans%20and%20unstructured%20environments%20through%0Atouch%20sensing.%20However%2C%20most%20commercial%20robots%20are%20not%20equipped%20with%20tactile%0Askins%2C%20making%20it%20challenging%20to%20achieve%20even%20basic%20touch-sensing%20functions%2C%0Asuch%20as%20contact%20localization.%20We%20present%20UniTac%2C%20a%20data-driven%20whole-body%0Atouch-sensing%20approach%20that%20uses%20only%20proprioceptive%20joint%20sensors%20and%20does%20not%0Arequire%20the%20installation%20of%20additional%20sensors.%20Our%20approach%20enables%20a%20robot%0Aequipped%20solely%20with%20joint%20sensors%20to%20localize%20contacts.%20Our%20goal%20is%20to%0Ademocratize%20touch%20sensing%20and%20provide%20an%20off-the-shelf%20tool%20for%20HRI%20researchers%0Ato%20provide%20their%20robots%20with%20touch-sensing%20capabilities.%20We%20validate%20our%0Aapproach%20on%20two%20platforms%3A%20the%20Franka%20robot%20arm%20and%20the%20Spot%20quadruped.%20On%0AFranka%2C%20we%20can%20localize%20contact%20to%20within%208.0%20centimeters%2C%20and%20on%20Spot%2C%20we%20can%0Alocalize%20to%20within%207.2%20centimeters%20at%20around%202%2C000%20Hz%20on%20an%20RTX%203090%20GPU%0Awithout%20adding%20any%20additional%20sensors%20to%20the%20robot.%20Project%20website%3A%0Ahttps%3A//ivl.cs.brown.edu/research/unitac.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07980v1&entry.124074799=Read"},
{"title": "Breast Ultrasound Tumor Generation via Mask Generator and Text-Guided\n  Network:A Clinically Controllable Framework with Downstream Evaluation", "author": "Haoyu Pan and Hongxin Lin and Zetian Feng and Chuxuan Lin and Junyang Mo and Chu Zhang and Zijian Wu and Yi Wang and Qingqing Zheng", "abstract": "  The development of robust deep learning models for breast ultrasound (BUS)\nimage analysis is significantly constrained by the scarcity of expert-annotated\ndata. To address this limitation, we propose a clinically controllable\ngenerative framework for synthesizing BUS images. This framework integrates\nclinical descriptions with structural masks to generate tumors, enabling\nfine-grained control over tumor characteristics such as morphology,\nechogencity, and shape. Furthermore, we design a semantic-curvature mask\ngenerator, which synthesizes structurally diverse tumor masks guided by\nclinical priors. During inference, synthetic tumor masks serve as input to the\ngenerative framework, producing highly personalized synthetic BUS images with\ntumors that reflect real-world morphological diversity. Quantitative\nevaluations on six public BUS datasets demonstrate the significant clinical\nutility of our synthetic images, showing their effectiveness in enhancing\ndownstream breast cancer diagnosis tasks. Furthermore, visual Turing tests\nconducted by experienced sonographers confirm the realism of the generated\nimages, indicating the framework's potential to support broader clinical\napplications.\n", "link": "http://arxiv.org/abs/2507.07721v1", "date": "2025-07-10", "relevancy": 2.1645, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5514}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5401}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breast%20Ultrasound%20Tumor%20Generation%20via%20Mask%20Generator%20and%20Text-Guided%0A%20%20Network%3AA%20Clinically%20Controllable%20Framework%20with%20Downstream%20Evaluation&body=Title%3A%20Breast%20Ultrasound%20Tumor%20Generation%20via%20Mask%20Generator%20and%20Text-Guided%0A%20%20Network%3AA%20Clinically%20Controllable%20Framework%20with%20Downstream%20Evaluation%0AAuthor%3A%20Haoyu%20Pan%20and%20Hongxin%20Lin%20and%20Zetian%20Feng%20and%20Chuxuan%20Lin%20and%20Junyang%20Mo%20and%20Chu%20Zhang%20and%20Zijian%20Wu%20and%20Yi%20Wang%20and%20Qingqing%20Zheng%0AAbstract%3A%20%20%20The%20development%20of%20robust%20deep%20learning%20models%20for%20breast%20ultrasound%20%28BUS%29%0Aimage%20analysis%20is%20significantly%20constrained%20by%20the%20scarcity%20of%20expert-annotated%0Adata.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20clinically%20controllable%0Agenerative%20framework%20for%20synthesizing%20BUS%20images.%20This%20framework%20integrates%0Aclinical%20descriptions%20with%20structural%20masks%20to%20generate%20tumors%2C%20enabling%0Afine-grained%20control%20over%20tumor%20characteristics%20such%20as%20morphology%2C%0Aechogencity%2C%20and%20shape.%20Furthermore%2C%20we%20design%20a%20semantic-curvature%20mask%0Agenerator%2C%20which%20synthesizes%20structurally%20diverse%20tumor%20masks%20guided%20by%0Aclinical%20priors.%20During%20inference%2C%20synthetic%20tumor%20masks%20serve%20as%20input%20to%20the%0Agenerative%20framework%2C%20producing%20highly%20personalized%20synthetic%20BUS%20images%20with%0Atumors%20that%20reflect%20real-world%20morphological%20diversity.%20Quantitative%0Aevaluations%20on%20six%20public%20BUS%20datasets%20demonstrate%20the%20significant%20clinical%0Autility%20of%20our%20synthetic%20images%2C%20showing%20their%20effectiveness%20in%20enhancing%0Adownstream%20breast%20cancer%20diagnosis%20tasks.%20Furthermore%2C%20visual%20Turing%20tests%0Aconducted%20by%20experienced%20sonographers%20confirm%20the%20realism%20of%20the%20generated%0Aimages%2C%20indicating%20the%20framework%27s%20potential%20to%20support%20broader%20clinical%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreast%2520Ultrasound%2520Tumor%2520Generation%2520via%2520Mask%2520Generator%2520and%2520Text-Guided%250A%2520%2520Network%253AA%2520Clinically%2520Controllable%2520Framework%2520with%2520Downstream%2520Evaluation%26entry.906535625%3DHaoyu%2520Pan%2520and%2520Hongxin%2520Lin%2520and%2520Zetian%2520Feng%2520and%2520Chuxuan%2520Lin%2520and%2520Junyang%2520Mo%2520and%2520Chu%2520Zhang%2520and%2520Zijian%2520Wu%2520and%2520Yi%2520Wang%2520and%2520Qingqing%2520Zheng%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520robust%2520deep%2520learning%2520models%2520for%2520breast%2520ultrasound%2520%2528BUS%2529%250Aimage%2520analysis%2520is%2520significantly%2520constrained%2520by%2520the%2520scarcity%2520of%2520expert-annotated%250Adata.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520clinically%2520controllable%250Agenerative%2520framework%2520for%2520synthesizing%2520BUS%2520images.%2520This%2520framework%2520integrates%250Aclinical%2520descriptions%2520with%2520structural%2520masks%2520to%2520generate%2520tumors%252C%2520enabling%250Afine-grained%2520control%2520over%2520tumor%2520characteristics%2520such%2520as%2520morphology%252C%250Aechogencity%252C%2520and%2520shape.%2520Furthermore%252C%2520we%2520design%2520a%2520semantic-curvature%2520mask%250Agenerator%252C%2520which%2520synthesizes%2520structurally%2520diverse%2520tumor%2520masks%2520guided%2520by%250Aclinical%2520priors.%2520During%2520inference%252C%2520synthetic%2520tumor%2520masks%2520serve%2520as%2520input%2520to%2520the%250Agenerative%2520framework%252C%2520producing%2520highly%2520personalized%2520synthetic%2520BUS%2520images%2520with%250Atumors%2520that%2520reflect%2520real-world%2520morphological%2520diversity.%2520Quantitative%250Aevaluations%2520on%2520six%2520public%2520BUS%2520datasets%2520demonstrate%2520the%2520significant%2520clinical%250Autility%2520of%2520our%2520synthetic%2520images%252C%2520showing%2520their%2520effectiveness%2520in%2520enhancing%250Adownstream%2520breast%2520cancer%2520diagnosis%2520tasks.%2520Furthermore%252C%2520visual%2520Turing%2520tests%250Aconducted%2520by%2520experienced%2520sonographers%2520confirm%2520the%2520realism%2520of%2520the%2520generated%250Aimages%252C%2520indicating%2520the%2520framework%2527s%2520potential%2520to%2520support%2520broader%2520clinical%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breast%20Ultrasound%20Tumor%20Generation%20via%20Mask%20Generator%20and%20Text-Guided%0A%20%20Network%3AA%20Clinically%20Controllable%20Framework%20with%20Downstream%20Evaluation&entry.906535625=Haoyu%20Pan%20and%20Hongxin%20Lin%20and%20Zetian%20Feng%20and%20Chuxuan%20Lin%20and%20Junyang%20Mo%20and%20Chu%20Zhang%20and%20Zijian%20Wu%20and%20Yi%20Wang%20and%20Qingqing%20Zheng&entry.1292438233=%20%20The%20development%20of%20robust%20deep%20learning%20models%20for%20breast%20ultrasound%20%28BUS%29%0Aimage%20analysis%20is%20significantly%20constrained%20by%20the%20scarcity%20of%20expert-annotated%0Adata.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20clinically%20controllable%0Agenerative%20framework%20for%20synthesizing%20BUS%20images.%20This%20framework%20integrates%0Aclinical%20descriptions%20with%20structural%20masks%20to%20generate%20tumors%2C%20enabling%0Afine-grained%20control%20over%20tumor%20characteristics%20such%20as%20morphology%2C%0Aechogencity%2C%20and%20shape.%20Furthermore%2C%20we%20design%20a%20semantic-curvature%20mask%0Agenerator%2C%20which%20synthesizes%20structurally%20diverse%20tumor%20masks%20guided%20by%0Aclinical%20priors.%20During%20inference%2C%20synthetic%20tumor%20masks%20serve%20as%20input%20to%20the%0Agenerative%20framework%2C%20producing%20highly%20personalized%20synthetic%20BUS%20images%20with%0Atumors%20that%20reflect%20real-world%20morphological%20diversity.%20Quantitative%0Aevaluations%20on%20six%20public%20BUS%20datasets%20demonstrate%20the%20significant%20clinical%0Autility%20of%20our%20synthetic%20images%2C%20showing%20their%20effectiveness%20in%20enhancing%0Adownstream%20breast%20cancer%20diagnosis%20tasks.%20Furthermore%2C%20visual%20Turing%20tests%0Aconducted%20by%20experienced%20sonographers%20confirm%20the%20realism%20of%20the%20generated%0Aimages%2C%20indicating%20the%20framework%27s%20potential%20to%20support%20broader%20clinical%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07721v1&entry.124074799=Read"},
{"title": "A Theory of Inference Compute Scaling: Reasoning through Directed\n  Stochastic Skill Search", "author": "Austin R. Ellis-Mohr and Anuj K. Nayak and Lav R. Varshney", "abstract": "  Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation.\n", "link": "http://arxiv.org/abs/2507.00004v2", "date": "2025-07-10", "relevancy": 2.1593, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Theory%20of%20Inference%20Compute%20Scaling%3A%20Reasoning%20through%20Directed%0A%20%20Stochastic%20Skill%20Search&body=Title%3A%20A%20Theory%20of%20Inference%20Compute%20Scaling%3A%20Reasoning%20through%20Directed%0A%20%20Stochastic%20Skill%20Search%0AAuthor%3A%20Austin%20R.%20Ellis-Mohr%20and%20Anuj%20K.%20Nayak%20and%20Lav%20R.%20Varshney%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20demand%20considerable%20computational%2C%20energy%2C%20and%0Afinancial%20resources%20during%20both%20training%20and%20deployment.%20While%20scaling%20laws%20for%0Atraining%20have%20guided%20much%20of%20the%20field%27s%20recent%20progress%2C%20inference%20costs%20now%0Arepresent%20a%20significant%20and%20growing%20component%20of%20the%20overall%20resource%20burden%2C%0Aparticularly%20for%20reasoning-focused%20models.%20Existing%20characterizations%20of%0Acompute-optimality%20that%20consider%20model%20size%2C%20dataset%20size%2C%20and%20inference%20tokens%0Ain%20isolation%20or%20in%20fixed%20combinations%20risk%20overlooking%20more%20efficient%20operating%0Apoints.%20We%20introduce%20directed%20stochastic%20skill%20search%20%28DS3%29%2C%20a%20general%0Aframework%20that%20represents%20inference%20as%20stochastic%20traversal%20over%20a%20learned%0Askill%20graph.%20From%20a%20simplified%20yet%20expressive%20instantiation%2C%20we%20derive%0Aclosed-form%20expressions%20for%20task%20success%20and%20compute%20cost%20across%20a%20wide%20range%0Aof%20inference%20strategies%20--%20including%20chain-of-thought%20%28CoT%29%20and%20tree-of-thought%0A%28ToT%29%20--%20enabling%20comparative%20analysis%20as%20a%20function%20of%20task%20difficulty%20and%0Amodel%20capability.%20To%20that%20end%2C%20we%20extend%20a%20prior%20first-principles%20tripartite%0Agraph%20framework%20of%20LLM%20training%20to%20incorporate%20inference%2C%20and%20separately%20bridge%0ADS3%20with%20empirical%20methods%20that%20characterize%20LLM%20scaling%20behavior.%20We%0Atheoretically%20recover%20empirically%20observed%20patterns%2C%20including%3A%20linear%20accuracy%0Ascaling%20with%20logarithmic%20compute%3B%20variation%20in%20preferred%20inference%20strategies%0Aas%20a%20function%20of%20task%20difficulty%20and%20model%20capability%3B%20emergent%20behavior%0Aelicited%20by%20reasoning%20even%20when%20performance%20plateaus%20under%20parameter%20scaling%3B%0Aand%20both%20best-of-N%20%28BoN%29%20and%20majority%20voting%20behavior%20captured%20within%20a%20unified%0Aanalytical%20framework.%20By%20explicitly%20characterizing%20training-inference%0Ainterdependencies%2C%20our%20framework%20deepens%20theoretical%20understanding%20and%20supports%0Aprincipled%20algorithmic%20design%20and%20resource%20allocation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00004v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Theory%2520of%2520Inference%2520Compute%2520Scaling%253A%2520Reasoning%2520through%2520Directed%250A%2520%2520Stochastic%2520Skill%2520Search%26entry.906535625%3DAustin%2520R.%2520Ellis-Mohr%2520and%2520Anuj%2520K.%2520Nayak%2520and%2520Lav%2520R.%2520Varshney%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520demand%2520considerable%2520computational%252C%2520energy%252C%2520and%250Afinancial%2520resources%2520during%2520both%2520training%2520and%2520deployment.%2520While%2520scaling%2520laws%2520for%250Atraining%2520have%2520guided%2520much%2520of%2520the%2520field%2527s%2520recent%2520progress%252C%2520inference%2520costs%2520now%250Arepresent%2520a%2520significant%2520and%2520growing%2520component%2520of%2520the%2520overall%2520resource%2520burden%252C%250Aparticularly%2520for%2520reasoning-focused%2520models.%2520Existing%2520characterizations%2520of%250Acompute-optimality%2520that%2520consider%2520model%2520size%252C%2520dataset%2520size%252C%2520and%2520inference%2520tokens%250Ain%2520isolation%2520or%2520in%2520fixed%2520combinations%2520risk%2520overlooking%2520more%2520efficient%2520operating%250Apoints.%2520We%2520introduce%2520directed%2520stochastic%2520skill%2520search%2520%2528DS3%2529%252C%2520a%2520general%250Aframework%2520that%2520represents%2520inference%2520as%2520stochastic%2520traversal%2520over%2520a%2520learned%250Askill%2520graph.%2520From%2520a%2520simplified%2520yet%2520expressive%2520instantiation%252C%2520we%2520derive%250Aclosed-form%2520expressions%2520for%2520task%2520success%2520and%2520compute%2520cost%2520across%2520a%2520wide%2520range%250Aof%2520inference%2520strategies%2520--%2520including%2520chain-of-thought%2520%2528CoT%2529%2520and%2520tree-of-thought%250A%2528ToT%2529%2520--%2520enabling%2520comparative%2520analysis%2520as%2520a%2520function%2520of%2520task%2520difficulty%2520and%250Amodel%2520capability.%2520To%2520that%2520end%252C%2520we%2520extend%2520a%2520prior%2520first-principles%2520tripartite%250Agraph%2520framework%2520of%2520LLM%2520training%2520to%2520incorporate%2520inference%252C%2520and%2520separately%2520bridge%250ADS3%2520with%2520empirical%2520methods%2520that%2520characterize%2520LLM%2520scaling%2520behavior.%2520We%250Atheoretically%2520recover%2520empirically%2520observed%2520patterns%252C%2520including%253A%2520linear%2520accuracy%250Ascaling%2520with%2520logarithmic%2520compute%253B%2520variation%2520in%2520preferred%2520inference%2520strategies%250Aas%2520a%2520function%2520of%2520task%2520difficulty%2520and%2520model%2520capability%253B%2520emergent%2520behavior%250Aelicited%2520by%2520reasoning%2520even%2520when%2520performance%2520plateaus%2520under%2520parameter%2520scaling%253B%250Aand%2520both%2520best-of-N%2520%2528BoN%2529%2520and%2520majority%2520voting%2520behavior%2520captured%2520within%2520a%2520unified%250Aanalytical%2520framework.%2520By%2520explicitly%2520characterizing%2520training-inference%250Ainterdependencies%252C%2520our%2520framework%2520deepens%2520theoretical%2520understanding%2520and%2520supports%250Aprincipled%2520algorithmic%2520design%2520and%2520resource%2520allocation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00004v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Theory%20of%20Inference%20Compute%20Scaling%3A%20Reasoning%20through%20Directed%0A%20%20Stochastic%20Skill%20Search&entry.906535625=Austin%20R.%20Ellis-Mohr%20and%20Anuj%20K.%20Nayak%20and%20Lav%20R.%20Varshney&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20demand%20considerable%20computational%2C%20energy%2C%20and%0Afinancial%20resources%20during%20both%20training%20and%20deployment.%20While%20scaling%20laws%20for%0Atraining%20have%20guided%20much%20of%20the%20field%27s%20recent%20progress%2C%20inference%20costs%20now%0Arepresent%20a%20significant%20and%20growing%20component%20of%20the%20overall%20resource%20burden%2C%0Aparticularly%20for%20reasoning-focused%20models.%20Existing%20characterizations%20of%0Acompute-optimality%20that%20consider%20model%20size%2C%20dataset%20size%2C%20and%20inference%20tokens%0Ain%20isolation%20or%20in%20fixed%20combinations%20risk%20overlooking%20more%20efficient%20operating%0Apoints.%20We%20introduce%20directed%20stochastic%20skill%20search%20%28DS3%29%2C%20a%20general%0Aframework%20that%20represents%20inference%20as%20stochastic%20traversal%20over%20a%20learned%0Askill%20graph.%20From%20a%20simplified%20yet%20expressive%20instantiation%2C%20we%20derive%0Aclosed-form%20expressions%20for%20task%20success%20and%20compute%20cost%20across%20a%20wide%20range%0Aof%20inference%20strategies%20--%20including%20chain-of-thought%20%28CoT%29%20and%20tree-of-thought%0A%28ToT%29%20--%20enabling%20comparative%20analysis%20as%20a%20function%20of%20task%20difficulty%20and%0Amodel%20capability.%20To%20that%20end%2C%20we%20extend%20a%20prior%20first-principles%20tripartite%0Agraph%20framework%20of%20LLM%20training%20to%20incorporate%20inference%2C%20and%20separately%20bridge%0ADS3%20with%20empirical%20methods%20that%20characterize%20LLM%20scaling%20behavior.%20We%0Atheoretically%20recover%20empirically%20observed%20patterns%2C%20including%3A%20linear%20accuracy%0Ascaling%20with%20logarithmic%20compute%3B%20variation%20in%20preferred%20inference%20strategies%0Aas%20a%20function%20of%20task%20difficulty%20and%20model%20capability%3B%20emergent%20behavior%0Aelicited%20by%20reasoning%20even%20when%20performance%20plateaus%20under%20parameter%20scaling%3B%0Aand%20both%20best-of-N%20%28BoN%29%20and%20majority%20voting%20behavior%20captured%20within%20a%20unified%0Aanalytical%20framework.%20By%20explicitly%20characterizing%20training-inference%0Ainterdependencies%2C%20our%20framework%20deepens%20theoretical%20understanding%20and%20supports%0Aprincipled%20algorithmic%20design%20and%20resource%20allocation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00004v2&entry.124074799=Read"},
{"title": "Implicit Counterfactual Data Augmentation for Robust Learning", "author": "Xiaoling Zhou and Ou Wu and Michael K. Ng", "abstract": "  Machine learning models are prone to capturing the spurious correlations\nbetween non-causal attributes and classes, with counterfactual data\naugmentation being a promising direction for breaking these spurious\nassociations. However, generating counterfactual data explicitly poses a\nchallenge, and incorporating augmented data into the training process decreases\ntraining efficiency. This study proposes an Implicit Counterfactual Data\nAugmentation (ICDA) method to remove spurious correlations and make stable\npredictions. Specifically, first, a novel sample-wise augmentation strategy is\ndeveloped that generates semantically and counterfactually meaningful deep\nfeatures with distinct augmentation strength for each sample. Second, we derive\nan easy-to-compute surrogate loss on the augmented feature set when the number\nof augmented samples becomes infinite. Third, two concrete schemes are\nproposed, including direct quantification and meta-learning, to derive the key\nparameters for the robust loss. In addition, ICDA is explained from a\nregularization perspective, revealing its capacity to improve intra-class\ncompactness and augment margins at both class and sample levels. Extensive\nexperiments have been conducted across various biased learning scenarios\ncovering both image and text datasets, demonstrating that ICDA consistently\nenhances the generalization and robustness performance of popular networks.\n", "link": "http://arxiv.org/abs/2304.13431v4", "date": "2025-07-10", "relevancy": 2.1562, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5595}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5337}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Counterfactual%20Data%20Augmentation%20for%20Robust%20Learning&body=Title%3A%20Implicit%20Counterfactual%20Data%20Augmentation%20for%20Robust%20Learning%0AAuthor%3A%20Xiaoling%20Zhou%20and%20Ou%20Wu%20and%20Michael%20K.%20Ng%0AAbstract%3A%20%20%20Machine%20learning%20models%20are%20prone%20to%20capturing%20the%20spurious%20correlations%0Abetween%20non-causal%20attributes%20and%20classes%2C%20with%20counterfactual%20data%0Aaugmentation%20being%20a%20promising%20direction%20for%20breaking%20these%20spurious%0Aassociations.%20However%2C%20generating%20counterfactual%20data%20explicitly%20poses%20a%0Achallenge%2C%20and%20incorporating%20augmented%20data%20into%20the%20training%20process%20decreases%0Atraining%20efficiency.%20This%20study%20proposes%20an%20Implicit%20Counterfactual%20Data%0AAugmentation%20%28ICDA%29%20method%20to%20remove%20spurious%20correlations%20and%20make%20stable%0Apredictions.%20Specifically%2C%20first%2C%20a%20novel%20sample-wise%20augmentation%20strategy%20is%0Adeveloped%20that%20generates%20semantically%20and%20counterfactually%20meaningful%20deep%0Afeatures%20with%20distinct%20augmentation%20strength%20for%20each%20sample.%20Second%2C%20we%20derive%0Aan%20easy-to-compute%20surrogate%20loss%20on%20the%20augmented%20feature%20set%20when%20the%20number%0Aof%20augmented%20samples%20becomes%20infinite.%20Third%2C%20two%20concrete%20schemes%20are%0Aproposed%2C%20including%20direct%20quantification%20and%20meta-learning%2C%20to%20derive%20the%20key%0Aparameters%20for%20the%20robust%20loss.%20In%20addition%2C%20ICDA%20is%20explained%20from%20a%0Aregularization%20perspective%2C%20revealing%20its%20capacity%20to%20improve%20intra-class%0Acompactness%20and%20augment%20margins%20at%20both%20class%20and%20sample%20levels.%20Extensive%0Aexperiments%20have%20been%20conducted%20across%20various%20biased%20learning%20scenarios%0Acovering%20both%20image%20and%20text%20datasets%2C%20demonstrating%20that%20ICDA%20consistently%0Aenhances%20the%20generalization%20and%20robustness%20performance%20of%20popular%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.13431v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Counterfactual%2520Data%2520Augmentation%2520for%2520Robust%2520Learning%26entry.906535625%3DXiaoling%2520Zhou%2520and%2520Ou%2520Wu%2520and%2520Michael%2520K.%2520Ng%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520are%2520prone%2520to%2520capturing%2520the%2520spurious%2520correlations%250Abetween%2520non-causal%2520attributes%2520and%2520classes%252C%2520with%2520counterfactual%2520data%250Aaugmentation%2520being%2520a%2520promising%2520direction%2520for%2520breaking%2520these%2520spurious%250Aassociations.%2520However%252C%2520generating%2520counterfactual%2520data%2520explicitly%2520poses%2520a%250Achallenge%252C%2520and%2520incorporating%2520augmented%2520data%2520into%2520the%2520training%2520process%2520decreases%250Atraining%2520efficiency.%2520This%2520study%2520proposes%2520an%2520Implicit%2520Counterfactual%2520Data%250AAugmentation%2520%2528ICDA%2529%2520method%2520to%2520remove%2520spurious%2520correlations%2520and%2520make%2520stable%250Apredictions.%2520Specifically%252C%2520first%252C%2520a%2520novel%2520sample-wise%2520augmentation%2520strategy%2520is%250Adeveloped%2520that%2520generates%2520semantically%2520and%2520counterfactually%2520meaningful%2520deep%250Afeatures%2520with%2520distinct%2520augmentation%2520strength%2520for%2520each%2520sample.%2520Second%252C%2520we%2520derive%250Aan%2520easy-to-compute%2520surrogate%2520loss%2520on%2520the%2520augmented%2520feature%2520set%2520when%2520the%2520number%250Aof%2520augmented%2520samples%2520becomes%2520infinite.%2520Third%252C%2520two%2520concrete%2520schemes%2520are%250Aproposed%252C%2520including%2520direct%2520quantification%2520and%2520meta-learning%252C%2520to%2520derive%2520the%2520key%250Aparameters%2520for%2520the%2520robust%2520loss.%2520In%2520addition%252C%2520ICDA%2520is%2520explained%2520from%2520a%250Aregularization%2520perspective%252C%2520revealing%2520its%2520capacity%2520to%2520improve%2520intra-class%250Acompactness%2520and%2520augment%2520margins%2520at%2520both%2520class%2520and%2520sample%2520levels.%2520Extensive%250Aexperiments%2520have%2520been%2520conducted%2520across%2520various%2520biased%2520learning%2520scenarios%250Acovering%2520both%2520image%2520and%2520text%2520datasets%252C%2520demonstrating%2520that%2520ICDA%2520consistently%250Aenhances%2520the%2520generalization%2520and%2520robustness%2520performance%2520of%2520popular%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.13431v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Counterfactual%20Data%20Augmentation%20for%20Robust%20Learning&entry.906535625=Xiaoling%20Zhou%20and%20Ou%20Wu%20and%20Michael%20K.%20Ng&entry.1292438233=%20%20Machine%20learning%20models%20are%20prone%20to%20capturing%20the%20spurious%20correlations%0Abetween%20non-causal%20attributes%20and%20classes%2C%20with%20counterfactual%20data%0Aaugmentation%20being%20a%20promising%20direction%20for%20breaking%20these%20spurious%0Aassociations.%20However%2C%20generating%20counterfactual%20data%20explicitly%20poses%20a%0Achallenge%2C%20and%20incorporating%20augmented%20data%20into%20the%20training%20process%20decreases%0Atraining%20efficiency.%20This%20study%20proposes%20an%20Implicit%20Counterfactual%20Data%0AAugmentation%20%28ICDA%29%20method%20to%20remove%20spurious%20correlations%20and%20make%20stable%0Apredictions.%20Specifically%2C%20first%2C%20a%20novel%20sample-wise%20augmentation%20strategy%20is%0Adeveloped%20that%20generates%20semantically%20and%20counterfactually%20meaningful%20deep%0Afeatures%20with%20distinct%20augmentation%20strength%20for%20each%20sample.%20Second%2C%20we%20derive%0Aan%20easy-to-compute%20surrogate%20loss%20on%20the%20augmented%20feature%20set%20when%20the%20number%0Aof%20augmented%20samples%20becomes%20infinite.%20Third%2C%20two%20concrete%20schemes%20are%0Aproposed%2C%20including%20direct%20quantification%20and%20meta-learning%2C%20to%20derive%20the%20key%0Aparameters%20for%20the%20robust%20loss.%20In%20addition%2C%20ICDA%20is%20explained%20from%20a%0Aregularization%20perspective%2C%20revealing%20its%20capacity%20to%20improve%20intra-class%0Acompactness%20and%20augment%20margins%20at%20both%20class%20and%20sample%20levels.%20Extensive%0Aexperiments%20have%20been%20conducted%20across%20various%20biased%20learning%20scenarios%0Acovering%20both%20image%20and%20text%20datasets%2C%20demonstrating%20that%20ICDA%20consistently%0Aenhances%20the%20generalization%20and%20robustness%20performance%20of%20popular%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.13431v4&entry.124074799=Read"},
{"title": "Tree-Mamba: A Tree-Aware Mamba for Underwater Monocular Depth Estimation", "author": "Peixian Zhuang and Yijian Wang and Zhenqi Fu and Hongliang Zhang and Sam Kwong and Chongyi Li", "abstract": "  Underwater Monocular Depth Estimation (UMDE) is a critical task that aims to\nestimate high-precision depth maps from underwater degraded images caused by\nlight absorption and scattering effects in marine environments. Recently,\nMamba-based methods have achieved promising performance across various vision\ntasks; however, they struggle with the UMDE task because their inflexible state\nscanning strategies fail to model the structural features of underwater images\neffectively. Meanwhile, existing UMDE datasets usually contain unreliable depth\nlabels, leading to incorrect object-depth relationships between underwater\nimages and their corresponding depth maps. To overcome these limitations, we\ndevelop a novel tree-aware Mamba method, dubbed Tree-Mamba, for estimating\naccurate monocular depth maps from underwater degraded images. Specifically, we\npropose a tree-aware scanning strategy that adaptively constructs a minimum\nspanning tree based on feature similarity. The spatial topological features\namong the tree nodes are then flexibly aggregated through bottom-up and\ntop-down traversals, enabling stronger multi-scale feature representation\ncapabilities. Moreover, we construct an underwater depth estimation benchmark\n(called BlueDepth), which consists of 38,162 underwater image pairs with\nreliable depth labels. This benchmark serves as a foundational dataset for\ntraining existing deep learning-based UMDE methods to learn accurate\nobject-depth relationships. Extensive experiments demonstrate the superiority\nof the proposed Tree-Mamba over several leading methods in both qualitative\nresults and quantitative evaluations with competitive computational efficiency.\nCode and dataset will be available at https://wyjgr.github.io/Tree-Mamba.html.\n", "link": "http://arxiv.org/abs/2507.07687v1", "date": "2025-07-10", "relevancy": 2.1541, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5713}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5397}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree-Mamba%3A%20A%20Tree-Aware%20Mamba%20for%20Underwater%20Monocular%20Depth%20Estimation&body=Title%3A%20Tree-Mamba%3A%20A%20Tree-Aware%20Mamba%20for%20Underwater%20Monocular%20Depth%20Estimation%0AAuthor%3A%20Peixian%20Zhuang%20and%20Yijian%20Wang%20and%20Zhenqi%20Fu%20and%20Hongliang%20Zhang%20and%20Sam%20Kwong%20and%20Chongyi%20Li%0AAbstract%3A%20%20%20Underwater%20Monocular%20Depth%20Estimation%20%28UMDE%29%20is%20a%20critical%20task%20that%20aims%20to%0Aestimate%20high-precision%20depth%20maps%20from%20underwater%20degraded%20images%20caused%20by%0Alight%20absorption%20and%20scattering%20effects%20in%20marine%20environments.%20Recently%2C%0AMamba-based%20methods%20have%20achieved%20promising%20performance%20across%20various%20vision%0Atasks%3B%20however%2C%20they%20struggle%20with%20the%20UMDE%20task%20because%20their%20inflexible%20state%0Ascanning%20strategies%20fail%20to%20model%20the%20structural%20features%20of%20underwater%20images%0Aeffectively.%20Meanwhile%2C%20existing%20UMDE%20datasets%20usually%20contain%20unreliable%20depth%0Alabels%2C%20leading%20to%20incorrect%20object-depth%20relationships%20between%20underwater%0Aimages%20and%20their%20corresponding%20depth%20maps.%20To%20overcome%20these%20limitations%2C%20we%0Adevelop%20a%20novel%20tree-aware%20Mamba%20method%2C%20dubbed%20Tree-Mamba%2C%20for%20estimating%0Aaccurate%20monocular%20depth%20maps%20from%20underwater%20degraded%20images.%20Specifically%2C%20we%0Apropose%20a%20tree-aware%20scanning%20strategy%20that%20adaptively%20constructs%20a%20minimum%0Aspanning%20tree%20based%20on%20feature%20similarity.%20The%20spatial%20topological%20features%0Aamong%20the%20tree%20nodes%20are%20then%20flexibly%20aggregated%20through%20bottom-up%20and%0Atop-down%20traversals%2C%20enabling%20stronger%20multi-scale%20feature%20representation%0Acapabilities.%20Moreover%2C%20we%20construct%20an%20underwater%20depth%20estimation%20benchmark%0A%28called%20BlueDepth%29%2C%20which%20consists%20of%2038%2C162%20underwater%20image%20pairs%20with%0Areliable%20depth%20labels.%20This%20benchmark%20serves%20as%20a%20foundational%20dataset%20for%0Atraining%20existing%20deep%20learning-based%20UMDE%20methods%20to%20learn%20accurate%0Aobject-depth%20relationships.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20the%20proposed%20Tree-Mamba%20over%20several%20leading%20methods%20in%20both%20qualitative%0Aresults%20and%20quantitative%20evaluations%20with%20competitive%20computational%20efficiency.%0ACode%20and%20dataset%20will%20be%20available%20at%20https%3A//wyjgr.github.io/Tree-Mamba.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree-Mamba%253A%2520A%2520Tree-Aware%2520Mamba%2520for%2520Underwater%2520Monocular%2520Depth%2520Estimation%26entry.906535625%3DPeixian%2520Zhuang%2520and%2520Yijian%2520Wang%2520and%2520Zhenqi%2520Fu%2520and%2520Hongliang%2520Zhang%2520and%2520Sam%2520Kwong%2520and%2520Chongyi%2520Li%26entry.1292438233%3D%2520%2520Underwater%2520Monocular%2520Depth%2520Estimation%2520%2528UMDE%2529%2520is%2520a%2520critical%2520task%2520that%2520aims%2520to%250Aestimate%2520high-precision%2520depth%2520maps%2520from%2520underwater%2520degraded%2520images%2520caused%2520by%250Alight%2520absorption%2520and%2520scattering%2520effects%2520in%2520marine%2520environments.%2520Recently%252C%250AMamba-based%2520methods%2520have%2520achieved%2520promising%2520performance%2520across%2520various%2520vision%250Atasks%253B%2520however%252C%2520they%2520struggle%2520with%2520the%2520UMDE%2520task%2520because%2520their%2520inflexible%2520state%250Ascanning%2520strategies%2520fail%2520to%2520model%2520the%2520structural%2520features%2520of%2520underwater%2520images%250Aeffectively.%2520Meanwhile%252C%2520existing%2520UMDE%2520datasets%2520usually%2520contain%2520unreliable%2520depth%250Alabels%252C%2520leading%2520to%2520incorrect%2520object-depth%2520relationships%2520between%2520underwater%250Aimages%2520and%2520their%2520corresponding%2520depth%2520maps.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Adevelop%2520a%2520novel%2520tree-aware%2520Mamba%2520method%252C%2520dubbed%2520Tree-Mamba%252C%2520for%2520estimating%250Aaccurate%2520monocular%2520depth%2520maps%2520from%2520underwater%2520degraded%2520images.%2520Specifically%252C%2520we%250Apropose%2520a%2520tree-aware%2520scanning%2520strategy%2520that%2520adaptively%2520constructs%2520a%2520minimum%250Aspanning%2520tree%2520based%2520on%2520feature%2520similarity.%2520The%2520spatial%2520topological%2520features%250Aamong%2520the%2520tree%2520nodes%2520are%2520then%2520flexibly%2520aggregated%2520through%2520bottom-up%2520and%250Atop-down%2520traversals%252C%2520enabling%2520stronger%2520multi-scale%2520feature%2520representation%250Acapabilities.%2520Moreover%252C%2520we%2520construct%2520an%2520underwater%2520depth%2520estimation%2520benchmark%250A%2528called%2520BlueDepth%2529%252C%2520which%2520consists%2520of%252038%252C162%2520underwater%2520image%2520pairs%2520with%250Areliable%2520depth%2520labels.%2520This%2520benchmark%2520serves%2520as%2520a%2520foundational%2520dataset%2520for%250Atraining%2520existing%2520deep%2520learning-based%2520UMDE%2520methods%2520to%2520learn%2520accurate%250Aobject-depth%2520relationships.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%250Aof%2520the%2520proposed%2520Tree-Mamba%2520over%2520several%2520leading%2520methods%2520in%2520both%2520qualitative%250Aresults%2520and%2520quantitative%2520evaluations%2520with%2520competitive%2520computational%2520efficiency.%250ACode%2520and%2520dataset%2520will%2520be%2520available%2520at%2520https%253A//wyjgr.github.io/Tree-Mamba.html.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree-Mamba%3A%20A%20Tree-Aware%20Mamba%20for%20Underwater%20Monocular%20Depth%20Estimation&entry.906535625=Peixian%20Zhuang%20and%20Yijian%20Wang%20and%20Zhenqi%20Fu%20and%20Hongliang%20Zhang%20and%20Sam%20Kwong%20and%20Chongyi%20Li&entry.1292438233=%20%20Underwater%20Monocular%20Depth%20Estimation%20%28UMDE%29%20is%20a%20critical%20task%20that%20aims%20to%0Aestimate%20high-precision%20depth%20maps%20from%20underwater%20degraded%20images%20caused%20by%0Alight%20absorption%20and%20scattering%20effects%20in%20marine%20environments.%20Recently%2C%0AMamba-based%20methods%20have%20achieved%20promising%20performance%20across%20various%20vision%0Atasks%3B%20however%2C%20they%20struggle%20with%20the%20UMDE%20task%20because%20their%20inflexible%20state%0Ascanning%20strategies%20fail%20to%20model%20the%20structural%20features%20of%20underwater%20images%0Aeffectively.%20Meanwhile%2C%20existing%20UMDE%20datasets%20usually%20contain%20unreliable%20depth%0Alabels%2C%20leading%20to%20incorrect%20object-depth%20relationships%20between%20underwater%0Aimages%20and%20their%20corresponding%20depth%20maps.%20To%20overcome%20these%20limitations%2C%20we%0Adevelop%20a%20novel%20tree-aware%20Mamba%20method%2C%20dubbed%20Tree-Mamba%2C%20for%20estimating%0Aaccurate%20monocular%20depth%20maps%20from%20underwater%20degraded%20images.%20Specifically%2C%20we%0Apropose%20a%20tree-aware%20scanning%20strategy%20that%20adaptively%20constructs%20a%20minimum%0Aspanning%20tree%20based%20on%20feature%20similarity.%20The%20spatial%20topological%20features%0Aamong%20the%20tree%20nodes%20are%20then%20flexibly%20aggregated%20through%20bottom-up%20and%0Atop-down%20traversals%2C%20enabling%20stronger%20multi-scale%20feature%20representation%0Acapabilities.%20Moreover%2C%20we%20construct%20an%20underwater%20depth%20estimation%20benchmark%0A%28called%20BlueDepth%29%2C%20which%20consists%20of%2038%2C162%20underwater%20image%20pairs%20with%0Areliable%20depth%20labels.%20This%20benchmark%20serves%20as%20a%20foundational%20dataset%20for%0Atraining%20existing%20deep%20learning-based%20UMDE%20methods%20to%20learn%20accurate%0Aobject-depth%20relationships.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20the%20proposed%20Tree-Mamba%20over%20several%20leading%20methods%20in%20both%20qualitative%0Aresults%20and%20quantitative%20evaluations%20with%20competitive%20computational%20efficiency.%0ACode%20and%20dataset%20will%20be%20available%20at%20https%3A//wyjgr.github.io/Tree-Mamba.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07687v1&entry.124074799=Read"},
{"title": "Bridging Logic and Learning: Decoding Temporal Logic Embeddings via\n  Transformers", "author": "Sara Candussio and Gaia Saveri and Gabriele Sarti and Luca Bortolussi", "abstract": "  Continuous representations of logic formulae allow us to integrate symbolic\nknowledge into data-driven learning algorithms. If such embeddings are\nsemantically consistent, i.e. if similar specifications are mapped into nearby\nvectors, they enable continuous learning and optimization directly in the\nsemantic space of formulae. However, to translate the optimal continuous\nrepresentation into a concrete requirement, such embeddings must be invertible.\nWe tackle this issue by training a Transformer-based decoder-only model to\ninvert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a\npowerful formalism that allows us to describe properties of signals varying\nover time in an expressive yet concise way. By constructing a small vocabulary\nfrom STL syntax, we demonstrate that our proposed model is able to generate\nvalid formulae after only 1 epoch and to generalize to the semantics of the\nlogic in about 10 epochs. Additionally, the model is able to decode a given\nembedding into formulae that are often simpler in terms of length and nesting\nwhile remaining semantically close (or equivalent) to gold references. We show\nthe effectiveness of our methodology across various levels of training formulae\ncomplexity to assess the impact of training data on the model's ability to\neffectively capture the semantic information contained in the embeddings and\ngeneralize out-of-distribution. Finally, we deploy our model for solving a\nrequirement mining task, i.e. inferring STL specifications that solve a\nclassification task on trajectories, performing the optimization directly in\nthe semantic space.\n", "link": "http://arxiv.org/abs/2507.07808v1", "date": "2025-07-10", "relevancy": 2.1512, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Logic%20and%20Learning%3A%20Decoding%20Temporal%20Logic%20Embeddings%20via%0A%20%20Transformers&body=Title%3A%20Bridging%20Logic%20and%20Learning%3A%20Decoding%20Temporal%20Logic%20Embeddings%20via%0A%20%20Transformers%0AAuthor%3A%20Sara%20Candussio%20and%20Gaia%20Saveri%20and%20Gabriele%20Sarti%20and%20Luca%20Bortolussi%0AAbstract%3A%20%20%20Continuous%20representations%20of%20logic%20formulae%20allow%20us%20to%20integrate%20symbolic%0Aknowledge%20into%20data-driven%20learning%20algorithms.%20If%20such%20embeddings%20are%0Asemantically%20consistent%2C%20i.e.%20if%20similar%20specifications%20are%20mapped%20into%20nearby%0Avectors%2C%20they%20enable%20continuous%20learning%20and%20optimization%20directly%20in%20the%0Asemantic%20space%20of%20formulae.%20However%2C%20to%20translate%20the%20optimal%20continuous%0Arepresentation%20into%20a%20concrete%20requirement%2C%20such%20embeddings%20must%20be%20invertible.%0AWe%20tackle%20this%20issue%20by%20training%20a%20Transformer-based%20decoder-only%20model%20to%0Ainvert%20semantic%20embeddings%20of%20Signal%20Temporal%20Logic%20%28STL%29%20formulae.%20STL%20is%20a%0Apowerful%20formalism%20that%20allows%20us%20to%20describe%20properties%20of%20signals%20varying%0Aover%20time%20in%20an%20expressive%20yet%20concise%20way.%20By%20constructing%20a%20small%20vocabulary%0Afrom%20STL%20syntax%2C%20we%20demonstrate%20that%20our%20proposed%20model%20is%20able%20to%20generate%0Avalid%20formulae%20after%20only%201%20epoch%20and%20to%20generalize%20to%20the%20semantics%20of%20the%0Alogic%20in%20about%2010%20epochs.%20Additionally%2C%20the%20model%20is%20able%20to%20decode%20a%20given%0Aembedding%20into%20formulae%20that%20are%20often%20simpler%20in%20terms%20of%20length%20and%20nesting%0Awhile%20remaining%20semantically%20close%20%28or%20equivalent%29%20to%20gold%20references.%20We%20show%0Athe%20effectiveness%20of%20our%20methodology%20across%20various%20levels%20of%20training%20formulae%0Acomplexity%20to%20assess%20the%20impact%20of%20training%20data%20on%20the%20model%27s%20ability%20to%0Aeffectively%20capture%20the%20semantic%20information%20contained%20in%20the%20embeddings%20and%0Ageneralize%20out-of-distribution.%20Finally%2C%20we%20deploy%20our%20model%20for%20solving%20a%0Arequirement%20mining%20task%2C%20i.e.%20inferring%20STL%20specifications%20that%20solve%20a%0Aclassification%20task%20on%20trajectories%2C%20performing%20the%20optimization%20directly%20in%0Athe%20semantic%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Logic%2520and%2520Learning%253A%2520Decoding%2520Temporal%2520Logic%2520Embeddings%2520via%250A%2520%2520Transformers%26entry.906535625%3DSara%2520Candussio%2520and%2520Gaia%2520Saveri%2520and%2520Gabriele%2520Sarti%2520and%2520Luca%2520Bortolussi%26entry.1292438233%3D%2520%2520Continuous%2520representations%2520of%2520logic%2520formulae%2520allow%2520us%2520to%2520integrate%2520symbolic%250Aknowledge%2520into%2520data-driven%2520learning%2520algorithms.%2520If%2520such%2520embeddings%2520are%250Asemantically%2520consistent%252C%2520i.e.%2520if%2520similar%2520specifications%2520are%2520mapped%2520into%2520nearby%250Avectors%252C%2520they%2520enable%2520continuous%2520learning%2520and%2520optimization%2520directly%2520in%2520the%250Asemantic%2520space%2520of%2520formulae.%2520However%252C%2520to%2520translate%2520the%2520optimal%2520continuous%250Arepresentation%2520into%2520a%2520concrete%2520requirement%252C%2520such%2520embeddings%2520must%2520be%2520invertible.%250AWe%2520tackle%2520this%2520issue%2520by%2520training%2520a%2520Transformer-based%2520decoder-only%2520model%2520to%250Ainvert%2520semantic%2520embeddings%2520of%2520Signal%2520Temporal%2520Logic%2520%2528STL%2529%2520formulae.%2520STL%2520is%2520a%250Apowerful%2520formalism%2520that%2520allows%2520us%2520to%2520describe%2520properties%2520of%2520signals%2520varying%250Aover%2520time%2520in%2520an%2520expressive%2520yet%2520concise%2520way.%2520By%2520constructing%2520a%2520small%2520vocabulary%250Afrom%2520STL%2520syntax%252C%2520we%2520demonstrate%2520that%2520our%2520proposed%2520model%2520is%2520able%2520to%2520generate%250Avalid%2520formulae%2520after%2520only%25201%2520epoch%2520and%2520to%2520generalize%2520to%2520the%2520semantics%2520of%2520the%250Alogic%2520in%2520about%252010%2520epochs.%2520Additionally%252C%2520the%2520model%2520is%2520able%2520to%2520decode%2520a%2520given%250Aembedding%2520into%2520formulae%2520that%2520are%2520often%2520simpler%2520in%2520terms%2520of%2520length%2520and%2520nesting%250Awhile%2520remaining%2520semantically%2520close%2520%2528or%2520equivalent%2529%2520to%2520gold%2520references.%2520We%2520show%250Athe%2520effectiveness%2520of%2520our%2520methodology%2520across%2520various%2520levels%2520of%2520training%2520formulae%250Acomplexity%2520to%2520assess%2520the%2520impact%2520of%2520training%2520data%2520on%2520the%2520model%2527s%2520ability%2520to%250Aeffectively%2520capture%2520the%2520semantic%2520information%2520contained%2520in%2520the%2520embeddings%2520and%250Ageneralize%2520out-of-distribution.%2520Finally%252C%2520we%2520deploy%2520our%2520model%2520for%2520solving%2520a%250Arequirement%2520mining%2520task%252C%2520i.e.%2520inferring%2520STL%2520specifications%2520that%2520solve%2520a%250Aclassification%2520task%2520on%2520trajectories%252C%2520performing%2520the%2520optimization%2520directly%2520in%250Athe%2520semantic%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Logic%20and%20Learning%3A%20Decoding%20Temporal%20Logic%20Embeddings%20via%0A%20%20Transformers&entry.906535625=Sara%20Candussio%20and%20Gaia%20Saveri%20and%20Gabriele%20Sarti%20and%20Luca%20Bortolussi&entry.1292438233=%20%20Continuous%20representations%20of%20logic%20formulae%20allow%20us%20to%20integrate%20symbolic%0Aknowledge%20into%20data-driven%20learning%20algorithms.%20If%20such%20embeddings%20are%0Asemantically%20consistent%2C%20i.e.%20if%20similar%20specifications%20are%20mapped%20into%20nearby%0Avectors%2C%20they%20enable%20continuous%20learning%20and%20optimization%20directly%20in%20the%0Asemantic%20space%20of%20formulae.%20However%2C%20to%20translate%20the%20optimal%20continuous%0Arepresentation%20into%20a%20concrete%20requirement%2C%20such%20embeddings%20must%20be%20invertible.%0AWe%20tackle%20this%20issue%20by%20training%20a%20Transformer-based%20decoder-only%20model%20to%0Ainvert%20semantic%20embeddings%20of%20Signal%20Temporal%20Logic%20%28STL%29%20formulae.%20STL%20is%20a%0Apowerful%20formalism%20that%20allows%20us%20to%20describe%20properties%20of%20signals%20varying%0Aover%20time%20in%20an%20expressive%20yet%20concise%20way.%20By%20constructing%20a%20small%20vocabulary%0Afrom%20STL%20syntax%2C%20we%20demonstrate%20that%20our%20proposed%20model%20is%20able%20to%20generate%0Avalid%20formulae%20after%20only%201%20epoch%20and%20to%20generalize%20to%20the%20semantics%20of%20the%0Alogic%20in%20about%2010%20epochs.%20Additionally%2C%20the%20model%20is%20able%20to%20decode%20a%20given%0Aembedding%20into%20formulae%20that%20are%20often%20simpler%20in%20terms%20of%20length%20and%20nesting%0Awhile%20remaining%20semantically%20close%20%28or%20equivalent%29%20to%20gold%20references.%20We%20show%0Athe%20effectiveness%20of%20our%20methodology%20across%20various%20levels%20of%20training%20formulae%0Acomplexity%20to%20assess%20the%20impact%20of%20training%20data%20on%20the%20model%27s%20ability%20to%0Aeffectively%20capture%20the%20semantic%20information%20contained%20in%20the%20embeddings%20and%0Ageneralize%20out-of-distribution.%20Finally%2C%20we%20deploy%20our%20model%20for%20solving%20a%0Arequirement%20mining%20task%2C%20i.e.%20inferring%20STL%20specifications%20that%20solve%20a%0Aclassification%20task%20on%20trajectories%2C%20performing%20the%20optimization%20directly%20in%0Athe%20semantic%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07808v1&entry.124074799=Read"},
{"title": "Rethinking Query-based Transformer for Continual Image Segmentation", "author": "Yuchen Zhu and Cheng Shi and Dingyou Wang and Jiajin Tang and Zhengxuan Wei and Yu Wu and Guanbin Li and Sibei Yang", "abstract": "  Class-incremental/Continual image segmentation (CIS) aims to train an image\nsegmenter in stages, where the set of available categories differs at each\nstage. To leverage the built-in objectness of query-based transformers, which\nmitigates catastrophic forgetting of mask proposals, current methods often\ndecouple mask generation from the continual learning process. This study,\nhowever, identifies two key issues with decoupled frameworks: loss of\nplasticity and heavy reliance on input data order. To address these, we conduct\nan in-depth investigation of the built-in objectness and find that highly\naggregated image features provide a shortcut for queries to generate masks\nthrough simple feature alignment. Based on this, we propose SimCIS, a simple\nyet powerful baseline for CIS. Its core idea is to directly select image\nfeatures for query assignment, ensuring \"perfect alignment\" to preserve\nobjectness, while simultaneously allowing queries to select new classes to\npromote plasticity. To further combat catastrophic forgetting of categories, we\nintroduce cross-stage consistency in selection and an innovative \"visual\nquery\"-based replay mechanism. Experiments demonstrate that SimCIS consistently\noutperforms state-of-the-art methods across various segmentation tasks,\nsettings, splits, and input data orders. All models and codes will be made\npublicly available at https://github.com/SooLab/SimCIS.\n", "link": "http://arxiv.org/abs/2507.07831v1", "date": "2025-07-10", "relevancy": 2.1468, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5467}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.538}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Query-based%20Transformer%20for%20Continual%20Image%20Segmentation&body=Title%3A%20Rethinking%20Query-based%20Transformer%20for%20Continual%20Image%20Segmentation%0AAuthor%3A%20Yuchen%20Zhu%20and%20Cheng%20Shi%20and%20Dingyou%20Wang%20and%20Jiajin%20Tang%20and%20Zhengxuan%20Wei%20and%20Yu%20Wu%20and%20Guanbin%20Li%20and%20Sibei%20Yang%0AAbstract%3A%20%20%20Class-incremental/Continual%20image%20segmentation%20%28CIS%29%20aims%20to%20train%20an%20image%0Asegmenter%20in%20stages%2C%20where%20the%20set%20of%20available%20categories%20differs%20at%20each%0Astage.%20To%20leverage%20the%20built-in%20objectness%20of%20query-based%20transformers%2C%20which%0Amitigates%20catastrophic%20forgetting%20of%20mask%20proposals%2C%20current%20methods%20often%0Adecouple%20mask%20generation%20from%20the%20continual%20learning%20process.%20This%20study%2C%0Ahowever%2C%20identifies%20two%20key%20issues%20with%20decoupled%20frameworks%3A%20loss%20of%0Aplasticity%20and%20heavy%20reliance%20on%20input%20data%20order.%20To%20address%20these%2C%20we%20conduct%0Aan%20in-depth%20investigation%20of%20the%20built-in%20objectness%20and%20find%20that%20highly%0Aaggregated%20image%20features%20provide%20a%20shortcut%20for%20queries%20to%20generate%20masks%0Athrough%20simple%20feature%20alignment.%20Based%20on%20this%2C%20we%20propose%20SimCIS%2C%20a%20simple%0Ayet%20powerful%20baseline%20for%20CIS.%20Its%20core%20idea%20is%20to%20directly%20select%20image%0Afeatures%20for%20query%20assignment%2C%20ensuring%20%22perfect%20alignment%22%20to%20preserve%0Aobjectness%2C%20while%20simultaneously%20allowing%20queries%20to%20select%20new%20classes%20to%0Apromote%20plasticity.%20To%20further%20combat%20catastrophic%20forgetting%20of%20categories%2C%20we%0Aintroduce%20cross-stage%20consistency%20in%20selection%20and%20an%20innovative%20%22visual%0Aquery%22-based%20replay%20mechanism.%20Experiments%20demonstrate%20that%20SimCIS%20consistently%0Aoutperforms%20state-of-the-art%20methods%20across%20various%20segmentation%20tasks%2C%0Asettings%2C%20splits%2C%20and%20input%20data%20orders.%20All%20models%20and%20codes%20will%20be%20made%0Apublicly%20available%20at%20https%3A//github.com/SooLab/SimCIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Query-based%2520Transformer%2520for%2520Continual%2520Image%2520Segmentation%26entry.906535625%3DYuchen%2520Zhu%2520and%2520Cheng%2520Shi%2520and%2520Dingyou%2520Wang%2520and%2520Jiajin%2520Tang%2520and%2520Zhengxuan%2520Wei%2520and%2520Yu%2520Wu%2520and%2520Guanbin%2520Li%2520and%2520Sibei%2520Yang%26entry.1292438233%3D%2520%2520Class-incremental/Continual%2520image%2520segmentation%2520%2528CIS%2529%2520aims%2520to%2520train%2520an%2520image%250Asegmenter%2520in%2520stages%252C%2520where%2520the%2520set%2520of%2520available%2520categories%2520differs%2520at%2520each%250Astage.%2520To%2520leverage%2520the%2520built-in%2520objectness%2520of%2520query-based%2520transformers%252C%2520which%250Amitigates%2520catastrophic%2520forgetting%2520of%2520mask%2520proposals%252C%2520current%2520methods%2520often%250Adecouple%2520mask%2520generation%2520from%2520the%2520continual%2520learning%2520process.%2520This%2520study%252C%250Ahowever%252C%2520identifies%2520two%2520key%2520issues%2520with%2520decoupled%2520frameworks%253A%2520loss%2520of%250Aplasticity%2520and%2520heavy%2520reliance%2520on%2520input%2520data%2520order.%2520To%2520address%2520these%252C%2520we%2520conduct%250Aan%2520in-depth%2520investigation%2520of%2520the%2520built-in%2520objectness%2520and%2520find%2520that%2520highly%250Aaggregated%2520image%2520features%2520provide%2520a%2520shortcut%2520for%2520queries%2520to%2520generate%2520masks%250Athrough%2520simple%2520feature%2520alignment.%2520Based%2520on%2520this%252C%2520we%2520propose%2520SimCIS%252C%2520a%2520simple%250Ayet%2520powerful%2520baseline%2520for%2520CIS.%2520Its%2520core%2520idea%2520is%2520to%2520directly%2520select%2520image%250Afeatures%2520for%2520query%2520assignment%252C%2520ensuring%2520%2522perfect%2520alignment%2522%2520to%2520preserve%250Aobjectness%252C%2520while%2520simultaneously%2520allowing%2520queries%2520to%2520select%2520new%2520classes%2520to%250Apromote%2520plasticity.%2520To%2520further%2520combat%2520catastrophic%2520forgetting%2520of%2520categories%252C%2520we%250Aintroduce%2520cross-stage%2520consistency%2520in%2520selection%2520and%2520an%2520innovative%2520%2522visual%250Aquery%2522-based%2520replay%2520mechanism.%2520Experiments%2520demonstrate%2520that%2520SimCIS%2520consistently%250Aoutperforms%2520state-of-the-art%2520methods%2520across%2520various%2520segmentation%2520tasks%252C%250Asettings%252C%2520splits%252C%2520and%2520input%2520data%2520orders.%2520All%2520models%2520and%2520codes%2520will%2520be%2520made%250Apublicly%2520available%2520at%2520https%253A//github.com/SooLab/SimCIS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Query-based%20Transformer%20for%20Continual%20Image%20Segmentation&entry.906535625=Yuchen%20Zhu%20and%20Cheng%20Shi%20and%20Dingyou%20Wang%20and%20Jiajin%20Tang%20and%20Zhengxuan%20Wei%20and%20Yu%20Wu%20and%20Guanbin%20Li%20and%20Sibei%20Yang&entry.1292438233=%20%20Class-incremental/Continual%20image%20segmentation%20%28CIS%29%20aims%20to%20train%20an%20image%0Asegmenter%20in%20stages%2C%20where%20the%20set%20of%20available%20categories%20differs%20at%20each%0Astage.%20To%20leverage%20the%20built-in%20objectness%20of%20query-based%20transformers%2C%20which%0Amitigates%20catastrophic%20forgetting%20of%20mask%20proposals%2C%20current%20methods%20often%0Adecouple%20mask%20generation%20from%20the%20continual%20learning%20process.%20This%20study%2C%0Ahowever%2C%20identifies%20two%20key%20issues%20with%20decoupled%20frameworks%3A%20loss%20of%0Aplasticity%20and%20heavy%20reliance%20on%20input%20data%20order.%20To%20address%20these%2C%20we%20conduct%0Aan%20in-depth%20investigation%20of%20the%20built-in%20objectness%20and%20find%20that%20highly%0Aaggregated%20image%20features%20provide%20a%20shortcut%20for%20queries%20to%20generate%20masks%0Athrough%20simple%20feature%20alignment.%20Based%20on%20this%2C%20we%20propose%20SimCIS%2C%20a%20simple%0Ayet%20powerful%20baseline%20for%20CIS.%20Its%20core%20idea%20is%20to%20directly%20select%20image%0Afeatures%20for%20query%20assignment%2C%20ensuring%20%22perfect%20alignment%22%20to%20preserve%0Aobjectness%2C%20while%20simultaneously%20allowing%20queries%20to%20select%20new%20classes%20to%0Apromote%20plasticity.%20To%20further%20combat%20catastrophic%20forgetting%20of%20categories%2C%20we%0Aintroduce%20cross-stage%20consistency%20in%20selection%20and%20an%20innovative%20%22visual%0Aquery%22-based%20replay%20mechanism.%20Experiments%20demonstrate%20that%20SimCIS%20consistently%0Aoutperforms%20state-of-the-art%20methods%20across%20various%20segmentation%20tasks%2C%0Asettings%2C%20splits%2C%20and%20input%20data%20orders.%20All%20models%20and%20codes%20will%20be%20made%0Apublicly%20available%20at%20https%3A//github.com/SooLab/SimCIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07831v1&entry.124074799=Read"},
{"title": "PyVision: Agentic Vision with Dynamic Tooling", "author": "Shitian Zhao and Haoquan Zhang and Shaoheng Lin and Ming Li and Qilong Wu and Kaipeng Zhang and Chen Wei", "abstract": "  LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.\n", "link": "http://arxiv.org/abs/2507.07998v1", "date": "2025-07-10", "relevancy": 2.1414, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PyVision%3A%20Agentic%20Vision%20with%20Dynamic%20Tooling&body=Title%3A%20PyVision%3A%20Agentic%20Vision%20with%20Dynamic%20Tooling%0AAuthor%3A%20Shitian%20Zhao%20and%20Haoquan%20Zhang%20and%20Shaoheng%20Lin%20and%20Ming%20Li%20and%20Qilong%20Wu%20and%20Kaipeng%20Zhang%20and%20Chen%20Wei%0AAbstract%3A%20%20%20LLMs%20are%20increasingly%20deployed%20as%20agents%2C%20systems%20capable%20of%20planning%2C%0Areasoning%2C%20and%20dynamically%20calling%20external%20tools.%20However%2C%20in%20visual%0Areasoning%2C%20prior%20approaches%20largely%20remain%20limited%20by%20predefined%20workflows%20and%0Astatic%20toolsets.%20In%20this%20report%2C%20we%20present%20PyVision%2C%20an%20interactive%2C%0Amulti-turn%20framework%20that%20enables%20MLLMs%20to%20autonomously%20generate%2C%20execute%2C%20and%0Arefine%20Python-based%20tools%20tailored%20to%20the%20task%20at%20hand%2C%20unlocking%20flexible%20and%0Ainterpretable%20problem-solving.%20We%20develop%20a%20taxonomy%20of%20the%20tools%20created%20by%0APyVision%20and%20analyze%20their%20usage%20across%20a%20diverse%20set%20of%20benchmarks.%0AQuantitatively%2C%20PyVision%20achieves%20consistent%20performance%20gains%2C%20boosting%0AGPT-4.1%20by%20%2B7.8%25%20on%20V%2A%20and%20Claude-4.0-Sonnet%20by%20%2B31.1%25%20on%20VLMsAreBlind-mini.%0AThese%20results%20point%20to%20a%20broader%20shift%3A%20dynamic%20tooling%20allows%20models%20not%20just%0Ato%20use%20tools%2C%20but%20to%20invent%20them%2C%20advancing%20toward%20more%20agentic%20visual%0Areasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPyVision%253A%2520Agentic%2520Vision%2520with%2520Dynamic%2520Tooling%26entry.906535625%3DShitian%2520Zhao%2520and%2520Haoquan%2520Zhang%2520and%2520Shaoheng%2520Lin%2520and%2520Ming%2520Li%2520and%2520Qilong%2520Wu%2520and%2520Kaipeng%2520Zhang%2520and%2520Chen%2520Wei%26entry.1292438233%3D%2520%2520LLMs%2520are%2520increasingly%2520deployed%2520as%2520agents%252C%2520systems%2520capable%2520of%2520planning%252C%250Areasoning%252C%2520and%2520dynamically%2520calling%2520external%2520tools.%2520However%252C%2520in%2520visual%250Areasoning%252C%2520prior%2520approaches%2520largely%2520remain%2520limited%2520by%2520predefined%2520workflows%2520and%250Astatic%2520toolsets.%2520In%2520this%2520report%252C%2520we%2520present%2520PyVision%252C%2520an%2520interactive%252C%250Amulti-turn%2520framework%2520that%2520enables%2520MLLMs%2520to%2520autonomously%2520generate%252C%2520execute%252C%2520and%250Arefine%2520Python-based%2520tools%2520tailored%2520to%2520the%2520task%2520at%2520hand%252C%2520unlocking%2520flexible%2520and%250Ainterpretable%2520problem-solving.%2520We%2520develop%2520a%2520taxonomy%2520of%2520the%2520tools%2520created%2520by%250APyVision%2520and%2520analyze%2520their%2520usage%2520across%2520a%2520diverse%2520set%2520of%2520benchmarks.%250AQuantitatively%252C%2520PyVision%2520achieves%2520consistent%2520performance%2520gains%252C%2520boosting%250AGPT-4.1%2520by%2520%252B7.8%2525%2520on%2520V%252A%2520and%2520Claude-4.0-Sonnet%2520by%2520%252B31.1%2525%2520on%2520VLMsAreBlind-mini.%250AThese%2520results%2520point%2520to%2520a%2520broader%2520shift%253A%2520dynamic%2520tooling%2520allows%2520models%2520not%2520just%250Ato%2520use%2520tools%252C%2520but%2520to%2520invent%2520them%252C%2520advancing%2520toward%2520more%2520agentic%2520visual%250Areasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PyVision%3A%20Agentic%20Vision%20with%20Dynamic%20Tooling&entry.906535625=Shitian%20Zhao%20and%20Haoquan%20Zhang%20and%20Shaoheng%20Lin%20and%20Ming%20Li%20and%20Qilong%20Wu%20and%20Kaipeng%20Zhang%20and%20Chen%20Wei&entry.1292438233=%20%20LLMs%20are%20increasingly%20deployed%20as%20agents%2C%20systems%20capable%20of%20planning%2C%0Areasoning%2C%20and%20dynamically%20calling%20external%20tools.%20However%2C%20in%20visual%0Areasoning%2C%20prior%20approaches%20largely%20remain%20limited%20by%20predefined%20workflows%20and%0Astatic%20toolsets.%20In%20this%20report%2C%20we%20present%20PyVision%2C%20an%20interactive%2C%0Amulti-turn%20framework%20that%20enables%20MLLMs%20to%20autonomously%20generate%2C%20execute%2C%20and%0Arefine%20Python-based%20tools%20tailored%20to%20the%20task%20at%20hand%2C%20unlocking%20flexible%20and%0Ainterpretable%20problem-solving.%20We%20develop%20a%20taxonomy%20of%20the%20tools%20created%20by%0APyVision%20and%20analyze%20their%20usage%20across%20a%20diverse%20set%20of%20benchmarks.%0AQuantitatively%2C%20PyVision%20achieves%20consistent%20performance%20gains%2C%20boosting%0AGPT-4.1%20by%20%2B7.8%25%20on%20V%2A%20and%20Claude-4.0-Sonnet%20by%20%2B31.1%25%20on%20VLMsAreBlind-mini.%0AThese%20results%20point%20to%20a%20broader%20shift%3A%20dynamic%20tooling%20allows%20models%20not%20just%0Ato%20use%20tools%2C%20but%20to%20invent%20them%2C%20advancing%20toward%20more%20agentic%20visual%0Areasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07998v1&entry.124074799=Read"},
{"title": "MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving", "author": "Lu Xu and Jiaqian Yu and Xiongfeng Peng and Yiwei Chen and Weiming Li and Jaewook Yoo and Sunghyun Chunag and Dongwook Lee and Daehyun Ji and Chao Zhang", "abstract": "  Recent studies show large language models (LLMs) and vision language models\n(VLMs) trained using web-scale data can empower end-to-end autonomous driving\nsystems for a better generalization and interpretation. Specifically, by\ndynamically routing inputs to specialized subsets of parameters, the\nMixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve\nsubstantial performance improvements while maintaining computational\nefficiency. However, general MoE models usually demands extensive training data\nand complex optimization. In this work, inspired by the learning process of\nhuman drivers, we propose a skill-oriented MoE, called MoSE, which mimics human\ndrivers' learning process and reasoning process, skill-by-skill and\nstep-by-step. We propose a skill-oriented routing mechanism that begins with\ndefining and annotating specific skills, enabling experts to identify the\nnecessary driving competencies for various scenarios and reasoning tasks,\nthereby facilitating skill-by-skill learning. Further align the driving process\nto multi-step planning in human reasoning and end-to-end driving models, we\nbuild a hierarchical skill dataset and pretrain the router to encourage the\nmodel to think step-by-step. Unlike multi-round dialogs, MoSE integrates\nvaluable auxiliary tasks (e.g.\\ description, reasoning, planning) in one single\nforward process without introducing any extra computational cost. With less\nthan 3B sparsely activated parameters, our model outperforms several 8B+\nparameters on CODA AD corner case reasoning task. Compared to existing methods\nbased on open-source models and data, our approach achieves state-of-the-art\nperformance with significantly reduced activated model size (at least by\n$62.5\\%$) with a single-turn conversation.\n", "link": "http://arxiv.org/abs/2507.07818v1", "date": "2025-07-10", "relevancy": 2.1408, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoSE%3A%20Skill-by-Skill%20Mixture-of-Expert%20Learning%20for%20Autonomous%20Driving&body=Title%3A%20MoSE%3A%20Skill-by-Skill%20Mixture-of-Expert%20Learning%20for%20Autonomous%20Driving%0AAuthor%3A%20Lu%20Xu%20and%20Jiaqian%20Yu%20and%20Xiongfeng%20Peng%20and%20Yiwei%20Chen%20and%20Weiming%20Li%20and%20Jaewook%20Yoo%20and%20Sunghyun%20Chunag%20and%20Dongwook%20Lee%20and%20Daehyun%20Ji%20and%20Chao%20Zhang%0AAbstract%3A%20%20%20Recent%20studies%20show%20large%20language%20models%20%28LLMs%29%20and%20vision%20language%20models%0A%28VLMs%29%20trained%20using%20web-scale%20data%20can%20empower%20end-to-end%20autonomous%20driving%0Asystems%20for%20a%20better%20generalization%20and%20interpretation.%20Specifically%2C%20by%0Adynamically%20routing%20inputs%20to%20specialized%20subsets%20of%20parameters%2C%20the%0AMixture-of-Experts%20%28MoE%29%20technique%20enables%20general%20LLMs%20or%20VLMs%20to%20achieve%0Asubstantial%20performance%20improvements%20while%20maintaining%20computational%0Aefficiency.%20However%2C%20general%20MoE%20models%20usually%20demands%20extensive%20training%20data%0Aand%20complex%20optimization.%20In%20this%20work%2C%20inspired%20by%20the%20learning%20process%20of%0Ahuman%20drivers%2C%20we%20propose%20a%20skill-oriented%20MoE%2C%20called%20MoSE%2C%20which%20mimics%20human%0Adrivers%27%20learning%20process%20and%20reasoning%20process%2C%20skill-by-skill%20and%0Astep-by-step.%20We%20propose%20a%20skill-oriented%20routing%20mechanism%20that%20begins%20with%0Adefining%20and%20annotating%20specific%20skills%2C%20enabling%20experts%20to%20identify%20the%0Anecessary%20driving%20competencies%20for%20various%20scenarios%20and%20reasoning%20tasks%2C%0Athereby%20facilitating%20skill-by-skill%20learning.%20Further%20align%20the%20driving%20process%0Ato%20multi-step%20planning%20in%20human%20reasoning%20and%20end-to-end%20driving%20models%2C%20we%0Abuild%20a%20hierarchical%20skill%20dataset%20and%20pretrain%20the%20router%20to%20encourage%20the%0Amodel%20to%20think%20step-by-step.%20Unlike%20multi-round%20dialogs%2C%20MoSE%20integrates%0Avaluable%20auxiliary%20tasks%20%28e.g.%5C%20description%2C%20reasoning%2C%20planning%29%20in%20one%20single%0Aforward%20process%20without%20introducing%20any%20extra%20computational%20cost.%20With%20less%0Athan%203B%20sparsely%20activated%20parameters%2C%20our%20model%20outperforms%20several%208B%2B%0Aparameters%20on%20CODA%20AD%20corner%20case%20reasoning%20task.%20Compared%20to%20existing%20methods%0Abased%20on%20open-source%20models%20and%20data%2C%20our%20approach%20achieves%20state-of-the-art%0Aperformance%20with%20significantly%20reduced%20activated%20model%20size%20%28at%20least%20by%0A%2462.5%5C%25%24%29%20with%20a%20single-turn%20conversation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoSE%253A%2520Skill-by-Skill%2520Mixture-of-Expert%2520Learning%2520for%2520Autonomous%2520Driving%26entry.906535625%3DLu%2520Xu%2520and%2520Jiaqian%2520Yu%2520and%2520Xiongfeng%2520Peng%2520and%2520Yiwei%2520Chen%2520and%2520Weiming%2520Li%2520and%2520Jaewook%2520Yoo%2520and%2520Sunghyun%2520Chunag%2520and%2520Dongwook%2520Lee%2520and%2520Daehyun%2520Ji%2520and%2520Chao%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520studies%2520show%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520vision%2520language%2520models%250A%2528VLMs%2529%2520trained%2520using%2520web-scale%2520data%2520can%2520empower%2520end-to-end%2520autonomous%2520driving%250Asystems%2520for%2520a%2520better%2520generalization%2520and%2520interpretation.%2520Specifically%252C%2520by%250Adynamically%2520routing%2520inputs%2520to%2520specialized%2520subsets%2520of%2520parameters%252C%2520the%250AMixture-of-Experts%2520%2528MoE%2529%2520technique%2520enables%2520general%2520LLMs%2520or%2520VLMs%2520to%2520achieve%250Asubstantial%2520performance%2520improvements%2520while%2520maintaining%2520computational%250Aefficiency.%2520However%252C%2520general%2520MoE%2520models%2520usually%2520demands%2520extensive%2520training%2520data%250Aand%2520complex%2520optimization.%2520In%2520this%2520work%252C%2520inspired%2520by%2520the%2520learning%2520process%2520of%250Ahuman%2520drivers%252C%2520we%2520propose%2520a%2520skill-oriented%2520MoE%252C%2520called%2520MoSE%252C%2520which%2520mimics%2520human%250Adrivers%2527%2520learning%2520process%2520and%2520reasoning%2520process%252C%2520skill-by-skill%2520and%250Astep-by-step.%2520We%2520propose%2520a%2520skill-oriented%2520routing%2520mechanism%2520that%2520begins%2520with%250Adefining%2520and%2520annotating%2520specific%2520skills%252C%2520enabling%2520experts%2520to%2520identify%2520the%250Anecessary%2520driving%2520competencies%2520for%2520various%2520scenarios%2520and%2520reasoning%2520tasks%252C%250Athereby%2520facilitating%2520skill-by-skill%2520learning.%2520Further%2520align%2520the%2520driving%2520process%250Ato%2520multi-step%2520planning%2520in%2520human%2520reasoning%2520and%2520end-to-end%2520driving%2520models%252C%2520we%250Abuild%2520a%2520hierarchical%2520skill%2520dataset%2520and%2520pretrain%2520the%2520router%2520to%2520encourage%2520the%250Amodel%2520to%2520think%2520step-by-step.%2520Unlike%2520multi-round%2520dialogs%252C%2520MoSE%2520integrates%250Avaluable%2520auxiliary%2520tasks%2520%2528e.g.%255C%2520description%252C%2520reasoning%252C%2520planning%2529%2520in%2520one%2520single%250Aforward%2520process%2520without%2520introducing%2520any%2520extra%2520computational%2520cost.%2520With%2520less%250Athan%25203B%2520sparsely%2520activated%2520parameters%252C%2520our%2520model%2520outperforms%2520several%25208B%252B%250Aparameters%2520on%2520CODA%2520AD%2520corner%2520case%2520reasoning%2520task.%2520Compared%2520to%2520existing%2520methods%250Abased%2520on%2520open-source%2520models%2520and%2520data%252C%2520our%2520approach%2520achieves%2520state-of-the-art%250Aperformance%2520with%2520significantly%2520reduced%2520activated%2520model%2520size%2520%2528at%2520least%2520by%250A%252462.5%255C%2525%2524%2529%2520with%2520a%2520single-turn%2520conversation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoSE%3A%20Skill-by-Skill%20Mixture-of-Expert%20Learning%20for%20Autonomous%20Driving&entry.906535625=Lu%20Xu%20and%20Jiaqian%20Yu%20and%20Xiongfeng%20Peng%20and%20Yiwei%20Chen%20and%20Weiming%20Li%20and%20Jaewook%20Yoo%20and%20Sunghyun%20Chunag%20and%20Dongwook%20Lee%20and%20Daehyun%20Ji%20and%20Chao%20Zhang&entry.1292438233=%20%20Recent%20studies%20show%20large%20language%20models%20%28LLMs%29%20and%20vision%20language%20models%0A%28VLMs%29%20trained%20using%20web-scale%20data%20can%20empower%20end-to-end%20autonomous%20driving%0Asystems%20for%20a%20better%20generalization%20and%20interpretation.%20Specifically%2C%20by%0Adynamically%20routing%20inputs%20to%20specialized%20subsets%20of%20parameters%2C%20the%0AMixture-of-Experts%20%28MoE%29%20technique%20enables%20general%20LLMs%20or%20VLMs%20to%20achieve%0Asubstantial%20performance%20improvements%20while%20maintaining%20computational%0Aefficiency.%20However%2C%20general%20MoE%20models%20usually%20demands%20extensive%20training%20data%0Aand%20complex%20optimization.%20In%20this%20work%2C%20inspired%20by%20the%20learning%20process%20of%0Ahuman%20drivers%2C%20we%20propose%20a%20skill-oriented%20MoE%2C%20called%20MoSE%2C%20which%20mimics%20human%0Adrivers%27%20learning%20process%20and%20reasoning%20process%2C%20skill-by-skill%20and%0Astep-by-step.%20We%20propose%20a%20skill-oriented%20routing%20mechanism%20that%20begins%20with%0Adefining%20and%20annotating%20specific%20skills%2C%20enabling%20experts%20to%20identify%20the%0Anecessary%20driving%20competencies%20for%20various%20scenarios%20and%20reasoning%20tasks%2C%0Athereby%20facilitating%20skill-by-skill%20learning.%20Further%20align%20the%20driving%20process%0Ato%20multi-step%20planning%20in%20human%20reasoning%20and%20end-to-end%20driving%20models%2C%20we%0Abuild%20a%20hierarchical%20skill%20dataset%20and%20pretrain%20the%20router%20to%20encourage%20the%0Amodel%20to%20think%20step-by-step.%20Unlike%20multi-round%20dialogs%2C%20MoSE%20integrates%0Avaluable%20auxiliary%20tasks%20%28e.g.%5C%20description%2C%20reasoning%2C%20planning%29%20in%20one%20single%0Aforward%20process%20without%20introducing%20any%20extra%20computational%20cost.%20With%20less%0Athan%203B%20sparsely%20activated%20parameters%2C%20our%20model%20outperforms%20several%208B%2B%0Aparameters%20on%20CODA%20AD%20corner%20case%20reasoning%20task.%20Compared%20to%20existing%20methods%0Abased%20on%20open-source%20models%20and%20data%2C%20our%20approach%20achieves%20state-of-the-art%0Aperformance%20with%20significantly%20reduced%20activated%20model%20size%20%28at%20least%20by%0A%2462.5%5C%25%24%29%20with%20a%20single-turn%20conversation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07818v1&entry.124074799=Read"},
{"title": "Low Resource Reconstruction Attacks Through Benign Prompts", "author": "Sol Yarkoni and Roi Livni", "abstract": "  The recent advances in generative models such as diffusion models have raised\nseveral risks and concerns related to privacy, copyright infringements and data\nstewardship. To better understand and control the risks, various researchers\nhave created techniques, experiments and attacks that reconstruct images, or\npart of images, from the training set. While these techniques already establish\nthat data from the training set can be reconstructed, they often rely on\nhigh-resources, excess to the training set as well as well-engineered and\ndesigned prompts.\n  In this work, we devise a new attack that requires low resources, assumes\nlittle to no access to the actual training set, and identifies, seemingly,\nbenign prompts that lead to potentially-risky image reconstruction. This\nhighlights the risk that images might even be reconstructed by an uninformed\nuser and unintentionally. For example, we identified that, with regard to one\nexisting model, the prompt ``blue Unisex T-Shirt'' can generate the face of a\nreal-life human model. Our method builds on an intuition from previous works\nwhich leverages domain knowledge and identifies a fundamental vulnerability\nthat stems from the use of scraped data from e-commerce platforms, where\ntemplated layouts and images are tied to pattern-like prompts.\n", "link": "http://arxiv.org/abs/2507.07947v1", "date": "2025-07-10", "relevancy": 2.1283, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5423}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5314}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low%20Resource%20Reconstruction%20Attacks%20Through%20Benign%20Prompts&body=Title%3A%20Low%20Resource%20Reconstruction%20Attacks%20Through%20Benign%20Prompts%0AAuthor%3A%20Sol%20Yarkoni%20and%20Roi%20Livni%0AAbstract%3A%20%20%20The%20recent%20advances%20in%20generative%20models%20such%20as%20diffusion%20models%20have%20raised%0Aseveral%20risks%20and%20concerns%20related%20to%20privacy%2C%20copyright%20infringements%20and%20data%0Astewardship.%20To%20better%20understand%20and%20control%20the%20risks%2C%20various%20researchers%0Ahave%20created%20techniques%2C%20experiments%20and%20attacks%20that%20reconstruct%20images%2C%20or%0Apart%20of%20images%2C%20from%20the%20training%20set.%20While%20these%20techniques%20already%20establish%0Athat%20data%20from%20the%20training%20set%20can%20be%20reconstructed%2C%20they%20often%20rely%20on%0Ahigh-resources%2C%20excess%20to%20the%20training%20set%20as%20well%20as%20well-engineered%20and%0Adesigned%20prompts.%0A%20%20In%20this%20work%2C%20we%20devise%20a%20new%20attack%20that%20requires%20low%20resources%2C%20assumes%0Alittle%20to%20no%20access%20to%20the%20actual%20training%20set%2C%20and%20identifies%2C%20seemingly%2C%0Abenign%20prompts%20that%20lead%20to%20potentially-risky%20image%20reconstruction.%20This%0Ahighlights%20the%20risk%20that%20images%20might%20even%20be%20reconstructed%20by%20an%20uninformed%0Auser%20and%20unintentionally.%20For%20example%2C%20we%20identified%20that%2C%20with%20regard%20to%20one%0Aexisting%20model%2C%20the%20prompt%20%60%60blue%20Unisex%20T-Shirt%27%27%20can%20generate%20the%20face%20of%20a%0Areal-life%20human%20model.%20Our%20method%20builds%20on%20an%20intuition%20from%20previous%20works%0Awhich%20leverages%20domain%20knowledge%20and%20identifies%20a%20fundamental%20vulnerability%0Athat%20stems%20from%20the%20use%20of%20scraped%20data%20from%20e-commerce%20platforms%2C%20where%0Atemplated%20layouts%20and%20images%20are%20tied%20to%20pattern-like%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow%2520Resource%2520Reconstruction%2520Attacks%2520Through%2520Benign%2520Prompts%26entry.906535625%3DSol%2520Yarkoni%2520and%2520Roi%2520Livni%26entry.1292438233%3D%2520%2520The%2520recent%2520advances%2520in%2520generative%2520models%2520such%2520as%2520diffusion%2520models%2520have%2520raised%250Aseveral%2520risks%2520and%2520concerns%2520related%2520to%2520privacy%252C%2520copyright%2520infringements%2520and%2520data%250Astewardship.%2520To%2520better%2520understand%2520and%2520control%2520the%2520risks%252C%2520various%2520researchers%250Ahave%2520created%2520techniques%252C%2520experiments%2520and%2520attacks%2520that%2520reconstruct%2520images%252C%2520or%250Apart%2520of%2520images%252C%2520from%2520the%2520training%2520set.%2520While%2520these%2520techniques%2520already%2520establish%250Athat%2520data%2520from%2520the%2520training%2520set%2520can%2520be%2520reconstructed%252C%2520they%2520often%2520rely%2520on%250Ahigh-resources%252C%2520excess%2520to%2520the%2520training%2520set%2520as%2520well%2520as%2520well-engineered%2520and%250Adesigned%2520prompts.%250A%2520%2520In%2520this%2520work%252C%2520we%2520devise%2520a%2520new%2520attack%2520that%2520requires%2520low%2520resources%252C%2520assumes%250Alittle%2520to%2520no%2520access%2520to%2520the%2520actual%2520training%2520set%252C%2520and%2520identifies%252C%2520seemingly%252C%250Abenign%2520prompts%2520that%2520lead%2520to%2520potentially-risky%2520image%2520reconstruction.%2520This%250Ahighlights%2520the%2520risk%2520that%2520images%2520might%2520even%2520be%2520reconstructed%2520by%2520an%2520uninformed%250Auser%2520and%2520unintentionally.%2520For%2520example%252C%2520we%2520identified%2520that%252C%2520with%2520regard%2520to%2520one%250Aexisting%2520model%252C%2520the%2520prompt%2520%2560%2560blue%2520Unisex%2520T-Shirt%2527%2527%2520can%2520generate%2520the%2520face%2520of%2520a%250Areal-life%2520human%2520model.%2520Our%2520method%2520builds%2520on%2520an%2520intuition%2520from%2520previous%2520works%250Awhich%2520leverages%2520domain%2520knowledge%2520and%2520identifies%2520a%2520fundamental%2520vulnerability%250Athat%2520stems%2520from%2520the%2520use%2520of%2520scraped%2520data%2520from%2520e-commerce%2520platforms%252C%2520where%250Atemplated%2520layouts%2520and%2520images%2520are%2520tied%2520to%2520pattern-like%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low%20Resource%20Reconstruction%20Attacks%20Through%20Benign%20Prompts&entry.906535625=Sol%20Yarkoni%20and%20Roi%20Livni&entry.1292438233=%20%20The%20recent%20advances%20in%20generative%20models%20such%20as%20diffusion%20models%20have%20raised%0Aseveral%20risks%20and%20concerns%20related%20to%20privacy%2C%20copyright%20infringements%20and%20data%0Astewardship.%20To%20better%20understand%20and%20control%20the%20risks%2C%20various%20researchers%0Ahave%20created%20techniques%2C%20experiments%20and%20attacks%20that%20reconstruct%20images%2C%20or%0Apart%20of%20images%2C%20from%20the%20training%20set.%20While%20these%20techniques%20already%20establish%0Athat%20data%20from%20the%20training%20set%20can%20be%20reconstructed%2C%20they%20often%20rely%20on%0Ahigh-resources%2C%20excess%20to%20the%20training%20set%20as%20well%20as%20well-engineered%20and%0Adesigned%20prompts.%0A%20%20In%20this%20work%2C%20we%20devise%20a%20new%20attack%20that%20requires%20low%20resources%2C%20assumes%0Alittle%20to%20no%20access%20to%20the%20actual%20training%20set%2C%20and%20identifies%2C%20seemingly%2C%0Abenign%20prompts%20that%20lead%20to%20potentially-risky%20image%20reconstruction.%20This%0Ahighlights%20the%20risk%20that%20images%20might%20even%20be%20reconstructed%20by%20an%20uninformed%0Auser%20and%20unintentionally.%20For%20example%2C%20we%20identified%20that%2C%20with%20regard%20to%20one%0Aexisting%20model%2C%20the%20prompt%20%60%60blue%20Unisex%20T-Shirt%27%27%20can%20generate%20the%20face%20of%20a%0Areal-life%20human%20model.%20Our%20method%20builds%20on%20an%20intuition%20from%20previous%20works%0Awhich%20leverages%20domain%20knowledge%20and%20identifies%20a%20fundamental%20vulnerability%0Athat%20stems%20from%20the%20use%20of%20scraped%20data%20from%20e-commerce%20platforms%2C%20where%0Atemplated%20layouts%20and%20images%20are%20tied%20to%20pattern-like%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07947v1&entry.124074799=Read"},
{"title": "TinierHAR: Towards Ultra-Lightweight Deep Learning Models for Efficient\n  Human Activity Recognition on Edge Devices", "author": "Sizhen Bian and Mengxi Liu and Vitor Fortes Rey and Daniel Geissler and Paul Lukowicz", "abstract": "  Human Activity Recognition (HAR) on resource-constrained wearable devices\ndemands inference models that harmonize accuracy with computational efficiency.\nThis paper introduces TinierHAR, an ultra-lightweight deep learning\narchitecture that synergizes residual depthwise separable convolutions, gated\nrecurrent units (GRUs), and temporal aggregation to achieve SOTA efficiency\nwithout compromising performance. Evaluated across 14 public HAR datasets,\nTinierHAR reduces Parameters by 2.7x (vs. TinyHAR) and 43.3x (vs.\nDeepConvLSTM), and MACs by 6.4x and 58.6x, respectively, while maintaining the\naveraged F1-scores. Beyond quantitative gains, this work provides the first\nsystematic ablation study dissecting the contributions of spatial-temporal\ncomponents across proposed TinierHAR, prior SOTA TinyHAR, and the classical\nDeepConvLSTM, offering actionable insights for designing efficient HAR systems.\nWe finally discussed the findings and suggested principled design guidelines\nfor future efficient HAR. To catalyze edge-HAR research, we open-source all\nmaterials in this work for future\nbenchmarking\\footnote{https://github.com/zhaxidele/TinierHAR}\n", "link": "http://arxiv.org/abs/2507.07949v1", "date": "2025-07-10", "relevancy": 2.1271, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5452}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5281}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TinierHAR%3A%20Towards%20Ultra-Lightweight%20Deep%20Learning%20Models%20for%20Efficient%0A%20%20Human%20Activity%20Recognition%20on%20Edge%20Devices&body=Title%3A%20TinierHAR%3A%20Towards%20Ultra-Lightweight%20Deep%20Learning%20Models%20for%20Efficient%0A%20%20Human%20Activity%20Recognition%20on%20Edge%20Devices%0AAuthor%3A%20Sizhen%20Bian%20and%20Mengxi%20Liu%20and%20Vitor%20Fortes%20Rey%20and%20Daniel%20Geissler%20and%20Paul%20Lukowicz%0AAbstract%3A%20%20%20Human%20Activity%20Recognition%20%28HAR%29%20on%20resource-constrained%20wearable%20devices%0Ademands%20inference%20models%20that%20harmonize%20accuracy%20with%20computational%20efficiency.%0AThis%20paper%20introduces%20TinierHAR%2C%20an%20ultra-lightweight%20deep%20learning%0Aarchitecture%20that%20synergizes%20residual%20depthwise%20separable%20convolutions%2C%20gated%0Arecurrent%20units%20%28GRUs%29%2C%20and%20temporal%20aggregation%20to%20achieve%20SOTA%20efficiency%0Awithout%20compromising%20performance.%20Evaluated%20across%2014%20public%20HAR%20datasets%2C%0ATinierHAR%20reduces%20Parameters%20by%202.7x%20%28vs.%20TinyHAR%29%20and%2043.3x%20%28vs.%0ADeepConvLSTM%29%2C%20and%20MACs%20by%206.4x%20and%2058.6x%2C%20respectively%2C%20while%20maintaining%20the%0Aaveraged%20F1-scores.%20Beyond%20quantitative%20gains%2C%20this%20work%20provides%20the%20first%0Asystematic%20ablation%20study%20dissecting%20the%20contributions%20of%20spatial-temporal%0Acomponents%20across%20proposed%20TinierHAR%2C%20prior%20SOTA%20TinyHAR%2C%20and%20the%20classical%0ADeepConvLSTM%2C%20offering%20actionable%20insights%20for%20designing%20efficient%20HAR%20systems.%0AWe%20finally%20discussed%20the%20findings%20and%20suggested%20principled%20design%20guidelines%0Afor%20future%20efficient%20HAR.%20To%20catalyze%20edge-HAR%20research%2C%20we%20open-source%20all%0Amaterials%20in%20this%20work%20for%20future%0Abenchmarking%5Cfootnote%7Bhttps%3A//github.com/zhaxidele/TinierHAR%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTinierHAR%253A%2520Towards%2520Ultra-Lightweight%2520Deep%2520Learning%2520Models%2520for%2520Efficient%250A%2520%2520Human%2520Activity%2520Recognition%2520on%2520Edge%2520Devices%26entry.906535625%3DSizhen%2520Bian%2520and%2520Mengxi%2520Liu%2520and%2520Vitor%2520Fortes%2520Rey%2520and%2520Daniel%2520Geissler%2520and%2520Paul%2520Lukowicz%26entry.1292438233%3D%2520%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529%2520on%2520resource-constrained%2520wearable%2520devices%250Ademands%2520inference%2520models%2520that%2520harmonize%2520accuracy%2520with%2520computational%2520efficiency.%250AThis%2520paper%2520introduces%2520TinierHAR%252C%2520an%2520ultra-lightweight%2520deep%2520learning%250Aarchitecture%2520that%2520synergizes%2520residual%2520depthwise%2520separable%2520convolutions%252C%2520gated%250Arecurrent%2520units%2520%2528GRUs%2529%252C%2520and%2520temporal%2520aggregation%2520to%2520achieve%2520SOTA%2520efficiency%250Awithout%2520compromising%2520performance.%2520Evaluated%2520across%252014%2520public%2520HAR%2520datasets%252C%250ATinierHAR%2520reduces%2520Parameters%2520by%25202.7x%2520%2528vs.%2520TinyHAR%2529%2520and%252043.3x%2520%2528vs.%250ADeepConvLSTM%2529%252C%2520and%2520MACs%2520by%25206.4x%2520and%252058.6x%252C%2520respectively%252C%2520while%2520maintaining%2520the%250Aaveraged%2520F1-scores.%2520Beyond%2520quantitative%2520gains%252C%2520this%2520work%2520provides%2520the%2520first%250Asystematic%2520ablation%2520study%2520dissecting%2520the%2520contributions%2520of%2520spatial-temporal%250Acomponents%2520across%2520proposed%2520TinierHAR%252C%2520prior%2520SOTA%2520TinyHAR%252C%2520and%2520the%2520classical%250ADeepConvLSTM%252C%2520offering%2520actionable%2520insights%2520for%2520designing%2520efficient%2520HAR%2520systems.%250AWe%2520finally%2520discussed%2520the%2520findings%2520and%2520suggested%2520principled%2520design%2520guidelines%250Afor%2520future%2520efficient%2520HAR.%2520To%2520catalyze%2520edge-HAR%2520research%252C%2520we%2520open-source%2520all%250Amaterials%2520in%2520this%2520work%2520for%2520future%250Abenchmarking%255Cfootnote%257Bhttps%253A//github.com/zhaxidele/TinierHAR%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TinierHAR%3A%20Towards%20Ultra-Lightweight%20Deep%20Learning%20Models%20for%20Efficient%0A%20%20Human%20Activity%20Recognition%20on%20Edge%20Devices&entry.906535625=Sizhen%20Bian%20and%20Mengxi%20Liu%20and%20Vitor%20Fortes%20Rey%20and%20Daniel%20Geissler%20and%20Paul%20Lukowicz&entry.1292438233=%20%20Human%20Activity%20Recognition%20%28HAR%29%20on%20resource-constrained%20wearable%20devices%0Ademands%20inference%20models%20that%20harmonize%20accuracy%20with%20computational%20efficiency.%0AThis%20paper%20introduces%20TinierHAR%2C%20an%20ultra-lightweight%20deep%20learning%0Aarchitecture%20that%20synergizes%20residual%20depthwise%20separable%20convolutions%2C%20gated%0Arecurrent%20units%20%28GRUs%29%2C%20and%20temporal%20aggregation%20to%20achieve%20SOTA%20efficiency%0Awithout%20compromising%20performance.%20Evaluated%20across%2014%20public%20HAR%20datasets%2C%0ATinierHAR%20reduces%20Parameters%20by%202.7x%20%28vs.%20TinyHAR%29%20and%2043.3x%20%28vs.%0ADeepConvLSTM%29%2C%20and%20MACs%20by%206.4x%20and%2058.6x%2C%20respectively%2C%20while%20maintaining%20the%0Aaveraged%20F1-scores.%20Beyond%20quantitative%20gains%2C%20this%20work%20provides%20the%20first%0Asystematic%20ablation%20study%20dissecting%20the%20contributions%20of%20spatial-temporal%0Acomponents%20across%20proposed%20TinierHAR%2C%20prior%20SOTA%20TinyHAR%2C%20and%20the%20classical%0ADeepConvLSTM%2C%20offering%20actionable%20insights%20for%20designing%20efficient%20HAR%20systems.%0AWe%20finally%20discussed%20the%20findings%20and%20suggested%20principled%20design%20guidelines%0Afor%20future%20efficient%20HAR.%20To%20catalyze%20edge-HAR%20research%2C%20we%20open-source%20all%0Amaterials%20in%20this%20work%20for%20future%0Abenchmarking%5Cfootnote%7Bhttps%3A//github.com/zhaxidele/TinierHAR%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07949v1&entry.124074799=Read"},
{"title": "MolCLIP: A Molecular-Auxiliary CLIP Framework for Identifying Drug\n  Mechanism of Action Based on Time-Lapsed Mitochondrial Images", "author": "Fengqian Pang and Chunyue Lei and Hongfei Zhao and Chenghao Liu and Zhiqiang Xing and Huafeng Wang and Chuyang Ye", "abstract": "  Drug Mechanism of Action (MoA) mainly investigates how drug molecules\ninteract with cells, which is crucial for drug discovery and clinical\napplication. Recently, deep learning models have been used to recognize MoA by\nrelying on high-content and fluorescence images of cells exposed to various\ndrugs. However, these methods focus on spatial characteristics while\noverlooking the temporal dynamics of live cells. Time-lapse imaging is more\nsuitable for observing the cell response to drugs. Additionally, drug molecules\ncan trigger cellular dynamic variations related to specific MoA. This indicates\nthat the drug molecule modality may complement the image counterpart. This\npaper proposes MolCLIP, the first visual language model to combine microscopic\ncell video- and molecule-modalities. MolCLIP designs a molecule-auxiliary CLIP\nframework to guide video features in learning the distribution of the molecular\nlatent space. Furthermore, we integrate a metric learning strategy with MolCLIP\nto optimize the aggregation of video features. Experimental results on the\nMitoDataset demonstrate that MolCLIP achieves improvements of 51.2% and 20.5%\nin mAP for drug identification and MoA recognition, respectively.\n", "link": "http://arxiv.org/abs/2507.07663v1", "date": "2025-07-10", "relevancy": 2.1267, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5629}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5103}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MolCLIP%3A%20A%20Molecular-Auxiliary%20CLIP%20Framework%20for%20Identifying%20Drug%0A%20%20Mechanism%20of%20Action%20Based%20on%20Time-Lapsed%20Mitochondrial%20Images&body=Title%3A%20MolCLIP%3A%20A%20Molecular-Auxiliary%20CLIP%20Framework%20for%20Identifying%20Drug%0A%20%20Mechanism%20of%20Action%20Based%20on%20Time-Lapsed%20Mitochondrial%20Images%0AAuthor%3A%20Fengqian%20Pang%20and%20Chunyue%20Lei%20and%20Hongfei%20Zhao%20and%20Chenghao%20Liu%20and%20Zhiqiang%20Xing%20and%20Huafeng%20Wang%20and%20Chuyang%20Ye%0AAbstract%3A%20%20%20Drug%20Mechanism%20of%20Action%20%28MoA%29%20mainly%20investigates%20how%20drug%20molecules%0Ainteract%20with%20cells%2C%20which%20is%20crucial%20for%20drug%20discovery%20and%20clinical%0Aapplication.%20Recently%2C%20deep%20learning%20models%20have%20been%20used%20to%20recognize%20MoA%20by%0Arelying%20on%20high-content%20and%20fluorescence%20images%20of%20cells%20exposed%20to%20various%0Adrugs.%20However%2C%20these%20methods%20focus%20on%20spatial%20characteristics%20while%0Aoverlooking%20the%20temporal%20dynamics%20of%20live%20cells.%20Time-lapse%20imaging%20is%20more%0Asuitable%20for%20observing%20the%20cell%20response%20to%20drugs.%20Additionally%2C%20drug%20molecules%0Acan%20trigger%20cellular%20dynamic%20variations%20related%20to%20specific%20MoA.%20This%20indicates%0Athat%20the%20drug%20molecule%20modality%20may%20complement%20the%20image%20counterpart.%20This%0Apaper%20proposes%20MolCLIP%2C%20the%20first%20visual%20language%20model%20to%20combine%20microscopic%0Acell%20video-%20and%20molecule-modalities.%20MolCLIP%20designs%20a%20molecule-auxiliary%20CLIP%0Aframework%20to%20guide%20video%20features%20in%20learning%20the%20distribution%20of%20the%20molecular%0Alatent%20space.%20Furthermore%2C%20we%20integrate%20a%20metric%20learning%20strategy%20with%20MolCLIP%0Ato%20optimize%20the%20aggregation%20of%20video%20features.%20Experimental%20results%20on%20the%0AMitoDataset%20demonstrate%20that%20MolCLIP%20achieves%20improvements%20of%2051.2%25%20and%2020.5%25%0Ain%20mAP%20for%20drug%20identification%20and%20MoA%20recognition%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolCLIP%253A%2520A%2520Molecular-Auxiliary%2520CLIP%2520Framework%2520for%2520Identifying%2520Drug%250A%2520%2520Mechanism%2520of%2520Action%2520Based%2520on%2520Time-Lapsed%2520Mitochondrial%2520Images%26entry.906535625%3DFengqian%2520Pang%2520and%2520Chunyue%2520Lei%2520and%2520Hongfei%2520Zhao%2520and%2520Chenghao%2520Liu%2520and%2520Zhiqiang%2520Xing%2520and%2520Huafeng%2520Wang%2520and%2520Chuyang%2520Ye%26entry.1292438233%3D%2520%2520Drug%2520Mechanism%2520of%2520Action%2520%2528MoA%2529%2520mainly%2520investigates%2520how%2520drug%2520molecules%250Ainteract%2520with%2520cells%252C%2520which%2520is%2520crucial%2520for%2520drug%2520discovery%2520and%2520clinical%250Aapplication.%2520Recently%252C%2520deep%2520learning%2520models%2520have%2520been%2520used%2520to%2520recognize%2520MoA%2520by%250Arelying%2520on%2520high-content%2520and%2520fluorescence%2520images%2520of%2520cells%2520exposed%2520to%2520various%250Adrugs.%2520However%252C%2520these%2520methods%2520focus%2520on%2520spatial%2520characteristics%2520while%250Aoverlooking%2520the%2520temporal%2520dynamics%2520of%2520live%2520cells.%2520Time-lapse%2520imaging%2520is%2520more%250Asuitable%2520for%2520observing%2520the%2520cell%2520response%2520to%2520drugs.%2520Additionally%252C%2520drug%2520molecules%250Acan%2520trigger%2520cellular%2520dynamic%2520variations%2520related%2520to%2520specific%2520MoA.%2520This%2520indicates%250Athat%2520the%2520drug%2520molecule%2520modality%2520may%2520complement%2520the%2520image%2520counterpart.%2520This%250Apaper%2520proposes%2520MolCLIP%252C%2520the%2520first%2520visual%2520language%2520model%2520to%2520combine%2520microscopic%250Acell%2520video-%2520and%2520molecule-modalities.%2520MolCLIP%2520designs%2520a%2520molecule-auxiliary%2520CLIP%250Aframework%2520to%2520guide%2520video%2520features%2520in%2520learning%2520the%2520distribution%2520of%2520the%2520molecular%250Alatent%2520space.%2520Furthermore%252C%2520we%2520integrate%2520a%2520metric%2520learning%2520strategy%2520with%2520MolCLIP%250Ato%2520optimize%2520the%2520aggregation%2520of%2520video%2520features.%2520Experimental%2520results%2520on%2520the%250AMitoDataset%2520demonstrate%2520that%2520MolCLIP%2520achieves%2520improvements%2520of%252051.2%2525%2520and%252020.5%2525%250Ain%2520mAP%2520for%2520drug%2520identification%2520and%2520MoA%2520recognition%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MolCLIP%3A%20A%20Molecular-Auxiliary%20CLIP%20Framework%20for%20Identifying%20Drug%0A%20%20Mechanism%20of%20Action%20Based%20on%20Time-Lapsed%20Mitochondrial%20Images&entry.906535625=Fengqian%20Pang%20and%20Chunyue%20Lei%20and%20Hongfei%20Zhao%20and%20Chenghao%20Liu%20and%20Zhiqiang%20Xing%20and%20Huafeng%20Wang%20and%20Chuyang%20Ye&entry.1292438233=%20%20Drug%20Mechanism%20of%20Action%20%28MoA%29%20mainly%20investigates%20how%20drug%20molecules%0Ainteract%20with%20cells%2C%20which%20is%20crucial%20for%20drug%20discovery%20and%20clinical%0Aapplication.%20Recently%2C%20deep%20learning%20models%20have%20been%20used%20to%20recognize%20MoA%20by%0Arelying%20on%20high-content%20and%20fluorescence%20images%20of%20cells%20exposed%20to%20various%0Adrugs.%20However%2C%20these%20methods%20focus%20on%20spatial%20characteristics%20while%0Aoverlooking%20the%20temporal%20dynamics%20of%20live%20cells.%20Time-lapse%20imaging%20is%20more%0Asuitable%20for%20observing%20the%20cell%20response%20to%20drugs.%20Additionally%2C%20drug%20molecules%0Acan%20trigger%20cellular%20dynamic%20variations%20related%20to%20specific%20MoA.%20This%20indicates%0Athat%20the%20drug%20molecule%20modality%20may%20complement%20the%20image%20counterpart.%20This%0Apaper%20proposes%20MolCLIP%2C%20the%20first%20visual%20language%20model%20to%20combine%20microscopic%0Acell%20video-%20and%20molecule-modalities.%20MolCLIP%20designs%20a%20molecule-auxiliary%20CLIP%0Aframework%20to%20guide%20video%20features%20in%20learning%20the%20distribution%20of%20the%20molecular%0Alatent%20space.%20Furthermore%2C%20we%20integrate%20a%20metric%20learning%20strategy%20with%20MolCLIP%0Ato%20optimize%20the%20aggregation%20of%20video%20features.%20Experimental%20results%20on%20the%0AMitoDataset%20demonstrate%20that%20MolCLIP%20achieves%20improvements%20of%2051.2%25%20and%2020.5%25%0Ain%20mAP%20for%20drug%20identification%20and%20MoA%20recognition%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07663v1&entry.124074799=Read"},
{"title": "MeD-3D: A Multimodal Deep Learning Framework for Precise Recurrence\n  Prediction in Clear Cell Renal Cell Carcinoma (ccRCC)", "author": "Hasaan Maqsood and Saif Ur Rehman Khan", "abstract": "  Accurate prediction of recurrence in clear cell renal cell carcinoma (ccRCC)\nremains a major clinical challenge due to the disease complex molecular,\npathological, and clinical heterogeneity. Traditional prognostic models, which\nrely on single data modalities such as radiology, histopathology, or genomics,\noften fail to capture the full spectrum of disease complexity, resulting in\nsuboptimal predictive accuracy. This study aims to overcome these limitations\nby proposing a deep learning (DL) framework that integrates multimodal data,\nincluding CT, MRI, histopathology whole slide images (WSI), clinical data, and\ngenomic profiles, to improve the prediction of ccRCC recurrence and enhance\nclinical decision-making. The proposed framework utilizes a comprehensive\ndataset curated from multiple publicly available sources, including TCGA, TCIA,\nand CPTAC. To process the diverse modalities, domain-specific models are\nemployed: CLAM, a ResNet50-based model, is used for histopathology WSIs, while\nMeD-3D, a pre-trained 3D-ResNet18 model, processes CT and MRI images. For\nstructured clinical and genomic data, a multi-layer perceptron (MLP) is used.\nThese models are designed to extract deep feature embeddings from each\nmodality, which are then fused through an early and late integration\narchitecture. This fusion strategy enables the model to combine complementary\ninformation from multiple sources. Additionally, the framework is designed to\nhandle incomplete data, a common challenge in clinical settings, by enabling\ninference even when certain modalities are missing.\n", "link": "http://arxiv.org/abs/2507.07839v1", "date": "2025-07-10", "relevancy": 2.1216, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5587}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5257}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeD-3D%3A%20A%20Multimodal%20Deep%20Learning%20Framework%20for%20Precise%20Recurrence%0A%20%20Prediction%20in%20Clear%20Cell%20Renal%20Cell%20Carcinoma%20%28ccRCC%29&body=Title%3A%20MeD-3D%3A%20A%20Multimodal%20Deep%20Learning%20Framework%20for%20Precise%20Recurrence%0A%20%20Prediction%20in%20Clear%20Cell%20Renal%20Cell%20Carcinoma%20%28ccRCC%29%0AAuthor%3A%20Hasaan%20Maqsood%20and%20Saif%20Ur%20Rehman%20Khan%0AAbstract%3A%20%20%20Accurate%20prediction%20of%20recurrence%20in%20clear%20cell%20renal%20cell%20carcinoma%20%28ccRCC%29%0Aremains%20a%20major%20clinical%20challenge%20due%20to%20the%20disease%20complex%20molecular%2C%0Apathological%2C%20and%20clinical%20heterogeneity.%20Traditional%20prognostic%20models%2C%20which%0Arely%20on%20single%20data%20modalities%20such%20as%20radiology%2C%20histopathology%2C%20or%20genomics%2C%0Aoften%20fail%20to%20capture%20the%20full%20spectrum%20of%20disease%20complexity%2C%20resulting%20in%0Asuboptimal%20predictive%20accuracy.%20This%20study%20aims%20to%20overcome%20these%20limitations%0Aby%20proposing%20a%20deep%20learning%20%28DL%29%20framework%20that%20integrates%20multimodal%20data%2C%0Aincluding%20CT%2C%20MRI%2C%20histopathology%20whole%20slide%20images%20%28WSI%29%2C%20clinical%20data%2C%20and%0Agenomic%20profiles%2C%20to%20improve%20the%20prediction%20of%20ccRCC%20recurrence%20and%20enhance%0Aclinical%20decision-making.%20The%20proposed%20framework%20utilizes%20a%20comprehensive%0Adataset%20curated%20from%20multiple%20publicly%20available%20sources%2C%20including%20TCGA%2C%20TCIA%2C%0Aand%20CPTAC.%20To%20process%20the%20diverse%20modalities%2C%20domain-specific%20models%20are%0Aemployed%3A%20CLAM%2C%20a%20ResNet50-based%20model%2C%20is%20used%20for%20histopathology%20WSIs%2C%20while%0AMeD-3D%2C%20a%20pre-trained%203D-ResNet18%20model%2C%20processes%20CT%20and%20MRI%20images.%20For%0Astructured%20clinical%20and%20genomic%20data%2C%20a%20multi-layer%20perceptron%20%28MLP%29%20is%20used.%0AThese%20models%20are%20designed%20to%20extract%20deep%20feature%20embeddings%20from%20each%0Amodality%2C%20which%20are%20then%20fused%20through%20an%20early%20and%20late%20integration%0Aarchitecture.%20This%20fusion%20strategy%20enables%20the%20model%20to%20combine%20complementary%0Ainformation%20from%20multiple%20sources.%20Additionally%2C%20the%20framework%20is%20designed%20to%0Ahandle%20incomplete%20data%2C%20a%20common%20challenge%20in%20clinical%20settings%2C%20by%20enabling%0Ainference%20even%20when%20certain%20modalities%20are%20missing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeD-3D%253A%2520A%2520Multimodal%2520Deep%2520Learning%2520Framework%2520for%2520Precise%2520Recurrence%250A%2520%2520Prediction%2520in%2520Clear%2520Cell%2520Renal%2520Cell%2520Carcinoma%2520%2528ccRCC%2529%26entry.906535625%3DHasaan%2520Maqsood%2520and%2520Saif%2520Ur%2520Rehman%2520Khan%26entry.1292438233%3D%2520%2520Accurate%2520prediction%2520of%2520recurrence%2520in%2520clear%2520cell%2520renal%2520cell%2520carcinoma%2520%2528ccRCC%2529%250Aremains%2520a%2520major%2520clinical%2520challenge%2520due%2520to%2520the%2520disease%2520complex%2520molecular%252C%250Apathological%252C%2520and%2520clinical%2520heterogeneity.%2520Traditional%2520prognostic%2520models%252C%2520which%250Arely%2520on%2520single%2520data%2520modalities%2520such%2520as%2520radiology%252C%2520histopathology%252C%2520or%2520genomics%252C%250Aoften%2520fail%2520to%2520capture%2520the%2520full%2520spectrum%2520of%2520disease%2520complexity%252C%2520resulting%2520in%250Asuboptimal%2520predictive%2520accuracy.%2520This%2520study%2520aims%2520to%2520overcome%2520these%2520limitations%250Aby%2520proposing%2520a%2520deep%2520learning%2520%2528DL%2529%2520framework%2520that%2520integrates%2520multimodal%2520data%252C%250Aincluding%2520CT%252C%2520MRI%252C%2520histopathology%2520whole%2520slide%2520images%2520%2528WSI%2529%252C%2520clinical%2520data%252C%2520and%250Agenomic%2520profiles%252C%2520to%2520improve%2520the%2520prediction%2520of%2520ccRCC%2520recurrence%2520and%2520enhance%250Aclinical%2520decision-making.%2520The%2520proposed%2520framework%2520utilizes%2520a%2520comprehensive%250Adataset%2520curated%2520from%2520multiple%2520publicly%2520available%2520sources%252C%2520including%2520TCGA%252C%2520TCIA%252C%250Aand%2520CPTAC.%2520To%2520process%2520the%2520diverse%2520modalities%252C%2520domain-specific%2520models%2520are%250Aemployed%253A%2520CLAM%252C%2520a%2520ResNet50-based%2520model%252C%2520is%2520used%2520for%2520histopathology%2520WSIs%252C%2520while%250AMeD-3D%252C%2520a%2520pre-trained%25203D-ResNet18%2520model%252C%2520processes%2520CT%2520and%2520MRI%2520images.%2520For%250Astructured%2520clinical%2520and%2520genomic%2520data%252C%2520a%2520multi-layer%2520perceptron%2520%2528MLP%2529%2520is%2520used.%250AThese%2520models%2520are%2520designed%2520to%2520extract%2520deep%2520feature%2520embeddings%2520from%2520each%250Amodality%252C%2520which%2520are%2520then%2520fused%2520through%2520an%2520early%2520and%2520late%2520integration%250Aarchitecture.%2520This%2520fusion%2520strategy%2520enables%2520the%2520model%2520to%2520combine%2520complementary%250Ainformation%2520from%2520multiple%2520sources.%2520Additionally%252C%2520the%2520framework%2520is%2520designed%2520to%250Ahandle%2520incomplete%2520data%252C%2520a%2520common%2520challenge%2520in%2520clinical%2520settings%252C%2520by%2520enabling%250Ainference%2520even%2520when%2520certain%2520modalities%2520are%2520missing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeD-3D%3A%20A%20Multimodal%20Deep%20Learning%20Framework%20for%20Precise%20Recurrence%0A%20%20Prediction%20in%20Clear%20Cell%20Renal%20Cell%20Carcinoma%20%28ccRCC%29&entry.906535625=Hasaan%20Maqsood%20and%20Saif%20Ur%20Rehman%20Khan&entry.1292438233=%20%20Accurate%20prediction%20of%20recurrence%20in%20clear%20cell%20renal%20cell%20carcinoma%20%28ccRCC%29%0Aremains%20a%20major%20clinical%20challenge%20due%20to%20the%20disease%20complex%20molecular%2C%0Apathological%2C%20and%20clinical%20heterogeneity.%20Traditional%20prognostic%20models%2C%20which%0Arely%20on%20single%20data%20modalities%20such%20as%20radiology%2C%20histopathology%2C%20or%20genomics%2C%0Aoften%20fail%20to%20capture%20the%20full%20spectrum%20of%20disease%20complexity%2C%20resulting%20in%0Asuboptimal%20predictive%20accuracy.%20This%20study%20aims%20to%20overcome%20these%20limitations%0Aby%20proposing%20a%20deep%20learning%20%28DL%29%20framework%20that%20integrates%20multimodal%20data%2C%0Aincluding%20CT%2C%20MRI%2C%20histopathology%20whole%20slide%20images%20%28WSI%29%2C%20clinical%20data%2C%20and%0Agenomic%20profiles%2C%20to%20improve%20the%20prediction%20of%20ccRCC%20recurrence%20and%20enhance%0Aclinical%20decision-making.%20The%20proposed%20framework%20utilizes%20a%20comprehensive%0Adataset%20curated%20from%20multiple%20publicly%20available%20sources%2C%20including%20TCGA%2C%20TCIA%2C%0Aand%20CPTAC.%20To%20process%20the%20diverse%20modalities%2C%20domain-specific%20models%20are%0Aemployed%3A%20CLAM%2C%20a%20ResNet50-based%20model%2C%20is%20used%20for%20histopathology%20WSIs%2C%20while%0AMeD-3D%2C%20a%20pre-trained%203D-ResNet18%20model%2C%20processes%20CT%20and%20MRI%20images.%20For%0Astructured%20clinical%20and%20genomic%20data%2C%20a%20multi-layer%20perceptron%20%28MLP%29%20is%20used.%0AThese%20models%20are%20designed%20to%20extract%20deep%20feature%20embeddings%20from%20each%0Amodality%2C%20which%20are%20then%20fused%20through%20an%20early%20and%20late%20integration%0Aarchitecture.%20This%20fusion%20strategy%20enables%20the%20model%20to%20combine%20complementary%0Ainformation%20from%20multiple%20sources.%20Additionally%2C%20the%20framework%20is%20designed%20to%0Ahandle%20incomplete%20data%2C%20a%20common%20challenge%20in%20clinical%20settings%2C%20by%20enabling%0Ainference%20even%20when%20certain%20modalities%20are%20missing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07839v1&entry.124074799=Read"},
{"title": "When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical\n  Advances, and Ethical Governance", "author": "Peizhang Shao and Linrui Xu and Jinxi Wang and Wei Zhou and Xingyu Wu", "abstract": "  This paper establishes the first comprehensive review of Large Language\nModels (LLMs) applied within the legal domain. It pioneers an innovative dual\nlens taxonomy that integrates legal reasoning frameworks and professional\nontologies to systematically unify historical research and contemporary\nbreakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such\nas contextual reasoning and generative argumentation, surmount traditional\nlimitations by dynamically capturing legal semantics and unifying evidence\nreasoning. Significant progress is documented in task generalization, reasoning\nformalization, workflow integration, and addressing core challenges in text\nprocessing, knowledge integration, and evaluation rigor via technical\ninnovations like sparse attention mechanisms and mixture-of-experts\narchitectures. However, widespread adoption of LLM introduces critical\nchallenges: hallucination, explainability deficits, jurisdictional adaptation\ndifficulties, and ethical asymmetry. This review proposes a novel taxonomy that\nmaps legal roles to NLP subtasks and computationally implements the Toulmin\nargumentation framework, thus systematizing advances in reasoning, retrieval,\nprediction, and dispute resolution. It identifies key frontiers including\nlow-resource systems, multimodal evidence integration, and dynamic rebuttal\nhandling. Ultimately, this work provides both a technical roadmap for\nresearchers and a conceptual framework for practitioners navigating the\nalgorithmic future, laying a robust foundation for the next era of legal\nartificial intelligence. We have created a GitHub repository to index the\nrelevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.\n", "link": "http://arxiv.org/abs/2507.07748v1", "date": "2025-07-10", "relevancy": 2.1178, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.534}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Large%20Language%20Models%20Meet%20Law%3A%20Dual-Lens%20Taxonomy%2C%20Technical%0A%20%20Advances%2C%20and%20Ethical%20Governance&body=Title%3A%20When%20Large%20Language%20Models%20Meet%20Law%3A%20Dual-Lens%20Taxonomy%2C%20Technical%0A%20%20Advances%2C%20and%20Ethical%20Governance%0AAuthor%3A%20Peizhang%20Shao%20and%20Linrui%20Xu%20and%20Jinxi%20Wang%20and%20Wei%20Zhou%20and%20Xingyu%20Wu%0AAbstract%3A%20%20%20This%20paper%20establishes%20the%20first%20comprehensive%20review%20of%20Large%20Language%0AModels%20%28LLMs%29%20applied%20within%20the%20legal%20domain.%20It%20pioneers%20an%20innovative%20dual%0Alens%20taxonomy%20that%20integrates%20legal%20reasoning%20frameworks%20and%20professional%0Aontologies%20to%20systematically%20unify%20historical%20research%20and%20contemporary%0Abreakthroughs.%20Transformer-based%20LLMs%2C%20which%20exhibit%20emergent%20capabilities%20such%0Aas%20contextual%20reasoning%20and%20generative%20argumentation%2C%20surmount%20traditional%0Alimitations%20by%20dynamically%20capturing%20legal%20semantics%20and%20unifying%20evidence%0Areasoning.%20Significant%20progress%20is%20documented%20in%20task%20generalization%2C%20reasoning%0Aformalization%2C%20workflow%20integration%2C%20and%20addressing%20core%20challenges%20in%20text%0Aprocessing%2C%20knowledge%20integration%2C%20and%20evaluation%20rigor%20via%20technical%0Ainnovations%20like%20sparse%20attention%20mechanisms%20and%20mixture-of-experts%0Aarchitectures.%20However%2C%20widespread%20adoption%20of%20LLM%20introduces%20critical%0Achallenges%3A%20hallucination%2C%20explainability%20deficits%2C%20jurisdictional%20adaptation%0Adifficulties%2C%20and%20ethical%20asymmetry.%20This%20review%20proposes%20a%20novel%20taxonomy%20that%0Amaps%20legal%20roles%20to%20NLP%20subtasks%20and%20computationally%20implements%20the%20Toulmin%0Aargumentation%20framework%2C%20thus%20systematizing%20advances%20in%20reasoning%2C%20retrieval%2C%0Aprediction%2C%20and%20dispute%20resolution.%20It%20identifies%20key%20frontiers%20including%0Alow-resource%20systems%2C%20multimodal%20evidence%20integration%2C%20and%20dynamic%20rebuttal%0Ahandling.%20Ultimately%2C%20this%20work%20provides%20both%20a%20technical%20roadmap%20for%0Aresearchers%20and%20a%20conceptual%20framework%20for%20practitioners%20navigating%20the%0Aalgorithmic%20future%2C%20laying%20a%20robust%20foundation%20for%20the%20next%20era%20of%20legal%0Aartificial%20intelligence.%20We%20have%20created%20a%20GitHub%20repository%20to%20index%20the%0Arelevant%20papers%3A%20https%3A//github.com/Kilimajaro/LLMs_Meet_Law.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Large%2520Language%2520Models%2520Meet%2520Law%253A%2520Dual-Lens%2520Taxonomy%252C%2520Technical%250A%2520%2520Advances%252C%2520and%2520Ethical%2520Governance%26entry.906535625%3DPeizhang%2520Shao%2520and%2520Linrui%2520Xu%2520and%2520Jinxi%2520Wang%2520and%2520Wei%2520Zhou%2520and%2520Xingyu%2520Wu%26entry.1292438233%3D%2520%2520This%2520paper%2520establishes%2520the%2520first%2520comprehensive%2520review%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520applied%2520within%2520the%2520legal%2520domain.%2520It%2520pioneers%2520an%2520innovative%2520dual%250Alens%2520taxonomy%2520that%2520integrates%2520legal%2520reasoning%2520frameworks%2520and%2520professional%250Aontologies%2520to%2520systematically%2520unify%2520historical%2520research%2520and%2520contemporary%250Abreakthroughs.%2520Transformer-based%2520LLMs%252C%2520which%2520exhibit%2520emergent%2520capabilities%2520such%250Aas%2520contextual%2520reasoning%2520and%2520generative%2520argumentation%252C%2520surmount%2520traditional%250Alimitations%2520by%2520dynamically%2520capturing%2520legal%2520semantics%2520and%2520unifying%2520evidence%250Areasoning.%2520Significant%2520progress%2520is%2520documented%2520in%2520task%2520generalization%252C%2520reasoning%250Aformalization%252C%2520workflow%2520integration%252C%2520and%2520addressing%2520core%2520challenges%2520in%2520text%250Aprocessing%252C%2520knowledge%2520integration%252C%2520and%2520evaluation%2520rigor%2520via%2520technical%250Ainnovations%2520like%2520sparse%2520attention%2520mechanisms%2520and%2520mixture-of-experts%250Aarchitectures.%2520However%252C%2520widespread%2520adoption%2520of%2520LLM%2520introduces%2520critical%250Achallenges%253A%2520hallucination%252C%2520explainability%2520deficits%252C%2520jurisdictional%2520adaptation%250Adifficulties%252C%2520and%2520ethical%2520asymmetry.%2520This%2520review%2520proposes%2520a%2520novel%2520taxonomy%2520that%250Amaps%2520legal%2520roles%2520to%2520NLP%2520subtasks%2520and%2520computationally%2520implements%2520the%2520Toulmin%250Aargumentation%2520framework%252C%2520thus%2520systematizing%2520advances%2520in%2520reasoning%252C%2520retrieval%252C%250Aprediction%252C%2520and%2520dispute%2520resolution.%2520It%2520identifies%2520key%2520frontiers%2520including%250Alow-resource%2520systems%252C%2520multimodal%2520evidence%2520integration%252C%2520and%2520dynamic%2520rebuttal%250Ahandling.%2520Ultimately%252C%2520this%2520work%2520provides%2520both%2520a%2520technical%2520roadmap%2520for%250Aresearchers%2520and%2520a%2520conceptual%2520framework%2520for%2520practitioners%2520navigating%2520the%250Aalgorithmic%2520future%252C%2520laying%2520a%2520robust%2520foundation%2520for%2520the%2520next%2520era%2520of%2520legal%250Aartificial%2520intelligence.%2520We%2520have%2520created%2520a%2520GitHub%2520repository%2520to%2520index%2520the%250Arelevant%2520papers%253A%2520https%253A//github.com/Kilimajaro/LLMs_Meet_Law.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Large%20Language%20Models%20Meet%20Law%3A%20Dual-Lens%20Taxonomy%2C%20Technical%0A%20%20Advances%2C%20and%20Ethical%20Governance&entry.906535625=Peizhang%20Shao%20and%20Linrui%20Xu%20and%20Jinxi%20Wang%20and%20Wei%20Zhou%20and%20Xingyu%20Wu&entry.1292438233=%20%20This%20paper%20establishes%20the%20first%20comprehensive%20review%20of%20Large%20Language%0AModels%20%28LLMs%29%20applied%20within%20the%20legal%20domain.%20It%20pioneers%20an%20innovative%20dual%0Alens%20taxonomy%20that%20integrates%20legal%20reasoning%20frameworks%20and%20professional%0Aontologies%20to%20systematically%20unify%20historical%20research%20and%20contemporary%0Abreakthroughs.%20Transformer-based%20LLMs%2C%20which%20exhibit%20emergent%20capabilities%20such%0Aas%20contextual%20reasoning%20and%20generative%20argumentation%2C%20surmount%20traditional%0Alimitations%20by%20dynamically%20capturing%20legal%20semantics%20and%20unifying%20evidence%0Areasoning.%20Significant%20progress%20is%20documented%20in%20task%20generalization%2C%20reasoning%0Aformalization%2C%20workflow%20integration%2C%20and%20addressing%20core%20challenges%20in%20text%0Aprocessing%2C%20knowledge%20integration%2C%20and%20evaluation%20rigor%20via%20technical%0Ainnovations%20like%20sparse%20attention%20mechanisms%20and%20mixture-of-experts%0Aarchitectures.%20However%2C%20widespread%20adoption%20of%20LLM%20introduces%20critical%0Achallenges%3A%20hallucination%2C%20explainability%20deficits%2C%20jurisdictional%20adaptation%0Adifficulties%2C%20and%20ethical%20asymmetry.%20This%20review%20proposes%20a%20novel%20taxonomy%20that%0Amaps%20legal%20roles%20to%20NLP%20subtasks%20and%20computationally%20implements%20the%20Toulmin%0Aargumentation%20framework%2C%20thus%20systematizing%20advances%20in%20reasoning%2C%20retrieval%2C%0Aprediction%2C%20and%20dispute%20resolution.%20It%20identifies%20key%20frontiers%20including%0Alow-resource%20systems%2C%20multimodal%20evidence%20integration%2C%20and%20dynamic%20rebuttal%0Ahandling.%20Ultimately%2C%20this%20work%20provides%20both%20a%20technical%20roadmap%20for%0Aresearchers%20and%20a%20conceptual%20framework%20for%20practitioners%20navigating%20the%0Aalgorithmic%20future%2C%20laying%20a%20robust%20foundation%20for%20the%20next%20era%20of%20legal%0Aartificial%20intelligence.%20We%20have%20created%20a%20GitHub%20repository%20to%20index%20the%0Arelevant%20papers%3A%20https%3A//github.com/Kilimajaro/LLMs_Meet_Law.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07748v1&entry.124074799=Read"},
{"title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative\n  Models", "author": "Yashar Deldjoo and Nikhil Mehta and Maheswaran Sathiamoorthy and Shuai Zhang and Pablo Castells and Julian McAuley", "abstract": "  Recommender systems powered by generative models (Gen-RecSys) extend beyond\nclassical item ranking by producing open-ended content, which simultaneously\nunlocks richer user experiences and introduces new risks. On one hand, these\nsystems can enhance personalization and appeal through dynamic explanations and\nmulti-turn dialogues. On the other hand, they might venture into unknown\nterritory-hallucinating nonexistent items, amplifying bias, or leaking private\ninformation. Traditional accuracy metrics cannot fully capture these\nchallenges, as they fail to measure factual correctness, content safety, or\nalignment with user intent.\n  This paper makes two main contributions. First, we categorize the evaluation\nchallenges of Gen-RecSys into two groups: (i) existing concerns that are\nexacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new\nrisks (e.g., item hallucinations, contradictory explanations). Second, we\npropose a holistic evaluation approach that includes scenario-based assessments\nand multi-metric checks-incorporating relevance, factual grounding, bias\ndetection, and policy compliance. Our goal is to provide a guiding framework so\nresearchers and practitioners can thoroughly assess Gen-RecSys, ensuring\neffective personalization and responsible deployment.\n", "link": "http://arxiv.org/abs/2504.06667v2", "date": "2025-07-10", "relevancy": 2.1146, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5504}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5359}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Holistic%20Evaluation%20of%20Recommender%20Systems%20Powered%20by%20Generative%0A%20%20Models&body=Title%3A%20Toward%20Holistic%20Evaluation%20of%20Recommender%20Systems%20Powered%20by%20Generative%0A%20%20Models%0AAuthor%3A%20Yashar%20Deldjoo%20and%20Nikhil%20Mehta%20and%20Maheswaran%20Sathiamoorthy%20and%20Shuai%20Zhang%20and%20Pablo%20Castells%20and%20Julian%20McAuley%0AAbstract%3A%20%20%20Recommender%20systems%20powered%20by%20generative%20models%20%28Gen-RecSys%29%20extend%20beyond%0Aclassical%20item%20ranking%20by%20producing%20open-ended%20content%2C%20which%20simultaneously%0Aunlocks%20richer%20user%20experiences%20and%20introduces%20new%20risks.%20On%20one%20hand%2C%20these%0Asystems%20can%20enhance%20personalization%20and%20appeal%20through%20dynamic%20explanations%20and%0Amulti-turn%20dialogues.%20On%20the%20other%20hand%2C%20they%20might%20venture%20into%20unknown%0Aterritory-hallucinating%20nonexistent%20items%2C%20amplifying%20bias%2C%20or%20leaking%20private%0Ainformation.%20Traditional%20accuracy%20metrics%20cannot%20fully%20capture%20these%0Achallenges%2C%20as%20they%20fail%20to%20measure%20factual%20correctness%2C%20content%20safety%2C%20or%0Aalignment%20with%20user%20intent.%0A%20%20This%20paper%20makes%20two%20main%20contributions.%20First%2C%20we%20categorize%20the%20evaluation%0Achallenges%20of%20Gen-RecSys%20into%20two%20groups%3A%20%28i%29%20existing%20concerns%20that%20are%0Aexacerbated%20by%20generative%20outputs%20%28e.g.%2C%20bias%2C%20privacy%29%20and%20%28ii%29%20entirely%20new%0Arisks%20%28e.g.%2C%20item%20hallucinations%2C%20contradictory%20explanations%29.%20Second%2C%20we%0Apropose%20a%20holistic%20evaluation%20approach%20that%20includes%20scenario-based%20assessments%0Aand%20multi-metric%20checks-incorporating%20relevance%2C%20factual%20grounding%2C%20bias%0Adetection%2C%20and%20policy%20compliance.%20Our%20goal%20is%20to%20provide%20a%20guiding%20framework%20so%0Aresearchers%20and%20practitioners%20can%20thoroughly%20assess%20Gen-RecSys%2C%20ensuring%0Aeffective%20personalization%20and%20responsible%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06667v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Holistic%2520Evaluation%2520of%2520Recommender%2520Systems%2520Powered%2520by%2520Generative%250A%2520%2520Models%26entry.906535625%3DYashar%2520Deldjoo%2520and%2520Nikhil%2520Mehta%2520and%2520Maheswaran%2520Sathiamoorthy%2520and%2520Shuai%2520Zhang%2520and%2520Pablo%2520Castells%2520and%2520Julian%2520McAuley%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520powered%2520by%2520generative%2520models%2520%2528Gen-RecSys%2529%2520extend%2520beyond%250Aclassical%2520item%2520ranking%2520by%2520producing%2520open-ended%2520content%252C%2520which%2520simultaneously%250Aunlocks%2520richer%2520user%2520experiences%2520and%2520introduces%2520new%2520risks.%2520On%2520one%2520hand%252C%2520these%250Asystems%2520can%2520enhance%2520personalization%2520and%2520appeal%2520through%2520dynamic%2520explanations%2520and%250Amulti-turn%2520dialogues.%2520On%2520the%2520other%2520hand%252C%2520they%2520might%2520venture%2520into%2520unknown%250Aterritory-hallucinating%2520nonexistent%2520items%252C%2520amplifying%2520bias%252C%2520or%2520leaking%2520private%250Ainformation.%2520Traditional%2520accuracy%2520metrics%2520cannot%2520fully%2520capture%2520these%250Achallenges%252C%2520as%2520they%2520fail%2520to%2520measure%2520factual%2520correctness%252C%2520content%2520safety%252C%2520or%250Aalignment%2520with%2520user%2520intent.%250A%2520%2520This%2520paper%2520makes%2520two%2520main%2520contributions.%2520First%252C%2520we%2520categorize%2520the%2520evaluation%250Achallenges%2520of%2520Gen-RecSys%2520into%2520two%2520groups%253A%2520%2528i%2529%2520existing%2520concerns%2520that%2520are%250Aexacerbated%2520by%2520generative%2520outputs%2520%2528e.g.%252C%2520bias%252C%2520privacy%2529%2520and%2520%2528ii%2529%2520entirely%2520new%250Arisks%2520%2528e.g.%252C%2520item%2520hallucinations%252C%2520contradictory%2520explanations%2529.%2520Second%252C%2520we%250Apropose%2520a%2520holistic%2520evaluation%2520approach%2520that%2520includes%2520scenario-based%2520assessments%250Aand%2520multi-metric%2520checks-incorporating%2520relevance%252C%2520factual%2520grounding%252C%2520bias%250Adetection%252C%2520and%2520policy%2520compliance.%2520Our%2520goal%2520is%2520to%2520provide%2520a%2520guiding%2520framework%2520so%250Aresearchers%2520and%2520practitioners%2520can%2520thoroughly%2520assess%2520Gen-RecSys%252C%2520ensuring%250Aeffective%2520personalization%2520and%2520responsible%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06667v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Holistic%20Evaluation%20of%20Recommender%20Systems%20Powered%20by%20Generative%0A%20%20Models&entry.906535625=Yashar%20Deldjoo%20and%20Nikhil%20Mehta%20and%20Maheswaran%20Sathiamoorthy%20and%20Shuai%20Zhang%20and%20Pablo%20Castells%20and%20Julian%20McAuley&entry.1292438233=%20%20Recommender%20systems%20powered%20by%20generative%20models%20%28Gen-RecSys%29%20extend%20beyond%0Aclassical%20item%20ranking%20by%20producing%20open-ended%20content%2C%20which%20simultaneously%0Aunlocks%20richer%20user%20experiences%20and%20introduces%20new%20risks.%20On%20one%20hand%2C%20these%0Asystems%20can%20enhance%20personalization%20and%20appeal%20through%20dynamic%20explanations%20and%0Amulti-turn%20dialogues.%20On%20the%20other%20hand%2C%20they%20might%20venture%20into%20unknown%0Aterritory-hallucinating%20nonexistent%20items%2C%20amplifying%20bias%2C%20or%20leaking%20private%0Ainformation.%20Traditional%20accuracy%20metrics%20cannot%20fully%20capture%20these%0Achallenges%2C%20as%20they%20fail%20to%20measure%20factual%20correctness%2C%20content%20safety%2C%20or%0Aalignment%20with%20user%20intent.%0A%20%20This%20paper%20makes%20two%20main%20contributions.%20First%2C%20we%20categorize%20the%20evaluation%0Achallenges%20of%20Gen-RecSys%20into%20two%20groups%3A%20%28i%29%20existing%20concerns%20that%20are%0Aexacerbated%20by%20generative%20outputs%20%28e.g.%2C%20bias%2C%20privacy%29%20and%20%28ii%29%20entirely%20new%0Arisks%20%28e.g.%2C%20item%20hallucinations%2C%20contradictory%20explanations%29.%20Second%2C%20we%0Apropose%20a%20holistic%20evaluation%20approach%20that%20includes%20scenario-based%20assessments%0Aand%20multi-metric%20checks-incorporating%20relevance%2C%20factual%20grounding%2C%20bias%0Adetection%2C%20and%20policy%20compliance.%20Our%20goal%20is%20to%20provide%20a%20guiding%20framework%20so%0Aresearchers%20and%20practitioners%20can%20thoroughly%20assess%20Gen-RecSys%2C%20ensuring%0Aeffective%20personalization%20and%20responsible%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06667v2&entry.124074799=Read"},
{"title": "MIRA: A Novel Framework for Fusing Modalities in Medical RAG", "author": "Jinhong Wang and Tajamul Ashraf and Zongyan Han and Jorma Laaksonen and Rao Mohammad Anwer", "abstract": "  Multimodal Large Language Models (MLLMs) have significantly advanced\nAI-assisted medical diagnosis, but they often generate factually inconsistent\nresponses that deviate from established medical knowledge. Retrieval-Augmented\nGeneration (RAG) enhances factual accuracy by integrating external sources, but\nit presents two key challenges. First, insufficient retrieval can miss critical\ninformation, whereas excessive retrieval can introduce irrelevant or misleading\ncontent, disrupting model output. Second, even when the model initially\nprovides correct answers, over-reliance on retrieved data can lead to factual\nerrors. To address these issues, we introduce the Multimodal Intelligent\nRetrieval and Augmentation (MIRA) framework, designed to optimize factual\naccuracy in MLLM. MIRA consists of two key components: (1) a calibrated\nRethinking and Rearrangement module that dynamically adjusts the number of\nretrieved contexts to manage factual risk, and (2) A medical RAG framework\nintegrating image embeddings and a medical knowledge base with a query-rewrite\nmodule for efficient multimodal reasoning. This enables the model to\neffectively integrate both its inherent knowledge and external references. Our\nevaluation of publicly available medical VQA and report generation benchmarks\ndemonstrates that MIRA substantially enhances factual accuracy and overall\nperformance, achieving new state-of-the-art results. Code is released at\nhttps://github.com/mbzuai-oryx/MIRA.\n", "link": "http://arxiv.org/abs/2507.07902v1", "date": "2025-07-10", "relevancy": 2.1101, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.542}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5287}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIRA%3A%20A%20Novel%20Framework%20for%20Fusing%20Modalities%20in%20Medical%20RAG&body=Title%3A%20MIRA%3A%20A%20Novel%20Framework%20for%20Fusing%20Modalities%20in%20Medical%20RAG%0AAuthor%3A%20Jinhong%20Wang%20and%20Tajamul%20Ashraf%20and%20Zongyan%20Han%20and%20Jorma%20Laaksonen%20and%20Rao%20Mohammad%20Anwer%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20advanced%0AAI-assisted%20medical%20diagnosis%2C%20but%20they%20often%20generate%20factually%20inconsistent%0Aresponses%20that%20deviate%20from%20established%20medical%20knowledge.%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20enhances%20factual%20accuracy%20by%20integrating%20external%20sources%2C%20but%0Ait%20presents%20two%20key%20challenges.%20First%2C%20insufficient%20retrieval%20can%20miss%20critical%0Ainformation%2C%20whereas%20excessive%20retrieval%20can%20introduce%20irrelevant%20or%20misleading%0Acontent%2C%20disrupting%20model%20output.%20Second%2C%20even%20when%20the%20model%20initially%0Aprovides%20correct%20answers%2C%20over-reliance%20on%20retrieved%20data%20can%20lead%20to%20factual%0Aerrors.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20Multimodal%20Intelligent%0ARetrieval%20and%20Augmentation%20%28MIRA%29%20framework%2C%20designed%20to%20optimize%20factual%0Aaccuracy%20in%20MLLM.%20MIRA%20consists%20of%20two%20key%20components%3A%20%281%29%20a%20calibrated%0ARethinking%20and%20Rearrangement%20module%20that%20dynamically%20adjusts%20the%20number%20of%0Aretrieved%20contexts%20to%20manage%20factual%20risk%2C%20and%20%282%29%20A%20medical%20RAG%20framework%0Aintegrating%20image%20embeddings%20and%20a%20medical%20knowledge%20base%20with%20a%20query-rewrite%0Amodule%20for%20efficient%20multimodal%20reasoning.%20This%20enables%20the%20model%20to%0Aeffectively%20integrate%20both%20its%20inherent%20knowledge%20and%20external%20references.%20Our%0Aevaluation%20of%20publicly%20available%20medical%20VQA%20and%20report%20generation%20benchmarks%0Ademonstrates%20that%20MIRA%20substantially%20enhances%20factual%20accuracy%20and%20overall%0Aperformance%2C%20achieving%20new%20state-of-the-art%20results.%20Code%20is%20released%20at%0Ahttps%3A//github.com/mbzuai-oryx/MIRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIRA%253A%2520A%2520Novel%2520Framework%2520for%2520Fusing%2520Modalities%2520in%2520Medical%2520RAG%26entry.906535625%3DJinhong%2520Wang%2520and%2520Tajamul%2520Ashraf%2520and%2520Zongyan%2520Han%2520and%2520Jorma%2520Laaksonen%2520and%2520Rao%2520Mohammad%2520Anwer%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520significantly%2520advanced%250AAI-assisted%2520medical%2520diagnosis%252C%2520but%2520they%2520often%2520generate%2520factually%2520inconsistent%250Aresponses%2520that%2520deviate%2520from%2520established%2520medical%2520knowledge.%2520Retrieval-Augmented%250AGeneration%2520%2528RAG%2529%2520enhances%2520factual%2520accuracy%2520by%2520integrating%2520external%2520sources%252C%2520but%250Ait%2520presents%2520two%2520key%2520challenges.%2520First%252C%2520insufficient%2520retrieval%2520can%2520miss%2520critical%250Ainformation%252C%2520whereas%2520excessive%2520retrieval%2520can%2520introduce%2520irrelevant%2520or%2520misleading%250Acontent%252C%2520disrupting%2520model%2520output.%2520Second%252C%2520even%2520when%2520the%2520model%2520initially%250Aprovides%2520correct%2520answers%252C%2520over-reliance%2520on%2520retrieved%2520data%2520can%2520lead%2520to%2520factual%250Aerrors.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520the%2520Multimodal%2520Intelligent%250ARetrieval%2520and%2520Augmentation%2520%2528MIRA%2529%2520framework%252C%2520designed%2520to%2520optimize%2520factual%250Aaccuracy%2520in%2520MLLM.%2520MIRA%2520consists%2520of%2520two%2520key%2520components%253A%2520%25281%2529%2520a%2520calibrated%250ARethinking%2520and%2520Rearrangement%2520module%2520that%2520dynamically%2520adjusts%2520the%2520number%2520of%250Aretrieved%2520contexts%2520to%2520manage%2520factual%2520risk%252C%2520and%2520%25282%2529%2520A%2520medical%2520RAG%2520framework%250Aintegrating%2520image%2520embeddings%2520and%2520a%2520medical%2520knowledge%2520base%2520with%2520a%2520query-rewrite%250Amodule%2520for%2520efficient%2520multimodal%2520reasoning.%2520This%2520enables%2520the%2520model%2520to%250Aeffectively%2520integrate%2520both%2520its%2520inherent%2520knowledge%2520and%2520external%2520references.%2520Our%250Aevaluation%2520of%2520publicly%2520available%2520medical%2520VQA%2520and%2520report%2520generation%2520benchmarks%250Ademonstrates%2520that%2520MIRA%2520substantially%2520enhances%2520factual%2520accuracy%2520and%2520overall%250Aperformance%252C%2520achieving%2520new%2520state-of-the-art%2520results.%2520Code%2520is%2520released%2520at%250Ahttps%253A//github.com/mbzuai-oryx/MIRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIRA%3A%20A%20Novel%20Framework%20for%20Fusing%20Modalities%20in%20Medical%20RAG&entry.906535625=Jinhong%20Wang%20and%20Tajamul%20Ashraf%20and%20Zongyan%20Han%20and%20Jorma%20Laaksonen%20and%20Rao%20Mohammad%20Anwer&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20advanced%0AAI-assisted%20medical%20diagnosis%2C%20but%20they%20often%20generate%20factually%20inconsistent%0Aresponses%20that%20deviate%20from%20established%20medical%20knowledge.%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20enhances%20factual%20accuracy%20by%20integrating%20external%20sources%2C%20but%0Ait%20presents%20two%20key%20challenges.%20First%2C%20insufficient%20retrieval%20can%20miss%20critical%0Ainformation%2C%20whereas%20excessive%20retrieval%20can%20introduce%20irrelevant%20or%20misleading%0Acontent%2C%20disrupting%20model%20output.%20Second%2C%20even%20when%20the%20model%20initially%0Aprovides%20correct%20answers%2C%20over-reliance%20on%20retrieved%20data%20can%20lead%20to%20factual%0Aerrors.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20Multimodal%20Intelligent%0ARetrieval%20and%20Augmentation%20%28MIRA%29%20framework%2C%20designed%20to%20optimize%20factual%0Aaccuracy%20in%20MLLM.%20MIRA%20consists%20of%20two%20key%20components%3A%20%281%29%20a%20calibrated%0ARethinking%20and%20Rearrangement%20module%20that%20dynamically%20adjusts%20the%20number%20of%0Aretrieved%20contexts%20to%20manage%20factual%20risk%2C%20and%20%282%29%20A%20medical%20RAG%20framework%0Aintegrating%20image%20embeddings%20and%20a%20medical%20knowledge%20base%20with%20a%20query-rewrite%0Amodule%20for%20efficient%20multimodal%20reasoning.%20This%20enables%20the%20model%20to%0Aeffectively%20integrate%20both%20its%20inherent%20knowledge%20and%20external%20references.%20Our%0Aevaluation%20of%20publicly%20available%20medical%20VQA%20and%20report%20generation%20benchmarks%0Ademonstrates%20that%20MIRA%20substantially%20enhances%20factual%20accuracy%20and%20overall%0Aperformance%2C%20achieving%20new%20state-of-the-art%20results.%20Code%20is%20released%20at%0Ahttps%3A//github.com/mbzuai-oryx/MIRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07902v1&entry.124074799=Read"},
{"title": "Investigating Context-Faithfulness in Large Language Models: The Roles\n  of Memory Strength and Evidence Style", "author": "Yuepei Li and Kang Zhou and Qiao Qiao and Bach Nguyen and Qing Wang and Qi Li", "abstract": "  Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by\nincorporating external information into the response generation process.\nHowever, how context-faithful LLMs are and what factors influence LLMs' context\nfaithfulness remain largely unexplored. In this study, we investigate the\nimpact of memory strength and evidence presentation on LLMs' receptiveness to\nexternal evidence. We quantify the memory strength of LLMs by measuring the\ndivergence in LLMs' responses to different paraphrases of the same question,\nwhich is not considered by previous works. We also generate evidence in various\nstyles to examine LLMs' behavior. Our results show that for questions with high\nmemory strength, LLMs are more likely to rely on internal memory. Furthermore,\npresenting paraphrased evidence significantly increases LLMs' receptiveness\ncompared to simple repetition or adding details. These findings provide key\ninsights for improving retrieval-augmented generation and context-aware LLMs.\nOur code is available at https://github.com/liyp0095/ContextFaithful.\n", "link": "http://arxiv.org/abs/2409.10955v2", "date": "2025-07-10", "relevancy": 2.0856, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Context-Faithfulness%20in%20Large%20Language%20Models%3A%20The%20Roles%0A%20%20of%20Memory%20Strength%20and%20Evidence%20Style&body=Title%3A%20Investigating%20Context-Faithfulness%20in%20Large%20Language%20Models%3A%20The%20Roles%0A%20%20of%20Memory%20Strength%20and%20Evidence%20Style%0AAuthor%3A%20Yuepei%20Li%20and%20Kang%20Zhou%20and%20Qiao%20Qiao%20and%20Bach%20Nguyen%20and%20Qing%20Wang%20and%20Qi%20Li%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20improves%20Large%20Language%20Models%20%28LLMs%29%20by%0Aincorporating%20external%20information%20into%20the%20response%20generation%20process.%0AHowever%2C%20how%20context-faithful%20LLMs%20are%20and%20what%20factors%20influence%20LLMs%27%20context%0Afaithfulness%20remain%20largely%20unexplored.%20In%20this%20study%2C%20we%20investigate%20the%0Aimpact%20of%20memory%20strength%20and%20evidence%20presentation%20on%20LLMs%27%20receptiveness%20to%0Aexternal%20evidence.%20We%20quantify%20the%20memory%20strength%20of%20LLMs%20by%20measuring%20the%0Adivergence%20in%20LLMs%27%20responses%20to%20different%20paraphrases%20of%20the%20same%20question%2C%0Awhich%20is%20not%20considered%20by%20previous%20works.%20We%20also%20generate%20evidence%20in%20various%0Astyles%20to%20examine%20LLMs%27%20behavior.%20Our%20results%20show%20that%20for%20questions%20with%20high%0Amemory%20strength%2C%20LLMs%20are%20more%20likely%20to%20rely%20on%20internal%20memory.%20Furthermore%2C%0Apresenting%20paraphrased%20evidence%20significantly%20increases%20LLMs%27%20receptiveness%0Acompared%20to%20simple%20repetition%20or%20adding%20details.%20These%20findings%20provide%20key%0Ainsights%20for%20improving%20retrieval-augmented%20generation%20and%20context-aware%20LLMs.%0AOur%20code%20is%20available%20at%20https%3A//github.com/liyp0095/ContextFaithful.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10955v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Context-Faithfulness%2520in%2520Large%2520Language%2520Models%253A%2520The%2520Roles%250A%2520%2520of%2520Memory%2520Strength%2520and%2520Evidence%2520Style%26entry.906535625%3DYuepei%2520Li%2520and%2520Kang%2520Zhou%2520and%2520Qiao%2520Qiao%2520and%2520Bach%2520Nguyen%2520and%2520Qing%2520Wang%2520and%2520Qi%2520Li%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520improves%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520by%250Aincorporating%2520external%2520information%2520into%2520the%2520response%2520generation%2520process.%250AHowever%252C%2520how%2520context-faithful%2520LLMs%2520are%2520and%2520what%2520factors%2520influence%2520LLMs%2527%2520context%250Afaithfulness%2520remain%2520largely%2520unexplored.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%250Aimpact%2520of%2520memory%2520strength%2520and%2520evidence%2520presentation%2520on%2520LLMs%2527%2520receptiveness%2520to%250Aexternal%2520evidence.%2520We%2520quantify%2520the%2520memory%2520strength%2520of%2520LLMs%2520by%2520measuring%2520the%250Adivergence%2520in%2520LLMs%2527%2520responses%2520to%2520different%2520paraphrases%2520of%2520the%2520same%2520question%252C%250Awhich%2520is%2520not%2520considered%2520by%2520previous%2520works.%2520We%2520also%2520generate%2520evidence%2520in%2520various%250Astyles%2520to%2520examine%2520LLMs%2527%2520behavior.%2520Our%2520results%2520show%2520that%2520for%2520questions%2520with%2520high%250Amemory%2520strength%252C%2520LLMs%2520are%2520more%2520likely%2520to%2520rely%2520on%2520internal%2520memory.%2520Furthermore%252C%250Apresenting%2520paraphrased%2520evidence%2520significantly%2520increases%2520LLMs%2527%2520receptiveness%250Acompared%2520to%2520simple%2520repetition%2520or%2520adding%2520details.%2520These%2520findings%2520provide%2520key%250Ainsights%2520for%2520improving%2520retrieval-augmented%2520generation%2520and%2520context-aware%2520LLMs.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/liyp0095/ContextFaithful.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10955v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Context-Faithfulness%20in%20Large%20Language%20Models%3A%20The%20Roles%0A%20%20of%20Memory%20Strength%20and%20Evidence%20Style&entry.906535625=Yuepei%20Li%20and%20Kang%20Zhou%20and%20Qiao%20Qiao%20and%20Bach%20Nguyen%20and%20Qing%20Wang%20and%20Qi%20Li&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20improves%20Large%20Language%20Models%20%28LLMs%29%20by%0Aincorporating%20external%20information%20into%20the%20response%20generation%20process.%0AHowever%2C%20how%20context-faithful%20LLMs%20are%20and%20what%20factors%20influence%20LLMs%27%20context%0Afaithfulness%20remain%20largely%20unexplored.%20In%20this%20study%2C%20we%20investigate%20the%0Aimpact%20of%20memory%20strength%20and%20evidence%20presentation%20on%20LLMs%27%20receptiveness%20to%0Aexternal%20evidence.%20We%20quantify%20the%20memory%20strength%20of%20LLMs%20by%20measuring%20the%0Adivergence%20in%20LLMs%27%20responses%20to%20different%20paraphrases%20of%20the%20same%20question%2C%0Awhich%20is%20not%20considered%20by%20previous%20works.%20We%20also%20generate%20evidence%20in%20various%0Astyles%20to%20examine%20LLMs%27%20behavior.%20Our%20results%20show%20that%20for%20questions%20with%20high%0Amemory%20strength%2C%20LLMs%20are%20more%20likely%20to%20rely%20on%20internal%20memory.%20Furthermore%2C%0Apresenting%20paraphrased%20evidence%20significantly%20increases%20LLMs%27%20receptiveness%0Acompared%20to%20simple%20repetition%20or%20adding%20details.%20These%20findings%20provide%20key%0Ainsights%20for%20improving%20retrieval-augmented%20generation%20and%20context-aware%20LLMs.%0AOur%20code%20is%20available%20at%20https%3A//github.com/liyp0095/ContextFaithful.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10955v2&entry.124074799=Read"},
{"title": "Prospective Learning in Retrospect", "author": "Yuxin Bai and Cecelia Shuai and Ashwin De Silva and Siyu Yu and Pratik Chaudhari and Joshua T. Vogelstein", "abstract": "  In most real-world applications of artificial intelligence, the distributions\nof the data and the goals of the learners tend to change over time. The\nProbably Approximately Correct (PAC) learning framework, which underpins most\nmachine learning algorithms, fails to account for dynamic data distributions\nand evolving objectives, often resulting in suboptimal performance. Prospective\nlearning is a recently introduced mathematical framework that overcomes some of\nthese limitations. We build on this framework to present preliminary results\nthat improve the algorithm and numerical results, and extend prospective\nlearning to sequential decision-making scenarios, specifically foraging. Code\nis available at: https://github.com/neurodata/prolearn2.\n", "link": "http://arxiv.org/abs/2507.07965v1", "date": "2025-07-10", "relevancy": 2.0225, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.53}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5063}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prospective%20Learning%20in%20Retrospect&body=Title%3A%20Prospective%20Learning%20in%20Retrospect%0AAuthor%3A%20Yuxin%20Bai%20and%20Cecelia%20Shuai%20and%20Ashwin%20De%20Silva%20and%20Siyu%20Yu%20and%20Pratik%20Chaudhari%20and%20Joshua%20T.%20Vogelstein%0AAbstract%3A%20%20%20In%20most%20real-world%20applications%20of%20artificial%20intelligence%2C%20the%20distributions%0Aof%20the%20data%20and%20the%20goals%20of%20the%20learners%20tend%20to%20change%20over%20time.%20The%0AProbably%20Approximately%20Correct%20%28PAC%29%20learning%20framework%2C%20which%20underpins%20most%0Amachine%20learning%20algorithms%2C%20fails%20to%20account%20for%20dynamic%20data%20distributions%0Aand%20evolving%20objectives%2C%20often%20resulting%20in%20suboptimal%20performance.%20Prospective%0Alearning%20is%20a%20recently%20introduced%20mathematical%20framework%20that%20overcomes%20some%20of%0Athese%20limitations.%20We%20build%20on%20this%20framework%20to%20present%20preliminary%20results%0Athat%20improve%20the%20algorithm%20and%20numerical%20results%2C%20and%20extend%20prospective%0Alearning%20to%20sequential%20decision-making%20scenarios%2C%20specifically%20foraging.%20Code%0Ais%20available%20at%3A%20https%3A//github.com/neurodata/prolearn2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProspective%2520Learning%2520in%2520Retrospect%26entry.906535625%3DYuxin%2520Bai%2520and%2520Cecelia%2520Shuai%2520and%2520Ashwin%2520De%2520Silva%2520and%2520Siyu%2520Yu%2520and%2520Pratik%2520Chaudhari%2520and%2520Joshua%2520T.%2520Vogelstein%26entry.1292438233%3D%2520%2520In%2520most%2520real-world%2520applications%2520of%2520artificial%2520intelligence%252C%2520the%2520distributions%250Aof%2520the%2520data%2520and%2520the%2520goals%2520of%2520the%2520learners%2520tend%2520to%2520change%2520over%2520time.%2520The%250AProbably%2520Approximately%2520Correct%2520%2528PAC%2529%2520learning%2520framework%252C%2520which%2520underpins%2520most%250Amachine%2520learning%2520algorithms%252C%2520fails%2520to%2520account%2520for%2520dynamic%2520data%2520distributions%250Aand%2520evolving%2520objectives%252C%2520often%2520resulting%2520in%2520suboptimal%2520performance.%2520Prospective%250Alearning%2520is%2520a%2520recently%2520introduced%2520mathematical%2520framework%2520that%2520overcomes%2520some%2520of%250Athese%2520limitations.%2520We%2520build%2520on%2520this%2520framework%2520to%2520present%2520preliminary%2520results%250Athat%2520improve%2520the%2520algorithm%2520and%2520numerical%2520results%252C%2520and%2520extend%2520prospective%250Alearning%2520to%2520sequential%2520decision-making%2520scenarios%252C%2520specifically%2520foraging.%2520Code%250Ais%2520available%2520at%253A%2520https%253A//github.com/neurodata/prolearn2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prospective%20Learning%20in%20Retrospect&entry.906535625=Yuxin%20Bai%20and%20Cecelia%20Shuai%20and%20Ashwin%20De%20Silva%20and%20Siyu%20Yu%20and%20Pratik%20Chaudhari%20and%20Joshua%20T.%20Vogelstein&entry.1292438233=%20%20In%20most%20real-world%20applications%20of%20artificial%20intelligence%2C%20the%20distributions%0Aof%20the%20data%20and%20the%20goals%20of%20the%20learners%20tend%20to%20change%20over%20time.%20The%0AProbably%20Approximately%20Correct%20%28PAC%29%20learning%20framework%2C%20which%20underpins%20most%0Amachine%20learning%20algorithms%2C%20fails%20to%20account%20for%20dynamic%20data%20distributions%0Aand%20evolving%20objectives%2C%20often%20resulting%20in%20suboptimal%20performance.%20Prospective%0Alearning%20is%20a%20recently%20introduced%20mathematical%20framework%20that%20overcomes%20some%20of%0Athese%20limitations.%20We%20build%20on%20this%20framework%20to%20present%20preliminary%20results%0Athat%20improve%20the%20algorithm%20and%20numerical%20results%2C%20and%20extend%20prospective%0Alearning%20to%20sequential%20decision-making%20scenarios%2C%20specifically%20foraging.%20Code%0Ais%20available%20at%3A%20https%3A//github.com/neurodata/prolearn2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07965v1&entry.124074799=Read"},
{"title": "Beyond Cox Models: Assessing the Performance of Machine-Learning Methods\n  in Non-Proportional Hazards and Non-Linear Survival Analysis", "author": "Ivan Rossi and Flavio Sartori and Cesare Rollo and Giovanni Birolo and Piero Fariselli and Tiziana Sanavia", "abstract": "  Survival analysis often relies on Cox models, assuming both linearity and\nproportional hazards (PH). This study evaluates machine and deep learning\nmethods that relax these constraints, comparing their performance with\npenalized Cox models on a benchmark of three synthetic and three real datasets.\nIn total, eight different models were tested, including six non-linear models\nof which four were also non-PH. Although Cox regression often yielded\nsatisfactory performance, we showed the conditions under which machine and deep\nlearning models can perform better. Indeed, the performance of these methods\nhas often been underestimated due to the improper use of Harrell's concordance\nindex (C-index) instead of more appropriate scores such as Antolini's\nconcordance index, which generalizes C-index in cases where the PH assumption\ndoes not hold. In addition, since occasionally high C-index models happen to be\nbadly calibrated, combining Antolini's C-index with Brier's score is useful to\nassess the overall performance of a survival method. Results on our benchmark\ndata showed that survival prediction should be approached by testing different\nmethods to select the most appropriate one according to sample size,\nnon-linearity and non-PH conditions. To allow an easy reproducibility of these\ntests on our benchmark data, code and documentation are freely available at\nhttps://github.com/compbiomed-unito/survhive.\n", "link": "http://arxiv.org/abs/2504.17568v2", "date": "2025-07-10", "relevancy": 1.292, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4499}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4306}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Cox%20Models%3A%20Assessing%20the%20Performance%20of%20Machine-Learning%20Methods%0A%20%20in%20Non-Proportional%20Hazards%20and%20Non-Linear%20Survival%20Analysis&body=Title%3A%20Beyond%20Cox%20Models%3A%20Assessing%20the%20Performance%20of%20Machine-Learning%20Methods%0A%20%20in%20Non-Proportional%20Hazards%20and%20Non-Linear%20Survival%20Analysis%0AAuthor%3A%20Ivan%20Rossi%20and%20Flavio%20Sartori%20and%20Cesare%20Rollo%20and%20Giovanni%20Birolo%20and%20Piero%20Fariselli%20and%20Tiziana%20Sanavia%0AAbstract%3A%20%20%20Survival%20analysis%20often%20relies%20on%20Cox%20models%2C%20assuming%20both%20linearity%20and%0Aproportional%20hazards%20%28PH%29.%20This%20study%20evaluates%20machine%20and%20deep%20learning%0Amethods%20that%20relax%20these%20constraints%2C%20comparing%20their%20performance%20with%0Apenalized%20Cox%20models%20on%20a%20benchmark%20of%20three%20synthetic%20and%20three%20real%20datasets.%0AIn%20total%2C%20eight%20different%20models%20were%20tested%2C%20including%20six%20non-linear%20models%0Aof%20which%20four%20were%20also%20non-PH.%20Although%20Cox%20regression%20often%20yielded%0Asatisfactory%20performance%2C%20we%20showed%20the%20conditions%20under%20which%20machine%20and%20deep%0Alearning%20models%20can%20perform%20better.%20Indeed%2C%20the%20performance%20of%20these%20methods%0Ahas%20often%20been%20underestimated%20due%20to%20the%20improper%20use%20of%20Harrell%27s%20concordance%0Aindex%20%28C-index%29%20instead%20of%20more%20appropriate%20scores%20such%20as%20Antolini%27s%0Aconcordance%20index%2C%20which%20generalizes%20C-index%20in%20cases%20where%20the%20PH%20assumption%0Adoes%20not%20hold.%20In%20addition%2C%20since%20occasionally%20high%20C-index%20models%20happen%20to%20be%0Abadly%20calibrated%2C%20combining%20Antolini%27s%20C-index%20with%20Brier%27s%20score%20is%20useful%20to%0Aassess%20the%20overall%20performance%20of%20a%20survival%20method.%20Results%20on%20our%20benchmark%0Adata%20showed%20that%20survival%20prediction%20should%20be%20approached%20by%20testing%20different%0Amethods%20to%20select%20the%20most%20appropriate%20one%20according%20to%20sample%20size%2C%0Anon-linearity%20and%20non-PH%20conditions.%20To%20allow%20an%20easy%20reproducibility%20of%20these%0Atests%20on%20our%20benchmark%20data%2C%20code%20and%20documentation%20are%20freely%20available%20at%0Ahttps%3A//github.com/compbiomed-unito/survhive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Cox%2520Models%253A%2520Assessing%2520the%2520Performance%2520of%2520Machine-Learning%2520Methods%250A%2520%2520in%2520Non-Proportional%2520Hazards%2520and%2520Non-Linear%2520Survival%2520Analysis%26entry.906535625%3DIvan%2520Rossi%2520and%2520Flavio%2520Sartori%2520and%2520Cesare%2520Rollo%2520and%2520Giovanni%2520Birolo%2520and%2520Piero%2520Fariselli%2520and%2520Tiziana%2520Sanavia%26entry.1292438233%3D%2520%2520Survival%2520analysis%2520often%2520relies%2520on%2520Cox%2520models%252C%2520assuming%2520both%2520linearity%2520and%250Aproportional%2520hazards%2520%2528PH%2529.%2520This%2520study%2520evaluates%2520machine%2520and%2520deep%2520learning%250Amethods%2520that%2520relax%2520these%2520constraints%252C%2520comparing%2520their%2520performance%2520with%250Apenalized%2520Cox%2520models%2520on%2520a%2520benchmark%2520of%2520three%2520synthetic%2520and%2520three%2520real%2520datasets.%250AIn%2520total%252C%2520eight%2520different%2520models%2520were%2520tested%252C%2520including%2520six%2520non-linear%2520models%250Aof%2520which%2520four%2520were%2520also%2520non-PH.%2520Although%2520Cox%2520regression%2520often%2520yielded%250Asatisfactory%2520performance%252C%2520we%2520showed%2520the%2520conditions%2520under%2520which%2520machine%2520and%2520deep%250Alearning%2520models%2520can%2520perform%2520better.%2520Indeed%252C%2520the%2520performance%2520of%2520these%2520methods%250Ahas%2520often%2520been%2520underestimated%2520due%2520to%2520the%2520improper%2520use%2520of%2520Harrell%2527s%2520concordance%250Aindex%2520%2528C-index%2529%2520instead%2520of%2520more%2520appropriate%2520scores%2520such%2520as%2520Antolini%2527s%250Aconcordance%2520index%252C%2520which%2520generalizes%2520C-index%2520in%2520cases%2520where%2520the%2520PH%2520assumption%250Adoes%2520not%2520hold.%2520In%2520addition%252C%2520since%2520occasionally%2520high%2520C-index%2520models%2520happen%2520to%2520be%250Abadly%2520calibrated%252C%2520combining%2520Antolini%2527s%2520C-index%2520with%2520Brier%2527s%2520score%2520is%2520useful%2520to%250Aassess%2520the%2520overall%2520performance%2520of%2520a%2520survival%2520method.%2520Results%2520on%2520our%2520benchmark%250Adata%2520showed%2520that%2520survival%2520prediction%2520should%2520be%2520approached%2520by%2520testing%2520different%250Amethods%2520to%2520select%2520the%2520most%2520appropriate%2520one%2520according%2520to%2520sample%2520size%252C%250Anon-linearity%2520and%2520non-PH%2520conditions.%2520To%2520allow%2520an%2520easy%2520reproducibility%2520of%2520these%250Atests%2520on%2520our%2520benchmark%2520data%252C%2520code%2520and%2520documentation%2520are%2520freely%2520available%2520at%250Ahttps%253A//github.com/compbiomed-unito/survhive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Cox%20Models%3A%20Assessing%20the%20Performance%20of%20Machine-Learning%20Methods%0A%20%20in%20Non-Proportional%20Hazards%20and%20Non-Linear%20Survival%20Analysis&entry.906535625=Ivan%20Rossi%20and%20Flavio%20Sartori%20and%20Cesare%20Rollo%20and%20Giovanni%20Birolo%20and%20Piero%20Fariselli%20and%20Tiziana%20Sanavia&entry.1292438233=%20%20Survival%20analysis%20often%20relies%20on%20Cox%20models%2C%20assuming%20both%20linearity%20and%0Aproportional%20hazards%20%28PH%29.%20This%20study%20evaluates%20machine%20and%20deep%20learning%0Amethods%20that%20relax%20these%20constraints%2C%20comparing%20their%20performance%20with%0Apenalized%20Cox%20models%20on%20a%20benchmark%20of%20three%20synthetic%20and%20three%20real%20datasets.%0AIn%20total%2C%20eight%20different%20models%20were%20tested%2C%20including%20six%20non-linear%20models%0Aof%20which%20four%20were%20also%20non-PH.%20Although%20Cox%20regression%20often%20yielded%0Asatisfactory%20performance%2C%20we%20showed%20the%20conditions%20under%20which%20machine%20and%20deep%0Alearning%20models%20can%20perform%20better.%20Indeed%2C%20the%20performance%20of%20these%20methods%0Ahas%20often%20been%20underestimated%20due%20to%20the%20improper%20use%20of%20Harrell%27s%20concordance%0Aindex%20%28C-index%29%20instead%20of%20more%20appropriate%20scores%20such%20as%20Antolini%27s%0Aconcordance%20index%2C%20which%20generalizes%20C-index%20in%20cases%20where%20the%20PH%20assumption%0Adoes%20not%20hold.%20In%20addition%2C%20since%20occasionally%20high%20C-index%20models%20happen%20to%20be%0Abadly%20calibrated%2C%20combining%20Antolini%27s%20C-index%20with%20Brier%27s%20score%20is%20useful%20to%0Aassess%20the%20overall%20performance%20of%20a%20survival%20method.%20Results%20on%20our%20benchmark%0Adata%20showed%20that%20survival%20prediction%20should%20be%20approached%20by%20testing%20different%0Amethods%20to%20select%20the%20most%20appropriate%20one%20according%20to%20sample%20size%2C%0Anon-linearity%20and%20non-PH%20conditions.%20To%20allow%20an%20easy%20reproducibility%20of%20these%0Atests%20on%20our%20benchmark%20data%2C%20code%20and%20documentation%20are%20freely%20available%20at%0Ahttps%3A//github.com/compbiomed-unito/survhive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17568v2&entry.124074799=Read"},
{"title": "Deep Learning is Not So Mysterious or Different", "author": "Andrew Gordon Wilson", "abstract": "  Deep neural networks are often seen as different from other model classes by\ndefying conventional notions of generalization. Popular examples of anomalous\ngeneralization behaviour include benign overfitting, double descent, and the\nsuccess of overparametrization. We argue that these phenomena are not distinct\nto neural networks, or particularly mysterious. Moreover, this generalization\nbehaviour can be intuitively understood, and rigorously characterized, using\nlong-standing generalization frameworks such as PAC-Bayes and countable\nhypothesis bounds. We present soft inductive biases as a key unifying principle\nin explaining these phenomena: rather than restricting the hypothesis space to\navoid overfitting, embrace a flexible hypothesis space, with a soft preference\nfor simpler solutions that are consistent with the data. This principle can be\nencoded in many model classes, and thus deep learning is not as mysterious or\ndifferent from other model classes as it might seem. However, we also highlight\nhow deep learning is relatively distinct in other ways, such as its ability for\nrepresentation learning, phenomena such as mode connectivity, and its relative\nuniversality.\n", "link": "http://arxiv.org/abs/2503.02113v2", "date": "2025-07-10", "relevancy": 1.5429, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5395}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20is%20Not%20So%20Mysterious%20or%20Different&body=Title%3A%20Deep%20Learning%20is%20Not%20So%20Mysterious%20or%20Different%0AAuthor%3A%20Andrew%20Gordon%20Wilson%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20often%20seen%20as%20different%20from%20other%20model%20classes%20by%0Adefying%20conventional%20notions%20of%20generalization.%20Popular%20examples%20of%20anomalous%0Ageneralization%20behaviour%20include%20benign%20overfitting%2C%20double%20descent%2C%20and%20the%0Asuccess%20of%20overparametrization.%20We%20argue%20that%20these%20phenomena%20are%20not%20distinct%0Ato%20neural%20networks%2C%20or%20particularly%20mysterious.%20Moreover%2C%20this%20generalization%0Abehaviour%20can%20be%20intuitively%20understood%2C%20and%20rigorously%20characterized%2C%20using%0Along-standing%20generalization%20frameworks%20such%20as%20PAC-Bayes%20and%20countable%0Ahypothesis%20bounds.%20We%20present%20soft%20inductive%20biases%20as%20a%20key%20unifying%20principle%0Ain%20explaining%20these%20phenomena%3A%20rather%20than%20restricting%20the%20hypothesis%20space%20to%0Aavoid%20overfitting%2C%20embrace%20a%20flexible%20hypothesis%20space%2C%20with%20a%20soft%20preference%0Afor%20simpler%20solutions%20that%20are%20consistent%20with%20the%20data.%20This%20principle%20can%20be%0Aencoded%20in%20many%20model%20classes%2C%20and%20thus%20deep%20learning%20is%20not%20as%20mysterious%20or%0Adifferent%20from%20other%20model%20classes%20as%20it%20might%20seem.%20However%2C%20we%20also%20highlight%0Ahow%20deep%20learning%20is%20relatively%20distinct%20in%20other%20ways%2C%20such%20as%20its%20ability%20for%0Arepresentation%20learning%2C%20phenomena%20such%20as%20mode%20connectivity%2C%20and%20its%20relative%0Auniversality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520is%2520Not%2520So%2520Mysterious%2520or%2520Different%26entry.906535625%3DAndrew%2520Gordon%2520Wilson%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520often%2520seen%2520as%2520different%2520from%2520other%2520model%2520classes%2520by%250Adefying%2520conventional%2520notions%2520of%2520generalization.%2520Popular%2520examples%2520of%2520anomalous%250Ageneralization%2520behaviour%2520include%2520benign%2520overfitting%252C%2520double%2520descent%252C%2520and%2520the%250Asuccess%2520of%2520overparametrization.%2520We%2520argue%2520that%2520these%2520phenomena%2520are%2520not%2520distinct%250Ato%2520neural%2520networks%252C%2520or%2520particularly%2520mysterious.%2520Moreover%252C%2520this%2520generalization%250Abehaviour%2520can%2520be%2520intuitively%2520understood%252C%2520and%2520rigorously%2520characterized%252C%2520using%250Along-standing%2520generalization%2520frameworks%2520such%2520as%2520PAC-Bayes%2520and%2520countable%250Ahypothesis%2520bounds.%2520We%2520present%2520soft%2520inductive%2520biases%2520as%2520a%2520key%2520unifying%2520principle%250Ain%2520explaining%2520these%2520phenomena%253A%2520rather%2520than%2520restricting%2520the%2520hypothesis%2520space%2520to%250Aavoid%2520overfitting%252C%2520embrace%2520a%2520flexible%2520hypothesis%2520space%252C%2520with%2520a%2520soft%2520preference%250Afor%2520simpler%2520solutions%2520that%2520are%2520consistent%2520with%2520the%2520data.%2520This%2520principle%2520can%2520be%250Aencoded%2520in%2520many%2520model%2520classes%252C%2520and%2520thus%2520deep%2520learning%2520is%2520not%2520as%2520mysterious%2520or%250Adifferent%2520from%2520other%2520model%2520classes%2520as%2520it%2520might%2520seem.%2520However%252C%2520we%2520also%2520highlight%250Ahow%2520deep%2520learning%2520is%2520relatively%2520distinct%2520in%2520other%2520ways%252C%2520such%2520as%2520its%2520ability%2520for%250Arepresentation%2520learning%252C%2520phenomena%2520such%2520as%2520mode%2520connectivity%252C%2520and%2520its%2520relative%250Auniversality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20is%20Not%20So%20Mysterious%20or%20Different&entry.906535625=Andrew%20Gordon%20Wilson&entry.1292438233=%20%20Deep%20neural%20networks%20are%20often%20seen%20as%20different%20from%20other%20model%20classes%20by%0Adefying%20conventional%20notions%20of%20generalization.%20Popular%20examples%20of%20anomalous%0Ageneralization%20behaviour%20include%20benign%20overfitting%2C%20double%20descent%2C%20and%20the%0Asuccess%20of%20overparametrization.%20We%20argue%20that%20these%20phenomena%20are%20not%20distinct%0Ato%20neural%20networks%2C%20or%20particularly%20mysterious.%20Moreover%2C%20this%20generalization%0Abehaviour%20can%20be%20intuitively%20understood%2C%20and%20rigorously%20characterized%2C%20using%0Along-standing%20generalization%20frameworks%20such%20as%20PAC-Bayes%20and%20countable%0Ahypothesis%20bounds.%20We%20present%20soft%20inductive%20biases%20as%20a%20key%20unifying%20principle%0Ain%20explaining%20these%20phenomena%3A%20rather%20than%20restricting%20the%20hypothesis%20space%20to%0Aavoid%20overfitting%2C%20embrace%20a%20flexible%20hypothesis%20space%2C%20with%20a%20soft%20preference%0Afor%20simpler%20solutions%20that%20are%20consistent%20with%20the%20data.%20This%20principle%20can%20be%0Aencoded%20in%20many%20model%20classes%2C%20and%20thus%20deep%20learning%20is%20not%20as%20mysterious%20or%0Adifferent%20from%20other%20model%20classes%20as%20it%20might%20seem.%20However%2C%20we%20also%20highlight%0Ahow%20deep%20learning%20is%20relatively%20distinct%20in%20other%20ways%2C%20such%20as%20its%20ability%20for%0Arepresentation%20learning%2C%20phenomena%20such%20as%20mode%20connectivity%2C%20and%20its%20relative%0Auniversality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02113v2&entry.124074799=Read"},
{"title": "Uncovering RL Integration in SSL Loss: Objective-Specific Implications\n  for Data-Efficient RL", "author": "\u00d6mer Veysel \u00c7a\u011fatan and Bar\u0131\u015f Akg\u00fcn", "abstract": "  In this study, we investigate the effect of SSL objective modifications\nwithin the SPR framework, focusing on specific adjustments such as terminal\nstate masking and prioritized replay weighting, which were not explicitly\naddressed in the original design. While these modifications are specific to RL,\nthey are not universally applicable across all RL algorithms. Therefore, we aim\nto assess their impact on performance and explore other SSL objectives that do\nnot accommodate these adjustments like Barlow Twins and VICReg. We evaluate six\nSPR variants on the Atari 100k benchmark, including versions both with and\nwithout these modifications. Additionally, we test the performance of these\nobjectives on the DeepMind Control Suite, where such modifications are absent.\nOur findings reveal that incorporating specific SSL modifications within SPR\nsignificantly enhances performance, and this influence extends to subsequent\nframeworks like SR-SPR and BBF, highlighting the critical importance of SSL\nobjective selection and related adaptations in achieving data efficiency in\nself-predictive reinforcement learning.\n", "link": "http://arxiv.org/abs/2410.17428v3", "date": "2025-07-10", "relevancy": 1.8087, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20RL%20Integration%20in%20SSL%20Loss%3A%20Objective-Specific%20Implications%0A%20%20for%20Data-Efficient%20RL&body=Title%3A%20Uncovering%20RL%20Integration%20in%20SSL%20Loss%3A%20Objective-Specific%20Implications%0A%20%20for%20Data-Efficient%20RL%0AAuthor%3A%20%C3%96mer%20Veysel%20%C3%87a%C4%9Fatan%20and%20Bar%C4%B1%C5%9F%20Akg%C3%BCn%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20investigate%20the%20effect%20of%20SSL%20objective%20modifications%0Awithin%20the%20SPR%20framework%2C%20focusing%20on%20specific%20adjustments%20such%20as%20terminal%0Astate%20masking%20and%20prioritized%20replay%20weighting%2C%20which%20were%20not%20explicitly%0Aaddressed%20in%20the%20original%20design.%20While%20these%20modifications%20are%20specific%20to%20RL%2C%0Athey%20are%20not%20universally%20applicable%20across%20all%20RL%20algorithms.%20Therefore%2C%20we%20aim%0Ato%20assess%20their%20impact%20on%20performance%20and%20explore%20other%20SSL%20objectives%20that%20do%0Anot%20accommodate%20these%20adjustments%20like%20Barlow%20Twins%20and%20VICReg.%20We%20evaluate%20six%0ASPR%20variants%20on%20the%20Atari%20100k%20benchmark%2C%20including%20versions%20both%20with%20and%0Awithout%20these%20modifications.%20Additionally%2C%20we%20test%20the%20performance%20of%20these%0Aobjectives%20on%20the%20DeepMind%20Control%20Suite%2C%20where%20such%20modifications%20are%20absent.%0AOur%20findings%20reveal%20that%20incorporating%20specific%20SSL%20modifications%20within%20SPR%0Asignificantly%20enhances%20performance%2C%20and%20this%20influence%20extends%20to%20subsequent%0Aframeworks%20like%20SR-SPR%20and%20BBF%2C%20highlighting%20the%20critical%20importance%20of%20SSL%0Aobjective%20selection%20and%20related%20adaptations%20in%20achieving%20data%20efficiency%20in%0Aself-predictive%20reinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17428v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520RL%2520Integration%2520in%2520SSL%2520Loss%253A%2520Objective-Specific%2520Implications%250A%2520%2520for%2520Data-Efficient%2520RL%26entry.906535625%3D%25C3%2596mer%2520Veysel%2520%25C3%2587a%25C4%259Fatan%2520and%2520Bar%25C4%25B1%25C5%259F%2520Akg%25C3%25BCn%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520effect%2520of%2520SSL%2520objective%2520modifications%250Awithin%2520the%2520SPR%2520framework%252C%2520focusing%2520on%2520specific%2520adjustments%2520such%2520as%2520terminal%250Astate%2520masking%2520and%2520prioritized%2520replay%2520weighting%252C%2520which%2520were%2520not%2520explicitly%250Aaddressed%2520in%2520the%2520original%2520design.%2520While%2520these%2520modifications%2520are%2520specific%2520to%2520RL%252C%250Athey%2520are%2520not%2520universally%2520applicable%2520across%2520all%2520RL%2520algorithms.%2520Therefore%252C%2520we%2520aim%250Ato%2520assess%2520their%2520impact%2520on%2520performance%2520and%2520explore%2520other%2520SSL%2520objectives%2520that%2520do%250Anot%2520accommodate%2520these%2520adjustments%2520like%2520Barlow%2520Twins%2520and%2520VICReg.%2520We%2520evaluate%2520six%250ASPR%2520variants%2520on%2520the%2520Atari%2520100k%2520benchmark%252C%2520including%2520versions%2520both%2520with%2520and%250Awithout%2520these%2520modifications.%2520Additionally%252C%2520we%2520test%2520the%2520performance%2520of%2520these%250Aobjectives%2520on%2520the%2520DeepMind%2520Control%2520Suite%252C%2520where%2520such%2520modifications%2520are%2520absent.%250AOur%2520findings%2520reveal%2520that%2520incorporating%2520specific%2520SSL%2520modifications%2520within%2520SPR%250Asignificantly%2520enhances%2520performance%252C%2520and%2520this%2520influence%2520extends%2520to%2520subsequent%250Aframeworks%2520like%2520SR-SPR%2520and%2520BBF%252C%2520highlighting%2520the%2520critical%2520importance%2520of%2520SSL%250Aobjective%2520selection%2520and%2520related%2520adaptations%2520in%2520achieving%2520data%2520efficiency%2520in%250Aself-predictive%2520reinforcement%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17428v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20RL%20Integration%20in%20SSL%20Loss%3A%20Objective-Specific%20Implications%0A%20%20for%20Data-Efficient%20RL&entry.906535625=%C3%96mer%20Veysel%20%C3%87a%C4%9Fatan%20and%20Bar%C4%B1%C5%9F%20Akg%C3%BCn&entry.1292438233=%20%20In%20this%20study%2C%20we%20investigate%20the%20effect%20of%20SSL%20objective%20modifications%0Awithin%20the%20SPR%20framework%2C%20focusing%20on%20specific%20adjustments%20such%20as%20terminal%0Astate%20masking%20and%20prioritized%20replay%20weighting%2C%20which%20were%20not%20explicitly%0Aaddressed%20in%20the%20original%20design.%20While%20these%20modifications%20are%20specific%20to%20RL%2C%0Athey%20are%20not%20universally%20applicable%20across%20all%20RL%20algorithms.%20Therefore%2C%20we%20aim%0Ato%20assess%20their%20impact%20on%20performance%20and%20explore%20other%20SSL%20objectives%20that%20do%0Anot%20accommodate%20these%20adjustments%20like%20Barlow%20Twins%20and%20VICReg.%20We%20evaluate%20six%0ASPR%20variants%20on%20the%20Atari%20100k%20benchmark%2C%20including%20versions%20both%20with%20and%0Awithout%20these%20modifications.%20Additionally%2C%20we%20test%20the%20performance%20of%20these%0Aobjectives%20on%20the%20DeepMind%20Control%20Suite%2C%20where%20such%20modifications%20are%20absent.%0AOur%20findings%20reveal%20that%20incorporating%20specific%20SSL%20modifications%20within%20SPR%0Asignificantly%20enhances%20performance%2C%20and%20this%20influence%20extends%20to%20subsequent%0Aframeworks%20like%20SR-SPR%20and%20BBF%2C%20highlighting%20the%20critical%20importance%20of%20SSL%0Aobjective%20selection%20and%20related%20adaptations%20in%20achieving%20data%20efficiency%20in%0Aself-predictive%20reinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17428v3&entry.124074799=Read"},
{"title": "Relative Pose Estimation for Nonholonomic Robot Formation with UWB-IO\n  Measurements", "author": "Kunrui Ze and Wei Wang and Shuoyu Yue and Guibin Sun and Kexin Liu and Jinhu L\u00fc", "abstract": "  This article studies the problem of distributed formation control for\nmultiple robots by using onboard ultra wide band (UWB) distance and inertial\nodometer (IO) measurements.\n  Although this problem has been widely studied, a fundamental limitation of\nmost works is that they require each robot's pose and sensor measurements are\nexpressed in a common reference frame.\n  However, it is inapplicable for nonholonomic robot formations due to the\npractical difficulty of aligning IO measurements of individual robot in a\ncommon frame.\n  To address this problem, firstly, a concurrent-learning based estimator is\nfirstly proposed to achieve relative localization between neighboring robots in\na local frame.\n  Different from most relative localization methods in a global frame, both\nrelative position and orientation in a local frame are estimated with only UWB\nranging and IO\n  measurements.\n  Secondly, to deal with information loss caused by directed communication\ntopology, a cooperative localization algorithm is introduced to estimate the\nrelative pose to the leader robot.\n  Thirdly, based on the theoretical results on relative pose estimation, a\ndistributed formation tracking controller is proposed for nonholonomic robots.\n  Both 3D and 2D real-world experiments conducted on aerial robots and grounded\nrobots are provided to demonstrate the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2411.05481v2", "date": "2025-07-10", "relevancy": 2.0757, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5318}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5282}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Pose%20Estimation%20for%20Nonholonomic%20Robot%20Formation%20with%20UWB-IO%0A%20%20Measurements&body=Title%3A%20Relative%20Pose%20Estimation%20for%20Nonholonomic%20Robot%20Formation%20with%20UWB-IO%0A%20%20Measurements%0AAuthor%3A%20Kunrui%20Ze%20and%20Wei%20Wang%20and%20Shuoyu%20Yue%20and%20Guibin%20Sun%20and%20Kexin%20Liu%20and%20Jinhu%20L%C3%BC%0AAbstract%3A%20%20%20This%20article%20studies%20the%20problem%20of%20distributed%20formation%20control%20for%0Amultiple%20robots%20by%20using%20onboard%20ultra%20wide%20band%20%28UWB%29%20distance%20and%20inertial%0Aodometer%20%28IO%29%20measurements.%0A%20%20Although%20this%20problem%20has%20been%20widely%20studied%2C%20a%20fundamental%20limitation%20of%0Amost%20works%20is%20that%20they%20require%20each%20robot%27s%20pose%20and%20sensor%20measurements%20are%0Aexpressed%20in%20a%20common%20reference%20frame.%0A%20%20However%2C%20it%20is%20inapplicable%20for%20nonholonomic%20robot%20formations%20due%20to%20the%0Apractical%20difficulty%20of%20aligning%20IO%20measurements%20of%20individual%20robot%20in%20a%0Acommon%20frame.%0A%20%20To%20address%20this%20problem%2C%20firstly%2C%20a%20concurrent-learning%20based%20estimator%20is%0Afirstly%20proposed%20to%20achieve%20relative%20localization%20between%20neighboring%20robots%20in%0Aa%20local%20frame.%0A%20%20Different%20from%20most%20relative%20localization%20methods%20in%20a%20global%20frame%2C%20both%0Arelative%20position%20and%20orientation%20in%20a%20local%20frame%20are%20estimated%20with%20only%20UWB%0Aranging%20and%20IO%0A%20%20measurements.%0A%20%20Secondly%2C%20to%20deal%20with%20information%20loss%20caused%20by%20directed%20communication%0Atopology%2C%20a%20cooperative%20localization%20algorithm%20is%20introduced%20to%20estimate%20the%0Arelative%20pose%20to%20the%20leader%20robot.%0A%20%20Thirdly%2C%20based%20on%20the%20theoretical%20results%20on%20relative%20pose%20estimation%2C%20a%0Adistributed%20formation%20tracking%20controller%20is%20proposed%20for%20nonholonomic%20robots.%0A%20%20Both%203D%20and%202D%20real-world%20experiments%20conducted%20on%20aerial%20robots%20and%20grounded%0Arobots%20are%20provided%20to%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05481v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Pose%2520Estimation%2520for%2520Nonholonomic%2520Robot%2520Formation%2520with%2520UWB-IO%250A%2520%2520Measurements%26entry.906535625%3DKunrui%2520Ze%2520and%2520Wei%2520Wang%2520and%2520Shuoyu%2520Yue%2520and%2520Guibin%2520Sun%2520and%2520Kexin%2520Liu%2520and%2520Jinhu%2520L%25C3%25BC%26entry.1292438233%3D%2520%2520This%2520article%2520studies%2520the%2520problem%2520of%2520distributed%2520formation%2520control%2520for%250Amultiple%2520robots%2520by%2520using%2520onboard%2520ultra%2520wide%2520band%2520%2528UWB%2529%2520distance%2520and%2520inertial%250Aodometer%2520%2528IO%2529%2520measurements.%250A%2520%2520Although%2520this%2520problem%2520has%2520been%2520widely%2520studied%252C%2520a%2520fundamental%2520limitation%2520of%250Amost%2520works%2520is%2520that%2520they%2520require%2520each%2520robot%2527s%2520pose%2520and%2520sensor%2520measurements%2520are%250Aexpressed%2520in%2520a%2520common%2520reference%2520frame.%250A%2520%2520However%252C%2520it%2520is%2520inapplicable%2520for%2520nonholonomic%2520robot%2520formations%2520due%2520to%2520the%250Apractical%2520difficulty%2520of%2520aligning%2520IO%2520measurements%2520of%2520individual%2520robot%2520in%2520a%250Acommon%2520frame.%250A%2520%2520To%2520address%2520this%2520problem%252C%2520firstly%252C%2520a%2520concurrent-learning%2520based%2520estimator%2520is%250Afirstly%2520proposed%2520to%2520achieve%2520relative%2520localization%2520between%2520neighboring%2520robots%2520in%250Aa%2520local%2520frame.%250A%2520%2520Different%2520from%2520most%2520relative%2520localization%2520methods%2520in%2520a%2520global%2520frame%252C%2520both%250Arelative%2520position%2520and%2520orientation%2520in%2520a%2520local%2520frame%2520are%2520estimated%2520with%2520only%2520UWB%250Aranging%2520and%2520IO%250A%2520%2520measurements.%250A%2520%2520Secondly%252C%2520to%2520deal%2520with%2520information%2520loss%2520caused%2520by%2520directed%2520communication%250Atopology%252C%2520a%2520cooperative%2520localization%2520algorithm%2520is%2520introduced%2520to%2520estimate%2520the%250Arelative%2520pose%2520to%2520the%2520leader%2520robot.%250A%2520%2520Thirdly%252C%2520based%2520on%2520the%2520theoretical%2520results%2520on%2520relative%2520pose%2520estimation%252C%2520a%250Adistributed%2520formation%2520tracking%2520controller%2520is%2520proposed%2520for%2520nonholonomic%2520robots.%250A%2520%2520Both%25203D%2520and%25202D%2520real-world%2520experiments%2520conducted%2520on%2520aerial%2520robots%2520and%2520grounded%250Arobots%2520are%2520provided%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05481v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Pose%20Estimation%20for%20Nonholonomic%20Robot%20Formation%20with%20UWB-IO%0A%20%20Measurements&entry.906535625=Kunrui%20Ze%20and%20Wei%20Wang%20and%20Shuoyu%20Yue%20and%20Guibin%20Sun%20and%20Kexin%20Liu%20and%20Jinhu%20L%C3%BC&entry.1292438233=%20%20This%20article%20studies%20the%20problem%20of%20distributed%20formation%20control%20for%0Amultiple%20robots%20by%20using%20onboard%20ultra%20wide%20band%20%28UWB%29%20distance%20and%20inertial%0Aodometer%20%28IO%29%20measurements.%0A%20%20Although%20this%20problem%20has%20been%20widely%20studied%2C%20a%20fundamental%20limitation%20of%0Amost%20works%20is%20that%20they%20require%20each%20robot%27s%20pose%20and%20sensor%20measurements%20are%0Aexpressed%20in%20a%20common%20reference%20frame.%0A%20%20However%2C%20it%20is%20inapplicable%20for%20nonholonomic%20robot%20formations%20due%20to%20the%0Apractical%20difficulty%20of%20aligning%20IO%20measurements%20of%20individual%20robot%20in%20a%0Acommon%20frame.%0A%20%20To%20address%20this%20problem%2C%20firstly%2C%20a%20concurrent-learning%20based%20estimator%20is%0Afirstly%20proposed%20to%20achieve%20relative%20localization%20between%20neighboring%20robots%20in%0Aa%20local%20frame.%0A%20%20Different%20from%20most%20relative%20localization%20methods%20in%20a%20global%20frame%2C%20both%0Arelative%20position%20and%20orientation%20in%20a%20local%20frame%20are%20estimated%20with%20only%20UWB%0Aranging%20and%20IO%0A%20%20measurements.%0A%20%20Secondly%2C%20to%20deal%20with%20information%20loss%20caused%20by%20directed%20communication%0Atopology%2C%20a%20cooperative%20localization%20algorithm%20is%20introduced%20to%20estimate%20the%0Arelative%20pose%20to%20the%20leader%20robot.%0A%20%20Thirdly%2C%20based%20on%20the%20theoretical%20results%20on%20relative%20pose%20estimation%2C%20a%0Adistributed%20formation%20tracking%20controller%20is%20proposed%20for%20nonholonomic%20robots.%0A%20%20Both%203D%20and%202D%20real-world%20experiments%20conducted%20on%20aerial%20robots%20and%20grounded%0Arobots%20are%20provided%20to%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05481v2&entry.124074799=Read"},
{"title": "Why is Your Language Model a Poor Implicit Reward Model?", "author": "Noam Razin and Yong Lin and Jiarui Yao and Sanjeev Arora", "abstract": "  Reward models are key to language model post-training and inference\npipelines. Conveniently, recent work showed that every language model defines\nan implicit reward model (IM-RM), without requiring any architectural changes.\nHowever, such IM-RMs tend to generalize worse, especially out-of-distribution,\ncompared to explicit reward models (EX-RMs) that apply a dedicated linear head\nover the hidden representations of a language model. The existence of a\ngeneralization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They\ncan be trained using the same data, loss function, and language model, and\ndiffer only in how the reward is computed. Towards a fundamental understanding\nof the implicit biases underlying different reward model types, we investigate\nthe root cause of this gap. Our main finding, backed by theory and experiments,\nis that IM-RMs rely more heavily on superficial token-level cues. Consequently,\nthey often generalize worse than EX-RMs under token-level distribution shifts,\nas well as in-distribution. Furthermore, we provide evidence against\nalternative hypotheses for the generalization gap. Most notably, we challenge\nthe intuitive claim that IM-RMs struggle in tasks where generation is harder\nthan verification because they can operate both as a verifier and a generator.\nTaken together, our results highlight that seemingly minor design choices can\nsubstantially impact the generalization behavior of reward models.\n", "link": "http://arxiv.org/abs/2507.07981v1", "date": "2025-07-10", "relevancy": 1.9284, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4853}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20is%20Your%20Language%20Model%20a%20Poor%20Implicit%20Reward%20Model%3F&body=Title%3A%20Why%20is%20Your%20Language%20Model%20a%20Poor%20Implicit%20Reward%20Model%3F%0AAuthor%3A%20Noam%20Razin%20and%20Yong%20Lin%20and%20Jiarui%20Yao%20and%20Sanjeev%20Arora%0AAbstract%3A%20%20%20Reward%20models%20are%20key%20to%20language%20model%20post-training%20and%20inference%0Apipelines.%20Conveniently%2C%20recent%20work%20showed%20that%20every%20language%20model%20defines%0Aan%20implicit%20reward%20model%20%28IM-RM%29%2C%20without%20requiring%20any%20architectural%20changes.%0AHowever%2C%20such%20IM-RMs%20tend%20to%20generalize%20worse%2C%20especially%20out-of-distribution%2C%0Acompared%20to%20explicit%20reward%20models%20%28EX-RMs%29%20that%20apply%20a%20dedicated%20linear%20head%0Aover%20the%20hidden%20representations%20of%20a%20language%20model.%20The%20existence%20of%20a%0Ageneralization%20gap%20is%20puzzling%2C%20as%20EX-RMs%20and%20IM-RMs%20are%20nearly%20identical.%20They%0Acan%20be%20trained%20using%20the%20same%20data%2C%20loss%20function%2C%20and%20language%20model%2C%20and%0Adiffer%20only%20in%20how%20the%20reward%20is%20computed.%20Towards%20a%20fundamental%20understanding%0Aof%20the%20implicit%20biases%20underlying%20different%20reward%20model%20types%2C%20we%20investigate%0Athe%20root%20cause%20of%20this%20gap.%20Our%20main%20finding%2C%20backed%20by%20theory%20and%20experiments%2C%0Ais%20that%20IM-RMs%20rely%20more%20heavily%20on%20superficial%20token-level%20cues.%20Consequently%2C%0Athey%20often%20generalize%20worse%20than%20EX-RMs%20under%20token-level%20distribution%20shifts%2C%0Aas%20well%20as%20in-distribution.%20Furthermore%2C%20we%20provide%20evidence%20against%0Aalternative%20hypotheses%20for%20the%20generalization%20gap.%20Most%20notably%2C%20we%20challenge%0Athe%20intuitive%20claim%20that%20IM-RMs%20struggle%20in%20tasks%20where%20generation%20is%20harder%0Athan%20verification%20because%20they%20can%20operate%20both%20as%20a%20verifier%20and%20a%20generator.%0ATaken%20together%2C%20our%20results%20highlight%20that%20seemingly%20minor%20design%20choices%20can%0Asubstantially%20impact%20the%20generalization%20behavior%20of%20reward%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520is%2520Your%2520Language%2520Model%2520a%2520Poor%2520Implicit%2520Reward%2520Model%253F%26entry.906535625%3DNoam%2520Razin%2520and%2520Yong%2520Lin%2520and%2520Jiarui%2520Yao%2520and%2520Sanjeev%2520Arora%26entry.1292438233%3D%2520%2520Reward%2520models%2520are%2520key%2520to%2520language%2520model%2520post-training%2520and%2520inference%250Apipelines.%2520Conveniently%252C%2520recent%2520work%2520showed%2520that%2520every%2520language%2520model%2520defines%250Aan%2520implicit%2520reward%2520model%2520%2528IM-RM%2529%252C%2520without%2520requiring%2520any%2520architectural%2520changes.%250AHowever%252C%2520such%2520IM-RMs%2520tend%2520to%2520generalize%2520worse%252C%2520especially%2520out-of-distribution%252C%250Acompared%2520to%2520explicit%2520reward%2520models%2520%2528EX-RMs%2529%2520that%2520apply%2520a%2520dedicated%2520linear%2520head%250Aover%2520the%2520hidden%2520representations%2520of%2520a%2520language%2520model.%2520The%2520existence%2520of%2520a%250Ageneralization%2520gap%2520is%2520puzzling%252C%2520as%2520EX-RMs%2520and%2520IM-RMs%2520are%2520nearly%2520identical.%2520They%250Acan%2520be%2520trained%2520using%2520the%2520same%2520data%252C%2520loss%2520function%252C%2520and%2520language%2520model%252C%2520and%250Adiffer%2520only%2520in%2520how%2520the%2520reward%2520is%2520computed.%2520Towards%2520a%2520fundamental%2520understanding%250Aof%2520the%2520implicit%2520biases%2520underlying%2520different%2520reward%2520model%2520types%252C%2520we%2520investigate%250Athe%2520root%2520cause%2520of%2520this%2520gap.%2520Our%2520main%2520finding%252C%2520backed%2520by%2520theory%2520and%2520experiments%252C%250Ais%2520that%2520IM-RMs%2520rely%2520more%2520heavily%2520on%2520superficial%2520token-level%2520cues.%2520Consequently%252C%250Athey%2520often%2520generalize%2520worse%2520than%2520EX-RMs%2520under%2520token-level%2520distribution%2520shifts%252C%250Aas%2520well%2520as%2520in-distribution.%2520Furthermore%252C%2520we%2520provide%2520evidence%2520against%250Aalternative%2520hypotheses%2520for%2520the%2520generalization%2520gap.%2520Most%2520notably%252C%2520we%2520challenge%250Athe%2520intuitive%2520claim%2520that%2520IM-RMs%2520struggle%2520in%2520tasks%2520where%2520generation%2520is%2520harder%250Athan%2520verification%2520because%2520they%2520can%2520operate%2520both%2520as%2520a%2520verifier%2520and%2520a%2520generator.%250ATaken%2520together%252C%2520our%2520results%2520highlight%2520that%2520seemingly%2520minor%2520design%2520choices%2520can%250Asubstantially%2520impact%2520the%2520generalization%2520behavior%2520of%2520reward%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20is%20Your%20Language%20Model%20a%20Poor%20Implicit%20Reward%20Model%3F&entry.906535625=Noam%20Razin%20and%20Yong%20Lin%20and%20Jiarui%20Yao%20and%20Sanjeev%20Arora&entry.1292438233=%20%20Reward%20models%20are%20key%20to%20language%20model%20post-training%20and%20inference%0Apipelines.%20Conveniently%2C%20recent%20work%20showed%20that%20every%20language%20model%20defines%0Aan%20implicit%20reward%20model%20%28IM-RM%29%2C%20without%20requiring%20any%20architectural%20changes.%0AHowever%2C%20such%20IM-RMs%20tend%20to%20generalize%20worse%2C%20especially%20out-of-distribution%2C%0Acompared%20to%20explicit%20reward%20models%20%28EX-RMs%29%20that%20apply%20a%20dedicated%20linear%20head%0Aover%20the%20hidden%20representations%20of%20a%20language%20model.%20The%20existence%20of%20a%0Ageneralization%20gap%20is%20puzzling%2C%20as%20EX-RMs%20and%20IM-RMs%20are%20nearly%20identical.%20They%0Acan%20be%20trained%20using%20the%20same%20data%2C%20loss%20function%2C%20and%20language%20model%2C%20and%0Adiffer%20only%20in%20how%20the%20reward%20is%20computed.%20Towards%20a%20fundamental%20understanding%0Aof%20the%20implicit%20biases%20underlying%20different%20reward%20model%20types%2C%20we%20investigate%0Athe%20root%20cause%20of%20this%20gap.%20Our%20main%20finding%2C%20backed%20by%20theory%20and%20experiments%2C%0Ais%20that%20IM-RMs%20rely%20more%20heavily%20on%20superficial%20token-level%20cues.%20Consequently%2C%0Athey%20often%20generalize%20worse%20than%20EX-RMs%20under%20token-level%20distribution%20shifts%2C%0Aas%20well%20as%20in-distribution.%20Furthermore%2C%20we%20provide%20evidence%20against%0Aalternative%20hypotheses%20for%20the%20generalization%20gap.%20Most%20notably%2C%20we%20challenge%0Athe%20intuitive%20claim%20that%20IM-RMs%20struggle%20in%20tasks%20where%20generation%20is%20harder%0Athan%20verification%20because%20they%20can%20operate%20both%20as%20a%20verifier%20and%20a%20generator.%0ATaken%20together%2C%20our%20results%20highlight%20that%20seemingly%20minor%20design%20choices%20can%0Asubstantially%20impact%20the%20generalization%20behavior%20of%20reward%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07981v1&entry.124074799=Read"},
{"title": "Some Theoretical Results on Layerwise Effective Dimension Oscillations\n  in Finite Width ReLU Networks", "author": "Darshan Makwana", "abstract": "  We analyze the layerwise effective dimension (rank of the feature matrix) in\nfully-connected ReLU networks of finite width. Specifically, for a fixed batch\nof $m$ inputs and random Gaussian weights, we derive closed-form expressions\nfor the expected rank of the \\$m\\times n\\$ hidden activation matrices. Our main\nresult shows that $\\mathbb{E}[EDim(\\ell)]=m[1-(1-2/\\pi)^\\ell]+O(e^{-c m})$ so\nthat the rank deficit decays geometrically with ratio $1-2 / \\pi \\approx\n0.3634$. We also prove a sub-Gaussian concentration bound, and identify the\n\"revival\" depths at which the expected rank attains local maxima. In\nparticular, these peaks occur at depths\n$\\ell_k^*\\approx(k+1/2)\\pi/\\log(1/\\rho)$ with height $\\approx (1-e^{-\\pi/2}) m\n\\approx 0.79m$. We further show that this oscillatory rank behavior is a\nfinite-width phenomenon: under orthogonal weight initialization or strong\nnegative-slope leaky-ReLU, the rank remains (nearly) full. These results\nprovide a precise characterization of how random ReLU layers alternately\ncollapse and partially revive the subspace of input variations, adding nuance\nto prior work on expressivity of deep networks.\n", "link": "http://arxiv.org/abs/2507.07675v1", "date": "2025-07-10", "relevancy": 1.3142, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4721}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4324}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Some%20Theoretical%20Results%20on%20Layerwise%20Effective%20Dimension%20Oscillations%0A%20%20in%20Finite%20Width%20ReLU%20Networks&body=Title%3A%20Some%20Theoretical%20Results%20on%20Layerwise%20Effective%20Dimension%20Oscillations%0A%20%20in%20Finite%20Width%20ReLU%20Networks%0AAuthor%3A%20Darshan%20Makwana%0AAbstract%3A%20%20%20We%20analyze%20the%20layerwise%20effective%20dimension%20%28rank%20of%20the%20feature%20matrix%29%20in%0Afully-connected%20ReLU%20networks%20of%20finite%20width.%20Specifically%2C%20for%20a%20fixed%20batch%0Aof%20%24m%24%20inputs%20and%20random%20Gaussian%20weights%2C%20we%20derive%20closed-form%20expressions%0Afor%20the%20expected%20rank%20of%20the%20%5C%24m%5Ctimes%20n%5C%24%20hidden%20activation%20matrices.%20Our%20main%0Aresult%20shows%20that%20%24%5Cmathbb%7BE%7D%5BEDim%28%5Cell%29%5D%3Dm%5B1-%281-2/%5Cpi%29%5E%5Cell%5D%2BO%28e%5E%7B-c%20m%7D%29%24%20so%0Athat%20the%20rank%20deficit%20decays%20geometrically%20with%20ratio%20%241-2%20/%20%5Cpi%20%5Capprox%0A0.3634%24.%20We%20also%20prove%20a%20sub-Gaussian%20concentration%20bound%2C%20and%20identify%20the%0A%22revival%22%20depths%20at%20which%20the%20expected%20rank%20attains%20local%20maxima.%20In%0Aparticular%2C%20these%20peaks%20occur%20at%20depths%0A%24%5Cell_k%5E%2A%5Capprox%28k%2B1/2%29%5Cpi/%5Clog%281/%5Crho%29%24%20with%20height%20%24%5Capprox%20%281-e%5E%7B-%5Cpi/2%7D%29%20m%0A%5Capprox%200.79m%24.%20We%20further%20show%20that%20this%20oscillatory%20rank%20behavior%20is%20a%0Afinite-width%20phenomenon%3A%20under%20orthogonal%20weight%20initialization%20or%20strong%0Anegative-slope%20leaky-ReLU%2C%20the%20rank%20remains%20%28nearly%29%20full.%20These%20results%0Aprovide%20a%20precise%20characterization%20of%20how%20random%20ReLU%20layers%20alternately%0Acollapse%20and%20partially%20revive%20the%20subspace%20of%20input%20variations%2C%20adding%20nuance%0Ato%20prior%20work%20on%20expressivity%20of%20deep%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSome%2520Theoretical%2520Results%2520on%2520Layerwise%2520Effective%2520Dimension%2520Oscillations%250A%2520%2520in%2520Finite%2520Width%2520ReLU%2520Networks%26entry.906535625%3DDarshan%2520Makwana%26entry.1292438233%3D%2520%2520We%2520analyze%2520the%2520layerwise%2520effective%2520dimension%2520%2528rank%2520of%2520the%2520feature%2520matrix%2529%2520in%250Afully-connected%2520ReLU%2520networks%2520of%2520finite%2520width.%2520Specifically%252C%2520for%2520a%2520fixed%2520batch%250Aof%2520%2524m%2524%2520inputs%2520and%2520random%2520Gaussian%2520weights%252C%2520we%2520derive%2520closed-form%2520expressions%250Afor%2520the%2520expected%2520rank%2520of%2520the%2520%255C%2524m%255Ctimes%2520n%255C%2524%2520hidden%2520activation%2520matrices.%2520Our%2520main%250Aresult%2520shows%2520that%2520%2524%255Cmathbb%257BE%257D%255BEDim%2528%255Cell%2529%255D%253Dm%255B1-%25281-2/%255Cpi%2529%255E%255Cell%255D%252BO%2528e%255E%257B-c%2520m%257D%2529%2524%2520so%250Athat%2520the%2520rank%2520deficit%2520decays%2520geometrically%2520with%2520ratio%2520%25241-2%2520/%2520%255Cpi%2520%255Capprox%250A0.3634%2524.%2520We%2520also%2520prove%2520a%2520sub-Gaussian%2520concentration%2520bound%252C%2520and%2520identify%2520the%250A%2522revival%2522%2520depths%2520at%2520which%2520the%2520expected%2520rank%2520attains%2520local%2520maxima.%2520In%250Aparticular%252C%2520these%2520peaks%2520occur%2520at%2520depths%250A%2524%255Cell_k%255E%252A%255Capprox%2528k%252B1/2%2529%255Cpi/%255Clog%25281/%255Crho%2529%2524%2520with%2520height%2520%2524%255Capprox%2520%25281-e%255E%257B-%255Cpi/2%257D%2529%2520m%250A%255Capprox%25200.79m%2524.%2520We%2520further%2520show%2520that%2520this%2520oscillatory%2520rank%2520behavior%2520is%2520a%250Afinite-width%2520phenomenon%253A%2520under%2520orthogonal%2520weight%2520initialization%2520or%2520strong%250Anegative-slope%2520leaky-ReLU%252C%2520the%2520rank%2520remains%2520%2528nearly%2529%2520full.%2520These%2520results%250Aprovide%2520a%2520precise%2520characterization%2520of%2520how%2520random%2520ReLU%2520layers%2520alternately%250Acollapse%2520and%2520partially%2520revive%2520the%2520subspace%2520of%2520input%2520variations%252C%2520adding%2520nuance%250Ato%2520prior%2520work%2520on%2520expressivity%2520of%2520deep%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Some%20Theoretical%20Results%20on%20Layerwise%20Effective%20Dimension%20Oscillations%0A%20%20in%20Finite%20Width%20ReLU%20Networks&entry.906535625=Darshan%20Makwana&entry.1292438233=%20%20We%20analyze%20the%20layerwise%20effective%20dimension%20%28rank%20of%20the%20feature%20matrix%29%20in%0Afully-connected%20ReLU%20networks%20of%20finite%20width.%20Specifically%2C%20for%20a%20fixed%20batch%0Aof%20%24m%24%20inputs%20and%20random%20Gaussian%20weights%2C%20we%20derive%20closed-form%20expressions%0Afor%20the%20expected%20rank%20of%20the%20%5C%24m%5Ctimes%20n%5C%24%20hidden%20activation%20matrices.%20Our%20main%0Aresult%20shows%20that%20%24%5Cmathbb%7BE%7D%5BEDim%28%5Cell%29%5D%3Dm%5B1-%281-2/%5Cpi%29%5E%5Cell%5D%2BO%28e%5E%7B-c%20m%7D%29%24%20so%0Athat%20the%20rank%20deficit%20decays%20geometrically%20with%20ratio%20%241-2%20/%20%5Cpi%20%5Capprox%0A0.3634%24.%20We%20also%20prove%20a%20sub-Gaussian%20concentration%20bound%2C%20and%20identify%20the%0A%22revival%22%20depths%20at%20which%20the%20expected%20rank%20attains%20local%20maxima.%20In%0Aparticular%2C%20these%20peaks%20occur%20at%20depths%0A%24%5Cell_k%5E%2A%5Capprox%28k%2B1/2%29%5Cpi/%5Clog%281/%5Crho%29%24%20with%20height%20%24%5Capprox%20%281-e%5E%7B-%5Cpi/2%7D%29%20m%0A%5Capprox%200.79m%24.%20We%20further%20show%20that%20this%20oscillatory%20rank%20behavior%20is%20a%0Afinite-width%20phenomenon%3A%20under%20orthogonal%20weight%20initialization%20or%20strong%0Anegative-slope%20leaky-ReLU%2C%20the%20rank%20remains%20%28nearly%29%20full.%20These%20results%0Aprovide%20a%20precise%20characterization%20of%20how%20random%20ReLU%20layers%20alternately%0Acollapse%20and%20partially%20revive%20the%20subspace%20of%20input%20variations%2C%20adding%20nuance%0Ato%20prior%20work%20on%20expressivity%20of%20deep%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07675v1&entry.124074799=Read"},
{"title": "SCOOTER: A Human Evaluation Framework for Unrestricted Adversarial\n  Examples", "author": "Dren Fazlija and Monty-Maximilian Z\u00fchlke and Johanna Schrader and Arkadij Orlov and Clara Stein and Iyiola E. Olatunji and Daniel Kudenko", "abstract": "  Unrestricted adversarial attacks aim to fool computer vision models without\nbeing constrained by $\\ell_p$-norm bounds to remain imperceptible to humans,\nfor example, by changing an object's color. This allows attackers to circumvent\ntraditional, norm-bounded defense strategies such as adversarial training or\ncertified defense strategies. However, due to their unrestricted nature, there\nare also no guarantees of norm-based imperceptibility, necessitating human\nevaluations to verify just how authentic these adversarial examples look. While\nsome related work assesses this vital quality of adversarial attacks, none\nprovide statistically significant insights. This issue necessitates a unified\nframework that supports and streamlines such an assessment for evaluating and\ncomparing unrestricted attacks. To close this gap, we introduce SCOOTER - an\nopen-source, statistically powered framework for evaluating unrestricted\nadversarial examples. Our contributions are: $(i)$ best-practice guidelines for\ncrowd-study power, compensation, and Likert equivalence bounds to measure\nimperceptibility; $(ii)$ the first large-scale human vs. model comparison\nacross 346 human participants showing that three color-space attacks and three\ndiffusion-based attacks fail to produce imperceptible images. Furthermore, we\nfound that GPT-4o can serve as a preliminary test for imperceptibility, but it\nonly consistently detects adversarial examples for four out of six tested\nattacks; $(iii)$ open-source software tools, including a browser-based task\ntemplate to collect annotations and analysis scripts in Python and R; $(iv)$ an\nImageNet-derived benchmark dataset containing 3K real images, 7K adversarial\nexamples, and over 34K human ratings. Our findings demonstrate that automated\nvision systems do not align with human perception, reinforcing the need for a\nground-truth SCOOTER benchmark.\n", "link": "http://arxiv.org/abs/2507.07776v1", "date": "2025-07-10", "relevancy": 2.0419, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCOOTER%3A%20A%20Human%20Evaluation%20Framework%20for%20Unrestricted%20Adversarial%0A%20%20Examples&body=Title%3A%20SCOOTER%3A%20A%20Human%20Evaluation%20Framework%20for%20Unrestricted%20Adversarial%0A%20%20Examples%0AAuthor%3A%20Dren%20Fazlija%20and%20Monty-Maximilian%20Z%C3%BChlke%20and%20Johanna%20Schrader%20and%20Arkadij%20Orlov%20and%20Clara%20Stein%20and%20Iyiola%20E.%20Olatunji%20and%20Daniel%20Kudenko%0AAbstract%3A%20%20%20Unrestricted%20adversarial%20attacks%20aim%20to%20fool%20computer%20vision%20models%20without%0Abeing%20constrained%20by%20%24%5Cell_p%24-norm%20bounds%20to%20remain%20imperceptible%20to%20humans%2C%0Afor%20example%2C%20by%20changing%20an%20object%27s%20color.%20This%20allows%20attackers%20to%20circumvent%0Atraditional%2C%20norm-bounded%20defense%20strategies%20such%20as%20adversarial%20training%20or%0Acertified%20defense%20strategies.%20However%2C%20due%20to%20their%20unrestricted%20nature%2C%20there%0Aare%20also%20no%20guarantees%20of%20norm-based%20imperceptibility%2C%20necessitating%20human%0Aevaluations%20to%20verify%20just%20how%20authentic%20these%20adversarial%20examples%20look.%20While%0Asome%20related%20work%20assesses%20this%20vital%20quality%20of%20adversarial%20attacks%2C%20none%0Aprovide%20statistically%20significant%20insights.%20This%20issue%20necessitates%20a%20unified%0Aframework%20that%20supports%20and%20streamlines%20such%20an%20assessment%20for%20evaluating%20and%0Acomparing%20unrestricted%20attacks.%20To%20close%20this%20gap%2C%20we%20introduce%20SCOOTER%20-%20an%0Aopen-source%2C%20statistically%20powered%20framework%20for%20evaluating%20unrestricted%0Aadversarial%20examples.%20Our%20contributions%20are%3A%20%24%28i%29%24%20best-practice%20guidelines%20for%0Acrowd-study%20power%2C%20compensation%2C%20and%20Likert%20equivalence%20bounds%20to%20measure%0Aimperceptibility%3B%20%24%28ii%29%24%20the%20first%20large-scale%20human%20vs.%20model%20comparison%0Aacross%20346%20human%20participants%20showing%20that%20three%20color-space%20attacks%20and%20three%0Adiffusion-based%20attacks%20fail%20to%20produce%20imperceptible%20images.%20Furthermore%2C%20we%0Afound%20that%20GPT-4o%20can%20serve%20as%20a%20preliminary%20test%20for%20imperceptibility%2C%20but%20it%0Aonly%20consistently%20detects%20adversarial%20examples%20for%20four%20out%20of%20six%20tested%0Aattacks%3B%20%24%28iii%29%24%20open-source%20software%20tools%2C%20including%20a%20browser-based%20task%0Atemplate%20to%20collect%20annotations%20and%20analysis%20scripts%20in%20Python%20and%20R%3B%20%24%28iv%29%24%20an%0AImageNet-derived%20benchmark%20dataset%20containing%203K%20real%20images%2C%207K%20adversarial%0Aexamples%2C%20and%20over%2034K%20human%20ratings.%20Our%20findings%20demonstrate%20that%20automated%0Avision%20systems%20do%20not%20align%20with%20human%20perception%2C%20reinforcing%20the%20need%20for%20a%0Aground-truth%20SCOOTER%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCOOTER%253A%2520A%2520Human%2520Evaluation%2520Framework%2520for%2520Unrestricted%2520Adversarial%250A%2520%2520Examples%26entry.906535625%3DDren%2520Fazlija%2520and%2520Monty-Maximilian%2520Z%25C3%25BChlke%2520and%2520Johanna%2520Schrader%2520and%2520Arkadij%2520Orlov%2520and%2520Clara%2520Stein%2520and%2520Iyiola%2520E.%2520Olatunji%2520and%2520Daniel%2520Kudenko%26entry.1292438233%3D%2520%2520Unrestricted%2520adversarial%2520attacks%2520aim%2520to%2520fool%2520computer%2520vision%2520models%2520without%250Abeing%2520constrained%2520by%2520%2524%255Cell_p%2524-norm%2520bounds%2520to%2520remain%2520imperceptible%2520to%2520humans%252C%250Afor%2520example%252C%2520by%2520changing%2520an%2520object%2527s%2520color.%2520This%2520allows%2520attackers%2520to%2520circumvent%250Atraditional%252C%2520norm-bounded%2520defense%2520strategies%2520such%2520as%2520adversarial%2520training%2520or%250Acertified%2520defense%2520strategies.%2520However%252C%2520due%2520to%2520their%2520unrestricted%2520nature%252C%2520there%250Aare%2520also%2520no%2520guarantees%2520of%2520norm-based%2520imperceptibility%252C%2520necessitating%2520human%250Aevaluations%2520to%2520verify%2520just%2520how%2520authentic%2520these%2520adversarial%2520examples%2520look.%2520While%250Asome%2520related%2520work%2520assesses%2520this%2520vital%2520quality%2520of%2520adversarial%2520attacks%252C%2520none%250Aprovide%2520statistically%2520significant%2520insights.%2520This%2520issue%2520necessitates%2520a%2520unified%250Aframework%2520that%2520supports%2520and%2520streamlines%2520such%2520an%2520assessment%2520for%2520evaluating%2520and%250Acomparing%2520unrestricted%2520attacks.%2520To%2520close%2520this%2520gap%252C%2520we%2520introduce%2520SCOOTER%2520-%2520an%250Aopen-source%252C%2520statistically%2520powered%2520framework%2520for%2520evaluating%2520unrestricted%250Aadversarial%2520examples.%2520Our%2520contributions%2520are%253A%2520%2524%2528i%2529%2524%2520best-practice%2520guidelines%2520for%250Acrowd-study%2520power%252C%2520compensation%252C%2520and%2520Likert%2520equivalence%2520bounds%2520to%2520measure%250Aimperceptibility%253B%2520%2524%2528ii%2529%2524%2520the%2520first%2520large-scale%2520human%2520vs.%2520model%2520comparison%250Aacross%2520346%2520human%2520participants%2520showing%2520that%2520three%2520color-space%2520attacks%2520and%2520three%250Adiffusion-based%2520attacks%2520fail%2520to%2520produce%2520imperceptible%2520images.%2520Furthermore%252C%2520we%250Afound%2520that%2520GPT-4o%2520can%2520serve%2520as%2520a%2520preliminary%2520test%2520for%2520imperceptibility%252C%2520but%2520it%250Aonly%2520consistently%2520detects%2520adversarial%2520examples%2520for%2520four%2520out%2520of%2520six%2520tested%250Aattacks%253B%2520%2524%2528iii%2529%2524%2520open-source%2520software%2520tools%252C%2520including%2520a%2520browser-based%2520task%250Atemplate%2520to%2520collect%2520annotations%2520and%2520analysis%2520scripts%2520in%2520Python%2520and%2520R%253B%2520%2524%2528iv%2529%2524%2520an%250AImageNet-derived%2520benchmark%2520dataset%2520containing%25203K%2520real%2520images%252C%25207K%2520adversarial%250Aexamples%252C%2520and%2520over%252034K%2520human%2520ratings.%2520Our%2520findings%2520demonstrate%2520that%2520automated%250Avision%2520systems%2520do%2520not%2520align%2520with%2520human%2520perception%252C%2520reinforcing%2520the%2520need%2520for%2520a%250Aground-truth%2520SCOOTER%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCOOTER%3A%20A%20Human%20Evaluation%20Framework%20for%20Unrestricted%20Adversarial%0A%20%20Examples&entry.906535625=Dren%20Fazlija%20and%20Monty-Maximilian%20Z%C3%BChlke%20and%20Johanna%20Schrader%20and%20Arkadij%20Orlov%20and%20Clara%20Stein%20and%20Iyiola%20E.%20Olatunji%20and%20Daniel%20Kudenko&entry.1292438233=%20%20Unrestricted%20adversarial%20attacks%20aim%20to%20fool%20computer%20vision%20models%20without%0Abeing%20constrained%20by%20%24%5Cell_p%24-norm%20bounds%20to%20remain%20imperceptible%20to%20humans%2C%0Afor%20example%2C%20by%20changing%20an%20object%27s%20color.%20This%20allows%20attackers%20to%20circumvent%0Atraditional%2C%20norm-bounded%20defense%20strategies%20such%20as%20adversarial%20training%20or%0Acertified%20defense%20strategies.%20However%2C%20due%20to%20their%20unrestricted%20nature%2C%20there%0Aare%20also%20no%20guarantees%20of%20norm-based%20imperceptibility%2C%20necessitating%20human%0Aevaluations%20to%20verify%20just%20how%20authentic%20these%20adversarial%20examples%20look.%20While%0Asome%20related%20work%20assesses%20this%20vital%20quality%20of%20adversarial%20attacks%2C%20none%0Aprovide%20statistically%20significant%20insights.%20This%20issue%20necessitates%20a%20unified%0Aframework%20that%20supports%20and%20streamlines%20such%20an%20assessment%20for%20evaluating%20and%0Acomparing%20unrestricted%20attacks.%20To%20close%20this%20gap%2C%20we%20introduce%20SCOOTER%20-%20an%0Aopen-source%2C%20statistically%20powered%20framework%20for%20evaluating%20unrestricted%0Aadversarial%20examples.%20Our%20contributions%20are%3A%20%24%28i%29%24%20best-practice%20guidelines%20for%0Acrowd-study%20power%2C%20compensation%2C%20and%20Likert%20equivalence%20bounds%20to%20measure%0Aimperceptibility%3B%20%24%28ii%29%24%20the%20first%20large-scale%20human%20vs.%20model%20comparison%0Aacross%20346%20human%20participants%20showing%20that%20three%20color-space%20attacks%20and%20three%0Adiffusion-based%20attacks%20fail%20to%20produce%20imperceptible%20images.%20Furthermore%2C%20we%0Afound%20that%20GPT-4o%20can%20serve%20as%20a%20preliminary%20test%20for%20imperceptibility%2C%20but%20it%0Aonly%20consistently%20detects%20adversarial%20examples%20for%20four%20out%20of%20six%20tested%0Aattacks%3B%20%24%28iii%29%24%20open-source%20software%20tools%2C%20including%20a%20browser-based%20task%0Atemplate%20to%20collect%20annotations%20and%20analysis%20scripts%20in%20Python%20and%20R%3B%20%24%28iv%29%24%20an%0AImageNet-derived%20benchmark%20dataset%20containing%203K%20real%20images%2C%207K%20adversarial%0Aexamples%2C%20and%20over%2034K%20human%20ratings.%20Our%20findings%20demonstrate%20that%20automated%0Avision%20systems%20do%20not%20align%20with%20human%20perception%2C%20reinforcing%20the%20need%20for%20a%0Aground-truth%20SCOOTER%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07776v1&entry.124074799=Read"},
{"title": "Discrete Optimal Transport and Voice Conversion", "author": "Anton Selitskiy and Maitreya Kocharekar", "abstract": "  In this work, we address the voice conversion (VC) task using a vector-based\ninterface. To align audio embeddings between speakers, we employ discrete\noptimal transport mapping. Our evaluation results demonstrate the high quality\nand effectiveness of this method. Additionally, we show that applying discrete\noptimal transport as a post-processing step in audio generation can lead to the\nincorrect classification of synthetic audio as real.\n", "link": "http://arxiv.org/abs/2505.04382v2", "date": "2025-07-10", "relevancy": 2.0846, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5285}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.517}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20Optimal%20Transport%20and%20Voice%20Conversion&body=Title%3A%20Discrete%20Optimal%20Transport%20and%20Voice%20Conversion%0AAuthor%3A%20Anton%20Selitskiy%20and%20Maitreya%20Kocharekar%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20the%20voice%20conversion%20%28VC%29%20task%20using%20a%20vector-based%0Ainterface.%20To%20align%20audio%20embeddings%20between%20speakers%2C%20we%20employ%20discrete%0Aoptimal%20transport%20mapping.%20Our%20evaluation%20results%20demonstrate%20the%20high%20quality%0Aand%20effectiveness%20of%20this%20method.%20Additionally%2C%20we%20show%20that%20applying%20discrete%0Aoptimal%20transport%20as%20a%20post-processing%20step%20in%20audio%20generation%20can%20lead%20to%20the%0Aincorrect%20classification%20of%20synthetic%20audio%20as%20real.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04382v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520Optimal%2520Transport%2520and%2520Voice%2520Conversion%26entry.906535625%3DAnton%2520Selitskiy%2520and%2520Maitreya%2520Kocharekar%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520voice%2520conversion%2520%2528VC%2529%2520task%2520using%2520a%2520vector-based%250Ainterface.%2520To%2520align%2520audio%2520embeddings%2520between%2520speakers%252C%2520we%2520employ%2520discrete%250Aoptimal%2520transport%2520mapping.%2520Our%2520evaluation%2520results%2520demonstrate%2520the%2520high%2520quality%250Aand%2520effectiveness%2520of%2520this%2520method.%2520Additionally%252C%2520we%2520show%2520that%2520applying%2520discrete%250Aoptimal%2520transport%2520as%2520a%2520post-processing%2520step%2520in%2520audio%2520generation%2520can%2520lead%2520to%2520the%250Aincorrect%2520classification%2520of%2520synthetic%2520audio%2520as%2520real.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04382v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20Optimal%20Transport%20and%20Voice%20Conversion&entry.906535625=Anton%20Selitskiy%20and%20Maitreya%20Kocharekar&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20the%20voice%20conversion%20%28VC%29%20task%20using%20a%20vector-based%0Ainterface.%20To%20align%20audio%20embeddings%20between%20speakers%2C%20we%20employ%20discrete%0Aoptimal%20transport%20mapping.%20Our%20evaluation%20results%20demonstrate%20the%20high%20quality%0Aand%20effectiveness%20of%20this%20method.%20Additionally%2C%20we%20show%20that%20applying%20discrete%0Aoptimal%20transport%20as%20a%20post-processing%20step%20in%20audio%20generation%20can%20lead%20to%20the%0Aincorrect%20classification%20of%20synthetic%20audio%20as%20real.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04382v2&entry.124074799=Read"},
{"title": "Collective Bayesian Decision-Making in a Swarm of Miniaturized Robots\n  for Surface Inspection", "author": "Thiemen Siemensma and Darren Chiu and Sneha Ramshanker and Radhika Nagpal and Bahar Haghighat", "abstract": "  Robot swarms can effectively serve a variety of sensing and inspection\napplications. Certain inspection tasks require a binary classification\ndecision. This work presents an experimental setup for a surface inspection\ntask based on vibration sensing and studies a Bayesian two-outcome\ndecision-making algorithm in a swarm of miniaturized wheeled robots. The robots\nare tasked with individually inspecting and collectively classifying a 1mx1m\ntiled surface consisting of vibrating and non-vibrating tiles based on the\nmajority type of tiles. The robots sense vibrations using onboard IMUs and\nperform collision avoidance using a set of IR sensors. We develop a simulation\nand optimization framework leveraging the Webots robotic simulator and a\nParticle Swarm Optimization (PSO) method. We consider two existing information\nsharing strategies and propose a new one that allows the swarm to rapidly reach\naccurate classification decisions. We first find optimal parameters that allow\nefficient sampling in simulation and then evaluate our proposed strategy\nagainst the two existing ones using 100 randomized simulation and 10 real\nexperiments. We find that our proposed method compels the swarm to make\ndecisions at an accelerated rate, with an improvement of up to 20.52% in mean\ndecision time at only 0.78% loss in accuracy.\n", "link": "http://arxiv.org/abs/2404.08390v2", "date": "2025-07-10", "relevancy": 1.6048, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6198}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collective%20Bayesian%20Decision-Making%20in%20a%20Swarm%20of%20Miniaturized%20Robots%0A%20%20for%20Surface%20Inspection&body=Title%3A%20Collective%20Bayesian%20Decision-Making%20in%20a%20Swarm%20of%20Miniaturized%20Robots%0A%20%20for%20Surface%20Inspection%0AAuthor%3A%20Thiemen%20Siemensma%20and%20Darren%20Chiu%20and%20Sneha%20Ramshanker%20and%20Radhika%20Nagpal%20and%20Bahar%20Haghighat%0AAbstract%3A%20%20%20Robot%20swarms%20can%20effectively%20serve%20a%20variety%20of%20sensing%20and%20inspection%0Aapplications.%20Certain%20inspection%20tasks%20require%20a%20binary%20classification%0Adecision.%20This%20work%20presents%20an%20experimental%20setup%20for%20a%20surface%20inspection%0Atask%20based%20on%20vibration%20sensing%20and%20studies%20a%20Bayesian%20two-outcome%0Adecision-making%20algorithm%20in%20a%20swarm%20of%20miniaturized%20wheeled%20robots.%20The%20robots%0Aare%20tasked%20with%20individually%20inspecting%20and%20collectively%20classifying%20a%201mx1m%0Atiled%20surface%20consisting%20of%20vibrating%20and%20non-vibrating%20tiles%20based%20on%20the%0Amajority%20type%20of%20tiles.%20The%20robots%20sense%20vibrations%20using%20onboard%20IMUs%20and%0Aperform%20collision%20avoidance%20using%20a%20set%20of%20IR%20sensors.%20We%20develop%20a%20simulation%0Aand%20optimization%20framework%20leveraging%20the%20Webots%20robotic%20simulator%20and%20a%0AParticle%20Swarm%20Optimization%20%28PSO%29%20method.%20We%20consider%20two%20existing%20information%0Asharing%20strategies%20and%20propose%20a%20new%20one%20that%20allows%20the%20swarm%20to%20rapidly%20reach%0Aaccurate%20classification%20decisions.%20We%20first%20find%20optimal%20parameters%20that%20allow%0Aefficient%20sampling%20in%20simulation%20and%20then%20evaluate%20our%20proposed%20strategy%0Aagainst%20the%20two%20existing%20ones%20using%20100%20randomized%20simulation%20and%2010%20real%0Aexperiments.%20We%20find%20that%20our%20proposed%20method%20compels%20the%20swarm%20to%20make%0Adecisions%20at%20an%20accelerated%20rate%2C%20with%20an%20improvement%20of%20up%20to%2020.52%25%20in%20mean%0Adecision%20time%20at%20only%200.78%25%20loss%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08390v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollective%2520Bayesian%2520Decision-Making%2520in%2520a%2520Swarm%2520of%2520Miniaturized%2520Robots%250A%2520%2520for%2520Surface%2520Inspection%26entry.906535625%3DThiemen%2520Siemensma%2520and%2520Darren%2520Chiu%2520and%2520Sneha%2520Ramshanker%2520and%2520Radhika%2520Nagpal%2520and%2520Bahar%2520Haghighat%26entry.1292438233%3D%2520%2520Robot%2520swarms%2520can%2520effectively%2520serve%2520a%2520variety%2520of%2520sensing%2520and%2520inspection%250Aapplications.%2520Certain%2520inspection%2520tasks%2520require%2520a%2520binary%2520classification%250Adecision.%2520This%2520work%2520presents%2520an%2520experimental%2520setup%2520for%2520a%2520surface%2520inspection%250Atask%2520based%2520on%2520vibration%2520sensing%2520and%2520studies%2520a%2520Bayesian%2520two-outcome%250Adecision-making%2520algorithm%2520in%2520a%2520swarm%2520of%2520miniaturized%2520wheeled%2520robots.%2520The%2520robots%250Aare%2520tasked%2520with%2520individually%2520inspecting%2520and%2520collectively%2520classifying%2520a%25201mx1m%250Atiled%2520surface%2520consisting%2520of%2520vibrating%2520and%2520non-vibrating%2520tiles%2520based%2520on%2520the%250Amajority%2520type%2520of%2520tiles.%2520The%2520robots%2520sense%2520vibrations%2520using%2520onboard%2520IMUs%2520and%250Aperform%2520collision%2520avoidance%2520using%2520a%2520set%2520of%2520IR%2520sensors.%2520We%2520develop%2520a%2520simulation%250Aand%2520optimization%2520framework%2520leveraging%2520the%2520Webots%2520robotic%2520simulator%2520and%2520a%250AParticle%2520Swarm%2520Optimization%2520%2528PSO%2529%2520method.%2520We%2520consider%2520two%2520existing%2520information%250Asharing%2520strategies%2520and%2520propose%2520a%2520new%2520one%2520that%2520allows%2520the%2520swarm%2520to%2520rapidly%2520reach%250Aaccurate%2520classification%2520decisions.%2520We%2520first%2520find%2520optimal%2520parameters%2520that%2520allow%250Aefficient%2520sampling%2520in%2520simulation%2520and%2520then%2520evaluate%2520our%2520proposed%2520strategy%250Aagainst%2520the%2520two%2520existing%2520ones%2520using%2520100%2520randomized%2520simulation%2520and%252010%2520real%250Aexperiments.%2520We%2520find%2520that%2520our%2520proposed%2520method%2520compels%2520the%2520swarm%2520to%2520make%250Adecisions%2520at%2520an%2520accelerated%2520rate%252C%2520with%2520an%2520improvement%2520of%2520up%2520to%252020.52%2525%2520in%2520mean%250Adecision%2520time%2520at%2520only%25200.78%2525%2520loss%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08390v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collective%20Bayesian%20Decision-Making%20in%20a%20Swarm%20of%20Miniaturized%20Robots%0A%20%20for%20Surface%20Inspection&entry.906535625=Thiemen%20Siemensma%20and%20Darren%20Chiu%20and%20Sneha%20Ramshanker%20and%20Radhika%20Nagpal%20and%20Bahar%20Haghighat&entry.1292438233=%20%20Robot%20swarms%20can%20effectively%20serve%20a%20variety%20of%20sensing%20and%20inspection%0Aapplications.%20Certain%20inspection%20tasks%20require%20a%20binary%20classification%0Adecision.%20This%20work%20presents%20an%20experimental%20setup%20for%20a%20surface%20inspection%0Atask%20based%20on%20vibration%20sensing%20and%20studies%20a%20Bayesian%20two-outcome%0Adecision-making%20algorithm%20in%20a%20swarm%20of%20miniaturized%20wheeled%20robots.%20The%20robots%0Aare%20tasked%20with%20individually%20inspecting%20and%20collectively%20classifying%20a%201mx1m%0Atiled%20surface%20consisting%20of%20vibrating%20and%20non-vibrating%20tiles%20based%20on%20the%0Amajority%20type%20of%20tiles.%20The%20robots%20sense%20vibrations%20using%20onboard%20IMUs%20and%0Aperform%20collision%20avoidance%20using%20a%20set%20of%20IR%20sensors.%20We%20develop%20a%20simulation%0Aand%20optimization%20framework%20leveraging%20the%20Webots%20robotic%20simulator%20and%20a%0AParticle%20Swarm%20Optimization%20%28PSO%29%20method.%20We%20consider%20two%20existing%20information%0Asharing%20strategies%20and%20propose%20a%20new%20one%20that%20allows%20the%20swarm%20to%20rapidly%20reach%0Aaccurate%20classification%20decisions.%20We%20first%20find%20optimal%20parameters%20that%20allow%0Aefficient%20sampling%20in%20simulation%20and%20then%20evaluate%20our%20proposed%20strategy%0Aagainst%20the%20two%20existing%20ones%20using%20100%20randomized%20simulation%20and%2010%20real%0Aexperiments.%20We%20find%20that%20our%20proposed%20method%20compels%20the%20swarm%20to%20make%0Adecisions%20at%20an%20accelerated%20rate%2C%20with%20an%20improvement%20of%20up%20to%2020.52%25%20in%20mean%0Adecision%20time%20at%20only%200.78%25%20loss%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08390v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


