<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250807.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field\n  Rendering", "author": "Junyu Zhou and Yuyang Huang and Wenrui Dai and Junni Zou and Ziyang Zheng and Nuowen Kan and Chenglin Li and Hongkai Xiong", "abstract": "  Recent prominence in 3D Gaussian Splatting (3DGS) has enabled real-time\nrendering while maintaining high-fidelity novel view synthesis. However, 3DGS\nresorts to the Gaussian function that is low-pass by nature and is restricted\nin representing high-frequency details in 3D scenes. Moreover, it causes\nredundant primitives with degraded training and rendering efficiency and\nexcessive memory overhead. To overcome these limitations, we propose 3D Gabor\nSplatting (3DGabSplat) that leverages a novel 3D Gabor-based primitive with\nmultiple directional 3D frequency responses for radiance field representation\nsupervised by multi-view images. The proposed 3D Gabor-based primitive forms a\nfilter bank incorporating multiple 3D Gabor kernels at different frequencies to\nenhance flexibility and efficiency in capturing fine 3D details. Furthermore,\nto achieve novel view rendering, an efficient CUDA-based rasterizer is\ndeveloped to project the multiple directional 3D frequency components\ncharacterized by 3D Gabor-based primitives onto the 2D image plane, and a\nfrequency-adaptive mechanism is presented for adaptive joint optimization of\nprimitives. 3DGabSplat is scalable to be a plug-and-play kernel for seamless\nintegration into existing 3DGS paradigms to enhance both efficiency and quality\nof novel view synthesis. Extensive experiments demonstrate that 3DGabSplat\noutperforms 3DGS and its variants using alternative primitives, and achieves\nstate-of-the-art rendering quality across both real-world and synthetic scenes.\nRemarkably, we achieve up to 1.35 dB PSNR gain over 3DGS with simultaneously\nreduced number of primitives and memory consumption.\n", "link": "http://arxiv.org/abs/2508.05343v1", "date": "2025-08-07", "relevancy": 3.3598, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7256}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6565}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGabSplat%3A%203D%20Gabor%20Splatting%20for%20Frequency-adaptive%20Radiance%20Field%0A%20%20Rendering&body=Title%3A%203DGabSplat%3A%203D%20Gabor%20Splatting%20for%20Frequency-adaptive%20Radiance%20Field%0A%20%20Rendering%0AAuthor%3A%20Junyu%20Zhou%20and%20Yuyang%20Huang%20and%20Wenrui%20Dai%20and%20Junni%20Zou%20and%20Ziyang%20Zheng%20and%20Nuowen%20Kan%20and%20Chenglin%20Li%20and%20Hongkai%20Xiong%0AAbstract%3A%20%20%20Recent%20prominence%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20enabled%20real-time%0Arendering%20while%20maintaining%20high-fidelity%20novel%20view%20synthesis.%20However%2C%203DGS%0Aresorts%20to%20the%20Gaussian%20function%20that%20is%20low-pass%20by%20nature%20and%20is%20restricted%0Ain%20representing%20high-frequency%20details%20in%203D%20scenes.%20Moreover%2C%20it%20causes%0Aredundant%20primitives%20with%20degraded%20training%20and%20rendering%20efficiency%20and%0Aexcessive%20memory%20overhead.%20To%20overcome%20these%20limitations%2C%20we%20propose%203D%20Gabor%0ASplatting%20%283DGabSplat%29%20that%20leverages%20a%20novel%203D%20Gabor-based%20primitive%20with%0Amultiple%20directional%203D%20frequency%20responses%20for%20radiance%20field%20representation%0Asupervised%20by%20multi-view%20images.%20The%20proposed%203D%20Gabor-based%20primitive%20forms%20a%0Afilter%20bank%20incorporating%20multiple%203D%20Gabor%20kernels%20at%20different%20frequencies%20to%0Aenhance%20flexibility%20and%20efficiency%20in%20capturing%20fine%203D%20details.%20Furthermore%2C%0Ato%20achieve%20novel%20view%20rendering%2C%20an%20efficient%20CUDA-based%20rasterizer%20is%0Adeveloped%20to%20project%20the%20multiple%20directional%203D%20frequency%20components%0Acharacterized%20by%203D%20Gabor-based%20primitives%20onto%20the%202D%20image%20plane%2C%20and%20a%0Afrequency-adaptive%20mechanism%20is%20presented%20for%20adaptive%20joint%20optimization%20of%0Aprimitives.%203DGabSplat%20is%20scalable%20to%20be%20a%20plug-and-play%20kernel%20for%20seamless%0Aintegration%20into%20existing%203DGS%20paradigms%20to%20enhance%20both%20efficiency%20and%20quality%0Aof%20novel%20view%20synthesis.%20Extensive%20experiments%20demonstrate%20that%203DGabSplat%0Aoutperforms%203DGS%20and%20its%20variants%20using%20alternative%20primitives%2C%20and%20achieves%0Astate-of-the-art%20rendering%20quality%20across%20both%20real-world%20and%20synthetic%20scenes.%0ARemarkably%2C%20we%20achieve%20up%20to%201.35%20dB%20PSNR%20gain%20over%203DGS%20with%20simultaneously%0Areduced%20number%20of%20primitives%20and%20memory%20consumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGabSplat%253A%25203D%2520Gabor%2520Splatting%2520for%2520Frequency-adaptive%2520Radiance%2520Field%250A%2520%2520Rendering%26entry.906535625%3DJunyu%2520Zhou%2520and%2520Yuyang%2520Huang%2520and%2520Wenrui%2520Dai%2520and%2520Junni%2520Zou%2520and%2520Ziyang%2520Zheng%2520and%2520Nuowen%2520Kan%2520and%2520Chenglin%2520Li%2520and%2520Hongkai%2520Xiong%26entry.1292438233%3D%2520%2520Recent%2520prominence%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520enabled%2520real-time%250Arendering%2520while%2520maintaining%2520high-fidelity%2520novel%2520view%2520synthesis.%2520However%252C%25203DGS%250Aresorts%2520to%2520the%2520Gaussian%2520function%2520that%2520is%2520low-pass%2520by%2520nature%2520and%2520is%2520restricted%250Ain%2520representing%2520high-frequency%2520details%2520in%25203D%2520scenes.%2520Moreover%252C%2520it%2520causes%250Aredundant%2520primitives%2520with%2520degraded%2520training%2520and%2520rendering%2520efficiency%2520and%250Aexcessive%2520memory%2520overhead.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%25203D%2520Gabor%250ASplatting%2520%25283DGabSplat%2529%2520that%2520leverages%2520a%2520novel%25203D%2520Gabor-based%2520primitive%2520with%250Amultiple%2520directional%25203D%2520frequency%2520responses%2520for%2520radiance%2520field%2520representation%250Asupervised%2520by%2520multi-view%2520images.%2520The%2520proposed%25203D%2520Gabor-based%2520primitive%2520forms%2520a%250Afilter%2520bank%2520incorporating%2520multiple%25203D%2520Gabor%2520kernels%2520at%2520different%2520frequencies%2520to%250Aenhance%2520flexibility%2520and%2520efficiency%2520in%2520capturing%2520fine%25203D%2520details.%2520Furthermore%252C%250Ato%2520achieve%2520novel%2520view%2520rendering%252C%2520an%2520efficient%2520CUDA-based%2520rasterizer%2520is%250Adeveloped%2520to%2520project%2520the%2520multiple%2520directional%25203D%2520frequency%2520components%250Acharacterized%2520by%25203D%2520Gabor-based%2520primitives%2520onto%2520the%25202D%2520image%2520plane%252C%2520and%2520a%250Afrequency-adaptive%2520mechanism%2520is%2520presented%2520for%2520adaptive%2520joint%2520optimization%2520of%250Aprimitives.%25203DGabSplat%2520is%2520scalable%2520to%2520be%2520a%2520plug-and-play%2520kernel%2520for%2520seamless%250Aintegration%2520into%2520existing%25203DGS%2520paradigms%2520to%2520enhance%2520both%2520efficiency%2520and%2520quality%250Aof%2520novel%2520view%2520synthesis.%2520Extensive%2520experiments%2520demonstrate%2520that%25203DGabSplat%250Aoutperforms%25203DGS%2520and%2520its%2520variants%2520using%2520alternative%2520primitives%252C%2520and%2520achieves%250Astate-of-the-art%2520rendering%2520quality%2520across%2520both%2520real-world%2520and%2520synthetic%2520scenes.%250ARemarkably%252C%2520we%2520achieve%2520up%2520to%25201.35%2520dB%2520PSNR%2520gain%2520over%25203DGS%2520with%2520simultaneously%250Areduced%2520number%2520of%2520primitives%2520and%2520memory%2520consumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGabSplat%3A%203D%20Gabor%20Splatting%20for%20Frequency-adaptive%20Radiance%20Field%0A%20%20Rendering&entry.906535625=Junyu%20Zhou%20and%20Yuyang%20Huang%20and%20Wenrui%20Dai%20and%20Junni%20Zou%20and%20Ziyang%20Zheng%20and%20Nuowen%20Kan%20and%20Chenglin%20Li%20and%20Hongkai%20Xiong&entry.1292438233=%20%20Recent%20prominence%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20enabled%20real-time%0Arendering%20while%20maintaining%20high-fidelity%20novel%20view%20synthesis.%20However%2C%203DGS%0Aresorts%20to%20the%20Gaussian%20function%20that%20is%20low-pass%20by%20nature%20and%20is%20restricted%0Ain%20representing%20high-frequency%20details%20in%203D%20scenes.%20Moreover%2C%20it%20causes%0Aredundant%20primitives%20with%20degraded%20training%20and%20rendering%20efficiency%20and%0Aexcessive%20memory%20overhead.%20To%20overcome%20these%20limitations%2C%20we%20propose%203D%20Gabor%0ASplatting%20%283DGabSplat%29%20that%20leverages%20a%20novel%203D%20Gabor-based%20primitive%20with%0Amultiple%20directional%203D%20frequency%20responses%20for%20radiance%20field%20representation%0Asupervised%20by%20multi-view%20images.%20The%20proposed%203D%20Gabor-based%20primitive%20forms%20a%0Afilter%20bank%20incorporating%20multiple%203D%20Gabor%20kernels%20at%20different%20frequencies%20to%0Aenhance%20flexibility%20and%20efficiency%20in%20capturing%20fine%203D%20details.%20Furthermore%2C%0Ato%20achieve%20novel%20view%20rendering%2C%20an%20efficient%20CUDA-based%20rasterizer%20is%0Adeveloped%20to%20project%20the%20multiple%20directional%203D%20frequency%20components%0Acharacterized%20by%203D%20Gabor-based%20primitives%20onto%20the%202D%20image%20plane%2C%20and%20a%0Afrequency-adaptive%20mechanism%20is%20presented%20for%20adaptive%20joint%20optimization%20of%0Aprimitives.%203DGabSplat%20is%20scalable%20to%20be%20a%20plug-and-play%20kernel%20for%20seamless%0Aintegration%20into%20existing%203DGS%20paradigms%20to%20enhance%20both%20efficiency%20and%20quality%0Aof%20novel%20view%20synthesis.%20Extensive%20experiments%20demonstrate%20that%203DGabSplat%0Aoutperforms%203DGS%20and%20its%20variants%20using%20alternative%20primitives%2C%20and%20achieves%0Astate-of-the-art%20rendering%20quality%20across%20both%20real-world%20and%20synthetic%20scenes.%0ARemarkably%2C%20we%20achieve%20up%20to%201.35%20dB%20PSNR%20gain%20over%203DGS%20with%20simultaneously%0Areduced%20number%20of%20primitives%20and%20memory%20consumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05343v1&entry.124074799=Read"},
{"title": "GAP: Gaussianize Any Point Clouds with Text Guidance", "author": "Weiqi Zhang and Junsheng Zhou and Haotian Geng and Wenyuan Zhang and Yu-Shen Liu", "abstract": "  3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving\nfast and high-quality rendering. As point clouds serve as a widely-used and\neasily accessible form of 3D representation, bridging the gap between point\nclouds and Gaussians becomes increasingly important. Recent studies have\nexplored how to convert the colored points into Gaussians, but directly\ngenerating Gaussians from colorless 3D point clouds remains an unsolved\nchallenge. In this paper, we propose GAP, a novel approach that gaussianizes\nraw point clouds into high-fidelity 3D Gaussians with text guidance. Our key\nidea is to design a multi-view optimization framework that leverages a\ndepth-aware image diffusion model to synthesize consistent appearances across\ndifferent viewpoints. To ensure geometric accuracy, we introduce a\nsurface-anchoring mechanism that effectively constrains Gaussians to lie on the\nsurfaces of 3D shapes during optimization. Furthermore, GAP incorporates a\ndiffuse-based inpainting strategy that specifically targets at completing\nhard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation\ntask across varying complexity levels, from synthetic point clouds to\nchallenging real-world scans, and even large-scale scenes. Project Page:\nhttps://weiqi-zhang.github.io/GAP.\n", "link": "http://arxiv.org/abs/2508.05631v1", "date": "2025-08-07", "relevancy": 3.2816, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6611}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6586}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAP%3A%20Gaussianize%20Any%20Point%20Clouds%20with%20Text%20Guidance&body=Title%3A%20GAP%3A%20Gaussianize%20Any%20Point%20Clouds%20with%20Text%20Guidance%0AAuthor%3A%20Weiqi%20Zhang%20and%20Junsheng%20Zhou%20and%20Haotian%20Geng%20and%20Wenyuan%20Zhang%20and%20Yu-Shen%20Liu%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20its%20advantages%20in%20achieving%0Afast%20and%20high-quality%20rendering.%20As%20point%20clouds%20serve%20as%20a%20widely-used%20and%0Aeasily%20accessible%20form%20of%203D%20representation%2C%20bridging%20the%20gap%20between%20point%0Aclouds%20and%20Gaussians%20becomes%20increasingly%20important.%20Recent%20studies%20have%0Aexplored%20how%20to%20convert%20the%20colored%20points%20into%20Gaussians%2C%20but%20directly%0Agenerating%20Gaussians%20from%20colorless%203D%20point%20clouds%20remains%20an%20unsolved%0Achallenge.%20In%20this%20paper%2C%20we%20propose%20GAP%2C%20a%20novel%20approach%20that%20gaussianizes%0Araw%20point%20clouds%20into%20high-fidelity%203D%20Gaussians%20with%20text%20guidance.%20Our%20key%0Aidea%20is%20to%20design%20a%20multi-view%20optimization%20framework%20that%20leverages%20a%0Adepth-aware%20image%20diffusion%20model%20to%20synthesize%20consistent%20appearances%20across%0Adifferent%20viewpoints.%20To%20ensure%20geometric%20accuracy%2C%20we%20introduce%20a%0Asurface-anchoring%20mechanism%20that%20effectively%20constrains%20Gaussians%20to%20lie%20on%20the%0Asurfaces%20of%203D%20shapes%20during%20optimization.%20Furthermore%2C%20GAP%20incorporates%20a%0Adiffuse-based%20inpainting%20strategy%20that%20specifically%20targets%20at%20completing%0Ahard-to-observe%20regions.%20We%20evaluate%20GAP%20on%20the%20Point-to-Gaussian%20generation%0Atask%20across%20varying%20complexity%20levels%2C%20from%20synthetic%20point%20clouds%20to%0Achallenging%20real-world%20scans%2C%20and%20even%20large-scale%20scenes.%20Project%20Page%3A%0Ahttps%3A//weiqi-zhang.github.io/GAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAP%253A%2520Gaussianize%2520Any%2520Point%2520Clouds%2520with%2520Text%2520Guidance%26entry.906535625%3DWeiqi%2520Zhang%2520and%2520Junsheng%2520Zhou%2520and%2520Haotian%2520Geng%2520and%2520Wenyuan%2520Zhang%2520and%2520Yu-Shen%2520Liu%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520demonstrated%2520its%2520advantages%2520in%2520achieving%250Afast%2520and%2520high-quality%2520rendering.%2520As%2520point%2520clouds%2520serve%2520as%2520a%2520widely-used%2520and%250Aeasily%2520accessible%2520form%2520of%25203D%2520representation%252C%2520bridging%2520the%2520gap%2520between%2520point%250Aclouds%2520and%2520Gaussians%2520becomes%2520increasingly%2520important.%2520Recent%2520studies%2520have%250Aexplored%2520how%2520to%2520convert%2520the%2520colored%2520points%2520into%2520Gaussians%252C%2520but%2520directly%250Agenerating%2520Gaussians%2520from%2520colorless%25203D%2520point%2520clouds%2520remains%2520an%2520unsolved%250Achallenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GAP%252C%2520a%2520novel%2520approach%2520that%2520gaussianizes%250Araw%2520point%2520clouds%2520into%2520high-fidelity%25203D%2520Gaussians%2520with%2520text%2520guidance.%2520Our%2520key%250Aidea%2520is%2520to%2520design%2520a%2520multi-view%2520optimization%2520framework%2520that%2520leverages%2520a%250Adepth-aware%2520image%2520diffusion%2520model%2520to%2520synthesize%2520consistent%2520appearances%2520across%250Adifferent%2520viewpoints.%2520To%2520ensure%2520geometric%2520accuracy%252C%2520we%2520introduce%2520a%250Asurface-anchoring%2520mechanism%2520that%2520effectively%2520constrains%2520Gaussians%2520to%2520lie%2520on%2520the%250Asurfaces%2520of%25203D%2520shapes%2520during%2520optimization.%2520Furthermore%252C%2520GAP%2520incorporates%2520a%250Adiffuse-based%2520inpainting%2520strategy%2520that%2520specifically%2520targets%2520at%2520completing%250Ahard-to-observe%2520regions.%2520We%2520evaluate%2520GAP%2520on%2520the%2520Point-to-Gaussian%2520generation%250Atask%2520across%2520varying%2520complexity%2520levels%252C%2520from%2520synthetic%2520point%2520clouds%2520to%250Achallenging%2520real-world%2520scans%252C%2520and%2520even%2520large-scale%2520scenes.%2520Project%2520Page%253A%250Ahttps%253A//weiqi-zhang.github.io/GAP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAP%3A%20Gaussianize%20Any%20Point%20Clouds%20with%20Text%20Guidance&entry.906535625=Weiqi%20Zhang%20and%20Junsheng%20Zhou%20and%20Haotian%20Geng%20and%20Wenyuan%20Zhang%20and%20Yu-Shen%20Liu&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20its%20advantages%20in%20achieving%0Afast%20and%20high-quality%20rendering.%20As%20point%20clouds%20serve%20as%20a%20widely-used%20and%0Aeasily%20accessible%20form%20of%203D%20representation%2C%20bridging%20the%20gap%20between%20point%0Aclouds%20and%20Gaussians%20becomes%20increasingly%20important.%20Recent%20studies%20have%0Aexplored%20how%20to%20convert%20the%20colored%20points%20into%20Gaussians%2C%20but%20directly%0Agenerating%20Gaussians%20from%20colorless%203D%20point%20clouds%20remains%20an%20unsolved%0Achallenge.%20In%20this%20paper%2C%20we%20propose%20GAP%2C%20a%20novel%20approach%20that%20gaussianizes%0Araw%20point%20clouds%20into%20high-fidelity%203D%20Gaussians%20with%20text%20guidance.%20Our%20key%0Aidea%20is%20to%20design%20a%20multi-view%20optimization%20framework%20that%20leverages%20a%0Adepth-aware%20image%20diffusion%20model%20to%20synthesize%20consistent%20appearances%20across%0Adifferent%20viewpoints.%20To%20ensure%20geometric%20accuracy%2C%20we%20introduce%20a%0Asurface-anchoring%20mechanism%20that%20effectively%20constrains%20Gaussians%20to%20lie%20on%20the%0Asurfaces%20of%203D%20shapes%20during%20optimization.%20Furthermore%2C%20GAP%20incorporates%20a%0Adiffuse-based%20inpainting%20strategy%20that%20specifically%20targets%20at%20completing%0Ahard-to-observe%20regions.%20We%20evaluate%20GAP%20on%20the%20Point-to-Gaussian%20generation%0Atask%20across%20varying%20complexity%20levels%2C%20from%20synthetic%20point%20clouds%20to%0Achallenging%20real-world%20scans%2C%20and%20even%20large-scale%20scenes.%20Project%20Page%3A%0Ahttps%3A//weiqi-zhang.github.io/GAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05631v1&entry.124074799=Read"},
{"title": "CF3: Compact and Fast 3D Feature Fields", "author": "Hyunjoon Lee and Joonkyu Min and Jaesik Park", "abstract": "  3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D\nfoundation models. However, most approaches rely on a bottom-up optimization\nprocess that treats raw 2D features as ground truth, incurring increased\ncomputational costs. We propose a top-down pipeline for constructing compact\nand fast 3D Gaussian feature fields, namely, CF3. We first perform a fast\nweighted fusion of multi-view 2D features with pre-trained Gaussians. This\napproach enables training a per-Gaussian autoencoder directly on the lifted\nfeatures, instead of training autoencoders in the 2D domain. As a result, the\nautoencoder better aligns with the feature distribution. More importantly, we\nintroduce an adaptive sparsification method that optimizes the Gaussian\nattributes of the feature field while pruning and merging the redundant\nGaussians, constructing an efficient representation with preserved geometric\ndetails. Our approach achieves a competitive 3D feature field using as little\nas 5% of the Gaussians compared to Feature-3DGS.\n", "link": "http://arxiv.org/abs/2508.05254v1", "date": "2025-08-07", "relevancy": 3.1504, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6537}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6419}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CF3%3A%20Compact%20and%20Fast%203D%20Feature%20Fields&body=Title%3A%20CF3%3A%20Compact%20and%20Fast%203D%20Feature%20Fields%0AAuthor%3A%20Hyunjoon%20Lee%20and%20Joonkyu%20Min%20and%20Jaesik%20Park%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20begun%20incorporating%20rich%20information%20from%202D%0Afoundation%20models.%20However%2C%20most%20approaches%20rely%20on%20a%20bottom-up%20optimization%0Aprocess%20that%20treats%20raw%202D%20features%20as%20ground%20truth%2C%20incurring%20increased%0Acomputational%20costs.%20We%20propose%20a%20top-down%20pipeline%20for%20constructing%20compact%0Aand%20fast%203D%20Gaussian%20feature%20fields%2C%20namely%2C%20CF3.%20We%20first%20perform%20a%20fast%0Aweighted%20fusion%20of%20multi-view%202D%20features%20with%20pre-trained%20Gaussians.%20This%0Aapproach%20enables%20training%20a%20per-Gaussian%20autoencoder%20directly%20on%20the%20lifted%0Afeatures%2C%20instead%20of%20training%20autoencoders%20in%20the%202D%20domain.%20As%20a%20result%2C%20the%0Aautoencoder%20better%20aligns%20with%20the%20feature%20distribution.%20More%20importantly%2C%20we%0Aintroduce%20an%20adaptive%20sparsification%20method%20that%20optimizes%20the%20Gaussian%0Aattributes%20of%20the%20feature%20field%20while%20pruning%20and%20merging%20the%20redundant%0AGaussians%2C%20constructing%20an%20efficient%20representation%20with%20preserved%20geometric%0Adetails.%20Our%20approach%20achieves%20a%20competitive%203D%20feature%20field%20using%20as%20little%0Aas%205%25%20of%20the%20Gaussians%20compared%20to%20Feature-3DGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCF3%253A%2520Compact%2520and%2520Fast%25203D%2520Feature%2520Fields%26entry.906535625%3DHyunjoon%2520Lee%2520and%2520Joonkyu%2520Min%2520and%2520Jaesik%2520Park%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520begun%2520incorporating%2520rich%2520information%2520from%25202D%250Afoundation%2520models.%2520However%252C%2520most%2520approaches%2520rely%2520on%2520a%2520bottom-up%2520optimization%250Aprocess%2520that%2520treats%2520raw%25202D%2520features%2520as%2520ground%2520truth%252C%2520incurring%2520increased%250Acomputational%2520costs.%2520We%2520propose%2520a%2520top-down%2520pipeline%2520for%2520constructing%2520compact%250Aand%2520fast%25203D%2520Gaussian%2520feature%2520fields%252C%2520namely%252C%2520CF3.%2520We%2520first%2520perform%2520a%2520fast%250Aweighted%2520fusion%2520of%2520multi-view%25202D%2520features%2520with%2520pre-trained%2520Gaussians.%2520This%250Aapproach%2520enables%2520training%2520a%2520per-Gaussian%2520autoencoder%2520directly%2520on%2520the%2520lifted%250Afeatures%252C%2520instead%2520of%2520training%2520autoencoders%2520in%2520the%25202D%2520domain.%2520As%2520a%2520result%252C%2520the%250Aautoencoder%2520better%2520aligns%2520with%2520the%2520feature%2520distribution.%2520More%2520importantly%252C%2520we%250Aintroduce%2520an%2520adaptive%2520sparsification%2520method%2520that%2520optimizes%2520the%2520Gaussian%250Aattributes%2520of%2520the%2520feature%2520field%2520while%2520pruning%2520and%2520merging%2520the%2520redundant%250AGaussians%252C%2520constructing%2520an%2520efficient%2520representation%2520with%2520preserved%2520geometric%250Adetails.%2520Our%2520approach%2520achieves%2520a%2520competitive%25203D%2520feature%2520field%2520using%2520as%2520little%250Aas%25205%2525%2520of%2520the%2520Gaussians%2520compared%2520to%2520Feature-3DGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CF3%3A%20Compact%20and%20Fast%203D%20Feature%20Fields&entry.906535625=Hyunjoon%20Lee%20and%20Joonkyu%20Min%20and%20Jaesik%20Park&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20begun%20incorporating%20rich%20information%20from%202D%0Afoundation%20models.%20However%2C%20most%20approaches%20rely%20on%20a%20bottom-up%20optimization%0Aprocess%20that%20treats%20raw%202D%20features%20as%20ground%20truth%2C%20incurring%20increased%0Acomputational%20costs.%20We%20propose%20a%20top-down%20pipeline%20for%20constructing%20compact%0Aand%20fast%203D%20Gaussian%20feature%20fields%2C%20namely%2C%20CF3.%20We%20first%20perform%20a%20fast%0Aweighted%20fusion%20of%20multi-view%202D%20features%20with%20pre-trained%20Gaussians.%20This%0Aapproach%20enables%20training%20a%20per-Gaussian%20autoencoder%20directly%20on%20the%20lifted%0Afeatures%2C%20instead%20of%20training%20autoencoders%20in%20the%202D%20domain.%20As%20a%20result%2C%20the%0Aautoencoder%20better%20aligns%20with%20the%20feature%20distribution.%20More%20importantly%2C%20we%0Aintroduce%20an%20adaptive%20sparsification%20method%20that%20optimizes%20the%20Gaussian%0Aattributes%20of%20the%20feature%20field%20while%20pruning%20and%20merging%20the%20redundant%0AGaussians%2C%20constructing%20an%20efficient%20representation%20with%20preserved%20geometric%0Adetails.%20Our%20approach%20achieves%20a%20competitive%203D%20feature%20field%20using%20as%20little%0Aas%205%25%20of%20the%20Gaussians%20compared%20to%20Feature-3DGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05254v1&entry.124074799=Read"},
{"title": "Point cloud segmentation for 3D Clothed Human Layering", "author": "Davide Garavaso and Federico Masi and Pietro Musoni and Umberto Castellani", "abstract": "  3D Cloth modeling and simulation is essential for avatars creation in several\nfields, such as fashion, entertainment, and animation. Achieving high-quality\nresults is challenging due to the large variability of clothed body especially\nin the generation of realistic wrinkles. 3D scan acquisitions provide more\naccuracy in the representation of real-world objects but lack semantic\ninformation that can be inferred with a reliable semantic reconstruction\npipeline. To this aim, shape segmentation plays a crucial role in identifying\nthe semantic shape parts. However, current 3D shape segmentation methods are\ndesigned for scene understanding and interpretation and only few work is\ndevoted to modeling. In the context of clothed body modeling the segmentation\nis a preliminary step for fully semantic shape parts reconstruction namely the\nunderlying body and the involved garments. These parts represent several layers\nwith strong overlap in contrast with standard segmentation methods that provide\ndisjoint sets. In this work we propose a new 3D point cloud segmentation\nparadigm where each 3D point can be simultaneously associated to different\nlayers. In this fashion we can estimate the underlying body parts and the\nunseen clothed regions, i.e., the part of a cloth occluded by the clothed-layer\nabove. We name this segmentation paradigm clothed human layering. We create a\nnew synthetic dataset that simulates very realistic 3D scans with the ground\ntruth of the involved clothing layers. We propose and evaluate different neural\nnetwork settings to deal with 3D clothing layering. We considered both coarse\nand fine grained per-layer garment identification. Our experiments demonstrates\nthe benefit in introducing proper strategies for the segmentation on the\ngarment domain on both the synthetic and real-world scan datasets.\n", "link": "http://arxiv.org/abs/2508.05531v1", "date": "2025-08-07", "relevancy": 3.0758, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6333}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6094}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20cloud%20segmentation%20for%203D%20Clothed%20Human%20Layering&body=Title%3A%20Point%20cloud%20segmentation%20for%203D%20Clothed%20Human%20Layering%0AAuthor%3A%20Davide%20Garavaso%20and%20Federico%20Masi%20and%20Pietro%20Musoni%20and%20Umberto%20Castellani%0AAbstract%3A%20%20%203D%20Cloth%20modeling%20and%20simulation%20is%20essential%20for%20avatars%20creation%20in%20several%0Afields%2C%20such%20as%20fashion%2C%20entertainment%2C%20and%20animation.%20Achieving%20high-quality%0Aresults%20is%20challenging%20due%20to%20the%20large%20variability%20of%20clothed%20body%20especially%0Ain%20the%20generation%20of%20realistic%20wrinkles.%203D%20scan%20acquisitions%20provide%20more%0Aaccuracy%20in%20the%20representation%20of%20real-world%20objects%20but%20lack%20semantic%0Ainformation%20that%20can%20be%20inferred%20with%20a%20reliable%20semantic%20reconstruction%0Apipeline.%20To%20this%20aim%2C%20shape%20segmentation%20plays%20a%20crucial%20role%20in%20identifying%0Athe%20semantic%20shape%20parts.%20However%2C%20current%203D%20shape%20segmentation%20methods%20are%0Adesigned%20for%20scene%20understanding%20and%20interpretation%20and%20only%20few%20work%20is%0Adevoted%20to%20modeling.%20In%20the%20context%20of%20clothed%20body%20modeling%20the%20segmentation%0Ais%20a%20preliminary%20step%20for%20fully%20semantic%20shape%20parts%20reconstruction%20namely%20the%0Aunderlying%20body%20and%20the%20involved%20garments.%20These%20parts%20represent%20several%20layers%0Awith%20strong%20overlap%20in%20contrast%20with%20standard%20segmentation%20methods%20that%20provide%0Adisjoint%20sets.%20In%20this%20work%20we%20propose%20a%20new%203D%20point%20cloud%20segmentation%0Aparadigm%20where%20each%203D%20point%20can%20be%20simultaneously%20associated%20to%20different%0Alayers.%20In%20this%20fashion%20we%20can%20estimate%20the%20underlying%20body%20parts%20and%20the%0Aunseen%20clothed%20regions%2C%20i.e.%2C%20the%20part%20of%20a%20cloth%20occluded%20by%20the%20clothed-layer%0Aabove.%20We%20name%20this%20segmentation%20paradigm%20clothed%20human%20layering.%20We%20create%20a%0Anew%20synthetic%20dataset%20that%20simulates%20very%20realistic%203D%20scans%20with%20the%20ground%0Atruth%20of%20the%20involved%20clothing%20layers.%20We%20propose%20and%20evaluate%20different%20neural%0Anetwork%20settings%20to%20deal%20with%203D%20clothing%20layering.%20We%20considered%20both%20coarse%0Aand%20fine%20grained%20per-layer%20garment%20identification.%20Our%20experiments%20demonstrates%0Athe%20benefit%20in%20introducing%20proper%20strategies%20for%20the%20segmentation%20on%20the%0Agarment%20domain%20on%20both%20the%20synthetic%20and%20real-world%20scan%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520cloud%2520segmentation%2520for%25203D%2520Clothed%2520Human%2520Layering%26entry.906535625%3DDavide%2520Garavaso%2520and%2520Federico%2520Masi%2520and%2520Pietro%2520Musoni%2520and%2520Umberto%2520Castellani%26entry.1292438233%3D%2520%25203D%2520Cloth%2520modeling%2520and%2520simulation%2520is%2520essential%2520for%2520avatars%2520creation%2520in%2520several%250Afields%252C%2520such%2520as%2520fashion%252C%2520entertainment%252C%2520and%2520animation.%2520Achieving%2520high-quality%250Aresults%2520is%2520challenging%2520due%2520to%2520the%2520large%2520variability%2520of%2520clothed%2520body%2520especially%250Ain%2520the%2520generation%2520of%2520realistic%2520wrinkles.%25203D%2520scan%2520acquisitions%2520provide%2520more%250Aaccuracy%2520in%2520the%2520representation%2520of%2520real-world%2520objects%2520but%2520lack%2520semantic%250Ainformation%2520that%2520can%2520be%2520inferred%2520with%2520a%2520reliable%2520semantic%2520reconstruction%250Apipeline.%2520To%2520this%2520aim%252C%2520shape%2520segmentation%2520plays%2520a%2520crucial%2520role%2520in%2520identifying%250Athe%2520semantic%2520shape%2520parts.%2520However%252C%2520current%25203D%2520shape%2520segmentation%2520methods%2520are%250Adesigned%2520for%2520scene%2520understanding%2520and%2520interpretation%2520and%2520only%2520few%2520work%2520is%250Adevoted%2520to%2520modeling.%2520In%2520the%2520context%2520of%2520clothed%2520body%2520modeling%2520the%2520segmentation%250Ais%2520a%2520preliminary%2520step%2520for%2520fully%2520semantic%2520shape%2520parts%2520reconstruction%2520namely%2520the%250Aunderlying%2520body%2520and%2520the%2520involved%2520garments.%2520These%2520parts%2520represent%2520several%2520layers%250Awith%2520strong%2520overlap%2520in%2520contrast%2520with%2520standard%2520segmentation%2520methods%2520that%2520provide%250Adisjoint%2520sets.%2520In%2520this%2520work%2520we%2520propose%2520a%2520new%25203D%2520point%2520cloud%2520segmentation%250Aparadigm%2520where%2520each%25203D%2520point%2520can%2520be%2520simultaneously%2520associated%2520to%2520different%250Alayers.%2520In%2520this%2520fashion%2520we%2520can%2520estimate%2520the%2520underlying%2520body%2520parts%2520and%2520the%250Aunseen%2520clothed%2520regions%252C%2520i.e.%252C%2520the%2520part%2520of%2520a%2520cloth%2520occluded%2520by%2520the%2520clothed-layer%250Aabove.%2520We%2520name%2520this%2520segmentation%2520paradigm%2520clothed%2520human%2520layering.%2520We%2520create%2520a%250Anew%2520synthetic%2520dataset%2520that%2520simulates%2520very%2520realistic%25203D%2520scans%2520with%2520the%2520ground%250Atruth%2520of%2520the%2520involved%2520clothing%2520layers.%2520We%2520propose%2520and%2520evaluate%2520different%2520neural%250Anetwork%2520settings%2520to%2520deal%2520with%25203D%2520clothing%2520layering.%2520We%2520considered%2520both%2520coarse%250Aand%2520fine%2520grained%2520per-layer%2520garment%2520identification.%2520Our%2520experiments%2520demonstrates%250Athe%2520benefit%2520in%2520introducing%2520proper%2520strategies%2520for%2520the%2520segmentation%2520on%2520the%250Agarment%2520domain%2520on%2520both%2520the%2520synthetic%2520and%2520real-world%2520scan%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20cloud%20segmentation%20for%203D%20Clothed%20Human%20Layering&entry.906535625=Davide%20Garavaso%20and%20Federico%20Masi%20and%20Pietro%20Musoni%20and%20Umberto%20Castellani&entry.1292438233=%20%203D%20Cloth%20modeling%20and%20simulation%20is%20essential%20for%20avatars%20creation%20in%20several%0Afields%2C%20such%20as%20fashion%2C%20entertainment%2C%20and%20animation.%20Achieving%20high-quality%0Aresults%20is%20challenging%20due%20to%20the%20large%20variability%20of%20clothed%20body%20especially%0Ain%20the%20generation%20of%20realistic%20wrinkles.%203D%20scan%20acquisitions%20provide%20more%0Aaccuracy%20in%20the%20representation%20of%20real-world%20objects%20but%20lack%20semantic%0Ainformation%20that%20can%20be%20inferred%20with%20a%20reliable%20semantic%20reconstruction%0Apipeline.%20To%20this%20aim%2C%20shape%20segmentation%20plays%20a%20crucial%20role%20in%20identifying%0Athe%20semantic%20shape%20parts.%20However%2C%20current%203D%20shape%20segmentation%20methods%20are%0Adesigned%20for%20scene%20understanding%20and%20interpretation%20and%20only%20few%20work%20is%0Adevoted%20to%20modeling.%20In%20the%20context%20of%20clothed%20body%20modeling%20the%20segmentation%0Ais%20a%20preliminary%20step%20for%20fully%20semantic%20shape%20parts%20reconstruction%20namely%20the%0Aunderlying%20body%20and%20the%20involved%20garments.%20These%20parts%20represent%20several%20layers%0Awith%20strong%20overlap%20in%20contrast%20with%20standard%20segmentation%20methods%20that%20provide%0Adisjoint%20sets.%20In%20this%20work%20we%20propose%20a%20new%203D%20point%20cloud%20segmentation%0Aparadigm%20where%20each%203D%20point%20can%20be%20simultaneously%20associated%20to%20different%0Alayers.%20In%20this%20fashion%20we%20can%20estimate%20the%20underlying%20body%20parts%20and%20the%0Aunseen%20clothed%20regions%2C%20i.e.%2C%20the%20part%20of%20a%20cloth%20occluded%20by%20the%20clothed-layer%0Aabove.%20We%20name%20this%20segmentation%20paradigm%20clothed%20human%20layering.%20We%20create%20a%0Anew%20synthetic%20dataset%20that%20simulates%20very%20realistic%203D%20scans%20with%20the%20ground%0Atruth%20of%20the%20involved%20clothing%20layers.%20We%20propose%20and%20evaluate%20different%20neural%0Anetwork%20settings%20to%20deal%20with%203D%20clothing%20layering.%20We%20considered%20both%20coarse%0Aand%20fine%20grained%20per-layer%20garment%20identification.%20Our%20experiments%20demonstrates%0Athe%20benefit%20in%20introducing%20proper%20strategies%20for%20the%20segmentation%20on%20the%0Agarment%20domain%20on%20both%20the%20synthetic%20and%20real-world%20scan%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05531v1&entry.124074799=Read"},
{"title": "Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity", "author": "Yuhan Zhang and Long Zhuo and Ziyang Chu and Tong Wu and Zhibing Li and Liang Pan and Dahua Lin and Ziwei Liu", "abstract": "  Despite rapid advances in 3D content generation, quality assessment for the\ngenerated 3D assets remains challenging. Existing methods mainly rely on\nimage-based metrics and operate solely at the object level, limiting their\nability to capture spatial coherence, material authenticity, and high-fidelity\nlocal details. 1) To address these challenges, we introduce Hi3DEval, a\nhierarchical evaluation framework tailored for 3D generative content. It\ncombines both object-level and part-level evaluation, enabling holistic\nassessments across multiple dimensions as well as fine-grained quality\nanalysis. Additionally, we extend texture evaluation beyond aesthetic\nappearance by explicitly assessing material realism, focusing on attributes\nsuch as albedo, saturation, and metallicness. 2) To support this framework, we\nconstruct Hi3DBench, a large-scale dataset comprising diverse 3D assets and\nhigh-quality annotations, accompanied by a reliable multi-agent annotation\npipeline. We further propose a 3D-aware automated scoring system based on\nhybrid 3D representations. Specifically, we leverage video-based\nrepresentations for object-level and material-subject evaluations to enhance\nmodeling of spatio-temporal consistency and employ pretrained 3D features for\npart-level perception. Extensive experiments demonstrate that our approach\noutperforms existing image-based metrics in modeling 3D characteristics and\nachieves superior alignment with human preference, providing a scalable\nalternative to manual evaluations. The project page is available at\nhttps://zyh482.github.io/Hi3DEval/.\n", "link": "http://arxiv.org/abs/2508.05609v1", "date": "2025-08-07", "relevancy": 3.0466, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6196}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6196}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hi3DEval%3A%20Advancing%203D%20Generation%20Evaluation%20with%20Hierarchical%20Validity&body=Title%3A%20Hi3DEval%3A%20Advancing%203D%20Generation%20Evaluation%20with%20Hierarchical%20Validity%0AAuthor%3A%20Yuhan%20Zhang%20and%20Long%20Zhuo%20and%20Ziyang%20Chu%20and%20Tong%20Wu%20and%20Zhibing%20Li%20and%20Liang%20Pan%20and%20Dahua%20Lin%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Despite%20rapid%20advances%20in%203D%20content%20generation%2C%20quality%20assessment%20for%20the%0Agenerated%203D%20assets%20remains%20challenging.%20Existing%20methods%20mainly%20rely%20on%0Aimage-based%20metrics%20and%20operate%20solely%20at%20the%20object%20level%2C%20limiting%20their%0Aability%20to%20capture%20spatial%20coherence%2C%20material%20authenticity%2C%20and%20high-fidelity%0Alocal%20details.%201%29%20To%20address%20these%20challenges%2C%20we%20introduce%20Hi3DEval%2C%20a%0Ahierarchical%20evaluation%20framework%20tailored%20for%203D%20generative%20content.%20It%0Acombines%20both%20object-level%20and%20part-level%20evaluation%2C%20enabling%20holistic%0Aassessments%20across%20multiple%20dimensions%20as%20well%20as%20fine-grained%20quality%0Aanalysis.%20Additionally%2C%20we%20extend%20texture%20evaluation%20beyond%20aesthetic%0Aappearance%20by%20explicitly%20assessing%20material%20realism%2C%20focusing%20on%20attributes%0Asuch%20as%20albedo%2C%20saturation%2C%20and%20metallicness.%202%29%20To%20support%20this%20framework%2C%20we%0Aconstruct%20Hi3DBench%2C%20a%20large-scale%20dataset%20comprising%20diverse%203D%20assets%20and%0Ahigh-quality%20annotations%2C%20accompanied%20by%20a%20reliable%20multi-agent%20annotation%0Apipeline.%20We%20further%20propose%20a%203D-aware%20automated%20scoring%20system%20based%20on%0Ahybrid%203D%20representations.%20Specifically%2C%20we%20leverage%20video-based%0Arepresentations%20for%20object-level%20and%20material-subject%20evaluations%20to%20enhance%0Amodeling%20of%20spatio-temporal%20consistency%20and%20employ%20pretrained%203D%20features%20for%0Apart-level%20perception.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%0Aoutperforms%20existing%20image-based%20metrics%20in%20modeling%203D%20characteristics%20and%0Aachieves%20superior%20alignment%20with%20human%20preference%2C%20providing%20a%20scalable%0Aalternative%20to%20manual%20evaluations.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//zyh482.github.io/Hi3DEval/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHi3DEval%253A%2520Advancing%25203D%2520Generation%2520Evaluation%2520with%2520Hierarchical%2520Validity%26entry.906535625%3DYuhan%2520Zhang%2520and%2520Long%2520Zhuo%2520and%2520Ziyang%2520Chu%2520and%2520Tong%2520Wu%2520and%2520Zhibing%2520Li%2520and%2520Liang%2520Pan%2520and%2520Dahua%2520Lin%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Despite%2520rapid%2520advances%2520in%25203D%2520content%2520generation%252C%2520quality%2520assessment%2520for%2520the%250Agenerated%25203D%2520assets%2520remains%2520challenging.%2520Existing%2520methods%2520mainly%2520rely%2520on%250Aimage-based%2520metrics%2520and%2520operate%2520solely%2520at%2520the%2520object%2520level%252C%2520limiting%2520their%250Aability%2520to%2520capture%2520spatial%2520coherence%252C%2520material%2520authenticity%252C%2520and%2520high-fidelity%250Alocal%2520details.%25201%2529%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Hi3DEval%252C%2520a%250Ahierarchical%2520evaluation%2520framework%2520tailored%2520for%25203D%2520generative%2520content.%2520It%250Acombines%2520both%2520object-level%2520and%2520part-level%2520evaluation%252C%2520enabling%2520holistic%250Aassessments%2520across%2520multiple%2520dimensions%2520as%2520well%2520as%2520fine-grained%2520quality%250Aanalysis.%2520Additionally%252C%2520we%2520extend%2520texture%2520evaluation%2520beyond%2520aesthetic%250Aappearance%2520by%2520explicitly%2520assessing%2520material%2520realism%252C%2520focusing%2520on%2520attributes%250Asuch%2520as%2520albedo%252C%2520saturation%252C%2520and%2520metallicness.%25202%2529%2520To%2520support%2520this%2520framework%252C%2520we%250Aconstruct%2520Hi3DBench%252C%2520a%2520large-scale%2520dataset%2520comprising%2520diverse%25203D%2520assets%2520and%250Ahigh-quality%2520annotations%252C%2520accompanied%2520by%2520a%2520reliable%2520multi-agent%2520annotation%250Apipeline.%2520We%2520further%2520propose%2520a%25203D-aware%2520automated%2520scoring%2520system%2520based%2520on%250Ahybrid%25203D%2520representations.%2520Specifically%252C%2520we%2520leverage%2520video-based%250Arepresentations%2520for%2520object-level%2520and%2520material-subject%2520evaluations%2520to%2520enhance%250Amodeling%2520of%2520spatio-temporal%2520consistency%2520and%2520employ%2520pretrained%25203D%2520features%2520for%250Apart-level%2520perception.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%250Aoutperforms%2520existing%2520image-based%2520metrics%2520in%2520modeling%25203D%2520characteristics%2520and%250Aachieves%2520superior%2520alignment%2520with%2520human%2520preference%252C%2520providing%2520a%2520scalable%250Aalternative%2520to%2520manual%2520evaluations.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//zyh482.github.io/Hi3DEval/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hi3DEval%3A%20Advancing%203D%20Generation%20Evaluation%20with%20Hierarchical%20Validity&entry.906535625=Yuhan%20Zhang%20and%20Long%20Zhuo%20and%20Ziyang%20Chu%20and%20Tong%20Wu%20and%20Zhibing%20Li%20and%20Liang%20Pan%20and%20Dahua%20Lin%20and%20Ziwei%20Liu&entry.1292438233=%20%20Despite%20rapid%20advances%20in%203D%20content%20generation%2C%20quality%20assessment%20for%20the%0Agenerated%203D%20assets%20remains%20challenging.%20Existing%20methods%20mainly%20rely%20on%0Aimage-based%20metrics%20and%20operate%20solely%20at%20the%20object%20level%2C%20limiting%20their%0Aability%20to%20capture%20spatial%20coherence%2C%20material%20authenticity%2C%20and%20high-fidelity%0Alocal%20details.%201%29%20To%20address%20these%20challenges%2C%20we%20introduce%20Hi3DEval%2C%20a%0Ahierarchical%20evaluation%20framework%20tailored%20for%203D%20generative%20content.%20It%0Acombines%20both%20object-level%20and%20part-level%20evaluation%2C%20enabling%20holistic%0Aassessments%20across%20multiple%20dimensions%20as%20well%20as%20fine-grained%20quality%0Aanalysis.%20Additionally%2C%20we%20extend%20texture%20evaluation%20beyond%20aesthetic%0Aappearance%20by%20explicitly%20assessing%20material%20realism%2C%20focusing%20on%20attributes%0Asuch%20as%20albedo%2C%20saturation%2C%20and%20metallicness.%202%29%20To%20support%20this%20framework%2C%20we%0Aconstruct%20Hi3DBench%2C%20a%20large-scale%20dataset%20comprising%20diverse%203D%20assets%20and%0Ahigh-quality%20annotations%2C%20accompanied%20by%20a%20reliable%20multi-agent%20annotation%0Apipeline.%20We%20further%20propose%20a%203D-aware%20automated%20scoring%20system%20based%20on%0Ahybrid%203D%20representations.%20Specifically%2C%20we%20leverage%20video-based%0Arepresentations%20for%20object-level%20and%20material-subject%20evaluations%20to%20enhance%0Amodeling%20of%20spatio-temporal%20consistency%20and%20employ%20pretrained%203D%20features%20for%0Apart-level%20perception.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%0Aoutperforms%20existing%20image-based%20metrics%20in%20modeling%203D%20characteristics%20and%0Aachieves%20superior%20alignment%20with%20human%20preference%2C%20providing%20a%20scalable%0Aalternative%20to%20manual%20evaluations.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//zyh482.github.io/Hi3DEval/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05609v1&entry.124074799=Read"},
{"title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs", "author": "Jen-Tse Huang and Dasen Dai and Jen-Yuan Huang and Youliang Yuan and Xiaoyuan Liu and Wenxuan Wang and Wenxiang Jiao and Pinjia He and Zhaopeng Tu and Haodong Duan", "abstract": "  Despite significant progress on popular multimodal benchmarks,\nstate-of-the-art Multimodal Large Language Models (MLLMs) continue to struggle\nwith basic visual reasoning tasks that are trivially solved by humans, such as\nrecognizing spatial relationships. To systematically investigate this gap, we\nintroduce VisFactor, a benchmark that digitizes 20 vision-centric subtests from\na well-established cognitive psychology assessment. These subtests span four\ncore domains of human visual cognition: (1) Visualization and Spatial\nProcessing, (2) Perceptual and Closure, (3) Memory, and (4) Reasoning. We\nevaluate 20 frontier MLLMs from GPT, Gemini, Claude, LLaMA, Qwen, and SEED\nfamilies. The best-performing model achieves a score of only 25.19 out of 100,\nwith consistent failures on tasks such as mental rotation, spatial relation\ninference, and figure-ground discrimination, regardless of model size or\nprompting strategy. These findings suggest that current MLLM performance gains\non high-level benchmarks do not reflect human-like low-level visual cognition,\nchallenging the assumption that large-scale pretraining naturally induces\ngestalt-like perceptual capabilities. The dataset and evaluation toolkit are\npublicly available at: https://github.com/CUHK-ARISE/VisFactor.\n", "link": "http://arxiv.org/abs/2502.16435v2", "date": "2025-08-07", "relevancy": 3.001, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6244}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Cognitive%20Benchmarks%20Reveal%20Foundational%20Visual%20Gaps%20in%20MLLMs&body=Title%3A%20Human%20Cognitive%20Benchmarks%20Reveal%20Foundational%20Visual%20Gaps%20in%20MLLMs%0AAuthor%3A%20Jen-Tse%20Huang%20and%20Dasen%20Dai%20and%20Jen-Yuan%20Huang%20and%20Youliang%20Yuan%20and%20Xiaoyuan%20Liu%20and%20Wenxuan%20Wang%20and%20Wenxiang%20Jiao%20and%20Pinjia%20He%20and%20Zhaopeng%20Tu%20and%20Haodong%20Duan%0AAbstract%3A%20%20%20Despite%20significant%20progress%20on%20popular%20multimodal%20benchmarks%2C%0Astate-of-the-art%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20continue%20to%20struggle%0Awith%20basic%20visual%20reasoning%20tasks%20that%20are%20trivially%20solved%20by%20humans%2C%20such%20as%0Arecognizing%20spatial%20relationships.%20To%20systematically%20investigate%20this%20gap%2C%20we%0Aintroduce%20VisFactor%2C%20a%20benchmark%20that%20digitizes%2020%20vision-centric%20subtests%20from%0Aa%20well-established%20cognitive%20psychology%20assessment.%20These%20subtests%20span%20four%0Acore%20domains%20of%20human%20visual%20cognition%3A%20%281%29%20Visualization%20and%20Spatial%0AProcessing%2C%20%282%29%20Perceptual%20and%20Closure%2C%20%283%29%20Memory%2C%20and%20%284%29%20Reasoning.%20We%0Aevaluate%2020%20frontier%20MLLMs%20from%20GPT%2C%20Gemini%2C%20Claude%2C%20LLaMA%2C%20Qwen%2C%20and%20SEED%0Afamilies.%20The%20best-performing%20model%20achieves%20a%20score%20of%20only%2025.19%20out%20of%20100%2C%0Awith%20consistent%20failures%20on%20tasks%20such%20as%20mental%20rotation%2C%20spatial%20relation%0Ainference%2C%20and%20figure-ground%20discrimination%2C%20regardless%20of%20model%20size%20or%0Aprompting%20strategy.%20These%20findings%20suggest%20that%20current%20MLLM%20performance%20gains%0Aon%20high-level%20benchmarks%20do%20not%20reflect%20human-like%20low-level%20visual%20cognition%2C%0Achallenging%20the%20assumption%20that%20large-scale%20pretraining%20naturally%20induces%0Agestalt-like%20perceptual%20capabilities.%20The%20dataset%20and%20evaluation%20toolkit%20are%0Apublicly%20available%20at%3A%20https%3A//github.com/CUHK-ARISE/VisFactor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Cognitive%2520Benchmarks%2520Reveal%2520Foundational%2520Visual%2520Gaps%2520in%2520MLLMs%26entry.906535625%3DJen-Tse%2520Huang%2520and%2520Dasen%2520Dai%2520and%2520Jen-Yuan%2520Huang%2520and%2520Youliang%2520Yuan%2520and%2520Xiaoyuan%2520Liu%2520and%2520Wenxuan%2520Wang%2520and%2520Wenxiang%2520Jiao%2520and%2520Pinjia%2520He%2520and%2520Zhaopeng%2520Tu%2520and%2520Haodong%2520Duan%26entry.1292438233%3D%2520%2520Despite%2520significant%2520progress%2520on%2520popular%2520multimodal%2520benchmarks%252C%250Astate-of-the-art%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520continue%2520to%2520struggle%250Awith%2520basic%2520visual%2520reasoning%2520tasks%2520that%2520are%2520trivially%2520solved%2520by%2520humans%252C%2520such%2520as%250Arecognizing%2520spatial%2520relationships.%2520To%2520systematically%2520investigate%2520this%2520gap%252C%2520we%250Aintroduce%2520VisFactor%252C%2520a%2520benchmark%2520that%2520digitizes%252020%2520vision-centric%2520subtests%2520from%250Aa%2520well-established%2520cognitive%2520psychology%2520assessment.%2520These%2520subtests%2520span%2520four%250Acore%2520domains%2520of%2520human%2520visual%2520cognition%253A%2520%25281%2529%2520Visualization%2520and%2520Spatial%250AProcessing%252C%2520%25282%2529%2520Perceptual%2520and%2520Closure%252C%2520%25283%2529%2520Memory%252C%2520and%2520%25284%2529%2520Reasoning.%2520We%250Aevaluate%252020%2520frontier%2520MLLMs%2520from%2520GPT%252C%2520Gemini%252C%2520Claude%252C%2520LLaMA%252C%2520Qwen%252C%2520and%2520SEED%250Afamilies.%2520The%2520best-performing%2520model%2520achieves%2520a%2520score%2520of%2520only%252025.19%2520out%2520of%2520100%252C%250Awith%2520consistent%2520failures%2520on%2520tasks%2520such%2520as%2520mental%2520rotation%252C%2520spatial%2520relation%250Ainference%252C%2520and%2520figure-ground%2520discrimination%252C%2520regardless%2520of%2520model%2520size%2520or%250Aprompting%2520strategy.%2520These%2520findings%2520suggest%2520that%2520current%2520MLLM%2520performance%2520gains%250Aon%2520high-level%2520benchmarks%2520do%2520not%2520reflect%2520human-like%2520low-level%2520visual%2520cognition%252C%250Achallenging%2520the%2520assumption%2520that%2520large-scale%2520pretraining%2520naturally%2520induces%250Agestalt-like%2520perceptual%2520capabilities.%2520The%2520dataset%2520and%2520evaluation%2520toolkit%2520are%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/CUHK-ARISE/VisFactor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Cognitive%20Benchmarks%20Reveal%20Foundational%20Visual%20Gaps%20in%20MLLMs&entry.906535625=Jen-Tse%20Huang%20and%20Dasen%20Dai%20and%20Jen-Yuan%20Huang%20and%20Youliang%20Yuan%20and%20Xiaoyuan%20Liu%20and%20Wenxuan%20Wang%20and%20Wenxiang%20Jiao%20and%20Pinjia%20He%20and%20Zhaopeng%20Tu%20and%20Haodong%20Duan&entry.1292438233=%20%20Despite%20significant%20progress%20on%20popular%20multimodal%20benchmarks%2C%0Astate-of-the-art%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20continue%20to%20struggle%0Awith%20basic%20visual%20reasoning%20tasks%20that%20are%20trivially%20solved%20by%20humans%2C%20such%20as%0Arecognizing%20spatial%20relationships.%20To%20systematically%20investigate%20this%20gap%2C%20we%0Aintroduce%20VisFactor%2C%20a%20benchmark%20that%20digitizes%2020%20vision-centric%20subtests%20from%0Aa%20well-established%20cognitive%20psychology%20assessment.%20These%20subtests%20span%20four%0Acore%20domains%20of%20human%20visual%20cognition%3A%20%281%29%20Visualization%20and%20Spatial%0AProcessing%2C%20%282%29%20Perceptual%20and%20Closure%2C%20%283%29%20Memory%2C%20and%20%284%29%20Reasoning.%20We%0Aevaluate%2020%20frontier%20MLLMs%20from%20GPT%2C%20Gemini%2C%20Claude%2C%20LLaMA%2C%20Qwen%2C%20and%20SEED%0Afamilies.%20The%20best-performing%20model%20achieves%20a%20score%20of%20only%2025.19%20out%20of%20100%2C%0Awith%20consistent%20failures%20on%20tasks%20such%20as%20mental%20rotation%2C%20spatial%20relation%0Ainference%2C%20and%20figure-ground%20discrimination%2C%20regardless%20of%20model%20size%20or%0Aprompting%20strategy.%20These%20findings%20suggest%20that%20current%20MLLM%20performance%20gains%0Aon%20high-level%20benchmarks%20do%20not%20reflect%20human-like%20low-level%20visual%20cognition%2C%0Achallenging%20the%20assumption%20that%20large-scale%20pretraining%20naturally%20induces%0Agestalt-like%20perceptual%20capabilities.%20The%20dataset%20and%20evaluation%20toolkit%20are%0Apublicly%20available%20at%3A%20https%3A//github.com/CUHK-ARISE/VisFactor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16435v2&entry.124074799=Read"},
{"title": "Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation\n  with Volumetric Visual Data Fusion", "author": "Ke Li and Reinhard Bacher and Susanne Schmidt and Wim Leemans and Frank Steinicke", "abstract": "  We introduce Reality Fusion, a novel robot teleoperation system that\nlocalizes, streams, projects, and merges a typical onboard depth sensor with a\nphotorealistic, high resolution, high framerate, and wide field of view (FoV)\nrendering of the complex remote environment represented as 3D Gaussian splats\n(3DGS). Our framework enables robust egocentric and exocentric robot\nteleoperation in immersive VR, with the 3DGS effectively extending spatial\ninformation of a depth sensor with limited FoV and balancing the trade-off\nbetween data streaming costs and data visual quality. We evaluated our\nframework through a user study with 24 participants, which revealed that\nReality Fusion leads to significantly better user performance, situation\nawareness, and user preferences. To support further research and development,\nwe provide an open-source implementation with an easy-to-replicate custom-made\ntelepresence robot, a high-performance virtual reality 3DGS renderer, and an\nimmersive robot control package. (Source code:\nhttps://github.com/uhhhci/RealityFusion)\n", "link": "http://arxiv.org/abs/2408.01225v2", "date": "2025-08-07", "relevancy": 2.9661, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.629}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5797}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reality%20Fusion%3A%20Robust%20Real-time%20Immersive%20Mobile%20Robot%20Teleoperation%0A%20%20with%20Volumetric%20Visual%20Data%20Fusion&body=Title%3A%20Reality%20Fusion%3A%20Robust%20Real-time%20Immersive%20Mobile%20Robot%20Teleoperation%0A%20%20with%20Volumetric%20Visual%20Data%20Fusion%0AAuthor%3A%20Ke%20Li%20and%20Reinhard%20Bacher%20and%20Susanne%20Schmidt%20and%20Wim%20Leemans%20and%20Frank%20Steinicke%0AAbstract%3A%20%20%20We%20introduce%20Reality%20Fusion%2C%20a%20novel%20robot%20teleoperation%20system%20that%0Alocalizes%2C%20streams%2C%20projects%2C%20and%20merges%20a%20typical%20onboard%20depth%20sensor%20with%20a%0Aphotorealistic%2C%20high%20resolution%2C%20high%20framerate%2C%20and%20wide%20field%20of%20view%20%28FoV%29%0Arendering%20of%20the%20complex%20remote%20environment%20represented%20as%203D%20Gaussian%20splats%0A%283DGS%29.%20Our%20framework%20enables%20robust%20egocentric%20and%20exocentric%20robot%0Ateleoperation%20in%20immersive%20VR%2C%20with%20the%203DGS%20effectively%20extending%20spatial%0Ainformation%20of%20a%20depth%20sensor%20with%20limited%20FoV%20and%20balancing%20the%20trade-off%0Abetween%20data%20streaming%20costs%20and%20data%20visual%20quality.%20We%20evaluated%20our%0Aframework%20through%20a%20user%20study%20with%2024%20participants%2C%20which%20revealed%20that%0AReality%20Fusion%20leads%20to%20significantly%20better%20user%20performance%2C%20situation%0Aawareness%2C%20and%20user%20preferences.%20To%20support%20further%20research%20and%20development%2C%0Awe%20provide%20an%20open-source%20implementation%20with%20an%20easy-to-replicate%20custom-made%0Atelepresence%20robot%2C%20a%20high-performance%20virtual%20reality%203DGS%20renderer%2C%20and%20an%0Aimmersive%20robot%20control%20package.%20%28Source%20code%3A%0Ahttps%3A//github.com/uhhhci/RealityFusion%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReality%2520Fusion%253A%2520Robust%2520Real-time%2520Immersive%2520Mobile%2520Robot%2520Teleoperation%250A%2520%2520with%2520Volumetric%2520Visual%2520Data%2520Fusion%26entry.906535625%3DKe%2520Li%2520and%2520Reinhard%2520Bacher%2520and%2520Susanne%2520Schmidt%2520and%2520Wim%2520Leemans%2520and%2520Frank%2520Steinicke%26entry.1292438233%3D%2520%2520We%2520introduce%2520Reality%2520Fusion%252C%2520a%2520novel%2520robot%2520teleoperation%2520system%2520that%250Alocalizes%252C%2520streams%252C%2520projects%252C%2520and%2520merges%2520a%2520typical%2520onboard%2520depth%2520sensor%2520with%2520a%250Aphotorealistic%252C%2520high%2520resolution%252C%2520high%2520framerate%252C%2520and%2520wide%2520field%2520of%2520view%2520%2528FoV%2529%250Arendering%2520of%2520the%2520complex%2520remote%2520environment%2520represented%2520as%25203D%2520Gaussian%2520splats%250A%25283DGS%2529.%2520Our%2520framework%2520enables%2520robust%2520egocentric%2520and%2520exocentric%2520robot%250Ateleoperation%2520in%2520immersive%2520VR%252C%2520with%2520the%25203DGS%2520effectively%2520extending%2520spatial%250Ainformation%2520of%2520a%2520depth%2520sensor%2520with%2520limited%2520FoV%2520and%2520balancing%2520the%2520trade-off%250Abetween%2520data%2520streaming%2520costs%2520and%2520data%2520visual%2520quality.%2520We%2520evaluated%2520our%250Aframework%2520through%2520a%2520user%2520study%2520with%252024%2520participants%252C%2520which%2520revealed%2520that%250AReality%2520Fusion%2520leads%2520to%2520significantly%2520better%2520user%2520performance%252C%2520situation%250Aawareness%252C%2520and%2520user%2520preferences.%2520To%2520support%2520further%2520research%2520and%2520development%252C%250Awe%2520provide%2520an%2520open-source%2520implementation%2520with%2520an%2520easy-to-replicate%2520custom-made%250Atelepresence%2520robot%252C%2520a%2520high-performance%2520virtual%2520reality%25203DGS%2520renderer%252C%2520and%2520an%250Aimmersive%2520robot%2520control%2520package.%2520%2528Source%2520code%253A%250Ahttps%253A//github.com/uhhhci/RealityFusion%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reality%20Fusion%3A%20Robust%20Real-time%20Immersive%20Mobile%20Robot%20Teleoperation%0A%20%20with%20Volumetric%20Visual%20Data%20Fusion&entry.906535625=Ke%20Li%20and%20Reinhard%20Bacher%20and%20Susanne%20Schmidt%20and%20Wim%20Leemans%20and%20Frank%20Steinicke&entry.1292438233=%20%20We%20introduce%20Reality%20Fusion%2C%20a%20novel%20robot%20teleoperation%20system%20that%0Alocalizes%2C%20streams%2C%20projects%2C%20and%20merges%20a%20typical%20onboard%20depth%20sensor%20with%20a%0Aphotorealistic%2C%20high%20resolution%2C%20high%20framerate%2C%20and%20wide%20field%20of%20view%20%28FoV%29%0Arendering%20of%20the%20complex%20remote%20environment%20represented%20as%203D%20Gaussian%20splats%0A%283DGS%29.%20Our%20framework%20enables%20robust%20egocentric%20and%20exocentric%20robot%0Ateleoperation%20in%20immersive%20VR%2C%20with%20the%203DGS%20effectively%20extending%20spatial%0Ainformation%20of%20a%20depth%20sensor%20with%20limited%20FoV%20and%20balancing%20the%20trade-off%0Abetween%20data%20streaming%20costs%20and%20data%20visual%20quality.%20We%20evaluated%20our%0Aframework%20through%20a%20user%20study%20with%2024%20participants%2C%20which%20revealed%20that%0AReality%20Fusion%20leads%20to%20significantly%20better%20user%20performance%2C%20situation%0Aawareness%2C%20and%20user%20preferences.%20To%20support%20further%20research%20and%20development%2C%0Awe%20provide%20an%20open-source%20implementation%20with%20an%20easy-to-replicate%20custom-made%0Atelepresence%20robot%2C%20a%20high-performance%20virtual%20reality%203DGS%20renderer%2C%20and%20an%0Aimmersive%20robot%20control%20package.%20%28Source%20code%3A%0Ahttps%3A//github.com/uhhhci/RealityFusion%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01225v2&entry.124074799=Read"},
{"title": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data\n  Synthesis", "author": "Kunyu Feng and Yue Ma and Xinhua Zhang and Boshi Liu and Yikuang Yuluo and Yinhan Zhang and Runtao Liu and Hongyu Liu and Zhiyuan Qin and Shanhui Mo and Qifeng Chen and Zeyu Wang", "abstract": "  With the growing demands of AI-generated content (AIGC), the need for\nhigh-quality, diverse, and scalable data has become increasingly crucial.\nHowever, collecting large-scale real-world data remains costly and\ntime-consuming, hindering the development of downstream applications. While\nsome works attempt to collect task-specific data via a rendering process, most\napproaches still rely on manual scene construction, limiting their scalability\nand accuracy. To address these challenges, we propose Follow-Your-Instruction,\na Multimodal Large Language Model (MLLM)-driven framework for automatically\nsynthesizing high-quality 2D, 3D, and 4D data. Our\n\\textbf{Follow-Your-Instruction} first collects assets and their associated\ndescriptions through multimodal inputs using the MLLM-Collector. Then it\nconstructs 3D layouts, and leverages Vision-Language Models (VLMs) for semantic\nrefinement through multi-view scenes with the MLLM-Generator and\nMLLM-Optimizer, respectively. Finally, it uses MLLM-Planner to generate\ntemporally coherent future frames. We evaluate the quality of the generated\ndata through comprehensive experiments on the 2D, 3D, and 4D generative tasks.\nThe results show that our synthetic data significantly boosts the performance\nof existing baseline models, demonstrating Follow-Your-Instruction's potential\nas a scalable and effective data engine for generative intelligence.\n", "link": "http://arxiv.org/abs/2508.05580v1", "date": "2025-08-07", "relevancy": 2.9334, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5997}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Follow-Your-Instruction%3A%20A%20Comprehensive%20MLLM%20Agent%20for%20World%20Data%0A%20%20Synthesis&body=Title%3A%20Follow-Your-Instruction%3A%20A%20Comprehensive%20MLLM%20Agent%20for%20World%20Data%0A%20%20Synthesis%0AAuthor%3A%20Kunyu%20Feng%20and%20Yue%20Ma%20and%20Xinhua%20Zhang%20and%20Boshi%20Liu%20and%20Yikuang%20Yuluo%20and%20Yinhan%20Zhang%20and%20Runtao%20Liu%20and%20Hongyu%20Liu%20and%20Zhiyuan%20Qin%20and%20Shanhui%20Mo%20and%20Qifeng%20Chen%20and%20Zeyu%20Wang%0AAbstract%3A%20%20%20With%20the%20growing%20demands%20of%20AI-generated%20content%20%28AIGC%29%2C%20the%20need%20for%0Ahigh-quality%2C%20diverse%2C%20and%20scalable%20data%20has%20become%20increasingly%20crucial.%0AHowever%2C%20collecting%20large-scale%20real-world%20data%20remains%20costly%20and%0Atime-consuming%2C%20hindering%20the%20development%20of%20downstream%20applications.%20While%0Asome%20works%20attempt%20to%20collect%20task-specific%20data%20via%20a%20rendering%20process%2C%20most%0Aapproaches%20still%20rely%20on%20manual%20scene%20construction%2C%20limiting%20their%20scalability%0Aand%20accuracy.%20To%20address%20these%20challenges%2C%20we%20propose%20Follow-Your-Instruction%2C%0Aa%20Multimodal%20Large%20Language%20Model%20%28MLLM%29-driven%20framework%20for%20automatically%0Asynthesizing%20high-quality%202D%2C%203D%2C%20and%204D%20data.%20Our%0A%5Ctextbf%7BFollow-Your-Instruction%7D%20first%20collects%20assets%20and%20their%20associated%0Adescriptions%20through%20multimodal%20inputs%20using%20the%20MLLM-Collector.%20Then%20it%0Aconstructs%203D%20layouts%2C%20and%20leverages%20Vision-Language%20Models%20%28VLMs%29%20for%20semantic%0Arefinement%20through%20multi-view%20scenes%20with%20the%20MLLM-Generator%20and%0AMLLM-Optimizer%2C%20respectively.%20Finally%2C%20it%20uses%20MLLM-Planner%20to%20generate%0Atemporally%20coherent%20future%20frames.%20We%20evaluate%20the%20quality%20of%20the%20generated%0Adata%20through%20comprehensive%20experiments%20on%20the%202D%2C%203D%2C%20and%204D%20generative%20tasks.%0AThe%20results%20show%20that%20our%20synthetic%20data%20significantly%20boosts%20the%20performance%0Aof%20existing%20baseline%20models%2C%20demonstrating%20Follow-Your-Instruction%27s%20potential%0Aas%20a%20scalable%20and%20effective%20data%20engine%20for%20generative%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFollow-Your-Instruction%253A%2520A%2520Comprehensive%2520MLLM%2520Agent%2520for%2520World%2520Data%250A%2520%2520Synthesis%26entry.906535625%3DKunyu%2520Feng%2520and%2520Yue%2520Ma%2520and%2520Xinhua%2520Zhang%2520and%2520Boshi%2520Liu%2520and%2520Yikuang%2520Yuluo%2520and%2520Yinhan%2520Zhang%2520and%2520Runtao%2520Liu%2520and%2520Hongyu%2520Liu%2520and%2520Zhiyuan%2520Qin%2520and%2520Shanhui%2520Mo%2520and%2520Qifeng%2520Chen%2520and%2520Zeyu%2520Wang%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520demands%2520of%2520AI-generated%2520content%2520%2528AIGC%2529%252C%2520the%2520need%2520for%250Ahigh-quality%252C%2520diverse%252C%2520and%2520scalable%2520data%2520has%2520become%2520increasingly%2520crucial.%250AHowever%252C%2520collecting%2520large-scale%2520real-world%2520data%2520remains%2520costly%2520and%250Atime-consuming%252C%2520hindering%2520the%2520development%2520of%2520downstream%2520applications.%2520While%250Asome%2520works%2520attempt%2520to%2520collect%2520task-specific%2520data%2520via%2520a%2520rendering%2520process%252C%2520most%250Aapproaches%2520still%2520rely%2520on%2520manual%2520scene%2520construction%252C%2520limiting%2520their%2520scalability%250Aand%2520accuracy.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Follow-Your-Instruction%252C%250Aa%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529-driven%2520framework%2520for%2520automatically%250Asynthesizing%2520high-quality%25202D%252C%25203D%252C%2520and%25204D%2520data.%2520Our%250A%255Ctextbf%257BFollow-Your-Instruction%257D%2520first%2520collects%2520assets%2520and%2520their%2520associated%250Adescriptions%2520through%2520multimodal%2520inputs%2520using%2520the%2520MLLM-Collector.%2520Then%2520it%250Aconstructs%25203D%2520layouts%252C%2520and%2520leverages%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520for%2520semantic%250Arefinement%2520through%2520multi-view%2520scenes%2520with%2520the%2520MLLM-Generator%2520and%250AMLLM-Optimizer%252C%2520respectively.%2520Finally%252C%2520it%2520uses%2520MLLM-Planner%2520to%2520generate%250Atemporally%2520coherent%2520future%2520frames.%2520We%2520evaluate%2520the%2520quality%2520of%2520the%2520generated%250Adata%2520through%2520comprehensive%2520experiments%2520on%2520the%25202D%252C%25203D%252C%2520and%25204D%2520generative%2520tasks.%250AThe%2520results%2520show%2520that%2520our%2520synthetic%2520data%2520significantly%2520boosts%2520the%2520performance%250Aof%2520existing%2520baseline%2520models%252C%2520demonstrating%2520Follow-Your-Instruction%2527s%2520potential%250Aas%2520a%2520scalable%2520and%2520effective%2520data%2520engine%2520for%2520generative%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Follow-Your-Instruction%3A%20A%20Comprehensive%20MLLM%20Agent%20for%20World%20Data%0A%20%20Synthesis&entry.906535625=Kunyu%20Feng%20and%20Yue%20Ma%20and%20Xinhua%20Zhang%20and%20Boshi%20Liu%20and%20Yikuang%20Yuluo%20and%20Yinhan%20Zhang%20and%20Runtao%20Liu%20and%20Hongyu%20Liu%20and%20Zhiyuan%20Qin%20and%20Shanhui%20Mo%20and%20Qifeng%20Chen%20and%20Zeyu%20Wang&entry.1292438233=%20%20With%20the%20growing%20demands%20of%20AI-generated%20content%20%28AIGC%29%2C%20the%20need%20for%0Ahigh-quality%2C%20diverse%2C%20and%20scalable%20data%20has%20become%20increasingly%20crucial.%0AHowever%2C%20collecting%20large-scale%20real-world%20data%20remains%20costly%20and%0Atime-consuming%2C%20hindering%20the%20development%20of%20downstream%20applications.%20While%0Asome%20works%20attempt%20to%20collect%20task-specific%20data%20via%20a%20rendering%20process%2C%20most%0Aapproaches%20still%20rely%20on%20manual%20scene%20construction%2C%20limiting%20their%20scalability%0Aand%20accuracy.%20To%20address%20these%20challenges%2C%20we%20propose%20Follow-Your-Instruction%2C%0Aa%20Multimodal%20Large%20Language%20Model%20%28MLLM%29-driven%20framework%20for%20automatically%0Asynthesizing%20high-quality%202D%2C%203D%2C%20and%204D%20data.%20Our%0A%5Ctextbf%7BFollow-Your-Instruction%7D%20first%20collects%20assets%20and%20their%20associated%0Adescriptions%20through%20multimodal%20inputs%20using%20the%20MLLM-Collector.%20Then%20it%0Aconstructs%203D%20layouts%2C%20and%20leverages%20Vision-Language%20Models%20%28VLMs%29%20for%20semantic%0Arefinement%20through%20multi-view%20scenes%20with%20the%20MLLM-Generator%20and%0AMLLM-Optimizer%2C%20respectively.%20Finally%2C%20it%20uses%20MLLM-Planner%20to%20generate%0Atemporally%20coherent%20future%20frames.%20We%20evaluate%20the%20quality%20of%20the%20generated%0Adata%20through%20comprehensive%20experiments%20on%20the%202D%2C%203D%2C%20and%204D%20generative%20tasks.%0AThe%20results%20show%20that%20our%20synthetic%20data%20significantly%20boosts%20the%20performance%0Aof%20existing%20baseline%20models%2C%20demonstrating%20Follow-Your-Instruction%27s%20potential%0Aas%20a%20scalable%20and%20effective%20data%20engine%20for%20generative%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05580v1&entry.124074799=Read"},
{"title": "RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning\n  Pre-trained Model for Medical Image Understanding", "author": "Tianchen Fang and Guiru Liu", "abstract": "  Medical image understanding plays a crucial role in enabling automated\ndiagnosis and data-driven clinical decision support. However, its progress is\nimpeded by two primary challenges: the limited availability of high-quality\nannotated medical data and an overreliance on global image features, which\noften miss subtle but clinically significant pathological regions. To address\nthese issues, we introduce RegionMed-CLIP, a region-aware multimodal\ncontrastive learning framework that explicitly incorporates localized\npathological signals along with holistic semantic representations. The core of\nour method is an innovative region-of-interest (ROI) processor that adaptively\nintegrates fine-grained regional features with the global context, supported by\na progressive training strategy that enhances hierarchical multimodal\nalignment. To enable large-scale region-level representation learning, we\nconstruct MedRegion-500k, a comprehensive medical image-text corpus that\nfeatures extensive regional annotations and multilevel clinical descriptions.\nExtensive experiments on image-text retrieval, zero-shot classification, and\nvisual question answering tasks demonstrate that RegionMed-CLIP consistently\nexceeds state-of-the-art vision language models by a wide margin. Our results\nhighlight the critical importance of region-aware contrastive pre-training and\nposition RegionMed-CLIP as a robust foundation for advancing multimodal medical\nimage understanding.\n", "link": "http://arxiv.org/abs/2508.05244v1", "date": "2025-08-07", "relevancy": 2.8777, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RegionMed-CLIP%3A%20A%20Region-Aware%20Multimodal%20Contrastive%20Learning%0A%20%20Pre-trained%20Model%20for%20Medical%20Image%20Understanding&body=Title%3A%20RegionMed-CLIP%3A%20A%20Region-Aware%20Multimodal%20Contrastive%20Learning%0A%20%20Pre-trained%20Model%20for%20Medical%20Image%20Understanding%0AAuthor%3A%20Tianchen%20Fang%20and%20Guiru%20Liu%0AAbstract%3A%20%20%20Medical%20image%20understanding%20plays%20a%20crucial%20role%20in%20enabling%20automated%0Adiagnosis%20and%20data-driven%20clinical%20decision%20support.%20However%2C%20its%20progress%20is%0Aimpeded%20by%20two%20primary%20challenges%3A%20the%20limited%20availability%20of%20high-quality%0Aannotated%20medical%20data%20and%20an%20overreliance%20on%20global%20image%20features%2C%20which%0Aoften%20miss%20subtle%20but%20clinically%20significant%20pathological%20regions.%20To%20address%0Athese%20issues%2C%20we%20introduce%20RegionMed-CLIP%2C%20a%20region-aware%20multimodal%0Acontrastive%20learning%20framework%20that%20explicitly%20incorporates%20localized%0Apathological%20signals%20along%20with%20holistic%20semantic%20representations.%20The%20core%20of%0Aour%20method%20is%20an%20innovative%20region-of-interest%20%28ROI%29%20processor%20that%20adaptively%0Aintegrates%20fine-grained%20regional%20features%20with%20the%20global%20context%2C%20supported%20by%0Aa%20progressive%20training%20strategy%20that%20enhances%20hierarchical%20multimodal%0Aalignment.%20To%20enable%20large-scale%20region-level%20representation%20learning%2C%20we%0Aconstruct%20MedRegion-500k%2C%20a%20comprehensive%20medical%20image-text%20corpus%20that%0Afeatures%20extensive%20regional%20annotations%20and%20multilevel%20clinical%20descriptions.%0AExtensive%20experiments%20on%20image-text%20retrieval%2C%20zero-shot%20classification%2C%20and%0Avisual%20question%20answering%20tasks%20demonstrate%20that%20RegionMed-CLIP%20consistently%0Aexceeds%20state-of-the-art%20vision%20language%20models%20by%20a%20wide%20margin.%20Our%20results%0Ahighlight%20the%20critical%20importance%20of%20region-aware%20contrastive%20pre-training%20and%0Aposition%20RegionMed-CLIP%20as%20a%20robust%20foundation%20for%20advancing%20multimodal%20medical%0Aimage%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegionMed-CLIP%253A%2520A%2520Region-Aware%2520Multimodal%2520Contrastive%2520Learning%250A%2520%2520Pre-trained%2520Model%2520for%2520Medical%2520Image%2520Understanding%26entry.906535625%3DTianchen%2520Fang%2520and%2520Guiru%2520Liu%26entry.1292438233%3D%2520%2520Medical%2520image%2520understanding%2520plays%2520a%2520crucial%2520role%2520in%2520enabling%2520automated%250Adiagnosis%2520and%2520data-driven%2520clinical%2520decision%2520support.%2520However%252C%2520its%2520progress%2520is%250Aimpeded%2520by%2520two%2520primary%2520challenges%253A%2520the%2520limited%2520availability%2520of%2520high-quality%250Aannotated%2520medical%2520data%2520and%2520an%2520overreliance%2520on%2520global%2520image%2520features%252C%2520which%250Aoften%2520miss%2520subtle%2520but%2520clinically%2520significant%2520pathological%2520regions.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520RegionMed-CLIP%252C%2520a%2520region-aware%2520multimodal%250Acontrastive%2520learning%2520framework%2520that%2520explicitly%2520incorporates%2520localized%250Apathological%2520signals%2520along%2520with%2520holistic%2520semantic%2520representations.%2520The%2520core%2520of%250Aour%2520method%2520is%2520an%2520innovative%2520region-of-interest%2520%2528ROI%2529%2520processor%2520that%2520adaptively%250Aintegrates%2520fine-grained%2520regional%2520features%2520with%2520the%2520global%2520context%252C%2520supported%2520by%250Aa%2520progressive%2520training%2520strategy%2520that%2520enhances%2520hierarchical%2520multimodal%250Aalignment.%2520To%2520enable%2520large-scale%2520region-level%2520representation%2520learning%252C%2520we%250Aconstruct%2520MedRegion-500k%252C%2520a%2520comprehensive%2520medical%2520image-text%2520corpus%2520that%250Afeatures%2520extensive%2520regional%2520annotations%2520and%2520multilevel%2520clinical%2520descriptions.%250AExtensive%2520experiments%2520on%2520image-text%2520retrieval%252C%2520zero-shot%2520classification%252C%2520and%250Avisual%2520question%2520answering%2520tasks%2520demonstrate%2520that%2520RegionMed-CLIP%2520consistently%250Aexceeds%2520state-of-the-art%2520vision%2520language%2520models%2520by%2520a%2520wide%2520margin.%2520Our%2520results%250Ahighlight%2520the%2520critical%2520importance%2520of%2520region-aware%2520contrastive%2520pre-training%2520and%250Aposition%2520RegionMed-CLIP%2520as%2520a%2520robust%2520foundation%2520for%2520advancing%2520multimodal%2520medical%250Aimage%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RegionMed-CLIP%3A%20A%20Region-Aware%20Multimodal%20Contrastive%20Learning%0A%20%20Pre-trained%20Model%20for%20Medical%20Image%20Understanding&entry.906535625=Tianchen%20Fang%20and%20Guiru%20Liu&entry.1292438233=%20%20Medical%20image%20understanding%20plays%20a%20crucial%20role%20in%20enabling%20automated%0Adiagnosis%20and%20data-driven%20clinical%20decision%20support.%20However%2C%20its%20progress%20is%0Aimpeded%20by%20two%20primary%20challenges%3A%20the%20limited%20availability%20of%20high-quality%0Aannotated%20medical%20data%20and%20an%20overreliance%20on%20global%20image%20features%2C%20which%0Aoften%20miss%20subtle%20but%20clinically%20significant%20pathological%20regions.%20To%20address%0Athese%20issues%2C%20we%20introduce%20RegionMed-CLIP%2C%20a%20region-aware%20multimodal%0Acontrastive%20learning%20framework%20that%20explicitly%20incorporates%20localized%0Apathological%20signals%20along%20with%20holistic%20semantic%20representations.%20The%20core%20of%0Aour%20method%20is%20an%20innovative%20region-of-interest%20%28ROI%29%20processor%20that%20adaptively%0Aintegrates%20fine-grained%20regional%20features%20with%20the%20global%20context%2C%20supported%20by%0Aa%20progressive%20training%20strategy%20that%20enhances%20hierarchical%20multimodal%0Aalignment.%20To%20enable%20large-scale%20region-level%20representation%20learning%2C%20we%0Aconstruct%20MedRegion-500k%2C%20a%20comprehensive%20medical%20image-text%20corpus%20that%0Afeatures%20extensive%20regional%20annotations%20and%20multilevel%20clinical%20descriptions.%0AExtensive%20experiments%20on%20image-text%20retrieval%2C%20zero-shot%20classification%2C%20and%0Avisual%20question%20answering%20tasks%20demonstrate%20that%20RegionMed-CLIP%20consistently%0Aexceeds%20state-of-the-art%20vision%20language%20models%20by%20a%20wide%20margin.%20Our%20results%0Ahighlight%20the%20critical%20importance%20of%20region-aware%20contrastive%20pre-training%20and%0Aposition%20RegionMed-CLIP%20as%20a%20robust%20foundation%20for%20advancing%20multimodal%20medical%0Aimage%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05244v1&entry.124074799=Read"},
{"title": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo", "author": "Qianli Ma and Yaowei Zheng and Zhelun Shi and Zhongkai Zhao and Bin Jia and Ziyue Huang and Zhiqi Lin and Youjie Li and Jiacheng Yang and Yanghua Peng and Zhi Zhang and Xin Liu", "abstract": "  Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.\n", "link": "http://arxiv.org/abs/2508.02317v3", "date": "2025-08-07", "relevancy": 2.8728, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VeOmni%3A%20Scaling%20Any%20Modality%20Model%20Training%20with%20Model-Centric%0A%20%20Distributed%20Recipe%20Zoo&body=Title%3A%20VeOmni%3A%20Scaling%20Any%20Modality%20Model%20Training%20with%20Model-Centric%0A%20%20Distributed%20Recipe%20Zoo%0AAuthor%3A%20Qianli%20Ma%20and%20Yaowei%20Zheng%20and%20Zhelun%20Shi%20and%20Zhongkai%20Zhao%20and%20Bin%20Jia%20and%20Ziyue%20Huang%20and%20Zhiqi%20Lin%20and%20Youjie%20Li%20and%20Jiacheng%20Yang%20and%20Yanghua%20Peng%20and%20Zhi%20Zhang%20and%20Xin%20Liu%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20driven%20impressive%0Aprogress%20in%20omni-modal%20understanding%20and%20generation.%20However%2C%20training%0Aomni-modal%20LLMs%20remains%20a%20significant%20challenge%20due%20to%20the%20heterogeneous%20model%0Aarchitectures%20required%20to%20process%20diverse%20modalities%2C%20necessitating%0Asophisticated%20system%20design%20for%20efficient%20large-scale%20training.%20Existing%0Aframeworks%20typically%20entangle%20model%20definition%20with%20parallel%20logic%2C%20incurring%0Alimited%20scalability%20and%20substantial%20engineering%20overhead%20for%20end-to-end%0Aomni-modal%20training.%20We%20present%20VeOmni%2C%20a%20modular%20and%20efficient%20training%0Aframework%20to%20accelerate%20the%20development%20of%20omni-modal%20LLMs.%20VeOmni%20introduces%0Amodel-centric%20distributed%20recipes%20that%20decouples%20communication%20from%0Acomputation%2C%20enabling%20efficient%203D%20parallelism%20on%20omni-modal%20LLMs.%20VeOmni%20also%0Afeatures%20a%20flexible%20configuration%20interface%20supporting%20seamless%20integration%20of%0Anew%20modalities%20with%20minimal%20code%20change.%20Using%20VeOmni%2C%20a%20omni-modal%0Amixture-of-experts%20%28MoE%29%20model%20with%2030B%20parameters%20can%20be%20trained%20with%20over%0A2%2C800%20tokens/sec/GPU%20throughput%20and%20scale%20to%20160K%20context%20lengths%20via%203D%0Aparallelism%20on%20128%20GPUs%2C%20showcasing%20its%20superior%20efficiency%20and%20scalability%20for%0Atraining%20large%20omni-modal%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02317v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeOmni%253A%2520Scaling%2520Any%2520Modality%2520Model%2520Training%2520with%2520Model-Centric%250A%2520%2520Distributed%2520Recipe%2520Zoo%26entry.906535625%3DQianli%2520Ma%2520and%2520Yaowei%2520Zheng%2520and%2520Zhelun%2520Shi%2520and%2520Zhongkai%2520Zhao%2520and%2520Bin%2520Jia%2520and%2520Ziyue%2520Huang%2520and%2520Zhiqi%2520Lin%2520and%2520Youjie%2520Li%2520and%2520Jiacheng%2520Yang%2520and%2520Yanghua%2520Peng%2520and%2520Zhi%2520Zhang%2520and%2520Xin%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520driven%2520impressive%250Aprogress%2520in%2520omni-modal%2520understanding%2520and%2520generation.%2520However%252C%2520training%250Aomni-modal%2520LLMs%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520heterogeneous%2520model%250Aarchitectures%2520required%2520to%2520process%2520diverse%2520modalities%252C%2520necessitating%250Asophisticated%2520system%2520design%2520for%2520efficient%2520large-scale%2520training.%2520Existing%250Aframeworks%2520typically%2520entangle%2520model%2520definition%2520with%2520parallel%2520logic%252C%2520incurring%250Alimited%2520scalability%2520and%2520substantial%2520engineering%2520overhead%2520for%2520end-to-end%250Aomni-modal%2520training.%2520We%2520present%2520VeOmni%252C%2520a%2520modular%2520and%2520efficient%2520training%250Aframework%2520to%2520accelerate%2520the%2520development%2520of%2520omni-modal%2520LLMs.%2520VeOmni%2520introduces%250Amodel-centric%2520distributed%2520recipes%2520that%2520decouples%2520communication%2520from%250Acomputation%252C%2520enabling%2520efficient%25203D%2520parallelism%2520on%2520omni-modal%2520LLMs.%2520VeOmni%2520also%250Afeatures%2520a%2520flexible%2520configuration%2520interface%2520supporting%2520seamless%2520integration%2520of%250Anew%2520modalities%2520with%2520minimal%2520code%2520change.%2520Using%2520VeOmni%252C%2520a%2520omni-modal%250Amixture-of-experts%2520%2528MoE%2529%2520model%2520with%252030B%2520parameters%2520can%2520be%2520trained%2520with%2520over%250A2%252C800%2520tokens/sec/GPU%2520throughput%2520and%2520scale%2520to%2520160K%2520context%2520lengths%2520via%25203D%250Aparallelism%2520on%2520128%2520GPUs%252C%2520showcasing%2520its%2520superior%2520efficiency%2520and%2520scalability%2520for%250Atraining%2520large%2520omni-modal%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02317v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VeOmni%3A%20Scaling%20Any%20Modality%20Model%20Training%20with%20Model-Centric%0A%20%20Distributed%20Recipe%20Zoo&entry.906535625=Qianli%20Ma%20and%20Yaowei%20Zheng%20and%20Zhelun%20Shi%20and%20Zhongkai%20Zhao%20and%20Bin%20Jia%20and%20Ziyue%20Huang%20and%20Zhiqi%20Lin%20and%20Youjie%20Li%20and%20Jiacheng%20Yang%20and%20Yanghua%20Peng%20and%20Zhi%20Zhang%20and%20Xin%20Liu&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20driven%20impressive%0Aprogress%20in%20omni-modal%20understanding%20and%20generation.%20However%2C%20training%0Aomni-modal%20LLMs%20remains%20a%20significant%20challenge%20due%20to%20the%20heterogeneous%20model%0Aarchitectures%20required%20to%20process%20diverse%20modalities%2C%20necessitating%0Asophisticated%20system%20design%20for%20efficient%20large-scale%20training.%20Existing%0Aframeworks%20typically%20entangle%20model%20definition%20with%20parallel%20logic%2C%20incurring%0Alimited%20scalability%20and%20substantial%20engineering%20overhead%20for%20end-to-end%0Aomni-modal%20training.%20We%20present%20VeOmni%2C%20a%20modular%20and%20efficient%20training%0Aframework%20to%20accelerate%20the%20development%20of%20omni-modal%20LLMs.%20VeOmni%20introduces%0Amodel-centric%20distributed%20recipes%20that%20decouples%20communication%20from%0Acomputation%2C%20enabling%20efficient%203D%20parallelism%20on%20omni-modal%20LLMs.%20VeOmni%20also%0Afeatures%20a%20flexible%20configuration%20interface%20supporting%20seamless%20integration%20of%0Anew%20modalities%20with%20minimal%20code%20change.%20Using%20VeOmni%2C%20a%20omni-modal%0Amixture-of-experts%20%28MoE%29%20model%20with%2030B%20parameters%20can%20be%20trained%20with%20over%0A2%2C800%20tokens/sec/GPU%20throughput%20and%20scale%20to%20160K%20context%20lengths%20via%203D%0Aparallelism%20on%20128%20GPUs%2C%20showcasing%20its%20superior%20efficiency%20and%20scalability%20for%0Atraining%20large%20omni-modal%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02317v3&entry.124074799=Read"},
{"title": "Learning Only with Images: Visual Reinforcement Learning with Reasoning,\n  Rendering, and Visual Feedback", "author": "Yang Chen and Yufan Shen and Wenxuan Huang and Sheng Zhou and Qunshu Lin and Xinyu Cai and Zhi Yu and Jiajun Bu and Botian Shi and Yu Qiao", "abstract": "  Multimodal Large Language Models (MLLMs) exhibit impressive performance\nacross various visual tasks. Subsequent investigations into enhancing their\nvisual reasoning abilities have significantly expanded their performance\nenvelope. However, a critical bottleneck in the advancement of MLLMs toward\ndeep visual reasoning is their heavy reliance on curated image-text\nsupervision. To solve this problem, we introduce a novel framework,\n``Reasoning-Rendering-Visual-Feedback'' (RRVF), that enables MLLMs to learn\ncomplex visual reasoning from only raw images. This framework builds on the\n``Asymmetry of Verification'' principle, i.e., verifying the rendered output\nagainst the source image is substantially easier than performing deep visual\nreasoning to generate a faithful, structured representation such as code. We\ndemonstrate that this relative ease provides an ideal reward signal for\noptimization via Reinforcement Learning (RL), thereby reducing reliance on\nimage-text supervision. RRVF implements a closed-loop iterative process\nencompassing reasoning, rendering, and visual feedback components, enabling the\nmodel to perform complex reasoning, including self-correction through\nmulti-turn interactions. This process is optimized end-to-end using the GRPO\nalgorithm. Extensive evaluations are conducted on image-to-code generation\nacross two diverse domains: data charts and web interfaces. The RRVF-trained\nmodel not only outperforms existing similarly sized open-source MLLMs and\nsupervised fine-tuning baselines but also exhibits superior generalization.\nNotably, the model outperforms the more advanced MLLM used to generate visual\nfeedback during training. Code is available at https://github.com/L-O-I/RRVF.\n", "link": "http://arxiv.org/abs/2507.20766v4", "date": "2025-08-07", "relevancy": 2.8652, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5836}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Only%20with%20Images%3A%20Visual%20Reinforcement%20Learning%20with%20Reasoning%2C%0A%20%20Rendering%2C%20and%20Visual%20Feedback&body=Title%3A%20Learning%20Only%20with%20Images%3A%20Visual%20Reinforcement%20Learning%20with%20Reasoning%2C%0A%20%20Rendering%2C%20and%20Visual%20Feedback%0AAuthor%3A%20Yang%20Chen%20and%20Yufan%20Shen%20and%20Wenxuan%20Huang%20and%20Sheng%20Zhou%20and%20Qunshu%20Lin%20and%20Xinyu%20Cai%20and%20Zhi%20Yu%20and%20Jiajun%20Bu%20and%20Botian%20Shi%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20exhibit%20impressive%20performance%0Aacross%20various%20visual%20tasks.%20Subsequent%20investigations%20into%20enhancing%20their%0Avisual%20reasoning%20abilities%20have%20significantly%20expanded%20their%20performance%0Aenvelope.%20However%2C%20a%20critical%20bottleneck%20in%20the%20advancement%20of%20MLLMs%20toward%0Adeep%20visual%20reasoning%20is%20their%20heavy%20reliance%20on%20curated%20image-text%0Asupervision.%20To%20solve%20this%20problem%2C%20we%20introduce%20a%20novel%20framework%2C%0A%60%60Reasoning-Rendering-Visual-Feedback%27%27%20%28RRVF%29%2C%20that%20enables%20MLLMs%20to%20learn%0Acomplex%20visual%20reasoning%20from%20only%20raw%20images.%20This%20framework%20builds%20on%20the%0A%60%60Asymmetry%20of%20Verification%27%27%20principle%2C%20i.e.%2C%20verifying%20the%20rendered%20output%0Aagainst%20the%20source%20image%20is%20substantially%20easier%20than%20performing%20deep%20visual%0Areasoning%20to%20generate%20a%20faithful%2C%20structured%20representation%20such%20as%20code.%20We%0Ademonstrate%20that%20this%20relative%20ease%20provides%20an%20ideal%20reward%20signal%20for%0Aoptimization%20via%20Reinforcement%20Learning%20%28RL%29%2C%20thereby%20reducing%20reliance%20on%0Aimage-text%20supervision.%20RRVF%20implements%20a%20closed-loop%20iterative%20process%0Aencompassing%20reasoning%2C%20rendering%2C%20and%20visual%20feedback%20components%2C%20enabling%20the%0Amodel%20to%20perform%20complex%20reasoning%2C%20including%20self-correction%20through%0Amulti-turn%20interactions.%20This%20process%20is%20optimized%20end-to-end%20using%20the%20GRPO%0Aalgorithm.%20Extensive%20evaluations%20are%20conducted%20on%20image-to-code%20generation%0Aacross%20two%20diverse%20domains%3A%20data%20charts%20and%20web%20interfaces.%20The%20RRVF-trained%0Amodel%20not%20only%20outperforms%20existing%20similarly%20sized%20open-source%20MLLMs%20and%0Asupervised%20fine-tuning%20baselines%20but%20also%20exhibits%20superior%20generalization.%0ANotably%2C%20the%20model%20outperforms%20the%20more%20advanced%20MLLM%20used%20to%20generate%20visual%0Afeedback%20during%20training.%20Code%20is%20available%20at%20https%3A//github.com/L-O-I/RRVF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20766v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Only%2520with%2520Images%253A%2520Visual%2520Reinforcement%2520Learning%2520with%2520Reasoning%252C%250A%2520%2520Rendering%252C%2520and%2520Visual%2520Feedback%26entry.906535625%3DYang%2520Chen%2520and%2520Yufan%2520Shen%2520and%2520Wenxuan%2520Huang%2520and%2520Sheng%2520Zhou%2520and%2520Qunshu%2520Lin%2520and%2520Xinyu%2520Cai%2520and%2520Zhi%2520Yu%2520and%2520Jiajun%2520Bu%2520and%2520Botian%2520Shi%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520exhibit%2520impressive%2520performance%250Aacross%2520various%2520visual%2520tasks.%2520Subsequent%2520investigations%2520into%2520enhancing%2520their%250Avisual%2520reasoning%2520abilities%2520have%2520significantly%2520expanded%2520their%2520performance%250Aenvelope.%2520However%252C%2520a%2520critical%2520bottleneck%2520in%2520the%2520advancement%2520of%2520MLLMs%2520toward%250Adeep%2520visual%2520reasoning%2520is%2520their%2520heavy%2520reliance%2520on%2520curated%2520image-text%250Asupervision.%2520To%2520solve%2520this%2520problem%252C%2520we%2520introduce%2520a%2520novel%2520framework%252C%250A%2560%2560Reasoning-Rendering-Visual-Feedback%2527%2527%2520%2528RRVF%2529%252C%2520that%2520enables%2520MLLMs%2520to%2520learn%250Acomplex%2520visual%2520reasoning%2520from%2520only%2520raw%2520images.%2520This%2520framework%2520builds%2520on%2520the%250A%2560%2560Asymmetry%2520of%2520Verification%2527%2527%2520principle%252C%2520i.e.%252C%2520verifying%2520the%2520rendered%2520output%250Aagainst%2520the%2520source%2520image%2520is%2520substantially%2520easier%2520than%2520performing%2520deep%2520visual%250Areasoning%2520to%2520generate%2520a%2520faithful%252C%2520structured%2520representation%2520such%2520as%2520code.%2520We%250Ademonstrate%2520that%2520this%2520relative%2520ease%2520provides%2520an%2520ideal%2520reward%2520signal%2520for%250Aoptimization%2520via%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520thereby%2520reducing%2520reliance%2520on%250Aimage-text%2520supervision.%2520RRVF%2520implements%2520a%2520closed-loop%2520iterative%2520process%250Aencompassing%2520reasoning%252C%2520rendering%252C%2520and%2520visual%2520feedback%2520components%252C%2520enabling%2520the%250Amodel%2520to%2520perform%2520complex%2520reasoning%252C%2520including%2520self-correction%2520through%250Amulti-turn%2520interactions.%2520This%2520process%2520is%2520optimized%2520end-to-end%2520using%2520the%2520GRPO%250Aalgorithm.%2520Extensive%2520evaluations%2520are%2520conducted%2520on%2520image-to-code%2520generation%250Aacross%2520two%2520diverse%2520domains%253A%2520data%2520charts%2520and%2520web%2520interfaces.%2520The%2520RRVF-trained%250Amodel%2520not%2520only%2520outperforms%2520existing%2520similarly%2520sized%2520open-source%2520MLLMs%2520and%250Asupervised%2520fine-tuning%2520baselines%2520but%2520also%2520exhibits%2520superior%2520generalization.%250ANotably%252C%2520the%2520model%2520outperforms%2520the%2520more%2520advanced%2520MLLM%2520used%2520to%2520generate%2520visual%250Afeedback%2520during%2520training.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/L-O-I/RRVF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20766v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Only%20with%20Images%3A%20Visual%20Reinforcement%20Learning%20with%20Reasoning%2C%0A%20%20Rendering%2C%20and%20Visual%20Feedback&entry.906535625=Yang%20Chen%20and%20Yufan%20Shen%20and%20Wenxuan%20Huang%20and%20Sheng%20Zhou%20and%20Qunshu%20Lin%20and%20Xinyu%20Cai%20and%20Zhi%20Yu%20and%20Jiajun%20Bu%20and%20Botian%20Shi%20and%20Yu%20Qiao&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20exhibit%20impressive%20performance%0Aacross%20various%20visual%20tasks.%20Subsequent%20investigations%20into%20enhancing%20their%0Avisual%20reasoning%20abilities%20have%20significantly%20expanded%20their%20performance%0Aenvelope.%20However%2C%20a%20critical%20bottleneck%20in%20the%20advancement%20of%20MLLMs%20toward%0Adeep%20visual%20reasoning%20is%20their%20heavy%20reliance%20on%20curated%20image-text%0Asupervision.%20To%20solve%20this%20problem%2C%20we%20introduce%20a%20novel%20framework%2C%0A%60%60Reasoning-Rendering-Visual-Feedback%27%27%20%28RRVF%29%2C%20that%20enables%20MLLMs%20to%20learn%0Acomplex%20visual%20reasoning%20from%20only%20raw%20images.%20This%20framework%20builds%20on%20the%0A%60%60Asymmetry%20of%20Verification%27%27%20principle%2C%20i.e.%2C%20verifying%20the%20rendered%20output%0Aagainst%20the%20source%20image%20is%20substantially%20easier%20than%20performing%20deep%20visual%0Areasoning%20to%20generate%20a%20faithful%2C%20structured%20representation%20such%20as%20code.%20We%0Ademonstrate%20that%20this%20relative%20ease%20provides%20an%20ideal%20reward%20signal%20for%0Aoptimization%20via%20Reinforcement%20Learning%20%28RL%29%2C%20thereby%20reducing%20reliance%20on%0Aimage-text%20supervision.%20RRVF%20implements%20a%20closed-loop%20iterative%20process%0Aencompassing%20reasoning%2C%20rendering%2C%20and%20visual%20feedback%20components%2C%20enabling%20the%0Amodel%20to%20perform%20complex%20reasoning%2C%20including%20self-correction%20through%0Amulti-turn%20interactions.%20This%20process%20is%20optimized%20end-to-end%20using%20the%20GRPO%0Aalgorithm.%20Extensive%20evaluations%20are%20conducted%20on%20image-to-code%20generation%0Aacross%20two%20diverse%20domains%3A%20data%20charts%20and%20web%20interfaces.%20The%20RRVF-trained%0Amodel%20not%20only%20outperforms%20existing%20similarly%20sized%20open-source%20MLLMs%20and%0Asupervised%20fine-tuning%20baselines%20but%20also%20exhibits%20superior%20generalization.%0ANotably%2C%20the%20model%20outperforms%20the%20more%20advanced%20MLLM%20used%20to%20generate%20visual%0Afeedback%20during%20training.%20Code%20is%20available%20at%20https%3A//github.com/L-O-I/RRVF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20766v4&entry.124074799=Read"},
{"title": "CountingFruit: Language-Guided 3D Fruit Counting with Semantic Gaussian\n  Splatting", "author": "Fengze Li and Yangle Liu and Jieming Ma and Hai-Ning Liang and Yaochun Shen and Huangxiang Li and Zhijing Wu", "abstract": "  Accurate 3D fruit counting in orchards is challenging due to heavy occlusion,\nsemantic ambiguity between fruits and surrounding structures, and the high\ncomputational cost of volumetric reconstruction. Existing pipelines often rely\non multi-view 2D segmentation and dense volumetric sampling, which lead to\naccumulated fusion errors and slow inference. We introduce FruitLangGS, a\nlanguage-guided 3D fruit counting framework that reconstructs orchard-scale\nscenes using an adaptive-density Gaussian Splatting pipeline with radius-aware\npruning and tile-based rasterization, enabling scalable 3D representation.\nDuring inference, compressed CLIP-aligned semantic vectors embedded in each\nGaussian are filtered via a dual-threshold cosine similarity mechanism,\nretrieving Gaussians relevant to target prompts while suppressing common\ndistractors (e.g., foliage), without requiring retraining or image-space masks.\nThe selected Gaussians are then sampled into dense point clouds and clustered\ngeometrically to estimate fruit instances, remaining robust under severe\nocclusion and viewpoint variation. Experiments on nine different orchard-scale\ndatasets demonstrate that FruitLangGS consistently outperforms existing\npipelines in instance counting recall, avoiding multi-view segmentation fusion\nerrors and achieving up to 99.7% recall on Pfuji-Size_Orch2018 orchard dataset.\nAblation studies further confirm that language-conditioned semantic embedding\nand dual-threshold prompt filtering are essential for suppressing distractors\nand improving counting accuracy under heavy occlusion. Beyond fruit counting,\nthe same framework enables prompt-driven 3D semantic retrieval without\nretraining, highlighting the potential of language-guided 3D perception for\nscalable agricultural scene understanding.\n", "link": "http://arxiv.org/abs/2506.01109v3", "date": "2025-08-07", "relevancy": 2.8398, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.597}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5772}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CountingFruit%3A%20Language-Guided%203D%20Fruit%20Counting%20with%20Semantic%20Gaussian%0A%20%20Splatting&body=Title%3A%20CountingFruit%3A%20Language-Guided%203D%20Fruit%20Counting%20with%20Semantic%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Fengze%20Li%20and%20Yangle%20Liu%20and%20Jieming%20Ma%20and%20Hai-Ning%20Liang%20and%20Yaochun%20Shen%20and%20Huangxiang%20Li%20and%20Zhijing%20Wu%0AAbstract%3A%20%20%20Accurate%203D%20fruit%20counting%20in%20orchards%20is%20challenging%20due%20to%20heavy%20occlusion%2C%0Asemantic%20ambiguity%20between%20fruits%20and%20surrounding%20structures%2C%20and%20the%20high%0Acomputational%20cost%20of%20volumetric%20reconstruction.%20Existing%20pipelines%20often%20rely%0Aon%20multi-view%202D%20segmentation%20and%20dense%20volumetric%20sampling%2C%20which%20lead%20to%0Aaccumulated%20fusion%20errors%20and%20slow%20inference.%20We%20introduce%20FruitLangGS%2C%20a%0Alanguage-guided%203D%20fruit%20counting%20framework%20that%20reconstructs%20orchard-scale%0Ascenes%20using%20an%20adaptive-density%20Gaussian%20Splatting%20pipeline%20with%20radius-aware%0Apruning%20and%20tile-based%20rasterization%2C%20enabling%20scalable%203D%20representation.%0ADuring%20inference%2C%20compressed%20CLIP-aligned%20semantic%20vectors%20embedded%20in%20each%0AGaussian%20are%20filtered%20via%20a%20dual-threshold%20cosine%20similarity%20mechanism%2C%0Aretrieving%20Gaussians%20relevant%20to%20target%20prompts%20while%20suppressing%20common%0Adistractors%20%28e.g.%2C%20foliage%29%2C%20without%20requiring%20retraining%20or%20image-space%20masks.%0AThe%20selected%20Gaussians%20are%20then%20sampled%20into%20dense%20point%20clouds%20and%20clustered%0Ageometrically%20to%20estimate%20fruit%20instances%2C%20remaining%20robust%20under%20severe%0Aocclusion%20and%20viewpoint%20variation.%20Experiments%20on%20nine%20different%20orchard-scale%0Adatasets%20demonstrate%20that%20FruitLangGS%20consistently%20outperforms%20existing%0Apipelines%20in%20instance%20counting%20recall%2C%20avoiding%20multi-view%20segmentation%20fusion%0Aerrors%20and%20achieving%20up%20to%2099.7%25%20recall%20on%20Pfuji-Size_Orch2018%20orchard%20dataset.%0AAblation%20studies%20further%20confirm%20that%20language-conditioned%20semantic%20embedding%0Aand%20dual-threshold%20prompt%20filtering%20are%20essential%20for%20suppressing%20distractors%0Aand%20improving%20counting%20accuracy%20under%20heavy%20occlusion.%20Beyond%20fruit%20counting%2C%0Athe%20same%20framework%20enables%20prompt-driven%203D%20semantic%20retrieval%20without%0Aretraining%2C%20highlighting%20the%20potential%20of%20language-guided%203D%20perception%20for%0Ascalable%20agricultural%20scene%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01109v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCountingFruit%253A%2520Language-Guided%25203D%2520Fruit%2520Counting%2520with%2520Semantic%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DFengze%2520Li%2520and%2520Yangle%2520Liu%2520and%2520Jieming%2520Ma%2520and%2520Hai-Ning%2520Liang%2520and%2520Yaochun%2520Shen%2520and%2520Huangxiang%2520Li%2520and%2520Zhijing%2520Wu%26entry.1292438233%3D%2520%2520Accurate%25203D%2520fruit%2520counting%2520in%2520orchards%2520is%2520challenging%2520due%2520to%2520heavy%2520occlusion%252C%250Asemantic%2520ambiguity%2520between%2520fruits%2520and%2520surrounding%2520structures%252C%2520and%2520the%2520high%250Acomputational%2520cost%2520of%2520volumetric%2520reconstruction.%2520Existing%2520pipelines%2520often%2520rely%250Aon%2520multi-view%25202D%2520segmentation%2520and%2520dense%2520volumetric%2520sampling%252C%2520which%2520lead%2520to%250Aaccumulated%2520fusion%2520errors%2520and%2520slow%2520inference.%2520We%2520introduce%2520FruitLangGS%252C%2520a%250Alanguage-guided%25203D%2520fruit%2520counting%2520framework%2520that%2520reconstructs%2520orchard-scale%250Ascenes%2520using%2520an%2520adaptive-density%2520Gaussian%2520Splatting%2520pipeline%2520with%2520radius-aware%250Apruning%2520and%2520tile-based%2520rasterization%252C%2520enabling%2520scalable%25203D%2520representation.%250ADuring%2520inference%252C%2520compressed%2520CLIP-aligned%2520semantic%2520vectors%2520embedded%2520in%2520each%250AGaussian%2520are%2520filtered%2520via%2520a%2520dual-threshold%2520cosine%2520similarity%2520mechanism%252C%250Aretrieving%2520Gaussians%2520relevant%2520to%2520target%2520prompts%2520while%2520suppressing%2520common%250Adistractors%2520%2528e.g.%252C%2520foliage%2529%252C%2520without%2520requiring%2520retraining%2520or%2520image-space%2520masks.%250AThe%2520selected%2520Gaussians%2520are%2520then%2520sampled%2520into%2520dense%2520point%2520clouds%2520and%2520clustered%250Ageometrically%2520to%2520estimate%2520fruit%2520instances%252C%2520remaining%2520robust%2520under%2520severe%250Aocclusion%2520and%2520viewpoint%2520variation.%2520Experiments%2520on%2520nine%2520different%2520orchard-scale%250Adatasets%2520demonstrate%2520that%2520FruitLangGS%2520consistently%2520outperforms%2520existing%250Apipelines%2520in%2520instance%2520counting%2520recall%252C%2520avoiding%2520multi-view%2520segmentation%2520fusion%250Aerrors%2520and%2520achieving%2520up%2520to%252099.7%2525%2520recall%2520on%2520Pfuji-Size_Orch2018%2520orchard%2520dataset.%250AAblation%2520studies%2520further%2520confirm%2520that%2520language-conditioned%2520semantic%2520embedding%250Aand%2520dual-threshold%2520prompt%2520filtering%2520are%2520essential%2520for%2520suppressing%2520distractors%250Aand%2520improving%2520counting%2520accuracy%2520under%2520heavy%2520occlusion.%2520Beyond%2520fruit%2520counting%252C%250Athe%2520same%2520framework%2520enables%2520prompt-driven%25203D%2520semantic%2520retrieval%2520without%250Aretraining%252C%2520highlighting%2520the%2520potential%2520of%2520language-guided%25203D%2520perception%2520for%250Ascalable%2520agricultural%2520scene%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01109v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CountingFruit%3A%20Language-Guided%203D%20Fruit%20Counting%20with%20Semantic%20Gaussian%0A%20%20Splatting&entry.906535625=Fengze%20Li%20and%20Yangle%20Liu%20and%20Jieming%20Ma%20and%20Hai-Ning%20Liang%20and%20Yaochun%20Shen%20and%20Huangxiang%20Li%20and%20Zhijing%20Wu&entry.1292438233=%20%20Accurate%203D%20fruit%20counting%20in%20orchards%20is%20challenging%20due%20to%20heavy%20occlusion%2C%0Asemantic%20ambiguity%20between%20fruits%20and%20surrounding%20structures%2C%20and%20the%20high%0Acomputational%20cost%20of%20volumetric%20reconstruction.%20Existing%20pipelines%20often%20rely%0Aon%20multi-view%202D%20segmentation%20and%20dense%20volumetric%20sampling%2C%20which%20lead%20to%0Aaccumulated%20fusion%20errors%20and%20slow%20inference.%20We%20introduce%20FruitLangGS%2C%20a%0Alanguage-guided%203D%20fruit%20counting%20framework%20that%20reconstructs%20orchard-scale%0Ascenes%20using%20an%20adaptive-density%20Gaussian%20Splatting%20pipeline%20with%20radius-aware%0Apruning%20and%20tile-based%20rasterization%2C%20enabling%20scalable%203D%20representation.%0ADuring%20inference%2C%20compressed%20CLIP-aligned%20semantic%20vectors%20embedded%20in%20each%0AGaussian%20are%20filtered%20via%20a%20dual-threshold%20cosine%20similarity%20mechanism%2C%0Aretrieving%20Gaussians%20relevant%20to%20target%20prompts%20while%20suppressing%20common%0Adistractors%20%28e.g.%2C%20foliage%29%2C%20without%20requiring%20retraining%20or%20image-space%20masks.%0AThe%20selected%20Gaussians%20are%20then%20sampled%20into%20dense%20point%20clouds%20and%20clustered%0Ageometrically%20to%20estimate%20fruit%20instances%2C%20remaining%20robust%20under%20severe%0Aocclusion%20and%20viewpoint%20variation.%20Experiments%20on%20nine%20different%20orchard-scale%0Adatasets%20demonstrate%20that%20FruitLangGS%20consistently%20outperforms%20existing%0Apipelines%20in%20instance%20counting%20recall%2C%20avoiding%20multi-view%20segmentation%20fusion%0Aerrors%20and%20achieving%20up%20to%2099.7%25%20recall%20on%20Pfuji-Size_Orch2018%20orchard%20dataset.%0AAblation%20studies%20further%20confirm%20that%20language-conditioned%20semantic%20embedding%0Aand%20dual-threshold%20prompt%20filtering%20are%20essential%20for%20suppressing%20distractors%0Aand%20improving%20counting%20accuracy%20under%20heavy%20occlusion.%20Beyond%20fruit%20counting%2C%0Athe%20same%20framework%20enables%20prompt-driven%203D%20semantic%20retrieval%20without%0Aretraining%2C%20highlighting%20the%20potential%20of%20language-guided%203D%20perception%20for%0Ascalable%20agricultural%20scene%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01109v3&entry.124074799=Read"},
{"title": "Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable\n  Rewards", "author": "Aybora Koksal and A. Aydin Alatan", "abstract": "  Recent advances in large language and vision-language models have enabled\nstrong reasoning capabilities, yet they remain impractical for specialized\ndomains like remote sensing, where annotated data is scarce and expensive. We\npresent the first few-shot reinforcement learning with verifiable reward (RLVR)\nframework for satellite imagery that eliminates the need for caption\nsupervision--relying solely on lightweight, rule-based binary or IoU-based\nrewards. Adapting the \"1-shot RLVR\" paradigm from language models to\nvision-language models, we employ policy-gradient optimization with as few as\none curated example to align model outputs for satellite reasoning tasks.\nComprehensive experiments across multiple remote sensing benchmarks--including\nclassification, visual question answering, and grounding--show that even a\nsingle example yields substantial improvements over the base model. Scaling to\n128 examples matches or exceeds models trained on thousands of annotated\nsamples. While the extreme one-shot setting can induce mild, task-specific\noverfitting, our approach consistently demonstrates robust generalization and\nefficiency across diverse tasks. Further, we find that prompt design and loss\nweighting significantly influence training stability and final accuracy. Our\nmethod enables cost-effective and data-efficient development of\ndomain-specialist vision-language reasoning models, offering a pragmatic recipe\nfor data-scarce fields: start from a compact VLM, curate a handful of\nreward-checkable cases, and train via RLVR.\n", "link": "http://arxiv.org/abs/2507.21745v2", "date": "2025-08-07", "relevancy": 2.7966, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5682}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Vision-Language%20Reasoning%20for%20Satellite%20Imagery%20via%20Verifiable%0A%20%20Rewards&body=Title%3A%20Few-Shot%20Vision-Language%20Reasoning%20for%20Satellite%20Imagery%20via%20Verifiable%0A%20%20Rewards%0AAuthor%3A%20Aybora%20Koksal%20and%20A.%20Aydin%20Alatan%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20and%20vision-language%20models%20have%20enabled%0Astrong%20reasoning%20capabilities%2C%20yet%20they%20remain%20impractical%20for%20specialized%0Adomains%20like%20remote%20sensing%2C%20where%20annotated%20data%20is%20scarce%20and%20expensive.%20We%0Apresent%20the%20first%20few-shot%20reinforcement%20learning%20with%20verifiable%20reward%20%28RLVR%29%0Aframework%20for%20satellite%20imagery%20that%20eliminates%20the%20need%20for%20caption%0Asupervision--relying%20solely%20on%20lightweight%2C%20rule-based%20binary%20or%20IoU-based%0Arewards.%20Adapting%20the%20%221-shot%20RLVR%22%20paradigm%20from%20language%20models%20to%0Avision-language%20models%2C%20we%20employ%20policy-gradient%20optimization%20with%20as%20few%20as%0Aone%20curated%20example%20to%20align%20model%20outputs%20for%20satellite%20reasoning%20tasks.%0AComprehensive%20experiments%20across%20multiple%20remote%20sensing%20benchmarks--including%0Aclassification%2C%20visual%20question%20answering%2C%20and%20grounding--show%20that%20even%20a%0Asingle%20example%20yields%20substantial%20improvements%20over%20the%20base%20model.%20Scaling%20to%0A128%20examples%20matches%20or%20exceeds%20models%20trained%20on%20thousands%20of%20annotated%0Asamples.%20While%20the%20extreme%20one-shot%20setting%20can%20induce%20mild%2C%20task-specific%0Aoverfitting%2C%20our%20approach%20consistently%20demonstrates%20robust%20generalization%20and%0Aefficiency%20across%20diverse%20tasks.%20Further%2C%20we%20find%20that%20prompt%20design%20and%20loss%0Aweighting%20significantly%20influence%20training%20stability%20and%20final%20accuracy.%20Our%0Amethod%20enables%20cost-effective%20and%20data-efficient%20development%20of%0Adomain-specialist%20vision-language%20reasoning%20models%2C%20offering%20a%20pragmatic%20recipe%0Afor%20data-scarce%20fields%3A%20start%20from%20a%20compact%20VLM%2C%20curate%20a%20handful%20of%0Areward-checkable%20cases%2C%20and%20train%20via%20RLVR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21745v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Vision-Language%2520Reasoning%2520for%2520Satellite%2520Imagery%2520via%2520Verifiable%250A%2520%2520Rewards%26entry.906535625%3DAybora%2520Koksal%2520and%2520A.%2520Aydin%2520Alatan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520and%2520vision-language%2520models%2520have%2520enabled%250Astrong%2520reasoning%2520capabilities%252C%2520yet%2520they%2520remain%2520impractical%2520for%2520specialized%250Adomains%2520like%2520remote%2520sensing%252C%2520where%2520annotated%2520data%2520is%2520scarce%2520and%2520expensive.%2520We%250Apresent%2520the%2520first%2520few-shot%2520reinforcement%2520learning%2520with%2520verifiable%2520reward%2520%2528RLVR%2529%250Aframework%2520for%2520satellite%2520imagery%2520that%2520eliminates%2520the%2520need%2520for%2520caption%250Asupervision--relying%2520solely%2520on%2520lightweight%252C%2520rule-based%2520binary%2520or%2520IoU-based%250Arewards.%2520Adapting%2520the%2520%25221-shot%2520RLVR%2522%2520paradigm%2520from%2520language%2520models%2520to%250Avision-language%2520models%252C%2520we%2520employ%2520policy-gradient%2520optimization%2520with%2520as%2520few%2520as%250Aone%2520curated%2520example%2520to%2520align%2520model%2520outputs%2520for%2520satellite%2520reasoning%2520tasks.%250AComprehensive%2520experiments%2520across%2520multiple%2520remote%2520sensing%2520benchmarks--including%250Aclassification%252C%2520visual%2520question%2520answering%252C%2520and%2520grounding--show%2520that%2520even%2520a%250Asingle%2520example%2520yields%2520substantial%2520improvements%2520over%2520the%2520base%2520model.%2520Scaling%2520to%250A128%2520examples%2520matches%2520or%2520exceeds%2520models%2520trained%2520on%2520thousands%2520of%2520annotated%250Asamples.%2520While%2520the%2520extreme%2520one-shot%2520setting%2520can%2520induce%2520mild%252C%2520task-specific%250Aoverfitting%252C%2520our%2520approach%2520consistently%2520demonstrates%2520robust%2520generalization%2520and%250Aefficiency%2520across%2520diverse%2520tasks.%2520Further%252C%2520we%2520find%2520that%2520prompt%2520design%2520and%2520loss%250Aweighting%2520significantly%2520influence%2520training%2520stability%2520and%2520final%2520accuracy.%2520Our%250Amethod%2520enables%2520cost-effective%2520and%2520data-efficient%2520development%2520of%250Adomain-specialist%2520vision-language%2520reasoning%2520models%252C%2520offering%2520a%2520pragmatic%2520recipe%250Afor%2520data-scarce%2520fields%253A%2520start%2520from%2520a%2520compact%2520VLM%252C%2520curate%2520a%2520handful%2520of%250Areward-checkable%2520cases%252C%2520and%2520train%2520via%2520RLVR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21745v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Vision-Language%20Reasoning%20for%20Satellite%20Imagery%20via%20Verifiable%0A%20%20Rewards&entry.906535625=Aybora%20Koksal%20and%20A.%20Aydin%20Alatan&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20and%20vision-language%20models%20have%20enabled%0Astrong%20reasoning%20capabilities%2C%20yet%20they%20remain%20impractical%20for%20specialized%0Adomains%20like%20remote%20sensing%2C%20where%20annotated%20data%20is%20scarce%20and%20expensive.%20We%0Apresent%20the%20first%20few-shot%20reinforcement%20learning%20with%20verifiable%20reward%20%28RLVR%29%0Aframework%20for%20satellite%20imagery%20that%20eliminates%20the%20need%20for%20caption%0Asupervision--relying%20solely%20on%20lightweight%2C%20rule-based%20binary%20or%20IoU-based%0Arewards.%20Adapting%20the%20%221-shot%20RLVR%22%20paradigm%20from%20language%20models%20to%0Avision-language%20models%2C%20we%20employ%20policy-gradient%20optimization%20with%20as%20few%20as%0Aone%20curated%20example%20to%20align%20model%20outputs%20for%20satellite%20reasoning%20tasks.%0AComprehensive%20experiments%20across%20multiple%20remote%20sensing%20benchmarks--including%0Aclassification%2C%20visual%20question%20answering%2C%20and%20grounding--show%20that%20even%20a%0Asingle%20example%20yields%20substantial%20improvements%20over%20the%20base%20model.%20Scaling%20to%0A128%20examples%20matches%20or%20exceeds%20models%20trained%20on%20thousands%20of%20annotated%0Asamples.%20While%20the%20extreme%20one-shot%20setting%20can%20induce%20mild%2C%20task-specific%0Aoverfitting%2C%20our%20approach%20consistently%20demonstrates%20robust%20generalization%20and%0Aefficiency%20across%20diverse%20tasks.%20Further%2C%20we%20find%20that%20prompt%20design%20and%20loss%0Aweighting%20significantly%20influence%20training%20stability%20and%20final%20accuracy.%20Our%0Amethod%20enables%20cost-effective%20and%20data-efficient%20development%20of%0Adomain-specialist%20vision-language%20reasoning%20models%2C%20offering%20a%20pragmatic%20recipe%0Afor%20data-scarce%20fields%3A%20start%20from%20a%20compact%20VLM%2C%20curate%20a%20handful%20of%0Areward-checkable%20cases%2C%20and%20train%20via%20RLVR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21745v2&entry.124074799=Read"},
{"title": "ESVQA: Perceptual Quality Assessment of Egocentric Spatial Videos", "author": "Xilei Zhu and Huiyu Duan and Liu Yang and Yucheng Zhu and Xiongkuo Min and Guangtao Zhai and Patrick Le Callet", "abstract": "  With the rapid development of eXtended Reality (XR), egocentric spatial\nshooting and display technologies have further enhanced immersion and\nengagement for users, delivering more captivating and interactive experiences.\nAssessing the quality of experience (QoE) of egocentric spatial videos is\ncrucial to ensure a high-quality viewing experience. However, the corresponding\nresearch is still lacking. In this paper, we use the concept of embodied\nexperience to highlight this more immersive experience and study the new\nproblem, i.e., embodied perceptual quality assessment for egocentric spatial\nvideos. Specifically, we introduce the first Egocentric Spatial Video Quality\nAssessment Database (ESVQAD), which comprises 600 egocentric spatial videos\ncaptured using the Apple Vision Pro and their corresponding mean opinion scores\n(MOSs). Furthermore, we propose a novel multi-dimensional binocular feature\nfusion model, termed ESVQAnet, which integrates binocular spatial, motion, and\nsemantic features to predict the overall perceptual quality. Experimental\nresults demonstrate the ESVQAnet significantly outperforms 16 state-of-the-art\nVQA models on the embodied perceptual quality assessment task, and exhibits\nstrong generalization capability on traditional VQA tasks. The database and\ncode are available at https://github.com/iamazxl/ESVQA.\n", "link": "http://arxiv.org/abs/2412.20423v2", "date": "2025-08-07", "relevancy": 2.7894, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ESVQA%3A%20Perceptual%20Quality%20Assessment%20of%20Egocentric%20Spatial%20Videos&body=Title%3A%20ESVQA%3A%20Perceptual%20Quality%20Assessment%20of%20Egocentric%20Spatial%20Videos%0AAuthor%3A%20Xilei%20Zhu%20and%20Huiyu%20Duan%20and%20Liu%20Yang%20and%20Yucheng%20Zhu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Patrick%20Le%20Callet%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20eXtended%20Reality%20%28XR%29%2C%20egocentric%20spatial%0Ashooting%20and%20display%20technologies%20have%20further%20enhanced%20immersion%20and%0Aengagement%20for%20users%2C%20delivering%20more%20captivating%20and%20interactive%20experiences.%0AAssessing%20the%20quality%20of%20experience%20%28QoE%29%20of%20egocentric%20spatial%20videos%20is%0Acrucial%20to%20ensure%20a%20high-quality%20viewing%20experience.%20However%2C%20the%20corresponding%0Aresearch%20is%20still%20lacking.%20In%20this%20paper%2C%20we%20use%20the%20concept%20of%20embodied%0Aexperience%20to%20highlight%20this%20more%20immersive%20experience%20and%20study%20the%20new%0Aproblem%2C%20i.e.%2C%20embodied%20perceptual%20quality%20assessment%20for%20egocentric%20spatial%0Avideos.%20Specifically%2C%20we%20introduce%20the%20first%20Egocentric%20Spatial%20Video%20Quality%0AAssessment%20Database%20%28ESVQAD%29%2C%20which%20comprises%20600%20egocentric%20spatial%20videos%0Acaptured%20using%20the%20Apple%20Vision%20Pro%20and%20their%20corresponding%20mean%20opinion%20scores%0A%28MOSs%29.%20Furthermore%2C%20we%20propose%20a%20novel%20multi-dimensional%20binocular%20feature%0Afusion%20model%2C%20termed%20ESVQAnet%2C%20which%20integrates%20binocular%20spatial%2C%20motion%2C%20and%0Asemantic%20features%20to%20predict%20the%20overall%20perceptual%20quality.%20Experimental%0Aresults%20demonstrate%20the%20ESVQAnet%20significantly%20outperforms%2016%20state-of-the-art%0AVQA%20models%20on%20the%20embodied%20perceptual%20quality%20assessment%20task%2C%20and%20exhibits%0Astrong%20generalization%20capability%20on%20traditional%20VQA%20tasks.%20The%20database%20and%0Acode%20are%20available%20at%20https%3A//github.com/iamazxl/ESVQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20423v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DESVQA%253A%2520Perceptual%2520Quality%2520Assessment%2520of%2520Egocentric%2520Spatial%2520Videos%26entry.906535625%3DXilei%2520Zhu%2520and%2520Huiyu%2520Duan%2520and%2520Liu%2520Yang%2520and%2520Yucheng%2520Zhu%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%2520and%2520Patrick%2520Le%2520Callet%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520eXtended%2520Reality%2520%2528XR%2529%252C%2520egocentric%2520spatial%250Ashooting%2520and%2520display%2520technologies%2520have%2520further%2520enhanced%2520immersion%2520and%250Aengagement%2520for%2520users%252C%2520delivering%2520more%2520captivating%2520and%2520interactive%2520experiences.%250AAssessing%2520the%2520quality%2520of%2520experience%2520%2528QoE%2529%2520of%2520egocentric%2520spatial%2520videos%2520is%250Acrucial%2520to%2520ensure%2520a%2520high-quality%2520viewing%2520experience.%2520However%252C%2520the%2520corresponding%250Aresearch%2520is%2520still%2520lacking.%2520In%2520this%2520paper%252C%2520we%2520use%2520the%2520concept%2520of%2520embodied%250Aexperience%2520to%2520highlight%2520this%2520more%2520immersive%2520experience%2520and%2520study%2520the%2520new%250Aproblem%252C%2520i.e.%252C%2520embodied%2520perceptual%2520quality%2520assessment%2520for%2520egocentric%2520spatial%250Avideos.%2520Specifically%252C%2520we%2520introduce%2520the%2520first%2520Egocentric%2520Spatial%2520Video%2520Quality%250AAssessment%2520Database%2520%2528ESVQAD%2529%252C%2520which%2520comprises%2520600%2520egocentric%2520spatial%2520videos%250Acaptured%2520using%2520the%2520Apple%2520Vision%2520Pro%2520and%2520their%2520corresponding%2520mean%2520opinion%2520scores%250A%2528MOSs%2529.%2520Furthermore%252C%2520we%2520propose%2520a%2520novel%2520multi-dimensional%2520binocular%2520feature%250Afusion%2520model%252C%2520termed%2520ESVQAnet%252C%2520which%2520integrates%2520binocular%2520spatial%252C%2520motion%252C%2520and%250Asemantic%2520features%2520to%2520predict%2520the%2520overall%2520perceptual%2520quality.%2520Experimental%250Aresults%2520demonstrate%2520the%2520ESVQAnet%2520significantly%2520outperforms%252016%2520state-of-the-art%250AVQA%2520models%2520on%2520the%2520embodied%2520perceptual%2520quality%2520assessment%2520task%252C%2520and%2520exhibits%250Astrong%2520generalization%2520capability%2520on%2520traditional%2520VQA%2520tasks.%2520The%2520database%2520and%250Acode%2520are%2520available%2520at%2520https%253A//github.com/iamazxl/ESVQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20423v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ESVQA%3A%20Perceptual%20Quality%20Assessment%20of%20Egocentric%20Spatial%20Videos&entry.906535625=Xilei%20Zhu%20and%20Huiyu%20Duan%20and%20Liu%20Yang%20and%20Yucheng%20Zhu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Patrick%20Le%20Callet&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20eXtended%20Reality%20%28XR%29%2C%20egocentric%20spatial%0Ashooting%20and%20display%20technologies%20have%20further%20enhanced%20immersion%20and%0Aengagement%20for%20users%2C%20delivering%20more%20captivating%20and%20interactive%20experiences.%0AAssessing%20the%20quality%20of%20experience%20%28QoE%29%20of%20egocentric%20spatial%20videos%20is%0Acrucial%20to%20ensure%20a%20high-quality%20viewing%20experience.%20However%2C%20the%20corresponding%0Aresearch%20is%20still%20lacking.%20In%20this%20paper%2C%20we%20use%20the%20concept%20of%20embodied%0Aexperience%20to%20highlight%20this%20more%20immersive%20experience%20and%20study%20the%20new%0Aproblem%2C%20i.e.%2C%20embodied%20perceptual%20quality%20assessment%20for%20egocentric%20spatial%0Avideos.%20Specifically%2C%20we%20introduce%20the%20first%20Egocentric%20Spatial%20Video%20Quality%0AAssessment%20Database%20%28ESVQAD%29%2C%20which%20comprises%20600%20egocentric%20spatial%20videos%0Acaptured%20using%20the%20Apple%20Vision%20Pro%20and%20their%20corresponding%20mean%20opinion%20scores%0A%28MOSs%29.%20Furthermore%2C%20we%20propose%20a%20novel%20multi-dimensional%20binocular%20feature%0Afusion%20model%2C%20termed%20ESVQAnet%2C%20which%20integrates%20binocular%20spatial%2C%20motion%2C%20and%0Asemantic%20features%20to%20predict%20the%20overall%20perceptual%20quality.%20Experimental%0Aresults%20demonstrate%20the%20ESVQAnet%20significantly%20outperforms%2016%20state-of-the-art%0AVQA%20models%20on%20the%20embodied%20perceptual%20quality%20assessment%20task%2C%20and%20exhibits%0Astrong%20generalization%20capability%20on%20traditional%20VQA%20tasks.%20The%20database%20and%0Acode%20are%20available%20at%20https%3A//github.com/iamazxl/ESVQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20423v2&entry.124074799=Read"},
{"title": "Adapting Vision-Language Models Without Labels: A Comprehensive Survey", "author": "Hao Dong and Lijun Sheng and Jian Liang and Ran He and Eleni Chatzi and Olga Fink", "abstract": "  Vision-Language Models (VLMs) have demonstrated remarkable generalization\ncapabilities across a wide range of tasks. However, their performance often\nremains suboptimal when directly applied to specific downstream scenarios\nwithout task-specific adaptation. To enhance their utility while preserving\ndata efficiency, recent research has increasingly focused on unsupervised\nadaptation methods that do not rely on labeled data. Despite the growing\ninterest in this area, there remains a lack of a unified, task-oriented survey\ndedicated to unsupervised VLM adaptation. To bridge this gap, we present a\ncomprehensive and structured overview of the field. We propose a taxonomy based\non the availability and nature of unlabeled visual data, categorizing existing\napproaches into four key paradigms: Data-Free Transfer (no data), Unsupervised\nDomain Transfer (abundant data), Episodic Test-Time Adaptation (batch data),\nand Online Test-Time Adaptation (streaming data). Within this framework, we\nanalyze core methodologies and adaptation strategies associated with each\nparadigm, aiming to establish a systematic understanding of the field.\nAdditionally, we review representative benchmarks across diverse applications\nand highlight open challenges and promising directions for future research. An\nactively maintained repository of relevant literature is available at\nhttps://github.com/tim-learn/Awesome-LabelFree-VLMs.\n", "link": "http://arxiv.org/abs/2508.05547v1", "date": "2025-08-07", "relevancy": 2.7876, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Vision-Language%20Models%20Without%20Labels%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Adapting%20Vision-Language%20Models%20Without%20Labels%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Hao%20Dong%20and%20Lijun%20Sheng%20and%20Jian%20Liang%20and%20Ran%20He%20and%20Eleni%20Chatzi%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20generalization%0Acapabilities%20across%20a%20wide%20range%20of%20tasks.%20However%2C%20their%20performance%20often%0Aremains%20suboptimal%20when%20directly%20applied%20to%20specific%20downstream%20scenarios%0Awithout%20task-specific%20adaptation.%20To%20enhance%20their%20utility%20while%20preserving%0Adata%20efficiency%2C%20recent%20research%20has%20increasingly%20focused%20on%20unsupervised%0Aadaptation%20methods%20that%20do%20not%20rely%20on%20labeled%20data.%20Despite%20the%20growing%0Ainterest%20in%20this%20area%2C%20there%20remains%20a%20lack%20of%20a%20unified%2C%20task-oriented%20survey%0Adedicated%20to%20unsupervised%20VLM%20adaptation.%20To%20bridge%20this%20gap%2C%20we%20present%20a%0Acomprehensive%20and%20structured%20overview%20of%20the%20field.%20We%20propose%20a%20taxonomy%20based%0Aon%20the%20availability%20and%20nature%20of%20unlabeled%20visual%20data%2C%20categorizing%20existing%0Aapproaches%20into%20four%20key%20paradigms%3A%20Data-Free%20Transfer%20%28no%20data%29%2C%20Unsupervised%0ADomain%20Transfer%20%28abundant%20data%29%2C%20Episodic%20Test-Time%20Adaptation%20%28batch%20data%29%2C%0Aand%20Online%20Test-Time%20Adaptation%20%28streaming%20data%29.%20Within%20this%20framework%2C%20we%0Aanalyze%20core%20methodologies%20and%20adaptation%20strategies%20associated%20with%20each%0Aparadigm%2C%20aiming%20to%20establish%20a%20systematic%20understanding%20of%20the%20field.%0AAdditionally%2C%20we%20review%20representative%20benchmarks%20across%20diverse%20applications%0Aand%20highlight%20open%20challenges%20and%20promising%20directions%20for%20future%20research.%20An%0Aactively%20maintained%20repository%20of%20relevant%20literature%20is%20available%20at%0Ahttps%3A//github.com/tim-learn/Awesome-LabelFree-VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Vision-Language%2520Models%2520Without%2520Labels%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DHao%2520Dong%2520and%2520Lijun%2520Sheng%2520and%2520Jian%2520Liang%2520and%2520Ran%2520He%2520and%2520Eleni%2520Chatzi%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520generalization%250Acapabilities%2520across%2520a%2520wide%2520range%2520of%2520tasks.%2520However%252C%2520their%2520performance%2520often%250Aremains%2520suboptimal%2520when%2520directly%2520applied%2520to%2520specific%2520downstream%2520scenarios%250Awithout%2520task-specific%2520adaptation.%2520To%2520enhance%2520their%2520utility%2520while%2520preserving%250Adata%2520efficiency%252C%2520recent%2520research%2520has%2520increasingly%2520focused%2520on%2520unsupervised%250Aadaptation%2520methods%2520that%2520do%2520not%2520rely%2520on%2520labeled%2520data.%2520Despite%2520the%2520growing%250Ainterest%2520in%2520this%2520area%252C%2520there%2520remains%2520a%2520lack%2520of%2520a%2520unified%252C%2520task-oriented%2520survey%250Adedicated%2520to%2520unsupervised%2520VLM%2520adaptation.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520a%250Acomprehensive%2520and%2520structured%2520overview%2520of%2520the%2520field.%2520We%2520propose%2520a%2520taxonomy%2520based%250Aon%2520the%2520availability%2520and%2520nature%2520of%2520unlabeled%2520visual%2520data%252C%2520categorizing%2520existing%250Aapproaches%2520into%2520four%2520key%2520paradigms%253A%2520Data-Free%2520Transfer%2520%2528no%2520data%2529%252C%2520Unsupervised%250ADomain%2520Transfer%2520%2528abundant%2520data%2529%252C%2520Episodic%2520Test-Time%2520Adaptation%2520%2528batch%2520data%2529%252C%250Aand%2520Online%2520Test-Time%2520Adaptation%2520%2528streaming%2520data%2529.%2520Within%2520this%2520framework%252C%2520we%250Aanalyze%2520core%2520methodologies%2520and%2520adaptation%2520strategies%2520associated%2520with%2520each%250Aparadigm%252C%2520aiming%2520to%2520establish%2520a%2520systematic%2520understanding%2520of%2520the%2520field.%250AAdditionally%252C%2520we%2520review%2520representative%2520benchmarks%2520across%2520diverse%2520applications%250Aand%2520highlight%2520open%2520challenges%2520and%2520promising%2520directions%2520for%2520future%2520research.%2520An%250Aactively%2520maintained%2520repository%2520of%2520relevant%2520literature%2520is%2520available%2520at%250Ahttps%253A//github.com/tim-learn/Awesome-LabelFree-VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Vision-Language%20Models%20Without%20Labels%3A%20A%20Comprehensive%20Survey&entry.906535625=Hao%20Dong%20and%20Lijun%20Sheng%20and%20Jian%20Liang%20and%20Ran%20He%20and%20Eleni%20Chatzi%20and%20Olga%20Fink&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20generalization%0Acapabilities%20across%20a%20wide%20range%20of%20tasks.%20However%2C%20their%20performance%20often%0Aremains%20suboptimal%20when%20directly%20applied%20to%20specific%20downstream%20scenarios%0Awithout%20task-specific%20adaptation.%20To%20enhance%20their%20utility%20while%20preserving%0Adata%20efficiency%2C%20recent%20research%20has%20increasingly%20focused%20on%20unsupervised%0Aadaptation%20methods%20that%20do%20not%20rely%20on%20labeled%20data.%20Despite%20the%20growing%0Ainterest%20in%20this%20area%2C%20there%20remains%20a%20lack%20of%20a%20unified%2C%20task-oriented%20survey%0Adedicated%20to%20unsupervised%20VLM%20adaptation.%20To%20bridge%20this%20gap%2C%20we%20present%20a%0Acomprehensive%20and%20structured%20overview%20of%20the%20field.%20We%20propose%20a%20taxonomy%20based%0Aon%20the%20availability%20and%20nature%20of%20unlabeled%20visual%20data%2C%20categorizing%20existing%0Aapproaches%20into%20four%20key%20paradigms%3A%20Data-Free%20Transfer%20%28no%20data%29%2C%20Unsupervised%0ADomain%20Transfer%20%28abundant%20data%29%2C%20Episodic%20Test-Time%20Adaptation%20%28batch%20data%29%2C%0Aand%20Online%20Test-Time%20Adaptation%20%28streaming%20data%29.%20Within%20this%20framework%2C%20we%0Aanalyze%20core%20methodologies%20and%20adaptation%20strategies%20associated%20with%20each%0Aparadigm%2C%20aiming%20to%20establish%20a%20systematic%20understanding%20of%20the%20field.%0AAdditionally%2C%20we%20review%20representative%20benchmarks%20across%20diverse%20applications%0Aand%20highlight%20open%20challenges%20and%20promising%20directions%20for%20future%20research.%20An%0Aactively%20maintained%20repository%20of%20relevant%20literature%20is%20available%20at%0Ahttps%3A//github.com/tim-learn/Awesome-LabelFree-VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05547v1&entry.124074799=Read"},
{"title": "TIME: Temporal-Sensitive Multi-Dimensional Instruction Tuning and Robust\n  Benchmarking for Video-LLMs", "author": "Yunxiao Wang and Meng Liu and Wenqi Liu and Xuemeng Song and Bin Wen and Fan Yang and Tingting Gao and Di Zhang and Guorui Zhou and Liqiang Nie", "abstract": "  Video large language models have achieved remarkable performance in tasks\nsuch as video question answering, however, their temporal understanding remains\nsuboptimal. To address this limitation, we curate a dedicated instruction\nfine-tuning dataset that focuses on enhancing temporal comprehension across\nfive key dimensions. In order to reduce reliance on costly temporal\nannotations, we introduce a multi-task prompt fine-tuning approach that\nseamlessly integrates temporal-sensitive tasks into existing instruction\ndatasets without requiring additional annotations. Furthermore, we develop a\nnovel benchmark for temporal-sensitive video understanding that not only fills\nthe gaps in dimension coverage left by existing benchmarks but also rigorously\nfilters out potential shortcuts, ensuring a more accurate evaluation. Extensive\nexperimental results demonstrate that our approach significantly enhances the\ntemporal understanding of video-LLMs while avoiding reliance on shortcuts.\n", "link": "http://arxiv.org/abs/2503.09994v2", "date": "2025-08-07", "relevancy": 2.7778, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5656}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIME%3A%20Temporal-Sensitive%20Multi-Dimensional%20Instruction%20Tuning%20and%20Robust%0A%20%20Benchmarking%20for%20Video-LLMs&body=Title%3A%20TIME%3A%20Temporal-Sensitive%20Multi-Dimensional%20Instruction%20Tuning%20and%20Robust%0A%20%20Benchmarking%20for%20Video-LLMs%0AAuthor%3A%20Yunxiao%20Wang%20and%20Meng%20Liu%20and%20Wenqi%20Liu%20and%20Xuemeng%20Song%20and%20Bin%20Wen%20and%20Fan%20Yang%20and%20Tingting%20Gao%20and%20Di%20Zhang%20and%20Guorui%20Zhou%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Video%20large%20language%20models%20have%20achieved%20remarkable%20performance%20in%20tasks%0Asuch%20as%20video%20question%20answering%2C%20however%2C%20their%20temporal%20understanding%20remains%0Asuboptimal.%20To%20address%20this%20limitation%2C%20we%20curate%20a%20dedicated%20instruction%0Afine-tuning%20dataset%20that%20focuses%20on%20enhancing%20temporal%20comprehension%20across%0Afive%20key%20dimensions.%20In%20order%20to%20reduce%20reliance%20on%20costly%20temporal%0Aannotations%2C%20we%20introduce%20a%20multi-task%20prompt%20fine-tuning%20approach%20that%0Aseamlessly%20integrates%20temporal-sensitive%20tasks%20into%20existing%20instruction%0Adatasets%20without%20requiring%20additional%20annotations.%20Furthermore%2C%20we%20develop%20a%0Anovel%20benchmark%20for%20temporal-sensitive%20video%20understanding%20that%20not%20only%20fills%0Athe%20gaps%20in%20dimension%20coverage%20left%20by%20existing%20benchmarks%20but%20also%20rigorously%0Afilters%20out%20potential%20shortcuts%2C%20ensuring%20a%20more%20accurate%20evaluation.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20our%20approach%20significantly%20enhances%20the%0Atemporal%20understanding%20of%20video-LLMs%20while%20avoiding%20reliance%20on%20shortcuts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09994v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIME%253A%2520Temporal-Sensitive%2520Multi-Dimensional%2520Instruction%2520Tuning%2520and%2520Robust%250A%2520%2520Benchmarking%2520for%2520Video-LLMs%26entry.906535625%3DYunxiao%2520Wang%2520and%2520Meng%2520Liu%2520and%2520Wenqi%2520Liu%2520and%2520Xuemeng%2520Song%2520and%2520Bin%2520Wen%2520and%2520Fan%2520Yang%2520and%2520Tingting%2520Gao%2520and%2520Di%2520Zhang%2520and%2520Guorui%2520Zhou%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Video%2520large%2520language%2520models%2520have%2520achieved%2520remarkable%2520performance%2520in%2520tasks%250Asuch%2520as%2520video%2520question%2520answering%252C%2520however%252C%2520their%2520temporal%2520understanding%2520remains%250Asuboptimal.%2520To%2520address%2520this%2520limitation%252C%2520we%2520curate%2520a%2520dedicated%2520instruction%250Afine-tuning%2520dataset%2520that%2520focuses%2520on%2520enhancing%2520temporal%2520comprehension%2520across%250Afive%2520key%2520dimensions.%2520In%2520order%2520to%2520reduce%2520reliance%2520on%2520costly%2520temporal%250Aannotations%252C%2520we%2520introduce%2520a%2520multi-task%2520prompt%2520fine-tuning%2520approach%2520that%250Aseamlessly%2520integrates%2520temporal-sensitive%2520tasks%2520into%2520existing%2520instruction%250Adatasets%2520without%2520requiring%2520additional%2520annotations.%2520Furthermore%252C%2520we%2520develop%2520a%250Anovel%2520benchmark%2520for%2520temporal-sensitive%2520video%2520understanding%2520that%2520not%2520only%2520fills%250Athe%2520gaps%2520in%2520dimension%2520coverage%2520left%2520by%2520existing%2520benchmarks%2520but%2520also%2520rigorously%250Afilters%2520out%2520potential%2520shortcuts%252C%2520ensuring%2520a%2520more%2520accurate%2520evaluation.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520enhances%2520the%250Atemporal%2520understanding%2520of%2520video-LLMs%2520while%2520avoiding%2520reliance%2520on%2520shortcuts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09994v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIME%3A%20Temporal-Sensitive%20Multi-Dimensional%20Instruction%20Tuning%20and%20Robust%0A%20%20Benchmarking%20for%20Video-LLMs&entry.906535625=Yunxiao%20Wang%20and%20Meng%20Liu%20and%20Wenqi%20Liu%20and%20Xuemeng%20Song%20and%20Bin%20Wen%20and%20Fan%20Yang%20and%20Tingting%20Gao%20and%20Di%20Zhang%20and%20Guorui%20Zhou%20and%20Liqiang%20Nie&entry.1292438233=%20%20Video%20large%20language%20models%20have%20achieved%20remarkable%20performance%20in%20tasks%0Asuch%20as%20video%20question%20answering%2C%20however%2C%20their%20temporal%20understanding%20remains%0Asuboptimal.%20To%20address%20this%20limitation%2C%20we%20curate%20a%20dedicated%20instruction%0Afine-tuning%20dataset%20that%20focuses%20on%20enhancing%20temporal%20comprehension%20across%0Afive%20key%20dimensions.%20In%20order%20to%20reduce%20reliance%20on%20costly%20temporal%0Aannotations%2C%20we%20introduce%20a%20multi-task%20prompt%20fine-tuning%20approach%20that%0Aseamlessly%20integrates%20temporal-sensitive%20tasks%20into%20existing%20instruction%0Adatasets%20without%20requiring%20additional%20annotations.%20Furthermore%2C%20we%20develop%20a%0Anovel%20benchmark%20for%20temporal-sensitive%20video%20understanding%20that%20not%20only%20fills%0Athe%20gaps%20in%20dimension%20coverage%20left%20by%20existing%20benchmarks%20but%20also%20rigorously%0Afilters%20out%20potential%20shortcuts%2C%20ensuring%20a%20more%20accurate%20evaluation.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20our%20approach%20significantly%20enhances%20the%0Atemporal%20understanding%20of%20video-LLMs%20while%20avoiding%20reliance%20on%20shortcuts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09994v2&entry.124074799=Read"},
{"title": "Genie Envisioner: A Unified World Foundation Platform for Robotic\n  Manipulation", "author": "Yue Liao and Pengfei Zhou and Siyuan Huang and Donglin Yang and Shengcong Chen and Yuxin Jiang and Yue Hu and Jingbin Cai and Si Liu and Jianlan Luo and Liliang Chen and Shuicheng Yan and Maoqing Yao and Guanghui Ren", "abstract": "  We introduce Genie Envisioner (GE), a unified world foundation platform for\nrobotic manipulation that integrates policy learning, evaluation, and\nsimulation within a single video-generative framework. At its core, GE-Base is\na large-scale, instruction-conditioned video diffusion model that captures the\nspatial, temporal, and semantic dynamics of real-world robotic interactions in\na structured latent space. Built upon this foundation, GE-Act maps latent\nrepresentations to executable action trajectories through a lightweight,\nflow-matching decoder, enabling precise and generalizable policy inference\nacross diverse embodiments with minimal supervision. To support scalable\nevaluation and training, GE-Sim serves as an action-conditioned neural\nsimulator, producing high-fidelity rollouts for closed-loop policy development.\nThe platform is further equipped with EWMBench, a standardized benchmark suite\nmeasuring visual fidelity, physical consistency, and instruction-action\nalignment. Together, these components establish Genie Envisioner as a scalable\nand practical foundation for instruction-driven, general-purpose embodied\nintelligence. All code, models, and benchmarks will be released publicly.\n", "link": "http://arxiv.org/abs/2508.05635v1", "date": "2025-08-07", "relevancy": 2.7171, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.7593}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6228}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Genie%20Envisioner%3A%20A%20Unified%20World%20Foundation%20Platform%20for%20Robotic%0A%20%20Manipulation&body=Title%3A%20Genie%20Envisioner%3A%20A%20Unified%20World%20Foundation%20Platform%20for%20Robotic%0A%20%20Manipulation%0AAuthor%3A%20Yue%20Liao%20and%20Pengfei%20Zhou%20and%20Siyuan%20Huang%20and%20Donglin%20Yang%20and%20Shengcong%20Chen%20and%20Yuxin%20Jiang%20and%20Yue%20Hu%20and%20Jingbin%20Cai%20and%20Si%20Liu%20and%20Jianlan%20Luo%20and%20Liliang%20Chen%20and%20Shuicheng%20Yan%20and%20Maoqing%20Yao%20and%20Guanghui%20Ren%0AAbstract%3A%20%20%20We%20introduce%20Genie%20Envisioner%20%28GE%29%2C%20a%20unified%20world%20foundation%20platform%20for%0Arobotic%20manipulation%20that%20integrates%20policy%20learning%2C%20evaluation%2C%20and%0Asimulation%20within%20a%20single%20video-generative%20framework.%20At%20its%20core%2C%20GE-Base%20is%0Aa%20large-scale%2C%20instruction-conditioned%20video%20diffusion%20model%20that%20captures%20the%0Aspatial%2C%20temporal%2C%20and%20semantic%20dynamics%20of%20real-world%20robotic%20interactions%20in%0Aa%20structured%20latent%20space.%20Built%20upon%20this%20foundation%2C%20GE-Act%20maps%20latent%0Arepresentations%20to%20executable%20action%20trajectories%20through%20a%20lightweight%2C%0Aflow-matching%20decoder%2C%20enabling%20precise%20and%20generalizable%20policy%20inference%0Aacross%20diverse%20embodiments%20with%20minimal%20supervision.%20To%20support%20scalable%0Aevaluation%20and%20training%2C%20GE-Sim%20serves%20as%20an%20action-conditioned%20neural%0Asimulator%2C%20producing%20high-fidelity%20rollouts%20for%20closed-loop%20policy%20development.%0AThe%20platform%20is%20further%20equipped%20with%20EWMBench%2C%20a%20standardized%20benchmark%20suite%0Ameasuring%20visual%20fidelity%2C%20physical%20consistency%2C%20and%20instruction-action%0Aalignment.%20Together%2C%20these%20components%20establish%20Genie%20Envisioner%20as%20a%20scalable%0Aand%20practical%20foundation%20for%20instruction-driven%2C%20general-purpose%20embodied%0Aintelligence.%20All%20code%2C%20models%2C%20and%20benchmarks%20will%20be%20released%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenie%2520Envisioner%253A%2520A%2520Unified%2520World%2520Foundation%2520Platform%2520for%2520Robotic%250A%2520%2520Manipulation%26entry.906535625%3DYue%2520Liao%2520and%2520Pengfei%2520Zhou%2520and%2520Siyuan%2520Huang%2520and%2520Donglin%2520Yang%2520and%2520Shengcong%2520Chen%2520and%2520Yuxin%2520Jiang%2520and%2520Yue%2520Hu%2520and%2520Jingbin%2520Cai%2520and%2520Si%2520Liu%2520and%2520Jianlan%2520Luo%2520and%2520Liliang%2520Chen%2520and%2520Shuicheng%2520Yan%2520and%2520Maoqing%2520Yao%2520and%2520Guanghui%2520Ren%26entry.1292438233%3D%2520%2520We%2520introduce%2520Genie%2520Envisioner%2520%2528GE%2529%252C%2520a%2520unified%2520world%2520foundation%2520platform%2520for%250Arobotic%2520manipulation%2520that%2520integrates%2520policy%2520learning%252C%2520evaluation%252C%2520and%250Asimulation%2520within%2520a%2520single%2520video-generative%2520framework.%2520At%2520its%2520core%252C%2520GE-Base%2520is%250Aa%2520large-scale%252C%2520instruction-conditioned%2520video%2520diffusion%2520model%2520that%2520captures%2520the%250Aspatial%252C%2520temporal%252C%2520and%2520semantic%2520dynamics%2520of%2520real-world%2520robotic%2520interactions%2520in%250Aa%2520structured%2520latent%2520space.%2520Built%2520upon%2520this%2520foundation%252C%2520GE-Act%2520maps%2520latent%250Arepresentations%2520to%2520executable%2520action%2520trajectories%2520through%2520a%2520lightweight%252C%250Aflow-matching%2520decoder%252C%2520enabling%2520precise%2520and%2520generalizable%2520policy%2520inference%250Aacross%2520diverse%2520embodiments%2520with%2520minimal%2520supervision.%2520To%2520support%2520scalable%250Aevaluation%2520and%2520training%252C%2520GE-Sim%2520serves%2520as%2520an%2520action-conditioned%2520neural%250Asimulator%252C%2520producing%2520high-fidelity%2520rollouts%2520for%2520closed-loop%2520policy%2520development.%250AThe%2520platform%2520is%2520further%2520equipped%2520with%2520EWMBench%252C%2520a%2520standardized%2520benchmark%2520suite%250Ameasuring%2520visual%2520fidelity%252C%2520physical%2520consistency%252C%2520and%2520instruction-action%250Aalignment.%2520Together%252C%2520these%2520components%2520establish%2520Genie%2520Envisioner%2520as%2520a%2520scalable%250Aand%2520practical%2520foundation%2520for%2520instruction-driven%252C%2520general-purpose%2520embodied%250Aintelligence.%2520All%2520code%252C%2520models%252C%2520and%2520benchmarks%2520will%2520be%2520released%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Genie%20Envisioner%3A%20A%20Unified%20World%20Foundation%20Platform%20for%20Robotic%0A%20%20Manipulation&entry.906535625=Yue%20Liao%20and%20Pengfei%20Zhou%20and%20Siyuan%20Huang%20and%20Donglin%20Yang%20and%20Shengcong%20Chen%20and%20Yuxin%20Jiang%20and%20Yue%20Hu%20and%20Jingbin%20Cai%20and%20Si%20Liu%20and%20Jianlan%20Luo%20and%20Liliang%20Chen%20and%20Shuicheng%20Yan%20and%20Maoqing%20Yao%20and%20Guanghui%20Ren&entry.1292438233=%20%20We%20introduce%20Genie%20Envisioner%20%28GE%29%2C%20a%20unified%20world%20foundation%20platform%20for%0Arobotic%20manipulation%20that%20integrates%20policy%20learning%2C%20evaluation%2C%20and%0Asimulation%20within%20a%20single%20video-generative%20framework.%20At%20its%20core%2C%20GE-Base%20is%0Aa%20large-scale%2C%20instruction-conditioned%20video%20diffusion%20model%20that%20captures%20the%0Aspatial%2C%20temporal%2C%20and%20semantic%20dynamics%20of%20real-world%20robotic%20interactions%20in%0Aa%20structured%20latent%20space.%20Built%20upon%20this%20foundation%2C%20GE-Act%20maps%20latent%0Arepresentations%20to%20executable%20action%20trajectories%20through%20a%20lightweight%2C%0Aflow-matching%20decoder%2C%20enabling%20precise%20and%20generalizable%20policy%20inference%0Aacross%20diverse%20embodiments%20with%20minimal%20supervision.%20To%20support%20scalable%0Aevaluation%20and%20training%2C%20GE-Sim%20serves%20as%20an%20action-conditioned%20neural%0Asimulator%2C%20producing%20high-fidelity%20rollouts%20for%20closed-loop%20policy%20development.%0AThe%20platform%20is%20further%20equipped%20with%20EWMBench%2C%20a%20standardized%20benchmark%20suite%0Ameasuring%20visual%20fidelity%2C%20physical%20consistency%2C%20and%20instruction-action%0Aalignment.%20Together%2C%20these%20components%20establish%20Genie%20Envisioner%20as%20a%20scalable%0Aand%20practical%20foundation%20for%20instruction-driven%2C%20general-purpose%20embodied%0Aintelligence.%20All%20code%2C%20models%2C%20and%20benchmarks%20will%20be%20released%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05635v1&entry.124074799=Read"},
{"title": "Sign Spotting Disambiguation using Large Language Models", "author": "JianHe Low and Ozge Mercanoglu Sincan and Richard Bowden", "abstract": "  Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting.\n", "link": "http://arxiv.org/abs/2507.03703v4", "date": "2025-08-07", "relevancy": 2.7022, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sign%20Spotting%20Disambiguation%20using%20Large%20Language%20Models&body=Title%3A%20Sign%20Spotting%20Disambiguation%20using%20Large%20Language%20Models%0AAuthor%3A%20JianHe%20Low%20and%20Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Sign%20spotting%2C%20the%20task%20of%20identifying%20and%20localizing%20individual%20signs%20within%0Acontinuous%20sign%20language%20video%2C%20plays%20a%20pivotal%20role%20in%20scaling%20dataset%0Aannotations%20and%20addressing%20the%20severe%20data%20scarcity%20issue%20in%20sign%20language%0Atranslation.%20While%20automatic%20sign%20spotting%20holds%20great%20promise%20for%20enabling%0Aframe-level%20supervision%20at%20scale%2C%20it%20grapples%20with%20challenges%20such%20as%0Avocabulary%20inflexibility%20and%20ambiguity%20inherent%20in%20continuous%20sign%20streams.%0AHence%2C%20we%20introduce%20a%20novel%2C%20training-free%20framework%20that%20integrates%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20significantly%20enhance%20sign%20spotting%20quality.%20Our%0Aapproach%20extracts%20global%20spatio-temporal%20and%20hand%20shape%20features%2C%20which%20are%0Athen%20matched%20against%20a%20large-scale%20sign%20dictionary%20using%20dynamic%20time%20warping%0Aand%20cosine%20similarity.%20This%20dictionary-based%20matching%20inherently%20offers%0Asuperior%20vocabulary%20flexibility%20without%20requiring%20model%20retraining.%20To%20mitigate%0Anoise%20and%20ambiguity%20from%20the%20matching%20process%2C%20an%20LLM%20performs%20context-aware%0Agloss%20disambiguation%20via%20beam%20search%2C%20notably%20without%20fine-tuning.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20sign%20language%20datasets%20demonstrate%0Aour%20method%27s%20superior%20accuracy%20and%20sentence%20fluency%20compared%20to%20traditional%0Aapproaches%2C%20highlighting%20the%20potential%20of%20LLMs%20in%20advancing%20sign%20spotting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03703v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSign%2520Spotting%2520Disambiguation%2520using%2520Large%2520Language%2520Models%26entry.906535625%3DJianHe%2520Low%2520and%2520Ozge%2520Mercanoglu%2520Sincan%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Sign%2520spotting%252C%2520the%2520task%2520of%2520identifying%2520and%2520localizing%2520individual%2520signs%2520within%250Acontinuous%2520sign%2520language%2520video%252C%2520plays%2520a%2520pivotal%2520role%2520in%2520scaling%2520dataset%250Aannotations%2520and%2520addressing%2520the%2520severe%2520data%2520scarcity%2520issue%2520in%2520sign%2520language%250Atranslation.%2520While%2520automatic%2520sign%2520spotting%2520holds%2520great%2520promise%2520for%2520enabling%250Aframe-level%2520supervision%2520at%2520scale%252C%2520it%2520grapples%2520with%2520challenges%2520such%2520as%250Avocabulary%2520inflexibility%2520and%2520ambiguity%2520inherent%2520in%2520continuous%2520sign%2520streams.%250AHence%252C%2520we%2520introduce%2520a%2520novel%252C%2520training-free%2520framework%2520that%2520integrates%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520significantly%2520enhance%2520sign%2520spotting%2520quality.%2520Our%250Aapproach%2520extracts%2520global%2520spatio-temporal%2520and%2520hand%2520shape%2520features%252C%2520which%2520are%250Athen%2520matched%2520against%2520a%2520large-scale%2520sign%2520dictionary%2520using%2520dynamic%2520time%2520warping%250Aand%2520cosine%2520similarity.%2520This%2520dictionary-based%2520matching%2520inherently%2520offers%250Asuperior%2520vocabulary%2520flexibility%2520without%2520requiring%2520model%2520retraining.%2520To%2520mitigate%250Anoise%2520and%2520ambiguity%2520from%2520the%2520matching%2520process%252C%2520an%2520LLM%2520performs%2520context-aware%250Agloss%2520disambiguation%2520via%2520beam%2520search%252C%2520notably%2520without%2520fine-tuning.%2520Extensive%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520sign%2520language%2520datasets%2520demonstrate%250Aour%2520method%2527s%2520superior%2520accuracy%2520and%2520sentence%2520fluency%2520compared%2520to%2520traditional%250Aapproaches%252C%2520highlighting%2520the%2520potential%2520of%2520LLMs%2520in%2520advancing%2520sign%2520spotting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03703v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sign%20Spotting%20Disambiguation%20using%20Large%20Language%20Models&entry.906535625=JianHe%20Low%20and%20Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden&entry.1292438233=%20%20Sign%20spotting%2C%20the%20task%20of%20identifying%20and%20localizing%20individual%20signs%20within%0Acontinuous%20sign%20language%20video%2C%20plays%20a%20pivotal%20role%20in%20scaling%20dataset%0Aannotations%20and%20addressing%20the%20severe%20data%20scarcity%20issue%20in%20sign%20language%0Atranslation.%20While%20automatic%20sign%20spotting%20holds%20great%20promise%20for%20enabling%0Aframe-level%20supervision%20at%20scale%2C%20it%20grapples%20with%20challenges%20such%20as%0Avocabulary%20inflexibility%20and%20ambiguity%20inherent%20in%20continuous%20sign%20streams.%0AHence%2C%20we%20introduce%20a%20novel%2C%20training-free%20framework%20that%20integrates%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20significantly%20enhance%20sign%20spotting%20quality.%20Our%0Aapproach%20extracts%20global%20spatio-temporal%20and%20hand%20shape%20features%2C%20which%20are%0Athen%20matched%20against%20a%20large-scale%20sign%20dictionary%20using%20dynamic%20time%20warping%0Aand%20cosine%20similarity.%20This%20dictionary-based%20matching%20inherently%20offers%0Asuperior%20vocabulary%20flexibility%20without%20requiring%20model%20retraining.%20To%20mitigate%0Anoise%20and%20ambiguity%20from%20the%20matching%20process%2C%20an%20LLM%20performs%20context-aware%0Agloss%20disambiguation%20via%20beam%20search%2C%20notably%20without%20fine-tuning.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20sign%20language%20datasets%20demonstrate%0Aour%20method%27s%20superior%20accuracy%20and%20sentence%20fluency%20compared%20to%20traditional%0Aapproaches%2C%20highlighting%20the%20potential%20of%20LLMs%20in%20advancing%20sign%20spotting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03703v4&entry.124074799=Read"},
{"title": "MOSEv2: A More Challenging Dataset for Video Object Segmentation in\n  Complex Scenes", "author": "Henghui Ding and Kaining Ying and Chang Liu and Shuting He and Xudong Jiang and Yu-Gang Jiang and Philip H. S. Torr and Song Bai", "abstract": "  Video object segmentation (VOS) aims to segment specified target objects\nthroughout a video. Although state-of-the-art methods have achieved impressive\nperformance (e.g., 90+% J&F) on existing benchmarks such as DAVIS and\nYouTube-VOS, these datasets primarily contain salient, dominant, and isolated\nobjects, limiting their generalization to real-world scenarios. To advance VOS\ntoward more realistic environments, coMplex video Object SEgmentation (MOSEv1)\nwas introduced to facilitate VOS research in complex scenes. Building on the\nstrengths and limitations of MOSEv1, we present MOSEv2, a significantly more\nchallenging dataset designed to further advance VOS methods under real-world\nconditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks\nfor 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2\nintroduces significantly greater scene complexity, including more frequent\nobject disappearance and reappearance, severe occlusions and crowding, smaller\nobjects, as well as a range of new challenges such as adverse weather (e.g.,\nrain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot\nsequences, camouflaged objects, non-physical targets (e.g., shadows,\nreflections), scenarios requiring external knowledge, etc. We benchmark 20\nrepresentative VOS methods under 5 different settings and observe consistent\nperformance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9%\non MOSEv2. We further evaluate 9 video object tracking methods and find similar\ndeclines, demonstrating that MOSEv2 presents challenges across tasks. These\nresults highlight that despite high accuracy on existing datasets, current VOS\nmethods still struggle under real-world complexities. MOSEv2 is publicly\navailable at https://MOSE.video.\n", "link": "http://arxiv.org/abs/2508.05630v1", "date": "2025-08-07", "relevancy": 2.7, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOSEv2%3A%20A%20More%20Challenging%20Dataset%20for%20Video%20Object%20Segmentation%20in%0A%20%20Complex%20Scenes&body=Title%3A%20MOSEv2%3A%20A%20More%20Challenging%20Dataset%20for%20Video%20Object%20Segmentation%20in%0A%20%20Complex%20Scenes%0AAuthor%3A%20Henghui%20Ding%20and%20Kaining%20Ying%20and%20Chang%20Liu%20and%20Shuting%20He%20and%20Xudong%20Jiang%20and%20Yu-Gang%20Jiang%20and%20Philip%20H.%20S.%20Torr%20and%20Song%20Bai%0AAbstract%3A%20%20%20Video%20object%20segmentation%20%28VOS%29%20aims%20to%20segment%20specified%20target%20objects%0Athroughout%20a%20video.%20Although%20state-of-the-art%20methods%20have%20achieved%20impressive%0Aperformance%20%28e.g.%2C%2090%2B%25%20J%26F%29%20on%20existing%20benchmarks%20such%20as%20DAVIS%20and%0AYouTube-VOS%2C%20these%20datasets%20primarily%20contain%20salient%2C%20dominant%2C%20and%20isolated%0Aobjects%2C%20limiting%20their%20generalization%20to%20real-world%20scenarios.%20To%20advance%20VOS%0Atoward%20more%20realistic%20environments%2C%20coMplex%20video%20Object%20SEgmentation%20%28MOSEv1%29%0Awas%20introduced%20to%20facilitate%20VOS%20research%20in%20complex%20scenes.%20Building%20on%20the%0Astrengths%20and%20limitations%20of%20MOSEv1%2C%20we%20present%20MOSEv2%2C%20a%20significantly%20more%0Achallenging%20dataset%20designed%20to%20further%20advance%20VOS%20methods%20under%20real-world%0Aconditions.%20MOSEv2%20consists%20of%205%2C024%20videos%20and%20over%20701%2C976%20high-quality%20masks%0Afor%2010%2C074%20objects%20across%20200%20categories.%20Compared%20to%20its%20predecessor%2C%20MOSEv2%0Aintroduces%20significantly%20greater%20scene%20complexity%2C%20including%20more%20frequent%0Aobject%20disappearance%20and%20reappearance%2C%20severe%20occlusions%20and%20crowding%2C%20smaller%0Aobjects%2C%20as%20well%20as%20a%20range%20of%20new%20challenges%20such%20as%20adverse%20weather%20%28e.g.%2C%0Arain%2C%20snow%2C%20fog%29%2C%20low-light%20scenes%20%28e.g.%2C%20nighttime%2C%20underwater%29%2C%20multi-shot%0Asequences%2C%20camouflaged%20objects%2C%20non-physical%20targets%20%28e.g.%2C%20shadows%2C%0Areflections%29%2C%20scenarios%20requiring%20external%20knowledge%2C%20etc.%20We%20benchmark%2020%0Arepresentative%20VOS%20methods%20under%205%20different%20settings%20and%20observe%20consistent%0Aperformance%20drops.%20For%20example%2C%20SAM2%20drops%20from%2076.4%25%20on%20MOSEv1%20to%20only%2050.9%25%0Aon%20MOSEv2.%20We%20further%20evaluate%209%20video%20object%20tracking%20methods%20and%20find%20similar%0Adeclines%2C%20demonstrating%20that%20MOSEv2%20presents%20challenges%20across%20tasks.%20These%0Aresults%20highlight%20that%20despite%20high%20accuracy%20on%20existing%20datasets%2C%20current%20VOS%0Amethods%20still%20struggle%20under%20real-world%20complexities.%20MOSEv2%20is%20publicly%0Aavailable%20at%20https%3A//MOSE.video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOSEv2%253A%2520A%2520More%2520Challenging%2520Dataset%2520for%2520Video%2520Object%2520Segmentation%2520in%250A%2520%2520Complex%2520Scenes%26entry.906535625%3DHenghui%2520Ding%2520and%2520Kaining%2520Ying%2520and%2520Chang%2520Liu%2520and%2520Shuting%2520He%2520and%2520Xudong%2520Jiang%2520and%2520Yu-Gang%2520Jiang%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Song%2520Bai%26entry.1292438233%3D%2520%2520Video%2520object%2520segmentation%2520%2528VOS%2529%2520aims%2520to%2520segment%2520specified%2520target%2520objects%250Athroughout%2520a%2520video.%2520Although%2520state-of-the-art%2520methods%2520have%2520achieved%2520impressive%250Aperformance%2520%2528e.g.%252C%252090%252B%2525%2520J%2526F%2529%2520on%2520existing%2520benchmarks%2520such%2520as%2520DAVIS%2520and%250AYouTube-VOS%252C%2520these%2520datasets%2520primarily%2520contain%2520salient%252C%2520dominant%252C%2520and%2520isolated%250Aobjects%252C%2520limiting%2520their%2520generalization%2520to%2520real-world%2520scenarios.%2520To%2520advance%2520VOS%250Atoward%2520more%2520realistic%2520environments%252C%2520coMplex%2520video%2520Object%2520SEgmentation%2520%2528MOSEv1%2529%250Awas%2520introduced%2520to%2520facilitate%2520VOS%2520research%2520in%2520complex%2520scenes.%2520Building%2520on%2520the%250Astrengths%2520and%2520limitations%2520of%2520MOSEv1%252C%2520we%2520present%2520MOSEv2%252C%2520a%2520significantly%2520more%250Achallenging%2520dataset%2520designed%2520to%2520further%2520advance%2520VOS%2520methods%2520under%2520real-world%250Aconditions.%2520MOSEv2%2520consists%2520of%25205%252C024%2520videos%2520and%2520over%2520701%252C976%2520high-quality%2520masks%250Afor%252010%252C074%2520objects%2520across%2520200%2520categories.%2520Compared%2520to%2520its%2520predecessor%252C%2520MOSEv2%250Aintroduces%2520significantly%2520greater%2520scene%2520complexity%252C%2520including%2520more%2520frequent%250Aobject%2520disappearance%2520and%2520reappearance%252C%2520severe%2520occlusions%2520and%2520crowding%252C%2520smaller%250Aobjects%252C%2520as%2520well%2520as%2520a%2520range%2520of%2520new%2520challenges%2520such%2520as%2520adverse%2520weather%2520%2528e.g.%252C%250Arain%252C%2520snow%252C%2520fog%2529%252C%2520low-light%2520scenes%2520%2528e.g.%252C%2520nighttime%252C%2520underwater%2529%252C%2520multi-shot%250Asequences%252C%2520camouflaged%2520objects%252C%2520non-physical%2520targets%2520%2528e.g.%252C%2520shadows%252C%250Areflections%2529%252C%2520scenarios%2520requiring%2520external%2520knowledge%252C%2520etc.%2520We%2520benchmark%252020%250Arepresentative%2520VOS%2520methods%2520under%25205%2520different%2520settings%2520and%2520observe%2520consistent%250Aperformance%2520drops.%2520For%2520example%252C%2520SAM2%2520drops%2520from%252076.4%2525%2520on%2520MOSEv1%2520to%2520only%252050.9%2525%250Aon%2520MOSEv2.%2520We%2520further%2520evaluate%25209%2520video%2520object%2520tracking%2520methods%2520and%2520find%2520similar%250Adeclines%252C%2520demonstrating%2520that%2520MOSEv2%2520presents%2520challenges%2520across%2520tasks.%2520These%250Aresults%2520highlight%2520that%2520despite%2520high%2520accuracy%2520on%2520existing%2520datasets%252C%2520current%2520VOS%250Amethods%2520still%2520struggle%2520under%2520real-world%2520complexities.%2520MOSEv2%2520is%2520publicly%250Aavailable%2520at%2520https%253A//MOSE.video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOSEv2%3A%20A%20More%20Challenging%20Dataset%20for%20Video%20Object%20Segmentation%20in%0A%20%20Complex%20Scenes&entry.906535625=Henghui%20Ding%20and%20Kaining%20Ying%20and%20Chang%20Liu%20and%20Shuting%20He%20and%20Xudong%20Jiang%20and%20Yu-Gang%20Jiang%20and%20Philip%20H.%20S.%20Torr%20and%20Song%20Bai&entry.1292438233=%20%20Video%20object%20segmentation%20%28VOS%29%20aims%20to%20segment%20specified%20target%20objects%0Athroughout%20a%20video.%20Although%20state-of-the-art%20methods%20have%20achieved%20impressive%0Aperformance%20%28e.g.%2C%2090%2B%25%20J%26F%29%20on%20existing%20benchmarks%20such%20as%20DAVIS%20and%0AYouTube-VOS%2C%20these%20datasets%20primarily%20contain%20salient%2C%20dominant%2C%20and%20isolated%0Aobjects%2C%20limiting%20their%20generalization%20to%20real-world%20scenarios.%20To%20advance%20VOS%0Atoward%20more%20realistic%20environments%2C%20coMplex%20video%20Object%20SEgmentation%20%28MOSEv1%29%0Awas%20introduced%20to%20facilitate%20VOS%20research%20in%20complex%20scenes.%20Building%20on%20the%0Astrengths%20and%20limitations%20of%20MOSEv1%2C%20we%20present%20MOSEv2%2C%20a%20significantly%20more%0Achallenging%20dataset%20designed%20to%20further%20advance%20VOS%20methods%20under%20real-world%0Aconditions.%20MOSEv2%20consists%20of%205%2C024%20videos%20and%20over%20701%2C976%20high-quality%20masks%0Afor%2010%2C074%20objects%20across%20200%20categories.%20Compared%20to%20its%20predecessor%2C%20MOSEv2%0Aintroduces%20significantly%20greater%20scene%20complexity%2C%20including%20more%20frequent%0Aobject%20disappearance%20and%20reappearance%2C%20severe%20occlusions%20and%20crowding%2C%20smaller%0Aobjects%2C%20as%20well%20as%20a%20range%20of%20new%20challenges%20such%20as%20adverse%20weather%20%28e.g.%2C%0Arain%2C%20snow%2C%20fog%29%2C%20low-light%20scenes%20%28e.g.%2C%20nighttime%2C%20underwater%29%2C%20multi-shot%0Asequences%2C%20camouflaged%20objects%2C%20non-physical%20targets%20%28e.g.%2C%20shadows%2C%0Areflections%29%2C%20scenarios%20requiring%20external%20knowledge%2C%20etc.%20We%20benchmark%2020%0Arepresentative%20VOS%20methods%20under%205%20different%20settings%20and%20observe%20consistent%0Aperformance%20drops.%20For%20example%2C%20SAM2%20drops%20from%2076.4%25%20on%20MOSEv1%20to%20only%2050.9%25%0Aon%20MOSEv2.%20We%20further%20evaluate%209%20video%20object%20tracking%20methods%20and%20find%20similar%0Adeclines%2C%20demonstrating%20that%20MOSEv2%20presents%20challenges%20across%20tasks.%20These%0Aresults%20highlight%20that%20despite%20high%20accuracy%20on%20existing%20datasets%2C%20current%20VOS%0Amethods%20still%20struggle%20under%20real-world%20complexities.%20MOSEv2%20is%20publicly%0Aavailable%20at%20https%3A//MOSE.video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05630v1&entry.124074799=Read"},
{"title": "Navigating the Trade-off: A Synthesis of Defensive Strategies for\n  Zero-Shot Adversarial Robustness in Vision-Language Models", "author": "Zane Xu and Jason Sun", "abstract": "  This report synthesizes eight seminal papers on the zero-shot adversarial\nrobustness of vision-language models (VLMs) like CLIP. A central challenge in\nthis domain is the inherent trade-off between enhancing adversarial robustness\nand preserving the model's zero-shot generalization capabilities. We analyze\ntwo primary defense paradigms: Adversarial Fine-Tuning (AFT), which modifies\nmodel parameters, and Training-Free/Test-Time Defenses, which preserve them. We\ntrace the evolution from alignment-preserving methods (TeCoA) to embedding\nspace re-engineering (LAAT, TIMA), and from input heuristics (AOM, TTC) to\nlatent-space purification (CLIPure). Finally, we identify key challenges and\nfuture directions including hybrid defense strategies and adversarial\npre-training.\n", "link": "http://arxiv.org/abs/2508.05237v1", "date": "2025-08-07", "relevancy": 2.6888, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20the%20Trade-off%3A%20A%20Synthesis%20of%20Defensive%20Strategies%20for%0A%20%20Zero-Shot%20Adversarial%20Robustness%20in%20Vision-Language%20Models&body=Title%3A%20Navigating%20the%20Trade-off%3A%20A%20Synthesis%20of%20Defensive%20Strategies%20for%0A%20%20Zero-Shot%20Adversarial%20Robustness%20in%20Vision-Language%20Models%0AAuthor%3A%20Zane%20Xu%20and%20Jason%20Sun%0AAbstract%3A%20%20%20This%20report%20synthesizes%20eight%20seminal%20papers%20on%20the%20zero-shot%20adversarial%0Arobustness%20of%20vision-language%20models%20%28VLMs%29%20like%20CLIP.%20A%20central%20challenge%20in%0Athis%20domain%20is%20the%20inherent%20trade-off%20between%20enhancing%20adversarial%20robustness%0Aand%20preserving%20the%20model%27s%20zero-shot%20generalization%20capabilities.%20We%20analyze%0Atwo%20primary%20defense%20paradigms%3A%20Adversarial%20Fine-Tuning%20%28AFT%29%2C%20which%20modifies%0Amodel%20parameters%2C%20and%20Training-Free/Test-Time%20Defenses%2C%20which%20preserve%20them.%20We%0Atrace%20the%20evolution%20from%20alignment-preserving%20methods%20%28TeCoA%29%20to%20embedding%0Aspace%20re-engineering%20%28LAAT%2C%20TIMA%29%2C%20and%20from%20input%20heuristics%20%28AOM%2C%20TTC%29%20to%0Alatent-space%20purification%20%28CLIPure%29.%20Finally%2C%20we%20identify%20key%20challenges%20and%0Afuture%20directions%20including%20hybrid%20defense%20strategies%20and%20adversarial%0Apre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520the%2520Trade-off%253A%2520A%2520Synthesis%2520of%2520Defensive%2520Strategies%2520for%250A%2520%2520Zero-Shot%2520Adversarial%2520Robustness%2520in%2520Vision-Language%2520Models%26entry.906535625%3DZane%2520Xu%2520and%2520Jason%2520Sun%26entry.1292438233%3D%2520%2520This%2520report%2520synthesizes%2520eight%2520seminal%2520papers%2520on%2520the%2520zero-shot%2520adversarial%250Arobustness%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520like%2520CLIP.%2520A%2520central%2520challenge%2520in%250Athis%2520domain%2520is%2520the%2520inherent%2520trade-off%2520between%2520enhancing%2520adversarial%2520robustness%250Aand%2520preserving%2520the%2520model%2527s%2520zero-shot%2520generalization%2520capabilities.%2520We%2520analyze%250Atwo%2520primary%2520defense%2520paradigms%253A%2520Adversarial%2520Fine-Tuning%2520%2528AFT%2529%252C%2520which%2520modifies%250Amodel%2520parameters%252C%2520and%2520Training-Free/Test-Time%2520Defenses%252C%2520which%2520preserve%2520them.%2520We%250Atrace%2520the%2520evolution%2520from%2520alignment-preserving%2520methods%2520%2528TeCoA%2529%2520to%2520embedding%250Aspace%2520re-engineering%2520%2528LAAT%252C%2520TIMA%2529%252C%2520and%2520from%2520input%2520heuristics%2520%2528AOM%252C%2520TTC%2529%2520to%250Alatent-space%2520purification%2520%2528CLIPure%2529.%2520Finally%252C%2520we%2520identify%2520key%2520challenges%2520and%250Afuture%2520directions%2520including%2520hybrid%2520defense%2520strategies%2520and%2520adversarial%250Apre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20the%20Trade-off%3A%20A%20Synthesis%20of%20Defensive%20Strategies%20for%0A%20%20Zero-Shot%20Adversarial%20Robustness%20in%20Vision-Language%20Models&entry.906535625=Zane%20Xu%20and%20Jason%20Sun&entry.1292438233=%20%20This%20report%20synthesizes%20eight%20seminal%20papers%20on%20the%20zero-shot%20adversarial%0Arobustness%20of%20vision-language%20models%20%28VLMs%29%20like%20CLIP.%20A%20central%20challenge%20in%0Athis%20domain%20is%20the%20inherent%20trade-off%20between%20enhancing%20adversarial%20robustness%0Aand%20preserving%20the%20model%27s%20zero-shot%20generalization%20capabilities.%20We%20analyze%0Atwo%20primary%20defense%20paradigms%3A%20Adversarial%20Fine-Tuning%20%28AFT%29%2C%20which%20modifies%0Amodel%20parameters%2C%20and%20Training-Free/Test-Time%20Defenses%2C%20which%20preserve%20them.%20We%0Atrace%20the%20evolution%20from%20alignment-preserving%20methods%20%28TeCoA%29%20to%20embedding%0Aspace%20re-engineering%20%28LAAT%2C%20TIMA%29%2C%20and%20from%20input%20heuristics%20%28AOM%2C%20TTC%29%20to%0Alatent-space%20purification%20%28CLIPure%29.%20Finally%2C%20we%20identify%20key%20challenges%20and%0Afuture%20directions%20including%20hybrid%20defense%20strategies%20and%20adversarial%0Apre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05237v1&entry.124074799=Read"},
{"title": "X-VFL: A New Vertical Federated Learning Framework with Cross Completion\n  and Decision Subspace Alignment", "author": "Qinghua Yao and Xiangrui Xu and Zhize Li", "abstract": "  Vertical Federated Learning (VFL) enables collaborative learning by\nintegrating disjoint feature subsets from multiple clients/parties. However,\nVFL typically faces two key challenges: i) the requirement for perfectly\naligned data samples across all clients (missing features are not allowed); ii)\nthe requirement for joint collaborative inference/prediction involving all\nclients (it does not support locally independent inference on a single client).\nTo address these challenges, we propose X-VFL, a new VFL framework designed to\ndeal with the non-aligned data samples with (partially) missing features and to\nsupport locally independent inference of new data samples for each client. In\nparticular, we design two novel modules in X-VFL: Cross Completion (XCom) and\nDecision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing\nfeatures for non-aligned data samples by leveraging information from other\nclients. DS-Align aligns local features with completed and global features\nacross all clients within the decision subspace, thus enabling locally\nindependent inference at each client. Moreover, we provide convergence theorems\nfor different algorithms used in training X-VFL, showing an $O(1/\\sqrt{T})$\nconvergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type\nalgorithms, where $T$ denotes the number of training update steps. Extensive\nexperiments on real-world datasets demonstrate that X-VFL significantly\noutperforms existing methods, e.g., achieving a 15% improvement in accuracy on\nthe image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III\ndataset. These results validate the practical effectiveness and superiority of\nX-VFL, particularly in scenarios involving partially missing features and\nlocally independent inference.\n", "link": "http://arxiv.org/abs/2508.05568v1", "date": "2025-08-07", "relevancy": 2.6884, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.559}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5422}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-VFL%3A%20A%20New%20Vertical%20Federated%20Learning%20Framework%20with%20Cross%20Completion%0A%20%20and%20Decision%20Subspace%20Alignment&body=Title%3A%20X-VFL%3A%20A%20New%20Vertical%20Federated%20Learning%20Framework%20with%20Cross%20Completion%0A%20%20and%20Decision%20Subspace%20Alignment%0AAuthor%3A%20Qinghua%20Yao%20and%20Xiangrui%20Xu%20and%20Zhize%20Li%0AAbstract%3A%20%20%20Vertical%20Federated%20Learning%20%28VFL%29%20enables%20collaborative%20learning%20by%0Aintegrating%20disjoint%20feature%20subsets%20from%20multiple%20clients/parties.%20However%2C%0AVFL%20typically%20faces%20two%20key%20challenges%3A%20i%29%20the%20requirement%20for%20perfectly%0Aaligned%20data%20samples%20across%20all%20clients%20%28missing%20features%20are%20not%20allowed%29%3B%20ii%29%0Athe%20requirement%20for%20joint%20collaborative%20inference/prediction%20involving%20all%0Aclients%20%28it%20does%20not%20support%20locally%20independent%20inference%20on%20a%20single%20client%29.%0ATo%20address%20these%20challenges%2C%20we%20propose%20X-VFL%2C%20a%20new%20VFL%20framework%20designed%20to%0Adeal%20with%20the%20non-aligned%20data%20samples%20with%20%28partially%29%20missing%20features%20and%20to%0Asupport%20locally%20independent%20inference%20of%20new%20data%20samples%20for%20each%20client.%20In%0Aparticular%2C%20we%20design%20two%20novel%20modules%20in%20X-VFL%3A%20Cross%20Completion%20%28XCom%29%20and%0ADecision%20Subspace%20Alignment%20%28DS-Align%29.%20XCom%20can%20complete/reconstruct%20missing%0Afeatures%20for%20non-aligned%20data%20samples%20by%20leveraging%20information%20from%20other%0Aclients.%20DS-Align%20aligns%20local%20features%20with%20completed%20and%20global%20features%0Aacross%20all%20clients%20within%20the%20decision%20subspace%2C%20thus%20enabling%20locally%0Aindependent%20inference%20at%20each%20client.%20Moreover%2C%20we%20provide%20convergence%20theorems%0Afor%20different%20algorithms%20used%20in%20training%20X-VFL%2C%20showing%20an%20%24O%281/%5Csqrt%7BT%7D%29%24%0Aconvergence%20rate%20for%20SGD-type%20algorithms%20and%20an%20%24O%281/T%29%24%20rate%20for%20PAGE-type%0Aalgorithms%2C%20where%20%24T%24%20denotes%20the%20number%20of%20training%20update%20steps.%20Extensive%0Aexperiments%20on%20real-world%20datasets%20demonstrate%20that%20X-VFL%20significantly%0Aoutperforms%20existing%20methods%2C%20e.g.%2C%20achieving%20a%2015%25%20improvement%20in%20accuracy%20on%0Athe%20image%20CIFAR-10%20dataset%20and%20a%2043%25%20improvement%20on%20the%20medical%20MIMIC-III%0Adataset.%20These%20results%20validate%20the%20practical%20effectiveness%20and%20superiority%20of%0AX-VFL%2C%20particularly%20in%20scenarios%20involving%20partially%20missing%20features%20and%0Alocally%20independent%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-VFL%253A%2520A%2520New%2520Vertical%2520Federated%2520Learning%2520Framework%2520with%2520Cross%2520Completion%250A%2520%2520and%2520Decision%2520Subspace%2520Alignment%26entry.906535625%3DQinghua%2520Yao%2520and%2520Xiangrui%2520Xu%2520and%2520Zhize%2520Li%26entry.1292438233%3D%2520%2520Vertical%2520Federated%2520Learning%2520%2528VFL%2529%2520enables%2520collaborative%2520learning%2520by%250Aintegrating%2520disjoint%2520feature%2520subsets%2520from%2520multiple%2520clients/parties.%2520However%252C%250AVFL%2520typically%2520faces%2520two%2520key%2520challenges%253A%2520i%2529%2520the%2520requirement%2520for%2520perfectly%250Aaligned%2520data%2520samples%2520across%2520all%2520clients%2520%2528missing%2520features%2520are%2520not%2520allowed%2529%253B%2520ii%2529%250Athe%2520requirement%2520for%2520joint%2520collaborative%2520inference/prediction%2520involving%2520all%250Aclients%2520%2528it%2520does%2520not%2520support%2520locally%2520independent%2520inference%2520on%2520a%2520single%2520client%2529.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520X-VFL%252C%2520a%2520new%2520VFL%2520framework%2520designed%2520to%250Adeal%2520with%2520the%2520non-aligned%2520data%2520samples%2520with%2520%2528partially%2529%2520missing%2520features%2520and%2520to%250Asupport%2520locally%2520independent%2520inference%2520of%2520new%2520data%2520samples%2520for%2520each%2520client.%2520In%250Aparticular%252C%2520we%2520design%2520two%2520novel%2520modules%2520in%2520X-VFL%253A%2520Cross%2520Completion%2520%2528XCom%2529%2520and%250ADecision%2520Subspace%2520Alignment%2520%2528DS-Align%2529.%2520XCom%2520can%2520complete/reconstruct%2520missing%250Afeatures%2520for%2520non-aligned%2520data%2520samples%2520by%2520leveraging%2520information%2520from%2520other%250Aclients.%2520DS-Align%2520aligns%2520local%2520features%2520with%2520completed%2520and%2520global%2520features%250Aacross%2520all%2520clients%2520within%2520the%2520decision%2520subspace%252C%2520thus%2520enabling%2520locally%250Aindependent%2520inference%2520at%2520each%2520client.%2520Moreover%252C%2520we%2520provide%2520convergence%2520theorems%250Afor%2520different%2520algorithms%2520used%2520in%2520training%2520X-VFL%252C%2520showing%2520an%2520%2524O%25281/%255Csqrt%257BT%257D%2529%2524%250Aconvergence%2520rate%2520for%2520SGD-type%2520algorithms%2520and%2520an%2520%2524O%25281/T%2529%2524%2520rate%2520for%2520PAGE-type%250Aalgorithms%252C%2520where%2520%2524T%2524%2520denotes%2520the%2520number%2520of%2520training%2520update%2520steps.%2520Extensive%250Aexperiments%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520X-VFL%2520significantly%250Aoutperforms%2520existing%2520methods%252C%2520e.g.%252C%2520achieving%2520a%252015%2525%2520improvement%2520in%2520accuracy%2520on%250Athe%2520image%2520CIFAR-10%2520dataset%2520and%2520a%252043%2525%2520improvement%2520on%2520the%2520medical%2520MIMIC-III%250Adataset.%2520These%2520results%2520validate%2520the%2520practical%2520effectiveness%2520and%2520superiority%2520of%250AX-VFL%252C%2520particularly%2520in%2520scenarios%2520involving%2520partially%2520missing%2520features%2520and%250Alocally%2520independent%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-VFL%3A%20A%20New%20Vertical%20Federated%20Learning%20Framework%20with%20Cross%20Completion%0A%20%20and%20Decision%20Subspace%20Alignment&entry.906535625=Qinghua%20Yao%20and%20Xiangrui%20Xu%20and%20Zhize%20Li&entry.1292438233=%20%20Vertical%20Federated%20Learning%20%28VFL%29%20enables%20collaborative%20learning%20by%0Aintegrating%20disjoint%20feature%20subsets%20from%20multiple%20clients/parties.%20However%2C%0AVFL%20typically%20faces%20two%20key%20challenges%3A%20i%29%20the%20requirement%20for%20perfectly%0Aaligned%20data%20samples%20across%20all%20clients%20%28missing%20features%20are%20not%20allowed%29%3B%20ii%29%0Athe%20requirement%20for%20joint%20collaborative%20inference/prediction%20involving%20all%0Aclients%20%28it%20does%20not%20support%20locally%20independent%20inference%20on%20a%20single%20client%29.%0ATo%20address%20these%20challenges%2C%20we%20propose%20X-VFL%2C%20a%20new%20VFL%20framework%20designed%20to%0Adeal%20with%20the%20non-aligned%20data%20samples%20with%20%28partially%29%20missing%20features%20and%20to%0Asupport%20locally%20independent%20inference%20of%20new%20data%20samples%20for%20each%20client.%20In%0Aparticular%2C%20we%20design%20two%20novel%20modules%20in%20X-VFL%3A%20Cross%20Completion%20%28XCom%29%20and%0ADecision%20Subspace%20Alignment%20%28DS-Align%29.%20XCom%20can%20complete/reconstruct%20missing%0Afeatures%20for%20non-aligned%20data%20samples%20by%20leveraging%20information%20from%20other%0Aclients.%20DS-Align%20aligns%20local%20features%20with%20completed%20and%20global%20features%0Aacross%20all%20clients%20within%20the%20decision%20subspace%2C%20thus%20enabling%20locally%0Aindependent%20inference%20at%20each%20client.%20Moreover%2C%20we%20provide%20convergence%20theorems%0Afor%20different%20algorithms%20used%20in%20training%20X-VFL%2C%20showing%20an%20%24O%281/%5Csqrt%7BT%7D%29%24%0Aconvergence%20rate%20for%20SGD-type%20algorithms%20and%20an%20%24O%281/T%29%24%20rate%20for%20PAGE-type%0Aalgorithms%2C%20where%20%24T%24%20denotes%20the%20number%20of%20training%20update%20steps.%20Extensive%0Aexperiments%20on%20real-world%20datasets%20demonstrate%20that%20X-VFL%20significantly%0Aoutperforms%20existing%20methods%2C%20e.g.%2C%20achieving%20a%2015%25%20improvement%20in%20accuracy%20on%0Athe%20image%20CIFAR-10%20dataset%20and%20a%2043%25%20improvement%20on%20the%20medical%20MIMIC-III%0Adataset.%20These%20results%20validate%20the%20practical%20effectiveness%20and%20superiority%20of%0AX-VFL%2C%20particularly%20in%20scenarios%20involving%20partially%20missing%20features%20and%0Alocally%20independent%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05568v1&entry.124074799=Read"},
{"title": "S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient\n  Data and Sparse Token Distillation", "author": "Weilun Feng and Haotong Qin and Chuanguang Yang and Xiangqi Li and Han Yang and Yuqi Li and Zhulin An and Libo Huang and Michele Magno and Yongjun Xu", "abstract": "  Diffusion transformers have emerged as the mainstream paradigm for video\ngeneration models. However, the use of up to billions of parameters incurs\nsignificant computational costs. Quantization offers a promising solution by\nreducing memory usage and accelerating inference. Nonetheless, we observe that\nthe joint modeling of spatial and temporal information in video diffusion\nmodels (V-DMs) leads to extremely long token sequences, which introduces high\ncalibration variance and learning challenges. To address these issues, we\npropose S$^2$Q-VDiT, a post-training quantization framework for V-DMs that\nleverages Salient data and Sparse token distillation. During the calibration\nphase, we identify that quantization performance is highly sensitive to the\nchoice of calibration data. To mitigate this, we introduce\n\\textit{Hessian-aware Salient Data Selection}, which constructs high-quality\ncalibration datasets by considering both diffusion and quantization\ncharacteristics unique to V-DMs. To tackle the learning challenges, we further\nanalyze the sparse attention patterns inherent in V-DMs. Based on this\nobservation, we propose \\textit{Attention-guided Sparse Token Distillation},\nwhich exploits token-wise attention distributions to emphasize tokens that are\nmore influential to the model's output. Under W4A6 quantization, S$^2$Q-VDiT\nachieves lossless performance while delivering $3.9\\times$ model compression\nand $1.3\\times$ inference acceleration. Code will be available at\nhttps://github.com/wlfeng0509/s2q-vdit.\n", "link": "http://arxiv.org/abs/2508.04016v2", "date": "2025-08-07", "relevancy": 2.6397, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6705}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6592}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S%24%5E2%24Q-VDiT%3A%20Accurate%20Quantized%20Video%20Diffusion%20Transformer%20with%20Salient%0A%20%20Data%20and%20Sparse%20Token%20Distillation&body=Title%3A%20S%24%5E2%24Q-VDiT%3A%20Accurate%20Quantized%20Video%20Diffusion%20Transformer%20with%20Salient%0A%20%20Data%20and%20Sparse%20Token%20Distillation%0AAuthor%3A%20Weilun%20Feng%20and%20Haotong%20Qin%20and%20Chuanguang%20Yang%20and%20Xiangqi%20Li%20and%20Han%20Yang%20and%20Yuqi%20Li%20and%20Zhulin%20An%20and%20Libo%20Huang%20and%20Michele%20Magno%20and%20Yongjun%20Xu%0AAbstract%3A%20%20%20Diffusion%20transformers%20have%20emerged%20as%20the%20mainstream%20paradigm%20for%20video%0Ageneration%20models.%20However%2C%20the%20use%20of%20up%20to%20billions%20of%20parameters%20incurs%0Asignificant%20computational%20costs.%20Quantization%20offers%20a%20promising%20solution%20by%0Areducing%20memory%20usage%20and%20accelerating%20inference.%20Nonetheless%2C%20we%20observe%20that%0Athe%20joint%20modeling%20of%20spatial%20and%20temporal%20information%20in%20video%20diffusion%0Amodels%20%28V-DMs%29%20leads%20to%20extremely%20long%20token%20sequences%2C%20which%20introduces%20high%0Acalibration%20variance%20and%20learning%20challenges.%20To%20address%20these%20issues%2C%20we%0Apropose%20S%24%5E2%24Q-VDiT%2C%20a%20post-training%20quantization%20framework%20for%20V-DMs%20that%0Aleverages%20Salient%20data%20and%20Sparse%20token%20distillation.%20During%20the%20calibration%0Aphase%2C%20we%20identify%20that%20quantization%20performance%20is%20highly%20sensitive%20to%20the%0Achoice%20of%20calibration%20data.%20To%20mitigate%20this%2C%20we%20introduce%0A%5Ctextit%7BHessian-aware%20Salient%20Data%20Selection%7D%2C%20which%20constructs%20high-quality%0Acalibration%20datasets%20by%20considering%20both%20diffusion%20and%20quantization%0Acharacteristics%20unique%20to%20V-DMs.%20To%20tackle%20the%20learning%20challenges%2C%20we%20further%0Aanalyze%20the%20sparse%20attention%20patterns%20inherent%20in%20V-DMs.%20Based%20on%20this%0Aobservation%2C%20we%20propose%20%5Ctextit%7BAttention-guided%20Sparse%20Token%20Distillation%7D%2C%0Awhich%20exploits%20token-wise%20attention%20distributions%20to%20emphasize%20tokens%20that%20are%0Amore%20influential%20to%20the%20model%27s%20output.%20Under%20W4A6%20quantization%2C%20S%24%5E2%24Q-VDiT%0Aachieves%20lossless%20performance%20while%20delivering%20%243.9%5Ctimes%24%20model%20compression%0Aand%20%241.3%5Ctimes%24%20inference%20acceleration.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/wlfeng0509/s2q-vdit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04016v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS%2524%255E2%2524Q-VDiT%253A%2520Accurate%2520Quantized%2520Video%2520Diffusion%2520Transformer%2520with%2520Salient%250A%2520%2520Data%2520and%2520Sparse%2520Token%2520Distillation%26entry.906535625%3DWeilun%2520Feng%2520and%2520Haotong%2520Qin%2520and%2520Chuanguang%2520Yang%2520and%2520Xiangqi%2520Li%2520and%2520Han%2520Yang%2520and%2520Yuqi%2520Li%2520and%2520Zhulin%2520An%2520and%2520Libo%2520Huang%2520and%2520Michele%2520Magno%2520and%2520Yongjun%2520Xu%26entry.1292438233%3D%2520%2520Diffusion%2520transformers%2520have%2520emerged%2520as%2520the%2520mainstream%2520paradigm%2520for%2520video%250Ageneration%2520models.%2520However%252C%2520the%2520use%2520of%2520up%2520to%2520billions%2520of%2520parameters%2520incurs%250Asignificant%2520computational%2520costs.%2520Quantization%2520offers%2520a%2520promising%2520solution%2520by%250Areducing%2520memory%2520usage%2520and%2520accelerating%2520inference.%2520Nonetheless%252C%2520we%2520observe%2520that%250Athe%2520joint%2520modeling%2520of%2520spatial%2520and%2520temporal%2520information%2520in%2520video%2520diffusion%250Amodels%2520%2528V-DMs%2529%2520leads%2520to%2520extremely%2520long%2520token%2520sequences%252C%2520which%2520introduces%2520high%250Acalibration%2520variance%2520and%2520learning%2520challenges.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520S%2524%255E2%2524Q-VDiT%252C%2520a%2520post-training%2520quantization%2520framework%2520for%2520V-DMs%2520that%250Aleverages%2520Salient%2520data%2520and%2520Sparse%2520token%2520distillation.%2520During%2520the%2520calibration%250Aphase%252C%2520we%2520identify%2520that%2520quantization%2520performance%2520is%2520highly%2520sensitive%2520to%2520the%250Achoice%2520of%2520calibration%2520data.%2520To%2520mitigate%2520this%252C%2520we%2520introduce%250A%255Ctextit%257BHessian-aware%2520Salient%2520Data%2520Selection%257D%252C%2520which%2520constructs%2520high-quality%250Acalibration%2520datasets%2520by%2520considering%2520both%2520diffusion%2520and%2520quantization%250Acharacteristics%2520unique%2520to%2520V-DMs.%2520To%2520tackle%2520the%2520learning%2520challenges%252C%2520we%2520further%250Aanalyze%2520the%2520sparse%2520attention%2520patterns%2520inherent%2520in%2520V-DMs.%2520Based%2520on%2520this%250Aobservation%252C%2520we%2520propose%2520%255Ctextit%257BAttention-guided%2520Sparse%2520Token%2520Distillation%257D%252C%250Awhich%2520exploits%2520token-wise%2520attention%2520distributions%2520to%2520emphasize%2520tokens%2520that%2520are%250Amore%2520influential%2520to%2520the%2520model%2527s%2520output.%2520Under%2520W4A6%2520quantization%252C%2520S%2524%255E2%2524Q-VDiT%250Aachieves%2520lossless%2520performance%2520while%2520delivering%2520%25243.9%255Ctimes%2524%2520model%2520compression%250Aand%2520%25241.3%255Ctimes%2524%2520inference%2520acceleration.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/wlfeng0509/s2q-vdit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04016v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S%24%5E2%24Q-VDiT%3A%20Accurate%20Quantized%20Video%20Diffusion%20Transformer%20with%20Salient%0A%20%20Data%20and%20Sparse%20Token%20Distillation&entry.906535625=Weilun%20Feng%20and%20Haotong%20Qin%20and%20Chuanguang%20Yang%20and%20Xiangqi%20Li%20and%20Han%20Yang%20and%20Yuqi%20Li%20and%20Zhulin%20An%20and%20Libo%20Huang%20and%20Michele%20Magno%20and%20Yongjun%20Xu&entry.1292438233=%20%20Diffusion%20transformers%20have%20emerged%20as%20the%20mainstream%20paradigm%20for%20video%0Ageneration%20models.%20However%2C%20the%20use%20of%20up%20to%20billions%20of%20parameters%20incurs%0Asignificant%20computational%20costs.%20Quantization%20offers%20a%20promising%20solution%20by%0Areducing%20memory%20usage%20and%20accelerating%20inference.%20Nonetheless%2C%20we%20observe%20that%0Athe%20joint%20modeling%20of%20spatial%20and%20temporal%20information%20in%20video%20diffusion%0Amodels%20%28V-DMs%29%20leads%20to%20extremely%20long%20token%20sequences%2C%20which%20introduces%20high%0Acalibration%20variance%20and%20learning%20challenges.%20To%20address%20these%20issues%2C%20we%0Apropose%20S%24%5E2%24Q-VDiT%2C%20a%20post-training%20quantization%20framework%20for%20V-DMs%20that%0Aleverages%20Salient%20data%20and%20Sparse%20token%20distillation.%20During%20the%20calibration%0Aphase%2C%20we%20identify%20that%20quantization%20performance%20is%20highly%20sensitive%20to%20the%0Achoice%20of%20calibration%20data.%20To%20mitigate%20this%2C%20we%20introduce%0A%5Ctextit%7BHessian-aware%20Salient%20Data%20Selection%7D%2C%20which%20constructs%20high-quality%0Acalibration%20datasets%20by%20considering%20both%20diffusion%20and%20quantization%0Acharacteristics%20unique%20to%20V-DMs.%20To%20tackle%20the%20learning%20challenges%2C%20we%20further%0Aanalyze%20the%20sparse%20attention%20patterns%20inherent%20in%20V-DMs.%20Based%20on%20this%0Aobservation%2C%20we%20propose%20%5Ctextit%7BAttention-guided%20Sparse%20Token%20Distillation%7D%2C%0Awhich%20exploits%20token-wise%20attention%20distributions%20to%20emphasize%20tokens%20that%20are%0Amore%20influential%20to%20the%20model%27s%20output.%20Under%20W4A6%20quantization%2C%20S%24%5E2%24Q-VDiT%0Aachieves%20lossless%20performance%20while%20delivering%20%243.9%5Ctimes%24%20model%20compression%0Aand%20%241.3%5Ctimes%24%20inference%20acceleration.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/wlfeng0509/s2q-vdit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04016v2&entry.124074799=Read"},
{"title": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs\n  to Enhance Reasoning Capabilities", "author": "Shuo Cai and Su Lu and Qi Zhou and Kejing Yang and Zhijie Sang and Congkai Xie and Hongxia Yang", "abstract": "  Large language models (LLMs) have exhibited impressive reasoning abilities on\na wide range of complex tasks. However, enhancing these capabilities through\npost-training remains resource intensive, particularly in terms of data and\ncomputational cost. Although recent efforts have sought to improve sample\nefficiency through selective data curation, existing methods often rely on\nheuristic or task-specific strategies that hinder scalability. In this work, we\nintroduce InfiAlign, a scalable and sample-efficient post-training framework\nthat integrates supervised fine-tuning (SFT) with Direct Preference\nOptimization (DPO) to align LLMs for enhanced reasoning. At the core of\nInfiAlign is a robust data selection pipeline that automatically curates\nhigh-quality alignment data from open-source reasoning datasets using\nmultidimensional quality metrics. This pipeline enables significant performance\ngains while drastically reducing data requirements and remains extensible to\nnew data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model\nachieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only\napproximately 12% of the training data, and demonstrates strong generalization\nacross diverse reasoning tasks. Additional improvements are obtained through\nthe application of DPO, with particularly notable gains in mathematical\nreasoning tasks. The model achieves an average improvement of 3.89% on AIME\n24/25 benchmarks. Our results highlight the effectiveness of combining\nprincipled data selection with full-stage post-training, offering a practical\nsolution for aligning large reasoning models in a scalable and data-efficient\nmanner. The model checkpoints are available at\nhttps://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.\n", "link": "http://arxiv.org/abs/2508.05496v1", "date": "2025-08-07", "relevancy": 2.635, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiAlign%3A%20A%20Scalable%20and%20Sample-Efficient%20Framework%20for%20Aligning%20LLMs%0A%20%20to%20Enhance%20Reasoning%20Capabilities&body=Title%3A%20InfiAlign%3A%20A%20Scalable%20and%20Sample-Efficient%20Framework%20for%20Aligning%20LLMs%0A%20%20to%20Enhance%20Reasoning%20Capabilities%0AAuthor%3A%20Shuo%20Cai%20and%20Su%20Lu%20and%20Qi%20Zhou%20and%20Kejing%20Yang%20and%20Zhijie%20Sang%20and%20Congkai%20Xie%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20impressive%20reasoning%20abilities%20on%0Aa%20wide%20range%20of%20complex%20tasks.%20However%2C%20enhancing%20these%20capabilities%20through%0Apost-training%20remains%20resource%20intensive%2C%20particularly%20in%20terms%20of%20data%20and%0Acomputational%20cost.%20Although%20recent%20efforts%20have%20sought%20to%20improve%20sample%0Aefficiency%20through%20selective%20data%20curation%2C%20existing%20methods%20often%20rely%20on%0Aheuristic%20or%20task-specific%20strategies%20that%20hinder%20scalability.%20In%20this%20work%2C%20we%0Aintroduce%20InfiAlign%2C%20a%20scalable%20and%20sample-efficient%20post-training%20framework%0Athat%20integrates%20supervised%20fine-tuning%20%28SFT%29%20with%20Direct%20Preference%0AOptimization%20%28DPO%29%20to%20align%20LLMs%20for%20enhanced%20reasoning.%20At%20the%20core%20of%0AInfiAlign%20is%20a%20robust%20data%20selection%20pipeline%20that%20automatically%20curates%0Ahigh-quality%20alignment%20data%20from%20open-source%20reasoning%20datasets%20using%0Amultidimensional%20quality%20metrics.%20This%20pipeline%20enables%20significant%20performance%0Agains%20while%20drastically%20reducing%20data%20requirements%20and%20remains%20extensible%20to%0Anew%20data%20sources.%20When%20applied%20to%20the%20Qwen2.5-Math-7B-Base%20model%2C%20our%20SFT%20model%0Aachieves%20performance%20on%20par%20with%20DeepSeek-R1-Distill-Qwen-7B%2C%20while%20using%20only%0Aapproximately%2012%25%20of%20the%20training%20data%2C%20and%20demonstrates%20strong%20generalization%0Aacross%20diverse%20reasoning%20tasks.%20Additional%20improvements%20are%20obtained%20through%0Athe%20application%20of%20DPO%2C%20with%20particularly%20notable%20gains%20in%20mathematical%0Areasoning%20tasks.%20The%20model%20achieves%20an%20average%20improvement%20of%203.89%25%20on%20AIME%0A24/25%20benchmarks.%20Our%20results%20highlight%20the%20effectiveness%20of%20combining%0Aprincipled%20data%20selection%20with%20full-stage%20post-training%2C%20offering%20a%20practical%0Asolution%20for%20aligning%20large%20reasoning%20models%20in%20a%20scalable%20and%20data-efficient%0Amanner.%20The%20model%20checkpoints%20are%20available%20at%0Ahttps%3A//huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiAlign%253A%2520A%2520Scalable%2520and%2520Sample-Efficient%2520Framework%2520for%2520Aligning%2520LLMs%250A%2520%2520to%2520Enhance%2520Reasoning%2520Capabilities%26entry.906535625%3DShuo%2520Cai%2520and%2520Su%2520Lu%2520and%2520Qi%2520Zhou%2520and%2520Kejing%2520Yang%2520and%2520Zhijie%2520Sang%2520and%2520Congkai%2520Xie%2520and%2520Hongxia%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520exhibited%2520impressive%2520reasoning%2520abilities%2520on%250Aa%2520wide%2520range%2520of%2520complex%2520tasks.%2520However%252C%2520enhancing%2520these%2520capabilities%2520through%250Apost-training%2520remains%2520resource%2520intensive%252C%2520particularly%2520in%2520terms%2520of%2520data%2520and%250Acomputational%2520cost.%2520Although%2520recent%2520efforts%2520have%2520sought%2520to%2520improve%2520sample%250Aefficiency%2520through%2520selective%2520data%2520curation%252C%2520existing%2520methods%2520often%2520rely%2520on%250Aheuristic%2520or%2520task-specific%2520strategies%2520that%2520hinder%2520scalability.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520InfiAlign%252C%2520a%2520scalable%2520and%2520sample-efficient%2520post-training%2520framework%250Athat%2520integrates%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520with%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529%2520to%2520align%2520LLMs%2520for%2520enhanced%2520reasoning.%2520At%2520the%2520core%2520of%250AInfiAlign%2520is%2520a%2520robust%2520data%2520selection%2520pipeline%2520that%2520automatically%2520curates%250Ahigh-quality%2520alignment%2520data%2520from%2520open-source%2520reasoning%2520datasets%2520using%250Amultidimensional%2520quality%2520metrics.%2520This%2520pipeline%2520enables%2520significant%2520performance%250Agains%2520while%2520drastically%2520reducing%2520data%2520requirements%2520and%2520remains%2520extensible%2520to%250Anew%2520data%2520sources.%2520When%2520applied%2520to%2520the%2520Qwen2.5-Math-7B-Base%2520model%252C%2520our%2520SFT%2520model%250Aachieves%2520performance%2520on%2520par%2520with%2520DeepSeek-R1-Distill-Qwen-7B%252C%2520while%2520using%2520only%250Aapproximately%252012%2525%2520of%2520the%2520training%2520data%252C%2520and%2520demonstrates%2520strong%2520generalization%250Aacross%2520diverse%2520reasoning%2520tasks.%2520Additional%2520improvements%2520are%2520obtained%2520through%250Athe%2520application%2520of%2520DPO%252C%2520with%2520particularly%2520notable%2520gains%2520in%2520mathematical%250Areasoning%2520tasks.%2520The%2520model%2520achieves%2520an%2520average%2520improvement%2520of%25203.89%2525%2520on%2520AIME%250A24/25%2520benchmarks.%2520Our%2520results%2520highlight%2520the%2520effectiveness%2520of%2520combining%250Aprincipled%2520data%2520selection%2520with%2520full-stage%2520post-training%252C%2520offering%2520a%2520practical%250Asolution%2520for%2520aligning%2520large%2520reasoning%2520models%2520in%2520a%2520scalable%2520and%2520data-efficient%250Amanner.%2520The%2520model%2520checkpoints%2520are%2520available%2520at%250Ahttps%253A//huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiAlign%3A%20A%20Scalable%20and%20Sample-Efficient%20Framework%20for%20Aligning%20LLMs%0A%20%20to%20Enhance%20Reasoning%20Capabilities&entry.906535625=Shuo%20Cai%20and%20Su%20Lu%20and%20Qi%20Zhou%20and%20Kejing%20Yang%20and%20Zhijie%20Sang%20and%20Congkai%20Xie%20and%20Hongxia%20Yang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20impressive%20reasoning%20abilities%20on%0Aa%20wide%20range%20of%20complex%20tasks.%20However%2C%20enhancing%20these%20capabilities%20through%0Apost-training%20remains%20resource%20intensive%2C%20particularly%20in%20terms%20of%20data%20and%0Acomputational%20cost.%20Although%20recent%20efforts%20have%20sought%20to%20improve%20sample%0Aefficiency%20through%20selective%20data%20curation%2C%20existing%20methods%20often%20rely%20on%0Aheuristic%20or%20task-specific%20strategies%20that%20hinder%20scalability.%20In%20this%20work%2C%20we%0Aintroduce%20InfiAlign%2C%20a%20scalable%20and%20sample-efficient%20post-training%20framework%0Athat%20integrates%20supervised%20fine-tuning%20%28SFT%29%20with%20Direct%20Preference%0AOptimization%20%28DPO%29%20to%20align%20LLMs%20for%20enhanced%20reasoning.%20At%20the%20core%20of%0AInfiAlign%20is%20a%20robust%20data%20selection%20pipeline%20that%20automatically%20curates%0Ahigh-quality%20alignment%20data%20from%20open-source%20reasoning%20datasets%20using%0Amultidimensional%20quality%20metrics.%20This%20pipeline%20enables%20significant%20performance%0Agains%20while%20drastically%20reducing%20data%20requirements%20and%20remains%20extensible%20to%0Anew%20data%20sources.%20When%20applied%20to%20the%20Qwen2.5-Math-7B-Base%20model%2C%20our%20SFT%20model%0Aachieves%20performance%20on%20par%20with%20DeepSeek-R1-Distill-Qwen-7B%2C%20while%20using%20only%0Aapproximately%2012%25%20of%20the%20training%20data%2C%20and%20demonstrates%20strong%20generalization%0Aacross%20diverse%20reasoning%20tasks.%20Additional%20improvements%20are%20obtained%20through%0Athe%20application%20of%20DPO%2C%20with%20particularly%20notable%20gains%20in%20mathematical%0Areasoning%20tasks.%20The%20model%20achieves%20an%20average%20improvement%20of%203.89%25%20on%20AIME%0A24/25%20benchmarks.%20Our%20results%20highlight%20the%20effectiveness%20of%20combining%0Aprincipled%20data%20selection%20with%20full-stage%20post-training%2C%20offering%20a%20practical%0Asolution%20for%20aligning%20large%20reasoning%20models%20in%20a%20scalable%20and%20data-efficient%0Amanner.%20The%20model%20checkpoints%20are%20available%20at%0Ahttps%3A//huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05496v1&entry.124074799=Read"},
{"title": "Symmetry Understanding of 3D Shapes via Chirality Disentanglement", "author": "Weikang Wang and Tobias Wei\u00dfberg and Nafie El Amrani and Florian Bernard", "abstract": "  Chirality information (i.e. information that allows distinguishing left from\nright) is ubiquitous for various data modes in computer vision, including\nimages, videos, point clouds, and meshes. While chirality has been extensively\nstudied in the image domain, its exploration in shape analysis (such as point\nclouds and meshes) remains underdeveloped. Although many shape vertex\ndescriptors have shown appealing properties (e.g. robustness to rigid-body\ntransformations), they are often not able to disambiguate between left and\nright symmetric parts. Considering the ubiquity of chirality information in\ndifferent shape analysis problems and the lack of chirality-aware features\nwithin current shape descriptors, developing a chirality feature extractor\nbecomes necessary and urgent. Based on the recent Diff3F framework, we propose\nan unsupervised chirality feature extraction pipeline to decorate shape\nvertices with chirality-aware information, extracted from 2D foundation models.\nWe evaluated the extracted chirality features through quantitative and\nqualitative experiments across diverse datasets. Results from downstream tasks\nincluding left-right disentanglement, shape matching, and part segmentation\ndemonstrate their effectiveness and practical utility. Project page:\nhttps://wei-kang-wang.github.io/chirality/\n", "link": "http://arxiv.org/abs/2508.05505v1", "date": "2025-08-07", "relevancy": 2.632, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5533}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.513}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symmetry%20Understanding%20of%203D%20Shapes%20via%20Chirality%20Disentanglement&body=Title%3A%20Symmetry%20Understanding%20of%203D%20Shapes%20via%20Chirality%20Disentanglement%0AAuthor%3A%20Weikang%20Wang%20and%20Tobias%20Wei%C3%9Fberg%20and%20Nafie%20El%20Amrani%20and%20Florian%20Bernard%0AAbstract%3A%20%20%20Chirality%20information%20%28i.e.%20information%20that%20allows%20distinguishing%20left%20from%0Aright%29%20is%20ubiquitous%20for%20various%20data%20modes%20in%20computer%20vision%2C%20including%0Aimages%2C%20videos%2C%20point%20clouds%2C%20and%20meshes.%20While%20chirality%20has%20been%20extensively%0Astudied%20in%20the%20image%20domain%2C%20its%20exploration%20in%20shape%20analysis%20%28such%20as%20point%0Aclouds%20and%20meshes%29%20remains%20underdeveloped.%20Although%20many%20shape%20vertex%0Adescriptors%20have%20shown%20appealing%20properties%20%28e.g.%20robustness%20to%20rigid-body%0Atransformations%29%2C%20they%20are%20often%20not%20able%20to%20disambiguate%20between%20left%20and%0Aright%20symmetric%20parts.%20Considering%20the%20ubiquity%20of%20chirality%20information%20in%0Adifferent%20shape%20analysis%20problems%20and%20the%20lack%20of%20chirality-aware%20features%0Awithin%20current%20shape%20descriptors%2C%20developing%20a%20chirality%20feature%20extractor%0Abecomes%20necessary%20and%20urgent.%20Based%20on%20the%20recent%20Diff3F%20framework%2C%20we%20propose%0Aan%20unsupervised%20chirality%20feature%20extraction%20pipeline%20to%20decorate%20shape%0Avertices%20with%20chirality-aware%20information%2C%20extracted%20from%202D%20foundation%20models.%0AWe%20evaluated%20the%20extracted%20chirality%20features%20through%20quantitative%20and%0Aqualitative%20experiments%20across%20diverse%20datasets.%20Results%20from%20downstream%20tasks%0Aincluding%20left-right%20disentanglement%2C%20shape%20matching%2C%20and%20part%20segmentation%0Ademonstrate%20their%20effectiveness%20and%20practical%20utility.%20Project%20page%3A%0Ahttps%3A//wei-kang-wang.github.io/chirality/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymmetry%2520Understanding%2520of%25203D%2520Shapes%2520via%2520Chirality%2520Disentanglement%26entry.906535625%3DWeikang%2520Wang%2520and%2520Tobias%2520Wei%25C3%259Fberg%2520and%2520Nafie%2520El%2520Amrani%2520and%2520Florian%2520Bernard%26entry.1292438233%3D%2520%2520Chirality%2520information%2520%2528i.e.%2520information%2520that%2520allows%2520distinguishing%2520left%2520from%250Aright%2529%2520is%2520ubiquitous%2520for%2520various%2520data%2520modes%2520in%2520computer%2520vision%252C%2520including%250Aimages%252C%2520videos%252C%2520point%2520clouds%252C%2520and%2520meshes.%2520While%2520chirality%2520has%2520been%2520extensively%250Astudied%2520in%2520the%2520image%2520domain%252C%2520its%2520exploration%2520in%2520shape%2520analysis%2520%2528such%2520as%2520point%250Aclouds%2520and%2520meshes%2529%2520remains%2520underdeveloped.%2520Although%2520many%2520shape%2520vertex%250Adescriptors%2520have%2520shown%2520appealing%2520properties%2520%2528e.g.%2520robustness%2520to%2520rigid-body%250Atransformations%2529%252C%2520they%2520are%2520often%2520not%2520able%2520to%2520disambiguate%2520between%2520left%2520and%250Aright%2520symmetric%2520parts.%2520Considering%2520the%2520ubiquity%2520of%2520chirality%2520information%2520in%250Adifferent%2520shape%2520analysis%2520problems%2520and%2520the%2520lack%2520of%2520chirality-aware%2520features%250Awithin%2520current%2520shape%2520descriptors%252C%2520developing%2520a%2520chirality%2520feature%2520extractor%250Abecomes%2520necessary%2520and%2520urgent.%2520Based%2520on%2520the%2520recent%2520Diff3F%2520framework%252C%2520we%2520propose%250Aan%2520unsupervised%2520chirality%2520feature%2520extraction%2520pipeline%2520to%2520decorate%2520shape%250Avertices%2520with%2520chirality-aware%2520information%252C%2520extracted%2520from%25202D%2520foundation%2520models.%250AWe%2520evaluated%2520the%2520extracted%2520chirality%2520features%2520through%2520quantitative%2520and%250Aqualitative%2520experiments%2520across%2520diverse%2520datasets.%2520Results%2520from%2520downstream%2520tasks%250Aincluding%2520left-right%2520disentanglement%252C%2520shape%2520matching%252C%2520and%2520part%2520segmentation%250Ademonstrate%2520their%2520effectiveness%2520and%2520practical%2520utility.%2520Project%2520page%253A%250Ahttps%253A//wei-kang-wang.github.io/chirality/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symmetry%20Understanding%20of%203D%20Shapes%20via%20Chirality%20Disentanglement&entry.906535625=Weikang%20Wang%20and%20Tobias%20Wei%C3%9Fberg%20and%20Nafie%20El%20Amrani%20and%20Florian%20Bernard&entry.1292438233=%20%20Chirality%20information%20%28i.e.%20information%20that%20allows%20distinguishing%20left%20from%0Aright%29%20is%20ubiquitous%20for%20various%20data%20modes%20in%20computer%20vision%2C%20including%0Aimages%2C%20videos%2C%20point%20clouds%2C%20and%20meshes.%20While%20chirality%20has%20been%20extensively%0Astudied%20in%20the%20image%20domain%2C%20its%20exploration%20in%20shape%20analysis%20%28such%20as%20point%0Aclouds%20and%20meshes%29%20remains%20underdeveloped.%20Although%20many%20shape%20vertex%0Adescriptors%20have%20shown%20appealing%20properties%20%28e.g.%20robustness%20to%20rigid-body%0Atransformations%29%2C%20they%20are%20often%20not%20able%20to%20disambiguate%20between%20left%20and%0Aright%20symmetric%20parts.%20Considering%20the%20ubiquity%20of%20chirality%20information%20in%0Adifferent%20shape%20analysis%20problems%20and%20the%20lack%20of%20chirality-aware%20features%0Awithin%20current%20shape%20descriptors%2C%20developing%20a%20chirality%20feature%20extractor%0Abecomes%20necessary%20and%20urgent.%20Based%20on%20the%20recent%20Diff3F%20framework%2C%20we%20propose%0Aan%20unsupervised%20chirality%20feature%20extraction%20pipeline%20to%20decorate%20shape%0Avertices%20with%20chirality-aware%20information%2C%20extracted%20from%202D%20foundation%20models.%0AWe%20evaluated%20the%20extracted%20chirality%20features%20through%20quantitative%20and%0Aqualitative%20experiments%20across%20diverse%20datasets.%20Results%20from%20downstream%20tasks%0Aincluding%20left-right%20disentanglement%2C%20shape%20matching%2C%20and%20part%20segmentation%0Ademonstrate%20their%20effectiveness%20and%20practical%20utility.%20Project%20page%3A%0Ahttps%3A//wei-kang-wang.github.io/chirality/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05505v1&entry.124074799=Read"},
{"title": "Deformable Attention Graph Representation Learning for Histopathology\n  Whole Slide Image Analysis", "author": "Mingxi Fu and Xitong Ling and Yuxuan Chen and Jiawen Li and fanglei fu and Huaitian Yuan and Tian Guan and Yonghong He and Lianghui Zhu", "abstract": "  Accurate classification of Whole Slide Images (WSIs) and Regions of Interest\n(ROIs) is a fundamental challenge in computational pathology. While mainstream\napproaches often adopt Multiple Instance Learning (MIL), they struggle to\ncapture the spatial dependencies among tissue structures. Graph Neural Networks\n(GNNs) have emerged as a solution to model inter-instance relationships, yet\nmost rely on static graph topologies and overlook the physical spatial\npositions of tissue patches. Moreover, conventional attention mechanisms lack\nspecificity, limiting their ability to focus on structurally relevant regions.\nIn this work, we propose a novel GNN framework with deformable attention for\npathology image analysis. We construct a dynamic weighted directed graph based\non patch features, where each node aggregates contextual information from its\nneighbors via attention-weighted edges. Specifically, we incorporate learnable\nspatial offsets informed by the real coordinates of each patch, enabling the\nmodel to adaptively attend to morphologically relevant regions across the\nslide. This design significantly enhances the contextual field while preserving\nspatial specificity. Our framework achieves state-of-the-art performance on\nfour benchmark datasets (TCGA-COAD, BRACS, gastric intestinal metaplasia\ngrading, and intestinal ROI classification), demonstrating the power of\ndeformable attention in capturing complex spatial structures in WSIs and ROIs.\n", "link": "http://arxiv.org/abs/2508.05382v1", "date": "2025-08-07", "relevancy": 2.5403, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5242}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deformable%20Attention%20Graph%20Representation%20Learning%20for%20Histopathology%0A%20%20Whole%20Slide%20Image%20Analysis&body=Title%3A%20Deformable%20Attention%20Graph%20Representation%20Learning%20for%20Histopathology%0A%20%20Whole%20Slide%20Image%20Analysis%0AAuthor%3A%20Mingxi%20Fu%20and%20Xitong%20Ling%20and%20Yuxuan%20Chen%20and%20Jiawen%20Li%20and%20fanglei%20fu%20and%20Huaitian%20Yuan%20and%20Tian%20Guan%20and%20Yonghong%20He%20and%20Lianghui%20Zhu%0AAbstract%3A%20%20%20Accurate%20classification%20of%20Whole%20Slide%20Images%20%28WSIs%29%20and%20Regions%20of%20Interest%0A%28ROIs%29%20is%20a%20fundamental%20challenge%20in%20computational%20pathology.%20While%20mainstream%0Aapproaches%20often%20adopt%20Multiple%20Instance%20Learning%20%28MIL%29%2C%20they%20struggle%20to%0Acapture%20the%20spatial%20dependencies%20among%20tissue%20structures.%20Graph%20Neural%20Networks%0A%28GNNs%29%20have%20emerged%20as%20a%20solution%20to%20model%20inter-instance%20relationships%2C%20yet%0Amost%20rely%20on%20static%20graph%20topologies%20and%20overlook%20the%20physical%20spatial%0Apositions%20of%20tissue%20patches.%20Moreover%2C%20conventional%20attention%20mechanisms%20lack%0Aspecificity%2C%20limiting%20their%20ability%20to%20focus%20on%20structurally%20relevant%20regions.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20GNN%20framework%20with%20deformable%20attention%20for%0Apathology%20image%20analysis.%20We%20construct%20a%20dynamic%20weighted%20directed%20graph%20based%0Aon%20patch%20features%2C%20where%20each%20node%20aggregates%20contextual%20information%20from%20its%0Aneighbors%20via%20attention-weighted%20edges.%20Specifically%2C%20we%20incorporate%20learnable%0Aspatial%20offsets%20informed%20by%20the%20real%20coordinates%20of%20each%20patch%2C%20enabling%20the%0Amodel%20to%20adaptively%20attend%20to%20morphologically%20relevant%20regions%20across%20the%0Aslide.%20This%20design%20significantly%20enhances%20the%20contextual%20field%20while%20preserving%0Aspatial%20specificity.%20Our%20framework%20achieves%20state-of-the-art%20performance%20on%0Afour%20benchmark%20datasets%20%28TCGA-COAD%2C%20BRACS%2C%20gastric%20intestinal%20metaplasia%0Agrading%2C%20and%20intestinal%20ROI%20classification%29%2C%20demonstrating%20the%20power%20of%0Adeformable%20attention%20in%20capturing%20complex%20spatial%20structures%20in%20WSIs%20and%20ROIs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeformable%2520Attention%2520Graph%2520Representation%2520Learning%2520for%2520Histopathology%250A%2520%2520Whole%2520Slide%2520Image%2520Analysis%26entry.906535625%3DMingxi%2520Fu%2520and%2520Xitong%2520Ling%2520and%2520Yuxuan%2520Chen%2520and%2520Jiawen%2520Li%2520and%2520fanglei%2520fu%2520and%2520Huaitian%2520Yuan%2520and%2520Tian%2520Guan%2520and%2520Yonghong%2520He%2520and%2520Lianghui%2520Zhu%26entry.1292438233%3D%2520%2520Accurate%2520classification%2520of%2520Whole%2520Slide%2520Images%2520%2528WSIs%2529%2520and%2520Regions%2520of%2520Interest%250A%2528ROIs%2529%2520is%2520a%2520fundamental%2520challenge%2520in%2520computational%2520pathology.%2520While%2520mainstream%250Aapproaches%2520often%2520adopt%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%252C%2520they%2520struggle%2520to%250Acapture%2520the%2520spatial%2520dependencies%2520among%2520tissue%2520structures.%2520Graph%2520Neural%2520Networks%250A%2528GNNs%2529%2520have%2520emerged%2520as%2520a%2520solution%2520to%2520model%2520inter-instance%2520relationships%252C%2520yet%250Amost%2520rely%2520on%2520static%2520graph%2520topologies%2520and%2520overlook%2520the%2520physical%2520spatial%250Apositions%2520of%2520tissue%2520patches.%2520Moreover%252C%2520conventional%2520attention%2520mechanisms%2520lack%250Aspecificity%252C%2520limiting%2520their%2520ability%2520to%2520focus%2520on%2520structurally%2520relevant%2520regions.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520GNN%2520framework%2520with%2520deformable%2520attention%2520for%250Apathology%2520image%2520analysis.%2520We%2520construct%2520a%2520dynamic%2520weighted%2520directed%2520graph%2520based%250Aon%2520patch%2520features%252C%2520where%2520each%2520node%2520aggregates%2520contextual%2520information%2520from%2520its%250Aneighbors%2520via%2520attention-weighted%2520edges.%2520Specifically%252C%2520we%2520incorporate%2520learnable%250Aspatial%2520offsets%2520informed%2520by%2520the%2520real%2520coordinates%2520of%2520each%2520patch%252C%2520enabling%2520the%250Amodel%2520to%2520adaptively%2520attend%2520to%2520morphologically%2520relevant%2520regions%2520across%2520the%250Aslide.%2520This%2520design%2520significantly%2520enhances%2520the%2520contextual%2520field%2520while%2520preserving%250Aspatial%2520specificity.%2520Our%2520framework%2520achieves%2520state-of-the-art%2520performance%2520on%250Afour%2520benchmark%2520datasets%2520%2528TCGA-COAD%252C%2520BRACS%252C%2520gastric%2520intestinal%2520metaplasia%250Agrading%252C%2520and%2520intestinal%2520ROI%2520classification%2529%252C%2520demonstrating%2520the%2520power%2520of%250Adeformable%2520attention%2520in%2520capturing%2520complex%2520spatial%2520structures%2520in%2520WSIs%2520and%2520ROIs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deformable%20Attention%20Graph%20Representation%20Learning%20for%20Histopathology%0A%20%20Whole%20Slide%20Image%20Analysis&entry.906535625=Mingxi%20Fu%20and%20Xitong%20Ling%20and%20Yuxuan%20Chen%20and%20Jiawen%20Li%20and%20fanglei%20fu%20and%20Huaitian%20Yuan%20and%20Tian%20Guan%20and%20Yonghong%20He%20and%20Lianghui%20Zhu&entry.1292438233=%20%20Accurate%20classification%20of%20Whole%20Slide%20Images%20%28WSIs%29%20and%20Regions%20of%20Interest%0A%28ROIs%29%20is%20a%20fundamental%20challenge%20in%20computational%20pathology.%20While%20mainstream%0Aapproaches%20often%20adopt%20Multiple%20Instance%20Learning%20%28MIL%29%2C%20they%20struggle%20to%0Acapture%20the%20spatial%20dependencies%20among%20tissue%20structures.%20Graph%20Neural%20Networks%0A%28GNNs%29%20have%20emerged%20as%20a%20solution%20to%20model%20inter-instance%20relationships%2C%20yet%0Amost%20rely%20on%20static%20graph%20topologies%20and%20overlook%20the%20physical%20spatial%0Apositions%20of%20tissue%20patches.%20Moreover%2C%20conventional%20attention%20mechanisms%20lack%0Aspecificity%2C%20limiting%20their%20ability%20to%20focus%20on%20structurally%20relevant%20regions.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20GNN%20framework%20with%20deformable%20attention%20for%0Apathology%20image%20analysis.%20We%20construct%20a%20dynamic%20weighted%20directed%20graph%20based%0Aon%20patch%20features%2C%20where%20each%20node%20aggregates%20contextual%20information%20from%20its%0Aneighbors%20via%20attention-weighted%20edges.%20Specifically%2C%20we%20incorporate%20learnable%0Aspatial%20offsets%20informed%20by%20the%20real%20coordinates%20of%20each%20patch%2C%20enabling%20the%0Amodel%20to%20adaptively%20attend%20to%20morphologically%20relevant%20regions%20across%20the%0Aslide.%20This%20design%20significantly%20enhances%20the%20contextual%20field%20while%20preserving%0Aspatial%20specificity.%20Our%20framework%20achieves%20state-of-the-art%20performance%20on%0Afour%20benchmark%20datasets%20%28TCGA-COAD%2C%20BRACS%2C%20gastric%20intestinal%20metaplasia%0Agrading%2C%20and%20intestinal%20ROI%20classification%29%2C%20demonstrating%20the%20power%20of%0Adeformable%20attention%20in%20capturing%20complex%20spatial%20structures%20in%20WSIs%20and%20ROIs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05382v1&entry.124074799=Read"},
{"title": "From Detection to Correction: Backdoor-Resilient Face Recognition via\n  Vision-Language Trigger Detection and Noise-Based Neutralization", "author": "Farah Wahida and M. A. P. Chamikara and Yashothara Shanmugarasa and Mohan Baruwal Chhetri and Thilina Ranbaduge and Ibrahim Khalil", "abstract": "  Biometric systems, such as face recognition systems powered by deep neural\nnetworks (DNNs), rely on large and highly sensitive datasets. Backdoor attacks\ncan subvert these systems by manipulating the training process. By inserting a\nsmall trigger, such as a sticker, make-up, or patterned mask, into a few\ntraining images, an adversary can later present the same trigger during\nauthentication to be falsely recognized as another individual, thereby gaining\nunauthorized access. Existing defense mechanisms against backdoor attacks still\nface challenges in precisely identifying and mitigating poisoned images without\ncompromising data utility, which undermines the overall reliability of the\nsystem. We propose a novel and generalizable approach, TrueBiometric:\nTrustworthy Biometrics, which accurately detects poisoned images using a\nmajority voting mechanism leveraging multiple state-of-the-art large vision\nlanguage models. Once identified, poisoned samples are corrected using targeted\nand calibrated corrective noise. Our extensive empirical results demonstrate\nthat TrueBiometric detects and corrects poisoned images with 100\\% accuracy\nwithout compromising accuracy on clean images. Compared to existing\nstate-of-the-art approaches, TrueBiometric offers a more practical, accurate,\nand effective solution for mitigating backdoor attacks in face recognition\nsystems.\n", "link": "http://arxiv.org/abs/2508.05409v1", "date": "2025-08-07", "relevancy": 2.503, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5068}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4977}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Detection%20to%20Correction%3A%20Backdoor-Resilient%20Face%20Recognition%20via%0A%20%20Vision-Language%20Trigger%20Detection%20and%20Noise-Based%20Neutralization&body=Title%3A%20From%20Detection%20to%20Correction%3A%20Backdoor-Resilient%20Face%20Recognition%20via%0A%20%20Vision-Language%20Trigger%20Detection%20and%20Noise-Based%20Neutralization%0AAuthor%3A%20Farah%20Wahida%20and%20M.%20A.%20P.%20Chamikara%20and%20Yashothara%20Shanmugarasa%20and%20Mohan%20Baruwal%20Chhetri%20and%20Thilina%20Ranbaduge%20and%20Ibrahim%20Khalil%0AAbstract%3A%20%20%20Biometric%20systems%2C%20such%20as%20face%20recognition%20systems%20powered%20by%20deep%20neural%0Anetworks%20%28DNNs%29%2C%20rely%20on%20large%20and%20highly%20sensitive%20datasets.%20Backdoor%20attacks%0Acan%20subvert%20these%20systems%20by%20manipulating%20the%20training%20process.%20By%20inserting%20a%0Asmall%20trigger%2C%20such%20as%20a%20sticker%2C%20make-up%2C%20or%20patterned%20mask%2C%20into%20a%20few%0Atraining%20images%2C%20an%20adversary%20can%20later%20present%20the%20same%20trigger%20during%0Aauthentication%20to%20be%20falsely%20recognized%20as%20another%20individual%2C%20thereby%20gaining%0Aunauthorized%20access.%20Existing%20defense%20mechanisms%20against%20backdoor%20attacks%20still%0Aface%20challenges%20in%20precisely%20identifying%20and%20mitigating%20poisoned%20images%20without%0Acompromising%20data%20utility%2C%20which%20undermines%20the%20overall%20reliability%20of%20the%0Asystem.%20We%20propose%20a%20novel%20and%20generalizable%20approach%2C%20TrueBiometric%3A%0ATrustworthy%20Biometrics%2C%20which%20accurately%20detects%20poisoned%20images%20using%20a%0Amajority%20voting%20mechanism%20leveraging%20multiple%20state-of-the-art%20large%20vision%0Alanguage%20models.%20Once%20identified%2C%20poisoned%20samples%20are%20corrected%20using%20targeted%0Aand%20calibrated%20corrective%20noise.%20Our%20extensive%20empirical%20results%20demonstrate%0Athat%20TrueBiometric%20detects%20and%20corrects%20poisoned%20images%20with%20100%5C%25%20accuracy%0Awithout%20compromising%20accuracy%20on%20clean%20images.%20Compared%20to%20existing%0Astate-of-the-art%20approaches%2C%20TrueBiometric%20offers%20a%20more%20practical%2C%20accurate%2C%0Aand%20effective%20solution%20for%20mitigating%20backdoor%20attacks%20in%20face%20recognition%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Detection%2520to%2520Correction%253A%2520Backdoor-Resilient%2520Face%2520Recognition%2520via%250A%2520%2520Vision-Language%2520Trigger%2520Detection%2520and%2520Noise-Based%2520Neutralization%26entry.906535625%3DFarah%2520Wahida%2520and%2520M.%2520A.%2520P.%2520Chamikara%2520and%2520Yashothara%2520Shanmugarasa%2520and%2520Mohan%2520Baruwal%2520Chhetri%2520and%2520Thilina%2520Ranbaduge%2520and%2520Ibrahim%2520Khalil%26entry.1292438233%3D%2520%2520Biometric%2520systems%252C%2520such%2520as%2520face%2520recognition%2520systems%2520powered%2520by%2520deep%2520neural%250Anetworks%2520%2528DNNs%2529%252C%2520rely%2520on%2520large%2520and%2520highly%2520sensitive%2520datasets.%2520Backdoor%2520attacks%250Acan%2520subvert%2520these%2520systems%2520by%2520manipulating%2520the%2520training%2520process.%2520By%2520inserting%2520a%250Asmall%2520trigger%252C%2520such%2520as%2520a%2520sticker%252C%2520make-up%252C%2520or%2520patterned%2520mask%252C%2520into%2520a%2520few%250Atraining%2520images%252C%2520an%2520adversary%2520can%2520later%2520present%2520the%2520same%2520trigger%2520during%250Aauthentication%2520to%2520be%2520falsely%2520recognized%2520as%2520another%2520individual%252C%2520thereby%2520gaining%250Aunauthorized%2520access.%2520Existing%2520defense%2520mechanisms%2520against%2520backdoor%2520attacks%2520still%250Aface%2520challenges%2520in%2520precisely%2520identifying%2520and%2520mitigating%2520poisoned%2520images%2520without%250Acompromising%2520data%2520utility%252C%2520which%2520undermines%2520the%2520overall%2520reliability%2520of%2520the%250Asystem.%2520We%2520propose%2520a%2520novel%2520and%2520generalizable%2520approach%252C%2520TrueBiometric%253A%250ATrustworthy%2520Biometrics%252C%2520which%2520accurately%2520detects%2520poisoned%2520images%2520using%2520a%250Amajority%2520voting%2520mechanism%2520leveraging%2520multiple%2520state-of-the-art%2520large%2520vision%250Alanguage%2520models.%2520Once%2520identified%252C%2520poisoned%2520samples%2520are%2520corrected%2520using%2520targeted%250Aand%2520calibrated%2520corrective%2520noise.%2520Our%2520extensive%2520empirical%2520results%2520demonstrate%250Athat%2520TrueBiometric%2520detects%2520and%2520corrects%2520poisoned%2520images%2520with%2520100%255C%2525%2520accuracy%250Awithout%2520compromising%2520accuracy%2520on%2520clean%2520images.%2520Compared%2520to%2520existing%250Astate-of-the-art%2520approaches%252C%2520TrueBiometric%2520offers%2520a%2520more%2520practical%252C%2520accurate%252C%250Aand%2520effective%2520solution%2520for%2520mitigating%2520backdoor%2520attacks%2520in%2520face%2520recognition%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Detection%20to%20Correction%3A%20Backdoor-Resilient%20Face%20Recognition%20via%0A%20%20Vision-Language%20Trigger%20Detection%20and%20Noise-Based%20Neutralization&entry.906535625=Farah%20Wahida%20and%20M.%20A.%20P.%20Chamikara%20and%20Yashothara%20Shanmugarasa%20and%20Mohan%20Baruwal%20Chhetri%20and%20Thilina%20Ranbaduge%20and%20Ibrahim%20Khalil&entry.1292438233=%20%20Biometric%20systems%2C%20such%20as%20face%20recognition%20systems%20powered%20by%20deep%20neural%0Anetworks%20%28DNNs%29%2C%20rely%20on%20large%20and%20highly%20sensitive%20datasets.%20Backdoor%20attacks%0Acan%20subvert%20these%20systems%20by%20manipulating%20the%20training%20process.%20By%20inserting%20a%0Asmall%20trigger%2C%20such%20as%20a%20sticker%2C%20make-up%2C%20or%20patterned%20mask%2C%20into%20a%20few%0Atraining%20images%2C%20an%20adversary%20can%20later%20present%20the%20same%20trigger%20during%0Aauthentication%20to%20be%20falsely%20recognized%20as%20another%20individual%2C%20thereby%20gaining%0Aunauthorized%20access.%20Existing%20defense%20mechanisms%20against%20backdoor%20attacks%20still%0Aface%20challenges%20in%20precisely%20identifying%20and%20mitigating%20poisoned%20images%20without%0Acompromising%20data%20utility%2C%20which%20undermines%20the%20overall%20reliability%20of%20the%0Asystem.%20We%20propose%20a%20novel%20and%20generalizable%20approach%2C%20TrueBiometric%3A%0ATrustworthy%20Biometrics%2C%20which%20accurately%20detects%20poisoned%20images%20using%20a%0Amajority%20voting%20mechanism%20leveraging%20multiple%20state-of-the-art%20large%20vision%0Alanguage%20models.%20Once%20identified%2C%20poisoned%20samples%20are%20corrected%20using%20targeted%0Aand%20calibrated%20corrective%20noise.%20Our%20extensive%20empirical%20results%20demonstrate%0Athat%20TrueBiometric%20detects%20and%20corrects%20poisoned%20images%20with%20100%5C%25%20accuracy%0Awithout%20compromising%20accuracy%20on%20clean%20images.%20Compared%20to%20existing%0Astate-of-the-art%20approaches%2C%20TrueBiometric%20offers%20a%20more%20practical%2C%20accurate%2C%0Aand%20effective%20solution%20for%20mitigating%20backdoor%20attacks%20in%20face%20recognition%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05409v1&entry.124074799=Read"},
{"title": "ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for\n  Autonomous Driving via Stable Diffusion Models", "author": "Yatong Lan and Jingfeng Chen and Yiru Wang and Lei He", "abstract": "  Arbitrary viewpoint image generation holds significant potential for\nautonomous driving, yet remains a challenging task due to the lack of\nground-truth data for extrapolated views, which hampers the training of\nhigh-fidelity generative models. In this work, we propose Arbiviewgen, a novel\ndiffusion-based framework for the generation of controllable camera images from\narbitrary points of view. To address the absence of ground-truth data in unseen\nviews, we introduce two key components: Feature-Aware Adaptive View Stitching\n(FAVS) and Cross-View Consistency Self-Supervised Learning (CVC-SSL). FAVS\nemploys a hierarchical matching strategy that first establishes coarse\ngeometric correspondences using camera poses, then performs fine-grained\nalignment through improved feature matching algorithms, and identifies\nhigh-confidence matching regions via clustering analysis. Building upon this,\nCVC-SSL adopts a self-supervised training paradigm where the model reconstructs\nthe original camera views from the synthesized stitched images using a\ndiffusion model, enforcing cross-view consistency without requiring supervision\nfrom extrapolated data. Our framework requires only multi-camera images and\ntheir associated poses for training, eliminating the need for additional\nsensors or depth maps. To our knowledge, Arbiviewgen is the first method\ncapable of controllable arbitrary view camera image generation in multiple\nvehicle configurations.\n", "link": "http://arxiv.org/abs/2508.05236v1", "date": "2025-08-07", "relevancy": 2.4987, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.628}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.628}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArbiViewGen%3A%20Controllable%20Arbitrary%20Viewpoint%20Camera%20Data%20Generation%20for%0A%20%20Autonomous%20Driving%20via%20Stable%20Diffusion%20Models&body=Title%3A%20ArbiViewGen%3A%20Controllable%20Arbitrary%20Viewpoint%20Camera%20Data%20Generation%20for%0A%20%20Autonomous%20Driving%20via%20Stable%20Diffusion%20Models%0AAuthor%3A%20Yatong%20Lan%20and%20Jingfeng%20Chen%20and%20Yiru%20Wang%20and%20Lei%20He%0AAbstract%3A%20%20%20Arbitrary%20viewpoint%20image%20generation%20holds%20significant%20potential%20for%0Aautonomous%20driving%2C%20yet%20remains%20a%20challenging%20task%20due%20to%20the%20lack%20of%0Aground-truth%20data%20for%20extrapolated%20views%2C%20which%20hampers%20the%20training%20of%0Ahigh-fidelity%20generative%20models.%20In%20this%20work%2C%20we%20propose%20Arbiviewgen%2C%20a%20novel%0Adiffusion-based%20framework%20for%20the%20generation%20of%20controllable%20camera%20images%20from%0Aarbitrary%20points%20of%20view.%20To%20address%20the%20absence%20of%20ground-truth%20data%20in%20unseen%0Aviews%2C%20we%20introduce%20two%20key%20components%3A%20Feature-Aware%20Adaptive%20View%20Stitching%0A%28FAVS%29%20and%20Cross-View%20Consistency%20Self-Supervised%20Learning%20%28CVC-SSL%29.%20FAVS%0Aemploys%20a%20hierarchical%20matching%20strategy%20that%20first%20establishes%20coarse%0Ageometric%20correspondences%20using%20camera%20poses%2C%20then%20performs%20fine-grained%0Aalignment%20through%20improved%20feature%20matching%20algorithms%2C%20and%20identifies%0Ahigh-confidence%20matching%20regions%20via%20clustering%20analysis.%20Building%20upon%20this%2C%0ACVC-SSL%20adopts%20a%20self-supervised%20training%20paradigm%20where%20the%20model%20reconstructs%0Athe%20original%20camera%20views%20from%20the%20synthesized%20stitched%20images%20using%20a%0Adiffusion%20model%2C%20enforcing%20cross-view%20consistency%20without%20requiring%20supervision%0Afrom%20extrapolated%20data.%20Our%20framework%20requires%20only%20multi-camera%20images%20and%0Atheir%20associated%20poses%20for%20training%2C%20eliminating%20the%20need%20for%20additional%0Asensors%20or%20depth%20maps.%20To%20our%20knowledge%2C%20Arbiviewgen%20is%20the%20first%20method%0Acapable%20of%20controllable%20arbitrary%20view%20camera%20image%20generation%20in%20multiple%0Avehicle%20configurations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArbiViewGen%253A%2520Controllable%2520Arbitrary%2520Viewpoint%2520Camera%2520Data%2520Generation%2520for%250A%2520%2520Autonomous%2520Driving%2520via%2520Stable%2520Diffusion%2520Models%26entry.906535625%3DYatong%2520Lan%2520and%2520Jingfeng%2520Chen%2520and%2520Yiru%2520Wang%2520and%2520Lei%2520He%26entry.1292438233%3D%2520%2520Arbitrary%2520viewpoint%2520image%2520generation%2520holds%2520significant%2520potential%2520for%250Aautonomous%2520driving%252C%2520yet%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520the%2520lack%2520of%250Aground-truth%2520data%2520for%2520extrapolated%2520views%252C%2520which%2520hampers%2520the%2520training%2520of%250Ahigh-fidelity%2520generative%2520models.%2520In%2520this%2520work%252C%2520we%2520propose%2520Arbiviewgen%252C%2520a%2520novel%250Adiffusion-based%2520framework%2520for%2520the%2520generation%2520of%2520controllable%2520camera%2520images%2520from%250Aarbitrary%2520points%2520of%2520view.%2520To%2520address%2520the%2520absence%2520of%2520ground-truth%2520data%2520in%2520unseen%250Aviews%252C%2520we%2520introduce%2520two%2520key%2520components%253A%2520Feature-Aware%2520Adaptive%2520View%2520Stitching%250A%2528FAVS%2529%2520and%2520Cross-View%2520Consistency%2520Self-Supervised%2520Learning%2520%2528CVC-SSL%2529.%2520FAVS%250Aemploys%2520a%2520hierarchical%2520matching%2520strategy%2520that%2520first%2520establishes%2520coarse%250Ageometric%2520correspondences%2520using%2520camera%2520poses%252C%2520then%2520performs%2520fine-grained%250Aalignment%2520through%2520improved%2520feature%2520matching%2520algorithms%252C%2520and%2520identifies%250Ahigh-confidence%2520matching%2520regions%2520via%2520clustering%2520analysis.%2520Building%2520upon%2520this%252C%250ACVC-SSL%2520adopts%2520a%2520self-supervised%2520training%2520paradigm%2520where%2520the%2520model%2520reconstructs%250Athe%2520original%2520camera%2520views%2520from%2520the%2520synthesized%2520stitched%2520images%2520using%2520a%250Adiffusion%2520model%252C%2520enforcing%2520cross-view%2520consistency%2520without%2520requiring%2520supervision%250Afrom%2520extrapolated%2520data.%2520Our%2520framework%2520requires%2520only%2520multi-camera%2520images%2520and%250Atheir%2520associated%2520poses%2520for%2520training%252C%2520eliminating%2520the%2520need%2520for%2520additional%250Asensors%2520or%2520depth%2520maps.%2520To%2520our%2520knowledge%252C%2520Arbiviewgen%2520is%2520the%2520first%2520method%250Acapable%2520of%2520controllable%2520arbitrary%2520view%2520camera%2520image%2520generation%2520in%2520multiple%250Avehicle%2520configurations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArbiViewGen%3A%20Controllable%20Arbitrary%20Viewpoint%20Camera%20Data%20Generation%20for%0A%20%20Autonomous%20Driving%20via%20Stable%20Diffusion%20Models&entry.906535625=Yatong%20Lan%20and%20Jingfeng%20Chen%20and%20Yiru%20Wang%20and%20Lei%20He&entry.1292438233=%20%20Arbitrary%20viewpoint%20image%20generation%20holds%20significant%20potential%20for%0Aautonomous%20driving%2C%20yet%20remains%20a%20challenging%20task%20due%20to%20the%20lack%20of%0Aground-truth%20data%20for%20extrapolated%20views%2C%20which%20hampers%20the%20training%20of%0Ahigh-fidelity%20generative%20models.%20In%20this%20work%2C%20we%20propose%20Arbiviewgen%2C%20a%20novel%0Adiffusion-based%20framework%20for%20the%20generation%20of%20controllable%20camera%20images%20from%0Aarbitrary%20points%20of%20view.%20To%20address%20the%20absence%20of%20ground-truth%20data%20in%20unseen%0Aviews%2C%20we%20introduce%20two%20key%20components%3A%20Feature-Aware%20Adaptive%20View%20Stitching%0A%28FAVS%29%20and%20Cross-View%20Consistency%20Self-Supervised%20Learning%20%28CVC-SSL%29.%20FAVS%0Aemploys%20a%20hierarchical%20matching%20strategy%20that%20first%20establishes%20coarse%0Ageometric%20correspondences%20using%20camera%20poses%2C%20then%20performs%20fine-grained%0Aalignment%20through%20improved%20feature%20matching%20algorithms%2C%20and%20identifies%0Ahigh-confidence%20matching%20regions%20via%20clustering%20analysis.%20Building%20upon%20this%2C%0ACVC-SSL%20adopts%20a%20self-supervised%20training%20paradigm%20where%20the%20model%20reconstructs%0Athe%20original%20camera%20views%20from%20the%20synthesized%20stitched%20images%20using%20a%0Adiffusion%20model%2C%20enforcing%20cross-view%20consistency%20without%20requiring%20supervision%0Afrom%20extrapolated%20data.%20Our%20framework%20requires%20only%20multi-camera%20images%20and%0Atheir%20associated%20poses%20for%20training%2C%20eliminating%20the%20need%20for%20additional%0Asensors%20or%20depth%20maps.%20To%20our%20knowledge%2C%20Arbiviewgen%20is%20the%20first%20method%0Acapable%20of%20controllable%20arbitrary%20view%20camera%20image%20generation%20in%20multiple%0Avehicle%20configurations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05236v1&entry.124074799=Read"},
{"title": "Don't Reach for the Stars: Rethinking Topology for Resilient Federated\n  Learning", "author": "Mirko Konstantin and Anirban Mukhopadhyay", "abstract": "  Federated learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy by keeping data local.\nTraditional FL approaches rely on a centralized, star-shaped topology, where a\ncentral server aggregates model updates from clients. However, this\narchitecture introduces several limitations, including a single point of\nfailure, limited personalization, and poor robustness to distribution shifts or\nvulnerability to malfunctioning clients. Moreover, update selection in\ncentralized FL often relies on low-level parameter differences, which can be\nunreliable when client data is not independent and identically distributed, and\noffer clients little control. In this work, we propose a decentralized,\npeer-to-peer (P2P) FL framework. It leverages the flexibility of the P2P\ntopology to enable each client to identify and aggregate a personalized set of\ntrustworthy and beneficial updates.This framework is the Local Inference Guided\nAggregation for Heterogeneous Training Environments to Yield Enhancement\nThrough Agreement and Regularization (LIGHTYEAR). Central to our method is an\nagreement score, computed on a local validation set, which quantifies the\nsemantic alignment of incoming updates in the function space with respect to\nthe clients reference model. Each client uses this score to select a tailored\nsubset of updates and performs aggregation with a regularization term that\nfurther stabilizes the training. Our empirical evaluation across two datasets\nshows that the proposed approach consistently outperforms both centralized\nbaselines and existing P2P methods in terms of client-level performance,\nparticularly under adversarial and heterogeneous conditions.\n", "link": "http://arxiv.org/abs/2508.05224v1", "date": "2025-08-07", "relevancy": 2.4747, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5103}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4952}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Reach%20for%20the%20Stars%3A%20Rethinking%20Topology%20for%20Resilient%20Federated%0A%20%20Learning&body=Title%3A%20Don%27t%20Reach%20for%20the%20Stars%3A%20Rethinking%20Topology%20for%20Resilient%20Federated%0A%20%20Learning%0AAuthor%3A%20Mirko%20Konstantin%20and%20Anirban%20Mukhopadhyay%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy%20by%20keeping%20data%20local.%0ATraditional%20FL%20approaches%20rely%20on%20a%20centralized%2C%20star-shaped%20topology%2C%20where%20a%0Acentral%20server%20aggregates%20model%20updates%20from%20clients.%20However%2C%20this%0Aarchitecture%20introduces%20several%20limitations%2C%20including%20a%20single%20point%20of%0Afailure%2C%20limited%20personalization%2C%20and%20poor%20robustness%20to%20distribution%20shifts%20or%0Avulnerability%20to%20malfunctioning%20clients.%20Moreover%2C%20update%20selection%20in%0Acentralized%20FL%20often%20relies%20on%20low-level%20parameter%20differences%2C%20which%20can%20be%0Aunreliable%20when%20client%20data%20is%20not%20independent%20and%20identically%20distributed%2C%20and%0Aoffer%20clients%20little%20control.%20In%20this%20work%2C%20we%20propose%20a%20decentralized%2C%0Apeer-to-peer%20%28P2P%29%20FL%20framework.%20It%20leverages%20the%20flexibility%20of%20the%20P2P%0Atopology%20to%20enable%20each%20client%20to%20identify%20and%20aggregate%20a%20personalized%20set%20of%0Atrustworthy%20and%20beneficial%20updates.This%20framework%20is%20the%20Local%20Inference%20Guided%0AAggregation%20for%20Heterogeneous%20Training%20Environments%20to%20Yield%20Enhancement%0AThrough%20Agreement%20and%20Regularization%20%28LIGHTYEAR%29.%20Central%20to%20our%20method%20is%20an%0Aagreement%20score%2C%20computed%20on%20a%20local%20validation%20set%2C%20which%20quantifies%20the%0Asemantic%20alignment%20of%20incoming%20updates%20in%20the%20function%20space%20with%20respect%20to%0Athe%20clients%20reference%20model.%20Each%20client%20uses%20this%20score%20to%20select%20a%20tailored%0Asubset%20of%20updates%20and%20performs%20aggregation%20with%20a%20regularization%20term%20that%0Afurther%20stabilizes%20the%20training.%20Our%20empirical%20evaluation%20across%20two%20datasets%0Ashows%20that%20the%20proposed%20approach%20consistently%20outperforms%20both%20centralized%0Abaselines%20and%20existing%20P2P%20methods%20in%20terms%20of%20client-level%20performance%2C%0Aparticularly%20under%20adversarial%20and%20heterogeneous%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Reach%2520for%2520the%2520Stars%253A%2520Rethinking%2520Topology%2520for%2520Resilient%2520Federated%250A%2520%2520Learning%26entry.906535625%3DMirko%2520Konstantin%2520and%2520Anirban%2520Mukhopadhyay%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%250Adistributed%2520clients%2520while%2520preserving%2520data%2520privacy%2520by%2520keeping%2520data%2520local.%250ATraditional%2520FL%2520approaches%2520rely%2520on%2520a%2520centralized%252C%2520star-shaped%2520topology%252C%2520where%2520a%250Acentral%2520server%2520aggregates%2520model%2520updates%2520from%2520clients.%2520However%252C%2520this%250Aarchitecture%2520introduces%2520several%2520limitations%252C%2520including%2520a%2520single%2520point%2520of%250Afailure%252C%2520limited%2520personalization%252C%2520and%2520poor%2520robustness%2520to%2520distribution%2520shifts%2520or%250Avulnerability%2520to%2520malfunctioning%2520clients.%2520Moreover%252C%2520update%2520selection%2520in%250Acentralized%2520FL%2520often%2520relies%2520on%2520low-level%2520parameter%2520differences%252C%2520which%2520can%2520be%250Aunreliable%2520when%2520client%2520data%2520is%2520not%2520independent%2520and%2520identically%2520distributed%252C%2520and%250Aoffer%2520clients%2520little%2520control.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520decentralized%252C%250Apeer-to-peer%2520%2528P2P%2529%2520FL%2520framework.%2520It%2520leverages%2520the%2520flexibility%2520of%2520the%2520P2P%250Atopology%2520to%2520enable%2520each%2520client%2520to%2520identify%2520and%2520aggregate%2520a%2520personalized%2520set%2520of%250Atrustworthy%2520and%2520beneficial%2520updates.This%2520framework%2520is%2520the%2520Local%2520Inference%2520Guided%250AAggregation%2520for%2520Heterogeneous%2520Training%2520Environments%2520to%2520Yield%2520Enhancement%250AThrough%2520Agreement%2520and%2520Regularization%2520%2528LIGHTYEAR%2529.%2520Central%2520to%2520our%2520method%2520is%2520an%250Aagreement%2520score%252C%2520computed%2520on%2520a%2520local%2520validation%2520set%252C%2520which%2520quantifies%2520the%250Asemantic%2520alignment%2520of%2520incoming%2520updates%2520in%2520the%2520function%2520space%2520with%2520respect%2520to%250Athe%2520clients%2520reference%2520model.%2520Each%2520client%2520uses%2520this%2520score%2520to%2520select%2520a%2520tailored%250Asubset%2520of%2520updates%2520and%2520performs%2520aggregation%2520with%2520a%2520regularization%2520term%2520that%250Afurther%2520stabilizes%2520the%2520training.%2520Our%2520empirical%2520evaluation%2520across%2520two%2520datasets%250Ashows%2520that%2520the%2520proposed%2520approach%2520consistently%2520outperforms%2520both%2520centralized%250Abaselines%2520and%2520existing%2520P2P%2520methods%2520in%2520terms%2520of%2520client-level%2520performance%252C%250Aparticularly%2520under%2520adversarial%2520and%2520heterogeneous%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Reach%20for%20the%20Stars%3A%20Rethinking%20Topology%20for%20Resilient%20Federated%0A%20%20Learning&entry.906535625=Mirko%20Konstantin%20and%20Anirban%20Mukhopadhyay&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy%20by%20keeping%20data%20local.%0ATraditional%20FL%20approaches%20rely%20on%20a%20centralized%2C%20star-shaped%20topology%2C%20where%20a%0Acentral%20server%20aggregates%20model%20updates%20from%20clients.%20However%2C%20this%0Aarchitecture%20introduces%20several%20limitations%2C%20including%20a%20single%20point%20of%0Afailure%2C%20limited%20personalization%2C%20and%20poor%20robustness%20to%20distribution%20shifts%20or%0Avulnerability%20to%20malfunctioning%20clients.%20Moreover%2C%20update%20selection%20in%0Acentralized%20FL%20often%20relies%20on%20low-level%20parameter%20differences%2C%20which%20can%20be%0Aunreliable%20when%20client%20data%20is%20not%20independent%20and%20identically%20distributed%2C%20and%0Aoffer%20clients%20little%20control.%20In%20this%20work%2C%20we%20propose%20a%20decentralized%2C%0Apeer-to-peer%20%28P2P%29%20FL%20framework.%20It%20leverages%20the%20flexibility%20of%20the%20P2P%0Atopology%20to%20enable%20each%20client%20to%20identify%20and%20aggregate%20a%20personalized%20set%20of%0Atrustworthy%20and%20beneficial%20updates.This%20framework%20is%20the%20Local%20Inference%20Guided%0AAggregation%20for%20Heterogeneous%20Training%20Environments%20to%20Yield%20Enhancement%0AThrough%20Agreement%20and%20Regularization%20%28LIGHTYEAR%29.%20Central%20to%20our%20method%20is%20an%0Aagreement%20score%2C%20computed%20on%20a%20local%20validation%20set%2C%20which%20quantifies%20the%0Asemantic%20alignment%20of%20incoming%20updates%20in%20the%20function%20space%20with%20respect%20to%0Athe%20clients%20reference%20model.%20Each%20client%20uses%20this%20score%20to%20select%20a%20tailored%0Asubset%20of%20updates%20and%20performs%20aggregation%20with%20a%20regularization%20term%20that%0Afurther%20stabilizes%20the%20training.%20Our%20empirical%20evaluation%20across%20two%20datasets%0Ashows%20that%20the%20proposed%20approach%20consistently%20outperforms%20both%20centralized%0Abaselines%20and%20existing%20P2P%20methods%20in%20terms%20of%20client-level%20performance%2C%0Aparticularly%20under%20adversarial%20and%20heterogeneous%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05224v1&entry.124074799=Read"},
{"title": "VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing\n  Projection Test", "author": "Meiqi Wu and Yaxuan Kang and Xuchen Li and Shiyu Hu and Xiaotang Chen and Yunfeng Kang and Weiqiang Wang and Kaiqi Huang", "abstract": "  The Drawing Projection Test (DPT) is an essential tool in art therapy,\nallowing psychologists to assess participants' mental states through their\nsketches. Specifically, through sketches with the theme of \"a person picking an\napple from a tree (PPAT)\", it can be revealed whether the participants are in\nmental states such as depression. Compared with scales, the DPT can enrich\npsychologists' understanding of an individual's mental state. However, the\ninterpretation of the PPAT is laborious and depends on the experience of the\npsychologists. To address this issue, we propose an effective identification\nmethod to support psychologists in conducting a large-scale automatic DPT.\nUnlike traditional sketch recognition, DPT more focus on the overall evaluation\nof the sketches, such as color usage and space utilization. Moreover, PPAT\nimposes a time limit and prohibits verbal reminders, resulting in low drawing\naccuracy and a lack of detailed depiction. To address these challenges, we\npropose the following efforts: (1) Providing an experimental environment for\nautomated analysis of PPAT sketches for depression assessment; (2) Offering a\nVisual-Semantic depression assessment based on LLM (VS-LLM) method; (3)\nExperimental results demonstrate that our method improves by 17.6% compared to\nthe psychologist assessment method. We anticipate that this work will\ncontribute to the research in mental state assessment based on PPAT sketches'\nelements recognition. Our datasets and codes are available at\nhttps://github.com/wmeiqi/VS-LLM.\n", "link": "http://arxiv.org/abs/2508.05299v1", "date": "2025-08-07", "relevancy": 2.4704, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5143}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VS-LLM%3A%20Visual-Semantic%20Depression%20Assessment%20based%20on%20LLM%20for%20Drawing%0A%20%20Projection%20Test&body=Title%3A%20VS-LLM%3A%20Visual-Semantic%20Depression%20Assessment%20based%20on%20LLM%20for%20Drawing%0A%20%20Projection%20Test%0AAuthor%3A%20Meiqi%20Wu%20and%20Yaxuan%20Kang%20and%20Xuchen%20Li%20and%20Shiyu%20Hu%20and%20Xiaotang%20Chen%20and%20Yunfeng%20Kang%20and%20Weiqiang%20Wang%20and%20Kaiqi%20Huang%0AAbstract%3A%20%20%20The%20Drawing%20Projection%20Test%20%28DPT%29%20is%20an%20essential%20tool%20in%20art%20therapy%2C%0Aallowing%20psychologists%20to%20assess%20participants%27%20mental%20states%20through%20their%0Asketches.%20Specifically%2C%20through%20sketches%20with%20the%20theme%20of%20%22a%20person%20picking%20an%0Aapple%20from%20a%20tree%20%28PPAT%29%22%2C%20it%20can%20be%20revealed%20whether%20the%20participants%20are%20in%0Amental%20states%20such%20as%20depression.%20Compared%20with%20scales%2C%20the%20DPT%20can%20enrich%0Apsychologists%27%20understanding%20of%20an%20individual%27s%20mental%20state.%20However%2C%20the%0Ainterpretation%20of%20the%20PPAT%20is%20laborious%20and%20depends%20on%20the%20experience%20of%20the%0Apsychologists.%20To%20address%20this%20issue%2C%20we%20propose%20an%20effective%20identification%0Amethod%20to%20support%20psychologists%20in%20conducting%20a%20large-scale%20automatic%20DPT.%0AUnlike%20traditional%20sketch%20recognition%2C%20DPT%20more%20focus%20on%20the%20overall%20evaluation%0Aof%20the%20sketches%2C%20such%20as%20color%20usage%20and%20space%20utilization.%20Moreover%2C%20PPAT%0Aimposes%20a%20time%20limit%20and%20prohibits%20verbal%20reminders%2C%20resulting%20in%20low%20drawing%0Aaccuracy%20and%20a%20lack%20of%20detailed%20depiction.%20To%20address%20these%20challenges%2C%20we%0Apropose%20the%20following%20efforts%3A%20%281%29%20Providing%20an%20experimental%20environment%20for%0Aautomated%20analysis%20of%20PPAT%20sketches%20for%20depression%20assessment%3B%20%282%29%20Offering%20a%0AVisual-Semantic%20depression%20assessment%20based%20on%20LLM%20%28VS-LLM%29%20method%3B%20%283%29%0AExperimental%20results%20demonstrate%20that%20our%20method%20improves%20by%2017.6%25%20compared%20to%0Athe%20psychologist%20assessment%20method.%20We%20anticipate%20that%20this%20work%20will%0Acontribute%20to%20the%20research%20in%20mental%20state%20assessment%20based%20on%20PPAT%20sketches%27%0Aelements%20recognition.%20Our%20datasets%20and%20codes%20are%20available%20at%0Ahttps%3A//github.com/wmeiqi/VS-LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVS-LLM%253A%2520Visual-Semantic%2520Depression%2520Assessment%2520based%2520on%2520LLM%2520for%2520Drawing%250A%2520%2520Projection%2520Test%26entry.906535625%3DMeiqi%2520Wu%2520and%2520Yaxuan%2520Kang%2520and%2520Xuchen%2520Li%2520and%2520Shiyu%2520Hu%2520and%2520Xiaotang%2520Chen%2520and%2520Yunfeng%2520Kang%2520and%2520Weiqiang%2520Wang%2520and%2520Kaiqi%2520Huang%26entry.1292438233%3D%2520%2520The%2520Drawing%2520Projection%2520Test%2520%2528DPT%2529%2520is%2520an%2520essential%2520tool%2520in%2520art%2520therapy%252C%250Aallowing%2520psychologists%2520to%2520assess%2520participants%2527%2520mental%2520states%2520through%2520their%250Asketches.%2520Specifically%252C%2520through%2520sketches%2520with%2520the%2520theme%2520of%2520%2522a%2520person%2520picking%2520an%250Aapple%2520from%2520a%2520tree%2520%2528PPAT%2529%2522%252C%2520it%2520can%2520be%2520revealed%2520whether%2520the%2520participants%2520are%2520in%250Amental%2520states%2520such%2520as%2520depression.%2520Compared%2520with%2520scales%252C%2520the%2520DPT%2520can%2520enrich%250Apsychologists%2527%2520understanding%2520of%2520an%2520individual%2527s%2520mental%2520state.%2520However%252C%2520the%250Ainterpretation%2520of%2520the%2520PPAT%2520is%2520laborious%2520and%2520depends%2520on%2520the%2520experience%2520of%2520the%250Apsychologists.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520an%2520effective%2520identification%250Amethod%2520to%2520support%2520psychologists%2520in%2520conducting%2520a%2520large-scale%2520automatic%2520DPT.%250AUnlike%2520traditional%2520sketch%2520recognition%252C%2520DPT%2520more%2520focus%2520on%2520the%2520overall%2520evaluation%250Aof%2520the%2520sketches%252C%2520such%2520as%2520color%2520usage%2520and%2520space%2520utilization.%2520Moreover%252C%2520PPAT%250Aimposes%2520a%2520time%2520limit%2520and%2520prohibits%2520verbal%2520reminders%252C%2520resulting%2520in%2520low%2520drawing%250Aaccuracy%2520and%2520a%2520lack%2520of%2520detailed%2520depiction.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520the%2520following%2520efforts%253A%2520%25281%2529%2520Providing%2520an%2520experimental%2520environment%2520for%250Aautomated%2520analysis%2520of%2520PPAT%2520sketches%2520for%2520depression%2520assessment%253B%2520%25282%2529%2520Offering%2520a%250AVisual-Semantic%2520depression%2520assessment%2520based%2520on%2520LLM%2520%2528VS-LLM%2529%2520method%253B%2520%25283%2529%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520improves%2520by%252017.6%2525%2520compared%2520to%250Athe%2520psychologist%2520assessment%2520method.%2520We%2520anticipate%2520that%2520this%2520work%2520will%250Acontribute%2520to%2520the%2520research%2520in%2520mental%2520state%2520assessment%2520based%2520on%2520PPAT%2520sketches%2527%250Aelements%2520recognition.%2520Our%2520datasets%2520and%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/wmeiqi/VS-LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VS-LLM%3A%20Visual-Semantic%20Depression%20Assessment%20based%20on%20LLM%20for%20Drawing%0A%20%20Projection%20Test&entry.906535625=Meiqi%20Wu%20and%20Yaxuan%20Kang%20and%20Xuchen%20Li%20and%20Shiyu%20Hu%20and%20Xiaotang%20Chen%20and%20Yunfeng%20Kang%20and%20Weiqiang%20Wang%20and%20Kaiqi%20Huang&entry.1292438233=%20%20The%20Drawing%20Projection%20Test%20%28DPT%29%20is%20an%20essential%20tool%20in%20art%20therapy%2C%0Aallowing%20psychologists%20to%20assess%20participants%27%20mental%20states%20through%20their%0Asketches.%20Specifically%2C%20through%20sketches%20with%20the%20theme%20of%20%22a%20person%20picking%20an%0Aapple%20from%20a%20tree%20%28PPAT%29%22%2C%20it%20can%20be%20revealed%20whether%20the%20participants%20are%20in%0Amental%20states%20such%20as%20depression.%20Compared%20with%20scales%2C%20the%20DPT%20can%20enrich%0Apsychologists%27%20understanding%20of%20an%20individual%27s%20mental%20state.%20However%2C%20the%0Ainterpretation%20of%20the%20PPAT%20is%20laborious%20and%20depends%20on%20the%20experience%20of%20the%0Apsychologists.%20To%20address%20this%20issue%2C%20we%20propose%20an%20effective%20identification%0Amethod%20to%20support%20psychologists%20in%20conducting%20a%20large-scale%20automatic%20DPT.%0AUnlike%20traditional%20sketch%20recognition%2C%20DPT%20more%20focus%20on%20the%20overall%20evaluation%0Aof%20the%20sketches%2C%20such%20as%20color%20usage%20and%20space%20utilization.%20Moreover%2C%20PPAT%0Aimposes%20a%20time%20limit%20and%20prohibits%20verbal%20reminders%2C%20resulting%20in%20low%20drawing%0Aaccuracy%20and%20a%20lack%20of%20detailed%20depiction.%20To%20address%20these%20challenges%2C%20we%0Apropose%20the%20following%20efforts%3A%20%281%29%20Providing%20an%20experimental%20environment%20for%0Aautomated%20analysis%20of%20PPAT%20sketches%20for%20depression%20assessment%3B%20%282%29%20Offering%20a%0AVisual-Semantic%20depression%20assessment%20based%20on%20LLM%20%28VS-LLM%29%20method%3B%20%283%29%0AExperimental%20results%20demonstrate%20that%20our%20method%20improves%20by%2017.6%25%20compared%20to%0Athe%20psychologist%20assessment%20method.%20We%20anticipate%20that%20this%20work%20will%0Acontribute%20to%20the%20research%20in%20mental%20state%20assessment%20based%20on%20PPAT%20sketches%27%0Aelements%20recognition.%20Our%20datasets%20and%20codes%20are%20available%20at%0Ahttps%3A//github.com/wmeiqi/VS-LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05299v1&entry.124074799=Read"},
{"title": "F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic\n  Surgery", "author": "Lumin Chen and Zhiying Wu and Tianye Lei and Xuexue Bai and Ming Feng and Yuxi Wang and Gaofeng Meng and Zhen Lei and Hongbin Liu", "abstract": "  Pituitary tumors often cause deformation or encapsulation of adjacent vital\nstructures. Anatomical structure segmentation can provide surgeons with early\nwarnings of regions that pose surgical risks, thereby enhancing the safety of\npituitary surgery. However, pixel-level annotated video stream datasets for\npituitary surgeries are extremely rare. To address this challenge, we introduce\na new dataset for Pituitary Anatomy Segmentation (PAS). PAS comprises 7,845\ntime-coherent images extracted from 120 videos. To mitigate class imbalance, we\napply data augmentation techniques that simulate the presence of surgical\ninstruments in the training data. One major challenge in pituitary anatomy\nsegmentation is the inconsistency in feature representation due to occlusions,\ncamera motion, and surgical bleeding. By incorporating a Feature Fusion module,\nF2PASeg is proposed to refine anatomical structure segmentation by leveraging\nboth high-resolution image features and deep semantic embeddings, enhancing\nrobustness against intraoperative variations. Experimental results demonstrate\nthat F2PASeg consistently segments critical anatomical structures in real time,\nproviding a reliable solution for intraoperative pituitary surgery planning.\nCode: https://github.com/paulili08/F2PASeg.\n", "link": "http://arxiv.org/abs/2508.05465v1", "date": "2025-08-07", "relevancy": 2.4683, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5024}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4941}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F2PASeg%3A%20Feature%20Fusion%20for%20Pituitary%20Anatomy%20Segmentation%20in%20Endoscopic%0A%20%20Surgery&body=Title%3A%20F2PASeg%3A%20Feature%20Fusion%20for%20Pituitary%20Anatomy%20Segmentation%20in%20Endoscopic%0A%20%20Surgery%0AAuthor%3A%20Lumin%20Chen%20and%20Zhiying%20Wu%20and%20Tianye%20Lei%20and%20Xuexue%20Bai%20and%20Ming%20Feng%20and%20Yuxi%20Wang%20and%20Gaofeng%20Meng%20and%20Zhen%20Lei%20and%20Hongbin%20Liu%0AAbstract%3A%20%20%20Pituitary%20tumors%20often%20cause%20deformation%20or%20encapsulation%20of%20adjacent%20vital%0Astructures.%20Anatomical%20structure%20segmentation%20can%20provide%20surgeons%20with%20early%0Awarnings%20of%20regions%20that%20pose%20surgical%20risks%2C%20thereby%20enhancing%20the%20safety%20of%0Apituitary%20surgery.%20However%2C%20pixel-level%20annotated%20video%20stream%20datasets%20for%0Apituitary%20surgeries%20are%20extremely%20rare.%20To%20address%20this%20challenge%2C%20we%20introduce%0Aa%20new%20dataset%20for%20Pituitary%20Anatomy%20Segmentation%20%28PAS%29.%20PAS%20comprises%207%2C845%0Atime-coherent%20images%20extracted%20from%20120%20videos.%20To%20mitigate%20class%20imbalance%2C%20we%0Aapply%20data%20augmentation%20techniques%20that%20simulate%20the%20presence%20of%20surgical%0Ainstruments%20in%20the%20training%20data.%20One%20major%20challenge%20in%20pituitary%20anatomy%0Asegmentation%20is%20the%20inconsistency%20in%20feature%20representation%20due%20to%20occlusions%2C%0Acamera%20motion%2C%20and%20surgical%20bleeding.%20By%20incorporating%20a%20Feature%20Fusion%20module%2C%0AF2PASeg%20is%20proposed%20to%20refine%20anatomical%20structure%20segmentation%20by%20leveraging%0Aboth%20high-resolution%20image%20features%20and%20deep%20semantic%20embeddings%2C%20enhancing%0Arobustness%20against%20intraoperative%20variations.%20Experimental%20results%20demonstrate%0Athat%20F2PASeg%20consistently%20segments%20critical%20anatomical%20structures%20in%20real%20time%2C%0Aproviding%20a%20reliable%20solution%20for%20intraoperative%20pituitary%20surgery%20planning.%0ACode%3A%20https%3A//github.com/paulili08/F2PASeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF2PASeg%253A%2520Feature%2520Fusion%2520for%2520Pituitary%2520Anatomy%2520Segmentation%2520in%2520Endoscopic%250A%2520%2520Surgery%26entry.906535625%3DLumin%2520Chen%2520and%2520Zhiying%2520Wu%2520and%2520Tianye%2520Lei%2520and%2520Xuexue%2520Bai%2520and%2520Ming%2520Feng%2520and%2520Yuxi%2520Wang%2520and%2520Gaofeng%2520Meng%2520and%2520Zhen%2520Lei%2520and%2520Hongbin%2520Liu%26entry.1292438233%3D%2520%2520Pituitary%2520tumors%2520often%2520cause%2520deformation%2520or%2520encapsulation%2520of%2520adjacent%2520vital%250Astructures.%2520Anatomical%2520structure%2520segmentation%2520can%2520provide%2520surgeons%2520with%2520early%250Awarnings%2520of%2520regions%2520that%2520pose%2520surgical%2520risks%252C%2520thereby%2520enhancing%2520the%2520safety%2520of%250Apituitary%2520surgery.%2520However%252C%2520pixel-level%2520annotated%2520video%2520stream%2520datasets%2520for%250Apituitary%2520surgeries%2520are%2520extremely%2520rare.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%250Aa%2520new%2520dataset%2520for%2520Pituitary%2520Anatomy%2520Segmentation%2520%2528PAS%2529.%2520PAS%2520comprises%25207%252C845%250Atime-coherent%2520images%2520extracted%2520from%2520120%2520videos.%2520To%2520mitigate%2520class%2520imbalance%252C%2520we%250Aapply%2520data%2520augmentation%2520techniques%2520that%2520simulate%2520the%2520presence%2520of%2520surgical%250Ainstruments%2520in%2520the%2520training%2520data.%2520One%2520major%2520challenge%2520in%2520pituitary%2520anatomy%250Asegmentation%2520is%2520the%2520inconsistency%2520in%2520feature%2520representation%2520due%2520to%2520occlusions%252C%250Acamera%2520motion%252C%2520and%2520surgical%2520bleeding.%2520By%2520incorporating%2520a%2520Feature%2520Fusion%2520module%252C%250AF2PASeg%2520is%2520proposed%2520to%2520refine%2520anatomical%2520structure%2520segmentation%2520by%2520leveraging%250Aboth%2520high-resolution%2520image%2520features%2520and%2520deep%2520semantic%2520embeddings%252C%2520enhancing%250Arobustness%2520against%2520intraoperative%2520variations.%2520Experimental%2520results%2520demonstrate%250Athat%2520F2PASeg%2520consistently%2520segments%2520critical%2520anatomical%2520structures%2520in%2520real%2520time%252C%250Aproviding%2520a%2520reliable%2520solution%2520for%2520intraoperative%2520pituitary%2520surgery%2520planning.%250ACode%253A%2520https%253A//github.com/paulili08/F2PASeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F2PASeg%3A%20Feature%20Fusion%20for%20Pituitary%20Anatomy%20Segmentation%20in%20Endoscopic%0A%20%20Surgery&entry.906535625=Lumin%20Chen%20and%20Zhiying%20Wu%20and%20Tianye%20Lei%20and%20Xuexue%20Bai%20and%20Ming%20Feng%20and%20Yuxi%20Wang%20and%20Gaofeng%20Meng%20and%20Zhen%20Lei%20and%20Hongbin%20Liu&entry.1292438233=%20%20Pituitary%20tumors%20often%20cause%20deformation%20or%20encapsulation%20of%20adjacent%20vital%0Astructures.%20Anatomical%20structure%20segmentation%20can%20provide%20surgeons%20with%20early%0Awarnings%20of%20regions%20that%20pose%20surgical%20risks%2C%20thereby%20enhancing%20the%20safety%20of%0Apituitary%20surgery.%20However%2C%20pixel-level%20annotated%20video%20stream%20datasets%20for%0Apituitary%20surgeries%20are%20extremely%20rare.%20To%20address%20this%20challenge%2C%20we%20introduce%0Aa%20new%20dataset%20for%20Pituitary%20Anatomy%20Segmentation%20%28PAS%29.%20PAS%20comprises%207%2C845%0Atime-coherent%20images%20extracted%20from%20120%20videos.%20To%20mitigate%20class%20imbalance%2C%20we%0Aapply%20data%20augmentation%20techniques%20that%20simulate%20the%20presence%20of%20surgical%0Ainstruments%20in%20the%20training%20data.%20One%20major%20challenge%20in%20pituitary%20anatomy%0Asegmentation%20is%20the%20inconsistency%20in%20feature%20representation%20due%20to%20occlusions%2C%0Acamera%20motion%2C%20and%20surgical%20bleeding.%20By%20incorporating%20a%20Feature%20Fusion%20module%2C%0AF2PASeg%20is%20proposed%20to%20refine%20anatomical%20structure%20segmentation%20by%20leveraging%0Aboth%20high-resolution%20image%20features%20and%20deep%20semantic%20embeddings%2C%20enhancing%0Arobustness%20against%20intraoperative%20variations.%20Experimental%20results%20demonstrate%0Athat%20F2PASeg%20consistently%20segments%20critical%20anatomical%20structures%20in%20real%20time%2C%0Aproviding%20a%20reliable%20solution%20for%20intraoperative%20pituitary%20surgery%20planning.%0ACode%3A%20https%3A//github.com/paulili08/F2PASeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05465v1&entry.124074799=Read"},
{"title": "Teaching LLMs How to Learn with Contextual Fine-Tuning", "author": "Younwoo Choi and Muhammad Adil Asif and Ziwen Han and John Willes and Rahul G. Krishnan", "abstract": "  Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains.\n", "link": "http://arxiv.org/abs/2503.09032v2", "date": "2025-08-07", "relevancy": 2.4594, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4987}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teaching%20LLMs%20How%20to%20Learn%20with%20Contextual%20Fine-Tuning&body=Title%3A%20Teaching%20LLMs%20How%20to%20Learn%20with%20Contextual%20Fine-Tuning%0AAuthor%3A%20Younwoo%20Choi%20and%20Muhammad%20Adil%20Asif%20and%20Ziwen%20Han%20and%20John%20Willes%20and%20Rahul%20G.%20Krishnan%0AAbstract%3A%20%20%20Prompting%20Large%20Language%20Models%20%28LLMs%29%2C%20or%20providing%20context%20on%20the%20expected%0Amodel%20of%20operation%2C%20is%20an%20effective%20way%20to%20steer%20the%20outputs%20of%20such%20models%20to%0Asatisfy%20human%20desiderata%20after%20they%20have%20been%20trained.%20But%20in%20rapidly%20evolving%0Adomains%2C%20there%20is%20often%20need%20to%20fine-tune%20LLMs%20to%20improve%20either%20the%20kind%20of%0Aknowledge%20in%20their%20memory%20or%20their%20abilities%20to%20perform%20open%20ended%20reasoning%20in%0Anew%20domains.%20When%20human%27s%20learn%20new%20concepts%2C%20we%20often%20do%20so%20by%20linking%20the%20new%0Amaterial%20that%20we%20are%20studying%20to%20concepts%20we%20have%20already%20learned%20before.%20To%0Athat%20end%2C%20we%20ask%2C%20%22can%20prompting%20help%20us%20teach%20LLMs%20how%20to%20learn%22.%20In%20this%0Awork%2C%20we%20study%20a%20novel%20generalization%20of%20instruction%20tuning%2C%20called%20contextual%0Afine-tuning%2C%20to%20fine-tune%20LLMs.%20Our%20method%20leverages%20instructional%20prompts%0Adesigned%20to%20mimic%20human%20cognitive%20strategies%20in%20learning%20and%20problem-solving%20to%0Aguide%20the%20learning%20process%20during%20training%2C%20aiming%20to%20improve%20the%20model%27s%0Ainterpretation%20and%20understanding%20of%20domain-specific%20knowledge.%20We%20empirically%0Ademonstrate%20that%20this%20simple%20yet%20effective%20modification%20improves%20the%20ability%20of%0ALLMs%20to%20be%20fine-tuned%20rapidly%20on%20new%20datasets%20both%20within%20the%20medical%20and%0Afinancial%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09032v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeaching%2520LLMs%2520How%2520to%2520Learn%2520with%2520Contextual%2520Fine-Tuning%26entry.906535625%3DYounwoo%2520Choi%2520and%2520Muhammad%2520Adil%2520Asif%2520and%2520Ziwen%2520Han%2520and%2520John%2520Willes%2520and%2520Rahul%2520G.%2520Krishnan%26entry.1292438233%3D%2520%2520Prompting%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520or%2520providing%2520context%2520on%2520the%2520expected%250Amodel%2520of%2520operation%252C%2520is%2520an%2520effective%2520way%2520to%2520steer%2520the%2520outputs%2520of%2520such%2520models%2520to%250Asatisfy%2520human%2520desiderata%2520after%2520they%2520have%2520been%2520trained.%2520But%2520in%2520rapidly%2520evolving%250Adomains%252C%2520there%2520is%2520often%2520need%2520to%2520fine-tune%2520LLMs%2520to%2520improve%2520either%2520the%2520kind%2520of%250Aknowledge%2520in%2520their%2520memory%2520or%2520their%2520abilities%2520to%2520perform%2520open%2520ended%2520reasoning%2520in%250Anew%2520domains.%2520When%2520human%2527s%2520learn%2520new%2520concepts%252C%2520we%2520often%2520do%2520so%2520by%2520linking%2520the%2520new%250Amaterial%2520that%2520we%2520are%2520studying%2520to%2520concepts%2520we%2520have%2520already%2520learned%2520before.%2520To%250Athat%2520end%252C%2520we%2520ask%252C%2520%2522can%2520prompting%2520help%2520us%2520teach%2520LLMs%2520how%2520to%2520learn%2522.%2520In%2520this%250Awork%252C%2520we%2520study%2520a%2520novel%2520generalization%2520of%2520instruction%2520tuning%252C%2520called%2520contextual%250Afine-tuning%252C%2520to%2520fine-tune%2520LLMs.%2520Our%2520method%2520leverages%2520instructional%2520prompts%250Adesigned%2520to%2520mimic%2520human%2520cognitive%2520strategies%2520in%2520learning%2520and%2520problem-solving%2520to%250Aguide%2520the%2520learning%2520process%2520during%2520training%252C%2520aiming%2520to%2520improve%2520the%2520model%2527s%250Ainterpretation%2520and%2520understanding%2520of%2520domain-specific%2520knowledge.%2520We%2520empirically%250Ademonstrate%2520that%2520this%2520simple%2520yet%2520effective%2520modification%2520improves%2520the%2520ability%2520of%250ALLMs%2520to%2520be%2520fine-tuned%2520rapidly%2520on%2520new%2520datasets%2520both%2520within%2520the%2520medical%2520and%250Afinancial%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09032v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching%20LLMs%20How%20to%20Learn%20with%20Contextual%20Fine-Tuning&entry.906535625=Younwoo%20Choi%20and%20Muhammad%20Adil%20Asif%20and%20Ziwen%20Han%20and%20John%20Willes%20and%20Rahul%20G.%20Krishnan&entry.1292438233=%20%20Prompting%20Large%20Language%20Models%20%28LLMs%29%2C%20or%20providing%20context%20on%20the%20expected%0Amodel%20of%20operation%2C%20is%20an%20effective%20way%20to%20steer%20the%20outputs%20of%20such%20models%20to%0Asatisfy%20human%20desiderata%20after%20they%20have%20been%20trained.%20But%20in%20rapidly%20evolving%0Adomains%2C%20there%20is%20often%20need%20to%20fine-tune%20LLMs%20to%20improve%20either%20the%20kind%20of%0Aknowledge%20in%20their%20memory%20or%20their%20abilities%20to%20perform%20open%20ended%20reasoning%20in%0Anew%20domains.%20When%20human%27s%20learn%20new%20concepts%2C%20we%20often%20do%20so%20by%20linking%20the%20new%0Amaterial%20that%20we%20are%20studying%20to%20concepts%20we%20have%20already%20learned%20before.%20To%0Athat%20end%2C%20we%20ask%2C%20%22can%20prompting%20help%20us%20teach%20LLMs%20how%20to%20learn%22.%20In%20this%0Awork%2C%20we%20study%20a%20novel%20generalization%20of%20instruction%20tuning%2C%20called%20contextual%0Afine-tuning%2C%20to%20fine-tune%20LLMs.%20Our%20method%20leverages%20instructional%20prompts%0Adesigned%20to%20mimic%20human%20cognitive%20strategies%20in%20learning%20and%20problem-solving%20to%0Aguide%20the%20learning%20process%20during%20training%2C%20aiming%20to%20improve%20the%20model%27s%0Ainterpretation%20and%20understanding%20of%20domain-specific%20knowledge.%20We%20empirically%0Ademonstrate%20that%20this%20simple%20yet%20effective%20modification%20improves%20the%20ability%20of%0ALLMs%20to%20be%20fine-tuned%20rapidly%20on%20new%20datasets%20both%20within%20the%20medical%20and%0Afinancial%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09032v2&entry.124074799=Read"},
{"title": "CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT\n  Report Generation", "author": "Hamza Kalisch and Fabian H\u00f6rst and Jens Kleesiek and Ken Herrmann and Constantin Seibold", "abstract": "  As medical imaging is central to diagnostic processes, automating the\ngeneration of radiology reports has become increasingly relevant to assist\nradiologists with their heavy workloads. Most current methods rely solely on\nglobal image features, failing to capture fine-grained organ relationships\ncrucial for accurate reporting. To this end, we propose CT-GRAPH, a\nhierarchical graph attention network that explicitly models radiological\nknowledge by structuring anatomical regions into a graph, linking fine-grained\norgan features to coarser anatomical systems and a global patient context. Our\nmethod leverages pretrained 3D medical feature encoders to obtain global and\norgan-level features by utilizing anatomical masks. These features are further\nrefined within the graph and then integrated into a large language model to\ngenerate detailed medical reports. We evaluate our approach for the task of\nreport generation on the large-scale chest CT dataset CT-RATE. We provide an\nin-depth analysis of pretrained feature encoders for CT report generation and\nshow that our method achieves a substantial improvement of absolute 7.9\\% in F1\nscore over current state-of-the-art methods. The code is publicly available at\nhttps://github.com/hakal104/CT-GRAPH.\n", "link": "http://arxiv.org/abs/2508.05375v1", "date": "2025-08-07", "relevancy": 2.4566, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5015}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4877}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CT-GRAPH%3A%20Hierarchical%20Graph%20Attention%20Network%20for%20Anatomy-Guided%20CT%0A%20%20Report%20Generation&body=Title%3A%20CT-GRAPH%3A%20Hierarchical%20Graph%20Attention%20Network%20for%20Anatomy-Guided%20CT%0A%20%20Report%20Generation%0AAuthor%3A%20Hamza%20Kalisch%20and%20Fabian%20H%C3%B6rst%20and%20Jens%20Kleesiek%20and%20Ken%20Herrmann%20and%20Constantin%20Seibold%0AAbstract%3A%20%20%20As%20medical%20imaging%20is%20central%20to%20diagnostic%20processes%2C%20automating%20the%0Ageneration%20of%20radiology%20reports%20has%20become%20increasingly%20relevant%20to%20assist%0Aradiologists%20with%20their%20heavy%20workloads.%20Most%20current%20methods%20rely%20solely%20on%0Aglobal%20image%20features%2C%20failing%20to%20capture%20fine-grained%20organ%20relationships%0Acrucial%20for%20accurate%20reporting.%20To%20this%20end%2C%20we%20propose%20CT-GRAPH%2C%20a%0Ahierarchical%20graph%20attention%20network%20that%20explicitly%20models%20radiological%0Aknowledge%20by%20structuring%20anatomical%20regions%20into%20a%20graph%2C%20linking%20fine-grained%0Aorgan%20features%20to%20coarser%20anatomical%20systems%20and%20a%20global%20patient%20context.%20Our%0Amethod%20leverages%20pretrained%203D%20medical%20feature%20encoders%20to%20obtain%20global%20and%0Aorgan-level%20features%20by%20utilizing%20anatomical%20masks.%20These%20features%20are%20further%0Arefined%20within%20the%20graph%20and%20then%20integrated%20into%20a%20large%20language%20model%20to%0Agenerate%20detailed%20medical%20reports.%20We%20evaluate%20our%20approach%20for%20the%20task%20of%0Areport%20generation%20on%20the%20large-scale%20chest%20CT%20dataset%20CT-RATE.%20We%20provide%20an%0Ain-depth%20analysis%20of%20pretrained%20feature%20encoders%20for%20CT%20report%20generation%20and%0Ashow%20that%20our%20method%20achieves%20a%20substantial%20improvement%20of%20absolute%207.9%5C%25%20in%20F1%0Ascore%20over%20current%20state-of-the-art%20methods.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/hakal104/CT-GRAPH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCT-GRAPH%253A%2520Hierarchical%2520Graph%2520Attention%2520Network%2520for%2520Anatomy-Guided%2520CT%250A%2520%2520Report%2520Generation%26entry.906535625%3DHamza%2520Kalisch%2520and%2520Fabian%2520H%25C3%25B6rst%2520and%2520Jens%2520Kleesiek%2520and%2520Ken%2520Herrmann%2520and%2520Constantin%2520Seibold%26entry.1292438233%3D%2520%2520As%2520medical%2520imaging%2520is%2520central%2520to%2520diagnostic%2520processes%252C%2520automating%2520the%250Ageneration%2520of%2520radiology%2520reports%2520has%2520become%2520increasingly%2520relevant%2520to%2520assist%250Aradiologists%2520with%2520their%2520heavy%2520workloads.%2520Most%2520current%2520methods%2520rely%2520solely%2520on%250Aglobal%2520image%2520features%252C%2520failing%2520to%2520capture%2520fine-grained%2520organ%2520relationships%250Acrucial%2520for%2520accurate%2520reporting.%2520To%2520this%2520end%252C%2520we%2520propose%2520CT-GRAPH%252C%2520a%250Ahierarchical%2520graph%2520attention%2520network%2520that%2520explicitly%2520models%2520radiological%250Aknowledge%2520by%2520structuring%2520anatomical%2520regions%2520into%2520a%2520graph%252C%2520linking%2520fine-grained%250Aorgan%2520features%2520to%2520coarser%2520anatomical%2520systems%2520and%2520a%2520global%2520patient%2520context.%2520Our%250Amethod%2520leverages%2520pretrained%25203D%2520medical%2520feature%2520encoders%2520to%2520obtain%2520global%2520and%250Aorgan-level%2520features%2520by%2520utilizing%2520anatomical%2520masks.%2520These%2520features%2520are%2520further%250Arefined%2520within%2520the%2520graph%2520and%2520then%2520integrated%2520into%2520a%2520large%2520language%2520model%2520to%250Agenerate%2520detailed%2520medical%2520reports.%2520We%2520evaluate%2520our%2520approach%2520for%2520the%2520task%2520of%250Areport%2520generation%2520on%2520the%2520large-scale%2520chest%2520CT%2520dataset%2520CT-RATE.%2520We%2520provide%2520an%250Ain-depth%2520analysis%2520of%2520pretrained%2520feature%2520encoders%2520for%2520CT%2520report%2520generation%2520and%250Ashow%2520that%2520our%2520method%2520achieves%2520a%2520substantial%2520improvement%2520of%2520absolute%25207.9%255C%2525%2520in%2520F1%250Ascore%2520over%2520current%2520state-of-the-art%2520methods.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/hakal104/CT-GRAPH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT-GRAPH%3A%20Hierarchical%20Graph%20Attention%20Network%20for%20Anatomy-Guided%20CT%0A%20%20Report%20Generation&entry.906535625=Hamza%20Kalisch%20and%20Fabian%20H%C3%B6rst%20and%20Jens%20Kleesiek%20and%20Ken%20Herrmann%20and%20Constantin%20Seibold&entry.1292438233=%20%20As%20medical%20imaging%20is%20central%20to%20diagnostic%20processes%2C%20automating%20the%0Ageneration%20of%20radiology%20reports%20has%20become%20increasingly%20relevant%20to%20assist%0Aradiologists%20with%20their%20heavy%20workloads.%20Most%20current%20methods%20rely%20solely%20on%0Aglobal%20image%20features%2C%20failing%20to%20capture%20fine-grained%20organ%20relationships%0Acrucial%20for%20accurate%20reporting.%20To%20this%20end%2C%20we%20propose%20CT-GRAPH%2C%20a%0Ahierarchical%20graph%20attention%20network%20that%20explicitly%20models%20radiological%0Aknowledge%20by%20structuring%20anatomical%20regions%20into%20a%20graph%2C%20linking%20fine-grained%0Aorgan%20features%20to%20coarser%20anatomical%20systems%20and%20a%20global%20patient%20context.%20Our%0Amethod%20leverages%20pretrained%203D%20medical%20feature%20encoders%20to%20obtain%20global%20and%0Aorgan-level%20features%20by%20utilizing%20anatomical%20masks.%20These%20features%20are%20further%0Arefined%20within%20the%20graph%20and%20then%20integrated%20into%20a%20large%20language%20model%20to%0Agenerate%20detailed%20medical%20reports.%20We%20evaluate%20our%20approach%20for%20the%20task%20of%0Areport%20generation%20on%20the%20large-scale%20chest%20CT%20dataset%20CT-RATE.%20We%20provide%20an%0Ain-depth%20analysis%20of%20pretrained%20feature%20encoders%20for%20CT%20report%20generation%20and%0Ashow%20that%20our%20method%20achieves%20a%20substantial%20improvement%20of%20absolute%207.9%5C%25%20in%20F1%0Ascore%20over%20current%20state-of-the-art%20methods.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/hakal104/CT-GRAPH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05375v1&entry.124074799=Read"},
{"title": "Coarse-to-Fine Joint Registration of MR and Ultrasound Images via\n  Imaging Style Transfer", "author": "Junyi Wang and Xi Zhu and Yikun Guo and Zixi Wang and Haichuan Gao and Le Zhang and Fan Zhang", "abstract": "  We developed a pipeline for registering pre-surgery Magnetic Resonance (MR)\nimages and post-resection Ultrasound (US) images. Our approach leverages\nunpaired style transfer using 3D CycleGAN to generate synthetic T1 images,\nthereby enhancing registration performance. Additionally, our registration\nprocess employs both affine and local deformable transformations for a\ncoarse-to-fine registration. The results demonstrate that our approach improves\nthe consistency between MR and US image pairs in most cases.\n", "link": "http://arxiv.org/abs/2508.05240v1", "date": "2025-08-07", "relevancy": 2.4564, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5038}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4926}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coarse-to-Fine%20Joint%20Registration%20of%20MR%20and%20Ultrasound%20Images%20via%0A%20%20Imaging%20Style%20Transfer&body=Title%3A%20Coarse-to-Fine%20Joint%20Registration%20of%20MR%20and%20Ultrasound%20Images%20via%0A%20%20Imaging%20Style%20Transfer%0AAuthor%3A%20Junyi%20Wang%20and%20Xi%20Zhu%20and%20Yikun%20Guo%20and%20Zixi%20Wang%20and%20Haichuan%20Gao%20and%20Le%20Zhang%20and%20Fan%20Zhang%0AAbstract%3A%20%20%20We%20developed%20a%20pipeline%20for%20registering%20pre-surgery%20Magnetic%20Resonance%20%28MR%29%0Aimages%20and%20post-resection%20Ultrasound%20%28US%29%20images.%20Our%20approach%20leverages%0Aunpaired%20style%20transfer%20using%203D%20CycleGAN%20to%20generate%20synthetic%20T1%20images%2C%0Athereby%20enhancing%20registration%20performance.%20Additionally%2C%20our%20registration%0Aprocess%20employs%20both%20affine%20and%20local%20deformable%20transformations%20for%20a%0Acoarse-to-fine%20registration.%20The%20results%20demonstrate%20that%20our%20approach%20improves%0Athe%20consistency%20between%20MR%20and%20US%20image%20pairs%20in%20most%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoarse-to-Fine%2520Joint%2520Registration%2520of%2520MR%2520and%2520Ultrasound%2520Images%2520via%250A%2520%2520Imaging%2520Style%2520Transfer%26entry.906535625%3DJunyi%2520Wang%2520and%2520Xi%2520Zhu%2520and%2520Yikun%2520Guo%2520and%2520Zixi%2520Wang%2520and%2520Haichuan%2520Gao%2520and%2520Le%2520Zhang%2520and%2520Fan%2520Zhang%26entry.1292438233%3D%2520%2520We%2520developed%2520a%2520pipeline%2520for%2520registering%2520pre-surgery%2520Magnetic%2520Resonance%2520%2528MR%2529%250Aimages%2520and%2520post-resection%2520Ultrasound%2520%2528US%2529%2520images.%2520Our%2520approach%2520leverages%250Aunpaired%2520style%2520transfer%2520using%25203D%2520CycleGAN%2520to%2520generate%2520synthetic%2520T1%2520images%252C%250Athereby%2520enhancing%2520registration%2520performance.%2520Additionally%252C%2520our%2520registration%250Aprocess%2520employs%2520both%2520affine%2520and%2520local%2520deformable%2520transformations%2520for%2520a%250Acoarse-to-fine%2520registration.%2520The%2520results%2520demonstrate%2520that%2520our%2520approach%2520improves%250Athe%2520consistency%2520between%2520MR%2520and%2520US%2520image%2520pairs%2520in%2520most%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coarse-to-Fine%20Joint%20Registration%20of%20MR%20and%20Ultrasound%20Images%20via%0A%20%20Imaging%20Style%20Transfer&entry.906535625=Junyi%20Wang%20and%20Xi%20Zhu%20and%20Yikun%20Guo%20and%20Zixi%20Wang%20and%20Haichuan%20Gao%20and%20Le%20Zhang%20and%20Fan%20Zhang&entry.1292438233=%20%20We%20developed%20a%20pipeline%20for%20registering%20pre-surgery%20Magnetic%20Resonance%20%28MR%29%0Aimages%20and%20post-resection%20Ultrasound%20%28US%29%20images.%20Our%20approach%20leverages%0Aunpaired%20style%20transfer%20using%203D%20CycleGAN%20to%20generate%20synthetic%20T1%20images%2C%0Athereby%20enhancing%20registration%20performance.%20Additionally%2C%20our%20registration%0Aprocess%20employs%20both%20affine%20and%20local%20deformable%20transformations%20for%20a%0Acoarse-to-fine%20registration.%20The%20results%20demonstrate%20that%20our%20approach%20improves%0Athe%20consistency%20between%20MR%20and%20US%20image%20pairs%20in%20most%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05240v1&entry.124074799=Read"},
{"title": "Pruning Large Language Models by Identifying and Preserving Functional\n  Networks", "author": "Yiheng Liu and Junhao Ning and Sichen Xia and Xiaohui Gao and Ning Qiang and Bao Ge and Junwei Han and Xintao Hu", "abstract": "  Structured pruning is one of the representative techniques for compressing\nlarge language models (LLMs) to reduce GPU memory consumption and accelerate\ninference speed. It offers significant practical value in improving the\nefficiency of LLMs in real-world applications. Current structured pruning\nmethods typically rely on assessment of the importance of the structure units\nand pruning the units with less importance. Most of them overlooks the\ninteraction and collaboration among artificial neurons that are crucial for the\nfunctionalities of LLMs, leading to a disruption in the macro functional\narchitecture of LLMs and consequently a pruning performance degradation.\nInspired by the inherent similarities between artificial neural networks and\nfunctional neural networks in the human brain, we alleviate this challenge and\npropose to prune LLMs by identifying and preserving functional networks within\nLLMs in this study. To achieve this, we treat an LLM as a digital brain and\ndecompose the LLM into functional networks, analogous to identifying functional\nbrain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving\nthe key neurons within these functional networks. Experimental results\ndemonstrate that the proposed method can successfully identify and locate\nfunctional networks and key neurons in LLMs, enabling efficient model pruning.\nOur code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.\n", "link": "http://arxiv.org/abs/2508.05239v1", "date": "2025-08-07", "relevancy": 2.4134, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pruning%20Large%20Language%20Models%20by%20Identifying%20and%20Preserving%20Functional%0A%20%20Networks&body=Title%3A%20Pruning%20Large%20Language%20Models%20by%20Identifying%20and%20Preserving%20Functional%0A%20%20Networks%0AAuthor%3A%20Yiheng%20Liu%20and%20Junhao%20Ning%20and%20Sichen%20Xia%20and%20Xiaohui%20Gao%20and%20Ning%20Qiang%20and%20Bao%20Ge%20and%20Junwei%20Han%20and%20Xintao%20Hu%0AAbstract%3A%20%20%20Structured%20pruning%20is%20one%20of%20the%20representative%20techniques%20for%20compressing%0Alarge%20language%20models%20%28LLMs%29%20to%20reduce%20GPU%20memory%20consumption%20and%20accelerate%0Ainference%20speed.%20It%20offers%20significant%20practical%20value%20in%20improving%20the%0Aefficiency%20of%20LLMs%20in%20real-world%20applications.%20Current%20structured%20pruning%0Amethods%20typically%20rely%20on%20assessment%20of%20the%20importance%20of%20the%20structure%20units%0Aand%20pruning%20the%20units%20with%20less%20importance.%20Most%20of%20them%20overlooks%20the%0Ainteraction%20and%20collaboration%20among%20artificial%20neurons%20that%20are%20crucial%20for%20the%0Afunctionalities%20of%20LLMs%2C%20leading%20to%20a%20disruption%20in%20the%20macro%20functional%0Aarchitecture%20of%20LLMs%20and%20consequently%20a%20pruning%20performance%20degradation.%0AInspired%20by%20the%20inherent%20similarities%20between%20artificial%20neural%20networks%20and%0Afunctional%20neural%20networks%20in%20the%20human%20brain%2C%20we%20alleviate%20this%20challenge%20and%0Apropose%20to%20prune%20LLMs%20by%20identifying%20and%20preserving%20functional%20networks%20within%0ALLMs%20in%20this%20study.%20To%20achieve%20this%2C%20we%20treat%20an%20LLM%20as%20a%20digital%20brain%20and%0Adecompose%20the%20LLM%20into%20functional%20networks%2C%20analogous%20to%20identifying%20functional%0Abrain%20networks%20in%20neuroimaging%20data.%20Afterwards%2C%20an%20LLM%20is%20pruned%20by%20preserving%0Athe%20key%20neurons%20within%20these%20functional%20networks.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20can%20successfully%20identify%20and%20locate%0Afunctional%20networks%20and%20key%20neurons%20in%20LLMs%2C%20enabling%20efficient%20model%20pruning.%0AOur%20code%20is%20available%20at%20https%3A//github.com/WhatAboutMyStar/LLM_ACTIVATION.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruning%2520Large%2520Language%2520Models%2520by%2520Identifying%2520and%2520Preserving%2520Functional%250A%2520%2520Networks%26entry.906535625%3DYiheng%2520Liu%2520and%2520Junhao%2520Ning%2520and%2520Sichen%2520Xia%2520and%2520Xiaohui%2520Gao%2520and%2520Ning%2520Qiang%2520and%2520Bao%2520Ge%2520and%2520Junwei%2520Han%2520and%2520Xintao%2520Hu%26entry.1292438233%3D%2520%2520Structured%2520pruning%2520is%2520one%2520of%2520the%2520representative%2520techniques%2520for%2520compressing%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520to%2520reduce%2520GPU%2520memory%2520consumption%2520and%2520accelerate%250Ainference%2520speed.%2520It%2520offers%2520significant%2520practical%2520value%2520in%2520improving%2520the%250Aefficiency%2520of%2520LLMs%2520in%2520real-world%2520applications.%2520Current%2520structured%2520pruning%250Amethods%2520typically%2520rely%2520on%2520assessment%2520of%2520the%2520importance%2520of%2520the%2520structure%2520units%250Aand%2520pruning%2520the%2520units%2520with%2520less%2520importance.%2520Most%2520of%2520them%2520overlooks%2520the%250Ainteraction%2520and%2520collaboration%2520among%2520artificial%2520neurons%2520that%2520are%2520crucial%2520for%2520the%250Afunctionalities%2520of%2520LLMs%252C%2520leading%2520to%2520a%2520disruption%2520in%2520the%2520macro%2520functional%250Aarchitecture%2520of%2520LLMs%2520and%2520consequently%2520a%2520pruning%2520performance%2520degradation.%250AInspired%2520by%2520the%2520inherent%2520similarities%2520between%2520artificial%2520neural%2520networks%2520and%250Afunctional%2520neural%2520networks%2520in%2520the%2520human%2520brain%252C%2520we%2520alleviate%2520this%2520challenge%2520and%250Apropose%2520to%2520prune%2520LLMs%2520by%2520identifying%2520and%2520preserving%2520functional%2520networks%2520within%250ALLMs%2520in%2520this%2520study.%2520To%2520achieve%2520this%252C%2520we%2520treat%2520an%2520LLM%2520as%2520a%2520digital%2520brain%2520and%250Adecompose%2520the%2520LLM%2520into%2520functional%2520networks%252C%2520analogous%2520to%2520identifying%2520functional%250Abrain%2520networks%2520in%2520neuroimaging%2520data.%2520Afterwards%252C%2520an%2520LLM%2520is%2520pruned%2520by%2520preserving%250Athe%2520key%2520neurons%2520within%2520these%2520functional%2520networks.%2520Experimental%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520can%2520successfully%2520identify%2520and%2520locate%250Afunctional%2520networks%2520and%2520key%2520neurons%2520in%2520LLMs%252C%2520enabling%2520efficient%2520model%2520pruning.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/WhatAboutMyStar/LLM_ACTIVATION.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning%20Large%20Language%20Models%20by%20Identifying%20and%20Preserving%20Functional%0A%20%20Networks&entry.906535625=Yiheng%20Liu%20and%20Junhao%20Ning%20and%20Sichen%20Xia%20and%20Xiaohui%20Gao%20and%20Ning%20Qiang%20and%20Bao%20Ge%20and%20Junwei%20Han%20and%20Xintao%20Hu&entry.1292438233=%20%20Structured%20pruning%20is%20one%20of%20the%20representative%20techniques%20for%20compressing%0Alarge%20language%20models%20%28LLMs%29%20to%20reduce%20GPU%20memory%20consumption%20and%20accelerate%0Ainference%20speed.%20It%20offers%20significant%20practical%20value%20in%20improving%20the%0Aefficiency%20of%20LLMs%20in%20real-world%20applications.%20Current%20structured%20pruning%0Amethods%20typically%20rely%20on%20assessment%20of%20the%20importance%20of%20the%20structure%20units%0Aand%20pruning%20the%20units%20with%20less%20importance.%20Most%20of%20them%20overlooks%20the%0Ainteraction%20and%20collaboration%20among%20artificial%20neurons%20that%20are%20crucial%20for%20the%0Afunctionalities%20of%20LLMs%2C%20leading%20to%20a%20disruption%20in%20the%20macro%20functional%0Aarchitecture%20of%20LLMs%20and%20consequently%20a%20pruning%20performance%20degradation.%0AInspired%20by%20the%20inherent%20similarities%20between%20artificial%20neural%20networks%20and%0Afunctional%20neural%20networks%20in%20the%20human%20brain%2C%20we%20alleviate%20this%20challenge%20and%0Apropose%20to%20prune%20LLMs%20by%20identifying%20and%20preserving%20functional%20networks%20within%0ALLMs%20in%20this%20study.%20To%20achieve%20this%2C%20we%20treat%20an%20LLM%20as%20a%20digital%20brain%20and%0Adecompose%20the%20LLM%20into%20functional%20networks%2C%20analogous%20to%20identifying%20functional%0Abrain%20networks%20in%20neuroimaging%20data.%20Afterwards%2C%20an%20LLM%20is%20pruned%20by%20preserving%0Athe%20key%20neurons%20within%20these%20functional%20networks.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20can%20successfully%20identify%20and%20locate%0Afunctional%20networks%20and%20key%20neurons%20in%20LLMs%2C%20enabling%20efficient%20model%20pruning.%0AOur%20code%20is%20available%20at%20https%3A//github.com/WhatAboutMyStar/LLM_ACTIVATION.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05239v1&entry.124074799=Read"},
{"title": "SincVAE: A new semi-supervised approach to improve anomaly detection on\n  EEG data using SincNet and variational autoencoder", "author": "Andrea Pollastro and Francesco Isgr\u00f2 and Roberto Prevete", "abstract": "  Over the past few decades, electroencephalography (EEG) monitoring has become\na pivotal tool for diagnosing neurological disorders, particularly for\ndetecting seizures. Epilepsy, one of the most prevalent neurological diseases\nworldwide, affects approximately the 1 \\% of the population. These patients\nface significant risks, underscoring the need for reliable, continuous seizure\nmonitoring in daily life. Most of the techniques discussed in the literature\nrely on supervised Machine Learning (ML) methods. However, the challenge of\naccurately labeling variations in epileptic EEG waveforms complicates the use\nof these approaches. Additionally, the rarity of ictal events introduces an\nhigh imbalancing within the data, which could lead to poor prediction\nperformance in supervised learning approaches. Instead, a semi-supervised\napproach allows to train the model only on data not containing seizures, thus\navoiding the issues related to the data imbalancing. This work proposes a\nsemi-supervised approach for detecting epileptic seizures from EEG data,\nutilizing a novel Deep Learning-based method called SincVAE. This proposal\nincorporates the learning of an ad-hoc array of bandpass filter as a first\nlayer of a Variational Autoencoder (VAE), potentially eliminating the\npreprocessing stage where informative band frequencies are identified and\nisolated. Results indicate that SincVAE improves seizure detection in EEG data\nand is capable of identifying early seizures during the preictal stage as well\nas monitoring patients throughout the postictal stage.\n", "link": "http://arxiv.org/abs/2406.17537v2", "date": "2025-08-07", "relevancy": 2.3993, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5042}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4726}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SincVAE%3A%20A%20new%20semi-supervised%20approach%20to%20improve%20anomaly%20detection%20on%0A%20%20EEG%20data%20using%20SincNet%20and%20variational%20autoencoder&body=Title%3A%20SincVAE%3A%20A%20new%20semi-supervised%20approach%20to%20improve%20anomaly%20detection%20on%0A%20%20EEG%20data%20using%20SincNet%20and%20variational%20autoencoder%0AAuthor%3A%20Andrea%20Pollastro%20and%20Francesco%20Isgr%C3%B2%20and%20Roberto%20Prevete%0AAbstract%3A%20%20%20Over%20the%20past%20few%20decades%2C%20electroencephalography%20%28EEG%29%20monitoring%20has%20become%0Aa%20pivotal%20tool%20for%20diagnosing%20neurological%20disorders%2C%20particularly%20for%0Adetecting%20seizures.%20Epilepsy%2C%20one%20of%20the%20most%20prevalent%20neurological%20diseases%0Aworldwide%2C%20affects%20approximately%20the%201%20%5C%25%20of%20the%20population.%20These%20patients%0Aface%20significant%20risks%2C%20underscoring%20the%20need%20for%20reliable%2C%20continuous%20seizure%0Amonitoring%20in%20daily%20life.%20Most%20of%20the%20techniques%20discussed%20in%20the%20literature%0Arely%20on%20supervised%20Machine%20Learning%20%28ML%29%20methods.%20However%2C%20the%20challenge%20of%0Aaccurately%20labeling%20variations%20in%20epileptic%20EEG%20waveforms%20complicates%20the%20use%0Aof%20these%20approaches.%20Additionally%2C%20the%20rarity%20of%20ictal%20events%20introduces%20an%0Ahigh%20imbalancing%20within%20the%20data%2C%20which%20could%20lead%20to%20poor%20prediction%0Aperformance%20in%20supervised%20learning%20approaches.%20Instead%2C%20a%20semi-supervised%0Aapproach%20allows%20to%20train%20the%20model%20only%20on%20data%20not%20containing%20seizures%2C%20thus%0Aavoiding%20the%20issues%20related%20to%20the%20data%20imbalancing.%20This%20work%20proposes%20a%0Asemi-supervised%20approach%20for%20detecting%20epileptic%20seizures%20from%20EEG%20data%2C%0Autilizing%20a%20novel%20Deep%20Learning-based%20method%20called%20SincVAE.%20This%20proposal%0Aincorporates%20the%20learning%20of%20an%20ad-hoc%20array%20of%20bandpass%20filter%20as%20a%20first%0Alayer%20of%20a%20Variational%20Autoencoder%20%28VAE%29%2C%20potentially%20eliminating%20the%0Apreprocessing%20stage%20where%20informative%20band%20frequencies%20are%20identified%20and%0Aisolated.%20Results%20indicate%20that%20SincVAE%20improves%20seizure%20detection%20in%20EEG%20data%0Aand%20is%20capable%20of%20identifying%20early%20seizures%20during%20the%20preictal%20stage%20as%20well%0Aas%20monitoring%20patients%20throughout%20the%20postictal%20stage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17537v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSincVAE%253A%2520A%2520new%2520semi-supervised%2520approach%2520to%2520improve%2520anomaly%2520detection%2520on%250A%2520%2520EEG%2520data%2520using%2520SincNet%2520and%2520variational%2520autoencoder%26entry.906535625%3DAndrea%2520Pollastro%2520and%2520Francesco%2520Isgr%25C3%25B2%2520and%2520Roberto%2520Prevete%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520few%2520decades%252C%2520electroencephalography%2520%2528EEG%2529%2520monitoring%2520has%2520become%250Aa%2520pivotal%2520tool%2520for%2520diagnosing%2520neurological%2520disorders%252C%2520particularly%2520for%250Adetecting%2520seizures.%2520Epilepsy%252C%2520one%2520of%2520the%2520most%2520prevalent%2520neurological%2520diseases%250Aworldwide%252C%2520affects%2520approximately%2520the%25201%2520%255C%2525%2520of%2520the%2520population.%2520These%2520patients%250Aface%2520significant%2520risks%252C%2520underscoring%2520the%2520need%2520for%2520reliable%252C%2520continuous%2520seizure%250Amonitoring%2520in%2520daily%2520life.%2520Most%2520of%2520the%2520techniques%2520discussed%2520in%2520the%2520literature%250Arely%2520on%2520supervised%2520Machine%2520Learning%2520%2528ML%2529%2520methods.%2520However%252C%2520the%2520challenge%2520of%250Aaccurately%2520labeling%2520variations%2520in%2520epileptic%2520EEG%2520waveforms%2520complicates%2520the%2520use%250Aof%2520these%2520approaches.%2520Additionally%252C%2520the%2520rarity%2520of%2520ictal%2520events%2520introduces%2520an%250Ahigh%2520imbalancing%2520within%2520the%2520data%252C%2520which%2520could%2520lead%2520to%2520poor%2520prediction%250Aperformance%2520in%2520supervised%2520learning%2520approaches.%2520Instead%252C%2520a%2520semi-supervised%250Aapproach%2520allows%2520to%2520train%2520the%2520model%2520only%2520on%2520data%2520not%2520containing%2520seizures%252C%2520thus%250Aavoiding%2520the%2520issues%2520related%2520to%2520the%2520data%2520imbalancing.%2520This%2520work%2520proposes%2520a%250Asemi-supervised%2520approach%2520for%2520detecting%2520epileptic%2520seizures%2520from%2520EEG%2520data%252C%250Autilizing%2520a%2520novel%2520Deep%2520Learning-based%2520method%2520called%2520SincVAE.%2520This%2520proposal%250Aincorporates%2520the%2520learning%2520of%2520an%2520ad-hoc%2520array%2520of%2520bandpass%2520filter%2520as%2520a%2520first%250Alayer%2520of%2520a%2520Variational%2520Autoencoder%2520%2528VAE%2529%252C%2520potentially%2520eliminating%2520the%250Apreprocessing%2520stage%2520where%2520informative%2520band%2520frequencies%2520are%2520identified%2520and%250Aisolated.%2520Results%2520indicate%2520that%2520SincVAE%2520improves%2520seizure%2520detection%2520in%2520EEG%2520data%250Aand%2520is%2520capable%2520of%2520identifying%2520early%2520seizures%2520during%2520the%2520preictal%2520stage%2520as%2520well%250Aas%2520monitoring%2520patients%2520throughout%2520the%2520postictal%2520stage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17537v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SincVAE%3A%20A%20new%20semi-supervised%20approach%20to%20improve%20anomaly%20detection%20on%0A%20%20EEG%20data%20using%20SincNet%20and%20variational%20autoencoder&entry.906535625=Andrea%20Pollastro%20and%20Francesco%20Isgr%C3%B2%20and%20Roberto%20Prevete&entry.1292438233=%20%20Over%20the%20past%20few%20decades%2C%20electroencephalography%20%28EEG%29%20monitoring%20has%20become%0Aa%20pivotal%20tool%20for%20diagnosing%20neurological%20disorders%2C%20particularly%20for%0Adetecting%20seizures.%20Epilepsy%2C%20one%20of%20the%20most%20prevalent%20neurological%20diseases%0Aworldwide%2C%20affects%20approximately%20the%201%20%5C%25%20of%20the%20population.%20These%20patients%0Aface%20significant%20risks%2C%20underscoring%20the%20need%20for%20reliable%2C%20continuous%20seizure%0Amonitoring%20in%20daily%20life.%20Most%20of%20the%20techniques%20discussed%20in%20the%20literature%0Arely%20on%20supervised%20Machine%20Learning%20%28ML%29%20methods.%20However%2C%20the%20challenge%20of%0Aaccurately%20labeling%20variations%20in%20epileptic%20EEG%20waveforms%20complicates%20the%20use%0Aof%20these%20approaches.%20Additionally%2C%20the%20rarity%20of%20ictal%20events%20introduces%20an%0Ahigh%20imbalancing%20within%20the%20data%2C%20which%20could%20lead%20to%20poor%20prediction%0Aperformance%20in%20supervised%20learning%20approaches.%20Instead%2C%20a%20semi-supervised%0Aapproach%20allows%20to%20train%20the%20model%20only%20on%20data%20not%20containing%20seizures%2C%20thus%0Aavoiding%20the%20issues%20related%20to%20the%20data%20imbalancing.%20This%20work%20proposes%20a%0Asemi-supervised%20approach%20for%20detecting%20epileptic%20seizures%20from%20EEG%20data%2C%0Autilizing%20a%20novel%20Deep%20Learning-based%20method%20called%20SincVAE.%20This%20proposal%0Aincorporates%20the%20learning%20of%20an%20ad-hoc%20array%20of%20bandpass%20filter%20as%20a%20first%0Alayer%20of%20a%20Variational%20Autoencoder%20%28VAE%29%2C%20potentially%20eliminating%20the%0Apreprocessing%20stage%20where%20informative%20band%20frequencies%20are%20identified%20and%0Aisolated.%20Results%20indicate%20that%20SincVAE%20improves%20seizure%20detection%20in%20EEG%20data%0Aand%20is%20capable%20of%20identifying%20early%20seizures%20during%20the%20preictal%20stage%20as%20well%0Aas%20monitoring%20patients%20throughout%20the%20postictal%20stage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17537v2&entry.124074799=Read"},
{"title": "Optimal Corpus Aware Training for Neural Machine Translation", "author": "Yi-Hsiu Liao and Cheng Shen and  Brenda and  Yang", "abstract": "  Corpus Aware Training (CAT) leverages valuable corpus metadata during\ntraining by injecting corpus information into each training example, and has\nbeen found effective in the literature, commonly known as the \"tagging\"\napproach. Models trained with CAT inherently learn the quality, domain and\nnuance between corpora directly from data, and can easily switch to different\ninference behavior. To achieve the best evaluation, CAT models pre-define a\ngroup of high quality data before training starts which can be error-prone and\ninefficient. In this work, we propose Optimal Corpus Aware Training (OCAT),\nwhich fine-tunes a CAT pre-trained model by freezing most of the model\nparameters and only tuning small set of corpus-related parameters. We show that\nOCAT is lightweight, resilient to overfitting, and effective in boosting model\naccuracy. We use WMT23 English to Chinese and English to German translation\ntasks as our test ground and show +3.6 and +1.8 chrF improvement, respectively,\nover vanilla training. Furthermore, our approach is on-par or slightly better\nthan other state-of-the-art fine-tuning techniques while being less sensitive\nto hyperparameter settings.\n", "link": "http://arxiv.org/abs/2508.05364v1", "date": "2025-08-07", "relevancy": 2.3881, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4659}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Corpus%20Aware%20Training%20for%20Neural%20Machine%20Translation&body=Title%3A%20Optimal%20Corpus%20Aware%20Training%20for%20Neural%20Machine%20Translation%0AAuthor%3A%20Yi-Hsiu%20Liao%20and%20Cheng%20Shen%20and%20%20Brenda%20and%20%20Yang%0AAbstract%3A%20%20%20Corpus%20Aware%20Training%20%28CAT%29%20leverages%20valuable%20corpus%20metadata%20during%0Atraining%20by%20injecting%20corpus%20information%20into%20each%20training%20example%2C%20and%20has%0Abeen%20found%20effective%20in%20the%20literature%2C%20commonly%20known%20as%20the%20%22tagging%22%0Aapproach.%20Models%20trained%20with%20CAT%20inherently%20learn%20the%20quality%2C%20domain%20and%0Anuance%20between%20corpora%20directly%20from%20data%2C%20and%20can%20easily%20switch%20to%20different%0Ainference%20behavior.%20To%20achieve%20the%20best%20evaluation%2C%20CAT%20models%20pre-define%20a%0Agroup%20of%20high%20quality%20data%20before%20training%20starts%20which%20can%20be%20error-prone%20and%0Ainefficient.%20In%20this%20work%2C%20we%20propose%20Optimal%20Corpus%20Aware%20Training%20%28OCAT%29%2C%0Awhich%20fine-tunes%20a%20CAT%20pre-trained%20model%20by%20freezing%20most%20of%20the%20model%0Aparameters%20and%20only%20tuning%20small%20set%20of%20corpus-related%20parameters.%20We%20show%20that%0AOCAT%20is%20lightweight%2C%20resilient%20to%20overfitting%2C%20and%20effective%20in%20boosting%20model%0Aaccuracy.%20We%20use%20WMT23%20English%20to%20Chinese%20and%20English%20to%20German%20translation%0Atasks%20as%20our%20test%20ground%20and%20show%20%2B3.6%20and%20%2B1.8%20chrF%20improvement%2C%20respectively%2C%0Aover%20vanilla%20training.%20Furthermore%2C%20our%20approach%20is%20on-par%20or%20slightly%20better%0Athan%20other%20state-of-the-art%20fine-tuning%20techniques%20while%20being%20less%20sensitive%0Ato%20hyperparameter%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Corpus%2520Aware%2520Training%2520for%2520Neural%2520Machine%2520Translation%26entry.906535625%3DYi-Hsiu%2520Liao%2520and%2520Cheng%2520Shen%2520and%2520%2520Brenda%2520and%2520%2520Yang%26entry.1292438233%3D%2520%2520Corpus%2520Aware%2520Training%2520%2528CAT%2529%2520leverages%2520valuable%2520corpus%2520metadata%2520during%250Atraining%2520by%2520injecting%2520corpus%2520information%2520into%2520each%2520training%2520example%252C%2520and%2520has%250Abeen%2520found%2520effective%2520in%2520the%2520literature%252C%2520commonly%2520known%2520as%2520the%2520%2522tagging%2522%250Aapproach.%2520Models%2520trained%2520with%2520CAT%2520inherently%2520learn%2520the%2520quality%252C%2520domain%2520and%250Anuance%2520between%2520corpora%2520directly%2520from%2520data%252C%2520and%2520can%2520easily%2520switch%2520to%2520different%250Ainference%2520behavior.%2520To%2520achieve%2520the%2520best%2520evaluation%252C%2520CAT%2520models%2520pre-define%2520a%250Agroup%2520of%2520high%2520quality%2520data%2520before%2520training%2520starts%2520which%2520can%2520be%2520error-prone%2520and%250Ainefficient.%2520In%2520this%2520work%252C%2520we%2520propose%2520Optimal%2520Corpus%2520Aware%2520Training%2520%2528OCAT%2529%252C%250Awhich%2520fine-tunes%2520a%2520CAT%2520pre-trained%2520model%2520by%2520freezing%2520most%2520of%2520the%2520model%250Aparameters%2520and%2520only%2520tuning%2520small%2520set%2520of%2520corpus-related%2520parameters.%2520We%2520show%2520that%250AOCAT%2520is%2520lightweight%252C%2520resilient%2520to%2520overfitting%252C%2520and%2520effective%2520in%2520boosting%2520model%250Aaccuracy.%2520We%2520use%2520WMT23%2520English%2520to%2520Chinese%2520and%2520English%2520to%2520German%2520translation%250Atasks%2520as%2520our%2520test%2520ground%2520and%2520show%2520%252B3.6%2520and%2520%252B1.8%2520chrF%2520improvement%252C%2520respectively%252C%250Aover%2520vanilla%2520training.%2520Furthermore%252C%2520our%2520approach%2520is%2520on-par%2520or%2520slightly%2520better%250Athan%2520other%2520state-of-the-art%2520fine-tuning%2520techniques%2520while%2520being%2520less%2520sensitive%250Ato%2520hyperparameter%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Corpus%20Aware%20Training%20for%20Neural%20Machine%20Translation&entry.906535625=Yi-Hsiu%20Liao%20and%20Cheng%20Shen%20and%20%20Brenda%20and%20%20Yang&entry.1292438233=%20%20Corpus%20Aware%20Training%20%28CAT%29%20leverages%20valuable%20corpus%20metadata%20during%0Atraining%20by%20injecting%20corpus%20information%20into%20each%20training%20example%2C%20and%20has%0Abeen%20found%20effective%20in%20the%20literature%2C%20commonly%20known%20as%20the%20%22tagging%22%0Aapproach.%20Models%20trained%20with%20CAT%20inherently%20learn%20the%20quality%2C%20domain%20and%0Anuance%20between%20corpora%20directly%20from%20data%2C%20and%20can%20easily%20switch%20to%20different%0Ainference%20behavior.%20To%20achieve%20the%20best%20evaluation%2C%20CAT%20models%20pre-define%20a%0Agroup%20of%20high%20quality%20data%20before%20training%20starts%20which%20can%20be%20error-prone%20and%0Ainefficient.%20In%20this%20work%2C%20we%20propose%20Optimal%20Corpus%20Aware%20Training%20%28OCAT%29%2C%0Awhich%20fine-tunes%20a%20CAT%20pre-trained%20model%20by%20freezing%20most%20of%20the%20model%0Aparameters%20and%20only%20tuning%20small%20set%20of%20corpus-related%20parameters.%20We%20show%20that%0AOCAT%20is%20lightweight%2C%20resilient%20to%20overfitting%2C%20and%20effective%20in%20boosting%20model%0Aaccuracy.%20We%20use%20WMT23%20English%20to%20Chinese%20and%20English%20to%20German%20translation%0Atasks%20as%20our%20test%20ground%20and%20show%20%2B3.6%20and%20%2B1.8%20chrF%20improvement%2C%20respectively%2C%0Aover%20vanilla%20training.%20Furthermore%2C%20our%20approach%20is%20on-par%20or%20slightly%20better%0Athan%20other%20state-of-the-art%20fine-tuning%20techniques%20while%20being%20less%20sensitive%0Ato%20hyperparameter%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05364v1&entry.124074799=Read"},
{"title": "Revealing Latent Information: A Physics-inspired Self-supervised\n  Pre-training Framework for Noisy and Sparse Events", "author": "Lin Zhu and Ruonan Liu and Xiao Wang and Lizhi Wang and Hua Huang", "abstract": "  Event camera, a novel neuromorphic vision sensor, records data with high\ntemporal resolution and wide dynamic range, offering new possibilities for\naccurate visual representation in challenging scenarios. However, event data is\ninherently sparse and noisy, mainly reflecting brightness changes, which\ncomplicates effective feature extraction. To address this, we propose a\nself-supervised pre-training framework to fully reveal latent information in\nevent data, including edge information and texture cues. Our framework consists\nof three stages: Difference-guided Masked Modeling, inspired by the event\nphysical sampling process, reconstructs temporal intensity difference maps to\nextract enhanced information from raw event data. Backbone-fixed Feature\nTransition contrasts event and image features without updating the backbone to\npreserve representations learned from masked modeling and stabilizing their\neffect on contrastive learning. Focus-aimed Contrastive Learning updates the\nentire model to improve semantic discrimination by focusing on high-value\nregions. Extensive experiments show our framework is robust and consistently\noutperforms state-of-the-art methods on various downstream tasks, including\nobject recognition, semantic segmentation, and optical flow estimation. The\ncode and dataset are available at https://github.com/BIT-Vision/EventPretrain.\n", "link": "http://arxiv.org/abs/2508.05507v1", "date": "2025-08-07", "relevancy": 2.3877, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6338}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20Latent%20Information%3A%20A%20Physics-inspired%20Self-supervised%0A%20%20Pre-training%20Framework%20for%20Noisy%20and%20Sparse%20Events&body=Title%3A%20Revealing%20Latent%20Information%3A%20A%20Physics-inspired%20Self-supervised%0A%20%20Pre-training%20Framework%20for%20Noisy%20and%20Sparse%20Events%0AAuthor%3A%20Lin%20Zhu%20and%20Ruonan%20Liu%20and%20Xiao%20Wang%20and%20Lizhi%20Wang%20and%20Hua%20Huang%0AAbstract%3A%20%20%20Event%20camera%2C%20a%20novel%20neuromorphic%20vision%20sensor%2C%20records%20data%20with%20high%0Atemporal%20resolution%20and%20wide%20dynamic%20range%2C%20offering%20new%20possibilities%20for%0Aaccurate%20visual%20representation%20in%20challenging%20scenarios.%20However%2C%20event%20data%20is%0Ainherently%20sparse%20and%20noisy%2C%20mainly%20reflecting%20brightness%20changes%2C%20which%0Acomplicates%20effective%20feature%20extraction.%20To%20address%20this%2C%20we%20propose%20a%0Aself-supervised%20pre-training%20framework%20to%20fully%20reveal%20latent%20information%20in%0Aevent%20data%2C%20including%20edge%20information%20and%20texture%20cues.%20Our%20framework%20consists%0Aof%20three%20stages%3A%20Difference-guided%20Masked%20Modeling%2C%20inspired%20by%20the%20event%0Aphysical%20sampling%20process%2C%20reconstructs%20temporal%20intensity%20difference%20maps%20to%0Aextract%20enhanced%20information%20from%20raw%20event%20data.%20Backbone-fixed%20Feature%0ATransition%20contrasts%20event%20and%20image%20features%20without%20updating%20the%20backbone%20to%0Apreserve%20representations%20learned%20from%20masked%20modeling%20and%20stabilizing%20their%0Aeffect%20on%20contrastive%20learning.%20Focus-aimed%20Contrastive%20Learning%20updates%20the%0Aentire%20model%20to%20improve%20semantic%20discrimination%20by%20focusing%20on%20high-value%0Aregions.%20Extensive%20experiments%20show%20our%20framework%20is%20robust%20and%20consistently%0Aoutperforms%20state-of-the-art%20methods%20on%20various%20downstream%20tasks%2C%20including%0Aobject%20recognition%2C%20semantic%20segmentation%2C%20and%20optical%20flow%20estimation.%20The%0Acode%20and%20dataset%20are%20available%20at%20https%3A//github.com/BIT-Vision/EventPretrain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520Latent%2520Information%253A%2520A%2520Physics-inspired%2520Self-supervised%250A%2520%2520Pre-training%2520Framework%2520for%2520Noisy%2520and%2520Sparse%2520Events%26entry.906535625%3DLin%2520Zhu%2520and%2520Ruonan%2520Liu%2520and%2520Xiao%2520Wang%2520and%2520Lizhi%2520Wang%2520and%2520Hua%2520Huang%26entry.1292438233%3D%2520%2520Event%2520camera%252C%2520a%2520novel%2520neuromorphic%2520vision%2520sensor%252C%2520records%2520data%2520with%2520high%250Atemporal%2520resolution%2520and%2520wide%2520dynamic%2520range%252C%2520offering%2520new%2520possibilities%2520for%250Aaccurate%2520visual%2520representation%2520in%2520challenging%2520scenarios.%2520However%252C%2520event%2520data%2520is%250Ainherently%2520sparse%2520and%2520noisy%252C%2520mainly%2520reflecting%2520brightness%2520changes%252C%2520which%250Acomplicates%2520effective%2520feature%2520extraction.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Aself-supervised%2520pre-training%2520framework%2520to%2520fully%2520reveal%2520latent%2520information%2520in%250Aevent%2520data%252C%2520including%2520edge%2520information%2520and%2520texture%2520cues.%2520Our%2520framework%2520consists%250Aof%2520three%2520stages%253A%2520Difference-guided%2520Masked%2520Modeling%252C%2520inspired%2520by%2520the%2520event%250Aphysical%2520sampling%2520process%252C%2520reconstructs%2520temporal%2520intensity%2520difference%2520maps%2520to%250Aextract%2520enhanced%2520information%2520from%2520raw%2520event%2520data.%2520Backbone-fixed%2520Feature%250ATransition%2520contrasts%2520event%2520and%2520image%2520features%2520without%2520updating%2520the%2520backbone%2520to%250Apreserve%2520representations%2520learned%2520from%2520masked%2520modeling%2520and%2520stabilizing%2520their%250Aeffect%2520on%2520contrastive%2520learning.%2520Focus-aimed%2520Contrastive%2520Learning%2520updates%2520the%250Aentire%2520model%2520to%2520improve%2520semantic%2520discrimination%2520by%2520focusing%2520on%2520high-value%250Aregions.%2520Extensive%2520experiments%2520show%2520our%2520framework%2520is%2520robust%2520and%2520consistently%250Aoutperforms%2520state-of-the-art%2520methods%2520on%2520various%2520downstream%2520tasks%252C%2520including%250Aobject%2520recognition%252C%2520semantic%2520segmentation%252C%2520and%2520optical%2520flow%2520estimation.%2520The%250Acode%2520and%2520dataset%2520are%2520available%2520at%2520https%253A//github.com/BIT-Vision/EventPretrain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20Latent%20Information%3A%20A%20Physics-inspired%20Self-supervised%0A%20%20Pre-training%20Framework%20for%20Noisy%20and%20Sparse%20Events&entry.906535625=Lin%20Zhu%20and%20Ruonan%20Liu%20and%20Xiao%20Wang%20and%20Lizhi%20Wang%20and%20Hua%20Huang&entry.1292438233=%20%20Event%20camera%2C%20a%20novel%20neuromorphic%20vision%20sensor%2C%20records%20data%20with%20high%0Atemporal%20resolution%20and%20wide%20dynamic%20range%2C%20offering%20new%20possibilities%20for%0Aaccurate%20visual%20representation%20in%20challenging%20scenarios.%20However%2C%20event%20data%20is%0Ainherently%20sparse%20and%20noisy%2C%20mainly%20reflecting%20brightness%20changes%2C%20which%0Acomplicates%20effective%20feature%20extraction.%20To%20address%20this%2C%20we%20propose%20a%0Aself-supervised%20pre-training%20framework%20to%20fully%20reveal%20latent%20information%20in%0Aevent%20data%2C%20including%20edge%20information%20and%20texture%20cues.%20Our%20framework%20consists%0Aof%20three%20stages%3A%20Difference-guided%20Masked%20Modeling%2C%20inspired%20by%20the%20event%0Aphysical%20sampling%20process%2C%20reconstructs%20temporal%20intensity%20difference%20maps%20to%0Aextract%20enhanced%20information%20from%20raw%20event%20data.%20Backbone-fixed%20Feature%0ATransition%20contrasts%20event%20and%20image%20features%20without%20updating%20the%20backbone%20to%0Apreserve%20representations%20learned%20from%20masked%20modeling%20and%20stabilizing%20their%0Aeffect%20on%20contrastive%20learning.%20Focus-aimed%20Contrastive%20Learning%20updates%20the%0Aentire%20model%20to%20improve%20semantic%20discrimination%20by%20focusing%20on%20high-value%0Aregions.%20Extensive%20experiments%20show%20our%20framework%20is%20robust%20and%20consistently%0Aoutperforms%20state-of-the-art%20methods%20on%20various%20downstream%20tasks%2C%20including%0Aobject%20recognition%2C%20semantic%20segmentation%2C%20and%20optical%20flow%20estimation.%20The%0Acode%20and%20dataset%20are%20available%20at%20https%3A//github.com/BIT-Vision/EventPretrain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05507v1&entry.124074799=Read"},
{"title": "H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language\n  Modelling in Morphologically-Rich Languages", "author": "Mehrdad Zakershahrak and Samira Ghodratnama", "abstract": "  Byte-level language models eliminate fragile tokenizers but face\ncomputational challenges in morphologically-rich languages (MRLs), where words\nspan many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that\nlearns linguistically-informed segmentation through end-to-end training. Key\ninnovations include: (1) a lightweight Transformer context-mixer (1.9M\nparameters) for cross-chunk attention, (2) a two-level latent hyper-prior for\ndocument-level consistency, (3) specialized handling of orthographic artifacts\n(e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence\nlengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art\nresults: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better\ncompression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ\ncorruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks\nalign with Persian morphology without explicit supervision, demonstrating that\nhierarchical dynamic chunking provides an effective tokenizer-free solution for\nMRLs while maintaining computational efficiency.\n", "link": "http://arxiv.org/abs/2508.05628v1", "date": "2025-08-07", "relevancy": 2.3849, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H-Net%2B%2B%3A%20Hierarchical%20Dynamic%20Chunking%20for%20Tokenizer-Free%20Language%0A%20%20Modelling%20in%20Morphologically-Rich%20Languages&body=Title%3A%20H-Net%2B%2B%3A%20Hierarchical%20Dynamic%20Chunking%20for%20Tokenizer-Free%20Language%0A%20%20Modelling%20in%20Morphologically-Rich%20Languages%0AAuthor%3A%20Mehrdad%20Zakershahrak%20and%20Samira%20Ghodratnama%0AAbstract%3A%20%20%20Byte-level%20language%20models%20eliminate%20fragile%20tokenizers%20but%20face%0Acomputational%20challenges%20in%20morphologically-rich%20languages%20%28MRLs%29%2C%20where%20words%0Aspan%20many%20bytes.%20We%20propose%20H-NET%2B%2B%2C%20a%20hierarchical%20dynamic-chunking%20model%20that%0Alearns%20linguistically-informed%20segmentation%20through%20end-to-end%20training.%20Key%0Ainnovations%20include%3A%20%281%29%20a%20lightweight%20Transformer%20context-mixer%20%281.9M%0Aparameters%29%20for%20cross-chunk%20attention%2C%20%282%29%20a%20two-level%20latent%20hyper-prior%20for%0Adocument-level%20consistency%2C%20%283%29%20specialized%20handling%20of%20orthographic%20artifacts%0A%28e.g.%20Persian%20ZWNJ%29%2C%20and%20%284%29%20curriculum-based%20training%20with%20staged%20sequence%0Alengths.%20On%20a%201.4B-token%20Persian%20corpus%2C%20H-NET%2B%2B%20achieves%20state-of-the-art%0Aresults%3A%200.159%20BPB%20reduction%20versus%20BPE-based%20GPT-2-fa%20%2812%25%20better%0Acompression%29%2C%205.4pp%20gain%20on%20ParsGLUE%2C%2053%25%20improved%20robustness%20to%20ZWNJ%0Acorruption%2C%20and%2073.8%25%20F1%20on%20gold%20morphological%20boundaries.%20Our%20learned%20chunks%0Aalign%20with%20Persian%20morphology%20without%20explicit%20supervision%2C%20demonstrating%20that%0Ahierarchical%20dynamic%20chunking%20provides%20an%20effective%20tokenizer-free%20solution%20for%0AMRLs%20while%20maintaining%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH-Net%252B%252B%253A%2520Hierarchical%2520Dynamic%2520Chunking%2520for%2520Tokenizer-Free%2520Language%250A%2520%2520Modelling%2520in%2520Morphologically-Rich%2520Languages%26entry.906535625%3DMehrdad%2520Zakershahrak%2520and%2520Samira%2520Ghodratnama%26entry.1292438233%3D%2520%2520Byte-level%2520language%2520models%2520eliminate%2520fragile%2520tokenizers%2520but%2520face%250Acomputational%2520challenges%2520in%2520morphologically-rich%2520languages%2520%2528MRLs%2529%252C%2520where%2520words%250Aspan%2520many%2520bytes.%2520We%2520propose%2520H-NET%252B%252B%252C%2520a%2520hierarchical%2520dynamic-chunking%2520model%2520that%250Alearns%2520linguistically-informed%2520segmentation%2520through%2520end-to-end%2520training.%2520Key%250Ainnovations%2520include%253A%2520%25281%2529%2520a%2520lightweight%2520Transformer%2520context-mixer%2520%25281.9M%250Aparameters%2529%2520for%2520cross-chunk%2520attention%252C%2520%25282%2529%2520a%2520two-level%2520latent%2520hyper-prior%2520for%250Adocument-level%2520consistency%252C%2520%25283%2529%2520specialized%2520handling%2520of%2520orthographic%2520artifacts%250A%2528e.g.%2520Persian%2520ZWNJ%2529%252C%2520and%2520%25284%2529%2520curriculum-based%2520training%2520with%2520staged%2520sequence%250Alengths.%2520On%2520a%25201.4B-token%2520Persian%2520corpus%252C%2520H-NET%252B%252B%2520achieves%2520state-of-the-art%250Aresults%253A%25200.159%2520BPB%2520reduction%2520versus%2520BPE-based%2520GPT-2-fa%2520%252812%2525%2520better%250Acompression%2529%252C%25205.4pp%2520gain%2520on%2520ParsGLUE%252C%252053%2525%2520improved%2520robustness%2520to%2520ZWNJ%250Acorruption%252C%2520and%252073.8%2525%2520F1%2520on%2520gold%2520morphological%2520boundaries.%2520Our%2520learned%2520chunks%250Aalign%2520with%2520Persian%2520morphology%2520without%2520explicit%2520supervision%252C%2520demonstrating%2520that%250Ahierarchical%2520dynamic%2520chunking%2520provides%2520an%2520effective%2520tokenizer-free%2520solution%2520for%250AMRLs%2520while%2520maintaining%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H-Net%2B%2B%3A%20Hierarchical%20Dynamic%20Chunking%20for%20Tokenizer-Free%20Language%0A%20%20Modelling%20in%20Morphologically-Rich%20Languages&entry.906535625=Mehrdad%20Zakershahrak%20and%20Samira%20Ghodratnama&entry.1292438233=%20%20Byte-level%20language%20models%20eliminate%20fragile%20tokenizers%20but%20face%0Acomputational%20challenges%20in%20morphologically-rich%20languages%20%28MRLs%29%2C%20where%20words%0Aspan%20many%20bytes.%20We%20propose%20H-NET%2B%2B%2C%20a%20hierarchical%20dynamic-chunking%20model%20that%0Alearns%20linguistically-informed%20segmentation%20through%20end-to-end%20training.%20Key%0Ainnovations%20include%3A%20%281%29%20a%20lightweight%20Transformer%20context-mixer%20%281.9M%0Aparameters%29%20for%20cross-chunk%20attention%2C%20%282%29%20a%20two-level%20latent%20hyper-prior%20for%0Adocument-level%20consistency%2C%20%283%29%20specialized%20handling%20of%20orthographic%20artifacts%0A%28e.g.%20Persian%20ZWNJ%29%2C%20and%20%284%29%20curriculum-based%20training%20with%20staged%20sequence%0Alengths.%20On%20a%201.4B-token%20Persian%20corpus%2C%20H-NET%2B%2B%20achieves%20state-of-the-art%0Aresults%3A%200.159%20BPB%20reduction%20versus%20BPE-based%20GPT-2-fa%20%2812%25%20better%0Acompression%29%2C%205.4pp%20gain%20on%20ParsGLUE%2C%2053%25%20improved%20robustness%20to%20ZWNJ%0Acorruption%2C%20and%2073.8%25%20F1%20on%20gold%20morphological%20boundaries.%20Our%20learned%20chunks%0Aalign%20with%20Persian%20morphology%20without%20explicit%20supervision%2C%20demonstrating%20that%0Ahierarchical%20dynamic%20chunking%20provides%20an%20effective%20tokenizer-free%20solution%20for%0AMRLs%20while%20maintaining%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05628v1&entry.124074799=Read"},
{"title": "MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction\n  from Short Monocular Video Clips", "author": "Shibo Wang and Haonan He and Maria Parelli and Christoph Gebhardt and Zicong Fan and Jie Song", "abstract": "  Most RGB-based hand-object reconstruction methods rely on object templates,\nwhile template-free methods typically assume full object visibility. This\nassumption often breaks in real-world settings, where fixed camera viewpoints\nand static grips leave parts of the object unobserved, resulting in implausible\nreconstructions. To overcome this, we present MagicHOI, a method for\nreconstructing hands and objects from short monocular interaction videos, even\nunder limited viewpoint variation. Our key insight is that, despite the\nscarcity of paired 3D hand-object data, large-scale novel view synthesis\ndiffusion models offer rich object supervision. This supervision serves as a\nprior to regularize unseen object regions during hand interactions. Leveraging\nthis insight, we integrate a novel view synthesis model into our hand-object\nreconstruction framework. We further align hand to object by incorporating\nvisible contact constraints. Our results demonstrate that MagicHOI\nsignificantly outperforms existing state-of-the-art hand-object reconstruction\nmethods. We also show that novel view synthesis diffusion priors effectively\nregularize unseen object regions, enhancing 3D hand-object reconstruction.\n", "link": "http://arxiv.org/abs/2508.05506v1", "date": "2025-08-07", "relevancy": 2.3775, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6208}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5769}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicHOI%3A%20Leveraging%203D%20Priors%20for%20Accurate%20Hand-object%20Reconstruction%0A%20%20from%20Short%20Monocular%20Video%20Clips&body=Title%3A%20MagicHOI%3A%20Leveraging%203D%20Priors%20for%20Accurate%20Hand-object%20Reconstruction%0A%20%20from%20Short%20Monocular%20Video%20Clips%0AAuthor%3A%20Shibo%20Wang%20and%20Haonan%20He%20and%20Maria%20Parelli%20and%20Christoph%20Gebhardt%20and%20Zicong%20Fan%20and%20Jie%20Song%0AAbstract%3A%20%20%20Most%20RGB-based%20hand-object%20reconstruction%20methods%20rely%20on%20object%20templates%2C%0Awhile%20template-free%20methods%20typically%20assume%20full%20object%20visibility.%20This%0Aassumption%20often%20breaks%20in%20real-world%20settings%2C%20where%20fixed%20camera%20viewpoints%0Aand%20static%20grips%20leave%20parts%20of%20the%20object%20unobserved%2C%20resulting%20in%20implausible%0Areconstructions.%20To%20overcome%20this%2C%20we%20present%20MagicHOI%2C%20a%20method%20for%0Areconstructing%20hands%20and%20objects%20from%20short%20monocular%20interaction%20videos%2C%20even%0Aunder%20limited%20viewpoint%20variation.%20Our%20key%20insight%20is%20that%2C%20despite%20the%0Ascarcity%20of%20paired%203D%20hand-object%20data%2C%20large-scale%20novel%20view%20synthesis%0Adiffusion%20models%20offer%20rich%20object%20supervision.%20This%20supervision%20serves%20as%20a%0Aprior%20to%20regularize%20unseen%20object%20regions%20during%20hand%20interactions.%20Leveraging%0Athis%20insight%2C%20we%20integrate%20a%20novel%20view%20synthesis%20model%20into%20our%20hand-object%0Areconstruction%20framework.%20We%20further%20align%20hand%20to%20object%20by%20incorporating%0Avisible%20contact%20constraints.%20Our%20results%20demonstrate%20that%20MagicHOI%0Asignificantly%20outperforms%20existing%20state-of-the-art%20hand-object%20reconstruction%0Amethods.%20We%20also%20show%20that%20novel%20view%20synthesis%20diffusion%20priors%20effectively%0Aregularize%20unseen%20object%20regions%2C%20enhancing%203D%20hand-object%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicHOI%253A%2520Leveraging%25203D%2520Priors%2520for%2520Accurate%2520Hand-object%2520Reconstruction%250A%2520%2520from%2520Short%2520Monocular%2520Video%2520Clips%26entry.906535625%3DShibo%2520Wang%2520and%2520Haonan%2520He%2520and%2520Maria%2520Parelli%2520and%2520Christoph%2520Gebhardt%2520and%2520Zicong%2520Fan%2520and%2520Jie%2520Song%26entry.1292438233%3D%2520%2520Most%2520RGB-based%2520hand-object%2520reconstruction%2520methods%2520rely%2520on%2520object%2520templates%252C%250Awhile%2520template-free%2520methods%2520typically%2520assume%2520full%2520object%2520visibility.%2520This%250Aassumption%2520often%2520breaks%2520in%2520real-world%2520settings%252C%2520where%2520fixed%2520camera%2520viewpoints%250Aand%2520static%2520grips%2520leave%2520parts%2520of%2520the%2520object%2520unobserved%252C%2520resulting%2520in%2520implausible%250Areconstructions.%2520To%2520overcome%2520this%252C%2520we%2520present%2520MagicHOI%252C%2520a%2520method%2520for%250Areconstructing%2520hands%2520and%2520objects%2520from%2520short%2520monocular%2520interaction%2520videos%252C%2520even%250Aunder%2520limited%2520viewpoint%2520variation.%2520Our%2520key%2520insight%2520is%2520that%252C%2520despite%2520the%250Ascarcity%2520of%2520paired%25203D%2520hand-object%2520data%252C%2520large-scale%2520novel%2520view%2520synthesis%250Adiffusion%2520models%2520offer%2520rich%2520object%2520supervision.%2520This%2520supervision%2520serves%2520as%2520a%250Aprior%2520to%2520regularize%2520unseen%2520object%2520regions%2520during%2520hand%2520interactions.%2520Leveraging%250Athis%2520insight%252C%2520we%2520integrate%2520a%2520novel%2520view%2520synthesis%2520model%2520into%2520our%2520hand-object%250Areconstruction%2520framework.%2520We%2520further%2520align%2520hand%2520to%2520object%2520by%2520incorporating%250Avisible%2520contact%2520constraints.%2520Our%2520results%2520demonstrate%2520that%2520MagicHOI%250Asignificantly%2520outperforms%2520existing%2520state-of-the-art%2520hand-object%2520reconstruction%250Amethods.%2520We%2520also%2520show%2520that%2520novel%2520view%2520synthesis%2520diffusion%2520priors%2520effectively%250Aregularize%2520unseen%2520object%2520regions%252C%2520enhancing%25203D%2520hand-object%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicHOI%3A%20Leveraging%203D%20Priors%20for%20Accurate%20Hand-object%20Reconstruction%0A%20%20from%20Short%20Monocular%20Video%20Clips&entry.906535625=Shibo%20Wang%20and%20Haonan%20He%20and%20Maria%20Parelli%20and%20Christoph%20Gebhardt%20and%20Zicong%20Fan%20and%20Jie%20Song&entry.1292438233=%20%20Most%20RGB-based%20hand-object%20reconstruction%20methods%20rely%20on%20object%20templates%2C%0Awhile%20template-free%20methods%20typically%20assume%20full%20object%20visibility.%20This%0Aassumption%20often%20breaks%20in%20real-world%20settings%2C%20where%20fixed%20camera%20viewpoints%0Aand%20static%20grips%20leave%20parts%20of%20the%20object%20unobserved%2C%20resulting%20in%20implausible%0Areconstructions.%20To%20overcome%20this%2C%20we%20present%20MagicHOI%2C%20a%20method%20for%0Areconstructing%20hands%20and%20objects%20from%20short%20monocular%20interaction%20videos%2C%20even%0Aunder%20limited%20viewpoint%20variation.%20Our%20key%20insight%20is%20that%2C%20despite%20the%0Ascarcity%20of%20paired%203D%20hand-object%20data%2C%20large-scale%20novel%20view%20synthesis%0Adiffusion%20models%20offer%20rich%20object%20supervision.%20This%20supervision%20serves%20as%20a%0Aprior%20to%20regularize%20unseen%20object%20regions%20during%20hand%20interactions.%20Leveraging%0Athis%20insight%2C%20we%20integrate%20a%20novel%20view%20synthesis%20model%20into%20our%20hand-object%0Areconstruction%20framework.%20We%20further%20align%20hand%20to%20object%20by%20incorporating%0Avisible%20contact%20constraints.%20Our%20results%20demonstrate%20that%20MagicHOI%0Asignificantly%20outperforms%20existing%20state-of-the-art%20hand-object%20reconstruction%0Amethods.%20We%20also%20show%20that%20novel%20view%20synthesis%20diffusion%20priors%20effectively%0Aregularize%20unseen%20object%20regions%2C%20enhancing%203D%20hand-object%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05506v1&entry.124074799=Read"},
{"title": "Embedding Alignment in Code Generation for Audio", "author": "Sam Kouteili and Hiren Madhu and George Typaldos and Mark Santolucito", "abstract": "  LLM-powered code generation has the potential to revolutionize creative\ncoding endeavors, such as live-coding, by enabling users to focus on structural\nmotifs over syntactic details. In such domains, when prompting an LLM, users\nmay benefit from considering multiple varied code candidates to better realize\ntheir musical intentions. Code generation models, however, struggle to present\nunique and diverse code candidates, with no direct insight into the code's\naudio output. To better establish a relationship between code candidates and\nproduced audio, we investigate the topology of the mapping between code and\naudio embedding spaces. We find that code and audio embeddings do not exhibit a\nsimple linear relationship, but supplement this with a constructed predictive\nmodel that shows an embedding alignment map could be learned. Supplementing the\naim for musically diverse output, we present a model that given code predicts\noutput audio embedding, constructing a code-audio embedding alignment map.\n", "link": "http://arxiv.org/abs/2508.05473v1", "date": "2025-08-07", "relevancy": 2.3775, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding%20Alignment%20in%20Code%20Generation%20for%20Audio&body=Title%3A%20Embedding%20Alignment%20in%20Code%20Generation%20for%20Audio%0AAuthor%3A%20Sam%20Kouteili%20and%20Hiren%20Madhu%20and%20George%20Typaldos%20and%20Mark%20Santolucito%0AAbstract%3A%20%20%20LLM-powered%20code%20generation%20has%20the%20potential%20to%20revolutionize%20creative%0Acoding%20endeavors%2C%20such%20as%20live-coding%2C%20by%20enabling%20users%20to%20focus%20on%20structural%0Amotifs%20over%20syntactic%20details.%20In%20such%20domains%2C%20when%20prompting%20an%20LLM%2C%20users%0Amay%20benefit%20from%20considering%20multiple%20varied%20code%20candidates%20to%20better%20realize%0Atheir%20musical%20intentions.%20Code%20generation%20models%2C%20however%2C%20struggle%20to%20present%0Aunique%20and%20diverse%20code%20candidates%2C%20with%20no%20direct%20insight%20into%20the%20code%27s%0Aaudio%20output.%20To%20better%20establish%20a%20relationship%20between%20code%20candidates%20and%0Aproduced%20audio%2C%20we%20investigate%20the%20topology%20of%20the%20mapping%20between%20code%20and%0Aaudio%20embedding%20spaces.%20We%20find%20that%20code%20and%20audio%20embeddings%20do%20not%20exhibit%20a%0Asimple%20linear%20relationship%2C%20but%20supplement%20this%20with%20a%20constructed%20predictive%0Amodel%20that%20shows%20an%20embedding%20alignment%20map%20could%20be%20learned.%20Supplementing%20the%0Aaim%20for%20musically%20diverse%20output%2C%20we%20present%20a%20model%20that%20given%20code%20predicts%0Aoutput%20audio%20embedding%2C%20constructing%20a%20code-audio%20embedding%20alignment%20map.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding%2520Alignment%2520in%2520Code%2520Generation%2520for%2520Audio%26entry.906535625%3DSam%2520Kouteili%2520and%2520Hiren%2520Madhu%2520and%2520George%2520Typaldos%2520and%2520Mark%2520Santolucito%26entry.1292438233%3D%2520%2520LLM-powered%2520code%2520generation%2520has%2520the%2520potential%2520to%2520revolutionize%2520creative%250Acoding%2520endeavors%252C%2520such%2520as%2520live-coding%252C%2520by%2520enabling%2520users%2520to%2520focus%2520on%2520structural%250Amotifs%2520over%2520syntactic%2520details.%2520In%2520such%2520domains%252C%2520when%2520prompting%2520an%2520LLM%252C%2520users%250Amay%2520benefit%2520from%2520considering%2520multiple%2520varied%2520code%2520candidates%2520to%2520better%2520realize%250Atheir%2520musical%2520intentions.%2520Code%2520generation%2520models%252C%2520however%252C%2520struggle%2520to%2520present%250Aunique%2520and%2520diverse%2520code%2520candidates%252C%2520with%2520no%2520direct%2520insight%2520into%2520the%2520code%2527s%250Aaudio%2520output.%2520To%2520better%2520establish%2520a%2520relationship%2520between%2520code%2520candidates%2520and%250Aproduced%2520audio%252C%2520we%2520investigate%2520the%2520topology%2520of%2520the%2520mapping%2520between%2520code%2520and%250Aaudio%2520embedding%2520spaces.%2520We%2520find%2520that%2520code%2520and%2520audio%2520embeddings%2520do%2520not%2520exhibit%2520a%250Asimple%2520linear%2520relationship%252C%2520but%2520supplement%2520this%2520with%2520a%2520constructed%2520predictive%250Amodel%2520that%2520shows%2520an%2520embedding%2520alignment%2520map%2520could%2520be%2520learned.%2520Supplementing%2520the%250Aaim%2520for%2520musically%2520diverse%2520output%252C%2520we%2520present%2520a%2520model%2520that%2520given%2520code%2520predicts%250Aoutput%2520audio%2520embedding%252C%2520constructing%2520a%2520code-audio%2520embedding%2520alignment%2520map.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20Alignment%20in%20Code%20Generation%20for%20Audio&entry.906535625=Sam%20Kouteili%20and%20Hiren%20Madhu%20and%20George%20Typaldos%20and%20Mark%20Santolucito&entry.1292438233=%20%20LLM-powered%20code%20generation%20has%20the%20potential%20to%20revolutionize%20creative%0Acoding%20endeavors%2C%20such%20as%20live-coding%2C%20by%20enabling%20users%20to%20focus%20on%20structural%0Amotifs%20over%20syntactic%20details.%20In%20such%20domains%2C%20when%20prompting%20an%20LLM%2C%20users%0Amay%20benefit%20from%20considering%20multiple%20varied%20code%20candidates%20to%20better%20realize%0Atheir%20musical%20intentions.%20Code%20generation%20models%2C%20however%2C%20struggle%20to%20present%0Aunique%20and%20diverse%20code%20candidates%2C%20with%20no%20direct%20insight%20into%20the%20code%27s%0Aaudio%20output.%20To%20better%20establish%20a%20relationship%20between%20code%20candidates%20and%0Aproduced%20audio%2C%20we%20investigate%20the%20topology%20of%20the%20mapping%20between%20code%20and%0Aaudio%20embedding%20spaces.%20We%20find%20that%20code%20and%20audio%20embeddings%20do%20not%20exhibit%20a%0Asimple%20linear%20relationship%2C%20but%20supplement%20this%20with%20a%20constructed%20predictive%0Amodel%20that%20shows%20an%20embedding%20alignment%20map%20could%20be%20learned.%20Supplementing%20the%0Aaim%20for%20musically%20diverse%20output%2C%20we%20present%20a%20model%20that%20given%20code%20predicts%0Aoutput%20audio%20embedding%2C%20constructing%20a%20code-audio%20embedding%20alignment%20map.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05473v1&entry.124074799=Read"},
{"title": "Optimal Brain Connection: Towards Efficient Structural Pruning", "author": "Shaowu Chen and Wei Ma and Binhua Huang and Qingyuan Wang and Guoxin Wang and Weize Sun and Lei Huang and Deepu John", "abstract": "  Structural pruning has been widely studied for its effectiveness in\ncompressing neural networks. However, existing methods often neglect the\ninterconnections among parameters. To address this limitation, this paper\nproposes a structural pruning framework termed Optimal Brain Connection. First,\nwe introduce the Jacobian Criterion, a first-order metric for evaluating the\nsaliency of structural parameters. Unlike existing first-order methods that\nassess parameters in isolation, our criterion explicitly captures both\nintra-component interactions and inter-layer dependencies. Second, we propose\nthe Equivalent Pruning mechanism, which utilizes autoencoders to retain the\ncontributions of all original connection--including pruned ones--during\nfine-tuning. Experimental results demonstrate that the Jacobian Criterion\noutperforms several popular metrics in preserving model performance, while the\nEquivalent Pruning mechanism effectively mitigates performance degradation\nafter fine-tuning. Code: https://github.com/ShaowuChen/Optimal_Brain_Connection\n", "link": "http://arxiv.org/abs/2508.05521v1", "date": "2025-08-07", "relevancy": 2.3769, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4899}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Brain%20Connection%3A%20Towards%20Efficient%20Structural%20Pruning&body=Title%3A%20Optimal%20Brain%20Connection%3A%20Towards%20Efficient%20Structural%20Pruning%0AAuthor%3A%20Shaowu%20Chen%20and%20Wei%20Ma%20and%20Binhua%20Huang%20and%20Qingyuan%20Wang%20and%20Guoxin%20Wang%20and%20Weize%20Sun%20and%20Lei%20Huang%20and%20Deepu%20John%0AAbstract%3A%20%20%20Structural%20pruning%20has%20been%20widely%20studied%20for%20its%20effectiveness%20in%0Acompressing%20neural%20networks.%20However%2C%20existing%20methods%20often%20neglect%20the%0Ainterconnections%20among%20parameters.%20To%20address%20this%20limitation%2C%20this%20paper%0Aproposes%20a%20structural%20pruning%20framework%20termed%20Optimal%20Brain%20Connection.%20First%2C%0Awe%20introduce%20the%20Jacobian%20Criterion%2C%20a%20first-order%20metric%20for%20evaluating%20the%0Asaliency%20of%20structural%20parameters.%20Unlike%20existing%20first-order%20methods%20that%0Aassess%20parameters%20in%20isolation%2C%20our%20criterion%20explicitly%20captures%20both%0Aintra-component%20interactions%20and%20inter-layer%20dependencies.%20Second%2C%20we%20propose%0Athe%20Equivalent%20Pruning%20mechanism%2C%20which%20utilizes%20autoencoders%20to%20retain%20the%0Acontributions%20of%20all%20original%20connection--including%20pruned%20ones--during%0Afine-tuning.%20Experimental%20results%20demonstrate%20that%20the%20Jacobian%20Criterion%0Aoutperforms%20several%20popular%20metrics%20in%20preserving%20model%20performance%2C%20while%20the%0AEquivalent%20Pruning%20mechanism%20effectively%20mitigates%20performance%20degradation%0Aafter%20fine-tuning.%20Code%3A%20https%3A//github.com/ShaowuChen/Optimal_Brain_Connection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Brain%2520Connection%253A%2520Towards%2520Efficient%2520Structural%2520Pruning%26entry.906535625%3DShaowu%2520Chen%2520and%2520Wei%2520Ma%2520and%2520Binhua%2520Huang%2520and%2520Qingyuan%2520Wang%2520and%2520Guoxin%2520Wang%2520and%2520Weize%2520Sun%2520and%2520Lei%2520Huang%2520and%2520Deepu%2520John%26entry.1292438233%3D%2520%2520Structural%2520pruning%2520has%2520been%2520widely%2520studied%2520for%2520its%2520effectiveness%2520in%250Acompressing%2520neural%2520networks.%2520However%252C%2520existing%2520methods%2520often%2520neglect%2520the%250Ainterconnections%2520among%2520parameters.%2520To%2520address%2520this%2520limitation%252C%2520this%2520paper%250Aproposes%2520a%2520structural%2520pruning%2520framework%2520termed%2520Optimal%2520Brain%2520Connection.%2520First%252C%250Awe%2520introduce%2520the%2520Jacobian%2520Criterion%252C%2520a%2520first-order%2520metric%2520for%2520evaluating%2520the%250Asaliency%2520of%2520structural%2520parameters.%2520Unlike%2520existing%2520first-order%2520methods%2520that%250Aassess%2520parameters%2520in%2520isolation%252C%2520our%2520criterion%2520explicitly%2520captures%2520both%250Aintra-component%2520interactions%2520and%2520inter-layer%2520dependencies.%2520Second%252C%2520we%2520propose%250Athe%2520Equivalent%2520Pruning%2520mechanism%252C%2520which%2520utilizes%2520autoencoders%2520to%2520retain%2520the%250Acontributions%2520of%2520all%2520original%2520connection--including%2520pruned%2520ones--during%250Afine-tuning.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520Jacobian%2520Criterion%250Aoutperforms%2520several%2520popular%2520metrics%2520in%2520preserving%2520model%2520performance%252C%2520while%2520the%250AEquivalent%2520Pruning%2520mechanism%2520effectively%2520mitigates%2520performance%2520degradation%250Aafter%2520fine-tuning.%2520Code%253A%2520https%253A//github.com/ShaowuChen/Optimal_Brain_Connection%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Brain%20Connection%3A%20Towards%20Efficient%20Structural%20Pruning&entry.906535625=Shaowu%20Chen%20and%20Wei%20Ma%20and%20Binhua%20Huang%20and%20Qingyuan%20Wang%20and%20Guoxin%20Wang%20and%20Weize%20Sun%20and%20Lei%20Huang%20and%20Deepu%20John&entry.1292438233=%20%20Structural%20pruning%20has%20been%20widely%20studied%20for%20its%20effectiveness%20in%0Acompressing%20neural%20networks.%20However%2C%20existing%20methods%20often%20neglect%20the%0Ainterconnections%20among%20parameters.%20To%20address%20this%20limitation%2C%20this%20paper%0Aproposes%20a%20structural%20pruning%20framework%20termed%20Optimal%20Brain%20Connection.%20First%2C%0Awe%20introduce%20the%20Jacobian%20Criterion%2C%20a%20first-order%20metric%20for%20evaluating%20the%0Asaliency%20of%20structural%20parameters.%20Unlike%20existing%20first-order%20methods%20that%0Aassess%20parameters%20in%20isolation%2C%20our%20criterion%20explicitly%20captures%20both%0Aintra-component%20interactions%20and%20inter-layer%20dependencies.%20Second%2C%20we%20propose%0Athe%20Equivalent%20Pruning%20mechanism%2C%20which%20utilizes%20autoencoders%20to%20retain%20the%0Acontributions%20of%20all%20original%20connection--including%20pruned%20ones--during%0Afine-tuning.%20Experimental%20results%20demonstrate%20that%20the%20Jacobian%20Criterion%0Aoutperforms%20several%20popular%20metrics%20in%20preserving%20model%20performance%2C%20while%20the%0AEquivalent%20Pruning%20mechanism%20effectively%20mitigates%20performance%20degradation%0Aafter%20fine-tuning.%20Code%3A%20https%3A//github.com/ShaowuChen/Optimal_Brain_Connection%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05521v1&entry.124074799=Read"},
{"title": "SteerPose: Simultaneous Extrinsic Camera Calibration and Matching from\n  Articulation", "author": "Sang-Eun Lee and Ko Nishino and Shohei Nobuhara", "abstract": "  Can freely moving humans or animals themselves serve as calibration targets\nfor multi-camera systems while simultaneously estimating their correspondences\nacross views? We humans can solve this problem by mentally rotating the\nobserved 2D poses and aligning them with those in the target views. Inspired by\nthis cognitive ability, we propose SteerPose, a neural network that performs\nthis rotation of 2D poses into another view. By integrating differentiable\nmatching, SteerPose simultaneously performs extrinsic camera calibration and\ncorrespondence search within a single unified framework. We also introduce a\nnovel geometric consistency loss that explicitly ensures that the estimated\nrotation and correspondences result in a valid translation estimation.\nExperimental results on diverse in-the-wild datasets of humans and animals\nvalidate the effectiveness and robustness of the proposed method. Furthermore,\nwe demonstrate that our method can reconstruct the 3D poses of novel animals in\nmulti-camera setups by leveraging off-the-shelf 2D pose estimators and our\nclass-agnostic model.\n", "link": "http://arxiv.org/abs/2506.01691v2", "date": "2025-08-07", "relevancy": 2.3745, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6003}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.595}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SteerPose%3A%20Simultaneous%20Extrinsic%20Camera%20Calibration%20and%20Matching%20from%0A%20%20Articulation&body=Title%3A%20SteerPose%3A%20Simultaneous%20Extrinsic%20Camera%20Calibration%20and%20Matching%20from%0A%20%20Articulation%0AAuthor%3A%20Sang-Eun%20Lee%20and%20Ko%20Nishino%20and%20Shohei%20Nobuhara%0AAbstract%3A%20%20%20Can%20freely%20moving%20humans%20or%20animals%20themselves%20serve%20as%20calibration%20targets%0Afor%20multi-camera%20systems%20while%20simultaneously%20estimating%20their%20correspondences%0Aacross%20views%3F%20We%20humans%20can%20solve%20this%20problem%20by%20mentally%20rotating%20the%0Aobserved%202D%20poses%20and%20aligning%20them%20with%20those%20in%20the%20target%20views.%20Inspired%20by%0Athis%20cognitive%20ability%2C%20we%20propose%20SteerPose%2C%20a%20neural%20network%20that%20performs%0Athis%20rotation%20of%202D%20poses%20into%20another%20view.%20By%20integrating%20differentiable%0Amatching%2C%20SteerPose%20simultaneously%20performs%20extrinsic%20camera%20calibration%20and%0Acorrespondence%20search%20within%20a%20single%20unified%20framework.%20We%20also%20introduce%20a%0Anovel%20geometric%20consistency%20loss%20that%20explicitly%20ensures%20that%20the%20estimated%0Arotation%20and%20correspondences%20result%20in%20a%20valid%20translation%20estimation.%0AExperimental%20results%20on%20diverse%20in-the-wild%20datasets%20of%20humans%20and%20animals%0Avalidate%20the%20effectiveness%20and%20robustness%20of%20the%20proposed%20method.%20Furthermore%2C%0Awe%20demonstrate%20that%20our%20method%20can%20reconstruct%20the%203D%20poses%20of%20novel%20animals%20in%0Amulti-camera%20setups%20by%20leveraging%20off-the-shelf%202D%20pose%20estimators%20and%20our%0Aclass-agnostic%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01691v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteerPose%253A%2520Simultaneous%2520Extrinsic%2520Camera%2520Calibration%2520and%2520Matching%2520from%250A%2520%2520Articulation%26entry.906535625%3DSang-Eun%2520Lee%2520and%2520Ko%2520Nishino%2520and%2520Shohei%2520Nobuhara%26entry.1292438233%3D%2520%2520Can%2520freely%2520moving%2520humans%2520or%2520animals%2520themselves%2520serve%2520as%2520calibration%2520targets%250Afor%2520multi-camera%2520systems%2520while%2520simultaneously%2520estimating%2520their%2520correspondences%250Aacross%2520views%253F%2520We%2520humans%2520can%2520solve%2520this%2520problem%2520by%2520mentally%2520rotating%2520the%250Aobserved%25202D%2520poses%2520and%2520aligning%2520them%2520with%2520those%2520in%2520the%2520target%2520views.%2520Inspired%2520by%250Athis%2520cognitive%2520ability%252C%2520we%2520propose%2520SteerPose%252C%2520a%2520neural%2520network%2520that%2520performs%250Athis%2520rotation%2520of%25202D%2520poses%2520into%2520another%2520view.%2520By%2520integrating%2520differentiable%250Amatching%252C%2520SteerPose%2520simultaneously%2520performs%2520extrinsic%2520camera%2520calibration%2520and%250Acorrespondence%2520search%2520within%2520a%2520single%2520unified%2520framework.%2520We%2520also%2520introduce%2520a%250Anovel%2520geometric%2520consistency%2520loss%2520that%2520explicitly%2520ensures%2520that%2520the%2520estimated%250Arotation%2520and%2520correspondences%2520result%2520in%2520a%2520valid%2520translation%2520estimation.%250AExperimental%2520results%2520on%2520diverse%2520in-the-wild%2520datasets%2520of%2520humans%2520and%2520animals%250Avalidate%2520the%2520effectiveness%2520and%2520robustness%2520of%2520the%2520proposed%2520method.%2520Furthermore%252C%250Awe%2520demonstrate%2520that%2520our%2520method%2520can%2520reconstruct%2520the%25203D%2520poses%2520of%2520novel%2520animals%2520in%250Amulti-camera%2520setups%2520by%2520leveraging%2520off-the-shelf%25202D%2520pose%2520estimators%2520and%2520our%250Aclass-agnostic%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01691v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SteerPose%3A%20Simultaneous%20Extrinsic%20Camera%20Calibration%20and%20Matching%20from%0A%20%20Articulation&entry.906535625=Sang-Eun%20Lee%20and%20Ko%20Nishino%20and%20Shohei%20Nobuhara&entry.1292438233=%20%20Can%20freely%20moving%20humans%20or%20animals%20themselves%20serve%20as%20calibration%20targets%0Afor%20multi-camera%20systems%20while%20simultaneously%20estimating%20their%20correspondences%0Aacross%20views%3F%20We%20humans%20can%20solve%20this%20problem%20by%20mentally%20rotating%20the%0Aobserved%202D%20poses%20and%20aligning%20them%20with%20those%20in%20the%20target%20views.%20Inspired%20by%0Athis%20cognitive%20ability%2C%20we%20propose%20SteerPose%2C%20a%20neural%20network%20that%20performs%0Athis%20rotation%20of%202D%20poses%20into%20another%20view.%20By%20integrating%20differentiable%0Amatching%2C%20SteerPose%20simultaneously%20performs%20extrinsic%20camera%20calibration%20and%0Acorrespondence%20search%20within%20a%20single%20unified%20framework.%20We%20also%20introduce%20a%0Anovel%20geometric%20consistency%20loss%20that%20explicitly%20ensures%20that%20the%20estimated%0Arotation%20and%20correspondences%20result%20in%20a%20valid%20translation%20estimation.%0AExperimental%20results%20on%20diverse%20in-the-wild%20datasets%20of%20humans%20and%20animals%0Avalidate%20the%20effectiveness%20and%20robustness%20of%20the%20proposed%20method.%20Furthermore%2C%0Awe%20demonstrate%20that%20our%20method%20can%20reconstruct%20the%203D%20poses%20of%20novel%20animals%20in%0Amulti-camera%20setups%20by%20leveraging%20off-the-shelf%202D%20pose%20estimators%20and%20our%0Aclass-agnostic%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01691v2&entry.124074799=Read"},
{"title": "WeTok: Powerful Discrete Tokenization for High-Fidelity Visual\n  Reconstruction", "author": "Shaobin Zhuang and Yiwei Guo and Canmiao Fu and Zhipeng Huang and Zeyue Tian and Ying Zhang and Chen Li and Yali Wang", "abstract": "  Visual tokenizer is a critical component for vision generation. However, the\nexisting tokenizers often face unsatisfactory trade-off between compression\nratios and reconstruction fidelity. To fill this gap, we introduce a powerful\nand concise WeTok tokenizer, which surpasses the previous leading tokenizers\nvia two core innovations. (1) Group-wise lookup-free Quantization (GQ). We\npartition the latent features into groups, and perform lookup-free quantization\nfor each group. As a result, GQ can efficiently overcome memory and computation\nlimitations of prior tokenizers, while achieving a reconstruction breakthrough\nwith more scalable codebooks. (2) Generative Decoding (GD). Different from\nprior tokenizers, we introduce a generative decoder with a prior of extra noise\nvariable. In this case, GD can probabilistically model the distribution of\nvisual data conditioned on discrete tokens, allowing WeTok to reconstruct\nvisual details, especially at high compression ratios. Extensive experiments on\nmainstream benchmarks show superior performance of our WeTok. On the ImageNet\n50k validation set, WeTok achieves a record-low zero-shot rFID (WeTok: 0.12 vs.\nFLUX-VAE: 0.18 vs. SD-VAE 3.5: 0.19). Furthermore, our highest compression\nmodel achieves a zero-shot rFID of 3.49 with a compression ratio of 768,\noutperforming Cosmos (384) 4.57 which has only 50% compression rate of ours.\nCode and models are available: https://github.com/zhuangshaobin/WeTok.\n", "link": "http://arxiv.org/abs/2508.05599v1", "date": "2025-08-07", "relevancy": 2.3714, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6418}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5606}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeTok%3A%20Powerful%20Discrete%20Tokenization%20for%20High-Fidelity%20Visual%0A%20%20Reconstruction&body=Title%3A%20WeTok%3A%20Powerful%20Discrete%20Tokenization%20for%20High-Fidelity%20Visual%0A%20%20Reconstruction%0AAuthor%3A%20Shaobin%20Zhuang%20and%20Yiwei%20Guo%20and%20Canmiao%20Fu%20and%20Zhipeng%20Huang%20and%20Zeyue%20Tian%20and%20Ying%20Zhang%20and%20Chen%20Li%20and%20Yali%20Wang%0AAbstract%3A%20%20%20Visual%20tokenizer%20is%20a%20critical%20component%20for%20vision%20generation.%20However%2C%20the%0Aexisting%20tokenizers%20often%20face%20unsatisfactory%20trade-off%20between%20compression%0Aratios%20and%20reconstruction%20fidelity.%20To%20fill%20this%20gap%2C%20we%20introduce%20a%20powerful%0Aand%20concise%20WeTok%20tokenizer%2C%20which%20surpasses%20the%20previous%20leading%20tokenizers%0Avia%20two%20core%20innovations.%20%281%29%20Group-wise%20lookup-free%20Quantization%20%28GQ%29.%20We%0Apartition%20the%20latent%20features%20into%20groups%2C%20and%20perform%20lookup-free%20quantization%0Afor%20each%20group.%20As%20a%20result%2C%20GQ%20can%20efficiently%20overcome%20memory%20and%20computation%0Alimitations%20of%20prior%20tokenizers%2C%20while%20achieving%20a%20reconstruction%20breakthrough%0Awith%20more%20scalable%20codebooks.%20%282%29%20Generative%20Decoding%20%28GD%29.%20Different%20from%0Aprior%20tokenizers%2C%20we%20introduce%20a%20generative%20decoder%20with%20a%20prior%20of%20extra%20noise%0Avariable.%20In%20this%20case%2C%20GD%20can%20probabilistically%20model%20the%20distribution%20of%0Avisual%20data%20conditioned%20on%20discrete%20tokens%2C%20allowing%20WeTok%20to%20reconstruct%0Avisual%20details%2C%20especially%20at%20high%20compression%20ratios.%20Extensive%20experiments%20on%0Amainstream%20benchmarks%20show%20superior%20performance%20of%20our%20WeTok.%20On%20the%20ImageNet%0A50k%20validation%20set%2C%20WeTok%20achieves%20a%20record-low%20zero-shot%20rFID%20%28WeTok%3A%200.12%20vs.%0AFLUX-VAE%3A%200.18%20vs.%20SD-VAE%203.5%3A%200.19%29.%20Furthermore%2C%20our%20highest%20compression%0Amodel%20achieves%20a%20zero-shot%20rFID%20of%203.49%20with%20a%20compression%20ratio%20of%20768%2C%0Aoutperforming%20Cosmos%20%28384%29%204.57%20which%20has%20only%2050%25%20compression%20rate%20of%20ours.%0ACode%20and%20models%20are%20available%3A%20https%3A//github.com/zhuangshaobin/WeTok.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeTok%253A%2520Powerful%2520Discrete%2520Tokenization%2520for%2520High-Fidelity%2520Visual%250A%2520%2520Reconstruction%26entry.906535625%3DShaobin%2520Zhuang%2520and%2520Yiwei%2520Guo%2520and%2520Canmiao%2520Fu%2520and%2520Zhipeng%2520Huang%2520and%2520Zeyue%2520Tian%2520and%2520Ying%2520Zhang%2520and%2520Chen%2520Li%2520and%2520Yali%2520Wang%26entry.1292438233%3D%2520%2520Visual%2520tokenizer%2520is%2520a%2520critical%2520component%2520for%2520vision%2520generation.%2520However%252C%2520the%250Aexisting%2520tokenizers%2520often%2520face%2520unsatisfactory%2520trade-off%2520between%2520compression%250Aratios%2520and%2520reconstruction%2520fidelity.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520a%2520powerful%250Aand%2520concise%2520WeTok%2520tokenizer%252C%2520which%2520surpasses%2520the%2520previous%2520leading%2520tokenizers%250Avia%2520two%2520core%2520innovations.%2520%25281%2529%2520Group-wise%2520lookup-free%2520Quantization%2520%2528GQ%2529.%2520We%250Apartition%2520the%2520latent%2520features%2520into%2520groups%252C%2520and%2520perform%2520lookup-free%2520quantization%250Afor%2520each%2520group.%2520As%2520a%2520result%252C%2520GQ%2520can%2520efficiently%2520overcome%2520memory%2520and%2520computation%250Alimitations%2520of%2520prior%2520tokenizers%252C%2520while%2520achieving%2520a%2520reconstruction%2520breakthrough%250Awith%2520more%2520scalable%2520codebooks.%2520%25282%2529%2520Generative%2520Decoding%2520%2528GD%2529.%2520Different%2520from%250Aprior%2520tokenizers%252C%2520we%2520introduce%2520a%2520generative%2520decoder%2520with%2520a%2520prior%2520of%2520extra%2520noise%250Avariable.%2520In%2520this%2520case%252C%2520GD%2520can%2520probabilistically%2520model%2520the%2520distribution%2520of%250Avisual%2520data%2520conditioned%2520on%2520discrete%2520tokens%252C%2520allowing%2520WeTok%2520to%2520reconstruct%250Avisual%2520details%252C%2520especially%2520at%2520high%2520compression%2520ratios.%2520Extensive%2520experiments%2520on%250Amainstream%2520benchmarks%2520show%2520superior%2520performance%2520of%2520our%2520WeTok.%2520On%2520the%2520ImageNet%250A50k%2520validation%2520set%252C%2520WeTok%2520achieves%2520a%2520record-low%2520zero-shot%2520rFID%2520%2528WeTok%253A%25200.12%2520vs.%250AFLUX-VAE%253A%25200.18%2520vs.%2520SD-VAE%25203.5%253A%25200.19%2529.%2520Furthermore%252C%2520our%2520highest%2520compression%250Amodel%2520achieves%2520a%2520zero-shot%2520rFID%2520of%25203.49%2520with%2520a%2520compression%2520ratio%2520of%2520768%252C%250Aoutperforming%2520Cosmos%2520%2528384%2529%25204.57%2520which%2520has%2520only%252050%2525%2520compression%2520rate%2520of%2520ours.%250ACode%2520and%2520models%2520are%2520available%253A%2520https%253A//github.com/zhuangshaobin/WeTok.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeTok%3A%20Powerful%20Discrete%20Tokenization%20for%20High-Fidelity%20Visual%0A%20%20Reconstruction&entry.906535625=Shaobin%20Zhuang%20and%20Yiwei%20Guo%20and%20Canmiao%20Fu%20and%20Zhipeng%20Huang%20and%20Zeyue%20Tian%20and%20Ying%20Zhang%20and%20Chen%20Li%20and%20Yali%20Wang&entry.1292438233=%20%20Visual%20tokenizer%20is%20a%20critical%20component%20for%20vision%20generation.%20However%2C%20the%0Aexisting%20tokenizers%20often%20face%20unsatisfactory%20trade-off%20between%20compression%0Aratios%20and%20reconstruction%20fidelity.%20To%20fill%20this%20gap%2C%20we%20introduce%20a%20powerful%0Aand%20concise%20WeTok%20tokenizer%2C%20which%20surpasses%20the%20previous%20leading%20tokenizers%0Avia%20two%20core%20innovations.%20%281%29%20Group-wise%20lookup-free%20Quantization%20%28GQ%29.%20We%0Apartition%20the%20latent%20features%20into%20groups%2C%20and%20perform%20lookup-free%20quantization%0Afor%20each%20group.%20As%20a%20result%2C%20GQ%20can%20efficiently%20overcome%20memory%20and%20computation%0Alimitations%20of%20prior%20tokenizers%2C%20while%20achieving%20a%20reconstruction%20breakthrough%0Awith%20more%20scalable%20codebooks.%20%282%29%20Generative%20Decoding%20%28GD%29.%20Different%20from%0Aprior%20tokenizers%2C%20we%20introduce%20a%20generative%20decoder%20with%20a%20prior%20of%20extra%20noise%0Avariable.%20In%20this%20case%2C%20GD%20can%20probabilistically%20model%20the%20distribution%20of%0Avisual%20data%20conditioned%20on%20discrete%20tokens%2C%20allowing%20WeTok%20to%20reconstruct%0Avisual%20details%2C%20especially%20at%20high%20compression%20ratios.%20Extensive%20experiments%20on%0Amainstream%20benchmarks%20show%20superior%20performance%20of%20our%20WeTok.%20On%20the%20ImageNet%0A50k%20validation%20set%2C%20WeTok%20achieves%20a%20record-low%20zero-shot%20rFID%20%28WeTok%3A%200.12%20vs.%0AFLUX-VAE%3A%200.18%20vs.%20SD-VAE%203.5%3A%200.19%29.%20Furthermore%2C%20our%20highest%20compression%0Amodel%20achieves%20a%20zero-shot%20rFID%20of%203.49%20with%20a%20compression%20ratio%20of%20768%2C%0Aoutperforming%20Cosmos%20%28384%29%204.57%20which%20has%20only%2050%25%20compression%20rate%20of%20ours.%0ACode%20and%20models%20are%20available%3A%20https%3A//github.com/zhuangshaobin/WeTok.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05599v1&entry.124074799=Read"},
{"title": "Building Effective Safety Guardrails in AI Education Tools", "author": "Hannah-Beth Clark and Laura Benton and Emma Searle and Margaux Dowland and Matthew Gregory and Will Gayne and John Roberts", "abstract": "  There has been rapid development in generative AI tools across the education\nsector, which in turn is leading to increased adoption by teachers. However,\nthis raises concerns regarding the safety and age-appropriateness of the\nAI-generated content that is being created for use in classrooms. This paper\nexplores Oak National Academy's approach to addressing these concerns within\nthe development of the UK Government's first publicly available generative AI\ntool - our AI-powered lesson planning assistant (Aila). Aila is intended to\nsupport teachers planning national curriculum-aligned lessons that are\nappropriate for pupils aged 5-16 years. To mitigate safety risks associated\nwith AI-generated content we have implemented four key safety guardrails - (1)\nprompt engineering to ensure AI outputs are generated within pedagogically\nsound and curriculum-aligned parameters, (2) input threat detection to mitigate\nattacks, (3) an Independent Asynchronous Content Moderation Agent (IACMA) to\nassess outputs against predefined safety categories, and (4) taking a\nhuman-in-the-loop approach, to encourage teachers to review generated content\nbefore it is used in the classroom. Through our on-going evaluation of these\nsafety guardrails we have identified several challenges and opportunities to\ntake into account when implementing and testing safety guardrails. This paper\nhighlights ways to build more effective safety guardrails in generative AI\neducation tools including the on-going iteration and refinement of guardrails,\nas well as enabling cross-sector collaboration through sharing both open-source\ncode, datasets and learnings.\n", "link": "http://arxiv.org/abs/2508.05360v1", "date": "2025-08-07", "relevancy": 2.369, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4816}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4721}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Effective%20Safety%20Guardrails%20in%20AI%20Education%20Tools&body=Title%3A%20Building%20Effective%20Safety%20Guardrails%20in%20AI%20Education%20Tools%0AAuthor%3A%20Hannah-Beth%20Clark%20and%20Laura%20Benton%20and%20Emma%20Searle%20and%20Margaux%20Dowland%20and%20Matthew%20Gregory%20and%20Will%20Gayne%20and%20John%20Roberts%0AAbstract%3A%20%20%20There%20has%20been%20rapid%20development%20in%20generative%20AI%20tools%20across%20the%20education%0Asector%2C%20which%20in%20turn%20is%20leading%20to%20increased%20adoption%20by%20teachers.%20However%2C%0Athis%20raises%20concerns%20regarding%20the%20safety%20and%20age-appropriateness%20of%20the%0AAI-generated%20content%20that%20is%20being%20created%20for%20use%20in%20classrooms.%20This%20paper%0Aexplores%20Oak%20National%20Academy%27s%20approach%20to%20addressing%20these%20concerns%20within%0Athe%20development%20of%20the%20UK%20Government%27s%20first%20publicly%20available%20generative%20AI%0Atool%20-%20our%20AI-powered%20lesson%20planning%20assistant%20%28Aila%29.%20Aila%20is%20intended%20to%0Asupport%20teachers%20planning%20national%20curriculum-aligned%20lessons%20that%20are%0Aappropriate%20for%20pupils%20aged%205-16%20years.%20To%20mitigate%20safety%20risks%20associated%0Awith%20AI-generated%20content%20we%20have%20implemented%20four%20key%20safety%20guardrails%20-%20%281%29%0Aprompt%20engineering%20to%20ensure%20AI%20outputs%20are%20generated%20within%20pedagogically%0Asound%20and%20curriculum-aligned%20parameters%2C%20%282%29%20input%20threat%20detection%20to%20mitigate%0Aattacks%2C%20%283%29%20an%20Independent%20Asynchronous%20Content%20Moderation%20Agent%20%28IACMA%29%20to%0Aassess%20outputs%20against%20predefined%20safety%20categories%2C%20and%20%284%29%20taking%20a%0Ahuman-in-the-loop%20approach%2C%20to%20encourage%20teachers%20to%20review%20generated%20content%0Abefore%20it%20is%20used%20in%20the%20classroom.%20Through%20our%20on-going%20evaluation%20of%20these%0Asafety%20guardrails%20we%20have%20identified%20several%20challenges%20and%20opportunities%20to%0Atake%20into%20account%20when%20implementing%20and%20testing%20safety%20guardrails.%20This%20paper%0Ahighlights%20ways%20to%20build%20more%20effective%20safety%20guardrails%20in%20generative%20AI%0Aeducation%20tools%20including%20the%20on-going%20iteration%20and%20refinement%20of%20guardrails%2C%0Aas%20well%20as%20enabling%20cross-sector%20collaboration%20through%20sharing%20both%20open-source%0Acode%2C%20datasets%20and%20learnings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Effective%2520Safety%2520Guardrails%2520in%2520AI%2520Education%2520Tools%26entry.906535625%3DHannah-Beth%2520Clark%2520and%2520Laura%2520Benton%2520and%2520Emma%2520Searle%2520and%2520Margaux%2520Dowland%2520and%2520Matthew%2520Gregory%2520and%2520Will%2520Gayne%2520and%2520John%2520Roberts%26entry.1292438233%3D%2520%2520There%2520has%2520been%2520rapid%2520development%2520in%2520generative%2520AI%2520tools%2520across%2520the%2520education%250Asector%252C%2520which%2520in%2520turn%2520is%2520leading%2520to%2520increased%2520adoption%2520by%2520teachers.%2520However%252C%250Athis%2520raises%2520concerns%2520regarding%2520the%2520safety%2520and%2520age-appropriateness%2520of%2520the%250AAI-generated%2520content%2520that%2520is%2520being%2520created%2520for%2520use%2520in%2520classrooms.%2520This%2520paper%250Aexplores%2520Oak%2520National%2520Academy%2527s%2520approach%2520to%2520addressing%2520these%2520concerns%2520within%250Athe%2520development%2520of%2520the%2520UK%2520Government%2527s%2520first%2520publicly%2520available%2520generative%2520AI%250Atool%2520-%2520our%2520AI-powered%2520lesson%2520planning%2520assistant%2520%2528Aila%2529.%2520Aila%2520is%2520intended%2520to%250Asupport%2520teachers%2520planning%2520national%2520curriculum-aligned%2520lessons%2520that%2520are%250Aappropriate%2520for%2520pupils%2520aged%25205-16%2520years.%2520To%2520mitigate%2520safety%2520risks%2520associated%250Awith%2520AI-generated%2520content%2520we%2520have%2520implemented%2520four%2520key%2520safety%2520guardrails%2520-%2520%25281%2529%250Aprompt%2520engineering%2520to%2520ensure%2520AI%2520outputs%2520are%2520generated%2520within%2520pedagogically%250Asound%2520and%2520curriculum-aligned%2520parameters%252C%2520%25282%2529%2520input%2520threat%2520detection%2520to%2520mitigate%250Aattacks%252C%2520%25283%2529%2520an%2520Independent%2520Asynchronous%2520Content%2520Moderation%2520Agent%2520%2528IACMA%2529%2520to%250Aassess%2520outputs%2520against%2520predefined%2520safety%2520categories%252C%2520and%2520%25284%2529%2520taking%2520a%250Ahuman-in-the-loop%2520approach%252C%2520to%2520encourage%2520teachers%2520to%2520review%2520generated%2520content%250Abefore%2520it%2520is%2520used%2520in%2520the%2520classroom.%2520Through%2520our%2520on-going%2520evaluation%2520of%2520these%250Asafety%2520guardrails%2520we%2520have%2520identified%2520several%2520challenges%2520and%2520opportunities%2520to%250Atake%2520into%2520account%2520when%2520implementing%2520and%2520testing%2520safety%2520guardrails.%2520This%2520paper%250Ahighlights%2520ways%2520to%2520build%2520more%2520effective%2520safety%2520guardrails%2520in%2520generative%2520AI%250Aeducation%2520tools%2520including%2520the%2520on-going%2520iteration%2520and%2520refinement%2520of%2520guardrails%252C%250Aas%2520well%2520as%2520enabling%2520cross-sector%2520collaboration%2520through%2520sharing%2520both%2520open-source%250Acode%252C%2520datasets%2520and%2520learnings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Effective%20Safety%20Guardrails%20in%20AI%20Education%20Tools&entry.906535625=Hannah-Beth%20Clark%20and%20Laura%20Benton%20and%20Emma%20Searle%20and%20Margaux%20Dowland%20and%20Matthew%20Gregory%20and%20Will%20Gayne%20and%20John%20Roberts&entry.1292438233=%20%20There%20has%20been%20rapid%20development%20in%20generative%20AI%20tools%20across%20the%20education%0Asector%2C%20which%20in%20turn%20is%20leading%20to%20increased%20adoption%20by%20teachers.%20However%2C%0Athis%20raises%20concerns%20regarding%20the%20safety%20and%20age-appropriateness%20of%20the%0AAI-generated%20content%20that%20is%20being%20created%20for%20use%20in%20classrooms.%20This%20paper%0Aexplores%20Oak%20National%20Academy%27s%20approach%20to%20addressing%20these%20concerns%20within%0Athe%20development%20of%20the%20UK%20Government%27s%20first%20publicly%20available%20generative%20AI%0Atool%20-%20our%20AI-powered%20lesson%20planning%20assistant%20%28Aila%29.%20Aila%20is%20intended%20to%0Asupport%20teachers%20planning%20national%20curriculum-aligned%20lessons%20that%20are%0Aappropriate%20for%20pupils%20aged%205-16%20years.%20To%20mitigate%20safety%20risks%20associated%0Awith%20AI-generated%20content%20we%20have%20implemented%20four%20key%20safety%20guardrails%20-%20%281%29%0Aprompt%20engineering%20to%20ensure%20AI%20outputs%20are%20generated%20within%20pedagogically%0Asound%20and%20curriculum-aligned%20parameters%2C%20%282%29%20input%20threat%20detection%20to%20mitigate%0Aattacks%2C%20%283%29%20an%20Independent%20Asynchronous%20Content%20Moderation%20Agent%20%28IACMA%29%20to%0Aassess%20outputs%20against%20predefined%20safety%20categories%2C%20and%20%284%29%20taking%20a%0Ahuman-in-the-loop%20approach%2C%20to%20encourage%20teachers%20to%20review%20generated%20content%0Abefore%20it%20is%20used%20in%20the%20classroom.%20Through%20our%20on-going%20evaluation%20of%20these%0Asafety%20guardrails%20we%20have%20identified%20several%20challenges%20and%20opportunities%20to%0Atake%20into%20account%20when%20implementing%20and%20testing%20safety%20guardrails.%20This%20paper%0Ahighlights%20ways%20to%20build%20more%20effective%20safety%20guardrails%20in%20generative%20AI%0Aeducation%20tools%20including%20the%20on-going%20iteration%20and%20refinement%20of%20guardrails%2C%0Aas%20well%20as%20enabling%20cross-sector%20collaboration%20through%20sharing%20both%20open-source%0Acode%2C%20datasets%20and%20learnings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05360v1&entry.124074799=Read"},
{"title": "Cross-View Localization via Redundant Sliced Observations and\n  A-Contrario Validation", "author": "Yongjun Zhang and Mingtao Xiong and Yi Wan and Gui-Song Xia", "abstract": "  Cross-view localization (CVL) matches ground-level images with aerial\nreferences to determine the geo-position of a camera, enabling smart vehicles\nto self-localize offline in GNSS-denied environments. However, most CVL methods\noutput only a single observation, the camera pose, and lack the redundant\nobservations required by surveying principles, making it challenging to assess\nlocalization reliability through the mutual validation of observational data.\nTo tackle this, we introduce Slice-Loc, a two-stage method featuring an\na-contrario reliability validation for CVL. Instead of using the query image as\na single input, Slice-Loc divides it into sub-images and estimates the 3-DoF\npose for each slice, creating redundant and independent observations. Then, a\ngeometric rigidity formula is proposed to filter out the erroneous 3-DoF poses,\nand the inliers are merged to generate the final camera pose. Furthermore, we\npropose a model that quantifies the meaningfulness of localization by\nestimating the number of false alarms (NFA), according to the distribution of\nthe locations of the sliced images. By eliminating gross errors, Slice-Loc\nboosts localization accuracy and effectively detects failures. After filtering\nout mislocalizations, Slice-Loc reduces the proportion of errors exceeding 10 m\nto under 3\\%. In cross-city tests on the DReSS dataset, Slice-Loc cuts the mean\nlocalization error from 4.47 m to 1.86 m and the mean orientation error from\n$\\mathbf{3.42^{\\circ}}$ to $\\mathbf{1.24^{\\circ}}$, outperforming\nstate-of-the-art methods. Code and dataset will be available at:\nhttps://github.com/bnothing/Slice-Loc.\n", "link": "http://arxiv.org/abs/2508.05369v1", "date": "2025-08-07", "relevancy": 2.3621, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6145}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5795}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-View%20Localization%20via%20Redundant%20Sliced%20Observations%20and%0A%20%20A-Contrario%20Validation&body=Title%3A%20Cross-View%20Localization%20via%20Redundant%20Sliced%20Observations%20and%0A%20%20A-Contrario%20Validation%0AAuthor%3A%20Yongjun%20Zhang%20and%20Mingtao%20Xiong%20and%20Yi%20Wan%20and%20Gui-Song%20Xia%0AAbstract%3A%20%20%20Cross-view%20localization%20%28CVL%29%20matches%20ground-level%20images%20with%20aerial%0Areferences%20to%20determine%20the%20geo-position%20of%20a%20camera%2C%20enabling%20smart%20vehicles%0Ato%20self-localize%20offline%20in%20GNSS-denied%20environments.%20However%2C%20most%20CVL%20methods%0Aoutput%20only%20a%20single%20observation%2C%20the%20camera%20pose%2C%20and%20lack%20the%20redundant%0Aobservations%20required%20by%20surveying%20principles%2C%20making%20it%20challenging%20to%20assess%0Alocalization%20reliability%20through%20the%20mutual%20validation%20of%20observational%20data.%0ATo%20tackle%20this%2C%20we%20introduce%20Slice-Loc%2C%20a%20two-stage%20method%20featuring%20an%0Aa-contrario%20reliability%20validation%20for%20CVL.%20Instead%20of%20using%20the%20query%20image%20as%0Aa%20single%20input%2C%20Slice-Loc%20divides%20it%20into%20sub-images%20and%20estimates%20the%203-DoF%0Apose%20for%20each%20slice%2C%20creating%20redundant%20and%20independent%20observations.%20Then%2C%20a%0Ageometric%20rigidity%20formula%20is%20proposed%20to%20filter%20out%20the%20erroneous%203-DoF%20poses%2C%0Aand%20the%20inliers%20are%20merged%20to%20generate%20the%20final%20camera%20pose.%20Furthermore%2C%20we%0Apropose%20a%20model%20that%20quantifies%20the%20meaningfulness%20of%20localization%20by%0Aestimating%20the%20number%20of%20false%20alarms%20%28NFA%29%2C%20according%20to%20the%20distribution%20of%0Athe%20locations%20of%20the%20sliced%20images.%20By%20eliminating%20gross%20errors%2C%20Slice-Loc%0Aboosts%20localization%20accuracy%20and%20effectively%20detects%20failures.%20After%20filtering%0Aout%20mislocalizations%2C%20Slice-Loc%20reduces%20the%20proportion%20of%20errors%20exceeding%2010%20m%0Ato%20under%203%5C%25.%20In%20cross-city%20tests%20on%20the%20DReSS%20dataset%2C%20Slice-Loc%20cuts%20the%20mean%0Alocalization%20error%20from%204.47%20m%20to%201.86%20m%20and%20the%20mean%20orientation%20error%20from%0A%24%5Cmathbf%7B3.42%5E%7B%5Ccirc%7D%7D%24%20to%20%24%5Cmathbf%7B1.24%5E%7B%5Ccirc%7D%7D%24%2C%20outperforming%0Astate-of-the-art%20methods.%20Code%20and%20dataset%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/bnothing/Slice-Loc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-View%2520Localization%2520via%2520Redundant%2520Sliced%2520Observations%2520and%250A%2520%2520A-Contrario%2520Validation%26entry.906535625%3DYongjun%2520Zhang%2520and%2520Mingtao%2520Xiong%2520and%2520Yi%2520Wan%2520and%2520Gui-Song%2520Xia%26entry.1292438233%3D%2520%2520Cross-view%2520localization%2520%2528CVL%2529%2520matches%2520ground-level%2520images%2520with%2520aerial%250Areferences%2520to%2520determine%2520the%2520geo-position%2520of%2520a%2520camera%252C%2520enabling%2520smart%2520vehicles%250Ato%2520self-localize%2520offline%2520in%2520GNSS-denied%2520environments.%2520However%252C%2520most%2520CVL%2520methods%250Aoutput%2520only%2520a%2520single%2520observation%252C%2520the%2520camera%2520pose%252C%2520and%2520lack%2520the%2520redundant%250Aobservations%2520required%2520by%2520surveying%2520principles%252C%2520making%2520it%2520challenging%2520to%2520assess%250Alocalization%2520reliability%2520through%2520the%2520mutual%2520validation%2520of%2520observational%2520data.%250ATo%2520tackle%2520this%252C%2520we%2520introduce%2520Slice-Loc%252C%2520a%2520two-stage%2520method%2520featuring%2520an%250Aa-contrario%2520reliability%2520validation%2520for%2520CVL.%2520Instead%2520of%2520using%2520the%2520query%2520image%2520as%250Aa%2520single%2520input%252C%2520Slice-Loc%2520divides%2520it%2520into%2520sub-images%2520and%2520estimates%2520the%25203-DoF%250Apose%2520for%2520each%2520slice%252C%2520creating%2520redundant%2520and%2520independent%2520observations.%2520Then%252C%2520a%250Ageometric%2520rigidity%2520formula%2520is%2520proposed%2520to%2520filter%2520out%2520the%2520erroneous%25203-DoF%2520poses%252C%250Aand%2520the%2520inliers%2520are%2520merged%2520to%2520generate%2520the%2520final%2520camera%2520pose.%2520Furthermore%252C%2520we%250Apropose%2520a%2520model%2520that%2520quantifies%2520the%2520meaningfulness%2520of%2520localization%2520by%250Aestimating%2520the%2520number%2520of%2520false%2520alarms%2520%2528NFA%2529%252C%2520according%2520to%2520the%2520distribution%2520of%250Athe%2520locations%2520of%2520the%2520sliced%2520images.%2520By%2520eliminating%2520gross%2520errors%252C%2520Slice-Loc%250Aboosts%2520localization%2520accuracy%2520and%2520effectively%2520detects%2520failures.%2520After%2520filtering%250Aout%2520mislocalizations%252C%2520Slice-Loc%2520reduces%2520the%2520proportion%2520of%2520errors%2520exceeding%252010%2520m%250Ato%2520under%25203%255C%2525.%2520In%2520cross-city%2520tests%2520on%2520the%2520DReSS%2520dataset%252C%2520Slice-Loc%2520cuts%2520the%2520mean%250Alocalization%2520error%2520from%25204.47%2520m%2520to%25201.86%2520m%2520and%2520the%2520mean%2520orientation%2520error%2520from%250A%2524%255Cmathbf%257B3.42%255E%257B%255Ccirc%257D%257D%2524%2520to%2520%2524%255Cmathbf%257B1.24%255E%257B%255Ccirc%257D%257D%2524%252C%2520outperforming%250Astate-of-the-art%2520methods.%2520Code%2520and%2520dataset%2520will%2520be%2520available%2520at%253A%250Ahttps%253A//github.com/bnothing/Slice-Loc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-View%20Localization%20via%20Redundant%20Sliced%20Observations%20and%0A%20%20A-Contrario%20Validation&entry.906535625=Yongjun%20Zhang%20and%20Mingtao%20Xiong%20and%20Yi%20Wan%20and%20Gui-Song%20Xia&entry.1292438233=%20%20Cross-view%20localization%20%28CVL%29%20matches%20ground-level%20images%20with%20aerial%0Areferences%20to%20determine%20the%20geo-position%20of%20a%20camera%2C%20enabling%20smart%20vehicles%0Ato%20self-localize%20offline%20in%20GNSS-denied%20environments.%20However%2C%20most%20CVL%20methods%0Aoutput%20only%20a%20single%20observation%2C%20the%20camera%20pose%2C%20and%20lack%20the%20redundant%0Aobservations%20required%20by%20surveying%20principles%2C%20making%20it%20challenging%20to%20assess%0Alocalization%20reliability%20through%20the%20mutual%20validation%20of%20observational%20data.%0ATo%20tackle%20this%2C%20we%20introduce%20Slice-Loc%2C%20a%20two-stage%20method%20featuring%20an%0Aa-contrario%20reliability%20validation%20for%20CVL.%20Instead%20of%20using%20the%20query%20image%20as%0Aa%20single%20input%2C%20Slice-Loc%20divides%20it%20into%20sub-images%20and%20estimates%20the%203-DoF%0Apose%20for%20each%20slice%2C%20creating%20redundant%20and%20independent%20observations.%20Then%2C%20a%0Ageometric%20rigidity%20formula%20is%20proposed%20to%20filter%20out%20the%20erroneous%203-DoF%20poses%2C%0Aand%20the%20inliers%20are%20merged%20to%20generate%20the%20final%20camera%20pose.%20Furthermore%2C%20we%0Apropose%20a%20model%20that%20quantifies%20the%20meaningfulness%20of%20localization%20by%0Aestimating%20the%20number%20of%20false%20alarms%20%28NFA%29%2C%20according%20to%20the%20distribution%20of%0Athe%20locations%20of%20the%20sliced%20images.%20By%20eliminating%20gross%20errors%2C%20Slice-Loc%0Aboosts%20localization%20accuracy%20and%20effectively%20detects%20failures.%20After%20filtering%0Aout%20mislocalizations%2C%20Slice-Loc%20reduces%20the%20proportion%20of%20errors%20exceeding%2010%20m%0Ato%20under%203%5C%25.%20In%20cross-city%20tests%20on%20the%20DReSS%20dataset%2C%20Slice-Loc%20cuts%20the%20mean%0Alocalization%20error%20from%204.47%20m%20to%201.86%20m%20and%20the%20mean%20orientation%20error%20from%0A%24%5Cmathbf%7B3.42%5E%7B%5Ccirc%7D%7D%24%20to%20%24%5Cmathbf%7B1.24%5E%7B%5Ccirc%7D%7D%24%2C%20outperforming%0Astate-of-the-art%20methods.%20Code%20and%20dataset%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/bnothing/Slice-Loc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05369v1&entry.124074799=Read"},
{"title": "Physically Controllable Relighting of Photographs", "author": "Chris Careaga and Ya\u011f\u0131z Aksoy", "abstract": "  We present a self-supervised approach to in-the-wild image relighting that\nenables fully controllable, physically based illumination editing. We achieve\nthis by combining the physical accuracy of traditional rendering with the\nphotorealistic appearance made possible by neural rendering. Our pipeline works\nby inferring a colored mesh representation of a given scene using monocular\nestimates of geometry and intrinsic components. This representation allows\nusers to define their desired illumination configuration in 3D. The scene under\nthe new lighting can then be rendered using a path-tracing engine. We send this\napproximate rendering of the scene through a feed-forward neural renderer to\npredict the final photorealistic relighting result. We develop a differentiable\nrendering process to reconstruct in-the-wild scene illumination, enabling\nself-supervised training of our neural renderer on raw image collections. Our\nmethod represents a significant step in bringing the explicit physical control\nover lights available in typical 3D computer graphics tools, such as Blender,\nto in-the-wild relighting.\n", "link": "http://arxiv.org/abs/2508.05626v1", "date": "2025-08-07", "relevancy": 2.3498, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5953}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5935}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physically%20Controllable%20Relighting%20of%20Photographs&body=Title%3A%20Physically%20Controllable%20Relighting%20of%20Photographs%0AAuthor%3A%20Chris%20Careaga%20and%20Ya%C4%9F%C4%B1z%20Aksoy%0AAbstract%3A%20%20%20We%20present%20a%20self-supervised%20approach%20to%20in-the-wild%20image%20relighting%20that%0Aenables%20fully%20controllable%2C%20physically%20based%20illumination%20editing.%20We%20achieve%0Athis%20by%20combining%20the%20physical%20accuracy%20of%20traditional%20rendering%20with%20the%0Aphotorealistic%20appearance%20made%20possible%20by%20neural%20rendering.%20Our%20pipeline%20works%0Aby%20inferring%20a%20colored%20mesh%20representation%20of%20a%20given%20scene%20using%20monocular%0Aestimates%20of%20geometry%20and%20intrinsic%20components.%20This%20representation%20allows%0Ausers%20to%20define%20their%20desired%20illumination%20configuration%20in%203D.%20The%20scene%20under%0Athe%20new%20lighting%20can%20then%20be%20rendered%20using%20a%20path-tracing%20engine.%20We%20send%20this%0Aapproximate%20rendering%20of%20the%20scene%20through%20a%20feed-forward%20neural%20renderer%20to%0Apredict%20the%20final%20photorealistic%20relighting%20result.%20We%20develop%20a%20differentiable%0Arendering%20process%20to%20reconstruct%20in-the-wild%20scene%20illumination%2C%20enabling%0Aself-supervised%20training%20of%20our%20neural%20renderer%20on%20raw%20image%20collections.%20Our%0Amethod%20represents%20a%20significant%20step%20in%20bringing%20the%20explicit%20physical%20control%0Aover%20lights%20available%20in%20typical%203D%20computer%20graphics%20tools%2C%20such%20as%20Blender%2C%0Ato%20in-the-wild%20relighting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysically%2520Controllable%2520Relighting%2520of%2520Photographs%26entry.906535625%3DChris%2520Careaga%2520and%2520Ya%25C4%259F%25C4%25B1z%2520Aksoy%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520self-supervised%2520approach%2520to%2520in-the-wild%2520image%2520relighting%2520that%250Aenables%2520fully%2520controllable%252C%2520physically%2520based%2520illumination%2520editing.%2520We%2520achieve%250Athis%2520by%2520combining%2520the%2520physical%2520accuracy%2520of%2520traditional%2520rendering%2520with%2520the%250Aphotorealistic%2520appearance%2520made%2520possible%2520by%2520neural%2520rendering.%2520Our%2520pipeline%2520works%250Aby%2520inferring%2520a%2520colored%2520mesh%2520representation%2520of%2520a%2520given%2520scene%2520using%2520monocular%250Aestimates%2520of%2520geometry%2520and%2520intrinsic%2520components.%2520This%2520representation%2520allows%250Ausers%2520to%2520define%2520their%2520desired%2520illumination%2520configuration%2520in%25203D.%2520The%2520scene%2520under%250Athe%2520new%2520lighting%2520can%2520then%2520be%2520rendered%2520using%2520a%2520path-tracing%2520engine.%2520We%2520send%2520this%250Aapproximate%2520rendering%2520of%2520the%2520scene%2520through%2520a%2520feed-forward%2520neural%2520renderer%2520to%250Apredict%2520the%2520final%2520photorealistic%2520relighting%2520result.%2520We%2520develop%2520a%2520differentiable%250Arendering%2520process%2520to%2520reconstruct%2520in-the-wild%2520scene%2520illumination%252C%2520enabling%250Aself-supervised%2520training%2520of%2520our%2520neural%2520renderer%2520on%2520raw%2520image%2520collections.%2520Our%250Amethod%2520represents%2520a%2520significant%2520step%2520in%2520bringing%2520the%2520explicit%2520physical%2520control%250Aover%2520lights%2520available%2520in%2520typical%25203D%2520computer%2520graphics%2520tools%252C%2520such%2520as%2520Blender%252C%250Ato%2520in-the-wild%2520relighting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physically%20Controllable%20Relighting%20of%20Photographs&entry.906535625=Chris%20Careaga%20and%20Ya%C4%9F%C4%B1z%20Aksoy&entry.1292438233=%20%20We%20present%20a%20self-supervised%20approach%20to%20in-the-wild%20image%20relighting%20that%0Aenables%20fully%20controllable%2C%20physically%20based%20illumination%20editing.%20We%20achieve%0Athis%20by%20combining%20the%20physical%20accuracy%20of%20traditional%20rendering%20with%20the%0Aphotorealistic%20appearance%20made%20possible%20by%20neural%20rendering.%20Our%20pipeline%20works%0Aby%20inferring%20a%20colored%20mesh%20representation%20of%20a%20given%20scene%20using%20monocular%0Aestimates%20of%20geometry%20and%20intrinsic%20components.%20This%20representation%20allows%0Ausers%20to%20define%20their%20desired%20illumination%20configuration%20in%203D.%20The%20scene%20under%0Athe%20new%20lighting%20can%20then%20be%20rendered%20using%20a%20path-tracing%20engine.%20We%20send%20this%0Aapproximate%20rendering%20of%20the%20scene%20through%20a%20feed-forward%20neural%20renderer%20to%0Apredict%20the%20final%20photorealistic%20relighting%20result.%20We%20develop%20a%20differentiable%0Arendering%20process%20to%20reconstruct%20in-the-wild%20scene%20illumination%2C%20enabling%0Aself-supervised%20training%20of%20our%20neural%20renderer%20on%20raw%20image%20collections.%20Our%0Amethod%20represents%20a%20significant%20step%20in%20bringing%20the%20explicit%20physical%20control%0Aover%20lights%20available%20in%20typical%203D%20computer%20graphics%20tools%2C%20such%20as%20Blender%2C%0Ato%20in-the-wild%20relighting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05626v1&entry.124074799=Read"},
{"title": "B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding", "author": "Changho Choi and Youngwoo Shin and Gyojin Han and Dong-Jae Lee and Junmo Kim", "abstract": "  Understanding dynamic outdoor environments requires capturing complex object\ninteractions and their evolution over time. LiDAR-based 4D point clouds provide\nprecise spatial geometry and rich temporal cues, making them ideal for\nrepresenting real-world scenes. However, despite their potential, 4D LiDAR\nremains underexplored in the context of Multimodal Large Language Models\n(MLLMs) due to the absence of high-quality, modality-specific annotations and\nthe lack of MLLM architectures capable of processing its high-dimensional\ncomposition. To address these challenges, we introduce B4DL, a new benchmark\nspecifically designed for training and evaluating MLLMs on 4D LiDAR\nunderstanding. In addition, we propose a scalable data generation pipeline and\nan MLLM model that, for the first time, directly processes raw 4D LiDAR by\nbridging it with language understanding. Combined with our dataset and\nbenchmark, our model offers a unified solution for spatio-temporal reasoning in\ndynamic outdoor environments. We provide rendered 4D LiDAR videos, generated\ndataset, and inference outputs on diverse scenarios at:\nhttps://mmb4dl.github.io/mmb4dl/\n", "link": "http://arxiv.org/abs/2508.05269v1", "date": "2025-08-07", "relevancy": 2.3469, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5883}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5883}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20B4DL%3A%20A%20Benchmark%20for%204D%20LiDAR%20LLM%20in%20Spatio-Temporal%20Understanding&body=Title%3A%20B4DL%3A%20A%20Benchmark%20for%204D%20LiDAR%20LLM%20in%20Spatio-Temporal%20Understanding%0AAuthor%3A%20Changho%20Choi%20and%20Youngwoo%20Shin%20and%20Gyojin%20Han%20and%20Dong-Jae%20Lee%20and%20Junmo%20Kim%0AAbstract%3A%20%20%20Understanding%20dynamic%20outdoor%20environments%20requires%20capturing%20complex%20object%0Ainteractions%20and%20their%20evolution%20over%20time.%20LiDAR-based%204D%20point%20clouds%20provide%0Aprecise%20spatial%20geometry%20and%20rich%20temporal%20cues%2C%20making%20them%20ideal%20for%0Arepresenting%20real-world%20scenes.%20However%2C%20despite%20their%20potential%2C%204D%20LiDAR%0Aremains%20underexplored%20in%20the%20context%20of%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20due%20to%20the%20absence%20of%20high-quality%2C%20modality-specific%20annotations%20and%0Athe%20lack%20of%20MLLM%20architectures%20capable%20of%20processing%20its%20high-dimensional%0Acomposition.%20To%20address%20these%20challenges%2C%20we%20introduce%20B4DL%2C%20a%20new%20benchmark%0Aspecifically%20designed%20for%20training%20and%20evaluating%20MLLMs%20on%204D%20LiDAR%0Aunderstanding.%20In%20addition%2C%20we%20propose%20a%20scalable%20data%20generation%20pipeline%20and%0Aan%20MLLM%20model%20that%2C%20for%20the%20first%20time%2C%20directly%20processes%20raw%204D%20LiDAR%20by%0Abridging%20it%20with%20language%20understanding.%20Combined%20with%20our%20dataset%20and%0Abenchmark%2C%20our%20model%20offers%20a%20unified%20solution%20for%20spatio-temporal%20reasoning%20in%0Adynamic%20outdoor%20environments.%20We%20provide%20rendered%204D%20LiDAR%20videos%2C%20generated%0Adataset%2C%20and%20inference%20outputs%20on%20diverse%20scenarios%20at%3A%0Ahttps%3A//mmb4dl.github.io/mmb4dl/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DB4DL%253A%2520A%2520Benchmark%2520for%25204D%2520LiDAR%2520LLM%2520in%2520Spatio-Temporal%2520Understanding%26entry.906535625%3DChangho%2520Choi%2520and%2520Youngwoo%2520Shin%2520and%2520Gyojin%2520Han%2520and%2520Dong-Jae%2520Lee%2520and%2520Junmo%2520Kim%26entry.1292438233%3D%2520%2520Understanding%2520dynamic%2520outdoor%2520environments%2520requires%2520capturing%2520complex%2520object%250Ainteractions%2520and%2520their%2520evolution%2520over%2520time.%2520LiDAR-based%25204D%2520point%2520clouds%2520provide%250Aprecise%2520spatial%2520geometry%2520and%2520rich%2520temporal%2520cues%252C%2520making%2520them%2520ideal%2520for%250Arepresenting%2520real-world%2520scenes.%2520However%252C%2520despite%2520their%2520potential%252C%25204D%2520LiDAR%250Aremains%2520underexplored%2520in%2520the%2520context%2520of%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520due%2520to%2520the%2520absence%2520of%2520high-quality%252C%2520modality-specific%2520annotations%2520and%250Athe%2520lack%2520of%2520MLLM%2520architectures%2520capable%2520of%2520processing%2520its%2520high-dimensional%250Acomposition.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520B4DL%252C%2520a%2520new%2520benchmark%250Aspecifically%2520designed%2520for%2520training%2520and%2520evaluating%2520MLLMs%2520on%25204D%2520LiDAR%250Aunderstanding.%2520In%2520addition%252C%2520we%2520propose%2520a%2520scalable%2520data%2520generation%2520pipeline%2520and%250Aan%2520MLLM%2520model%2520that%252C%2520for%2520the%2520first%2520time%252C%2520directly%2520processes%2520raw%25204D%2520LiDAR%2520by%250Abridging%2520it%2520with%2520language%2520understanding.%2520Combined%2520with%2520our%2520dataset%2520and%250Abenchmark%252C%2520our%2520model%2520offers%2520a%2520unified%2520solution%2520for%2520spatio-temporal%2520reasoning%2520in%250Adynamic%2520outdoor%2520environments.%2520We%2520provide%2520rendered%25204D%2520LiDAR%2520videos%252C%2520generated%250Adataset%252C%2520and%2520inference%2520outputs%2520on%2520diverse%2520scenarios%2520at%253A%250Ahttps%253A//mmb4dl.github.io/mmb4dl/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=B4DL%3A%20A%20Benchmark%20for%204D%20LiDAR%20LLM%20in%20Spatio-Temporal%20Understanding&entry.906535625=Changho%20Choi%20and%20Youngwoo%20Shin%20and%20Gyojin%20Han%20and%20Dong-Jae%20Lee%20and%20Junmo%20Kim&entry.1292438233=%20%20Understanding%20dynamic%20outdoor%20environments%20requires%20capturing%20complex%20object%0Ainteractions%20and%20their%20evolution%20over%20time.%20LiDAR-based%204D%20point%20clouds%20provide%0Aprecise%20spatial%20geometry%20and%20rich%20temporal%20cues%2C%20making%20them%20ideal%20for%0Arepresenting%20real-world%20scenes.%20However%2C%20despite%20their%20potential%2C%204D%20LiDAR%0Aremains%20underexplored%20in%20the%20context%20of%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20due%20to%20the%20absence%20of%20high-quality%2C%20modality-specific%20annotations%20and%0Athe%20lack%20of%20MLLM%20architectures%20capable%20of%20processing%20its%20high-dimensional%0Acomposition.%20To%20address%20these%20challenges%2C%20we%20introduce%20B4DL%2C%20a%20new%20benchmark%0Aspecifically%20designed%20for%20training%20and%20evaluating%20MLLMs%20on%204D%20LiDAR%0Aunderstanding.%20In%20addition%2C%20we%20propose%20a%20scalable%20data%20generation%20pipeline%20and%0Aan%20MLLM%20model%20that%2C%20for%20the%20first%20time%2C%20directly%20processes%20raw%204D%20LiDAR%20by%0Abridging%20it%20with%20language%20understanding.%20Combined%20with%20our%20dataset%20and%0Abenchmark%2C%20our%20model%20offers%20a%20unified%20solution%20for%20spatio-temporal%20reasoning%20in%0Adynamic%20outdoor%20environments.%20We%20provide%20rendered%204D%20LiDAR%20videos%2C%20generated%0Adataset%2C%20and%20inference%20outputs%20on%20diverse%20scenarios%20at%3A%0Ahttps%3A//mmb4dl.github.io/mmb4dl/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05269v1&entry.124074799=Read"},
{"title": "Online Graph Topology Learning via Time-Vertex Adaptive Filters: From\n  Theory to Cardiac Fibrillation", "author": "Alexander Jenkins and Thiernithi Variddhisai and Ahmed El-Medany and Fu Siong Ng and Danilo Mandic", "abstract": "  Graph Signal Processing (GSP) provides a powerful framework for analysing\ncomplex, interconnected systems by modelling data as signals on graphs. While\nrecent advances have enabled graph topology learning from observed signals,\nexisting methods often struggle with time-varying systems and real-time\napplications. To address this gap, we introduce AdaCGP, a sparsity-aware\nadaptive algorithm for dynamic graph topology estimation from multivariate time\nseries. AdaCGP estimates the Graph Shift Operator (GSO) through recursive\nupdate formulae designed to address sparsity, shift-invariance, and bias.\nThrough comprehensive simulations, we demonstrate that AdaCGP consistently\noutperforms multiple baselines across diverse graph topologies, achieving\nimprovements exceeding 83% in GSO estimation compared to state-of-the-art\nmethods while maintaining favourable computational scaling properties. Our\nvariable splitting approach enables reliable identification of causal\nconnections with near-zero false alarm rates and minimal missed edges. Applied\nto cardiac fibrillation recordings, AdaCGP tracks dynamic changes in\npropagation patterns more effectively than established methods like Granger\ncausality, capturing temporal variations in graph topology that static\napproaches miss. The algorithm successfully identifies stability\ncharacteristics in conduction patterns that may maintain arrhythmias,\ndemonstrating potential for clinical applications in diagnosis and treatment of\ncomplex biomedical systems.\n", "link": "http://arxiv.org/abs/2411.01567v2", "date": "2025-08-07", "relevancy": 2.3391, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4806}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4639}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Graph%20Topology%20Learning%20via%20Time-Vertex%20Adaptive%20Filters%3A%20From%0A%20%20Theory%20to%20Cardiac%20Fibrillation&body=Title%3A%20Online%20Graph%20Topology%20Learning%20via%20Time-Vertex%20Adaptive%20Filters%3A%20From%0A%20%20Theory%20to%20Cardiac%20Fibrillation%0AAuthor%3A%20Alexander%20Jenkins%20and%20Thiernithi%20Variddhisai%20and%20Ahmed%20El-Medany%20and%20Fu%20Siong%20Ng%20and%20Danilo%20Mandic%0AAbstract%3A%20%20%20Graph%20Signal%20Processing%20%28GSP%29%20provides%20a%20powerful%20framework%20for%20analysing%0Acomplex%2C%20interconnected%20systems%20by%20modelling%20data%20as%20signals%20on%20graphs.%20While%0Arecent%20advances%20have%20enabled%20graph%20topology%20learning%20from%20observed%20signals%2C%0Aexisting%20methods%20often%20struggle%20with%20time-varying%20systems%20and%20real-time%0Aapplications.%20To%20address%20this%20gap%2C%20we%20introduce%20AdaCGP%2C%20a%20sparsity-aware%0Aadaptive%20algorithm%20for%20dynamic%20graph%20topology%20estimation%20from%20multivariate%20time%0Aseries.%20AdaCGP%20estimates%20the%20Graph%20Shift%20Operator%20%28GSO%29%20through%20recursive%0Aupdate%20formulae%20designed%20to%20address%20sparsity%2C%20shift-invariance%2C%20and%20bias.%0AThrough%20comprehensive%20simulations%2C%20we%20demonstrate%20that%20AdaCGP%20consistently%0Aoutperforms%20multiple%20baselines%20across%20diverse%20graph%20topologies%2C%20achieving%0Aimprovements%20exceeding%2083%25%20in%20GSO%20estimation%20compared%20to%20state-of-the-art%0Amethods%20while%20maintaining%20favourable%20computational%20scaling%20properties.%20Our%0Avariable%20splitting%20approach%20enables%20reliable%20identification%20of%20causal%0Aconnections%20with%20near-zero%20false%20alarm%20rates%20and%20minimal%20missed%20edges.%20Applied%0Ato%20cardiac%20fibrillation%20recordings%2C%20AdaCGP%20tracks%20dynamic%20changes%20in%0Apropagation%20patterns%20more%20effectively%20than%20established%20methods%20like%20Granger%0Acausality%2C%20capturing%20temporal%20variations%20in%20graph%20topology%20that%20static%0Aapproaches%20miss.%20The%20algorithm%20successfully%20identifies%20stability%0Acharacteristics%20in%20conduction%20patterns%20that%20may%20maintain%20arrhythmias%2C%0Ademonstrating%20potential%20for%20clinical%20applications%20in%20diagnosis%20and%20treatment%20of%0Acomplex%20biomedical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Graph%2520Topology%2520Learning%2520via%2520Time-Vertex%2520Adaptive%2520Filters%253A%2520From%250A%2520%2520Theory%2520to%2520Cardiac%2520Fibrillation%26entry.906535625%3DAlexander%2520Jenkins%2520and%2520Thiernithi%2520Variddhisai%2520and%2520Ahmed%2520El-Medany%2520and%2520Fu%2520Siong%2520Ng%2520and%2520Danilo%2520Mandic%26entry.1292438233%3D%2520%2520Graph%2520Signal%2520Processing%2520%2528GSP%2529%2520provides%2520a%2520powerful%2520framework%2520for%2520analysing%250Acomplex%252C%2520interconnected%2520systems%2520by%2520modelling%2520data%2520as%2520signals%2520on%2520graphs.%2520While%250Arecent%2520advances%2520have%2520enabled%2520graph%2520topology%2520learning%2520from%2520observed%2520signals%252C%250Aexisting%2520methods%2520often%2520struggle%2520with%2520time-varying%2520systems%2520and%2520real-time%250Aapplications.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520AdaCGP%252C%2520a%2520sparsity-aware%250Aadaptive%2520algorithm%2520for%2520dynamic%2520graph%2520topology%2520estimation%2520from%2520multivariate%2520time%250Aseries.%2520AdaCGP%2520estimates%2520the%2520Graph%2520Shift%2520Operator%2520%2528GSO%2529%2520through%2520recursive%250Aupdate%2520formulae%2520designed%2520to%2520address%2520sparsity%252C%2520shift-invariance%252C%2520and%2520bias.%250AThrough%2520comprehensive%2520simulations%252C%2520we%2520demonstrate%2520that%2520AdaCGP%2520consistently%250Aoutperforms%2520multiple%2520baselines%2520across%2520diverse%2520graph%2520topologies%252C%2520achieving%250Aimprovements%2520exceeding%252083%2525%2520in%2520GSO%2520estimation%2520compared%2520to%2520state-of-the-art%250Amethods%2520while%2520maintaining%2520favourable%2520computational%2520scaling%2520properties.%2520Our%250Avariable%2520splitting%2520approach%2520enables%2520reliable%2520identification%2520of%2520causal%250Aconnections%2520with%2520near-zero%2520false%2520alarm%2520rates%2520and%2520minimal%2520missed%2520edges.%2520Applied%250Ato%2520cardiac%2520fibrillation%2520recordings%252C%2520AdaCGP%2520tracks%2520dynamic%2520changes%2520in%250Apropagation%2520patterns%2520more%2520effectively%2520than%2520established%2520methods%2520like%2520Granger%250Acausality%252C%2520capturing%2520temporal%2520variations%2520in%2520graph%2520topology%2520that%2520static%250Aapproaches%2520miss.%2520The%2520algorithm%2520successfully%2520identifies%2520stability%250Acharacteristics%2520in%2520conduction%2520patterns%2520that%2520may%2520maintain%2520arrhythmias%252C%250Ademonstrating%2520potential%2520for%2520clinical%2520applications%2520in%2520diagnosis%2520and%2520treatment%2520of%250Acomplex%2520biomedical%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Graph%20Topology%20Learning%20via%20Time-Vertex%20Adaptive%20Filters%3A%20From%0A%20%20Theory%20to%20Cardiac%20Fibrillation&entry.906535625=Alexander%20Jenkins%20and%20Thiernithi%20Variddhisai%20and%20Ahmed%20El-Medany%20and%20Fu%20Siong%20Ng%20and%20Danilo%20Mandic&entry.1292438233=%20%20Graph%20Signal%20Processing%20%28GSP%29%20provides%20a%20powerful%20framework%20for%20analysing%0Acomplex%2C%20interconnected%20systems%20by%20modelling%20data%20as%20signals%20on%20graphs.%20While%0Arecent%20advances%20have%20enabled%20graph%20topology%20learning%20from%20observed%20signals%2C%0Aexisting%20methods%20often%20struggle%20with%20time-varying%20systems%20and%20real-time%0Aapplications.%20To%20address%20this%20gap%2C%20we%20introduce%20AdaCGP%2C%20a%20sparsity-aware%0Aadaptive%20algorithm%20for%20dynamic%20graph%20topology%20estimation%20from%20multivariate%20time%0Aseries.%20AdaCGP%20estimates%20the%20Graph%20Shift%20Operator%20%28GSO%29%20through%20recursive%0Aupdate%20formulae%20designed%20to%20address%20sparsity%2C%20shift-invariance%2C%20and%20bias.%0AThrough%20comprehensive%20simulations%2C%20we%20demonstrate%20that%20AdaCGP%20consistently%0Aoutperforms%20multiple%20baselines%20across%20diverse%20graph%20topologies%2C%20achieving%0Aimprovements%20exceeding%2083%25%20in%20GSO%20estimation%20compared%20to%20state-of-the-art%0Amethods%20while%20maintaining%20favourable%20computational%20scaling%20properties.%20Our%0Avariable%20splitting%20approach%20enables%20reliable%20identification%20of%20causal%0Aconnections%20with%20near-zero%20false%20alarm%20rates%20and%20minimal%20missed%20edges.%20Applied%0Ato%20cardiac%20fibrillation%20recordings%2C%20AdaCGP%20tracks%20dynamic%20changes%20in%0Apropagation%20patterns%20more%20effectively%20than%20established%20methods%20like%20Granger%0Acausality%2C%20capturing%20temporal%20variations%20in%20graph%20topology%20that%20static%0Aapproaches%20miss.%20The%20algorithm%20successfully%20identifies%20stability%0Acharacteristics%20in%20conduction%20patterns%20that%20may%20maintain%20arrhythmias%2C%0Ademonstrating%20potential%20for%20clinical%20applications%20in%20diagnosis%20and%20treatment%20of%0Acomplex%20biomedical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01567v2&entry.124074799=Read"},
{"title": "Towards Generalizable Safety in Crowd Navigation via Conformal\n  Uncertainty Handling", "author": "Jianpeng Yao and Xiaopan Zhang and Yu Xia and Zejin Wang and Amit K. Roy-Chowdhury and Jiachen Li", "abstract": "  Mobile robots navigating in crowds trained using reinforcement learning are\nknown to suffer performance degradation when faced with out-of-distribution\nscenarios. We propose that by properly accounting for the uncertainties of\npedestrians, a robot can learn safe navigation policies that are robust to\ndistribution shifts. Our method augments agent observations with prediction\nuncertainty estimates generated by adaptive conformal inference, and it uses\nthese estimates to guide the agent's behavior through constrained reinforcement\nlearning. The system helps regulate the agent's actions and enables it to adapt\nto distribution shifts. In the in-distribution setting, our approach achieves a\n96.93% success rate, which is over 8.80% higher than the previous\nstate-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times\nfewer intrusions into ground-truth human future trajectories. In three\nout-of-distribution scenarios, our method shows much stronger robustness when\nfacing distribution shifts in velocity variations, policy changes, and\ntransitions from individual to group dynamics. We deploy our method on a real\nrobot, and experiments show that the robot makes safe and robust decisions when\ninteracting with both sparse and dense crowds. Our code and videos are\navailable on https://gen-safe-nav.github.io/.\n", "link": "http://arxiv.org/abs/2508.05634v1", "date": "2025-08-07", "relevancy": 2.333, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.614}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.612}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalizable%20Safety%20in%20Crowd%20Navigation%20via%20Conformal%0A%20%20Uncertainty%20Handling&body=Title%3A%20Towards%20Generalizable%20Safety%20in%20Crowd%20Navigation%20via%20Conformal%0A%20%20Uncertainty%20Handling%0AAuthor%3A%20Jianpeng%20Yao%20and%20Xiaopan%20Zhang%20and%20Yu%20Xia%20and%20Zejin%20Wang%20and%20Amit%20K.%20Roy-Chowdhury%20and%20Jiachen%20Li%0AAbstract%3A%20%20%20Mobile%20robots%20navigating%20in%20crowds%20trained%20using%20reinforcement%20learning%20are%0Aknown%20to%20suffer%20performance%20degradation%20when%20faced%20with%20out-of-distribution%0Ascenarios.%20We%20propose%20that%20by%20properly%20accounting%20for%20the%20uncertainties%20of%0Apedestrians%2C%20a%20robot%20can%20learn%20safe%20navigation%20policies%20that%20are%20robust%20to%0Adistribution%20shifts.%20Our%20method%20augments%20agent%20observations%20with%20prediction%0Auncertainty%20estimates%20generated%20by%20adaptive%20conformal%20inference%2C%20and%20it%20uses%0Athese%20estimates%20to%20guide%20the%20agent%27s%20behavior%20through%20constrained%20reinforcement%0Alearning.%20The%20system%20helps%20regulate%20the%20agent%27s%20actions%20and%20enables%20it%20to%20adapt%0Ato%20distribution%20shifts.%20In%20the%20in-distribution%20setting%2C%20our%20approach%20achieves%20a%0A96.93%25%20success%20rate%2C%20which%20is%20over%208.80%25%20higher%20than%20the%20previous%0Astate-of-the-art%20baselines%20with%20over%203.72%20times%20fewer%20collisions%20and%202.43%20times%0Afewer%20intrusions%20into%20ground-truth%20human%20future%20trajectories.%20In%20three%0Aout-of-distribution%20scenarios%2C%20our%20method%20shows%20much%20stronger%20robustness%20when%0Afacing%20distribution%20shifts%20in%20velocity%20variations%2C%20policy%20changes%2C%20and%0Atransitions%20from%20individual%20to%20group%20dynamics.%20We%20deploy%20our%20method%20on%20a%20real%0Arobot%2C%20and%20experiments%20show%20that%20the%20robot%20makes%20safe%20and%20robust%20decisions%20when%0Ainteracting%20with%20both%20sparse%20and%20dense%20crowds.%20Our%20code%20and%20videos%20are%0Aavailable%20on%20https%3A//gen-safe-nav.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalizable%2520Safety%2520in%2520Crowd%2520Navigation%2520via%2520Conformal%250A%2520%2520Uncertainty%2520Handling%26entry.906535625%3DJianpeng%2520Yao%2520and%2520Xiaopan%2520Zhang%2520and%2520Yu%2520Xia%2520and%2520Zejin%2520Wang%2520and%2520Amit%2520K.%2520Roy-Chowdhury%2520and%2520Jiachen%2520Li%26entry.1292438233%3D%2520%2520Mobile%2520robots%2520navigating%2520in%2520crowds%2520trained%2520using%2520reinforcement%2520learning%2520are%250Aknown%2520to%2520suffer%2520performance%2520degradation%2520when%2520faced%2520with%2520out-of-distribution%250Ascenarios.%2520We%2520propose%2520that%2520by%2520properly%2520accounting%2520for%2520the%2520uncertainties%2520of%250Apedestrians%252C%2520a%2520robot%2520can%2520learn%2520safe%2520navigation%2520policies%2520that%2520are%2520robust%2520to%250Adistribution%2520shifts.%2520Our%2520method%2520augments%2520agent%2520observations%2520with%2520prediction%250Auncertainty%2520estimates%2520generated%2520by%2520adaptive%2520conformal%2520inference%252C%2520and%2520it%2520uses%250Athese%2520estimates%2520to%2520guide%2520the%2520agent%2527s%2520behavior%2520through%2520constrained%2520reinforcement%250Alearning.%2520The%2520system%2520helps%2520regulate%2520the%2520agent%2527s%2520actions%2520and%2520enables%2520it%2520to%2520adapt%250Ato%2520distribution%2520shifts.%2520In%2520the%2520in-distribution%2520setting%252C%2520our%2520approach%2520achieves%2520a%250A96.93%2525%2520success%2520rate%252C%2520which%2520is%2520over%25208.80%2525%2520higher%2520than%2520the%2520previous%250Astate-of-the-art%2520baselines%2520with%2520over%25203.72%2520times%2520fewer%2520collisions%2520and%25202.43%2520times%250Afewer%2520intrusions%2520into%2520ground-truth%2520human%2520future%2520trajectories.%2520In%2520three%250Aout-of-distribution%2520scenarios%252C%2520our%2520method%2520shows%2520much%2520stronger%2520robustness%2520when%250Afacing%2520distribution%2520shifts%2520in%2520velocity%2520variations%252C%2520policy%2520changes%252C%2520and%250Atransitions%2520from%2520individual%2520to%2520group%2520dynamics.%2520We%2520deploy%2520our%2520method%2520on%2520a%2520real%250Arobot%252C%2520and%2520experiments%2520show%2520that%2520the%2520robot%2520makes%2520safe%2520and%2520robust%2520decisions%2520when%250Ainteracting%2520with%2520both%2520sparse%2520and%2520dense%2520crowds.%2520Our%2520code%2520and%2520videos%2520are%250Aavailable%2520on%2520https%253A//gen-safe-nav.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalizable%20Safety%20in%20Crowd%20Navigation%20via%20Conformal%0A%20%20Uncertainty%20Handling&entry.906535625=Jianpeng%20Yao%20and%20Xiaopan%20Zhang%20and%20Yu%20Xia%20and%20Zejin%20Wang%20and%20Amit%20K.%20Roy-Chowdhury%20and%20Jiachen%20Li&entry.1292438233=%20%20Mobile%20robots%20navigating%20in%20crowds%20trained%20using%20reinforcement%20learning%20are%0Aknown%20to%20suffer%20performance%20degradation%20when%20faced%20with%20out-of-distribution%0Ascenarios.%20We%20propose%20that%20by%20properly%20accounting%20for%20the%20uncertainties%20of%0Apedestrians%2C%20a%20robot%20can%20learn%20safe%20navigation%20policies%20that%20are%20robust%20to%0Adistribution%20shifts.%20Our%20method%20augments%20agent%20observations%20with%20prediction%0Auncertainty%20estimates%20generated%20by%20adaptive%20conformal%20inference%2C%20and%20it%20uses%0Athese%20estimates%20to%20guide%20the%20agent%27s%20behavior%20through%20constrained%20reinforcement%0Alearning.%20The%20system%20helps%20regulate%20the%20agent%27s%20actions%20and%20enables%20it%20to%20adapt%0Ato%20distribution%20shifts.%20In%20the%20in-distribution%20setting%2C%20our%20approach%20achieves%20a%0A96.93%25%20success%20rate%2C%20which%20is%20over%208.80%25%20higher%20than%20the%20previous%0Astate-of-the-art%20baselines%20with%20over%203.72%20times%20fewer%20collisions%20and%202.43%20times%0Afewer%20intrusions%20into%20ground-truth%20human%20future%20trajectories.%20In%20three%0Aout-of-distribution%20scenarios%2C%20our%20method%20shows%20much%20stronger%20robustness%20when%0Afacing%20distribution%20shifts%20in%20velocity%20variations%2C%20policy%20changes%2C%20and%0Atransitions%20from%20individual%20to%20group%20dynamics.%20We%20deploy%20our%20method%20on%20a%20real%0Arobot%2C%20and%20experiments%20show%20that%20the%20robot%20makes%20safe%20and%20robust%20decisions%20when%0Ainteracting%20with%20both%20sparse%20and%20dense%20crowds.%20Our%20code%20and%20videos%20are%0Aavailable%20on%20https%3A//gen-safe-nav.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05634v1&entry.124074799=Read"},
{"title": "DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning", "author": "Xinrun Xu and Pi Bu and Ye Wang and B\u00f6rje F. Karlsson and Ziming Wang and Tengtao Song and Qi Zhu and Jun Song and Zhiming Ding and Bo Zheng", "abstract": "  Although Vision Language Models (VLMs) exhibit strong perceptual abilities\nand impressive visual reasoning, they struggle with attention to detail and\nprecise action planning in complex, dynamic environments, leading to subpar\nperformance. Real-world tasks typically require complex interactions, advanced\nspatial reasoning, long-term planning, and continuous strategy refinement,\nusually necessitating understanding the physics rules of the target scenario.\nHowever, evaluating these capabilities in real-world scenarios is often\nprohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel\nbenchmark framework designed to systematically evaluate VLMs' understanding and\nreasoning about fundamental physical principles through a series of challenging\nsimulated environments. DeepPHY integrates multiple physical reasoning\nenvironments of varying difficulty levels and incorporates fine-grained\nevaluation metrics. Our evaluation finds that even state-of-the-art VLMs\nstruggle to translate descriptive physical knowledge into precise, predictive\ncontrol.\n", "link": "http://arxiv.org/abs/2508.05405v1", "date": "2025-08-07", "relevancy": 2.3265, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5813}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepPHY%3A%20Benchmarking%20Agentic%20VLMs%20on%20Physical%20Reasoning&body=Title%3A%20DeepPHY%3A%20Benchmarking%20Agentic%20VLMs%20on%20Physical%20Reasoning%0AAuthor%3A%20Xinrun%20Xu%20and%20Pi%20Bu%20and%20Ye%20Wang%20and%20B%C3%B6rje%20F.%20Karlsson%20and%20Ziming%20Wang%20and%20Tengtao%20Song%20and%20Qi%20Zhu%20and%20Jun%20Song%20and%20Zhiming%20Ding%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Although%20Vision%20Language%20Models%20%28VLMs%29%20exhibit%20strong%20perceptual%20abilities%0Aand%20impressive%20visual%20reasoning%2C%20they%20struggle%20with%20attention%20to%20detail%20and%0Aprecise%20action%20planning%20in%20complex%2C%20dynamic%20environments%2C%20leading%20to%20subpar%0Aperformance.%20Real-world%20tasks%20typically%20require%20complex%20interactions%2C%20advanced%0Aspatial%20reasoning%2C%20long-term%20planning%2C%20and%20continuous%20strategy%20refinement%2C%0Ausually%20necessitating%20understanding%20the%20physics%20rules%20of%20the%20target%20scenario.%0AHowever%2C%20evaluating%20these%20capabilities%20in%20real-world%20scenarios%20is%20often%0Aprohibitively%20expensive.%20To%20bridge%20this%20gap%2C%20we%20introduce%20DeepPHY%2C%20a%20novel%0Abenchmark%20framework%20designed%20to%20systematically%20evaluate%20VLMs%27%20understanding%20and%0Areasoning%20about%20fundamental%20physical%20principles%20through%20a%20series%20of%20challenging%0Asimulated%20environments.%20DeepPHY%20integrates%20multiple%20physical%20reasoning%0Aenvironments%20of%20varying%20difficulty%20levels%20and%20incorporates%20fine-grained%0Aevaluation%20metrics.%20Our%20evaluation%20finds%20that%20even%20state-of-the-art%20VLMs%0Astruggle%20to%20translate%20descriptive%20physical%20knowledge%20into%20precise%2C%20predictive%0Acontrol.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepPHY%253A%2520Benchmarking%2520Agentic%2520VLMs%2520on%2520Physical%2520Reasoning%26entry.906535625%3DXinrun%2520Xu%2520and%2520Pi%2520Bu%2520and%2520Ye%2520Wang%2520and%2520B%25C3%25B6rje%2520F.%2520Karlsson%2520and%2520Ziming%2520Wang%2520and%2520Tengtao%2520Song%2520and%2520Qi%2520Zhu%2520and%2520Jun%2520Song%2520and%2520Zhiming%2520Ding%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Although%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520exhibit%2520strong%2520perceptual%2520abilities%250Aand%2520impressive%2520visual%2520reasoning%252C%2520they%2520struggle%2520with%2520attention%2520to%2520detail%2520and%250Aprecise%2520action%2520planning%2520in%2520complex%252C%2520dynamic%2520environments%252C%2520leading%2520to%2520subpar%250Aperformance.%2520Real-world%2520tasks%2520typically%2520require%2520complex%2520interactions%252C%2520advanced%250Aspatial%2520reasoning%252C%2520long-term%2520planning%252C%2520and%2520continuous%2520strategy%2520refinement%252C%250Ausually%2520necessitating%2520understanding%2520the%2520physics%2520rules%2520of%2520the%2520target%2520scenario.%250AHowever%252C%2520evaluating%2520these%2520capabilities%2520in%2520real-world%2520scenarios%2520is%2520often%250Aprohibitively%2520expensive.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520DeepPHY%252C%2520a%2520novel%250Abenchmark%2520framework%2520designed%2520to%2520systematically%2520evaluate%2520VLMs%2527%2520understanding%2520and%250Areasoning%2520about%2520fundamental%2520physical%2520principles%2520through%2520a%2520series%2520of%2520challenging%250Asimulated%2520environments.%2520DeepPHY%2520integrates%2520multiple%2520physical%2520reasoning%250Aenvironments%2520of%2520varying%2520difficulty%2520levels%2520and%2520incorporates%2520fine-grained%250Aevaluation%2520metrics.%2520Our%2520evaluation%2520finds%2520that%2520even%2520state-of-the-art%2520VLMs%250Astruggle%2520to%2520translate%2520descriptive%2520physical%2520knowledge%2520into%2520precise%252C%2520predictive%250Acontrol.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepPHY%3A%20Benchmarking%20Agentic%20VLMs%20on%20Physical%20Reasoning&entry.906535625=Xinrun%20Xu%20and%20Pi%20Bu%20and%20Ye%20Wang%20and%20B%C3%B6rje%20F.%20Karlsson%20and%20Ziming%20Wang%20and%20Tengtao%20Song%20and%20Qi%20Zhu%20and%20Jun%20Song%20and%20Zhiming%20Ding%20and%20Bo%20Zheng&entry.1292438233=%20%20Although%20Vision%20Language%20Models%20%28VLMs%29%20exhibit%20strong%20perceptual%20abilities%0Aand%20impressive%20visual%20reasoning%2C%20they%20struggle%20with%20attention%20to%20detail%20and%0Aprecise%20action%20planning%20in%20complex%2C%20dynamic%20environments%2C%20leading%20to%20subpar%0Aperformance.%20Real-world%20tasks%20typically%20require%20complex%20interactions%2C%20advanced%0Aspatial%20reasoning%2C%20long-term%20planning%2C%20and%20continuous%20strategy%20refinement%2C%0Ausually%20necessitating%20understanding%20the%20physics%20rules%20of%20the%20target%20scenario.%0AHowever%2C%20evaluating%20these%20capabilities%20in%20real-world%20scenarios%20is%20often%0Aprohibitively%20expensive.%20To%20bridge%20this%20gap%2C%20we%20introduce%20DeepPHY%2C%20a%20novel%0Abenchmark%20framework%20designed%20to%20systematically%20evaluate%20VLMs%27%20understanding%20and%0Areasoning%20about%20fundamental%20physical%20principles%20through%20a%20series%20of%20challenging%0Asimulated%20environments.%20DeepPHY%20integrates%20multiple%20physical%20reasoning%0Aenvironments%20of%20varying%20difficulty%20levels%20and%20incorporates%20fine-grained%0Aevaluation%20metrics.%20Our%20evaluation%20finds%20that%20even%20state-of-the-art%20VLMs%0Astruggle%20to%20translate%20descriptive%20physical%20knowledge%20into%20precise%2C%20predictive%0Acontrol.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05405v1&entry.124074799=Read"},
{"title": "Di-NeRF: Distributed NeRF for Collaborative Learning with Relative Pose\n  Refinement", "author": "Mahboubeh Asadi and Kourosh Zareinia and Sajad Saeedi", "abstract": "  Collaborative mapping of unknown environments can be done faster and more\nrobustly than a single robot. However, a collaborative approach requires a\ndistributed paradigm to be scalable and deal with communication issues. This\nwork presents a fully distributed algorithm enabling a group of robots to\ncollectively optimize the parameters of a Neural Radiance Field (NeRF). The\nalgorithm involves the communication of each robot's trained NeRF parameters\nover a mesh network, where each robot trains its NeRF and has access to its own\nvisual data only. Additionally, the relative poses of all robots are jointly\noptimized alongside the model parameters, enabling mapping with less accurate\nrelative camera poses. We show that multi-robot systems can benefit from\ndifferentiable and robust 3D reconstruction optimized from multiple NeRFs.\nExperiments on real-world and synthetic data demonstrate the efficiency of the\nproposed algorithm. See the website of the project for videos of the\nexperiments and supplementary material\n(https://sites.google.com/view/di-nerf/home).\n", "link": "http://arxiv.org/abs/2402.01485v3", "date": "2025-08-07", "relevancy": 2.3222, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5888}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.576}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Di-NeRF%3A%20Distributed%20NeRF%20for%20Collaborative%20Learning%20with%20Relative%20Pose%0A%20%20Refinement&body=Title%3A%20Di-NeRF%3A%20Distributed%20NeRF%20for%20Collaborative%20Learning%20with%20Relative%20Pose%0A%20%20Refinement%0AAuthor%3A%20Mahboubeh%20Asadi%20and%20Kourosh%20Zareinia%20and%20Sajad%20Saeedi%0AAbstract%3A%20%20%20Collaborative%20mapping%20of%20unknown%20environments%20can%20be%20done%20faster%20and%20more%0Arobustly%20than%20a%20single%20robot.%20However%2C%20a%20collaborative%20approach%20requires%20a%0Adistributed%20paradigm%20to%20be%20scalable%20and%20deal%20with%20communication%20issues.%20This%0Awork%20presents%20a%20fully%20distributed%20algorithm%20enabling%20a%20group%20of%20robots%20to%0Acollectively%20optimize%20the%20parameters%20of%20a%20Neural%20Radiance%20Field%20%28NeRF%29.%20The%0Aalgorithm%20involves%20the%20communication%20of%20each%20robot%27s%20trained%20NeRF%20parameters%0Aover%20a%20mesh%20network%2C%20where%20each%20robot%20trains%20its%20NeRF%20and%20has%20access%20to%20its%20own%0Avisual%20data%20only.%20Additionally%2C%20the%20relative%20poses%20of%20all%20robots%20are%20jointly%0Aoptimized%20alongside%20the%20model%20parameters%2C%20enabling%20mapping%20with%20less%20accurate%0Arelative%20camera%20poses.%20We%20show%20that%20multi-robot%20systems%20can%20benefit%20from%0Adifferentiable%20and%20robust%203D%20reconstruction%20optimized%20from%20multiple%20NeRFs.%0AExperiments%20on%20real-world%20and%20synthetic%20data%20demonstrate%20the%20efficiency%20of%20the%0Aproposed%20algorithm.%20See%20the%20website%20of%20the%20project%20for%20videos%20of%20the%0Aexperiments%20and%20supplementary%20material%0A%28https%3A//sites.google.com/view/di-nerf/home%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01485v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDi-NeRF%253A%2520Distributed%2520NeRF%2520for%2520Collaborative%2520Learning%2520with%2520Relative%2520Pose%250A%2520%2520Refinement%26entry.906535625%3DMahboubeh%2520Asadi%2520and%2520Kourosh%2520Zareinia%2520and%2520Sajad%2520Saeedi%26entry.1292438233%3D%2520%2520Collaborative%2520mapping%2520of%2520unknown%2520environments%2520can%2520be%2520done%2520faster%2520and%2520more%250Arobustly%2520than%2520a%2520single%2520robot.%2520However%252C%2520a%2520collaborative%2520approach%2520requires%2520a%250Adistributed%2520paradigm%2520to%2520be%2520scalable%2520and%2520deal%2520with%2520communication%2520issues.%2520This%250Awork%2520presents%2520a%2520fully%2520distributed%2520algorithm%2520enabling%2520a%2520group%2520of%2520robots%2520to%250Acollectively%2520optimize%2520the%2520parameters%2520of%2520a%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529.%2520The%250Aalgorithm%2520involves%2520the%2520communication%2520of%2520each%2520robot%2527s%2520trained%2520NeRF%2520parameters%250Aover%2520a%2520mesh%2520network%252C%2520where%2520each%2520robot%2520trains%2520its%2520NeRF%2520and%2520has%2520access%2520to%2520its%2520own%250Avisual%2520data%2520only.%2520Additionally%252C%2520the%2520relative%2520poses%2520of%2520all%2520robots%2520are%2520jointly%250Aoptimized%2520alongside%2520the%2520model%2520parameters%252C%2520enabling%2520mapping%2520with%2520less%2520accurate%250Arelative%2520camera%2520poses.%2520We%2520show%2520that%2520multi-robot%2520systems%2520can%2520benefit%2520from%250Adifferentiable%2520and%2520robust%25203D%2520reconstruction%2520optimized%2520from%2520multiple%2520NeRFs.%250AExperiments%2520on%2520real-world%2520and%2520synthetic%2520data%2520demonstrate%2520the%2520efficiency%2520of%2520the%250Aproposed%2520algorithm.%2520See%2520the%2520website%2520of%2520the%2520project%2520for%2520videos%2520of%2520the%250Aexperiments%2520and%2520supplementary%2520material%250A%2528https%253A//sites.google.com/view/di-nerf/home%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01485v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Di-NeRF%3A%20Distributed%20NeRF%20for%20Collaborative%20Learning%20with%20Relative%20Pose%0A%20%20Refinement&entry.906535625=Mahboubeh%20Asadi%20and%20Kourosh%20Zareinia%20and%20Sajad%20Saeedi&entry.1292438233=%20%20Collaborative%20mapping%20of%20unknown%20environments%20can%20be%20done%20faster%20and%20more%0Arobustly%20than%20a%20single%20robot.%20However%2C%20a%20collaborative%20approach%20requires%20a%0Adistributed%20paradigm%20to%20be%20scalable%20and%20deal%20with%20communication%20issues.%20This%0Awork%20presents%20a%20fully%20distributed%20algorithm%20enabling%20a%20group%20of%20robots%20to%0Acollectively%20optimize%20the%20parameters%20of%20a%20Neural%20Radiance%20Field%20%28NeRF%29.%20The%0Aalgorithm%20involves%20the%20communication%20of%20each%20robot%27s%20trained%20NeRF%20parameters%0Aover%20a%20mesh%20network%2C%20where%20each%20robot%20trains%20its%20NeRF%20and%20has%20access%20to%20its%20own%0Avisual%20data%20only.%20Additionally%2C%20the%20relative%20poses%20of%20all%20robots%20are%20jointly%0Aoptimized%20alongside%20the%20model%20parameters%2C%20enabling%20mapping%20with%20less%20accurate%0Arelative%20camera%20poses.%20We%20show%20that%20multi-robot%20systems%20can%20benefit%20from%0Adifferentiable%20and%20robust%203D%20reconstruction%20optimized%20from%20multiple%20NeRFs.%0AExperiments%20on%20real-world%20and%20synthetic%20data%20demonstrate%20the%20efficiency%20of%20the%0Aproposed%20algorithm.%20See%20the%20website%20of%20the%20project%20for%20videos%20of%20the%0Aexperiments%20and%20supplementary%20material%0A%28https%3A//sites.google.com/view/di-nerf/home%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01485v3&entry.124074799=Read"},
{"title": "A Study of Gender Classification Techniques Based on Iris Images: A Deep\n  Survey and Analysis", "author": "Basna Mohammed Salih Hasan and Ramadhan J. Mstafa", "abstract": "  Gender classification is attractive in a range of applications, including\nsurveillance and monitoring, corporate profiling, and human-computer\ninteraction. Individuals' identities may be gleaned from information about\ntheir gender, which is a kind of soft biometric.Over the years, several methods\nfor determining a person's gender have been devised. Some of the most\nwell-known ones are based on physical characteristics like face, fingerprint,\npalmprint, DNA, ears, gait, and iris. On the other hand, facial features\naccount for the vast majority of gender classification methods. Also, the iris\nis a significant biometric trait because the iris, according to research,\nremains basically constant during an individual's life. Besides that, the iris\nis externally visible and is non-invasive to the user, which is important for\npractical applications. Furthermore, there are already high-quality methods for\nsegmenting and encoding iris images, and the current methods facilitate\nselecting and extracting attribute vectors from iris textures. This study\ndiscusses several approaches to determining gender. The previous works of\nliterature are briefly reviewed. Additionally, there are a variety of\nmethodologies for different steps of gender classification. This study provides\nresearchers with knowledge and analysis of the existing gender classification\napproaches. Also, it will assist researchers who are interested in this\nspecific area, as well as highlight the gaps and challenges in the field, and\nfinally provide suggestions and future paths for improvement.\n", "link": "http://arxiv.org/abs/2508.05246v1", "date": "2025-08-07", "relevancy": 2.3189, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4757}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4583}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20of%20Gender%20Classification%20Techniques%20Based%20on%20Iris%20Images%3A%20A%20Deep%0A%20%20Survey%20and%20Analysis&body=Title%3A%20A%20Study%20of%20Gender%20Classification%20Techniques%20Based%20on%20Iris%20Images%3A%20A%20Deep%0A%20%20Survey%20and%20Analysis%0AAuthor%3A%20Basna%20Mohammed%20Salih%20Hasan%20and%20Ramadhan%20J.%20Mstafa%0AAbstract%3A%20%20%20Gender%20classification%20is%20attractive%20in%20a%20range%20of%20applications%2C%20including%0Asurveillance%20and%20monitoring%2C%20corporate%20profiling%2C%20and%20human-computer%0Ainteraction.%20Individuals%27%20identities%20may%20be%20gleaned%20from%20information%20about%0Atheir%20gender%2C%20which%20is%20a%20kind%20of%20soft%20biometric.Over%20the%20years%2C%20several%20methods%0Afor%20determining%20a%20person%27s%20gender%20have%20been%20devised.%20Some%20of%20the%20most%0Awell-known%20ones%20are%20based%20on%20physical%20characteristics%20like%20face%2C%20fingerprint%2C%0Apalmprint%2C%20DNA%2C%20ears%2C%20gait%2C%20and%20iris.%20On%20the%20other%20hand%2C%20facial%20features%0Aaccount%20for%20the%20vast%20majority%20of%20gender%20classification%20methods.%20Also%2C%20the%20iris%0Ais%20a%20significant%20biometric%20trait%20because%20the%20iris%2C%20according%20to%20research%2C%0Aremains%20basically%20constant%20during%20an%20individual%27s%20life.%20Besides%20that%2C%20the%20iris%0Ais%20externally%20visible%20and%20is%20non-invasive%20to%20the%20user%2C%20which%20is%20important%20for%0Apractical%20applications.%20Furthermore%2C%20there%20are%20already%20high-quality%20methods%20for%0Asegmenting%20and%20encoding%20iris%20images%2C%20and%20the%20current%20methods%20facilitate%0Aselecting%20and%20extracting%20attribute%20vectors%20from%20iris%20textures.%20This%20study%0Adiscusses%20several%20approaches%20to%20determining%20gender.%20The%20previous%20works%20of%0Aliterature%20are%20briefly%20reviewed.%20Additionally%2C%20there%20are%20a%20variety%20of%0Amethodologies%20for%20different%20steps%20of%20gender%20classification.%20This%20study%20provides%0Aresearchers%20with%20knowledge%20and%20analysis%20of%20the%20existing%20gender%20classification%0Aapproaches.%20Also%2C%20it%20will%20assist%20researchers%20who%20are%20interested%20in%20this%0Aspecific%20area%2C%20as%20well%20as%20highlight%20the%20gaps%20and%20challenges%20in%20the%20field%2C%20and%0Afinally%20provide%20suggestions%20and%20future%20paths%20for%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520of%2520Gender%2520Classification%2520Techniques%2520Based%2520on%2520Iris%2520Images%253A%2520A%2520Deep%250A%2520%2520Survey%2520and%2520Analysis%26entry.906535625%3DBasna%2520Mohammed%2520Salih%2520Hasan%2520and%2520Ramadhan%2520J.%2520Mstafa%26entry.1292438233%3D%2520%2520Gender%2520classification%2520is%2520attractive%2520in%2520a%2520range%2520of%2520applications%252C%2520including%250Asurveillance%2520and%2520monitoring%252C%2520corporate%2520profiling%252C%2520and%2520human-computer%250Ainteraction.%2520Individuals%2527%2520identities%2520may%2520be%2520gleaned%2520from%2520information%2520about%250Atheir%2520gender%252C%2520which%2520is%2520a%2520kind%2520of%2520soft%2520biometric.Over%2520the%2520years%252C%2520several%2520methods%250Afor%2520determining%2520a%2520person%2527s%2520gender%2520have%2520been%2520devised.%2520Some%2520of%2520the%2520most%250Awell-known%2520ones%2520are%2520based%2520on%2520physical%2520characteristics%2520like%2520face%252C%2520fingerprint%252C%250Apalmprint%252C%2520DNA%252C%2520ears%252C%2520gait%252C%2520and%2520iris.%2520On%2520the%2520other%2520hand%252C%2520facial%2520features%250Aaccount%2520for%2520the%2520vast%2520majority%2520of%2520gender%2520classification%2520methods.%2520Also%252C%2520the%2520iris%250Ais%2520a%2520significant%2520biometric%2520trait%2520because%2520the%2520iris%252C%2520according%2520to%2520research%252C%250Aremains%2520basically%2520constant%2520during%2520an%2520individual%2527s%2520life.%2520Besides%2520that%252C%2520the%2520iris%250Ais%2520externally%2520visible%2520and%2520is%2520non-invasive%2520to%2520the%2520user%252C%2520which%2520is%2520important%2520for%250Apractical%2520applications.%2520Furthermore%252C%2520there%2520are%2520already%2520high-quality%2520methods%2520for%250Asegmenting%2520and%2520encoding%2520iris%2520images%252C%2520and%2520the%2520current%2520methods%2520facilitate%250Aselecting%2520and%2520extracting%2520attribute%2520vectors%2520from%2520iris%2520textures.%2520This%2520study%250Adiscusses%2520several%2520approaches%2520to%2520determining%2520gender.%2520The%2520previous%2520works%2520of%250Aliterature%2520are%2520briefly%2520reviewed.%2520Additionally%252C%2520there%2520are%2520a%2520variety%2520of%250Amethodologies%2520for%2520different%2520steps%2520of%2520gender%2520classification.%2520This%2520study%2520provides%250Aresearchers%2520with%2520knowledge%2520and%2520analysis%2520of%2520the%2520existing%2520gender%2520classification%250Aapproaches.%2520Also%252C%2520it%2520will%2520assist%2520researchers%2520who%2520are%2520interested%2520in%2520this%250Aspecific%2520area%252C%2520as%2520well%2520as%2520highlight%2520the%2520gaps%2520and%2520challenges%2520in%2520the%2520field%252C%2520and%250Afinally%2520provide%2520suggestions%2520and%2520future%2520paths%2520for%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20of%20Gender%20Classification%20Techniques%20Based%20on%20Iris%20Images%3A%20A%20Deep%0A%20%20Survey%20and%20Analysis&entry.906535625=Basna%20Mohammed%20Salih%20Hasan%20and%20Ramadhan%20J.%20Mstafa&entry.1292438233=%20%20Gender%20classification%20is%20attractive%20in%20a%20range%20of%20applications%2C%20including%0Asurveillance%20and%20monitoring%2C%20corporate%20profiling%2C%20and%20human-computer%0Ainteraction.%20Individuals%27%20identities%20may%20be%20gleaned%20from%20information%20about%0Atheir%20gender%2C%20which%20is%20a%20kind%20of%20soft%20biometric.Over%20the%20years%2C%20several%20methods%0Afor%20determining%20a%20person%27s%20gender%20have%20been%20devised.%20Some%20of%20the%20most%0Awell-known%20ones%20are%20based%20on%20physical%20characteristics%20like%20face%2C%20fingerprint%2C%0Apalmprint%2C%20DNA%2C%20ears%2C%20gait%2C%20and%20iris.%20On%20the%20other%20hand%2C%20facial%20features%0Aaccount%20for%20the%20vast%20majority%20of%20gender%20classification%20methods.%20Also%2C%20the%20iris%0Ais%20a%20significant%20biometric%20trait%20because%20the%20iris%2C%20according%20to%20research%2C%0Aremains%20basically%20constant%20during%20an%20individual%27s%20life.%20Besides%20that%2C%20the%20iris%0Ais%20externally%20visible%20and%20is%20non-invasive%20to%20the%20user%2C%20which%20is%20important%20for%0Apractical%20applications.%20Furthermore%2C%20there%20are%20already%20high-quality%20methods%20for%0Asegmenting%20and%20encoding%20iris%20images%2C%20and%20the%20current%20methods%20facilitate%0Aselecting%20and%20extracting%20attribute%20vectors%20from%20iris%20textures.%20This%20study%0Adiscusses%20several%20approaches%20to%20determining%20gender.%20The%20previous%20works%20of%0Aliterature%20are%20briefly%20reviewed.%20Additionally%2C%20there%20are%20a%20variety%20of%0Amethodologies%20for%20different%20steps%20of%20gender%20classification.%20This%20study%20provides%0Aresearchers%20with%20knowledge%20and%20analysis%20of%20the%20existing%20gender%20classification%0Aapproaches.%20Also%2C%20it%20will%20assist%20researchers%20who%20are%20interested%20in%20this%0Aspecific%20area%2C%20as%20well%20as%20highlight%20the%20gaps%20and%20challenges%20in%20the%20field%2C%20and%0Afinally%20provide%20suggestions%20and%20future%20paths%20for%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05246v1&entry.124074799=Read"},
{"title": "DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label\n  Recognition", "author": "Haijing Liu and Tao Pu and Hefeng Wu and Keze Wang and Liang Lin", "abstract": "  Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple\nseen and unseen object categories within an image, requiring both precise\nintra-class localization to pinpoint objects and effective inter-class\nreasoning to model complex category dependencies. While Vision-Language\nPre-training (VLP) models offer a strong open-vocabulary foundation, they often\nstruggle with fine-grained localization under weak supervision and typically\nfail to explicitly leverage structured relational knowledge beyond basic\nsemantics, limiting performance especially for unseen classes. To overcome\nthese limitations, we propose the Dual Adaptive Refinement Transfer (DART)\nframework. DART enhances a frozen VLP backbone via two synergistic adaptive\nmodules. For intra-class refinement, an Adaptive Refinement Module (ARM)\nrefines patch features adaptively, coupled with a novel Weakly Supervised Patch\nSelecting (WPS) loss that enables discriminative localization using only\nimage-level labels. Concurrently, for inter-class transfer, an Adaptive\nTransfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed\nusing structured knowledge mined from a Large Language Model (LLM), and employs\ngraph attention network to adaptively transfer relational information between\nclass representations. DART is the first framework, to our knowledge, to\nexplicitly integrate external LLM-derived relational knowledge for adaptive\ninter-class transfer while simultaneously performing adaptive intra-class\nrefinement under weak supervision for OV-MLR. Extensive experiments on\nchallenging benchmarks demonstrate that our DART achieves new state-of-the-art\nperformance, validating its effectiveness.\n", "link": "http://arxiv.org/abs/2508.05585v1", "date": "2025-08-07", "relevancy": 2.3118, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6206}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DART%3A%20Dual%20Adaptive%20Refinement%20Transfer%20for%20Open-Vocabulary%20Multi-Label%0A%20%20Recognition&body=Title%3A%20DART%3A%20Dual%20Adaptive%20Refinement%20Transfer%20for%20Open-Vocabulary%20Multi-Label%0A%20%20Recognition%0AAuthor%3A%20Haijing%20Liu%20and%20Tao%20Pu%20and%20Hefeng%20Wu%20and%20Keze%20Wang%20and%20Liang%20Lin%0AAbstract%3A%20%20%20Open-Vocabulary%20Multi-Label%20Recognition%20%28OV-MLR%29%20aims%20to%20identify%20multiple%0Aseen%20and%20unseen%20object%20categories%20within%20an%20image%2C%20requiring%20both%20precise%0Aintra-class%20localization%20to%20pinpoint%20objects%20and%20effective%20inter-class%0Areasoning%20to%20model%20complex%20category%20dependencies.%20While%20Vision-Language%0APre-training%20%28VLP%29%20models%20offer%20a%20strong%20open-vocabulary%20foundation%2C%20they%20often%0Astruggle%20with%20fine-grained%20localization%20under%20weak%20supervision%20and%20typically%0Afail%20to%20explicitly%20leverage%20structured%20relational%20knowledge%20beyond%20basic%0Asemantics%2C%20limiting%20performance%20especially%20for%20unseen%20classes.%20To%20overcome%0Athese%20limitations%2C%20we%20propose%20the%20Dual%20Adaptive%20Refinement%20Transfer%20%28DART%29%0Aframework.%20DART%20enhances%20a%20frozen%20VLP%20backbone%20via%20two%20synergistic%20adaptive%0Amodules.%20For%20intra-class%20refinement%2C%20an%20Adaptive%20Refinement%20Module%20%28ARM%29%0Arefines%20patch%20features%20adaptively%2C%20coupled%20with%20a%20novel%20Weakly%20Supervised%20Patch%0ASelecting%20%28WPS%29%20loss%20that%20enables%20discriminative%20localization%20using%20only%0Aimage-level%20labels.%20Concurrently%2C%20for%20inter-class%20transfer%2C%20an%20Adaptive%0ATransfer%20Module%20%28ATM%29%20leverages%20a%20Class%20Relationship%20Graph%20%28CRG%29%2C%20constructed%0Ausing%20structured%20knowledge%20mined%20from%20a%20Large%20Language%20Model%20%28LLM%29%2C%20and%20employs%0Agraph%20attention%20network%20to%20adaptively%20transfer%20relational%20information%20between%0Aclass%20representations.%20DART%20is%20the%20first%20framework%2C%20to%20our%20knowledge%2C%20to%0Aexplicitly%20integrate%20external%20LLM-derived%20relational%20knowledge%20for%20adaptive%0Ainter-class%20transfer%20while%20simultaneously%20performing%20adaptive%20intra-class%0Arefinement%20under%20weak%20supervision%20for%20OV-MLR.%20Extensive%20experiments%20on%0Achallenging%20benchmarks%20demonstrate%20that%20our%20DART%20achieves%20new%20state-of-the-art%0Aperformance%2C%20validating%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDART%253A%2520Dual%2520Adaptive%2520Refinement%2520Transfer%2520for%2520Open-Vocabulary%2520Multi-Label%250A%2520%2520Recognition%26entry.906535625%3DHaijing%2520Liu%2520and%2520Tao%2520Pu%2520and%2520Hefeng%2520Wu%2520and%2520Keze%2520Wang%2520and%2520Liang%2520Lin%26entry.1292438233%3D%2520%2520Open-Vocabulary%2520Multi-Label%2520Recognition%2520%2528OV-MLR%2529%2520aims%2520to%2520identify%2520multiple%250Aseen%2520and%2520unseen%2520object%2520categories%2520within%2520an%2520image%252C%2520requiring%2520both%2520precise%250Aintra-class%2520localization%2520to%2520pinpoint%2520objects%2520and%2520effective%2520inter-class%250Areasoning%2520to%2520model%2520complex%2520category%2520dependencies.%2520While%2520Vision-Language%250APre-training%2520%2528VLP%2529%2520models%2520offer%2520a%2520strong%2520open-vocabulary%2520foundation%252C%2520they%2520often%250Astruggle%2520with%2520fine-grained%2520localization%2520under%2520weak%2520supervision%2520and%2520typically%250Afail%2520to%2520explicitly%2520leverage%2520structured%2520relational%2520knowledge%2520beyond%2520basic%250Asemantics%252C%2520limiting%2520performance%2520especially%2520for%2520unseen%2520classes.%2520To%2520overcome%250Athese%2520limitations%252C%2520we%2520propose%2520the%2520Dual%2520Adaptive%2520Refinement%2520Transfer%2520%2528DART%2529%250Aframework.%2520DART%2520enhances%2520a%2520frozen%2520VLP%2520backbone%2520via%2520two%2520synergistic%2520adaptive%250Amodules.%2520For%2520intra-class%2520refinement%252C%2520an%2520Adaptive%2520Refinement%2520Module%2520%2528ARM%2529%250Arefines%2520patch%2520features%2520adaptively%252C%2520coupled%2520with%2520a%2520novel%2520Weakly%2520Supervised%2520Patch%250ASelecting%2520%2528WPS%2529%2520loss%2520that%2520enables%2520discriminative%2520localization%2520using%2520only%250Aimage-level%2520labels.%2520Concurrently%252C%2520for%2520inter-class%2520transfer%252C%2520an%2520Adaptive%250ATransfer%2520Module%2520%2528ATM%2529%2520leverages%2520a%2520Class%2520Relationship%2520Graph%2520%2528CRG%2529%252C%2520constructed%250Ausing%2520structured%2520knowledge%2520mined%2520from%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%252C%2520and%2520employs%250Agraph%2520attention%2520network%2520to%2520adaptively%2520transfer%2520relational%2520information%2520between%250Aclass%2520representations.%2520DART%2520is%2520the%2520first%2520framework%252C%2520to%2520our%2520knowledge%252C%2520to%250Aexplicitly%2520integrate%2520external%2520LLM-derived%2520relational%2520knowledge%2520for%2520adaptive%250Ainter-class%2520transfer%2520while%2520simultaneously%2520performing%2520adaptive%2520intra-class%250Arefinement%2520under%2520weak%2520supervision%2520for%2520OV-MLR.%2520Extensive%2520experiments%2520on%250Achallenging%2520benchmarks%2520demonstrate%2520that%2520our%2520DART%2520achieves%2520new%2520state-of-the-art%250Aperformance%252C%2520validating%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DART%3A%20Dual%20Adaptive%20Refinement%20Transfer%20for%20Open-Vocabulary%20Multi-Label%0A%20%20Recognition&entry.906535625=Haijing%20Liu%20and%20Tao%20Pu%20and%20Hefeng%20Wu%20and%20Keze%20Wang%20and%20Liang%20Lin&entry.1292438233=%20%20Open-Vocabulary%20Multi-Label%20Recognition%20%28OV-MLR%29%20aims%20to%20identify%20multiple%0Aseen%20and%20unseen%20object%20categories%20within%20an%20image%2C%20requiring%20both%20precise%0Aintra-class%20localization%20to%20pinpoint%20objects%20and%20effective%20inter-class%0Areasoning%20to%20model%20complex%20category%20dependencies.%20While%20Vision-Language%0APre-training%20%28VLP%29%20models%20offer%20a%20strong%20open-vocabulary%20foundation%2C%20they%20often%0Astruggle%20with%20fine-grained%20localization%20under%20weak%20supervision%20and%20typically%0Afail%20to%20explicitly%20leverage%20structured%20relational%20knowledge%20beyond%20basic%0Asemantics%2C%20limiting%20performance%20especially%20for%20unseen%20classes.%20To%20overcome%0Athese%20limitations%2C%20we%20propose%20the%20Dual%20Adaptive%20Refinement%20Transfer%20%28DART%29%0Aframework.%20DART%20enhances%20a%20frozen%20VLP%20backbone%20via%20two%20synergistic%20adaptive%0Amodules.%20For%20intra-class%20refinement%2C%20an%20Adaptive%20Refinement%20Module%20%28ARM%29%0Arefines%20patch%20features%20adaptively%2C%20coupled%20with%20a%20novel%20Weakly%20Supervised%20Patch%0ASelecting%20%28WPS%29%20loss%20that%20enables%20discriminative%20localization%20using%20only%0Aimage-level%20labels.%20Concurrently%2C%20for%20inter-class%20transfer%2C%20an%20Adaptive%0ATransfer%20Module%20%28ATM%29%20leverages%20a%20Class%20Relationship%20Graph%20%28CRG%29%2C%20constructed%0Ausing%20structured%20knowledge%20mined%20from%20a%20Large%20Language%20Model%20%28LLM%29%2C%20and%20employs%0Agraph%20attention%20network%20to%20adaptively%20transfer%20relational%20information%20between%0Aclass%20representations.%20DART%20is%20the%20first%20framework%2C%20to%20our%20knowledge%2C%20to%0Aexplicitly%20integrate%20external%20LLM-derived%20relational%20knowledge%20for%20adaptive%0Ainter-class%20transfer%20while%20simultaneously%20performing%20adaptive%20intra-class%0Arefinement%20under%20weak%20supervision%20for%20OV-MLR.%20Extensive%20experiments%20on%0Achallenging%20benchmarks%20demonstrate%20that%20our%20DART%20achieves%20new%20state-of-the-art%0Aperformance%2C%20validating%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05585v1&entry.124074799=Read"},
{"title": "StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable\n  Reward Models", "author": "Xiangxiang Zhang and Jingxuan Wei and Donghong Zhong and Qi Chen and Caijun Jia and Cheng Tan and Jinming Gu and Xiaobo Qin and Zhiping Liu and Liang Hu and Tong Sun and Yuchen Wu and Zewei Sun and Chenwei Lou and Hua Zheng and Tianyang Zhan and Changbao Wang and Shuangzhi Wu and Zefa Lin and Chang Guo and Sihang Yuan and Riwei Chen and Shixiong Zhao and Yingping Zhang and Gaowei Wu and Bihui Yu and Jiahui Wu and Zhehui Zhao and Qianqian Liu and Ruofeng Tang and Xingyue Huang and Bing Zhao and Mengyang Zhang and Youqiang Zhou", "abstract": "  Existing Vision-Language Models often struggle with complex, multi-question\nreasoning tasks where partial correctness is crucial for effective learning.\nTraditional reward mechanisms, which provide a single binary score for an\nentire response, are too coarse to guide models through intricate problems with\nmultiple sub-parts. To address this, we introduce StructVRM, a method that\naligns multimodal reasoning with Structured and Verifiable Reward Models. At\nits core is a model-based verifier trained to provide fine-grained,\nsub-question-level feedback, assessing semantic and mathematical equivalence\nrather than relying on rigid string matching. This allows for nuanced, partial\ncredit scoring in previously intractable problem formats. Extensive experiments\ndemonstrate the effectiveness of StructVRM. Our trained model, Seed-StructVRM,\nachieves state-of-the-art performance on six out of twelve public multimodal\nbenchmarks and our newly curated, high-difficulty STEM-Bench. The success of\nStructVRM validates that training with structured, verifiable rewards is a\nhighly effective approach for advancing the capabilities of multimodal models\nin complex, real-world reasoning domains.\n", "link": "http://arxiv.org/abs/2508.05383v1", "date": "2025-08-07", "relevancy": 2.3113, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.582}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StructVRM%3A%20Aligning%20Multimodal%20Reasoning%20with%20Structured%20and%20Verifiable%0A%20%20Reward%20Models&body=Title%3A%20StructVRM%3A%20Aligning%20Multimodal%20Reasoning%20with%20Structured%20and%20Verifiable%0A%20%20Reward%20Models%0AAuthor%3A%20Xiangxiang%20Zhang%20and%20Jingxuan%20Wei%20and%20Donghong%20Zhong%20and%20Qi%20Chen%20and%20Caijun%20Jia%20and%20Cheng%20Tan%20and%20Jinming%20Gu%20and%20Xiaobo%20Qin%20and%20Zhiping%20Liu%20and%20Liang%20Hu%20and%20Tong%20Sun%20and%20Yuchen%20Wu%20and%20Zewei%20Sun%20and%20Chenwei%20Lou%20and%20Hua%20Zheng%20and%20Tianyang%20Zhan%20and%20Changbao%20Wang%20and%20Shuangzhi%20Wu%20and%20Zefa%20Lin%20and%20Chang%20Guo%20and%20Sihang%20Yuan%20and%20Riwei%20Chen%20and%20Shixiong%20Zhao%20and%20Yingping%20Zhang%20and%20Gaowei%20Wu%20and%20Bihui%20Yu%20and%20Jiahui%20Wu%20and%20Zhehui%20Zhao%20and%20Qianqian%20Liu%20and%20Ruofeng%20Tang%20and%20Xingyue%20Huang%20and%20Bing%20Zhao%20and%20Mengyang%20Zhang%20and%20Youqiang%20Zhou%0AAbstract%3A%20%20%20Existing%20Vision-Language%20Models%20often%20struggle%20with%20complex%2C%20multi-question%0Areasoning%20tasks%20where%20partial%20correctness%20is%20crucial%20for%20effective%20learning.%0ATraditional%20reward%20mechanisms%2C%20which%20provide%20a%20single%20binary%20score%20for%20an%0Aentire%20response%2C%20are%20too%20coarse%20to%20guide%20models%20through%20intricate%20problems%20with%0Amultiple%20sub-parts.%20To%20address%20this%2C%20we%20introduce%20StructVRM%2C%20a%20method%20that%0Aaligns%20multimodal%20reasoning%20with%20Structured%20and%20Verifiable%20Reward%20Models.%20At%0Aits%20core%20is%20a%20model-based%20verifier%20trained%20to%20provide%20fine-grained%2C%0Asub-question-level%20feedback%2C%20assessing%20semantic%20and%20mathematical%20equivalence%0Arather%20than%20relying%20on%20rigid%20string%20matching.%20This%20allows%20for%20nuanced%2C%20partial%0Acredit%20scoring%20in%20previously%20intractable%20problem%20formats.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20StructVRM.%20Our%20trained%20model%2C%20Seed-StructVRM%2C%0Aachieves%20state-of-the-art%20performance%20on%20six%20out%20of%20twelve%20public%20multimodal%0Abenchmarks%20and%20our%20newly%20curated%2C%20high-difficulty%20STEM-Bench.%20The%20success%20of%0AStructVRM%20validates%20that%20training%20with%20structured%2C%20verifiable%20rewards%20is%20a%0Ahighly%20effective%20approach%20for%20advancing%20the%20capabilities%20of%20multimodal%20models%0Ain%20complex%2C%20real-world%20reasoning%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructVRM%253A%2520Aligning%2520Multimodal%2520Reasoning%2520with%2520Structured%2520and%2520Verifiable%250A%2520%2520Reward%2520Models%26entry.906535625%3DXiangxiang%2520Zhang%2520and%2520Jingxuan%2520Wei%2520and%2520Donghong%2520Zhong%2520and%2520Qi%2520Chen%2520and%2520Caijun%2520Jia%2520and%2520Cheng%2520Tan%2520and%2520Jinming%2520Gu%2520and%2520Xiaobo%2520Qin%2520and%2520Zhiping%2520Liu%2520and%2520Liang%2520Hu%2520and%2520Tong%2520Sun%2520and%2520Yuchen%2520Wu%2520and%2520Zewei%2520Sun%2520and%2520Chenwei%2520Lou%2520and%2520Hua%2520Zheng%2520and%2520Tianyang%2520Zhan%2520and%2520Changbao%2520Wang%2520and%2520Shuangzhi%2520Wu%2520and%2520Zefa%2520Lin%2520and%2520Chang%2520Guo%2520and%2520Sihang%2520Yuan%2520and%2520Riwei%2520Chen%2520and%2520Shixiong%2520Zhao%2520and%2520Yingping%2520Zhang%2520and%2520Gaowei%2520Wu%2520and%2520Bihui%2520Yu%2520and%2520Jiahui%2520Wu%2520and%2520Zhehui%2520Zhao%2520and%2520Qianqian%2520Liu%2520and%2520Ruofeng%2520Tang%2520and%2520Xingyue%2520Huang%2520and%2520Bing%2520Zhao%2520and%2520Mengyang%2520Zhang%2520and%2520Youqiang%2520Zhou%26entry.1292438233%3D%2520%2520Existing%2520Vision-Language%2520Models%2520often%2520struggle%2520with%2520complex%252C%2520multi-question%250Areasoning%2520tasks%2520where%2520partial%2520correctness%2520is%2520crucial%2520for%2520effective%2520learning.%250ATraditional%2520reward%2520mechanisms%252C%2520which%2520provide%2520a%2520single%2520binary%2520score%2520for%2520an%250Aentire%2520response%252C%2520are%2520too%2520coarse%2520to%2520guide%2520models%2520through%2520intricate%2520problems%2520with%250Amultiple%2520sub-parts.%2520To%2520address%2520this%252C%2520we%2520introduce%2520StructVRM%252C%2520a%2520method%2520that%250Aaligns%2520multimodal%2520reasoning%2520with%2520Structured%2520and%2520Verifiable%2520Reward%2520Models.%2520At%250Aits%2520core%2520is%2520a%2520model-based%2520verifier%2520trained%2520to%2520provide%2520fine-grained%252C%250Asub-question-level%2520feedback%252C%2520assessing%2520semantic%2520and%2520mathematical%2520equivalence%250Arather%2520than%2520relying%2520on%2520rigid%2520string%2520matching.%2520This%2520allows%2520for%2520nuanced%252C%2520partial%250Acredit%2520scoring%2520in%2520previously%2520intractable%2520problem%2520formats.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520StructVRM.%2520Our%2520trained%2520model%252C%2520Seed-StructVRM%252C%250Aachieves%2520state-of-the-art%2520performance%2520on%2520six%2520out%2520of%2520twelve%2520public%2520multimodal%250Abenchmarks%2520and%2520our%2520newly%2520curated%252C%2520high-difficulty%2520STEM-Bench.%2520The%2520success%2520of%250AStructVRM%2520validates%2520that%2520training%2520with%2520structured%252C%2520verifiable%2520rewards%2520is%2520a%250Ahighly%2520effective%2520approach%2520for%2520advancing%2520the%2520capabilities%2520of%2520multimodal%2520models%250Ain%2520complex%252C%2520real-world%2520reasoning%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StructVRM%3A%20Aligning%20Multimodal%20Reasoning%20with%20Structured%20and%20Verifiable%0A%20%20Reward%20Models&entry.906535625=Xiangxiang%20Zhang%20and%20Jingxuan%20Wei%20and%20Donghong%20Zhong%20and%20Qi%20Chen%20and%20Caijun%20Jia%20and%20Cheng%20Tan%20and%20Jinming%20Gu%20and%20Xiaobo%20Qin%20and%20Zhiping%20Liu%20and%20Liang%20Hu%20and%20Tong%20Sun%20and%20Yuchen%20Wu%20and%20Zewei%20Sun%20and%20Chenwei%20Lou%20and%20Hua%20Zheng%20and%20Tianyang%20Zhan%20and%20Changbao%20Wang%20and%20Shuangzhi%20Wu%20and%20Zefa%20Lin%20and%20Chang%20Guo%20and%20Sihang%20Yuan%20and%20Riwei%20Chen%20and%20Shixiong%20Zhao%20and%20Yingping%20Zhang%20and%20Gaowei%20Wu%20and%20Bihui%20Yu%20and%20Jiahui%20Wu%20and%20Zhehui%20Zhao%20and%20Qianqian%20Liu%20and%20Ruofeng%20Tang%20and%20Xingyue%20Huang%20and%20Bing%20Zhao%20and%20Mengyang%20Zhang%20and%20Youqiang%20Zhou&entry.1292438233=%20%20Existing%20Vision-Language%20Models%20often%20struggle%20with%20complex%2C%20multi-question%0Areasoning%20tasks%20where%20partial%20correctness%20is%20crucial%20for%20effective%20learning.%0ATraditional%20reward%20mechanisms%2C%20which%20provide%20a%20single%20binary%20score%20for%20an%0Aentire%20response%2C%20are%20too%20coarse%20to%20guide%20models%20through%20intricate%20problems%20with%0Amultiple%20sub-parts.%20To%20address%20this%2C%20we%20introduce%20StructVRM%2C%20a%20method%20that%0Aaligns%20multimodal%20reasoning%20with%20Structured%20and%20Verifiable%20Reward%20Models.%20At%0Aits%20core%20is%20a%20model-based%20verifier%20trained%20to%20provide%20fine-grained%2C%0Asub-question-level%20feedback%2C%20assessing%20semantic%20and%20mathematical%20equivalence%0Arather%20than%20relying%20on%20rigid%20string%20matching.%20This%20allows%20for%20nuanced%2C%20partial%0Acredit%20scoring%20in%20previously%20intractable%20problem%20formats.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20StructVRM.%20Our%20trained%20model%2C%20Seed-StructVRM%2C%0Aachieves%20state-of-the-art%20performance%20on%20six%20out%20of%20twelve%20public%20multimodal%0Abenchmarks%20and%20our%20newly%20curated%2C%20high-difficulty%20STEM-Bench.%20The%20success%20of%0AStructVRM%20validates%20that%20training%20with%20structured%2C%20verifiable%20rewards%20is%20a%0Ahighly%20effective%20approach%20for%20advancing%20the%20capabilities%20of%20multimodal%20models%0Ain%20complex%2C%20real-world%20reasoning%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05383v1&entry.124074799=Read"},
{"title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and\n  Vision", "author": "Luozheng Qin and Jia Gong and Yuqing Sun and Tianjiao Li and Mengping Yang and Xiaomeng Yang and Chao Qu and Zhiyu Tan and Hao Li", "abstract": "  Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/\n", "link": "http://arxiv.org/abs/2508.05606v1", "date": "2025-08-07", "relevancy": 2.3013, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5768}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5768}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uni-cot%3A%20Towards%20Unified%20Chain-of-Thought%20Reasoning%20Across%20Text%20and%0A%20%20Vision&body=Title%3A%20Uni-cot%3A%20Towards%20Unified%20Chain-of-Thought%20Reasoning%20Across%20Text%20and%0A%20%20Vision%0AAuthor%3A%20Luozheng%20Qin%20and%20Jia%20Gong%20and%20Yuqing%20Sun%20and%20Tianjiao%20Li%20and%20Mengping%20Yang%20and%20Xiaomeng%20Yang%20and%20Chao%20Qu%20and%20Zhiyu%20Tan%20and%20Hao%20Li%0AAbstract%3A%20%20%20Chain-of-Thought%20%28CoT%29%20reasoning%20has%20been%20widely%20adopted%20to%20enhance%20Large%0ALanguage%20Models%20%28LLMs%29%20by%20decomposing%20complex%20tasks%20into%20simpler%2C%20sequential%0Asubtasks.%20However%2C%20extending%20CoT%20to%20vision-language%20reasoning%20tasks%20remains%0Achallenging%2C%20as%20it%20often%20requires%20interpreting%20transitions%20of%20visual%20states%20to%0Asupport%20reasoning.%20Existing%20methods%20often%20struggle%20with%20this%20due%20to%20limited%0Acapacity%20of%20modeling%20visual%20state%20transitions%20or%20incoherent%20visual%20trajectories%0Acaused%20by%20fragmented%20architectures.%0A%20%20To%20overcome%20these%20limitations%2C%20we%20propose%20Uni-CoT%2C%20a%20Unified%20Chain-of-Thought%0Aframework%20that%20enables%20coherent%20and%20grounded%20multimodal%20reasoning%20within%20a%0Asingle%20unified%20model.%20The%20key%20idea%20is%20to%20leverage%20a%20model%20capable%20of%20both%20image%0Aunderstanding%20and%20generation%20to%20reason%20over%20visual%20content%20and%20model%20evolving%0Avisual%20states.%20However%2C%20empowering%20a%20unified%20model%20to%20achieve%20that%20is%0Anon-trivial%2C%20given%20the%20high%20computational%20cost%20and%20the%20burden%20of%20training.%20To%0Aaddress%20this%2C%20Uni-CoT%20introduces%20a%20novel%20two-level%20reasoning%20paradigm%3A%20A%0AMacro-Level%20CoT%20for%20high-level%20task%20planning%20and%20A%20Micro-Level%20CoT%20for%20subtask%0Aexecution.%20This%20design%20significantly%20reduces%20the%20computational%20overhead.%0AFurthermore%2C%20we%20introduce%20a%20structured%20training%20paradigm%20that%20combines%0Ainterleaved%20image-text%20supervision%20for%20macro-level%20CoT%20with%20multi-task%0Aobjectives%20for%20micro-level%20CoT.%20Together%2C%20these%20innovations%20allow%20Uni-CoT%20to%0Aperform%20scalable%20and%20coherent%20multi-modal%20reasoning.%20Furthermore%2C%20thanks%20to%20our%0Adesign%2C%20all%20experiments%20can%20be%20efficiently%20completed%20using%20only%208%20A100%20GPUs%0Awith%2080GB%20VRAM%20each.%20Experimental%20results%20on%20reasoning-driven%20image%20generation%0Abenchmark%20%28WISE%29%20and%20editing%20benchmarks%20%28RISE%20and%20KRIS%29%20indicates%20that%20Uni-CoT%0Ademonstrates%20SOTA%20performance%20and%20strong%20generalization%2C%20establishing%20Uni-CoT%0Aas%20a%20promising%20solution%20for%20multi-modal%20reasoning.%20Project%20Page%20and%20Code%3A%0Ahttps%3A//sais-fuxi.github.io/projects/uni-cot/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUni-cot%253A%2520Towards%2520Unified%2520Chain-of-Thought%2520Reasoning%2520Across%2520Text%2520and%250A%2520%2520Vision%26entry.906535625%3DLuozheng%2520Qin%2520and%2520Jia%2520Gong%2520and%2520Yuqing%2520Sun%2520and%2520Tianjiao%2520Li%2520and%2520Mengping%2520Yang%2520and%2520Xiaomeng%2520Yang%2520and%2520Chao%2520Qu%2520and%2520Zhiyu%2520Tan%2520and%2520Hao%2520Li%26entry.1292438233%3D%2520%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520has%2520been%2520widely%2520adopted%2520to%2520enhance%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520by%2520decomposing%2520complex%2520tasks%2520into%2520simpler%252C%2520sequential%250Asubtasks.%2520However%252C%2520extending%2520CoT%2520to%2520vision-language%2520reasoning%2520tasks%2520remains%250Achallenging%252C%2520as%2520it%2520often%2520requires%2520interpreting%2520transitions%2520of%2520visual%2520states%2520to%250Asupport%2520reasoning.%2520Existing%2520methods%2520often%2520struggle%2520with%2520this%2520due%2520to%2520limited%250Acapacity%2520of%2520modeling%2520visual%2520state%2520transitions%2520or%2520incoherent%2520visual%2520trajectories%250Acaused%2520by%2520fragmented%2520architectures.%250A%2520%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Uni-CoT%252C%2520a%2520Unified%2520Chain-of-Thought%250Aframework%2520that%2520enables%2520coherent%2520and%2520grounded%2520multimodal%2520reasoning%2520within%2520a%250Asingle%2520unified%2520model.%2520The%2520key%2520idea%2520is%2520to%2520leverage%2520a%2520model%2520capable%2520of%2520both%2520image%250Aunderstanding%2520and%2520generation%2520to%2520reason%2520over%2520visual%2520content%2520and%2520model%2520evolving%250Avisual%2520states.%2520However%252C%2520empowering%2520a%2520unified%2520model%2520to%2520achieve%2520that%2520is%250Anon-trivial%252C%2520given%2520the%2520high%2520computational%2520cost%2520and%2520the%2520burden%2520of%2520training.%2520To%250Aaddress%2520this%252C%2520Uni-CoT%2520introduces%2520a%2520novel%2520two-level%2520reasoning%2520paradigm%253A%2520A%250AMacro-Level%2520CoT%2520for%2520high-level%2520task%2520planning%2520and%2520A%2520Micro-Level%2520CoT%2520for%2520subtask%250Aexecution.%2520This%2520design%2520significantly%2520reduces%2520the%2520computational%2520overhead.%250AFurthermore%252C%2520we%2520introduce%2520a%2520structured%2520training%2520paradigm%2520that%2520combines%250Ainterleaved%2520image-text%2520supervision%2520for%2520macro-level%2520CoT%2520with%2520multi-task%250Aobjectives%2520for%2520micro-level%2520CoT.%2520Together%252C%2520these%2520innovations%2520allow%2520Uni-CoT%2520to%250Aperform%2520scalable%2520and%2520coherent%2520multi-modal%2520reasoning.%2520Furthermore%252C%2520thanks%2520to%2520our%250Adesign%252C%2520all%2520experiments%2520can%2520be%2520efficiently%2520completed%2520using%2520only%25208%2520A100%2520GPUs%250Awith%252080GB%2520VRAM%2520each.%2520Experimental%2520results%2520on%2520reasoning-driven%2520image%2520generation%250Abenchmark%2520%2528WISE%2529%2520and%2520editing%2520benchmarks%2520%2528RISE%2520and%2520KRIS%2529%2520indicates%2520that%2520Uni-CoT%250Ademonstrates%2520SOTA%2520performance%2520and%2520strong%2520generalization%252C%2520establishing%2520Uni-CoT%250Aas%2520a%2520promising%2520solution%2520for%2520multi-modal%2520reasoning.%2520Project%2520Page%2520and%2520Code%253A%250Ahttps%253A//sais-fuxi.github.io/projects/uni-cot/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uni-cot%3A%20Towards%20Unified%20Chain-of-Thought%20Reasoning%20Across%20Text%20and%0A%20%20Vision&entry.906535625=Luozheng%20Qin%20and%20Jia%20Gong%20and%20Yuqing%20Sun%20and%20Tianjiao%20Li%20and%20Mengping%20Yang%20and%20Xiaomeng%20Yang%20and%20Chao%20Qu%20and%20Zhiyu%20Tan%20and%20Hao%20Li&entry.1292438233=%20%20Chain-of-Thought%20%28CoT%29%20reasoning%20has%20been%20widely%20adopted%20to%20enhance%20Large%0ALanguage%20Models%20%28LLMs%29%20by%20decomposing%20complex%20tasks%20into%20simpler%2C%20sequential%0Asubtasks.%20However%2C%20extending%20CoT%20to%20vision-language%20reasoning%20tasks%20remains%0Achallenging%2C%20as%20it%20often%20requires%20interpreting%20transitions%20of%20visual%20states%20to%0Asupport%20reasoning.%20Existing%20methods%20often%20struggle%20with%20this%20due%20to%20limited%0Acapacity%20of%20modeling%20visual%20state%20transitions%20or%20incoherent%20visual%20trajectories%0Acaused%20by%20fragmented%20architectures.%0A%20%20To%20overcome%20these%20limitations%2C%20we%20propose%20Uni-CoT%2C%20a%20Unified%20Chain-of-Thought%0Aframework%20that%20enables%20coherent%20and%20grounded%20multimodal%20reasoning%20within%20a%0Asingle%20unified%20model.%20The%20key%20idea%20is%20to%20leverage%20a%20model%20capable%20of%20both%20image%0Aunderstanding%20and%20generation%20to%20reason%20over%20visual%20content%20and%20model%20evolving%0Avisual%20states.%20However%2C%20empowering%20a%20unified%20model%20to%20achieve%20that%20is%0Anon-trivial%2C%20given%20the%20high%20computational%20cost%20and%20the%20burden%20of%20training.%20To%0Aaddress%20this%2C%20Uni-CoT%20introduces%20a%20novel%20two-level%20reasoning%20paradigm%3A%20A%0AMacro-Level%20CoT%20for%20high-level%20task%20planning%20and%20A%20Micro-Level%20CoT%20for%20subtask%0Aexecution.%20This%20design%20significantly%20reduces%20the%20computational%20overhead.%0AFurthermore%2C%20we%20introduce%20a%20structured%20training%20paradigm%20that%20combines%0Ainterleaved%20image-text%20supervision%20for%20macro-level%20CoT%20with%20multi-task%0Aobjectives%20for%20micro-level%20CoT.%20Together%2C%20these%20innovations%20allow%20Uni-CoT%20to%0Aperform%20scalable%20and%20coherent%20multi-modal%20reasoning.%20Furthermore%2C%20thanks%20to%20our%0Adesign%2C%20all%20experiments%20can%20be%20efficiently%20completed%20using%20only%208%20A100%20GPUs%0Awith%2080GB%20VRAM%20each.%20Experimental%20results%20on%20reasoning-driven%20image%20generation%0Abenchmark%20%28WISE%29%20and%20editing%20benchmarks%20%28RISE%20and%20KRIS%29%20indicates%20that%20Uni-CoT%0Ademonstrates%20SOTA%20performance%20and%20strong%20generalization%2C%20establishing%20Uni-CoT%0Aas%20a%20promising%20solution%20for%20multi-modal%20reasoning.%20Project%20Page%20and%20Code%3A%0Ahttps%3A//sais-fuxi.github.io/projects/uni-cot/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05606v1&entry.124074799=Read"},
{"title": "TSPO: Temporal Sampling Policy Optimization for Long-form Video Language\n  Understanding", "author": "Canhui Tang and Zifan Han and Hongbo Sun and Sanping Zhou and Xuchong Zhang and Xin Wei and Ye Yuan and Jinglin Xu and Hao Sun", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in vision-language tasks, yet they still face challenges when\nprocessing long-duration video inputs. The limitation arises from MLLMs'\ncontext limit and training costs, necessitating sparse frame sampling before\nfeeding videos into MLLMs. Existing video MLLMs adopt training-free uniform\nsampling or keyframe search, which may miss critical events or be constrained\nby the pre-trained models' event understanding capabilities. Meanwhile,\nbuilding a training-based method remains challenging due to the unsupervised\nand non-differentiable nature of sparse frame sampling. To address these\nproblems, we propose Temporal Sampling Policy Optimization (TSPO), advancing\nMLLMs' long-form video-language understanding via reinforcement learning.\nSpecifically, we first propose a trainable event-aware temporal agent, which\ncaptures event-query correlation for performing probabilistic keyframe\nselection. Then, we propose the TSPO reinforcement learning paradigm, which\nmodels keyframe selection and language generation as a joint decision-making\nprocess, enabling end-to-end group relative optimization with efficient\nrule-based rewards. Furthermore, for the TSPO's training, we propose a long\nvideo training data construction pipeline with comprehensive temporal data and\nvideo Needle-in-a-Haystack data. Finally, we incorporate rule-based answering\naccuracy and temporal locating reward mechanisms to optimize the temporal\nsampling policy. Comprehensive experiments show that our TSPO achieves\nstate-of-the-art performance across multiple long video understanding\nbenchmarks, and shows transferable ability across different cutting-edge\nVideo-MLLMs. Our code is available at https://github.com/Hui-design/TSPO\n", "link": "http://arxiv.org/abs/2508.04369v2", "date": "2025-08-07", "relevancy": 2.2814, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5839}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5709}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSPO%3A%20Temporal%20Sampling%20Policy%20Optimization%20for%20Long-form%20Video%20Language%0A%20%20Understanding&body=Title%3A%20TSPO%3A%20Temporal%20Sampling%20Policy%20Optimization%20for%20Long-form%20Video%20Language%0A%20%20Understanding%0AAuthor%3A%20Canhui%20Tang%20and%20Zifan%20Han%20and%20Hongbo%20Sun%20and%20Sanping%20Zhou%20and%20Xuchong%20Zhang%20and%20Xin%20Wei%20and%20Ye%20Yuan%20and%20Jinglin%20Xu%20and%20Hao%20Sun%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aprogress%20in%20vision-language%20tasks%2C%20yet%20they%20still%20face%20challenges%20when%0Aprocessing%20long-duration%20video%20inputs.%20The%20limitation%20arises%20from%20MLLMs%27%0Acontext%20limit%20and%20training%20costs%2C%20necessitating%20sparse%20frame%20sampling%20before%0Afeeding%20videos%20into%20MLLMs.%20Existing%20video%20MLLMs%20adopt%20training-free%20uniform%0Asampling%20or%20keyframe%20search%2C%20which%20may%20miss%20critical%20events%20or%20be%20constrained%0Aby%20the%20pre-trained%20models%27%20event%20understanding%20capabilities.%20Meanwhile%2C%0Abuilding%20a%20training-based%20method%20remains%20challenging%20due%20to%20the%20unsupervised%0Aand%20non-differentiable%20nature%20of%20sparse%20frame%20sampling.%20To%20address%20these%0Aproblems%2C%20we%20propose%20Temporal%20Sampling%20Policy%20Optimization%20%28TSPO%29%2C%20advancing%0AMLLMs%27%20long-form%20video-language%20understanding%20via%20reinforcement%20learning.%0ASpecifically%2C%20we%20first%20propose%20a%20trainable%20event-aware%20temporal%20agent%2C%20which%0Acaptures%20event-query%20correlation%20for%20performing%20probabilistic%20keyframe%0Aselection.%20Then%2C%20we%20propose%20the%20TSPO%20reinforcement%20learning%20paradigm%2C%20which%0Amodels%20keyframe%20selection%20and%20language%20generation%20as%20a%20joint%20decision-making%0Aprocess%2C%20enabling%20end-to-end%20group%20relative%20optimization%20with%20efficient%0Arule-based%20rewards.%20Furthermore%2C%20for%20the%20TSPO%27s%20training%2C%20we%20propose%20a%20long%0Avideo%20training%20data%20construction%20pipeline%20with%20comprehensive%20temporal%20data%20and%0Avideo%20Needle-in-a-Haystack%20data.%20Finally%2C%20we%20incorporate%20rule-based%20answering%0Aaccuracy%20and%20temporal%20locating%20reward%20mechanisms%20to%20optimize%20the%20temporal%0Asampling%20policy.%20Comprehensive%20experiments%20show%20that%20our%20TSPO%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20long%20video%20understanding%0Abenchmarks%2C%20and%20shows%20transferable%20ability%20across%20different%20cutting-edge%0AVideo-MLLMs.%20Our%20code%20is%20available%20at%20https%3A//github.com/Hui-design/TSPO%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04369v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSPO%253A%2520Temporal%2520Sampling%2520Policy%2520Optimization%2520for%2520Long-form%2520Video%2520Language%250A%2520%2520Understanding%26entry.906535625%3DCanhui%2520Tang%2520and%2520Zifan%2520Han%2520and%2520Hongbo%2520Sun%2520and%2520Sanping%2520Zhou%2520and%2520Xuchong%2520Zhang%2520and%2520Xin%2520Wei%2520and%2520Ye%2520Yuan%2520and%2520Jinglin%2520Xu%2520and%2520Hao%2520Sun%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520significant%250Aprogress%2520in%2520vision-language%2520tasks%252C%2520yet%2520they%2520still%2520face%2520challenges%2520when%250Aprocessing%2520long-duration%2520video%2520inputs.%2520The%2520limitation%2520arises%2520from%2520MLLMs%2527%250Acontext%2520limit%2520and%2520training%2520costs%252C%2520necessitating%2520sparse%2520frame%2520sampling%2520before%250Afeeding%2520videos%2520into%2520MLLMs.%2520Existing%2520video%2520MLLMs%2520adopt%2520training-free%2520uniform%250Asampling%2520or%2520keyframe%2520search%252C%2520which%2520may%2520miss%2520critical%2520events%2520or%2520be%2520constrained%250Aby%2520the%2520pre-trained%2520models%2527%2520event%2520understanding%2520capabilities.%2520Meanwhile%252C%250Abuilding%2520a%2520training-based%2520method%2520remains%2520challenging%2520due%2520to%2520the%2520unsupervised%250Aand%2520non-differentiable%2520nature%2520of%2520sparse%2520frame%2520sampling.%2520To%2520address%2520these%250Aproblems%252C%2520we%2520propose%2520Temporal%2520Sampling%2520Policy%2520Optimization%2520%2528TSPO%2529%252C%2520advancing%250AMLLMs%2527%2520long-form%2520video-language%2520understanding%2520via%2520reinforcement%2520learning.%250ASpecifically%252C%2520we%2520first%2520propose%2520a%2520trainable%2520event-aware%2520temporal%2520agent%252C%2520which%250Acaptures%2520event-query%2520correlation%2520for%2520performing%2520probabilistic%2520keyframe%250Aselection.%2520Then%252C%2520we%2520propose%2520the%2520TSPO%2520reinforcement%2520learning%2520paradigm%252C%2520which%250Amodels%2520keyframe%2520selection%2520and%2520language%2520generation%2520as%2520a%2520joint%2520decision-making%250Aprocess%252C%2520enabling%2520end-to-end%2520group%2520relative%2520optimization%2520with%2520efficient%250Arule-based%2520rewards.%2520Furthermore%252C%2520for%2520the%2520TSPO%2527s%2520training%252C%2520we%2520propose%2520a%2520long%250Avideo%2520training%2520data%2520construction%2520pipeline%2520with%2520comprehensive%2520temporal%2520data%2520and%250Avideo%2520Needle-in-a-Haystack%2520data.%2520Finally%252C%2520we%2520incorporate%2520rule-based%2520answering%250Aaccuracy%2520and%2520temporal%2520locating%2520reward%2520mechanisms%2520to%2520optimize%2520the%2520temporal%250Asampling%2520policy.%2520Comprehensive%2520experiments%2520show%2520that%2520our%2520TSPO%2520achieves%250Astate-of-the-art%2520performance%2520across%2520multiple%2520long%2520video%2520understanding%250Abenchmarks%252C%2520and%2520shows%2520transferable%2520ability%2520across%2520different%2520cutting-edge%250AVideo-MLLMs.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Hui-design/TSPO%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04369v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSPO%3A%20Temporal%20Sampling%20Policy%20Optimization%20for%20Long-form%20Video%20Language%0A%20%20Understanding&entry.906535625=Canhui%20Tang%20and%20Zifan%20Han%20and%20Hongbo%20Sun%20and%20Sanping%20Zhou%20and%20Xuchong%20Zhang%20and%20Xin%20Wei%20and%20Ye%20Yuan%20and%20Jinglin%20Xu%20and%20Hao%20Sun&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aprogress%20in%20vision-language%20tasks%2C%20yet%20they%20still%20face%20challenges%20when%0Aprocessing%20long-duration%20video%20inputs.%20The%20limitation%20arises%20from%20MLLMs%27%0Acontext%20limit%20and%20training%20costs%2C%20necessitating%20sparse%20frame%20sampling%20before%0Afeeding%20videos%20into%20MLLMs.%20Existing%20video%20MLLMs%20adopt%20training-free%20uniform%0Asampling%20or%20keyframe%20search%2C%20which%20may%20miss%20critical%20events%20or%20be%20constrained%0Aby%20the%20pre-trained%20models%27%20event%20understanding%20capabilities.%20Meanwhile%2C%0Abuilding%20a%20training-based%20method%20remains%20challenging%20due%20to%20the%20unsupervised%0Aand%20non-differentiable%20nature%20of%20sparse%20frame%20sampling.%20To%20address%20these%0Aproblems%2C%20we%20propose%20Temporal%20Sampling%20Policy%20Optimization%20%28TSPO%29%2C%20advancing%0AMLLMs%27%20long-form%20video-language%20understanding%20via%20reinforcement%20learning.%0ASpecifically%2C%20we%20first%20propose%20a%20trainable%20event-aware%20temporal%20agent%2C%20which%0Acaptures%20event-query%20correlation%20for%20performing%20probabilistic%20keyframe%0Aselection.%20Then%2C%20we%20propose%20the%20TSPO%20reinforcement%20learning%20paradigm%2C%20which%0Amodels%20keyframe%20selection%20and%20language%20generation%20as%20a%20joint%20decision-making%0Aprocess%2C%20enabling%20end-to-end%20group%20relative%20optimization%20with%20efficient%0Arule-based%20rewards.%20Furthermore%2C%20for%20the%20TSPO%27s%20training%2C%20we%20propose%20a%20long%0Avideo%20training%20data%20construction%20pipeline%20with%20comprehensive%20temporal%20data%20and%0Avideo%20Needle-in-a-Haystack%20data.%20Finally%2C%20we%20incorporate%20rule-based%20answering%0Aaccuracy%20and%20temporal%20locating%20reward%20mechanisms%20to%20optimize%20the%20temporal%0Asampling%20policy.%20Comprehensive%20experiments%20show%20that%20our%20TSPO%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20long%20video%20understanding%0Abenchmarks%2C%20and%20shows%20transferable%20ability%20across%20different%20cutting-edge%0AVideo-MLLMs.%20Our%20code%20is%20available%20at%20https%3A//github.com/Hui-design/TSPO%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04369v2&entry.124074799=Read"},
{"title": "Head Anchor Enhanced Detection and Association for Crowded Pedestrian\n  Tracking", "author": "Zewei Wu and C\u00e9sar Teixeira and Wei Ke and Zhang Xiong", "abstract": "  Visual pedestrian tracking represents a promising research field, with\nextensive applications in intelligent surveillance, behavior analysis, and\nhuman-computer interaction. However, real-world applications face significant\nocclusion challenges. When multiple pedestrians interact or overlap, the loss\nof target features severely compromises the tracker's ability to maintain\nstable trajectories. Traditional tracking methods, which typically rely on\nfull-body bounding box features extracted from {Re-ID} models and linear\nconstant-velocity motion assumptions, often struggle in severe occlusion\nscenarios. To address these limitations, this work proposes an enhanced\ntracking framework that leverages richer feature representations and a more\nrobust motion model. Specifically, the proposed method incorporates detection\nfeatures from both the regression and classification branches of an object\ndetector, embedding spatial and positional information directly into the\nfeature representations. To further mitigate occlusion challenges, a head\nkeypoint detection model is introduced, as the head is less prone to occlusion\ncompared to the full body. In terms of motion modeling, we propose an iterative\nKalman filtering approach designed to align with modern detector assumptions,\nintegrating 3D priors to better complete motion trajectories in complex scenes.\nBy combining these advancements in appearance and motion modeling, the proposed\nmethod offers a more robust solution for multi-object tracking in crowded\nenvironments where occlusions are prevalent.\n", "link": "http://arxiv.org/abs/2508.05514v1", "date": "2025-08-07", "relevancy": 2.2805, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5731}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5702}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Head%20Anchor%20Enhanced%20Detection%20and%20Association%20for%20Crowded%20Pedestrian%0A%20%20Tracking&body=Title%3A%20Head%20Anchor%20Enhanced%20Detection%20and%20Association%20for%20Crowded%20Pedestrian%0A%20%20Tracking%0AAuthor%3A%20Zewei%20Wu%20and%20C%C3%A9sar%20Teixeira%20and%20Wei%20Ke%20and%20Zhang%20Xiong%0AAbstract%3A%20%20%20Visual%20pedestrian%20tracking%20represents%20a%20promising%20research%20field%2C%20with%0Aextensive%20applications%20in%20intelligent%20surveillance%2C%20behavior%20analysis%2C%20and%0Ahuman-computer%20interaction.%20However%2C%20real-world%20applications%20face%20significant%0Aocclusion%20challenges.%20When%20multiple%20pedestrians%20interact%20or%20overlap%2C%20the%20loss%0Aof%20target%20features%20severely%20compromises%20the%20tracker%27s%20ability%20to%20maintain%0Astable%20trajectories.%20Traditional%20tracking%20methods%2C%20which%20typically%20rely%20on%0Afull-body%20bounding%20box%20features%20extracted%20from%20%7BRe-ID%7D%20models%20and%20linear%0Aconstant-velocity%20motion%20assumptions%2C%20often%20struggle%20in%20severe%20occlusion%0Ascenarios.%20To%20address%20these%20limitations%2C%20this%20work%20proposes%20an%20enhanced%0Atracking%20framework%20that%20leverages%20richer%20feature%20representations%20and%20a%20more%0Arobust%20motion%20model.%20Specifically%2C%20the%20proposed%20method%20incorporates%20detection%0Afeatures%20from%20both%20the%20regression%20and%20classification%20branches%20of%20an%20object%0Adetector%2C%20embedding%20spatial%20and%20positional%20information%20directly%20into%20the%0Afeature%20representations.%20To%20further%20mitigate%20occlusion%20challenges%2C%20a%20head%0Akeypoint%20detection%20model%20is%20introduced%2C%20as%20the%20head%20is%20less%20prone%20to%20occlusion%0Acompared%20to%20the%20full%20body.%20In%20terms%20of%20motion%20modeling%2C%20we%20propose%20an%20iterative%0AKalman%20filtering%20approach%20designed%20to%20align%20with%20modern%20detector%20assumptions%2C%0Aintegrating%203D%20priors%20to%20better%20complete%20motion%20trajectories%20in%20complex%20scenes.%0ABy%20combining%20these%20advancements%20in%20appearance%20and%20motion%20modeling%2C%20the%20proposed%0Amethod%20offers%20a%20more%20robust%20solution%20for%20multi-object%20tracking%20in%20crowded%0Aenvironments%20where%20occlusions%20are%20prevalent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHead%2520Anchor%2520Enhanced%2520Detection%2520and%2520Association%2520for%2520Crowded%2520Pedestrian%250A%2520%2520Tracking%26entry.906535625%3DZewei%2520Wu%2520and%2520C%25C3%25A9sar%2520Teixeira%2520and%2520Wei%2520Ke%2520and%2520Zhang%2520Xiong%26entry.1292438233%3D%2520%2520Visual%2520pedestrian%2520tracking%2520represents%2520a%2520promising%2520research%2520field%252C%2520with%250Aextensive%2520applications%2520in%2520intelligent%2520surveillance%252C%2520behavior%2520analysis%252C%2520and%250Ahuman-computer%2520interaction.%2520However%252C%2520real-world%2520applications%2520face%2520significant%250Aocclusion%2520challenges.%2520When%2520multiple%2520pedestrians%2520interact%2520or%2520overlap%252C%2520the%2520loss%250Aof%2520target%2520features%2520severely%2520compromises%2520the%2520tracker%2527s%2520ability%2520to%2520maintain%250Astable%2520trajectories.%2520Traditional%2520tracking%2520methods%252C%2520which%2520typically%2520rely%2520on%250Afull-body%2520bounding%2520box%2520features%2520extracted%2520from%2520%257BRe-ID%257D%2520models%2520and%2520linear%250Aconstant-velocity%2520motion%2520assumptions%252C%2520often%2520struggle%2520in%2520severe%2520occlusion%250Ascenarios.%2520To%2520address%2520these%2520limitations%252C%2520this%2520work%2520proposes%2520an%2520enhanced%250Atracking%2520framework%2520that%2520leverages%2520richer%2520feature%2520representations%2520and%2520a%2520more%250Arobust%2520motion%2520model.%2520Specifically%252C%2520the%2520proposed%2520method%2520incorporates%2520detection%250Afeatures%2520from%2520both%2520the%2520regression%2520and%2520classification%2520branches%2520of%2520an%2520object%250Adetector%252C%2520embedding%2520spatial%2520and%2520positional%2520information%2520directly%2520into%2520the%250Afeature%2520representations.%2520To%2520further%2520mitigate%2520occlusion%2520challenges%252C%2520a%2520head%250Akeypoint%2520detection%2520model%2520is%2520introduced%252C%2520as%2520the%2520head%2520is%2520less%2520prone%2520to%2520occlusion%250Acompared%2520to%2520the%2520full%2520body.%2520In%2520terms%2520of%2520motion%2520modeling%252C%2520we%2520propose%2520an%2520iterative%250AKalman%2520filtering%2520approach%2520designed%2520to%2520align%2520with%2520modern%2520detector%2520assumptions%252C%250Aintegrating%25203D%2520priors%2520to%2520better%2520complete%2520motion%2520trajectories%2520in%2520complex%2520scenes.%250ABy%2520combining%2520these%2520advancements%2520in%2520appearance%2520and%2520motion%2520modeling%252C%2520the%2520proposed%250Amethod%2520offers%2520a%2520more%2520robust%2520solution%2520for%2520multi-object%2520tracking%2520in%2520crowded%250Aenvironments%2520where%2520occlusions%2520are%2520prevalent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Head%20Anchor%20Enhanced%20Detection%20and%20Association%20for%20Crowded%20Pedestrian%0A%20%20Tracking&entry.906535625=Zewei%20Wu%20and%20C%C3%A9sar%20Teixeira%20and%20Wei%20Ke%20and%20Zhang%20Xiong&entry.1292438233=%20%20Visual%20pedestrian%20tracking%20represents%20a%20promising%20research%20field%2C%20with%0Aextensive%20applications%20in%20intelligent%20surveillance%2C%20behavior%20analysis%2C%20and%0Ahuman-computer%20interaction.%20However%2C%20real-world%20applications%20face%20significant%0Aocclusion%20challenges.%20When%20multiple%20pedestrians%20interact%20or%20overlap%2C%20the%20loss%0Aof%20target%20features%20severely%20compromises%20the%20tracker%27s%20ability%20to%20maintain%0Astable%20trajectories.%20Traditional%20tracking%20methods%2C%20which%20typically%20rely%20on%0Afull-body%20bounding%20box%20features%20extracted%20from%20%7BRe-ID%7D%20models%20and%20linear%0Aconstant-velocity%20motion%20assumptions%2C%20often%20struggle%20in%20severe%20occlusion%0Ascenarios.%20To%20address%20these%20limitations%2C%20this%20work%20proposes%20an%20enhanced%0Atracking%20framework%20that%20leverages%20richer%20feature%20representations%20and%20a%20more%0Arobust%20motion%20model.%20Specifically%2C%20the%20proposed%20method%20incorporates%20detection%0Afeatures%20from%20both%20the%20regression%20and%20classification%20branches%20of%20an%20object%0Adetector%2C%20embedding%20spatial%20and%20positional%20information%20directly%20into%20the%0Afeature%20representations.%20To%20further%20mitigate%20occlusion%20challenges%2C%20a%20head%0Akeypoint%20detection%20model%20is%20introduced%2C%20as%20the%20head%20is%20less%20prone%20to%20occlusion%0Acompared%20to%20the%20full%20body.%20In%20terms%20of%20motion%20modeling%2C%20we%20propose%20an%20iterative%0AKalman%20filtering%20approach%20designed%20to%20align%20with%20modern%20detector%20assumptions%2C%0Aintegrating%203D%20priors%20to%20better%20complete%20motion%20trajectories%20in%20complex%20scenes.%0ABy%20combining%20these%20advancements%20in%20appearance%20and%20motion%20modeling%2C%20the%20proposed%0Amethod%20offers%20a%20more%20robust%20solution%20for%20multi-object%20tracking%20in%20crowded%0Aenvironments%20where%20occlusions%20are%20prevalent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05514v1&entry.124074799=Read"},
{"title": "CWEFS: Brain volume conduction effects inspired channel-wise EEG feature\n  selection for multi-dimensional emotion recognition", "author": "Xueyuan Xu and Wenjia Dong and Fulin Wei and Li Zhuo", "abstract": "  Due to the intracranial volume conduction effects, high-dimensional\nmulti-channel electroencephalography (EEG) features often contain substantial\nredundant and irrelevant information. This issue not only hinders the\nextraction of discriminative emotional representations but also compromises the\nreal-time performance. Feature selection has been established as an effective\napproach to address the challenges while enhancing the transparency and\ninterpretability of emotion recognition models. However, existing EEG feature\nselection research overlooks the influence of latent EEG feature structures on\nemotional label correlations and assumes uniform importance across various\nchannels, directly limiting the precise construction of EEG feature selection\nmodels for multi-dimensional affective computing. To address these limitations,\na novel channel-wise EEG feature selection (CWEFS) method is proposed for\nmulti-dimensional emotion recognition. Specifically, inspired by brain volume\nconduction effects, CWEFS integrates EEG emotional feature selection into a\nshared latent structure model designed to construct a consensus latent space\nacross diverse EEG channels. To preserve the local geometric structure, this\nconsensus space is further integrated with the latent semantic analysis of\nmulti-dimensional emotional labels. Additionally, CWEFS incorporates adaptive\nchannel-weight learning to automatically determine the significance of\ndifferent EEG channels in the emotional feature selection task. The\neffectiveness of CWEFS was validated using three popular EEG datasets with\nmulti-dimensional emotional labels. Comprehensive experimental results,\ncompared against nineteen feature selection methods, demonstrate that the EEG\nfeature subsets chosen by CWEFS achieve optimal emotion recognition performance\nacross six evaluation metrics.\n", "link": "http://arxiv.org/abs/2508.05228v1", "date": "2025-08-07", "relevancy": 2.2741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CWEFS%3A%20Brain%20volume%20conduction%20effects%20inspired%20channel-wise%20EEG%20feature%0A%20%20selection%20for%20multi-dimensional%20emotion%20recognition&body=Title%3A%20CWEFS%3A%20Brain%20volume%20conduction%20effects%20inspired%20channel-wise%20EEG%20feature%0A%20%20selection%20for%20multi-dimensional%20emotion%20recognition%0AAuthor%3A%20Xueyuan%20Xu%20and%20Wenjia%20Dong%20and%20Fulin%20Wei%20and%20Li%20Zhuo%0AAbstract%3A%20%20%20Due%20to%20the%20intracranial%20volume%20conduction%20effects%2C%20high-dimensional%0Amulti-channel%20electroencephalography%20%28EEG%29%20features%20often%20contain%20substantial%0Aredundant%20and%20irrelevant%20information.%20This%20issue%20not%20only%20hinders%20the%0Aextraction%20of%20discriminative%20emotional%20representations%20but%20also%20compromises%20the%0Areal-time%20performance.%20Feature%20selection%20has%20been%20established%20as%20an%20effective%0Aapproach%20to%20address%20the%20challenges%20while%20enhancing%20the%20transparency%20and%0Ainterpretability%20of%20emotion%20recognition%20models.%20However%2C%20existing%20EEG%20feature%0Aselection%20research%20overlooks%20the%20influence%20of%20latent%20EEG%20feature%20structures%20on%0Aemotional%20label%20correlations%20and%20assumes%20uniform%20importance%20across%20various%0Achannels%2C%20directly%20limiting%20the%20precise%20construction%20of%20EEG%20feature%20selection%0Amodels%20for%20multi-dimensional%20affective%20computing.%20To%20address%20these%20limitations%2C%0Aa%20novel%20channel-wise%20EEG%20feature%20selection%20%28CWEFS%29%20method%20is%20proposed%20for%0Amulti-dimensional%20emotion%20recognition.%20Specifically%2C%20inspired%20by%20brain%20volume%0Aconduction%20effects%2C%20CWEFS%20integrates%20EEG%20emotional%20feature%20selection%20into%20a%0Ashared%20latent%20structure%20model%20designed%20to%20construct%20a%20consensus%20latent%20space%0Aacross%20diverse%20EEG%20channels.%20To%20preserve%20the%20local%20geometric%20structure%2C%20this%0Aconsensus%20space%20is%20further%20integrated%20with%20the%20latent%20semantic%20analysis%20of%0Amulti-dimensional%20emotional%20labels.%20Additionally%2C%20CWEFS%20incorporates%20adaptive%0Achannel-weight%20learning%20to%20automatically%20determine%20the%20significance%20of%0Adifferent%20EEG%20channels%20in%20the%20emotional%20feature%20selection%20task.%20The%0Aeffectiveness%20of%20CWEFS%20was%20validated%20using%20three%20popular%20EEG%20datasets%20with%0Amulti-dimensional%20emotional%20labels.%20Comprehensive%20experimental%20results%2C%0Acompared%20against%20nineteen%20feature%20selection%20methods%2C%20demonstrate%20that%20the%20EEG%0Afeature%20subsets%20chosen%20by%20CWEFS%20achieve%20optimal%20emotion%20recognition%20performance%0Aacross%20six%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCWEFS%253A%2520Brain%2520volume%2520conduction%2520effects%2520inspired%2520channel-wise%2520EEG%2520feature%250A%2520%2520selection%2520for%2520multi-dimensional%2520emotion%2520recognition%26entry.906535625%3DXueyuan%2520Xu%2520and%2520Wenjia%2520Dong%2520and%2520Fulin%2520Wei%2520and%2520Li%2520Zhuo%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520intracranial%2520volume%2520conduction%2520effects%252C%2520high-dimensional%250Amulti-channel%2520electroencephalography%2520%2528EEG%2529%2520features%2520often%2520contain%2520substantial%250Aredundant%2520and%2520irrelevant%2520information.%2520This%2520issue%2520not%2520only%2520hinders%2520the%250Aextraction%2520of%2520discriminative%2520emotional%2520representations%2520but%2520also%2520compromises%2520the%250Areal-time%2520performance.%2520Feature%2520selection%2520has%2520been%2520established%2520as%2520an%2520effective%250Aapproach%2520to%2520address%2520the%2520challenges%2520while%2520enhancing%2520the%2520transparency%2520and%250Ainterpretability%2520of%2520emotion%2520recognition%2520models.%2520However%252C%2520existing%2520EEG%2520feature%250Aselection%2520research%2520overlooks%2520the%2520influence%2520of%2520latent%2520EEG%2520feature%2520structures%2520on%250Aemotional%2520label%2520correlations%2520and%2520assumes%2520uniform%2520importance%2520across%2520various%250Achannels%252C%2520directly%2520limiting%2520the%2520precise%2520construction%2520of%2520EEG%2520feature%2520selection%250Amodels%2520for%2520multi-dimensional%2520affective%2520computing.%2520To%2520address%2520these%2520limitations%252C%250Aa%2520novel%2520channel-wise%2520EEG%2520feature%2520selection%2520%2528CWEFS%2529%2520method%2520is%2520proposed%2520for%250Amulti-dimensional%2520emotion%2520recognition.%2520Specifically%252C%2520inspired%2520by%2520brain%2520volume%250Aconduction%2520effects%252C%2520CWEFS%2520integrates%2520EEG%2520emotional%2520feature%2520selection%2520into%2520a%250Ashared%2520latent%2520structure%2520model%2520designed%2520to%2520construct%2520a%2520consensus%2520latent%2520space%250Aacross%2520diverse%2520EEG%2520channels.%2520To%2520preserve%2520the%2520local%2520geometric%2520structure%252C%2520this%250Aconsensus%2520space%2520is%2520further%2520integrated%2520with%2520the%2520latent%2520semantic%2520analysis%2520of%250Amulti-dimensional%2520emotional%2520labels.%2520Additionally%252C%2520CWEFS%2520incorporates%2520adaptive%250Achannel-weight%2520learning%2520to%2520automatically%2520determine%2520the%2520significance%2520of%250Adifferent%2520EEG%2520channels%2520in%2520the%2520emotional%2520feature%2520selection%2520task.%2520The%250Aeffectiveness%2520of%2520CWEFS%2520was%2520validated%2520using%2520three%2520popular%2520EEG%2520datasets%2520with%250Amulti-dimensional%2520emotional%2520labels.%2520Comprehensive%2520experimental%2520results%252C%250Acompared%2520against%2520nineteen%2520feature%2520selection%2520methods%252C%2520demonstrate%2520that%2520the%2520EEG%250Afeature%2520subsets%2520chosen%2520by%2520CWEFS%2520achieve%2520optimal%2520emotion%2520recognition%2520performance%250Aacross%2520six%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CWEFS%3A%20Brain%20volume%20conduction%20effects%20inspired%20channel-wise%20EEG%20feature%0A%20%20selection%20for%20multi-dimensional%20emotion%20recognition&entry.906535625=Xueyuan%20Xu%20and%20Wenjia%20Dong%20and%20Fulin%20Wei%20and%20Li%20Zhuo&entry.1292438233=%20%20Due%20to%20the%20intracranial%20volume%20conduction%20effects%2C%20high-dimensional%0Amulti-channel%20electroencephalography%20%28EEG%29%20features%20often%20contain%20substantial%0Aredundant%20and%20irrelevant%20information.%20This%20issue%20not%20only%20hinders%20the%0Aextraction%20of%20discriminative%20emotional%20representations%20but%20also%20compromises%20the%0Areal-time%20performance.%20Feature%20selection%20has%20been%20established%20as%20an%20effective%0Aapproach%20to%20address%20the%20challenges%20while%20enhancing%20the%20transparency%20and%0Ainterpretability%20of%20emotion%20recognition%20models.%20However%2C%20existing%20EEG%20feature%0Aselection%20research%20overlooks%20the%20influence%20of%20latent%20EEG%20feature%20structures%20on%0Aemotional%20label%20correlations%20and%20assumes%20uniform%20importance%20across%20various%0Achannels%2C%20directly%20limiting%20the%20precise%20construction%20of%20EEG%20feature%20selection%0Amodels%20for%20multi-dimensional%20affective%20computing.%20To%20address%20these%20limitations%2C%0Aa%20novel%20channel-wise%20EEG%20feature%20selection%20%28CWEFS%29%20method%20is%20proposed%20for%0Amulti-dimensional%20emotion%20recognition.%20Specifically%2C%20inspired%20by%20brain%20volume%0Aconduction%20effects%2C%20CWEFS%20integrates%20EEG%20emotional%20feature%20selection%20into%20a%0Ashared%20latent%20structure%20model%20designed%20to%20construct%20a%20consensus%20latent%20space%0Aacross%20diverse%20EEG%20channels.%20To%20preserve%20the%20local%20geometric%20structure%2C%20this%0Aconsensus%20space%20is%20further%20integrated%20with%20the%20latent%20semantic%20analysis%20of%0Amulti-dimensional%20emotional%20labels.%20Additionally%2C%20CWEFS%20incorporates%20adaptive%0Achannel-weight%20learning%20to%20automatically%20determine%20the%20significance%20of%0Adifferent%20EEG%20channels%20in%20the%20emotional%20feature%20selection%20task.%20The%0Aeffectiveness%20of%20CWEFS%20was%20validated%20using%20three%20popular%20EEG%20datasets%20with%0Amulti-dimensional%20emotional%20labels.%20Comprehensive%20experimental%20results%2C%0Acompared%20against%20nineteen%20feature%20selection%20methods%2C%20demonstrate%20that%20the%20EEG%0Afeature%20subsets%20chosen%20by%20CWEFS%20achieve%20optimal%20emotion%20recognition%20performance%0Aacross%20six%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05228v1&entry.124074799=Read"},
{"title": "Minimal Model Reasoning in Description Logics: Don't Try This at Home!", "author": "Federica Di Stefano and Quentin Mani\u00e8re and Magdalena Ortiz and Mantas \u0160imkus", "abstract": "  Reasoning with minimal models has always been at the core of many knowledge\nrepresentation techniques, but we still have only a limited understanding of\nthis problem in Description Logics (DLs). Minimization of some selected\npredicates, letting the remaining predicates vary or be fixed, as proposed in\ncircumscription, has been explored and exhibits high complexity. The case of\n`pure' minimal models, where the extension of all predicates must be minimal,\nhas remained largely uncharted. We address this problem in popular DLs and\nobtain surprisingly negative results: concept satisfiability in minimal models\nis undecidable already for $\\mathcal{EL}$. This undecidability also extends to\na very restricted fragment of tuple-generating dependencies. To regain\ndecidability, we impose acyclicity conditions on the TBox that bring the\nworst-case complexity below double exponential time and allow us to establish a\nconnection with the recently studied pointwise circumscription; we also derive\nresults in data complexity. We conclude with a brief excursion to the DL-Lite\nfamily, where a positive result was known for DL-Lite$_{\\text{core}}$, but our\ninvestigation establishes ExpSpace-hardness already for its extension\nDL-Lite$_{\\text{horn}}$.\n", "link": "http://arxiv.org/abs/2508.05350v1", "date": "2025-08-07", "relevancy": 2.2631, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4641}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4641}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimal%20Model%20Reasoning%20in%20Description%20Logics%3A%20Don%27t%20Try%20This%20at%20Home%21&body=Title%3A%20Minimal%20Model%20Reasoning%20in%20Description%20Logics%3A%20Don%27t%20Try%20This%20at%20Home%21%0AAuthor%3A%20Federica%20Di%20Stefano%20and%20Quentin%20Mani%C3%A8re%20and%20Magdalena%20Ortiz%20and%20Mantas%20%C5%A0imkus%0AAbstract%3A%20%20%20Reasoning%20with%20minimal%20models%20has%20always%20been%20at%20the%20core%20of%20many%20knowledge%0Arepresentation%20techniques%2C%20but%20we%20still%20have%20only%20a%20limited%20understanding%20of%0Athis%20problem%20in%20Description%20Logics%20%28DLs%29.%20Minimization%20of%20some%20selected%0Apredicates%2C%20letting%20the%20remaining%20predicates%20vary%20or%20be%20fixed%2C%20as%20proposed%20in%0Acircumscription%2C%20has%20been%20explored%20and%20exhibits%20high%20complexity.%20The%20case%20of%0A%60pure%27%20minimal%20models%2C%20where%20the%20extension%20of%20all%20predicates%20must%20be%20minimal%2C%0Ahas%20remained%20largely%20uncharted.%20We%20address%20this%20problem%20in%20popular%20DLs%20and%0Aobtain%20surprisingly%20negative%20results%3A%20concept%20satisfiability%20in%20minimal%20models%0Ais%20undecidable%20already%20for%20%24%5Cmathcal%7BEL%7D%24.%20This%20undecidability%20also%20extends%20to%0Aa%20very%20restricted%20fragment%20of%20tuple-generating%20dependencies.%20To%20regain%0Adecidability%2C%20we%20impose%20acyclicity%20conditions%20on%20the%20TBox%20that%20bring%20the%0Aworst-case%20complexity%20below%20double%20exponential%20time%20and%20allow%20us%20to%20establish%20a%0Aconnection%20with%20the%20recently%20studied%20pointwise%20circumscription%3B%20we%20also%20derive%0Aresults%20in%20data%20complexity.%20We%20conclude%20with%20a%20brief%20excursion%20to%20the%20DL-Lite%0Afamily%2C%20where%20a%20positive%20result%20was%20known%20for%20DL-Lite%24_%7B%5Ctext%7Bcore%7D%7D%24%2C%20but%20our%0Ainvestigation%20establishes%20ExpSpace-hardness%20already%20for%20its%20extension%0ADL-Lite%24_%7B%5Ctext%7Bhorn%7D%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimal%2520Model%2520Reasoning%2520in%2520Description%2520Logics%253A%2520Don%2527t%2520Try%2520This%2520at%2520Home%2521%26entry.906535625%3DFederica%2520Di%2520Stefano%2520and%2520Quentin%2520Mani%25C3%25A8re%2520and%2520Magdalena%2520Ortiz%2520and%2520Mantas%2520%25C5%25A0imkus%26entry.1292438233%3D%2520%2520Reasoning%2520with%2520minimal%2520models%2520has%2520always%2520been%2520at%2520the%2520core%2520of%2520many%2520knowledge%250Arepresentation%2520techniques%252C%2520but%2520we%2520still%2520have%2520only%2520a%2520limited%2520understanding%2520of%250Athis%2520problem%2520in%2520Description%2520Logics%2520%2528DLs%2529.%2520Minimization%2520of%2520some%2520selected%250Apredicates%252C%2520letting%2520the%2520remaining%2520predicates%2520vary%2520or%2520be%2520fixed%252C%2520as%2520proposed%2520in%250Acircumscription%252C%2520has%2520been%2520explored%2520and%2520exhibits%2520high%2520complexity.%2520The%2520case%2520of%250A%2560pure%2527%2520minimal%2520models%252C%2520where%2520the%2520extension%2520of%2520all%2520predicates%2520must%2520be%2520minimal%252C%250Ahas%2520remained%2520largely%2520uncharted.%2520We%2520address%2520this%2520problem%2520in%2520popular%2520DLs%2520and%250Aobtain%2520surprisingly%2520negative%2520results%253A%2520concept%2520satisfiability%2520in%2520minimal%2520models%250Ais%2520undecidable%2520already%2520for%2520%2524%255Cmathcal%257BEL%257D%2524.%2520This%2520undecidability%2520also%2520extends%2520to%250Aa%2520very%2520restricted%2520fragment%2520of%2520tuple-generating%2520dependencies.%2520To%2520regain%250Adecidability%252C%2520we%2520impose%2520acyclicity%2520conditions%2520on%2520the%2520TBox%2520that%2520bring%2520the%250Aworst-case%2520complexity%2520below%2520double%2520exponential%2520time%2520and%2520allow%2520us%2520to%2520establish%2520a%250Aconnection%2520with%2520the%2520recently%2520studied%2520pointwise%2520circumscription%253B%2520we%2520also%2520derive%250Aresults%2520in%2520data%2520complexity.%2520We%2520conclude%2520with%2520a%2520brief%2520excursion%2520to%2520the%2520DL-Lite%250Afamily%252C%2520where%2520a%2520positive%2520result%2520was%2520known%2520for%2520DL-Lite%2524_%257B%255Ctext%257Bcore%257D%257D%2524%252C%2520but%2520our%250Ainvestigation%2520establishes%2520ExpSpace-hardness%2520already%2520for%2520its%2520extension%250ADL-Lite%2524_%257B%255Ctext%257Bhorn%257D%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimal%20Model%20Reasoning%20in%20Description%20Logics%3A%20Don%27t%20Try%20This%20at%20Home%21&entry.906535625=Federica%20Di%20Stefano%20and%20Quentin%20Mani%C3%A8re%20and%20Magdalena%20Ortiz%20and%20Mantas%20%C5%A0imkus&entry.1292438233=%20%20Reasoning%20with%20minimal%20models%20has%20always%20been%20at%20the%20core%20of%20many%20knowledge%0Arepresentation%20techniques%2C%20but%20we%20still%20have%20only%20a%20limited%20understanding%20of%0Athis%20problem%20in%20Description%20Logics%20%28DLs%29.%20Minimization%20of%20some%20selected%0Apredicates%2C%20letting%20the%20remaining%20predicates%20vary%20or%20be%20fixed%2C%20as%20proposed%20in%0Acircumscription%2C%20has%20been%20explored%20and%20exhibits%20high%20complexity.%20The%20case%20of%0A%60pure%27%20minimal%20models%2C%20where%20the%20extension%20of%20all%20predicates%20must%20be%20minimal%2C%0Ahas%20remained%20largely%20uncharted.%20We%20address%20this%20problem%20in%20popular%20DLs%20and%0Aobtain%20surprisingly%20negative%20results%3A%20concept%20satisfiability%20in%20minimal%20models%0Ais%20undecidable%20already%20for%20%24%5Cmathcal%7BEL%7D%24.%20This%20undecidability%20also%20extends%20to%0Aa%20very%20restricted%20fragment%20of%20tuple-generating%20dependencies.%20To%20regain%0Adecidability%2C%20we%20impose%20acyclicity%20conditions%20on%20the%20TBox%20that%20bring%20the%0Aworst-case%20complexity%20below%20double%20exponential%20time%20and%20allow%20us%20to%20establish%20a%0Aconnection%20with%20the%20recently%20studied%20pointwise%20circumscription%3B%20we%20also%20derive%0Aresults%20in%20data%20complexity.%20We%20conclude%20with%20a%20brief%20excursion%20to%20the%20DL-Lite%0Afamily%2C%20where%20a%20positive%20result%20was%20known%20for%20DL-Lite%24_%7B%5Ctext%7Bcore%7D%7D%24%2C%20but%20our%0Ainvestigation%20establishes%20ExpSpace-hardness%20already%20for%20its%20extension%0ADL-Lite%24_%7B%5Ctext%7Bhorn%7D%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05350v1&entry.124074799=Read"},
{"title": "CoCAViT: Compact Vision Transformer with Robust Global Coordination", "author": "Xuyang Wang and Lingjuan Miao and Zhiqiang Zhou", "abstract": "  In recent years, large-scale visual backbones have demonstrated remarkable\ncapabilities in learning general-purpose features from images via extensive\npre-training. Concurrently, many efficient architectures have emerged that have\nperformance comparable to that of larger models on in-domain benchmarks.\nHowever, we observe that for smaller models, the performance drop on\nout-of-distribution (OOD) data is disproportionately larger, indicating a\ndeficiency in the generalization performance of existing efficient models. To\naddress this, we identify key architectural bottlenecks and inappropriate\ndesign choices that contribute to this issue, retaining robustness for smaller\nmodels. To restore the global field of pure window attention, we further\nintroduce a Coordinator-patch Cross Attention (CoCA) mechanism, featuring\ndynamic, domain-aware global tokens that enhance local-global feature modeling\nand adaptively capture robust patterns across domains with minimal\ncomputational overhead. Integrating these advancements, we present CoCAViT, a\nnovel visual backbone designed for robust real-time visual representation.\nExtensive experiments empirically validate our design. At a resolution of\n224*224, CoCAViT-28M achieves 84.0% top-1 accuracy on ImageNet-1K, with\nsignificant gains on multiple OOD benchmarks, compared to competing models. It\nalso attains 52.2 mAP on COCO object detection and 51.3 mIOU on ADE20K semantic\nsegmentation, while maintaining low latency.\n", "link": "http://arxiv.org/abs/2508.05307v1", "date": "2025-08-07", "relevancy": 2.2602, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5727}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5622}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoCAViT%3A%20Compact%20Vision%20Transformer%20with%20Robust%20Global%20Coordination&body=Title%3A%20CoCAViT%3A%20Compact%20Vision%20Transformer%20with%20Robust%20Global%20Coordination%0AAuthor%3A%20Xuyang%20Wang%20and%20Lingjuan%20Miao%20and%20Zhiqiang%20Zhou%0AAbstract%3A%20%20%20In%20recent%20years%2C%20large-scale%20visual%20backbones%20have%20demonstrated%20remarkable%0Acapabilities%20in%20learning%20general-purpose%20features%20from%20images%20via%20extensive%0Apre-training.%20Concurrently%2C%20many%20efficient%20architectures%20have%20emerged%20that%20have%0Aperformance%20comparable%20to%20that%20of%20larger%20models%20on%20in-domain%20benchmarks.%0AHowever%2C%20we%20observe%20that%20for%20smaller%20models%2C%20the%20performance%20drop%20on%0Aout-of-distribution%20%28OOD%29%20data%20is%20disproportionately%20larger%2C%20indicating%20a%0Adeficiency%20in%20the%20generalization%20performance%20of%20existing%20efficient%20models.%20To%0Aaddress%20this%2C%20we%20identify%20key%20architectural%20bottlenecks%20and%20inappropriate%0Adesign%20choices%20that%20contribute%20to%20this%20issue%2C%20retaining%20robustness%20for%20smaller%0Amodels.%20To%20restore%20the%20global%20field%20of%20pure%20window%20attention%2C%20we%20further%0Aintroduce%20a%20Coordinator-patch%20Cross%20Attention%20%28CoCA%29%20mechanism%2C%20featuring%0Adynamic%2C%20domain-aware%20global%20tokens%20that%20enhance%20local-global%20feature%20modeling%0Aand%20adaptively%20capture%20robust%20patterns%20across%20domains%20with%20minimal%0Acomputational%20overhead.%20Integrating%20these%20advancements%2C%20we%20present%20CoCAViT%2C%20a%0Anovel%20visual%20backbone%20designed%20for%20robust%20real-time%20visual%20representation.%0AExtensive%20experiments%20empirically%20validate%20our%20design.%20At%20a%20resolution%20of%0A224%2A224%2C%20CoCAViT-28M%20achieves%2084.0%25%20top-1%20accuracy%20on%20ImageNet-1K%2C%20with%0Asignificant%20gains%20on%20multiple%20OOD%20benchmarks%2C%20compared%20to%20competing%20models.%20It%0Aalso%20attains%2052.2%20mAP%20on%20COCO%20object%20detection%20and%2051.3%20mIOU%20on%20ADE20K%20semantic%0Asegmentation%2C%20while%20maintaining%20low%20latency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoCAViT%253A%2520Compact%2520Vision%2520Transformer%2520with%2520Robust%2520Global%2520Coordination%26entry.906535625%3DXuyang%2520Wang%2520and%2520Lingjuan%2520Miao%2520and%2520Zhiqiang%2520Zhou%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520large-scale%2520visual%2520backbones%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520learning%2520general-purpose%2520features%2520from%2520images%2520via%2520extensive%250Apre-training.%2520Concurrently%252C%2520many%2520efficient%2520architectures%2520have%2520emerged%2520that%2520have%250Aperformance%2520comparable%2520to%2520that%2520of%2520larger%2520models%2520on%2520in-domain%2520benchmarks.%250AHowever%252C%2520we%2520observe%2520that%2520for%2520smaller%2520models%252C%2520the%2520performance%2520drop%2520on%250Aout-of-distribution%2520%2528OOD%2529%2520data%2520is%2520disproportionately%2520larger%252C%2520indicating%2520a%250Adeficiency%2520in%2520the%2520generalization%2520performance%2520of%2520existing%2520efficient%2520models.%2520To%250Aaddress%2520this%252C%2520we%2520identify%2520key%2520architectural%2520bottlenecks%2520and%2520inappropriate%250Adesign%2520choices%2520that%2520contribute%2520to%2520this%2520issue%252C%2520retaining%2520robustness%2520for%2520smaller%250Amodels.%2520To%2520restore%2520the%2520global%2520field%2520of%2520pure%2520window%2520attention%252C%2520we%2520further%250Aintroduce%2520a%2520Coordinator-patch%2520Cross%2520Attention%2520%2528CoCA%2529%2520mechanism%252C%2520featuring%250Adynamic%252C%2520domain-aware%2520global%2520tokens%2520that%2520enhance%2520local-global%2520feature%2520modeling%250Aand%2520adaptively%2520capture%2520robust%2520patterns%2520across%2520domains%2520with%2520minimal%250Acomputational%2520overhead.%2520Integrating%2520these%2520advancements%252C%2520we%2520present%2520CoCAViT%252C%2520a%250Anovel%2520visual%2520backbone%2520designed%2520for%2520robust%2520real-time%2520visual%2520representation.%250AExtensive%2520experiments%2520empirically%2520validate%2520our%2520design.%2520At%2520a%2520resolution%2520of%250A224%252A224%252C%2520CoCAViT-28M%2520achieves%252084.0%2525%2520top-1%2520accuracy%2520on%2520ImageNet-1K%252C%2520with%250Asignificant%2520gains%2520on%2520multiple%2520OOD%2520benchmarks%252C%2520compared%2520to%2520competing%2520models.%2520It%250Aalso%2520attains%252052.2%2520mAP%2520on%2520COCO%2520object%2520detection%2520and%252051.3%2520mIOU%2520on%2520ADE20K%2520semantic%250Asegmentation%252C%2520while%2520maintaining%2520low%2520latency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoCAViT%3A%20Compact%20Vision%20Transformer%20with%20Robust%20Global%20Coordination&entry.906535625=Xuyang%20Wang%20and%20Lingjuan%20Miao%20and%20Zhiqiang%20Zhou&entry.1292438233=%20%20In%20recent%20years%2C%20large-scale%20visual%20backbones%20have%20demonstrated%20remarkable%0Acapabilities%20in%20learning%20general-purpose%20features%20from%20images%20via%20extensive%0Apre-training.%20Concurrently%2C%20many%20efficient%20architectures%20have%20emerged%20that%20have%0Aperformance%20comparable%20to%20that%20of%20larger%20models%20on%20in-domain%20benchmarks.%0AHowever%2C%20we%20observe%20that%20for%20smaller%20models%2C%20the%20performance%20drop%20on%0Aout-of-distribution%20%28OOD%29%20data%20is%20disproportionately%20larger%2C%20indicating%20a%0Adeficiency%20in%20the%20generalization%20performance%20of%20existing%20efficient%20models.%20To%0Aaddress%20this%2C%20we%20identify%20key%20architectural%20bottlenecks%20and%20inappropriate%0Adesign%20choices%20that%20contribute%20to%20this%20issue%2C%20retaining%20robustness%20for%20smaller%0Amodels.%20To%20restore%20the%20global%20field%20of%20pure%20window%20attention%2C%20we%20further%0Aintroduce%20a%20Coordinator-patch%20Cross%20Attention%20%28CoCA%29%20mechanism%2C%20featuring%0Adynamic%2C%20domain-aware%20global%20tokens%20that%20enhance%20local-global%20feature%20modeling%0Aand%20adaptively%20capture%20robust%20patterns%20across%20domains%20with%20minimal%0Acomputational%20overhead.%20Integrating%20these%20advancements%2C%20we%20present%20CoCAViT%2C%20a%0Anovel%20visual%20backbone%20designed%20for%20robust%20real-time%20visual%20representation.%0AExtensive%20experiments%20empirically%20validate%20our%20design.%20At%20a%20resolution%20of%0A224%2A224%2C%20CoCAViT-28M%20achieves%2084.0%25%20top-1%20accuracy%20on%20ImageNet-1K%2C%20with%0Asignificant%20gains%20on%20multiple%20OOD%20benchmarks%2C%20compared%20to%20competing%20models.%20It%0Aalso%20attains%2052.2%20mAP%20on%20COCO%20object%20detection%20and%2051.3%20mIOU%20on%20ADE20K%20semantic%0Asegmentation%2C%20while%20maintaining%20low%20latency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05307v1&entry.124074799=Read"},
{"title": "PerSense: Training-Free Personalized Instance Segmentation in Dense\n  Images", "author": "Muhammad Ibraheem Siddiqui and Muhammad Umer Sheikh and Hassan Abid and Muhammad Haris Khan", "abstract": "  The emergence of foundational models has significantly advanced segmentation\napproaches. However, challenges still remain in dense scenarios, where\nocclusions, scale variations, and clutter impede precise instance delineation.\nTo address this, we propose PerSense, an end-to-end, training-free, and\nmodel-agnostic one-shot framework for Personalized instance Segmentation in\ndense images. We start with developing a new baseline capable of automatically\ngenerating instance-level point prompts via proposing a novel Instance\nDetection Module (IDM) that leverages density maps (DMs), encapsulating spatial\ndistribution of objects in an image. To reduce false positives, we design the\nPoint Prompt Selection Module (PPSM), which refines the output of IDM based on\nadaptive threshold and spatial gating. Both IDM and PPSM seamlessly integrate\ninto our model-agnostic framework. Furthermore, we introduce a feedback\nmechanism that enables PerSense to improve the accuracy of DMs by automating\nthe exemplar selection process for DM generation. Finally, to advance research\nin this relatively underexplored area, we introduce PerSense-D, an evaluation\nbenchmark for instance segmentation in dense images. Our extensive experiments\nestablish PerSense's superiority over SOTA in dense settings.\n", "link": "http://arxiv.org/abs/2405.13518v4", "date": "2025-08-07", "relevancy": 2.2515, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5676}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerSense%3A%20Training-Free%20Personalized%20Instance%20Segmentation%20in%20Dense%0A%20%20Images&body=Title%3A%20PerSense%3A%20Training-Free%20Personalized%20Instance%20Segmentation%20in%20Dense%0A%20%20Images%0AAuthor%3A%20Muhammad%20Ibraheem%20Siddiqui%20and%20Muhammad%20Umer%20Sheikh%20and%20Hassan%20Abid%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20The%20emergence%20of%20foundational%20models%20has%20significantly%20advanced%20segmentation%0Aapproaches.%20However%2C%20challenges%20still%20remain%20in%20dense%20scenarios%2C%20where%0Aocclusions%2C%20scale%20variations%2C%20and%20clutter%20impede%20precise%20instance%20delineation.%0ATo%20address%20this%2C%20we%20propose%20PerSense%2C%20an%20end-to-end%2C%20training-free%2C%20and%0Amodel-agnostic%20one-shot%20framework%20for%20Personalized%20instance%20Segmentation%20in%0Adense%20images.%20We%20start%20with%20developing%20a%20new%20baseline%20capable%20of%20automatically%0Agenerating%20instance-level%20point%20prompts%20via%20proposing%20a%20novel%20Instance%0ADetection%20Module%20%28IDM%29%20that%20leverages%20density%20maps%20%28DMs%29%2C%20encapsulating%20spatial%0Adistribution%20of%20objects%20in%20an%20image.%20To%20reduce%20false%20positives%2C%20we%20design%20the%0APoint%20Prompt%20Selection%20Module%20%28PPSM%29%2C%20which%20refines%20the%20output%20of%20IDM%20based%20on%0Aadaptive%20threshold%20and%20spatial%20gating.%20Both%20IDM%20and%20PPSM%20seamlessly%20integrate%0Ainto%20our%20model-agnostic%20framework.%20Furthermore%2C%20we%20introduce%20a%20feedback%0Amechanism%20that%20enables%20PerSense%20to%20improve%20the%20accuracy%20of%20DMs%20by%20automating%0Athe%20exemplar%20selection%20process%20for%20DM%20generation.%20Finally%2C%20to%20advance%20research%0Ain%20this%20relatively%20underexplored%20area%2C%20we%20introduce%20PerSense-D%2C%20an%20evaluation%0Abenchmark%20for%20instance%20segmentation%20in%20dense%20images.%20Our%20extensive%20experiments%0Aestablish%20PerSense%27s%20superiority%20over%20SOTA%20in%20dense%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13518v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerSense%253A%2520Training-Free%2520Personalized%2520Instance%2520Segmentation%2520in%2520Dense%250A%2520%2520Images%26entry.906535625%3DMuhammad%2520Ibraheem%2520Siddiqui%2520and%2520Muhammad%2520Umer%2520Sheikh%2520and%2520Hassan%2520Abid%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520foundational%2520models%2520has%2520significantly%2520advanced%2520segmentation%250Aapproaches.%2520However%252C%2520challenges%2520still%2520remain%2520in%2520dense%2520scenarios%252C%2520where%250Aocclusions%252C%2520scale%2520variations%252C%2520and%2520clutter%2520impede%2520precise%2520instance%2520delineation.%250ATo%2520address%2520this%252C%2520we%2520propose%2520PerSense%252C%2520an%2520end-to-end%252C%2520training-free%252C%2520and%250Amodel-agnostic%2520one-shot%2520framework%2520for%2520Personalized%2520instance%2520Segmentation%2520in%250Adense%2520images.%2520We%2520start%2520with%2520developing%2520a%2520new%2520baseline%2520capable%2520of%2520automatically%250Agenerating%2520instance-level%2520point%2520prompts%2520via%2520proposing%2520a%2520novel%2520Instance%250ADetection%2520Module%2520%2528IDM%2529%2520that%2520leverages%2520density%2520maps%2520%2528DMs%2529%252C%2520encapsulating%2520spatial%250Adistribution%2520of%2520objects%2520in%2520an%2520image.%2520To%2520reduce%2520false%2520positives%252C%2520we%2520design%2520the%250APoint%2520Prompt%2520Selection%2520Module%2520%2528PPSM%2529%252C%2520which%2520refines%2520the%2520output%2520of%2520IDM%2520based%2520on%250Aadaptive%2520threshold%2520and%2520spatial%2520gating.%2520Both%2520IDM%2520and%2520PPSM%2520seamlessly%2520integrate%250Ainto%2520our%2520model-agnostic%2520framework.%2520Furthermore%252C%2520we%2520introduce%2520a%2520feedback%250Amechanism%2520that%2520enables%2520PerSense%2520to%2520improve%2520the%2520accuracy%2520of%2520DMs%2520by%2520automating%250Athe%2520exemplar%2520selection%2520process%2520for%2520DM%2520generation.%2520Finally%252C%2520to%2520advance%2520research%250Ain%2520this%2520relatively%2520underexplored%2520area%252C%2520we%2520introduce%2520PerSense-D%252C%2520an%2520evaluation%250Abenchmark%2520for%2520instance%2520segmentation%2520in%2520dense%2520images.%2520Our%2520extensive%2520experiments%250Aestablish%2520PerSense%2527s%2520superiority%2520over%2520SOTA%2520in%2520dense%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13518v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerSense%3A%20Training-Free%20Personalized%20Instance%20Segmentation%20in%20Dense%0A%20%20Images&entry.906535625=Muhammad%20Ibraheem%20Siddiqui%20and%20Muhammad%20Umer%20Sheikh%20and%20Hassan%20Abid%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20The%20emergence%20of%20foundational%20models%20has%20significantly%20advanced%20segmentation%0Aapproaches.%20However%2C%20challenges%20still%20remain%20in%20dense%20scenarios%2C%20where%0Aocclusions%2C%20scale%20variations%2C%20and%20clutter%20impede%20precise%20instance%20delineation.%0ATo%20address%20this%2C%20we%20propose%20PerSense%2C%20an%20end-to-end%2C%20training-free%2C%20and%0Amodel-agnostic%20one-shot%20framework%20for%20Personalized%20instance%20Segmentation%20in%0Adense%20images.%20We%20start%20with%20developing%20a%20new%20baseline%20capable%20of%20automatically%0Agenerating%20instance-level%20point%20prompts%20via%20proposing%20a%20novel%20Instance%0ADetection%20Module%20%28IDM%29%20that%20leverages%20density%20maps%20%28DMs%29%2C%20encapsulating%20spatial%0Adistribution%20of%20objects%20in%20an%20image.%20To%20reduce%20false%20positives%2C%20we%20design%20the%0APoint%20Prompt%20Selection%20Module%20%28PPSM%29%2C%20which%20refines%20the%20output%20of%20IDM%20based%20on%0Aadaptive%20threshold%20and%20spatial%20gating.%20Both%20IDM%20and%20PPSM%20seamlessly%20integrate%0Ainto%20our%20model-agnostic%20framework.%20Furthermore%2C%20we%20introduce%20a%20feedback%0Amechanism%20that%20enables%20PerSense%20to%20improve%20the%20accuracy%20of%20DMs%20by%20automating%0Athe%20exemplar%20selection%20process%20for%20DM%20generation.%20Finally%2C%20to%20advance%20research%0Ain%20this%20relatively%20underexplored%20area%2C%20we%20introduce%20PerSense-D%2C%20an%20evaluation%0Abenchmark%20for%20instance%20segmentation%20in%20dense%20images.%20Our%20extensive%20experiments%0Aestablish%20PerSense%27s%20superiority%20over%20SOTA%20in%20dense%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13518v4&entry.124074799=Read"},
{"title": "SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible\n  Image Fusion", "author": "Xiaoyang Zhang and Zhen Hua and Yakun Ju and Wei Zhou and Jun Liu and Alex C. Kot", "abstract": "  Infrared and visible image fusion (IVIF) aims to combine the thermal\nradiation information from infrared images with the rich texture details from\nvisible images to enhance perceptual capabilities for downstream visual tasks.\nHowever, existing methods often fail to preserve key targets due to a lack of\ndeep semantic understanding of the scene, while the fusion process itself can\nalso introduce artifacts and detail loss, severely compromising both image\nquality and task performance. To address these issues, this paper proposes\nSGDFuse, a conditional diffusion model guided by the Segment Anything Model\n(SAM), to achieve high-fidelity and semantically-aware image fusion. The core\nof our method is to utilize high-quality semantic masks generated by SAM as\nexplicit priors to guide the optimization of the fusion process via a\nconditional diffusion model. Specifically, the framework operates in a\ntwo-stage process: it first performs a preliminary fusion of multi-modal\nfeatures, and then utilizes the semantic masks from SAM jointly with the\npreliminary fused image as a condition to drive the diffusion model's\ncoarse-to-fine denoising generation. This ensures the fusion process not only\nhas explicit semantic directionality but also guarantees the high fidelity of\nthe final result. Extensive experiments demonstrate that SGDFuse achieves\nstate-of-the-art performance in both subjective and objective evaluations, as\nwell as in its adaptability to downstream tasks, providing a powerful solution\nto the core challenges in image fusion. The code of SGDFuse is available at\nhttps://github.com/boshizhang123/SGDFuse.\n", "link": "http://arxiv.org/abs/2508.05264v1", "date": "2025-08-07", "relevancy": 2.2424, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6083}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5585}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGDFuse%3A%20SAM-Guided%20Diffusion%20for%20High-Fidelity%20Infrared%20and%20Visible%0A%20%20Image%20Fusion&body=Title%3A%20SGDFuse%3A%20SAM-Guided%20Diffusion%20for%20High-Fidelity%20Infrared%20and%20Visible%0A%20%20Image%20Fusion%0AAuthor%3A%20Xiaoyang%20Zhang%20and%20Zhen%20Hua%20and%20Yakun%20Ju%20and%20Wei%20Zhou%20and%20Jun%20Liu%20and%20Alex%20C.%20Kot%0AAbstract%3A%20%20%20Infrared%20and%20visible%20image%20fusion%20%28IVIF%29%20aims%20to%20combine%20the%20thermal%0Aradiation%20information%20from%20infrared%20images%20with%20the%20rich%20texture%20details%20from%0Avisible%20images%20to%20enhance%20perceptual%20capabilities%20for%20downstream%20visual%20tasks.%0AHowever%2C%20existing%20methods%20often%20fail%20to%20preserve%20key%20targets%20due%20to%20a%20lack%20of%0Adeep%20semantic%20understanding%20of%20the%20scene%2C%20while%20the%20fusion%20process%20itself%20can%0Aalso%20introduce%20artifacts%20and%20detail%20loss%2C%20severely%20compromising%20both%20image%0Aquality%20and%20task%20performance.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%0ASGDFuse%2C%20a%20conditional%20diffusion%20model%20guided%20by%20the%20Segment%20Anything%20Model%0A%28SAM%29%2C%20to%20achieve%20high-fidelity%20and%20semantically-aware%20image%20fusion.%20The%20core%0Aof%20our%20method%20is%20to%20utilize%20high-quality%20semantic%20masks%20generated%20by%20SAM%20as%0Aexplicit%20priors%20to%20guide%20the%20optimization%20of%20the%20fusion%20process%20via%20a%0Aconditional%20diffusion%20model.%20Specifically%2C%20the%20framework%20operates%20in%20a%0Atwo-stage%20process%3A%20it%20first%20performs%20a%20preliminary%20fusion%20of%20multi-modal%0Afeatures%2C%20and%20then%20utilizes%20the%20semantic%20masks%20from%20SAM%20jointly%20with%20the%0Apreliminary%20fused%20image%20as%20a%20condition%20to%20drive%20the%20diffusion%20model%27s%0Acoarse-to-fine%20denoising%20generation.%20This%20ensures%20the%20fusion%20process%20not%20only%0Ahas%20explicit%20semantic%20directionality%20but%20also%20guarantees%20the%20high%20fidelity%20of%0Athe%20final%20result.%20Extensive%20experiments%20demonstrate%20that%20SGDFuse%20achieves%0Astate-of-the-art%20performance%20in%20both%20subjective%20and%20objective%20evaluations%2C%20as%0Awell%20as%20in%20its%20adaptability%20to%20downstream%20tasks%2C%20providing%20a%20powerful%20solution%0Ato%20the%20core%20challenges%20in%20image%20fusion.%20The%20code%20of%20SGDFuse%20is%20available%20at%0Ahttps%3A//github.com/boshizhang123/SGDFuse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGDFuse%253A%2520SAM-Guided%2520Diffusion%2520for%2520High-Fidelity%2520Infrared%2520and%2520Visible%250A%2520%2520Image%2520Fusion%26entry.906535625%3DXiaoyang%2520Zhang%2520and%2520Zhen%2520Hua%2520and%2520Yakun%2520Ju%2520and%2520Wei%2520Zhou%2520and%2520Jun%2520Liu%2520and%2520Alex%2520C.%2520Kot%26entry.1292438233%3D%2520%2520Infrared%2520and%2520visible%2520image%2520fusion%2520%2528IVIF%2529%2520aims%2520to%2520combine%2520the%2520thermal%250Aradiation%2520information%2520from%2520infrared%2520images%2520with%2520the%2520rich%2520texture%2520details%2520from%250Avisible%2520images%2520to%2520enhance%2520perceptual%2520capabilities%2520for%2520downstream%2520visual%2520tasks.%250AHowever%252C%2520existing%2520methods%2520often%2520fail%2520to%2520preserve%2520key%2520targets%2520due%2520to%2520a%2520lack%2520of%250Adeep%2520semantic%2520understanding%2520of%2520the%2520scene%252C%2520while%2520the%2520fusion%2520process%2520itself%2520can%250Aalso%2520introduce%2520artifacts%2520and%2520detail%2520loss%252C%2520severely%2520compromising%2520both%2520image%250Aquality%2520and%2520task%2520performance.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%2520proposes%250ASGDFuse%252C%2520a%2520conditional%2520diffusion%2520model%2520guided%2520by%2520the%2520Segment%2520Anything%2520Model%250A%2528SAM%2529%252C%2520to%2520achieve%2520high-fidelity%2520and%2520semantically-aware%2520image%2520fusion.%2520The%2520core%250Aof%2520our%2520method%2520is%2520to%2520utilize%2520high-quality%2520semantic%2520masks%2520generated%2520by%2520SAM%2520as%250Aexplicit%2520priors%2520to%2520guide%2520the%2520optimization%2520of%2520the%2520fusion%2520process%2520via%2520a%250Aconditional%2520diffusion%2520model.%2520Specifically%252C%2520the%2520framework%2520operates%2520in%2520a%250Atwo-stage%2520process%253A%2520it%2520first%2520performs%2520a%2520preliminary%2520fusion%2520of%2520multi-modal%250Afeatures%252C%2520and%2520then%2520utilizes%2520the%2520semantic%2520masks%2520from%2520SAM%2520jointly%2520with%2520the%250Apreliminary%2520fused%2520image%2520as%2520a%2520condition%2520to%2520drive%2520the%2520diffusion%2520model%2527s%250Acoarse-to-fine%2520denoising%2520generation.%2520This%2520ensures%2520the%2520fusion%2520process%2520not%2520only%250Ahas%2520explicit%2520semantic%2520directionality%2520but%2520also%2520guarantees%2520the%2520high%2520fidelity%2520of%250Athe%2520final%2520result.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SGDFuse%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520subjective%2520and%2520objective%2520evaluations%252C%2520as%250Awell%2520as%2520in%2520its%2520adaptability%2520to%2520downstream%2520tasks%252C%2520providing%2520a%2520powerful%2520solution%250Ato%2520the%2520core%2520challenges%2520in%2520image%2520fusion.%2520The%2520code%2520of%2520SGDFuse%2520is%2520available%2520at%250Ahttps%253A//github.com/boshizhang123/SGDFuse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGDFuse%3A%20SAM-Guided%20Diffusion%20for%20High-Fidelity%20Infrared%20and%20Visible%0A%20%20Image%20Fusion&entry.906535625=Xiaoyang%20Zhang%20and%20Zhen%20Hua%20and%20Yakun%20Ju%20and%20Wei%20Zhou%20and%20Jun%20Liu%20and%20Alex%20C.%20Kot&entry.1292438233=%20%20Infrared%20and%20visible%20image%20fusion%20%28IVIF%29%20aims%20to%20combine%20the%20thermal%0Aradiation%20information%20from%20infrared%20images%20with%20the%20rich%20texture%20details%20from%0Avisible%20images%20to%20enhance%20perceptual%20capabilities%20for%20downstream%20visual%20tasks.%0AHowever%2C%20existing%20methods%20often%20fail%20to%20preserve%20key%20targets%20due%20to%20a%20lack%20of%0Adeep%20semantic%20understanding%20of%20the%20scene%2C%20while%20the%20fusion%20process%20itself%20can%0Aalso%20introduce%20artifacts%20and%20detail%20loss%2C%20severely%20compromising%20both%20image%0Aquality%20and%20task%20performance.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%0ASGDFuse%2C%20a%20conditional%20diffusion%20model%20guided%20by%20the%20Segment%20Anything%20Model%0A%28SAM%29%2C%20to%20achieve%20high-fidelity%20and%20semantically-aware%20image%20fusion.%20The%20core%0Aof%20our%20method%20is%20to%20utilize%20high-quality%20semantic%20masks%20generated%20by%20SAM%20as%0Aexplicit%20priors%20to%20guide%20the%20optimization%20of%20the%20fusion%20process%20via%20a%0Aconditional%20diffusion%20model.%20Specifically%2C%20the%20framework%20operates%20in%20a%0Atwo-stage%20process%3A%20it%20first%20performs%20a%20preliminary%20fusion%20of%20multi-modal%0Afeatures%2C%20and%20then%20utilizes%20the%20semantic%20masks%20from%20SAM%20jointly%20with%20the%0Apreliminary%20fused%20image%20as%20a%20condition%20to%20drive%20the%20diffusion%20model%27s%0Acoarse-to-fine%20denoising%20generation.%20This%20ensures%20the%20fusion%20process%20not%20only%0Ahas%20explicit%20semantic%20directionality%20but%20also%20guarantees%20the%20high%20fidelity%20of%0Athe%20final%20result.%20Extensive%20experiments%20demonstrate%20that%20SGDFuse%20achieves%0Astate-of-the-art%20performance%20in%20both%20subjective%20and%20objective%20evaluations%2C%20as%0Awell%20as%20in%20its%20adaptability%20to%20downstream%20tasks%2C%20providing%20a%20powerful%20solution%0Ato%20the%20core%20challenges%20in%20image%20fusion.%20The%20code%20of%20SGDFuse%20is%20available%20at%0Ahttps%3A//github.com/boshizhang123/SGDFuse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05264v1&entry.124074799=Read"},
{"title": "Cross-Image Contrastive Decoding: Precise, Lossless Suppression of\n  Language Priors in Large Vision-Language Models", "author": "Jianfei Zhao and Feng Zhang and Xin Sun and Chong Feng", "abstract": "  Over-reliance on language priors is a major cause of hallucinations in Large\nVision-Language Models (LVLMs), often leading to outputs that are\nlinguistically plausible but visually inconsistent. Recent studies have\nexplored contrastive decoding as a training-free solution. However, these\nmethods typically construct contrastive visual inputs by perturbing the\noriginal image, resulting in distorted contrastive distributions, incomplete\ncontrastive signals, and excessive suppression of language priors. Motivated by\nthe observation that language priors tend to remain consistent across different\nimages, we propose Cross-Image Contrastive Decoding (CICD), a simple yet\neffective training-free method that uses unrelated images as contrastive visual\ninputs. To address the issue of over-suppressing language priors, which can\nnegatively affect the quality of generated responses, we further introduce a\ndynamic selection mechanism based on the cross-image differences in model\nbehavior. By selectively suppressing language priors, our method reduces\nhallucinations without compromising the model's performance. Extensive\nexperiments across multiple benchmarks and LVLMs confirm the effectiveness and\ngeneralizability of CICD, particularly in image captioning, where language\npriors are especially dominant.\n", "link": "http://arxiv.org/abs/2505.10634v4", "date": "2025-08-07", "relevancy": 2.24, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Image%20Contrastive%20Decoding%3A%20Precise%2C%20Lossless%20Suppression%20of%0A%20%20Language%20Priors%20in%20Large%20Vision-Language%20Models&body=Title%3A%20Cross-Image%20Contrastive%20Decoding%3A%20Precise%2C%20Lossless%20Suppression%20of%0A%20%20Language%20Priors%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Jianfei%20Zhao%20and%20Feng%20Zhang%20and%20Xin%20Sun%20and%20Chong%20Feng%0AAbstract%3A%20%20%20Over-reliance%20on%20language%20priors%20is%20a%20major%20cause%20of%20hallucinations%20in%20Large%0AVision-Language%20Models%20%28LVLMs%29%2C%20often%20leading%20to%20outputs%20that%20are%0Alinguistically%20plausible%20but%20visually%20inconsistent.%20Recent%20studies%20have%0Aexplored%20contrastive%20decoding%20as%20a%20training-free%20solution.%20However%2C%20these%0Amethods%20typically%20construct%20contrastive%20visual%20inputs%20by%20perturbing%20the%0Aoriginal%20image%2C%20resulting%20in%20distorted%20contrastive%20distributions%2C%20incomplete%0Acontrastive%20signals%2C%20and%20excessive%20suppression%20of%20language%20priors.%20Motivated%20by%0Athe%20observation%20that%20language%20priors%20tend%20to%20remain%20consistent%20across%20different%0Aimages%2C%20we%20propose%20Cross-Image%20Contrastive%20Decoding%20%28CICD%29%2C%20a%20simple%20yet%0Aeffective%20training-free%20method%20that%20uses%20unrelated%20images%20as%20contrastive%20visual%0Ainputs.%20To%20address%20the%20issue%20of%20over-suppressing%20language%20priors%2C%20which%20can%0Anegatively%20affect%20the%20quality%20of%20generated%20responses%2C%20we%20further%20introduce%20a%0Adynamic%20selection%20mechanism%20based%20on%20the%20cross-image%20differences%20in%20model%0Abehavior.%20By%20selectively%20suppressing%20language%20priors%2C%20our%20method%20reduces%0Ahallucinations%20without%20compromising%20the%20model%27s%20performance.%20Extensive%0Aexperiments%20across%20multiple%20benchmarks%20and%20LVLMs%20confirm%20the%20effectiveness%20and%0Ageneralizability%20of%20CICD%2C%20particularly%20in%20image%20captioning%2C%20where%20language%0Apriors%20are%20especially%20dominant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10634v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Image%2520Contrastive%2520Decoding%253A%2520Precise%252C%2520Lossless%2520Suppression%2520of%250A%2520%2520Language%2520Priors%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DJianfei%2520Zhao%2520and%2520Feng%2520Zhang%2520and%2520Xin%2520Sun%2520and%2520Chong%2520Feng%26entry.1292438233%3D%2520%2520Over-reliance%2520on%2520language%2520priors%2520is%2520a%2520major%2520cause%2520of%2520hallucinations%2520in%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%252C%2520often%2520leading%2520to%2520outputs%2520that%2520are%250Alinguistically%2520plausible%2520but%2520visually%2520inconsistent.%2520Recent%2520studies%2520have%250Aexplored%2520contrastive%2520decoding%2520as%2520a%2520training-free%2520solution.%2520However%252C%2520these%250Amethods%2520typically%2520construct%2520contrastive%2520visual%2520inputs%2520by%2520perturbing%2520the%250Aoriginal%2520image%252C%2520resulting%2520in%2520distorted%2520contrastive%2520distributions%252C%2520incomplete%250Acontrastive%2520signals%252C%2520and%2520excessive%2520suppression%2520of%2520language%2520priors.%2520Motivated%2520by%250Athe%2520observation%2520that%2520language%2520priors%2520tend%2520to%2520remain%2520consistent%2520across%2520different%250Aimages%252C%2520we%2520propose%2520Cross-Image%2520Contrastive%2520Decoding%2520%2528CICD%2529%252C%2520a%2520simple%2520yet%250Aeffective%2520training-free%2520method%2520that%2520uses%2520unrelated%2520images%2520as%2520contrastive%2520visual%250Ainputs.%2520To%2520address%2520the%2520issue%2520of%2520over-suppressing%2520language%2520priors%252C%2520which%2520can%250Anegatively%2520affect%2520the%2520quality%2520of%2520generated%2520responses%252C%2520we%2520further%2520introduce%2520a%250Adynamic%2520selection%2520mechanism%2520based%2520on%2520the%2520cross-image%2520differences%2520in%2520model%250Abehavior.%2520By%2520selectively%2520suppressing%2520language%2520priors%252C%2520our%2520method%2520reduces%250Ahallucinations%2520without%2520compromising%2520the%2520model%2527s%2520performance.%2520Extensive%250Aexperiments%2520across%2520multiple%2520benchmarks%2520and%2520LVLMs%2520confirm%2520the%2520effectiveness%2520and%250Ageneralizability%2520of%2520CICD%252C%2520particularly%2520in%2520image%2520captioning%252C%2520where%2520language%250Apriors%2520are%2520especially%2520dominant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10634v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Image%20Contrastive%20Decoding%3A%20Precise%2C%20Lossless%20Suppression%20of%0A%20%20Language%20Priors%20in%20Large%20Vision-Language%20Models&entry.906535625=Jianfei%20Zhao%20and%20Feng%20Zhang%20and%20Xin%20Sun%20and%20Chong%20Feng&entry.1292438233=%20%20Over-reliance%20on%20language%20priors%20is%20a%20major%20cause%20of%20hallucinations%20in%20Large%0AVision-Language%20Models%20%28LVLMs%29%2C%20often%20leading%20to%20outputs%20that%20are%0Alinguistically%20plausible%20but%20visually%20inconsistent.%20Recent%20studies%20have%0Aexplored%20contrastive%20decoding%20as%20a%20training-free%20solution.%20However%2C%20these%0Amethods%20typically%20construct%20contrastive%20visual%20inputs%20by%20perturbing%20the%0Aoriginal%20image%2C%20resulting%20in%20distorted%20contrastive%20distributions%2C%20incomplete%0Acontrastive%20signals%2C%20and%20excessive%20suppression%20of%20language%20priors.%20Motivated%20by%0Athe%20observation%20that%20language%20priors%20tend%20to%20remain%20consistent%20across%20different%0Aimages%2C%20we%20propose%20Cross-Image%20Contrastive%20Decoding%20%28CICD%29%2C%20a%20simple%20yet%0Aeffective%20training-free%20method%20that%20uses%20unrelated%20images%20as%20contrastive%20visual%0Ainputs.%20To%20address%20the%20issue%20of%20over-suppressing%20language%20priors%2C%20which%20can%0Anegatively%20affect%20the%20quality%20of%20generated%20responses%2C%20we%20further%20introduce%20a%0Adynamic%20selection%20mechanism%20based%20on%20the%20cross-image%20differences%20in%20model%0Abehavior.%20By%20selectively%20suppressing%20language%20priors%2C%20our%20method%20reduces%0Ahallucinations%20without%20compromising%20the%20model%27s%20performance.%20Extensive%0Aexperiments%20across%20multiple%20benchmarks%20and%20LVLMs%20confirm%20the%20effectiveness%20and%0Ageneralizability%20of%20CICD%2C%20particularly%20in%20image%20captioning%2C%20where%20language%0Apriors%20are%20especially%20dominant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10634v4&entry.124074799=Read"},
{"title": "Simulating Human-Like Learning Dynamics with LLM-Empowered Agents", "author": "Yu Yuan and Lili Zhao and Wei Chen and Guangting Zheng and Kai Zhang and Mengdi Zhang and Qi Liu", "abstract": "  Capturing human learning behavior based on deep learning methods has become a\nmajor research focus in both psychology and intelligent systems. Recent\napproaches rely on controlled experiments or rule-based models to explore\ncognitive processes. However, they struggle to capture learning dynamics, track\nprogress over time, or provide explainability. To address these challenges, we\nintroduce LearnerAgent, a novel multi-agent framework based on Large Language\nModels (LLMs) to simulate a realistic teaching environment. To explore\nhuman-like learning dynamics, we construct learners with psychologically\ngrounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free\nGeneral Learner to inspect the base LLM's default behavior. Through weekly\nknowledge acquisition, monthly strategic choices, periodic tests, and peer\ninteraction, we can track the dynamic learning progress of individual learners\nover a full-year journey. Our findings are fourfold: 1) Longitudinal analysis\nreveals that only Deep Learner achieves sustained cognitive growth. Our\nspecially designed \"trap questions\" effectively diagnose Surface Learner's\nshallow knowledge. 2) The behavioral and cognitive patterns of distinct\nlearners align closely with their psychological profiles. 3) Learners'\nself-concept scores evolve realistically, with the General Learner developing\nsurprisingly high self-efficacy despite its cognitive limitations. 4)\nCritically, the default profile of base LLM is a \"diligent but brittle Surface\nLearner\"-an agent that mimics the behaviors of a good student but lacks true,\ngeneralizable understanding. Extensive simulation experiments demonstrate that\nLearnerAgent aligns well with real scenarios, yielding more insightful findings\nabout LLMs' behavior.\n", "link": "http://arxiv.org/abs/2508.05622v1", "date": "2025-08-07", "relevancy": 2.2168, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5857}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5324}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simulating%20Human-Like%20Learning%20Dynamics%20with%20LLM-Empowered%20Agents&body=Title%3A%20Simulating%20Human-Like%20Learning%20Dynamics%20with%20LLM-Empowered%20Agents%0AAuthor%3A%20Yu%20Yuan%20and%20Lili%20Zhao%20and%20Wei%20Chen%20and%20Guangting%20Zheng%20and%20Kai%20Zhang%20and%20Mengdi%20Zhang%20and%20Qi%20Liu%0AAbstract%3A%20%20%20Capturing%20human%20learning%20behavior%20based%20on%20deep%20learning%20methods%20has%20become%20a%0Amajor%20research%20focus%20in%20both%20psychology%20and%20intelligent%20systems.%20Recent%0Aapproaches%20rely%20on%20controlled%20experiments%20or%20rule-based%20models%20to%20explore%0Acognitive%20processes.%20However%2C%20they%20struggle%20to%20capture%20learning%20dynamics%2C%20track%0Aprogress%20over%20time%2C%20or%20provide%20explainability.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20LearnerAgent%2C%20a%20novel%20multi-agent%20framework%20based%20on%20Large%20Language%0AModels%20%28LLMs%29%20to%20simulate%20a%20realistic%20teaching%20environment.%20To%20explore%0Ahuman-like%20learning%20dynamics%2C%20we%20construct%20learners%20with%20psychologically%0Agrounded%20profiles-such%20as%20Deep%2C%20Surface%2C%20and%20Lazy-as%20well%20as%20a%20persona-free%0AGeneral%20Learner%20to%20inspect%20the%20base%20LLM%27s%20default%20behavior.%20Through%20weekly%0Aknowledge%20acquisition%2C%20monthly%20strategic%20choices%2C%20periodic%20tests%2C%20and%20peer%0Ainteraction%2C%20we%20can%20track%20the%20dynamic%20learning%20progress%20of%20individual%20learners%0Aover%20a%20full-year%20journey.%20Our%20findings%20are%20fourfold%3A%201%29%20Longitudinal%20analysis%0Areveals%20that%20only%20Deep%20Learner%20achieves%20sustained%20cognitive%20growth.%20Our%0Aspecially%20designed%20%22trap%20questions%22%20effectively%20diagnose%20Surface%20Learner%27s%0Ashallow%20knowledge.%202%29%20The%20behavioral%20and%20cognitive%20patterns%20of%20distinct%0Alearners%20align%20closely%20with%20their%20psychological%20profiles.%203%29%20Learners%27%0Aself-concept%20scores%20evolve%20realistically%2C%20with%20the%20General%20Learner%20developing%0Asurprisingly%20high%20self-efficacy%20despite%20its%20cognitive%20limitations.%204%29%0ACritically%2C%20the%20default%20profile%20of%20base%20LLM%20is%20a%20%22diligent%20but%20brittle%20Surface%0ALearner%22-an%20agent%20that%20mimics%20the%20behaviors%20of%20a%20good%20student%20but%20lacks%20true%2C%0Ageneralizable%20understanding.%20Extensive%20simulation%20experiments%20demonstrate%20that%0ALearnerAgent%20aligns%20well%20with%20real%20scenarios%2C%20yielding%20more%20insightful%20findings%0Aabout%20LLMs%27%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimulating%2520Human-Like%2520Learning%2520Dynamics%2520with%2520LLM-Empowered%2520Agents%26entry.906535625%3DYu%2520Yuan%2520and%2520Lili%2520Zhao%2520and%2520Wei%2520Chen%2520and%2520Guangting%2520Zheng%2520and%2520Kai%2520Zhang%2520and%2520Mengdi%2520Zhang%2520and%2520Qi%2520Liu%26entry.1292438233%3D%2520%2520Capturing%2520human%2520learning%2520behavior%2520based%2520on%2520deep%2520learning%2520methods%2520has%2520become%2520a%250Amajor%2520research%2520focus%2520in%2520both%2520psychology%2520and%2520intelligent%2520systems.%2520Recent%250Aapproaches%2520rely%2520on%2520controlled%2520experiments%2520or%2520rule-based%2520models%2520to%2520explore%250Acognitive%2520processes.%2520However%252C%2520they%2520struggle%2520to%2520capture%2520learning%2520dynamics%252C%2520track%250Aprogress%2520over%2520time%252C%2520or%2520provide%2520explainability.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520LearnerAgent%252C%2520a%2520novel%2520multi-agent%2520framework%2520based%2520on%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520to%2520simulate%2520a%2520realistic%2520teaching%2520environment.%2520To%2520explore%250Ahuman-like%2520learning%2520dynamics%252C%2520we%2520construct%2520learners%2520with%2520psychologically%250Agrounded%2520profiles-such%2520as%2520Deep%252C%2520Surface%252C%2520and%2520Lazy-as%2520well%2520as%2520a%2520persona-free%250AGeneral%2520Learner%2520to%2520inspect%2520the%2520base%2520LLM%2527s%2520default%2520behavior.%2520Through%2520weekly%250Aknowledge%2520acquisition%252C%2520monthly%2520strategic%2520choices%252C%2520periodic%2520tests%252C%2520and%2520peer%250Ainteraction%252C%2520we%2520can%2520track%2520the%2520dynamic%2520learning%2520progress%2520of%2520individual%2520learners%250Aover%2520a%2520full-year%2520journey.%2520Our%2520findings%2520are%2520fourfold%253A%25201%2529%2520Longitudinal%2520analysis%250Areveals%2520that%2520only%2520Deep%2520Learner%2520achieves%2520sustained%2520cognitive%2520growth.%2520Our%250Aspecially%2520designed%2520%2522trap%2520questions%2522%2520effectively%2520diagnose%2520Surface%2520Learner%2527s%250Ashallow%2520knowledge.%25202%2529%2520The%2520behavioral%2520and%2520cognitive%2520patterns%2520of%2520distinct%250Alearners%2520align%2520closely%2520with%2520their%2520psychological%2520profiles.%25203%2529%2520Learners%2527%250Aself-concept%2520scores%2520evolve%2520realistically%252C%2520with%2520the%2520General%2520Learner%2520developing%250Asurprisingly%2520high%2520self-efficacy%2520despite%2520its%2520cognitive%2520limitations.%25204%2529%250ACritically%252C%2520the%2520default%2520profile%2520of%2520base%2520LLM%2520is%2520a%2520%2522diligent%2520but%2520brittle%2520Surface%250ALearner%2522-an%2520agent%2520that%2520mimics%2520the%2520behaviors%2520of%2520a%2520good%2520student%2520but%2520lacks%2520true%252C%250Ageneralizable%2520understanding.%2520Extensive%2520simulation%2520experiments%2520demonstrate%2520that%250ALearnerAgent%2520aligns%2520well%2520with%2520real%2520scenarios%252C%2520yielding%2520more%2520insightful%2520findings%250Aabout%2520LLMs%2527%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simulating%20Human-Like%20Learning%20Dynamics%20with%20LLM-Empowered%20Agents&entry.906535625=Yu%20Yuan%20and%20Lili%20Zhao%20and%20Wei%20Chen%20and%20Guangting%20Zheng%20and%20Kai%20Zhang%20and%20Mengdi%20Zhang%20and%20Qi%20Liu&entry.1292438233=%20%20Capturing%20human%20learning%20behavior%20based%20on%20deep%20learning%20methods%20has%20become%20a%0Amajor%20research%20focus%20in%20both%20psychology%20and%20intelligent%20systems.%20Recent%0Aapproaches%20rely%20on%20controlled%20experiments%20or%20rule-based%20models%20to%20explore%0Acognitive%20processes.%20However%2C%20they%20struggle%20to%20capture%20learning%20dynamics%2C%20track%0Aprogress%20over%20time%2C%20or%20provide%20explainability.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20LearnerAgent%2C%20a%20novel%20multi-agent%20framework%20based%20on%20Large%20Language%0AModels%20%28LLMs%29%20to%20simulate%20a%20realistic%20teaching%20environment.%20To%20explore%0Ahuman-like%20learning%20dynamics%2C%20we%20construct%20learners%20with%20psychologically%0Agrounded%20profiles-such%20as%20Deep%2C%20Surface%2C%20and%20Lazy-as%20well%20as%20a%20persona-free%0AGeneral%20Learner%20to%20inspect%20the%20base%20LLM%27s%20default%20behavior.%20Through%20weekly%0Aknowledge%20acquisition%2C%20monthly%20strategic%20choices%2C%20periodic%20tests%2C%20and%20peer%0Ainteraction%2C%20we%20can%20track%20the%20dynamic%20learning%20progress%20of%20individual%20learners%0Aover%20a%20full-year%20journey.%20Our%20findings%20are%20fourfold%3A%201%29%20Longitudinal%20analysis%0Areveals%20that%20only%20Deep%20Learner%20achieves%20sustained%20cognitive%20growth.%20Our%0Aspecially%20designed%20%22trap%20questions%22%20effectively%20diagnose%20Surface%20Learner%27s%0Ashallow%20knowledge.%202%29%20The%20behavioral%20and%20cognitive%20patterns%20of%20distinct%0Alearners%20align%20closely%20with%20their%20psychological%20profiles.%203%29%20Learners%27%0Aself-concept%20scores%20evolve%20realistically%2C%20with%20the%20General%20Learner%20developing%0Asurprisingly%20high%20self-efficacy%20despite%20its%20cognitive%20limitations.%204%29%0ACritically%2C%20the%20default%20profile%20of%20base%20LLM%20is%20a%20%22diligent%20but%20brittle%20Surface%0ALearner%22-an%20agent%20that%20mimics%20the%20behaviors%20of%20a%20good%20student%20but%20lacks%20true%2C%0Ageneralizable%20understanding.%20Extensive%20simulation%20experiments%20demonstrate%20that%0ALearnerAgent%20aligns%20well%20with%20real%20scenarios%2C%20yielding%20more%20insightful%20findings%0Aabout%20LLMs%27%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05622v1&entry.124074799=Read"},
{"title": "Streamlining Admission with LOR Insights: AI-Based Leadership Assessment\n  in Online Master's Program", "author": "Meryem Yilmaz Soylu and Adrian Gallard and Jeonghyun Lee and Gayane Grigoryan and Rushil Desai and Stephen Harmon", "abstract": "  Letters of recommendation (LORs) provide valuable insights into candidates'\ncapabilities and experiences beyond standardized test scores. However,\nreviewing these text-heavy materials is time-consuming and labor-intensive. To\naddress this challenge and support the admission committee in providing\nfeedback for students' professional growth, our study introduces LORI: LOR\nInsights, a novel AI-based detection tool for assessing leadership skills in\nLORs submitted by online master's program applicants. By employing natural\nlanguage processing and leveraging large language models using RoBERTa and\nLLAMA, we seek to identify leadership attributes such as teamwork,\ncommunication, and innovation. Our latest RoBERTa model achieves a weighted F1\nscore of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong\nlevel of consistency in our test data. With the growing importance of\nleadership skills in the STEM sector, integrating LORI into the graduate\nadmissions process is crucial for accurately assessing applicants' leadership\ncapabilities. This approach not only streamlines the admissions process but\nalso automates and ensures a more comprehensive evaluation of candidates'\ncapabilities.\n", "link": "http://arxiv.org/abs/2508.05513v1", "date": "2025-08-07", "relevancy": 2.2163, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4368}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streamlining%20Admission%20with%20LOR%20Insights%3A%20AI-Based%20Leadership%20Assessment%0A%20%20in%20Online%20Master%27s%20Program&body=Title%3A%20Streamlining%20Admission%20with%20LOR%20Insights%3A%20AI-Based%20Leadership%20Assessment%0A%20%20in%20Online%20Master%27s%20Program%0AAuthor%3A%20Meryem%20Yilmaz%20Soylu%20and%20Adrian%20Gallard%20and%20Jeonghyun%20Lee%20and%20Gayane%20Grigoryan%20and%20Rushil%20Desai%20and%20Stephen%20Harmon%0AAbstract%3A%20%20%20Letters%20of%20recommendation%20%28LORs%29%20provide%20valuable%20insights%20into%20candidates%27%0Acapabilities%20and%20experiences%20beyond%20standardized%20test%20scores.%20However%2C%0Areviewing%20these%20text-heavy%20materials%20is%20time-consuming%20and%20labor-intensive.%20To%0Aaddress%20this%20challenge%20and%20support%20the%20admission%20committee%20in%20providing%0Afeedback%20for%20students%27%20professional%20growth%2C%20our%20study%20introduces%20LORI%3A%20LOR%0AInsights%2C%20a%20novel%20AI-based%20detection%20tool%20for%20assessing%20leadership%20skills%20in%0ALORs%20submitted%20by%20online%20master%27s%20program%20applicants.%20By%20employing%20natural%0Alanguage%20processing%20and%20leveraging%20large%20language%20models%20using%20RoBERTa%20and%0ALLAMA%2C%20we%20seek%20to%20identify%20leadership%20attributes%20such%20as%20teamwork%2C%0Acommunication%2C%20and%20innovation.%20Our%20latest%20RoBERTa%20model%20achieves%20a%20weighted%20F1%0Ascore%20of%2091.6%25%2C%20a%20precision%20of%2092.4%25%2C%20and%20a%20recall%20of%2091.6%25%2C%20showing%20a%20strong%0Alevel%20of%20consistency%20in%20our%20test%20data.%20With%20the%20growing%20importance%20of%0Aleadership%20skills%20in%20the%20STEM%20sector%2C%20integrating%20LORI%20into%20the%20graduate%0Aadmissions%20process%20is%20crucial%20for%20accurately%20assessing%20applicants%27%20leadership%0Acapabilities.%20This%20approach%20not%20only%20streamlines%20the%20admissions%20process%20but%0Aalso%20automates%20and%20ensures%20a%20more%20comprehensive%20evaluation%20of%20candidates%27%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamlining%2520Admission%2520with%2520LOR%2520Insights%253A%2520AI-Based%2520Leadership%2520Assessment%250A%2520%2520in%2520Online%2520Master%2527s%2520Program%26entry.906535625%3DMeryem%2520Yilmaz%2520Soylu%2520and%2520Adrian%2520Gallard%2520and%2520Jeonghyun%2520Lee%2520and%2520Gayane%2520Grigoryan%2520and%2520Rushil%2520Desai%2520and%2520Stephen%2520Harmon%26entry.1292438233%3D%2520%2520Letters%2520of%2520recommendation%2520%2528LORs%2529%2520provide%2520valuable%2520insights%2520into%2520candidates%2527%250Acapabilities%2520and%2520experiences%2520beyond%2520standardized%2520test%2520scores.%2520However%252C%250Areviewing%2520these%2520text-heavy%2520materials%2520is%2520time-consuming%2520and%2520labor-intensive.%2520To%250Aaddress%2520this%2520challenge%2520and%2520support%2520the%2520admission%2520committee%2520in%2520providing%250Afeedback%2520for%2520students%2527%2520professional%2520growth%252C%2520our%2520study%2520introduces%2520LORI%253A%2520LOR%250AInsights%252C%2520a%2520novel%2520AI-based%2520detection%2520tool%2520for%2520assessing%2520leadership%2520skills%2520in%250ALORs%2520submitted%2520by%2520online%2520master%2527s%2520program%2520applicants.%2520By%2520employing%2520natural%250Alanguage%2520processing%2520and%2520leveraging%2520large%2520language%2520models%2520using%2520RoBERTa%2520and%250ALLAMA%252C%2520we%2520seek%2520to%2520identify%2520leadership%2520attributes%2520such%2520as%2520teamwork%252C%250Acommunication%252C%2520and%2520innovation.%2520Our%2520latest%2520RoBERTa%2520model%2520achieves%2520a%2520weighted%2520F1%250Ascore%2520of%252091.6%2525%252C%2520a%2520precision%2520of%252092.4%2525%252C%2520and%2520a%2520recall%2520of%252091.6%2525%252C%2520showing%2520a%2520strong%250Alevel%2520of%2520consistency%2520in%2520our%2520test%2520data.%2520With%2520the%2520growing%2520importance%2520of%250Aleadership%2520skills%2520in%2520the%2520STEM%2520sector%252C%2520integrating%2520LORI%2520into%2520the%2520graduate%250Aadmissions%2520process%2520is%2520crucial%2520for%2520accurately%2520assessing%2520applicants%2527%2520leadership%250Acapabilities.%2520This%2520approach%2520not%2520only%2520streamlines%2520the%2520admissions%2520process%2520but%250Aalso%2520automates%2520and%2520ensures%2520a%2520more%2520comprehensive%2520evaluation%2520of%2520candidates%2527%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streamlining%20Admission%20with%20LOR%20Insights%3A%20AI-Based%20Leadership%20Assessment%0A%20%20in%20Online%20Master%27s%20Program&entry.906535625=Meryem%20Yilmaz%20Soylu%20and%20Adrian%20Gallard%20and%20Jeonghyun%20Lee%20and%20Gayane%20Grigoryan%20and%20Rushil%20Desai%20and%20Stephen%20Harmon&entry.1292438233=%20%20Letters%20of%20recommendation%20%28LORs%29%20provide%20valuable%20insights%20into%20candidates%27%0Acapabilities%20and%20experiences%20beyond%20standardized%20test%20scores.%20However%2C%0Areviewing%20these%20text-heavy%20materials%20is%20time-consuming%20and%20labor-intensive.%20To%0Aaddress%20this%20challenge%20and%20support%20the%20admission%20committee%20in%20providing%0Afeedback%20for%20students%27%20professional%20growth%2C%20our%20study%20introduces%20LORI%3A%20LOR%0AInsights%2C%20a%20novel%20AI-based%20detection%20tool%20for%20assessing%20leadership%20skills%20in%0ALORs%20submitted%20by%20online%20master%27s%20program%20applicants.%20By%20employing%20natural%0Alanguage%20processing%20and%20leveraging%20large%20language%20models%20using%20RoBERTa%20and%0ALLAMA%2C%20we%20seek%20to%20identify%20leadership%20attributes%20such%20as%20teamwork%2C%0Acommunication%2C%20and%20innovation.%20Our%20latest%20RoBERTa%20model%20achieves%20a%20weighted%20F1%0Ascore%20of%2091.6%25%2C%20a%20precision%20of%2092.4%25%2C%20and%20a%20recall%20of%2091.6%25%2C%20showing%20a%20strong%0Alevel%20of%20consistency%20in%20our%20test%20data.%20With%20the%20growing%20importance%20of%0Aleadership%20skills%20in%20the%20STEM%20sector%2C%20integrating%20LORI%20into%20the%20graduate%0Aadmissions%20process%20is%20crucial%20for%20accurately%20assessing%20applicants%27%20leadership%0Acapabilities.%20This%20approach%20not%20only%20streamlines%20the%20admissions%20process%20but%0Aalso%20automates%20and%20ensures%20a%20more%20comprehensive%20evaluation%20of%20candidates%27%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05513v1&entry.124074799=Read"},
{"title": "When Deepfake Detection Meets Graph Neural Network:a Unified and\n  Lightweight Learning Framework", "author": "Haoyu Liu and Chaoyu Gong and Mengke He and Jiate Li and Kai Han and Siqiang Luo", "abstract": "  The proliferation of generative video models has made detecting AI-generated\nand manipulated videos an urgent challenge. Existing detection approaches often\nfail to generalize across diverse manipulation types due to their reliance on\nisolated spatial, temporal, or spectral information, and typically require\nlarge models to perform well. This paper introduces SSTGNN, a lightweight\nSpatial-Spectral-Temporal Graph Neural Network framework that represents videos\nas structured graphs, enabling joint reasoning over spatial inconsistencies,\ntemporal artifacts, and spectral distortions. SSTGNN incorporates learnable\nspectral filters and temporal differential modeling into a graph-based\narchitecture, capturing subtle manipulation traces more effectively. Extensive\nexperiments on diverse benchmark datasets demonstrate that SSTGNN not only\nachieves superior performance in both in-domain and cross-domain settings, but\nalso offers strong robustness against unseen manipulations. Remarkably, SSTGNN\naccomplishes these results with up to 42.4$\\times$ fewer parameters than\nstate-of-the-art models, making it highly lightweight and scalable for\nreal-world deployment.\n", "link": "http://arxiv.org/abs/2508.05526v1", "date": "2025-08-07", "relevancy": 2.2146, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5594}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5505}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Deepfake%20Detection%20Meets%20Graph%20Neural%20Network%3Aa%20Unified%20and%0A%20%20Lightweight%20Learning%20Framework&body=Title%3A%20When%20Deepfake%20Detection%20Meets%20Graph%20Neural%20Network%3Aa%20Unified%20and%0A%20%20Lightweight%20Learning%20Framework%0AAuthor%3A%20Haoyu%20Liu%20and%20Chaoyu%20Gong%20and%20Mengke%20He%20and%20Jiate%20Li%20and%20Kai%20Han%20and%20Siqiang%20Luo%0AAbstract%3A%20%20%20The%20proliferation%20of%20generative%20video%20models%20has%20made%20detecting%20AI-generated%0Aand%20manipulated%20videos%20an%20urgent%20challenge.%20Existing%20detection%20approaches%20often%0Afail%20to%20generalize%20across%20diverse%20manipulation%20types%20due%20to%20their%20reliance%20on%0Aisolated%20spatial%2C%20temporal%2C%20or%20spectral%20information%2C%20and%20typically%20require%0Alarge%20models%20to%20perform%20well.%20This%20paper%20introduces%20SSTGNN%2C%20a%20lightweight%0ASpatial-Spectral-Temporal%20Graph%20Neural%20Network%20framework%20that%20represents%20videos%0Aas%20structured%20graphs%2C%20enabling%20joint%20reasoning%20over%20spatial%20inconsistencies%2C%0Atemporal%20artifacts%2C%20and%20spectral%20distortions.%20SSTGNN%20incorporates%20learnable%0Aspectral%20filters%20and%20temporal%20differential%20modeling%20into%20a%20graph-based%0Aarchitecture%2C%20capturing%20subtle%20manipulation%20traces%20more%20effectively.%20Extensive%0Aexperiments%20on%20diverse%20benchmark%20datasets%20demonstrate%20that%20SSTGNN%20not%20only%0Aachieves%20superior%20performance%20in%20both%20in-domain%20and%20cross-domain%20settings%2C%20but%0Aalso%20offers%20strong%20robustness%20against%20unseen%20manipulations.%20Remarkably%2C%20SSTGNN%0Aaccomplishes%20these%20results%20with%20up%20to%2042.4%24%5Ctimes%24%20fewer%20parameters%20than%0Astate-of-the-art%20models%2C%20making%20it%20highly%20lightweight%20and%20scalable%20for%0Areal-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Deepfake%2520Detection%2520Meets%2520Graph%2520Neural%2520Network%253Aa%2520Unified%2520and%250A%2520%2520Lightweight%2520Learning%2520Framework%26entry.906535625%3DHaoyu%2520Liu%2520and%2520Chaoyu%2520Gong%2520and%2520Mengke%2520He%2520and%2520Jiate%2520Li%2520and%2520Kai%2520Han%2520and%2520Siqiang%2520Luo%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520generative%2520video%2520models%2520has%2520made%2520detecting%2520AI-generated%250Aand%2520manipulated%2520videos%2520an%2520urgent%2520challenge.%2520Existing%2520detection%2520approaches%2520often%250Afail%2520to%2520generalize%2520across%2520diverse%2520manipulation%2520types%2520due%2520to%2520their%2520reliance%2520on%250Aisolated%2520spatial%252C%2520temporal%252C%2520or%2520spectral%2520information%252C%2520and%2520typically%2520require%250Alarge%2520models%2520to%2520perform%2520well.%2520This%2520paper%2520introduces%2520SSTGNN%252C%2520a%2520lightweight%250ASpatial-Spectral-Temporal%2520Graph%2520Neural%2520Network%2520framework%2520that%2520represents%2520videos%250Aas%2520structured%2520graphs%252C%2520enabling%2520joint%2520reasoning%2520over%2520spatial%2520inconsistencies%252C%250Atemporal%2520artifacts%252C%2520and%2520spectral%2520distortions.%2520SSTGNN%2520incorporates%2520learnable%250Aspectral%2520filters%2520and%2520temporal%2520differential%2520modeling%2520into%2520a%2520graph-based%250Aarchitecture%252C%2520capturing%2520subtle%2520manipulation%2520traces%2520more%2520effectively.%2520Extensive%250Aexperiments%2520on%2520diverse%2520benchmark%2520datasets%2520demonstrate%2520that%2520SSTGNN%2520not%2520only%250Aachieves%2520superior%2520performance%2520in%2520both%2520in-domain%2520and%2520cross-domain%2520settings%252C%2520but%250Aalso%2520offers%2520strong%2520robustness%2520against%2520unseen%2520manipulations.%2520Remarkably%252C%2520SSTGNN%250Aaccomplishes%2520these%2520results%2520with%2520up%2520to%252042.4%2524%255Ctimes%2524%2520fewer%2520parameters%2520than%250Astate-of-the-art%2520models%252C%2520making%2520it%2520highly%2520lightweight%2520and%2520scalable%2520for%250Areal-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Deepfake%20Detection%20Meets%20Graph%20Neural%20Network%3Aa%20Unified%20and%0A%20%20Lightweight%20Learning%20Framework&entry.906535625=Haoyu%20Liu%20and%20Chaoyu%20Gong%20and%20Mengke%20He%20and%20Jiate%20Li%20and%20Kai%20Han%20and%20Siqiang%20Luo&entry.1292438233=%20%20The%20proliferation%20of%20generative%20video%20models%20has%20made%20detecting%20AI-generated%0Aand%20manipulated%20videos%20an%20urgent%20challenge.%20Existing%20detection%20approaches%20often%0Afail%20to%20generalize%20across%20diverse%20manipulation%20types%20due%20to%20their%20reliance%20on%0Aisolated%20spatial%2C%20temporal%2C%20or%20spectral%20information%2C%20and%20typically%20require%0Alarge%20models%20to%20perform%20well.%20This%20paper%20introduces%20SSTGNN%2C%20a%20lightweight%0ASpatial-Spectral-Temporal%20Graph%20Neural%20Network%20framework%20that%20represents%20videos%0Aas%20structured%20graphs%2C%20enabling%20joint%20reasoning%20over%20spatial%20inconsistencies%2C%0Atemporal%20artifacts%2C%20and%20spectral%20distortions.%20SSTGNN%20incorporates%20learnable%0Aspectral%20filters%20and%20temporal%20differential%20modeling%20into%20a%20graph-based%0Aarchitecture%2C%20capturing%20subtle%20manipulation%20traces%20more%20effectively.%20Extensive%0Aexperiments%20on%20diverse%20benchmark%20datasets%20demonstrate%20that%20SSTGNN%20not%20only%0Aachieves%20superior%20performance%20in%20both%20in-domain%20and%20cross-domain%20settings%2C%20but%0Aalso%20offers%20strong%20robustness%20against%20unseen%20manipulations.%20Remarkably%2C%20SSTGNN%0Aaccomplishes%20these%20results%20with%20up%20to%2042.4%24%5Ctimes%24%20fewer%20parameters%20than%0Astate-of-the-art%20models%2C%20making%20it%20highly%20lightweight%20and%20scalable%20for%0Areal-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05526v1&entry.124074799=Read"},
{"title": "ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language\n  Tracking", "author": "Xiao Wang and Liye Jin and Xufeng Lou and Shiao Wang and Lan Chen and Bo Jiang and Zhipeng Zhang", "abstract": "  Vision-language tracking has received increasing attention in recent years,\nas textual information can effectively address the inflexibility and inaccuracy\nassociated with specifying the target object to be tracked. Existing works\neither directly fuse the fixed language with vision features or simply modify\nusing attention, however, their performance is still limited. Recently, some\nresearchers have explored using text generation to adapt to the variations in\nthe target during tracking, however, these works fail to provide insights into\nthe model's reasoning process and do not fully leverage the advantages of large\nmodels, which further limits their overall performance. To address the\naforementioned issues, this paper proposes a novel reasoning-based\nvision-language tracking framework, named ReasoningTrack, based on a\npre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning)\nand reinforcement learning GRPO are used for the optimization of reasoning and\nlanguage generation. We embed the updated language descriptions and feed them\ninto a unified tracking backbone network together with vision features. Then,\nwe adopt a tracking head to predict the specific location of the target object.\nIn addition, we propose a large-scale long-term vision-language tracking\nbenchmark dataset, termed TNLLT, which contains 200 video sequences. 20\nbaseline visual trackers are re-trained and evaluated on this dataset, which\nbuilds a solid foundation for the vision-language visual tracking task.\nExtensive experiments on multiple vision-language tracking benchmark datasets\nfully validated the effectiveness of our proposed reasoning-based natural\nlanguage generation strategy. The source code of this paper will be released on\nhttps://github.com/Event-AHU/Open_VLTrack\n", "link": "http://arxiv.org/abs/2508.05221v1", "date": "2025-08-07", "relevancy": 2.2117, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReasoningTrack%3A%20Chain-of-Thought%20Reasoning%20for%20Long-term%20Vision-Language%0A%20%20Tracking&body=Title%3A%20ReasoningTrack%3A%20Chain-of-Thought%20Reasoning%20for%20Long-term%20Vision-Language%0A%20%20Tracking%0AAuthor%3A%20Xiao%20Wang%20and%20Liye%20Jin%20and%20Xufeng%20Lou%20and%20Shiao%20Wang%20and%20Lan%20Chen%20and%20Bo%20Jiang%20and%20Zhipeng%20Zhang%0AAbstract%3A%20%20%20Vision-language%20tracking%20has%20received%20increasing%20attention%20in%20recent%20years%2C%0Aas%20textual%20information%20can%20effectively%20address%20the%20inflexibility%20and%20inaccuracy%0Aassociated%20with%20specifying%20the%20target%20object%20to%20be%20tracked.%20Existing%20works%0Aeither%20directly%20fuse%20the%20fixed%20language%20with%20vision%20features%20or%20simply%20modify%0Ausing%20attention%2C%20however%2C%20their%20performance%20is%20still%20limited.%20Recently%2C%20some%0Aresearchers%20have%20explored%20using%20text%20generation%20to%20adapt%20to%20the%20variations%20in%0Athe%20target%20during%20tracking%2C%20however%2C%20these%20works%20fail%20to%20provide%20insights%20into%0Athe%20model%27s%20reasoning%20process%20and%20do%20not%20fully%20leverage%20the%20advantages%20of%20large%0Amodels%2C%20which%20further%20limits%20their%20overall%20performance.%20To%20address%20the%0Aaforementioned%20issues%2C%20this%20paper%20proposes%20a%20novel%20reasoning-based%0Avision-language%20tracking%20framework%2C%20named%20ReasoningTrack%2C%20based%20on%20a%0Apre-trained%20vision-language%20model%20Qwen2.5-VL.%20Both%20SFT%20%28Supervised%20Fine-Tuning%29%0Aand%20reinforcement%20learning%20GRPO%20are%20used%20for%20the%20optimization%20of%20reasoning%20and%0Alanguage%20generation.%20We%20embed%20the%20updated%20language%20descriptions%20and%20feed%20them%0Ainto%20a%20unified%20tracking%20backbone%20network%20together%20with%20vision%20features.%20Then%2C%0Awe%20adopt%20a%20tracking%20head%20to%20predict%20the%20specific%20location%20of%20the%20target%20object.%0AIn%20addition%2C%20we%20propose%20a%20large-scale%20long-term%20vision-language%20tracking%0Abenchmark%20dataset%2C%20termed%20TNLLT%2C%20which%20contains%20200%20video%20sequences.%2020%0Abaseline%20visual%20trackers%20are%20re-trained%20and%20evaluated%20on%20this%20dataset%2C%20which%0Abuilds%20a%20solid%20foundation%20for%20the%20vision-language%20visual%20tracking%20task.%0AExtensive%20experiments%20on%20multiple%20vision-language%20tracking%20benchmark%20datasets%0Afully%20validated%20the%20effectiveness%20of%20our%20proposed%20reasoning-based%20natural%0Alanguage%20generation%20strategy.%20The%20source%20code%20of%20this%20paper%20will%20be%20released%20on%0Ahttps%3A//github.com/Event-AHU/Open_VLTrack%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoningTrack%253A%2520Chain-of-Thought%2520Reasoning%2520for%2520Long-term%2520Vision-Language%250A%2520%2520Tracking%26entry.906535625%3DXiao%2520Wang%2520and%2520Liye%2520Jin%2520and%2520Xufeng%2520Lou%2520and%2520Shiao%2520Wang%2520and%2520Lan%2520Chen%2520and%2520Bo%2520Jiang%2520and%2520Zhipeng%2520Zhang%26entry.1292438233%3D%2520%2520Vision-language%2520tracking%2520has%2520received%2520increasing%2520attention%2520in%2520recent%2520years%252C%250Aas%2520textual%2520information%2520can%2520effectively%2520address%2520the%2520inflexibility%2520and%2520inaccuracy%250Aassociated%2520with%2520specifying%2520the%2520target%2520object%2520to%2520be%2520tracked.%2520Existing%2520works%250Aeither%2520directly%2520fuse%2520the%2520fixed%2520language%2520with%2520vision%2520features%2520or%2520simply%2520modify%250Ausing%2520attention%252C%2520however%252C%2520their%2520performance%2520is%2520still%2520limited.%2520Recently%252C%2520some%250Aresearchers%2520have%2520explored%2520using%2520text%2520generation%2520to%2520adapt%2520to%2520the%2520variations%2520in%250Athe%2520target%2520during%2520tracking%252C%2520however%252C%2520these%2520works%2520fail%2520to%2520provide%2520insights%2520into%250Athe%2520model%2527s%2520reasoning%2520process%2520and%2520do%2520not%2520fully%2520leverage%2520the%2520advantages%2520of%2520large%250Amodels%252C%2520which%2520further%2520limits%2520their%2520overall%2520performance.%2520To%2520address%2520the%250Aaforementioned%2520issues%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520reasoning-based%250Avision-language%2520tracking%2520framework%252C%2520named%2520ReasoningTrack%252C%2520based%2520on%2520a%250Apre-trained%2520vision-language%2520model%2520Qwen2.5-VL.%2520Both%2520SFT%2520%2528Supervised%2520Fine-Tuning%2529%250Aand%2520reinforcement%2520learning%2520GRPO%2520are%2520used%2520for%2520the%2520optimization%2520of%2520reasoning%2520and%250Alanguage%2520generation.%2520We%2520embed%2520the%2520updated%2520language%2520descriptions%2520and%2520feed%2520them%250Ainto%2520a%2520unified%2520tracking%2520backbone%2520network%2520together%2520with%2520vision%2520features.%2520Then%252C%250Awe%2520adopt%2520a%2520tracking%2520head%2520to%2520predict%2520the%2520specific%2520location%2520of%2520the%2520target%2520object.%250AIn%2520addition%252C%2520we%2520propose%2520a%2520large-scale%2520long-term%2520vision-language%2520tracking%250Abenchmark%2520dataset%252C%2520termed%2520TNLLT%252C%2520which%2520contains%2520200%2520video%2520sequences.%252020%250Abaseline%2520visual%2520trackers%2520are%2520re-trained%2520and%2520evaluated%2520on%2520this%2520dataset%252C%2520which%250Abuilds%2520a%2520solid%2520foundation%2520for%2520the%2520vision-language%2520visual%2520tracking%2520task.%250AExtensive%2520experiments%2520on%2520multiple%2520vision-language%2520tracking%2520benchmark%2520datasets%250Afully%2520validated%2520the%2520effectiveness%2520of%2520our%2520proposed%2520reasoning-based%2520natural%250Alanguage%2520generation%2520strategy.%2520The%2520source%2520code%2520of%2520this%2520paper%2520will%2520be%2520released%2520on%250Ahttps%253A//github.com/Event-AHU/Open_VLTrack%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReasoningTrack%3A%20Chain-of-Thought%20Reasoning%20for%20Long-term%20Vision-Language%0A%20%20Tracking&entry.906535625=Xiao%20Wang%20and%20Liye%20Jin%20and%20Xufeng%20Lou%20and%20Shiao%20Wang%20and%20Lan%20Chen%20and%20Bo%20Jiang%20and%20Zhipeng%20Zhang&entry.1292438233=%20%20Vision-language%20tracking%20has%20received%20increasing%20attention%20in%20recent%20years%2C%0Aas%20textual%20information%20can%20effectively%20address%20the%20inflexibility%20and%20inaccuracy%0Aassociated%20with%20specifying%20the%20target%20object%20to%20be%20tracked.%20Existing%20works%0Aeither%20directly%20fuse%20the%20fixed%20language%20with%20vision%20features%20or%20simply%20modify%0Ausing%20attention%2C%20however%2C%20their%20performance%20is%20still%20limited.%20Recently%2C%20some%0Aresearchers%20have%20explored%20using%20text%20generation%20to%20adapt%20to%20the%20variations%20in%0Athe%20target%20during%20tracking%2C%20however%2C%20these%20works%20fail%20to%20provide%20insights%20into%0Athe%20model%27s%20reasoning%20process%20and%20do%20not%20fully%20leverage%20the%20advantages%20of%20large%0Amodels%2C%20which%20further%20limits%20their%20overall%20performance.%20To%20address%20the%0Aaforementioned%20issues%2C%20this%20paper%20proposes%20a%20novel%20reasoning-based%0Avision-language%20tracking%20framework%2C%20named%20ReasoningTrack%2C%20based%20on%20a%0Apre-trained%20vision-language%20model%20Qwen2.5-VL.%20Both%20SFT%20%28Supervised%20Fine-Tuning%29%0Aand%20reinforcement%20learning%20GRPO%20are%20used%20for%20the%20optimization%20of%20reasoning%20and%0Alanguage%20generation.%20We%20embed%20the%20updated%20language%20descriptions%20and%20feed%20them%0Ainto%20a%20unified%20tracking%20backbone%20network%20together%20with%20vision%20features.%20Then%2C%0Awe%20adopt%20a%20tracking%20head%20to%20predict%20the%20specific%20location%20of%20the%20target%20object.%0AIn%20addition%2C%20we%20propose%20a%20large-scale%20long-term%20vision-language%20tracking%0Abenchmark%20dataset%2C%20termed%20TNLLT%2C%20which%20contains%20200%20video%20sequences.%2020%0Abaseline%20visual%20trackers%20are%20re-trained%20and%20evaluated%20on%20this%20dataset%2C%20which%0Abuilds%20a%20solid%20foundation%20for%20the%20vision-language%20visual%20tracking%20task.%0AExtensive%20experiments%20on%20multiple%20vision-language%20tracking%20benchmark%20datasets%0Afully%20validated%20the%20effectiveness%20of%20our%20proposed%20reasoning-based%20natural%0Alanguage%20generation%20strategy.%20The%20source%20code%20of%20this%20paper%20will%20be%20released%20on%0Ahttps%3A//github.com/Event-AHU/Open_VLTrack%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05221v1&entry.124074799=Read"},
{"title": "Efficient Attention Mechanisms for Large Language Models: A Survey", "author": "Yutao Sun and Zhenyu Li and Yike Zhang and Tengyu Pan and Bowen Dong and Yuyi Guo and Jianyong Wang", "abstract": "  Transformer-based architectures have become the prevailing backbone of large\nlanguage models. However, the quadratic time and memory complexity of\nself-attention remains a fundamental obstacle to efficient long-context\nmodeling. To address this limitation, recent research has introduced two\nprincipal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent\nformulations, or fastweight dynamics, thereby enabling scalable inference with\nreduced computational overhead. Sparse attention techniques, in contrast, limit\nattention computation to selected subsets of tokens based on fixed patterns,\nblock-wise routing, or clustering strategies, enhancing efficiency while\npreserving contextual coverage. This survey provides a systematic and\ncomprehensive overview of these developments, integrating both algorithmic\ninnovations and hardware-level considerations. In addition, we analyze the\nincorporation of efficient attention into largescale pre-trained language\nmodels, including both architectures built entirely on efficient attention and\nhybrid designs that combine local and global components. By aligning\ntheoretical foundations with practical deployment strategies, this work aims to\nserve as a foundational reference for advancing the design of scalable and\nefficient language models.\n", "link": "http://arxiv.org/abs/2507.19595v2", "date": "2025-08-07", "relevancy": 2.1965, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5767}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Attention%20Mechanisms%20for%20Large%20Language%20Models%3A%20A%20Survey&body=Title%3A%20Efficient%20Attention%20Mechanisms%20for%20Large%20Language%20Models%3A%20A%20Survey%0AAuthor%3A%20Yutao%20Sun%20and%20Zhenyu%20Li%20and%20Yike%20Zhang%20and%20Tengyu%20Pan%20and%20Bowen%20Dong%20and%20Yuyi%20Guo%20and%20Jianyong%20Wang%0AAbstract%3A%20%20%20Transformer-based%20architectures%20have%20become%20the%20prevailing%20backbone%20of%20large%0Alanguage%20models.%20However%2C%20the%20quadratic%20time%20and%20memory%20complexity%20of%0Aself-attention%20remains%20a%20fundamental%20obstacle%20to%20efficient%20long-context%0Amodeling.%20To%20address%20this%20limitation%2C%20recent%20research%20has%20introduced%20two%0Aprincipal%20categories%20of%20efficient%20attention%20mechanisms.%20Linear%20attention%0Amethods%20achieve%20linear%20complexity%20through%20kernel%20approximations%2C%20recurrent%0Aformulations%2C%20or%20fastweight%20dynamics%2C%20thereby%20enabling%20scalable%20inference%20with%0Areduced%20computational%20overhead.%20Sparse%20attention%20techniques%2C%20in%20contrast%2C%20limit%0Aattention%20computation%20to%20selected%20subsets%20of%20tokens%20based%20on%20fixed%20patterns%2C%0Ablock-wise%20routing%2C%20or%20clustering%20strategies%2C%20enhancing%20efficiency%20while%0Apreserving%20contextual%20coverage.%20This%20survey%20provides%20a%20systematic%20and%0Acomprehensive%20overview%20of%20these%20developments%2C%20integrating%20both%20algorithmic%0Ainnovations%20and%20hardware-level%20considerations.%20In%20addition%2C%20we%20analyze%20the%0Aincorporation%20of%20efficient%20attention%20into%20largescale%20pre-trained%20language%0Amodels%2C%20including%20both%20architectures%20built%20entirely%20on%20efficient%20attention%20and%0Ahybrid%20designs%20that%20combine%20local%20and%20global%20components.%20By%20aligning%0Atheoretical%20foundations%20with%20practical%20deployment%20strategies%2C%20this%20work%20aims%20to%0Aserve%20as%20a%20foundational%20reference%20for%20advancing%20the%20design%20of%20scalable%20and%0Aefficient%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19595v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Attention%2520Mechanisms%2520for%2520Large%2520Language%2520Models%253A%2520A%2520Survey%26entry.906535625%3DYutao%2520Sun%2520and%2520Zhenyu%2520Li%2520and%2520Yike%2520Zhang%2520and%2520Tengyu%2520Pan%2520and%2520Bowen%2520Dong%2520and%2520Yuyi%2520Guo%2520and%2520Jianyong%2520Wang%26entry.1292438233%3D%2520%2520Transformer-based%2520architectures%2520have%2520become%2520the%2520prevailing%2520backbone%2520of%2520large%250Alanguage%2520models.%2520However%252C%2520the%2520quadratic%2520time%2520and%2520memory%2520complexity%2520of%250Aself-attention%2520remains%2520a%2520fundamental%2520obstacle%2520to%2520efficient%2520long-context%250Amodeling.%2520To%2520address%2520this%2520limitation%252C%2520recent%2520research%2520has%2520introduced%2520two%250Aprincipal%2520categories%2520of%2520efficient%2520attention%2520mechanisms.%2520Linear%2520attention%250Amethods%2520achieve%2520linear%2520complexity%2520through%2520kernel%2520approximations%252C%2520recurrent%250Aformulations%252C%2520or%2520fastweight%2520dynamics%252C%2520thereby%2520enabling%2520scalable%2520inference%2520with%250Areduced%2520computational%2520overhead.%2520Sparse%2520attention%2520techniques%252C%2520in%2520contrast%252C%2520limit%250Aattention%2520computation%2520to%2520selected%2520subsets%2520of%2520tokens%2520based%2520on%2520fixed%2520patterns%252C%250Ablock-wise%2520routing%252C%2520or%2520clustering%2520strategies%252C%2520enhancing%2520efficiency%2520while%250Apreserving%2520contextual%2520coverage.%2520This%2520survey%2520provides%2520a%2520systematic%2520and%250Acomprehensive%2520overview%2520of%2520these%2520developments%252C%2520integrating%2520both%2520algorithmic%250Ainnovations%2520and%2520hardware-level%2520considerations.%2520In%2520addition%252C%2520we%2520analyze%2520the%250Aincorporation%2520of%2520efficient%2520attention%2520into%2520largescale%2520pre-trained%2520language%250Amodels%252C%2520including%2520both%2520architectures%2520built%2520entirely%2520on%2520efficient%2520attention%2520and%250Ahybrid%2520designs%2520that%2520combine%2520local%2520and%2520global%2520components.%2520By%2520aligning%250Atheoretical%2520foundations%2520with%2520practical%2520deployment%2520strategies%252C%2520this%2520work%2520aims%2520to%250Aserve%2520as%2520a%2520foundational%2520reference%2520for%2520advancing%2520the%2520design%2520of%2520scalable%2520and%250Aefficient%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19595v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Attention%20Mechanisms%20for%20Large%20Language%20Models%3A%20A%20Survey&entry.906535625=Yutao%20Sun%20and%20Zhenyu%20Li%20and%20Yike%20Zhang%20and%20Tengyu%20Pan%20and%20Bowen%20Dong%20and%20Yuyi%20Guo%20and%20Jianyong%20Wang&entry.1292438233=%20%20Transformer-based%20architectures%20have%20become%20the%20prevailing%20backbone%20of%20large%0Alanguage%20models.%20However%2C%20the%20quadratic%20time%20and%20memory%20complexity%20of%0Aself-attention%20remains%20a%20fundamental%20obstacle%20to%20efficient%20long-context%0Amodeling.%20To%20address%20this%20limitation%2C%20recent%20research%20has%20introduced%20two%0Aprincipal%20categories%20of%20efficient%20attention%20mechanisms.%20Linear%20attention%0Amethods%20achieve%20linear%20complexity%20through%20kernel%20approximations%2C%20recurrent%0Aformulations%2C%20or%20fastweight%20dynamics%2C%20thereby%20enabling%20scalable%20inference%20with%0Areduced%20computational%20overhead.%20Sparse%20attention%20techniques%2C%20in%20contrast%2C%20limit%0Aattention%20computation%20to%20selected%20subsets%20of%20tokens%20based%20on%20fixed%20patterns%2C%0Ablock-wise%20routing%2C%20or%20clustering%20strategies%2C%20enhancing%20efficiency%20while%0Apreserving%20contextual%20coverage.%20This%20survey%20provides%20a%20systematic%20and%0Acomprehensive%20overview%20of%20these%20developments%2C%20integrating%20both%20algorithmic%0Ainnovations%20and%20hardware-level%20considerations.%20In%20addition%2C%20we%20analyze%20the%0Aincorporation%20of%20efficient%20attention%20into%20largescale%20pre-trained%20language%0Amodels%2C%20including%20both%20architectures%20built%20entirely%20on%20efficient%20attention%20and%0Ahybrid%20designs%20that%20combine%20local%20and%20global%20components.%20By%20aligning%0Atheoretical%20foundations%20with%20practical%20deployment%20strategies%2C%20this%20work%20aims%20to%0Aserve%20as%20a%20foundational%20reference%20for%20advancing%20the%20design%20of%20scalable%20and%0Aefficient%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19595v2&entry.124074799=Read"},
{"title": "Smoothing Slot Attention Iterations and Recurrences", "author": "Rongzhen Zhao and Wenyan Yang and Juho Kannala and Joni Pajarinen", "abstract": "  Slot Attention (SA) and its variants lie at the heart of mainstream\nObject-Centric Learning (OCL). Objects in an image can be aggregated into\nrespective slot vectors, by \\textit{iteratively} refining cold-start query\nvectors, typically three times, via SA on image features. For video, such\naggregation is \\textit{recurrently} shared across frames, with queries\ncold-started on the first frame while transitioned from the previous frame's\nslots on non-first frames. However, the cold-start queries lack sample-specific\ncues thus hinder precise aggregation on the image or video's first frame; Also,\nnon-first frames' queries are already sample-specific thus require transforms\ndifferent from the first frame's aggregation. We address these issues for the\nfirst time with our \\textit{SmoothSA}: (1) To smooth SA iterations on the image\nor video's first frame, we \\textit{preheat} the cold-start queries with rich\ninformation of input features, via a tiny module self-distilled inside OCL; (2)\nTo smooth SA recurrences across all video frames, we \\textit{differentiate} the\nhomogeneous transforms on the first and non-first frames, by using full and\nsingle iterations respectively. Comprehensive experiments on object discovery,\nrecognition and downstream benchmarks validate our method's effectiveness.\nFurther analyses intuitively illuminate how our method smooths SA iterations\nand recurrences. Our code is available in the supplement.\n", "link": "http://arxiv.org/abs/2508.05417v1", "date": "2025-08-07", "relevancy": 2.1956, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5557}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5455}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smoothing%20Slot%20Attention%20Iterations%20and%20Recurrences&body=Title%3A%20Smoothing%20Slot%20Attention%20Iterations%20and%20Recurrences%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Wenyan%20Yang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Slot%20Attention%20%28SA%29%20and%20its%20variants%20lie%20at%20the%20heart%20of%20mainstream%0AObject-Centric%20Learning%20%28OCL%29.%20Objects%20in%20an%20image%20can%20be%20aggregated%20into%0Arespective%20slot%20vectors%2C%20by%20%5Ctextit%7Biteratively%7D%20refining%20cold-start%20query%0Avectors%2C%20typically%20three%20times%2C%20via%20SA%20on%20image%20features.%20For%20video%2C%20such%0Aaggregation%20is%20%5Ctextit%7Brecurrently%7D%20shared%20across%20frames%2C%20with%20queries%0Acold-started%20on%20the%20first%20frame%20while%20transitioned%20from%20the%20previous%20frame%27s%0Aslots%20on%20non-first%20frames.%20However%2C%20the%20cold-start%20queries%20lack%20sample-specific%0Acues%20thus%20hinder%20precise%20aggregation%20on%20the%20image%20or%20video%27s%20first%20frame%3B%20Also%2C%0Anon-first%20frames%27%20queries%20are%20already%20sample-specific%20thus%20require%20transforms%0Adifferent%20from%20the%20first%20frame%27s%20aggregation.%20We%20address%20these%20issues%20for%20the%0Afirst%20time%20with%20our%20%5Ctextit%7BSmoothSA%7D%3A%20%281%29%20To%20smooth%20SA%20iterations%20on%20the%20image%0Aor%20video%27s%20first%20frame%2C%20we%20%5Ctextit%7Bpreheat%7D%20the%20cold-start%20queries%20with%20rich%0Ainformation%20of%20input%20features%2C%20via%20a%20tiny%20module%20self-distilled%20inside%20OCL%3B%20%282%29%0ATo%20smooth%20SA%20recurrences%20across%20all%20video%20frames%2C%20we%20%5Ctextit%7Bdifferentiate%7D%20the%0Ahomogeneous%20transforms%20on%20the%20first%20and%20non-first%20frames%2C%20by%20using%20full%20and%0Asingle%20iterations%20respectively.%20Comprehensive%20experiments%20on%20object%20discovery%2C%0Arecognition%20and%20downstream%20benchmarks%20validate%20our%20method%27s%20effectiveness.%0AFurther%20analyses%20intuitively%20illuminate%20how%20our%20method%20smooths%20SA%20iterations%0Aand%20recurrences.%20Our%20code%20is%20available%20in%20the%20supplement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmoothing%2520Slot%2520Attention%2520Iterations%2520and%2520Recurrences%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Wenyan%2520Yang%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Slot%2520Attention%2520%2528SA%2529%2520and%2520its%2520variants%2520lie%2520at%2520the%2520heart%2520of%2520mainstream%250AObject-Centric%2520Learning%2520%2528OCL%2529.%2520Objects%2520in%2520an%2520image%2520can%2520be%2520aggregated%2520into%250Arespective%2520slot%2520vectors%252C%2520by%2520%255Ctextit%257Biteratively%257D%2520refining%2520cold-start%2520query%250Avectors%252C%2520typically%2520three%2520times%252C%2520via%2520SA%2520on%2520image%2520features.%2520For%2520video%252C%2520such%250Aaggregation%2520is%2520%255Ctextit%257Brecurrently%257D%2520shared%2520across%2520frames%252C%2520with%2520queries%250Acold-started%2520on%2520the%2520first%2520frame%2520while%2520transitioned%2520from%2520the%2520previous%2520frame%2527s%250Aslots%2520on%2520non-first%2520frames.%2520However%252C%2520the%2520cold-start%2520queries%2520lack%2520sample-specific%250Acues%2520thus%2520hinder%2520precise%2520aggregation%2520on%2520the%2520image%2520or%2520video%2527s%2520first%2520frame%253B%2520Also%252C%250Anon-first%2520frames%2527%2520queries%2520are%2520already%2520sample-specific%2520thus%2520require%2520transforms%250Adifferent%2520from%2520the%2520first%2520frame%2527s%2520aggregation.%2520We%2520address%2520these%2520issues%2520for%2520the%250Afirst%2520time%2520with%2520our%2520%255Ctextit%257BSmoothSA%257D%253A%2520%25281%2529%2520To%2520smooth%2520SA%2520iterations%2520on%2520the%2520image%250Aor%2520video%2527s%2520first%2520frame%252C%2520we%2520%255Ctextit%257Bpreheat%257D%2520the%2520cold-start%2520queries%2520with%2520rich%250Ainformation%2520of%2520input%2520features%252C%2520via%2520a%2520tiny%2520module%2520self-distilled%2520inside%2520OCL%253B%2520%25282%2529%250ATo%2520smooth%2520SA%2520recurrences%2520across%2520all%2520video%2520frames%252C%2520we%2520%255Ctextit%257Bdifferentiate%257D%2520the%250Ahomogeneous%2520transforms%2520on%2520the%2520first%2520and%2520non-first%2520frames%252C%2520by%2520using%2520full%2520and%250Asingle%2520iterations%2520respectively.%2520Comprehensive%2520experiments%2520on%2520object%2520discovery%252C%250Arecognition%2520and%2520downstream%2520benchmarks%2520validate%2520our%2520method%2527s%2520effectiveness.%250AFurther%2520analyses%2520intuitively%2520illuminate%2520how%2520our%2520method%2520smooths%2520SA%2520iterations%250Aand%2520recurrences.%2520Our%2520code%2520is%2520available%2520in%2520the%2520supplement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smoothing%20Slot%20Attention%20Iterations%20and%20Recurrences&entry.906535625=Rongzhen%20Zhao%20and%20Wenyan%20Yang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Slot%20Attention%20%28SA%29%20and%20its%20variants%20lie%20at%20the%20heart%20of%20mainstream%0AObject-Centric%20Learning%20%28OCL%29.%20Objects%20in%20an%20image%20can%20be%20aggregated%20into%0Arespective%20slot%20vectors%2C%20by%20%5Ctextit%7Biteratively%7D%20refining%20cold-start%20query%0Avectors%2C%20typically%20three%20times%2C%20via%20SA%20on%20image%20features.%20For%20video%2C%20such%0Aaggregation%20is%20%5Ctextit%7Brecurrently%7D%20shared%20across%20frames%2C%20with%20queries%0Acold-started%20on%20the%20first%20frame%20while%20transitioned%20from%20the%20previous%20frame%27s%0Aslots%20on%20non-first%20frames.%20However%2C%20the%20cold-start%20queries%20lack%20sample-specific%0Acues%20thus%20hinder%20precise%20aggregation%20on%20the%20image%20or%20video%27s%20first%20frame%3B%20Also%2C%0Anon-first%20frames%27%20queries%20are%20already%20sample-specific%20thus%20require%20transforms%0Adifferent%20from%20the%20first%20frame%27s%20aggregation.%20We%20address%20these%20issues%20for%20the%0Afirst%20time%20with%20our%20%5Ctextit%7BSmoothSA%7D%3A%20%281%29%20To%20smooth%20SA%20iterations%20on%20the%20image%0Aor%20video%27s%20first%20frame%2C%20we%20%5Ctextit%7Bpreheat%7D%20the%20cold-start%20queries%20with%20rich%0Ainformation%20of%20input%20features%2C%20via%20a%20tiny%20module%20self-distilled%20inside%20OCL%3B%20%282%29%0ATo%20smooth%20SA%20recurrences%20across%20all%20video%20frames%2C%20we%20%5Ctextit%7Bdifferentiate%7D%20the%0Ahomogeneous%20transforms%20on%20the%20first%20and%20non-first%20frames%2C%20by%20using%20full%20and%0Asingle%20iterations%20respectively.%20Comprehensive%20experiments%20on%20object%20discovery%2C%0Arecognition%20and%20downstream%20benchmarks%20validate%20our%20method%27s%20effectiveness.%0AFurther%20analyses%20intuitively%20illuminate%20how%20our%20method%20smooths%20SA%20iterations%0Aand%20recurrences.%20Our%20code%20is%20available%20in%20the%20supplement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05417v1&entry.124074799=Read"},
{"title": "Understanding Large Language Model Behaviors through Interactive\n  Counterfactual Generation and Analysis", "author": "Furui Cheng and Vil\u00e9m Zouhar and Robin Shing Moon Chan and Daniel F\u00fcrst and Hendrik Strobelt and Mennatallah El-Assady", "abstract": "  Understanding the behavior of large language models (LLMs) is crucial for\nensuring their safe and reliable use. However, existing explainable AI (XAI)\nmethods for LLMs primarily rely on word-level explanations, which are often\ncomputationally inefficient and misaligned with human reasoning processes.\nMoreover, these methods often treat explanation as a one-time output,\noverlooking its inherently interactive and iterative nature. In this paper, we\npresent LLM Analyzer, an interactive visualization system that addresses these\nlimitations by enabling intuitive and efficient exploration of LLM behaviors\nthrough counterfactual analysis. Our system features a novel algorithm that\ngenerates fluent and semantically meaningful counterfactuals via targeted\nremoval and replacement operations at user-defined levels of granularity. These\ncounterfactuals are used to compute feature attribution scores, which are then\nintegrated with concrete examples in a table-based visualization, supporting\ndynamic analysis of model behavior. A user study with LLM practitioners and\ninterviews with experts demonstrate the system's usability and effectiveness,\nemphasizing the importance of involving humans in the explanation process as\nactive participants rather than passive recipients.\n", "link": "http://arxiv.org/abs/2405.00708v2", "date": "2025-08-07", "relevancy": 2.1846, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Large%20Language%20Model%20Behaviors%20through%20Interactive%0A%20%20Counterfactual%20Generation%20and%20Analysis&body=Title%3A%20Understanding%20Large%20Language%20Model%20Behaviors%20through%20Interactive%0A%20%20Counterfactual%20Generation%20and%20Analysis%0AAuthor%3A%20Furui%20Cheng%20and%20Vil%C3%A9m%20Zouhar%20and%20Robin%20Shing%20Moon%20Chan%20and%20Daniel%20F%C3%BCrst%20and%20Hendrik%20Strobelt%20and%20Mennatallah%20El-Assady%0AAbstract%3A%20%20%20Understanding%20the%20behavior%20of%20large%20language%20models%20%28LLMs%29%20is%20crucial%20for%0Aensuring%20their%20safe%20and%20reliable%20use.%20However%2C%20existing%20explainable%20AI%20%28XAI%29%0Amethods%20for%20LLMs%20primarily%20rely%20on%20word-level%20explanations%2C%20which%20are%20often%0Acomputationally%20inefficient%20and%20misaligned%20with%20human%20reasoning%20processes.%0AMoreover%2C%20these%20methods%20often%20treat%20explanation%20as%20a%20one-time%20output%2C%0Aoverlooking%20its%20inherently%20interactive%20and%20iterative%20nature.%20In%20this%20paper%2C%20we%0Apresent%20LLM%20Analyzer%2C%20an%20interactive%20visualization%20system%20that%20addresses%20these%0Alimitations%20by%20enabling%20intuitive%20and%20efficient%20exploration%20of%20LLM%20behaviors%0Athrough%20counterfactual%20analysis.%20Our%20system%20features%20a%20novel%20algorithm%20that%0Agenerates%20fluent%20and%20semantically%20meaningful%20counterfactuals%20via%20targeted%0Aremoval%20and%20replacement%20operations%20at%20user-defined%20levels%20of%20granularity.%20These%0Acounterfactuals%20are%20used%20to%20compute%20feature%20attribution%20scores%2C%20which%20are%20then%0Aintegrated%20with%20concrete%20examples%20in%20a%20table-based%20visualization%2C%20supporting%0Adynamic%20analysis%20of%20model%20behavior.%20A%20user%20study%20with%20LLM%20practitioners%20and%0Ainterviews%20with%20experts%20demonstrate%20the%20system%27s%20usability%20and%20effectiveness%2C%0Aemphasizing%20the%20importance%20of%20involving%20humans%20in%20the%20explanation%20process%20as%0Aactive%20participants%20rather%20than%20passive%20recipients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Large%2520Language%2520Model%2520Behaviors%2520through%2520Interactive%250A%2520%2520Counterfactual%2520Generation%2520and%2520Analysis%26entry.906535625%3DFurui%2520Cheng%2520and%2520Vil%25C3%25A9m%2520Zouhar%2520and%2520Robin%2520Shing%2520Moon%2520Chan%2520and%2520Daniel%2520F%25C3%25BCrst%2520and%2520Hendrik%2520Strobelt%2520and%2520Mennatallah%2520El-Assady%26entry.1292438233%3D%2520%2520Understanding%2520the%2520behavior%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520crucial%2520for%250Aensuring%2520their%2520safe%2520and%2520reliable%2520use.%2520However%252C%2520existing%2520explainable%2520AI%2520%2528XAI%2529%250Amethods%2520for%2520LLMs%2520primarily%2520rely%2520on%2520word-level%2520explanations%252C%2520which%2520are%2520often%250Acomputationally%2520inefficient%2520and%2520misaligned%2520with%2520human%2520reasoning%2520processes.%250AMoreover%252C%2520these%2520methods%2520often%2520treat%2520explanation%2520as%2520a%2520one-time%2520output%252C%250Aoverlooking%2520its%2520inherently%2520interactive%2520and%2520iterative%2520nature.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520LLM%2520Analyzer%252C%2520an%2520interactive%2520visualization%2520system%2520that%2520addresses%2520these%250Alimitations%2520by%2520enabling%2520intuitive%2520and%2520efficient%2520exploration%2520of%2520LLM%2520behaviors%250Athrough%2520counterfactual%2520analysis.%2520Our%2520system%2520features%2520a%2520novel%2520algorithm%2520that%250Agenerates%2520fluent%2520and%2520semantically%2520meaningful%2520counterfactuals%2520via%2520targeted%250Aremoval%2520and%2520replacement%2520operations%2520at%2520user-defined%2520levels%2520of%2520granularity.%2520These%250Acounterfactuals%2520are%2520used%2520to%2520compute%2520feature%2520attribution%2520scores%252C%2520which%2520are%2520then%250Aintegrated%2520with%2520concrete%2520examples%2520in%2520a%2520table-based%2520visualization%252C%2520supporting%250Adynamic%2520analysis%2520of%2520model%2520behavior.%2520A%2520user%2520study%2520with%2520LLM%2520practitioners%2520and%250Ainterviews%2520with%2520experts%2520demonstrate%2520the%2520system%2527s%2520usability%2520and%2520effectiveness%252C%250Aemphasizing%2520the%2520importance%2520of%2520involving%2520humans%2520in%2520the%2520explanation%2520process%2520as%250Aactive%2520participants%2520rather%2520than%2520passive%2520recipients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Large%20Language%20Model%20Behaviors%20through%20Interactive%0A%20%20Counterfactual%20Generation%20and%20Analysis&entry.906535625=Furui%20Cheng%20and%20Vil%C3%A9m%20Zouhar%20and%20Robin%20Shing%20Moon%20Chan%20and%20Daniel%20F%C3%BCrst%20and%20Hendrik%20Strobelt%20and%20Mennatallah%20El-Assady&entry.1292438233=%20%20Understanding%20the%20behavior%20of%20large%20language%20models%20%28LLMs%29%20is%20crucial%20for%0Aensuring%20their%20safe%20and%20reliable%20use.%20However%2C%20existing%20explainable%20AI%20%28XAI%29%0Amethods%20for%20LLMs%20primarily%20rely%20on%20word-level%20explanations%2C%20which%20are%20often%0Acomputationally%20inefficient%20and%20misaligned%20with%20human%20reasoning%20processes.%0AMoreover%2C%20these%20methods%20often%20treat%20explanation%20as%20a%20one-time%20output%2C%0Aoverlooking%20its%20inherently%20interactive%20and%20iterative%20nature.%20In%20this%20paper%2C%20we%0Apresent%20LLM%20Analyzer%2C%20an%20interactive%20visualization%20system%20that%20addresses%20these%0Alimitations%20by%20enabling%20intuitive%20and%20efficient%20exploration%20of%20LLM%20behaviors%0Athrough%20counterfactual%20analysis.%20Our%20system%20features%20a%20novel%20algorithm%20that%0Agenerates%20fluent%20and%20semantically%20meaningful%20counterfactuals%20via%20targeted%0Aremoval%20and%20replacement%20operations%20at%20user-defined%20levels%20of%20granularity.%20These%0Acounterfactuals%20are%20used%20to%20compute%20feature%20attribution%20scores%2C%20which%20are%20then%0Aintegrated%20with%20concrete%20examples%20in%20a%20table-based%20visualization%2C%20supporting%0Adynamic%20analysis%20of%20model%20behavior.%20A%20user%20study%20with%20LLM%20practitioners%20and%0Ainterviews%20with%20experts%20demonstrate%20the%20system%27s%20usability%20and%20effectiveness%2C%0Aemphasizing%20the%20importance%20of%20involving%20humans%20in%20the%20explanation%20process%20as%0Aactive%20participants%20rather%20than%20passive%20recipients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00708v2&entry.124074799=Read"},
{"title": "Real-Time Iteration Scheme for Diffusion Policy", "author": "Yufei Duan and Hang Yin and Danica Kragic", "abstract": "  Diffusion Policies have demonstrated impressive performance in robotic\nmanipulation tasks. However, their long inference time, resulting from an\nextensive iterative denoising process, and the need to execute an action chunk\nbefore the next prediction to maintain consistent actions limit their\napplicability to latency-critical tasks or simple tasks with a short cycle\ntime. While recent methods explored distillation or alternative policy\nstructures to accelerate inference, these often demand additional training,\nwhich can be resource-intensive for large robotic models. In this paper, we\nintroduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a\nmethod from optimal control that accelerates optimization by leveraging\nsolutions from previous time steps as initial guesses for subsequent\niterations. We explore the application of this scheme in diffusion inference\nand propose a scaling-based method to effectively handle discrete actions, such\nas grasping, in robotic manipulation. The proposed scheme significantly reduces\nruntime computational costs without the need for distillation or policy\nredesign. This enables a seamless integration into many pre-trained\ndiffusion-based models, in particular, to resource-demanding large models. We\nalso provide theoretical conditions for the contractivity which could be useful\nfor estimating the initial denoising step. Quantitative results from extensive\nsimulation experiments show a substantial reduction in inference time, with\ncomparable overall performance compared with Diffusion Policy using full-step\ndenoising. Our project page with additional resources is available at:\nhttps://rti-dp.github.io/.\n", "link": "http://arxiv.org/abs/2508.05396v1", "date": "2025-08-07", "relevancy": 2.1836, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5965}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.539}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Iteration%20Scheme%20for%20Diffusion%20Policy&body=Title%3A%20Real-Time%20Iteration%20Scheme%20for%20Diffusion%20Policy%0AAuthor%3A%20Yufei%20Duan%20and%20Hang%20Yin%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20Diffusion%20Policies%20have%20demonstrated%20impressive%20performance%20in%20robotic%0Amanipulation%20tasks.%20However%2C%20their%20long%20inference%20time%2C%20resulting%20from%20an%0Aextensive%20iterative%20denoising%20process%2C%20and%20the%20need%20to%20execute%20an%20action%20chunk%0Abefore%20the%20next%20prediction%20to%20maintain%20consistent%20actions%20limit%20their%0Aapplicability%20to%20latency-critical%20tasks%20or%20simple%20tasks%20with%20a%20short%20cycle%0Atime.%20While%20recent%20methods%20explored%20distillation%20or%20alternative%20policy%0Astructures%20to%20accelerate%20inference%2C%20these%20often%20demand%20additional%20training%2C%0Awhich%20can%20be%20resource-intensive%20for%20large%20robotic%20models.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20approach%20inspired%20by%20the%20Real-Time%20Iteration%20%28RTI%29%20Scheme%2C%20a%0Amethod%20from%20optimal%20control%20that%20accelerates%20optimization%20by%20leveraging%0Asolutions%20from%20previous%20time%20steps%20as%20initial%20guesses%20for%20subsequent%0Aiterations.%20We%20explore%20the%20application%20of%20this%20scheme%20in%20diffusion%20inference%0Aand%20propose%20a%20scaling-based%20method%20to%20effectively%20handle%20discrete%20actions%2C%20such%0Aas%20grasping%2C%20in%20robotic%20manipulation.%20The%20proposed%20scheme%20significantly%20reduces%0Aruntime%20computational%20costs%20without%20the%20need%20for%20distillation%20or%20policy%0Aredesign.%20This%20enables%20a%20seamless%20integration%20into%20many%20pre-trained%0Adiffusion-based%20models%2C%20in%20particular%2C%20to%20resource-demanding%20large%20models.%20We%0Aalso%20provide%20theoretical%20conditions%20for%20the%20contractivity%20which%20could%20be%20useful%0Afor%20estimating%20the%20initial%20denoising%20step.%20Quantitative%20results%20from%20extensive%0Asimulation%20experiments%20show%20a%20substantial%20reduction%20in%20inference%20time%2C%20with%0Acomparable%20overall%20performance%20compared%20with%20Diffusion%20Policy%20using%20full-step%0Adenoising.%20Our%20project%20page%20with%20additional%20resources%20is%20available%20at%3A%0Ahttps%3A//rti-dp.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Iteration%2520Scheme%2520for%2520Diffusion%2520Policy%26entry.906535625%3DYufei%2520Duan%2520and%2520Hang%2520Yin%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520Diffusion%2520Policies%2520have%2520demonstrated%2520impressive%2520performance%2520in%2520robotic%250Amanipulation%2520tasks.%2520However%252C%2520their%2520long%2520inference%2520time%252C%2520resulting%2520from%2520an%250Aextensive%2520iterative%2520denoising%2520process%252C%2520and%2520the%2520need%2520to%2520execute%2520an%2520action%2520chunk%250Abefore%2520the%2520next%2520prediction%2520to%2520maintain%2520consistent%2520actions%2520limit%2520their%250Aapplicability%2520to%2520latency-critical%2520tasks%2520or%2520simple%2520tasks%2520with%2520a%2520short%2520cycle%250Atime.%2520While%2520recent%2520methods%2520explored%2520distillation%2520or%2520alternative%2520policy%250Astructures%2520to%2520accelerate%2520inference%252C%2520these%2520often%2520demand%2520additional%2520training%252C%250Awhich%2520can%2520be%2520resource-intensive%2520for%2520large%2520robotic%2520models.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520approach%2520inspired%2520by%2520the%2520Real-Time%2520Iteration%2520%2528RTI%2529%2520Scheme%252C%2520a%250Amethod%2520from%2520optimal%2520control%2520that%2520accelerates%2520optimization%2520by%2520leveraging%250Asolutions%2520from%2520previous%2520time%2520steps%2520as%2520initial%2520guesses%2520for%2520subsequent%250Aiterations.%2520We%2520explore%2520the%2520application%2520of%2520this%2520scheme%2520in%2520diffusion%2520inference%250Aand%2520propose%2520a%2520scaling-based%2520method%2520to%2520effectively%2520handle%2520discrete%2520actions%252C%2520such%250Aas%2520grasping%252C%2520in%2520robotic%2520manipulation.%2520The%2520proposed%2520scheme%2520significantly%2520reduces%250Aruntime%2520computational%2520costs%2520without%2520the%2520need%2520for%2520distillation%2520or%2520policy%250Aredesign.%2520This%2520enables%2520a%2520seamless%2520integration%2520into%2520many%2520pre-trained%250Adiffusion-based%2520models%252C%2520in%2520particular%252C%2520to%2520resource-demanding%2520large%2520models.%2520We%250Aalso%2520provide%2520theoretical%2520conditions%2520for%2520the%2520contractivity%2520which%2520could%2520be%2520useful%250Afor%2520estimating%2520the%2520initial%2520denoising%2520step.%2520Quantitative%2520results%2520from%2520extensive%250Asimulation%2520experiments%2520show%2520a%2520substantial%2520reduction%2520in%2520inference%2520time%252C%2520with%250Acomparable%2520overall%2520performance%2520compared%2520with%2520Diffusion%2520Policy%2520using%2520full-step%250Adenoising.%2520Our%2520project%2520page%2520with%2520additional%2520resources%2520is%2520available%2520at%253A%250Ahttps%253A//rti-dp.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Iteration%20Scheme%20for%20Diffusion%20Policy&entry.906535625=Yufei%20Duan%20and%20Hang%20Yin%20and%20Danica%20Kragic&entry.1292438233=%20%20Diffusion%20Policies%20have%20demonstrated%20impressive%20performance%20in%20robotic%0Amanipulation%20tasks.%20However%2C%20their%20long%20inference%20time%2C%20resulting%20from%20an%0Aextensive%20iterative%20denoising%20process%2C%20and%20the%20need%20to%20execute%20an%20action%20chunk%0Abefore%20the%20next%20prediction%20to%20maintain%20consistent%20actions%20limit%20their%0Aapplicability%20to%20latency-critical%20tasks%20or%20simple%20tasks%20with%20a%20short%20cycle%0Atime.%20While%20recent%20methods%20explored%20distillation%20or%20alternative%20policy%0Astructures%20to%20accelerate%20inference%2C%20these%20often%20demand%20additional%20training%2C%0Awhich%20can%20be%20resource-intensive%20for%20large%20robotic%20models.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20approach%20inspired%20by%20the%20Real-Time%20Iteration%20%28RTI%29%20Scheme%2C%20a%0Amethod%20from%20optimal%20control%20that%20accelerates%20optimization%20by%20leveraging%0Asolutions%20from%20previous%20time%20steps%20as%20initial%20guesses%20for%20subsequent%0Aiterations.%20We%20explore%20the%20application%20of%20this%20scheme%20in%20diffusion%20inference%0Aand%20propose%20a%20scaling-based%20method%20to%20effectively%20handle%20discrete%20actions%2C%20such%0Aas%20grasping%2C%20in%20robotic%20manipulation.%20The%20proposed%20scheme%20significantly%20reduces%0Aruntime%20computational%20costs%20without%20the%20need%20for%20distillation%20or%20policy%0Aredesign.%20This%20enables%20a%20seamless%20integration%20into%20many%20pre-trained%0Adiffusion-based%20models%2C%20in%20particular%2C%20to%20resource-demanding%20large%20models.%20We%0Aalso%20provide%20theoretical%20conditions%20for%20the%20contractivity%20which%20could%20be%20useful%0Afor%20estimating%20the%20initial%20denoising%20step.%20Quantitative%20results%20from%20extensive%0Asimulation%20experiments%20show%20a%20substantial%20reduction%20in%20inference%20time%2C%20with%0Acomparable%20overall%20performance%20compared%20with%20Diffusion%20Policy%20using%20full-step%0Adenoising.%20Our%20project%20page%20with%20additional%20resources%20is%20available%20at%3A%0Ahttps%3A//rti-dp.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05396v1&entry.124074799=Read"},
{"title": "Real-Time Iteration Scheme for Diffusion Policy", "author": "Yufei Duan and Hang Yin and Danica Kragic", "abstract": "  Diffusion Policies have demonstrated impressive performance in robotic\nmanipulation tasks. However, their long inference time, resulting from an\nextensive iterative denoising process, and the need to execute an action chunk\nbefore the next prediction to maintain consistent actions limit their\napplicability to latency-critical tasks or simple tasks with a short cycle\ntime. While recent methods explored distillation or alternative policy\nstructures to accelerate inference, these often demand additional training,\nwhich can be resource-intensive for large robotic models. In this paper, we\nintroduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a\nmethod from optimal control that accelerates optimization by leveraging\nsolutions from previous time steps as initial guesses for subsequent\niterations. We explore the application of this scheme in diffusion inference\nand propose a scaling-based method to effectively handle discrete actions, such\nas grasping, in robotic manipulation. The proposed scheme significantly reduces\nruntime computational costs without the need for distillation or policy\nredesign. This enables a seamless integration into many pre-trained\ndiffusion-based models, in particular, to resource-demanding large models. We\nalso provide theoretical conditions for the contractivity which could be useful\nfor estimating the initial denoising step. Quantitative results from extensive\nsimulation experiments show a substantial reduction in inference time, with\ncomparable overall performance compared with Diffusion Policy using full-step\ndenoising. Our project page with additional resources is available at:\nhttps://rti-dp.github.io/.\n", "link": "http://arxiv.org/abs/2508.05396v1", "date": "2025-08-07", "relevancy": 2.1836, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5965}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.539}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Iteration%20Scheme%20for%20Diffusion%20Policy&body=Title%3A%20Real-Time%20Iteration%20Scheme%20for%20Diffusion%20Policy%0AAuthor%3A%20Yufei%20Duan%20and%20Hang%20Yin%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20Diffusion%20Policies%20have%20demonstrated%20impressive%20performance%20in%20robotic%0Amanipulation%20tasks.%20However%2C%20their%20long%20inference%20time%2C%20resulting%20from%20an%0Aextensive%20iterative%20denoising%20process%2C%20and%20the%20need%20to%20execute%20an%20action%20chunk%0Abefore%20the%20next%20prediction%20to%20maintain%20consistent%20actions%20limit%20their%0Aapplicability%20to%20latency-critical%20tasks%20or%20simple%20tasks%20with%20a%20short%20cycle%0Atime.%20While%20recent%20methods%20explored%20distillation%20or%20alternative%20policy%0Astructures%20to%20accelerate%20inference%2C%20these%20often%20demand%20additional%20training%2C%0Awhich%20can%20be%20resource-intensive%20for%20large%20robotic%20models.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20approach%20inspired%20by%20the%20Real-Time%20Iteration%20%28RTI%29%20Scheme%2C%20a%0Amethod%20from%20optimal%20control%20that%20accelerates%20optimization%20by%20leveraging%0Asolutions%20from%20previous%20time%20steps%20as%20initial%20guesses%20for%20subsequent%0Aiterations.%20We%20explore%20the%20application%20of%20this%20scheme%20in%20diffusion%20inference%0Aand%20propose%20a%20scaling-based%20method%20to%20effectively%20handle%20discrete%20actions%2C%20such%0Aas%20grasping%2C%20in%20robotic%20manipulation.%20The%20proposed%20scheme%20significantly%20reduces%0Aruntime%20computational%20costs%20without%20the%20need%20for%20distillation%20or%20policy%0Aredesign.%20This%20enables%20a%20seamless%20integration%20into%20many%20pre-trained%0Adiffusion-based%20models%2C%20in%20particular%2C%20to%20resource-demanding%20large%20models.%20We%0Aalso%20provide%20theoretical%20conditions%20for%20the%20contractivity%20which%20could%20be%20useful%0Afor%20estimating%20the%20initial%20denoising%20step.%20Quantitative%20results%20from%20extensive%0Asimulation%20experiments%20show%20a%20substantial%20reduction%20in%20inference%20time%2C%20with%0Acomparable%20overall%20performance%20compared%20with%20Diffusion%20Policy%20using%20full-step%0Adenoising.%20Our%20project%20page%20with%20additional%20resources%20is%20available%20at%3A%0Ahttps%3A//rti-dp.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Iteration%2520Scheme%2520for%2520Diffusion%2520Policy%26entry.906535625%3DYufei%2520Duan%2520and%2520Hang%2520Yin%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520Diffusion%2520Policies%2520have%2520demonstrated%2520impressive%2520performance%2520in%2520robotic%250Amanipulation%2520tasks.%2520However%252C%2520their%2520long%2520inference%2520time%252C%2520resulting%2520from%2520an%250Aextensive%2520iterative%2520denoising%2520process%252C%2520and%2520the%2520need%2520to%2520execute%2520an%2520action%2520chunk%250Abefore%2520the%2520next%2520prediction%2520to%2520maintain%2520consistent%2520actions%2520limit%2520their%250Aapplicability%2520to%2520latency-critical%2520tasks%2520or%2520simple%2520tasks%2520with%2520a%2520short%2520cycle%250Atime.%2520While%2520recent%2520methods%2520explored%2520distillation%2520or%2520alternative%2520policy%250Astructures%2520to%2520accelerate%2520inference%252C%2520these%2520often%2520demand%2520additional%2520training%252C%250Awhich%2520can%2520be%2520resource-intensive%2520for%2520large%2520robotic%2520models.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520approach%2520inspired%2520by%2520the%2520Real-Time%2520Iteration%2520%2528RTI%2529%2520Scheme%252C%2520a%250Amethod%2520from%2520optimal%2520control%2520that%2520accelerates%2520optimization%2520by%2520leveraging%250Asolutions%2520from%2520previous%2520time%2520steps%2520as%2520initial%2520guesses%2520for%2520subsequent%250Aiterations.%2520We%2520explore%2520the%2520application%2520of%2520this%2520scheme%2520in%2520diffusion%2520inference%250Aand%2520propose%2520a%2520scaling-based%2520method%2520to%2520effectively%2520handle%2520discrete%2520actions%252C%2520such%250Aas%2520grasping%252C%2520in%2520robotic%2520manipulation.%2520The%2520proposed%2520scheme%2520significantly%2520reduces%250Aruntime%2520computational%2520costs%2520without%2520the%2520need%2520for%2520distillation%2520or%2520policy%250Aredesign.%2520This%2520enables%2520a%2520seamless%2520integration%2520into%2520many%2520pre-trained%250Adiffusion-based%2520models%252C%2520in%2520particular%252C%2520to%2520resource-demanding%2520large%2520models.%2520We%250Aalso%2520provide%2520theoretical%2520conditions%2520for%2520the%2520contractivity%2520which%2520could%2520be%2520useful%250Afor%2520estimating%2520the%2520initial%2520denoising%2520step.%2520Quantitative%2520results%2520from%2520extensive%250Asimulation%2520experiments%2520show%2520a%2520substantial%2520reduction%2520in%2520inference%2520time%252C%2520with%250Acomparable%2520overall%2520performance%2520compared%2520with%2520Diffusion%2520Policy%2520using%2520full-step%250Adenoising.%2520Our%2520project%2520page%2520with%2520additional%2520resources%2520is%2520available%2520at%253A%250Ahttps%253A//rti-dp.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Iteration%20Scheme%20for%20Diffusion%20Policy&entry.906535625=Yufei%20Duan%20and%20Hang%20Yin%20and%20Danica%20Kragic&entry.1292438233=%20%20Diffusion%20Policies%20have%20demonstrated%20impressive%20performance%20in%20robotic%0Amanipulation%20tasks.%20However%2C%20their%20long%20inference%20time%2C%20resulting%20from%20an%0Aextensive%20iterative%20denoising%20process%2C%20and%20the%20need%20to%20execute%20an%20action%20chunk%0Abefore%20the%20next%20prediction%20to%20maintain%20consistent%20actions%20limit%20their%0Aapplicability%20to%20latency-critical%20tasks%20or%20simple%20tasks%20with%20a%20short%20cycle%0Atime.%20While%20recent%20methods%20explored%20distillation%20or%20alternative%20policy%0Astructures%20to%20accelerate%20inference%2C%20these%20often%20demand%20additional%20training%2C%0Awhich%20can%20be%20resource-intensive%20for%20large%20robotic%20models.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20approach%20inspired%20by%20the%20Real-Time%20Iteration%20%28RTI%29%20Scheme%2C%20a%0Amethod%20from%20optimal%20control%20that%20accelerates%20optimization%20by%20leveraging%0Asolutions%20from%20previous%20time%20steps%20as%20initial%20guesses%20for%20subsequent%0Aiterations.%20We%20explore%20the%20application%20of%20this%20scheme%20in%20diffusion%20inference%0Aand%20propose%20a%20scaling-based%20method%20to%20effectively%20handle%20discrete%20actions%2C%20such%0Aas%20grasping%2C%20in%20robotic%20manipulation.%20The%20proposed%20scheme%20significantly%20reduces%0Aruntime%20computational%20costs%20without%20the%20need%20for%20distillation%20or%20policy%0Aredesign.%20This%20enables%20a%20seamless%20integration%20into%20many%20pre-trained%0Adiffusion-based%20models%2C%20in%20particular%2C%20to%20resource-demanding%20large%20models.%20We%0Aalso%20provide%20theoretical%20conditions%20for%20the%20contractivity%20which%20could%20be%20useful%0Afor%20estimating%20the%20initial%20denoising%20step.%20Quantitative%20results%20from%20extensive%0Asimulation%20experiments%20show%20a%20substantial%20reduction%20in%20inference%20time%2C%20with%0Acomparable%20overall%20performance%20compared%20with%20Diffusion%20Policy%20using%20full-step%0Adenoising.%20Our%20project%20page%20with%20additional%20resources%20is%20available%20at%3A%0Ahttps%3A//rti-dp.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05396v1&entry.124074799=Read"},
{"title": "A Multi-view Landmark Representation Approach with Application to\n  GNSS-Visual-Inertial Odometry", "author": "Tong Hua and Jiale Han and Wei Ouyang", "abstract": "  Invariant Extended Kalman Filter (IEKF) has been a significant technique in\nvision-aided sensor fusion. However, it usually suffers from high computational\nburden when jointly optimizing camera poses and the landmarks. To improve its\nefficiency and applicability for multi-sensor fusion, we present a multi-view\npose-only estimation approach with its application to GNSS-Visual-Inertial\nOdometry (GVIO) in this paper. Our main contribution is deriving a visual\nmeasurement model which directly associates landmark representation with\nmultiple camera poses and observations. Such a pose-only measurement is proven\nto be tightly-coupled between landmarks and poses, and maintain a perfect null\nspace that is independent of estimated poses. Finally, we apply the proposed\napproach to a filter based GVIO with a novel feature management strategy. Both\nsimulation tests and real-world experiments are conducted to demonstrate the\nsuperiority of the proposed method in terms of efficiency and accuracy.\n", "link": "http://arxiv.org/abs/2508.05368v1", "date": "2025-08-07", "relevancy": 2.1736, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5445}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-view%20Landmark%20Representation%20Approach%20with%20Application%20to%0A%20%20GNSS-Visual-Inertial%20Odometry&body=Title%3A%20A%20Multi-view%20Landmark%20Representation%20Approach%20with%20Application%20to%0A%20%20GNSS-Visual-Inertial%20Odometry%0AAuthor%3A%20Tong%20Hua%20and%20Jiale%20Han%20and%20Wei%20Ouyang%0AAbstract%3A%20%20%20Invariant%20Extended%20Kalman%20Filter%20%28IEKF%29%20has%20been%20a%20significant%20technique%20in%0Avision-aided%20sensor%20fusion.%20However%2C%20it%20usually%20suffers%20from%20high%20computational%0Aburden%20when%20jointly%20optimizing%20camera%20poses%20and%20the%20landmarks.%20To%20improve%20its%0Aefficiency%20and%20applicability%20for%20multi-sensor%20fusion%2C%20we%20present%20a%20multi-view%0Apose-only%20estimation%20approach%20with%20its%20application%20to%20GNSS-Visual-Inertial%0AOdometry%20%28GVIO%29%20in%20this%20paper.%20Our%20main%20contribution%20is%20deriving%20a%20visual%0Ameasurement%20model%20which%20directly%20associates%20landmark%20representation%20with%0Amultiple%20camera%20poses%20and%20observations.%20Such%20a%20pose-only%20measurement%20is%20proven%0Ato%20be%20tightly-coupled%20between%20landmarks%20and%20poses%2C%20and%20maintain%20a%20perfect%20null%0Aspace%20that%20is%20independent%20of%20estimated%20poses.%20Finally%2C%20we%20apply%20the%20proposed%0Aapproach%20to%20a%20filter%20based%20GVIO%20with%20a%20novel%20feature%20management%20strategy.%20Both%0Asimulation%20tests%20and%20real-world%20experiments%20are%20conducted%20to%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method%20in%20terms%20of%20efficiency%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-view%2520Landmark%2520Representation%2520Approach%2520with%2520Application%2520to%250A%2520%2520GNSS-Visual-Inertial%2520Odometry%26entry.906535625%3DTong%2520Hua%2520and%2520Jiale%2520Han%2520and%2520Wei%2520Ouyang%26entry.1292438233%3D%2520%2520Invariant%2520Extended%2520Kalman%2520Filter%2520%2528IEKF%2529%2520has%2520been%2520a%2520significant%2520technique%2520in%250Avision-aided%2520sensor%2520fusion.%2520However%252C%2520it%2520usually%2520suffers%2520from%2520high%2520computational%250Aburden%2520when%2520jointly%2520optimizing%2520camera%2520poses%2520and%2520the%2520landmarks.%2520To%2520improve%2520its%250Aefficiency%2520and%2520applicability%2520for%2520multi-sensor%2520fusion%252C%2520we%2520present%2520a%2520multi-view%250Apose-only%2520estimation%2520approach%2520with%2520its%2520application%2520to%2520GNSS-Visual-Inertial%250AOdometry%2520%2528GVIO%2529%2520in%2520this%2520paper.%2520Our%2520main%2520contribution%2520is%2520deriving%2520a%2520visual%250Ameasurement%2520model%2520which%2520directly%2520associates%2520landmark%2520representation%2520with%250Amultiple%2520camera%2520poses%2520and%2520observations.%2520Such%2520a%2520pose-only%2520measurement%2520is%2520proven%250Ato%2520be%2520tightly-coupled%2520between%2520landmarks%2520and%2520poses%252C%2520and%2520maintain%2520a%2520perfect%2520null%250Aspace%2520that%2520is%2520independent%2520of%2520estimated%2520poses.%2520Finally%252C%2520we%2520apply%2520the%2520proposed%250Aapproach%2520to%2520a%2520filter%2520based%2520GVIO%2520with%2520a%2520novel%2520feature%2520management%2520strategy.%2520Both%250Asimulation%2520tests%2520and%2520real-world%2520experiments%2520are%2520conducted%2520to%2520demonstrate%2520the%250Asuperiority%2520of%2520the%2520proposed%2520method%2520in%2520terms%2520of%2520efficiency%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-view%20Landmark%20Representation%20Approach%20with%20Application%20to%0A%20%20GNSS-Visual-Inertial%20Odometry&entry.906535625=Tong%20Hua%20and%20Jiale%20Han%20and%20Wei%20Ouyang&entry.1292438233=%20%20Invariant%20Extended%20Kalman%20Filter%20%28IEKF%29%20has%20been%20a%20significant%20technique%20in%0Avision-aided%20sensor%20fusion.%20However%2C%20it%20usually%20suffers%20from%20high%20computational%0Aburden%20when%20jointly%20optimizing%20camera%20poses%20and%20the%20landmarks.%20To%20improve%20its%0Aefficiency%20and%20applicability%20for%20multi-sensor%20fusion%2C%20we%20present%20a%20multi-view%0Apose-only%20estimation%20approach%20with%20its%20application%20to%20GNSS-Visual-Inertial%0AOdometry%20%28GVIO%29%20in%20this%20paper.%20Our%20main%20contribution%20is%20deriving%20a%20visual%0Ameasurement%20model%20which%20directly%20associates%20landmark%20representation%20with%0Amultiple%20camera%20poses%20and%20observations.%20Such%20a%20pose-only%20measurement%20is%20proven%0Ato%20be%20tightly-coupled%20between%20landmarks%20and%20poses%2C%20and%20maintain%20a%20perfect%20null%0Aspace%20that%20is%20independent%20of%20estimated%20poses.%20Finally%2C%20we%20apply%20the%20proposed%0Aapproach%20to%20a%20filter%20based%20GVIO%20with%20a%20novel%20feature%20management%20strategy.%20Both%0Asimulation%20tests%20and%20real-world%20experiments%20are%20conducted%20to%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method%20in%20terms%20of%20efficiency%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05368v1&entry.124074799=Read"},
{"title": "Bayesian Optimization applied for accelerated Virtual Validation of the\n  Autonomous Driving Function", "author": "Satyesh Shanker Awasthi and Mohammed Irshadh Ismaaeel Sathyamangalam Imran and Stefano Arrigoni and Francesco Braghin", "abstract": "  Rigorous Verification and Validation (V&V) of Autonomous Driving Functions\n(ADFs) is paramount for ensuring the safety and public acceptance of Autonomous\nVehicles (AVs). Current validation relies heavily on simulation to achieve\nsufficient test coverage within the Operational Design Domain (ODD) of a\nvehicle, but exhaustively exploring the vast parameter space of possible\nscenarios is computationally expensive and time-consuming. This work introduces\na framework based on Bayesian Optimization (BO) to accelerate the discovery of\ncritical scenarios. We demonstrate the effectiveness of the framework on an\nModel Predictive Controller (MPC)-based motion planner, showing that it\nidentifies hazardous situations, such as off-road events, using orders of\nmagnitude fewer simulations than brute-force Design of Experiments (DoE)\nmethods. Furthermore, this study investigates the scalability of the framework\nin higher-dimensional parameter spaces and its ability to identify multiple,\ndistinct critical regions within the ODD of the motion planner used as the case\nstudy .\n", "link": "http://arxiv.org/abs/2507.22769v2", "date": "2025-08-07", "relevancy": 2.1475, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5954}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.541}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Optimization%20applied%20for%20accelerated%20Virtual%20Validation%20of%20the%0A%20%20Autonomous%20Driving%20Function&body=Title%3A%20Bayesian%20Optimization%20applied%20for%20accelerated%20Virtual%20Validation%20of%20the%0A%20%20Autonomous%20Driving%20Function%0AAuthor%3A%20Satyesh%20Shanker%20Awasthi%20and%20Mohammed%20Irshadh%20Ismaaeel%20Sathyamangalam%20Imran%20and%20Stefano%20Arrigoni%20and%20Francesco%20Braghin%0AAbstract%3A%20%20%20Rigorous%20Verification%20and%20Validation%20%28V%26V%29%20of%20Autonomous%20Driving%20Functions%0A%28ADFs%29%20is%20paramount%20for%20ensuring%20the%20safety%20and%20public%20acceptance%20of%20Autonomous%0AVehicles%20%28AVs%29.%20Current%20validation%20relies%20heavily%20on%20simulation%20to%20achieve%0Asufficient%20test%20coverage%20within%20the%20Operational%20Design%20Domain%20%28ODD%29%20of%20a%0Avehicle%2C%20but%20exhaustively%20exploring%20the%20vast%20parameter%20space%20of%20possible%0Ascenarios%20is%20computationally%20expensive%20and%20time-consuming.%20This%20work%20introduces%0Aa%20framework%20based%20on%20Bayesian%20Optimization%20%28BO%29%20to%20accelerate%20the%20discovery%20of%0Acritical%20scenarios.%20We%20demonstrate%20the%20effectiveness%20of%20the%20framework%20on%20an%0AModel%20Predictive%20Controller%20%28MPC%29-based%20motion%20planner%2C%20showing%20that%20it%0Aidentifies%20hazardous%20situations%2C%20such%20as%20off-road%20events%2C%20using%20orders%20of%0Amagnitude%20fewer%20simulations%20than%20brute-force%20Design%20of%20Experiments%20%28DoE%29%0Amethods.%20Furthermore%2C%20this%20study%20investigates%20the%20scalability%20of%20the%20framework%0Ain%20higher-dimensional%20parameter%20spaces%20and%20its%20ability%20to%20identify%20multiple%2C%0Adistinct%20critical%20regions%20within%20the%20ODD%20of%20the%20motion%20planner%20used%20as%20the%20case%0Astudy%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Optimization%2520applied%2520for%2520accelerated%2520Virtual%2520Validation%2520of%2520the%250A%2520%2520Autonomous%2520Driving%2520Function%26entry.906535625%3DSatyesh%2520Shanker%2520Awasthi%2520and%2520Mohammed%2520Irshadh%2520Ismaaeel%2520Sathyamangalam%2520Imran%2520and%2520Stefano%2520Arrigoni%2520and%2520Francesco%2520Braghin%26entry.1292438233%3D%2520%2520Rigorous%2520Verification%2520and%2520Validation%2520%2528V%2526V%2529%2520of%2520Autonomous%2520Driving%2520Functions%250A%2528ADFs%2529%2520is%2520paramount%2520for%2520ensuring%2520the%2520safety%2520and%2520public%2520acceptance%2520of%2520Autonomous%250AVehicles%2520%2528AVs%2529.%2520Current%2520validation%2520relies%2520heavily%2520on%2520simulation%2520to%2520achieve%250Asufficient%2520test%2520coverage%2520within%2520the%2520Operational%2520Design%2520Domain%2520%2528ODD%2529%2520of%2520a%250Avehicle%252C%2520but%2520exhaustively%2520exploring%2520the%2520vast%2520parameter%2520space%2520of%2520possible%250Ascenarios%2520is%2520computationally%2520expensive%2520and%2520time-consuming.%2520This%2520work%2520introduces%250Aa%2520framework%2520based%2520on%2520Bayesian%2520Optimization%2520%2528BO%2529%2520to%2520accelerate%2520the%2520discovery%2520of%250Acritical%2520scenarios.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520framework%2520on%2520an%250AModel%2520Predictive%2520Controller%2520%2528MPC%2529-based%2520motion%2520planner%252C%2520showing%2520that%2520it%250Aidentifies%2520hazardous%2520situations%252C%2520such%2520as%2520off-road%2520events%252C%2520using%2520orders%2520of%250Amagnitude%2520fewer%2520simulations%2520than%2520brute-force%2520Design%2520of%2520Experiments%2520%2528DoE%2529%250Amethods.%2520Furthermore%252C%2520this%2520study%2520investigates%2520the%2520scalability%2520of%2520the%2520framework%250Ain%2520higher-dimensional%2520parameter%2520spaces%2520and%2520its%2520ability%2520to%2520identify%2520multiple%252C%250Adistinct%2520critical%2520regions%2520within%2520the%2520ODD%2520of%2520the%2520motion%2520planner%2520used%2520as%2520the%2520case%250Astudy%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Optimization%20applied%20for%20accelerated%20Virtual%20Validation%20of%20the%0A%20%20Autonomous%20Driving%20Function&entry.906535625=Satyesh%20Shanker%20Awasthi%20and%20Mohammed%20Irshadh%20Ismaaeel%20Sathyamangalam%20Imran%20and%20Stefano%20Arrigoni%20and%20Francesco%20Braghin&entry.1292438233=%20%20Rigorous%20Verification%20and%20Validation%20%28V%26V%29%20of%20Autonomous%20Driving%20Functions%0A%28ADFs%29%20is%20paramount%20for%20ensuring%20the%20safety%20and%20public%20acceptance%20of%20Autonomous%0AVehicles%20%28AVs%29.%20Current%20validation%20relies%20heavily%20on%20simulation%20to%20achieve%0Asufficient%20test%20coverage%20within%20the%20Operational%20Design%20Domain%20%28ODD%29%20of%20a%0Avehicle%2C%20but%20exhaustively%20exploring%20the%20vast%20parameter%20space%20of%20possible%0Ascenarios%20is%20computationally%20expensive%20and%20time-consuming.%20This%20work%20introduces%0Aa%20framework%20based%20on%20Bayesian%20Optimization%20%28BO%29%20to%20accelerate%20the%20discovery%20of%0Acritical%20scenarios.%20We%20demonstrate%20the%20effectiveness%20of%20the%20framework%20on%20an%0AModel%20Predictive%20Controller%20%28MPC%29-based%20motion%20planner%2C%20showing%20that%20it%0Aidentifies%20hazardous%20situations%2C%20such%20as%20off-road%20events%2C%20using%20orders%20of%0Amagnitude%20fewer%20simulations%20than%20brute-force%20Design%20of%20Experiments%20%28DoE%29%0Amethods.%20Furthermore%2C%20this%20study%20investigates%20the%20scalability%20of%20the%20framework%0Ain%20higher-dimensional%20parameter%20spaces%20and%20its%20ability%20to%20identify%20multiple%2C%0Adistinct%20critical%20regions%20within%20the%20ODD%20of%20the%20motion%20planner%20used%20as%20the%20case%0Astudy%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22769v2&entry.124074799=Read"},
{"title": "LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large\n  Language Model", "author": "Tao Sun and Oliver Liu and JinJin Li and Lan Ma", "abstract": "  Multimodal generative AI usually involves generating image or text responses\ngiven inputs in another modality. The evaluation of image-text relevancy is\nessential for measuring response quality or ranking candidate responses. In\nparticular, binary relevancy evaluation, i.e., ``Relevant'' vs. ``Not\nRelevant'', is a fundamental problem. However, this is a challenging task\nconsidering that texts have diverse formats and the definition of relevancy\nvaries in different scenarios. We find that Multimodal Large Language Models\n(MLLMs) are an ideal choice to build such evaluators, as they can flexibly\nhandle complex text formats and take in additional task information. In this\npaper, we present LLaVA-RE, a first attempt for binary image-text relevancy\nevaluation with MLLM. It follows the LLaVA architecture and adopts detailed\ntask instructions and multimodal in-context samples. In addition, we propose a\nnovel binary relevancy data set that covers various tasks. Experimental results\nvalidate the effectiveness of our framework.\n", "link": "http://arxiv.org/abs/2508.05602v1", "date": "2025-08-07", "relevancy": 2.1318, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5362}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA-RE%3A%20Binary%20Image-Text%20Relevancy%20Evaluation%20with%20Multimodal%20Large%0A%20%20Language%20Model&body=Title%3A%20LLaVA-RE%3A%20Binary%20Image-Text%20Relevancy%20Evaluation%20with%20Multimodal%20Large%0A%20%20Language%20Model%0AAuthor%3A%20Tao%20Sun%20and%20Oliver%20Liu%20and%20JinJin%20Li%20and%20Lan%20Ma%0AAbstract%3A%20%20%20Multimodal%20generative%20AI%20usually%20involves%20generating%20image%20or%20text%20responses%0Agiven%20inputs%20in%20another%20modality.%20The%20evaluation%20of%20image-text%20relevancy%20is%0Aessential%20for%20measuring%20response%20quality%20or%20ranking%20candidate%20responses.%20In%0Aparticular%2C%20binary%20relevancy%20evaluation%2C%20i.e.%2C%20%60%60Relevant%27%27%20vs.%20%60%60Not%0ARelevant%27%27%2C%20is%20a%20fundamental%20problem.%20However%2C%20this%20is%20a%20challenging%20task%0Aconsidering%20that%20texts%20have%20diverse%20formats%20and%20the%20definition%20of%20relevancy%0Avaries%20in%20different%20scenarios.%20We%20find%20that%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20are%20an%20ideal%20choice%20to%20build%20such%20evaluators%2C%20as%20they%20can%20flexibly%0Ahandle%20complex%20text%20formats%20and%20take%20in%20additional%20task%20information.%20In%20this%0Apaper%2C%20we%20present%20LLaVA-RE%2C%20a%20first%20attempt%20for%20binary%20image-text%20relevancy%0Aevaluation%20with%20MLLM.%20It%20follows%20the%20LLaVA%20architecture%20and%20adopts%20detailed%0Atask%20instructions%20and%20multimodal%20in-context%20samples.%20In%20addition%2C%20we%20propose%20a%0Anovel%20binary%20relevancy%20data%20set%20that%20covers%20various%20tasks.%20Experimental%20results%0Avalidate%20the%20effectiveness%20of%20our%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA-RE%253A%2520Binary%2520Image-Text%2520Relevancy%2520Evaluation%2520with%2520Multimodal%2520Large%250A%2520%2520Language%2520Model%26entry.906535625%3DTao%2520Sun%2520and%2520Oliver%2520Liu%2520and%2520JinJin%2520Li%2520and%2520Lan%2520Ma%26entry.1292438233%3D%2520%2520Multimodal%2520generative%2520AI%2520usually%2520involves%2520generating%2520image%2520or%2520text%2520responses%250Agiven%2520inputs%2520in%2520another%2520modality.%2520The%2520evaluation%2520of%2520image-text%2520relevancy%2520is%250Aessential%2520for%2520measuring%2520response%2520quality%2520or%2520ranking%2520candidate%2520responses.%2520In%250Aparticular%252C%2520binary%2520relevancy%2520evaluation%252C%2520i.e.%252C%2520%2560%2560Relevant%2527%2527%2520vs.%2520%2560%2560Not%250ARelevant%2527%2527%252C%2520is%2520a%2520fundamental%2520problem.%2520However%252C%2520this%2520is%2520a%2520challenging%2520task%250Aconsidering%2520that%2520texts%2520have%2520diverse%2520formats%2520and%2520the%2520definition%2520of%2520relevancy%250Avaries%2520in%2520different%2520scenarios.%2520We%2520find%2520that%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520are%2520an%2520ideal%2520choice%2520to%2520build%2520such%2520evaluators%252C%2520as%2520they%2520can%2520flexibly%250Ahandle%2520complex%2520text%2520formats%2520and%2520take%2520in%2520additional%2520task%2520information.%2520In%2520this%250Apaper%252C%2520we%2520present%2520LLaVA-RE%252C%2520a%2520first%2520attempt%2520for%2520binary%2520image-text%2520relevancy%250Aevaluation%2520with%2520MLLM.%2520It%2520follows%2520the%2520LLaVA%2520architecture%2520and%2520adopts%2520detailed%250Atask%2520instructions%2520and%2520multimodal%2520in-context%2520samples.%2520In%2520addition%252C%2520we%2520propose%2520a%250Anovel%2520binary%2520relevancy%2520data%2520set%2520that%2520covers%2520various%2520tasks.%2520Experimental%2520results%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-RE%3A%20Binary%20Image-Text%20Relevancy%20Evaluation%20with%20Multimodal%20Large%0A%20%20Language%20Model&entry.906535625=Tao%20Sun%20and%20Oliver%20Liu%20and%20JinJin%20Li%20and%20Lan%20Ma&entry.1292438233=%20%20Multimodal%20generative%20AI%20usually%20involves%20generating%20image%20or%20text%20responses%0Agiven%20inputs%20in%20another%20modality.%20The%20evaluation%20of%20image-text%20relevancy%20is%0Aessential%20for%20measuring%20response%20quality%20or%20ranking%20candidate%20responses.%20In%0Aparticular%2C%20binary%20relevancy%20evaluation%2C%20i.e.%2C%20%60%60Relevant%27%27%20vs.%20%60%60Not%0ARelevant%27%27%2C%20is%20a%20fundamental%20problem.%20However%2C%20this%20is%20a%20challenging%20task%0Aconsidering%20that%20texts%20have%20diverse%20formats%20and%20the%20definition%20of%20relevancy%0Avaries%20in%20different%20scenarios.%20We%20find%20that%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20are%20an%20ideal%20choice%20to%20build%20such%20evaluators%2C%20as%20they%20can%20flexibly%0Ahandle%20complex%20text%20formats%20and%20take%20in%20additional%20task%20information.%20In%20this%0Apaper%2C%20we%20present%20LLaVA-RE%2C%20a%20first%20attempt%20for%20binary%20image-text%20relevancy%0Aevaluation%20with%20MLLM.%20It%20follows%20the%20LLaVA%20architecture%20and%20adopts%20detailed%0Atask%20instructions%20and%20multimodal%20in-context%20samples.%20In%20addition%2C%20we%20propose%20a%0Anovel%20binary%20relevancy%20data%20set%20that%20covers%20various%20tasks.%20Experimental%20results%0Avalidate%20the%20effectiveness%20of%20our%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05602v1&entry.124074799=Read"},
{"title": "Guided Random Forest and its application to data approximation", "author": "Prashant Gupta and Aashi Jindal and  Jayadeva and Debarka Sengupta", "abstract": "  We present a new way of constructing an ensemble classifier, named the Guided\nRandom Forest (GRAF) in the sequel. GRAF extends the idea of building oblique\ndecision trees with localized partitioning to obtain a global partitioning. We\nshow that global partitioning bridges the gap between decision trees and\nboosting algorithms. We empirically demonstrate that global partitioning\nreduces the generalization error bound. Results on 115 benchmark datasets show\nthat GRAF yields comparable or better results on a majority of datasets. We\nalso present a new way of approximating the datasets in the framework of random\nforests.\n", "link": "http://arxiv.org/abs/1909.00659v2", "date": "2025-08-07", "relevancy": 2.1304, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4576}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.415}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Random%20Forest%20and%20its%20application%20to%20data%20approximation&body=Title%3A%20Guided%20Random%20Forest%20and%20its%20application%20to%20data%20approximation%0AAuthor%3A%20Prashant%20Gupta%20and%20Aashi%20Jindal%20and%20%20Jayadeva%20and%20Debarka%20Sengupta%0AAbstract%3A%20%20%20We%20present%20a%20new%20way%20of%20constructing%20an%20ensemble%20classifier%2C%20named%20the%20Guided%0ARandom%20Forest%20%28GRAF%29%20in%20the%20sequel.%20GRAF%20extends%20the%20idea%20of%20building%20oblique%0Adecision%20trees%20with%20localized%20partitioning%20to%20obtain%20a%20global%20partitioning.%20We%0Ashow%20that%20global%20partitioning%20bridges%20the%20gap%20between%20decision%20trees%20and%0Aboosting%20algorithms.%20We%20empirically%20demonstrate%20that%20global%20partitioning%0Areduces%20the%20generalization%20error%20bound.%20Results%20on%20115%20benchmark%20datasets%20show%0Athat%20GRAF%20yields%20comparable%20or%20better%20results%20on%20a%20majority%20of%20datasets.%20We%0Aalso%20present%20a%20new%20way%20of%20approximating%20the%20datasets%20in%20the%20framework%20of%20random%0Aforests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1909.00659v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Random%2520Forest%2520and%2520its%2520application%2520to%2520data%2520approximation%26entry.906535625%3DPrashant%2520Gupta%2520and%2520Aashi%2520Jindal%2520and%2520%2520Jayadeva%2520and%2520Debarka%2520Sengupta%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520way%2520of%2520constructing%2520an%2520ensemble%2520classifier%252C%2520named%2520the%2520Guided%250ARandom%2520Forest%2520%2528GRAF%2529%2520in%2520the%2520sequel.%2520GRAF%2520extends%2520the%2520idea%2520of%2520building%2520oblique%250Adecision%2520trees%2520with%2520localized%2520partitioning%2520to%2520obtain%2520a%2520global%2520partitioning.%2520We%250Ashow%2520that%2520global%2520partitioning%2520bridges%2520the%2520gap%2520between%2520decision%2520trees%2520and%250Aboosting%2520algorithms.%2520We%2520empirically%2520demonstrate%2520that%2520global%2520partitioning%250Areduces%2520the%2520generalization%2520error%2520bound.%2520Results%2520on%2520115%2520benchmark%2520datasets%2520show%250Athat%2520GRAF%2520yields%2520comparable%2520or%2520better%2520results%2520on%2520a%2520majority%2520of%2520datasets.%2520We%250Aalso%2520present%2520a%2520new%2520way%2520of%2520approximating%2520the%2520datasets%2520in%2520the%2520framework%2520of%2520random%250Aforests.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1909.00659v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Random%20Forest%20and%20its%20application%20to%20data%20approximation&entry.906535625=Prashant%20Gupta%20and%20Aashi%20Jindal%20and%20%20Jayadeva%20and%20Debarka%20Sengupta&entry.1292438233=%20%20We%20present%20a%20new%20way%20of%20constructing%20an%20ensemble%20classifier%2C%20named%20the%20Guided%0ARandom%20Forest%20%28GRAF%29%20in%20the%20sequel.%20GRAF%20extends%20the%20idea%20of%20building%20oblique%0Adecision%20trees%20with%20localized%20partitioning%20to%20obtain%20a%20global%20partitioning.%20We%0Ashow%20that%20global%20partitioning%20bridges%20the%20gap%20between%20decision%20trees%20and%0Aboosting%20algorithms.%20We%20empirically%20demonstrate%20that%20global%20partitioning%0Areduces%20the%20generalization%20error%20bound.%20Results%20on%20115%20benchmark%20datasets%20show%0Athat%20GRAF%20yields%20comparable%20or%20better%20results%20on%20a%20majority%20of%20datasets.%20We%0Aalso%20present%20a%20new%20way%20of%20approximating%20the%20datasets%20in%20the%20framework%20of%20random%0Aforests.%0A&entry.1838667208=http%3A//arxiv.org/abs/1909.00659v2&entry.124074799=Read"},
{"title": "WeatherEdit: Controllable Weather Editing with 4D Gaussian Field", "author": "Chenghao Qian and Wenjing Li and Yuhu Guo and Gustav Markkula", "abstract": "  In this work, we present WeatherEdit, a novel weather editing pipeline for\ngenerating realistic weather effects with controllable types and severity in 3D\nscenes. Our approach is structured into two key components: weather background\nediting and weather particle construction. For weather background editing, we\nintroduce an all-in-one adapter that integrates multiple weather styles into a\nsingle pretrained diffusion model, enabling the generation of diverse weather\neffects in 2D image backgrounds. During inference, we design a Temporal-View\n(TV-) attention mechanism that follows a specific order to aggregate temporal\nand spatial information, ensuring consistent editing across multi-frame and\nmulti-view images. To construct the weather particles, we first reconstruct a\n3D scene using the edited images and then introduce a dynamic 4D Gaussian field\nto generate snowflakes, raindrops and fog in the scene. The attributes and\ndynamics of these particles are precisely controlled through physical-based\nmodelling and simulation, ensuring realistic weather representation and\nflexible severity adjustments. Finally, we integrate the 4D Gaussian field with\nthe 3D scene to render consistent and highly realistic weather effects.\nExperiments on multiple driving datasets demonstrate that WeatherEdit can\ngenerate diverse weather effects with controllable condition severity,\nhighlighting its potential for autonomous driving simulation in adverse\nweather. See project page: https://jumponthemoon.github.io/w-edit\n", "link": "http://arxiv.org/abs/2505.20471v3", "date": "2025-08-07", "relevancy": 2.1164, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5335}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5308}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeatherEdit%3A%20Controllable%20Weather%20Editing%20with%204D%20Gaussian%20Field&body=Title%3A%20WeatherEdit%3A%20Controllable%20Weather%20Editing%20with%204D%20Gaussian%20Field%0AAuthor%3A%20Chenghao%20Qian%20and%20Wenjing%20Li%20and%20Yuhu%20Guo%20and%20Gustav%20Markkula%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20WeatherEdit%2C%20a%20novel%20weather%20editing%20pipeline%20for%0Agenerating%20realistic%20weather%20effects%20with%20controllable%20types%20and%20severity%20in%203D%0Ascenes.%20Our%20approach%20is%20structured%20into%20two%20key%20components%3A%20weather%20background%0Aediting%20and%20weather%20particle%20construction.%20For%20weather%20background%20editing%2C%20we%0Aintroduce%20an%20all-in-one%20adapter%20that%20integrates%20multiple%20weather%20styles%20into%20a%0Asingle%20pretrained%20diffusion%20model%2C%20enabling%20the%20generation%20of%20diverse%20weather%0Aeffects%20in%202D%20image%20backgrounds.%20During%20inference%2C%20we%20design%20a%20Temporal-View%0A%28TV-%29%20attention%20mechanism%20that%20follows%20a%20specific%20order%20to%20aggregate%20temporal%0Aand%20spatial%20information%2C%20ensuring%20consistent%20editing%20across%20multi-frame%20and%0Amulti-view%20images.%20To%20construct%20the%20weather%20particles%2C%20we%20first%20reconstruct%20a%0A3D%20scene%20using%20the%20edited%20images%20and%20then%20introduce%20a%20dynamic%204D%20Gaussian%20field%0Ato%20generate%20snowflakes%2C%20raindrops%20and%20fog%20in%20the%20scene.%20The%20attributes%20and%0Adynamics%20of%20these%20particles%20are%20precisely%20controlled%20through%20physical-based%0Amodelling%20and%20simulation%2C%20ensuring%20realistic%20weather%20representation%20and%0Aflexible%20severity%20adjustments.%20Finally%2C%20we%20integrate%20the%204D%20Gaussian%20field%20with%0Athe%203D%20scene%20to%20render%20consistent%20and%20highly%20realistic%20weather%20effects.%0AExperiments%20on%20multiple%20driving%20datasets%20demonstrate%20that%20WeatherEdit%20can%0Agenerate%20diverse%20weather%20effects%20with%20controllable%20condition%20severity%2C%0Ahighlighting%20its%20potential%20for%20autonomous%20driving%20simulation%20in%20adverse%0Aweather.%20See%20project%20page%3A%20https%3A//jumponthemoon.github.io/w-edit%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20471v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeatherEdit%253A%2520Controllable%2520Weather%2520Editing%2520with%25204D%2520Gaussian%2520Field%26entry.906535625%3DChenghao%2520Qian%2520and%2520Wenjing%2520Li%2520and%2520Yuhu%2520Guo%2520and%2520Gustav%2520Markkula%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520WeatherEdit%252C%2520a%2520novel%2520weather%2520editing%2520pipeline%2520for%250Agenerating%2520realistic%2520weather%2520effects%2520with%2520controllable%2520types%2520and%2520severity%2520in%25203D%250Ascenes.%2520Our%2520approach%2520is%2520structured%2520into%2520two%2520key%2520components%253A%2520weather%2520background%250Aediting%2520and%2520weather%2520particle%2520construction.%2520For%2520weather%2520background%2520editing%252C%2520we%250Aintroduce%2520an%2520all-in-one%2520adapter%2520that%2520integrates%2520multiple%2520weather%2520styles%2520into%2520a%250Asingle%2520pretrained%2520diffusion%2520model%252C%2520enabling%2520the%2520generation%2520of%2520diverse%2520weather%250Aeffects%2520in%25202D%2520image%2520backgrounds.%2520During%2520inference%252C%2520we%2520design%2520a%2520Temporal-View%250A%2528TV-%2529%2520attention%2520mechanism%2520that%2520follows%2520a%2520specific%2520order%2520to%2520aggregate%2520temporal%250Aand%2520spatial%2520information%252C%2520ensuring%2520consistent%2520editing%2520across%2520multi-frame%2520and%250Amulti-view%2520images.%2520To%2520construct%2520the%2520weather%2520particles%252C%2520we%2520first%2520reconstruct%2520a%250A3D%2520scene%2520using%2520the%2520edited%2520images%2520and%2520then%2520introduce%2520a%2520dynamic%25204D%2520Gaussian%2520field%250Ato%2520generate%2520snowflakes%252C%2520raindrops%2520and%2520fog%2520in%2520the%2520scene.%2520The%2520attributes%2520and%250Adynamics%2520of%2520these%2520particles%2520are%2520precisely%2520controlled%2520through%2520physical-based%250Amodelling%2520and%2520simulation%252C%2520ensuring%2520realistic%2520weather%2520representation%2520and%250Aflexible%2520severity%2520adjustments.%2520Finally%252C%2520we%2520integrate%2520the%25204D%2520Gaussian%2520field%2520with%250Athe%25203D%2520scene%2520to%2520render%2520consistent%2520and%2520highly%2520realistic%2520weather%2520effects.%250AExperiments%2520on%2520multiple%2520driving%2520datasets%2520demonstrate%2520that%2520WeatherEdit%2520can%250Agenerate%2520diverse%2520weather%2520effects%2520with%2520controllable%2520condition%2520severity%252C%250Ahighlighting%2520its%2520potential%2520for%2520autonomous%2520driving%2520simulation%2520in%2520adverse%250Aweather.%2520See%2520project%2520page%253A%2520https%253A//jumponthemoon.github.io/w-edit%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20471v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeatherEdit%3A%20Controllable%20Weather%20Editing%20with%204D%20Gaussian%20Field&entry.906535625=Chenghao%20Qian%20and%20Wenjing%20Li%20and%20Yuhu%20Guo%20and%20Gustav%20Markkula&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20WeatherEdit%2C%20a%20novel%20weather%20editing%20pipeline%20for%0Agenerating%20realistic%20weather%20effects%20with%20controllable%20types%20and%20severity%20in%203D%0Ascenes.%20Our%20approach%20is%20structured%20into%20two%20key%20components%3A%20weather%20background%0Aediting%20and%20weather%20particle%20construction.%20For%20weather%20background%20editing%2C%20we%0Aintroduce%20an%20all-in-one%20adapter%20that%20integrates%20multiple%20weather%20styles%20into%20a%0Asingle%20pretrained%20diffusion%20model%2C%20enabling%20the%20generation%20of%20diverse%20weather%0Aeffects%20in%202D%20image%20backgrounds.%20During%20inference%2C%20we%20design%20a%20Temporal-View%0A%28TV-%29%20attention%20mechanism%20that%20follows%20a%20specific%20order%20to%20aggregate%20temporal%0Aand%20spatial%20information%2C%20ensuring%20consistent%20editing%20across%20multi-frame%20and%0Amulti-view%20images.%20To%20construct%20the%20weather%20particles%2C%20we%20first%20reconstruct%20a%0A3D%20scene%20using%20the%20edited%20images%20and%20then%20introduce%20a%20dynamic%204D%20Gaussian%20field%0Ato%20generate%20snowflakes%2C%20raindrops%20and%20fog%20in%20the%20scene.%20The%20attributes%20and%0Adynamics%20of%20these%20particles%20are%20precisely%20controlled%20through%20physical-based%0Amodelling%20and%20simulation%2C%20ensuring%20realistic%20weather%20representation%20and%0Aflexible%20severity%20adjustments.%20Finally%2C%20we%20integrate%20the%204D%20Gaussian%20field%20with%0Athe%203D%20scene%20to%20render%20consistent%20and%20highly%20realistic%20weather%20effects.%0AExperiments%20on%20multiple%20driving%20datasets%20demonstrate%20that%20WeatherEdit%20can%0Agenerate%20diverse%20weather%20effects%20with%20controllable%20condition%20severity%2C%0Ahighlighting%20its%20potential%20for%20autonomous%20driving%20simulation%20in%20adverse%0Aweather.%20See%20project%20page%3A%20https%3A//jumponthemoon.github.io/w-edit%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20471v3&entry.124074799=Read"},
{"title": "Conformal Sets in Multiple-Choice Question Answering under Black-Box\n  Settings with Provable Coverage Guarantees", "author": "Guang Yang and Xinyang Liu", "abstract": "  Large Language Models (LLMs) have shown remarkable progress in\nmultiple-choice question answering (MCQA), but their inherent unreliability,\nsuch as hallucination and overconfidence, limits their application in high-risk\ndomains. To address this, we propose a frequency-based uncertainty\nquantification method under black-box settings, leveraging conformal prediction\n(CP) to ensure provable coverage guarantees. Our approach involves multiple\nindependent samplings of the model's output distribution for each input, with\nthe most frequent sample serving as a reference to calculate predictive entropy\n(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,\nMedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms\nlogit-based PE in distinguishing between correct and incorrect predictions, as\nmeasured by AUROC. Furthermore, the method effectively controls the empirical\nmiscoverage rate under user-specified risk levels, validating that sampling\nfrequency can serve as a viable substitute for logit-based probabilities in\nblack-box scenarios. This work provides a distribution-free model-agnostic\nframework for reliable uncertainty quantification in MCQA with guaranteed\ncoverage, enhancing the trustworthiness of LLMs in practical applications.\n", "link": "http://arxiv.org/abs/2508.05544v1", "date": "2025-08-07", "relevancy": 2.0937, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5403}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5247}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Sets%20in%20Multiple-Choice%20Question%20Answering%20under%20Black-Box%0A%20%20Settings%20with%20Provable%20Coverage%20Guarantees&body=Title%3A%20Conformal%20Sets%20in%20Multiple-Choice%20Question%20Answering%20under%20Black-Box%0A%20%20Settings%20with%20Provable%20Coverage%20Guarantees%0AAuthor%3A%20Guang%20Yang%20and%20Xinyang%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20progress%20in%0Amultiple-choice%20question%20answering%20%28MCQA%29%2C%20but%20their%20inherent%20unreliability%2C%0Asuch%20as%20hallucination%20and%20overconfidence%2C%20limits%20their%20application%20in%20high-risk%0Adomains.%20To%20address%20this%2C%20we%20propose%20a%20frequency-based%20uncertainty%0Aquantification%20method%20under%20black-box%20settings%2C%20leveraging%20conformal%20prediction%0A%28CP%29%20to%20ensure%20provable%20coverage%20guarantees.%20Our%20approach%20involves%20multiple%0Aindependent%20samplings%20of%20the%20model%27s%20output%20distribution%20for%20each%20input%2C%20with%0Athe%20most%20frequent%20sample%20serving%20as%20a%20reference%20to%20calculate%20predictive%20entropy%0A%28PE%29.%20Experimental%20evaluations%20across%20six%20LLMs%20and%20four%20datasets%20%28MedMCQA%2C%0AMedQA%2C%20MMLU%2C%20MMLU-Pro%29%20demonstrate%20that%20frequency-based%20PE%20outperforms%0Alogit-based%20PE%20in%20distinguishing%20between%20correct%20and%20incorrect%20predictions%2C%20as%0Ameasured%20by%20AUROC.%20Furthermore%2C%20the%20method%20effectively%20controls%20the%20empirical%0Amiscoverage%20rate%20under%20user-specified%20risk%20levels%2C%20validating%20that%20sampling%0Afrequency%20can%20serve%20as%20a%20viable%20substitute%20for%20logit-based%20probabilities%20in%0Ablack-box%20scenarios.%20This%20work%20provides%20a%20distribution-free%20model-agnostic%0Aframework%20for%20reliable%20uncertainty%20quantification%20in%20MCQA%20with%20guaranteed%0Acoverage%2C%20enhancing%20the%20trustworthiness%20of%20LLMs%20in%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Sets%2520in%2520Multiple-Choice%2520Question%2520Answering%2520under%2520Black-Box%250A%2520%2520Settings%2520with%2520Provable%2520Coverage%2520Guarantees%26entry.906535625%3DGuang%2520Yang%2520and%2520Xinyang%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520progress%2520in%250Amultiple-choice%2520question%2520answering%2520%2528MCQA%2529%252C%2520but%2520their%2520inherent%2520unreliability%252C%250Asuch%2520as%2520hallucination%2520and%2520overconfidence%252C%2520limits%2520their%2520application%2520in%2520high-risk%250Adomains.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520frequency-based%2520uncertainty%250Aquantification%2520method%2520under%2520black-box%2520settings%252C%2520leveraging%2520conformal%2520prediction%250A%2528CP%2529%2520to%2520ensure%2520provable%2520coverage%2520guarantees.%2520Our%2520approach%2520involves%2520multiple%250Aindependent%2520samplings%2520of%2520the%2520model%2527s%2520output%2520distribution%2520for%2520each%2520input%252C%2520with%250Athe%2520most%2520frequent%2520sample%2520serving%2520as%2520a%2520reference%2520to%2520calculate%2520predictive%2520entropy%250A%2528PE%2529.%2520Experimental%2520evaluations%2520across%2520six%2520LLMs%2520and%2520four%2520datasets%2520%2528MedMCQA%252C%250AMedQA%252C%2520MMLU%252C%2520MMLU-Pro%2529%2520demonstrate%2520that%2520frequency-based%2520PE%2520outperforms%250Alogit-based%2520PE%2520in%2520distinguishing%2520between%2520correct%2520and%2520incorrect%2520predictions%252C%2520as%250Ameasured%2520by%2520AUROC.%2520Furthermore%252C%2520the%2520method%2520effectively%2520controls%2520the%2520empirical%250Amiscoverage%2520rate%2520under%2520user-specified%2520risk%2520levels%252C%2520validating%2520that%2520sampling%250Afrequency%2520can%2520serve%2520as%2520a%2520viable%2520substitute%2520for%2520logit-based%2520probabilities%2520in%250Ablack-box%2520scenarios.%2520This%2520work%2520provides%2520a%2520distribution-free%2520model-agnostic%250Aframework%2520for%2520reliable%2520uncertainty%2520quantification%2520in%2520MCQA%2520with%2520guaranteed%250Acoverage%252C%2520enhancing%2520the%2520trustworthiness%2520of%2520LLMs%2520in%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Sets%20in%20Multiple-Choice%20Question%20Answering%20under%20Black-Box%0A%20%20Settings%20with%20Provable%20Coverage%20Guarantees&entry.906535625=Guang%20Yang%20and%20Xinyang%20Liu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20progress%20in%0Amultiple-choice%20question%20answering%20%28MCQA%29%2C%20but%20their%20inherent%20unreliability%2C%0Asuch%20as%20hallucination%20and%20overconfidence%2C%20limits%20their%20application%20in%20high-risk%0Adomains.%20To%20address%20this%2C%20we%20propose%20a%20frequency-based%20uncertainty%0Aquantification%20method%20under%20black-box%20settings%2C%20leveraging%20conformal%20prediction%0A%28CP%29%20to%20ensure%20provable%20coverage%20guarantees.%20Our%20approach%20involves%20multiple%0Aindependent%20samplings%20of%20the%20model%27s%20output%20distribution%20for%20each%20input%2C%20with%0Athe%20most%20frequent%20sample%20serving%20as%20a%20reference%20to%20calculate%20predictive%20entropy%0A%28PE%29.%20Experimental%20evaluations%20across%20six%20LLMs%20and%20four%20datasets%20%28MedMCQA%2C%0AMedQA%2C%20MMLU%2C%20MMLU-Pro%29%20demonstrate%20that%20frequency-based%20PE%20outperforms%0Alogit-based%20PE%20in%20distinguishing%20between%20correct%20and%20incorrect%20predictions%2C%20as%0Ameasured%20by%20AUROC.%20Furthermore%2C%20the%20method%20effectively%20controls%20the%20empirical%0Amiscoverage%20rate%20under%20user-specified%20risk%20levels%2C%20validating%20that%20sampling%0Afrequency%20can%20serve%20as%20a%20viable%20substitute%20for%20logit-based%20probabilities%20in%0Ablack-box%20scenarios.%20This%20work%20provides%20a%20distribution-free%20model-agnostic%0Aframework%20for%20reliable%20uncertainty%20quantification%20in%20MCQA%20with%20guaranteed%0Acoverage%2C%20enhancing%20the%20trustworthiness%20of%20LLMs%20in%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05544v1&entry.124074799=Read"},
{"title": "Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI\n  Perspective", "author": "Wei Xu", "abstract": "  This chapter systematically promotes an emerging interdisciplinary field of\nhuman-artificial intelligence interaction (human-AI interaction, HAII) from a\nhuman-centered AI (HCAI) perspective. It introduces a framework of\nhuman-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII\nresearch and applications, emphasizing the importance of adopting a\nhuman-centered approach over a technology-centered one. The chapter presents\nthe HC-HAII methodology, including human-centered methods, process,\ninterdisciplinary teams, and multi-level design paradigms. It also highlights\nkey research challenges and future directions. As the first chapter, this\nchapter also provides a structural overview of this book, which brings together\ncontributions from an interdisciplinary community of researchers and\npractitioners to advance the theory, methodology, and applications of HCAI in\ndiverse domains of HAII. The purpose of this chapter is to provide a\nfundamental framework for this book, centered on HAII research and applications\nbased on the HCAI approach, which will pave the way for the content of\nsubsequent chapters.\n", "link": "http://arxiv.org/abs/2508.03969v2", "date": "2025-08-07", "relevancy": 2.0875, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4295}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.416}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Centered%20Human-AI%20Interaction%20%28HC-HAII%29%3A%20A%20Human-Centered%20AI%0A%20%20Perspective&body=Title%3A%20Human-Centered%20Human-AI%20Interaction%20%28HC-HAII%29%3A%20A%20Human-Centered%20AI%0A%20%20Perspective%0AAuthor%3A%20Wei%20Xu%0AAbstract%3A%20%20%20This%20chapter%20systematically%20promotes%20an%20emerging%20interdisciplinary%20field%20of%0Ahuman-artificial%20intelligence%20interaction%20%28human-AI%20interaction%2C%20HAII%29%20from%20a%0Ahuman-centered%20AI%20%28HCAI%29%20perspective.%20It%20introduces%20a%20framework%20of%0Ahuman-centered%20HAII%20%28HC-HAII%29.%20HC-HAII%20places%20humans%20at%20the%20core%20of%20HAII%0Aresearch%20and%20applications%2C%20emphasizing%20the%20importance%20of%20adopting%20a%0Ahuman-centered%20approach%20over%20a%20technology-centered%20one.%20The%20chapter%20presents%0Athe%20HC-HAII%20methodology%2C%20including%20human-centered%20methods%2C%20process%2C%0Ainterdisciplinary%20teams%2C%20and%20multi-level%20design%20paradigms.%20It%20also%20highlights%0Akey%20research%20challenges%20and%20future%20directions.%20As%20the%20first%20chapter%2C%20this%0Achapter%20also%20provides%20a%20structural%20overview%20of%20this%20book%2C%20which%20brings%20together%0Acontributions%20from%20an%20interdisciplinary%20community%20of%20researchers%20and%0Apractitioners%20to%20advance%20the%20theory%2C%20methodology%2C%20and%20applications%20of%20HCAI%20in%0Adiverse%20domains%20of%20HAII.%20The%20purpose%20of%20this%20chapter%20is%20to%20provide%20a%0Afundamental%20framework%20for%20this%20book%2C%20centered%20on%20HAII%20research%20and%20applications%0Abased%20on%20the%20HCAI%20approach%2C%20which%20will%20pave%20the%20way%20for%20the%20content%20of%0Asubsequent%20chapters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Centered%2520Human-AI%2520Interaction%2520%2528HC-HAII%2529%253A%2520A%2520Human-Centered%2520AI%250A%2520%2520Perspective%26entry.906535625%3DWei%2520Xu%26entry.1292438233%3D%2520%2520This%2520chapter%2520systematically%2520promotes%2520an%2520emerging%2520interdisciplinary%2520field%2520of%250Ahuman-artificial%2520intelligence%2520interaction%2520%2528human-AI%2520interaction%252C%2520HAII%2529%2520from%2520a%250Ahuman-centered%2520AI%2520%2528HCAI%2529%2520perspective.%2520It%2520introduces%2520a%2520framework%2520of%250Ahuman-centered%2520HAII%2520%2528HC-HAII%2529.%2520HC-HAII%2520places%2520humans%2520at%2520the%2520core%2520of%2520HAII%250Aresearch%2520and%2520applications%252C%2520emphasizing%2520the%2520importance%2520of%2520adopting%2520a%250Ahuman-centered%2520approach%2520over%2520a%2520technology-centered%2520one.%2520The%2520chapter%2520presents%250Athe%2520HC-HAII%2520methodology%252C%2520including%2520human-centered%2520methods%252C%2520process%252C%250Ainterdisciplinary%2520teams%252C%2520and%2520multi-level%2520design%2520paradigms.%2520It%2520also%2520highlights%250Akey%2520research%2520challenges%2520and%2520future%2520directions.%2520As%2520the%2520first%2520chapter%252C%2520this%250Achapter%2520also%2520provides%2520a%2520structural%2520overview%2520of%2520this%2520book%252C%2520which%2520brings%2520together%250Acontributions%2520from%2520an%2520interdisciplinary%2520community%2520of%2520researchers%2520and%250Apractitioners%2520to%2520advance%2520the%2520theory%252C%2520methodology%252C%2520and%2520applications%2520of%2520HCAI%2520in%250Adiverse%2520domains%2520of%2520HAII.%2520The%2520purpose%2520of%2520this%2520chapter%2520is%2520to%2520provide%2520a%250Afundamental%2520framework%2520for%2520this%2520book%252C%2520centered%2520on%2520HAII%2520research%2520and%2520applications%250Abased%2520on%2520the%2520HCAI%2520approach%252C%2520which%2520will%2520pave%2520the%2520way%2520for%2520the%2520content%2520of%250Asubsequent%2520chapters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Centered%20Human-AI%20Interaction%20%28HC-HAII%29%3A%20A%20Human-Centered%20AI%0A%20%20Perspective&entry.906535625=Wei%20Xu&entry.1292438233=%20%20This%20chapter%20systematically%20promotes%20an%20emerging%20interdisciplinary%20field%20of%0Ahuman-artificial%20intelligence%20interaction%20%28human-AI%20interaction%2C%20HAII%29%20from%20a%0Ahuman-centered%20AI%20%28HCAI%29%20perspective.%20It%20introduces%20a%20framework%20of%0Ahuman-centered%20HAII%20%28HC-HAII%29.%20HC-HAII%20places%20humans%20at%20the%20core%20of%20HAII%0Aresearch%20and%20applications%2C%20emphasizing%20the%20importance%20of%20adopting%20a%0Ahuman-centered%20approach%20over%20a%20technology-centered%20one.%20The%20chapter%20presents%0Athe%20HC-HAII%20methodology%2C%20including%20human-centered%20methods%2C%20process%2C%0Ainterdisciplinary%20teams%2C%20and%20multi-level%20design%20paradigms.%20It%20also%20highlights%0Akey%20research%20challenges%20and%20future%20directions.%20As%20the%20first%20chapter%2C%20this%0Achapter%20also%20provides%20a%20structural%20overview%20of%20this%20book%2C%20which%20brings%20together%0Acontributions%20from%20an%20interdisciplinary%20community%20of%20researchers%20and%0Apractitioners%20to%20advance%20the%20theory%2C%20methodology%2C%20and%20applications%20of%20HCAI%20in%0Adiverse%20domains%20of%20HAII.%20The%20purpose%20of%20this%20chapter%20is%20to%20provide%20a%0Afundamental%20framework%20for%20this%20book%2C%20centered%20on%20HAII%20research%20and%20applications%0Abased%20on%20the%20HCAI%20approach%2C%20which%20will%20pave%20the%20way%20for%20the%20content%20of%0Asubsequent%20chapters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03969v2&entry.124074799=Read"},
{"title": "The World According to LLMs: How Geographic Origin Influences LLMs'\n  Entity Deduction Capabilities", "author": "Harsh Nishant Lalai and Raj Sanjay Shah and Jiaxin Pei and Sashank Varma and Yi-Chia Wang and Ali Emami", "abstract": "  Large Language Models (LLMs) have been extensively tuned to mitigate explicit\nbiases, yet they often exhibit subtle implicit biases rooted in their\npre-training data. Rather than directly probing LLMs with human-crafted\nquestions that may trigger guardrails, we propose studying how models behave\nwhen they proactively ask questions themselves. The 20 Questions game, a\nmulti-turn deduction task, serves as an ideal testbed for this purpose. We\nsystematically evaluate geographic performance disparities in entity deduction\nusing a new dataset, Geo20Q+, consisting of both notable people and culturally\nsignificant objects (e.g., foods, landmarks, animals) from diverse regions. We\ntest popular LLMs across two gameplay configurations (canonical 20-question and\nunlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,\nFrench, Spanish, and Turkish). Our results reveal geographic disparities: LLMs\nare substantially more successful at deducing entities from the Global North\nthan the Global South, and the Global West than the Global East. While\nWikipedia pageviews and pre-training corpus frequency correlate mildly with\nperformance, they fail to fully explain these disparities. Notably, the\nlanguage in which the game is played has minimal impact on performance gaps.\nThese findings demonstrate the value of creative, free-form evaluation\nframeworks for uncovering subtle biases in LLMs that remain hidden in standard\nprompting setups. By analyzing how models initiate and pursue reasoning goals\nover multiple turns, we find geographic and cultural disparities embedded in\ntheir reasoning processes. We release the dataset (Geo20Q+) and code at\nhttps://sites.google.com/view/llmbias20q/home.\n", "link": "http://arxiv.org/abs/2508.05525v1", "date": "2025-08-07", "relevancy": 2.087, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20World%20According%20to%20LLMs%3A%20How%20Geographic%20Origin%20Influences%20LLMs%27%0A%20%20Entity%20Deduction%20Capabilities&body=Title%3A%20The%20World%20According%20to%20LLMs%3A%20How%20Geographic%20Origin%20Influences%20LLMs%27%0A%20%20Entity%20Deduction%20Capabilities%0AAuthor%3A%20Harsh%20Nishant%20Lalai%20and%20Raj%20Sanjay%20Shah%20and%20Jiaxin%20Pei%20and%20Sashank%20Varma%20and%20Yi-Chia%20Wang%20and%20Ali%20Emami%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20extensively%20tuned%20to%20mitigate%20explicit%0Abiases%2C%20yet%20they%20often%20exhibit%20subtle%20implicit%20biases%20rooted%20in%20their%0Apre-training%20data.%20Rather%20than%20directly%20probing%20LLMs%20with%20human-crafted%0Aquestions%20that%20may%20trigger%20guardrails%2C%20we%20propose%20studying%20how%20models%20behave%0Awhen%20they%20proactively%20ask%20questions%20themselves.%20The%2020%20Questions%20game%2C%20a%0Amulti-turn%20deduction%20task%2C%20serves%20as%20an%20ideal%20testbed%20for%20this%20purpose.%20We%0Asystematically%20evaluate%20geographic%20performance%20disparities%20in%20entity%20deduction%0Ausing%20a%20new%20dataset%2C%20Geo20Q%2B%2C%20consisting%20of%20both%20notable%20people%20and%20culturally%0Asignificant%20objects%20%28e.g.%2C%20foods%2C%20landmarks%2C%20animals%29%20from%20diverse%20regions.%20We%0Atest%20popular%20LLMs%20across%20two%20gameplay%20configurations%20%28canonical%2020-question%20and%0Aunlimited%20turns%29%20and%20in%20seven%20languages%20%28English%2C%20Hindi%2C%20Mandarin%2C%20Japanese%2C%0AFrench%2C%20Spanish%2C%20and%20Turkish%29.%20Our%20results%20reveal%20geographic%20disparities%3A%20LLMs%0Aare%20substantially%20more%20successful%20at%20deducing%20entities%20from%20the%20Global%20North%0Athan%20the%20Global%20South%2C%20and%20the%20Global%20West%20than%20the%20Global%20East.%20While%0AWikipedia%20pageviews%20and%20pre-training%20corpus%20frequency%20correlate%20mildly%20with%0Aperformance%2C%20they%20fail%20to%20fully%20explain%20these%20disparities.%20Notably%2C%20the%0Alanguage%20in%20which%20the%20game%20is%20played%20has%20minimal%20impact%20on%20performance%20gaps.%0AThese%20findings%20demonstrate%20the%20value%20of%20creative%2C%20free-form%20evaluation%0Aframeworks%20for%20uncovering%20subtle%20biases%20in%20LLMs%20that%20remain%20hidden%20in%20standard%0Aprompting%20setups.%20By%20analyzing%20how%20models%20initiate%20and%20pursue%20reasoning%20goals%0Aover%20multiple%20turns%2C%20we%20find%20geographic%20and%20cultural%20disparities%20embedded%20in%0Atheir%20reasoning%20processes.%20We%20release%20the%20dataset%20%28Geo20Q%2B%29%20and%20code%20at%0Ahttps%3A//sites.google.com/view/llmbias20q/home.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520World%2520According%2520to%2520LLMs%253A%2520How%2520Geographic%2520Origin%2520Influences%2520LLMs%2527%250A%2520%2520Entity%2520Deduction%2520Capabilities%26entry.906535625%3DHarsh%2520Nishant%2520Lalai%2520and%2520Raj%2520Sanjay%2520Shah%2520and%2520Jiaxin%2520Pei%2520and%2520Sashank%2520Varma%2520and%2520Yi-Chia%2520Wang%2520and%2520Ali%2520Emami%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520extensively%2520tuned%2520to%2520mitigate%2520explicit%250Abiases%252C%2520yet%2520they%2520often%2520exhibit%2520subtle%2520implicit%2520biases%2520rooted%2520in%2520their%250Apre-training%2520data.%2520Rather%2520than%2520directly%2520probing%2520LLMs%2520with%2520human-crafted%250Aquestions%2520that%2520may%2520trigger%2520guardrails%252C%2520we%2520propose%2520studying%2520how%2520models%2520behave%250Awhen%2520they%2520proactively%2520ask%2520questions%2520themselves.%2520The%252020%2520Questions%2520game%252C%2520a%250Amulti-turn%2520deduction%2520task%252C%2520serves%2520as%2520an%2520ideal%2520testbed%2520for%2520this%2520purpose.%2520We%250Asystematically%2520evaluate%2520geographic%2520performance%2520disparities%2520in%2520entity%2520deduction%250Ausing%2520a%2520new%2520dataset%252C%2520Geo20Q%252B%252C%2520consisting%2520of%2520both%2520notable%2520people%2520and%2520culturally%250Asignificant%2520objects%2520%2528e.g.%252C%2520foods%252C%2520landmarks%252C%2520animals%2529%2520from%2520diverse%2520regions.%2520We%250Atest%2520popular%2520LLMs%2520across%2520two%2520gameplay%2520configurations%2520%2528canonical%252020-question%2520and%250Aunlimited%2520turns%2529%2520and%2520in%2520seven%2520languages%2520%2528English%252C%2520Hindi%252C%2520Mandarin%252C%2520Japanese%252C%250AFrench%252C%2520Spanish%252C%2520and%2520Turkish%2529.%2520Our%2520results%2520reveal%2520geographic%2520disparities%253A%2520LLMs%250Aare%2520substantially%2520more%2520successful%2520at%2520deducing%2520entities%2520from%2520the%2520Global%2520North%250Athan%2520the%2520Global%2520South%252C%2520and%2520the%2520Global%2520West%2520than%2520the%2520Global%2520East.%2520While%250AWikipedia%2520pageviews%2520and%2520pre-training%2520corpus%2520frequency%2520correlate%2520mildly%2520with%250Aperformance%252C%2520they%2520fail%2520to%2520fully%2520explain%2520these%2520disparities.%2520Notably%252C%2520the%250Alanguage%2520in%2520which%2520the%2520game%2520is%2520played%2520has%2520minimal%2520impact%2520on%2520performance%2520gaps.%250AThese%2520findings%2520demonstrate%2520the%2520value%2520of%2520creative%252C%2520free-form%2520evaluation%250Aframeworks%2520for%2520uncovering%2520subtle%2520biases%2520in%2520LLMs%2520that%2520remain%2520hidden%2520in%2520standard%250Aprompting%2520setups.%2520By%2520analyzing%2520how%2520models%2520initiate%2520and%2520pursue%2520reasoning%2520goals%250Aover%2520multiple%2520turns%252C%2520we%2520find%2520geographic%2520and%2520cultural%2520disparities%2520embedded%2520in%250Atheir%2520reasoning%2520processes.%2520We%2520release%2520the%2520dataset%2520%2528Geo20Q%252B%2529%2520and%2520code%2520at%250Ahttps%253A//sites.google.com/view/llmbias20q/home.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20World%20According%20to%20LLMs%3A%20How%20Geographic%20Origin%20Influences%20LLMs%27%0A%20%20Entity%20Deduction%20Capabilities&entry.906535625=Harsh%20Nishant%20Lalai%20and%20Raj%20Sanjay%20Shah%20and%20Jiaxin%20Pei%20and%20Sashank%20Varma%20and%20Yi-Chia%20Wang%20and%20Ali%20Emami&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20extensively%20tuned%20to%20mitigate%20explicit%0Abiases%2C%20yet%20they%20often%20exhibit%20subtle%20implicit%20biases%20rooted%20in%20their%0Apre-training%20data.%20Rather%20than%20directly%20probing%20LLMs%20with%20human-crafted%0Aquestions%20that%20may%20trigger%20guardrails%2C%20we%20propose%20studying%20how%20models%20behave%0Awhen%20they%20proactively%20ask%20questions%20themselves.%20The%2020%20Questions%20game%2C%20a%0Amulti-turn%20deduction%20task%2C%20serves%20as%20an%20ideal%20testbed%20for%20this%20purpose.%20We%0Asystematically%20evaluate%20geographic%20performance%20disparities%20in%20entity%20deduction%0Ausing%20a%20new%20dataset%2C%20Geo20Q%2B%2C%20consisting%20of%20both%20notable%20people%20and%20culturally%0Asignificant%20objects%20%28e.g.%2C%20foods%2C%20landmarks%2C%20animals%29%20from%20diverse%20regions.%20We%0Atest%20popular%20LLMs%20across%20two%20gameplay%20configurations%20%28canonical%2020-question%20and%0Aunlimited%20turns%29%20and%20in%20seven%20languages%20%28English%2C%20Hindi%2C%20Mandarin%2C%20Japanese%2C%0AFrench%2C%20Spanish%2C%20and%20Turkish%29.%20Our%20results%20reveal%20geographic%20disparities%3A%20LLMs%0Aare%20substantially%20more%20successful%20at%20deducing%20entities%20from%20the%20Global%20North%0Athan%20the%20Global%20South%2C%20and%20the%20Global%20West%20than%20the%20Global%20East.%20While%0AWikipedia%20pageviews%20and%20pre-training%20corpus%20frequency%20correlate%20mildly%20with%0Aperformance%2C%20they%20fail%20to%20fully%20explain%20these%20disparities.%20Notably%2C%20the%0Alanguage%20in%20which%20the%20game%20is%20played%20has%20minimal%20impact%20on%20performance%20gaps.%0AThese%20findings%20demonstrate%20the%20value%20of%20creative%2C%20free-form%20evaluation%0Aframeworks%20for%20uncovering%20subtle%20biases%20in%20LLMs%20that%20remain%20hidden%20in%20standard%0Aprompting%20setups.%20By%20analyzing%20how%20models%20initiate%20and%20pursue%20reasoning%20goals%0Aover%20multiple%20turns%2C%20we%20find%20geographic%20and%20cultural%20disparities%20embedded%20in%0Atheir%20reasoning%20processes.%20We%20release%20the%20dataset%20%28Geo20Q%2B%29%20and%20code%20at%0Ahttps%3A//sites.google.com/view/llmbias20q/home.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05525v1&entry.124074799=Read"},
{"title": "Robust Tracking with Particle Filtering for Fluorescent Cardiac Imaging", "author": "Suresh Guttikonda and Maximilian Neidhart and Johanna Sprenger and Johannes Petersen and Christian Detter and Alexander Schlaefer", "abstract": "  Intraoperative fluorescent cardiac imaging enables quality control following\ncoronary bypass grafting surgery. We can estimate local quantitative\nindicators, such as cardiac perfusion, by tracking local feature points.\nHowever, heart motion and significant fluctuations in image characteristics\ncaused by vessel structural enrichment limit traditional tracking methods. We\npropose a particle filtering tracker based on cyclicconsistency checks to\nrobustly track particles sampled to follow target landmarks. Our method tracks\n117 targets simultaneously at 25.4 fps, allowing real-time estimates during\ninterventions. It achieves a tracking error of (5.00 +/- 0.22 px) and\noutperforms other deep learning trackers (22.3 +/- 1.1 px) and conventional\ntrackers (58.1 +/- 27.1 px).\n", "link": "http://arxiv.org/abs/2508.05262v1", "date": "2025-08-07", "relevancy": 2.0753, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5419}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.508}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Tracking%20with%20Particle%20Filtering%20for%20Fluorescent%20Cardiac%20Imaging&body=Title%3A%20Robust%20Tracking%20with%20Particle%20Filtering%20for%20Fluorescent%20Cardiac%20Imaging%0AAuthor%3A%20Suresh%20Guttikonda%20and%20Maximilian%20Neidhart%20and%20Johanna%20Sprenger%20and%20Johannes%20Petersen%20and%20Christian%20Detter%20and%20Alexander%20Schlaefer%0AAbstract%3A%20%20%20Intraoperative%20fluorescent%20cardiac%20imaging%20enables%20quality%20control%20following%0Acoronary%20bypass%20grafting%20surgery.%20We%20can%20estimate%20local%20quantitative%0Aindicators%2C%20such%20as%20cardiac%20perfusion%2C%20by%20tracking%20local%20feature%20points.%0AHowever%2C%20heart%20motion%20and%20significant%20fluctuations%20in%20image%20characteristics%0Acaused%20by%20vessel%20structural%20enrichment%20limit%20traditional%20tracking%20methods.%20We%0Apropose%20a%20particle%20filtering%20tracker%20based%20on%20cyclicconsistency%20checks%20to%0Arobustly%20track%20particles%20sampled%20to%20follow%20target%20landmarks.%20Our%20method%20tracks%0A117%20targets%20simultaneously%20at%2025.4%20fps%2C%20allowing%20real-time%20estimates%20during%0Ainterventions.%20It%20achieves%20a%20tracking%20error%20of%20%285.00%20%2B/-%200.22%20px%29%20and%0Aoutperforms%20other%20deep%20learning%20trackers%20%2822.3%20%2B/-%201.1%20px%29%20and%20conventional%0Atrackers%20%2858.1%20%2B/-%2027.1%20px%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Tracking%2520with%2520Particle%2520Filtering%2520for%2520Fluorescent%2520Cardiac%2520Imaging%26entry.906535625%3DSuresh%2520Guttikonda%2520and%2520Maximilian%2520Neidhart%2520and%2520Johanna%2520Sprenger%2520and%2520Johannes%2520Petersen%2520and%2520Christian%2520Detter%2520and%2520Alexander%2520Schlaefer%26entry.1292438233%3D%2520%2520Intraoperative%2520fluorescent%2520cardiac%2520imaging%2520enables%2520quality%2520control%2520following%250Acoronary%2520bypass%2520grafting%2520surgery.%2520We%2520can%2520estimate%2520local%2520quantitative%250Aindicators%252C%2520such%2520as%2520cardiac%2520perfusion%252C%2520by%2520tracking%2520local%2520feature%2520points.%250AHowever%252C%2520heart%2520motion%2520and%2520significant%2520fluctuations%2520in%2520image%2520characteristics%250Acaused%2520by%2520vessel%2520structural%2520enrichment%2520limit%2520traditional%2520tracking%2520methods.%2520We%250Apropose%2520a%2520particle%2520filtering%2520tracker%2520based%2520on%2520cyclicconsistency%2520checks%2520to%250Arobustly%2520track%2520particles%2520sampled%2520to%2520follow%2520target%2520landmarks.%2520Our%2520method%2520tracks%250A117%2520targets%2520simultaneously%2520at%252025.4%2520fps%252C%2520allowing%2520real-time%2520estimates%2520during%250Ainterventions.%2520It%2520achieves%2520a%2520tracking%2520error%2520of%2520%25285.00%2520%252B/-%25200.22%2520px%2529%2520and%250Aoutperforms%2520other%2520deep%2520learning%2520trackers%2520%252822.3%2520%252B/-%25201.1%2520px%2529%2520and%2520conventional%250Atrackers%2520%252858.1%2520%252B/-%252027.1%2520px%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Tracking%20with%20Particle%20Filtering%20for%20Fluorescent%20Cardiac%20Imaging&entry.906535625=Suresh%20Guttikonda%20and%20Maximilian%20Neidhart%20and%20Johanna%20Sprenger%20and%20Johannes%20Petersen%20and%20Christian%20Detter%20and%20Alexander%20Schlaefer&entry.1292438233=%20%20Intraoperative%20fluorescent%20cardiac%20imaging%20enables%20quality%20control%20following%0Acoronary%20bypass%20grafting%20surgery.%20We%20can%20estimate%20local%20quantitative%0Aindicators%2C%20such%20as%20cardiac%20perfusion%2C%20by%20tracking%20local%20feature%20points.%0AHowever%2C%20heart%20motion%20and%20significant%20fluctuations%20in%20image%20characteristics%0Acaused%20by%20vessel%20structural%20enrichment%20limit%20traditional%20tracking%20methods.%20We%0Apropose%20a%20particle%20filtering%20tracker%20based%20on%20cyclicconsistency%20checks%20to%0Arobustly%20track%20particles%20sampled%20to%20follow%20target%20landmarks.%20Our%20method%20tracks%0A117%20targets%20simultaneously%20at%2025.4%20fps%2C%20allowing%20real-time%20estimates%20during%0Ainterventions.%20It%20achieves%20a%20tracking%20error%20of%20%285.00%20%2B/-%200.22%20px%29%20and%0Aoutperforms%20other%20deep%20learning%20trackers%20%2822.3%20%2B/-%201.1%20px%29%20and%20conventional%0Atrackers%20%2858.1%20%2B/-%2027.1%20px%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05262v1&entry.124074799=Read"},
{"title": "ADSEL: Adaptive dual self-expression learning for EEG feature selection\n  via incomplete multi-dimensional emotional tagging", "author": "Tianze Yu and Junming Zhang and Wenjia Dong and Xueyuan Xu and Li Zhuo", "abstract": "  EEG based multi-dimension emotion recognition has attracted substantial\nresearch interest in human computer interfaces. However, the high\ndimensionality of EEG features, coupled with limited sample sizes, frequently\nleads to classifier overfitting and high computational complexity. Feature\nselection constitutes a critical strategy for mitigating these challenges. Most\nexisting EEG feature selection methods assume complete multi-dimensional\nemotion labels. In practice, open acquisition environment, and the inherent\nsubjectivity of emotion perception often result in incomplete label data, which\ncan compromise model generalization. Additionally, existing feature selection\nmethods for handling incomplete multi-dimensional labels primarily focus on\ncorrelations among various dimensions during label recovery, neglecting the\ncorrelation between samples in the label space and their interaction with\nvarious dimensions. To address these issues, we propose a novel incomplete\nmulti-dimensional feature selection algorithm for EEG-based emotion\nrecognition. The proposed method integrates an adaptive dual self-expression\nlearning (ADSEL) with least squares regression. ADSEL establishes a\nbidirectional pathway between sample-level and dimension-level self-expression\nlearning processes within the label space. It could facilitate the\ncross-sharing of learned information between these processes, enabling the\nsimultaneous exploitation of effective information across both samples and\ndimensions for label reconstruction. Consequently, ADSEL could enhances label\nrecovery accuracy and effectively identifies the optimal EEG feature subset for\nmulti-dimensional emotion recognition.\n", "link": "http://arxiv.org/abs/2508.05229v1", "date": "2025-08-07", "relevancy": 2.0709, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5535}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADSEL%3A%20Adaptive%20dual%20self-expression%20learning%20for%20EEG%20feature%20selection%0A%20%20via%20incomplete%20multi-dimensional%20emotional%20tagging&body=Title%3A%20ADSEL%3A%20Adaptive%20dual%20self-expression%20learning%20for%20EEG%20feature%20selection%0A%20%20via%20incomplete%20multi-dimensional%20emotional%20tagging%0AAuthor%3A%20Tianze%20Yu%20and%20Junming%20Zhang%20and%20Wenjia%20Dong%20and%20Xueyuan%20Xu%20and%20Li%20Zhuo%0AAbstract%3A%20%20%20EEG%20based%20multi-dimension%20emotion%20recognition%20has%20attracted%20substantial%0Aresearch%20interest%20in%20human%20computer%20interfaces.%20However%2C%20the%20high%0Adimensionality%20of%20EEG%20features%2C%20coupled%20with%20limited%20sample%20sizes%2C%20frequently%0Aleads%20to%20classifier%20overfitting%20and%20high%20computational%20complexity.%20Feature%0Aselection%20constitutes%20a%20critical%20strategy%20for%20mitigating%20these%20challenges.%20Most%0Aexisting%20EEG%20feature%20selection%20methods%20assume%20complete%20multi-dimensional%0Aemotion%20labels.%20In%20practice%2C%20open%20acquisition%20environment%2C%20and%20the%20inherent%0Asubjectivity%20of%20emotion%20perception%20often%20result%20in%20incomplete%20label%20data%2C%20which%0Acan%20compromise%20model%20generalization.%20Additionally%2C%20existing%20feature%20selection%0Amethods%20for%20handling%20incomplete%20multi-dimensional%20labels%20primarily%20focus%20on%0Acorrelations%20among%20various%20dimensions%20during%20label%20recovery%2C%20neglecting%20the%0Acorrelation%20between%20samples%20in%20the%20label%20space%20and%20their%20interaction%20with%0Avarious%20dimensions.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20incomplete%0Amulti-dimensional%20feature%20selection%20algorithm%20for%20EEG-based%20emotion%0Arecognition.%20The%20proposed%20method%20integrates%20an%20adaptive%20dual%20self-expression%0Alearning%20%28ADSEL%29%20with%20least%20squares%20regression.%20ADSEL%20establishes%20a%0Abidirectional%20pathway%20between%20sample-level%20and%20dimension-level%20self-expression%0Alearning%20processes%20within%20the%20label%20space.%20It%20could%20facilitate%20the%0Across-sharing%20of%20learned%20information%20between%20these%20processes%2C%20enabling%20the%0Asimultaneous%20exploitation%20of%20effective%20information%20across%20both%20samples%20and%0Adimensions%20for%20label%20reconstruction.%20Consequently%2C%20ADSEL%20could%20enhances%20label%0Arecovery%20accuracy%20and%20effectively%20identifies%20the%20optimal%20EEG%20feature%20subset%20for%0Amulti-dimensional%20emotion%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADSEL%253A%2520Adaptive%2520dual%2520self-expression%2520learning%2520for%2520EEG%2520feature%2520selection%250A%2520%2520via%2520incomplete%2520multi-dimensional%2520emotional%2520tagging%26entry.906535625%3DTianze%2520Yu%2520and%2520Junming%2520Zhang%2520and%2520Wenjia%2520Dong%2520and%2520Xueyuan%2520Xu%2520and%2520Li%2520Zhuo%26entry.1292438233%3D%2520%2520EEG%2520based%2520multi-dimension%2520emotion%2520recognition%2520has%2520attracted%2520substantial%250Aresearch%2520interest%2520in%2520human%2520computer%2520interfaces.%2520However%252C%2520the%2520high%250Adimensionality%2520of%2520EEG%2520features%252C%2520coupled%2520with%2520limited%2520sample%2520sizes%252C%2520frequently%250Aleads%2520to%2520classifier%2520overfitting%2520and%2520high%2520computational%2520complexity.%2520Feature%250Aselection%2520constitutes%2520a%2520critical%2520strategy%2520for%2520mitigating%2520these%2520challenges.%2520Most%250Aexisting%2520EEG%2520feature%2520selection%2520methods%2520assume%2520complete%2520multi-dimensional%250Aemotion%2520labels.%2520In%2520practice%252C%2520open%2520acquisition%2520environment%252C%2520and%2520the%2520inherent%250Asubjectivity%2520of%2520emotion%2520perception%2520often%2520result%2520in%2520incomplete%2520label%2520data%252C%2520which%250Acan%2520compromise%2520model%2520generalization.%2520Additionally%252C%2520existing%2520feature%2520selection%250Amethods%2520for%2520handling%2520incomplete%2520multi-dimensional%2520labels%2520primarily%2520focus%2520on%250Acorrelations%2520among%2520various%2520dimensions%2520during%2520label%2520recovery%252C%2520neglecting%2520the%250Acorrelation%2520between%2520samples%2520in%2520the%2520label%2520space%2520and%2520their%2520interaction%2520with%250Avarious%2520dimensions.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520incomplete%250Amulti-dimensional%2520feature%2520selection%2520algorithm%2520for%2520EEG-based%2520emotion%250Arecognition.%2520The%2520proposed%2520method%2520integrates%2520an%2520adaptive%2520dual%2520self-expression%250Alearning%2520%2528ADSEL%2529%2520with%2520least%2520squares%2520regression.%2520ADSEL%2520establishes%2520a%250Abidirectional%2520pathway%2520between%2520sample-level%2520and%2520dimension-level%2520self-expression%250Alearning%2520processes%2520within%2520the%2520label%2520space.%2520It%2520could%2520facilitate%2520the%250Across-sharing%2520of%2520learned%2520information%2520between%2520these%2520processes%252C%2520enabling%2520the%250Asimultaneous%2520exploitation%2520of%2520effective%2520information%2520across%2520both%2520samples%2520and%250Adimensions%2520for%2520label%2520reconstruction.%2520Consequently%252C%2520ADSEL%2520could%2520enhances%2520label%250Arecovery%2520accuracy%2520and%2520effectively%2520identifies%2520the%2520optimal%2520EEG%2520feature%2520subset%2520for%250Amulti-dimensional%2520emotion%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADSEL%3A%20Adaptive%20dual%20self-expression%20learning%20for%20EEG%20feature%20selection%0A%20%20via%20incomplete%20multi-dimensional%20emotional%20tagging&entry.906535625=Tianze%20Yu%20and%20Junming%20Zhang%20and%20Wenjia%20Dong%20and%20Xueyuan%20Xu%20and%20Li%20Zhuo&entry.1292438233=%20%20EEG%20based%20multi-dimension%20emotion%20recognition%20has%20attracted%20substantial%0Aresearch%20interest%20in%20human%20computer%20interfaces.%20However%2C%20the%20high%0Adimensionality%20of%20EEG%20features%2C%20coupled%20with%20limited%20sample%20sizes%2C%20frequently%0Aleads%20to%20classifier%20overfitting%20and%20high%20computational%20complexity.%20Feature%0Aselection%20constitutes%20a%20critical%20strategy%20for%20mitigating%20these%20challenges.%20Most%0Aexisting%20EEG%20feature%20selection%20methods%20assume%20complete%20multi-dimensional%0Aemotion%20labels.%20In%20practice%2C%20open%20acquisition%20environment%2C%20and%20the%20inherent%0Asubjectivity%20of%20emotion%20perception%20often%20result%20in%20incomplete%20label%20data%2C%20which%0Acan%20compromise%20model%20generalization.%20Additionally%2C%20existing%20feature%20selection%0Amethods%20for%20handling%20incomplete%20multi-dimensional%20labels%20primarily%20focus%20on%0Acorrelations%20among%20various%20dimensions%20during%20label%20recovery%2C%20neglecting%20the%0Acorrelation%20between%20samples%20in%20the%20label%20space%20and%20their%20interaction%20with%0Avarious%20dimensions.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20incomplete%0Amulti-dimensional%20feature%20selection%20algorithm%20for%20EEG-based%20emotion%0Arecognition.%20The%20proposed%20method%20integrates%20an%20adaptive%20dual%20self-expression%0Alearning%20%28ADSEL%29%20with%20least%20squares%20regression.%20ADSEL%20establishes%20a%0Abidirectional%20pathway%20between%20sample-level%20and%20dimension-level%20self-expression%0Alearning%20processes%20within%20the%20label%20space.%20It%20could%20facilitate%20the%0Across-sharing%20of%20learned%20information%20between%20these%20processes%2C%20enabling%20the%0Asimultaneous%20exploitation%20of%20effective%20information%20across%20both%20samples%20and%0Adimensions%20for%20label%20reconstruction.%20Consequently%2C%20ADSEL%20could%20enhances%20label%0Arecovery%20accuracy%20and%20effectively%20identifies%20the%20optimal%20EEG%20feature%20subset%20for%0Amulti-dimensional%20emotion%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05229v1&entry.124074799=Read"},
{"title": "CLOT: Closed Loop Optimal Transport for Unsupervised Action Segmentation", "author": "Elena Bueno-Benito and Mariella Dimiccoli", "abstract": "  Unsupervised action segmentation has recently pushed its limits with ASOT, an\noptimal transport (OT)-based method that simultaneously learns action\nrepresentations and performs clustering using pseudo-labels. Unlike other\nOT-based approaches, ASOT makes no assumptions about action ordering and can\ndecode a temporally consistent segmentation from a noisy cost matrix between\nvideo frames and action labels. However, the resulting segmentation lacks\nsegment-level supervision, limiting the effectiveness of feedback between\nframes and action representations. To address this limitation, we propose\nClosed Loop Optimal Transport (CLOT), a novel OT-based framework with a\nmulti-level cyclic feature learning mechanism. Leveraging its encoder-decoder\narchitecture, CLOT learns pseudo-labels alongside frame and segment embeddings\nby solving two separate OT problems. It then refines both frame embeddings and\npseudo-labels through cross-attention between the learned frame and segment\nembeddings, by integrating a third OT problem. Experimental results on four\nbenchmark datasets demonstrate the benefits of cyclical learning for\nunsupervised action segmentation.\n", "link": "http://arxiv.org/abs/2507.03539v2", "date": "2025-08-07", "relevancy": 2.0623, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5276}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5081}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLOT%3A%20Closed%20Loop%20Optimal%20Transport%20for%20Unsupervised%20Action%20Segmentation&body=Title%3A%20CLOT%3A%20Closed%20Loop%20Optimal%20Transport%20for%20Unsupervised%20Action%20Segmentation%0AAuthor%3A%20Elena%20Bueno-Benito%20and%20Mariella%20Dimiccoli%0AAbstract%3A%20%20%20Unsupervised%20action%20segmentation%20has%20recently%20pushed%20its%20limits%20with%20ASOT%2C%20an%0Aoptimal%20transport%20%28OT%29-based%20method%20that%20simultaneously%20learns%20action%0Arepresentations%20and%20performs%20clustering%20using%20pseudo-labels.%20Unlike%20other%0AOT-based%20approaches%2C%20ASOT%20makes%20no%20assumptions%20about%20action%20ordering%20and%20can%0Adecode%20a%20temporally%20consistent%20segmentation%20from%20a%20noisy%20cost%20matrix%20between%0Avideo%20frames%20and%20action%20labels.%20However%2C%20the%20resulting%20segmentation%20lacks%0Asegment-level%20supervision%2C%20limiting%20the%20effectiveness%20of%20feedback%20between%0Aframes%20and%20action%20representations.%20To%20address%20this%20limitation%2C%20we%20propose%0AClosed%20Loop%20Optimal%20Transport%20%28CLOT%29%2C%20a%20novel%20OT-based%20framework%20with%20a%0Amulti-level%20cyclic%20feature%20learning%20mechanism.%20Leveraging%20its%20encoder-decoder%0Aarchitecture%2C%20CLOT%20learns%20pseudo-labels%20alongside%20frame%20and%20segment%20embeddings%0Aby%20solving%20two%20separate%20OT%20problems.%20It%20then%20refines%20both%20frame%20embeddings%20and%0Apseudo-labels%20through%20cross-attention%20between%20the%20learned%20frame%20and%20segment%0Aembeddings%2C%20by%20integrating%20a%20third%20OT%20problem.%20Experimental%20results%20on%20four%0Abenchmark%20datasets%20demonstrate%20the%20benefits%20of%20cyclical%20learning%20for%0Aunsupervised%20action%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLOT%253A%2520Closed%2520Loop%2520Optimal%2520Transport%2520for%2520Unsupervised%2520Action%2520Segmentation%26entry.906535625%3DElena%2520Bueno-Benito%2520and%2520Mariella%2520Dimiccoli%26entry.1292438233%3D%2520%2520Unsupervised%2520action%2520segmentation%2520has%2520recently%2520pushed%2520its%2520limits%2520with%2520ASOT%252C%2520an%250Aoptimal%2520transport%2520%2528OT%2529-based%2520method%2520that%2520simultaneously%2520learns%2520action%250Arepresentations%2520and%2520performs%2520clustering%2520using%2520pseudo-labels.%2520Unlike%2520other%250AOT-based%2520approaches%252C%2520ASOT%2520makes%2520no%2520assumptions%2520about%2520action%2520ordering%2520and%2520can%250Adecode%2520a%2520temporally%2520consistent%2520segmentation%2520from%2520a%2520noisy%2520cost%2520matrix%2520between%250Avideo%2520frames%2520and%2520action%2520labels.%2520However%252C%2520the%2520resulting%2520segmentation%2520lacks%250Asegment-level%2520supervision%252C%2520limiting%2520the%2520effectiveness%2520of%2520feedback%2520between%250Aframes%2520and%2520action%2520representations.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250AClosed%2520Loop%2520Optimal%2520Transport%2520%2528CLOT%2529%252C%2520a%2520novel%2520OT-based%2520framework%2520with%2520a%250Amulti-level%2520cyclic%2520feature%2520learning%2520mechanism.%2520Leveraging%2520its%2520encoder-decoder%250Aarchitecture%252C%2520CLOT%2520learns%2520pseudo-labels%2520alongside%2520frame%2520and%2520segment%2520embeddings%250Aby%2520solving%2520two%2520separate%2520OT%2520problems.%2520It%2520then%2520refines%2520both%2520frame%2520embeddings%2520and%250Apseudo-labels%2520through%2520cross-attention%2520between%2520the%2520learned%2520frame%2520and%2520segment%250Aembeddings%252C%2520by%2520integrating%2520a%2520third%2520OT%2520problem.%2520Experimental%2520results%2520on%2520four%250Abenchmark%2520datasets%2520demonstrate%2520the%2520benefits%2520of%2520cyclical%2520learning%2520for%250Aunsupervised%2520action%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLOT%3A%20Closed%20Loop%20Optimal%20Transport%20for%20Unsupervised%20Action%20Segmentation&entry.906535625=Elena%20Bueno-Benito%20and%20Mariella%20Dimiccoli&entry.1292438233=%20%20Unsupervised%20action%20segmentation%20has%20recently%20pushed%20its%20limits%20with%20ASOT%2C%20an%0Aoptimal%20transport%20%28OT%29-based%20method%20that%20simultaneously%20learns%20action%0Arepresentations%20and%20performs%20clustering%20using%20pseudo-labels.%20Unlike%20other%0AOT-based%20approaches%2C%20ASOT%20makes%20no%20assumptions%20about%20action%20ordering%20and%20can%0Adecode%20a%20temporally%20consistent%20segmentation%20from%20a%20noisy%20cost%20matrix%20between%0Avideo%20frames%20and%20action%20labels.%20However%2C%20the%20resulting%20segmentation%20lacks%0Asegment-level%20supervision%2C%20limiting%20the%20effectiveness%20of%20feedback%20between%0Aframes%20and%20action%20representations.%20To%20address%20this%20limitation%2C%20we%20propose%0AClosed%20Loop%20Optimal%20Transport%20%28CLOT%29%2C%20a%20novel%20OT-based%20framework%20with%20a%0Amulti-level%20cyclic%20feature%20learning%20mechanism.%20Leveraging%20its%20encoder-decoder%0Aarchitecture%2C%20CLOT%20learns%20pseudo-labels%20alongside%20frame%20and%20segment%20embeddings%0Aby%20solving%20two%20separate%20OT%20problems.%20It%20then%20refines%20both%20frame%20embeddings%20and%0Apseudo-labels%20through%20cross-attention%20between%20the%20learned%20frame%20and%20segment%0Aembeddings%2C%20by%20integrating%20a%20third%20OT%20problem.%20Experimental%20results%20on%20four%0Abenchmark%20datasets%20demonstrate%20the%20benefits%20of%20cyclical%20learning%20for%0Aunsupervised%20action%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03539v2&entry.124074799=Read"},
{"title": "Eliciting Latent Predictions from Transformers with the Tuned Lens", "author": "Nora Belrose and Igor Ostrovsky and Lev McKinney and Zach Furman and Logan Smith and Danny Halawi and Stella Biderman and Jacob Steinhardt", "abstract": "  We analyze transformers from the perspective of iterative inference, seeking\nto understand how model predictions are refined layer by layer. To do so, we\ntrain an affine probe for each block in a frozen pretrained model, making it\npossible to decode every hidden state into a distribution over the vocabulary.\nOur method, the tuned lens, is a refinement of the earlier \"logit lens\"\ntechnique, which yielded useful insights but is often brittle.\n  We test our method on various autoregressive language models with up to 20B\nparameters, showing it to be more predictive, reliable and unbiased than the\nlogit lens. With causal experiments, we show the tuned lens uses similar\nfeatures to the model itself. We also find the trajectory of latent predictions\ncan be used to detect malicious inputs with high accuracy. All code needed to\nreproduce our results can be found at\nhttps://github.com/AlignmentResearch/tuned-lens.\n", "link": "http://arxiv.org/abs/2303.08112v5", "date": "2025-08-07", "relevancy": 2.0623, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eliciting%20Latent%20Predictions%20from%20Transformers%20with%20the%20Tuned%20Lens&body=Title%3A%20Eliciting%20Latent%20Predictions%20from%20Transformers%20with%20the%20Tuned%20Lens%0AAuthor%3A%20Nora%20Belrose%20and%20Igor%20Ostrovsky%20and%20Lev%20McKinney%20and%20Zach%20Furman%20and%20Logan%20Smith%20and%20Danny%20Halawi%20and%20Stella%20Biderman%20and%20Jacob%20Steinhardt%0AAbstract%3A%20%20%20We%20analyze%20transformers%20from%20the%20perspective%20of%20iterative%20inference%2C%20seeking%0Ato%20understand%20how%20model%20predictions%20are%20refined%20layer%20by%20layer.%20To%20do%20so%2C%20we%0Atrain%20an%20affine%20probe%20for%20each%20block%20in%20a%20frozen%20pretrained%20model%2C%20making%20it%0Apossible%20to%20decode%20every%20hidden%20state%20into%20a%20distribution%20over%20the%20vocabulary.%0AOur%20method%2C%20the%20tuned%20lens%2C%20is%20a%20refinement%20of%20the%20earlier%20%22logit%20lens%22%0Atechnique%2C%20which%20yielded%20useful%20insights%20but%20is%20often%20brittle.%0A%20%20We%20test%20our%20method%20on%20various%20autoregressive%20language%20models%20with%20up%20to%2020B%0Aparameters%2C%20showing%20it%20to%20be%20more%20predictive%2C%20reliable%20and%20unbiased%20than%20the%0Alogit%20lens.%20With%20causal%20experiments%2C%20we%20show%20the%20tuned%20lens%20uses%20similar%0Afeatures%20to%20the%20model%20itself.%20We%20also%20find%20the%20trajectory%20of%20latent%20predictions%0Acan%20be%20used%20to%20detect%20malicious%20inputs%20with%20high%20accuracy.%20All%20code%20needed%20to%0Areproduce%20our%20results%20can%20be%20found%20at%0Ahttps%3A//github.com/AlignmentResearch/tuned-lens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.08112v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEliciting%2520Latent%2520Predictions%2520from%2520Transformers%2520with%2520the%2520Tuned%2520Lens%26entry.906535625%3DNora%2520Belrose%2520and%2520Igor%2520Ostrovsky%2520and%2520Lev%2520McKinney%2520and%2520Zach%2520Furman%2520and%2520Logan%2520Smith%2520and%2520Danny%2520Halawi%2520and%2520Stella%2520Biderman%2520and%2520Jacob%2520Steinhardt%26entry.1292438233%3D%2520%2520We%2520analyze%2520transformers%2520from%2520the%2520perspective%2520of%2520iterative%2520inference%252C%2520seeking%250Ato%2520understand%2520how%2520model%2520predictions%2520are%2520refined%2520layer%2520by%2520layer.%2520To%2520do%2520so%252C%2520we%250Atrain%2520an%2520affine%2520probe%2520for%2520each%2520block%2520in%2520a%2520frozen%2520pretrained%2520model%252C%2520making%2520it%250Apossible%2520to%2520decode%2520every%2520hidden%2520state%2520into%2520a%2520distribution%2520over%2520the%2520vocabulary.%250AOur%2520method%252C%2520the%2520tuned%2520lens%252C%2520is%2520a%2520refinement%2520of%2520the%2520earlier%2520%2522logit%2520lens%2522%250Atechnique%252C%2520which%2520yielded%2520useful%2520insights%2520but%2520is%2520often%2520brittle.%250A%2520%2520We%2520test%2520our%2520method%2520on%2520various%2520autoregressive%2520language%2520models%2520with%2520up%2520to%252020B%250Aparameters%252C%2520showing%2520it%2520to%2520be%2520more%2520predictive%252C%2520reliable%2520and%2520unbiased%2520than%2520the%250Alogit%2520lens.%2520With%2520causal%2520experiments%252C%2520we%2520show%2520the%2520tuned%2520lens%2520uses%2520similar%250Afeatures%2520to%2520the%2520model%2520itself.%2520We%2520also%2520find%2520the%2520trajectory%2520of%2520latent%2520predictions%250Acan%2520be%2520used%2520to%2520detect%2520malicious%2520inputs%2520with%2520high%2520accuracy.%2520All%2520code%2520needed%2520to%250Areproduce%2520our%2520results%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/AlignmentResearch/tuned-lens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.08112v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eliciting%20Latent%20Predictions%20from%20Transformers%20with%20the%20Tuned%20Lens&entry.906535625=Nora%20Belrose%20and%20Igor%20Ostrovsky%20and%20Lev%20McKinney%20and%20Zach%20Furman%20and%20Logan%20Smith%20and%20Danny%20Halawi%20and%20Stella%20Biderman%20and%20Jacob%20Steinhardt&entry.1292438233=%20%20We%20analyze%20transformers%20from%20the%20perspective%20of%20iterative%20inference%2C%20seeking%0Ato%20understand%20how%20model%20predictions%20are%20refined%20layer%20by%20layer.%20To%20do%20so%2C%20we%0Atrain%20an%20affine%20probe%20for%20each%20block%20in%20a%20frozen%20pretrained%20model%2C%20making%20it%0Apossible%20to%20decode%20every%20hidden%20state%20into%20a%20distribution%20over%20the%20vocabulary.%0AOur%20method%2C%20the%20tuned%20lens%2C%20is%20a%20refinement%20of%20the%20earlier%20%22logit%20lens%22%0Atechnique%2C%20which%20yielded%20useful%20insights%20but%20is%20often%20brittle.%0A%20%20We%20test%20our%20method%20on%20various%20autoregressive%20language%20models%20with%20up%20to%2020B%0Aparameters%2C%20showing%20it%20to%20be%20more%20predictive%2C%20reliable%20and%20unbiased%20than%20the%0Alogit%20lens.%20With%20causal%20experiments%2C%20we%20show%20the%20tuned%20lens%20uses%20similar%0Afeatures%20to%20the%20model%20itself.%20We%20also%20find%20the%20trajectory%20of%20latent%20predictions%0Acan%20be%20used%20to%20detect%20malicious%20inputs%20with%20high%20accuracy.%20All%20code%20needed%20to%0Areproduce%20our%20results%20can%20be%20found%20at%0Ahttps%3A//github.com/AlignmentResearch/tuned-lens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.08112v5&entry.124074799=Read"},
{"title": "AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient\n  Sequence Modeling", "author": "Preslav Aleksandrov and Meghdad Kurmanji and Fernando Garcia Redondo and David O'Shea and William Shen and Alex Iacob and Lorenzo Sani and Xinchi Qiu and Nicola Cancedda and Nicholas D. Lane", "abstract": "  We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a\nnovel recursive generalization of the encoder-only Transformer architecture,\nwhich achieves better perplexity than a standard Transformer and allows for the\ndynamic scaling of compute resources at test time. This simple, recursive\napproach is a complement to scaling large language model (LLM) performance\nthrough parameter and token counts. AbbIE performs its iterations in latent\nspace, but unlike latent reasoning models, does not require a specialized\ndataset or training protocol. We show that AbbIE upward generalizes (ability to\ngeneralize to arbitrary iteration lengths) at test time by only using 2\niterations during train time, far outperforming alternative iterative methods.\nAbbIE's ability to scale its computational expenditure based on the complexity\nof the task gives it an up to \\textbf{12\\%} improvement in zero-shot in-context\nlearning tasks versus other iterative and standard methods and up to 5\\%\nimprovement in language perplexity. The results from this study open a new\navenue to Transformer performance scaling. We perform all of our evaluations on\nmodel sizes up to 350M parameters.\n", "link": "http://arxiv.org/abs/2507.08567v2", "date": "2025-08-07", "relevancy": 2.0598, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5351}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5153}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AbbIE%3A%20Autoregressive%20Block-Based%20Iterative%20Encoder%20for%20Efficient%0A%20%20Sequence%20Modeling&body=Title%3A%20AbbIE%3A%20Autoregressive%20Block-Based%20Iterative%20Encoder%20for%20Efficient%0A%20%20Sequence%20Modeling%0AAuthor%3A%20Preslav%20Aleksandrov%20and%20Meghdad%20Kurmanji%20and%20Fernando%20Garcia%20Redondo%20and%20David%20O%27Shea%20and%20William%20Shen%20and%20Alex%20Iacob%20and%20Lorenzo%20Sani%20and%20Xinchi%20Qiu%20and%20Nicola%20Cancedda%20and%20Nicholas%20D.%20Lane%0AAbstract%3A%20%20%20We%20introduce%20the%20Autoregressive%20Block-Based%20Iterative%20Encoder%20%28AbbIE%29%2C%20a%0Anovel%20recursive%20generalization%20of%20the%20encoder-only%20Transformer%20architecture%2C%0Awhich%20achieves%20better%20perplexity%20than%20a%20standard%20Transformer%20and%20allows%20for%20the%0Adynamic%20scaling%20of%20compute%20resources%20at%20test%20time.%20This%20simple%2C%20recursive%0Aapproach%20is%20a%20complement%20to%20scaling%20large%20language%20model%20%28LLM%29%20performance%0Athrough%20parameter%20and%20token%20counts.%20AbbIE%20performs%20its%20iterations%20in%20latent%0Aspace%2C%20but%20unlike%20latent%20reasoning%20models%2C%20does%20not%20require%20a%20specialized%0Adataset%20or%20training%20protocol.%20We%20show%20that%20AbbIE%20upward%20generalizes%20%28ability%20to%0Ageneralize%20to%20arbitrary%20iteration%20lengths%29%20at%20test%20time%20by%20only%20using%202%0Aiterations%20during%20train%20time%2C%20far%20outperforming%20alternative%20iterative%20methods.%0AAbbIE%27s%20ability%20to%20scale%20its%20computational%20expenditure%20based%20on%20the%20complexity%0Aof%20the%20task%20gives%20it%20an%20up%20to%20%5Ctextbf%7B12%5C%25%7D%20improvement%20in%20zero-shot%20in-context%0Alearning%20tasks%20versus%20other%20iterative%20and%20standard%20methods%20and%20up%20to%205%5C%25%0Aimprovement%20in%20language%20perplexity.%20The%20results%20from%20this%20study%20open%20a%20new%0Aavenue%20to%20Transformer%20performance%20scaling.%20We%20perform%20all%20of%20our%20evaluations%20on%0Amodel%20sizes%20up%20to%20350M%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbbIE%253A%2520Autoregressive%2520Block-Based%2520Iterative%2520Encoder%2520for%2520Efficient%250A%2520%2520Sequence%2520Modeling%26entry.906535625%3DPreslav%2520Aleksandrov%2520and%2520Meghdad%2520Kurmanji%2520and%2520Fernando%2520Garcia%2520Redondo%2520and%2520David%2520O%2527Shea%2520and%2520William%2520Shen%2520and%2520Alex%2520Iacob%2520and%2520Lorenzo%2520Sani%2520and%2520Xinchi%2520Qiu%2520and%2520Nicola%2520Cancedda%2520and%2520Nicholas%2520D.%2520Lane%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Autoregressive%2520Block-Based%2520Iterative%2520Encoder%2520%2528AbbIE%2529%252C%2520a%250Anovel%2520recursive%2520generalization%2520of%2520the%2520encoder-only%2520Transformer%2520architecture%252C%250Awhich%2520achieves%2520better%2520perplexity%2520than%2520a%2520standard%2520Transformer%2520and%2520allows%2520for%2520the%250Adynamic%2520scaling%2520of%2520compute%2520resources%2520at%2520test%2520time.%2520This%2520simple%252C%2520recursive%250Aapproach%2520is%2520a%2520complement%2520to%2520scaling%2520large%2520language%2520model%2520%2528LLM%2529%2520performance%250Athrough%2520parameter%2520and%2520token%2520counts.%2520AbbIE%2520performs%2520its%2520iterations%2520in%2520latent%250Aspace%252C%2520but%2520unlike%2520latent%2520reasoning%2520models%252C%2520does%2520not%2520require%2520a%2520specialized%250Adataset%2520or%2520training%2520protocol.%2520We%2520show%2520that%2520AbbIE%2520upward%2520generalizes%2520%2528ability%2520to%250Ageneralize%2520to%2520arbitrary%2520iteration%2520lengths%2529%2520at%2520test%2520time%2520by%2520only%2520using%25202%250Aiterations%2520during%2520train%2520time%252C%2520far%2520outperforming%2520alternative%2520iterative%2520methods.%250AAbbIE%2527s%2520ability%2520to%2520scale%2520its%2520computational%2520expenditure%2520based%2520on%2520the%2520complexity%250Aof%2520the%2520task%2520gives%2520it%2520an%2520up%2520to%2520%255Ctextbf%257B12%255C%2525%257D%2520improvement%2520in%2520zero-shot%2520in-context%250Alearning%2520tasks%2520versus%2520other%2520iterative%2520and%2520standard%2520methods%2520and%2520up%2520to%25205%255C%2525%250Aimprovement%2520in%2520language%2520perplexity.%2520The%2520results%2520from%2520this%2520study%2520open%2520a%2520new%250Aavenue%2520to%2520Transformer%2520performance%2520scaling.%2520We%2520perform%2520all%2520of%2520our%2520evaluations%2520on%250Amodel%2520sizes%2520up%2520to%2520350M%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AbbIE%3A%20Autoregressive%20Block-Based%20Iterative%20Encoder%20for%20Efficient%0A%20%20Sequence%20Modeling&entry.906535625=Preslav%20Aleksandrov%20and%20Meghdad%20Kurmanji%20and%20Fernando%20Garcia%20Redondo%20and%20David%20O%27Shea%20and%20William%20Shen%20and%20Alex%20Iacob%20and%20Lorenzo%20Sani%20and%20Xinchi%20Qiu%20and%20Nicola%20Cancedda%20and%20Nicholas%20D.%20Lane&entry.1292438233=%20%20We%20introduce%20the%20Autoregressive%20Block-Based%20Iterative%20Encoder%20%28AbbIE%29%2C%20a%0Anovel%20recursive%20generalization%20of%20the%20encoder-only%20Transformer%20architecture%2C%0Awhich%20achieves%20better%20perplexity%20than%20a%20standard%20Transformer%20and%20allows%20for%20the%0Adynamic%20scaling%20of%20compute%20resources%20at%20test%20time.%20This%20simple%2C%20recursive%0Aapproach%20is%20a%20complement%20to%20scaling%20large%20language%20model%20%28LLM%29%20performance%0Athrough%20parameter%20and%20token%20counts.%20AbbIE%20performs%20its%20iterations%20in%20latent%0Aspace%2C%20but%20unlike%20latent%20reasoning%20models%2C%20does%20not%20require%20a%20specialized%0Adataset%20or%20training%20protocol.%20We%20show%20that%20AbbIE%20upward%20generalizes%20%28ability%20to%0Ageneralize%20to%20arbitrary%20iteration%20lengths%29%20at%20test%20time%20by%20only%20using%202%0Aiterations%20during%20train%20time%2C%20far%20outperforming%20alternative%20iterative%20methods.%0AAbbIE%27s%20ability%20to%20scale%20its%20computational%20expenditure%20based%20on%20the%20complexity%0Aof%20the%20task%20gives%20it%20an%20up%20to%20%5Ctextbf%7B12%5C%25%7D%20improvement%20in%20zero-shot%20in-context%0Alearning%20tasks%20versus%20other%20iterative%20and%20standard%20methods%20and%20up%20to%205%5C%25%0Aimprovement%20in%20language%20perplexity.%20The%20results%20from%20this%20study%20open%20a%20new%0Aavenue%20to%20Transformer%20performance%20scaling.%20We%20perform%20all%20of%20our%20evaluations%20on%0Amodel%20sizes%20up%20to%20350M%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08567v2&entry.124074799=Read"},
{"title": "How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and\n  Localization", "author": "Liangwei Li and Lin Liu and Juanxiu Liu and Jing Zhang and Ruqian Hao and Xiaohui Du", "abstract": "  We propose a new paradigm for unsupervised anomaly detection and localization\nusing Flow Matching (FM), which fundamentally addresses the model expressivity\nlimitations of conventional flow-based methods. To this end, we formalize the\nconcept of time-reversed Flow Matching (rFM) as a vector field regression along\na predefined probability path to transform unknown data distributions into\nstandard Gaussian. We bring two core observations that reshape our\nunderstanding of FM. First, we rigorously prove that FM with linear\ninterpolation probability paths is inherently non-invertible. Second, our\nanalysis reveals that employing reversed Gaussian probability paths in\nhigh-dimensional spaces can lead to trivial vector fields. This issue arises\ndue to the manifold-related constraints. Building on the second observation, we\npropose Worst Transport (WT) displacement interpolation to reconstruct a\nnon-probabilistic evolution path. The proposed WT-Flow enhances dynamical\ncontrol over sample trajectories, constructing ''degenerate potential wells''\nfor anomaly-free samples while allowing anomalous samples to escape. This novel\nunsupervised paradigm offers a theoretically grounded separation mechanism for\nanomalous samples. Notably, FM provides a computationally tractable framework\nthat scales to complex data. We present the first successful application of FM\nfor the unsupervised anomaly detection task, achieving state-of-the-art\nperformance at a single scale on the MVTec dataset. The reproducible code for\ntraining will be released upon camera-ready submission.\n", "link": "http://arxiv.org/abs/2508.05461v1", "date": "2025-08-07", "relevancy": 2.0569, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5573}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5194}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20and%20Why%3A%20Taming%20Flow%20Matching%20for%20Unsupervised%20Anomaly%20Detection%20and%0A%20%20Localization&body=Title%3A%20How%20and%20Why%3A%20Taming%20Flow%20Matching%20for%20Unsupervised%20Anomaly%20Detection%20and%0A%20%20Localization%0AAuthor%3A%20Liangwei%20Li%20and%20Lin%20Liu%20and%20Juanxiu%20Liu%20and%20Jing%20Zhang%20and%20Ruqian%20Hao%20and%20Xiaohui%20Du%0AAbstract%3A%20%20%20We%20propose%20a%20new%20paradigm%20for%20unsupervised%20anomaly%20detection%20and%20localization%0Ausing%20Flow%20Matching%20%28FM%29%2C%20which%20fundamentally%20addresses%20the%20model%20expressivity%0Alimitations%20of%20conventional%20flow-based%20methods.%20To%20this%20end%2C%20we%20formalize%20the%0Aconcept%20of%20time-reversed%20Flow%20Matching%20%28rFM%29%20as%20a%20vector%20field%20regression%20along%0Aa%20predefined%20probability%20path%20to%20transform%20unknown%20data%20distributions%20into%0Astandard%20Gaussian.%20We%20bring%20two%20core%20observations%20that%20reshape%20our%0Aunderstanding%20of%20FM.%20First%2C%20we%20rigorously%20prove%20that%20FM%20with%20linear%0Ainterpolation%20probability%20paths%20is%20inherently%20non-invertible.%20Second%2C%20our%0Aanalysis%20reveals%20that%20employing%20reversed%20Gaussian%20probability%20paths%20in%0Ahigh-dimensional%20spaces%20can%20lead%20to%20trivial%20vector%20fields.%20This%20issue%20arises%0Adue%20to%20the%20manifold-related%20constraints.%20Building%20on%20the%20second%20observation%2C%20we%0Apropose%20Worst%20Transport%20%28WT%29%20displacement%20interpolation%20to%20reconstruct%20a%0Anon-probabilistic%20evolution%20path.%20The%20proposed%20WT-Flow%20enhances%20dynamical%0Acontrol%20over%20sample%20trajectories%2C%20constructing%20%27%27degenerate%20potential%20wells%27%27%0Afor%20anomaly-free%20samples%20while%20allowing%20anomalous%20samples%20to%20escape.%20This%20novel%0Aunsupervised%20paradigm%20offers%20a%20theoretically%20grounded%20separation%20mechanism%20for%0Aanomalous%20samples.%20Notably%2C%20FM%20provides%20a%20computationally%20tractable%20framework%0Athat%20scales%20to%20complex%20data.%20We%20present%20the%20first%20successful%20application%20of%20FM%0Afor%20the%20unsupervised%20anomaly%20detection%20task%2C%20achieving%20state-of-the-art%0Aperformance%20at%20a%20single%20scale%20on%20the%20MVTec%20dataset.%20The%20reproducible%20code%20for%0Atraining%20will%20be%20released%20upon%20camera-ready%20submission.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520and%2520Why%253A%2520Taming%2520Flow%2520Matching%2520for%2520Unsupervised%2520Anomaly%2520Detection%2520and%250A%2520%2520Localization%26entry.906535625%3DLiangwei%2520Li%2520and%2520Lin%2520Liu%2520and%2520Juanxiu%2520Liu%2520and%2520Jing%2520Zhang%2520and%2520Ruqian%2520Hao%2520and%2520Xiaohui%2520Du%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520paradigm%2520for%2520unsupervised%2520anomaly%2520detection%2520and%2520localization%250Ausing%2520Flow%2520Matching%2520%2528FM%2529%252C%2520which%2520fundamentally%2520addresses%2520the%2520model%2520expressivity%250Alimitations%2520of%2520conventional%2520flow-based%2520methods.%2520To%2520this%2520end%252C%2520we%2520formalize%2520the%250Aconcept%2520of%2520time-reversed%2520Flow%2520Matching%2520%2528rFM%2529%2520as%2520a%2520vector%2520field%2520regression%2520along%250Aa%2520predefined%2520probability%2520path%2520to%2520transform%2520unknown%2520data%2520distributions%2520into%250Astandard%2520Gaussian.%2520We%2520bring%2520two%2520core%2520observations%2520that%2520reshape%2520our%250Aunderstanding%2520of%2520FM.%2520First%252C%2520we%2520rigorously%2520prove%2520that%2520FM%2520with%2520linear%250Ainterpolation%2520probability%2520paths%2520is%2520inherently%2520non-invertible.%2520Second%252C%2520our%250Aanalysis%2520reveals%2520that%2520employing%2520reversed%2520Gaussian%2520probability%2520paths%2520in%250Ahigh-dimensional%2520spaces%2520can%2520lead%2520to%2520trivial%2520vector%2520fields.%2520This%2520issue%2520arises%250Adue%2520to%2520the%2520manifold-related%2520constraints.%2520Building%2520on%2520the%2520second%2520observation%252C%2520we%250Apropose%2520Worst%2520Transport%2520%2528WT%2529%2520displacement%2520interpolation%2520to%2520reconstruct%2520a%250Anon-probabilistic%2520evolution%2520path.%2520The%2520proposed%2520WT-Flow%2520enhances%2520dynamical%250Acontrol%2520over%2520sample%2520trajectories%252C%2520constructing%2520%2527%2527degenerate%2520potential%2520wells%2527%2527%250Afor%2520anomaly-free%2520samples%2520while%2520allowing%2520anomalous%2520samples%2520to%2520escape.%2520This%2520novel%250Aunsupervised%2520paradigm%2520offers%2520a%2520theoretically%2520grounded%2520separation%2520mechanism%2520for%250Aanomalous%2520samples.%2520Notably%252C%2520FM%2520provides%2520a%2520computationally%2520tractable%2520framework%250Athat%2520scales%2520to%2520complex%2520data.%2520We%2520present%2520the%2520first%2520successful%2520application%2520of%2520FM%250Afor%2520the%2520unsupervised%2520anomaly%2520detection%2520task%252C%2520achieving%2520state-of-the-art%250Aperformance%2520at%2520a%2520single%2520scale%2520on%2520the%2520MVTec%2520dataset.%2520The%2520reproducible%2520code%2520for%250Atraining%2520will%2520be%2520released%2520upon%2520camera-ready%2520submission.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20and%20Why%3A%20Taming%20Flow%20Matching%20for%20Unsupervised%20Anomaly%20Detection%20and%0A%20%20Localization&entry.906535625=Liangwei%20Li%20and%20Lin%20Liu%20and%20Juanxiu%20Liu%20and%20Jing%20Zhang%20and%20Ruqian%20Hao%20and%20Xiaohui%20Du&entry.1292438233=%20%20We%20propose%20a%20new%20paradigm%20for%20unsupervised%20anomaly%20detection%20and%20localization%0Ausing%20Flow%20Matching%20%28FM%29%2C%20which%20fundamentally%20addresses%20the%20model%20expressivity%0Alimitations%20of%20conventional%20flow-based%20methods.%20To%20this%20end%2C%20we%20formalize%20the%0Aconcept%20of%20time-reversed%20Flow%20Matching%20%28rFM%29%20as%20a%20vector%20field%20regression%20along%0Aa%20predefined%20probability%20path%20to%20transform%20unknown%20data%20distributions%20into%0Astandard%20Gaussian.%20We%20bring%20two%20core%20observations%20that%20reshape%20our%0Aunderstanding%20of%20FM.%20First%2C%20we%20rigorously%20prove%20that%20FM%20with%20linear%0Ainterpolation%20probability%20paths%20is%20inherently%20non-invertible.%20Second%2C%20our%0Aanalysis%20reveals%20that%20employing%20reversed%20Gaussian%20probability%20paths%20in%0Ahigh-dimensional%20spaces%20can%20lead%20to%20trivial%20vector%20fields.%20This%20issue%20arises%0Adue%20to%20the%20manifold-related%20constraints.%20Building%20on%20the%20second%20observation%2C%20we%0Apropose%20Worst%20Transport%20%28WT%29%20displacement%20interpolation%20to%20reconstruct%20a%0Anon-probabilistic%20evolution%20path.%20The%20proposed%20WT-Flow%20enhances%20dynamical%0Acontrol%20over%20sample%20trajectories%2C%20constructing%20%27%27degenerate%20potential%20wells%27%27%0Afor%20anomaly-free%20samples%20while%20allowing%20anomalous%20samples%20to%20escape.%20This%20novel%0Aunsupervised%20paradigm%20offers%20a%20theoretically%20grounded%20separation%20mechanism%20for%0Aanomalous%20samples.%20Notably%2C%20FM%20provides%20a%20computationally%20tractable%20framework%0Athat%20scales%20to%20complex%20data.%20We%20present%20the%20first%20successful%20application%20of%20FM%0Afor%20the%20unsupervised%20anomaly%20detection%20task%2C%20achieving%20state-of-the-art%0Aperformance%20at%20a%20single%20scale%20on%20the%20MVTec%20dataset.%20The%20reproducible%20code%20for%0Atraining%20will%20be%20released%20upon%20camera-ready%20submission.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05461v1&entry.124074799=Read"},
{"title": "Test-Time Reinforcement Learning for GUI Grounding via Region\n  Consistency", "author": "Yong Du and Yuchen Yan and Fei Tang and Zhengxi Lu and Chang Zong and Weiming Lu and Shengpei Jiang and Yongliang Shen", "abstract": "  Graphical User Interface (GUI) grounding, the task of mapping natural\nlanguage instructions to precise screen coordinates, is fundamental to\nautonomous GUI agents. While existing methods achieve strong performance\nthrough extensive supervised training or reinforcement learning with labeled\nrewards, they remain constrained by the cost and availability of pixel-level\nannotations. We observe that when models generate multiple predictions for the\nsame GUI element, the spatial overlap patterns reveal implicit confidence\nsignals that can guide more accurate localization. Leveraging this insight, we\npropose GUI-RC (Region Consistency), a test-time scaling method that constructs\nspatial voting grids from multiple sampled predictions to identify consensus\nregions where models show highest agreement. Without any training, GUI-RC\nimproves accuracy by 2-3% across various architectures on ScreenSpot\nbenchmarks. We further introduce GUI-RCPO (Region Consistency Policy\nOptimization), which transforms these consistency patterns into rewards for\ntest-time reinforcement learning. By computing how well each prediction aligns\nwith the collective consensus, GUI-RCPO enables models to iteratively refine\ntheir outputs on unlabeled data during inference. Extensive experiments\ndemonstrate the generality of our approach: GUI-RC boosts\nQwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO\nfurther improves it to 85.14% through self-supervised optimization. Our\napproach reveals the untapped potential of test-time scaling and test-time\nreinforcement learning for GUI grounding, offering a promising path toward more\nrobust and data-efficient GUI agents.\n", "link": "http://arxiv.org/abs/2508.05615v1", "date": "2025-08-07", "relevancy": 2.0536, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5383}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.509}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Reinforcement%20Learning%20for%20GUI%20Grounding%20via%20Region%0A%20%20Consistency&body=Title%3A%20Test-Time%20Reinforcement%20Learning%20for%20GUI%20Grounding%20via%20Region%0A%20%20Consistency%0AAuthor%3A%20Yong%20Du%20and%20Yuchen%20Yan%20and%20Fei%20Tang%20and%20Zhengxi%20Lu%20and%20Chang%20Zong%20and%20Weiming%20Lu%20and%20Shengpei%20Jiang%20and%20Yongliang%20Shen%0AAbstract%3A%20%20%20Graphical%20User%20Interface%20%28GUI%29%20grounding%2C%20the%20task%20of%20mapping%20natural%0Alanguage%20instructions%20to%20precise%20screen%20coordinates%2C%20is%20fundamental%20to%0Aautonomous%20GUI%20agents.%20While%20existing%20methods%20achieve%20strong%20performance%0Athrough%20extensive%20supervised%20training%20or%20reinforcement%20learning%20with%20labeled%0Arewards%2C%20they%20remain%20constrained%20by%20the%20cost%20and%20availability%20of%20pixel-level%0Aannotations.%20We%20observe%20that%20when%20models%20generate%20multiple%20predictions%20for%20the%0Asame%20GUI%20element%2C%20the%20spatial%20overlap%20patterns%20reveal%20implicit%20confidence%0Asignals%20that%20can%20guide%20more%20accurate%20localization.%20Leveraging%20this%20insight%2C%20we%0Apropose%20GUI-RC%20%28Region%20Consistency%29%2C%20a%20test-time%20scaling%20method%20that%20constructs%0Aspatial%20voting%20grids%20from%20multiple%20sampled%20predictions%20to%20identify%20consensus%0Aregions%20where%20models%20show%20highest%20agreement.%20Without%20any%20training%2C%20GUI-RC%0Aimproves%20accuracy%20by%202-3%25%20across%20various%20architectures%20on%20ScreenSpot%0Abenchmarks.%20We%20further%20introduce%20GUI-RCPO%20%28Region%20Consistency%20Policy%0AOptimization%29%2C%20which%20transforms%20these%20consistency%20patterns%20into%20rewards%20for%0Atest-time%20reinforcement%20learning.%20By%20computing%20how%20well%20each%20prediction%20aligns%0Awith%20the%20collective%20consensus%2C%20GUI-RCPO%20enables%20models%20to%20iteratively%20refine%0Atheir%20outputs%20on%20unlabeled%20data%20during%20inference.%20Extensive%20experiments%0Ademonstrate%20the%20generality%20of%20our%20approach%3A%20GUI-RC%20boosts%0AQwen2.5-VL-3B-Instruct%20from%2080.11%25%20to%2083.57%25%20on%20ScreenSpot-v2%2C%20while%20GUI-RCPO%0Afurther%20improves%20it%20to%2085.14%25%20through%20self-supervised%20optimization.%20Our%0Aapproach%20reveals%20the%20untapped%20potential%20of%20test-time%20scaling%20and%20test-time%0Areinforcement%20learning%20for%20GUI%20grounding%2C%20offering%20a%20promising%20path%20toward%20more%0Arobust%20and%20data-efficient%20GUI%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Reinforcement%2520Learning%2520for%2520GUI%2520Grounding%2520via%2520Region%250A%2520%2520Consistency%26entry.906535625%3DYong%2520Du%2520and%2520Yuchen%2520Yan%2520and%2520Fei%2520Tang%2520and%2520Zhengxi%2520Lu%2520and%2520Chang%2520Zong%2520and%2520Weiming%2520Lu%2520and%2520Shengpei%2520Jiang%2520and%2520Yongliang%2520Shen%26entry.1292438233%3D%2520%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520grounding%252C%2520the%2520task%2520of%2520mapping%2520natural%250Alanguage%2520instructions%2520to%2520precise%2520screen%2520coordinates%252C%2520is%2520fundamental%2520to%250Aautonomous%2520GUI%2520agents.%2520While%2520existing%2520methods%2520achieve%2520strong%2520performance%250Athrough%2520extensive%2520supervised%2520training%2520or%2520reinforcement%2520learning%2520with%2520labeled%250Arewards%252C%2520they%2520remain%2520constrained%2520by%2520the%2520cost%2520and%2520availability%2520of%2520pixel-level%250Aannotations.%2520We%2520observe%2520that%2520when%2520models%2520generate%2520multiple%2520predictions%2520for%2520the%250Asame%2520GUI%2520element%252C%2520the%2520spatial%2520overlap%2520patterns%2520reveal%2520implicit%2520confidence%250Asignals%2520that%2520can%2520guide%2520more%2520accurate%2520localization.%2520Leveraging%2520this%2520insight%252C%2520we%250Apropose%2520GUI-RC%2520%2528Region%2520Consistency%2529%252C%2520a%2520test-time%2520scaling%2520method%2520that%2520constructs%250Aspatial%2520voting%2520grids%2520from%2520multiple%2520sampled%2520predictions%2520to%2520identify%2520consensus%250Aregions%2520where%2520models%2520show%2520highest%2520agreement.%2520Without%2520any%2520training%252C%2520GUI-RC%250Aimproves%2520accuracy%2520by%25202-3%2525%2520across%2520various%2520architectures%2520on%2520ScreenSpot%250Abenchmarks.%2520We%2520further%2520introduce%2520GUI-RCPO%2520%2528Region%2520Consistency%2520Policy%250AOptimization%2529%252C%2520which%2520transforms%2520these%2520consistency%2520patterns%2520into%2520rewards%2520for%250Atest-time%2520reinforcement%2520learning.%2520By%2520computing%2520how%2520well%2520each%2520prediction%2520aligns%250Awith%2520the%2520collective%2520consensus%252C%2520GUI-RCPO%2520enables%2520models%2520to%2520iteratively%2520refine%250Atheir%2520outputs%2520on%2520unlabeled%2520data%2520during%2520inference.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520generality%2520of%2520our%2520approach%253A%2520GUI-RC%2520boosts%250AQwen2.5-VL-3B-Instruct%2520from%252080.11%2525%2520to%252083.57%2525%2520on%2520ScreenSpot-v2%252C%2520while%2520GUI-RCPO%250Afurther%2520improves%2520it%2520to%252085.14%2525%2520through%2520self-supervised%2520optimization.%2520Our%250Aapproach%2520reveals%2520the%2520untapped%2520potential%2520of%2520test-time%2520scaling%2520and%2520test-time%250Areinforcement%2520learning%2520for%2520GUI%2520grounding%252C%2520offering%2520a%2520promising%2520path%2520toward%2520more%250Arobust%2520and%2520data-efficient%2520GUI%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Reinforcement%20Learning%20for%20GUI%20Grounding%20via%20Region%0A%20%20Consistency&entry.906535625=Yong%20Du%20and%20Yuchen%20Yan%20and%20Fei%20Tang%20and%20Zhengxi%20Lu%20and%20Chang%20Zong%20and%20Weiming%20Lu%20and%20Shengpei%20Jiang%20and%20Yongliang%20Shen&entry.1292438233=%20%20Graphical%20User%20Interface%20%28GUI%29%20grounding%2C%20the%20task%20of%20mapping%20natural%0Alanguage%20instructions%20to%20precise%20screen%20coordinates%2C%20is%20fundamental%20to%0Aautonomous%20GUI%20agents.%20While%20existing%20methods%20achieve%20strong%20performance%0Athrough%20extensive%20supervised%20training%20or%20reinforcement%20learning%20with%20labeled%0Arewards%2C%20they%20remain%20constrained%20by%20the%20cost%20and%20availability%20of%20pixel-level%0Aannotations.%20We%20observe%20that%20when%20models%20generate%20multiple%20predictions%20for%20the%0Asame%20GUI%20element%2C%20the%20spatial%20overlap%20patterns%20reveal%20implicit%20confidence%0Asignals%20that%20can%20guide%20more%20accurate%20localization.%20Leveraging%20this%20insight%2C%20we%0Apropose%20GUI-RC%20%28Region%20Consistency%29%2C%20a%20test-time%20scaling%20method%20that%20constructs%0Aspatial%20voting%20grids%20from%20multiple%20sampled%20predictions%20to%20identify%20consensus%0Aregions%20where%20models%20show%20highest%20agreement.%20Without%20any%20training%2C%20GUI-RC%0Aimproves%20accuracy%20by%202-3%25%20across%20various%20architectures%20on%20ScreenSpot%0Abenchmarks.%20We%20further%20introduce%20GUI-RCPO%20%28Region%20Consistency%20Policy%0AOptimization%29%2C%20which%20transforms%20these%20consistency%20patterns%20into%20rewards%20for%0Atest-time%20reinforcement%20learning.%20By%20computing%20how%20well%20each%20prediction%20aligns%0Awith%20the%20collective%20consensus%2C%20GUI-RCPO%20enables%20models%20to%20iteratively%20refine%0Atheir%20outputs%20on%20unlabeled%20data%20during%20inference.%20Extensive%20experiments%0Ademonstrate%20the%20generality%20of%20our%20approach%3A%20GUI-RC%20boosts%0AQwen2.5-VL-3B-Instruct%20from%2080.11%25%20to%2083.57%25%20on%20ScreenSpot-v2%2C%20while%20GUI-RCPO%0Afurther%20improves%20it%20to%2085.14%25%20through%20self-supervised%20optimization.%20Our%0Aapproach%20reveals%20the%20untapped%20potential%20of%20test-time%20scaling%20and%20test-time%0Areinforcement%20learning%20for%20GUI%20grounding%2C%20offering%20a%20promising%20path%20toward%20more%0Arobust%20and%20data-efficient%20GUI%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05615v1&entry.124074799=Read"},
{"title": "Learning Geometric-Aware Quadrature Rules for Functional Minimization", "author": "Costas Smaragdakis", "abstract": "  Accurate numerical integration over non-uniform point clouds is a challenge\nfor modern mesh-free machine learning solvers for partial differential\nequations (PDEs) using variational principles. While standard Monte Carlo (MC)\nmethods are not capable of handling a non-uniform point cloud, modern neural\nnetwork architectures can deal with permutation-invariant inputs, creating\nquadrature rules for any point cloud. In this work, we introduce QuadrANN, a\nGraph Neural Network (GNN) architecture designed to learn optimal quadrature\nweights directly from the underlying geometry of point clouds. The design of\nthe model exploits a deep message-passing scheme where the initial layer\nencodes rich local geometric features from absolute and relative positions as\nwell as an explicit local density measure. In contrast, the following layers\nincorporate a global context vector. These architectural choices allow the\nQuadrANN to generate a data-driven quadrature rule that is\npermutation-invariant and adaptive to both local point density and the overall\ndomain shape. We test our methodology on a series of challenging test cases,\nincluding integration on convex and non-convex domains and estimating the\nsolution of the Heat and Fokker-Planck equations. Across all the tests,\nQuadrANN reduces the variance of the integral estimation compared to standard\nQuasi-Monte Carlo methods by warping the point clouds to be more dense in\ncritical areas where the integrands present certain singularities. This\nenhanced stability in critical areas of the domain at hand is critical for the\noptimization of energy functionals, leading to improved deep learning-based\nvariational solvers.\n", "link": "http://arxiv.org/abs/2508.05445v1", "date": "2025-08-07", "relevancy": 2.0471, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5463}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4873}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Geometric-Aware%20Quadrature%20Rules%20for%20Functional%20Minimization&body=Title%3A%20Learning%20Geometric-Aware%20Quadrature%20Rules%20for%20Functional%20Minimization%0AAuthor%3A%20Costas%20Smaragdakis%0AAbstract%3A%20%20%20Accurate%20numerical%20integration%20over%20non-uniform%20point%20clouds%20is%20a%20challenge%0Afor%20modern%20mesh-free%20machine%20learning%20solvers%20for%20partial%20differential%0Aequations%20%28PDEs%29%20using%20variational%20principles.%20While%20standard%20Monte%20Carlo%20%28MC%29%0Amethods%20are%20not%20capable%20of%20handling%20a%20non-uniform%20point%20cloud%2C%20modern%20neural%0Anetwork%20architectures%20can%20deal%20with%20permutation-invariant%20inputs%2C%20creating%0Aquadrature%20rules%20for%20any%20point%20cloud.%20In%20this%20work%2C%20we%20introduce%20QuadrANN%2C%20a%0AGraph%20Neural%20Network%20%28GNN%29%20architecture%20designed%20to%20learn%20optimal%20quadrature%0Aweights%20directly%20from%20the%20underlying%20geometry%20of%20point%20clouds.%20The%20design%20of%0Athe%20model%20exploits%20a%20deep%20message-passing%20scheme%20where%20the%20initial%20layer%0Aencodes%20rich%20local%20geometric%20features%20from%20absolute%20and%20relative%20positions%20as%0Awell%20as%20an%20explicit%20local%20density%20measure.%20In%20contrast%2C%20the%20following%20layers%0Aincorporate%20a%20global%20context%20vector.%20These%20architectural%20choices%20allow%20the%0AQuadrANN%20to%20generate%20a%20data-driven%20quadrature%20rule%20that%20is%0Apermutation-invariant%20and%20adaptive%20to%20both%20local%20point%20density%20and%20the%20overall%0Adomain%20shape.%20We%20test%20our%20methodology%20on%20a%20series%20of%20challenging%20test%20cases%2C%0Aincluding%20integration%20on%20convex%20and%20non-convex%20domains%20and%20estimating%20the%0Asolution%20of%20the%20Heat%20and%20Fokker-Planck%20equations.%20Across%20all%20the%20tests%2C%0AQuadrANN%20reduces%20the%20variance%20of%20the%20integral%20estimation%20compared%20to%20standard%0AQuasi-Monte%20Carlo%20methods%20by%20warping%20the%20point%20clouds%20to%20be%20more%20dense%20in%0Acritical%20areas%20where%20the%20integrands%20present%20certain%20singularities.%20This%0Aenhanced%20stability%20in%20critical%20areas%20of%20the%20domain%20at%20hand%20is%20critical%20for%20the%0Aoptimization%20of%20energy%20functionals%2C%20leading%20to%20improved%20deep%20learning-based%0Avariational%20solvers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Geometric-Aware%2520Quadrature%2520Rules%2520for%2520Functional%2520Minimization%26entry.906535625%3DCostas%2520Smaragdakis%26entry.1292438233%3D%2520%2520Accurate%2520numerical%2520integration%2520over%2520non-uniform%2520point%2520clouds%2520is%2520a%2520challenge%250Afor%2520modern%2520mesh-free%2520machine%2520learning%2520solvers%2520for%2520partial%2520differential%250Aequations%2520%2528PDEs%2529%2520using%2520variational%2520principles.%2520While%2520standard%2520Monte%2520Carlo%2520%2528MC%2529%250Amethods%2520are%2520not%2520capable%2520of%2520handling%2520a%2520non-uniform%2520point%2520cloud%252C%2520modern%2520neural%250Anetwork%2520architectures%2520can%2520deal%2520with%2520permutation-invariant%2520inputs%252C%2520creating%250Aquadrature%2520rules%2520for%2520any%2520point%2520cloud.%2520In%2520this%2520work%252C%2520we%2520introduce%2520QuadrANN%252C%2520a%250AGraph%2520Neural%2520Network%2520%2528GNN%2529%2520architecture%2520designed%2520to%2520learn%2520optimal%2520quadrature%250Aweights%2520directly%2520from%2520the%2520underlying%2520geometry%2520of%2520point%2520clouds.%2520The%2520design%2520of%250Athe%2520model%2520exploits%2520a%2520deep%2520message-passing%2520scheme%2520where%2520the%2520initial%2520layer%250Aencodes%2520rich%2520local%2520geometric%2520features%2520from%2520absolute%2520and%2520relative%2520positions%2520as%250Awell%2520as%2520an%2520explicit%2520local%2520density%2520measure.%2520In%2520contrast%252C%2520the%2520following%2520layers%250Aincorporate%2520a%2520global%2520context%2520vector.%2520These%2520architectural%2520choices%2520allow%2520the%250AQuadrANN%2520to%2520generate%2520a%2520data-driven%2520quadrature%2520rule%2520that%2520is%250Apermutation-invariant%2520and%2520adaptive%2520to%2520both%2520local%2520point%2520density%2520and%2520the%2520overall%250Adomain%2520shape.%2520We%2520test%2520our%2520methodology%2520on%2520a%2520series%2520of%2520challenging%2520test%2520cases%252C%250Aincluding%2520integration%2520on%2520convex%2520and%2520non-convex%2520domains%2520and%2520estimating%2520the%250Asolution%2520of%2520the%2520Heat%2520and%2520Fokker-Planck%2520equations.%2520Across%2520all%2520the%2520tests%252C%250AQuadrANN%2520reduces%2520the%2520variance%2520of%2520the%2520integral%2520estimation%2520compared%2520to%2520standard%250AQuasi-Monte%2520Carlo%2520methods%2520by%2520warping%2520the%2520point%2520clouds%2520to%2520be%2520more%2520dense%2520in%250Acritical%2520areas%2520where%2520the%2520integrands%2520present%2520certain%2520singularities.%2520This%250Aenhanced%2520stability%2520in%2520critical%2520areas%2520of%2520the%2520domain%2520at%2520hand%2520is%2520critical%2520for%2520the%250Aoptimization%2520of%2520energy%2520functionals%252C%2520leading%2520to%2520improved%2520deep%2520learning-based%250Avariational%2520solvers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Geometric-Aware%20Quadrature%20Rules%20for%20Functional%20Minimization&entry.906535625=Costas%20Smaragdakis&entry.1292438233=%20%20Accurate%20numerical%20integration%20over%20non-uniform%20point%20clouds%20is%20a%20challenge%0Afor%20modern%20mesh-free%20machine%20learning%20solvers%20for%20partial%20differential%0Aequations%20%28PDEs%29%20using%20variational%20principles.%20While%20standard%20Monte%20Carlo%20%28MC%29%0Amethods%20are%20not%20capable%20of%20handling%20a%20non-uniform%20point%20cloud%2C%20modern%20neural%0Anetwork%20architectures%20can%20deal%20with%20permutation-invariant%20inputs%2C%20creating%0Aquadrature%20rules%20for%20any%20point%20cloud.%20In%20this%20work%2C%20we%20introduce%20QuadrANN%2C%20a%0AGraph%20Neural%20Network%20%28GNN%29%20architecture%20designed%20to%20learn%20optimal%20quadrature%0Aweights%20directly%20from%20the%20underlying%20geometry%20of%20point%20clouds.%20The%20design%20of%0Athe%20model%20exploits%20a%20deep%20message-passing%20scheme%20where%20the%20initial%20layer%0Aencodes%20rich%20local%20geometric%20features%20from%20absolute%20and%20relative%20positions%20as%0Awell%20as%20an%20explicit%20local%20density%20measure.%20In%20contrast%2C%20the%20following%20layers%0Aincorporate%20a%20global%20context%20vector.%20These%20architectural%20choices%20allow%20the%0AQuadrANN%20to%20generate%20a%20data-driven%20quadrature%20rule%20that%20is%0Apermutation-invariant%20and%20adaptive%20to%20both%20local%20point%20density%20and%20the%20overall%0Adomain%20shape.%20We%20test%20our%20methodology%20on%20a%20series%20of%20challenging%20test%20cases%2C%0Aincluding%20integration%20on%20convex%20and%20non-convex%20domains%20and%20estimating%20the%0Asolution%20of%20the%20Heat%20and%20Fokker-Planck%20equations.%20Across%20all%20the%20tests%2C%0AQuadrANN%20reduces%20the%20variance%20of%20the%20integral%20estimation%20compared%20to%20standard%0AQuasi-Monte%20Carlo%20methods%20by%20warping%20the%20point%20clouds%20to%20be%20more%20dense%20in%0Acritical%20areas%20where%20the%20integrands%20present%20certain%20singularities.%20This%0Aenhanced%20stability%20in%20critical%20areas%20of%20the%20domain%20at%20hand%20is%20critical%20for%20the%0Aoptimization%20of%20energy%20functionals%2C%20leading%20to%20improved%20deep%20learning-based%0Avariational%20solvers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05445v1&entry.124074799=Read"},
{"title": "Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and\n  Plasticity in Semi-supervised Continual Learning", "author": "Yue Duan and Taicai Chen and Lei Qi and Yinghuan Shi", "abstract": "  Semi-supervised continual learning (SSCL) seeks to leverage both labeled and\nunlabeled data in a sequential learning setup, aiming to reduce annotation\ncosts while managing continual data arrival. SSCL introduces complex\nchallenges, including ensuring effective unlabeled learning (UL), while\nbalancing memory stability (MS) and learning plasticity (LP). Previous SSCL\nefforts have typically focused on isolated aspects of the three, while this\nwork presents USP, a divide-and-conquer framework designed to synergistically\nenhance these three aspects: (1) Feature Space Reservation (FSR) strategy for\nLP, which constructs reserved feature locations for future classes by shaping\nold classes into an equiangular tight frame; (2) Divide-and-Conquer\nPseudo-labeling (DCP) approach for UL, which assigns reliable pseudo-labels\nacross both high- and low-confidence unlabeled data; and (3)\nClass-mean-anchored Unlabeled Distillation (CUD) for MS, which reuses DCP's\noutputs to anchor unlabeled data to stable class means for distillation to\nprevent forgetting. Comprehensive evaluations show USP outperforms prior SSCL\nmethods, with gains up to 5.94% in the last accuracy, validating its\neffectiveness. The code is available at https://github.com/NJUyued/USP4SSCL.\n", "link": "http://arxiv.org/abs/2508.05316v1", "date": "2025-08-07", "relevancy": 2.047, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5182}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5138}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divide-and-Conquer%20for%20Enhancing%20Unlabeled%20Learning%2C%20Stability%2C%20and%0A%20%20Plasticity%20in%20Semi-supervised%20Continual%20Learning&body=Title%3A%20Divide-and-Conquer%20for%20Enhancing%20Unlabeled%20Learning%2C%20Stability%2C%20and%0A%20%20Plasticity%20in%20Semi-supervised%20Continual%20Learning%0AAuthor%3A%20Yue%20Duan%20and%20Taicai%20Chen%20and%20Lei%20Qi%20and%20Yinghuan%20Shi%0AAbstract%3A%20%20%20Semi-supervised%20continual%20learning%20%28SSCL%29%20seeks%20to%20leverage%20both%20labeled%20and%0Aunlabeled%20data%20in%20a%20sequential%20learning%20setup%2C%20aiming%20to%20reduce%20annotation%0Acosts%20while%20managing%20continual%20data%20arrival.%20SSCL%20introduces%20complex%0Achallenges%2C%20including%20ensuring%20effective%20unlabeled%20learning%20%28UL%29%2C%20while%0Abalancing%20memory%20stability%20%28MS%29%20and%20learning%20plasticity%20%28LP%29.%20Previous%20SSCL%0Aefforts%20have%20typically%20focused%20on%20isolated%20aspects%20of%20the%20three%2C%20while%20this%0Awork%20presents%20USP%2C%20a%20divide-and-conquer%20framework%20designed%20to%20synergistically%0Aenhance%20these%20three%20aspects%3A%20%281%29%20Feature%20Space%20Reservation%20%28FSR%29%20strategy%20for%0ALP%2C%20which%20constructs%20reserved%20feature%20locations%20for%20future%20classes%20by%20shaping%0Aold%20classes%20into%20an%20equiangular%20tight%20frame%3B%20%282%29%20Divide-and-Conquer%0APseudo-labeling%20%28DCP%29%20approach%20for%20UL%2C%20which%20assigns%20reliable%20pseudo-labels%0Aacross%20both%20high-%20and%20low-confidence%20unlabeled%20data%3B%20and%20%283%29%0AClass-mean-anchored%20Unlabeled%20Distillation%20%28CUD%29%20for%20MS%2C%20which%20reuses%20DCP%27s%0Aoutputs%20to%20anchor%20unlabeled%20data%20to%20stable%20class%20means%20for%20distillation%20to%0Aprevent%20forgetting.%20Comprehensive%20evaluations%20show%20USP%20outperforms%20prior%20SSCL%0Amethods%2C%20with%20gains%20up%20to%205.94%25%20in%20the%20last%20accuracy%2C%20validating%20its%0Aeffectiveness.%20The%20code%20is%20available%20at%20https%3A//github.com/NJUyued/USP4SSCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivide-and-Conquer%2520for%2520Enhancing%2520Unlabeled%2520Learning%252C%2520Stability%252C%2520and%250A%2520%2520Plasticity%2520in%2520Semi-supervised%2520Continual%2520Learning%26entry.906535625%3DYue%2520Duan%2520and%2520Taicai%2520Chen%2520and%2520Lei%2520Qi%2520and%2520Yinghuan%2520Shi%26entry.1292438233%3D%2520%2520Semi-supervised%2520continual%2520learning%2520%2528SSCL%2529%2520seeks%2520to%2520leverage%2520both%2520labeled%2520and%250Aunlabeled%2520data%2520in%2520a%2520sequential%2520learning%2520setup%252C%2520aiming%2520to%2520reduce%2520annotation%250Acosts%2520while%2520managing%2520continual%2520data%2520arrival.%2520SSCL%2520introduces%2520complex%250Achallenges%252C%2520including%2520ensuring%2520effective%2520unlabeled%2520learning%2520%2528UL%2529%252C%2520while%250Abalancing%2520memory%2520stability%2520%2528MS%2529%2520and%2520learning%2520plasticity%2520%2528LP%2529.%2520Previous%2520SSCL%250Aefforts%2520have%2520typically%2520focused%2520on%2520isolated%2520aspects%2520of%2520the%2520three%252C%2520while%2520this%250Awork%2520presents%2520USP%252C%2520a%2520divide-and-conquer%2520framework%2520designed%2520to%2520synergistically%250Aenhance%2520these%2520three%2520aspects%253A%2520%25281%2529%2520Feature%2520Space%2520Reservation%2520%2528FSR%2529%2520strategy%2520for%250ALP%252C%2520which%2520constructs%2520reserved%2520feature%2520locations%2520for%2520future%2520classes%2520by%2520shaping%250Aold%2520classes%2520into%2520an%2520equiangular%2520tight%2520frame%253B%2520%25282%2529%2520Divide-and-Conquer%250APseudo-labeling%2520%2528DCP%2529%2520approach%2520for%2520UL%252C%2520which%2520assigns%2520reliable%2520pseudo-labels%250Aacross%2520both%2520high-%2520and%2520low-confidence%2520unlabeled%2520data%253B%2520and%2520%25283%2529%250AClass-mean-anchored%2520Unlabeled%2520Distillation%2520%2528CUD%2529%2520for%2520MS%252C%2520which%2520reuses%2520DCP%2527s%250Aoutputs%2520to%2520anchor%2520unlabeled%2520data%2520to%2520stable%2520class%2520means%2520for%2520distillation%2520to%250Aprevent%2520forgetting.%2520Comprehensive%2520evaluations%2520show%2520USP%2520outperforms%2520prior%2520SSCL%250Amethods%252C%2520with%2520gains%2520up%2520to%25205.94%2525%2520in%2520the%2520last%2520accuracy%252C%2520validating%2520its%250Aeffectiveness.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/NJUyued/USP4SSCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide-and-Conquer%20for%20Enhancing%20Unlabeled%20Learning%2C%20Stability%2C%20and%0A%20%20Plasticity%20in%20Semi-supervised%20Continual%20Learning&entry.906535625=Yue%20Duan%20and%20Taicai%20Chen%20and%20Lei%20Qi%20and%20Yinghuan%20Shi&entry.1292438233=%20%20Semi-supervised%20continual%20learning%20%28SSCL%29%20seeks%20to%20leverage%20both%20labeled%20and%0Aunlabeled%20data%20in%20a%20sequential%20learning%20setup%2C%20aiming%20to%20reduce%20annotation%0Acosts%20while%20managing%20continual%20data%20arrival.%20SSCL%20introduces%20complex%0Achallenges%2C%20including%20ensuring%20effective%20unlabeled%20learning%20%28UL%29%2C%20while%0Abalancing%20memory%20stability%20%28MS%29%20and%20learning%20plasticity%20%28LP%29.%20Previous%20SSCL%0Aefforts%20have%20typically%20focused%20on%20isolated%20aspects%20of%20the%20three%2C%20while%20this%0Awork%20presents%20USP%2C%20a%20divide-and-conquer%20framework%20designed%20to%20synergistically%0Aenhance%20these%20three%20aspects%3A%20%281%29%20Feature%20Space%20Reservation%20%28FSR%29%20strategy%20for%0ALP%2C%20which%20constructs%20reserved%20feature%20locations%20for%20future%20classes%20by%20shaping%0Aold%20classes%20into%20an%20equiangular%20tight%20frame%3B%20%282%29%20Divide-and-Conquer%0APseudo-labeling%20%28DCP%29%20approach%20for%20UL%2C%20which%20assigns%20reliable%20pseudo-labels%0Aacross%20both%20high-%20and%20low-confidence%20unlabeled%20data%3B%20and%20%283%29%0AClass-mean-anchored%20Unlabeled%20Distillation%20%28CUD%29%20for%20MS%2C%20which%20reuses%20DCP%27s%0Aoutputs%20to%20anchor%20unlabeled%20data%20to%20stable%20class%20means%20for%20distillation%20to%0Aprevent%20forgetting.%20Comprehensive%20evaluations%20show%20USP%20outperforms%20prior%20SSCL%0Amethods%2C%20with%20gains%20up%20to%205.94%25%20in%20the%20last%20accuracy%2C%20validating%20its%0Aeffectiveness.%20The%20code%20is%20available%20at%20https%3A//github.com/NJUyued/USP4SSCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05316v1&entry.124074799=Read"},
{"title": "GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval\n  Augmented Reasoning", "author": "Ge Chang and Jinbo Su and Jiacheng Liu and Pengfei Yang and Yuhao Shang and Huiwen Zheng and Hongli Ma and Yan Liang and Yuanchun Li and Yunxin Liu", "abstract": "  Large Language Models (LLMs) integrated with Retrieval-Augmented Generation\n(RAG) techniques have exhibited remarkable performance across a wide range of\ndomains. However, existing RAG approaches primarily operate on unstructured\ndata and demonstrate limited capability in handling structured knowledge such\nas knowledge graphs. Meanwhile, current graph retrieval methods fundamentally\nstruggle to capture holistic graph structures while simultaneously facing\nprecision control challenges that manifest as either critical information gaps\nor excessive redundant connections, collectively undermining reasoning\nperformance. To address this challenge, we propose GRAIL: Graph-Retrieval\nAugmented Interactive Learning, a framework designed to interact with\nlarge-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL\nintegrates LLM-guided random exploration with path filtering to establish a\ndata synthesis pipeline, where a fine-grained reasoning trajectory is\nautomatically generated for each task. Based on the synthesized data, we then\nemploy a two-stage training process to learn a policy that dynamically decides\nthe optimal actions at each reasoning step. The overall objective of\nprecision-conciseness balance in graph retrieval is decoupled into fine-grained\nprocess-supervised rewards to enhance data efficiency and training stability.\nIn practical deployment, GRAIL adopts an interactive retrieval paradigm,\nenabling the model to autonomously explore graph paths while dynamically\nbalancing retrieval breadth and precision. Extensive experiments have shown\nthat GRAIL achieves an average accuracy improvement of 21.01% and F1\nimprovement of 22.43% on three knowledge graph question-answering datasets. Our\nsource code and datasets is available at https://github.com/Changgeww/GRAIL.\n", "link": "http://arxiv.org/abs/2508.05498v1", "date": "2025-08-07", "relevancy": 2.0441, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.546}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5208}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAIL%3ALearning%20to%20Interact%20with%20Large%20Knowledge%20Graphs%20for%20Retrieval%0A%20%20Augmented%20Reasoning&body=Title%3A%20GRAIL%3ALearning%20to%20Interact%20with%20Large%20Knowledge%20Graphs%20for%20Retrieval%0A%20%20Augmented%20Reasoning%0AAuthor%3A%20Ge%20Chang%20and%20Jinbo%20Su%20and%20Jiacheng%20Liu%20and%20Pengfei%20Yang%20and%20Yuhao%20Shang%20and%20Huiwen%20Zheng%20and%20Hongli%20Ma%20and%20Yan%20Liang%20and%20Yuanchun%20Li%20and%20Yunxin%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20integrated%20with%20Retrieval-Augmented%20Generation%0A%28RAG%29%20techniques%20have%20exhibited%20remarkable%20performance%20across%20a%20wide%20range%20of%0Adomains.%20However%2C%20existing%20RAG%20approaches%20primarily%20operate%20on%20unstructured%0Adata%20and%20demonstrate%20limited%20capability%20in%20handling%20structured%20knowledge%20such%0Aas%20knowledge%20graphs.%20Meanwhile%2C%20current%20graph%20retrieval%20methods%20fundamentally%0Astruggle%20to%20capture%20holistic%20graph%20structures%20while%20simultaneously%20facing%0Aprecision%20control%20challenges%20that%20manifest%20as%20either%20critical%20information%20gaps%0Aor%20excessive%20redundant%20connections%2C%20collectively%20undermining%20reasoning%0Aperformance.%20To%20address%20this%20challenge%2C%20we%20propose%20GRAIL%3A%20Graph-Retrieval%0AAugmented%20Interactive%20Learning%2C%20a%20framework%20designed%20to%20interact%20with%0Alarge-scale%20graphs%20for%20retrieval-augmented%20reasoning.%20Specifically%2C%20GRAIL%0Aintegrates%20LLM-guided%20random%20exploration%20with%20path%20filtering%20to%20establish%20a%0Adata%20synthesis%20pipeline%2C%20where%20a%20fine-grained%20reasoning%20trajectory%20is%0Aautomatically%20generated%20for%20each%20task.%20Based%20on%20the%20synthesized%20data%2C%20we%20then%0Aemploy%20a%20two-stage%20training%20process%20to%20learn%20a%20policy%20that%20dynamically%20decides%0Athe%20optimal%20actions%20at%20each%20reasoning%20step.%20The%20overall%20objective%20of%0Aprecision-conciseness%20balance%20in%20graph%20retrieval%20is%20decoupled%20into%20fine-grained%0Aprocess-supervised%20rewards%20to%20enhance%20data%20efficiency%20and%20training%20stability.%0AIn%20practical%20deployment%2C%20GRAIL%20adopts%20an%20interactive%20retrieval%20paradigm%2C%0Aenabling%20the%20model%20to%20autonomously%20explore%20graph%20paths%20while%20dynamically%0Abalancing%20retrieval%20breadth%20and%20precision.%20Extensive%20experiments%20have%20shown%0Athat%20GRAIL%20achieves%20an%20average%20accuracy%20improvement%20of%2021.01%25%20and%20F1%0Aimprovement%20of%2022.43%25%20on%20three%20knowledge%20graph%20question-answering%20datasets.%20Our%0Asource%20code%20and%20datasets%20is%20available%20at%20https%3A//github.com/Changgeww/GRAIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAIL%253ALearning%2520to%2520Interact%2520with%2520Large%2520Knowledge%2520Graphs%2520for%2520Retrieval%250A%2520%2520Augmented%2520Reasoning%26entry.906535625%3DGe%2520Chang%2520and%2520Jinbo%2520Su%2520and%2520Jiacheng%2520Liu%2520and%2520Pengfei%2520Yang%2520and%2520Yuhao%2520Shang%2520and%2520Huiwen%2520Zheng%2520and%2520Hongli%2520Ma%2520and%2520Yan%2520Liang%2520and%2520Yuanchun%2520Li%2520and%2520Yunxin%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520integrated%2520with%2520Retrieval-Augmented%2520Generation%250A%2528RAG%2529%2520techniques%2520have%2520exhibited%2520remarkable%2520performance%2520across%2520a%2520wide%2520range%2520of%250Adomains.%2520However%252C%2520existing%2520RAG%2520approaches%2520primarily%2520operate%2520on%2520unstructured%250Adata%2520and%2520demonstrate%2520limited%2520capability%2520in%2520handling%2520structured%2520knowledge%2520such%250Aas%2520knowledge%2520graphs.%2520Meanwhile%252C%2520current%2520graph%2520retrieval%2520methods%2520fundamentally%250Astruggle%2520to%2520capture%2520holistic%2520graph%2520structures%2520while%2520simultaneously%2520facing%250Aprecision%2520control%2520challenges%2520that%2520manifest%2520as%2520either%2520critical%2520information%2520gaps%250Aor%2520excessive%2520redundant%2520connections%252C%2520collectively%2520undermining%2520reasoning%250Aperformance.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520GRAIL%253A%2520Graph-Retrieval%250AAugmented%2520Interactive%2520Learning%252C%2520a%2520framework%2520designed%2520to%2520interact%2520with%250Alarge-scale%2520graphs%2520for%2520retrieval-augmented%2520reasoning.%2520Specifically%252C%2520GRAIL%250Aintegrates%2520LLM-guided%2520random%2520exploration%2520with%2520path%2520filtering%2520to%2520establish%2520a%250Adata%2520synthesis%2520pipeline%252C%2520where%2520a%2520fine-grained%2520reasoning%2520trajectory%2520is%250Aautomatically%2520generated%2520for%2520each%2520task.%2520Based%2520on%2520the%2520synthesized%2520data%252C%2520we%2520then%250Aemploy%2520a%2520two-stage%2520training%2520process%2520to%2520learn%2520a%2520policy%2520that%2520dynamically%2520decides%250Athe%2520optimal%2520actions%2520at%2520each%2520reasoning%2520step.%2520The%2520overall%2520objective%2520of%250Aprecision-conciseness%2520balance%2520in%2520graph%2520retrieval%2520is%2520decoupled%2520into%2520fine-grained%250Aprocess-supervised%2520rewards%2520to%2520enhance%2520data%2520efficiency%2520and%2520training%2520stability.%250AIn%2520practical%2520deployment%252C%2520GRAIL%2520adopts%2520an%2520interactive%2520retrieval%2520paradigm%252C%250Aenabling%2520the%2520model%2520to%2520autonomously%2520explore%2520graph%2520paths%2520while%2520dynamically%250Abalancing%2520retrieval%2520breadth%2520and%2520precision.%2520Extensive%2520experiments%2520have%2520shown%250Athat%2520GRAIL%2520achieves%2520an%2520average%2520accuracy%2520improvement%2520of%252021.01%2525%2520and%2520F1%250Aimprovement%2520of%252022.43%2525%2520on%2520three%2520knowledge%2520graph%2520question-answering%2520datasets.%2520Our%250Asource%2520code%2520and%2520datasets%2520is%2520available%2520at%2520https%253A//github.com/Changgeww/GRAIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAIL%3ALearning%20to%20Interact%20with%20Large%20Knowledge%20Graphs%20for%20Retrieval%0A%20%20Augmented%20Reasoning&entry.906535625=Ge%20Chang%20and%20Jinbo%20Su%20and%20Jiacheng%20Liu%20and%20Pengfei%20Yang%20and%20Yuhao%20Shang%20and%20Huiwen%20Zheng%20and%20Hongli%20Ma%20and%20Yan%20Liang%20and%20Yuanchun%20Li%20and%20Yunxin%20Liu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20integrated%20with%20Retrieval-Augmented%20Generation%0A%28RAG%29%20techniques%20have%20exhibited%20remarkable%20performance%20across%20a%20wide%20range%20of%0Adomains.%20However%2C%20existing%20RAG%20approaches%20primarily%20operate%20on%20unstructured%0Adata%20and%20demonstrate%20limited%20capability%20in%20handling%20structured%20knowledge%20such%0Aas%20knowledge%20graphs.%20Meanwhile%2C%20current%20graph%20retrieval%20methods%20fundamentally%0Astruggle%20to%20capture%20holistic%20graph%20structures%20while%20simultaneously%20facing%0Aprecision%20control%20challenges%20that%20manifest%20as%20either%20critical%20information%20gaps%0Aor%20excessive%20redundant%20connections%2C%20collectively%20undermining%20reasoning%0Aperformance.%20To%20address%20this%20challenge%2C%20we%20propose%20GRAIL%3A%20Graph-Retrieval%0AAugmented%20Interactive%20Learning%2C%20a%20framework%20designed%20to%20interact%20with%0Alarge-scale%20graphs%20for%20retrieval-augmented%20reasoning.%20Specifically%2C%20GRAIL%0Aintegrates%20LLM-guided%20random%20exploration%20with%20path%20filtering%20to%20establish%20a%0Adata%20synthesis%20pipeline%2C%20where%20a%20fine-grained%20reasoning%20trajectory%20is%0Aautomatically%20generated%20for%20each%20task.%20Based%20on%20the%20synthesized%20data%2C%20we%20then%0Aemploy%20a%20two-stage%20training%20process%20to%20learn%20a%20policy%20that%20dynamically%20decides%0Athe%20optimal%20actions%20at%20each%20reasoning%20step.%20The%20overall%20objective%20of%0Aprecision-conciseness%20balance%20in%20graph%20retrieval%20is%20decoupled%20into%20fine-grained%0Aprocess-supervised%20rewards%20to%20enhance%20data%20efficiency%20and%20training%20stability.%0AIn%20practical%20deployment%2C%20GRAIL%20adopts%20an%20interactive%20retrieval%20paradigm%2C%0Aenabling%20the%20model%20to%20autonomously%20explore%20graph%20paths%20while%20dynamically%0Abalancing%20retrieval%20breadth%20and%20precision.%20Extensive%20experiments%20have%20shown%0Athat%20GRAIL%20achieves%20an%20average%20accuracy%20improvement%20of%2021.01%25%20and%20F1%0Aimprovement%20of%2022.43%25%20on%20three%20knowledge%20graph%20question-answering%20datasets.%20Our%0Asource%20code%20and%20datasets%20is%20available%20at%20https%3A//github.com/Changgeww/GRAIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05498v1&entry.124074799=Read"},
{"title": "Predicting the Lifespan of Industrial Printheads with Survival Analysis", "author": "Dan Parii and Evelyne Janssen and Guangzhi Tang and Charalampos Kouzinopoulos and Marcin Pietrasik", "abstract": "  Accurately predicting the lifespan of critical device components is essential\nfor maintenance planning and production optimization, making it a topic of\nsignificant interest in both academia and industry. In this work, we\ninvestigate the use of survival analysis for predicting the lifespan of\nproduction printheads developed by Canon Production Printing. Specifically, we\nfocus on the application of five techniques to estimate survival probabilities\nand failure rates: the Kaplan-Meier estimator, Cox proportional hazard model,\nWeibull accelerated failure time model, random survival forest, and gradient\nboosting. The resulting estimates are further refined using isotonic regression\nand subsequently aggregated to determine the expected number of failures. The\npredictions are then validated against real-world ground truth data across\nmultiple time windows to assess model reliability. Our quantitative evaluation\nusing three performance metrics demonstrates that survival analysis outperforms\nindustry-standard baseline methods for printhead lifespan prediction.\n", "link": "http://arxiv.org/abs/2504.07638v2", "date": "2025-08-07", "relevancy": 1.1358, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3946}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3861}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20the%20Lifespan%20of%20Industrial%20Printheads%20with%20Survival%20Analysis&body=Title%3A%20Predicting%20the%20Lifespan%20of%20Industrial%20Printheads%20with%20Survival%20Analysis%0AAuthor%3A%20Dan%20Parii%20and%20Evelyne%20Janssen%20and%20Guangzhi%20Tang%20and%20Charalampos%20Kouzinopoulos%20and%20Marcin%20Pietrasik%0AAbstract%3A%20%20%20Accurately%20predicting%20the%20lifespan%20of%20critical%20device%20components%20is%20essential%0Afor%20maintenance%20planning%20and%20production%20optimization%2C%20making%20it%20a%20topic%20of%0Asignificant%20interest%20in%20both%20academia%20and%20industry.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20use%20of%20survival%20analysis%20for%20predicting%20the%20lifespan%20of%0Aproduction%20printheads%20developed%20by%20Canon%20Production%20Printing.%20Specifically%2C%20we%0Afocus%20on%20the%20application%20of%20five%20techniques%20to%20estimate%20survival%20probabilities%0Aand%20failure%20rates%3A%20the%20Kaplan-Meier%20estimator%2C%20Cox%20proportional%20hazard%20model%2C%0AWeibull%20accelerated%20failure%20time%20model%2C%20random%20survival%20forest%2C%20and%20gradient%0Aboosting.%20The%20resulting%20estimates%20are%20further%20refined%20using%20isotonic%20regression%0Aand%20subsequently%20aggregated%20to%20determine%20the%20expected%20number%20of%20failures.%20The%0Apredictions%20are%20then%20validated%20against%20real-world%20ground%20truth%20data%20across%0Amultiple%20time%20windows%20to%20assess%20model%20reliability.%20Our%20quantitative%20evaluation%0Ausing%20three%20performance%20metrics%20demonstrates%20that%20survival%20analysis%20outperforms%0Aindustry-standard%20baseline%20methods%20for%20printhead%20lifespan%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07638v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520the%2520Lifespan%2520of%2520Industrial%2520Printheads%2520with%2520Survival%2520Analysis%26entry.906535625%3DDan%2520Parii%2520and%2520Evelyne%2520Janssen%2520and%2520Guangzhi%2520Tang%2520and%2520Charalampos%2520Kouzinopoulos%2520and%2520Marcin%2520Pietrasik%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520the%2520lifespan%2520of%2520critical%2520device%2520components%2520is%2520essential%250Afor%2520maintenance%2520planning%2520and%2520production%2520optimization%252C%2520making%2520it%2520a%2520topic%2520of%250Asignificant%2520interest%2520in%2520both%2520academia%2520and%2520industry.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520the%2520use%2520of%2520survival%2520analysis%2520for%2520predicting%2520the%2520lifespan%2520of%250Aproduction%2520printheads%2520developed%2520by%2520Canon%2520Production%2520Printing.%2520Specifically%252C%2520we%250Afocus%2520on%2520the%2520application%2520of%2520five%2520techniques%2520to%2520estimate%2520survival%2520probabilities%250Aand%2520failure%2520rates%253A%2520the%2520Kaplan-Meier%2520estimator%252C%2520Cox%2520proportional%2520hazard%2520model%252C%250AWeibull%2520accelerated%2520failure%2520time%2520model%252C%2520random%2520survival%2520forest%252C%2520and%2520gradient%250Aboosting.%2520The%2520resulting%2520estimates%2520are%2520further%2520refined%2520using%2520isotonic%2520regression%250Aand%2520subsequently%2520aggregated%2520to%2520determine%2520the%2520expected%2520number%2520of%2520failures.%2520The%250Apredictions%2520are%2520then%2520validated%2520against%2520real-world%2520ground%2520truth%2520data%2520across%250Amultiple%2520time%2520windows%2520to%2520assess%2520model%2520reliability.%2520Our%2520quantitative%2520evaluation%250Ausing%2520three%2520performance%2520metrics%2520demonstrates%2520that%2520survival%2520analysis%2520outperforms%250Aindustry-standard%2520baseline%2520methods%2520for%2520printhead%2520lifespan%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07638v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20the%20Lifespan%20of%20Industrial%20Printheads%20with%20Survival%20Analysis&entry.906535625=Dan%20Parii%20and%20Evelyne%20Janssen%20and%20Guangzhi%20Tang%20and%20Charalampos%20Kouzinopoulos%20and%20Marcin%20Pietrasik&entry.1292438233=%20%20Accurately%20predicting%20the%20lifespan%20of%20critical%20device%20components%20is%20essential%0Afor%20maintenance%20planning%20and%20production%20optimization%2C%20making%20it%20a%20topic%20of%0Asignificant%20interest%20in%20both%20academia%20and%20industry.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20use%20of%20survival%20analysis%20for%20predicting%20the%20lifespan%20of%0Aproduction%20printheads%20developed%20by%20Canon%20Production%20Printing.%20Specifically%2C%20we%0Afocus%20on%20the%20application%20of%20five%20techniques%20to%20estimate%20survival%20probabilities%0Aand%20failure%20rates%3A%20the%20Kaplan-Meier%20estimator%2C%20Cox%20proportional%20hazard%20model%2C%0AWeibull%20accelerated%20failure%20time%20model%2C%20random%20survival%20forest%2C%20and%20gradient%0Aboosting.%20The%20resulting%20estimates%20are%20further%20refined%20using%20isotonic%20regression%0Aand%20subsequently%20aggregated%20to%20determine%20the%20expected%20number%20of%20failures.%20The%0Apredictions%20are%20then%20validated%20against%20real-world%20ground%20truth%20data%20across%0Amultiple%20time%20windows%20to%20assess%20model%20reliability.%20Our%20quantitative%20evaluation%0Ausing%20three%20performance%20metrics%20demonstrates%20that%20survival%20analysis%20outperforms%0Aindustry-standard%20baseline%20methods%20for%20printhead%20lifespan%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07638v2&entry.124074799=Read"},
{"title": "Learned Single-Pass Multitasking Perceptual Graphics for Immersive\n  Displays", "author": "Do\u011fa Y\u0131lmaz and He Wang and Towaki Takikawa and Duygu Ceylan and Kaan Ak\u015fit", "abstract": "  Emerging immersive display technologies efficiently utilize resources with\nperceptual graphics methods such as foveated rendering and denoising. Running\nmultiple perceptual graphics methods challenges devices with limited power and\ncomputational resources. We propose a computationally-lightweight learned\nmultitasking perceptual graphics model. Given RGB images and text-prompts, our\nmodel performs text-described perceptual tasks in a single inference step.\nSimply daisy-chaining multiple models or training dedicated models can lead to\nmodel management issues and exhaust computational resources. In contrast, our\nflexible method unlocks consistent high quality perceptual effects with\nreasonable compute, supporting various permutations at varied intensities using\nadjectives in text prompts (e.g. mildly, lightly). Text-guidance provides ease\nof use for dynamic requirements such as creative processes. To train our model,\nwe propose a dataset containing source and perceptually enhanced images with\ncorresponding text prompts. We evaluate our model on desktop and embedded\nplatforms and validate perceptual quality through a user study.\n", "link": "http://arxiv.org/abs/2408.07836v2", "date": "2025-08-07", "relevancy": 1.2332, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6436}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6165}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20Single-Pass%20Multitasking%20Perceptual%20Graphics%20for%20Immersive%0A%20%20Displays&body=Title%3A%20Learned%20Single-Pass%20Multitasking%20Perceptual%20Graphics%20for%20Immersive%0A%20%20Displays%0AAuthor%3A%20Do%C4%9Fa%20Y%C4%B1lmaz%20and%20He%20Wang%20and%20Towaki%20Takikawa%20and%20Duygu%20Ceylan%20and%20Kaan%20Ak%C5%9Fit%0AAbstract%3A%20%20%20Emerging%20immersive%20display%20technologies%20efficiently%20utilize%20resources%20with%0Aperceptual%20graphics%20methods%20such%20as%20foveated%20rendering%20and%20denoising.%20Running%0Amultiple%20perceptual%20graphics%20methods%20challenges%20devices%20with%20limited%20power%20and%0Acomputational%20resources.%20We%20propose%20a%20computationally-lightweight%20learned%0Amultitasking%20perceptual%20graphics%20model.%20Given%20RGB%20images%20and%20text-prompts%2C%20our%0Amodel%20performs%20text-described%20perceptual%20tasks%20in%20a%20single%20inference%20step.%0ASimply%20daisy-chaining%20multiple%20models%20or%20training%20dedicated%20models%20can%20lead%20to%0Amodel%20management%20issues%20and%20exhaust%20computational%20resources.%20In%20contrast%2C%20our%0Aflexible%20method%20unlocks%20consistent%20high%20quality%20perceptual%20effects%20with%0Areasonable%20compute%2C%20supporting%20various%20permutations%20at%20varied%20intensities%20using%0Aadjectives%20in%20text%20prompts%20%28e.g.%20mildly%2C%20lightly%29.%20Text-guidance%20provides%20ease%0Aof%20use%20for%20dynamic%20requirements%20such%20as%20creative%20processes.%20To%20train%20our%20model%2C%0Awe%20propose%20a%20dataset%20containing%20source%20and%20perceptually%20enhanced%20images%20with%0Acorresponding%20text%20prompts.%20We%20evaluate%20our%20model%20on%20desktop%20and%20embedded%0Aplatforms%20and%20validate%20perceptual%20quality%20through%20a%20user%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07836v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520Single-Pass%2520Multitasking%2520Perceptual%2520Graphics%2520for%2520Immersive%250A%2520%2520Displays%26entry.906535625%3DDo%25C4%259Fa%2520Y%25C4%25B1lmaz%2520and%2520He%2520Wang%2520and%2520Towaki%2520Takikawa%2520and%2520Duygu%2520Ceylan%2520and%2520Kaan%2520Ak%25C5%259Fit%26entry.1292438233%3D%2520%2520Emerging%2520immersive%2520display%2520technologies%2520efficiently%2520utilize%2520resources%2520with%250Aperceptual%2520graphics%2520methods%2520such%2520as%2520foveated%2520rendering%2520and%2520denoising.%2520Running%250Amultiple%2520perceptual%2520graphics%2520methods%2520challenges%2520devices%2520with%2520limited%2520power%2520and%250Acomputational%2520resources.%2520We%2520propose%2520a%2520computationally-lightweight%2520learned%250Amultitasking%2520perceptual%2520graphics%2520model.%2520Given%2520RGB%2520images%2520and%2520text-prompts%252C%2520our%250Amodel%2520performs%2520text-described%2520perceptual%2520tasks%2520in%2520a%2520single%2520inference%2520step.%250ASimply%2520daisy-chaining%2520multiple%2520models%2520or%2520training%2520dedicated%2520models%2520can%2520lead%2520to%250Amodel%2520management%2520issues%2520and%2520exhaust%2520computational%2520resources.%2520In%2520contrast%252C%2520our%250Aflexible%2520method%2520unlocks%2520consistent%2520high%2520quality%2520perceptual%2520effects%2520with%250Areasonable%2520compute%252C%2520supporting%2520various%2520permutations%2520at%2520varied%2520intensities%2520using%250Aadjectives%2520in%2520text%2520prompts%2520%2528e.g.%2520mildly%252C%2520lightly%2529.%2520Text-guidance%2520provides%2520ease%250Aof%2520use%2520for%2520dynamic%2520requirements%2520such%2520as%2520creative%2520processes.%2520To%2520train%2520our%2520model%252C%250Awe%2520propose%2520a%2520dataset%2520containing%2520source%2520and%2520perceptually%2520enhanced%2520images%2520with%250Acorresponding%2520text%2520prompts.%2520We%2520evaluate%2520our%2520model%2520on%2520desktop%2520and%2520embedded%250Aplatforms%2520and%2520validate%2520perceptual%2520quality%2520through%2520a%2520user%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07836v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Single-Pass%20Multitasking%20Perceptual%20Graphics%20for%20Immersive%0A%20%20Displays&entry.906535625=Do%C4%9Fa%20Y%C4%B1lmaz%20and%20He%20Wang%20and%20Towaki%20Takikawa%20and%20Duygu%20Ceylan%20and%20Kaan%20Ak%C5%9Fit&entry.1292438233=%20%20Emerging%20immersive%20display%20technologies%20efficiently%20utilize%20resources%20with%0Aperceptual%20graphics%20methods%20such%20as%20foveated%20rendering%20and%20denoising.%20Running%0Amultiple%20perceptual%20graphics%20methods%20challenges%20devices%20with%20limited%20power%20and%0Acomputational%20resources.%20We%20propose%20a%20computationally-lightweight%20learned%0Amultitasking%20perceptual%20graphics%20model.%20Given%20RGB%20images%20and%20text-prompts%2C%20our%0Amodel%20performs%20text-described%20perceptual%20tasks%20in%20a%20single%20inference%20step.%0ASimply%20daisy-chaining%20multiple%20models%20or%20training%20dedicated%20models%20can%20lead%20to%0Amodel%20management%20issues%20and%20exhaust%20computational%20resources.%20In%20contrast%2C%20our%0Aflexible%20method%20unlocks%20consistent%20high%20quality%20perceptual%20effects%20with%0Areasonable%20compute%2C%20supporting%20various%20permutations%20at%20varied%20intensities%20using%0Aadjectives%20in%20text%20prompts%20%28e.g.%20mildly%2C%20lightly%29.%20Text-guidance%20provides%20ease%0Aof%20use%20for%20dynamic%20requirements%20such%20as%20creative%20processes.%20To%20train%20our%20model%2C%0Awe%20propose%20a%20dataset%20containing%20source%20and%20perceptually%20enhanced%20images%20with%0Acorresponding%20text%20prompts.%20We%20evaluate%20our%20model%20on%20desktop%20and%20embedded%0Aplatforms%20and%20validate%20perceptual%20quality%20through%20a%20user%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07836v2&entry.124074799=Read"},
{"title": "Parameter-free entropy-regularized multi-view clustering with\n  hierarchical feature selection", "author": "Kristina P. Sinaga and Sara Colantonio and Miin-Shen Yang", "abstract": "  Multi-view clustering faces critical challenges in automatically discovering\npatterns across heterogeneous data while managing high-dimensional features and\neliminating irrelevant information. Traditional approaches suffer from manual\nparameter tuning and lack principled cross-view integration mechanisms. This\nwork introduces two complementary algorithms: AMVFCM-U and AAMVFCM-U, providing\na unified parameter-free framework. Our approach replaces fuzzification\nparameters with entropy regularization terms that enforce adaptive cross-view\nconsensus. The core innovation employs signal-to-noise ratio based\nregularization ($\\delta_j^h = \\frac{\\bar{x}_j^h}{(\\sigma_j^h)^2}$) for\nprincipled feature weighting with convergence guarantees, coupled with\ndual-level entropy terms that automatically balance view and feature\ncontributions. AAMVFCM-U extends this with hierarchical dimensionality\nreduction operating at feature and view levels through adaptive thresholding\n($\\theta^{h^{(t)}} = \\frac{d_h^{(t)}}{n}$). Evaluation across five diverse\nbenchmarks demonstrates superiority over 15 state-of-the-art methods. AAMVFCM-U\nachieves up to 97% computational efficiency gains, reduces dimensionality to\n0.45% of original size, and automatically identifies critical view combinations\nfor optimal pattern discovery.\n", "link": "http://arxiv.org/abs/2508.05504v1", "date": "2025-08-07", "relevancy": 1.942, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4799}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-free%20entropy-regularized%20multi-view%20clustering%20with%0A%20%20hierarchical%20feature%20selection&body=Title%3A%20Parameter-free%20entropy-regularized%20multi-view%20clustering%20with%0A%20%20hierarchical%20feature%20selection%0AAuthor%3A%20Kristina%20P.%20Sinaga%20and%20Sara%20Colantonio%20and%20Miin-Shen%20Yang%0AAbstract%3A%20%20%20Multi-view%20clustering%20faces%20critical%20challenges%20in%20automatically%20discovering%0Apatterns%20across%20heterogeneous%20data%20while%20managing%20high-dimensional%20features%20and%0Aeliminating%20irrelevant%20information.%20Traditional%20approaches%20suffer%20from%20manual%0Aparameter%20tuning%20and%20lack%20principled%20cross-view%20integration%20mechanisms.%20This%0Awork%20introduces%20two%20complementary%20algorithms%3A%20AMVFCM-U%20and%20AAMVFCM-U%2C%20providing%0Aa%20unified%20parameter-free%20framework.%20Our%20approach%20replaces%20fuzzification%0Aparameters%20with%20entropy%20regularization%20terms%20that%20enforce%20adaptive%20cross-view%0Aconsensus.%20The%20core%20innovation%20employs%20signal-to-noise%20ratio%20based%0Aregularization%20%28%24%5Cdelta_j%5Eh%20%3D%20%5Cfrac%7B%5Cbar%7Bx%7D_j%5Eh%7D%7B%28%5Csigma_j%5Eh%29%5E2%7D%24%29%20for%0Aprincipled%20feature%20weighting%20with%20convergence%20guarantees%2C%20coupled%20with%0Adual-level%20entropy%20terms%20that%20automatically%20balance%20view%20and%20feature%0Acontributions.%20AAMVFCM-U%20extends%20this%20with%20hierarchical%20dimensionality%0Areduction%20operating%20at%20feature%20and%20view%20levels%20through%20adaptive%20thresholding%0A%28%24%5Ctheta%5E%7Bh%5E%7B%28t%29%7D%7D%20%3D%20%5Cfrac%7Bd_h%5E%7B%28t%29%7D%7D%7Bn%7D%24%29.%20Evaluation%20across%20five%20diverse%0Abenchmarks%20demonstrates%20superiority%20over%2015%20state-of-the-art%20methods.%20AAMVFCM-U%0Aachieves%20up%20to%2097%25%20computational%20efficiency%20gains%2C%20reduces%20dimensionality%20to%0A0.45%25%20of%20original%20size%2C%20and%20automatically%20identifies%20critical%20view%20combinations%0Afor%20optimal%20pattern%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-free%2520entropy-regularized%2520multi-view%2520clustering%2520with%250A%2520%2520hierarchical%2520feature%2520selection%26entry.906535625%3DKristina%2520P.%2520Sinaga%2520and%2520Sara%2520Colantonio%2520and%2520Miin-Shen%2520Yang%26entry.1292438233%3D%2520%2520Multi-view%2520clustering%2520faces%2520critical%2520challenges%2520in%2520automatically%2520discovering%250Apatterns%2520across%2520heterogeneous%2520data%2520while%2520managing%2520high-dimensional%2520features%2520and%250Aeliminating%2520irrelevant%2520information.%2520Traditional%2520approaches%2520suffer%2520from%2520manual%250Aparameter%2520tuning%2520and%2520lack%2520principled%2520cross-view%2520integration%2520mechanisms.%2520This%250Awork%2520introduces%2520two%2520complementary%2520algorithms%253A%2520AMVFCM-U%2520and%2520AAMVFCM-U%252C%2520providing%250Aa%2520unified%2520parameter-free%2520framework.%2520Our%2520approach%2520replaces%2520fuzzification%250Aparameters%2520with%2520entropy%2520regularization%2520terms%2520that%2520enforce%2520adaptive%2520cross-view%250Aconsensus.%2520The%2520core%2520innovation%2520employs%2520signal-to-noise%2520ratio%2520based%250Aregularization%2520%2528%2524%255Cdelta_j%255Eh%2520%253D%2520%255Cfrac%257B%255Cbar%257Bx%257D_j%255Eh%257D%257B%2528%255Csigma_j%255Eh%2529%255E2%257D%2524%2529%2520for%250Aprincipled%2520feature%2520weighting%2520with%2520convergence%2520guarantees%252C%2520coupled%2520with%250Adual-level%2520entropy%2520terms%2520that%2520automatically%2520balance%2520view%2520and%2520feature%250Acontributions.%2520AAMVFCM-U%2520extends%2520this%2520with%2520hierarchical%2520dimensionality%250Areduction%2520operating%2520at%2520feature%2520and%2520view%2520levels%2520through%2520adaptive%2520thresholding%250A%2528%2524%255Ctheta%255E%257Bh%255E%257B%2528t%2529%257D%257D%2520%253D%2520%255Cfrac%257Bd_h%255E%257B%2528t%2529%257D%257D%257Bn%257D%2524%2529.%2520Evaluation%2520across%2520five%2520diverse%250Abenchmarks%2520demonstrates%2520superiority%2520over%252015%2520state-of-the-art%2520methods.%2520AAMVFCM-U%250Aachieves%2520up%2520to%252097%2525%2520computational%2520efficiency%2520gains%252C%2520reduces%2520dimensionality%2520to%250A0.45%2525%2520of%2520original%2520size%252C%2520and%2520automatically%2520identifies%2520critical%2520view%2520combinations%250Afor%2520optimal%2520pattern%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-free%20entropy-regularized%20multi-view%20clustering%20with%0A%20%20hierarchical%20feature%20selection&entry.906535625=Kristina%20P.%20Sinaga%20and%20Sara%20Colantonio%20and%20Miin-Shen%20Yang&entry.1292438233=%20%20Multi-view%20clustering%20faces%20critical%20challenges%20in%20automatically%20discovering%0Apatterns%20across%20heterogeneous%20data%20while%20managing%20high-dimensional%20features%20and%0Aeliminating%20irrelevant%20information.%20Traditional%20approaches%20suffer%20from%20manual%0Aparameter%20tuning%20and%20lack%20principled%20cross-view%20integration%20mechanisms.%20This%0Awork%20introduces%20two%20complementary%20algorithms%3A%20AMVFCM-U%20and%20AAMVFCM-U%2C%20providing%0Aa%20unified%20parameter-free%20framework.%20Our%20approach%20replaces%20fuzzification%0Aparameters%20with%20entropy%20regularization%20terms%20that%20enforce%20adaptive%20cross-view%0Aconsensus.%20The%20core%20innovation%20employs%20signal-to-noise%20ratio%20based%0Aregularization%20%28%24%5Cdelta_j%5Eh%20%3D%20%5Cfrac%7B%5Cbar%7Bx%7D_j%5Eh%7D%7B%28%5Csigma_j%5Eh%29%5E2%7D%24%29%20for%0Aprincipled%20feature%20weighting%20with%20convergence%20guarantees%2C%20coupled%20with%0Adual-level%20entropy%20terms%20that%20automatically%20balance%20view%20and%20feature%0Acontributions.%20AAMVFCM-U%20extends%20this%20with%20hierarchical%20dimensionality%0Areduction%20operating%20at%20feature%20and%20view%20levels%20through%20adaptive%20thresholding%0A%28%24%5Ctheta%5E%7Bh%5E%7B%28t%29%7D%7D%20%3D%20%5Cfrac%7Bd_h%5E%7B%28t%29%7D%7D%7Bn%7D%24%29.%20Evaluation%20across%20five%20diverse%0Abenchmarks%20demonstrates%20superiority%20over%2015%20state-of-the-art%20methods.%20AAMVFCM-U%0Aachieves%20up%20to%2097%25%20computational%20efficiency%20gains%2C%20reduces%20dimensionality%20to%0A0.45%25%20of%20original%20size%2C%20and%20automatically%20identifies%20critical%20view%20combinations%0Afor%20optimal%20pattern%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05504v1&entry.124074799=Read"},
{"title": "Towards Personalized Conversational Sales Agents: Contextual User\n  Profiling for Strategic Action", "author": "Tongyoung Kim and Jeongeun Lee and Soojin Yoon and Sunghwan Kim and Dongha Lee", "abstract": "  Conversational Recommender Systems (CRSs)aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To capture this complexity, we introduce Conversational\nSales (CSALES), a novel task that integrates preference elicitation,\nrecommendation, and persuasion within a unified conversational framework. To\nsupport realistic and systematic evaluation, we present CSUSER, an evaluation\nprotocol with LLM-based user simulator grounded in real-world behavioral data\nby modeling fine-grained user profiles for personalized interaction. We also\npropose CSI, a conversational sales agent that proactively infers contextual\nuser profiles and strategically selects actions through conversation.\nComprehensive experiments show that CSI significantly improves both\nrecommendation success and persuasive effectiveness across diverse user\nprofiles.\n", "link": "http://arxiv.org/abs/2504.08754v5", "date": "2025-08-07", "relevancy": 1.3379, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.51}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.428}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Personalized%20Conversational%20Sales%20Agents%3A%20Contextual%20User%0A%20%20Profiling%20for%20Strategic%20Action&body=Title%3A%20Towards%20Personalized%20Conversational%20Sales%20Agents%3A%20Contextual%20User%0A%20%20Profiling%20for%20Strategic%20Action%0AAuthor%3A%20Tongyoung%20Kim%20and%20Jeongeun%20Lee%20and%20Soojin%20Yoon%20and%20Sunghwan%20Kim%20and%20Dongha%20Lee%0AAbstract%3A%20%20%20Conversational%20Recommender%20Systems%20%28CRSs%29aim%20to%20engage%20users%20in%20dialogue%20to%0Aprovide%20tailored%20recommendations.%20While%20traditional%20CRSs%20focus%20on%20eliciting%0Apreferences%20and%20retrieving%20items%2C%20real-world%20e-commerce%20interactions%20involve%0Amore%20complex%20decision-making%2C%20where%20users%20consider%20multiple%20factors%20beyond%0Asimple%20attributes.%20To%20capture%20this%20complexity%2C%20we%20introduce%20Conversational%0ASales%20%28CSALES%29%2C%20a%20novel%20task%20that%20integrates%20preference%20elicitation%2C%0Arecommendation%2C%20and%20persuasion%20within%20a%20unified%20conversational%20framework.%20To%0Asupport%20realistic%20and%20systematic%20evaluation%2C%20we%20present%20CSUSER%2C%20an%20evaluation%0Aprotocol%20with%20LLM-based%20user%20simulator%20grounded%20in%20real-world%20behavioral%20data%0Aby%20modeling%20fine-grained%20user%20profiles%20for%20personalized%20interaction.%20We%20also%0Apropose%20CSI%2C%20a%20conversational%20sales%20agent%20that%20proactively%20infers%20contextual%0Auser%20profiles%20and%20strategically%20selects%20actions%20through%20conversation.%0AComprehensive%20experiments%20show%20that%20CSI%20significantly%20improves%20both%0Arecommendation%20success%20and%20persuasive%20effectiveness%20across%20diverse%20user%0Aprofiles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08754v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Personalized%2520Conversational%2520Sales%2520Agents%253A%2520Contextual%2520User%250A%2520%2520Profiling%2520for%2520Strategic%2520Action%26entry.906535625%3DTongyoung%2520Kim%2520and%2520Jeongeun%2520Lee%2520and%2520Soojin%2520Yoon%2520and%2520Sunghwan%2520Kim%2520and%2520Dongha%2520Lee%26entry.1292438233%3D%2520%2520Conversational%2520Recommender%2520Systems%2520%2528CRSs%2529aim%2520to%2520engage%2520users%2520in%2520dialogue%2520to%250Aprovide%2520tailored%2520recommendations.%2520While%2520traditional%2520CRSs%2520focus%2520on%2520eliciting%250Apreferences%2520and%2520retrieving%2520items%252C%2520real-world%2520e-commerce%2520interactions%2520involve%250Amore%2520complex%2520decision-making%252C%2520where%2520users%2520consider%2520multiple%2520factors%2520beyond%250Asimple%2520attributes.%2520To%2520capture%2520this%2520complexity%252C%2520we%2520introduce%2520Conversational%250ASales%2520%2528CSALES%2529%252C%2520a%2520novel%2520task%2520that%2520integrates%2520preference%2520elicitation%252C%250Arecommendation%252C%2520and%2520persuasion%2520within%2520a%2520unified%2520conversational%2520framework.%2520To%250Asupport%2520realistic%2520and%2520systematic%2520evaluation%252C%2520we%2520present%2520CSUSER%252C%2520an%2520evaluation%250Aprotocol%2520with%2520LLM-based%2520user%2520simulator%2520grounded%2520in%2520real-world%2520behavioral%2520data%250Aby%2520modeling%2520fine-grained%2520user%2520profiles%2520for%2520personalized%2520interaction.%2520We%2520also%250Apropose%2520CSI%252C%2520a%2520conversational%2520sales%2520agent%2520that%2520proactively%2520infers%2520contextual%250Auser%2520profiles%2520and%2520strategically%2520selects%2520actions%2520through%2520conversation.%250AComprehensive%2520experiments%2520show%2520that%2520CSI%2520significantly%2520improves%2520both%250Arecommendation%2520success%2520and%2520persuasive%2520effectiveness%2520across%2520diverse%2520user%250Aprofiles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08754v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Personalized%20Conversational%20Sales%20Agents%3A%20Contextual%20User%0A%20%20Profiling%20for%20Strategic%20Action&entry.906535625=Tongyoung%20Kim%20and%20Jeongeun%20Lee%20and%20Soojin%20Yoon%20and%20Sunghwan%20Kim%20and%20Dongha%20Lee&entry.1292438233=%20%20Conversational%20Recommender%20Systems%20%28CRSs%29aim%20to%20engage%20users%20in%20dialogue%20to%0Aprovide%20tailored%20recommendations.%20While%20traditional%20CRSs%20focus%20on%20eliciting%0Apreferences%20and%20retrieving%20items%2C%20real-world%20e-commerce%20interactions%20involve%0Amore%20complex%20decision-making%2C%20where%20users%20consider%20multiple%20factors%20beyond%0Asimple%20attributes.%20To%20capture%20this%20complexity%2C%20we%20introduce%20Conversational%0ASales%20%28CSALES%29%2C%20a%20novel%20task%20that%20integrates%20preference%20elicitation%2C%0Arecommendation%2C%20and%20persuasion%20within%20a%20unified%20conversational%20framework.%20To%0Asupport%20realistic%20and%20systematic%20evaluation%2C%20we%20present%20CSUSER%2C%20an%20evaluation%0Aprotocol%20with%20LLM-based%20user%20simulator%20grounded%20in%20real-world%20behavioral%20data%0Aby%20modeling%20fine-grained%20user%20profiles%20for%20personalized%20interaction.%20We%20also%0Apropose%20CSI%2C%20a%20conversational%20sales%20agent%20that%20proactively%20infers%20contextual%0Auser%20profiles%20and%20strategically%20selects%20actions%20through%20conversation.%0AComprehensive%20experiments%20show%20that%20CSI%20significantly%20improves%20both%0Arecommendation%20success%20and%20persuasive%20effectiveness%20across%20diverse%20user%0Aprofiles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08754v5&entry.124074799=Read"},
{"title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "author": "Shangke Lyu and Linjuan Wu and Yuchen Yan and Xingyu Wu and Hao Li and Yongliang Shen and Peisheng Jiang and Weiming Lu and Jun Xiao and Yueting Zhuang", "abstract": "  Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet they suffer from a critical inefficiency:\napplying uniformly extensive reasoning regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. Unlike existing approaches that impose\nrigid constraints or rely on discrete mode selection, HBPO partitions the\nexploration space into budget-constrained hierarchies (512-2560 tokens), each\nwith differentiated reward structures that preserve both efficiency incentives\nand reasoning capabilities. This design addresses a fundamental challenge in\nefficient reasoning training: traditional length penalties systematically bias\nmodels away from necessary long reasoning paths, causing exploration space\ncollapse. Through hierarchical sampling and budget-aware rewards, HBPO\nmaintains exploration diversity while teaching models to recognize when\nextended deliberation is warranted. Extensive experiments demonstrate that HBPO\nreduces average token usage by up to 60.6% while improving accuracy by 3.14%\nacross four reasoning benchmarks. Most notably, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.\n", "link": "http://arxiv.org/abs/2507.15844v3", "date": "2025-08-07", "relevancy": 1.4422, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.492}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4868}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Budget%20Policy%20Optimization%20for%20Adaptive%20Reasoning&body=Title%3A%20Hierarchical%20Budget%20Policy%20Optimization%20for%20Adaptive%20Reasoning%0AAuthor%3A%20Shangke%20Lyu%20and%20Linjuan%20Wu%20and%20Yuchen%20Yan%20and%20Xingyu%20Wu%20and%20Hao%20Li%20and%20Yongliang%20Shen%20and%20Peisheng%20Jiang%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Large%20reasoning%20models%20achieve%20remarkable%20performance%20through%20extensive%0Achain-of-thought%20generation%2C%20yet%20they%20suffer%20from%20a%20critical%20inefficiency%3A%0Aapplying%20uniformly%20extensive%20reasoning%20regardless%20of%20problem%20complexity.%20We%0Apresent%20Hierarchical%20Budget%20Policy%20Optimization%20%28HBPO%29%2C%20a%20reinforcement%0Alearning%20framework%20that%20enables%20models%20to%20learn%20problem-specific%20reasoning%0Adepths%20without%20sacrificing%20capability.%20Unlike%20existing%20approaches%20that%20impose%0Arigid%20constraints%20or%20rely%20on%20discrete%20mode%20selection%2C%20HBPO%20partitions%20the%0Aexploration%20space%20into%20budget-constrained%20hierarchies%20%28512-2560%20tokens%29%2C%20each%0Awith%20differentiated%20reward%20structures%20that%20preserve%20both%20efficiency%20incentives%0Aand%20reasoning%20capabilities.%20This%20design%20addresses%20a%20fundamental%20challenge%20in%0Aefficient%20reasoning%20training%3A%20traditional%20length%20penalties%20systematically%20bias%0Amodels%20away%20from%20necessary%20long%20reasoning%20paths%2C%20causing%20exploration%20space%0Acollapse.%20Through%20hierarchical%20sampling%20and%20budget-aware%20rewards%2C%20HBPO%0Amaintains%20exploration%20diversity%20while%20teaching%20models%20to%20recognize%20when%0Aextended%20deliberation%20is%20warranted.%20Extensive%20experiments%20demonstrate%20that%20HBPO%0Areduces%20average%20token%20usage%20by%20up%20to%2060.6%25%20while%20improving%20accuracy%20by%203.14%25%0Aacross%20four%20reasoning%20benchmarks.%20Most%20notably%2C%20HBPO%20exhibits%20emergent%20adaptive%0Abehavior%20where%20models%20automatically%20adjust%20reasoning%20depth%20based%20on%20problem%0Acomplexity.%20Our%20results%20suggest%20that%20reasoning%20efficiency%20and%20capability%20are%0Anot%20inherently%20conflicting%2C%20and%20can%20be%20simultaneously%20optimized%20through%0Aappropriately%20structured%20hierarchical%20training%20that%20preserves%20exploration%0Adiversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15844v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Budget%2520Policy%2520Optimization%2520for%2520Adaptive%2520Reasoning%26entry.906535625%3DShangke%2520Lyu%2520and%2520Linjuan%2520Wu%2520and%2520Yuchen%2520Yan%2520and%2520Xingyu%2520Wu%2520and%2520Hao%2520Li%2520and%2520Yongliang%2520Shen%2520and%2520Peisheng%2520Jiang%2520and%2520Weiming%2520Lu%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520achieve%2520remarkable%2520performance%2520through%2520extensive%250Achain-of-thought%2520generation%252C%2520yet%2520they%2520suffer%2520from%2520a%2520critical%2520inefficiency%253A%250Aapplying%2520uniformly%2520extensive%2520reasoning%2520regardless%2520of%2520problem%2520complexity.%2520We%250Apresent%2520Hierarchical%2520Budget%2520Policy%2520Optimization%2520%2528HBPO%2529%252C%2520a%2520reinforcement%250Alearning%2520framework%2520that%2520enables%2520models%2520to%2520learn%2520problem-specific%2520reasoning%250Adepths%2520without%2520sacrificing%2520capability.%2520Unlike%2520existing%2520approaches%2520that%2520impose%250Arigid%2520constraints%2520or%2520rely%2520on%2520discrete%2520mode%2520selection%252C%2520HBPO%2520partitions%2520the%250Aexploration%2520space%2520into%2520budget-constrained%2520hierarchies%2520%2528512-2560%2520tokens%2529%252C%2520each%250Awith%2520differentiated%2520reward%2520structures%2520that%2520preserve%2520both%2520efficiency%2520incentives%250Aand%2520reasoning%2520capabilities.%2520This%2520design%2520addresses%2520a%2520fundamental%2520challenge%2520in%250Aefficient%2520reasoning%2520training%253A%2520traditional%2520length%2520penalties%2520systematically%2520bias%250Amodels%2520away%2520from%2520necessary%2520long%2520reasoning%2520paths%252C%2520causing%2520exploration%2520space%250Acollapse.%2520Through%2520hierarchical%2520sampling%2520and%2520budget-aware%2520rewards%252C%2520HBPO%250Amaintains%2520exploration%2520diversity%2520while%2520teaching%2520models%2520to%2520recognize%2520when%250Aextended%2520deliberation%2520is%2520warranted.%2520Extensive%2520experiments%2520demonstrate%2520that%2520HBPO%250Areduces%2520average%2520token%2520usage%2520by%2520up%2520to%252060.6%2525%2520while%2520improving%2520accuracy%2520by%25203.14%2525%250Aacross%2520four%2520reasoning%2520benchmarks.%2520Most%2520notably%252C%2520HBPO%2520exhibits%2520emergent%2520adaptive%250Abehavior%2520where%2520models%2520automatically%2520adjust%2520reasoning%2520depth%2520based%2520on%2520problem%250Acomplexity.%2520Our%2520results%2520suggest%2520that%2520reasoning%2520efficiency%2520and%2520capability%2520are%250Anot%2520inherently%2520conflicting%252C%2520and%2520can%2520be%2520simultaneously%2520optimized%2520through%250Aappropriately%2520structured%2520hierarchical%2520training%2520that%2520preserves%2520exploration%250Adiversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15844v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Budget%20Policy%20Optimization%20for%20Adaptive%20Reasoning&entry.906535625=Shangke%20Lyu%20and%20Linjuan%20Wu%20and%20Yuchen%20Yan%20and%20Xingyu%20Wu%20and%20Hao%20Li%20and%20Yongliang%20Shen%20and%20Peisheng%20Jiang%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Large%20reasoning%20models%20achieve%20remarkable%20performance%20through%20extensive%0Achain-of-thought%20generation%2C%20yet%20they%20suffer%20from%20a%20critical%20inefficiency%3A%0Aapplying%20uniformly%20extensive%20reasoning%20regardless%20of%20problem%20complexity.%20We%0Apresent%20Hierarchical%20Budget%20Policy%20Optimization%20%28HBPO%29%2C%20a%20reinforcement%0Alearning%20framework%20that%20enables%20models%20to%20learn%20problem-specific%20reasoning%0Adepths%20without%20sacrificing%20capability.%20Unlike%20existing%20approaches%20that%20impose%0Arigid%20constraints%20or%20rely%20on%20discrete%20mode%20selection%2C%20HBPO%20partitions%20the%0Aexploration%20space%20into%20budget-constrained%20hierarchies%20%28512-2560%20tokens%29%2C%20each%0Awith%20differentiated%20reward%20structures%20that%20preserve%20both%20efficiency%20incentives%0Aand%20reasoning%20capabilities.%20This%20design%20addresses%20a%20fundamental%20challenge%20in%0Aefficient%20reasoning%20training%3A%20traditional%20length%20penalties%20systematically%20bias%0Amodels%20away%20from%20necessary%20long%20reasoning%20paths%2C%20causing%20exploration%20space%0Acollapse.%20Through%20hierarchical%20sampling%20and%20budget-aware%20rewards%2C%20HBPO%0Amaintains%20exploration%20diversity%20while%20teaching%20models%20to%20recognize%20when%0Aextended%20deliberation%20is%20warranted.%20Extensive%20experiments%20demonstrate%20that%20HBPO%0Areduces%20average%20token%20usage%20by%20up%20to%2060.6%25%20while%20improving%20accuracy%20by%203.14%25%0Aacross%20four%20reasoning%20benchmarks.%20Most%20notably%2C%20HBPO%20exhibits%20emergent%20adaptive%0Abehavior%20where%20models%20automatically%20adjust%20reasoning%20depth%20based%20on%20problem%0Acomplexity.%20Our%20results%20suggest%20that%20reasoning%20efficiency%20and%20capability%20are%0Anot%20inherently%20conflicting%2C%20and%20can%20be%20simultaneously%20optimized%20through%0Aappropriately%20structured%20hierarchical%20training%20that%20preserves%20exploration%0Adiversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15844v3&entry.124074799=Read"},
{"title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation", "author": "Jungkoo Kang", "abstract": "  Effective agent performance relies on the ability to compose tools and agents\ninto effective workflows. However, progress in Large Language Model (LLM)\nplanning and reasoning is limited by the scarcity of scalable, reliable\nevaluation data. This study addresses this limitation by identifying a suitable\nworkflow domain for LLM application. I introduce NL2Flow, a fully automated\nsystem for parametrically generating planning problems, which are expressed in\nnatural language, a structured intermediate representation, and formal PDDL,\nand rigorously evaluating the quality of generated plans. NL2Flow generates a\ndataset of 2296 low-difficulty problems in automated workflow generation and\nevaluates multiple open-sourced, instruct-tuned LLMs without task-specific\noptimization or architectural modifications. Results reveal that the highest\nperforming model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans, specifically for problems with feasible plans.\nRegression analysis shows that the influence of problem characteristics on plan\ngeneration is contingent on both model and prompt design. To investigate the\npotential of LLMs as natural language-to-JSON translators for workflow\ndefinition, and to facilitate integration with downstream symbolic computation\ntools and a symbolic planner, I evaluated the LLM's translation performance on\nnatural language workflow descriptions. I observed that translating natural\nlanguage into a JSON representation of a workflow problem yielded a lower\nsuccess rate than generating a plan directly, suggesting that unnecessary\ndecomposition of the reasoning task may degrade performance and highlighting\nthe benefit of models capable of reasoning directly from natural language to\naction. As LLM reasoning scales to increasingly complex problems, understanding\nthe shifting bottlenecks and sources of error within these systems will be\ncrucial.\n", "link": "http://arxiv.org/abs/2507.02253v3", "date": "2025-08-07", "relevancy": 2.0002, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20LLM%20Planning%3A%20NL2FLOW%20for%20Parametric%20Problem%20Generation%20and%0A%20%20Rigorous%20Evaluation&body=Title%3A%20Scaling%20LLM%20Planning%3A%20NL2FLOW%20for%20Parametric%20Problem%20Generation%20and%0A%20%20Rigorous%20Evaluation%0AAuthor%3A%20Jungkoo%20Kang%0AAbstract%3A%20%20%20Effective%20agent%20performance%20relies%20on%20the%20ability%20to%20compose%20tools%20and%20agents%0Ainto%20effective%20workflows.%20However%2C%20progress%20in%20Large%20Language%20Model%20%28LLM%29%0Aplanning%20and%20reasoning%20is%20limited%20by%20the%20scarcity%20of%20scalable%2C%20reliable%0Aevaluation%20data.%20This%20study%20addresses%20this%20limitation%20by%20identifying%20a%20suitable%0Aworkflow%20domain%20for%20LLM%20application.%20I%20introduce%20NL2Flow%2C%20a%20fully%20automated%0Asystem%20for%20parametrically%20generating%20planning%20problems%2C%20which%20are%20expressed%20in%0Anatural%20language%2C%20a%20structured%20intermediate%20representation%2C%20and%20formal%20PDDL%2C%0Aand%20rigorously%20evaluating%20the%20quality%20of%20generated%20plans.%20NL2Flow%20generates%20a%0Adataset%20of%202296%20low-difficulty%20problems%20in%20automated%20workflow%20generation%20and%0Aevaluates%20multiple%20open-sourced%2C%20instruct-tuned%20LLMs%20without%20task-specific%0Aoptimization%20or%20architectural%20modifications.%20Results%20reveal%20that%20the%20highest%0Aperforming%20model%20achieved%2086%25%20success%20in%20generating%20valid%20plans%20and%2069%25%20in%0Agenerating%20optimal%20plans%2C%20specifically%20for%20problems%20with%20feasible%20plans.%0ARegression%20analysis%20shows%20that%20the%20influence%20of%20problem%20characteristics%20on%20plan%0Ageneration%20is%20contingent%20on%20both%20model%20and%20prompt%20design.%20To%20investigate%20the%0Apotential%20of%20LLMs%20as%20natural%20language-to-JSON%20translators%20for%20workflow%0Adefinition%2C%20and%20to%20facilitate%20integration%20with%20downstream%20symbolic%20computation%0Atools%20and%20a%20symbolic%20planner%2C%20I%20evaluated%20the%20LLM%27s%20translation%20performance%20on%0Anatural%20language%20workflow%20descriptions.%20I%20observed%20that%20translating%20natural%0Alanguage%20into%20a%20JSON%20representation%20of%20a%20workflow%20problem%20yielded%20a%20lower%0Asuccess%20rate%20than%20generating%20a%20plan%20directly%2C%20suggesting%20that%20unnecessary%0Adecomposition%20of%20the%20reasoning%20task%20may%20degrade%20performance%20and%20highlighting%0Athe%20benefit%20of%20models%20capable%20of%20reasoning%20directly%20from%20natural%20language%20to%0Aaction.%20As%20LLM%20reasoning%20scales%20to%20increasingly%20complex%20problems%2C%20understanding%0Athe%20shifting%20bottlenecks%20and%20sources%20of%20error%20within%20these%20systems%20will%20be%0Acrucial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02253v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520LLM%2520Planning%253A%2520NL2FLOW%2520for%2520Parametric%2520Problem%2520Generation%2520and%250A%2520%2520Rigorous%2520Evaluation%26entry.906535625%3DJungkoo%2520Kang%26entry.1292438233%3D%2520%2520Effective%2520agent%2520performance%2520relies%2520on%2520the%2520ability%2520to%2520compose%2520tools%2520and%2520agents%250Ainto%2520effective%2520workflows.%2520However%252C%2520progress%2520in%2520Large%2520Language%2520Model%2520%2528LLM%2529%250Aplanning%2520and%2520reasoning%2520is%2520limited%2520by%2520the%2520scarcity%2520of%2520scalable%252C%2520reliable%250Aevaluation%2520data.%2520This%2520study%2520addresses%2520this%2520limitation%2520by%2520identifying%2520a%2520suitable%250Aworkflow%2520domain%2520for%2520LLM%2520application.%2520I%2520introduce%2520NL2Flow%252C%2520a%2520fully%2520automated%250Asystem%2520for%2520parametrically%2520generating%2520planning%2520problems%252C%2520which%2520are%2520expressed%2520in%250Anatural%2520language%252C%2520a%2520structured%2520intermediate%2520representation%252C%2520and%2520formal%2520PDDL%252C%250Aand%2520rigorously%2520evaluating%2520the%2520quality%2520of%2520generated%2520plans.%2520NL2Flow%2520generates%2520a%250Adataset%2520of%25202296%2520low-difficulty%2520problems%2520in%2520automated%2520workflow%2520generation%2520and%250Aevaluates%2520multiple%2520open-sourced%252C%2520instruct-tuned%2520LLMs%2520without%2520task-specific%250Aoptimization%2520or%2520architectural%2520modifications.%2520Results%2520reveal%2520that%2520the%2520highest%250Aperforming%2520model%2520achieved%252086%2525%2520success%2520in%2520generating%2520valid%2520plans%2520and%252069%2525%2520in%250Agenerating%2520optimal%2520plans%252C%2520specifically%2520for%2520problems%2520with%2520feasible%2520plans.%250ARegression%2520analysis%2520shows%2520that%2520the%2520influence%2520of%2520problem%2520characteristics%2520on%2520plan%250Ageneration%2520is%2520contingent%2520on%2520both%2520model%2520and%2520prompt%2520design.%2520To%2520investigate%2520the%250Apotential%2520of%2520LLMs%2520as%2520natural%2520language-to-JSON%2520translators%2520for%2520workflow%250Adefinition%252C%2520and%2520to%2520facilitate%2520integration%2520with%2520downstream%2520symbolic%2520computation%250Atools%2520and%2520a%2520symbolic%2520planner%252C%2520I%2520evaluated%2520the%2520LLM%2527s%2520translation%2520performance%2520on%250Anatural%2520language%2520workflow%2520descriptions.%2520I%2520observed%2520that%2520translating%2520natural%250Alanguage%2520into%2520a%2520JSON%2520representation%2520of%2520a%2520workflow%2520problem%2520yielded%2520a%2520lower%250Asuccess%2520rate%2520than%2520generating%2520a%2520plan%2520directly%252C%2520suggesting%2520that%2520unnecessary%250Adecomposition%2520of%2520the%2520reasoning%2520task%2520may%2520degrade%2520performance%2520and%2520highlighting%250Athe%2520benefit%2520of%2520models%2520capable%2520of%2520reasoning%2520directly%2520from%2520natural%2520language%2520to%250Aaction.%2520As%2520LLM%2520reasoning%2520scales%2520to%2520increasingly%2520complex%2520problems%252C%2520understanding%250Athe%2520shifting%2520bottlenecks%2520and%2520sources%2520of%2520error%2520within%2520these%2520systems%2520will%2520be%250Acrucial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02253v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20LLM%20Planning%3A%20NL2FLOW%20for%20Parametric%20Problem%20Generation%20and%0A%20%20Rigorous%20Evaluation&entry.906535625=Jungkoo%20Kang&entry.1292438233=%20%20Effective%20agent%20performance%20relies%20on%20the%20ability%20to%20compose%20tools%20and%20agents%0Ainto%20effective%20workflows.%20However%2C%20progress%20in%20Large%20Language%20Model%20%28LLM%29%0Aplanning%20and%20reasoning%20is%20limited%20by%20the%20scarcity%20of%20scalable%2C%20reliable%0Aevaluation%20data.%20This%20study%20addresses%20this%20limitation%20by%20identifying%20a%20suitable%0Aworkflow%20domain%20for%20LLM%20application.%20I%20introduce%20NL2Flow%2C%20a%20fully%20automated%0Asystem%20for%20parametrically%20generating%20planning%20problems%2C%20which%20are%20expressed%20in%0Anatural%20language%2C%20a%20structured%20intermediate%20representation%2C%20and%20formal%20PDDL%2C%0Aand%20rigorously%20evaluating%20the%20quality%20of%20generated%20plans.%20NL2Flow%20generates%20a%0Adataset%20of%202296%20low-difficulty%20problems%20in%20automated%20workflow%20generation%20and%0Aevaluates%20multiple%20open-sourced%2C%20instruct-tuned%20LLMs%20without%20task-specific%0Aoptimization%20or%20architectural%20modifications.%20Results%20reveal%20that%20the%20highest%0Aperforming%20model%20achieved%2086%25%20success%20in%20generating%20valid%20plans%20and%2069%25%20in%0Agenerating%20optimal%20plans%2C%20specifically%20for%20problems%20with%20feasible%20plans.%0ARegression%20analysis%20shows%20that%20the%20influence%20of%20problem%20characteristics%20on%20plan%0Ageneration%20is%20contingent%20on%20both%20model%20and%20prompt%20design.%20To%20investigate%20the%0Apotential%20of%20LLMs%20as%20natural%20language-to-JSON%20translators%20for%20workflow%0Adefinition%2C%20and%20to%20facilitate%20integration%20with%20downstream%20symbolic%20computation%0Atools%20and%20a%20symbolic%20planner%2C%20I%20evaluated%20the%20LLM%27s%20translation%20performance%20on%0Anatural%20language%20workflow%20descriptions.%20I%20observed%20that%20translating%20natural%0Alanguage%20into%20a%20JSON%20representation%20of%20a%20workflow%20problem%20yielded%20a%20lower%0Asuccess%20rate%20than%20generating%20a%20plan%20directly%2C%20suggesting%20that%20unnecessary%0Adecomposition%20of%20the%20reasoning%20task%20may%20degrade%20performance%20and%20highlighting%0Athe%20benefit%20of%20models%20capable%20of%20reasoning%20directly%20from%20natural%20language%20to%0Aaction.%20As%20LLM%20reasoning%20scales%20to%20increasingly%20complex%20problems%2C%20understanding%0Athe%20shifting%20bottlenecks%20and%20sources%20of%20error%20within%20these%20systems%20will%20be%0Acrucial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02253v3&entry.124074799=Read"},
{"title": "SpectrumWorld: Artificial Intelligence Foundation for Spectroscopy", "author": "Zhuo Yang and Jiaqing Xie and Shuaike Shen and Daolang Wang and Yeyun Chen and Ben Gao and Shuzhou Sun and Biqing Qi and Dongzhan Zhou and Lei Bai and Linjiang Chen and Shufei Zhang and Jun Jiang and Tianfan Fu and Yuqiang Li", "abstract": "  Deep learning holds immense promise for spectroscopy, yet research and\nevaluation in this emerging field often lack standardized formulations. To\naddress this issue, we introduce SpectrumLab, a pioneering unified platform\ndesigned to systematize and accelerate deep learning research in spectroscopy.\nSpectrumLab integrates three core components: a comprehensive Python library\nfeaturing essential data processing and evaluation tools, along with\nleaderboards; an innovative SpectrumAnnotator module that generates\nhigh-quality benchmarks from limited seed data; and SpectrumBench, a\nmulti-layered benchmark suite covering 14 spectroscopic tasks and over 10\nspectrum types, featuring spectra curated from over 1.2 million distinct\nchemical substances. Thorough empirical studies on SpectrumBench with 18\ncutting-edge multimodal LLMs reveal critical limitations of current approaches.\nWe hope SpectrumLab will serve as a crucial foundation for future advancements\nin deep learning-driven spectroscopy.\n", "link": "http://arxiv.org/abs/2508.01188v3", "date": "2025-08-07", "relevancy": 2.0387, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpectrumWorld%3A%20Artificial%20Intelligence%20Foundation%20for%20Spectroscopy&body=Title%3A%20SpectrumWorld%3A%20Artificial%20Intelligence%20Foundation%20for%20Spectroscopy%0AAuthor%3A%20Zhuo%20Yang%20and%20Jiaqing%20Xie%20and%20Shuaike%20Shen%20and%20Daolang%20Wang%20and%20Yeyun%20Chen%20and%20Ben%20Gao%20and%20Shuzhou%20Sun%20and%20Biqing%20Qi%20and%20Dongzhan%20Zhou%20and%20Lei%20Bai%20and%20Linjiang%20Chen%20and%20Shufei%20Zhang%20and%20Jun%20Jiang%20and%20Tianfan%20Fu%20and%20Yuqiang%20Li%0AAbstract%3A%20%20%20Deep%20learning%20holds%20immense%20promise%20for%20spectroscopy%2C%20yet%20research%20and%0Aevaluation%20in%20this%20emerging%20field%20often%20lack%20standardized%20formulations.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20SpectrumLab%2C%20a%20pioneering%20unified%20platform%0Adesigned%20to%20systematize%20and%20accelerate%20deep%20learning%20research%20in%20spectroscopy.%0ASpectrumLab%20integrates%20three%20core%20components%3A%20a%20comprehensive%20Python%20library%0Afeaturing%20essential%20data%20processing%20and%20evaluation%20tools%2C%20along%20with%0Aleaderboards%3B%20an%20innovative%20SpectrumAnnotator%20module%20that%20generates%0Ahigh-quality%20benchmarks%20from%20limited%20seed%20data%3B%20and%20SpectrumBench%2C%20a%0Amulti-layered%20benchmark%20suite%20covering%2014%20spectroscopic%20tasks%20and%20over%2010%0Aspectrum%20types%2C%20featuring%20spectra%20curated%20from%20over%201.2%20million%20distinct%0Achemical%20substances.%20Thorough%20empirical%20studies%20on%20SpectrumBench%20with%2018%0Acutting-edge%20multimodal%20LLMs%20reveal%20critical%20limitations%20of%20current%20approaches.%0AWe%20hope%20SpectrumLab%20will%20serve%20as%20a%20crucial%20foundation%20for%20future%20advancements%0Ain%20deep%20learning-driven%20spectroscopy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01188v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectrumWorld%253A%2520Artificial%2520Intelligence%2520Foundation%2520for%2520Spectroscopy%26entry.906535625%3DZhuo%2520Yang%2520and%2520Jiaqing%2520Xie%2520and%2520Shuaike%2520Shen%2520and%2520Daolang%2520Wang%2520and%2520Yeyun%2520Chen%2520and%2520Ben%2520Gao%2520and%2520Shuzhou%2520Sun%2520and%2520Biqing%2520Qi%2520and%2520Dongzhan%2520Zhou%2520and%2520Lei%2520Bai%2520and%2520Linjiang%2520Chen%2520and%2520Shufei%2520Zhang%2520and%2520Jun%2520Jiang%2520and%2520Tianfan%2520Fu%2520and%2520Yuqiang%2520Li%26entry.1292438233%3D%2520%2520Deep%2520learning%2520holds%2520immense%2520promise%2520for%2520spectroscopy%252C%2520yet%2520research%2520and%250Aevaluation%2520in%2520this%2520emerging%2520field%2520often%2520lack%2520standardized%2520formulations.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520introduce%2520SpectrumLab%252C%2520a%2520pioneering%2520unified%2520platform%250Adesigned%2520to%2520systematize%2520and%2520accelerate%2520deep%2520learning%2520research%2520in%2520spectroscopy.%250ASpectrumLab%2520integrates%2520three%2520core%2520components%253A%2520a%2520comprehensive%2520Python%2520library%250Afeaturing%2520essential%2520data%2520processing%2520and%2520evaluation%2520tools%252C%2520along%2520with%250Aleaderboards%253B%2520an%2520innovative%2520SpectrumAnnotator%2520module%2520that%2520generates%250Ahigh-quality%2520benchmarks%2520from%2520limited%2520seed%2520data%253B%2520and%2520SpectrumBench%252C%2520a%250Amulti-layered%2520benchmark%2520suite%2520covering%252014%2520spectroscopic%2520tasks%2520and%2520over%252010%250Aspectrum%2520types%252C%2520featuring%2520spectra%2520curated%2520from%2520over%25201.2%2520million%2520distinct%250Achemical%2520substances.%2520Thorough%2520empirical%2520studies%2520on%2520SpectrumBench%2520with%252018%250Acutting-edge%2520multimodal%2520LLMs%2520reveal%2520critical%2520limitations%2520of%2520current%2520approaches.%250AWe%2520hope%2520SpectrumLab%2520will%2520serve%2520as%2520a%2520crucial%2520foundation%2520for%2520future%2520advancements%250Ain%2520deep%2520learning-driven%2520spectroscopy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01188v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectrumWorld%3A%20Artificial%20Intelligence%20Foundation%20for%20Spectroscopy&entry.906535625=Zhuo%20Yang%20and%20Jiaqing%20Xie%20and%20Shuaike%20Shen%20and%20Daolang%20Wang%20and%20Yeyun%20Chen%20and%20Ben%20Gao%20and%20Shuzhou%20Sun%20and%20Biqing%20Qi%20and%20Dongzhan%20Zhou%20and%20Lei%20Bai%20and%20Linjiang%20Chen%20and%20Shufei%20Zhang%20and%20Jun%20Jiang%20and%20Tianfan%20Fu%20and%20Yuqiang%20Li&entry.1292438233=%20%20Deep%20learning%20holds%20immense%20promise%20for%20spectroscopy%2C%20yet%20research%20and%0Aevaluation%20in%20this%20emerging%20field%20often%20lack%20standardized%20formulations.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20SpectrumLab%2C%20a%20pioneering%20unified%20platform%0Adesigned%20to%20systematize%20and%20accelerate%20deep%20learning%20research%20in%20spectroscopy.%0ASpectrumLab%20integrates%20three%20core%20components%3A%20a%20comprehensive%20Python%20library%0Afeaturing%20essential%20data%20processing%20and%20evaluation%20tools%2C%20along%20with%0Aleaderboards%3B%20an%20innovative%20SpectrumAnnotator%20module%20that%20generates%0Ahigh-quality%20benchmarks%20from%20limited%20seed%20data%3B%20and%20SpectrumBench%2C%20a%0Amulti-layered%20benchmark%20suite%20covering%2014%20spectroscopic%20tasks%20and%20over%2010%0Aspectrum%20types%2C%20featuring%20spectra%20curated%20from%20over%201.2%20million%20distinct%0Achemical%20substances.%20Thorough%20empirical%20studies%20on%20SpectrumBench%20with%2018%0Acutting-edge%20multimodal%20LLMs%20reveal%20critical%20limitations%20of%20current%20approaches.%0AWe%20hope%20SpectrumLab%20will%20serve%20as%20a%20crucial%20foundation%20for%20future%20advancements%0Ain%20deep%20learning-driven%20spectroscopy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01188v3&entry.124074799=Read"},
{"title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM\n  Agents", "author": "Andrew Kiruluta", "abstract": "  We propose a hybrid architecture that integrates decision tree-based symbolic\nreasoning with the generative capabilities of large language models (LLMs)\nwithin a coordinated multi-agent framework. Unlike prior approaches that\nloosely couple symbolic and neural modules, our design embeds decision trees\nand random forests as callable oracles within a unified reasoning system.\nTree-based modules enable interpretable rule inference and causal logic, while\nLLM agents handle abductive reasoning, generalization, and interactive\nplanning. A central orchestrator maintains belief state consistency and\nmediates communication across agents and external tools, enabling reasoning\nover both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On\n\\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through\nlogic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in\nmultistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it\nboosts abstraction accuracy by +6.0\\% through integration of symbolic oracles.\nApplications in clinical decision support and scientific discovery show how the\nsystem encodes domain rules symbolically while leveraging LLMs for contextual\ninference and hypothesis generation. This architecture offers a robust,\ninterpretable, and extensible solution for general-purpose neuro-symbolic\nreasoning.\n", "link": "http://arxiv.org/abs/2508.05311v1", "date": "2025-08-07", "relevancy": 1.5055, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5192}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5153}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Architecture%20for%20Symbolic%20Reasoning%20with%20Decision%20Trees%20and%20LLM%0A%20%20Agents&body=Title%3A%20A%20Novel%20Architecture%20for%20Symbolic%20Reasoning%20with%20Decision%20Trees%20and%20LLM%0A%20%20Agents%0AAuthor%3A%20Andrew%20Kiruluta%0AAbstract%3A%20%20%20We%20propose%20a%20hybrid%20architecture%20that%20integrates%20decision%20tree-based%20symbolic%0Areasoning%20with%20the%20generative%20capabilities%20of%20large%20language%20models%20%28LLMs%29%0Awithin%20a%20coordinated%20multi-agent%20framework.%20Unlike%20prior%20approaches%20that%0Aloosely%20couple%20symbolic%20and%20neural%20modules%2C%20our%20design%20embeds%20decision%20trees%0Aand%20random%20forests%20as%20callable%20oracles%20within%20a%20unified%20reasoning%20system.%0ATree-based%20modules%20enable%20interpretable%20rule%20inference%20and%20causal%20logic%2C%20while%0ALLM%20agents%20handle%20abductive%20reasoning%2C%20generalization%2C%20and%20interactive%0Aplanning.%20A%20central%20orchestrator%20maintains%20belief%20state%20consistency%20and%0Amediates%20communication%20across%20agents%20and%20external%20tools%2C%20enabling%20reasoning%0Aover%20both%20structured%20and%20unstructured%20inputs.%0A%20%20The%20system%20achieves%20strong%20performance%20on%20reasoning%20benchmarks.%20On%0A%5Ctextit%7BProofWriter%7D%2C%20it%20improves%20entailment%20consistency%20by%20%2B7.2%5C%25%20through%0Alogic-grounded%20tree%20validation.%20On%20GSM8k%2C%20it%20achieves%20%2B5.3%5C%25%20accuracy%20gains%20in%0Amultistep%20mathematical%20problems%20via%20symbolic%20augmentation.%20On%20%5Ctextit%7BARC%7D%2C%20it%0Aboosts%20abstraction%20accuracy%20by%20%2B6.0%5C%25%20through%20integration%20of%20symbolic%20oracles.%0AApplications%20in%20clinical%20decision%20support%20and%20scientific%20discovery%20show%20how%20the%0Asystem%20encodes%20domain%20rules%20symbolically%20while%20leveraging%20LLMs%20for%20contextual%0Ainference%20and%20hypothesis%20generation.%20This%20architecture%20offers%20a%20robust%2C%0Ainterpretable%2C%20and%20extensible%20solution%20for%20general-purpose%20neuro-symbolic%0Areasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Architecture%2520for%2520Symbolic%2520Reasoning%2520with%2520Decision%2520Trees%2520and%2520LLM%250A%2520%2520Agents%26entry.906535625%3DAndrew%2520Kiruluta%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520hybrid%2520architecture%2520that%2520integrates%2520decision%2520tree-based%2520symbolic%250Areasoning%2520with%2520the%2520generative%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%250Awithin%2520a%2520coordinated%2520multi-agent%2520framework.%2520Unlike%2520prior%2520approaches%2520that%250Aloosely%2520couple%2520symbolic%2520and%2520neural%2520modules%252C%2520our%2520design%2520embeds%2520decision%2520trees%250Aand%2520random%2520forests%2520as%2520callable%2520oracles%2520within%2520a%2520unified%2520reasoning%2520system.%250ATree-based%2520modules%2520enable%2520interpretable%2520rule%2520inference%2520and%2520causal%2520logic%252C%2520while%250ALLM%2520agents%2520handle%2520abductive%2520reasoning%252C%2520generalization%252C%2520and%2520interactive%250Aplanning.%2520A%2520central%2520orchestrator%2520maintains%2520belief%2520state%2520consistency%2520and%250Amediates%2520communication%2520across%2520agents%2520and%2520external%2520tools%252C%2520enabling%2520reasoning%250Aover%2520both%2520structured%2520and%2520unstructured%2520inputs.%250A%2520%2520The%2520system%2520achieves%2520strong%2520performance%2520on%2520reasoning%2520benchmarks.%2520On%250A%255Ctextit%257BProofWriter%257D%252C%2520it%2520improves%2520entailment%2520consistency%2520by%2520%252B7.2%255C%2525%2520through%250Alogic-grounded%2520tree%2520validation.%2520On%2520GSM8k%252C%2520it%2520achieves%2520%252B5.3%255C%2525%2520accuracy%2520gains%2520in%250Amultistep%2520mathematical%2520problems%2520via%2520symbolic%2520augmentation.%2520On%2520%255Ctextit%257BARC%257D%252C%2520it%250Aboosts%2520abstraction%2520accuracy%2520by%2520%252B6.0%255C%2525%2520through%2520integration%2520of%2520symbolic%2520oracles.%250AApplications%2520in%2520clinical%2520decision%2520support%2520and%2520scientific%2520discovery%2520show%2520how%2520the%250Asystem%2520encodes%2520domain%2520rules%2520symbolically%2520while%2520leveraging%2520LLMs%2520for%2520contextual%250Ainference%2520and%2520hypothesis%2520generation.%2520This%2520architecture%2520offers%2520a%2520robust%252C%250Ainterpretable%252C%2520and%2520extensible%2520solution%2520for%2520general-purpose%2520neuro-symbolic%250Areasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Architecture%20for%20Symbolic%20Reasoning%20with%20Decision%20Trees%20and%20LLM%0A%20%20Agents&entry.906535625=Andrew%20Kiruluta&entry.1292438233=%20%20We%20propose%20a%20hybrid%20architecture%20that%20integrates%20decision%20tree-based%20symbolic%0Areasoning%20with%20the%20generative%20capabilities%20of%20large%20language%20models%20%28LLMs%29%0Awithin%20a%20coordinated%20multi-agent%20framework.%20Unlike%20prior%20approaches%20that%0Aloosely%20couple%20symbolic%20and%20neural%20modules%2C%20our%20design%20embeds%20decision%20trees%0Aand%20random%20forests%20as%20callable%20oracles%20within%20a%20unified%20reasoning%20system.%0ATree-based%20modules%20enable%20interpretable%20rule%20inference%20and%20causal%20logic%2C%20while%0ALLM%20agents%20handle%20abductive%20reasoning%2C%20generalization%2C%20and%20interactive%0Aplanning.%20A%20central%20orchestrator%20maintains%20belief%20state%20consistency%20and%0Amediates%20communication%20across%20agents%20and%20external%20tools%2C%20enabling%20reasoning%0Aover%20both%20structured%20and%20unstructured%20inputs.%0A%20%20The%20system%20achieves%20strong%20performance%20on%20reasoning%20benchmarks.%20On%0A%5Ctextit%7BProofWriter%7D%2C%20it%20improves%20entailment%20consistency%20by%20%2B7.2%5C%25%20through%0Alogic-grounded%20tree%20validation.%20On%20GSM8k%2C%20it%20achieves%20%2B5.3%5C%25%20accuracy%20gains%20in%0Amultistep%20mathematical%20problems%20via%20symbolic%20augmentation.%20On%20%5Ctextit%7BARC%7D%2C%20it%0Aboosts%20abstraction%20accuracy%20by%20%2B6.0%5C%25%20through%20integration%20of%20symbolic%20oracles.%0AApplications%20in%20clinical%20decision%20support%20and%20scientific%20discovery%20show%20how%20the%0Asystem%20encodes%20domain%20rules%20symbolically%20while%20leveraging%20LLMs%20for%20contextual%0Ainference%20and%20hypothesis%20generation.%20This%20architecture%20offers%20a%20robust%2C%0Ainterpretable%2C%20and%20extensible%20solution%20for%20general-purpose%20neuro-symbolic%0Areasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05311v1&entry.124074799=Read"},
{"title": "Online Sparsification of Bipartite-Like Clusters in Graphs", "author": "Joyentanuj Das and Suranjan De and He Sun", "abstract": "  Graph clustering is an important algorithmic technique for analysing massive\ngraphs, and has been widely applied in many research fields of data science.\nWhile the objective of most graph clustering algorithms is to find a vertex set\nof low conductance, a sequence of recent studies highlights the importance of\nthe inter-connection between vertex sets when analysing real-world datasets.\nFollowing this line of research, in this work we study bipartite-like clusters\nand present efficient and online sparsification algorithms that find such\nclusters in both undirected graphs and directed ones. We conduct experimental\nstudies on both synthetic and real-world datasets, and show that our algorithms\nsignificantly speedup the running time of existing clustering algorithms while\npreserving their effectiveness.\n", "link": "http://arxiv.org/abs/2508.05437v1", "date": "2025-08-07", "relevancy": 1.6311, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4162}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4029}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Sparsification%20of%20Bipartite-Like%20Clusters%20in%20Graphs&body=Title%3A%20Online%20Sparsification%20of%20Bipartite-Like%20Clusters%20in%20Graphs%0AAuthor%3A%20Joyentanuj%20Das%20and%20Suranjan%20De%20and%20He%20Sun%0AAbstract%3A%20%20%20Graph%20clustering%20is%20an%20important%20algorithmic%20technique%20for%20analysing%20massive%0Agraphs%2C%20and%20has%20been%20widely%20applied%20in%20many%20research%20fields%20of%20data%20science.%0AWhile%20the%20objective%20of%20most%20graph%20clustering%20algorithms%20is%20to%20find%20a%20vertex%20set%0Aof%20low%20conductance%2C%20a%20sequence%20of%20recent%20studies%20highlights%20the%20importance%20of%0Athe%20inter-connection%20between%20vertex%20sets%20when%20analysing%20real-world%20datasets.%0AFollowing%20this%20line%20of%20research%2C%20in%20this%20work%20we%20study%20bipartite-like%20clusters%0Aand%20present%20efficient%20and%20online%20sparsification%20algorithms%20that%20find%20such%0Aclusters%20in%20both%20undirected%20graphs%20and%20directed%20ones.%20We%20conduct%20experimental%0Astudies%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20and%20show%20that%20our%20algorithms%0Asignificantly%20speedup%20the%20running%20time%20of%20existing%20clustering%20algorithms%20while%0Apreserving%20their%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Sparsification%2520of%2520Bipartite-Like%2520Clusters%2520in%2520Graphs%26entry.906535625%3DJoyentanuj%2520Das%2520and%2520Suranjan%2520De%2520and%2520He%2520Sun%26entry.1292438233%3D%2520%2520Graph%2520clustering%2520is%2520an%2520important%2520algorithmic%2520technique%2520for%2520analysing%2520massive%250Agraphs%252C%2520and%2520has%2520been%2520widely%2520applied%2520in%2520many%2520research%2520fields%2520of%2520data%2520science.%250AWhile%2520the%2520objective%2520of%2520most%2520graph%2520clustering%2520algorithms%2520is%2520to%2520find%2520a%2520vertex%2520set%250Aof%2520low%2520conductance%252C%2520a%2520sequence%2520of%2520recent%2520studies%2520highlights%2520the%2520importance%2520of%250Athe%2520inter-connection%2520between%2520vertex%2520sets%2520when%2520analysing%2520real-world%2520datasets.%250AFollowing%2520this%2520line%2520of%2520research%252C%2520in%2520this%2520work%2520we%2520study%2520bipartite-like%2520clusters%250Aand%2520present%2520efficient%2520and%2520online%2520sparsification%2520algorithms%2520that%2520find%2520such%250Aclusters%2520in%2520both%2520undirected%2520graphs%2520and%2520directed%2520ones.%2520We%2520conduct%2520experimental%250Astudies%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%2520and%2520show%2520that%2520our%2520algorithms%250Asignificantly%2520speedup%2520the%2520running%2520time%2520of%2520existing%2520clustering%2520algorithms%2520while%250Apreserving%2520their%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Sparsification%20of%20Bipartite-Like%20Clusters%20in%20Graphs&entry.906535625=Joyentanuj%20Das%20and%20Suranjan%20De%20and%20He%20Sun&entry.1292438233=%20%20Graph%20clustering%20is%20an%20important%20algorithmic%20technique%20for%20analysing%20massive%0Agraphs%2C%20and%20has%20been%20widely%20applied%20in%20many%20research%20fields%20of%20data%20science.%0AWhile%20the%20objective%20of%20most%20graph%20clustering%20algorithms%20is%20to%20find%20a%20vertex%20set%0Aof%20low%20conductance%2C%20a%20sequence%20of%20recent%20studies%20highlights%20the%20importance%20of%0Athe%20inter-connection%20between%20vertex%20sets%20when%20analysing%20real-world%20datasets.%0AFollowing%20this%20line%20of%20research%2C%20in%20this%20work%20we%20study%20bipartite-like%20clusters%0Aand%20present%20efficient%20and%20online%20sparsification%20algorithms%20that%20find%20such%0Aclusters%20in%20both%20undirected%20graphs%20and%20directed%20ones.%20We%20conduct%20experimental%0Astudies%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20and%20show%20that%20our%20algorithms%0Asignificantly%20speedup%20the%20running%20time%20of%20existing%20clustering%20algorithms%20while%0Apreserving%20their%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05437v1&entry.124074799=Read"},
{"title": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource\n  Language Constraints", "author": "Zhong Ken Hew and Jia Xin Low and Sze Jue Yang and Chee Seng chan", "abstract": "  Large Language Models (LLMs) often exhibit cultural biases due to training\ndata dominated by high-resource languages like English and Chinese. This poses\nchallenges for accurately representing and evaluating diverse cultural\ncontexts, particularly in low-resource language settings. To address this, we\nintroduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on\nMalaysian culture across six pillars: arts, attire, customs, entertainment,\nfood, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,\nMyCulture employs a novel open-ended multiple-choice question format without\npredefined options, thereby reducing guessing and mitigating format bias. We\nprovide a theoretical justification for the effectiveness of this open-ended\nstructure in improving both fairness and discriminative power. Furthermore, we\nanalyze structural bias by comparing model performance on structured versus\nfree-form outputs, and assess language bias through multilingual prompt\nvariations. Our evaluation across a range of regional and international LLMs\nreveals significant disparities in cultural comprehension, highlighting the\nurgent need for culturally grounded and linguistically inclusive benchmarks in\nthe development and assessment of LLMs.\n", "link": "http://arxiv.org/abs/2508.05429v1", "date": "2025-08-07", "relevancy": 1.7572, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4426}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MyCulture%3A%20Exploring%20Malaysia%27s%20Diverse%20Culture%20under%20Low-Resource%0A%20%20Language%20Constraints&body=Title%3A%20MyCulture%3A%20Exploring%20Malaysia%27s%20Diverse%20Culture%20under%20Low-Resource%0A%20%20Language%20Constraints%0AAuthor%3A%20Zhong%20Ken%20Hew%20and%20Jia%20Xin%20Low%20and%20Sze%20Jue%20Yang%20and%20Chee%20Seng%20chan%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20often%20exhibit%20cultural%20biases%20due%20to%20training%0Adata%20dominated%20by%20high-resource%20languages%20like%20English%20and%20Chinese.%20This%20poses%0Achallenges%20for%20accurately%20representing%20and%20evaluating%20diverse%20cultural%0Acontexts%2C%20particularly%20in%20low-resource%20language%20settings.%20To%20address%20this%2C%20we%0Aintroduce%20MyCulture%2C%20a%20benchmark%20designed%20to%20comprehensively%20evaluate%20LLMs%20on%0AMalaysian%20culture%20across%20six%20pillars%3A%20arts%2C%20attire%2C%20customs%2C%20entertainment%2C%0Afood%2C%20and%20religion%20presented%20in%20Bahasa%20Melayu.%20Unlike%20conventional%20benchmarks%2C%0AMyCulture%20employs%20a%20novel%20open-ended%20multiple-choice%20question%20format%20without%0Apredefined%20options%2C%20thereby%20reducing%20guessing%20and%20mitigating%20format%20bias.%20We%0Aprovide%20a%20theoretical%20justification%20for%20the%20effectiveness%20of%20this%20open-ended%0Astructure%20in%20improving%20both%20fairness%20and%20discriminative%20power.%20Furthermore%2C%20we%0Aanalyze%20structural%20bias%20by%20comparing%20model%20performance%20on%20structured%20versus%0Afree-form%20outputs%2C%20and%20assess%20language%20bias%20through%20multilingual%20prompt%0Avariations.%20Our%20evaluation%20across%20a%20range%20of%20regional%20and%20international%20LLMs%0Areveals%20significant%20disparities%20in%20cultural%20comprehension%2C%20highlighting%20the%0Aurgent%20need%20for%20culturally%20grounded%20and%20linguistically%20inclusive%20benchmarks%20in%0Athe%20development%20and%20assessment%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMyCulture%253A%2520Exploring%2520Malaysia%2527s%2520Diverse%2520Culture%2520under%2520Low-Resource%250A%2520%2520Language%2520Constraints%26entry.906535625%3DZhong%2520Ken%2520Hew%2520and%2520Jia%2520Xin%2520Low%2520and%2520Sze%2520Jue%2520Yang%2520and%2520Chee%2520Seng%2520chan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520exhibit%2520cultural%2520biases%2520due%2520to%2520training%250Adata%2520dominated%2520by%2520high-resource%2520languages%2520like%2520English%2520and%2520Chinese.%2520This%2520poses%250Achallenges%2520for%2520accurately%2520representing%2520and%2520evaluating%2520diverse%2520cultural%250Acontexts%252C%2520particularly%2520in%2520low-resource%2520language%2520settings.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520MyCulture%252C%2520a%2520benchmark%2520designed%2520to%2520comprehensively%2520evaluate%2520LLMs%2520on%250AMalaysian%2520culture%2520across%2520six%2520pillars%253A%2520arts%252C%2520attire%252C%2520customs%252C%2520entertainment%252C%250Afood%252C%2520and%2520religion%2520presented%2520in%2520Bahasa%2520Melayu.%2520Unlike%2520conventional%2520benchmarks%252C%250AMyCulture%2520employs%2520a%2520novel%2520open-ended%2520multiple-choice%2520question%2520format%2520without%250Apredefined%2520options%252C%2520thereby%2520reducing%2520guessing%2520and%2520mitigating%2520format%2520bias.%2520We%250Aprovide%2520a%2520theoretical%2520justification%2520for%2520the%2520effectiveness%2520of%2520this%2520open-ended%250Astructure%2520in%2520improving%2520both%2520fairness%2520and%2520discriminative%2520power.%2520Furthermore%252C%2520we%250Aanalyze%2520structural%2520bias%2520by%2520comparing%2520model%2520performance%2520on%2520structured%2520versus%250Afree-form%2520outputs%252C%2520and%2520assess%2520language%2520bias%2520through%2520multilingual%2520prompt%250Avariations.%2520Our%2520evaluation%2520across%2520a%2520range%2520of%2520regional%2520and%2520international%2520LLMs%250Areveals%2520significant%2520disparities%2520in%2520cultural%2520comprehension%252C%2520highlighting%2520the%250Aurgent%2520need%2520for%2520culturally%2520grounded%2520and%2520linguistically%2520inclusive%2520benchmarks%2520in%250Athe%2520development%2520and%2520assessment%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MyCulture%3A%20Exploring%20Malaysia%27s%20Diverse%20Culture%20under%20Low-Resource%0A%20%20Language%20Constraints&entry.906535625=Zhong%20Ken%20Hew%20and%20Jia%20Xin%20Low%20and%20Sze%20Jue%20Yang%20and%20Chee%20Seng%20chan&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20often%20exhibit%20cultural%20biases%20due%20to%20training%0Adata%20dominated%20by%20high-resource%20languages%20like%20English%20and%20Chinese.%20This%20poses%0Achallenges%20for%20accurately%20representing%20and%20evaluating%20diverse%20cultural%0Acontexts%2C%20particularly%20in%20low-resource%20language%20settings.%20To%20address%20this%2C%20we%0Aintroduce%20MyCulture%2C%20a%20benchmark%20designed%20to%20comprehensively%20evaluate%20LLMs%20on%0AMalaysian%20culture%20across%20six%20pillars%3A%20arts%2C%20attire%2C%20customs%2C%20entertainment%2C%0Afood%2C%20and%20religion%20presented%20in%20Bahasa%20Melayu.%20Unlike%20conventional%20benchmarks%2C%0AMyCulture%20employs%20a%20novel%20open-ended%20multiple-choice%20question%20format%20without%0Apredefined%20options%2C%20thereby%20reducing%20guessing%20and%20mitigating%20format%20bias.%20We%0Aprovide%20a%20theoretical%20justification%20for%20the%20effectiveness%20of%20this%20open-ended%0Astructure%20in%20improving%20both%20fairness%20and%20discriminative%20power.%20Furthermore%2C%20we%0Aanalyze%20structural%20bias%20by%20comparing%20model%20performance%20on%20structured%20versus%0Afree-form%20outputs%2C%20and%20assess%20language%20bias%20through%20multilingual%20prompt%0Avariations.%20Our%20evaluation%20across%20a%20range%20of%20regional%20and%20international%20LLMs%0Areveals%20significant%20disparities%20in%20cultural%20comprehension%2C%20highlighting%20the%0Aurgent%20need%20for%20culturally%20grounded%20and%20linguistically%20inclusive%20benchmarks%20in%0Athe%20development%20and%20assessment%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05429v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


